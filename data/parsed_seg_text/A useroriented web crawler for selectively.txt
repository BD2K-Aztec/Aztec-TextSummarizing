Motivation: Life stories of diseased and healthy individuals are abundantly available on the Internet. Collecting and mining such online content can offer many valuable insights into patients physical and emotional states throughout the pre diagnosis diagnosis, treatment and post-treatment stages of the disease compared with those of healthy subjects. However, such content is widely dispersed across the web. Using traditional query based search engines to manually collect relevant materials is rather labor intensive and often incomplete due to resource constraints in terms of human query composition and result parsing efforts. The alternative option, blindly crawling the whole web, has proven inefficient and unaffordable for e health researchers. Results: We propose a user oriented web crawler that adaptively acquires user desired content on the Internet to meet the specific online data source acquisition needs of e health researchers. Experimental results on two cancer related case studies show that the new crawler can substantially accelerate the acquisition of highly relevant online content compared with the existing state of the art adaptive web crawling technology. For the breast cancer case study using the full training set, the new method achieves a cumulative precision between 74.7 and 79.4% after 5 h of execution till the end of the 20-h long crawling session as compared with the cumulative precision between 32.8 and 37.0% using the peer method for the same time period. For the lung cancer case study using the full training set, the new method achieves a cumulative precision between 56.7 and 61.2% after 5 h of execution till the end of the 20-h long crawling session as compared with the cumulative precision between 29.3 and 32.4% using the peer method. Using the reduced training set in the breast cancer case study, the cumulative precision of our method is between 44.6 and 54.9%, whereas the cumulative precision of the peer method is between 24.3 and 26.3%; for the lung cancer case study using the reduced training set, the cumulative precisions of our method and the peer method are, respectively, between 35.7 and 46.7% versus between 24.1 and 29.6%. These numbers clearly show a consistently superior accuracy of our method in discovering and acquiring user desired online content for e health research. Availability and implementation: The implementation of our user oriented web crawler is freely available to non-commercial users via the following Web site: http://bsec.ornl.gov/AdaptiveCrawler.shtml. The Web site provides a step by step guide on how to execute the web crawler implementation. In addition, the Web site provides the two study datasets including manually labeled ground truth, initial seeds and the crawling results reported in this article.

introduction the Internet carries abundant and ever enriching user generated content on a wide range of social, cultural, political and other topics. Life stories of patients are no exception to this trend. Collecting and mining such personal content can offer many valuable insights on patients' experiences with respect to disease symptoms and progression, treatment management, side effects and effectiveness, as well as many additional factors and aspects of a patient's physical and emotional states throughout the whole disease cycle. The breadth and depth of understanding attainable through mining this voluntarily contributed web content would be extremely expensive and time consuming to capture via traditional data collection mechanisms used in clinical studies. Despite the merits and rich availability of user generated patient content on the Internet, collecting such information using conventional query based web search is labor intensive for the following two reasons. First, it is not clear what are the right queries to use to retrieve the desired content accurately and comprehensively. For example, a general query such as 'breast cancer stories' would pull up over 182 million results using Google web search wherein only a selected portion, usually small (such as 50:1%), of the whole search result set may meet the researcher's specific needs. Manually examining and selecting the qualified search results require extensive human effort. Second, clinical researchers have specific requirements regarding the user generated disease content they need to collect. query based search engines can not always support such requirements. Let us assume that a researcher wants to collect the personal stories of two groups of female breast cancer patients, those who have had children and those who have not. With much manual effort, the researcher might be able to obtain some stories of the first group, but so far no off the shelf general purpose search engine that we are aware of allows users to retrieve information that does not carry undesirable content (i.e. the support of negative queries). Given the steadily growing volume of patient generated disease specific online content, it is highly desirable to minimize *To whom correspondence should be addressed. y The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors. the manual intervention involved in source acquisition and subsequent mining processes. Although an extensive collection of automatic or largely automatic text mining algorithms and tools exists for analyzing social media content, limited efforts have been dedicated to developing automatic or largely automatic content acquisition tools and methods for obtaining online patient generated content meeting certain e health research needs and requirements. To meet this challenge in the e health research community as well as the broader bioinformatics communities, we propose a user oriented web crawler, which can acquire user generated content satisfying particular content requirements with minimum intervention. We use cancer as the case study to demonstrate the value and impact of the proposed web crawling technology.

discussion comparing the design of our newly proposed adaptive crawler with that of the peer method presented in Barbosa and Freire (2007), there exist four key aspects of similarities between the two approaches as follows i both methods are designed with an online learning capability, which automatically learns on the fly to capture characteristics of promising crawling paths or destination web regions that lead to the most rewarding discoveries of online content relevant to the current crawling needs,; (ii) both methods are equipped with a randomized link visitation strategy where the likelihood of selecting a certain link from the candidate URL pool for visit in the next crawling step is probabilistically modulated by the estimated reward that the link can lead to the discovery of relevant content, (iii) both methods are equipped with some self assessment and self critic module that can autonomously determine the relevance of any harvested web page with respect to the crawling needs for selective output of crawling results and (iv) both methods extract text features and select the most reliable subset of candidate features to construct the predictive estimator on the crawling utility of a link the key differences between the two methods include (i) to forecast the utility of a link u i , our method introduces a light weighted learner wpu i ,  that predicts the utility score before crawling the web content pointed to by u i according to the information provided by the snippet associated with u i. Such snippet is always available for search results returned by a typical search engine (such as Google and Yahoo). The snippet includes u i 's URL, a running head text of u i and a brief piece of selected text from the search result associated with u i. In comparison, the adaptive link learner introduced in the peer method only examines the text information encoded in a link's URL when performing the link utility estimation. The extended scope of textual information available from the snippet of a link allows our predicting function to be able to estimate the link utility more accurately (as confirmed by the experimental results), which results in better accuracy in targeted web crawling. (ii) In addition to the light weighted learner wpu i , , our method also carries a more powerful web page utility assessment function wp,  that measures the utility of a web page wp with respect to the information need after the web page is crawled. Based on the output of the function wp, , we dynamically retrain the function of wpu i ,  so that the particular machine learning model selection and configuration are optimized on the fly according to all the cumulative quality assessment scores produced by the function wp,  since the beginning of the current crawling session. This novel two tier online learning framework, which was not present in the peer method, allows our method to be able to train a more tailored and task optimized link utility predictor wpu i ,  in an autonomous fashion. (iii) The peer both crawling methods were initialized using the same set of labeled samples. To explore the influence of the training sample size, we randomly selected a subset of the available positive and negative training samples and repeated the aforementioned comparable analysis (c and d) method uses a specialized classifier for recognizing online searchable forms as a critic to judge the relevance of their online crawling results. This feedback mechanism is only applicable for the particular crawling needs to discover web pages of searchable forms as hidden entries to online databases. In contrast, when assessing the utility of a crawled online content web page, our method comprehensively considers the content words in the main body of an HTML file, words in the heading and subtitles of an HTML file and the anchor text embedded in an HTML file. Benefited by this more generic design of the self assessment mechanism, which is coupled by a corresponding more advanced text feature extraction, selection and predictive modeling implementation, our adaptive crawler is able to detect online content meeting a much wider spectrum of users' needs for a more generic scope of applications. (iv) Our new crawler design explicitly models the confidence and reliability of its link utility prediction performance during the execution of online web crawling and considers such uncertainty during its planning of adaptive strategies for the current crawling task. This uncertainty modeling feature is missing from the design of the peer method. (v) Our crawler design explicitly models the time required to access a given web page for balancing the time spent between developing more carefully planned crawling strategies versus executing more operations of web page harvesting with less deliberated crawling strategies. Such feature is also absent from the design of the peer method. To better understand the behavior characteristics of the two crawling methods, we conducted some further investigative analysis to comparatively examine crawling results obtained by each crawler for the two case studies reported in this article. First, we examined the overlap between crawling results harvested by the two crawlers using three metrics, which assess the amount of common content between two crawling result sets S 1 and S 2 on the levels of key words, documents and sites, respectively. For the key word level overlap between S 1 and S 2 , we first extracted the key words from each result set using the RAKE algorithm (). Next, we ranked the two lists of key words according to each key word's term frequency inverse document frequency value among its corresponding crawling result set. We then counted the number of common key words included in the top ranked key word lists of S 1 and S 2 as the key word level overlap between S 1 and S 2 .reports results of the estimated key word level overlap between the two sets of crawling results where both crawlers are trained with the full set of example search results shows that the key word level overlap remains roughly in the same value range (between 60 and 80%) even when estimated using different sizes of top ranked key words. We then chose a representative window size (300) for the top ranked key words and estimated the key word level overlap throughout the entire crawling sessions. The results are shown in, according to which we can see the overlap between the two crawling result sets also stably remains within the same value range of 60 and 80% during the progression of the crawling processes. For the document level overlap, we use the cosine distance to measure pairwise document similarity. We then estimated the overlap between S 1 and S 2 as the the peer method appears to collect online content richer in medical terms, whereas the adaptive crawler appears to be able to harvest cancer patient stories that expose more abundant details in lifestyle and emotionally related matters. It is noted the latter type of crawling results acquired by our crawler agrees better with the true information collection needs of e health researchers in both case studies one about gathering life stories of breast cancer patients and the other about smoking of lung cancer patients. We speculate this better alignment of crawling results with researchers' information acquisition needs is achieved by the more content sensitive adaptive crawling mechanism of our new crawler, benefited by its more advanced selective online content detection and acquisition algorithm proposed in this article.

conclusion we propose a user oriented web crawler that can adaptively acquire social media content on the Internet to meet the specific online data source acquisition needs of the end user. We evaluated the new crawler in the context of cancer epidemiological research with two case studies. Experimental results show that the new crawler can substantially accelerate the online user generated content acquisition efforts for cancer researchers than using the existing state of the art adaptive web crawling technology.
