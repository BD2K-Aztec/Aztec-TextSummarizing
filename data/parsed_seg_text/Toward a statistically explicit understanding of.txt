Motivation: Draft de novo genome assemblies are now available for many organisms. These assemblies are point estimates of the true genome sequences. Each is a specific hypothesis, drawn from among many alternative hypotheses, of the sequence of a genome. Assembly uncertainty, the inability to distinguish between multiple alternative assembly hypotheses, can be due to real variation between copies of the genome in the sample, errors and ambiguities in the sequenced data and assumptions and heuristics of the assemblers. Most assemblers select a single assembly according to ad hoc criteria, and do not yet report and quantify the uncertainty of their outputs. Those assemblers that do report uncertainty take different approaches to describing multiple assembly hypotheses and the support for each. Results: Here we review and examine the problem of representing and measuring uncertainty in assemblies. A promising recent development is the implementation of assemblers that are built according to explicit statistical models. Some new assembly methods, for example, estimate and maximize assembly likelihood. These advances, combined with technical advances in the representation of alternative assembly hypotheses, will lead to a more complete and biologically relevant understanding of assembly uncertainty. This will in turn facilitate the interpretation of downstream analyses and tests of specific biological hypotheses.

introduction the low cost and increasing availability of next generation sequencing data have driven a growing interest in methods and software tools for de novo genome assembly of short read sequences. Recent surveys of assembly tools (), practical guides (), competitions like the assemb lath on () and benchmarking tools like GAGE () highlight the diverse ecosystem of available assemblers. New data structures, algorithms and software tools for assembly continue to be published every month. Many investigators have claimed that it is now possible to assemble high quality genomes from next generation sequencing data when using appropriate protocols and assembly methods (). Yet, others have expressed concern over the integrity of publicly available draft genomes assembled from such data. Some have described errors and shortcomings in specific draft assemblies (), whereas others have questioned the quality of publicly available draft assemblies in general, and advocated better quality standards for the community (). In particular, the assemb lath on 2 competition () found large scale inconsistencies among current assembly methods, suggesting they are not robust to changes in parameters and input data, and that there is a need for unambiguous measures of assembly uncertainty. A genome assembly is a hypothesis consisting of a collection of contigs (contiguous sequences) and scaffolds (groups of contigs with gaps of known length between them) that typically cover 90% or more of the genome (), but are often fragmented and unordered. Current de novo assemblers use various heuristics and algorithms to select an assembly that optimizes some criteria, such as path length or graph complexity (); however, these optimization criteria are typically ad hoc. This is largely because of the computational difficulty of performing assembly on short reads, and a primary goal for existing assembly methods has been computational tractability and efficiency. As a result, assemblers choose a single point estimate as their final output with sparse information about the quality, certainty or validity of the chosen assembly, or of alternative assembly hypotheses (many of which may have almost as much support). In most cases, it is difficult, if not impossible, to answer even basic questions like, 'How well is this contig supported by the read sequences?' or 'Are there alternative assemblies that have similar support from the data?' Downstream analysis tools use assemblies to make their own point estimates of other aspects of biology, such as multiple sequence alignments, differential gene expression analyses or phylogenetic trees. In the end, there is no accounting for how the uncertainty is compounded at each stage. Existing tools can not be integrated into pipelines that propagate uncertainty through a large multistep analysis, for example integrating assembly uncertainty with tree uncertainty when constructing phylogenies. The ability to propagate uncertainty about point estimates or, preferably, to propagate entire sets of multiple alternative hypotheses will become increasingly important as analyses grow in complexity. *To whom correspondence should be addressed thanks to the progress on computational efficiency of genome assembly, it is now possible to tackle the difficult goal of placing de novo sequence assembly within an explicit statistical framework. In such a framework, single assembly hypotheses selected according to ad hoc optimality criteria are replaced by sets of hypotheses accompanied by statistics that summarize confidence in each.

conclusion the pieces are now falling in place for assembly to move away from point estimates that are selected according to ad hoc criteria, toward a statistically explicit framework that provides not only biologically relevant measures of certainty but also sets of alternative hypotheses. This will greatly facilitate the evaluation of assemblies, their application to specific biological questions, improvements in assembly algorithms and integration with downstream analyses that can then take assembly uncertainty into account. Bioinformatics workflow frameworks, such as the web based framework Galaxy () and the lightweight command line framework bio lite (), already provide biologists with functionality for establishing provenance and reproducibility for computational analyses. These workflow frameworks are the logical foundation for implementing pipelines that propagate uncertainty through complex multistage analyses.
