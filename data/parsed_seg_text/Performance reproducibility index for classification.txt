Motivation: A common practice in biomarker discovery is to decide whether a large laboratory experiment should be carried out based on the results of a preliminary study on a small set of specimens. Consideration of the efficacy of this approach motivates the introduction of a probabilistic measure, for whether a classifier showing promising results in a small sample preliminary study will perform similarly on a large independent sample. Given the error estimate from the preliminary study, if the probability of reproducible error is low, then there is really no purpose in substantially allocating more resources to a large follow on study. Indeed, if the probability of the preliminary study providing likely reproducible results is small, then why even perform the preliminary study? Results: This article introduces a reproducibility index for classification , measuring the probability that a sufficiently small error estimate on a small sample will motivate a large follow on study. We provide a simulation study based on synthetic distribution models that possess known intrinsic classification difficulties and emulate real world scenarios. We also set up similar simulations on four real datasets to show the consistency of results. The reproducibility indices for different dis-tributional models, real datasets and classification schemes are empirically calculated. The effects of reporting and multiple rule biases on the reproducibility index are also analyzed. Availability: We have implemented in C code the synthetic data distribution model, classification rules, feature selection routine and error estimation methods. The source code is available at http://gsp.tamu edu publications supplementary you sefi12a. Supplementary simulation results are also included.

introduction perhaps no problem in translational genomics has received more attention than the discovery of biomarkers for phenotypic discrimination. To date, there has been little success in developing clinically useful biomarkers and much has been said concerning the lack of reproducibility in biomarker discovery (). In particular, recently a report concerning comments made by US Food and Drug Administration (FDA) drug division head Janet Woodcock stated: Janet Woodcock, drug division head at the FDA, this week expressed cautious optimism for the future of personalized drug development, noting that 'we may be out of the general skepticism phase, but we're in the long slog phase.. .'. The 'major barrier' to personalized medicine, as Woodcock sees it is 'coming up with the right diagnostics'. The reason for this problem is the dearth of valid biomarkers linked to disease prognosis and drug response. Based on conversations Woodcock has had with genomics researchers, she estimated that as much as 75% of published biomarker associations are not replicable. 'This poses a huge challenge for industry in biomarker identification and diagnostics development', she said (). Evaluating the consistency of biomarker discoveries across different platforms, experiments and datasets has attracted the attention of researchers. The studies addressing this issue mainly revolve around the reproducibility of signals (for example, lists of differentially expressed genes), their significance scores and rankings in a prepared list. They try to answer the following question: Do the same genes appear differentially expressed when the experiment is rerun and the references therein suggest several solutions to this and related questions. Our interest is different. A prototypical reproducibility paradigm arises when a classifier is designed on a preliminary study based on a small sample, and, based on promising reported results, a follow on study is performed using a large independent data sample to check whether the classifier performs well as reported in the preliminary study. Many issues affect reproducibility, including the measurement platform, specimen handling, data normalization and sample compatibility between the original and subsequent studies. These may be categorized as laboratory issues; note that here we are not talking about the issue of providing access to data and software for follow-up studies on published results (). One can conjecture mitigation of these issues as laboratory technique improves; however, there is a more fundamental methodological issue, namely, error estimation. In particular, inaccurate error estimation can lead to over optimism in reported results ().The typical analysis proceeds in the following fashion: (i) based on the data, a feature set is chosen from the 30 000; (ii) a classifier is designed, with feature selection perhaps being performed in conjunction with classifier design and (iii) the classification error is measured by some procedure using the same sample data upon which feature selection and classifier design have been performed. Given no lack of reproducibility owing to laboratory issues, if the error estimate is sufficiently deemed small and a follow on study with 1000 independent data specimens is carried out, can we expect the preliminary error estimate on a sample of 50 to be reproduced on a test sample of size 1000? Since the root mean square (RMS) error between the true and estimated errors for independent test data error estimation is bounded by 2 ffi ffi ffi ffi m p  1 , where m is the size of the test sample (), a test sample of 1000 insures RMS 0:016, so that the test sample estimate can be taken as the true error. There are two fundamental related questions (Dougherty, 2012): (i) Given the reported estimate from the preliminary study, is it prudent to commit large resources to the follow on study in the hope that a new biomarker diagnostic will result? (ii) Prior to that, is it possible that the preliminary study can obtain an error estimate that would warrant a decision to perform a follow on study? A large follow on study requires substantially more resources than those required for a preliminary study. If the preliminary study has a very low probability of producing reproducible results, then there is really no purpose in doing it. We propose a reproducibility index that simultaneously addresses both questions posed earlier. Our focal point is not that independent validation data should be used this has been well argued, for instance, in the context of bioinformatics to avoid over optimism (); rather, the issue addressed by the reproducibility index is the efficacy of small sample preliminary studies to determine whether a large validating study should be performed. We set up a simulation study on synthetic models that emulate real world scenarios and on some real datasets. We calculate the reproducibility index for different distributional models (and real datasets) and classification schemes. We consider two other scenarios: (i) multiple independent preliminary studies with small samples are carried out and only the best results (minimum errors) reported and (ii) multiple classification schemes are applied to the preliminary study with small samples and only the results (minimum errors) of the best class fier are reported. A decision is made for a large follow on study because the reported errors show very good performance show that there is a poor statistical relationship between the reported results and true classifier performance in these scenarios, namely, there is a potential for significant optimistic 'reporting bias' or multiple rule bias'. These two biases can substantially impact the reproducibility index.
