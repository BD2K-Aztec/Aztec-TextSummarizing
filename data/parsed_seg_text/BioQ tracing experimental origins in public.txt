Motivation: Public genomic databases, which are often used to guide genetic studies of human disease, are now being applied to genomic medicine through in silico integrative genomics. These databases, however, often lack tools for systematically determining the experimental origins of the data. Results: We introduce a new data provenance model that we have implemented in a public web application, bio q for assessing the reliability of the data by systematically tracing its experimental origins to the original subjects and biologics. bio q allows investigators to both visualize data provenance as well as explore individual elements of experimental process flow using precise tools for detailed data exploration and documentation. It includes a number of human genetic variation databases such as the HapMap and 1000 Genomes projects.

introduction the rapid expansion of biotechnology continues to drive the public availability of massive, complex genomic databases. These databases are often used in applications of in silico integrative genomics to genetic studies of human disease () as well as genomic medicine (). It is therefore critical for investigators to have tools for systematically assessing the credibility of the data (). We have developed a straightforward model for tracing experimental process flow in genomic databases. The goal is to isolate the key entities in the database the biologics, the experiments and the experimental results and to express their relationships in terms of experimental process flow. We call this the biologic experiment result (BERT) model and have implemented this model in our bio q web application (http://bioq.saclab.net). bio q builds on our dbsnp q application () to allow investigators to visualize experimental process flow and retrieve process related data with powerful query tools tables containing data on subjects, biologics and key experimental results are indicated by the labels 'S', 'B' and 'R', respectively. The goal is to trace the results back to subjects and biologics. Some groups and individual tables contain auxiliary reference data for processes and flow groups, respectively; reference nodes are indicated by dashed lines. See the Supplementary Materials for addition information on the BERT model. way with numerous tools for data retrieval. Other models, such as FuGE (), are more appropriate for capturing the full spectrum of experimental detail. The relative simplicity of the BERT model will allow applications such as bio q to adapt as the requirements of investigators evolve. Very few tools allow investigators to trace the experimental source of genomic data and to assess QC in detail as in bio q. Without tools such as bio q these undertakings can take days, even weeks given the enormity of tests required to thoroughly assess QC, the sheer size of the datasets involved and the difficulty in locating provenance data. It is therefore likely that in many cases these assessments will not be done, and this may lead to the use of faulty data. A recent incident at Duke University (), for example, involving the use of flawed data in a translational genomic study has led to the creation of a new framework at Duke on the quality of translational genomic medicine in which data provenance plays a minor role (http://tinyurl.com/6pkfdgd, accessed). bio q is a direct extension of our previous web application dbsnp q (). While dbsnp q provides powerful query and documentation tools for the dbSNP database, it does not implement the BERT model. These methods and tools for systematically tracing experimental process flow are unique to bio q. Resources such as the UCSC database () focus on graphical representations of genomic data. These resources do not provide tools for systematically tracing experimental process flow and for querying the underlying relational databases in a web browser as in bio q. The BERT model and the bio q application are designed to complement these resources by providing tools for systematically determining the origins of and assessing the quality of the data used in other tools such as the UCSC genome browser. The ability to establish the experimental origins of genomic data and to systematically assess QC at all stages of the experimental process is a systemic issue relevant to all genomic applications. It is therefore useful to have a separate resource dedicated to resolving this issue that can be used in conjunction with other applications. As new bioinformatics applications emerge that further bridge the gap between genomics and medicine, the availability of tools for systematically determining experimental origins will be crucial for maintaining reasonable levels of credibility. The bio q project has taken a number of measures to ensure that updates to external genomic databases are incorporated punctually and accurately. To each database there corresponds a custom command line driven program, written in Perl, that downloads the data, processes it, checks for errors, performs QC analyses and creates the MySQL databases used in bio q. Barring any major changes to the formats released by these databases, these programs allow the bio q update process to be completely automated. All code is publicly available from our Subversion server (http://svn.saclab.net) and our code review resource (http://fisheye.saclab.net). These programs can be easily modified to accommodate any changes encountered in updates to external genomic databases and will be used to incorporate these updates as they become available bio q
