Motivation: Mired by its connection to a well known N p complete combinatorial optimization problem namely the Shortest Common Superstring Problem scsp historically the whole genome sequence assembly wgs a problem has been assumed to be amenable only to greedy and heuristic methods. By placing efficiency as their first priority, these methods opted to rely only on local searches, and are thus inherently approximate, ambiguous or error prone, especially, for genomes with complex structures. Furthermore, since choice of the best heuristics depended critically on the properties of (e.g. errors in) the input data and the available long range information, these approaches hindered designing an error free wgs a pipeline. Results: We dispense with the idea of limiting the solutions to just the approximated ones, and instead favor an approach that could potentially lead to an exhaustive exponential time search of all possible layouts. Its computational complexity thus must be tamed through a constrained search branch and bound and quick identification and pruning of implausible overlays. For his purpose, such a method necessarily relies on a set of score functions (oracles) that can combine different structural properties (e.g. transitivity, coverage, physical maps, etc.). We give a detailed description of this novel assembly framework, referred to as scoring and unfolding Trimmed Tree Assembler (SUTTA), and present experimental results on several bacterial genomes using next generation sequencing technology data. We also report experimental evidence that the assembly quality strongly depends on the choice of the minimum overlap parameter k.

introduction since the pioneering work of Frederick Sanger in 1975, when he developed the basic DNA sequencing technology, Sanger chemistry has continued to be employed widely () in * To whom correspondence should be addressed. practically all large scale genome projects. However, whole genome sequence assembly wgs a pipelines in these projects have usually resorted to the shotgun sequencing strategy in order to reconstruct a genome sequence, despite the limitation that Sanger chemistry could only generate moderate sized reads (around 1000 bp) with no location information. While many recent advances in sequencing technology has yielded higher throughput and lower cost, the limitations imposed by the read lengths (ranging between 35 and 500 bp) still plague the genomics science, forcing it to work with draft quality unfinished, genotypic and mis assembled genomic data. The problem is, however, complicated by the presence of haplo typic ambiguities, base calling errors and repetitive genomic sections. Recall that to obtain the input read data, the DNA polymer is first sheared into a large number of small fragments; and then either the entire fragment or just its ends are sequenced. The resulting sequences are then combined into a consensus sequence using a computer program: DNA sequence assembler. It is desired that the consensus has as small a base level discrepancy with respect to the original DNA polymer as possible. Researchers first approximated the shotgun sequence assembly problem as one of finding the shortest common superstring of a set of sequences (). Although this was an elegant theoretical abstraction, it was oblivious to what biology needs to make correct interpretation of genomic data. In fact, it misses the correct model for the assembly problem for at least three different reasons: (i) it does not model possible errors arising from sequencing the fragments; (ii) it does not model fragment orientation (the sequence source can be one of the two DNA strands); (iii) most importantly it fails in the presence of repeats in the genome. Faced with this theoretical computational intractability (N p complete most of the practical approaches for genome sequence assembly were devised to use greedy and heuristic methods that, by definition, restrict themselves to find suboptimal solutions (see). Note that if the DNA was totally random then the overlap information would be sufficient to reassemble the target sequence and greedy algorithms would perform always well (). However, this argument is mostly irrelevant, since the problem is complicated by the presence of various non-random structures, in particular in eukaryotic genomes (e.g. repeated regions, rearrangements, segmental duplications). In the case of human genome, initially two unfinished draft sequences were produced by different methods, one by the International Human Genome Sequencing Consortium ihg sc and another by CELERA genomics (CG), with the published ihg sc assembly constructed by the program gig assembler devised at

conclusion sequence assembly accuracy has now become particularly important in: (i) genome wide association studies, (ii) detecting new polymorphisms and (iii) finding rare and de novo mutations. new sequencing technologies have reduced cost and increased the throughput; however, they have sacrificed read length and accuracy by allowing more single nucleotide base calling and in del (e.g. due to homo-polymer) errors. Overcoming these difficulties without paying for high computational cost requires (i) better algorithmic framework (not greedy), (ii) being able to adapt to new and combined hybrid technologies (allowing significantly large coverage and auxiliary long rage information) and (iii) prudent experiment design. We have presented a novel assembly algorithm, SUTTA, that has been designed to satisfy these goals as it exploits many new algorithmic ideas. Challenging the popular intuition, SUTTA enables 'fast' global optimization of the wgs a problem by taming the complexity using the BB method. Because of the generality of the proposed approach, SUTTA has the potential to adapt to future sequencing technologies without major changes to its infrastructure: technology dependent features can be encapsulated into the look ahead procedure and well chosen score functions.
