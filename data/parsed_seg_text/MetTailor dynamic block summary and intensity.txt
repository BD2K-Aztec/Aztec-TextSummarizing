Motivation: Accurate cross sample peak alignment and reliable intensity normalization is a critical step for robust quantitative analysis in un targetted metabolomics since tandem mass spectrometry (MS/MS) is rarely used for compound identification. Therefore shortcomings in the data processing steps can easily introduce false positives due to misalignments and erroneous normalization adjustments in large sample studies. Results: In this work, we developed a software package met tailor featuring two novel data preprocess-ing steps to remedy drawbacks in the existing processing tools. First, we propose a novel dynamic block summarization (DBS) method for correcting misalignments from peak alignment algorithms, which alleviates missing data problem due to misalignments. For the purpose of verifying correct realignments , we propose to use the cross sample consistency in isotopic intensity ratios as a quality metric. Second, we developed a flexible intensity normalization procedure that adjusts normalizing factors against the temporal variations in total ion chromatogram (TIC) along the chromatographic retention time (RT). We first evaluated the DBS algorithm using a curated metabolomics dataset, illustrating that the algorithm identifies misaligned peaks and correctly realigns them with good sensitivity. We next demonstrated the DBS algorithm and the rt based normalization procedure in a large scale data-set featuring 100 sera samples in primary Dengue infection study. Although the initial alignment was successful for the majority of peaks, the DBS algorithm still corrected $7000 misaligned peaks in this data and many recovered peaks showed consistent isotopic patterns with the peaks they were realigned to. In addition, the rt based normalization algorithm efficiently removed visible local variations in TIC along the RT, without sacrificing the sensitivity of detecting differentially expressed metabolites. Availability and implementation: The R package met tailor is freely available at the SourceForge website http://mettailor.sourceforge.net/.

introduction mass spectrometry (MS) coupled with gas or liquid chromatography (GC-MS or LC-MS) is already the technology of choice in the metabolomics literature (). In the experiment, hundreds to thousands of compounds elute through the chromatography column at varying rates, resulting in separation of compounds across the retention time (RT). The compounds are then ionized and analysed by the MS, in which the mass to charge ratio (m/z) and the intensity of ions are determined. In the untargeted setting, compound identification is typically achieved by searching the mono isotopic mass of each peak against a large scale database of compounds by exact mass values, such as the hm db (), mass bank () and met lin (), to name a few. Quantitative data analysis is then performed using the corresponding peak intensity or integrated peak area in each sample. Despite conceptual similarity, extraction of peak features with precise compound identification has proved to be a challenging task in untargeted metabolomics. In particular, the lack of the identification step via global scale MS/MS fragmentation makes cross sample matching of peak feature data, so called alignment, as the most crucial data extraction step. This is because multiple compounds of a similar or identical mass with slightly different chemical structure (isomers) and different elemental composition, can co elute and they are simultaneously analysed, creating mixed signals. Without the MS/MS evidence, the intensity signals for such molecules will be aligned together, being treated as the same compound. On the other hand, when thousands of features are processed across a large number of samples, it is also possible that the same compound can be misaligned across the samples and subsequently treated as different compounds in the statistical analysis stage. Hence accurate data extraction and robust preprocessing steps are prerequisite for successful untargeted metabolomics analysis. The general workflow for MS data preprocessing consists of several steps. First, peak picking algorithms are applied to identify the ion chromatograms with robust isotopic patterns and each of them is reported as a peak feature with three dimensional coordinates (m/ z value, retention time and peak areas intensities often aggregated over isotopes and adducts into a major peak feature). Next, the peak alignment step removes the variations in both RT and m/z axes for the same compounds across the samples, aligning the extracted peaks to the same m/z and RT grid to a single identifier. The aligned peak intensity data are then further processed by a normalization procedure prior to statistical analysis, to remove the systematic bias introduced during sample preparation and the variation in the ionization efficiency and instrumental analysis. Common choices for normalizing metabolomics MS data include total intensity sum (TIS) (), internal standard calibration is c (), or a statistical model that combines standards and TIS (). Although the existing open source software packages such as mz mine () and x cms () perform peak picking and alignment, there is room for further improvement (). In our observations, for example, even the most sophisticated multi-sample alignment algorithms have been prone to misalignment error at the individual compound level, especially for less abundant compounds or multiply charged compounds. Misalignment tends to happen when (i) signal detection algorithms fail to separate co eluting compounds, (ii) peak picking algorithms identify incorrect major isotopic peaks or (iii) minor temporal variation outlasts the alignment step by a few seconds of RT or a few decimals in the m/z beyond the tolerance level specified in those algorithms. Besides the misalignment issue, the existing methods for data normalization also have limitations. For example, the internal standard measurements can be inaccurate in some samples, or mixed with co eluting compounds. The TIS method may fluctuate due to poor chromatographic separation or the influence of a few dominantly abundant compounds, and it can also be inapplicable when the detected compounds are genuinely heterogeneous between comparison groups. Most importantly, the procedures mentioned above are corrections by a single constant, which adjusts all intensity values for global bias only and does not account for temporal or local variations along the RT axis across different samples (). To address these two key limitations in the current data processing pipeline, we developed a software package met tailor which implements two post extraction processing steps including a method for block wise quantitative summary and a novel rt based local normalization procedure.

conclusion overall our contribution in this work is to provide an open source software package implementing two data processing steps, as a complimentary tool to remedy some gaps unaddressed by the popular data extraction tools in the context of large sample experiments. There are a number of experimental factors that are unique to MS platforms and the two proposed methods are different from the existing alternatives that had been developed for other omics platforms such as gene expression microarrays. We provide these tools in a popular R programming environment, and will continue to adapt the tools for constantly evolving instrumentation in the future.
