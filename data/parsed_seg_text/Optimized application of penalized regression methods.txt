Motivation: Penalized regression methods have been adopted widely for high dimensional feature selection and prediction in many bioinformatic and biostatistical contexts. While their theoretical properties are well understood specific methodology for their optimal application to genomic data has not been determined. Results: Through simulation of contrasting scenarios of correlated high dimensional survival data, we compared the LASSO, Ridge and Elastic Net penalties for prediction and variable selection. We found that a 2D tuning of the Elastic Net penalties was necessary to avoid mimicking the performance of LASSO or Ridge regression. Furthermore, we found that in a simulated scenario favoring the LASSO penalty, a univariate pre-filter made the Elastic Net be have more like Ridge regression, which was detrimental to prediction performance. We demonstrate the real life application of these methods to predicting the survival of cancer patients from microarray data, and to classification of obese and lean individuals from meta genomic data. Based on these results, we provide an optimized set of guidelines for the application of penalized regression for reproducible class comparison and prediction with genomic data. Availability and Implementation: A parallelized implementation of the methods presented for regression and for simulation of synthetic data is provided as the pens im R package, available at http://cran.r-project.org/web/packages/pensim/index.html.

introduction multivariate regression is a flexible machine learning method, suited to prediction of discrete, continuous and censored time to event (survival) outcomes from arbitrary combinations of predictor variable classes. In genomic settings, collinear predictors typically greatly out number available samples (p  n), a now classic example being the prediction of cancer patient survival from tumor gene * To whom correspondence should be addressed. expression data (). In this setting, ordinary regression is subject to overfitting and instability of coefficients (), and stepwise variable selection methods do not scale well (). Regression has been successfully adapted to high dimensional situations by penalization methods (review by), and penalized regression has been shown to outperform univariate and other multivariate regression methods in multiple genomic datasets (). Two penalization methods, and a hybrid of these, are most commonly used. Ridge regression () uses a penalty on the L 2 norm of the coefficients, which introduces bias in the prediction error in exchange for reduced variance. However, ridge regression keeps all variables in the model and thus can not produce a parsimonious model from many variables. LASSO regression (penalizes the L 1 norm, which tends to reduce many coefficients to exactly zero and thus performs variable selection in addition to prediction. However, the LASSO has been noted to be inferior to Ridge regression for prediction in lower dimensional situations, and tends to select only one of a group of collinear variables, which may not always be desirable ().thus proposed the Elastic Net, penalizing both the L 1 and L 2 norms with individual tuning parameters, as a way to achieve the best of both LASSO and Ridge. These three variants of penalized regression lasso Ridge and Elastic net have since been applied to a variety of phenotype prediction tasks using genomic data (for example,). Several previous simulation studies have investigated properties of the Elastic Net (), the LASSO and Ridge regression (), but have not compared all these methods with alternative strategies for their application. We present a comprehensive assessment and optimization of these methods, using two contrasting configurations of simulated genomic data and two genome scale experimental datasets. Comparative studies of this nature provide the most realistic and unbiased assessments of available machine learning methods, issues which have been identified as critical to researchers' selection of appropriate methodology (). We introduce a 2D optimization of the Elastic Net penalty parameters, and show that it or a comparable procedure is necessary
