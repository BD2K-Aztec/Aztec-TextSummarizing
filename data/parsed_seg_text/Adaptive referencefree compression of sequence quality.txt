Motivation: Rapid technological progress in DNA sequencing has stimulated interest in compressing the vast datasets that are now routinely produced. Relatively little attention has been paid to compressing the quality scores that are assigned to each sequence, even though these scores may be harder to compress than the sequences themselves. By aggregating a set of reads into a compressed index, we find that the majority of bases can be predicted from the sequence of bases that are adjacent to them and, hence, are likely to be less informative for variant calling or other applications. The quality scores for such bases are aggressively compressed, leaving a relatively small number at full resolution. As our approach relies directly on redundancy present in the reads, it does not need a reference sequence and is, therefore, applicable to data from meta genomics and de novo experiments as well as to re-sequencing data. Results: We show that a conservative smoothing strategy affecting 75% of the quality scores above Q2 leads to an overall quality score compression of 1 bit per value with a negligible effect on variant calling. A compression of 0.68 bit per quality value is achieved using a more aggressive smoothing strategy, again with a very small effect on variant calling. Availability: Code to construct the BWT and lcp array on large genomic data sets is part of the be etl library, available as a git hub repository at

introduction the raw output of a DNA sequencer is converted by a program known as a base caller into nucleotide bases, each of which is typically assigned a quality score that estimates the probability that the base has been sequenced correctly. Quality scores have long been used to trim the low quality ends of reads and for accurate consensus sequence determination (). More recently, they have enabled more accurate alignments of the shorter sequences produced by next generation technologies, by allowing the aligner to give lower weight to mismatches at less reliable base positions (). Often quality scores are expressed on an integer scale derived from the error probability P via the formula 10log 10 p, a scoring scheme named after the Phred base caller () that first used it the widely used fast q format () stores the sequence and metadata of a set of DNA reads as ASCII text, together with one character per score quality strings that encode the Phred scores of their bases. The different properties of these three data types have meant that many fast q compression methods have treated them as three distinct data streams and applied separate compression strategies to each. The metadata field tends to be formatted in ways that are specific to the technology that was used to generate the sequence, and some fast q compressors have exploited such structure to improve compression. However, there is no global format specification for the metadata; therefore, any universally applicable method for its compression must necessarily be a generic exercise in the compression of ASCII text. Although standard text compressors, such as gzip www gzip. org, jean loup Gailly and Mark Adler), do not significantly outperform a nave nave 2 bits per base encoding on DNA sequence data, applications such as re-sequencing and de novo assembly typically rely on a 20-fold or more oversampling of the underlying genome, and this redundancy can be exploited to improve compression of the sequences themselves. reference based compression tools, such as CRAM (), encode reads in terms of differences between their sequences, and the sites they align to on a reference sequence. Sorting the reads by the coordinates of these alignments saves most of the overhead of storing their positions and is a convenient ordering for applications, such as SNP calling and visualization. Despite these advantages, reference based compression suffers when the reference is incomplete (reads that do not align can not be compressed), subject to change or not present at all (as in meta genomics motivating an interest in reference free compression methods. The tool QUIP () creates an on the fly de novo assembly to perform reference based compression against, whereas scal ce () places similar reads near to each other in a sorted file, facilitating good performance by standard tools such as gzip that operate on a buffer of text at a time. Another widely used generic compression tool bzip2 (www. bzip org Julian Seward) exemplifies burrows wheeler transform (BWT) compression: text is split into 900 kb blocks, and the BWT of each block is computed. The BWT is a reversible permutation of the text that acts as a compression booster for the pipeline of standard compression steps that bzip2 subsequently applies. In, two of the present authors showed that although bzip2 performs comparably with gzip on DNA sequence reads, the compression achieved by bwt based methods improves by 43-fold if the BWT of the entire read set is built, as *To whom correspondence should be addressed. this captures redundancy between reads that were widely spaced in the original file. It was shown that compression can be further boosted by pre sorting the reads or applying an implicit sorting strategy while the BWT is being built, enabling compression of better than 0.5 bits per base to be achieved. Lossless approaches to the compression of quality scores have exploited empirical relationships between the scores assigned to bases within a read: for instance, Illumina quality scores tend to be monotone decreasing along a read with a decrease in scores between adjacent bases that is usually small. An overreliance on such observations potentially ties a compression scheme to a given sequencing technology and makes it sensitive to changes in sequencing protocol. Moreover, it is likely that Illumina quality scores at their full resolution contain a proportion of random noise that is impossible to compress: striking evidence for this is given by in Bonfield and Mahoney (2013), which shows multiple entrants to the sequence squeeze competition for fast q compression achieving similar lossless compressions of $2.94 bits per score on a test dataset, but that no entrants were able to improve on this figure. It is undesirable that when compressed, the quality scores should take up several times more space than the sequences themselves; therefore, we are led to consider compressing them in a lossy way found that a global reduction in the resolution of the scores from 40 to 8 values (thus permitting each score to be stored in 3 bits) had no significant impact on the quality of variant calls, whereas strategies for global re quantization of quality scores were studied in more detail by. However, treating all scores in the same way ignores the fact that most of them could likely be reduced in resolution or even discarded entirely with little impact on our ultimate goal of ensuring that analyses performed with the reduced scores closely reflect the results obtained from the original data. In a human re-sequencing context, for example, if a large coverage of high quality bases unanimously supports a homozygous match to the reference genome, then a confident call can be made without the full resolution quality scores of each individual base needing to be kept. With this in mind, CRAM allows an adaptive approach where only quality scores that contribute to variant calls that do not match the reference are kept. This enables the vast majority of scores to be omitted, but it means compression can not take place until analysis has been finalized. This means any pre analysis transfer or storage of the data will not benefit from compression and is potentially problematic if the data subsequently need to be re analysed. Here, we present an adaptive and reference free approach to lossy quality score compression. Our central premise is that if a base in a read can, with high probability, be predicted by the context of bases that are next to it, then the base itself is imparting little additional information, and its quality score can be discarded or aggressively compressed at little detriment to downstream analysis. Such predictions are made by considering all possible contexts present in the reads: if every occurrence of some string Q is followed by the same character p then the presence of a context Q in a read can be said to predict that p will come next. In the rest of this article, we formalize this intuition and give algorithms that use the BWT of a set of reads to identify non-essential quality scores. The BWT places all characters that precede a given context next to each other in a permuted string, whereas another standard data structure the longest common prefix array (LCP) then allows stretches of characters that precede contexts of a given length to be enumerated in a single pass through the two data structures. This enables the majority of scores to be smoothed to an average value, greatly improving compression. We derive a formula to quantify the information lost during this smoothing process and justify our compression scheme empirically by showing that results using the compressed scores closely match the original data when our scheme is applied to whole genome re-sequencing data. We also show that we can use the BWT alone to compress quality scores in a way that is almost as effective as bwt lcp compression, thus avoiding the overhead of computing the LCP array. Moreover, we demonstrate that our methods can be used in tandem with other approaches to boost the compression obtained.

discussion this article aims to introduce the general idea of smoothing quality scores based on the BWT and LCP of their associated reads. Section 2.2 describes perhaps the simplest and most conservative approach to this, but the theoretical framework presented in Section 2.3 allows comparison of future more sophisticated quality smoothing strategies. Note that the effect of smoothing or otherwise adjusting the quality scores is to change the weightings of the different nucleotides in the distributions Xw, QV. We can take a step further and consider adjusting low probability bases in Xw, QV to zero. Our work can thus be extended to a new quality based view on de novo error correction that may provide an interesting alternative to existing approaches, many of which are based on the counting of km ers (as surveyed by). Such a strategy could be thought of as a quality aware extension of the HiTEC algorithm (), while enjoying the considerable space advantage of being based on a (potentially compressed) BWT instead of a suffix array. Although it is an advantage that the relative entropy measures the information lost by quality smoothing in an application neutral way, we also recognize that a single numerical quantity can not fully model the effect of quality smoothing on the often complex multi-step analysis pipelines that are applied to sequence data. We investigated this by measuring the effect of smoothed quality scores on the results of widely accepted tools for a well understood application. Transforming the smoothed scores back into their original reads added a significant boost to the compression already achieved by 7-Zip from exploiting similarities between the quality scores of individual read. It is equally simple to combine our approach with the application to the unsmoothed scores of one of the lossy re quantization schemes studied by. However, using our method in this way involves building the BWT (and possibly LCP) of a set of reads and then applying the inverse BWT permutation to the quality scores to obtain a smoothed quality string for each read. The overhead of these tasks may limit its practicality for downstream applications that operate on reads and does not use the potential of the BWT. As well as allowing excellent lossless compression, storing sequences in BWT form also facilitates rapid analysis: the sga () and Fermi () assemblers both operate directly on bwt based compressed indexes of sets of reads, and we ourselves have shown that similar data structures can be the basis of both rnase q () and meta genomic () analyses. Although the compression achieved on bwt space reads is less than in read space (although the difference may be less clear cut on a dataset where the q2m asking of read ends is less prevalent or has been switched off), we, therefore, envisage that a key application of our work is to allow quality scores to be used in a bwt space context while being stored in as compact a manner as the reads themselves. Conflict of Interest: L.J. and A.J.C. are employees of Illumina Inc., a public company that develops and markets systems for genetic analysis. They receive shares as part of their compensation.
