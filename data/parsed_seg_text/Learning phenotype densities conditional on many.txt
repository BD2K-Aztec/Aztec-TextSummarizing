Motivation: Estimating a phenotype distribution conditional on a set of discrete valued predictors is a commonly encountered task. For example , interest may be in how the density of a quantitative trait varies with single nucleotide polymorphisms and patient characteristics. The subset of important predictors is not usually known in advance. This becomes more challenging with a high dimensional predictor set when there is the possibility of interaction. Results: We demonstrate a novel non-parametric Bayes method based on a tensor factorization of predictor dependent weights for Gaussian kernels. The method uses multistage predictor selection for dimension reduction, providing succinct models for the phenotype distribution. The resulting conditional density morphs flexibly with the selected predictors. In a simulation study and an application to molecular epidemiology data, we demonstrate advantages over commonly used methods. Availability and implementation: MATLAB code available at https:// google drive comhost0bw6kifbk4ioowq0d fjtsvzxne0ktdctf
introduction many areas of research are concerned with learning the distribution of a response conditional on numerous categorical (discrete) predictors. The important predictors for characterization of this distribution are not usually known in advance, and there may be hundreds or thousands of candidates. Methods that attempt to accommodate interactions among these predictors become mired in the enormous model space. For example, in a moderate dimensional case involving p  40 categorical predictors, each with d j  4 possible realizations, considering all possible levels of interaction leads to a space of 4 40 % 10 24 possible models. Parallelization and technical tricks may work for smaller examples, but data sparsity and the sheer volume of models force us to consider different approaches. The conditional density may vary in more than just location illustrated this in an application to the conditional density of blood glucose levels given insulin sensitivity and age. In the work that follows, we present a novel non-parametric Bayes (NPB) approach to learning conditional densities that makes use of a conditional tensor factorization to characterize the conditional distribution given the predictor set, allowing for complex interactions between the predictors. The particular form assumed for the conditional density gives rise to an attractive predictor selection procedure, providing support for distinct predictor selection steps. This addresses the challenges of high dimensional data and produces conditional density estimates that allow assessment of tail risks and other complex quantities.

DISCUSSION

conclusion we have presented a novel method for flexible conditional density regression in the common case of a continuous response and categorical predictors. The simulation study and real data example suggest that this conditional tensor factorization method can have better performance than other modeling tools when there is substantial interaction between the predictors of interest. The CTF does have a higher computational time requirement than the competitor methods, but the improvement in prediction accuracy and coverage still make the CTF an attractive method. A particularly appealing aspect of the CTF is predictor selection, which finds low dimensional structure in the high dimensional predictor set. This reduction to more parsimonious models yields a succinct description of the ways in which the phenotype varies given exposure and SNPs. Finally, a distinct advantage of the CTF is its ability to produce conditional density estimates. This property of the CTF provides insight beyond a simple conditional expectation and makes it possible to answer more complex questions about the relationship between the response and the predictors.
