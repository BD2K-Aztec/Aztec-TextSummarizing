Motivation: Univariate statistical tests are widely used for biomarker discovery in bioinformatics. These procedures are simple, fast and their output is easily interpretable by biologists but they can only identify variables that provide a significant amount of information in isolation from the other variables. As biological processes are expected to involve complex interactions between variables, univariate methods thus potentially miss some informative biomarkers. Variable relevance scores provided by machine learning techniques, however, are potentially able to highlight multivariate interacting effects, but unlike the p values returned by univariate tests, these relevance scores are usually not statistically interpretable. This lack of interpretability hampers the determination of a relevance threshold for extracting a feature subset from the rankings and also prevents the wide adoption of these methods by practic ians. Results: We evaluated several, existing and novel, procedures that extract relevant features from rankings derived from machine learning approaches. These procedures replace the relevance scores with measures that can be interpreted in a statistical way, such as p values false discovery rates, or family wise error rates, for which it is easier to determine a significance level. Experiments were performed on several artificial problems as well as on real microarray datasets. Although the methods differ in terms of computing times and the tradeoff, they achieve in terms of false positives and false negatives, some of them greatly help in the extraction of truly relevant biomarkers and should thus be of great practical interest for biologists and physicians. As a side conclusion, our experiments also clearly highlight that using model performance as a criterion for feature selection is often counter-productive. Availability and implementation: Python source codes of all tested methods, as well as the MATLAB scripts used for data simulation, can be found in the Supplementary Material.

introduction univariate hypothesis testing is widely used in the context of biomarker discovery in bioinformatics, where one seeks to identify biological variables (e.g. genes or genetic polymorphisms) that truly provide information about some phenotype of interest (e.g. disease status or treatment response). A classic procedure consists in applying a statistical test to compute a p value for each variable of the considered problem and selecting variables that have a p value lower than a chosen threshold. To cope with multiple hypothesis problems, p values are typically replaced with an estimation of the false discovery rate (FDR) or the family wise error rate f wer (). Univariate tests can only identify variables that provide a significant amount of information about the output variable in isolation from the other inputs. Since biological processes are expected to involve complex interactions between variables, these procedures potentially miss some informative biomarkers. Nowadays, when one seeks multivariate interacting effects between features, one can resort to relevance scores provided by machine learning techniques. Among these, the most popular methods include importance scores derived from a tree based ensemble method () or feature weights computed for example from a linear support vector machine (SVM) (). However, unlike the p values returned by univariate tests, these relevance scores are usually not statistically interpretable. This lack of interpretability prevents the wide adoption of these methods by practic ians biologists or physicians and also makes the identification of the truly relevant variables among the top ranked ones, i.e. the determination of a relevance threshold, a very difficult task in practice. In this article, we evaluate several, existing and novel, procedures that extract relevant features from a ranking returned by a multivariate algorithm. These procedures replace the original relevance score with a measure that can be interpreted in a statistical way and hence allow the user to determine a significance threshold in a more informed way. Most of these methods exploit a resampling procedure to estimate the FDR or f wer among the k top ranked features, for increasing values of k. Just like for standard univariate tests, the user can then choose a threshold on this new measure depending on the risk he/she is ready to take when deeming that all features above this threshold are relevant. Experiments on several artificial problems, as well as on real microarray datasets, show that some of these measures greatly help in the extraction of truly relevant features from a ranking derived from a multivariate approach.

discussion in this article, we evaluated several procedures that aim to identify a maximal subset of variables that truly provide some information about an output variable. These procedures assume that a (multivariate) ranking method A was first used to compute a relevance score for each variable of the considered problem and then extract relevant features from this ranking, by replacing the original relevance score with a measure that can be interpreted in a statistical way. Depending on the procedure, this measure is either the generalization error of a predictive model (err-A and err trt the FDR n fdr ef dr the f wer (CER, m probes or a p value mr test 1Probe). Although there is still a need to determine a threshold on this new measure, the determination of this threshold is clearly easier due to its interpretability. This threshold is also not dependent anymore on the problem at hand and on the ranking method A. Among the feature selection methods that we evaluated, err-A and err trt are the only ones that do not require to choose a significance level a priori. However, on artificial problems, they always have the lowest precision among all methods and they wrongly select a non negligible number of variables on the permuted prostate 1 datasets. Moreover, they are subject to the selection bias problem, preventing the selection of an adequate number of variables. Prediction performance is thus clearly not an appropriate measure for the identification of relevant features. Although they clearly highlight a threshold on the feature ranking, several of the remaining methods have also some disadvantages. The n fdr method is the simplest one but was shown to overestimate the real FDR in the case of dependent scores (). The drawback of the 1Probe procedure is that the selected variables depend on the chosen distribution of the random probe, which makes it a parametric method. The mr test method estimates the rank distribution of an irrelevant variable from the k variables with the highest observed ranks. The determination of k thus introduces some dependency on the problem and ranking method used. Although our default choice seems to be robust, an inappropriate value of k can lead to a dramatic over or underestimation of the p values. The mr test also includes as a second parameter the number of resampled instances at each iteration. Among the three remaining methods, CER and m probes are highly selective methods that avoid the inclusion of any irrelevant feature as much as possible. m probes has a computational advantage
