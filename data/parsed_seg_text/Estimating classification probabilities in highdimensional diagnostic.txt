Motivation: Classification algorithms for high dimensional biological data like gene expression profiles or meta bolo mic fingerprints are typically evaluated by the number of misclassifications across a test dataset. However, to judge the classification of a single case in the context of clinical diagnosis, we need to assess the uncertainties associated with that individual case rather than the average accuracy across many cases. Reliability of individual classifications can be expressed in terms of class probabilities. While classification algorithms are a well developed area of research, the estimation of class probabilities is considerably less progressed in biology, with only a few classification algorithms that provide estimated class probabilities. Results: We compared several probability estimators in the context of classification of metabolomics profiles. Evaluation criteria included sparseness biases, calibration of the estimator, the variance of the estimator and its performance in identifying highly reliable classifications. We observed that several of them display artifacts that compromise their use in practice. Classification probabilities based on a combination of local cross validation error rates and monotone regression prove superior in meta bolo mic profiling. Availability: The source code written in R is freely available at

introduction diagnosis prognosis and prediction of treatment response based on transcript omic proteomic or meta bolo mic profiles is a well developed field (). A plethora of classification algorithms have been proposed and critically compared (). It very much depends on the classification problem at hand, whether an almost error free classifier can be developed or whether classification errors are unavoidable regardless of what algorithm is chosen. In the latter case, it is natural that a clinician asks for the reliability of an individual diagnosis before moving on to treatment decisions. Classification algorithms are typically evaluated by the frequency of misclassifications in cross validation or on an independent test set. These performances are averages over many predicted cases. They say little about the reliability of an individuals diagnosis. The case * To whom correspondence should be addressed. might be easier or more difficult to diagnose than the average in the test set. For each case, every class is assigned a value p j , which is an estimated probability that the case belongs to that class, given the profiling data. In microarray based classification, the performance of classification algorithms has been analyzed and compared in great detail (). However, little attention has been given to the usefulness of probability estimates and this is even more true for meta bolo mic analyses. In fact, only relatively few classification algorithms estimate class probabilities and in the majority of clinical papers on the performance of classifiers, case specific probabilities are not shown. A class probability estimator is most useful, if it flags incorrect classifications as low confidence classifications. In other words: if a classifier produces confident class probabilities close to one, these should be correct classifications. In this article, we compare class probability estimators in the context of high dimensional data based diagnosis. We briefly review a selection of class probability estimators including those that are most frequently used in the context of gene expression analysis like Naive Bayes estimators or binary regression. In addition, we discuss alternative approaches from different fields of application like text categorization and digit recognition and adapt them to metabolomics analysis. We complement the pool of methods by a novel approach based on smooth local error rates. The approaches are compared on a recently published metabolomics dataset of patients with various types of kidney disease. We found that artifacts can compromise the utility of some frequently used methods. A widely observed problem is the dependence of classification probabilities on the number of features used in a diagnostic signature. The more features are used by a classifier, the more confident the classification probabilities, even in cases where the classification is incorrect. Moreover, class probabilities need to be estimated from test data or cross validated classification scores since training scores display a better but unrealistic separation of classes. This overfitting phenomenon can greatly affect class probabilities. In our comparative metabolomics study, class probabilities derived from local error rates proved to be the method of choice.
