Motivation: Rapid advances in next generation sequencing (NGS) technology have led to exponential increase in the amount of genomic information. However, NGS reads contain far more errors than data from traditional sequencing methods, and downstream genomic analysis results can be improved by correcting the errors. Unfortunately, all the previous error correction methods required a large amount of memory, making it unsuitable to process reads from large genomes with commodity computers. Results: We present a novel algorithm that produces accurate correction results with much less memory compared with previous solutions. The algorithm, named bloom filter based Error correction Solution for high throughput Sequencing reads (BLESS), uses a single minimum sized Bloom filter, and is also able to tolerate a higher false positive rate, thus allowing us to correct errors with a 40Ã‚ memory usage reduction on average compared with previous methods. Meanwhile, BLESS can extend reads like DNA assemblers to correct errors at the end of reads. Evaluations using real and simulated reads showed that BLESS could generate more accurate results than existing solutions. After errors were corrected using BLESS, 69% of initially unaligned reads could be aligned correctly. Additionally, de novo assembly results became 50% longer with 66% fewer assembly errors.

introduction recent advances in next generation sequencing (NGS) technologies have made it possible to rapidly generate high throughput data at a much lower cost than traditional Sanger sequencing technology (). NGS technologies enable cost efficient genomic applications, including de novo assembly of many non model organisms (), identifying functional elements in genomes (), and finding variations within a population (). In addition to short read length, a main challenge in analyzing NGS data is its higher error rate than traditional sequencing technology (), and it has been demonstrated that error correction can improve the quality of genome assembly () and population genomics analysis (). Previous error correction methods can be divided into four major categories (): (i) km er spectrum based (), (ii) suffix tree array based (, b), (iii) multiple sequence alignment msa based () and (iv) hidden Markov model hmm based (). However, none of these previous methods has successfully corrected errors in NGS reads from large genomes without consuming a large amount of memory that is not accessible to most researchers (see detailed discussions in the Supplementary Methods). Previous evaluations showed that some error correction tools require 4128 gigabyte (GB) of memory to correct errors in genomes with 120 Mb, and the others need tens of GB of memory (). For a human genome, previous approaches would need hundreds of GB of memory. Even if a computer with hundreds of GB of memory is available, running such memory hungry tools degrades the efficiency of the computer. While the error correction tool runs, we can not do any other job using the computer if most of the memory is occupied by the error correction tool. This can be a critical problem for data centers, where a large amount of data should be processed in parallel. In several works, Bloom filters () or counting Bloom filters () were used to save a km er spectrum, which includes all the strings of length k (i.e. km ers that exist more than a certain number of times in reads (, b). Although Bloom filter is a memory efficient data structure, the memory reduction by previous Bloom filter based methods did not reach their maximum potential because of the following four reasons: (i) The size of a Bloom filter should be proportional to the number of distinct km ers in reads, and the number of distinct km ers was conservatively estimated, thus could be much higher than the actual number. (ii) They could not remove the effect of false positives from Bloom filters. To make the false positive rate of the Bloom filters small, the size of Bloom filters were made large. (iii) Because they could not distinguish error free km ers from erroneous ones before a Bloom filter was constructed, both of the km ers needed to be saved in Bloom filters. (iv) Multiple Bloom filters (or counting Bloom filters) were needed to count the multiplicity of each km er. Besides the large memory consumption of the existing methods, another problem encountered during the error correction process is that there exist many identical or very similar subsequences in a genome (i.e. repeats). Because of these repeats, an erroneous subsequence can sometimes be converted to multiple error free subsequences, making it difficult to determine the right choice. In this article, we present a new Bloom filter based error correction algorithm, called BLESS. BLESS belongs to the km er spectrum based method, but it is designed to remove the aforementioned limitations that previous km er spectrum based solutions had. Our new approach has three important new features 1 BLESS is designed to target high memory efficiency for error correction to be run on a commodity computer. The km ers that exist more than a certain number of times in reads are sorted out and programmed into a Bloom filter 2 BLESS can handle repeats in genomes better than previous km er spectrum based methods, which leads to higher accuracy. This is because BLESS is able to use longer km ers compared with previous methods. Longer km ers resolve repeats better 3 BLESS can extend reads to correct errors at the end of reads as accurately as other parts of the reads. Sometimes an erroneous km er may be identified as an error free one because of an irregularly large multiplicity of the km er. False positives from the Bloom filter can also cause the same problem. BLESS extends the reads to find multiple km ers that cover the erroneous bases at the end of the reads to improve error correction at the end of the reads to identify erroneous km ers in reads, we need to count the multiplicity of each km er. Counting km ers without extensive memory is challenging (). BLESS uses the disk based km er counting algorithm like Disk Streaming of km ers (DSK) () and km er Counter (KMC) (). However, BLESS needs to save only half of the km ers that DSK does in hash tables, because it does not distinguish a km er and its reverse complement. To evaluate the performance of BLESS, this study used real NGS reads generated with the Illumina technology as well as simulated reads. These reads were corrected using BLESS as well as six previously published methods. Our results show that the accuracy of BLESS is the best while it only consumes 2.5% of the memory usage of all the compared methods on average. Our results further show that correcting errors using BLESS allowed us to align 69% of previously unaligned reads to the reference genome accurately. BLESS also increased NG50 of scaffolds by 50% and decreased assembly errors by 66% based on the results from Velvet ().

discussion current NGS technologies produce errors in reads, which can influence the quality of downstream analysis. Many methods have been developed to correct such sequencing errors. However, most previous methods can not correct errors in large genomes. Even if a few methods succeeded in correcting large genomes, their outputs still contain many uncorrected errors. In addition, the memory requirement for the existing tools has been still too large for most researchers, who might only have access to computers with a moderate amount of memory. In this work, we present a novel error correction algorithm for NGS reads, called BLESS, which has two novel features: (i) BLESS consumes much less memory than previous methods. BLESS can sort out minimum km ers needed to correct errors, and program the km ers in a minimum sized Bloom filter. This makes BLESS consume much less memory than any other error correction method including previous Bloom filter based ones. (ii) BLESS also generates more accurate results for reads from genomes with many short repeats. This is mainly because BLESS is not limited by the choices of the length of the km er (see Software options in the Supplementary Document for details). While the maximum k is usually 2030 in other methods, BLESS is able to remove ambiguities in the error correction process by choosing large numbers for k without increasing memory usage. Furthermore, BLESS is efficient at correcting errors close to the ends of the reads. BLESS corrects errors at the ends of the reads by extending the ends. BLESS was compared with previous top performers using real and simulated reads. The experimental results showed that BLESS generated more accurate results than previous algorithms while consuming only 2.5% of the memory usage of the compared methods on average. Moreover, running BLESS improved the length and accuracy of de novo assembly results for all the three widely used assemblers, Velvet, soap de novo and SGA. BLESS also made 69% of unaligned reads exactly aligned to reference sequences. As pointed out before, BLESS can choose large values for k. The only drawback to large k values is that the average multiplicity of such km ers drops. If the average multiplicity of km ers is too low, we can not precisely distinguish erroneous km ers using their multiplicity. Nevertheless, it is important to note that because the DNA sequencing cost keeps dropping and the throughput keeps increasing, we expect to solve this problem by increasing the depth of reads. The memory usage of BLESS is proportional only to the number of solid km ers. Because the solid km ers eventually represent the km ers that exist in the genome sequence, the number of solid km ers remains constant even as the number of input reads escalates. Therefore, memory consumption of BLESS will not increase even when read depth increases as shown in Supplementary Table S1. There are two future avenues to pursue. First, although the runtime of BLESS is already competitive as shown in, supporting multiple threads will improve bless s runtime further. bless s wall clock time decomposition for D4 is depicted in Supplementary. Most of the time is spent counting the number of distinct solid km ers and correcting errors. The runtime of both processes can be improved through parallelization. In the counting step, km ers are distributed into N files, and each file is processed in succession. This step can be parallelized without degrading memory usage. The error correction process of a read is independent of other reads, and therefore the error correction part can be easily parallelized. Second, developing a method to automatically choose k may be added. Recent work () showed that a km er multiplicity histogram can be made in a short time by sampling reads and an optimal k value can be found using the histograms. This sampling based approach will also work for BLESS. It will be helpful to reduce the runtime because it can prevent users from running BLESS multiple times with different k values.
