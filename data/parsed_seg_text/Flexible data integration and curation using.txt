Motivation: The increasing diversity of data available to the biomedical scientist holds promise for better understanding of diseases and discovery of new treatments for patients. In order to provide a complete picture of a biomedical question, data from many different origins needs to be combined into a unified representation. During this data integration process, inevitable errors and ambiguities present in the initial sources compromise the quality of the resulting data warehouse, and greatly diminish the scientific value of the content. Expensive and time consuming manual curation is then required to improve the quality of the information. However, it becomes increasingly difficult to dedicate and optimize the resources for data integration projects as available repositories are growing both in size and in number everyday. Results: We present a new generic methodology to identify problematic records, causing what we describe as data hairball structures. The approach is graph based and relies on two metrics traditionally used in social sciences: the graph density and the betweenness centrality. We evaluate and discuss these measures and show their relevance for flexible, optimized and automated data curation and linkage. The methodology focuses on information coherence and correctness to improve the scientific meaningfulness of data integration endeavors, such as knowledge bases and large data warehouses.

introduction with the current quantity and diversity of data available in the biomedical domain (), it becomes increasingly necessary to combine the information coming from multiple sources, in a variety of formats, into a unified representation. This practice goes by the name of data integration or record linkage (). Different reasons motivate the exercise: it can be for instance the appealing possibility to query and analyze information about complementary thematics (e.g. gene disease relationship), consolidation of some knowledge existing about one topic, or the absorption of a source into another for maintenance needs (e.g. dataset coming from company acquisitions). Recently, data integration efforts have been particularly active in the drug discovery domain, with platforms such as trans mart () or Open ph acts (), focused on building a pharmacological space from public repositories. Other examples include chem spider (), a resource providing a central hub related to chemical names and structures from various sources or identifiers org (), a cross reference platform for biomedical identifiers and data connections. Fundamentally, data integration can be seen as a problem of creating the correct links between equivalent, yet disconnected, database records. This process is sometimes called 'stitching', or 'reconciliation' (). Once records are rightfully associated, it becomes possible to query across or merge them if necessary. As links are created between entries, it is V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals permission soup com intuitive to rely on an abstract graph structure to represent the problem faced. Vertices or nodes are records or entities of interest; edges represent the associations between them. An illustration of the graph abstraction is the Semantic Web: this series of standards relies on the Resource Description Framework (RDF) and a graph structure to facilitate the interoperability and integration of independent pieces of information on the World Wide Web (). The Semantic Web also provides means to establish equivalence between segregated records same as or exact match relations), with the use of rules or reasoners for instance. Moreover, the biomedical domain is rich in unique identifiers (e.g. chemical structure identifiers like inch i keys and cross references which can be used to automatically assess equivalence between database entries: if one record references another, it might be possible to deduce that the two entities are the same, for instance. All these strategies are automat able and reliable in theory, however complications can arise quickly if cross references are absent, errors present in the original sources or if the data is fuzzy and ambiguous by nature, such as with drug names and chemical structures for instance (). The effect of erroneous information is dramatic for data integration: records can get incorrectly associated with other entries, themselves recursively linked to other records. This cascade of events leads to the creation of unwanted 'hairballs', which we define as groups of records not equivalents and not supposed to be linked, but yet connected because of the deficient state of the data. In such a scenario, manual intervention from expert curators is required in order to improve the quality and scientific validity of the dataset. Unfortunately, with the increasing size of biomedical databases and repositories, it becomes more likely that such errors will arise by chance, and more expensive and time consuming to correct them by hand. In this document, we use an in house data integration project related to the creation of a drug product terminology in order to illustrate a generic and flexible approach to identify and handle problematic and erroneous records during the integration process. The drug terminology is built from millions of database entries present in eight heterogeneous sources, including four from third party vendors (Integrity, cort ellis pharma project and ad is insight one developed internally and three public drug databases drug bank part of ch embl and ChEBI). Our motivation is to build and maintain an integrated database (also called data warehouse) from these various sources, each of which contains a part of the desired information (see). The graph based method presented allows researchers to isolate and prioritize problematic entries and flexibly adjust the curation work required over an automatic integration to maximize the quality and scientific usefulness of the data.

discussion we present in this manuscript a generic methodology to integrate and merge data coming from different repositories into a central warehouse containing the consolidated information. As opposite to previous approaches (see Section 1), the method emphasizes error detection and is qualified as flexible, for multiple reasons: First, the of the records with high BC after the first cleaning step, the lipitor related entries are still mixed with other products (blue graph, low density). Some graphs are now dense and do not need further cleaning, for instance (B1) refers to gemfibrozil and (B2) to adenosine. (C) The hairball is subject to a second iteration step, where more records are excluded (arrows on the graph). (D) The entries related to the atorvastatin are eventually well isolated (blue graph) and ready to be merged as a high quality consolidated record. The other drug product (D1) is fenofibrate. See Supplementary Material for bigger version of this figure assertion of equivalence between records can be implemented as a series of steps including for instance, data cleaning and transformation, in order to best match the desired outcome (e.g. chemical structures, cross references free-text label). Secondly, the approach can handle n numbers of data sources, and is not limited to handle only pairs of records. Thirdly, the density measure helps to extract numbers regarding how confident one can be in regards to the quality of the integrated information and to determine how much curation is required on the top of the data. Finally, the betweenness centrality identifies the erroneous records, and can be used to resolve issues either automatically or manually. Thresholds for graph measures can be flexibly adjusted too, in order to optimise the work based on the time and resources available. The abstraction of the data integration problem as a graph representation has been already introduced by previous work (), yet the use of graph measures on the top of the data to assess the quality of the integration and to detect anomalies is novel as far as our knowledge goes. From a usability perspective, we believe that the quality of the integrated data is more important than the technology or standards used; in this regard, the methodology can be implemented in a variety of frameworks, from RDF graphs to traditional relational databases, as needed by the user. Other graph indicators could be used in the future for similar tasks and to quantify different problem types coming from data integration: As an example, we decided not to assign weights to the edges of the graphs. This choice was motivated by the sparsity of the data; we estimated that records sharing one label were just as likely to be equivalent to records sharing multiple labels. Considering weights could result in the computation of different graph metrics, useful to characterize a particular type of records, assuming a rationale could be defined for the alternative indicators the same way it was done here with the density and BC. A particularly challenging task resulting from data integration initiatives is the maintenance of the created resource, the update of records and the incorporation of new information. This can be implemented alongside the methodology proposed, in different ways: First it is possible to create incremental versions of the warehouse. With this approach, the new data sources are downloaded following a certain time period, and the integrated dataset is rebuilt from scratch each time, following the same graph based methodology. New and corrected records are tested again, and excluded if necessary. The main drawback of this option comes from the difficulty of maintaining unique identifiers, as equivalences can vary from one release to the other. The second possibility for data update considers the current data warehouse just as any standard starting source, and tries to match and disambiguate new records following the same methodology as presented in this article. Identifiers are easier to maintain as the new version derives from the old one, but it might become challenging to keep track of the origin of the data. Removed information from starting sources, such as synonyms, can also be trickier to detect and correct. In conclusion, the goal of data integration is to provide a complete picture of a scientific problem; we introduced here a methodology emphasizing data coherence and correctness to fulfill this greater task, where ambiguities, redundancies and errors are identified and removed. In summary, the methodology presented addresses practical concerns faced by large scale data integration exercises, prevalent nowadays in the drug discovery domain. In an ideal world, database entries and cross references would be perfectly maintained and errors non-existent, which would enable the straightforward creation of a semantic network between entities. In practice, a costly and tedious curation step is often necessary in order to control the quality of the resource and the scientific value of its content. The presented methodology focuses on this aspect, in order to best prepare the integrated data for the derivation of scientific knowledge.
