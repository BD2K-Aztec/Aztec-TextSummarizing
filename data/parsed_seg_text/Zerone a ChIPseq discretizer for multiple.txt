Motivation: Chromatin immunoprecipitation followed by high throughput sequencing chips eq is the standard method to investigate chromatin protein composition. As the number of community available chips eq profiles increases, it becomes more common to use data from different sources, which makes joint analysis challenging. Issues such as lack of reproducibility, heterogeneous quality and conflicts between replicates become evident when comparing datasets, especially when they are produced by different laboratories. Results: Here, we present zero ne a chips eq discretize r with built in quality control. zero ne is powered by a Hidden Markov Model with zero inflated negative multinomial emissions, which allows it to merge several replicates into a single discretized profile. To identify low quality or ir reproducible data, we trained a Support Vector Machine and integrated it as part of the discretization process. The result is a classifier reaching 95% accuracy in detecting low quality profiles. We also introduce a graphical representation to compare discretization quality and we show that zero ne achieves outstanding accuracy. Finally, on current hardware, zero ne discretize s a chips eq experiment on mammalian genomes in about 5 min using less than 700 MB of memory. Availability and Implementation: zero ne is available as a command line tool and as an R package. The C source code and R scripts can be downloaded from https://github.com/nanakiksc/zerone. The information to reproduce the benchmark and the figures is stored in a public Docker image that can be downloaded from https://hub.docker.com/r/nanakiksc/zerone/.

introduction one of the major challenges of biology is to understand how transcription factors and chromatin proteins coordinate transcription, replication and repair. In front of this colossal task, the community invests massive research efforts into collecting protein genome interaction data. Chromatin immunoprecipitation followed by high throughput sequencing chips eq has become the standard method to identify the targets of a transcription factor or a histone modification in a cell population. However, ChIP is not fully understood and artifacts are still discovered more than 10 years after its adoption as a standard (). Besides, the constant improvement of sequencing technologies makes analysis of chips eq profiles difficult to standardize. There is thus a need to continuously develop and improve computational tools to analyze chips eq data. One of the most common analyses performed on chips eq profiles is to discretize the signal, i.e. identify the loci where the transcription factor (or other feature) is present. This makes the signal simpler to interpret, it removes part of the experimental noise, it simplifies downstream analyses and it allows comparing or combining profiles of different natures. This raises a challenge at the computational level because discretization has to be carried out uniformly for signals that may be very different. For instance, lamins bind in mega base scale domains covering 40% of the genome (), whereas transcription factors may bind as few as 6 bp with a coverage below 1%. Large consortia such as ENCODE have brought to light severe issues related to the quality of chips eq data. Conflicts between replicates are common, and sometimes laboratory effects are clearly detectable in the data, even when experimentalists use the same material and follow the same protocol (our unpublished observations). The most popular remedy is to use a metric called IDR ir reproducible Discovery Rate,), which allows weeding out poorly reproducible signal. This approach is a significant step forward, but the IDR is undefined when more than two replicates are available. Besides, keeping only the reproducible chips eq peaks is not always the best option. If one of the replicates is mis labelled for instance, it is more appropriate to reject the dataset than to keep the common peaks. In summary, how to integrate chips eq data from different sources and with variable qualities is still an open problem. Here, we propose an approach to discretize chips eq data where conflict resolution and quality control are integrated in a tool that we called zero ne (Pronounced zi roun or z iron i.e. as inserting 'ear' in 'zone'.). The key idea of zero ne is to combine an arbitrary number of chips eq replicates in a single discretized profile, where conflicts are resolved by maximizing the likelihood of the underlying statistical model. Following discretization, zero ne controls the quality of its output in order to detect potential anomalies, and when applicable rejects the output as a whole. Internally, the first step implements a Hidden Markov Model (HMM) with zero inflated negative multinomial zin m emissions, and the second implements a Support Vector Machine (SVM) trained using ENCODE chips eq data. hmm based discretization is agnostic about the shape of the signal (broad domains or sharp peaks) and the zin m distribution captures the essential features of the read count distribution in chips eq data. zero ne is designed for large volume pipelines aiming to combine many chips eq profiles with little human intervention. To this end, it is compatible with the standard BED, samba m and GEM formats, it produces congruent window based outputs, and it can process hundreds of experiments per day on average hardware. We benchmarked zero ne against MACS (), bayes peak () and JAMM () on the core task of discretizing chips eq profiles of CTCF and H3K36me3. Our results show that zero ne is competitive in terms of speed and accuracy.
