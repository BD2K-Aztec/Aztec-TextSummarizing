Motivation: Bioinformatics tools, such as assemblers and aligners, are expected to produce more accurate results when given better quality sequence data as their starting point. This expectation has led to the development of stand-alone tools whose sole purpose is to detect and remove sequencing errors. A good error correcting tool would be a transparent component in a bioinformatics pipeline, simply taking sequence data in any of the standard formats and producing a higher quality version of the same data containing far fewer errors. It should not only be able to correct all of the types of errors found in real sequence data (substitutions, insertions, deletions and uncalled bases), but it has to be both fast enough and scalable enough to be usable on the large datasets being produced by current sequencing technologies, and work on data derived from both haploid and diploid organisms. Results: This article presents Blue, an error correction algorithm based on km er consensus and context. Blue can correct substitution, deletion and insertion errors, as well as uncalled bases. It accepts both fast q and fast a formats, and corrects quality scores for corrected bases. Blue also maintains the pairing of reads, both within a file and between pairs of files, making it compatible with downstream tools that depend on read pairing. Blue is memory efficient, scalable and faster than other published tools, and usable on large sequencing datasets. On the tests undertaken, Blue also proved to be generally more accurate than other published algorithms, resulting in more accurately aligned reads and the assembly of longer contigs containing fewer errors. One significant feature of Blue is that its km er consensus table does not have to be derived from the set of reads being corrected. This decoupling makes it possible to correct one dataset, such as small set of 454 mate pair reads, with the consensus derived from another dataset, such as Illumina reads derived from the same DNA sample. Such cross correction can greatly improve the quality of small (and expensive) sets of long reads, leading to even better assemblies and higher quality finished genomes. Availability and implementation: The code for Blue and its related tools are available from http://www.bioinformatics.csiro.au/Blue. These programs are written in C# and run natively under Windows and under Mono on Linux.

introduction the introduction of the first 454 Life Sciences sequencer in 2005 marked the beginning of a revolution in biological research. Sequencing technology has continued to advance rapidly, producing ever more data at a lower cost, but the quality of these data have improved at a much slower rate. A single run on an Illumina his eq 2500 system can now produce up to 8 billion paired end reads, but these will still have an overall error rate of 12% (). The nature of these errors depends on the sequencing technology being used and its underlying biochemistry. The single base at a time 'sequencing by synthesis' technique used by Illumina results mostly in substitution errors (). Technologies based on different chemistries, such as those used by 454 and Ion Torrent systems, are prone to mis report the length of strings of the same base (homopolymers), resulting in insertion and deletion errors (). The tools used to analyze sequence data are all error tolerant to some extent. Aligners will tolerate some number of mismatches when they are mapping reads to a reference, some of which will prove to be errors and other genuine differences between the organism being sequenced and the reference (). Similarly, assemblers can be built to tolerate errors to some degree, and their success at doing this is a significant factor in their overall effectiveness and accuracy (). An alternative way of addressing the problem of sequencing errors is to use a stand-alone error correction tool whose sole purpose is to take a set of reads and improve their quality by finding and fixing errors. Such tools are founded on the high levels of redundancy present in typical sequencing datasets, with each location in the sequenced genome being covered by many reads, most of which will agree about which base is actually present recently surveyed a number of the published error correction tools and categorized them into three classes of algorithms: k spectrum based, suffix tree array based and multiple sequence alignment based. The three classes of algorithm differ both in how they detect errors and how these errors are corrected. We refer the reader to for a full discussion of these three classes of algorithms and their history. Blue is a k spectrum algorithm that uses read context to choose between alternative replacement km ers with the overall goal of minimizing the number of changes needed to correct an entire read. All k spectrum based algorithms first tile their input reads to produce a set of distinct overlapping subsequences of length k to whom correspondence should be addressed. km ers together with their repetition counts. Such a set can then be used to distinguish km ers that come from the organism being sequenced (and so recur many times) from those that are derived from reads containing sequencing errors (typically only appearing once or a few times shows a km er repetition histogram for a set of Illumina reads derived from a typical bacterium (Clostridium sp oro genes PA3679). Those km ers from the error free parts of reads will have repetition counts that lie somewhere on the right hand side (RHS) peak in this histogram shows a comparable histogram for a strongly heterozygous diploid organism (Helicoverpa armigera) with two (overlapping) peaks, one corresponding to the km ers found on both alleles, and the other to those km ers found on only one. finally shows the histogram derived from tiling four lanes of Homo sapiens data (Illumina his eq from SRA ERR091571 to ERR091574). Given such a 'consensus' table of km ers and counts, a repetition depth threshold can then be used to identify 'good' km ers as shown in. A simple dataset wide threshold is unlikely to be usable though, as uneven coverage along a genome and the presence of repetitive regions is likely to result in the rejection of correct km ers in poorly covered areas and the acceptance of sequencing errors in high coverage areas. Blue uses a partitioned hash table to hold the km ers corresponding to the RHS peaks: the 'consensus' about what km ers are really present in the genome being sequenced. The data loaded into this table is generated by the associated tiling tool, tess el which simply takes a set of reads, tiles it into overlapping km ers and writes out a file of distinct canonical km ers and their repetition counts (for each strand). Decoupling the building of the consensus from the correction algorithm in this way makes it possible to use Blue to cross correct read datasets, such as using a large and inexpensive Illumina dataset to correct a smaller, more expensive but longer set of 454 reads. This style of cross correction results in a 454 dataset that conforms to the consensus found in the Illumina data, effectively generating long Illumina reads that can be used to great effect in assemblies and when finishing genomes. Repetitive regions in genomes, including ribosomes, transposons and shared regulatory sequences, are challenging for all error correction algorithms. Reads that cross the boundaries of these repeated regions may be erroneously 'corrected', as the change in depth of coverage at their edges may look very much like an error. The choice of which possible fix is correct (including doing nothing) really depends on context, and can not simply be decided purely by considering a single km er or similar short sequence in isolation. Blue addresses this problem by evaluating alternative fixes in the context of the read being corrected. The metrics computed for every alternative reflect the impact that each one would have on the rest of the read will this fix get us to the end of the read with no (or few) additional fixes, or will we have to effectively rewrite much of the rest of the read? It does this by recursively exploring the tree of potential corrected reads. The next section discusses the approach we took to testing Blue's performance and effectiveness, and comparing it with other published tools. Section 3 describes the Blue error correction algorithm, and Section 4 discusses the results of the performance and effectiveness tests. Section 5 discusses future work and possible improvements.

discussion the primary goal in the development of Blue was to create a practical tool that would help biologists get more accurate results from their sequencing datasets. Blue had to be sufficiently fast and memory efficient to allow it to correct to days large datasets using reasonable resources, and effectively transparent so it could be used within existing analytical workflow tools such as Galaxy, just taking in a sequencing dataset and writing it out again after removing as many errors as possible while maintaining file formats, quality scores and read pairings our tests have shown that Blue meets these goals. It is faster than the other algorithms tested, and its low memory requirements make it practical to use with current large sequencing datasets. Blue has been shown to be more accurate than any of the other algorithms we tested on both Illumina and 454 data. The assembly tests showed that blue corrected reads consistently produced the longest and most error free contigs of all the tools tested. Blue's ability to correct insertion and deletion errors allows it to be used with great effect on datasets generated on the 454 and Ion Torrent platforms. Decoupling the reads being corrected from the set of reads used to generate the km er consensus table allows for cross correcting long homopolymer prone reads with short but cheaper Illumina reads, resulting in even better correction of these datasets. Blue has already been used to improve the assemblies for published microbial genomes derived from pure cultures (). It has also been used on meta genomic datasets to improve draft genome assemblies of the dominant organisms in these communities (). Correcting meta genomic sequence datasets works only when the dominant organisms are taxonomically distant, and so share few km ers (). In this case, correcting the reads has the useful side effect of removing rare variants of the dominant organisms, giving both better assemblies and improving the performance of the assemblers themselves. Blue has also been successfully used on diploid data, both human and insect. Blue is currently being used on a major insect genome project, and its ability to cross correct long mate pair 454 reads with Illumina data have proven to be useful to this team. Blue will continue to be tested and refined on new types of sequencing data as these emerge, with an immediate focus on pac bio. Another area of anticipated work is improving the correction of diploid data at those places where differences between the two alleles cause difficulties for assemblers.
