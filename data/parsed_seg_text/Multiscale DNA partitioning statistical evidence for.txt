Motivation: DNA segmentation, i.e. the partitioning of DNA in com-positionally homogeneous segments, is a basic task in bioinformatics. Different algorithms have been proposed for various partitioning criteria such as guanine cytosine (GC) content, local ancestry in population genetics or copy number variation. A critical component of any such method is the choice of an appropriate number of segments. Some methods use model selection criteria and do not provide a suitable error control. Other methods that are based on simulating a statistic under a null model provide suitable error control only if the correct null model is chosen. Results: Here, we focus on partitioning with respect to GC content and propose a new approach that provides statistical error control: as in statistical hypothesis testing, it guarantees with a user specified probability 1 Ã€ that the number of identified segments does not exceed the number of actually present segments. The method is based on a statistical multiscale criterion, rendering this as a segmen-tation method that searches segments of any length (on all scales) simultaneously. It is also accurate in localizing segments: under benchmark scenarios, our approach leads to a segmentation that is more accurate than the approaches discussed in the comparative review of el haik et al. In our real data examples, we find segments that often correspond well to features taken from standard University of California at Santa Cruz (UCSC) genome annotation tracks. Availability and implementation: Our method is implemented in function smu cer of the r package step r available at

introduction it has been observed a long time ago () that DNA sequences are not composed homogeneously and that bases fluctuate in their frequency. These inhomogeneities often have an evolutionary or a functional interpretation, and can be relevant for the subsequent analysis of sequence data. Because it correlates with many features of interest, the GC content, i.e. the relative frequency of the bases G and C, is one of the most commonly studied sequence properties. large scale regions, typically 300 kb (), of approximately homogeneous GC content have been called iso chores. In view of the somewhat vague notion of 'approximate homogeneity' and conceptual criticism in studies such as or there is less interest in iso chores nowadays. However, there is no doubt about variation in GC content along genomes, and search is done instead for domains of any length exhibiting distinct local GC content; see, for instance, el haik et al. (2010b). Several factors can influence the GC content of a region. At larger scales, it correlates with the density of genes, with generic h regions typically exhibiting an elevated GC content compared with regions of low gene density. At smaller scales, there is fluctuation in the GC content, for instance, because of repetitive elements and GpC islands. The GC content is also known to vary between exons and introns. Especially for long introns, their lower GC content seems to play a role in splice site recognition (). There is also a correlation between the GC content and the local recombination rate (). For a further discussion of features correlated to the GC content, see. In gene expression studies, regions of homogeneous GC content are of interest because the GC content of a region affects the number of reads mapped to this region. For DNA and rnase q experiments with the Illumina Genome Analyzer platform, this has been, for instance, investigated in benjamin i and Speed (2012) and. Segmentation algorithms aim to partition a given DNA sequence into stretches that are homogeneous in their composition but differ from neighboring segments. The classical approach of using moving windows is simple and available, for instance, as an option with the UCSC and Ensembl genome browsers. However, it has some disadvantages. For instance, the choice of the window size is difficult because it defines implicitly a fixed scale at which segments primarily will be detected. Further, the involved smoothing blurs abrupt changes. Without additional statistical criteria, the method also does not tell us whether differences between neighboring windows are statistically significant. Therefore, several more sophisticated approaches have been proposed. These methods include hidden Markov models () and walking Markov models (). There are also change point methods available; see, for instance,. A Bayesian approach *To whom correspondence should be addressed. that relies on the Gibbs sampler has been proposed by. An older approach based on information criteria can be found in. Furthermore, recently developed methods based on entropy criteria have been shown to perform particularly well; see el haik et al. (2010a) and el haik et al. (2010b). A review of segmentation methods can be found in the article by Braun and Muler (1998), and for a more recent comparative evaluation of the more popular approaches, see. In this paper, we focus on binary segmentation, where the four letter alphabet of a DNA sequence is converted into a two letter code. For GC content, we set the response to be '1' for G or C at a position and 0 for A/T; we use Y i to denote the response at position i and summarize the responses for a sequence of length n by Y  Y 1 , Y 2 ,. .. , Y n : 1We model the responses Y i to be independent and Bernoulli Bin1, i  distributed, and also assume that there is a partition 0  0 5 1 5    5 K  n into an unknown number K of segments on which the i are piecewise constant, i.e. i  p j for i 2 I j. Here, I j :  j1 , j  denotes the j'th segment with response probability p j for 1 j K. A segmentation algorithm provides estimates ^ K for the number of segments, for the internal segment boundaries and for the response probabilities, ^ p j , on the estimated segments in the following, we will identify a segmentation with p, I, where p  p 1 ,. .. , p K  and I  I 1 ,. .. , I K . Our proposed algorithm provides a parsimonious estimate ^ K for K: ^ K will not exceed the actual number of segments K, except for a small user specified error probability ; as a default value, we suggest  5%, the error probability also chosen in our simulations and data analyses. Relaxing this significance level to a larger value, say  20%, will typically lead to more identified segments but at the cost of statistical accuracy.
