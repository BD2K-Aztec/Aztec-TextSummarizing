Motivation: Counting the frequencies of km ers in read libraries is often a first step in the analysis of high throughput sequencing data. Infrequent km ers are assumed to be a result of sequencing errors. The frequent km ers constitute a reduced but error free representation of the experiment, which can inform read error correction or serve as the input to de novo assembly methods. Ideally, the memory requirement for counting should be linear in the number of frequent km ers and not in the, typically much larger, total number of km ers in the read library. Results: We present a novel method that balances time, space and accuracy requirements to efficiently extract frequent km ers even for high coverage libraries and large genomes such as human. Our method is designed to minimize cache misses in a cache efficient manner by using a pattern blocked Bloom filter to remove infrequent km ers from consideration in combination with a novel sort and compact scheme, instead of a hash, for the actual counting. Although this increases theoretical complexity, the savings in cache misses reduce the empirical running times. A variant of method can resort to a counting Bloom filter for even larger savings in memory at the expense of false negative rates in addition to the false positive rates common to all Bloom filter based approaches. A comparison with the state of the art shows reduced memory requirements and running times. Availability and implementation: The tools are freely available for download at

introduction km ers play an important role in many methods in bioinformatics because they are at the core of the de Bruijn graph structure () that underlies many of to days popular de novo assemblers (). They are also used in assemblers based on the overlap layout consensus paradigm like Celera () and Arachne () as seeds to find overlap between reads. Several read correction tools () use km er frequencies for error correction. Their main motivation for counting km ers is to filter out or correct sequencing errors by relying on km ers that appear multiple times and can thus be assumed to reflect the true sequence of the donor genome. In contrast, km ers that appear only once are assumed to contain sequencing errors. mels ted and Pritchard (2011) and marca is and Kingsford (2011) make a more detailed compelling argument about the importance of km er counting. In a genome of size g, we expect up to g unique km ers. This number can be smaller because of repeated regions (which produce the same km ers and small k, as smaller km ers are less likely to be unique, but is usually close to g for reasonable values of k. However, depending on the amount of sequencing errors, the total number of km ers in the read library can be substantially larger than g. For example, in the DM dataset (), the total number of 31-mers is $289.20 M, whereas the number of 31-mers occurring at least twice is $131.82 M. The size of the genome is 122 Mb mega base pairs). This is not surprising because one base call error in a read can introduce up to k false km ers. Consequently, counting the frequency of all km ers as done by Jellyfish (), which is limited to k 31, requires O(N) space where N is the number of km ers in the read library. This makes the problem of km er frequency counting time and memory intensive for large read libraries like human. We encounter similar problems for large libraries while using Khmer (), which uses a Bloom filter based () approach for counting frequencies of all km ers. Ideally, the frequent km er identifier should use O(n) space where n is the number of frequent km ers (n ( N). The approach taken by bf counter () achieves something close to this optimum by ignoring the infrequent km ers with a Bloom filter and explicitly storing only frequent km ers. This makes bf counter more memory efficient compared with Jellyfish. However, the running time of bf counter is large for two reasons. First, it is not multi-threaded. Second, both the Bloom filter and the hash table used for counting incur frequent cache misses. The latter has recently been identified as a major obstacle to achieving high performance on modern architectures, motivating the development of cache oblivious algorithms and data structures (), which optimize the cache behavior without relying on information of cache layout and sizes. Additionally, bf counter is also limited to a count range of 0255, which will often be exceeded in single cell experiments because of the large local coverage produced by whole genome amplification artifacts. A different approach is taken by DSK () to improve memory efficiency. DSK makes many passes over the read file and uses temporary disk space to trade off the memory requirement. although claimed DSK to be faster than bf counter on our machine *To whom correspondence should be addressed. using an 18 TB Raid-6 storage system; DSK required more wall clock time compared with bf counter. Therefore, we consider DSK without dedicated high performance disks, e.g. solid state, and bf counter to be too slow for practical use on large datasets. A disk based sorting and compaction approach is taken by KMC (), which was published very recently, and it is capable of counting km ers of large read libraries with a limited amount of memory. However, in our test environment, we found it to be slower than the method described here. We present a novel approach that reduces the memory footprint to accommodate large genomes and high coverage libraries. One of our tools sc turtle can report frequent 31mers with counts (with a very low false positive rate) from a human read set with 135.3 Gb using 109 GB of memory in 52 h using 19 worker threads. Like bf counter our approach also uses a Bloom filter to screen out km ers with frequency one (with a small false positive rate), but in contrast to bf counter we use a pattern blocked Bloom filter (). The expected number of cache misses for each inquiry update in such a Bloom filter is one. The frequency of the remaining km ers is counted with a novel sorting and compaction based algorithm. Our compaction step is similar to run length encoding (). Note that this is similar to the strategy of KMC, which was developed as a concurrent and independent work. Though the complexity of sorting in our compression step is On log n, it has sequential and localized memory access that helps in avoiding cache misses and will run faster than an O(n) algorithm that has O(n) cache misses as long as log n is much smaller than the penalty issued by a cache miss. For larger datasets, where O(n) space is not available, the aforementioned method will fail. We show that it is possible to get a reasonable approximate solution to this problem by accepting small false positive and false negative rates. The method is based on a counting Bloom filter implementation. The error rates can be made arbitrarily small by making the Bloom filter larger. Because the count is not maintained in this method, it reports only the km ers seen more than once (with a small false positive and false negative rate), but not their frequency. We call the first tool sc turtle and the second one c turtle
conclusion identifying correct km ers out of the km er spectrum of a read library is an important step in many methods in bioinformatics. Usually, this distinction is made by the frequency of the km ers. Fast tools for counting km er frequencies exist, but for large read libraries, they may demand a significant amount of memory, which can make the problem computationally unsolvable on machines with moderate amounts of memory resource ( 128 GB or even with 256 GB for large datasets). Simple memory efficient methods, on the other hand, can be time consuming. Unfortunately, there is no single tool that achieves a reasonable compromise between memory and time. Here we present a set of tools that make some compromises and simultaneously achieve memory and time requirements that are matching the current state of the art in both aspects. With our first tool sc turtle we achieve memory efficiency by filtering km ers of frequency one with a Bloom filter. Our pattern blocked Bloom filter implementation is more time efficient compared with a regular Bloom filter. We present a novel strategy based on sorting and compaction for storing frequent km ers and their counts. Because of its sequential memory access pattern, our algorithm is cache efficient and achieves good running time. However, because of the Bloom filters, we incur a small false positive rate. The second tool c turtle is designed to be more memory efficient at the cost of giving up the frequency values and allowing both false positive and false negative rates. The implementation note The tools ran with fast mod and 31 worker threads. Each reported number is an average of five runs note For the large datasets, because of memory constraints, the exact counts for all km ers could not be obtained, and therefore, these rates could not be computed. is based on a counting Bloom filter that keeps track of whether a km er was observed and whether it has been stored in external media. This tool does not report the frequency count of the km ers. Both tools allow a km er size of up to 64. They also allow the user to decide how much memory should be consumed. Of course, there is a minimum memory requirement for each dataset, and the amount of memory directly influences the running time and error rate. However, we believe, with the proper compromises, the approximate frequent km er extraction problem is now computationally feasible for large read libraries within reasonable wall clock time using a moderate amount of memory.
