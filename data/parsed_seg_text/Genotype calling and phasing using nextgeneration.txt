Motivation: Given the current costs of next generation sequencing, large studies carry out low coverage sequencing followed by application of methods that leverage linkage disequilibrium to infer genotypes. We propose a novel method that assumes study samples are sequenced at low coverage and genotyped on a genome wide micro-array, as in the 1000 Genomes Project (1KGP). We assume poly-morphic sites have been detected from the sequencing data and that genotype likelihoods are available at these sites. We also assume that the microarray genotypes have been phased to construct a haplotype scaffold. We then phase each polymorphic site using an MCMC algorithm that iteratively updates the unobserved alleles based on the genotype likelihoods at that site and local haplotype information. We use a multivariate normal model to capture both allele frequency and linkage disequilibrium information around each site. When sequencing data are available from trios, Mendelian transmission constraints are easily accommodated into the updates. The method is highly parallelizable, as it analyses one position at a time. Results: We illustrate the performance of the method compared with other methods using data from Phase 1 of the 1KGP in terms of genotype accuracy, phasing accuracy and downstream imputation performance. We show that the haplotype panel we infer in African samples, which was based on a trio phased scaffold, increases downstream imputation accuracy for rare variants (R2 increases by 40.05 for minor allele frequency 51%), and this will translate into a boost in power to detect associations. These results highlight the value of incorporating microarray genotypes when calling variants from next generation sequence data.

introduction genome wide association studies g was are known to be well powered for the detection of common variants that increase disease risk, and have uncovered many replicated associations in recent years. However, the effect sizes of many associated loci are relatively small and together explain only a small proportion of the heritability for many diseases and traits. It has been argued that rare variants with larger effects might harbour the remaining genetic variability (). next generation sequencing allows whole genome sequencing of a large sample of individuals so that a more detailed picture of human polymorphism can be obtained. Projects such as the 1KGP () are working to produce a catalogue of polymorphisms with low (minor allele frequency 2 and rare (MAF50.005) allele frequencies. The current analysis pipeline starts by detecting sites exhibiting evidence of polymorphism. Genotypes are then called at the polymorphic sites, and then haplotypes are estimated across all such sites, often as a by product of calling the genotypes. The sets of haplotypes produced can be used as reference panels for imputation into g was () and allow investigations of rare variant associations that go beyond previous imputation analyses from less complete reference panels such as HapMap. Given the current costs for sequencing, it is still not viable to deeply sequence a large number of individuals, so given a fixed amount of resources, there is a trade-off between sample size and coverage. The consensus view () favours low coverage sequencing of large numbers of samples. For example, the 1KGP will be sequencing $2500 individuals at $4, the UK10K project will sequence 4000 whole genomes at $6 (http://www.uk10k.org/), and the Genome of the Netherlands is sequencing 750 individuals at $12 (http:// www nl genome com. The current paradigm for detecting, genotyping and phasing polymorphic sites from low coverage sequence data starts by mapping sequence reads to a reference genome. Mapped reads that overlap a given site (s) in a single individual (i) are then combined together to form genotype likelihoods (GLs). GLs are the probabilities of observing the reads given the underlying (unknown) genotype and can be written as PR is jG is , where R is and G is denote the reads at site s and the true underlying genotype, respectively. Methods exist for calculating these GLs () and involve combining information about the bases observed in reads, while allowing for the possibility of sequencing errors via base and mapping quality scores. So, for example, if the set of reads only contains the A allele, then the GL PR is jG is  AA will be larger than the other likelihoods, but other genotypes will still have non-zero likelihood. As sequence *To whom correspondence should be addressed. coverage increases, the likelihoods should become more 'peaked' around the true genotype. In low coverage sequencing, there may be no reads spanning a site, in which case, the GLs will be identical across all possible genotypes. Detecting polymorphic sites involves combining information across individuals at a site to infer whether there are at least two alleles observed across all individuals in the sample (). Once a site is detected, the site's genotypes can be called using a missing data likelihood that is optimised via an EM algorithm (), but this will only work well when coverage is high and the GLs contain very good information about the genotypes of all samples. When coverage is low, power to call genotypes can be gained by taking advantage of linkage disequilibrium (LD) between sites in close proximity. Methods that do this are extensions of phasing and imputation algorithms that pool information across samples and sites to infer multi-site genotypes and their underlying haplotypes (e.g IMPUTE2, Beagle [Browning and and MACH). In some studies, such as g was the individuals being sequenced will also have been genotyped on a genome wide single nucleotide polymorphism (SNP) chip. For example, in the 1KGP, individuals are both sequenced and genotyped on the Illumina OMNI2.5M chip. Some commercial vendors also return g was array data with sequencing results, so this design may become more prominent in the future. Up to now, chip genotypes have primarily been used to validate sequencing based genotype calls, as the error rates of SNP chips are generally low (50.5%) (O'). In this article, we present a method that uses SNP chip genotypes to aid the process of genotype calling from sequencing reads. The first step of our method involves estimating haplotypes at the chip SNPs using an accurate phasing method () to form a haplotype scaffold. We then call genotypes at a polymorphic site using a model of the LD between the site and the surrounding sites in the haplotype scaffold. We use an Markov Chain Monte Carlo (MCMC) algorithm that iteratively updates the unobserved alleles at a site using the GLs and the local LD. We use a multivariate normal model to capture both allele frequency and LD information around each site that results in fast MCMC updates. When sequencing data are available from mother father child trios, Mendelian transmission constraints are easily accommodated into the updates. The method analyses one position at a time and results in a highly parallelizable method that can be applied to a large sample. We call our method mvn call. Our approach is similar to the q call approach (), which also analyses one SNP at a time using a haplotype scaffold. q call builds approximate coalescent trees at each site and then infers phased genotypes by considering possible mutations on branches of the trees. This approach does not scale well as the number of sequences increases and is the reason why this method was not applied to the Phase 1 of the 1KGP. mvn call is much faster and has enabled us to analyse the Phase 1 dataset. In the sections that follow, we present in detail the underlying model developed for genotype calling and phasing. We apply our method and other methods to chromosome 20 of the Phase 1 of the 1KGP. We carry out an imputation analysis using the haplotype sets inferred by the different methods as reference panels, and we compare imputation accuracy.

discussion our proposed model for genotype calling and phasing assumes a study design where samples have been both sequenced and genotyped. The 1KGP uses exactly this design and this was a major motivation for developing such an approach. In addition, many cohorts of samples from g was have been genotyped using microarray chips, and it is likely that these samples might be sequenced in the future, providing a combined set of data that also fits in with our approach. In fact, some commercial vendors also return g was array data with sequencing results. The genotyped data from the microarrays can be fed into a phasing algorithm, and the resulting haplotypes serve as a haplotype scaffold in our model so that LD information between the scaffold sites and non scaffold sites can be utilised. To capture this LD information, we use an approximate model that summarises the allele frequencies and the pairwise correlation between SNPs using a multivariate normal distribution. There is an increasing body of literature that the use of a normal distribution can capture enough information about correlations in alleles both between SNPs and between individuals to provide useful levels of inference (). Future extensions might utilise a normal approximation to efficiently call genotypes from low coverage sequencing data together with a haplotype reference panel () and without a haplotype scaffold. Our model would also be relatively easy to extend to other types of polymorphism such as indels and multi allelic variants. Utilising phase informative reads when constructing haplotype panels from sequence data may also prove to be beneficial (). When applied to data from Phase 1 of the 1KGP that include 1092 individuals from 14 distinct population backgrounds sequenced at 4 and genotyped on the OMNI2.5M chip, we find that our method outperforms Beagle in all population groups in terms of genotype accuracy. This is an interesting result because mvn call analyses one site at a time rather than all sites jointly as in Beagle. It may be argued that jointly modelling all sites at once is a more appropriate strategy because this approach utilises LD information between all the sites being called. On the other hand, when the calling of genotypes is difficult, which can be the case when sequence coverage is low, it may be that the Beagle's joint approach suffers from convergence problems and the resulting haplotypes include errors due to this problem. The methods snp tools and Thunder also model all sites jointly but perform slightly better than mvn call on the European and Asian comparisons. It may be that the underlying models they use are much better than the model used in Beagle, or that the computational strategies that they use do a better job at avoiding convergence problems. A major use of the haplotype panels produced by the 1KGP is imputation of genotypes into g was. It was thus natural that we assess the ability of mvn call to produce haplotype panels in terms of downstream imputation performance. A key finding of this work is that the panel of haplotypes that we produce from the 1KGP African samples results in a clear boost in downstream imputation performance at rare variants, when compared with the haplotype panels produced by snp tools Thunder or Beagle. mvn call results in a 0.05 increase in mean R 2 at rare variants below 1% frequency when compared with other methods. This is a substantial boost in accuracy, as studies of rare variants are in general less powerful. The R 2 metric used for our assessments has direct relevance to the power of such studies (). In Admixed samples, mvn call produces similar results to snp tools and both these methods have better performance than Thunder and Beagle at rare variants. We do not see the same increase in imputation performance when imputing European samples. This may be because a substantial proportion (47%) of African samples in the reference panel have a haplotype scaffold that has been derived using trio phasing. We have shown that the haplotypes our method constructs when using a trio phased scaffold have much lower switch error rate compared with the haplotypes from a scaffold phased without family information. It may be that it is this increase in haplotype accuracy that translates into a downstream effect on imputation performance. In contrast, only 4.4% of the European haplotypes in the reference panel were derived from a trio phased scaffold. Almost 85% of the Mexican samples in the haplotype reference panel were derived using a trio based scaffold, which might lead us to expect that imputation in Mexican samples should be better when using mvn call reference haplotypes. The ancestry of haplotype diversity across the genome and this degrades imputation performance and drives the similarity between methods. Imputation of rare variants in Mexican samples is noticeably worse (Supplementary) than in the African and European samples (and Supplementary). Overall these results highlight the gains that can be made when using microarray genotypes to improve genotype calls and phasing using low coverage sequence data. Future analysis of the 1000 Genomes Project data would benefit from fully incorporating the Illumina OMNI2.5M genotypes into the genotype calling and haplotype estimation process.
