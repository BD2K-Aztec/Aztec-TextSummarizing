Motivation: high coverage sequencing data have significant, yet hard to exploit, redundancy. Most fast q compressors can not efficiently compress the DNA stream of large datasets, since the redundancy between overlapping reads can not be easily captured in the (relatively small) main memory. More interesting solutions for this problem are disk based, where the better of these two, from Cox et al. (2012), is based on the burrows wheeler transform (BWT) and achieves 0.518 bits per base for a 134.0 Gbp human genome sequencing collection with almost 45-fold coverage. Results: We propose overlapping reads compression with minimizers, a compression algorithm dedicated to sequencing reads (DNA only). Our method makes use of a conceptually simple and easily parallelizable idea of minimizers, to obtain 0.317 bits per base as the compression ratio, allowing to fit the 134.0 Gbp dataset into only 5.31 GB of space. Availability and implementation: http://sun.aei.polsl.pl/orcom under a free license.

introduction it is well known that the growth of the amount of genome sequencing data produced in the last years outpaces the famous Moore's law predicting the developments in computer hardware (). Confronted with this deluge of data, we can only hope for better algorithms protecting us from drowning. Speaking about big data management in general, there are two main algorithmic concerns: faster processing of the data (at preserved other aspects, like mapping quality in de novo or referential assemblers) and more succinct data representations (for compressed storage or indexes). In this article, we focus on the latter concern. Raw sequencing data are usually kept in fast q format, with two main streams: the DNA symbols and their corresponding quality scores. Older specialized fast q compressors were lossless, squeezing the DNA stream down to about 1.51.8 bpb (bits per base) and the quality stream to 34 bpb, but more recently it was noticed that a reasonable solution for lossy compression of the qualities has negligible impact on further analyzes, for example, referential mapping or variant calling performance (). This scenario became thus immediately practical, with scores loss ily compressed to about 1 bpb () or less (). Note also that Illumina software for their his eq 2500 equipment contains an option to reduce the number of quality scores (even to a few), since it was shown that the fraction of discrepant single nucleotide polymorphisms grows slowly with diminishing number of quality scores in illumina s casa va package (http://support.illumina.com/sequencing/sequencing_software/casava.ilmn). It is easy to notice that now the DNA stream becomes the main compression challenge. Even if higher order modeling () or lz77 style compression () can lead to some improvement in DNA stream compression, we are aware of only two much more promising approaches. Both solutions are disk based. Yanovsky (2011) creates a similarity graph for the dataset, defined as a weighted undirected graph with vertices corresponding to the reads of the dataset. For any two reads s 1 and s 2 the edge weight between them is related to the 'profitability' of storing s 1 and the edit script for transforming it into s 2 versus storing both reads explicitly. For this graph, its minimum spanning tree (MST) is found. During the MST traversal, each node is encoded using the set of maximum exact matches between the node's read and the read of its parent in the MST. As a backend compressor, the popular 7zip is used. ReCoil compresses a dataset of 192 M Illumina 36 bp reads (http://www.ncbi.nlm.nih.gov/sra/SRX001540), with coverage below 3-fold, to 1.34 bpb. This is an interesting result, but ReCoil is hardly scalable; the test took about 14 h on a machine with 1.6 GHz Intel Celeron CPU and four hard disks. More recently, Cox et al. (2012) took a different approach, based on the burrows wheeler transform (BWT). Their result for the same dataset was 1.21 bpb in less than 65 min, on a Xeon X5450 quad core 3 GHz processor. The achievement is however more spectacular if the dataset coverage grows. For 44.5-fold coverage of real human genome sequence data, the compressed size improves to as little as 0.518 bpb (Actually in the authors report 0.484 bpb, but their dataset is seemingly no longer available and in our experiments we use a slightly different one.) allowing to represent the 134.0 Gbp of input data in 8.7 GB of space. Note that if the reference sequence is available, either explicit or can be reconstructed ('presumed', in the terminology of C nov as and Moffat 2013), then compressing DNA reads is much easier and high compression ratios are possible. Several fast q or samba m compressors make use of a reference sequence, to name Quip (), fast qz and f qz comp () in one of their modes, slim gene (), CRAM (), go by (), DeeZ () and fq zip (). Several techniques for compressing SAM files, including mapping reads to a presumed reference sequence, were also explored in C nov as and Moffat (2013). In this article, we present a new reference free compressor for fast q data, Overlapping Reads COmpression with Minimizers or com achieving compression ratios surpassing the best known solutions. For the two mentioned human datasets it obtains the compression ratios of 1.005 and 0.317 bpb, respectively. or com is also fast, producing the archives in about 8 and 77 min, respectively, using eight threads on an AMD Opteron 6136 2.4 GHz machine.
