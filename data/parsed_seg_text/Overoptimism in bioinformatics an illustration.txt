Motivation: In statistical bioinformatics research, different optimization mechanisms potentially lead to over optimism in published papers. So far, however, a systematic critical study concerning the various sources underlying this over optimism is lacking. Results: We present an empirical study on over optimism using high dimensional classification as example. Specifically, we consider a promising new classification algorithm, namely linear discriminant analysis incorporating prior knowledge on gene functional groups through an appropriate shrinkage of the within group covariance matrix. While this approach yields poor results in terms of error rate, we quantitatively demonstrate that it can artificially seem superior to existing approaches if we fish for significance. The investigated sources of over optimism include the optimization of datasets, of settings, of competing methods and, most importantly, of the methods characteristics. We conclude that, if the improvement of a quantitative criterion such as the error rate is the main contribution of a paper, the superiority of new algorithms should always be demonstrated on independent validation data. Availability: The R codes and relevant data can be downloaded from

introduction in statistical bioinformatics research, the reported results on the performance of new algorithms are known to be over optimistic as recently discussed in a letter to the editors of Bioinformatics (). The current article aims at illustrating the different mechanisms leading to over optimism through a concrete example from an active methodological research field. The first and perhaps most obvious reason for over optimism is that researchers sometimes randomly search for a specific dataset such that their new method works better than existing approaches, yielding a so called 'dataset bias'. While a method can not reasonably be expected to yield 'universally better' results in all datasets, * To whom correspondence should be addressed. it would be wrong to report only favorable datasets without mentioning and or discussing the other results. This strategy induces an optimistic bias. This aspect of over optimism is quantitatively investigated in the study by) and termed as 'optimization of the dataset' in this article. The second source of over optimism which is related to the optimal choice of the dataset mentioned above, is the optimal choice of a particular setting in which the superiority of the new algorithm is more pronounced. For example, researchers could report the results obtained after a particular feature filtering which favors the new algorithm compared with existing benchmark approaches. This mechanism, which is strongly related to data overfitting, is termed as 'optimization of the settings' in this article. The third source of over optimism is related to the choice of the existing benchmark methods applied for comparison purposes. Researchers are supposed to compare their new algorithm to state of the art methods, but may consciously or subconsciously choose suboptimal existing methods and exclude the best competing methods from the comparison for any reason, e.g. because running the software demands very particular knowledge, because previous authors excluded these methods as well, because the methods induce high computational expense or because they belong to a completely different family of approaches and thus do not fit in the considered framework. Then the new algorithm artificially seems better than competing approaches and over optimistic results on the superiority of the new algorithm are reported because the best competing approaches are disregarded. Since the definition of state of the art methods is often ambiguous, such problems may occur even when researchers are decided to perform a fair comparison. This mechanism, also known as 'straw-man phenomenon' is termed as 'optimization of the competing methods' in this article. Finally, researchers often tend to optimize their new algorithms to the datasets they consider during the development phase (). This mechanism essentially affects all research fields related to data analysis such as statistics, machine learning or bioinformatics. Indeed, the trial and error process constitutes an important component of data analysis research. As most inventive ideas have to be improved sequentially before reaching an acceptable maturity, the development of a new method is per se an unpredictable search process. The problem is that, as stated by the Bioinformatics editorial team (), this search process leads to an artificial optimization of the method's characteristics to the considered datasets. Hence, the superiority of the novel method over an existing method [as measured, e.g. through the difference between page
DISCUSSION
