An unprecedented quantity of genome sequence data is currently being generated using next generation sequencing platforms. This has necessitated the development of novel bioinformatics approaches and algorithms that not only facilitate a meaningful analysis of these data but also aid in efficient compression, storage, retrieval and transmission of huge volumes of the generated data. We present a novel compression algorithm d eliminate that can rapidly compress genomic sequence data in a loss-less fashion. Validation results indicate relatively higher compression efficiency of d eliminate when compared with popular general purpose compression algorithms, namely, gzip, bzip2 and lz ma. Availability and implementation: Linux, Windows and Mac implementations (both 32 and 64-bit) of d eliminate are freely available for download at: http://metagenomics.atc.tcs.com/compression/ d eliminate. Contact:

introduction the volume of sequence data being deposited in major public sequence repositories is witnessing an exponential rate of growth. This presents an important challenge with respect to developing efficient compression and storage methods. Addressing this challenge directly indirectly impacts bandwidth and cost related issues of data transmission dissemination. Major public sequence repositories store sequence data either in its raw format (as fast q files) or in a processed format (as single multi fast a files). This study pertains to compression of files containing nucleotide sequences in single multi fast a format. Such files contain information of sequences along with their corresponding headers. Four nucleotide bases, i.e. A, T/U, G and C, usually constitute the majority of text characters in the sequence portion of fast a files. Using a two bit encoding for these four characters is obviously the simplest way of reducing the file data size by a factor of four. Several studies (see review by) have proposed various approaches to further reduce the bits base ratio to less than 2. While a majority of these approaches work by tracing (and optimally encoding) repeat patterns in genomic data, others work by capturing differences between various sequences constituting a multi fast a file. A few reference based strategies () have also been reported recently. In spite of the availability of several Specialized Genome Compression Algorithms sgc as which achieve reasonably high compression gains, it is observed that the majority of public sequence repositories still use y General Purpose Compression Algorithms gpc as such as gzip and bzip2 for compressing and storing sequence data. The likely reasons for this observation are the following: (1) unlike gpc as most sgc as are not equipped to handle non at gc characters. This limits their capability to perform a loss-less compression decompression. (2) In contrast to gpc as the compression efficiency of most sgc as is generally observed to be achieved at the cost of huge time and memory requirements. (3) The utility of reference based compression approaches is also subject to availability of closely related reference genomes. In summary, the above observations indicate the need for a compression algorithm (and a practical implementation of the same) that (1) achieves loss-less compression and decompression as rapidly as gpc as (2) has significantly better 'compression efficiency' than gpc as (3) has low memory requirements thus enabling it to handle files of any size and (4) is compatible with popular software platforms unix linux Windows, etc.) and system architectures (32-bit or 64-bit). In this article, we present d eliminate (a combination of delta encoding and progressive elimination of nucleotide characters) a novel method that implements the above mentioned features of an ideal compression algorithm.
