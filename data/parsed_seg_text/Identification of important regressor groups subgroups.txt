Motivation: Gut microbiota can be classified at multiple taxonomy levels. Strategies to use changes in microbiota composition to effect health improvements require knowing at which taxonomy level interventions should be aimed. Identifying these important levels is difficult, however, because most statistical methods only consider when the microbiota are classified at one taxonomy level, not multiple. Results: Using L 1 and L 2 regularization s we developed a new variable selection method that identifies important features at multiple taxonomy levels. The regularization parameters are chosen by a new, data adaptive repeated cross validation approach, which performed well. In simulation studies, our method outperformed competing methods: it more often selected significant variables, and had small false discovery rates and acceptable false positive rates. Applying our method to gut microbiota data, we found which taxonomic levels were most altered by specific interventions or physiological status.

introduction with improved culture independent techniques, a typical study of gut microbiota now involves data from numerous microbes. The microbes are classified at multiple taxonomy levels, namely, phylum, class, order, family, genus and species. Each taxonomy level has many subdivisions, and the number of subdivisions increase on progression from phylum to species level. Strategies to use changes in microbiota composition to effect health improvements require knowing at which taxonomy level interventions should be aimed. Levels to target are those with subdivisions identified as having an impact on the target health outcome. From a biological perspective, only a few subdivisions at each level are believed to play a role in certain health outcomes. Identifying the few important subdivisions at each level is difficult, however, because of the increasing number of subdivisions on progression from phylum to species level and because the microbial data are typically based on small sample sizes. Thus, a method that overcomes these difficulties and identifies important subdivisions at multiple taxonomy levels is needed. This biological problem corresponds to a variable selection problem where the variables are grouped at multiple levels, and the number of variables (p) far exceeds the sample size (n). We suppose that each level has sparse effects. In the microbiota data, sparse effects mean that only a few subdivisions within a particular taxonomy level actually impact the health phenotypes of interest. For our purposes, we consider the case where variables are divided into groups and subgroups within the groups. Our interest, thus, is developing a method that selects important groups (e.g. phyla), subgroups (e.g. families) and individual predictors (e.g. genera). Selecting variables clustered into groups and subgroups is challenging. When the variables are divided only into groups (without subgroups), a popular technique is the group Lasso (), which selects an entire group of variables to be included or excluded from the model. The group Lasso, however, has substantial drawbacks. First, the method assumes that the model submatrices for each group are orthonormal. When ortho normality is not satisfied, the group Lasso may select an incorrect model (). Second, the group Lasso does not achieve sparsity within each group, which can be useful. For the microbial data, we could design more specific strategies for changing microbiota composition if we knew which particular families (i.e. subgroups) in phyla (i.e. group) impacted health phenotypes of interest. To overcome the deficiencies of the group lasso recently proposed the sparse group Lasso (SGL). The method imposes no ortho normality requirements on the group model submatrices and achieves sparsity between and within groups through a clever use of the method for generalized gradient descent. The SGL works well when variables are clustered into groups, but not when they are clustered at more than one level a feature inherent to gut microbiota data. *To whom correspondence should be addressed to accommodate selecting important groups, subgroups and individual predictors, we propose three new algorithms. The first algorithm, the sparse group subgroup Lasso s gsl generalizes the work of. It is based on using L 1 and L 2 regularization s in a linear regression model; convex non-linear regression models are discussed in the Supplementary Material. Our two other proposed algorithms use appropriate combinations of already existing variable selection procedures. First, we propose applying the group Lasso to the groups followed by SGL applied to the subgroups. Second, we propose applying the group Lasso to both the groups and subgroups followed by applying the Lasso () to select among the individual predictors. We demonstrate in a simulation study that our first algorithm outperforms the other two. s gsl is a special case of the tree structured group Lasso (), where nodes on the tree represent groups or subgroups of features and 'leaf' nodes represent individual features. The tree structured group Lasso, however, uses a smoothing proximal gradient method () to 'prune' the entire tree collectively, whereas our method uses an accelerated generalized gradient descent approach to determine sparsity among groups, then subgroups and then individual features. Moreover, we consider a tree without cycles, meaning there is no overlap between groups subgroups of features; i.e. each individual feature only belongs to one subgroup, and each subgroup only belongs to one group. Hence, our problem differs from the overlapping group Lasso as in the analysis of breast cancer gene expression data () where the interest is finding important pathways among overlapping genes. Our problem also differs from a hierarchical variable selection () where a feature is subject to selection only after another feature is selected first. We do not impose this requirement. Like other lasso based procedures, s gsl also requires selecting tuning parameters, for which we propose a data adaptive approach. Our approach involves multiple applications of 10-fold cross validation that we show performs well in selecting the tuning parameters through various simulation studies. Therefore, the main contributions from our work include (i) a new variable selection procedure s gsl which identifies important groups, subgroups and individual predictors through combined L 1 and L 2 regularization s. (ii) We show that achieving sparsity at multiple levels can not be achieved through simple combinations of existing Lasso approaches. We show that such combinations will select relevant features less often than s gsl or never (Section 3). (iii) We provide a data adaptive cross validation approach that improves over the traditional cross validation to select the tuning parameters. (iv) In microbio me data, our method identifies which taxonomic levels were most altered by specific interventions or physiological status. The rest of the article is as follows. Section 2 describes s gsl and Section 3 evaluates its performance compared with competing methods. In Section 4, we describe the microbiota data that motivated this methodology and analyze the data. Section 5 concludes the article.
