big bwa is a new tool that uses the Big Data technology had oop to boost the performance of the burrows wheeler aligner (BWA). Important reductions in the execution times were observed when using this tool. In addition, big bwa is fault tolerant and it does not require any modification of the original BWA source code.

introduction burrows wheeler aligner (BWA) is a very popular software for mapping sequence reads to a large reference genome. It consists of three algorithms: bwa backtrack (), b was w () and bwa mem (). The first algorithm is designed for short Illumina sequence reads up to 100 bp, whereas the others are focused on longer reads. bwa mem which is the latest, is preferred over b was w for 70 bp or longer reads as it is faster and more accurate. In addition, bwa mem has shown better performance than other several state of art read aligners for mapping 100 bp or longer reads. Sequence alignment is a very time consuming process. This problem becomes even more noticeable as millions and billions of reads need to be aligned. For instance, new sequencing technologies, such as Illumina his eq x Ten, generate up to 6 billion reads per run, requiring more than 4 days to be processed by BWA on a single 16core machine. Therefore, NGS professionals demand scalable solutions to boost the performance of the aligners in order to obtain the results in reasonable time. In this article, we introduce big bwa a new tool that takes advantage of had oop as Big Data technology to increase the performance of BWA. The main advantages of our tool are the following. First, the alignment process is performed in parallel using a tested and scalable technology, which reduces the execution times dramatically. Second, big bwa is fault tolerant, exploiting the fault tolerance capabilities of the underlying Big Data technology on which it is based. And finally, no modifications to BWA are required to use big bwa. As a consequence, any release of BWA (future or legacy) will be compatible with big bwa
discussion performance big bwa was tested using data from the 1000 Genomes Project () (for details). Measurements were performed on a five node AWS cluster with 16 cores per node (Intel Xeon E5-2670 at 2.5 GHz CPUs), running had oop 2.6.0. Detailed information about the experimental setup is provided in the Supplementary Material. Performance results for big bwa and the other evaluated tools only take into consideration all the datasets were extracted from the 1000 Genomes Project ().Highlighted the best tool for a particular number of cores. For fair comparison with the other tools, big bwa obtains these results using BWA version 0.5.10. Tool versions: pbw a 0.5.9 and SEAL 040 highlighted the best tool for a particular number of cores. These results were obtained using BWA version 0.7.12. the alignment process time, which was calculated as the average of 5 runs per data point after one warm up execution shows a comparison with SEAL and pbw a for the bwa backtrack algorithm. In this case, big bwa clearly outperforms these tools, especially when the number of cores used is high. In this way, speedups of 36.4 were reached with respect to the sequential case (using the original BWA tool as reference). It can also be observed that the scalability of SEAL is worse, caused by the overhead introduced by py doop with respect to the use of JNI. Performance of bwa mem is shown in. It was measured using only BWA (threaded version) and big bwa because SEAL and pbw a do not support this algorithm. We have also included results for a hybrid version that uses big bwa in such a way that each mapper processes the inputs using BWA with two threads. Results show that, with a small number of cores, BWA behaves slightly better than big bwa. Note that BWA is limited to execute on just one cluster node and, therefore, we can not provide results using more than 16 cores. Considering 16 cores, big bwa is always the best solution but, due to the memory assigned per map task in our cluster configuration, only 13 concurrent tasks can be executed on one node. In this way, big bwa always distributes the tasks between two nodes when using 16 cores. In addition, big bwa shows good behavior in terms of scalability for all the datasets considered, executing up to 36.6 faster than the sequential case. Additional performance results are shown in the Supplementary Material. Correctness: We verified the correctness of big bwa by comparing its output file with the one generated by BWA. Differences range from 0.06% to 1% on uniquely mapped reads (mapping quality greater than zero), similarly to the differences shown by the threaded version of BWA with respect to the sequential case.
