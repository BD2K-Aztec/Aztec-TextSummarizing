Motivation: New sequencing technologies generate larger amount of short reads data at decreasing cost. De novo sequence assembly is the problem of combining these reads back to the original genome sequence, without relying on a reference genome. This presents algo-rithmic and computational challenges, especially for long and repetitive genome sequences. Most existing approaches to the assembly problem operate in the framework of de Bruijn graphs. Yet, a number of recent works use the paradigm of string graph, using a variety of methods for storing and processing suffixes and prefixes, like suffix arrays, the burrows wheeler transform or the FM index. Our work is motivated by a search for new approaches to constructing the string graph, using alternative yet simple data structures and algorithmic concepts. Results: We introduce a novel hash based method for constructing the string graph. We use incremental hashing, and specifically a modification of the karp rabin fingerprint, and Bloom filters. Using these probabilistic methods might create false positive and false negative edges during the algorithms execution, but these are all detected and corrected. The advantages of the proposed approach over existing methods are its simplicity and the incorporation of established probabilistic techniques in the context of de novo genome sequen-cing. Our preliminary implementation is favorably comparable with the first string graph construction of Simpson and Durbin (2010) (but not with subsequent improvements). Further research and optimizations will hopefully enable the algorithm to be incorporated, with noticeable performance improvement, in state of the art string graph based assemblers. Availability and implementation: A beta version of all source code used in this work can be downloaded from

introduction de novo sequence assembly, namely, reconstructing an unknown genome sequence from a set of overlapping sequence reads, is an important problem in bioinformatics. There has been an extensive research in this area in the past two decades, yielding several efficient methods for the task of sequence assembly. Yet, next generation sequencing technologies pose challenges to current assemblers. Read sets produced by modern sequencing machines contain up to hundreds of millions short reads. As a consequence, there is an ongoing need for new assembly approaches, which should provide a significant decrease both in memory consumption and running time. The first assembly paradigm to be commonly used was the overlap layout consensus (OLC) one, used in several assemblers (such as Celera and TIGR). The first stage of these assemblers aims to construct an overlap graph representing the sequence reads. In this graph, reads are represented as nodes, and two nodes are connected by an edge if and only if the corresponding reads overlap. Constructing this graph is one of the biggest problems of the OLC paradigm, as this stage is both time and memory intensive. One possible solution for this problem was recently introduced in the LEAP assembler, which uses compact data structures to represent the overlap graph (). The de Bruijn paradigm, which was first proposed by, is more space efficient compared with the OLC paradigm, as it merges reads from different instances of a repeat. In this approach, reads are broken into km ers which serve as the nodes in a de Bruijn graph. Thus, reads that come from the same repeat (but from different locations in the genome) share the same path in the de Bruijn graph. However, this approach might increase the ambiguity of assembling short repeats. Several short read assemblers have implemented this approach, e.g. Euler (), Velvet () and Abyss (). In recent years, several improvements of the de Bruijn method have led to a significant decrease in the memory consumption needed for de Bruijn assemblers saved considerable memory usage by not recording read locations and paired end information used sparse bit arrays to store an implicit representation of the de Bruijn graph took advantage of the redundancy in the reads set and constructed roughly an equivalent de Bruijn graph by storing only one out of g nodes (where 10 g 25). An efficient assembly (both time and memory wise) of a human genome was reported by, who used Bloom filters to represent the de Bruijn graph, as well as additional data structures for avoiding false positive nodes. These results were further improved by the usage of cascading Bloom filters (). A different framework is based on the string graph, where the edges of the overlap graph are partitioned to two different types: irreducible edges, which are retained, and transitive edges, which are removed. The notion of transitive edges removal was first introduced in, while the term string graph was first defined in. It was used in the Celera assembler ().developed the String Graph Assembler (SGA) further, implementing a new algorithm that outputs the string graph directly (without the need to first construct the overlap graph and only then to remove transitive edges). The string graph approach has the advantage of repeats sharing the same path, without the need to break the reads into km ers (as in the de Bruijn graph approach). The read joiner (RJ) assembly () improved on the SGA assembler in terms of time and memory complexity. This was achieved by first producing a relevant subset of all overlaps between read pairs (using matches between smaller strings as a filter), and then outputting the set of irreducible edges by applying a traversal algorithm on a graph representing the sorted set of candidate overlaps. In this article, we present a different approach for the construction of the string graph, which resemble the SGA algorithm of Simpson and Durbin (2010), but relies on different theory. We apply hash functions to efficiently store, access and process prefixes and suffixes of reads. This method relies on computing hash values modulo a large prime for all prefixes and suffixes of the reads. Our algorithm deals solely with these hash values, except during a verification process, performed on the reads themselves. The probabilistic method might introduce false positive and false negative results, and methods to overcome them are also detailed. The algorithm is relatively easy to implement, as it simplifies the task of identifying irreducible edges, by using probabilistic techniques, such as incremental (rolling) hash and Bloom filters. To the best of our knowledge, this is the first incorporation of these two techniques together in the genome assembly context. We hope and expect these probabilistic techniques will lead to a simplification and improved performance of state of the art assemblers as well. Right now, our initial results improve on the first version of the SGA string graph construction method described in Simpson and Durbin (2010). Further optimizations may substantially reduce the required computational resources, as was the case with the latest version of the SGA assembler. At this point, the method is only a proof of a new concept, and not a complete assembler. However, it can be smoothly combined with other assemblers that are based on the string graph representation of reads.

discussion our main contribution in this work is the introduction of a novel and efficient method for the construction of the string graph from a set of sequence reads. probabilistic methods are still not widely used in the domain of assembly [apart from the usage of Bloom filter by. The combination of the probabilistic nature and the usage of easily computed hash functions yield a rather efficient and fast index. An encouraging feature of our algorithms is that no false positive edges have reached the verification stage in all four chromosomes tested. We argued that this desired property is expected to occur for larger instances as well. At this stage, our tool is a proof of concept, and not a full fledged assembler. To assemble contigs, the string graph can be traversed and processed. A strength of our approach, as well as of SGA, is the possibility to access a large number of reads by referring only to a single entity the hash value of a common prefix (in SGA, this will be a range of consecutive indexes). This implies that the number values in each ol prefixes set is bounded by the length of the read rather than the number of overlapping reads. The hash functions used must be incremental to enable us not to store the set of ol prefixes in memory. The overlap extension phase deals mainly with hash values, and would otherwise be much less memory and time efficient. In addition, the incremental nature of the hash function enable indexing all reads in time linear in the total length of the reads. We made two assumptions in the design of this algorithm: error free data and equal length reads. The first assumption can be dealt with by using existing algorithms that preprocess the reads and use statistical methods to correct sequencing errors. As for the second assumption, modifications can be done in the algorithm to not rely on reads having the same length. For example, we consider an irreducible edge to belong to a read whose extension process is terminated first. If some reads are shorter, this condition is violated, and we need to revert to checking the overlap lengths as well. The verification process we currently perform requires accessing the reads during the construction of the string graph. We observed that false positive edges hardly ever appear under the current choice of modulus, m, in the KR fingerprint and the parameters of the Bloom filter. This should enable us to remove all verifications, thus making it possible not to store and access the original reads after the indexing phase, resulting in a somewhat lower memory consumption. Our present implementation uses a separate Bloom filter for every suffix length, to reduce the number of possible collisions. It may be possible to use a single Bloom filter for all lengths, by properly adjusting the values of the relevant parameters. This could potentially lead to a substantial saving in memory. There are additional aspects that could be improved. For example, we have used a version of the KR fingerprint, but other IH functions (D. Lemire and O. Kaser, submitted for publication) may perform even better. In addition, many parts of our algorithm can be parallelized. The impact of the hash function moduli and the Bloom filter parameters on the performance can be further investigated. We believe that we have not yet reached the optimal choice of parameter values, which can save more memory and running time, even at the expanse of an increased number of false positive results (which are subsequently detected). Memory can be slightly saved by representing bases in reads using 2 bits per base, instead of a single character (however, this will not effect the peak memory used). The preliminary results shown here make us believe that the hash based approach can yield even better results in the future. The performance of our algorithm is favorably comparable with the first implementation of the fm index based assembler, by Simpson and Durbin (2010). A number of newer assemblers are based on string graphs, such as eden a (), SGA (), LEAP () and read joiner (). These assemblers differ in the data structures and algorithms used to construct the string graph, and in particular how they compute suffix prefix matches. Their reported performances are substantially better than the performance of our initial implementation. However, we believe that our algorithm is of interest due to its simplicity and the probabilistic techniques that are incorporated in it, and that improvements as outlined above can make it competitive with state of the art string graph algorithms. More generally, we expect that probabilistic approaches can play a key factor in improving other string graph based approaches.
