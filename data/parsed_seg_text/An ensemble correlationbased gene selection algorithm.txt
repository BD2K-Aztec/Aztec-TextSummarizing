Motivation: Gene selection for cancer classification is one of the most important topics in the biomedical field. However, microarray data pose a severe challenge for computational techniques. We need dimension reduction techniques that identify a small set of genes to achieve better learning performance. From the perspective of machine learning, the selection of genes can be considered to be a feature selection problem that aims to find a small subset of features that has the most discriminative information for the target. Results: In this article, we proposed an Ensemble correlation based Gene Selection algorithm based on symmetrical uncertainty and Support Vector Machine. In our method, symmetrical uncertainty was used to analyze the relevance of the genes, the different starting points of the relevant subset were used to generate the gene subsets and the Support Vector Machine was used as an evaluation criterion of the wrapper. The efficiency and effectiveness of our method were demonstrated through comparisons with other feature selection techniques , and the results show that our method outperformed other methods published in the literature.

introduction recently there has been increasing interests in changing the emphasis of cancer classification from morphologic to molecular (). Cancers are usually marked by a change in the expression levels of certain genes; thus, the selection of relevant genes for cancer classification is an important task in most cancer gene expression studies. These discriminative genes are very useful in clinical applications, such as in recognizing disease profiles (). However, microarray data pose a severe computational challenge because of its high dimensionality and small sample size (). From the perspective of machine learning, the selection of genes is a feature selection problem that aims to find a small subset of features with the most discriminative information for the target. Feature selection is an important pre-processing step in eliminating irrelevant and redundant features for classification. The growing dimensionality of recorded data demands dimension reduction techniques that identify small sets of features that lead to better learning performance. The objective of feature selection is to provide faster and more effective models, and also to avoid overfitting and the curse of dimensionality. Feature selection methods can be broadly categorized into three types: filter, wrapper and hybrid (). The filter methods use specific evaluation criteria that are independent of a learning algorithm to identify a feature subset from the original feature set. Filter techniques () are fast and scale easily to high dimensional datasets, but they ignore interaction with the classifier. Wrapper methods () use the classifier to evaluate the performance of each subset with a search algorithm. Wrapper methods tend to find the most suitable feature subset for the learning algorithm, but they are very computationally expensive. Hybrid methods () combine the advantages of filter and wrapper techniques. These algorithms aim to achieve the best learning performance with a predetermined learning algorithm and a similar time complexity to filter algorithms (). A feature selection procedure can usually be divided into two steps: subset generation and subset evaluation (). The most important issue in generating a feature subset is how to choose the search strategy and the starting point. Complete search, sequential search and random search are the typical search methods used for subset generation. Complete search methods consider every feature subset to be a potential candidate to guarantee finding the optimal result. However, the computational time is intractable when the dimensionality is high. Sequential search methods, such as Sequential Forward Selection (SFS) and Sequential Backward Selection (SBS), sacrifice completeness by applying the greedy hill climbing approach (), which adds or removes features one at a time. These algorithms are computationally simpler and faster than a complete search strategy, but they can still lead to local optima. Random search methods, such as random start hill climbing and simulated annealing (), start with a randomly selected subset, and these algorithms help to escape local optima in the search space. During the past few years, the Support Vector Machine (SVM) has become very popular because of its good performance on high dimensional data. SVM was developed by to successfully solve the problems of handwritten digit recognition (), object recognition (), text classification (), cancer diagnosis () and bioinformatics ().In this article, we proposed a hybrid feature selection algorithm named Ensemble correlation based Gene Selection ecb gs based on symmetrical uncertainty (SU) and SVM for gene selection. Our proposed method combined a filter approach and a wrapper method to remove the redundant features and to find the relevant features from the original feature set. For the original feature set, SU was used as an evaluation criterion for the filter, using the different starting points as a subset generation strategy and SVM as the evaluation learning algorithm of a wrapper. It was observed that the classifier combined with our proposed feature selection method obtained promising classification accuracy with a small gene subset on six gene expression datasets.

discussion in this study, we used SVM classification method to analyze the gene expression data. A lot of research has been shown that SVM is the most effective classifier in performing accurate cancer diagnosis from gene expression data (). SVM is interesting () because the number of parameters to be estimated essentially depends on the number of samples rather than on the number of features, which is particularly relevant with very small sample to feature ratios. Moreover, SVM has many mathematical features that make them attractive for gene expression analysis (), including their flexibility in choosing a similarity function, sparseness of solution when dealing with large datasets, the ability to handle large feature spaces and the ability to identify outliers. We have first examined the performance of our method using SVM on six microarray datasets in terms of precision, TP rate, FP rate and AUC. In ecb gs there is a parameter, the relevance threshold. Different settings of will directly affect the number of selected genes. The closer is set to 1, the smaller the number of selected genes is. From the experiments, we found that the larger number of genes does not always lead to better performance. Therefore, it is very important to choose the appropriate the last row is the total average accuracy of four methods on six datasets. The bold values indicate the highest classification accuracy. threshold for improving classification accuracy. However, choosing a proper threshold is difficult because the distinct values for each dataset are different. Through the experiments on six datasets, we found that the threshold was closely related with the 'degree' of the relevance of the genes. For example, in the Lymphoma dataset, there are 4300 genes with SU value that is 40.5. In contrast, there are only 37 genes that satisfy the threshold 0.5 in breast b dataset. Thus, the appropriate threshold for the dataset that has higher 'degree' of the relevance has to be larger than the one for the dataset that has lower degree h indicate the result on Lymphoma dataset, (i) and (j) indicate the results on mll leukemia dataset, (k) and (l) indicate the results on Prostate dataset. We performed two experiments for each dataset: one applied the SBS strategy, and the other one applied the SFS strategy for three feature selection methods. As in the figure, when the training dataset is small, the proposed method shows high prediction accuracy, and other methods primarily make poor predictions. Moreover, the proposed method outperforms other methods in most cases based on this observation, we can make some general recommendations based on the experiments. (i) The threshold could be selected as the mean of all the SU value of the genes respect to the class or (ii) decide the threshold as the following equation: SU max  SU min   0:7 5 where SU max indicates the SU value of the most relevant gene respect to the class, and SU min refers to the lowest one we have also compared ecb gs to the f cbf algorithm. The result shows that the proposed ecb gs is able to generate the most meaningful and discriminative genes in most cases. However, if the number of features is 550, f cbf tends to be more effective; it is because ecb gs produces the feature subsets by removing top informative features. If the datasets have a small number of features with a lack of useful information, removing some informative features will result in less discriminative or meaningless subsets to train the classifier. Furthermore, we found that relevant and non-redundant features are selected before repeating our method more than 10 times. We have also made a comparison between the proposed method and other feature selection methods in terms of classification accuracy and speed. One interesting observation is that our method is still more powerful than other methods even when small data are given. It indicates that ecb gs is more appropriate than others for analyzing small datasets, such as gene expression data. Moreover, our method is significantly faster than other feature selection methods. Additionally, when the number of selected features is 42000, the computational costs of the other three feature selection methods are very expensive. The selection of discriminant genes is a common task for cancer classification. Research in Biology and Medicine may benefit from the examination of the top ranking genes to confirm recent discoveries in cancer research, or suggest new avenues to be explored (). Recently, several gene selection approaches jira pech have been proposed to solve the cancer classification problem. In contrast to these methods, the prediction accuracy of our method is competitive with a small subset of genes.
