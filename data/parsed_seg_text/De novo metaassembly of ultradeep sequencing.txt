We introduce a new divide and conquer approach to deal with the problem of de novo genome assembly in the presence of ultra deep sequencing data (i.e. coverage of 1000x or higher). Our proposed meta assembler slice mb ler partitions the input data into optimal sized slices and uses a standard assembly tool (e.g. Velvet, SPAdes, id baud and Ray) to assemble each slice individually. slice mb ler uses majority voting among the individual assemblies to identify long contigs that can be merged to the consensus assembly. To improve its efficiency, slice mb ler uses a generalized suffix tree to identify these frequent contigs (or fraction thereof). Extensive experimental results on real ultra deep sequencing data (8000x coverage) and simulated data show that slice mb ler significantly improves the quality of the assembly compared with the performance of the base assem-bler. In fact, most of the times, slice mb ler generates error free assemblies. We also show that slice mb ler is much more resistant against high sequencing error rate than the base assembler. Availability and implementation: slice mb ler can be accessed at

introduction since the early days of DNA sequencing, the problem of de novo genome assembly has been characterized by insufficient and or uneven depth of sequencing coverage (see e.g.). Insufficient sequencing coverage, along with other shortcomings of sequencing instruments (e.g. short read length and sequencing errors) exacerbated the algorithmic challenges in assembling large, complex genome in particular those with high repetitive content. Some of the third generation of sequencing technology currently on the market, e.g. Pacific Biosciences () and Oxford nano pore (), offers very long reads at a higher cost per base, but sequencing error rate is much higher. As a consequence, long reads are more commonly used for scaffolding contigs created from second generation data, rather than for de novo assembly (). Thanks to continuous improvements in sequencing technologies, life scientists can now easily sequence DNA at depth of sequencing coverage in excess of 1000x, especially for smaller genomes like viruses, bacteria or bacterial artificial chromosome bac yac clones. ultra deep sequencing (i.e. 1000 x or higher) has already been used in the literature for detecting rare DNA variants including mutations causing cancer (), for studing viruses (), as well as other applications (). As it becomes more and more common, ultra deep sequencing data are expected to create new algorithmic challenges in the analysis pipeline. In this article, we focus on one of these challenges, namely the problem of de novo assembly. We showed recently that modern de novo assemblers SPAdes (), id baud () and Velvet (are unable to take advantage of ultra deep coverage (). Even more surprising was the finding that the assembly quality produced by these assemblers starts degrading when the sequencing depth exceeds 500x1000x (depending on the assembler and the sequencing error rate). By means of simulations on synthetic reads, we also showed in that the likely culprit is the presence of sequencing errors: the assembly quality degradation can not be observed with error free reads, whereas higher sequencing error rate intensifies the problem. The 'message' of our study () is that when the data are noisy, more data are not necessarily better. Rather, there is an error rate dependent optimum. Independently from us, study () reached similar conclusions: the authors assembled E. coli (4.6 Mb), S. kudriavzevii (11.18 Mb) and C. elegans (100 Mb) using soap de novo Velvet, ABySS, mera culo us and id baud at increasing sequencing depths up to 200x (which is not ultra deep according to our definition). Their analysis showed an optimum sequencing depth (around 100x)
