

introduction the typhoon of technological advances witnessed during the last decade has left in its wake a flood of life science data, and an increasingly impenetrable mass of biomedical literature describing and analysing those data. Importantly, the modern frenzy to gather more and more information has left us without adequate tools either to mine the rapidly increasing data and literature collections efficiently, or to extract useful knowledge from them. To be usable, information needs to be stored and organized in ways that allow us to access, analyze and annotate it, and ultimately to relate it to other information. Unfortunately, however, much of the data accumulating in databases and documents has not been stored and organized in rigorous, principled ways. Consequently, finding what we want and, crucially, pinpointing and understanding what we already know, have become increasingly difficult and costly tasks (). A group of scientists for whom these problems have become especially troublesome are bio curators who must routinely inspect thousands of articles and hundreds of related entries in different databases in order to be able to attach sufficient information to a new database entry to make it meaningful. With something like 25 000 peer reviewed journals publishing around 2.5 million articles per year, it is simply not possible for curators to keep abreast of developments, to find all the relevant papers they need, to locate the most relevant facts within them, and simultaneously to keep * To whom correspondence should be addressed. pace with the inexorable data deluge from ongoing high throughput biology projects (i.e. from whole genome sequencing). For example, to put this in context, Bairoch estimates that it has taken 23 years to manually annotate about half of swissprot s 516 081 entries (), a painfully small number relative to the size of its parent resource, UniProtKB (The UniProt), which currently contains 11 million entries. Hardly surprising, then, that he should opine, 'It is quite depressive to think that we are spending millions in grants for people to perform experiments, produce new knowledge, hide this knowledge in a often badly written text and then spend some more millions trying to second guess what the authors really did and found' (). The work of curators, and indeed of all researchers, would be far easier if articles could provide seamless access to their underlying research data. It has been argued that the distinction between an online paper and a database is already diminishing (); however, as is evident from the success stories of recent initiatives to access and extract the knowledge embedded in the scholarly literature, there is still work to be done. Some of these initiatives are outlined below. The Royal Society of Chemistry (RSC) took pioneering steps towards enriching their published content with data from external resources, creating computer readable chemistry' with their Prospect software (). They now offer some of their journal articles in an enhanced HTML form, annotated using Prospect: features that may be marked up include compound names, bio and chemical ontology terms, etc. marked up terms provide definitions from the various ontologies used by the system, together with inch i (IUPAC International Chemical Identifier) codes, lists of other RSC articles that reference these terms, synonym lists, links to structural formulae, patent information and so on. Articles enriched in this way make navigation to additional information trivial, and significantly increase the appeal to readers. In a related project, the chem spider Journal of Chemistry exploits the chem mantis System to mark up its articles (http://www.chemmantis.com). With the chem spider database at its heart, chem mantis identifies and extracts chemical names, converting them to chemical structures using name to structure conversion algorithms and dictionary look-ups; it also marks up chemical families, groups and reaction types, and provides links to Wikipedia definitions where appropriate. In an initiative more closely related to the life sciences, FEBS Letters ran a pilot study (with the curators of the MINT interaction database (), focusing on integration of published protein protein interaction and posttranslational modification data with information stored in MINT and UniProtKB. Key to the experiment was the Structured Digital
