Motivation: In small sample settings, bolstered error estimation has been shown to perform better than cross validation and competitively with bootstrap with regard to various criteria. The key issue for bolstering performance is the variance setting for the bolstering kernel. Heretofore, this variance has been determined in a non-parametric manner from the data. Although bolstering based on this variance setting works well for small feature sets, results can deteriorate for high dimensional feature spaces. Results: This article computes an optimal kernel variance depending on the classification rule, sample size, model and feature space, both the original number and the number remaining after feature selection. A key point is that the optimal variance is robust relative to the model. This allows us to develop a method for selecting a suitable variance to use in real world applications where the model is not known, but the other factors in determining the optimal kernel are known. Availability: Companion website at

introduction throughout most of the history of pattern recognition, the number of features was much smaller than the numbers currently being generated in high throughput biology. Less than 15 years ago, in two studies on feature selection most cases considered involved 30 features and the maximum number considered was 65 (). The advent of high throughput technologies has radically altered the landscape. In conjunction with large numbers of features, bioinformatics is confronted by small sample sizes, often 100, which forces one to train and test on the same data, where bias, variance (Braga) and lack of correlation with the true error () can severely degrade error estimation. Performance can degrade even further in the presence of feature selection (). Recent articles have pointed out the difficulty in establishing performance advantages for proposed classification rules (). Two statistically grounded sources of over optimism have been highlighted: (i) applying a classification rule to numerous datasets and then reporting only the results on the dataset for which the designed classifier possesses the lowest estimated error * To whom correspondence should be addressed.(); and (ii) applying multiple classification rules to a dataset and comparing the classification rules according to the estimated errors of the designed classifiers (). In both cases, optimism is a result of inaccurate error estimation. A good error estimator ideally would have small bias and small variance. This is a difficult trade-off in small sample settings. In small sample cases, re substitution generally has small variance but tends to be quite optimistically biased. cross validation has small bias, but tends to display high variance. Bolstered error estimation (Braga) attempts to achieve a compromise to this bias variance dilemma in small sample settings. It is based on the idea of modifying ('bolstering') the empirical distribution of the data by placing kernels at each data point and then estimating classifier error by the error on this bolstered empirical distribution in such a way that it reduces bias, while at the same time reducing variance. Bolstered error estimation has shown good performance when compared with popular error estimators in small sample settings, in particular, for feature set ranking and when used internally within a features election algorithm () and for ranking feature sets (). Its good performance, including the latter applications, has been demonstrated in the context a small number of features, including feature selection via sequential forward selection (SFS), where it is applied to small potential feature sets in the SFS algorithm. A critical aspect of the method is selecting the right amount of bolstering, which is given by the variance of the bolstering kernels. The original bolstering paper (Braga) proposed a non-parametric estimator for the kernel variance, which was found empirically to perform well in low dimensional spaces; however, estimation was found to degrade in high dimensions, so that a correction factor can be required (). In fact, it was demonstrated in a preliminary study that a correction factor can also be beneficial for low dimensional bolstering (). This leads us to consider optimal bolstering, specifically, finding an optimal variance for the bolstering kernels. Error estimators like re substitution and cross validation (assuming the number of folds is preset) are non-parametric. They contain no free parameters. This is not the case for bootstrap. In general, bootstrap has the form of a convex error estimator, namely wher  re sub and and and zero are the re substitution and zero bootstrap estimators and 0  a  1. The zero bootstrap utilizes the empirical distribution F * , which puts mass 1 n on each of the n available
