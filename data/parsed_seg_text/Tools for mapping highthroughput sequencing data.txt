Motivation: A ubiquitous and fundamental step in high throughput sequencing analysis is the alignment (mapping) of the generated reads to a reference sequence. To accomplish this task, numerous software tools have been proposed. Determining the mappers that are most suitable for a specific application is not trivial. Results: This survey focuses on classifying mappers through a wide number of characteristics. The goal is to allow practitioners to compare the mappers more easily and find those that are most suitable for their specific problem. Availability: A regularly updated compendium of mappers can be found at

introduction in the past decade, high throughput sequencing (HTS) has changed the way life sciences research is done. The decreasing costs have made HTS technology more mainstream, and it is now exploited in a growing number of biological applications, the so called seq experiments: dna seq (, chips eq (), rnase q (), bs seq () and numerous other applications, such as investigating the spatial organization of the genome inside the cell nucleus (). We refer to for an overview of sequencing technologies and related applications. A common feature of all HTS technologies and applications is the generation of relatively short reads (fragments of DNA sequences), which have to be aligned (mapped) to a reference sequence. The primary challenge is to efficiently find the true location of each read from a potentially large quantity of reference data while distinguishing between technical sequencing errors and true genetic variation within the sample. Presently, more than 60 mappers are available see for a list of mappers and for a timeline), most of them proposed after 2008, concurrent with developments in sequencing technologies. Mappers have had to adapt to: (i) handle growing quantities of data generated by HTS; (ii) exploit technological developments (); and (iii) tackle protocol developments. For instance, paired end library protocols motivated the development of mappers that exploit read pairing information (). Furthermore, the appearance of novel protocols may result in specific biases (). One consequence of the increasing number of mappers is that making a suitable choice for a specific application is not easy. Resources such as the seq answers forum Wiki pages () have collated information about different mappers, such as the operating system supported and different technologies that the mappers have been designed to handle. However, information about other equally important features/ characteristics of the mappers is difficult to find, being still scattered through publications, source code (when available), manuals and other documentation. This survey aims to help overcome these challenges by allowing practitioners to compare mappers more easily and, thus, find those that are most suitable for their specific problem. It does not evaluate mappers in terms of their accuracy, but instead it presents an overview of their characteristics. It complements previous studies that focused mainly on a reduced subset of mappers and or empirically compared the performance of a small number of mappers (;).

discussion the development of numerous mappers for HTS data is motivated not only by novel developments of HTS technology but also by the growing number of biological applications. The variety of applications has led to the appearance of specific types of one commonly asked question is what is the best mapper for a given application. Although the 'best mapper' criterion involves application specific requirements such as how well it work in conjunction with downstream analysis tools (e.g. variant callers), it often also includes speed and, in particular, accuracy. Despite some recent evaluation studies (), determining the most accurate and fastest mappers for a particular application is still difficult. The primary challenge in assessing mappers is the lack of gold standard datasets for different applications and sequencing technologies. These datasets would not only include the reads but also their true locations and could be based on true data or data generated in silico, using novel or existing simulators such as ART (), BEERS () or flux simulator (). The research community has started to address these issues in the context of different projects, such as the r gasp (http://www.gencodegenes.org/ r gasp and the align athon (http://compbio.soe.ucsc.edu/alignathon/) projects, which aim, respectively, to assess the status of computational methods to map human rnase q data and dna seq data to whole genomes. However, no results are publicly available at this time. More generally, a common approach for comparing mappers has been to count the number of reads aligned. However, increasing the number of reads is not useful if the probability of the reads being correctly mapped decreases, i.e. if the increase in mapped reads is done at the expense of increasing the proportion of incorrectly mapped reads. One way to address this problem would be to compute the likelihood of a read being correctly mapped (e.g. as available in r map or ZOOM) and allow the users to choose only the alignments above some threshold. Users may want to consider several mappers in their HTS analysis and to incorporate them in pipelines, such as in array express hts (). This raises the issue of mapper interoperability. To achieve interoperability, input and output formats need to be standardized. Currently, the majority of the mappers accept input files in fast q or c fast q format and generate samba m files as output. Hence, the level of interoperability is high. However, there is still room to improve because fast q files include quality values encoded in different formats and BAM files can also come in different 'flavours' (their standardization should be encouraged). Moreover, in the future, mappers may also include the option to output files in the CRAM format (), which may prove useful for efficiently compressing DNA sequences. The input parameters of the mappers are far from being normalized, which makes it more difficult for a practitioner to switch between mappers. Hence, it would be useful if there was an effort to standardize the most commonly used parameters (e.g. for defining seed lengths, input output files and formats). Finally, the great flexibility and configurability of most mappers comes with a price: a considerable number of parameters that have to be set. Determining the best parameter values to achieve some pre-defined level of mapping specificity sensitivity is far from being trivial. Mappers with the ability to automatically tune their parameters to achieve some user defined specificity sensitivity may be a solution to this problem.
