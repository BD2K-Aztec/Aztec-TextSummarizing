Motivation: Annotations are a key feature of many biological databases, used to convey our knowledge of a sequence to the reader. Ideally, annotations are curated manually, however manual curation is costly, time consuming and requires expert knowledge and training. Given these issues and the exponential increase of data, many databases implement automated annotation pipelines in an attempt to avoid un-annotated entries. Both manual and automated annotations vary in quality between databases and annotators, making assessment of annotation reliability problematic for users. The community lacks a generic measure for determining annotation quality and correctness, which we look at addressing within this article. Specifically we investigate word reuse within bulk textual annotations and relate this to zip fs Principle of Least Effort. We use the UniProt Knowledgebase (UniProtKB) as a case study to demonstrate this approach since it allows us to compare annotation change, both over time and between automated and manually curated annotations. Results: By applying power law distributions to word reuse in annotation, we show clear trends in UniProtKB over time, which are consistent with existing studies of quality on free text English. Further, we show a clear distinction between manual and automated analysis and investigate cohorts of protein records as they mature. These results suggest that this approach holds distinct promise as a mechanism for judging annotation quality. Availability: Source code is available at the authors website:

introduction a key descriptive feature of biological data is its annotation: a textual representation of the biology associated with the data. Biologists use these annotations to understand and contextualize data in biological sequence databases. Annotations play an essential role in describing and developing the users' knowledge of a given sequence and can form the foundation for further research (). Some annotation is structured, for example, using an ontology () or keyword list. However, free text annotation often contains the richest biological knowledge (), but while free text is appropriate for human comprehension it is difficult to interpret computationally. * To whom correspondence should be addressed.  Present address: Oxford Gene Technology, Yarnton, Oxfordshire, OX5 1QU, uk the current 'gold standard' for annotation is a set of reviewed and manually curated entries (). However, manually curated annotation is labour intensive time consuming and costly. To cope with the amount of data, which is typically increasing exponentially, many resources and projects generate annotations computationally (). Automated annotations are more prone to errors than their manual counterparts (), with several studies suggesting high levels of mis annotation in automated annotation (). It can be hard, even impossible, to determine the source from which an error has propagated () causing significant problems for biologists. Annotation quality is not consistent across all databases and annotators (), whether curated manually or automatically. It can, therefore, be difficult to determine the level of quality, maturity or correctness of a given textual annotation. However users often incorrectly assume that annotations are of consistent quality and correctness (). Currently there are few standard metrics for assessing annotation quality. Annotations are frequently assigned a score, using a variety of methods. These approaches include assigning confidence scores to annotations based on their stability () or combining the breadth (coverage of gene product) and the depth (level of detail) for the terms in the Gene Ontology (GO) (). However, while deeper nodes within an ontology are generally more specialized, these measures are problematic; first GO has three root domains and second an ontology, such as GO, is a graph not a tree, therefore depth is not necessarily meaningful. Other methods () use evidence codes as a basis for an annotations reliability, although ironically, the GO annotation manual explicitly states that evidence codes should NOT be used in this way (), describing rather the type of evidence not its strength. All of these approaches rely upon additional information to determine annotation quality. Resources, such as sequence databases, vary in their structures and in the additional information stored. For example, not all resources use evidence codes and these codes are not comparable between resources (); likewise, it is not generally possible to use methods based on an ontological hierarchy for non ontological resources. Most resources carry some annotations which are unstructured, free text. Therefore, a quality metric that can be derived purely from textual annotation would potentially allow any resource to be analysed and scored. There are various measures for analysing the quality of text, such as the flesch kincaid Readability Test () and SMOG Grading (). These metrics are generally based around readability, or reading age that is the

discussion the biological community lacks a generic quality metric that allows biological annotation to be quantitatively assessed and compared. In this article we applied power law distributions to the UniProtKB database and linked the extracted  values to zip fs principle of least effort in an attempt to derive such a generic quality metric. The results within this article give confidence to our initial hypothesis that this approach holds promise as a quality metric for textual annotation. Initially, our analysis focused on the manually curated SwissProt. As shown in, early versions of Swiss-Prot give  values that suggest annotations were of high quality; that is, they were of least effort for the reader. However, over time we see a steady reduction in the  value, which does suggest that the average annotation is now harder for readers to interpret, requiring more expert ize to consume the data than was previously required. This i566 result is perhaps best explained by the exponential increase of data that is added to Swiss-Prot. Manual annotations are regarded as the highest quality annotation available and it is inevitable that the acknowledged pressure on manual annotation is going to increase. This conclusion appears to fit with previous research () that shows manual curation techniques can not keep up with the increasing rate of data. Many of the patterns exhibited by Swiss-Prot are also shown in our analysis of TrEMBL. From, we conclude that annotation in Swiss-Prot and TrEMBL show similar characteristics in that, for both cases, annotation appears to be increasingly optimized to minimize efforts for the annotator rather than the reader; unsurprisingly, this appears to be more pronounced for TrEMBL than for Swiss-Prot. We are currently unclear whether this form of direct numerical comparison over the two different resources is highly meaningful, although this distinction between the two resources appears to be more pronounced over time rather than less. Therefore, these results are consistent with the conclusion that manual annotation behaves as a significantly more mature language than automated annotation. This fits with our a priori assumptions which again is suggestive that this form of analysis is operating as a measure of quality. In addition to analysing whole UniProtKB datasets, we also investigated how sets of entries mature over time () and the quality of annotations within new entries (). Within the mature entries we interestingly see a decrease in quality over time, rather than increasing or maintaining a similar quality level. However this decrease is much slower than the Swiss-Prot database as a whole over time, and is still of much higher quality than the remainder of entries in UniProtKB version 15. For the new annotations we also see a general decrease in quality over time. It is plausible that these results stem from the manner of management and curation of annotations; the UniProt annotation protocol consists of six key steps (), one of which is identifying similar entries (from the same gene and homologues by using BLAST against UniProtKB, uniref s and phylo genomic resources). If two entries from the same gene and species are identified then they are merged; annotations between the remaining entries are then standardized. It would appear that attempts to standardize growing sets of similar entries is having a detrimental effect on the quality of both individual entries and the overall database. In addition to being used as a quality measure, the approach described here could be used for arte factual error detection. Our early analysis identified information with no biological significance (copyright statements) included within the comment lines.
