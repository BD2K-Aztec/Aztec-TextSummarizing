Motivation: Genomes in higher eukaryotic organisms contain a substantial amount of repeated sequences. Tandem Repeats (TRs) constitute a large class of repetitive sequences that are originated via phenomena such as replication slippage and are characterized by close spatial contiguity. They play an important role in several molecular regulatory mechanisms, and also in several diseases (e.g. in the group of trinucleotide repeat disorders). While for TRs with a low or medium level of divergence the current methods are rather effective, the problem of detecting TRs with higher divergence (fuzzy TRs) is still open. The detection of fuzzy TRs is prop aed eu tic to enriching our view of their role in regulatory mechanisms and diseases. Fuzzy TRs are also important as tools to shed light on the evolutionary history of the genome, where higher divergence correlates with more remote duplication events. Results: We have developed an algorithm (christened tr stalker with the aim of detecting efficiently TRs that are hard to detect because of their inherent fuzziness, due to high levels of base substitutions, insertions and deletions. To attain this goal, we developed heuristics to solve a Steiner version of the problem for which the fuzziness is measured with respect to a motif string not necessarily present in the input string. This problem is akin to the generalized median string that is known to be an np hard problem. Experiments with both synthetic and biological sequences demonstrate that our method performs better than current state of the art for fuzzy TRs and that the fuzzy TRs of the type we detect are indeed present in important biological sequences. Availability: tr stalker will be integrated in the web based TRs Discovery Service (TReaDS) at

introduction repeats (TRs) are multiple (two or more) duplications of substrings in the DNA that occur contiguously, and may involve some base mutations (such as substitutions, insertions and deletions). TRs of several forms (satellites, microsatellites, mini satellites and others) have been studied extensively because of their role in several biological processes. In fact, TRs are privileged targets in activities such as fingerprinting or tracing the evolution of populations (). Several diseases, disorders and addictive behaviors are linked to specific TR loci (). The role of TRs has been studied also within coding regions (O') and in relation to gene functions (). Large scale comparative studies on TRs of the human genome are described in and. Data Bases of repetitive elements * To whom correspondence should be addressed. such as rep base () and Tandem Repeats Database tr db () are now available; and the detection of repetitive elements via library based similarity matching, for example by using the tool Repeatmasker (), is a popular practice. However, tools for ab initio detection of repetitive elements that are not based on prior knowledge accumulated in data bases are still important in order to extend our comprehension of the role of TRs in biological mechanisms. Existing ab initio tools are successful when the TR exhibits a moderate amount of divergence and when the TR is easily validated. However, there is an emerging need for new tools that are able to cope with higher levels of sequence divergence and or TR computationally more difficult to validate. For example study so called Fuzzy TRs and their role in gene expression. The technique in works well for the Hamming metric (only substitutions and no insertions deletions allowed) and for short repeat units (from 3 to 24 bp) that are common in micro and mini-satellite families. Some of the most successful ab initio tools, such as TRF () and at r hunter (), are based on a multistage filtering approach [see also (. In the first stage the input sequence is analyzed to detect, via statistical criteria, likely position and length of candidate subsequences. The final stage is the validation one in which a more expensive test is applied to candidate substrings passing the first stages, so to determine an output that matches the implicit definition of TR and the user defined filtering parameters.

discussion we have performed comparative experiments both with synthetic and with biological sequences. Here we describe the experimental page i363 i358i366set up, how the synthetic sequences are generated and the outcome of the comparison. For biological data, we briefly indicate the reason why that sequence has been selected, and the new TRs found by the application of tr stalker
conclusion tr stalker is a novel efficient heuristic algorithm for finding Fuzzy TRs in biological sequences. tr stalker aims at improving the capability of TR detection for a class of fuzzy TRs for which existing methods do not perform well. Initial testing on biological data show that fuzzy TRs not previously reported are present in biologically relevant sequences. In the case of the fra tax in sequence, the fuzzy TR reported is associated with the known variable copy number breakpoint of frederich s ataxia. Future work will involve testing tr stalker on relevant families of repetitive elements such as centromeric -satellites. An extension of tr stalker to handle amino acid sequences is under development.
