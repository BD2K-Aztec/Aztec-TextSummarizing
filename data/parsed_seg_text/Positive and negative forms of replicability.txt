Motivation: Gene networks have become a central tool in the analysis of genomic data but are widely regarded as hard to interpret. This has motivated a great deal of comparative evaluation and research into best practices. We explore the possibility that this may lead to overfitting in the field as a whole. Results: We construct a model of research communities sampling from real gene network data and machine learning methods to characterize performance trends. Our analysis reveals an important principle limiting the value of replication, namely that targeting it directly causes easy or unin-formative replication to dominate analyses. We find that when sampling across network data and algorithms with similar variability, the relationship between replicability and accuracy is positive spearman s correlation, r s $0.33) but where no such constraint is imposed, the relationship becomes negative for a given gene function (r s $ Ã€0.13). We predict factors driving replicability in some prior analyses of gene networks and show that they are unconnected with the correctness of the original result, instead reflecting replicable biases. Without these biases, the original results also vanish replica bly. We show these effects can occur quite far upstream in network data and that there is a strong tendency within protein protein interaction data for highly replicable interactions to be associated with poor quality control.

introduction increasingly biologists have turned to computational methods to sift through the vast array of pre-existing genomics data for validation that a gene has a molecular role in the phenotype of interest or to prioritize a candidate as disease causal (). These computational methods usually fit under the rubric of 'machine learning' and use network data that represent the interaction of genes or their products. Many of these computational methods depend on a form of 'guilt by association', in which a gene is inferred to possess a particular function based on its similarity to other genes with that function (). The most common form of similarity used in these tasks is that of genomic sequence similarity which is easily implemented through supervised use of BLAST () and comparatively straightforward to interpret. While sequence based analysis is essentially routine within biology, one of the promises of systems biology has been to extend the form of 'association' used to relate genes to potentially subtler relationships, such as protein protein interaction (PPI), co-expression, genetic interaction or phylogenetic profiles. systems based prediction of gene function has found particular application in the interpretation of disease causal variants due to the difficulty of finding overlaps in V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals permission soup com known functions among candidate genes (). However, progress in the context of both data and methodology has been surprisingly uncertain (). The need for better assessment of methods in function inference and network analysis is widely recognized and has led to numerous field wide evaluations, often called critical assessments (). The two principal goals of critical assessments are (i) to make the performances of individual methods less prone to overfitting and (ii) for comparisons between methods to be within the same framework. Overfitting is minimized since participants are truly blind to the success of their method prior to assessment and thus can not 'tailor' their solutions to the benchmarking metric. Gene networks possess unusually prominent consensus resources [e.g. the Gene Ontology (GO) (), bio grid (, making evaluation within a well defined framework possible. By reducing overfitting and making methods directly comparable, critical assessments endeavor to make science more replicable; their outputs and comparative evaluations can be trusted to generalize. The difficulty of characterizing the features in gene networks that drive successful uses has contributed to making replicability in their output, which can be more easily measured, particularly important to evaluation within their critical assessments [e.g. the DREAM challenges () and the Critical Assessment of protein Function Annotation algorithms, CAFA challenge (. In performing this evaluation, critical assessments are simply performing a more top down version of the usual scientific process of refinement through replicability (). While this may be desirable in some ways, it creates a new potential for overfitting for the field in its entirety. We decided to explore this possibility by simulating multiple gene function prediction tasks and outcomes and hence the field of gene network analysis as a whole. In our model of research in gene network analysis, each separate researcher is represented by an individually developed machine learning algorithm with access to particular data. The algorithms are both diverse and in common use for diverse bioinformatics problems and thus reasonably reflecting ordinary practice. The data resources (or 'library') given to these algorithms are similarly diverse and frequently used sources of human gene network information, varying from individual expression profiles to consensus pathway information. We refer to a specific combination of algorithm and data as a 'researcher' (). For example, a researcher may consist of the algorithm 'random walk with restarts' using specific co-expression data. The individual sampled resources do not represent partial data sets but rather ones which are at least as comprehensive as is typical of any given study. Because it is a central characteristic by which we judge results, our focus is on using these model researchers to understand replicability in gene network analysis. After deriving general principles through our simulations, we focus on two important applications affecting the interpretation of disease genes and PPI data in current research, with a focus on psychiatric genetics.
