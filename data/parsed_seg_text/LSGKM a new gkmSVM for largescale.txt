gkm svm is a sequence based method for predicting and detecting the regulatory vocabulary encoded in functional DNA elements, and is a commonly used tool for studying gene regulatory mechanisms. Here we introduce new software, lsg km which removes several limitations of our previous releases, enabling training on much larger scale (LS) datasets. lsg km also provides additional advanced gapped km er based kernel functions. With these improvements, lsg km achieves considerably higher accuracy than the original gkm svm. Availability and implementation: C/C þþ source codes and related scripts are freely available from http://github.com/Dongwon-Lee/lsgkm/, and supported on Linux and Mac OS X. Contact: dw lee jhu edu Supplementary information: Supplementary data are available at Bioinformatics online. We have previously introduced a sequence based method, km ers vm flet ez brant et al., 2013; Lee et al., 2011) to predict regulatory elements from DNA sequence and epigenetic data using support vector machines (SVM) (Vapnik, 1995). It has been successfully applied to studies of regulatory elements in different cellular contexts g orkin et al., 2012; pim kin et al., 2014), and further improved by using gapped km ers as new features gkm svm (Ghandi et al., 2014)). We have also recently demonstrated its ability to predict regulatory sequence variants (Lee et al., 2015). Since then, gkm svm has gained increasing attention (Setty and Leslie, 2015; Zhou and troyan s kaya 2015). Our general strategy is to build an SVM classifier that distinguishes regulatory sequences from non regulatory genomic sequences in the km er or gapped km er frequency feature vector space. Training of SVM involves evaluation of a kernel matrix (or Gram matrix), defined as an nb yn matrix of all possible inner products (or kernel functions) between a set of vectors of n training examples. However, as n increases, direct computation of the kernel matrix quickly becomes impractical with gapped km ers as features. To resolve this issue, gkm svm employs an efficient algorithm that calculates a full kernel matrix with a runtime that linearly scales with n instead of n 2. An SVM classifier is then trained using the pre-calculated kernel matrix and standard SVM training methods. Yet, the full kernel matrix evaluation required in the original implementation has hindered optimal training of gkm svm on larger datasets because it needs substantial memory resources proportional to n 2. Sub-sampling strategies to circumvent this issue can be helpful (Ghandi et al., 2014), but training on smaller datasets may yield sub-optimal SVM classifiers. To tackle this problem, I have developed an improved software, lsg km by implementing gapped km er kernel gkm kernel functions within the libs vm framework (Chang and Lin, 2011). Most SVM tools such as libs vm utilize decomposition methods for SVM training. It iteratively finds and solves a small subset SVM problem that only needs a partial kernel matrix. For example, libs vm evaluates just two columns of the kernel matrix in each of the problem solving steps in its sequential minimal optimization algorithm (Fan et al., 2005). Therefore, replacing the libs vm kernel routines with the gkm kernel functions can essentially solve the memory resource issue and, consequently, allows us to train SVM on much larger datasets. To this end, I adopted and modified the original gkm kernel algorithm, substituting for the original libs vm kernels so that it can efficiently evaluate one column of the kernel matrix in the same manner as the original gkm svm does for the full matrix. Multi-thread functionality is also implemented in lsg km for further speed up (Supplementary Methods for more details). I first compared runtime and memory usage of the new software to the original gkm svm by varying the training set size n (Supplementary Methods, Fig. S1). As expected, our previous method exhibits quadratic growth of memory usage as n increases. gkm train (the new SVM training module of lsg km with the large Downloaded from cache (8 Gb) also exhibits quadratic memory expansion when n  60 000. However, the overall memory usage is much less than the original method ($20%). Moreover, once the cache is full, the memory only linearly increases. Regarding runtime, the original method initially shows better computational efficiency than gkm train with the default setting (1 thread, 100 Mb). However, with either the large cache or the four threads, the new program can run faster than the original one. Furthermore, it can run almost 5Â faster when both options are used. Most notably, we can now regularly train lsg km on much larger datasets with reasonable time and memory. In addition to the integration of kernel functions into libs vm three new kernel functions have been developed. Analogous to the basic RBF kernel, gkm rbf kernel is defined as the radial basis function , in the space of gapped km er frequency vectors (Supplementary Methods). The second option, denoted as center weighted gkm kernel or wg km kernel is inspired by the observation that most chips eq and dnasei seq signals are concentrated in the central regions within peaks. In this new kernel, the gapped km ers are differentially weighted based on their distances from the center of the peak (Supplementary Methods, Fig. S1). The last kernel, wg km rbf kernel is the combination of the previous two. To demonstrate the utility of lsg km I assessed how much classification accuracy can be improved by lsg km for predicting regulatory elements. Uniformly processed 322 ENCODE chips eq datasets (The ENCODE Project Consortium, 2012) containing at least 5000 regions were considered, and standard training and test procedures developed in the previous studies (Ghandi et al., 2014; Lee et al., 2011) were applied with some modifications (Supplementary Methods). First, the new models trained on the whole datasets exhibit considerably better AUC than the models trained on the sub-sampled sets (n ¼ 10 000), especially when the training set is large (n  60 000) (Supplementary Methods, Fig.S2A). A grid search of the C parameter on selected datasets confirms that this result is not an artifact caused by a sub-optimal choice of C (Supplementary Table S1). In fact, our default value (C ¼ 1) was optimal or near optimal in almost all cases we tested. Second, the model trained with gkm rbf kernel further increases the AUC as compared to the original gkm kernel in every case, but the improvement is marginal (Supplementary Methods, Fig.S2B). This result implies that the advantage of using non-linear decision boundaries is limited with gkm kernel. Third, wg km kernel can also significantly improve the AUC, when the datasets already exhibit AUC values 0.9 (Supplementary Methods, Fig.S2C). Closer investigation reveals that most of the less predictive datasets (AUC  0.9) are chips eq on Pol2 and its related factors (TAF1, TAF7 and TBP). This suggests that many of these peaks may represent transient binding of the factors and, thus, contain less predictive sequence features. Fourth, similar to the gkm rbf kernel wg km rbf kernel marginally improves AUC when compared to wg km kernel (Supplementary Methods, Fig.S2D). Note that, in some cases such as Pol2 chips eq the best a ucs are achieved by gkm rbf kernel not by wg km rbf kernel. Therefore, for the final comparison, the better kernels were chosen based on classification performance with independent training and evaluation (Supplementary Methods, Fig. S3). Supplementary Figure S4 compares the baseline a ucs (trained on 10 000 regions with gkm kernel from the original gkm svm to the best a ucs achieved by lsg km. The average gain of AUC is significant (0.912 ! 0.941). If Pol2 and the related chips eq datasets are removed, the average of the best a ucs is remarkably high, 0.960. To determine whether lsg km can also improve delta svm a major application of gkm svm for predicting regulatory sequence variants (Lee et al., 2015), the ds qtl test set was reanalyzed using the new lsg km models trained on a larger GM12878 DHS data-set (Supplementary Methods, Fig. S5). The Precision Recall curves show that the new models consistently outperform the original model. However, no further improvement is achieved with new kernels, suggesting that larger datasets primarily contribute to the improvement of delta svm accuracy. In this study, I have presented new and improved software, lsg km which offers several new functionalities and considerably improves the classification accuracy on predicting regulatory elements. We strongly encourage all users of our software to train models on the largest datasets available, which can produce significantly more accurate predictions. In this regard lsg km should improve the performance considerably, and in combination with its enhanced functions, lsg km is expected to significantly contribute to our understanding of gene regulation.
