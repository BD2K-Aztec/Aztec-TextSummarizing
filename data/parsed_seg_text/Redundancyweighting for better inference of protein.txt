Motivation: Structural knowledge, extracted from the Protein Data Bank (PDB), underlies numerous potential functions and prediction methods. The PDB, however, is highly biased: many proteins have more than one entry, while entire protein families are represented by a single structure, or even not at all. The standard solution to this problem is to limit the studies to non-redundant subsets of the PDB. While alleviating biases, this solution hides the many to many relations between sequences and structures. That is, non-redundant datasets conceal the diversity of sequences that share the same fold and the existence of multiple conformations for the same protein. A particularly disturbing aspect of non-redundant subsets is that they hardly benefit from the rapid pace of protein structure determination, as most newly solved structures fall within existing families. Results: In this study we explore the concept of redundancy weighted datasets, originally suggested by Miyazawa and Jernigan. redundancy weighted datasets include all available structures and associate them (or features thereof) with weights that are inversely proportional to the number of their homologs. Here, we provide the first systematic comparison of redundancy weighted datasets with non-redundant ones. We test three weighting schemes and show that the distributions of structural features that they produce are smoother (having higher entropy) compared with the distributions inferred from non-redundant datasets. We further show that these smoothed distributions are both more robust and more correct than their non-redundant counterparts. We suggest that the better distributions, inferred using redundancy weighting may improve the accuracy of knowledge based potentials and increase the power of protein structure prediction methods. Consequently, they may enhance model driven molecular biology.

introduction mining the riches of experimentally determined data in the Protein Data Bank (PDB) () has been a major source of structural knowledge over the past four decades. In a reverse engineering like fashion it allows the derivation of rules and methods for the prediction of secondary structure (), solvent accessibility () and trans-membrane regions (), as well as knowledge based potentials (;). These methods and potentials have had a considerable impact on our understanding of the protein universe and accelerated progress in biology, chemistry and medicine. This study aims to enhance these data mining efforts by attacking their major obstacle, data redundancy. The underlying premise behind data mining of protein structures is that recurring patterns may result from physical aspects of protein folding and stability (e.g. the hydrophobic effect) (). However, the data, which are available for mining, are not uniform samples of sequence and structure spaces. Certain folds (e.g. TIM barrels and g protein coupled receptors) are far more abundant than others in genomes for some suggestions of why it is so]. The PDB content is further skewed by research interests of the contributing experimentalists and by methodological constraints. This bias, often referred to as the 'PDB redundancy', may amplify or diminish the signal of recurring patterns. For example, the stabilizing effect of the beta alphabet a super secondary structure may be overestimated because of the abundance of folds that include it (e.g. TIM barrels). The common solution to data redundancy is to use a nonredundant subset of the PDB, composed of family representatives pioneered this solution using a 62 membered subset of the 75 high quality entries of the PDB, leaving out homologs with sequence identity of 50% or higher (a rather promiscuous threshold by current standards). This approach has been adopted by numerous studies, and became the field's norm, with publicly available and standardized tools for data culling (). Yet, notwithstanding the evident utility of nonredundant PDB subsets, they have inherent limitations in scalability and descriptive power. First and foremost, they do not benefit from the rapid growth of the PDB because almost all new entries are mapped to already known folds [. More importantly, non-redundant datasets conceal much of the complexity of protein universe. Specifically, they hide the compatibility of diverse sequences with the same fold and the flexibility of protein structures (). Our working hypothesis is that this oversimplified perspective manifests itself in artificially 'bumpy' distributions of the measurable features. An important alternative was presented more than a decade ago by), who weighted structures in their knowledge based potentials. In those studies, they considered all the PDB structures (of sufficient quality and length), yet assigned non-uniform weights to protein chains, so that the weights of chains with many homologs are scaled down. Thus, all the data are considered, yet biases are alleviated. Somewhat surprisingly, we are unaware of any recent studies that use this approach. Further, to the best of our knowledge, no study has yet compared its performance with the standard, representative subset, approach. Interestingly, the redundancy, which is removed from structural datasets, is valuable when investigating families of homologous sequences. There, evolutionary conserved patterns characterize a family and deviations from these patterns shed light on the uniqueness of specific members. The distribution of sequence similarities across a family is typically uneven; that is, there may be subsets of sequences that are more closely related. This might bias the analysis toward patterns that appear in such highly similar subsets. In this context, however, a non-redundant subset (i.e. one without recognizable similarities) would consist of a single sequence, and be devoid of any information. Instead, scholars have successfully used various weighting schemes that downscale contributions from large subsets of sequences, and references therein], most notably for multiple sequence alignment () and sequence search (). Structural data mining is a different computational task: most importantly, it does not focus on a single protein family but rather must deal with multiple families and account for their varying sizes. Nonetheless, the success of including more, and even all, available proteins in the context of search and alignment tasks, suggests that similar approaches may be valuable in structural data mining as well. Our study revisits the redundancy weighting approach and provides the first systematic assessment of its correctness and robustness. To this end, we compare interatomic distance distributions, a central component in knowledge based potentials, sampled from either non-redundant or redundancy weighted datasets; for the sake of completeness, we also consider distributions that were sampled from an unweighted, redundant dataset. We estimate the complexity of these distributions by their entropy and observe that the redundancy weighted datasets have higher entropy than their non-redundant counterparts. We further demonstrate that the higher entropy distributions are more correct and robust. Our observations suggest that structure prediction methods could benefit considerably from training on redundancy weighted datasets rather than on non-redundant ones. This in turn, can improve our understanding of the forces that shape the protein structure universe.

discussion the dominant approach for data mining of the PDB is to extract the knowledge from a relatively small subset of non-homologous structures and consider their homologs (all other structures) as redundant and, thus, non informative. Although this approach circumvents much of the inherent biases in the PDB, it also artificially reduces the variability of the structural landscape. an alternative approach, which we term here redundancy weighting considers all available structures but assigns them (or features thereof) lower weights proportionally to the number of homologs they have in the dataset. redundancy weighting originally proposed by Miyazawa and Jernigan almost two decades ago (), has been rarely used since and, to the best of our knowledge, never been systematically benchmarked against the standard approach. Here, we compare the correctness, complexity and robustness of feature distributions, which were inferred using non-redundant and redundant datasets, and three redundancy weighting schemes: the algebraic sample weighting of Miyazawa and Jernigan (MJ) (), as well as our novel RWs and RWf weightings. To this end, we quantify and compare the correctness, complexity and robustness of distance distributions, which were derived from training and test datasets according to these schemes. Our most significant contribution is demonstrating that the three redundancy weighting schemes outperform the 'standard' non-redundant approach in all tested metrics. The MJ scheme, which is the most robust, is probably not scalable enough for practical use. This scheme requires an eigenvalue decomposition of an all against all similarity matrix. As the PDB is quickly approaching the 100 000 structures milestone, such decomposition becomes challenging both in terms of the computational requirements (space and time) and the numerical stability (). RWf, which is the most correct scheme, has a milder limitation: it requires recomputing of weights for each structural feature. The RWs scheme is less accurate and robust than the other two, but does not suffer from these limitations. Thus, it may serve as a simple, off the shelf general weighting scheme, which performs better than non-redundant datasets. Here, we focused on a relatively small set (only a few thousands) of the pdb s most accurately solved structures (with resolution better than 1.5 A  ). Focusing on this set had several advantages: most importantly, the computational cost of exploring alternative weighting schemes is tractable and, in particular, we could easily calculate the eigenvalue decomposition needed for the MJ weighting scheme. Notice that homology is not too common within this set, which includes 450% singletons (see). Thus, one could expect that refining the weighting scheme will not have any impact on the accuracy and robustness when inferring structural features from the set. However, it does. Thus, we believe that such weighting schemes can similarly benefit others, even more so when considering larger subsets of the PDB, culled using more lax experimental quality thresholds. While most residue residue contact potentials [e.g. Miyazawa and] consider un directional residue pairs (thus combining pairs of two same residues with different orders to a single unique pair), we chose to focus on directional pairs to increase the dynamic range of the explored features (). Notably, the trends shown in are preserved when un directional pairs are considered (data not shown).) sample weighting; RWs: per sequence redundancy weighting RWf: per feature redundancy weighting Importantly, our study shows that the gain in accuracy and robustness is proportional to the rarity of the studied feature (Figs. 4 and 5). Current applications of PDB data mining (e.g. derivation of pairwise potentials and secondary structure prediction) are dominated by highly prevalent features (e.g. contacts between specific residues and the three major secondary structure elements). However, in more subtle applications [e.g. multi-body potentials () and fragment prediction (] the number of relevant features grows while their prevalence in the non-redundant dataset drops. We speculate that shifting to the redundancy weighting paradigm may be essential for the advancement of computational structural biology beyond pairwise potentials and prediction of simple features. Although two of the redundancy weighting schemes presented here are already useful, this study is mostly a proof of concept. A promising route for improvement is to take advantage of rapid structural search methods () and replace the currently used sequence alignments by more reliable and sensitive structural alignments. Another possible direction is to focus on domains rather than peptide chains, which often harbor several, repeating domains (e.g. Zn fingers) and are, thus redundant by nature. Finally, the weighting scheme may apply some evolutionary model to gain better estimate of the interdependencies of homologous proteins (). In a wider context, redundant datasets are abundant in other domains of knowledge, e.g. genomics, natural language processing and computer vision. While the current work focuses on protein structure datasets, we hope insights obtained in this domain can have impact on data mining in other, unrelated ones.
