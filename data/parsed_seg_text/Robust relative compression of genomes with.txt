Motivation: Storing, transferring and maintaining genomic databases becomes a major challenge because of the rapid technology progress in DNA sequencing and correspondingly growing pace at which the sequencing data are being produced. Efficient compression, with support for extraction of arbitrary snippets of any sequence, is the key to maintaining those huge amounts of data. Results: We present an lz77 style compression scheme for relative compression of multiple genomes of the same species. While the solution bears similarity to known algorithms, it offers significantly higher compression ratios at compression speed over an order of magnitude greater. In particular, 69 differentially encoded human genomes are compressed over 400 times at fast compression, or even 1000 times at slower compression (the reference genome itself needs much more space). Adding fast random access to text snippets decreases the ratio to âˆ¼300. Availability: GDC is available at http://sun.aei.polsl.pl/gdc.

introduction rapid development in DNA sequencing technologies led to drastic growth of data publicly available in sequence databases, for example, GenBank at NCBI or 1000 Genomes project. The low cost of acquiring an individual human genome opens the door to 'personalized medicine'. DNA sequences within the same species are both large and highly repetitive. For example, only 0.1% of the 3 GB human genome is specific; the rest is common to all humans. This poses interesting challenges for efficient storage and fast access to those data. Most classic data compression techniques fail to recognize this tremendous redundancy, simply because finding matches with e.g. an LZ77 variant with a sliding window would require a multi gigabyte buffer, not counting the match finding structures. On the other hand, using a context based statistical coding (for example, PPM) may require maintaining a high order statistical model, otherwise the context statistics will be polluted with 'accidental' DNA matches. Using such a model is problematic, due to its enormous memory requirements. * To whom correspondence should be addressed interestingly most of the dna specialized compressors from the literature are inappropriate to handle modern genomic databases. There are a number of reasons for that: (i) most of them care about compression ratio rather than compression and decompression speed or the memory use during the compression process; for these reasons, they are not practical for sequences larger than, say, several megabytes; (ii) most effort has been focused on succinctly representing a single genome (which is believed to be almost incompressible anyway, hence only tiny improvements were at the stake), not to be particularly efficient in detecting inter genome redundancy; (iii) extracting a range of symbols from the middle of the compressed stream is a rarely supported feature. Only since around 2009 can we observe a surge of interest in practical, multi sequence oriented DNA compressors, usually coupled with random access capabilities and sometimes also offering indexed search. The first algorithms from 2009 () were soon followed by more mature proposals, which will be presented below added index functionalities to compressed DNA sequences: display (which can also be called the random access functionality) returning the substring specified by its start and end position, count telling the number of times the given pattern occurs in the text and locate listing the positions of the pattern in the text. The authors noticed that the existing general solutions, paying no attention to long repeats in the input, are not very effective here, and they proposed novel self indexes for the problem. Other full text indexes for repetitive sequences were proposed in and Kreft and Navarro (2011). The former work presents two schemes, one using an inverted index on q grams tailored to the repetitive nature of the input data, and the other being a grammar based index. The latter paper introduced a self index based on the l zend (), which is an algorithm interesting in its own, as it is an LZ77 variant enabling efficient extraction of arbitrary phrases presented a scheme for referential compression of genomes, working on the chromosome level. If the pair of respective chromosomes is similar enough, one of them is encoded with respect to the chromosome from the reference sequence. If not, it is split into several pieces for which the best alignments are then found in the reference chromosome. Then, longest common subsequences between matching parts are found and the differences encoded using Huffman coding proposed a simple yet quite efficient compression scheme with random access (it is not an index though), dubbed rlz std. They choose one of the sequences as the reference sequence and compress it with a self index (in the cited work, however, a general purpose compressor, 7z, with no random access
