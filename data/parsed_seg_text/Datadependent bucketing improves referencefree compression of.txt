Motivation: The storage and transmission of high throughput sequencing data consumes significant resources. As our capacity to produce such data continues to increase, this burden will only grow. One approach to reduce storage and transmission requirements is to compress this sequencing data. Results: We present a novel technique to boost the compression of sequencing that is based on the concept of bucket ing similar reads so that they appear nearby in the file. We demonstrate that, by adopting a data dependent bucket ing scheme and employing a number of encoding ideas, we can achieve substantially better compression ratios than existing de novo sequence compression tools, including other bucket ing and reordering schemes. Our method, Mince, achieves up to a 45% reduction in file sizes (28% on average) compared with existing state of the art de novo compression schemes.

introduction the tremendous quantity of data generated by high throughput sequencing experiments poses many challenges to data storage and transmission. The most common approach to reduce these space requirements is to use an off the shelf compression program such as gzip (by Gailly and Adler, http://www.gnu.org/software/gzip/) or bzip2 (by Seward, http://www.bzip.org) to compress the raw read files. This approach can result in substantial savings during storage and transmission. These programs are general purpose, well tested and highly scalable. However, research over the past few years has demonstrated that approaches specifically tailored to compressing genomic data can achieve significantly better compression rates than general purpose tools. We introduce Mince, a compression method specifically designed for the compression of high throughput sequencing reads, that achieves state of the art compression ratios by encoding the read sequences in a manner that vastly increases the effectiveness of off the shelf compressors. This approach, known as compression boosting, has been effectively applied in other contexts, and is responsible for the widely observed phenomenon that BAM files become smaller when alignments are ordered by genomic location. This places more similar alignments nearby in the file and results in more effective compression being possible. Mince is able to produce files that are 28% smaller than those of existing compression methods in a comparable amount of time. Existing work on compressing sequencing reads falls into two main categories: reference based and de novo compression. reference based methods most often, but not always (), attempt to compress aligned reads (e.g. BAM format files) rather than raw, unaligned sequences. They assume that the reference sequence used for alignment is available at both the sender and receiver. Most reference based approaches attempt to take advantage of shared information between reads aligned to genomic ally close regions, and to represent the aligned reads via relatively small 'edits' with respect to the reference sequence (). These methods can, in general, be very effective at compressing alignments, but this does not necessarily imply effective compression of the original read sequences (). Thus, if one is interested in the most efficient methods to compress the raw reads, reference based methods can have drawbacks when compared with de novo approaches. They are generally slower, since they require that reads be mapped to a reference before being compressed. They assume that the sender and receiver have a copy of the reference (which, itself, would have to be transferred) and that the set of reads can be mapped with relatively high quality to this reference (such methods may perform poorly if there are many unmapped reads). Furthermore, since different types of analysis may require different types of alignments, recovering the original BAM file may not always be sufficient, in which case further processing, such as extracting the original sequences from the alignment file, may be required. Conversely, de novo approaches compress the raw sequencing reads directly, and because they do not require aligning the reads to a reference, are often able to compress the reads much more quickly. De novo compression methods often work by trying to exploit redundancy within the set of reads themselves rather than between the reads and a particular reference (). Although most approaches tend to fall into one or the other of these categories, some tools expose both reference based and reference free modes (). notably introduced a novel approach for obtaining some of the benefits of reference based compression, even when no reference is available, by constructing one on the fly. Another similar area of research is the compression of collections of related genomes (). These approaches are able to achieve a very high degree of compression, but generally rely on encoding a sparse and relatively small set of differences between otherwise identical sequences. Unfortunately, the reads of a sequencing experiment are much more numerous and diverse than a collection of related genomes, and hence, these methods do not apply to the compression of raw or aligned sequencing reads. We focus on the problem of de novo compression of raw sequencing reads, since it is the most generally applicable. Mince was inspired by the approach of of compression 'boosting'. Mince only compresses the actual sequences, because the compression of quality scores and other metadata can be delegated to other approaches () that are specifically designed for compressing those types of data. At the core of Mince is the idea of bucket ing or grouping together, reads that share similar sub-sequences. After reads are assigned to buckets, they are reordered within each bucket to further expose similarities between nearby reads and deterministically transformed in a manner that explicitly removes a shared 'core' substring, which is the label of the bucket to which they have been assigned. The information encoding this reordered collection of reads is then written to a number of different output streams, each of which is compressed with a general purpose compressor. Depending on the type of read library being compressed, we also take advantage of the ability to reverse complement reads to gain better compression. In the presence of a reference, placing reads in the order in which they appear when sorted by their position in the reference reveals their overlapping and shared sequences. Without a reference, we can not directly know the reference order. The bucket ing strategy described here attempts to recover an ordering that works as well as a reference based order without the advantage of being able to examine the reference. We demonstrate that the bucket ing scheme originally introduced by, though very effective, can be substantially improved (on average  15%) by grouping reads in a data dependent manner and choosing a more effective encoding scheme. Choosing a better downstream compressor, l zip (by Diaz, http://www.nongnu. or gl zip l zip html leads to a further reduction in size of 10%. Overall, Mince is able to obtain significantly better compression ratios than other de novo sequence compressors, yielding compressed sequences that are, on average, 28% smaller than those of scal ce
discussion we introduced Mince, a de novo approach to sequence read compression that outperforms existing de novo compression techniques and works by boosting the already impressive l zip general purpose compressor. Rather than rely on a set of pre-specified 'core substrings' like scal ce (), Mince takes a data driven approach, by considering all km ers of a read before deciding the bucket into which it should be placed. Further, Mince improves on the 'heaviest bucket' heuristic used by scal ce and instead defines a more representative model for the marginal benefit of particular bucket assignment. This model takes into account the '-mer composition of the read and how similar it is to the set of '-mers of reads that have already been placed in this bucket. Early on in the processing of a file, when little information exists about the relative abundance of different km ers ties between buckets are broken consistently by preferring to bucket a read based on its minimizer. This approach allows the selection of core substrings that are among the most frequent km ers in the provided set of reads, and the improved model for bucket assignment leads to more coherent buckets and better downstream compression. In the rare situations where a specific order is required for the reads, Mince is not the most appropriate compression approach. Further, in addition to reordering, Mince exploits other transformations of a read, such as reverse complementing, that may or may not be performed in a lossy fashion. Regardless of whether or not these transformations need to be reversed during decoding, they lead to improvements in compression that overcome the cost of storing the 'sideband' information necessary to reverse them.
