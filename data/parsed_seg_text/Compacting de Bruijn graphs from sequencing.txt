Motivation: As the quantity of data per sequencing experiment increases, the challenges of fragment assembly are becoming increasingly computational. The de Bruijn graph is a widely used data structure in fragment assembly algorithms, used to represent the information from a set of reads. Compaction is an important data reduction step in most de Bruijn graph based algorithms where long simple paths are compacted into single vertices. Compaction has recently become the bottleneck in assembly pipelines, and improving its running time and memory usage is an important problem. Results: We present an algorithm and a tool b calm 2 for the compaction of de Bruijn graphs. b calm 2 is a parallel algorithm that distributes the input based on a minimizer hashing technique, allowing for good balance of memory usage throughout its execution. For human sequencing data, b calm 2 reduces the computational burden of compacting the de Bruijn graph to roughly an hour and 3 GB of memory. We also applied b calm 2 to the 22 Gbp loblolly pine and 20 Gbp white spruce sequenc-ing datasets. Compacted graphs were constructed from raw reads in less than 2 days and 40 GB of memory on a single machine. Hence, b calm 2 is at least an order of magnitude more efficient than other available methods. Availability and Implementation: Source code of b calm 2 is freely available at: https://github.com/ gat bb calm Contact: rayan.chikhi@univ-lille1.fr

introduction modern sequencing technology can generate billions of reads from a sample, whether it is RNA, genomic DNA, or a meta genome. In some applications, a reference genome can allow for the mapping of these reads; however, in many others, the goal is to reconstruct long contigs. This problem is known as fragment assembly and continues to be one of the most important challenges in bioinformatics. Fragment assembly is the central algorithmic component behind the assembly of novel genomes, detection of gene transcripts rnase q (), species discovery from meta genomes structural variant calling (). Continued improvement to sequencing technologies and increases to the quantity of data produced per experiment present a serious challenge to fragment assembly algorithms. For instance, while there exist many genome assemblers that can assemble bacterial sized genomes, the number of assemblers that can assemble a high quality mammalian genome is limited, with most of them developed by large teams and requiring extensive resources (). For even larger genomes, such as the 20 Gbp Picea glauca (white spruce), graph construction and compaction took 4.3 TB of memory, 38 h and 1380 CPU cores (). In another instance, the whole genome assembly of 22 Gbp Pinus taeda (loblolly pine) required 800 GB of memory and three months of running time on a single machine (). Most short read fragment assembly algorithms use the de Bruijn graph to represent the information from a set of reads. Given a set of reads R, every distinct km er in R forms a vertex of the graph, while an edge connects two km ers if they overlap by k  1 characters. The use of the de Bruijn graph in fragment assembly consists of a multi-step pipeline, however, the most data intensive steps are usually the first three: nodes enumeration, compaction and graph cleaning. In the first step (sometimes called km er counting), the set of distinct km ers is extracted from the reads. In the second step, all unit igs (paths with all but the first vertex having in degree 1 and all but the last vertex having out degree 1) are compacted into a single vertex. In the third step, artifacts due to sequencing errors and polymorphism are removed from the graph. The second and third step are sometimes alternated to further compact the graph. After these initial steps, the size of the data is reduced gradually, e.g. for a human dataset with 45 coverage, To overcome the scalability challenges of fragment assembly of large sequencing datasets, there has been a focus on improving the resource utilization of de Bruijn graph construction. In particular, km er counting has seen orders of magnitude improvements in memory usage and speed. As a result, graph compaction is becoming the new bottleneck; but, it has received little attention (). Recently, we developed a compaction tool that uses low memory, but without an improvement in time (). Other parallel approaches for compaction have been proposed, as part of genome assemblers. However, most are only implemented within the context of a specific assembler, and can not be used as modules for the construction of other fragment assemblers or for other applications of de Bruijn graphs (e.g. meta genomics. In this paper, we present a fast and low memory algorithm for graph compaction. Our algorithm consists of three stages: careful distribution of input km ers into buckets, parallel compaction of the buckets, and a parallel reunification step to glue together the compacted strings into unit igs. The algorithm builds upon the use of minimizers to partition the graph (); however, the partitioning strategy is completely novel since the strategy of does not lend itself to parallelization. Due to the algorithm's complexity, we formally prove its correctness. We then evaluate it on whole genome human, pine and spruce sequencing data. The de Bruijn graph for a whole human genome dataset is compacted in roughly an hour and 3 GB of memory using 16 cores. For the 20 Gbp pine and spruce genomes, km er counting and graph compaction take only 2 days and 40 GB of memory, improving on previously published results by at least an order of magnitude.

discussion in this paper, we present b calm 2, an open source parallel and low memory tool for the compaction of de Bruijn graphs. b calm 2 constructed the compacted de Bruijn graph of a human genome sequencing dataset in 76 mins and 3 GB of memory. Furthermore, km er counting and graph compaction using b calm 2 of the 20 Gbp white spruce and the 22 Gbp loblolly pine sequencing datasets required only 2 days and 40 GB of memory each. b calm 2 is different from previous approaches in several regards. First, it is a separate module for compaction, with the goal that it can be used as part of any other tools that build the de Bruijn graph. While parallel genome assemblers offer impressive performance, there are many situations where differences in data require the development of a new assembler, and hence it is desirable to build for b calm 2 and b calm we used k  55, and '  8 and '  10, respectively; abundance cutoffs were set to 5 for Chr 14 and 3 for whole human. We used 16 cores for the parallel algorithms ABySS, mera culo us 2 and b calm 2. mera culo us 2 aborted with a validation failure due to insufficient peak km er depth when we ran it with abundance cutoffs of 5. We were able to execute it on chromosome 14 with a cut off of 8, but not for the whole genome. (  )For the whole genome, we show the running times given in. The exact memory usage was unreported there but is less than 1 TB. mera culo us 2 was executed with 32 prefix blocks the km er size was 31 and the abundance cut off for km er counting was 7.
