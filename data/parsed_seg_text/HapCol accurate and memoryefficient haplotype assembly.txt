Motivation: Haplotype assembly is the computational problem of reconstructing haplotypes in dip-loid organisms and is of fundamental importance for characterizing the effects of single nucleotide polymorphisms on the expression of phenotypic traits. Haplotype assembly highly benefits from the advent of future generation sequencing technologies and their capability to produce long reads at increasing coverage. Existing methods are not able to deal with such data in a fully satisfactory way, either because accuracy or performances degrade as read length and sequencing coverage increase or because they are based on restrictive assumptions. Results: By exploiting a feature of future generation technologies the uniform distribution of sequencing errors we designed an exact algorithm, called hap col that is exponential in the maximum number of corrections for each single nucleotide polymorphism position and that minimizes the overall error correction score. We performed an experimental analysis, comparing hap col with the current state of the art combinatorial methods both on real and simulated data. On a standard benchmark of real data, we show that hap col is competitive with state of the art methods, improving the accuracy and the number of phased positions. Furthermore, experiments on realistically simulated data-sets revealed that hap col requires significantly less computing resources, especially memory. Thanks to its computational efficiency, hap col can overcome the limits of previous approaches, allowing to phase datasets with higher coverage and without the traditional all heterozygous assumption.

introduction diploid organisms such as humans contain two sets of chromosomes, one from each parent. Reconstructing the two distinct copies of each chromosome, called haplotypes, is crucial for characterizing the genome of an individual. The process is known as phasing or haplo typing and the provided information may be of fundamental importance for many applications, such as analyzing the relationships between genetic variation and gene function, or between genetic variation and disease susceptibility (). In diploid species, haplo typing requires V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals permission soup com assigning the variants to the two parental copies of each chromosome, which exhibit differences in terms of single nucleotide polymorphisms (SNPs). Since a large scale direct experimental reconstruction of the haplotypes from the collected samples is not yet cost effective (), a computational approach called haplotype assembly that considers a set of reads, each one sequenced from a chromosome copy, has been proposed. Reads (also called fragments) have to be assigned to the unknown haplotypes, using a reference genome in a preliminary mapping phase, if available. This involves dealing in some way with sequencing and mapping errors and leads to a computational task that is generally modelled as an optimization problem (). Minimum error correction (MEC) () is one of the prominent combinatorial approaches for haplotype assembly. It aims at correcting the input data with the minimum number of corrections to the SNP values, such that the resulting reads can be unambiguously partitioned into two sets, each one identifying a haplotype. w mec () is the weighted variant of the problem, where each possible correction is associated with a weight that represents the confidence degree assigned to that SNP value at the corresponding position. This confidence degree is a combination of the probability that an error occurred during sequencing phred based error probability) for that base call and of the confidence of the read mapping to that genome position. The usage of such weights has been experimentally validated as a powerful way to improve accuracy (). Haplotype assembly benefits from technological developments in genome sequencing. In fact, the advent of next generation sequencing (NGS) technologies provided a cost effective way of assembling the genome of diploid organisms. However, to assemble accurate haplotypes, it is necessary to have reads that are long enough to span several different heterozygous positions (). This kind of data is becoming increasingly available with the advent of future generation sequencing technologies such as single molecule real time technologies like pac bio RS II (http:// www pacific biosciences com products and Oxford nano pore flow cell technologies like MinION (https://www.nanoporetech.com/). These technologies, thanks to their ability of producing single end reads longer than 10 000 bases, eliminate the need of paired end data and have already been used for tasks like genome finishing and haplotype assembly (). Besides read length, the future generation sequencing technologies produce fragments with novel features, such as the uniform distribution of sequencing errors, that are not properly addressed (or exploited) in most of the existing methods that, instead, are tailored to the characteristics of traditional NGS technologies. Recently, MEC and w mec approaches have been used in the context of long reads, confirming that long fragments allow to assemble haplotypes more accurately than traditional short reads (). Since MEC is np hard (), exact solutions have exponential complexity. Different approaches tackling the computational hardness of the problem have been proposed in literature. Integer linear programming techniques have been recently used (), but the approach failed to optimally solve some 'difficult blocks'. There were also proposed fixed parameter tractable (FPT) algorithms that take time exponential in the number of variants per read () and, hence, are well suited for short reads but become unfeasible for long reads. For this kind of data, heuristic approaches have been proposed to respond to the lack of exact solutions (). Most of the proposed heuristics, such as ref hap (), make use of the traditional all heterozygous assumption, that forces the heterozygosity of all the phased positions. These heuristics have good performances but do not offer guarantees on the optimality of the returned solution (). Two recent articles () aim at processing future generation long reads by introducing algorithms exponential in the sequencing coverage, a parameter which is not expected to grow as fast as read length with the advent of future generation technologies. The first algorithm, called prob hap (), is a probabilistic dynamic programming algorithm that optimizes a likelihood function generalizing the objective function of MEC. Albeit prob hap is significantly slower than the previous heuristics, it obtained a noticeable improvement in accuracy. The second approach, called what shap (), is the first exact algorithm for w mec that is able to process long reads. It was shown to be able to obtain a good accuracy on simulated data of long reads at coverages up to 20 and to outperforms all the previous exact approaches. However, it can not handle coverages higher than 20, and its performance evidently decreases when approaching that limit. In this article, we exploit a characteristic of future generation technologies, namely the uniform distribution of sequencing errors, for introducing (Section 2) an exact FPT algorithm for a new variant, called k cmec of the w mec problem where the parameters are (i) the maximum number k of corrections that are allowed on each SNP position and (ii) the coverage. The new algorithm, called hap col is based on a characterization of feasible solutions given in and its time complexity is o cov k1 Lm (albeit it is possible to prove a stricter bound), where cov is the maximum coverage, L is the read length and m is the number of SNP positions. hap col is able to work without the all heterozygous assumption. In Section 3, we experimentally compare accuracy and performance of hap col on real and realistically simulated datasets with three state of the art approaches for haplotype assembly ref hap prob hap and what shap. On a real standard benchmark of long reads (), we executed each tool under the all heterozygous assumption, since this dataset has low coverage ($3 on average) and since the covered positions are heterozygous with high confidence. hap col turns out to be competitive with the considered methods, improving the accuracy and the number of phased positions. We also assessed accuracy and performance of hap col on a large collection of realistically simulated datasets reflecting the characteristics of future generation sequencing technologies that are currently (or soon) available (coverage up to 25, read length from 10000 to 50 000 bases, substitution error rate up to 5% and in del rate equal to 10%) (). When considering higher coverages, interesting applications such as SNP calling or heterozygous SNPs validation become feasible and reliable (). Since these applications require that haplotypes are reconstructed without the all heterozygous assumption, on the simulated datasets we only considered the tools that do not rely on this assumption what shap and hap col. Results on the simulated datasets with coverage 1520 show that hap col while being as accurate as what shap (they achieve an average error of $2%), is faster and significantly more memory efficient ($2 times faster and $28 times less memory). The efficiency of hap col allows to further improve accuracy. Indeed, the experimental results show that hap col is able to process datasets with coverage 25 on standard workstations/ small servers (whereas what shap exhausted all the available memory, 256 GB) and that, since the number of ambiguous uncalled positions decreases, the haplotypes reconstructed by hap col at coverage 25 are $9% more accurate than those reconstructed at coverage 20.
