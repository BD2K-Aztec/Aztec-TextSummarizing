Gene prioritization refers to a family of computational techniques for inferring disease genes through a set of training genes and carefully chosen similarity criteria. Test genes are scored based on their average similarity to the training set, and the rankings of genes under various similarity criteria are aggregated via statistical methods. The contributions of our work are threefold: (i) first, based on the realization that there is no unique way to define an optimal aggregate for rank-ings, we investigate the predictive quality of a number of new aggregation methods and known fusion techniques from machine learning and social choice theory. Within this context, we quantify the influence of the number of training genes and similarity criteria on the diagnostic quality of the aggregate and perform in depth cross validation studies; (ii) second, we propose a new approach to genomic data aggregation, termed HyDRA (Hybrid distance score Rank Aggregation), which combines the advantages of score based and combinatorial aggregation techniques. We also propose incorporating a new top versus bottom (TvB) weighting feature into the hybrid schemes. The TvB feature ensures that aggregates are more reliable at the top of the list, rather than at the bottom, since only top candidates are tested experimentally; (iii) third, we propose an iterative procedure for gene discovery that operates via successful augmentation of the set of training genes by genes discovered in previous rounds, checked for consistency. Motivation: Fundamental results from social choice theory, political and computer sciences, and statistics have shown that there exists no consistent, fair and unique way to aggregate rankings. Instead, one has to decide on an aggregation approach using predefined set of desirable properties for the aggregate. The aggregation methods fall into two categories, score and distance based approaches, each of which has its own drawbacks and advantages. This work is motivated by the observation that merging these two techniques in a computationally efficient manner, and by incorporating additional constraints, one can ensure that the predictive quality of the resulting aggregation algorithm is very high. Results: We tested HyDRA on a number of gene sets, including autism, breast cancer, colorectal cancer, endometriosis, ischaemic stroke, leukemia, lymphoma and osteoarthritis. Furthermore, we performed iterative gene discovery for glioblastoma, meningioma and breast cancer, using a sequentially augmented list of training genes related to the turco t syndrome, li fraumeni condition and other diseases. The methods outperform state of the art software tools such as to pp gene and Endeavour. Despite this finding, we recommend as best practice to take the union of top ranked items produced by different methods for the final aggregated list. Availability and implementation: The HyDRA software may be downloaded from: http://web. engr illinois edumkim158hydrazip
introduction identification of genes that predispose an individual to a disease is a problem of great interest in medical sciences and systems biology (). The most accurate and powerful methods used for identification are experimental in nature, involving normal and disease samples (). Experiments are time consuming and costly, complicated by the fact that typically, multiple genes have to be jointly mutated to trigger the onset of a disease. Given the large number of human genes (!25 000), testing even relatively small subsets of pairs of candidate genes is prohibitively expensive (). To mitigate this issue, a set of predictive analytical and computational methods have been proposed under the collective name gene prioritization techniques. Gene prioritization refers to the complex procedure of ranking genes according to their likelihoods of being linked to a certain disease. The likelihood function is computed based on multiple sources of evidence, such as sequence similarity, linkage analysis, gene annotation, functionality and expression activity, gene product attributes all determined with respect to a set of training genes. A wide range of tools has been developed for identifying genes involved in a disease (), as surveyed (). Existing software includes techniques based on network information, such as guild if y () and gene mania (), data mining and machine learning based approaches as described in (), POCUS () SUSPECTS () and (), and methods using statistical analysis, including Endeavour (), to pp gene () and network prioritize r (). Here, we focus on statistical approaches coupled with new combinatorial algorithms for gene prioritization, and emphasize one aspect of the prioritization procedure: rank aggregation. The problem of aggregating rankings of distinct objects or entities provided by a number of experts, voters or search engines has a rich history (). One of the key findings is that various voting paradoxes arise when more than three candidates are to be ranked: it is frequently possible not to have a candidate that wins all pairwise competitions (the Condorcet paradox) and it is theoretically impossible to guarantee the existence of an aggregate solution that meets certain predefined set of criteria [such as those imposed by Arrow's impossibility theorem (. These issues carry over to aggregation methods used for gene discovery, and as a result, the rank ordered lists of genes heavily depend on the particular aggregation method used. Two families of methods have found wide applications in rank aggregation: combinatorial methods (including score and distance based approaches) () and statistical methods. In the bioinformatics literature, the aggregation methods of choice are statistical in nature, relying on pre-specified hypotheses to evaluate the distribution of the gene rankings. One of the earliest prioritization softwares, Endeavour, uses the q statistics for multiple significance testing, and measures the minimum false discovery rate at which a test may be called significant. In particular, rankings based on different similarity criteria are combined via order statistics approaches. For this purpose, one uses the rank ratio (normalized ranking) of a gene g for m different criteria, r 1 g;.. .; r m g and recursively computes the q value defined as post processed q values are used to create the resulting ranking of genes. The drawbacks of the method are that it is based on a null hypothesis that is difficult to verify in practice, and that it is computationally expensive, as it involves evaluating an m fold integral. To enable efficient scaling of the method, Endeavour resorts to approximating the q integral. The influence of the approximation errors on the final ranking is hard to assess, as small changes in scores may result in significant changes of the aggregate orderings. Likewise, to pp gene uses a well known statistical approach, called the Fisher v 2 method. It first determines the p values of similarity score indexed by j, denoted by p(j), for j  1;.. .; m. The p values are computed through multiple preprocessing stages, involving estimation of the information contents (i.e. weights) of annotation terms, setting up a similarity criteria based on Sugeno fuzzy measures (i.e. non additive measures) (), and performing meta testing. The use of fuzzy measures ensures that all similarities are non-negative. Then, under the hypothesis of independent tests, to pp gene uses Fisher's inverse v 2 result, stating that 2 X m j1 log pj ! v 2 2m. Here, v 2 (2m) stands for the chisquare distribution with 2 m degrees of freedom. The result is asymptotic in nature, and based on possibly impractical independence assumptions. A number of methods, and additive scoring methods in particular, have the drawback that they tacitly or implicitly rely on the assumption that (i) only the total score matters, and the balance between the number of criteria that highly ranked the gene and those that ranked it very low is irrelevant. For example, outlier rankings may reduce the overall ranking of a gene to the point that it is not considered a disease gene candidate, while the outlier itself may be a problematic criterion. To illustrate this observation, consider a gene that was ranked 1st, 2nd, 1st, 20th by four criteria. At the same time, consider another gene that was ranked 6th by all four criteria. It may be unclear which of these two genes is more likely to be involved in the disease, given that additive score methods would rank the two genes equally (as one has (1  2  1  20)/4  6). Nevertheless, it appears reasonable to assume that the first candidate is a more reliable choice for a disease gene, as it had a very high ranking for three out of four criteria; and (ii) no distinction is made about the accuracy of ranking genes in any part of the list; i.e. the aggregate ranking has to be uniformly accurate at the top, middle and bottom of the list. Clearly, neither of the two aforementioned assumptions is justified in the gene prioritization process: there are many instances where genes similar only under a few criteria (such as sequence similarity or linkage distance) are involved in the same disease pathway. Furthermore, as the goal of prioritization is to produce a list of genes to be experimentally tested, only the highest ranked candidate genes are important and should have higher accuracy than other genes in the list. In addition, most known aggregation methods are highly sensitive to outliers and ranking errors. We propose a new approach to gene prioritization by introducing a number of novel aggregation paradigms, which we collectively Gene prioritization via hydra refer to as HyDRA (Hybrid distance score Rank Aggregation). The gist of HyDRA is to combine combinatorial approaches that have universal axiomatic underpinnings with statistical evidence pertaining to the accuracy of individual rankings. Our preferred distance measure for combinatorial aggregation is the Kendall distance (), which counts the number of pairwise disagreement between two rankings, and was axiomatically postulated by Kemeny (1959). The Kendall distance is closely related to the Kendall rank correlation coefficient (). As such, it has many properties useful for gene prioritization, such as monotonicity, reinforcement and Pareto efficiency (). The Kendall distance can be generalized to take into account positional relevance of items, as was done in our companion article (). There, it was shown that by assigning weights to pairs of positions in rankings, it is possible to (i) eliminate negative outliers from the aggregation process, (ii) include quantitative data into the aggregate and (iii) ensure higher accuracy at the top of the ranking than at the bottom. The contributions of this work are threefold. First, we introduce new weighted distance measures, where we compute the weights based on statistical evidence of a function of the difference between p values of adjacent ly ranked items. Aggregation weights based on statistical evidence improve the accuracy of the combinatorial aggregation procedure and make them more robust to estimation errors. Second, we describe how to scale the weights obtained based on statistical evidence by a decreasing sequence of TvB (Top versus Bottom) multipliers that ensure even higher accuracy at the top of the aggregated list. As aggregation under the Kendall metric is np hard (Non-deterministic polynomial time hard) (), and the same is true of the weighted Kendall metric, we propose a 2 approximation method that is stable under small perturbations. Aggregation is accomplished via weighted bipartite matching, such as the Hungarian algorithm and derivatives thereof (). Third, we test HyDRA within two operational scenarios: cross validation and disease gene discovery. In the former case, we assess the performance of different hybrid methods with respect to the choice of the weighting function and different number of test and training genes. In the latter case, we adapt aggregation methods to gene discovery via a new iterative re ranking procedure.

discussion we start by discussing the results in. The first observation is that the Lov sz bregman method performs worse than any other aggregation method. This finding may be attributed to the fact that the p values have a large span, and small values may be 'masked' by larger ones. Scaling all p values may be a means to improve the performance of this technique, but how exactly to accomplish this task remains a question. In almost all cases, except for Leukemia and Lymphoma, the average rankings produced by to pp gene and the Weighted Kendall distance appear to be almost identical. But average values may be misleading, as individual rankings of genes may vary substantially between the methods, as can be seen from the supplementary material. It is due to this reason that we recommend merging lists
