Motivation: The biomarker discovery process in high throughput genomic profiles has presented the statistical learning community with a challenging problem, namely learning when the number of variables is comparable or exceeding the sample size. In these settings, many classical techniques including linear discriminant analysis (LDA) falter. Poor performance of LDA is attributed to the ill conditioned nature of sample covariance matrix when the dimension and sample size are comparable. To alleviate this problem, regularized LDA rld a has been classically proposed in which the sample covariance matrix is replaced by its ridge estimate. However, the performance of rld a depends heavily on the regularization parameter used in the ridge estimate of sample covari-ance matrix. Results: We propose a range search technique for efficient estimation of the optimum regulariza-tion parameter. Using an extensive set of simulations based on synthetic and gene expression microarray data, we demonstrate the robustness of the proposed technique to Gaussianity, an assumption used in developing the core estimator. We compare the performance of the technique in terms of accuracy and efficiency with classical techniques for estimating the regularization parameter. In terms of accuracy, the results indicate that the proposed method vastly improves on similar techniques that use classical plug-in estimator. In that respect, it is better or comparable to cross validation based search strategies while, depending on the sample size and dimensionality, being tens to hundreds of times faster to compute. Availability and Implementation: The source code is available at https://github.com/

introduction ridge estimation is a type of shrinkage and traces back to the pioneering work of b on estimating regression parameters. They considered the standard linear model y  Xb  e ;where y is the n dimensional observation vector, X is a known n  p matrix, b  b 1 ; b 2 ;. .. ; b p  T is a p dimensional parameter vector to be estimated, and e is the n dimensional error vector with mean 0 and covariance matrix r 2 I p. If we assume X is a full (column) rank matrix (p  n), the ordinary least square solution to this familiar linear model is given by b b  X T X 1 X T y however when p  n, the solution (2) does not exist because X T X becomes degenerate. Even the solution obtained by generalized inverse form of matrix X T X is not working well b then formulated a problem in which the residual sum of squares is replaced by its ' 2 penalized form given by L 2 bjj y  x bjj 2  kjj bjj 2 ;where k  0 denotes a penalty factor controlling the length of b. Minimizing L 2 b results in the so called ridge regression given by b b  X T X  kI p  1 X T y:In this way, the inverse of possibly ill conditioned X T X is stabilized by adding the scalar matrix kI p. This idea was then used by Di Pillo (1976) to replace the estimate of the sample covariance matrix used in linear discriminant analysis (LDA) by its ridge estimate resulting in the so called regularized LDA rld a. The goal is to improve the performance of LDA in situations where dimensionality of observations, p, is larger or comparable to the number of measurements, n. Di Pillo (1979) attempts to determine the optimum value of the optimum regularization parameter in rld a. On this Di pill os study, Peck and Ness (1982) comment that 'He found the analytical solution to this problem intractable, and so used a simulation study to choose an optimum value for k [the regularization parameter]. He concluded that if an algorithm can be found which leads to a value of k near the optimum value, then considerable improvement in the PCC [probability of correct classification] should occur suggested the use of cross validation in finding the optimum value of regularization parameter. In this procedure, cross validation is used to estimate the true error of rld a for each value of the regularization parameter selected from a pre-specified set of size 2550. The estimate of the optimum regularization parameter is then the one that results in the minimum cross validation estimate of true error. Despite the computational complexity of cross validation in such a search algorithm [e.g. see comments in and tas jud in and, this approach has remained the most popular method in estimating the optimum value of regularization parameter in r lda for instance, see and Ye and Xiong (2006) to cite just a few articles. Recently, we constructed a generalized consistent estimator of true error of rld a. In this regard, we proposed an estimator that converges to true error in a double asymptotic sense. In this setting, the estimator converges to the actual parameter in an asymptotic scenario in which dimension and sample size increase in a proportional manner (n ! 1; p ! 1 and p=n ! J  0) (). In developing this estimator, we assumed that the true distributions governing the data follow multivariate Gaussian model. However, the underlying mechanism to develop the estimator was based on double asymptotics and random matrix theory, both of which suggest applicability of the estimator in non gaussian settings as well [see p. xii in, p. 335 in Bai and Silverstein (2010) and. In this work, we employ this estimator of true error in a one dimensional search to estimate the optimum regularization parameter of rld a. As such, we employ data taken from seven gene expression microarray studies as well as synthetically generated Gaussian and non gaussian data. We compare the performance (in terms of accuracy and efficiency) of the search technique that uses this estimator with similar search schemes that use cross validation or plug-in estimators. Using an extensive set of simulations, we observe that the proposed technique is an efficient method that can outperform cross validation and plugin estimate based schemes in estimating the optimum regularization parameter of rld a. Throughout this work, we use boldface lower case letters to denote a column vector. A boldface upper case letter denotes a matrix and tr: is the trace operator. The identity matrix of p dimension is denoted by I p .
