Modern sequencing instruments have the capability to produce millions of short reads every day. The large number of reads produced in conjunction with variations between reads and reference genomic sequences caused both by legitimate differences, such as single nucleotide polymorphisms and insertions deletions (indels), and by sequencer errors make alignment a difficult and computationally expensive task, and many reads can not be aligned. Here, we introduce a new alignment tool, sr mapper which in tests using real data can align 10s of billions of base pairs from short reads to the human genome per computer processor day. sr mapper tolerates a higher number of mismatches than current programs based on Burrows– Wheeler transform and finds about the same number of alignments in 2–8Â less time depending on read length (with higher performance gain for longer read length). The current version of sr mapper aligns both single and pair end reads in base space fast q format and outputs alignments in Sequence alignment map format. sr mapper uses a probabilistic approach to set a default number of mismatches allowed and determines alignment quality. sr mappers memory footprint ($2.5 GB) is small enough that it can be run on a computer with 4 GB of random access memory for a genome the size of a human. Finally, sr mapper is designed so that its function can be extended to finding small indels as well as long deletions and chromosomal trans-locations in future versions.

introduction with the advent of next generation sequencing (NGS) instruments, the amount of raw genetic sequence information has exponentially increased during the past few years, and it is expected to continue to grow at a high rate as sequencing cost continue to decrease. Instruments such as the HiSeq2000 (Illumina), GS FLX titanium (Roche) and SOLiD 4 (ABI) can generate billions of base pairs giga bases or Gb) of data per day with increasingly high accuracies, 498.5% for Illumina and 499.5% for Roche and ABI, respectively, and with costs that have decreased to $$10 000 per human genome (). Newer instruments, such as the Ion Proton (Life Technologies), can produce even higher amounts of data and are approaching the goal of a $1000 genome (). With the speed and cost factors making whole genome sequencing practical, researchers are sequencing and analysing large numbers of genomes in hopes of finding genetic origins of many diseases, such as cancers. For example, one recent study sequenced 457 human genomes searching for rare mutations involved in ovarian cancer (). With the dramatically increased amount of raw data, analysis has become more challenging. This is because of two factors: the sheer amount of data gathered and the relatively short lengths of reads produced by current NGS instruments (). For example, in the study aforementioned, 412 tera bases of sequence data would be gathered for a 10 coverage of the 457 genomes. The short length of the reads, typically between 30 and 100 bp for Illumina and ABI and $400 bp for Roche, complicates the building of a genome de novo (). For example, a genome may contain repetitive regions. It is difficult to reconstruct these regions and their flanking sequences if the length of the reads is much shorter than that of the repeating units. NGS instruments can now perform pair end sequencing, in which the sequence of the two ends of longer fragments are determined, which has helped to resolve these problems (). Nevertheless, de novo assembly remains challenging and is memory intensive and, therefore, difficult to perform on genomes larger than those of bacteria (). A popular alternative to de novo assembly is reference assembly, in which reads are aligned against a pre-existing reference genome. There are three main classes of alignment tools: read hashing tools, reference hashing tools and Burrows Wheeler transform (BWT) tools. Examples of reference hashing tools include b fast (), shrimp 2 () and WHAM (). Tools that use the BWT include burrows wheeler Aligner (BWA) (), bowtie () and SOAP2 (). Most genome hashing algorithms require large memory that they must be run on expensive large memory machines. On the other hand, many BWT methods carry a small memory footprint that they can be run on computers with 4 GB of random access memory (RAM), accessible by many users possessing only desktop machines. Among the genome hashing methods listed previously, only b fast can be run on a computer with 4 GB of RAM. For methods based on BWT, both BWA and bowtie can be run on computers with 4 GB of RAM. Among these three, bowtie is the most restrictive in terms of allowing variants between the reference and short read, allowing a maximum of three mismatches and no insertions or deletions *To whom correspondence should be addressed in dels making it a less attractive option for sequencing longer reads, which would be expected to have a higher number of mismatches and errors. BWA and b fast both allow indels and mismatches. Among the three, bowtie is slightly faster than BWA, and both are significantly faster than b fast. However, b fast has been shown to be more sensitive than the bwt based methods for most datasets evaluated (in). Here, we introduce a new genome hashing alignment tool, sr mapper. The original design goal for sr mapper was to build an alignment tool that was not restrictively slow, had as high sensitivity as other widely used alignment tools and was capable of finding long deletions and other more complicated genomic alternations, such as chromosomal translocations from short read sequences. The current version of sr mapper already achieves the first two goals and uses a novel approach to determine the initial number of mismatches allowed and calculates alignment scores. In evaluations on real data, sr mapper is 28 faster than BWA on datasets of length !51 bp while aligning comparable number of reads as BWA. For short reads, 32 bp, sr mapper was 640 faster than BWA, but somewhat less sensitive. This article explains the approach taken by sr mapper compares its performance against the popular BWA package for multiple datasets of different read length, describes how we envision this first version of sr mapper being used and discusses future extensions and improvements.
