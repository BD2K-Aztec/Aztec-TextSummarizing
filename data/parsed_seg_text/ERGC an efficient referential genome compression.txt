Motivation: Genome sequencing has become faster and more affordable. Consequently, the number of available complete genomic sequences is increasing rapidly. As a result, the cost to store, process, analyze and transmit the data is becoming a bottleneck for research and future medical applications. So, the need for devising efficient data compression and data reduction techniques for biological sequencing data is growing by the day. Although there exists a number of standard data compression algorithms, they are not efficient in compressing biological data. These generic algorithms do not exploit some inherent properties of the sequencing data while compressing. To exploit statistical and information theoretic properties of genomic sequences, we need specialized compression algorithms. Five different next generation sequencing data compression problems have been identified and studied in the literature. We propose a novel algorithm for one of these problems known as reference based genome compression. Results: We have done extensive experiments using five real sequencing datasets. The results on real genomes show that our proposed algorithm is indeed competitive and performs better than the best known algorithms for this problem. It achieves compression ratios that are better than those of the currently best performing algorithms. The time to compress and decompress the whole genome is also very promising. Availability and implementation: The implementations are freely available for non-commercial purposes. They can be downloaded from

introduction next generation sequencing (NGS) technologies are producing millions to billions of short reads from DNA molecules simultaneously in a single run within a very short time period, leading to a sharp decline in whole genome sequencing costs. As a result, we are observing an explosion of genomic data from various species. Storing these data is an important task that the biologists have to perform on a daily basis. To save space, compression could play an important role. Also, when the size of the data transmitted through the Internet increases, the transmission cost and congestion in the network also increase proportionally. Here again compression could help. Although we can compress the sequencing data through standard general purpose algorithms, these algorithms may not compress the biological sequences effectively, since they do not exploit inherent properties of the biological data. Genomic sequences often contain repetitive elements, e.g. microsatellite sequences. The input sequences might exhibit high levels of similarity. An example will be multiple genome sequences from the same species. Additionally, the statistical and information theoretic properties of genomic sequences can potentially be exploited. General purpose algorithms do not exploit these properties. In this article, we offer a novel algorithm to compress genomic sequences effectively and efficiently. Our algorithm achieves compression ratios that are better than the currently best performing algorithms in this domain. By compression ratio, we mean the ratio of the uncompressed data size to the compressed data size the following five versions of the compression problem have been identified in the literature: (i) genome compression with a reference: here we are given many (hopefully very similar) genomic sequences. The goal is to compress all the sequences using one of them as the reference. The idea is to utilize the fact that the sequences are very similar. For every sequence other than the reference, we only have to store the difference between the reference and the sequence itself; (ii) reference free genome compression: this is the same as Problem 1, except that there is no reference sequence. Each sequence has to be compressed independently; (iii) reference free reads compression: it deals with compressing biological reads where there is no clear choice for a reference; (iv) reference based reads compression: in this technique, complete reads data need not be stored but only the variations with respect to a reference genome are stored; and (v) metadata and quality scores compression: in this problem, we are required to compress quality sequences associated with the reads and metadata such as read name, platform and project identifiers. In this article, we focus on Problem 1. We present an algorithm called erg c (Efficient Referential Genome Compressor) based on a reference genome. It employs a divide and conquer strategy. At first it divides both the target and reference sequences into some parts of equal size and finds one to one maps of similar regions from each part. It then outputs identical maps along with dissimilar regions of the target sequence. The rest of this article is organized as follows: Section 2 has a literature survey. Section 3 describes the proposed algorithm and analyses its time complexity. Our experimental platform is explained in Section 4. This section also contains the experimental results and discussions. Section 5 concludes the article.

discussion next we present details on the performance evaluation of our proposed algorithm erg c with respect to both compression and running time. We have compared erg c with two of the three best performing algorithms namely GDC and i do comp using several standard benchmark datasets. GReEn is another state of the art algorithm existing in the literature. But we could not compare it with our algorithm, as the site containing the code was down at the time of experiments. GDC, GReEn and i do comp are highly specialized algorithms designed to compress genomic sequences with the help of a reference genome. These are the best performing algorithms in this area as of now. Given a reference sequence, our algorithm compresses the target sequence by exploiting the reference. So, it needs the reference sequence at the time of decompression also. We use the target and reference pairs of sequences illustrated in sion ratios. But it may not be possible to find variation files for every species and these algorithms will not work without variation files. Our algorithm does not employ variation files and so it can compress any genomic sequence given a reference. As a result, we feel that algorithms that employ variation files form a separate class of algorithms and are not comparable to our algorithm. Again our proposed algorithm is devised in such a way that it is able to work with any alphabet used in the genomic sequence. Every other algorithm works only with valid alphabets intended for genomic sequence e.g. P fA; a; C; c; G; g; T; t; N; ng. The characters most commonly seen in sequences are in P but there are several other valid characters that are used in clones to indicate ambiguity about the identity of certain bases in the sequence. It is not uncommon to see these 'wobble' codes at polymorphic positions in DNA sequences. It also differentiates between lower-case and upper-case letters. GDC, GReEn and i do comp can differentiate between upper-case and lower-case letters specified in P but previous algorithms like GRS or rlz opt only work with A, C, G, T and N in the alphabet. i do comp replaces the character in the genomic sequence that does not belong to P with n specifically erg c will compress the target genome file regardless of the alphabets used and decompress the compressed file which is exactly identical to the target file. This is the case for GDC and i do comp also but GReEn does not include the metadata information and output the sequence as a single line instead of multiple lines. Effectiveness of various algorithms including erg c is measured using several performance metrics such as compression size, compression time, decompression time, etc. Gain measures the percentage improvement over the compression achieved by erg c with respect to GDC and i do comp. Comparison results are shown in. Clearly, our proposed algorithm is competitive and performs better than all the best known algorithms. In Tables 3 and 4, we show a comparison between compressed size (from different algorithms) and the actual size of individual chromosomes for some datasets. Memory consumption is also very low in our algorithm as it only processes one and only one part from the target and reference sequences at any time. Please note that we did not report the performance evaluation of GDC for every dataset, as it ran at least 1 h but did not able to compress a single chromosome for some datasets. As stated above, erg c differentiates upper-case and lowercase characters. It compresses target file containing the genomic sequence to be compressed and metadata if any with the help of a reference. The decompression procedure produces exactly the same file as the input. It does not depend on the alphabets and is universal in this sense. Consider dataset D 1 where the target and reference sequences chromosomes are from YH and hg18, respectively (). In this setting, GDC runs indefinitely. i do comp for details). Now consider dataset D 2 where the target and reference sequences are from YH and KO224, respectively. The compressions achieved by GDC and i do comp are roughly equal, whereas erg c is about 3 better than them. g dcs compression time is longer than both of i do comp and erg c but it decompresses the sequences very quickly. er gcs compression is approximately 2:5 and 7:5 faster than i do comp and GDC, respectively. Next consider D 5. GDC runs indefinitely for this dataset. The percentage improvement erg c achieves with respect to i do comp is 90.73%. Specifically, erg c takes 11  fewer disk space compared to i do comp for this particular dataset. erg c is also faster than i do comp in terms of both compression (2) and decompression (6) times shows a comparative study of different algorithms including erg c with respect to compression ratio, compression and decompression time. In brief, the minimum and maximum improvements observed from datasets D 1  D 5 were 27.97% and 90.73% with respect to i do comp respectively. The minimum and maximum improvements over GDC observed were 57.9% and 75.24%, respectively. erg c compresses at least 2:12 and at most 5:21 faster than i do comp. Although it is better than i do comp and GDC in compression time for every dataset, it is slower than GDC with respect to decompression for datasets D 2  D 4 .
