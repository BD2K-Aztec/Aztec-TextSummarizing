Motivation: We address a common problem in large scale data analysis, and especially the field of genetics, the huge scale testing problem, where millions to billions of hypotheses are tested together creating a computational challenge to control the inflation of the false discovery rate. As a solution we propose an alternative algorithm for the famous Linear Step Up procedure of benjamin i and Hochberg. Results: Our algorithm requires linear time and does not require any p value ordering. It permits separating huge scale testing problems arbitrarily into computationally feasible sets or chunks. Results from the chunks are combined by our algorithm to produce the same results as the controlling procedure on the entire set of tests, thus controlling the global false discovery rate even when p values are arbitrarily divided. The practical memory usage may also be determined arbitrarily by the size of available memory. Availability and implementation: R code is provided in the supplementary material.

introduction in many fields the substantially increased scale of data available has resulted in a significant increase in the size of multiple hypotheses testing problems. In genetics, in particular, typical g was studies consist of 10 5  10 6 SNPs () while e qtl studies (), newly advanced methylation studies (), and imaging studies () usually start with 10 9 tests. These testing problems are huge scale as opposed to large scale used by to describe studies consisting of hundreds to thousands of hypotheses. It is preferable to control the false discovery proportion rather than the number of false positives for a huge scale testing problem. Therefore the FDR or the pf dr approaches are favored and both tend to offer larger, more powerful sets of results than those yielded by the conservative f wer control. These huge scale multiple hypotheses testing problems create numerous computational challenges when many tests, say of the order 10 6 , are performed with all of the p values of more or less equal importance. As a result some simpler testing procedures such as rigid p value thresholds may be used that sacrifice power and correctness. Alternatively tests may be separated or chunked into smaller sets or chunks that are more computationally feasible notes that the problem of separating hypotheses tests has not received great attention and warns of some pitfalls in chunking p values but focuses on grouping tests that share a biological property rather than arbitrary, computationally feasible chunks. Cai and Sun (2009) and later () propose alternative solutions to efron s grouping problem but do not address the problem of arbitrary, computationally feasible chunking. We confront the computationally feasible chunking problem for the benjamin i hochberg false discovery rate (). We show on data from Stranger's HapMap study () that if results from separate tests are not combined correctly, there is considerable inflation of type I error, offer an explication for this occurrence, and propose our algorithm as a solution. Consider a huge scale testing problem of size m where our goal is to select exactly R ! 0 significant tests. Of the R significant discoveries, exactly V ! 0 tests will be false discoveries (i.e. truly nonsignificant tests that are declared significant). A common approach in multiple hypotheses testing problems is to control the family wise error rate, f wer  PrV ! 1, the probability of selecting at least one false discovery. For huge scale testing a more favorable alternative is to control the false discovery proportion, FDP  vmax r 1, the proportion of truly false tests among the significant R. Some will prefer to control the positive FDR, pf dr  efd pjr  0, the expectation of the FDP when significant tests are selected, while others will opt to control the false discovery rate, FDR, the expectation of the FDP, e fdp. The FDR is always of a potentially smaller magnitude than the f wer and of the pf dr (FDR  pf dr  PrR  0). Yet, in reality for huge scale testing, f wer   FDR and sometimes, pf dr % FDR. Therefore both FDR and pf dr control approaches tend to offer larger, more powerful sets of results than those that might be offered by the conservative f wer control. For a further discussion about f wer FDR and pf dr refer to far come ni (2004). In huge scale testing when the m p values are partitioned into chunks, it is challenging to control the f wer pf dr or FDR over the entire collection of m p values. Controlling these error rates on a per chunk basis, if not done correctly, may interfere with the overall results by introducing more false discoveries. Although the same difficulty arises for the FDR and pf dr control (), it is easier to illustrate this for the f wer. Consider, for instance, an example of f wer control using the Bonferroni approach by collecting all p values less than a=m. Assuming that the number m of p values happen to be very large so that m should be divided into k chunks, each of of size m i so that m  P k i1 m i. Applying Bonferroni in chunks of size m i will tend to select more significant results than applying it over the entire set of m  P m i p values since a=m is less than a=m i. In the case of f wer control, using a fixed bound of a=m for all the chunks is theoretically preferred but often yields no significant results. A stricter constant cut off on all sets of tests as suggested by dud bridge and gus nan to (2008) for g was was developed based on results from simulated g was. However such an ad hoc approach eliminates from the entire multiple hypotheses testing problem any knowledge of the actual significance level a used.
