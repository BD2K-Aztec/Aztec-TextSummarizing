Motivation: The advent of new sequencing technologies has led to increasing amounts of data being available to perform phylogenetic analyses, with genomic data giving rise to the field of phylo genomics. high performance computing is becoming an indispensable research tool to fit complex evolutionary models, which take into account specific genomic properties, to large datasets. Here, we perform an extensive Bayesian phylogenetic model selection study, comparing codon and nucleotide substitution models, including codon position partitioning for nucleotide data as well gene specific substitution models for both data types. For the best fitting partitioned models, we also compare independent partitioning with standard diffuse prior specification to conditional partitioning via hierarchical prior specification. To compare the different models, we use state of the art marginal likelihood estimation techniques, including path sampling and stepping stone sampling. Results: We show that a full codon model best describes the features of a whole mitochondrial genome dataset, consisting of 12 protein coding genes, but only when each gene is allowed to evolve under a separate codon model. However, when using hierarchical prior specification for the partition specific parameters instead of independent diffuse priors, codon position partitioned nucleotide models can still outperform standard codon models. We demonstrate the feasibility of fitting such a combination of complex models using the BEAGLE library for BEAST in combination with recent graphics cards. We argue that development and use of such models needs to be accompanied by state of the art marginal likelihood estimators because the more traditional and computationally less demanding estimators do not offer adequate accuracy. Contact:

introduction the startling advances in sequencing technology have led to a dramatic increase in the scale and ambition of phylogenetic analyses. As more and more complete genomes are sequenced, phylogenetics is entering a new era that of phylo genomics which uses phylogenetic principles to extract information from genomic data (). Until recently, molecular phylogenies based on a single or few orthologous genes often yielded contradictory results (). One branch of the expanding field of phylo genomics aims to reconstruct the evolutionary history of organisms on the basis of their genomes, as opposed to performing single gene studies, which have long dominated the field (). By expanding the number of characters that can be used in phylogenetic reconstruction from a few thousand to tens of thousands, access to genomic data could potentially alleviate sampling problems that hampered previous phylogenetic analyses. phylo genomic analyses involve estimating the underlying evolutionary history of sequences either as an intermediate goal or as an end point. Statistical phylogenetics provides a framework for estimating historical patterns, inferring intrinsic parameters of evolutionary processes, and testing hypotheses under the auspices of the neutral theory of molecular evolution (). In contrast to the few genes that were previously available, the complete genomes of many species that are now accessible for inferring evolutionary relationships constitute large quantities of data that lead to reduced estimation errors associated with site sampling, to very high power in the rejection of simple evolutionary hypotheses and to high confidence in estimated phylogenetic patterns. Traditionally, phylogenetic analyses based on many genes combined data into a contiguous block, a practice that is still commonly used today. Under this concatenated model, all genes are not only assumed to evolve under the same tree but also to evolve at the same rate. Although multigene datasets have the advantage of providing greater resolution with more information it is likely to find trees that more accurately reflect evolutionary history it may prove challenging to account for the heterogeneous nature of the data. Different genes undergo different selective pressures, and the degree of site rate heterogeneity may vary from gene to gene (). Likelihood calculations on trees have been shown to clearly benefit from accommodating different evolutionary pressures (as observed in different codon positions, or different genes) in heterogeneous data (). Selection is a key evolutionary process in shaping genetic diversity and a major focus of phylo genomics investigations (). Using statistical methods in evolutionary genetics, researchers frequently evaluate the strength of selection operating on genes or even individual codons in the entire phylogeny or in a subset of branches. Codon substitution models have been particularly useful for this purpose because they allow estimating the ratio of non-synonymous and synonymous substitution rates (dN/dS) in a phylogenetic framework. Two different versions of codon substitution models were simultaneously introduced that both allowed estimating a single dN/dS ratio across all sites and branches (). Various extensions have since been proposed, such as codon models that model variation in dN/dS among sites complemented with empirical Bayes approaches to identify the sites under specific selection regimes (). Codon substitution models can to some extent be approximated by partitioning nucleotide models according to codon positions, which accommodates differences in evolutionary dynamics at the three codon positions. For example, Yang (1996a, b) takes into account the nucleotide frequency bias, the substitution rate bias and the difference in the extent of rate variation among the three codon positions and shows that incorporating these features can yield drastically different divergence time estimates compared with models not incorporating this complexity. Although full codon models approximate biological reality more closely, codon position partitioned nucleotide models have the advantage to be far more computationally efficient. While Yang (1996a, b) only used the HKY evolutionary model () in his analyses also included the GTR model tav are tav are 1986) in a comprehensive model evaluation study (see also showed that codon position partitioned nucleotide models are biologically motivated, computationally practical alternatives to codon models for the analysis of protein coding sequences. It is therefore not surprising that such approaches are also being exploited for detecting positively selected sites (). The large state space of full codon substitution models renders these models computationally expensive compared with standard nucleotide substitution models, which explains why they are frequently fit to trees to scrutinize selection processes but generally not used to reconstruct phylogenies. However, as computational power has increased, phylogenetic inference using codon based models is becoming more and more realistic. Here, we examine whether the use of full codon models is feasible in the phylo genomics era by fitting several codon models to a full genome mitochondrial dataset in a Bayesian framework. We use state of the art model selection approaches to assess whether increased biological realism, as obtained by modelling gene specific properties, goes hand in hand with increased model performance. Finally, we provide estimates of computation time required on both multi-core CPU systems and a system equipped with one of the latest graphics cards available.

discussion the Bayesian phylogenetic model comparison we present here consistently shows that partitioning by gene yields an increased model fit. Using standard diffuse priors, a separate codon model for each gene accompanied with gene specific among codon rate variation and gene specific relative substitution rates offers the best performance, followed by codon partition models and trailed by standard nucleotide models. However, when substituting the independent diffuse priors by a hierarchical prior specification over the gene specific parameters, a more parameter rich gtr based nucleotide substitution model partitioned according to gene and codon position emerges again as the best fitting model. These results can only be uncovered using recent model selection approaches, such as path sampling and stepping stone sampling. By demonstrating an increased model fit for gene partitioning, we corroborate the results of earlier studies, e.g. by, who combined morphology and nucleotide data from four genes in a study on model heterogeneity across data partitions. Through Bayes factor comparisons the authors showed a dramatic increase in model fit when extending two partition models (one partition for the morphology data and one joint partition for the four genes) to five partition equivalents (one partition for each of the four genes), emphasizing the importance of accommodating across partition heterogeneity. The authors also showed that within partition rate variation was by far the most important model component (i.e. much more than across partition heterogeneity), but that the difference in fit between substitution models was only pronounced when comparing JC () and GTR tav are tav are 1986). It is important to note that the Bayes factor comparisons reported in () are calculated using the HME (). Because convincing evidence has been presented for the poor performance of the HME in recent years (,b), we advocate for caution when interpreting such results and encourage the use of PS and SS (over HME and sh me. Consistent with the increased model fit for the gene partitioned models, we observed considerable variation in evolutionary parameters across the 12 mitochondrial genes. We examined whether this parameter variation observations could be associated with the asymmetrical mutation bias gradients in vertebrate mitochondrial genomes. Faith and Pollock (2003) and obtained the same relative gene order with respect to the duration of the single strand state of the parental H strand: cox15cox25atp85atp65cox3 5nd35 n d4l5nd45nd15nd55nd25cytb. When comparing the relative orders of , , ! and the relative rate in against this relative gene order, only ranking according to the relative rate results in a similar order, with the exception of ATP6 and the ND genes: COX15COX25ATP85COX35ND4L 5nd55nd15nd25nd35nd45atp 65cytb. Our comparison of independent diffuse priors and hierarchical priors for the gene partitioned models illustrates the impact of prior specification on marginal likelihood estimation and model selection. We have previously highlighted that the outcome of Bayesian model selection is dependent on prior choices (). Through hierarchical prior specification, HPMs offer a middle ground between the extreme scenarios of independently fitting different models across genes and fitting a single model to all genes (). While accommodating parameter variation among genes will be appropriate in most cases, there is less information available in each gene to inform the gene specific parameters. HPMs allow borrowing of strength of information from one partition by another, providing more precise gene specific parameter estimates, and resulting in further model improvements in our comparisons. BEAST supports a generic implementation of hierarchical prior specification and HPM approaches can therefore be applied to different problems, such as HIV within host evolution for different (groups of) patients () and phylo geographic problems (). By adopting hierarchical prior specification across gene specific parameters in GTR nucleotide substitution models with both gene and codon position partitions, we notice a better model fit compared with gene partitioning with a standard codon substitution models. However, we are essentially comparing the most complex parametrization among conventional nucleotide substitution models with the simplest codon substitution model, and many assumptions can still be relaxed to make codon models more realistic. To illustrate this point, we observe a marginal likelihood improvement of about 300 log units or more between the hky based and gtr based codon position partitioned nucleotide models. So, we also expect model fit improvements for a codon model that would consider different substitution rates for the different types of nucleotide substitutions within a codon, instead of merely distinguishing between transitions and transversions as is done in the GY94 model. Such a gtr type of model applied at the nucleotide level, but with the constraint that the nucleotide sequence must encode some full length amino acid sequence, is the rationale of the codon substitution models in the style of the codon model of Muse and Gaut (1994). The codon model of Muse and Gaut (1994) may offer a more realistic parameterization than the GY94 model, which has no natural mechanistic interpretation at the nucleotide level (), resulting in a possible increase in model fit over the GY94 model. More importantly, codon models can model varying selective pressure, but a gene specific dN/dS is a very coarse approximation of this variation and many realistic codon codons now accommodate among site variation in dN/dS (), in dN and dS separately kosak ov sky and or among lineage variation in dN/dS (). Further research is needed to implement such models in BEAST () and to assess their model fit. For each of the models tested in this manuscript, be it with or without gene partitioning, a relaxed molecular clock (with underlying lognormal distribution; ucl d is shown to outperform a strict clock, using all of the estimators. Given that mammalian datasets of mitochondrial DNA exhibit a wide variation in substitution rate across lineages, additional clock models, such as autocorrelated [see e.g.] or random local clocks () should ideally be included in our model comparison. For example have shown that the distribution of estimated mitochondrial substitution rates across species shows a very large variance, with the rates spanning two orders of magnitude. The authors also show that the family taxonomic level explains 75% of this variance, while the order taxonomic level explains 21%, indicating that entire orders could all have (for example) low substitution rates, which may be appropriately modeled using autocorrelated relaxed clock models (). Finally, we have reported massive increases in computation speed using the BEAGLE library for BEAST in combination with the latest graphics cards. However, the GTX 590 we used here is essentially designed to offer tremendous single precision performance, as required for visualization purposes in the gaming community. Hence, its double precision performance is not keeping the same development pace, which will also be the case for future cards from the same series. Double precision performance has recently increased with the advent of a new line of nVidia Tesla K20 graphics cards, designed for scientific computing. Further research will be needed to determine to what extent these new cards can improve efficiency in the field of phylogenetics.
