Motivation: Phylogenetic estimates from published studies can be archived using general platforms like Dryad (Vision, 2010) or tree base (Sanderson et al., 1994). Such services fulfill a crucial role in ensuring transparency and reproducibility in phylogenetic research. However, digital tree data files often require some editing (e.g. re rooting to improve the accuracy and reusability of the phylogenetic statements. Furthermore, establishing the mapping between tip labels used in a tree and tax a in a single common taxonomy dramatically improves the ability of other researchers to re-use phylogenetic estimates. As the process of curating a published phylogenetic estimate is not error free retaining a full record of the provenance of edits to a tree is crucial for openness, allowing editors to receive credit for their work and making errors introduced during curation easier to correct. Results: Here, we report the development of software infrastructure to support the open curation of phylogenetic data by the community of biologists. The backend of the system provides an interface for the standard database operations of creating, reading, updating and deleting records by making commits to a git repository. The record of the history of edits to a tree is preserved by g its version control features. Hosting this data store on git hub (http://github.com/) provides open access to the data store using tools familiar to many developers. We have deployed a server running the phy le system api which wraps the interactions with git and git hub. The Open Tree of Life project has also developed and deployed a JavaScript application that uses the phy le system api and other web services to enable input and curation of published phylogenetic statements. Availability and implementation: Source code for the web service layer is available at https:// git hub com open tree of life phy le system api. The data store can be cloned from: https://github. com open tree of life phy le system. A web application that uses the phy le system web services is deployed at http://tree.opentreeoflife.org/curator. Code for that tool is available from https://github. com open tree of life open tree. Contact:

introduction characterizing and systematizing relationships among species has been a goal of biologists since Linnaeus (1758). The phylogenetic systematics 'revolution' of the 1960s focused most of the effort toward this goal on the task of estimating phylogenetic relationships. The rapid growth in availability of molecular data, the development of models and software implementations for inferring phylogenies, and the appreciation of the explanatory power of phylogenetically aware comparative methods () have led to dramatic increases in the number of published phylogenetic hypotheses. Unfortunately, capturing the inputs and outputs of a phylogenetic analysis in a rich, standardized form is difficult and error prone. There is no single standard for phylogenetic data, and developers of phylogenetic software often create new formats or extend existing formats in ways that make them incompatible with other programs. Some archiving services, such as Dryad (), have reacted to this challenge by accepting a wide range of inputs and making few or no guarantees to users of the database that the records will be in any particular form. This encourages sharing of data by making the submission process fast and easy. However, if the authors submitting the data are not conscientious in their explanations of the data, it can be difficult for users of the data to reliably extract all the necessary information from the archive, or the data may not contain sufficient metadata for reuse. On the opposite end of the spectrum, some databases require data providers to conform to strict standards with respect to input format and content. Examples of this approach are tree base () and its successor tree base version 2 (). Since 1994, tree base has served as the primary archival data store for phylogenetic estimates and the data that these trees are based on. As of 2014, tree base contains 4076 studies and over 12 800 trees (http://treebase.org/treebase-web/home.html). Many journals encourage or require that each publication that estimates phylogenetic trees cite a tree base deposit containing the associated trees. tree base produces a much more feature rich interface for phylogenetic queries than Dryad, because each tree base submission is parsed thoroughly and converted to a set of records in a relational database. The downside of this approach is that it requires that submissions correspond to a uniform format. Without constant updating, that format may not reflect new types of phylogenetic data being generated. Unfortunately, tree estimation tools read and write file formats which are idiosyncratic and often so terse that they omit useful information about the analysis. see for discussions of this topic and other challenges relating to the archiving of phylogenetic estimates. As a result, the tree base submission process usually requires the authors of a data package to reformat their data, correct the rooting of the tree, alter the labels of the tips of the tree, etc. Because these steps are often taken after the analyses for a publication are complete, it is likely that errors introduced in the process of preparing a submission will not be corrected until an observant user tries to reuse the data. If there were a rich, universally used file format for phylogenetics analyses, then import constraints such as those used by tree base would be less of a hurdle, because the critical metadata regarding rooting, labels, etc. would be stored alongside the tree itself. The xml based ne xml specification () defines such a format, but it is not currently used widely by tree estimation tools. Thus, the current state of phylo informatic archival infrastructure is ripe for improvement in numerous ways. One such opportunity is the development of a data store that allows the community of biologists to improve the accuracy and reusability of phylogenetic statements associated with publications. We describe here an implementation of one such system, which we call phy le system because it uses a set of versioned text files on the filesystem. The phy le system component of the Open Tree of Life web infrastructure was built to fulfill a critical need for that project: in the process of producing a synthetic estimate of what we know about the phylogeny of life on Earth (), information from published studies had to be curated. The goal of phy le system is not to replace systems like Dryad or tree base but to complement them by storing phylogenetic statements and associated metadata in a consistent format while retaining the history of edits that were made to the data themselves. We anticipate that most of the trees stored in phy le system will be associated with permanent, static archives elsewhere (usually either tree base or Dryad).

discussion phylogenetic hypotheses are simultaneously the products of analyses and the raw material for future analyses. A phylogenetic data store has the advantage of collapsing potentially enormous raw sequence datasets into compact files capturing the inferred evolutionary history of studied tax a (). However, the raw output of phylogenetic analyses is rarely sufficient for immediate reuse by other researchers. This curation primarily consists of correcting the rooting of the tree (so that it reflects the finding of the published study), adding metadata about the data or analyses that produced the tree, and mapping the OTU labels to tax a in a taxonomy. Therefore, having a specialty data store to capture the appropriate metadata is essential. The phy le system tool was designed to fill this role in a way that would make the curated data as widely available as possible. By using git as a primary data store, the system allows other interested parties to easily maintain local clones of all the data. Data archives in bioinformatics have a wide range of goals and requirements. Using git in place of a typical database is feasible for only a small set of uses which do not require fast processing of a large number of requests. When it is feasible to use distributed version control as a data store, there are several benefits that make this approach appealing. Provenance information in the form of a commit message associated with an identification of the user creating the edit is stored 'for free' in such an architecture. Each modification to the data store is backed up efficiently using the v css 'push' functionality. The corpus of the data store can be made openly available in a form that is very convenient to other bioinformatic ians. Rather than having to unpack a new snapshot and write a script to identify what information has changed since the last snapshot was retrieved, a user can easily pull down the latest changes (with a 'git pull origin master' command in the case of a git based store). Not only will this update be fast, the user knows that he or she can back up to a previous version of the data store if needed using the standard version control features. This git based curated database model could be applied to other data stores requiring community curation. The file format of the data to be versioned also has an impact on whether or not it is feasible to use a VCS as a data store. The native tools for comparing versions of a file are line based. Ideally, such a system would version file formats in which: (i) each datum in a collection is described on a different line, (ii) each line is relatively independent, and (iii) the order of elements in the serialized file can be made consistent. The ot nex son format that we are currently using is not ideal in these respects. Some operations, such as re rooting a tree, affect many lines in a file (i.e. many branches change source to target orientation). Thus far, the rate of curation has been low enough that most merges have been unambiguous because only one branch of the git history has changed a particular study. The phy le system currently avoids potentially incompatible merges, and warns the user who committed later that his or her changes have been saved but not merged onto the primary branch. A diff and merge tool that operates on the object model has been written and is currently under testing. Git does not handle very large files well, and git hub limits the size of individual files to 100 MB and repositories to around 1 GB. s harding precludes problems with overall repository size. We have specifically limited the data types to be included in the phy le system. Alignments and bootstrapping or MCMC chain replicates of trees should not be included in uploads, and there is currently a 20 MB filesize cap on files committed to the repository. The choice to exclude alignments and tree replicates has the advantages of limiting the file sizes in the repository and maintaining the focus of the database. phy le system is for the storage and curation of phylogenetic estimates. Other resources, such as data dryad (), are more appropriate for other static data associated with analyses. If it becomes necessary due to increases in the size of trees, it is straightforward to alter the 20 MB limit. While there are general approaches to expand the scale of versioned data tools (see, e.g. git annex and dat [http://dat-data.com/]), we anticipate that phy le system will continue to rely on links outs to the full archives of data to accommodate large studies. While phy le system currently uses git hub for user authentication and to host a readily accessible copy of the repository, the database does not inherently rely on git hub and could be migrated should the need arise. This would require only altering the authentication procedure in the curation webapp and implementing an alternate trigger for updates to OTI. Because the push of data to git hub triggers the web hooks the code which performs the push could be extended to call the web hooks if the project migrates away from git hub. As discussed by others (), the rate of deposit of phylogenetic estimates into public archives is currently low. It is also clear that the trees available in digital archives often need some curation. The two most common curation needs are rooting of phylogenies, and the mapping of idiosyncratic tip labels to a uniform naming system or taxonomy. Rooting is necessary because many phylogenetics software return unrooted trees as product of analyses. These trees are correctly rooted using an out group and displayed in figures in publications, but often this rooting information is not included in the tree file deposited in repositories. This is a large task because the number of phylogenies published each year is large. Furthermore, some aspects of the curation (most notably verifying that the tip labels are correctly aligned to a taxonomy) require a significant amount of expert ize and time investment. It is unclear whether there is a way to motivate the broader community of systematic biologists to invest their time in helping curate a collection of phylogenetic knowledge like phy le system. Many of the design decisions behind phy le system reflect a desire to alleviate some potential concerns of data curators. By making the data store publicly accessible as flat files which can be synchronized using robust version control operations, we have tried to lessen concerns that the curation effort is being donated to a resource which might disappear after the end of the Open Tree of Life project. By preserving the history of each commit, we hope to make the data transformation process more transparent, but also make it easier for curators to obtain proper credit for their work. While the curated trees in phy le system could in theory be exported back to static repositories such as dryad or tree base this is not necessary. The aim of these resources differs and complement one another. Curation edits to data in phy le system make the data more interoperable and reusable, while the archival storage is crucial for replicating the results of a published study exactly. The original labels on tips and rooting position are maintained in the ot nex son files (as well as in the git history). Thus, it is straightforward to associate the edits made during curation back to the original data. The design of the data store was also intended to motivate other bioinformatic ians to build tools to work with these data. In a traditional database driven resource, the code used to pull information from the private database is quite distinct from the code written by users of the data. However, in phy le system the server code and client code both deal with the same JSON file format. Thus, developers can easily reuse the codebase of the phy le system api as they write new functionalities that use data from phy le system or even host their own web services using the data. Almost all of the git operations and ot nex son handling operations are implemented in the standalone library, peyo tl. This is intended to make it easier for other programmers to clone the data store and work with it locally. To improve the transparency of the software development process, we use issue trackers to report problematic behavior and request new features. This also improves the sustainability of the software development efforts because the motivations behind the implementation decisions are documented. unit tests in the peyo tl library verify that the ot nex son validation and git interactions are be having as expected. The phy le system api repository contains a set of integration tests that automate testing of the series of API calls that are expected during curation. Automated testing also improves the software sustainability by helping to identify regressions in behavior as new features are implemented.

conclusion we have developed a git based data store for archiving and curating phylogenetic estimates of species relationships. By incorporating curation into the data storage, we have lowered the activation cost of entering data into an archive while also allowing continued curation, whether by the original authors or researchers interested in re-using these data, to improve the associated metadata. Using the git VCS allows us to track data curation and maintain provenance, while simultaneously making it straightforward for researchers to maintain their own updatable copies of the database.
