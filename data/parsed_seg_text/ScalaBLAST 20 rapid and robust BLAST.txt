Motivation: BLAST remains one of the most widely used tools in computational biology. The rate at which new sequence data is available continues to grow exponentially, driving the emergence of new fields of biological research. At the same time, multicore systems and conventional clusters are more accessible. scala blast has been designed to run on conventional multiprocessor systems with an eye to extreme parallelism, enabling parallel BLAST calculations using 416 000 processing cores with a portable, robust, fault resilient design that introduces little to no overhead with respect to serial BLAST. Availability: scala blast 2.0 source code can be freely downloaded from

introduction genome and protein sequence analysis using BLAST continues to be among the most used tools for computational bioinformatics. The continued exponential growth in throughput of sequencing platforms has continued to drive the need for ever expanding capacity for BLAST () calculations to support genome annotation, functional predictions and a host of other foundational analysis for sequence data. Parallel BLAST accelerators have been implemented in the past including mpi blast () and scala blast 1.0 (). Parallel BLAST drivers accelerate large lists of BLAST calculations using multiprocessor systems. scala blast 1.0 used a hybrid parallelization scheme in which the sequence list was statically partitioned among processor pairs (process groups). Process groups performed independent BLAST calculations simultaneously, gaining a degree of speed up on the overall calculation in proportion to the number of process groups used in the calculation. The main limitation of scala blast 1.0 was the use of static data partitioning that did not have fault resilience properties. By contrast, the main limitation of mpi blast is the need for pre formatting datasets to achieve optimized run-time, sometimes requiring repeated attempts on the same dataset to find the right pre formatting configuration. We have addressed these limitations in scala blast 2.0 by (i) re-implementing the task scheduling layer by introduction of a dynamic task management scheme that (ii) does not require pre formatting. This technique allows processors to obtain work units independently and at run-time based on their availability. This is a highly tolerant and fault resilient approach that ensures that all processors are doing as close as possible to the same amount of work throughout a calculation. In addition, this implementation allows for continued operation even in the presence of processor or other system failures. This is critical for all large scale calculations and is independent of the code being run because the longer the run and the larger the system, the more likely one is to encounter a component failure during a calculation. As the expected run-time increases, the likelihood of successfully completing the calculation before the next failure tends to zero. We demonstrate near ideal scaling using scala blast 2.0 calculations to machine capacity on a Linux cluster having 418 000 compute cores even during process failure events. scala blast 2.0 can be downloaded freely from http:// omics pnl gov software scala blast php. output and input on globally mounted or local file systems or combinations of both. After file distribution is complete, the manager is responsible for tracking which tasks have been assigned and which tasks have been completed. The manager is also responsible for processing the fast a input files (both query and target database are in fast a format, eliminating the need for pre formatting database files) and distributing these processed files. The task groups can be controlled by the user and can span multiple compute nodes. For instance, a system with eight core nodes can have a task group size of 24 in which sets of three nodes work together as a single task group, having one sub manager core and 23 worker cores. This dynamic scheduling layer ensures that when processes fail or get loaded down with tasks taking a long processing time, other processes continue to do meaningful work. This allows for highly skewed input sets to be processed as much as possible in an even run-time. Dynamic scheduling is implemented by having the manager 'hand out' tasks to sub managers. Workers completing a task do not write their output until they verify from the manager (via the sub manager whether the task has already been checked back in. Workers then request a new assignment from the manager. When all the tasks have been assigned, any workers reporting for new work are given a duplicate task that has not yet been completed. In this way, nodes that fail during a calculation are simply ignored. Any tasks assigned to them will be re-assigned to other workers until one of them completes the calculation.
