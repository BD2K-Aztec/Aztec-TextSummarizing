Motivation: The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics? Results: Through a simulation study using data models and analysis of real microarray data, we show that (i) for small samples the root mean square differences of the estimated and true metrics are considerable; (ii) even for large samples, there is only weak correlation between the true and estimated metrics; and (iii) generally, there is weak regression of the true metric on the estimated metric. For classification rules, we consider linear discriminant analysis, linear support vector machine (SVM) and radial basis function SVM. For error estimation, we consider re substitution three kinds of cross validation and bootstrap. Using resampling, we show the unreliability of some published ROC results. Availability: Companion web site at http://compbio.tgen.org/ papers up pro croc html Contact:

introduction high throughput technologies, such as those based on microarrays or next generation sequencing, make it possible to generate data on large numbers of genes, transcripts or proteins simultaneously in biological samples. Typical variables assessed include mutations, DNA copy number, DNA methylation, mRNA expression, microRNA expression, protein expression and post-translational modifications. A central goal of current biomedical research is to use those molecular profiles to identify biomarkers or multi-gene bio signatures for 'personalization' of medicine that is, to use them for the full range of medical management choices in disease risk assessment, sub-classification of disease, early diagnosis, prognosis, choice of optimal therapy, evaluation of response to therapy and or identification of relapse. * To whom correspondence should be addressed the profile data are used to develop univariate or multivariate predictors of biologically or medically interesting outcomes. Often, the aim is to develop a binary classifier, for example, diseased versus normal, disease subtype 1 versus disease subtype 2, response versus non-response to a drug, or 5 year survival versus death. A large literature has developed on such classifiers, but the recurring question is, 'How accurate are their predictions and classifications?' This question is supposed to be answered by the error rate; however, recent Monte Carlo simulations have shown large uncertainty in the error estimates. In the presence of high dimensional feature spaces and small samples, a ubiquitous situation with high throughput technologies, resampling error estimation methods, for example, cross validation (CV), suffer from high deviation variance, that is, the variance of the difference between the true and estimated errors is large braga for an early criticism of. Moreover, there tends to be a lack of correlation and regression between the true and estimated errors, to the extent that the regression line of the true error on the estimated error is nearly horizontal (). These Monte Carlo studies have been supported by analytical studies in the case of the discrete histogram rule (Braga) and linear discriminant analysis (LDA;). For assessment of binary classifiers, in addition to the error rate, a favorite analytical tool is the receiver operator characteristic (ROC) representation ()for instance, with regard to gene expression profiling in cancer, see on the companion web site (http://compbio.tgen.org/paper_supp/ROC/ roc html. An ROC curve is formulated by plotting the sensitivity and specificity of the classifier against each other as a function of some threshold criterion, for example, based on a biomarker or bio signature. The resulting ROC curve presents graphically the trade-off between false positives (FP) and false negatives (FN) in the classification process. The area under the ROC curve provides a scalar parameter that reflects the overall quality of the classifier. A natural question is whether parameters associated with ROC curves, such as the area under the curve (AUC), would suffer the same degree of uncertainty as discovered in the previous analyses of classifier error. Accordingly, we have established the computational machinery to address this question for both simulated and real datasets, and have performed a variety of analyses based on different predictive algorithms and methods of validation. We have analyzed the effect of sample size and the effect of an unbalance in the number of cases per class. That type of imbalance is common in biological datasets.
