Motivation: Topological entropy has been one of the most difficult to implement of all the entropy theoretic notions. This is primarily due to finite sample effects and high dimensionality problems. In particular, topological entropy has been implemented in previous literature to conclude that entropy of exons is higher than of introns, thus implying that exons are more random than introns. Results: We define a new approximation to topological entropy free from the aforementioned difficulties. We compute its expected value and apply this definition to the intron and exon regions of the human genome to observe that as expected, the entropy of introns are significantly higher than that of exons. We also find that introns are less random than expected: their entropy is lower than the computed expected value. We also observe the perplexing phenomena that introns on chromosome Y have a typically low and bimodal entropy, possibly corresponding to random sequences (high entropy) and sequences that posses hidden structure or function (low entropy). Availability: A Mathematica implementation is available at

introduction entropy as a measure of information content and complexity, was first introduced by. Since then entropy has taken on many forms, namely topological, metric (due to Shannon), kolmogorov sinai and rny i entropy. These entropies were defined for the purpose of classifying a system via some measure of complexity or simplicity. These definitions of entropy have been applied to DNA sequences with varying levels of success. Topological entropy, in particular, is infrequently used due to high dimensionality problems and finite sample effects. These issues stem from the fact that the mathematical concept of topological entropy was introduced to study infinite length sequences. It is universally recognized that the most difficult issue in implementing entropy is the convergence problem due to finite sample effects (). A few different approaches to circumvent these problems with topological entropy and adapt it to finite length sequences have been attempted before. For example, in, linguistic complexity (the fraction of total sub words to total possible sub words is utilized to circumvent finite sample problems. This leads to the observation that the complexity randomness of intron regions is lower than the complexity randomness of exon regions. However in, it is found that the complexity of randomly produced sequences is higher than that of DNA sequences, a result one would expect given the commonly held notion that intron regions of DNA are free from selective pressure and so evolve more randomly than do exon regions. Also, little has been done in the way of mathematically analyzing other finitary implementations of entropy due to most previous implementations using an entire function instead of a single value to represent entropy (thus the expected value would be very difficult to calculate). In this article, we focus on topological entropy, introducing a new definition that has all the desired properties of an entropy and still retains connections to information theory. This approximation, as opposed to previous implementations, is a single number as opposed to an entire function, thus greatly speeding up the calculation time and removing high dimensionality problems while allowing more mathematical analysis. This definition will allow the comparison of entropies of sequences of differing length, a property no other implementation of topological entropy has been able to incorporate. We will also calculate the expected value of the topological entropy to precisely draw out the connections between topological entropy and information content. We will then apply this definition to the human genome to observe that the entropy of intron regions is in fact lower than that of exon regions as one would expect.
