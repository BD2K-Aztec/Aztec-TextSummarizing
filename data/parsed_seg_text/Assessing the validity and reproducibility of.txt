Motivation: Validation and reproducibility of results is a central and pressing issue in genomics. Several recent embarrassing incidents involving the ir reproducibility of high profile studies have illustrated the importance of this issue and the need for rigorous methods for the assessment of reproducibility. Results: Here, we describe an existing statistical model that is very well suited to this problem. We explain its utility for assessing the reproducibility of validation experiments, and apply it to a genome scale study of adenosine deaminase acting on RNA adar mediated RNA editing in Drosophila. We also introduce a statistical method for planning validation experiments that will obtain the tightest reproduci-bility confidence limits, which, for a fixed total number of experiments, returns the optimal number of replicates for the study. Availability: Downloadable software and a web service for both the analysis of data from a reproducibility study and for the optimal design of these studies is provided at

introduction the issue of validation and reproducibility of scientific results has recently been the subject of intense discussion in the scientific community. Several eye opening reports have either claimed insufficient validation of bold research findings or shown an inability to replicate such results in genomics (), genetics (), oncology (), neuroscience (), pharmacology (), proteomics () and psychology (). Problems with reproducibility have been demonstrated with widely used technologies such as microarrays (), sirna based screens () and mass spectrometry (). This has led to appeals for increased statistical rigor (), platforms for the publication of neutral studies () and attempted replicates, whether successful or not (), and a system-wide committed effort toward generating work that is reproducible (), placing at least as much emphasis on reproducibility as is currently placed on novelty (). Recent attempts at addressing the issue of reproducibility include the Reproducibility Initiative, which, for a fee, will carry out independent validation of research findings and issue a 'certificate of reproducibility' for those studies that validate (https:// www science exchange com reproducibility and science check which provides a platform for researchers to report on the 'reproducibility and utility of the literature method(s) that they have worked with' (http://www.sciencecheck.org). Although these are extremely important contributions, neither organization provides a quantitative measure of reproducibility. In light of this, there is an urgent need for statistical tools for quantitatively evaluating reproducibility. To help address this need, we introduce the application of a well suited Bayesian hierarchical model for assessing the reproducibility of validation experiments in the context of evaluating top tier predictions of high throughput genomic studies. We focus on studies in which a large number of predictions are made concerning a biological phenomenon of interest. There are many studies of this type in the recent literature, in Drosophila alone predict 2000 new gene promoters identify thousands of targets of six transcription factors involved in regulation of the anterior posterior axis in the embry on gre et al find 414 000 binding sites of six proteins associated with insulators, DNA sequences that block the spread of regions of modified chromatin and interaction between other regulatory elements, and find evidence for 1600 genes whose transcription start sites are sites of polymerase II stalling. Because validation of all predictions is typically infeasible, often a few compelling and biologically interesting cases are selected for further study (), leaving a long list of unvalidated predictions. The reader is left unsure about both the fraction of the list that is valid and the effect of biological and sample preparation variation. Our model takes advantage of multiple biological and technical replicates, in each of which validation of a random sample *To whom correspondence should be addressed. of the top tier list is carried out. From these data, we can assess the reproducibility of the validation studies and predict what another investigator could reasonably expect to see in a followup study. The use of replicates, whether technical, biological or simulated, has been shown to be useful in many contexts and Kerr and Churchill (2001) simulate microarray replicates to determine the stability of clusters of genes that exhibit similar expression patterns. In the search for differentially expressed genes, technical replicates provide additional power for microarrays (), and biological replicates reduce false positives in conclusions drawn from serial analysis of gene expression data () and improve accuracy in calls made from rnase q data ().use replicate time series datasets to capture time delayed associations between microbes. In a larger scale take on the replicates, meta analyses of genome wide association studies like those described in zeg gini and Ioannidis (2009) combine datasets from multiple laboratories to gain enough power to detect associations between particular genes and diseases such as type II diabetes () and Crohn's disease (). In some cases, replicates have been incorporated into optimal study design. Many articles have been written on the number of replicates required to detect a certain fold change in gene expression via microarray studies (). For genome wide association studies find the required number of samples to replicate an association across studies with a certain level of between study heterogeneity, and propose multistage designs which, for a given budget, maximize the power to find associations. Auer and Doerge (2010) advocate careful design of rnase q experiments, including sampling, randomization, replication and blocking. To our knowledge, no one in the biology community has used biological or technical replicates to assess the reproducibility of validation studies like those discussed here, or has proposed a method for optimal design of such experiments with respect to reproducibility. There has been considerable work on assessing the reproducibility of high throughput experiments, especially in the context of ranked lists of putative sites (). In this context, reproducibility is most closely related to precision (or stability), in that the relevant issue is the similarity of two ranked lists generated from biological replicates (or different high throughput platforms, different ranking algorithms, etc. . .). There are many different measures used to assess the similarity of two or more ranked lists, from spearman s rank correlation () to overlap counts for the top k sites () to weighted overlap counts that emphasize correlation between high ranking sites over that of low ranking sites ().improve on these measures with a mixture model consisting of reproducible and ir reproducible sites, which assigns each signal a reproducibility index based on its consistency across replicates, which approximates its probability of being reproducible. They define the ir reproducible discovery rate' (IDR), an analog of the false discovery rate for multiple hypothesis testing (), which determines the 'expected rate of ir reproducible discoveries' for sites whose probability of being ir reproducible is below some threshold. Their methods provide a principled method for selecting sites for further study and for evaluating ranking algorithms. Although here we also address the issue of reproducibility, our focus is different. We are not concerned with the precision of high throughput technologies or ranking algorithms, but rather with the reproducibility of independent validation experiments that seek to verify findings of such high throughput experiments. The validation experiments taken individually give us information about the accuracy of the findings, whereas our model of biological replicates assesses the reproducibility of the given validation scheme in the face of biological and sample preparation variation. Because the model we describe depends on validation of random samples, here we first review how a single simple random sample drawn from the top tier list can be used to estimate the valid fraction of top tier predictions. Because this method does not account for biological and sample preparation variability, it is not sufficient to assess reproducibility, as factors as seemingly benign as laboratory conditions, reagent lots, cell generations and individual experimenter techniques have been shown to affect results of biological experiments (). So motivated, we describe how our hierarchical model uses data from multiple replicates to compute a probability distribution of validation results for an as yet unseen replicate. Hierarchical models, described in many statistical textbooks including, have many uses in computational genomics (), and are well suited to the task of assessing reproducibility, as they provide a way to simultaneously model similarities and differences between groups.

discussion the need for reproducibility in scientific research has always been central, but has only recently become a major focus of the greater scientific community. Here, we present a procedure that addresses these issues in the context of high throughput studies like that described in our companion article, where thousands of predictions are made, and only a relatively small fraction can be validated. We studied closely the example of ADAR editing sites, but the method is generalizable, and could just as easily apply to any of the high throughput site prediction experiments described earlier. Studies of this kind have become more frequent with the advent of high throughput technologies like Illumina and large collaborations like ENCODE, mod encode and the 1000 Genomes Project. Most studies already have their own schemes for validation, which they carry out with varying levels of statistical rigor. The authors and any investigators hoping to carry out follow-up studies would all benefit from a carefully designed validation study using statistically random samples in biological replicates to assess reproducibility. As the accuracy of technology inevitably grows, it may be tempting for investigators to assume a single replicate is sufficient to address reproducibility, implicitly assuming that technical variation is the only variation that matters. However, even with perfect technology, multiple biological replicates are still necessary to assess the reproducibility of a set of results. It is worth noting that each of our validation replicates was performed in a pool of 10 flies each. We expect that biological replicates will be even more crucial in studies where replicates consist of individual model organisms, such as mice or rats. In our analysis of adar mediated editing data, we found that the 95% confidence intervals for individual replicates showed substantial variation. This was not unexpected, given the documented variation in ADAR activity in individual flies () and observations on the effects of experimental conditions described earlier, and it underscored the need for statistical tools that can address the effect of such variation on the reproducibility of results. Our software predicted that a new experiment using the same protocol under the same experimental conditions would validate on average 67% of sites, and that 90% of the time, the percentage validated would be at least 55%. We emphasize the lower bound percentile, e.g. the 10th percentile, because it represents a worst case scenario for a future experiment. Of course, because these results are affected by our choice of the diffuse prior on and , the intervals we generate may be too conservative in the case where more is known about these parameters a priori. There is a technical limitation to our model that arises from the statistical formulation. The predictive distribution we describe is well defined everywhere except in the precise circumstance in which every replicate pool has a validation rate of either 100 or 0% (). We consider such extremes to be very unlikely in most validation studies; however, this limitation occasionally comes to bear in experimental design, where we simulate thousands of distributions. This affects almost all simulations with means 495% and most simulations with means $90% and with wide variances. Otherwise, the effect is negligible. Our software will not return results in the few non negligibly affected cases. Our model makes two major assumptions: that the validation experiments in each replicate follow a binomial distribution, and that the proportions valid in each replicate follow a beta distribution. Because we require that each replicate test a random sample drawn from the whole population of predictions, the results of each replicate follow a hypergeometric distribution, which is very well approximated by a binomial distribution as long as the number of predictions n tested in a single replicate is. Influence of parameters on optimal predictive distribution. Here we see the effect of varying the four input parameters: expected mean, expected standard deviation, expected fraction of successful experiments and total number of experiments. For each curve, we held three parameters fixed and varied the parameter of interest. For each 4-tuple, we found the maximum 10th percentile over all predictive distributions for all assignments of experiments to replicates. From the plot, we see that the desirable higher 10th percentiles result from data with high mean, low standard deviation, large numbers of experiments and high fraction of successful experiments much smaller than the total number of predictions N (a typical rule of thumb is N ! 20n). The beta distribution was chosen for the model primarily because as a conjugate prior to the binomial distribution, it makes computation feasible. However, another real advantage is that the beta distribution is extremely flexible, in that it can approximate most smooth unimodal distributions. We illustrate this flexibility in the context of our model in Supplementary. Together, this suggests that the assumptions of our model will be appropriate for most applications. There are three places in our model where we rely on numerical approximations: during predictive inference, we compute the posterior distribution of the hyperparameters over a grid, as there is no closed form expression for computing it directly, then we sample from the grid to approximate the predictive distribution; and in our optimal study design, we rely on a sampling approximation to find the average mean, standard deviation and 10th percentile of the population of predictive distributions resulting from particular input parameters. For predictive inference, we follow the procedure outlined in Gelman et al., computing over a dense enough grid that we believe captures the important features of the posterior distribution, and sampling a large number of points (1000) to approximate the predictive distribution. We found that neither varying the grid density nor increasing the number of samples noticeably changed the results (data not shown). For study design, we sample a large number of distributions (3000 is the default), and if the user downloads our software, he or she can increase the number of samples if desired, so as to obtain narrower error bars. Finally, it should be noted that the results of any reproducibility analysis can only be generalized to the population to which the replicates belong (just as the results of any study should only be generalized to the population from which the data are drawn). We assume that follow-up validation studies follow the original protocol under the exact experimental conditions as the original experiments. To the extent that this is not possible, the credibility intervals that we report may be too narrow to accurately reflect the population of follow-up validation studies performed by other investigators in other laboratories. Therefore, care must be exercised in how claims of reproducibility are made, and authors should be sure to specify the population to which their results generalize. In some cases, large collaborations between laboratories (such as those associated with mod encode will be able to carry out replicates that represent a larger portion of the possible variability, and will be able to make even stronger claims about the reproducibility of their findings. Validation and reproducibility are bedrock principles throughout science that have until recently received limited attention. We present this work as an aid in advancing these crucial principles in the field of genomics.
