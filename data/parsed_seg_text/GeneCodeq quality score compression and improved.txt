Motivation: The exponential reduction in cost of genome sequencing has resulted in a rapid growth of genomic data. Most of the entropy of short read data lies not in the sequence of read bases themselves but in their Quality scores the confidence measurement that each base has been sequenced correctly. Lossless compression methods are now close to their theoretical limits and hence there is a need for lossy methods that further reduce the complexity of these data without impacting downstream analyses. Results: We here propose gene code q a Bayesian method inspired by coding theory for adjusting quality scores to improve the compressibility of quality scores without adversely impacting geno-typing accuracy. Our model leverages a corpus of km ers to reduce the entropy of the quality scores and thereby the compressibility of these data (in fast q or samba mcr am files), resulting in compression ratios that significantly exceeds those of other methods. Our approach can also be combined with existing lossy compression schemes to further reduce entropy and allows the user to specify a reference panel of expected sequence variations to improve the model accuracy. In addition to extensive empirical evaluation, we also derive novel theoretical insights that explain the empirical performance and pitfalls of corpus based quality score compression schemes in general. Finally, we show that as a positive side effect of compression, the model can lead to improved genotyping accuracy. Availability and implementation: gene code q is available at:

introduction over the past decade, unprecedented advances in next generation sequencing (NGS) technologies have led to a dramatic reduction in sequencing cost () and much faster data production rates (). These technological advances are fostering increasingly wide ranging applications in biotechnology, public healthcare () and personalized medicine (). Furthermore, as genomic sequencing data has grown exponentially, they have outpaced advances in some information technologies that they indirectly rely on: computing power and storage (). In particular, genome sequencing results in a large storage footprint for each genome. Thus, storing and transferring raw sequencing information is becoming prohibitively expensive () and effective compression schemes of raw sequencing data are indispensable the majority of NGS data output consists of read sequence information whereby each nucleotide base is associated with a sequence confidence level (also referred to as quality score), produced by the base caller (). The Phred scale () Q  10log 10 P   encodes with integer Quality Score Q an estimate of the probability P that the base has been called incorrectly. This information is used both for the quality control of raw read data and for downstream processing, including genome assembly, read mapping and genotyping (). In compressed genomic datasets, quality score values take up the dominating share. For example, when compared to the read sequence data, quality scores of Illumina reads take at least 2.3 more storage, although this can be an even higher ratio when using more aggressive sequence compression (). Quality scores are more difficult to compress due to a larger alphabet (6394 in original form) and intrinsically have a higher entropy (). With lossless compression algorithms and entropy encoders reaching their theoretical limits and delivering only moderate compression ratios (), there is a growing interest to develop lossy compression schemes to improve compressibility further. Quantizing quality scores (i.e. reducing the alphabet size) is the most basic approach to improve compressibility in a lossy manner. One such approach of reducing all quality values to eight levels (bins) () has become a widely used standard for the Illumina platform and is enabled by default on the most recent machines (). Another approach called P) involves local quantization so that a representative quality score replaces a contiguous set of quality scores that are within a fixed distance of the representative score. Similarly the R) scheme replaces contiguous quality scores that are within a fixed relative distance of a representative score. Other lossy approaches improve compressibility and preserve higher fidelity by minimizing a distortion metric such as mean squared error or l1 based errors qual comp and qv z (). However, adoption of lossy compression schemes for quality scores has been slow due to concerns about adverse effects on downstream analyses, in particular genotyping accuracy (). However, there are also reports that compression schemes such as P-Block, r block qv z and qual comp can, under some circumstances, lead to a slight improvement in genotyping accuracy. A number of more recent approaches utilize the sequence data itself to guide the quality score compression. Quartz achieves this using a reference corpus built from frequent 32-mers across reads from individuals sequenced in the 1000 Genomes Project (). Read base pairs that match any one 32-mer in the corpus (up to one allowed mismatch per 32-mer) have their quality score set to a fixed high value. This spar sification of quality scores reduces entropy, thus improving quality score compressibility. A different approach, Leon, () utilizes the dataset itself for building its set of km ers and to generate a reference probabilistic de bru jin Graph. In this case, bases in a read that have enough highly frequent km ers covering it within the dataset are set to a fixed high value quality score. Both methods were reported to improve genotyping accuracy. In this article we present gene code q a lossy compression scheme that is inspired by coding theory () and Bayesian inference. Uniquely, gene code q uses a statistical approach to objectively reason about the compressibility of quality scores. Briefly, our model estimates the posterior probability of a sequencing error given the evidence of the full read, including quality scores, together with information from a reference corpus. As a result, the posterior estimates of most quality scores are boosted above a saturation point of the Phred scheme, corresponding to very high confidence. This approach results both in a significant reduction in entropy and better genotyping accuracy when compared to existing methods.
