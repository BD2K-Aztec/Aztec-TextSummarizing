Motivation: Computational approaches for the annotation of phenotypes from image data have shown promising results across many applications, and provide rich and valuable information for studying gene function and interactions. While data are often available both at high spatial resolution and across multiple time points, phenotypes are frequently annotated independently, for individual time points only. In particular, for the analysis of developmental gene expression patterns, it is biologically sensible when images across multiple time points are jointly accounted for, such that spatial and temporal dependencies are captured simultaneously. Methods: We describe a discriminative undirected graphical model to label gene expression time series image data, with an efficient training and decoding method based on the junction tree algorithm. The approach is based on an effective feature selection technique, consisting of a non-parametric sparse Bayesian factor analysis model. The result is a flexible framework, which can handle large scale data with noisy incomplete samples, i.e. it can tolerate data missing from individual time points. Results: Using the annotation of gene expression patterns across stages of Drosophila embryonic development as an example, we demonstrate that our method achieves superior accuracy, gained by jointly annotating phenotype sequences, when compared with previous models that annotate each stage in isolation. The experimental results on missing data indicate that our joint learning method successfully annotates genes for which no expression data are available for one or more stages.

introduction the use of high throughput image acquisition, such as in phenotypic screens, has been quickly increasing and thus provides a new source of data for computational biologists. Microscopy of colored or fluorescent probes, followed by imaging, is able to deliver spatial and temporal quantitative phenotype information such as gene expression at high resolution (). In addition, expression patterns can be documented and distributed over the internet as a valuable resource to the research community. Recent advances in throughput, or long term investment in specific projects, have by now generated large collections of images. Such image databases are traditionally analyzed through direct inspection by human curators; an example is the Berkeley Drosophila Genome Project (BDGP) gene expression pattern database (). In this dataset, images are assigned to stage ranges within the 17 embryonic stages defined by developmental features, and annotated collectively in small groups using a controlled vocabulary (CV), i.e. annotation terms. This allows researchers to search image databases and compare spatial and temporal embryonic development. Given the very diverse nature of imaging technology, samples and biological questions, computational approaches have often been tailored to a specific dataset. For example, the image based profiling of gene expression patterns via in situ hybridization (ISH) requires the development of accurate and automatic image analysis systems for using such data, to understand regulatory networks and development of multicellular organisms. Images are affected by multiple sources of noise due to experiments or microscopy (incomplete or multiple embryos, variance of probes across genes, illumination artifacts), making the extraction and registration of embryos non-trivial (). Peng and Myers (2004) and introduced an automatic image annotation framework using various high dimensional feature representations and classifying frameworks: Principal Component Analysis pc a wavelets, Gaussian mixture models, Support Vector Machines (SVM), Quadratic Discriminant Analysis. Each image may show the embryo under different views: lateral, dorsal or ventral; this is a challenge for gene annotation, as embryonic structures may be visible in only certain views. Yet, recent studies have shown that incorporating images from multiple views could consistently improve the annotation accuracy (). It is desirable to represent images in a way that takes advantage of image features and offers robustness to image distortions. In contrast to such large feature sets prone to high redundancy and high computational costs identified a set of basic expression patterns in Drosophila. A set of 39 well defined clusters describing specific regions of embryo expression were determined from 2693 lateral views of early development. As with the majority of described approaches, this study involved a high level of human intervention in selecting 'good' images for training testing purposes a potential drawback, considering the rapid increase in the size of ISH image collections. In contrast, pru tea nu proposed a new approach for automatic annotation of spatial expression patterns using a 'vocabulary' of basic patterns that involved little to no human intervention. This work provided a flexible unsupervised framework in competitively predicting gene annotation terms, while using only a small set of features. *To whom correspondence should be addressed.  The Author 2013. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals permission soup com A particular aspect that has been largely neglected by computational approaches so far is that data acquired from such experiments often span multiple time points or conditions. Phenotypes are typically annotated stage by stage without jointly learning the salient temporal dependencies across multiple time points, which should allow for an overall higher accuracy; e.g. the annotation terms predicted for earlier stages should inform the prediction at later stages. Furthermore, many genes are annotated with more than one term from the vocabulary, creating an additional dependency structure between annotations within the same stage range. In this article, we address the automatic annotation of Drosophila embryo gene expression sequences, building on state of the art models from computer vision and machine learning. There are several challenges that need to be addressed when approaching this problem through computational methods. As we mentioned previously, the image acquisition process results in embryonic structures with multiple perspectives, shapes and locations. Moreover, the shape position of the same embryonic structure may vary from image to image: 'variation in morphology and incomplete knowledge of the shape and position of various embryonic structures' have made the gene annotation task more prohibitive (). We first show that a non-parametric Bayesian factor analysis (BFA) approach, the infinite factor model, allows for an efficient and sparse feature representation of the Drosophila gene expression dataset. Then, we propose a conditional random field (CRF) to tackle the time evolving annotation task. Experiments show that the exploitation of dependencies across adjacent developmental stages leads to annotation accuracy superior to existing Drosophila gene expression annotation approaches. The proposed framework also tackles the missing data scenario: for many genes, one or more stage ranges are absent from the image collection; in such cases, human annotators would take into account the entire set of expression data from adjacent stages to successfully annotate the available images. The challenge to automati ze this process is novel and represents a step closer toward a fully automatic gene annotation pipeline. These predictions can be later analyzed by biologists to assess the correctness of the image acquisition and the level of interest for that particular gene. Finally, for a given gene, the described framework predicts the entire set of annotation terms simultaneously, taking full advantage of the term dependencies that exist at the stage range level. The rest of this article is organized as follows: in Section 2, we focus on data description and introduce the sparse bfa crf framework. Experimental results are reported in Section 3, followed by conclusions and future work in Section 4 expression is visualized by RNA ISH, which provides an effective way of locating specific mRNA sequences by hybridizing complementary mrna binding oligonucleotides and a suitable dye (). The mRNA expression apparent in the captured in situ images was verified by independently derived microarray time course analysis using Affymetrix GeneChip technology (more details can be found at http:// in situ fruit fly org and in). Gene expression patterns were documented by taking low (2) and high (20) magnification images, at multiple developmental stages. The low magnification digital images were taken to capture groups of embryos, to provide a permanent record of the hybridization in each well. Each slide was then further examined under higher magnification using a Zeiss axio phot optical microscope. Images were assigned to developmental stage ranges following the sequence of events taking place at specific times after fertilization, using the 17 stages defined in (). In this analysis, we focused on the first 15 hr of Drosophila development, spanning embryonic stages 46, 78, 910, 1112 and 1316. Developmental stages 13 were skipped owing to predominant ubiquitous expression patterns not of interest to our analysis. Any gene is represented, on average, by approximately 12 images; however, the number of images per gene varies from 1 to 80. This variability reflects the BDGP strategy to document highly dynamic, complex and novel patterns, while lowering the number of images documenting common expression patterns. Among those, there are images with non informative patterns due to poor quality staining washing or non tissue specific expression (maternal or ubiquitous). Images within the same window can show different patterns due to embryo orientation or the relatively long developmental time spanned by a stage range. Images are annotated with ontology terms from a CV describing developmental expression patterns (). This vocabulary has been developed and refined by FlyBase () over the past few years, allowing human curators to compare their findings with expression data assembled from the literature, expansion of annotations to greater detail and thorough searches of datasets based on Gene Ontology schema. The annotations used throughout this project consisted of a subset of about 300 of the 5800 annotation terms in the FlyBase CV, many of which only apply to later stages of development. As mentioned previously, we use all available images in our approach, i.e. including those taken with any embryo orientation: lateral, dorsal and ventral. Before extracting features, we segmented and registered images using a previously described probabilistic segmentation approach based on statistical shape models (). This provides us with 240  120 pixel images, mostly containing a single embryo in a standard dorsal up anterior down orientation and no background. In, we show a particular gene expression pattern across five developmental stage ranges of interest. The complexity and variability of the image data led to competitive but not perfect results, in terms of precise embryo extraction as well as embryo orientation, which increased the challenge of automatic gene annotation. We here use the average intensities in a down-sampled fixed grid size of 80  40 pixels as input features for the entire collection of images within the BDGP dataset.
