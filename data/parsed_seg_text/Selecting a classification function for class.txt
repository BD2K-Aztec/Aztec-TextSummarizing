Motivation: Class predicting with gene expression is widely used to generate diagnostic and or prognostic models. The literature reveals that classification functions perform differently across gene expression datasets. The question, which classification function should be used for a given dataset remains to be answered. In this study, a predictive model for choosing an optimal function for class prediction on a given dataset was devised. Results: To achieve this, gene expression data were simulated for different values of gene pairs correlations, sample size, genes variances, deferentially expressed genes and fold changes. For each simulated dataset, ten classifiers were built and evaluated using ten classification functions. The resulting accuracies from 1152 different simulation scenarios by ten classification functions were then modeled using a linear mixed effects regression on the studied data characteristics, yielding a model that predicts the accuracy of the functions on a given data. An application of our model on eight real life datasets showed positive correlations (0.33â€“0.82) between the predicted and expected accuracies. Conclusion: The here presented predictive model might serve as a guide to choose an optimal classification function among the 10 studied functions, for any given gene expression data. Availability and implementation: The R source code for the analysis and an r package sp refuge d are available at Bioinformatics online.

introduction microarray gene expression profiling has become a widely used tool to identify particular disease subpopulations and to perform diagnostic and prognostic predictions (). In clinical practice, they are used in diagnostic and prognostic analyses while in preclinical studies (toxicogenomics), they involve predicting the toxicity of compounds in animal models with the goal of speeding up the evaluation of toxicity for new drug candidates (). Though class prediction analysis is a common practice, the question that remains to be addressed is, given the wide availability of classification functions nowadays, which classification function do we use for a particular dataset? Classification functions have been shown to perform differently across gene expression datasets (). Moreover, the ma qc ii initiative has pointed out that classification function is one of the variables that explains the variability between gene expression class prediction performance (). While substantial amount of information is known about the characteristics of classification functions and class prediction building procedures, little is known about which data characteristics have impact on the performance of a class prediction model. For instance, diagonal linear discriminant analysis dld a assumes no covariances and hence no correlations between variables and might fail if the data is highly correlated. On the other hand, linear discriminant analysis (LDA) assumes a common covariance matrix for the classes and thus to some extent, accounts for correlations (). In addition, pern alized regressions like ridge, lasso, elastic net are capable to handle correlated variables. Support Vector Machine (SVM), though commonly understood as a method of finding the maximum margin hyperplane, may also be seen as a regularization function estimation problem, corresponding to a hinge loss function with a quadratic penalty as that of ridge regression (). And it has been shown by () that if a group of non distinct variables are selected as input variable set, its training time lengthened and the errors become bigger. On the other hand, tree based methods are by nature designed to capture interactions between variables while neural networks might capture other complex structures within a given dataset. Given the above observations, it is obvious that the performance of these functions depends on the characteristics of the data in question. Despite this, the literature on how to choose a classification function for a given dataset is sparse. A common practice is comparing several classification functions and selecting the one with the minimum error rate but this has been pointed by, Tibshirani and Tibshirani (2009) and Varma and Simon (2006) to lead to selection bias. As such, some experimenters adhere to one or a few classification functions irrespective of the dataset, disease or medical question being addressed. While others choose a classification function for their datasets by affinity or familiarity without taking into account the characteristics of such data. A simulation study by shows that correlation is one of the data characteristics that affect the performance of most probabilistic classification functions. In addition showed that correlation structures differ across gene expression data of different etiological diseases. The study by shows that microarray gene expression data characteristics like log 2 fold change of expression values, number of deferentially expressed genes and pairwise correlations between genes are associated to the accuracy of classification functions. However, this study was conducted in real life gene expression datasets, where the magnitude and or direction of association might have been confounded by unobserved data characteristics. In this study, we aim to provide a guideline for making a choice of a classification function for a binary class prediction problem based on observed magnitudes and directions of the data characteristics, using accuracy as a measure of evaluation. We investigate the effect of sample size, proportion of deferentially expressed (DE) genes, genes' variances, log fold changes, pairwise correlations between DE and noisy genes on the accuracy of classification functions using extensive simulations. The remainder of this article is organized as follows: methodology to simulate data, classification functions considered and the building and evaluation of class prediction models are presented in Section 2; Section 3 contains a predictive summary of the results of class prediction models for different simulated scenarios; Section 4 provides an application of our predictive model from the simulated results on real life microarray gene expression datasets and Section 5 presents a discussion.

application to evaluate the predictive ability of the here presented random effects regression model on real life data, eight Affymetrix gene classification in gene expression expression datasets of the 25 non-cancerous datasets described in one of our previous studies () were used. These datasets were selected to include a variety of Array platforms, both class balance and class imbalance number of DE probe sets as well as various sample sizes. Three of these datasets were preprocessed without filtering while the other five were preprocessed and filtered as described by. We quantified the data characteristics studied and presented on as follows i samp size by counting the samples in the study, (ii) prop de by ranking the probe sets using limma () and computing the proportion of DE probe sets based on a log 2 fold change cut off of 1 if the number of DE is !10 or 0.5 otherwise, (iii) variance, was determined as the mean of the variances of all the probe sets (iv) log2FC, computed as the mean log 2 fold changes of the DE probe sets (v) decor r as the mean of the elements of the upper lower triangular of the correlation matrix of the DE probe sets and (vi) other corr was computed as the standard deviation (SD) of the elements of the upper lower triangular of the correlation matrix of non de probe sets. This matrix was computed from all non de probe sets if they were less than 20 000 or a sample of 20 000 from these non de probe sets otherwise. These data characteristics were standardized using the mean and SD of the respective variables from the simulated data. And our model was used to predict the accuracies for all classification functions in each dataset (Supplementary). We then built and evaluated classifiers using the classification functions by splitting the data into 2 3 learning set and 1 3 test set with stratification and a 3-fold inner cross validation on the learning set for parameters optimization. This step was repeated a hundred times, each time predicting the accuracies of classification functions on the learning set using the random effects model and also recording the expected (observed) accuracies on the test set. These predicted and observed accuracies over the 100 repetitions are respectively presented on supplementary and B. To compare the predicted to observed accuracies, and considering that we are interested in the ordering of performance (i.e. determining an optimal function for a given data), we used the ranked base Spearman correlation between the average predicted accuracies and the average observed accuracies. The results of this comparison for each dataset are presented on. The positive correlation values on this figure indicate agreement between our predicted and observed accuracies. Though these correlations are not very high in some datasets, our model more or less determined an optimal classification function for all the datasets except for UC7 where Ridge regression and SVM emerged first instead of fourth as predicted (i.e. 87.5% sensitivity). Nevertheless, the model was able to rule out on which classification(s) will perform poor on a given dataset, with approximately 100% certainty. As expected, the performance of the functions deteriorate on CF (small sample size and low proportion of DE probe sets Dia2 (high class imbalance and small fold changes), UC2 (low proportion of DE probe sets and small fold changes), UC3 (large variances and high correlations) and UC7 (low proportion of DE probe sets. from and supplementary and B, one sees that except on the UC3 data, our model's accuracies are less than or equal to observed accuracies. The model performs well on dataset with large sample sizes and balanced classes (UC2, UC3 and UC5). It attained its lowest performance on Dia2 where there is high class imbalance and hence few samples of the small class in the learning set and on HIV2 and CF datasets with small sample sizes.

discussion we hypothesized that the performance of classification functions on gene expression data depends on sample size, proportion of DE genes, genes' variances, log 2 fold changes between DE genes and magnitude of the pairwise correlation within DE genes and non de genes, and showed their association to the accuracies of ten often used and clinically relevant classification functions using simulations. Additionally, we built a predictive model to determine an optimal classification function among the studied functions using the simulation results. An application of the predictive model on eight non-cancerous real life gene expression datasets predicted optimal function(s) for seven out of the eight and was able to rule out function(s) that will perform poor on almost all the datasets. This model may serve as a guide for choosing a classification function for a given gene expression data. Classification functions have been shown to perform differently across gene expression datasets () and data characteristics have been shown to differ across datasets and are associated to the performance of classification functions (). While sufficient knowledge is available on the properties of most classification functions and procedures to build class prediction models using gene expression data have been outlined by, little is known about data characteristics that accounts for the variability in the performance of classification functions and how to use these characteristics to choose an optimal classification function for a specific dataset. As such, most researchers adhere to specific classification function(s) or randomly choose a classification for their class prediction models irrespective of the disease or data under study. A common practice is to evaluate several classification functions and select the one with smallest misclassification error but this leads to selection bias (). In this study, we outlined data characteristics together with clinically relevant and often used classification functions and investigated their effects on classification performance using simulation studies. Based on these simulation studies, we provided a guide for choosing an optimal classification function for a specific dataset using the data s characteristics and the studied classification functions through a linear random effects predictive model. As a metamodel one would expect it to explain close to 100% of the variance in the simulated data but our predictive model accounts for approximately 70% of the variability in the simulated data. The remaining 30% unexplained variance may be associated to sampling variability stemming from the several (192) random covariance matrices used to generate both learning and test sets as well as the different learning and test sets generated at each iteration. Although we used different classification functions and evaluated these functions using accuracy, our simulation results confirm the findings of Kim and Simon (2011) that classifiers tend to have poor performance on highly correlated data. Our results also agree with those of that correlations, the absolute log 2 fold changes and the number of DE probe sets are associated to the accuracy of a class prediction model. In addition, these results specify clearly the directions of the association and point out the effects of other data characteristics like sample size, genes' variances that were not previously identified most importantly, we have provided a predictive model that can serve as a guide to choose a classification function for a given dataset and its application on eight real life datasets (both filtered and unfiltered) indicated a good predictive ability of the model. Although our model was reasonably good in its prediction on real life data, we want to point out that it might have failed in some datasets because of the following reasons: (i) most of the eight non-cancerous datasets had small sample sizes and splitting these datasets to learning and test sets yielded even smaller sample sizes of the learning sets and hence might have led to poor estimates of the characteristics under study and (ii) the observed accuracies might not be the true accuracies because of the few Bootstrap samples. It could have been better if we had the means to perform several Bootstraps but due to the small sample sizes, the number of independent Bootstrap samples is limited. The fact that our predictions were most often slightly lower than the observed accuracies for almost all classification functions might indicate the general trend that the performance of a model usually decreases on an independent dataset. Hence, our model's predictions might reflect expected accuracies on independent datasets. In the simulated data, we assumed exponential and normal distributions for the variances and pair-wise correlation of non de genes respectively. These distributional assumptions might be violated in some datasets. As such, it will be worth trying different distributions. Also, we used accuracy as a measure of evaluation by minimizing the loss function but in clinical applications, probabilities are more informative than simple yes or no predictions because they quantify the uncertainty of a prediction (). As such, it is worth evaluating these data characteristics on probabilistic classification functions where by the log likelihood function is optimized, this might possibly provide a predictive model that will be most useful in clinical applications. Despite these limitations, our model was found to work well with data containing reasonably large and balanced sample sizes (n ! 30). As such, our results apply to balanced class data. For data with class imbalance some classification functions will have deteriorating performance, for which several solutions are proposed. However, this topic is outside the focus of the current study. In summary, our results serve as a guide to use data characteristics to choose an optimal classification function for a given dataset.
