Motivation: Advantages of statistical testing of high throughput screens include p values which provide objective benchmarks of compound activity, and false discovery rate estimation. The cost of replication required for statistical testing, however, may often be prohibitive. We introduce the single assay wide variance experimental (SAVE) design whereby a small replicated subset of an entire screen is used to derive empirical Bayes random error estimates, which are applied to the remaining majority of un replicated measurements. Results: The SAVE design is able to generate p values comparable with those generated with full replication data. It performs almost as well as the random variance model t test with duplicate data and outperforms the commonly used z scores with un replicated data and the standard t test. We illustrate the approach with simulated data and with experimental small molecule and small interfering RNA screens. The SAVE design provides substantial performance improvements over un replicated screens with only slight increases in cost.

introduction high throughput screening (HTS) provides concurrent testing of large libraries for detecting biological activity among large numbers of compounds. For ease of exposition, we refer throughout to compounds although the points made apply to other screened objects such as small interfering RNAs (siRNAs). Advances in combinatorial chemistry, robotic processing and plate miniaturization have dramatically increased throughput. Large screens of one million compounds or more, termed ultra hts are increasingly common in industry (). This has led to pressure to reduce costs associated with the increased volume of screens techniques that can maintain performance in identifying rare biological events while reducing costs are invaluable in the drug discovery process. Replicated screens, which allow for statistical testing of biological activity, are becoming more common (; http://nsrb.med.harvard.edu/_downloads/ NSRB_newscreener201106.pdf;). Statistical testing offers numerous advantages for detecting active compounds relative to ranking or using biologically motivated thresholds, neither of which takes compound variability into account. With statistical testing, estimates of uncertainty can be associated with activity measurements; power analyses can be performed to determine cost effective number of replicates for future screens; false discovery rate procedures can guide decisions for selecting compounds for follow-up screen validation. Obtaining replicates can be prohibitively expensive, however, particularly for ultra hts (). One way to reduce costs is to use shrinkage statistical tests that combine individual compound variances obtained from replicates with a screen wide estimate of the variance. Developed specifically for studies with few replicates, these methods produce more accurate and more precise variance estimates for statistical testing than do classical methods such as the t test (). The random variance model (RVM), for example, has been shown to be substantially superior to the standard t test () and has been successfully applied to HTS data (). Although the RVM test performs well with as few as two replicates, cost considerations may argue against obtaining even this minimal level of replication for an entire screen. We introduce the single assay wide variance experimental (SAVE) design as a means of generating an RVM error estimate that is based on a small replicated subset of a screen, which can be applied to the remaining majority of un replicated measurements. We show that SAVE produces p value distributions and rank order statistics comparable with fully replicated screens for both simulated and experimental data. SAVE also outperforms the popular Z-score and standard one sample t test. Finally, we provide guidelines for determining the number of compounds and replicates required to produce error estimates for use within the SAVE design. *To whom correspondence should be addressed.  The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals permission soup com
discussion statistical testing and concomitant p values provide an objective way to benchmark false positive and false negative rates in HTS assays. The SAVE method can be used to generate p values that are comparable with those generated with full replication, providing a reasonable alternative when the latter is not possible due to cost considerations. Replication that captures the true variability of the assay rather than only technical variation is the preferred approach (e.g. by replicating sample preparation and by randomly assigning replicated compounds to different well locations across plates). This approach to the minimal replication required of the SAVE design would improve generalizability of results and their validation. As with other statistical approaches, SAVE results are best when data are adequately normalized, which is often not the case with frequently used methods such as the Z-score or normalized percent inhibition activation. As a consequence of not correcting for spatial biases within plates, compound activity with data normalized by these latter methods will appear higher or lower than the true biological signal, depending on where the compound is located on a plate (). An additional problem of using z scores for purposes of generating p values is the incorrect assumption that all compounds share a common population variance (). The incorrectness of the p values associated with z scores can be readily observed by their deviations from a uniform distribution for inactive molecules. This graphical approach can be used to assess SAVE's appropriateness for datasets and normalization methods not addressed in our study; when used with primary screen data of mostly inactive compounds, p values should be approximately uniformly distributed in all but the lower p value range. We demonstrated that SPAWN normalization provides good results in line with statistical expectation; other methods that similarly correct for spatial bias, such as the R score (), may also be considered. The standard t test has been widely criticized for use with high throughput studies because of its low power and poor random error estimation when applied to screens with few replicates ().We showed that SAVE error estimates provided by partial replication are superior to full replication t test estimates. This superiority is reflected in part by the typically larger degrees of freedom associated with the SAVE method. Standard t tests with n  2 provide only one degree of freedom (df), whereas the SAVE t test provides 2a dfs. Shape parameter values (a) of 1 through 3 are common. In the two empirical datasets we examined, for example, a was $1.6, providing 3.2 dfs for the statistical test. Thus, by using the assay wide variance as the random error estimate rather than the individual feature compound error estimates of the standard t test SAVE provides greater statistical power that comes with more dfs, while avoiding the unrealistically 'small denominator' problem of the t test observed in high throughput data (). Obtaining accurate and precise shape estimates is crucial to the success of the SAVE approach because it is the critical variable in estimating the prior variance distribution and because it defines the degrees of freedom used in generating p values (). In general, SAVE shape estimates improve with increasing numbers of plates and replicates but the overall number of plates required is small relative to the number of plates in a typical screen, in particular for ultra hts screens for which the SAVE design is most useful. Finally, although SAVE was created for and tested with HTS assays, the method is readily transferrable to other types of high throughput assays whose variances are distributed according to an inverse gamma distribution (e.g. gene expression microarrays).
