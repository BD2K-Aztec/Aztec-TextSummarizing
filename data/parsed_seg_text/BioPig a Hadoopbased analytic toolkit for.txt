Motivation: The recent revolution in sequencing technologies has led to an exponential growth of sequence data. As a result, most of the current bioinformatics tools become obsolete as they fail to scale with data. To tackle this data deluge, here we introduce the bio pig sequence analysis toolkit as one of the solutions that scale to data and computation. Results: We built bio pig on the Apaches had oop map reduce system and the Pig data flow language. Compared with traditional serial and mpi based algorithms, bio pig has three major advantages: first, bio pigs programmability greatly reduces development time for parallel bioinformatics applications; second, testing bio pig with up to 500 Gb sequences demonstrates that it scales automatically with size of data; and finally, bio pig can be ported without modification on many had oop infrastructures, as tested with Magellan system at National Energy Research Scientific Computing Center and the Amazon Elastic Compute Cloud. In summary, bio pig represents a novel program framework with the potential to greatly accelerate data intensive bioinformatics analysis. Availability and implementation: bio pig is released as open source software under the BSD license at https://sites.google.com/a/lbl.gov/ bio pig Contact: zhong wang lbl gov
introduction advances in DNA sequencing technologies are enabling new applications ranging from personalized medicine to biofuel development to environmental sampling. Historically, the bottleneck for such applications has been the cost of sequencing the estimated cost of sequencing the first human genome (3 GB) completed a decade ago is estimated at $2.7 billion (http:// www.genome.gov/11006943). Current next generation sequencing technologies () have extraordinary throughput at a much lower cost, thereby greatly reducing the cost of sequencing a human genome to 5$10 000 (http://www.genome. gov sequencing costs accessed December 2012). The price hit $5495 by the end of 2012 (http://dnadtc.com/products.aspx, accessed December 2012). With the cost of sequencing rapidly dropping, extremely large scale sequencing projects are emerging, such as the 1000 Genomes Project that aims to study the genomic variation among large populations of humans (1000 Genomes), and the cow rumen deep meta genomes project that aims to discover new biomass degrading enzymes encoded by complex microbial community (). As a result, the rate of growth of sequence data is now outpacing the underlying advances in storage technologies and compute technologies (Moore's law). Taking meta genomic studies as an example, data have grown from 70 Mb (million bases) from termite hindgut () and 800 Mb Tamar wallaby (), to 268 Gb (billion bases) from cow rumen microbio me () within the past 4 years. The recently published DOE Joint Genome Institute's (JGI) sequencing productivity showed that 30 Tb (trillion bases) of sequences were generated in 2011 alone (http://1.usa.gov/JGI-Progress-2011). Data analysis at tera base scales requires state of the art parallel computing strategies that are capable of distributing the analysis across thousands of computing elements to achieve scalability and performance. Most of the current bioinformatics analysis tools, however, do not support parallelization. As a result, this exponential data growth has made most of the current bioinformatics analytic tools obsolete because they fail to scale with data, either by taking too much time or too much memory. For example, it would take 80 CPU years to BLAST the 268 Gb cow rumen meta genome data () against NCBI non-redundant database. De novo assembly of the full dataset by a short read assembler, such as Velvet (), would require computers with 41 TB RAM and take several weeks to complete. Re-engineering the current bioinformatics tools to fit into parallel programming models requires software engineers with expertise in high performance computing, and parallel algorithms take significantly longer time to develop than serial algorithms (). In addition, at this scale of computing hardware failures become more frequent, and most bioinformatics software lack robustness so that once they fail they have to be restarted manually. All these challenges contribute to the bottleneck in large scale sequencing analysis. Cloud computing has emerged recently as an effective technology to process petabytes of data per day at large Internet companies. map reduce is a data parallel framework popularized by Google Inc. to process petabytes of data using large numbers of commodity servers and disks (). had oop is an open source implementation of the map reduce framework and is available through the Apache Software Foundation (http://wiki.apache.org/hadoop). had oop uses a *To whom correspondence should be addressed. y Present address: Amazon Web Services, New York, NY 10019, USA distributed file system (HDFS) that brings computation to the data as opposed to moving the data to the computation as is done in traditional computing paradigms. In had oop node to node data transfers are minimized, as had oop tries its best to perform automatic co-location of data and program on the same node. Furthermore, had oop provides robustness through a job handling system that can automatically restart failed jobs. Because of these advantages, had oop has been explored by the bioinformatics community () in several areas including BLAST (), SNP discovery (), short read alignment () and transcriptome analysis (). In addition, there are solutions that reduce the hurdle to run had oop based sequence analysis applications (). Using had oop requires a good understanding of this framework; however, to break up programs into map and reduce steps, skills in programming languages, such as Java or C, are also required. To overcome the programmability limitations of had oop several strategies have been developed. For example, Cascading is an application framework for Java developers by Concurrent Inc. (http://www.cascading.org/), which simplifies the processes to build data flows on had oop. Apache's Pig data flow language was developed to enable non programmer data analysts to develop and run had oop programs (http://pig.apache.org/). Somewhat similar in nature to SQL, the Pig language provides primitives for loading, filtering and performing basic calculations over datasets. Pig's infrastructure layer consists of a compiler on the user's client machine that turns the user's Pig Latin programs into sequences of map reduce programs that run in parallel on the nodes of the had oop cluster in a similar fashion to how a database engine generates a logical and physical execution plan for SQL queries. In this article, we describe bio pig a set of extensions to the Pig language to support large sequence analysis tasks. We will discuss the design principles, give examples on use of this toolkit for specific sequence analysis tasks and compare its performance with alternative solutions on different platforms. There is a similar framework, seq pig (http://seqpig.sourceforge.net/), developed in parallel to bio pig which is also based on had oop and Pig. We provide a detailed comparison of the two in Section 3.4.

discussion in this work, we present a solution for improved processing of large sequence datasets in many bioinformatics applications. Using only a few core modules, we believe we have demonstrated the usefulness of this toolkit, while its modular design should enable many similar applications that fit in the map reduce framework. bio pig has several advantages over alternative parallel programming paradigm: it is easy to program; it is scalable to process large datasets (with 500 Gb being the largest one tested); and it is generically portable to several examples of had oop infrastructure, including Amazon EC2, without modification. We also noticed several limitations of bio pig most of which likely derived from had oop itself. For example, it is slower than handcrafted MPI solutions. This is due to both the latency of had oops initialization and the fact that generic map reduce algorithms are not optimized for specific problems. For big datasets this limitation may not be a problem, as the time spent on data analysis far exceeds the cost for the start-up latency. Recently, the had oop community has started to address this problem. Certain commercial implementations of map reduce such as IBM's Symphony product, have been developed to reduce had oops start-up latency. Another promising solution is SPARK, which can speed up had oop applications 100 times by using a low latency, in memory cluster computing (). Another issue is computing resource demand. When dealing with huge datasets, bio pig shifts the need from expensive resources such as large memory (1TB RAM) machines and or parallel programming expertise, to large disk space on commodity hardware. For example, a km er read index in bio pig needs
