Motivation: genome wide association studies g was have largely failed to identify most of the genetic basis of highly heritable diseases and complex traits. Recent work has suggested this could be because many genetic variants, each with individually small effects, compose their genetic architecture, limiting the power of g was given currently obtainable sample sizes. In this scenario, bonferroni derived thresholds are severely underpowered to detect the vast majority of associations. Local false discovery rate (fdr) methods provide more power to detect non-null associations, but implicit assumptions about the ex-changeability of single nucleotide polymorphisms (SNPs) limit their ability to discover non-null loci. Methods: We propose a novel covariate modulated local false discovery rate cmf dr that incorporates prior information about gene element based functional annotations of SNPs, so that SNPs from categories enriched for non-null associations have a lower fdr for a given value of a test statistic than SNPs in unenriched categories. This readjustment of fdr based on functional annotations is achieved empirically by fitting a covariate modulated parametric two group mixture model. The proposed cmf dr methodology is applied to a large Crohns disease g was. Results: Use of cmf dr dramatically improves power, e.g. increasing the number of loci declared significant at the 0.05 fdr level by a factor of 5.4. We also demonstrate that SNPs were declared significant using cmf dr compared with usual fdr replicate in much higher numbers, while maintaining similar replication rates for a given fdr cut off in de novo samples, using the eight Crohns disease sub studies as independent training and test datasets. Availability an implementation: https://sites.google.com/site/covmo df dr
introduction large scale hypothesis testing has emerged as a critical component of genetic analysis with the advent of high throughput microarrays (). For example, it is now possible to survey a large number of single nucleotide polymorphisms (SNPs) across the entire genome in an attempt to locate genetic variations associated with trait variability or disease risk. An advantage of large scale genome wide association studies g was is the ability to discover the potential effect of any number of variants across the genome, without making strong a priori hypotheses about the subset of the genome to consider (). A disadvantage is that a large number of false positives may occur when many hypothesis tests are conducted simultaneously (). Consequently, modern g was have adopted a stringent bonferroni derived multiple testing threshold of P 5  10 8 for declaring individual SNP associations significant. Unfortunately, these g was have largely failed to identify substantial portions of the genetic basis of highly heritable diseases and complex traits (). Recent work has strongly suggested this could be because many genetic variants, each with individually small effects, compose their genetic architecture, limiting the power of g was to detect true associations, given currently obtainable sample sizes (). This scenario is especially damaging to power if all SNPs are treated as a priori exchangeable and hence equally likely to be related to the phenotype of interest, an implicit assumption of Bonferroni thresholds and false discovery rate (FDR) control (). Other work has placed an emphasis on characterizing the biological function of genetic variants across the genome (). Typically, this work has focused on understanding how differences in the protein coding region of genes may damage or alter the corresponding protein structure. However, recent efforts have attempted to characterize the potential effect of variants within non-coding elements, which may alter the timing, amount or location of gene expression (). Emerging from this research is a picture of widespread heterogeneity in the potential biological functionality of variants across the genome. A number of researchers have suggested that this heterogeneity of function *To whom correspondence should be addressed translates to association studies, with certain genetic elements or categories of variants containing more or less trait associated variants (). Given this, it is potentially of use to leverage functional annotations or other locus specific covariates to improve gene discovery and replication of associations in de novo samples. Classical multiple comparison procedures, such as the Bonferroni correction, control the family wise error rate f wer or the probability of committing one or more Type I errors in a family of hypothesis tests. These procedures tend to be underpowered in large scale testing paradigms (). In other words, f wer procedures can be excessively conservative when thousands or millions of cases are tested. benjamin i and proposed an alternative approach to Type I error control termed the FDR, defined as the expected proportion of errors among the rejected hypotheses. Variants of their algorithm are applied to p values of test statistics (null hypothesis tail probabilities) from many tests to control FDR to a specified level under various conditions. Efron and developed an extension of FDR called the local false discovery rate (fdr) from an empirical Bayes point of view, defining fdr as the posterior probability that the null hypothesis is true, given the observed test statistic. The empirical Bayes approach to fdr is closely related to the algorithm for FDR control (). These groundbreaking methodologies for controlling multiplicity under large scale hypothesis testing have received widespread attention and development ().proposed a mixture model of non central 2 test statistics, where the probability of being associated with a phenotype (having a non centrality parameter different from zero) depends on multiple covariates proposed an estimator that allows for modulating the fdr of each null hypothesis based on external covariates. If fdr depends on levels of a measured covariate, then the exchangeability assumption implicit in the definition of fdr is not optimal, and sizeable gains in power can be realized by accounting for this dependence (). The key technique to account for the dependence of fdr on the covariate x in the approach of was to bin the data into B sets according to ordered values of x. The assumption was that the influence of x on the posterior probability is nearly constant in each bin if bins are small enough (in practice, B  10 to 20). The fdr is then estimated in each bin, possibly with smoothing across the bins. This approach works best for one covariate and becomes impractical as the number of covariates increases. It has been applied to large scale testing of neuroimaging data (). In prior work, we have developed a scheme to assign gene element based functional annotations for SNPs genome wide which takes into account the locus locus correlations [linkage disequilibrium (LD)] that g was depend on for whole genome coverage (). This ld weighted annotation scheme provides multiple scores for each SNP in several genic categories, including exon, intron, 5 0 untranslated regions (5 0 UTR) and 3 0 untranslated region (3 0 UTR). Scores incorporate not only the category of a given variant but also the categories of all variants for which it is in LD (correlated with). Intergenic SNPs are defined as having zero scores in all functional categories and being 4100 kb away from a protein coding gene, providing a hypothesized 'null' collection. Using these functional annotations and summary statistics from 14 large g was we showed that test statistics resulting from SNPs that are in LD with the 5 0 UTR of genes show the largest abundance of associations, while SNPs in LD with exons and the 3 0 UTR are also enriched. SNPs in LD with introns are modestly enriched and intergenic SNPs show a depletion of associations, relative to the average SNP (). A more detailed description of how the ld weighted genic annotations were produced is given in the Supplementary Materials. This situation is illustrated in, which displays QQ plots of  log 10 transformed p values from a g was of Crohn's Disease (CD) of 51 109 subjects, obtained through a publicly accessible database (). Enrichment for true associations is expressed as a leftward deflection of the QQ plots stratified by genic category, representing an overabundance of low p values compared with that expected under the global null hypothesis of no associations. Leftward deflections are directly related to decreased fdr for a given p value threshold. The 5 0 UTR SNPs are most enriched, followed by exons, 3 0 UTR and introns. Intergenic SNPs are impoverished for true effects. These results were consistent across all assessed phenotypes () and strongly suggest that all SNPs should not be treated as a priori exchangeable for purposes of hypothesis testing but that certain categories are much more likely to show an association. The current article leverages the information available in genic annotation categories for large scale g was hypothesis testing by presenting a novel, fully Bayesian approach for generalized covariate modulated local false discovery rate cmf dr estimation, implemented using a Markov chain Monte Carlo (MCMC) sampling algorithm. Through this approach, we are able to model the influence of a vector of covariates on the distribution of the test statistics and hence on the fdr. Section 2 gives a brief review of fdr () and introduces cmf dr constructed from a Bayesian two group mixture model that incorporates covariates. Section 3 presents the MCMC algorithm for fitting the model and drawing inferences and applies cmf dr to examples involving both simulated and real data. The last section is devoted to a discussion of results and future work.

discussion methods for large scale hypothesis testing that control Type I error rates without being overly conservative are crucial in g was (). It has become increasingly evident that many complex phenotypes and diseases have many genetic determinants, each with small effect (). Hence, traditional f wer correction is too conservative and severely underpowered. FDR () and fdr () have come to be accepted broadly as routine techniques to control for the rate of false positive in large scale hypothesis testing settings in a number of fields. However, even these methods do not account for the vast majority of phenotypic variance explained by common variants (). A problem with these and other multiple testing methods is that all SNPs are treated as exchangeable. In particular, each SNP is given the same a priori probability of being non-null. On the contrary, we () and others () have shown that the functional role of SNPs has a strong impact on the probability of association across a broad array of complex phenotypes and diseases. This work proposes a novel Bayesian approach cmf dr to incorporate a set of important covariates into the fdr under a heteroscedastic model, where the probability of non-null status and the distribution of the test statistic under the non-null hypothesis are both modulated by covariates. The primary advantage of our methodology over traditional fdr methods is that two SNPs with the same z score can have different values of cmf dr if one is in a more enriched category than the other. Hence, by using SNP annotations to modulate fdr, more SNPs can be discovered for a given level of fdr control. In other words, methods such as cmf dr that break the exchangeability assumption are potentially more powerful than traditional fdr methods that assume exchangeability. In the CD example, we discovered 5.4 times as many SNPs un pruned using cmf dr compared with usual fdr for an identical 0.05 cut off. The increase in number of replicated SNPs in de novo subsamples from fdr to cmf dr was even more dramatic. Parameter estimates of covariates can also be biologically informative about the relative functionality of different biological classifications of variants. It is crucial to note that our ld weighted SNP annotations were computed independently of the phenotypes investigated. Thus, modifying the fdr based on information from genic categories does not bias results toward rejecting more null hypotheses. Moreover, the cmf dr methodology is capable of handling any relevant source of information, including, for example the proposed methodology has some drawbacks. First, as currently formulated, it assumes all hypothesis tests are independent. This is not true for SNPs in LD, and our 95% credible intervals are probably too small. Moreover, it remains unclear what impact LD has on FDP control because it may be the case that all or almost all 'tag SNPs' are in partial LD with causal SNPs but are not themselves causal. Correlation across SNPs can be handled, for example, by repeatedly and randomly pruning SNPs for independence before running the MCMC algorithm, by using a discrete Markov random field formulation () or by modeling SNPs simultaneously using, for example, a multivariate mixed effects model framework (). We have implemented a random pruning option available with the R code distribution. Second, it may be the case for some applications that the gamma distribution does not fit the tail probabilities of the non-null distribution well. We have used other distributions (e.g. the skewed generalized normal) and are currently developing a non-parametric alternative that produces flexible fits to tail probabilities. Although non-parametric estimates of the non-null density avoid bias from lack of model fit, parametric alternatives can be more powerful if the fit is adequate. Finally, it appears from simulations that the cmf dr methodology can be overly liberal in scenarios where 1 is close to 0. Care must therefore be taken when applying cmf dr in these circumstances.
