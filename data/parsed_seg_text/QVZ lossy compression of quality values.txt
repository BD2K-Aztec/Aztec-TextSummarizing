Motivation: Recent advancements in sequencing technology have led to a drastic reduction in the cost of sequencing a genome. This has generated an unprecedented amount of genomic data that must be stored, processed and transmitted. To facilitate this effort, we propose a new lossy com-pressor for the quality values presented in genomic data files (e.g. fast q and SAM files), which comprise roughly half of the storage space (in the uncompressed domain). Lossy compression allows for compression of data beyond its lossless limit. Results: The proposed algorithm qv z exhibits better rate distortion performance than the previously proposed algorithms, for several distortion metrics and for the lossless case. Moreover, it allows the user to define any quasi convex distortion function to be minimized, a feature not supported by the previous algorithms. Finally, we show that qv z compressed data exhibit better performance in the genotyping than data compressed with previously proposed algorithms, in the sense that for a similar rate, a genotyping closer to that achieved with the original quality values is obtained. Availability and implementation: qv z is written in C and can be downloaded from https://github. com/mikelhernaez/qvz.

introduction there has been a recent explosion of interest in genome sequencing, driven by advancements in the sequencing technology. Although early sequencing technologies required years to capture a 3 billion nucleotide genome (), genomes as large as 22 billion nucleotides are now being sequenced within days () using next generation sequencing technologies (). Further, the cost of sequencing a human length genome has dropped from billions of dollars to merely $4000 (http://systems.illumina.com/systems/hiseq-x-sequencing-system.ilmn) within the past 15 years (). These developments in efficiency and affordability have allowed many to envision whole genome sequencing as an invaluable tool to be used in both personalized medical care and public health (). In anticipation of the storage challenges that increasingly large and ubiquitous genomic datasets could present, compression of the raw data generated by sequencing machines has become an important topic. The output data of the sequencing machines is generally stored in the widely accepted fast q format (). A fast q file dedicates four lines to each fragment of a genome (a 'read') analyzed by the sequencing machine. The first line contains a header with some identifying information, the second lists the nucleotides in the read, the third is similar to the first one and the fourth lists a 'quality value' (also referred to as quality score) for each nucleotide. The quality values are generally stored using the Phred score, which corresponds to the particular number Q  10log 10 P, where P is an estimate (calculated by the base calling software running on the sequencing machine) of the probability that the corresponding nucleotide in the read is in error. These scores are commonly represented in the fast q file with an ASCII alphabet 33 : 73 or 64 : 104, where the value corresponds to Q  33 or Q  64, respectively. In addition, the information contained in the fast q files is also found in the SAM files (), which store the information pertaining to the alignment of the reads to a reference. Quality values, which comprise more than half of the compressed data, have proven to be more difficult to compress than the reads (). Thus, generating better compression tools for quality values is crucial for reducing the storage required for large files. Unlike nucleotide information, the quality values generated by sequencing machines tend to exhibit predictable behavior within each read. Strong correlations exist between adjacent quality values as well as the trend that quality values degrade drastically as a read progresses (). There is also evidence that quality values are corrupted by some amount of noise introduced during sequencing (). These features are well explained by imperfections in the base calling algorithms, which estimate the probability that the corresponding nucleotide in the read is in error (). Further, applications which operate on reads (referred to as 'downstream applications') often make use of the quality values in a heuristic manner. This is particularly true for sequence alignment algorithms () and single nucleotide polymorphism (SNP) calling (), the latter having been shown to be resilient to changes in the quality values (in the sense that, in general, little is compromised in performance when quality values are modified () (http://www.illumina.com/documents/products/ white papers white paper data compression pdf. Based on these observations, lossy (as opposed to lossless) compression of quality values emerges as a natural candidate for significantly reducing storage requirements while maintaining adequate performance of downstream applications. While rate distortion theory provides a framework to evaluate lossy compression algorithms, the criterion under which the goodness of the reconstruction should be assessed is a crucial question. It makes sense to pick a distortion measure by examining how different distortion measures affect the performance of downstream applications, but the abundance of applications and variations in how quality values are used makes this choice too dependent on the specifics of the applications considered. These trade-offs suggest that an ideal lossy compressor for quality values should not only provide the best possible compression and accommodate downstream applications, but it should provide flexibility to allow a user to pick a desired distortion measure and or rate. In this work, we present such a scheme which we call qv z ('Quality Values Zip'), which achieves significantly better rate distortion performance than any of the existing algorithms. Specifically, the proposed algorithm obtains up to four times better compression than previously proposed algorithms for the same average distortion. In addition, qv z achieves lossless compression. Moreover, we analyze the effect of qv z on the genotyping and show that better results are obtained than with the previously proposed algorithms. Finally, we present some preliminary results that suggest that lossy compression could potentially improve the genotyping with respect to the uncompressed data. This may be due to the inherently noisy nature of the quality values, in ways that will be thoroughly investigated in future work.

conclusion in this work, we have presented qv z a new lossy compression algorithm for quality scores in genomic data. The proposed algorithm can work for several distortion metrics, including any quasi convex distortion metric provided by the user, a feature not supported by the previously proposed algorithms. Moreover, it exhibits better rate distortion performance. Unlike some of the previously proposed algorithms, qv z also allows for lossless compression and a seamless transition from lossy to the lossless with increasing rate. Moreover, we have shown that in comparison to previously proposed lossy algorithms, using qv z compressed data achieves genotyping performance closer to that obtained with uncompressed quality values, for similar compression rates. Finally, we have obtained some preliminary and promising results which suggest that lossy compression could be beneficial not only for storage and transmission but also for boosting performance in downstream applications. The extent of this phenomenon, the relation between the distortion criterion, the compression rate, the characteristics of the noise in the quality values and the resulting performance boosts are due further investigation.
