Motivation: Sequence-variation analysis is conventionally performed on mapping results that are highly redundant and occasionally contain undesirable heuristic biases. A straightforward approach to single-nucleotide polymorphism (SNP) analysis, using the Burrowsâ€“Wheeler transform (BWT) of short-read data, is proposed. Results: The BWT makes it possible to simultaneously process collections of read fragments of the same sequences; accordingly, SNPs were found from the BWT much faster than from the mapping results. It took only a few minutes to find SNPs from the BWT (with a supplementary data, fragment depth of coverage [FDC]) using a desktop workstation in the case of human exome or transcrip-tome sequencing data and 20 min using a dual-CPU server in the case of human genome sequenc-ing data. The SNPs found with the proposed method almost agreed with those found by a time-consuming state-of-the-art tool, except for the cases in which the use of fragments of reads led to sensitivity loss or sequencing depth was not sufficient. These exceptions were predictable in advance on the basis of minimum length for uniqueness (MLU) and FDC defined on the reference genome. Moreover, BWT and FDC were computed in less time than it took to get the mapping results , provided that the data were large enough. Availability and implementation: A proof-of-concept binary code for a Linux platform is available on request to the corresponding
IntroductionSince the advent of so-called next-generation DNA sequencers (NGSs), which rapidly and cost-effectively generate billions of short reads, large-scale analysis of sequence data of a few-hundred giga base pairs (Gbp), requiring a large computational resources, is not uncommon anymore. The first step to extract biologically meaningful information from sequence data often involves analysis of the variation (mutation) of that data in comparison with a reference genome sequence data. Billions of short reads are first mapped onto the reference genome, and unambiguous and recurrent mismatches between the short reads and the reference genome are identified as candidate mutations (). This line of approach is hereafter referred to as the mapping-based approach. Although it is the most appreciated and most commonly used approach, it has the following basic weak points. (i) The computation of mapping is highly redundant because of large sequencing depth (typically ranging from 30 to 100). (ii) Some mutations can be lost by mapping tools because such tools use certain heuristics of their own to resolve mapping ambiguities. (iii) It is not easy to switch from one reference genome to another after the computation of mapping has been completed. To address these weak points, an alternative solution, the dictionary-based approach, is proposed. The short-read data are converted into a dictionary of reads, so that numbers of occurrences of any sequence in the short-read data are immediately obtained. Then,mutations can be inferred on the basis of these numbers by means of querying genomic subsequences with and without the mutations (). The dictionary can be implemented efficiently by means of the BurrowsWheeler transform (BWT) () [a.k.a. FM index (] because it is simple and particularly suitable for DNA sequences. Although the proposed approach appears to be too navenave, it has the following potential advantages. (i) Redundancy due to deep sequencing coverage is efficiently managed by the dictionary of reads. (ii) The dictionary of reads does not suffer information loss or heuristic bias because it is essentially constructed by means of sorting the data in alphabetical order. (iii) It is easy to switch from one reference genome to another one after the dictionary of reads is constructed. However, the following issues of the proposed approach remain to be addressed. 1. The construction of BWT for large data (more than 100 Gbp) is very time-consuming even with the fastest known algorithm, BCRext (). 2. A large number of occurrences in the short-read data of a genomic subsequence with a candidate mutation do not necessarily imply the existence of the mutation because some of them might be derived from different genomic regions with similar subsequences. 3. To get useful information from the dictionary of reads, it is necessary to prepare effective queries that are likely to contain candidate mutations. Namely, it is necessary to locate genomic positions with a significant chance of finding mutations.To address these issues, the following algorithm and concepts are introduced in this study. 1. BWT/WT, a modified and parallelized BCRext algorithm for computing the BWT of reads. 2. The minimum length for uniqueness (MLU), a simple criteria for evaluating the uniqueness of the subsequence. 3. The fragment depth of coverage (FDC), an estimate of sequencing depth of coverage on the basis of exact matching of read fragments at single-base resolution.
Dictionary-based approach
NotationsFor any DNA sequence, A  a 0 a 1    a L1 ; Ai; j denotes the subsequence a i a i1    a j for 0 i j < L, and A denotes the reverse complement. Let C 0 ; C 1 ;. .. ; C I1 be the DNA sequences of chromosomes (or contigs) in the reference genome, where I is the number of them.the concatenations on the positive and negative strand in mutually reverse order, where $ denotes a sentinel (punctuation) symbol. The whole-genome sequence on both strands is represented byand the BWT, denoted by TG  , is used as the dictionary of the reference genome, where the alphabetical order $ < A < C < G < T < N is assumed and $ does not to match any other symbols, including itself. Let L be the total length of the genome on a single strand including the sentinels; namely, L  j G  j  j G  j. For a genomic coordinate, 0 x < L, the reference base at x on the positive (negative) strand is given by G  x (G  x), where x  2L  1  x. Similarly, a genomic subsequence on the positive (negative) strand of length equal to ' with the left (right) end at x is given by G  x; '  G  x; x  '  1 (G  x; '  G  x; x  '  1). Let r 1 ; r 2 ;. .. ; r J be the DNA sequences of the short reads, and R  r 1 $r 2 $    $r J $ be the concatenation, where J denotes the number of reads. The BWT of R, denoted by TR, is used as the dictionary of the reads. For any DNA sequence, w, the number of occurrences of w in the reference genome and that in the short-read data, denoted by N G  w and N R w, are immediately computed from TG   and TR, respectively, by using rank functions (
MLUThe MLU at x in the positive (negative) direction, denoted by k  x (k  x), is defined as the minimum length of the subsequence with the left (right) end at x, such that the subsequence appears only once in both strands of the genome. Namely, k 6 x  min f' j N G  G 6 x; '  1g:MLU is closely related to the suffix array (SA) and the longest common prefix (LCP) array () and is thereby computed efficiently (see Section 3). MLU varies with position on the genome and mostly takes a moderate value, except in the case of repetitive or duplicated regions. For example, MLU is 40 or less (more than 100) in 88.2% (3.9%) of the reference human genome, hg19, excluding long runs of N with more than 500 Kbp.
FDCWhen a genomic subsequence with the left end at x is taken as a query, namely, w  G  x; ', the number of occurrences of w in short-read data, N R w, reflects the sequencing depth at x, provided that the length ' is properly chosen. If ' is less than k  x, the number is clearly overestimated because some of the occurrences come from different positions. If ' is equal to k  x, the number is expected to be a proper estimation of the sequencing depth because of the uniqueness condition. However, it is in fact prone to be affected by occasional contributions from reads [with sequencing errors or single-nucleotide polymorphisms (SNPs)] derived from different genomic regions with similar sequences. Therefore, ' somewhat larger than MLU should be taken. On the other hand, if ' is too large, the number is underestimated because of the finite read length. The FDC at x in the positive (negative) direction, denoted by d  x (d  x), is defined as the number of occurrences of w in the short-read data when the length is chosen, such that '  k  x  a ('  k  x  a) for a small positive constant, a. Namely, d 6 x  N R Gx; ' 6 x; ' 6 x  k 6 x  a:As is clear from the above definitions, FDC has single-base resolution and is sensitive to direction (). For example, FDCs in. Basic concept of dictionary-based SNP analysis. Given a SNP candidate on the genome, appropriate genomic subsequences with and without the SNP, namely, q1 and q0, are chosen as queries. The numbers of occurrences of the queries in short read-data, n1 and n0, are immediately obtained from the dictionary of short reads. The candidate is evaluated on the basis of n1 and n0 as follows. (a) If both n0 and n1 are sufficiently large and almost equal, the candidate is likely to be a heterozygous SNP. (b) If n0 is sufficiently small, and n 1 is sufficiently large, the candidate is likely to be a homozygous SNP. (c) Conversely, if n0 is sufficiently large, and n1 is sufficiently small, the candidate is likely to be false both directions suddenly drop in the vicinity of a SNP in a characteristic pattern (). Although FDC is an approximation of sequencing depths, it is progressively underestimated as MLU increases because of the finite read length. In particular, when MLU is greater than the read length, FDC is zero and useless. Otherwise, on the assumption that the sequencing errors are randomly distributed according to a Poisson distribution with an average frequency of r E per base, the underestimation factor is given by 1  ' 6 x=' R e rE' 6 x , where ' R denotes read length.
Overall schemeThe reference genome sequence and the short-read data are separately transformed into dictionaries (BWTs). The MLU is computed from the dictionary of the reference genome sequence alone. The FDC is computed from the dictionary of the short-read data and the MLU. These precomputations are necessary for genetic-variation analysis downstream (). In contrast, as for the conventional mapping-based approach, the reference genome sequence is formatted into a convenient form, which is sometimes implemented by means of BWT (). The short reads are mapped onto the reference genome using the formatted data, and the results are sorted and indexed according to the positions on the genome (). These precomputations are necessary for downstream analysis (). In contrast to the mapping-based approach (by which reads are treated individually), the dictionary-based approach is expected to be efficient in the downstream-analysis phase because collections of read fragments with the same sequences can be processed simultaneously.
Methods
Calculation of BWTsContigs separated by long runs of N (500 Kbp or more), C 1 ; C 2 ;. .. ; C I , are extracted from the reference genome sequence, and the concatenated genome sequence (on both strands), G  , is obtained. In the case of the reference human genome, hg19, I  47 contigs and G  of length 2L ' 5:75  10 9 are thus obtained. The BWT and SA of G  are calculated using the induced-sorting algorithm (). The SA-IS code presented inis modified, so that it can treat data larger than 4 Gbp and cope with multiple occurrences of sentinels. Large short-read data of more than 100 Gbp is beyond the scope of the induced-sorting algorithm. The BWT of short reads is incrementally calculated from smaller partial BWTs in a cache-oblivious manner, which basically follows the BCRext algorithm (). The kth partial BWT is defined as the BWT of k-suffixes of reads, where the k-suffix is a suffix of length k (if the read length is larger than k) or the entire read otherwise. Both of the partial BWTs and the remaining prefix data are compactly encoded into wavelet trees (), resulting in a memory requirement of about 0:6N GB for data of N Gbp. The incremental calculations are executed in parallel according to the first bases of the suffixes, thus accelerating the calculation three to four times. The modified BCRext algorithm is hereafter referred to as 'BWT/WT'.
Calculation of MLUThe SA of both strands of the reference genome,is sorted alphabetically (). The LCP array of the genome,is an array of integers, where LCP G  i is the length of the LCP of suffixes G  j; 2L with j  SA G  i and j  SA G  i  1 (). The LCP can be efficiently calculated from the SA and its relatives (for 0 x < L, where SA 1 G  denotes the inverse suffix array, i.e. the inverse permutation of the SA. All of the values of the MLU are compactly encoded into a bit array as follows. Since G  x  1; k  x  1 occurs exactly once in both strands of the genome, its leftward one-base extension, G  x; k  x  1  1, occurs at most once in both strands. This fact implies that k  x k  x  1  1 0 x < L;and hence 2x  k  x is strictly increasing with x. Similarly, k  x k  x  1  1 0 x < L;Fig. 2. FDC has base-level resolution and is sensitive to direction. (a) The FDCs at x in the positive and negative directions, d  x  and d  x , are defined as the number of occurrences in the short-read data of genomic fragments on the positive and negative strands starting at x with length '  x  and '  x , which are chosen to be larger than MLU: ' 6 x   k 6 x   a for a small positive constant a. (b) The FDC in the positive (negative) direction drops at a SNP position and in its left (right) vicinity because of the difference between the reference genome and short reads at the SNP position. The widths of drops are determined by the MLU and a. The depths of drops are halved when the SNP is heterozygousUltrafast SNP analysisand hence 2x  k  x is strictly decreasing with x. On the basis of these implications, the MLU values are compactly encoded into bit array M of length 4L as follows:My  1 y  2x  k  x for some 0 x < L; 1 y  2x  k  x for some 0 x < L; 0 otherwise:Conversely, they are immediately decoded from M as follows:for 0 x < L, where select M x (i.e., the select function on M) gives the index of the xth occurrence of a set bit ('1') in M (Gonz lez et al., 2005). The select function is efficiently calculated using the hierarchical binary strings (HBS) ().
Calculation of FDCThe SA of the short reads (only in the direct strand),is a permutation of 0; 1;. .. ; N  1, such that the sequence of suffixes, RSA R i; N  1 i0;1; .
.. ;N1 ;is sorted alphabetically, where N is the total length of the short-read data including sentinels, namely, N  j R j. For any DNA sequence, w, a collection of all occurrences of w in the short-read data is represented by the SA interval of w (), I R w; I R w, which is defined by I R w  minf0 i < N j w is a prefix of RSA R i; N  1g; (12)I R w  maxf0 i < N j w is a prefix of RSA R i; N  1g: (13)The initial value for the empty sequence (w  e) is given by Ie; Ie  0; N  1. It is then recursively calculated as follows.I R aw  Ca  rank TR a; I R w  1;for a  A; C; G; T and N, where Ca is the number of bases in R that are lexicographically smaller than a, and rank TR a; i is the number of occurrences of a in TR0; i  1. The rank function is efficiently computed using the HBS. Then, FDCs are given by the lengths of the SA intervals as follows:for 0 x < L. Therefore, FDCs are calculated using Equations (1416). Moreover, the calculation is accelerated according to an idea similar to (). It is common for adjacent positions, x and x61, to have the subsequences, G 6 x; ' and G 6 x61; ', in Equation (1), such that they have the same end position, namely x6k 6 x  x616k 6 x61. Then, d 6 x61 is reducible in the sense that it is immediately given bywith the known values of I R and I R that are obtained during the calculation of d 6 x using Equations (1416).
Search for SNP candidatesTwo methods for searching for SNP candidates, namely the dropscan method and the step-scan method, are proposed in the following. As for the drop-scan method, SNPs are searched for only around significant drops in the precomputed FDC (since they are unlikely to be found elsewhere). As for the step-scan method, the whole genome is exhaustively scanned by a sliding window of a fixed size, and reads with exactly matching subsequences around the window on either side are collected, and their extensions into the window are examined to find any SNPs therein. The latter method does not require the precomputed FDC.
Drop-scan methodA simple criterion for genomic coordinate x to be a significant leftward (rightward) drop in the positive (negative) direction iswhere 0 < r < 1 is a small constant, referred to as drop ratio. However, random fluctuations of the FDC may sometimes satisfy the criteria; besides, reads that are derived from different homologous genomic regions and altered by SNPs or sequencing errors may affect the criteria. The criteria are therefore made more stringent by the additional following procedures (). 1. Take s  G 6 x61; ' 6 x61, a seed (of a sufficient length) adjacent on the right (left) of x, and collect all of its occurrences in the forward (reverse) reads. 2. Collect all possible leftward (rightward) extensions beyond x in a sufficient length, '  x  1  1. 3. Align the extensions with the reference genome using a fast dynamic programming (DP) algorithm (). 4. Select valid extensions that have at most n e mismatches or small indels (insertions or deletions), where n e is a positive constant integer. 5. Find any mismatches or small indels that are repeatedly observed in the alignments of the valid extensions in at least n m cases and with a relative frequency of at least r, where n m is a positive constant integer. 6. Filter out any mismatches or small indels that are not found consistently from both strands.
Step-scan methodSNP candidates are located by using a sliding window of fixed length W along the genome in the following steps ().. Drop-scan method. A leftward drop of the FDC at x in the positive direction, such that d  x  < 1  rd  x  1 is located by scanning the whole genome, where 0 < r < 1 is the drop ratio. For such x, a seed is taken as a genomic subsequence on the positive strand started at x  1 and with length equal to '  x  1. The occurrences of the seed in the short read-data and all possible leftward extensions are collected by using the dictionary of short reads. The extensions (including x and beyond) are aligned with the reference genome sequence, and repeatedly observed mismatches or small indels are extracted. Likewise, rightward drops of the FDC in the negative direction are considered. The mismatches and indels consistently extracted from both directions are then obtained as SNP candidates 1. Take a window of length W with the left end at x  kW=2 for k  0; 1; 2;. .. ; b2L=Wc. 2. Take s  G  x  W; '  x  W (s  G  x  1; '  x  1), a seed (of a sufficient length) adjacent on the right (left) side of the window and collect all occurrences of s in the forward (reverse) reads. 3. Collect all possible leftward (rightward) extensions into the window. 4. Align the extensions with the reference genome using a fast DP algorithm. 5. Select valid extensions that have at most n e mismatches or small indels. 6. Find any mismatches or small indels that are repeatedly observed in the alignments of the valid extensions in at least n m cases and with a relative frequency of at least r. 7. Filter out any mismatches or small indels that are not found consistently from both strands.
Results and discussionThe dictionary-based methods were implemented in C and Perl for proof-of-concept experiments. The test data included actual biological data downloaded from public websites and simulation data ().
Precomputation timeThe BWT and MLU of both strands of the reference human genome sequence (hg19) were calculated once, and the calculation results were stored and reused. The total computation time was 2 h and 16 min by a single core of an Intel Xeon CPU (X7560, 2.3 GHz); and the maximum memory usage was 71.3 GB. The precomputation for short-read data was performed in parallel; 10 threads were used for the exome and transcriptome sequencing data, and 24 threads were used for the wholegenome sequencing data. In the case of transcriptome and genome sequencing data, the computation time was much shorter than that required by the conventional mapping-based method (). Although the computation time of BWT/WT was almost linear in relation to the data size, the computation time of FDC was mostly dominated by the length of the reference genome and was not much affected by the data size owing to the nature of the dictionary. Thus, the precomputation time for the smaller data size was largely occupied by the latter time.
Example of FDC plotsAlthough FDC plots, as illustrated in, are informative and meaningful, they are usually degraded by noises induced by randomly matched sequences. However, as parameter a increases, the noise reduces rapidly, while the signal degrades slowly. Thus, a  3 was chosen tentatively so as to make the plots clear and sensitive. In the case of the exome data, the signals were apparently localized around captured regions and clearly dropped at SNPs. An example of actual biological data (E1) is shown in. Similarly, in the case of transcriptome data, the signals were apparently localized inside exons; besides, considerable amounts of signals and noises, seemingly to reflect complex alternative splicing and other miscellaneous transcriptional activities, were also observed. In the case of theActual biological data were downloaded from the NCBI Sequence Read Archive. They were paired-ended reads of human samples obtained by Illumina Genome Analyzer II, IIx and HiSeq 2000. The simulation data were generated from the human reference genome sequence (hg19) around NCBI RefSeq coding exons with randomly introduced homozygous and heterozygous SNPs (0.1%) and sequencing errors (1%), each of which consists of single-base substitutions (98%), insertions (1%) and deletions (1%); the insertion lengths of the paired reads were assumed to be distributed normally with 300-bp mean and 20-bp standard deviation.. Step-scan method. The whole-genome region is scanned by a sliding window of length W at every W =2 bp position. Two adjacent seeds on the right and left sides, starting, respectively, at x  W and x  1, are taken on the positive and negative strand. Their lengths are, respectively, equal to '  x  W  and '  x  1. The occurrences of the right-hand (left-hand) seed in the short-read data and all possible leftward (rightward) extensions into the window are collected by using the dictionary of short reads. The extensions are aligned with the reference genome sequence, and consistent mismatches or small indels are extracted as SNP candidates) was used for the exome and genome data, and TopHat 2.0.7 () was used for the transcriptome data; conversion into BAM files, sorting, and merging was done by SAMtools 0.1.19 (). A Linux workstation with a single CPU (Intel Core i7-4930 K, 3.4 GHz) and 64-GB memory was used with 10 threads in parallel for the exome and transcriptome data; and a Linux server with double CPUs (Intel Xeon X7560, 2.3 GHz) and 256-GB memory was used with 24 threads in parallel for the genome data.
Ultrafast SNP analysiswhole-genome data, copy-number variations and loss of heterogeneity were also observed as expected.
Search for SNP candidatesParameters n e  8; n m  2; r  0:2 and W  40 were chosen experimentally using simulation data (S1), so as to trade-off computation time and sensitivity. The sensitivities of finding homozygous and heterozygous single-base substitutions were 94.3% and 93.6% by the drop-scan method and 94.5% and 94.3% by the step-scan method. They were slightly smaller than 95.8% and 95.4% attained by a conventional mapping-based method, GATK (). The primary reason for slightly lower sensitivity of the two proposed methods is thought to be the fact that the methods do not use full lengths of reads (with paired-end information); instead, they only use fragments of reads. It is expected that the sensitivity is prone to decrease as MLU increases. In fact, the sensitivity of the drop-scan method for the simulated exome data was 99.2% when MLU was at most 40 (93.3% of the cases), while it decreased to 26.9% when MLU was >40 (6.7% of the cases). Times for searching for SNPs in the whole human genome by different methods are compared in. As expected, the drop-scan method is much faster than the step-scan method, and the latter is much faster than the conventional mapping-based method. The agreement of the results given by the proposed and conventional methods is assessed as follows. It is known that GATK is one of the most-sensitive mapping-based tools and that the ratio of agreement of results given by different tools is not generally high (). Therefore, the result given by the proposed method (the drop-scan method) and that given by GATK confidently (after Q-filtering, where the quality value was not <300) in less repetitive (with M-filtering, where the MLU was at most 40 bp) and deeply covered (with D-filtering, where FDCs around the drops were at least 10) regions were compared (). As indicated by the relative sensitivity and specificity of the proposed method on the assumption that the results given by GATK were correct, high ratios of agreement were obtained.Precomputed FDC was used for the drop-scan method but not for the stepscan method. GenomeAnalysisTK 2.1-8 () and Picard tools 1.77 (http://picard.sourceforge.net/) were used. Each data were computed in parallel with the same number of threads by the same computer as for Table 2.. MLU and FDC plots in a captured region of exome sequencing data (E1). The MLUs on the positive and negative strand, k  x  and k  x , along the right-hand ordinate (in length), and the FDCs on the positive and negative strand, d  x  and d  x , along the left-hand ordinate (in count), are plotted against the relative coordinate in a genomic region (chr17:3 100 001-3 102 500 in hg19). The SNP candidates, located at sharp drops of FDCs, are indicated by downward arrowsNumbers indicate the number of SNP candidates (only for single-base substitutions) found by each method and those filtered under specified conditions. Q-filter removed those with a quality value <300; M-filter removed those with MLU > 40 and D-filter removed those with FDC around the drops <10. The mappingbased results were obtained by GATK 2.1-8, and the dictionary-based results were obtained by the drop-scan method. The Q-filtering eliminates the SNP candidates to which GATK does not give high confidence. The M-filtering eliminates those in regions where the drop-scan method is known to be insensitive. The ratios of the numbers in the third and fourth columns are given in the fifth column; they are larger in the exome and transcriptome data than in the genome data (see text). The D-filtering eliminates those in regions where the sequencing coverage is not deep enough. a Final results given by the dictionary-based method. b Final results given by the mapping-based method, tentatively assumed to be correct. (TP: true positives, FP: false positives). The M-filtering eliminates the SNP candidates in regions where the dictionary-based methods are known to be insensitive. The reduction ratios (in the fifth column in) are generally larger for the exome and transcriptome data and smaller in the genome data in comparison to 88.2%, which is the proportion of genomic regions where MLU is 40 or less. This result seems to be reasonable because the former data mostly consist of reads from gene regions where MLU generally takes smaller values and because the latter data also contain many reads from repetitive regions where MLU is very large and mutation rate (including SNPs) is relatively high. Thus, the ratio of useful SNPs that are predicted to be undetectable by the drop-scan method is estimated to be around 10%. Similar results to those given inwere also obtained by the step-scan method (Supplementary Information, Supplementary). The step-scan method generally gives higher relative accuracy than the drop-scan method for exome and genome sequencing data but not for transcriptome data. The primary reason for this lower accuracy in the latter case is thought to be the fact that the sliding windows often cross the exon boundaries, making it impossible to start effective searches from either side of the windows.
Conclusion and future worksIn contrast to the conventional mapping-based approach, a dictionary-based approach to sequence analysis is proposed. It is expected to be efficient because the dictionary (BWT) of short-read data makes it possible to simultaneously process collections of read fragments with the same sequences. In particular, SNPs were found from the dictionary much faster than from the mapping results. It was experimentally shown that it took only a few minutes to find SNPs from the BWT and FDC using a desktop workstation in the case of human exome or transcriptome sequencing data and 20 min using a double-CPU server in the case of human genome sequencing data. However, the use of read fragments (instead of full-lengths of reads with paired-end information) sometimes leads to sensitivity loss. Such cases are predictable in advance on the basis of MLU and are estimated to generally occupy about 10% of the cases; therefore, the proposed approach should be taken only in the majority of other cases. The SNPs obtained by the proposed methods mostly agreed with those obtained by a time-consuming state-of-the-art tool, except for the cases in which loss of sensitivity was predicted in advance on the basis of MLU or sequencing depth was estimated to be low on the basis of FDC. The dictionary of short-read data was computed in less time than it took to map them onto a reference genome and to sort the mapping results along the genome, provided that the data was large enough. It was free from heuristic bias or information loss, unlike the mapping results. Since it does not depend on any particular reference genome sequence, the dictionary-based approach will be advantageous when multiple reference sequences are available. Although this study focuses exclusively on SNP analysis, it is clear that the proposed approach is generally applicable to many other kinds of sequence analysis. In particular, straightforward and promising applications include:
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
K.Kimura and A.Koike at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
