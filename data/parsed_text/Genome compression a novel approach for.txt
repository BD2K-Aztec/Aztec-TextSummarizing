Motivation: Genomic repositories are rapidly growing, as witnessed by the 1000 Genomes or the UK10K projects. Hence, compression of multiple genomes of the same species has become an active research area in the past years. The well-known large redundancy in human sequences is not easy to exploit because of huge memory requirements from traditional compression algorithms. Results: We show how to obtain several times higher compression ratio than of the best reported results, on two large genome collections (1092 human and 775 plant genomes). Our inputs are variant call format files restricted to their essential fields. More precisely, our novel Ziv-Lempel-style compression algorithm squeezes a single human genome to $400 KB. The key to high compression is to look for similarities across the whole collection, not just against one reference sequence, what is typical for existing solutions.
INTRODUCTIONThe DNA sequencing technology has become so affordable that there are several large-scale projects in which at least hundreds of individuals of some species are sequenced. From many perspectives, including the advent of personalized medicine, the Homo sapiens data belong to the most interesting, and this is the reason why large projects like the 1000 Genomes Project (1000GP) () and the UK10K Project (http://www.uk10k.org), with thousands of human genomes sequenced so far, were initiated. Among such projects, the most ambitious perhaps is the Personal Genome Project (PGP) (), with genomes of 100 000 individuals as the anticipated outcome. Large repositories are built not only for human genomes, to mention 1001 Genomes Project (1001GP), with Arabidopsis thaliana genetic variation (http://www. 1001genomes.org/about.html). It is a well-known fact that two organisms of the same species are highly similar; it was estimated that the genomes of two persons are identical in 99.5% (). The huge amount of data obtained in the large-scale projects demands efficient ways of storing them. Taking into account the high similarity of organisms, it becomes obvious that some compression method may be effectively applied. Compression of single genomic sequences is hardly efficient, as the best obtained compression ratios achieve a factor of 4 or 5 () only. When realized that instead of the complete genomic sequence, storing only differences between it and some referential sequence is enough, the task became easier. In their seminal paper,showed how to store the description of variations between James Watson's (JW) genome and a referential genome in only 4.1 MB. However, the authors' prior knowledge was not only the reference sequence but also the single nucleotide polymorphism (SNP) map. As the input, they took the information about the SNPs and insertion or deletion (indels) variations between JW genome and referential genome, and compressed these data using some clever, but simple, techniques. Comparing with $3.1 Gbases of human genome, this means $750-fold compression. In the following years, a number of articles on relative compression of genomes were published (). In all the articles, the input sequences were complete genomes, not differences between genomes and a reference genome. This complicates the compression problem, as it is necessary to find the differences between genomes without any prior knowledge and without a database of variants (i.e. SNP and indel database). The most successful of the algorithms seems to be GDC (), which differentially compressed a collection of 69 human genomes from Complete Genomics Inc. to 215 MB (3.1 MB per individual). It is based on the ZivLempel () paradigm and finds approximate matches between the genome sequences. Recently,showed how to improve the technique from, compressing the JW genome to 2.5 MB, with very similar average results on multiple 1000GP genomes. The introduced novelties are partly biologically inspired, e.g. making use of tag SNPs characterizing haplotypes. Another line of research concerns indexing genomic collections (or more generally, repetitive sequences), i.e. building data structures enabling fast pattern search in the genomes (). Such indexes are efficient when the index resides in the main computer memory, which is challenging considering the sheer volume of indexed data. Some of the listed works are rather theoretical and their *To whom correspondence should be addressed. implementations are not yet available, whereas the works that have been implemented () are tested on relatively small collections, not exceeding $1 GB. In this article, we try to answer the question how well a collection of genomes of the same species can be compressed, when knowledge of the possible variants is given. The cited works ofare so far the only attempts to compress a (single) genome sequence with a variant database. In this work, we take two large collections of genomes (H.sapiens and A.thaliana) and try to exploit cross-sequence correlations in the variant loci. Our solution is a specialized Ziv-Lempel-style compressor, where the input sequences are basically formed by binary flags denoting if successive variants from the database were found in the individuals. This approach appears highly successful, allowing to store the human collection in 432 MB (395 KB per individual) and the plant collection in 110 MB (142 KB per individual). We point out that the general idea of exploiting common features for improved compression is known for some other NGS tasks, including compression of (both mapped and unmapped) reads (). In the next section, we present the input data and the general idea of our approach. Then, we show some details of the proposed compression algorithm. Finally, we evaluate the compressor. The last section concludes the article.
MATERIALS AND METHODS
DatasetsLarge collections of genomes of the same species in public repositories are nowadays often represented as one reference genome and multiple variant files. There are several formats for storing the variants, e.g. variant call format (VCF) () used in the 1000GP, general feature format (GFF) used in the PGP. These formats are much more compact than, e.g. FASTA (with raw genomic sequences), yet large collections may still require hundreds of gigabytes of storage. We use two large datasets in the experiments. The publicly available database of Phase 1 of the 1000GP contains data for 1092 human individuals. The genomes are in VCF files, one file for each chromosome, and so there are 24 files in total. These files contain the information about each variant [SNP, insertion (INS), deletion (DEL) and structural variant (SV)] that was found in at least one genome in the dataset. The genomes are phased, i.e. there is information on which of the two chromosomes of each pair (or on none/both) each variant is found. Similar information about variants is present in the 1001GP for A.thaliana. For this collection, we have 775 haploid sequences, each consisting of seven chromosomes. These data were scattered, and we gather them from four 'subprojects'. Our research goal concerns only genome collection compression [similarly as in the works of, and VCF files usually contain much more information than needed to recover the DNA sequences (e.g. in FASTA format). We ignore the non-essential VCF fields, i.e. keep only the information on which positions the changes in each genome may be found. For the 1000GP dataset, it meant removing extra fields from the original VCF files. For the 1001GP dataset, we directly converted the available data to a stripped VCF (sub)format, which we call VCF minimal (VCFmin). We point out that these are valid VCF files. As a side note, let us remark that the GFF data (http://evidence.personalgenomes.org/guide_upload_and_annotated_file_ formats) used in the large-scale PGP () are compatible with that of ours stored in VCFmin. The basic dataset characteristics are presented in(URLs and other technical descriptions of the data, including preprocessing details for the 1001 dataset, are given in the Supplementary Material).
The compression algorithmOur tool, Thousands Genomes Compressor (TGC), assumes that the input data are in VCFmin form. Such textual file consists of rows, one per each variant. A single row contains the following data: Description of a variant (position, type and information about the changes to the reference genome), Evidence of occurrence of the variant in each single genome. The VCFmin files can be compressed quite efficiently by general tools, but much better results are possible. The biggest hurdle for a generic compressor is the 'non-locality' of the VCFmin format, i.e. the genomes are stored in columns, so the occurrences of the successive variants of the same genome are at long distances. This means that if two genomes have similarities, the compressor must find and encode their similar (identical) areas, which are far away and are relatively short (the description of the occurrence of a variant in a single genome takes a few bytes). The main idea of our algorithm is to transform the input data in a way to increase the locality and lengths of similar (identical) genome areas. To this end,A single database of variants containing only the basic information about each variant (see); multiple variant alleles are also supported, 2N (for diploid) or N (for haploid genomes) bit-vectors. Value 1 at some jth position in this vector means that jth variant in the database is found in the genome. To reduce the space, these bit-vectors are packed into byte-vectors (8 bits in a byte). A byte is then the smallest processing unit in the compression scheme.For example, if the 1000th variant in the database is 776646 SNP A, and the 1000th bit for some genome is 1, then we know that an SNP occurs in this genome at position 776646 and the resulting nucleotide there is A. The collection of the database of variants and the byte vectors is later called as variant database  byte vectors (VDBV) format. Using the dense byte vectors has a few advantages:Processing compact input is usually faster and less memory demanding than with more 'bloated' input;Identical patterns of successive variants in different genomes are represented with repeating byte sequences (which can be easily handled with standard dictionary compression techniques); Same variants in different genomes have the same positions, and thus encoding the repetitions of the common patterns in successive genomes is cheaper.The resulting VDBV representation is already well compressible with generic tools, especially 7z, but universal solutions neglect some existing redundancy; in particular, they are not 'aware' about the aligned repetitions between genomes in our byte vectors. To exploit this, we devised a specialized compressor loosely based on the LZSS algorithm (). Each vector is processed from left to right. At every analyzed position k, we look for the longest match (identical byte substring) starting at kth position in any previously processed byte vector. The match position restriction is obvious, as we look for common haplotypes, and matches elsewhere in the vectors are accidental and thus unlikely to be long. (Moreover, with the restriction, the matches need fewer bits to encode.) The already processed data from the vectors are indexed using two hash tables (HTs) to make the search faster; one HT is for searching for long matches and if none such is found then the other HT is used (some more details on the hash scheme are given in the Supplementary). Similarly as for the classical LZSS algorithm, at each position we can find a match of length at least mml (minimal match length, set experimentally to five in our implementation) or a literal (if no sufficiently long match can be found). A match is described as a triple h1, vid, leni, where vid is the id of the vector in which we found the longest match, and len is the match length. A literal is represented with a pair h0, bvi, where bv is the value of the byte at position k in the byte vector. The first fields (flags), 0 or 1, distinguish between literals and matches. After processing a match, we shift to k  lenth position, and in a case of literal to k  1th position. The sequence of pairs and triples is then compressed using an arithmetic coder () (We use a popular and fast arithmetic coding variant by Schindler (http://www.compressconsult. com/rangecoder/), also known as a range coder.). Broadly speaking, arithmetic coder encodes each symbol occurrence on (in general, fractional) number of bits related to the probability of the symbol occurrence. These probabilities are estimated on the basis of already encoded symbols, i.e. if a symbol has occurred frequently, the corresponding probability is high and the number of bits spent for encoding it is small. We note that using Huffman coding instead for our data would result in a few percent compression loss. There are several contextual models for compressing various fields, which means that different by-products of our scheme are handled based on different collected statistics (more details are given in the Supplementary data). The flags are compressed in one model. For the literals, another model is used, but before passing their byte values to the entropy coder, we do some trick. The byte value of the first literal after a match h1, vid, leni is 'xored' (if its value is not 0) with the byte following the repetition in vidth byte vector, i.e. the byte of index k  len. We know these two byte values cannot be equal (otherwise the match could be extended by at least 1 byte). It was found experimentally that it is more likely to have a decreased number of resulting set bits in bv after the 'xor' operation than to have it increased (even if in most cases, the number of set bits is unchanged). In this way, the distribution of the bv values gets more skewed, which helps the compression. Contextual compression models are usually more practical if they are not too large, and this is the reason why numbers from a broad interval are often split before being processed by a statistical model (this approach is used, e.g. in gzip, bzip2). Following this rule, in our scheme, the length of the match is stored in two parts, both compressed with an entropy coder. First, we compress the binary logarithm of the match length (rounded to an integer) and then the remaining bits needed to recover its length. More precisely, the first part is log 2 len  mml  1 AE  , and if len  mml ! 2, then the value of len  mml  2 log 2 lenmml1 d e 1 is encoded. Similarly, the vid field is split into two (byte) parts: vid=256   and vid  256 vid=256   , both encoded with an entropy coder. The compression of variant database (excerpt of which is presented in) is rather straightforward. The main idea is to compress each variant type separately: SNP, insertion, deletion and SV. Thus, for each variant, we first store its type. The variants positions are then differentially encoded, i.e distances between consecutive SNPs, consecutive DELs, etc, are stored. Then, for SNP, we store the substituting symbol. For INS, we store its length and the symbols to insert. For DEL, only its length is stored. For SV, we encode the deletion length, the insertion length and finally (if necessary) the inserted symbols. All the values are encoded using a variant of arithmetic coding with appropriate contextual models (details can be found in the Supplementary data).
RESULTSAs mentioned earlier, for the evaluation of the proposed compression algorithm, we use two datasets of 1092 H.sapiens and 775 A.thaliana genomes. In all experiments, the data are processed chromosome by chromosome. This approach, typical in the genomic compression literature (see, e.g. Deorowicz and), reduces the memory footprint, speeds up computations and improves the compression ratio for the generic algorithms. There are several tools for compressing genomic sequences in FASTA format. Unfortunately, the amount of our test data, $6.8 TB of raw sequences if converted to FASTA, is so huge that the running times of some of those compressors would be counted in months. Thus, we started from a preliminary test in which we evaluated the most powerful as well as the most recent tools for only two human chromosomes (14 and 21) and also two plant chromosomes (1 and 4). The results are presented in. This and all further experiments were performed on a computer equipped with four 4-core 2.4 GHz AMD Opteron CPUs with 128-GB RAM running Red Hat 4.1.2-46 linux. Two of the presented compressors may be considered fast with regard to the compression speed: ABRC () with $100 MB/s (run with 8 threads) and GDC-normal () with $40 MB/s speed (only a serial implementation exists), while the others are by about one [GDC-ultra () and RLZ (or about two (7z) orders of magnitude slower. Interestingly, the only generic compressor in the tests, 7z, is the second best in the compression ratio (after GDC-ultra), but its compression speed is low (0.4 MB/s). The memory available for the compression has a major impact on the compression ratio. For example, 7z was run with its maximum setting, 1 GB for its LZ-buffer (translating to 410 GB of total memory use), yet it fit only a small part of the input for H.sapiens data: 510 individuals (each of size $108 MB) for chromosome 14 dataset and 21 individuals (each of size $48 MB) for chromosome 21 dataset. This, supposedly, was the main reason for which its compression ratio in the latter case is significantly higher. The hypothesis is indirectly confirmed by the results for much shorter A.thaliana chromosomes, for which more individuals fit the 1-GB LZ-buffer and the compression ratios are close to GDC-ultra. In the next experiment, we compared a few well-known generic compressors (gzip, bzip2, 7z) on VCFmin input files (). Compressed sizes (in megabytes) and compression times are reported for selected chromosomes and the collections in total. Surprisingly perhaps, the best compression was obtained by bzip2 compressor.is similar, but now the inputs are in our temporary 'dense' representation, VDBV. Here, the generic compressors are compared against our proposal, TGC. Two simple observations can be made: (i) the more compact of these two input representations, VDBV, is clearly more appropriate for the best generic compressor, 7z, both from the point of compression ratio and from speed; (ii) TGC is significantly better than VDBV7z in both measured aspects, which demonstrates that designing a specialized compression algorithm was a worthy goal in this case. The results are summarized in. For comparison purposes, we also show the sizes of the raw sequences as well as the compression results of the best compressor working on such representation, GDC-ultra. We resigned, however, from presenting the compression and decompression times of GDC-ultra, as it works on a completely different representation than VCFmin,which we use in the article. The compression ratios of GDC can be treated as a reference point. The most important 'numbers' from this summary are the average sizes of genomes in the most compact, TGC, representation. The obtained 395 KB (for the human data) is more than six times smaller than offered by the best so far genome sequence, GDC-ultra, compressor. Also, the very recent paper by, working on a representation similar to our VCFmin, reports more than six times larger files. When expressing the compression ratios in relation to the raw genome sequence sizes, it means that our algorithm squeezes H.sapiens genomes $15 500 times and A.thaliana $850 times.In the last experiment, we compared TGC against SpeedGene (), an algorithm for efficient storage of SNP datasets. For an honest comparison, we restricted the 1000GP set of variants to SNPs only. SpeedGene requires the input data to be in LINKAGE format, in which there is no distinction between chromosomes in each pair, i.e. for each SNP it describes only whether no SNP is found in a genome, one SNP (on any chromosome) is found, two SNPs are found (on both chromosomes) or the status of SNP is unknown. Thus, we changed our algorithm slightly and instead of processing each single chromosome, we joined chromosomes of each pair, and obtained vectors of 'dibits', i.e. bit pairs. These vectors are then transformed intoNote: There are two columns containing ratios: 'Ratio to raw' tells how many times the compressed file is smaller than the genome sequences in FASTA format. 'Ratio to VCFmin' is the compression ratio according to the size of VCFmin files. For GDC-ultra, compression and decompression times are not given, as this compressor uses a different input form than others and such a comparison would be irrelevant. The values marked in bold indicate best compression.Note: All sizes are in megabytes and times are in seconds. The 'VDBV c-time' column contains the conversion times from VCFmin format to VDBV format. In the remaining columns titled 'c-time', total compression time is given (i.e. 'VDBV  7z c-time' denotes the sum VCFmin-to-VDBV conversion time and 7z compression time; 'TGC c-time' is the total TGC processing time, comprising the conversion to VDBV and the actual compression). Note that the variant database (part of the VDBV representation) is of size 933 MB for H.sapiens and 320 MB for A.thaliana. After compressing by TGC, their sizes (included in 'TGC size' column) are $51.0 MB and 12.5 MB, respectively. The values marked in bold indicate best compression. The extended version of this table (with results for all chromosomes) can be found in Supplementary Table S2. byte vectors (each byte contains four consecutive dibits). In this way, our tool can compress these byte vectors without any change.presents the compressed sizes obtained by SpeedGene and TGC. We show the results for three chromosomes, as well as for the complete genome. As one can see, TGC reduces the dataset size more than four times better than SpeedGene.
DISCUSSIONWe examined the possibility of obtaining much better compression ratios of genomic collections than from existing tools, when additional knowledge is given. The knowledge was the information about the possible variants in genomes and the occurrence of these variants in specific genomes. This helps a lot in compression of genomic sequences, as all input sequences are perfectly aligned and the task of finding repetitions in data (usually the most important and time-consuming task handled by data compression algorithms) becomes rather simple. We should mention that in theory such perfect alignments can be found by compression algorithms, but the computational burden would be enormous. Thus, compression tools usually make some heuristic decisions when comparing the sequences in the hope that they do not lose too much. The success of our algorithm was possible not only because of the variant database, but also because we searched for crosscorrelations between individuals. In other words, for each individual, similarities to any other previously processed individual (i.e. runs of repeating variants) can be found. In principle, the available memory may be a limiting factor but processing the collection on the chromosome level resulted in 52.5 GB memory use for the larger (human) of the tested datasets. In the future, when much more genomes are available, we may need to re-address the memory issue though, possibly via working on blocks smaller than whole chromosomes, or trying to re-order the sequences in a way to maximize local similarities. In the compression method design, we sometimes traded compression ratio for reduced memory requirements, e.g. some (rather minor) improvements in compression would be possible owing to higher-order contextual modeling. Probably, a more practical approach is to make use of more biological knowledge; the very recent work ofgives new insight, which might be possible to use in our scheme, but we leave it for future work. Why such experiments can be interesting? Although accurate and efficient analyses of such huge (several terabytes in raw format) genomic collections remain a major challenge, we believe that the mere compressibility of human genomes (e.g. as a 'lower bound' for memory requirements of future algorithms and tools) is a question worth investigating. For example, our compressed collection takes $430 MB, so including also a compressed reference genome (at most 700 MB) requires $1.1 GB of space, which seems quite modest. Naturally, running efficient queries over such data is another matter (clearly with some overhead in space use), but our results suggest this is not impossible. The information kept in VCF or genome variation format (GVF) files is often more detailed (e.g. may include quality scores) than what our tool preserves. Although clearly efficient compression methods for such data are also needed, we do not anticipate a possibility to obtain similar compression ratios to TGC, unless a (strongly) lossy mode is used. Unfortunately, we cannot see a way to easily adapt our compression techniques for such data. TGC allows extracting an arbitrary chromosome (or a whole genome) from the compressed collection, yet this solution is simple and rather slow. Making this extraction faster, or (even better) allowing for quick access to position-restricted arbitrary snippets of the genomes in the collection, is an important task left for future work. Clearly, there must be some space-time tradeoffs for such functionalities. A somewhat related functionality will be to add or remove an individual genome to/from the collection. Currently, changing the archive content requires recompressing the collection from scratch. The performed experiments showed that even the best genomic sequence compressor, GDC-ultra, is significantly (up to seven times) poorer in compression ratio than what can be obtained with extra knowledge. The main conclusions from our work are:Note: VCFmin means a simplified VCF with SNP calls only that spends only 4 bytes for each genotype. All sizes are in megabytes. The values marked in bold indicate best compression.
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Genome compression at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
S.Deorowicz et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Modern genomic sequence compressors cannot come close in compression ratio to the proposed algorithm basically because of two, not fully independent, reasons: (i) (almost) all of them ignore external knowledge (variant information), and (ii) working on consensus sequences is extremely resource-consuming and keeping full statistics needed for efficient compression is practically impossible for a large collection even on a 128-GB machine. Even huge human genome databases can be stored in relatively small space, as the data size of a single individual is only 395 KB on average. When extrapolated, this would mean that modern 2-TB hard drive is sufficient to store the genomes of $5 million humans, size of a large city.
