Motivation: Previous studies reported that labelling errors are not uncommon in microarray datasets. In such cases, the training set may become misleading, and the ability of classifiers to make reliable inferences from the data is compromised. Yet, few methods are currently available in the bioinformatics literature to deal with this problem. The few existing methods focus on data cleansing alone, without reference to classification, and their performance crucially depends on some tuning parameters. Results: In this article, we develop a new method to detect mislabelled arrays simultaneously with learning a sparse logistic regression classifier. Our method may be seen as a label-noise robust extension of the well-known and successful Bayesian logistic regression classifier. To account for possible mislabelling, we formulate a label-flipping process as part of the classifier. The regularization parameter is automatically set using Bayesian regularization, which not only saves the computation time that cross-validation would take, but also eliminates any unwanted effects of label noise when setting the regularization parameter. Extensive experiments with both synthetic data and real microarray datasets demonstrate that our approach is able to counter the bad effects of labelling errors in terms of predictive performance, it is effective at identifying marker genes and simultaneously it detects mislabelled arrays to high accuracy. Availability: The code is available from
INTRODUCTIONHigh-throughput microarray technologies make it possible to measure the expression levels of thousands of genes. Our ability to use these data to reliably predict the presence of a certain disease and to better understand the biological mechanisms underlying the development of disease is of fundamental importance from the perspective of treatment and prevention. Statistical machine learning methods have already shown a lot of promise towards these goals, and methods that can deal with high dimensional and low sample size settings have been the subject of considerable research efforts over the last decade. However, the classical machinery of learning a classifier relies on a set of labelled examples, and the quality of a classifier depends crucially on the accurate labelling of these data. Unfortunately, the task of labelling is complex and not without ambiguities. As a result, there is no guarantee that the class labels are all correct; in fact, there is an increasing realization that labelling errors are not uncommon in microarray datasee. The presence of class label noise in training sets has been reported to deteriorate the performance of the existing classifiers in a broad range of classification problems (). Although, the problem posed by the presence of class label noise is acknowledged, often it is naively ignored in practice. Part of the reason may be that symmetric label noise can be relatively harmlesshowever, asymmetric noise inevitably deteriorates the performance, as it changes the decision boundary between the true classes (). Various approaches have been devised in the machine learning literature to address the issue of learning from samples with label noise. The seemingly straightforward approach is by means of data preprocessing where any suspect samples are removed or relabelled (). However, these approaches hold the risk of removing useful data too, which is unsuitable in microarray classification, as the number of training examples is limited. In sharp contrast with the multitude of methods for microarray classification, there are few attempts to address the problem of label noise in the bioinformatics literature.pointed out the difference between mislabelled arrays and outliers, and proposed two methods to detect mislabellings based on data perturbation.developed this work further and obtained improved precision and recall in both synthetic and real data settings. Both of these works are based on data perturbation, and their main focus is to detect suspects that are potentially mislabelled. These methods can help repairing the labels, so we can imagine a two-stage procedure of creating a repaired training set first and feed this to existing classifiers in a second stage. However, one must be aware that any errors made in separate stages of analysis will necessarily accumulate. In this article, we address the above problems by developing an integrated approach where the ambiguity of the given label assignments is modelled explicitly during the training of a *To whom correspondence should be addressed.classifier. This allows us to build on classifiers that have been successful for microarray classification by developing an extension to account for possible label noise. Specifically, here we will harness the sparse Bayesian logistic regression (BLogReg) model proposed bywith a robustness against label noise. From our model formulation, we then derive a new algorithm that alternates between training the classifier and estimating the label noise probabilities. Straightforward calculations further provide the posterior probability of mislabelling for each of the training points. This enables us to detect the suspect samples for possible follow-up study. In addition, our experimental validation results, using both synthetic and real microarray datasets, demonstrate that the proposed method improves on traditional algorithms and achieves a reduced classification error rate. A variant of our approach appears in Bootkrajang and KabanKaban (2012).
METHODS
A model for label-noise robust logistic regressionWe now describe our label-noise robust Logistic Regression (RLogReg) model. We will use the term 'robust' to differentiate this from traditional logistic regression. Consider a set of training data S  fx 1 , ~ y 1 ,. .. , x D , ~ y D g, where x i 2 < M and ~ y i 2 f0, 1g, where ~ y i denotes the observed label of x i. As in the classical scenario for binary classification, we start with defining the log likelihood:1jx i , w  1  ~ y i  log p ~ y i  1jx i , w 1 where w is the weight vector orthogonal to the decision boundary and it determines the orientation of the separating hyperplane. If the labels were presumed to be correct, then for a point x i we would take p ~ y i  1jx i , w  w T x i   1 1  e w T xi 2 and whenever this is above 0.5 we would decide that x i belongs to class 1. However, when there is label noise present, making predictions in this way is no longer valid. Instead, we will introduce a latent variable y to represent the true label, and we rewrite p ~ y i  kjx i , w as the following:In Equation (3), p ~ y  kjy  j  def jk represents the probability that the label has flipped from the true label j to the observed label k. These parameters form a transition table, which we will call the 'gamma table', , and these label flipping probabilities may be estimated. Using this model, instead of Equation (2) we will have:We decide that x belongs to class 1 whenever py  1jx, w ! 0:5.
Sparsity priorMicroarray data are high dimensional with more features than observations while only a subset of the features is relevant to the target. A vast literature demonstrates that sparsity-inducing regularization approaches are effective in such cases (). Hence, we now incorporate sparsity in our model described in the previous section. Following Shevade and Keerthiwhere is the Lagrange multiplier (or regularization parameter) that balances between fitting the data well and having small parameter values. The L1-norm in the regularization term is defined as,Now, the regularization parameter needs be determined. We cannot use cross-validation, not only for its computational demand, but primarily because it would need a validation set with trusted correct labels, which may be not available. Hence, we adopt the Bayesian regularization approach of Cawley and, which bypasses the need for cross-validation and determines automatically by putting a Jeffrey's prior on and integrating it out from the model. This yields the following (see, for details):where N denotes the number of non-zero parameters, i.e. those with w d 6  0 so N M.
Parameter estimationIt now remains to estimate w and . Notice that Equation (5) is not differentiable at the origin. Shevade andproposed a simple, yet effective, algorithm to optimize the non-smooth but convex objective function of sparse logistic regression (SLogReg) using the Gauss-Seidel method and using coordinate-wise descent. We will create a modification of this approach to make it applicable to our non-convex objective. Define F d  @Lw @wd , where w d0 is the bias term that is usually left unregularized. The optimality conditions for Equation (5), which are the same as inand Cawley and Talbot (2006) can be stated algebraically as the following:Accordingly, the violation from optimality of w d may be summarized as:We start optimizing the component w d that makes the largest violation to an optimality condition. At this point, if the objective function was convex then it would be possible to use gradient information to bracket the region where the optimal w d lies by specifying upper and lower limits (H and L). For example, Shevade and Keerthi (2003) identify 10 different cases for their sparse logistic regression model. However, since our likelihood term is non-convex, the cases identified there are not applicable because the sign of gradients give no information about the interval where the optimal solution resides. Therefore we introduce a simple modification by performing two searches: one in the range R  [ f0g and another in the range R  [ f0g. We then choose the solution that returns a higher value of the objective function. This modified searching approach is more general and will work on any locally differentiable function at the expense of a slight increase in computation time. In practice, L and H are finiteprovided that the design matrix is standardized and appropriate regularization is imposed on the solution, it is sufficient to search in the (0, 1000) and (1000, 0) intervals. Finally, having completed the optimization of w, it remains to derive the update rule for the label-flipping probabilities. Conveniently, these can be estimated via fixed point update equations. By introducing a Lagrange multiplier to ensure that the probabilities in each row of the  table sum to 1 and solving the stationary equations, we obtain the following update equations (for details see Bootkrajang and KabanKaban, 2012):Derivation details are given in the Supplementary Material. The optimization of the log-likelihood is then to alternate between optimizing w along with updating according to Equation (7) until convergence is reached, and we alternate this with the fixed point update equations of the label-flipping probabilities. The entire optimization procedure is summarized in Algorithms 12.
Algorithm 1 Main loopInput: Training examples. Initialize w 0, 0, I nz fw 0 g, I z fw d, d2f1, ng g, . while Optimality violator exists in I z do Find the greatest optimality violator, , in I z repeat Optimize w using Algorithm 2Find the maximum optimality violator, , in I nz until No violator exists in I nz Update the entries of  by Equations (8) and (9) Update regularization parameter, by Equation (7) end while Output: Optimized weight vector, w. Optimized .
Detecting mislabelled pointsFor an observation x i , ~ y i , the probability of it being mislabelled can be computed as the following:This may be thought of as the models 'degree of belief' that x i 's label is incorrect. We may use it either in this form, or in a hard-thresholded form (i.e. predict that the point x i is mislabelled if py 6  ~ y i jx i  ! 0:5).
A note on low sample size, high dimensional dataSince additional parameters  are being estimated from the data, we expect that RLogReg will require more training examples to deliver its full potential. In microarray datasets, the training set size is often of the order of tens only. A possible workaround in such cases is to guide the algorithm by presetting the gamma table from domain knowledge about the likely proportion of mislabelled data. When such knowledge exists, the values of gamma may either be fixed throughout the optimization process or they may be seeded initially and then optimized.) where BLogReg was shown to be superior. We shall demonstrate that our proposed robust extension of BLogReg performs better than the original BLogReg in terms of classification performance when there is label noise present in the training set. Moreover, our model can be used to identify mislabelled arrays for potential follow-on study. Before proceeding, we should comment that symmetric and asymmetric label flipping have very different consequences in classification. Symmetric or uniform flipping means that each class is affected by label flipping in the same proportion. In contrast, asymmetric or non-uniform flipping is when the label flips from one class to another more often than vice-versa. The latter type of label flipping has been theoretically shown () to degrade the performance of an algorithm to a much larger degree, as it modifies the decision boundary between the true classes. Our empirical study () also demonstrated this. Therefore, we will mainly focus our attention on datasets with asymmetric label noise and indeed expect the advantages of our approach to be most apparent in that setting. To demonstrate the benefit of having a label noise model embedded in the classifier, we start with experiments on synthetic data where labels were asymmetrically flipped at the rate of 30%. The use of synthetic data for controlled experiments is standard in bioinformatics (see e.g.), as it allows us assess the performance of a new approach against a ground truth. We shall then move on to analysing real microarray datasets where label noises have not been injected artificially. These datasets have been previously reported to contain wrongly labelled samples. Finally, we shall assess the ability of our proposed approach to identifying mislabelled arrays using Receiver Operating Characteristics (ROC) analysis.
RESULTS
Experiment setting
DatasetsWe generate synthetic data by sampling points from a standard Gaussian distribution where the class label associated with each point is assigned by a logistic function with a predefined weight vector w having only three relevant features, w 1  w 2  w 3  10=3, w i  0, 8i43, following Ng (2004). We create sets with 500 training points and sets with 100 training points together with independent test sets of 100 points each time, and call these datasets Synth-500 and Synth-100, respectively. The dimensionality of the synthetic datasets ranges from 100 up to 1000. Asymmetric label noise was artificially injected into each synthetic dataset at the 30% rate. Further, we use two real microarray datasets: Colon cancer () and Breast cancer ()both of which are known to contain some mislabelled arrays. No artificial label flipping is injected in these data. We standardize these datasets so the rows of the D  M design matrix (where D is the number of observations and M is the dimensionality) of the input sample will have zero mean and unit variance.summarizes the characteristics of all of these datasets used. Additional datasets and results are given in the Supplementary Material.
Error measures Whilein the case of synthetic data the true labels can be used to validate the predictive accuracy of our algorithm, in the real microarray data there is no absolute ground truth. Since the labels given in the datasets may be incorrect, the issue of what should count as a misclassification must be defined. We define two variants for measuring out-of-sample error rates:Corrected (CRT): Count misclassification errors against the 'corrected' labels where corrections are made cf. the mislabellings reported in the literature.Cleansed (CLN): Exclude any mislabelled suspects (known in the literature) from the test sets for the purpose of evaluation, so these are always placed into the training set instead; then count the misclassification errors on test sets in the usual way.
Results and analysis
Results on synthetic dataThe average misclassification error rates on the Synth-500 and Synth-100 datasets are shown inas the data dimension is varied. Each point on these plots represents the average misclassification rate on the test sets, where the average is taken over 500 independent repetitions of the experiment. The error bars are too small to be visible. We see that RLogReg achieves significantly lower error rates than BLogReg on the datasets that contain more training examples (Synth-500). This clearly demonstrates the advantage of modelling the label noise process. On the smaller size dataset (Synth-100), however, the performance gain becomes marginalthis is because the accurate estimation of the additional parameters (label-flipping probabilities) requires sufficient training data for our approach to achieve its full potential. Nevertheless, it is should be noticed that even in the small sample setting, RLogreg performs no worse than BLogReg on all the datasets tested (additional results are given in the Supplementary Material.). More importantly, the rightmost plot shows that we can counter the problem of small sample sizes by using prior knowledge about the extent of label noise, e.g. by pre-defining the gamma table. We denote this version as RLogReg-F in the figure, and we see this significantly improves the classification accuracy in the small sample setting. Beyond classification performance, it is of interest to evaluate the methods' ability to identify the relevant predictive genes.shows the estimated weight vectors as obtained by BLogReg and RLogReg respectively from 100-dimensional synthetic data with only the first three features being relevant. The classifiers were trained on 250 training examples per class that were subjected to 30% asymmetric label flipping. We see that RLogReg achieved a more accurate estimation of the weight vector, while BLogReg became confused by the noisy labels and selected too many false non-zero weights. This is an important advantage of RLogReg over BLogReg when it comes to finding a small set of predictive marker genes.
Results on colon cancerThe colon cancer classification task aims to distinguish between normal tissue and tumour. According to, there is biological evidence that the samples T2, T30, T33, T36, T37, N8, N12, N34, N36 may be mislabelled. The proportion of mislabelling in the two classes is unequal; hence, this is a case of asymmetric labelflipping that can distort the correct decision boundary of the classes. The limited number of training observations implies that a good estimate of the gamma table may be difficult to obtain from the data alone (as we have seen in the previous section), nevertheless prior knowledge of the noise proportions may still allow us to exploit the advantages of having a noise model as integral part of our classifier. Therefore, we include RLogReg-F in our experiments, with the gamma table set to the true label-flipping proportions.reports the leave-one-out (LOO) errors in terms of the error measures defined in Section 3.2.1, and we also give the average number of genes selected by the three methods considered. The results confirm the expectations. RLogReg that attempts to estimate the gamma table along with all other parameters is marginally worse than BLogReg (although not statistically significantly so, according to the unpaired t-test), while RLogReg-F improves over BLogReg in all validation criteria used, and it also selects a smaller fraction of relevant features.shows the average magnitude of each gene according to BLogReg and RLogReg-F, respectively. These are averages of w estimates across 1000 bootstrap repetitions to inspect possible systematic differences. These average weights turned out to be quite similar for BLogReg and RLogReg-F, with the exception of a few genes that had been ranked differently by the two methods. To see this, a summary of top ten selected genes and their estimated weights are given in Tables 3 and 4.
Results on breast cancerWe further apply the proposed model on the Breast Cancer dataset from. The aim is to discriminate between oestrogen-positive and oestrogen-negative observations. According to, there is biological evidence that the arrays 11, 14, 16, 31, 33, 40, 43, 45, 46 are mislabelled. However, unlike the Colon dataset, we observe the nature of label flipping in the Breast cancer dataset is rather close to symmetric. As a consequence, mislabelling might do less harm to traditional classifiers in terms of class prediction on future arrays.summarizes LOO error rates together with the numbers of genes selected by the classifiers. The picture is quite similar to what we have seen in the case of Colon, although the differences tend to be smaller, as the label noise here is more symmetric. We also see that RLogReg did pretty well with a limited amount of training data, but of course the difficulty of accurate estimation of the gamma table from such few points remains an issue. In fact, the estimated gamma table of RLogReg may converge to identity in such conditions, which statistically will result in a weight vector that is identical to that of BLogReg. As previously, knowledge of the extent of noise can be used here, resulting in a slight improvement for RLogReg-F. Finally, as somewhat expected, the average magnitude of gene weights from BLogreg and RLogreg-F look similar, as shown in, which was expected by the symmetric nature of the label noise in this dataset.
Computation timeWe should give an indication of the added computation overhead required by our noise modelling relative to the existing BLogReg. One LOO loop on all datasets considered took on average 4 s for RLogReg, while BLogReg required roughly 0.2 s on an Intel's Core-i5 3.2 GHz machine. We believe this extra computation time is most worthwhile especially when the training set size is sufficiently large to exploit the full potential of the presented approach.
Detecting mislabelled instancesOne of the most appealing features of our proposed algorithm is the possibility to detect mislabelled examples from the data, in addition to classification and gene selection. There are two types of possible errors: (i) a false positive is when a sample is believed to be mislabelled despite it is in fact labelled correctly; and (ii) a false negative is when a sample is believed to be labelled correctly despite its label is in fact incorrect. A good way to summarize both, while also making use of the probabilistic outputs given by the sigmoid function, is byconstructing the ROC curves. The area under the ROC curve signifies the probability that a randomly drawn and mislabelled example would be flagged by the proposed algorithms.shows the ROC curves for Synth-500 and Colon cancer datasets. Superimposed for reference, we also plotted the ROC curves that correspond to BLogReg. BLogReg considers that all points have the correct labels, and it has not been designed to spot mislabelled points. The best we can do is to take that mistakes made on the training points are mislabelling predictions. From. Average ROC curves for BLogReg, RLogReg and RLogReg-F on Synth-500 and colon cancer benchmarks. For consistency with classification result bootstrap is performed on Synth-500 while LOO is used to obtain the result for colon cancer. The prediction is based on hard-thresholded rule, we see the gap between the two curves is significant and well apparent in the experiment on Synth-500. This quantifies the gain that our modelling approach is able to obtain. The gain for Colon is smaller but still significant, despite the dataset size is so limited, provided that RLogReg incorporates knowledge about the proportion of mislabelling (i.e. RLogReg-F).
Comparison with previous findingsIn addition to comparisons that quantify the benefits of having a noise model, we compare our results with previously identified mislabelling in the Colon cancer samples. We conduct 100 bootstrap repetitions drawing subsets of size 50 from the total of 62 points randomly while imposing that none of the suspects from the literature are left out. In, after quoting the previous detections from the literature, we report the mislabelling detections obtained by BLogReg-F and BLogReg respectively, in two forms: (i) from the run that returned the largest number of detections, and (ii) the percentage that a particular array was flagged up as a mislabelling during the 100 repetitions. It is interesting to note that RLogReg-F was able to identify up to seven mislabelled points, and these also agree with the majority of previously reported detections using other algorithms (i.e. for T30, T33, T36, N34 and N36). BLogReg is also able to find up to seven mislabelled samples but with fewer true positives and more false positives. From both figures, we see that RLogReg-F is able to identify mislabelled arrays more often than BLogReg can.The detections for RLogReg-F are based on the hard threshold rule (p ~ y 6  yjx, w ! 0:5). The first line is the 'gold standard' that is backed up by biological evidence in the literature.
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Robust sparse logistic regression at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
J.Bootkrajang and A.Kab n at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
CONCLUSIONS We proposed a robust extension of sparse Bayesian logistic regression for classification in the presence of labelling errors. The numerical experiments suggest that our approach is superior to its traditional counterpart when the training data contains labelling errors, and more significantly so when the label-flipping distribution is asymmetric. Simultaneously, our methods are effective in identifying marker genes and detecting mislabelled data. Since our robust model needs to estimate the label-flipping probabilities together with the parameters of the classifier, it does require more training data to achieve its full potential. However, in our experience, RLogReg performs statistically no worse than BLogReg even when the training set sizes are small. The need for more data can also be relaxed by incorporating knowledge about the extent of label noise. Funding: J.B. is supported by the Royal Thai Government. A.K. acknowledges the MRC Discipline Hopping Award G0701858 (ID no. 85545). Conflict of Interest: none declared.
