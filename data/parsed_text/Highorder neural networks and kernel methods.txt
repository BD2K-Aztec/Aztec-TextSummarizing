Motivation: Effective computational methods for peptide-protein binding prediction can greatly help clinical peptide vaccine search and design. However, previous computational methods fail to capture key nonlinear high-order dependencies between different amino acid positions. As a result, they often produce low-quality rankings of strong binding peptides. To solve this problem, we propose nonlinear high-order machine learning methods including high-order neural networks (HONNs) with possible deep extensions and high-order kernel support vector machines to predict major histocompatibility complex-peptide binding. Results: The proposed high-order methods improve quality of binding predictions over other prediction methods. With the proposed methods, a significant gain of up to 25â€“40% is observed on the benchmark and reference peptide datasets and tasks. In addition, for the first time, our experiments show that pre-training with high-order semi-restricted Boltzmann machines significantly improves the performance of feed-forward HONNs. Moreover, our experiments show that the proposed shallow HONN outperform the popular pre-trained deep neural network on most tasks, which demonstrates the effectiveness of modelling high-order feature interactions for predicting major histo-compatibility complex-peptide binding. Availability and implementation: There is no associated distributable software.
IntroductionComplex biological functions in living cells are often performed through different types of proteinprotein interactions. An important class of proteinprotein interactions are peptide (i.e. short chains of amino acids)-mediated interactions, and they regulate important biological processes such as protein localization, endocytosis, post-translational modifications, signalling pathways and immune responses, etc. Moreover, peptide-mediated interactions play important roles in the development of several human diseases including cancer and viral infections. Because of the high medical value of peptide-protein interactions, a lot of research has been done to identify ideal peptides for therapeutic and cosmetic purposes, which renders in silico peptide-protein binding prediction by computational methods an important problem in immunomics and bioinformatics(). In this article, we propose novel machine learning methods to study a specific type of peptide-protein interaction, i.e. the interaction between peptides and major histocompatibility complex class I (MHC I) proteins, although our methods can be readily applicable to other types of peptide-protein interactions. Peptide-MHC I protein interactions are essential in cell-mediated immunity, regulation of immune responses, transplant rejection and vaccine design. Therefore, effective computational methods for peptide-MHC I binding prediction will significantly reduce cost and time in clinical peptide vaccine search and design. Previous computational approaches to predicting peptide-MHC interactions are mainly based on linear or bi-linear models, and they fail to capture key non-linear high-order dependencies between different amino acid positions. Although previous kernel support vector machine (SVM) and Neural Network (NetMHC) () approaches can capture nonlinear interactions between input features, they fail to model the direct strong high-order interactions between features. As a result, the quality of the peptide rankings produced by previous methods is not good. Producing high-quality rankings of peptide vaccine candidates is essential to the successful deployment of computational methods for vaccine design. For this purpose, we need to effectively model direct non-linear high-order feature interactions to directly capture interactions between primary (anchor) and secondary amino acid residues involved in the formation of peptide-MHC complexes. Deep learning models such as deep neural networks (DNNs) pretrained with restricted Boltzmann machine (RBM) have been successfully applied to handwritten digit classification, embedding, image recognition and many other applications (). But they have never been successfully applied to peptide-protein interaction problems. In this article, we propose using high-order semi-RBMs to pretrain a feed-forward high-order neural network (HONN) and propose high-order kernel SVM for peptide-MHC binding prediction, including identification of MHC-binding, naturally processed and presented (NPP) and immunogenic peptides (T-cell epitopes). Our proposed models achieved a significant gain of up to 2540% over the state-ofthe-art approach on benchmark and reference peptide datasets and tasks. Furthermore, our shallow HONNs even outperformed popular powerful pre-trained DNNs that was applied to model peptide-MHC binding prediction for the first time by this work.
Related workPosition-specific scoring matrix (PSSM) and matrix-based methods: InReche and Reinherz (2007) and, PSSMs were derived from a set of known binding peptides and PSSM matching score was used as an indicator of the binding potential of a query peptide. In, the peptide binding task was solved as a matrix-vector regression problem. Neural network-based methods: In, neural networks were built to predict peptide binding potentials by encoding peptides and contact residues on the MHC molecules as a fixed-dimensional vector of amino acid and contact residues. Similarly, in, neural networks and committees of networks with peptide representations combining sparse, BLOSUM and profile HMM encodings of the peptides were used. In, both the peptide sequence and MHC protein sequence were used as input to neural networks to enhance predictive ability for MHC alleles with limited peptide binding data. Kernel-based methods: The work inused the local alignment kernel method for predicting MHC-II-peptide binding. In, weighted-degree kernels were adopted to identify immunogenic peptides. The work inemployed support vector regression (with RBF, polynomial, etc. kernels) using sparse encoding of a peptide sequence and 11-dim physicochemical amino-acid descriptors. Recent work () used kernel logistic regression for MHC-II-peptide binding prediction using both peptide and MHC sequences. In, an SVM with kernel from () was used for NPP ('eluted') peptide prediction.
MethodsFor the peptides to bind to a particular MHC allele (i.e. its peptidebinding groove), the sequences of the binding peptides should be approximately superimposable: contain amino acids or strings of amino acids (k-mers) with similar physicochemical properties at approximately the same positions along the peptide chain. It is then natural to model peptide sequences X  x 1 ; x 2 ;. .. ; x n ; x i 2 R (i.e. sequences of amino acid residues) as a sequences of descriptor vectors d 1 ;. .. ; d n , encoding relevant properties of amino acids observed along the peptide chain and/or MHC-peptide interaction terms.
Descriptor sequence peptide representationsAlthough the descriptor vectors d i in general may be of unequal length, in the matrix form (equal-sized vectors d i 2 R R ) of this representation ('feature-spatial-position matrix'), the rows are indexed by features (e.g. individual amino acids, strings of amino acids, k-mers, physicochemical properties and peptide-MHC interaction features), while the columns correspond to their spatial positions (coordinates).illustrates descriptor sequence representation of a nonamer. In this descriptor sequence representation, each position in the peptide is described by a feature vector, with features derived from the amino acid occupying this position or from a set of amino acids (e.g. a k-mer starting at this position or a window of amino acids centred at this position) and/or amino acids present in the MHC protein molecule and interacting with the amino acids in the peptide.The purpose of a descriptor is to capture relevant information (e.g. physicochemical properties) that can be used by our HONNs and kernel functions to differentiate peptides into binding, nonbinding, immunogenic, etc. A real-valued descriptor of an amino acid is a quantitative descriptor encoding (i) relevant properties of amino acids such as their physicochemical properties and substitution probabilities by other amino acids and/or (ii) interaction features (such as binding energy) between the amino acids in the peptide and those in the MHC molecule. An example of the real-valued descriptor sequence representation of a peptide using 5-dim physicochemical amino acid descriptors is given in.
DNN and HONNGiven the matrix-form descriptor representation of each peptide based on BLOSUM substitution matrix as illustrated above, we concatenate all the columns of the matrix into a long vector as input feature vector to our neural networks. In this representation, a 9-mer peptide is represented by a 180-dimensional continuous vector, with each amino acid represented by its corresponding 20-dimensional substitution probabilities. Instead of using an ensemble of traditional neural networks to predict MHC class-peptide bindings as in the state-of-the-art approach NetMHC (), we propose to use HONNs pre-trained with a special type of high-order semi-RBMs called mean-covariance RBMs (mcRBMs) (), capable of capturing strong highorder interactions of feature descriptors of input peptides, to produce high-quality rankings of binding peptides (T-cell epitopes). The pretraining strategy has been widely adopted for training a popular powerful model called DNN (). DNN has attracted world-wide attention in the machine learning community recently. In, it has been shown that DNN is more powerful than shallow neural networks and performs much better than shallow ones on a benchmark dataset widely used in machine learning. In this article, for the first time, we apply DNN to predict peptide-MHC binding, and we compare its performance to our proposed HONN. DNN is shown on the left panel of. We use Gaussian RBM to pre-train the network weights of its first layer, and we use binary RBM to pre-train the connection weights of upper layers in a greedy layer-wise fashion (see Hinton, 2006 for detailed descriptions about pre-training). Our proposed HONN is shown on the right panel of. We use mcRBM to pre-train the network weights of it first layer, and we optionally add upper layers, and we use binary RBM to pre-train the connection weights in possibly available upper layers. In both DNN and HONN, we use a logistic unit as our final output layer, and then we use backpropagation to fine-tune the final network weights by minimizing the cross entropy between predicted binding probabilities P n and target binding probabilities t n as follows,where N is the total number of training peptides. The pre-training module mcRBM of HONN extends traditional Gaussian RBM to model both mean and explicit pairwise interactions of input feature values, and it has two sets of hidden units, mean hidden units h m modelling the mean of input features and covariance hidden units h g gating pairwise interactions between input features. If the gating hidden units are binary, they act as binary switches controlling the pairwise interactions between input features. The energy function of mcRBM with factorized weights for reducing computational complexity is defined as follows,where i indexes visible units such as peptide sequence features, j indexes hidden units and f indexes the factors. Using this energy function, we can derive the conditional probabilities of hidden units given visible units, as well the respective gradients for training the network. The structure of this factorized mcRBM is shown on the bottom of the right panel of, the hidden units on the left model the mean of input features and those on the right model the input covariance. During pre-training, we used Contrastive Divergence () to learn the factorized weights in mcRBM as in Gaussian RBM, and we used Hybrid Monte Carlo sampling to generate the negative samples as inwith 20 leap-frog steps. The structures and parameters of both DNN and HONN are decided based on performance on validation sets. In fact, for our HONN, only the learning rates, batch size and the number of hidden units need to be carefully tuned, and the final performance is not sensitive to other hyper-parameters. During the training phase, our algorithm randomly selects 10% of the original training data as validation set for early stopping. When the algorithm monitors that the validation error increases up to 10 times even if the training error is still decreasing, we end the training process for early stopping. Although HONN can be easily extended to have many upper layers to form a deep architecture, HONN without deep extensions works best in all our experiments, which is probably due to the limited training data we have.
High-order kernel modelsThe sequence of the descriptors corresponding to the peptide X  x 1 ; x 2 ;. .. ; x jXj ; x i 2 R (as in, e.g.) can be modelled as an attributed set of descriptors corresponding to different positions (or groups of positions) in the peptide and amino acids or strings of amino acids occupying these positions:where p i is the coordinate (position) or a set (vector) of coordinates and d i is the descriptor vector associated with the p i , with n indicatingthe cardinality of the attributed set description X A of peptide X. The cardinality of the description X A corresponds to the length of the peptide (i.e. the number of positions) or to in general to the number of unique descriptors in the descriptor sequence representation. A unified descriptor sequence representation of the peptides as a sequence of descriptor vectors is used to derive attributed set descriptions X A .
High-order kernel functions on peptide descriptor sequence representationsIn the following, we define kernel functions for peptides based on peptide descriptor sequence representations (such as in). The proposed kernel functions for peptide sequences X and Y have the following general form:where M is a descriptor sequence (e.g. spatial feature matrix) representation of a peptide, X A Y A  is an attributed set corresponding to M(X) (M(Y)), k d ; ; k p ; , are kernel functions on descriptors and context/positions, respectively, and i X , i Y index elements of the attributed sets X A , Y A. While k d measures similarity between descriptors, the context/position kernel k p measures similarity of the of the descriptor context (e.g. position and spatial distribution of amino acids). A number of kernel functions for descriptor sequence (e.g. matrix) forms M is described below. Using real-valued descriptors (e.g. vectors of physicochemical attributes), with RBF or polynomial kernel function on descriptors, the k d d a ; d b  is defined as exp c d jjd a  d b jj where c d is an appropriately chosen weight parameter, or hd a ; d b i  c p where p is the degree (interaction order) parameter and c is a parameter controlling contribution of lower order terms. Kernel functions k p ;  on position sets p i and p j are defined as a set kernelki; jja; b where ki; jja; b  1 ji  jj a  b  expalogji  jj  b is a kernel function on pairs of position coordinates (i, j). The position set kernel function above assigns weights to interactions between positions (i, j) according to ki; jja; b. The descriptor kernel function (e.g. RBF or polynomial) between two descriptors d i  d i 1 ; d i 2 ;. .. ; d i R  and d j  d j 1 ; d j 2 ;. .. ; d j R  induces high-order (i.e. products-of-features) interaction features (such as d i1 d i2. .. d ip for polynomial of degree p) between positions/attributes. The proposed kernel function (Eq. 3) captures high-order interactions between amino acids/positions by considering essentially all possible products of features encoded in descriptors d of two or more positions. The feature map corresponding to this kernel is composed of individual feature maps capturing interactions between particular combinations of the positions. The interaction maps between different positions p a and p b are weighted by the position/context kernel function k p p a ; p b .
DataTo assess the performance of our high-order methods, we tested our methods on three prediction tasks: 1. MHC-I binding prediction. The datasets used for MHC-I binding prediction task are listed inFor all the tasks, we focused on the 9-mer peptides. For MHC-I binding prediction, we threshold at a standard value IC50  500 to separate binding peptides (IC50 < 500) and non-binding (IC50 > 500) peptides and focus on three alleles, HLA-A*0201, HLA-A*0206 and HLA-A*2402. The choice of these alleles is motivated by the target population group (Japanese) in our research lab. The application of our method to other alleles or peptide lengths would be straightforward.
Training and testing protocolFor MHC-I binding prediction, we train our models for each allele on the publicly available data from the Immune Epitope Database and Analysis Resource (IEDB) (). The datasets (http://www.iedb.org) are labelled with IEDB suffix in. For testing, we use the experimental data from our lab for each allele. These datasets are denoted with 'Japanese' suffix in. For 'Japanese' data, the experimentally determined binding strength is measured as log K d , where K d is a dissociation coefficient, i.e. higher negative values of log K d  suggest stronger binders. The training 'IEDB' datasets and the test 'Japanese' datasets are completely disjoint. The average sequence identity between any peptide in the 'Japanese' datasets and the most similar peptide from IEDB data is about 4655% (Supplementary).
Evaluation metricsTo assess performance, we use two sets of metrics, classical binary metrics and non-binary relevance metrics. Binary performance metrics. We used (i) area under ROC curve (AUC) and (ii) area under ROC curve up to first n false positives (ROC-n). Non-binary relevance/quality metrics. While classical binary performance metrics use binary relevance (i.e. '1'  relevant, '0'  nonrelevant), to take into account more 'precise' relevance measure, i.e. theHigh-order neural networks and kernel methodsbinding strength of the peptides, we use normalized discounted cumulative gain (nDCG), a classical non-binary (graded) relevance metric. Given a list of peptides P 1 ;. .. ; P N ordered by the output scores of the predictor f P 1 ;. .. ; f P N , the discounted cumulative gain (DCG N ) is defined as a sum of individual peptide relevance scores (experimentally determined binding strength) q 1 ; q 2 ;. .. ; q n discounted by the log of their position i in the list:The normalized DCG N is defined as a ratio between DCG of the method and an ideal DCG iDCG N (i.e. DCG of an ideal ordering of peptides from the highest degree of binding affinity to the lowest binding affinity):The normalized DCG N value is then ranges between 0 and 1, with nDCG N  1 corresponding to the ideal value (i.e. normalized DCG  1 when the predictor orders peptides according to their actual binding strength). We find this measure (nDCG) to be more indicative of the prediction performance of the MHC-I binding prediction method as it directly assesses whether the predictor ranks stronger binders higher than weaker binders [as opposed to binary measures (e.g. area under ROC curve) that measure whether 'binders' are ranked higher than 'nonbinders' irrespectively of the actual peptide binding strength. This measure is popular for assessing performance of the document retrieval systems (e.g. Web search engines) as it is maximized if the most relevant documents appear at the top of search results, but it has not been used to differentiate performance of the MHC binding predictors. In the case of the peptide-MHC prediction, the nDCG is maximized if peptides are placed (according to the predictor output) in the ideal order: from the strongest binders to the weakest/non-binders. We emphasize that the two methods with the same AUC scores may differ significantly with respect to their nDCG scores: even with the equally good separation between 'binders' and 'non-binders' for the two methods, the method that correctly ranks stronger binders higher than weaker binder will have a higher nDCG score.
ResultsWe first present results for MHC-I binding prediction on benchmark datasets and experimental data from our lab (section 5.1). We show next results on predicting peptides NP by the MHC pathway (section 5.2). Finally, we show results for predicting promising T-cell epitopes for clinical development (section 5.3). The following AUC and nDCG scores are shown in percentage.
MHC-I binding predictionWe train a DNN, a high-order semi-RBM (HONN) and a highorder kernel SVM (hkSVM) on IEDB data. In our experiments, we use BLOSUM substitution matrix as continuous descriptors of input peptide sequences. We compare with the popular NetMHC method that has been shown to yield state-of-the-art accuracy for MHC-I binding prediction with respect to other best published methods (see e.g.). We first use 'Japanese' datasets to test our methods. Results are shown in Tables 35 for target alleles on Japanese test datasets. Corresponding ROC curves are shown in(top row). We also plot nDCG@n curves in(bottom row), where nDCG @n is nDCG up to nth peptide in the sorted output (i.e. nDCG of the top-n predicted peptides). As evident from the AUC and ROC-n results in the tables and ROC plots, our method achieves significant improvements in separating 'binders' versus 'non-binders'. For example, for A2402 allele, ROC-n  10 score increases from 66.88 for NetMHC to 77.76 for HONN and hkSVM. Similar improvements are observed on A0201 allele data where ROC-n  10 score improves from 26.61 for NetMHC to 35.59 with HONN and hkSVM. Observed improvements in the AUC and ROC-n scores across all alleles are significant (paired signed rank test, P value 1.22e-4). To further validate our methods, we used recent benchmark MHC-I binding data proposed inconsisting of the training data (BD2009) and independent (BLIND) test data (Supplementary). We report performance on the independent test data (BLIND) in Supplementary Table S9. As can be seen from the results in the table, while the area under ROC curve (AUC) scores are very similar for both our method and the NetMHCmethod, for the very highest ranked peptides [low false-positive (FP) rates], both hkSVM and HONN  hkSVM perform better on average compared with NetMHC as measured by ROC-n scores [e.g. ROC-1 scores of hkSVM or HONN are higher in about 67% (31/46) of the. Observed improvements in ROC-n scores (low FP rates) are significant (paired signed rank test P values  7e-3 and 1.38e-2 for hkSVM and HONN  hkSVM, respectively). At the same time, the results in terms of nDCG quality scores suggest significant increase in ranking quality (Tables 68). Our method ranks peptides by their actual binding strength significantly better than other methods. We observe that strong binders are placed much higher in the classification results compared with the state-of-the-art NetMHC method. For instance, for the A0201 allele, nDCG@n scores improve from 60.98, 63.50 achieved by NetMHC to 65.94, 70.61 using our HONN method for n  20 and n  30 respectively. We note that for both HONN and DNN, the pre-training is critical to achieve good performance. The performance comparisons of DNN and HONN with and without pre-training are in the supplementary material (Supplementary Tables S2S7). All the results of DNN and HONN reported in the main article are based on pretraining and fine-tuning. Using a combination of network and kernel models further improves peptide-MHC recognition as evident by the increase in both area under ROC curve scores (improved 'binder' versus 'non-binder' separation) and nDCG metric quality scores (improved ranking of peptides by binding strength). We note that unlike the previous approaches that utilized quantitative binding information during training, no quantitative information regarding actual binding strength was used to train our models. However, even with only binary training data [i.e. only with binding (B) versus non-binding (NB) information], our models correctly order peptides according to their binding strength. This can be attributed to explicit high-order interaction modelling by our method(f) Normalized DCG curves on test A2402 allele.High-order neural networks and kernel methodsthat allows to capture intrinsic binding strength information. Nevertheless, our models can easily use quantitative training data (e.g. IC50) to further improve our results. To visualize the learned weights of HONN, we used 8 mean hidden units, 1 covariance hidden unit and 1 factor unit to train HONN on the training data of A2402. We obtained AUC score 86.02 and nDCG score 85.01 that are slightly worse than the ones in Tables 5 and 8. In, the factorized rank-1 interaction weight vector with absolute values greater than 0.1 is shown in the top, and the weight matrix connecting input features and mean hidden units with absolute values greater than 0.02 is shown at the bottom. Thisshows that positions 2, 8, 9 and the interaction between middle position and position 9 are very important for predicting 9-mer peptide binding, which has experimental support from the crystal structure of the interaction complex ().
NP peptide predictionWe test ability of our methods on a difficult task that aims at predicting whether a peptide is NP by the MHC pathway ('eluted'). This is a very important task as only a fraction of binding peptides (see 'MHC-I binding task' in Section 5.1) constitute a set of peptides that are processed to the surface of a cell and may serve as epitopes. Eluted peptide prediction thus aims at verifying whether a peptide not only binds to a given MHC molecule, but that it is also NP by MHC pathway in vivo. To train our models, we used the data provided by 2012 Machine Learning in Immunology competition (MLI-II) http://bio. dfci.harvard.edu/DFRMLI/ HTML/natural.php.We directly train our models to recognize NPP peptides, using 'eluted' peptides as a positive set, and all other peptides (non-binders  non-eluted binders) as a negative set. We then test our models on the data composed of non-eluted binding peptides, non-binding peptides and NP ('eluted') peptides. We used the same training and test split as specified in the competition. We compare our approach with the popular NetMHC method, which was used as a benchmark in the competition, as well as the recently introduced MHC-NP () method that yielded state-ofthe-art accuracy for NP peptide prediction.lists results of NP peptide prediction (9-mers) on the test set in terms of AUC, ROC-n and F1 scores. Our approach significantly outperforms both NetMHC method and the MHC-NP () method. Supplementary table S11 shows the performance of hkSVM for the other test alleles with similar improvements on test peptides with all varying lengths (8-mers to 11-mers).
Epitope predictionWe demonstrate ability of the method to predict promising peptides for clinical development using as an example WT1-derived strong binding peptides WT-TEST-PEPTIDE1 and WT-TEST-PEPTIDE2 discovered by NEC-Kochi Univ. We compare the performance of our method and NetMHC by 'predicting' in a retrospective way these T-cell epitopes from WT1 antigen. Peptides (441 9-mers) that are part of WT1 antigen are ranked by the output scores of NetMHC and our method (HONN and hkSVM). The order of the WT-TEST-PEPTIDE1 and WT-TEST-PEPTIDE2 peptides in the output (out of the 441 peptides) of the two prediction methods is given in. As evident from the table, our method ranks these peptides higher than NetMHC method.
Discussion and future workIn this article, we propose using nonlinear high-order machine learning methods including HONN and hkSVM for peptide-MHC I protein binding prediction. Experimental results on both public and private evaluation datasets according to both binary and non-binary performance metrics (AUC and nDCG) clearly demonstrate the advantages of our methods over the state-of-the-art approach NetMHC, which suggests the importance of directly modelling nonlinear high-order feature interactions across different amino acid positions of peptides. Our results are even more encouraging considering that our models were only trained on a subset of the binary binding datasets used by NetMHC and NetMHC was also trained on private quantitative binding datasets. In the future, we will use available quantitative binding datasets to refine our HONN model with possible deep extensions, and we will incorporate the descriptors of structural contacting amino acids on MHC proteins into current feature descriptors. The addition of peptide binding strength and structural information will potentially further improve the performance of our current models.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
P.P.Kuksa et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
