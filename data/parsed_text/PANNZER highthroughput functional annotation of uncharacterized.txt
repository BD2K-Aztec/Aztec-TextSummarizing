Motivation: The last decade has seen a remarkable growth in protein databases. This growth comes at a price: a growing number of submitted protein sequences lack functional annotation. Approximately 32% of sequences submitted to the most comprehensive protein database UniProtKB are labelled as 'Unknown protein' or alike. Also the functionally annotated parts are reported to contain 30â€“40% of errors. Here, we introduce a high-throughput tool for more reliable functional annotation called Protein ANNotation with Z-score (PANNZER). PANNZER predicts Gene Ontology (GO) classes and free text descriptions about protein functionality. PANNZER uses weighted k-nearest neighbour methods with statistical testing to maximize the reliability of a functional annotation. Results: Our results in free text description line prediction show that we outperformed all competing methods with a clear margin. In GO prediction we show clear improvement to our older method that performed well in CAFA 2011 challenge. Availability and implementation: The PANNZER program was developed using the Python programming language (Version 2.6). The stand-alone installation of the PANNZER requires MySQL database for data storage and the BLAST (BLASTALL v.2.2.21) tools for the sequence similarity search. The tutorial, evaluation test sets and results are available on the PANNZER web site. PANNZER is freely available at http://ekhidna.biocenter.helsinki.fi/pannzer.
IntroductionA correctly annotated proteome is the cornerstone of a successful genome research project and therefore accurate and reliable functional annotation tools are needed. However, due to the huge amount of various sequence data and diverse methods used in the functional annotation processes, a large part of these sequences are at risk of being annotated incorrectly (). The last decade has seen an explosion in the number of genomes being sequenced, and the near future will increase the number far higher. Experimental characterization is not a viable option for). Low-throughput methods are time consuming, complex and expensive and therefore restricted only to small subsets of proteins of interest (). Annotations are also generated by biocurators by interpretation of experiments from literature. The quality of these literature-based annotations relies heavily on the expertise of biocurators (). While experimental methods have problems, the computational methods struggle on a whole new level of challenges. The error rate of computationally annotated databases has been increasing rapidly in recent years. A recent study estimates the error level has risen from 5 to 40% within the last decade (). In the Gene Ontology (GO) databases the error levels grow even higher: among computationally created GO annotations, the error level has been estimated to be as high as 49% and even within manually curated GO annotations between 28 and 30% (). The increasing error rate in these databases is believed to stem mostly from the propagation of erroneous annotations with usage of poorly performing in silico functional annotation tools (). We have designed a high-throughput annotation tool called Protein ANNotation with Z-score (PANNZER) in order to create more reliable annotations and thereby reduce further error propagation in annotation projects. PANNZER uses the whole sequence similarity neighbourhood and weighted statistical testing in the annotation process in an attempt to maximize the evidence for correct annotation. In doing so, PANNZER prevents function transfer from incorrectly annotated sequences to an uncharacterized sequence. Here, we evaluate PANNZER in two separate tasks: in the prediction of free text description lines (DE) and in the prediction of GO classes. The description line is a free text sentence about the protein function. Written by biologists, it contains valuable information in human readable format. Therefore, it is surprising how little attention correct DE annotation has gotten in recent years. Some methods do exists, e.g. GeneQuiz (), PEDANT (), AutoFACT () and Blannotator (). We introduce a principled metric for the evaluation of description prediction, which allows numerical comparison of description similarities. In the prediction of free text description line, we show a clear improvement to other existing methods. The GO functional annotation has become the standard tool in computationally based bioinformatics analyses. Due to this, the majority of method development in functional annotation is nowadays focused on GO classes, e.g. GOtcha (), Argot 2 () and Blast2GO (). A more comprehensive list of GO prediction tools can be found from. Our results show an improved performance over alternative scoring methods and we also show improvement to our earlier version of PANNZER that was ranked third in Critical Assessment of protein Function Annotation algorithms (CAFA) 2011 challenge ().
MethodsThe PANNZER method predicts protein function using a weighted k-nearest neighbours approach with statistical testing.The functional annotation can be predicted as description lines or GO classes. PANNZER starts with a sequence search against the sequence database. The resulting Sequence Similarity Result List (SSRL) provides the candidate descriptions. The SSRL is then partitioned into clusters according to description similarity. The support for the candidate clusters is evaluated using a sophisticated regression model. The description prediction outputs the most representative description. The GO prediction performs an enrichment analysis of GO classes in the SSRL. These steps are outlined inand described in detail below.
Sequence search and filtering of resultsPANNZER starts the analysis with a sequence similarity search against UniProtKB database (). Here we have used standard BLAST () search, but other sequence search methods could be used (e.g. SANS). A large number of locally similar but globally dissimilar sequences in the result list sometimes biases results towards large sequence families. Therefore, we limit the number of sequences taken to the analysis and focus only on the sequences that obtained the strongest results from the sequence scoring and apply pre-set filtering thresholds on alignment coverage, identity percentage, sequence length and informative descriptions. The alignment coverage values are obtained by evaluating the sequence alignment areas covered in the query and target sequence. The information density threshold restricts sequences monitored only to the sequences with informative descriptions and omits uninformative descriptions like 'putative uncharacterized protein'. The selection of informative descriptions is based on Information Density Score (IDS):where w is a word in description D and n is the number of words in a description D. idf(w, D) is an Inverse Document Frequency score for a word: idfw; D  log jDj jfd 2 D : w 2 dgj ;Sequencewhere jDj is the total number of descriptions in the corpus (i.e. database), and jfd 2 D : w 2 dgj is the number of descriptions d where the word w occurs. IDS emphasizes the descriptions that contain specific terms over descriptions containing only general terms. The assumption here is that the words conveying specific functional information are rarer in corpus than the general words. Filtering thresholds are shown in the Supplementary Material. These values were optimized against the training set. The filtered SSRL is used in later steps of PANNZER (see).
Non-linear weighting of taxonomic distancesOne information source that is omitted by the standard sequence search is the evolutionary distance between the query and target species. It is intuitive to give more emphasis to sequence matches found from species that have smaller evolutionary distance. Unfortunately we do not have evolutionary distances available across all the species and therefore we decided to use the distances in the NCBI taxonomic tree as an approximation for the evolutionary distances. Correlation between these distances and description similarities were not linear since according to the general paradigm, paralogs are more likely functionally differentiated. This was corrected with a non-linear similarity function between the descriptions of compared query and target sequence. More details on this process can be found in the Supplementary Material. The output from this process, non-linear taxonomic distance score, was one of the inputs for the re-scoring of sequence hits.
Re-scoring sequence hitsIn the second step of the PANNZER pipeline we re-score the sequence hits using a sparse regression model that combines various signals from sequence alignment and the signal from the non-linear taxonomic distance score. All regression models were created in the R analysis environment using Lasso and LEAPS packages (). We computed for each found sequence match various values that measure the goodness of the match to the target sequence. These are the BLAST bit score, the BLAST e-value, the sequence identity, the query alignment coverage and the target alignment coverage. In addition, we had the taxonomic distance-based weight score. The regression model was trained against the Description Similarity Measure (DSM) of training data (see Supplementary Material on training). We found that the BLAST bit score or the e-value alone are not the best correlating scores against the functional annotation. The final model, RegressionModel1, combines several measures: RegressionModel1  0:21  log 10 idp  0:26  Cov q  Cov t   0:40  Cov t  TaxDist q;t ;where idp is the alignment identity percentage and Cov is the alignment coverage in query (q) and target (t) sequences. The TaxDist (Supplementary) is the non-linear taxonomic distance score between query and target species.
Description similarity measureIn PANNZER we use the DSM to cluster candidates in the SSRL. The DSM is based on the Term Frequency-Inverse Document Frequency (tfidf) which is a standard information retrieval and text mining method and is commonly used in clustering related documents in e.g. web search engines. The tfidf is a weighting scheme for measuring how important a word is to a document in a corpus.The score upweights words which are frequent in the document and at the same time downweights words which are frequent in the corpus. This kind of an approach helps to control the fact that some words are generally more common than others and therefore possibly have a trivial meaning. For example, the word 'protein' is frequently used in non-related cases in biological databases and is not very informative to a functional description, whereas the word 'hydroxyltransferase' occurs rarely and only in certain descriptions and is therefore more descriptive. tfidf is defined as: tfidf w; d; D  tfw; d  idfw; D;where tf(w, d) is the frequency of word w in document d. D is the corpus (i.e. database). To measure how similar two descriptions are to each other, the tfidf is used to weight common words in descriptions. In PANNZER the description similarity is calculated by using the tfidf weighting in a cosine similarity function. The cosine similarity is a dot product of the tfidf scores of common terms between two descriptions. It is calculated using the following equation:where A and B are vectors holding tfidf weights of common words between two descriptions. Examples of DSM scores between descriptions are presented in. In the PANNZER methodology the DSM is used to generate description clusters of SSRL hits by using hierarchical clustering with average linkage.
Selecting the best clusterThe next step in PANNZER methodology is to select one of the clusters as the best representative for the query sequence. We define a relevance score that is used to separate good representative clusters from the randomly occurring clusters. It would be intuitive to use a single score function for cluster voting. We, however, propose a combination of score functions that is obtained with sparse regression by training the model against the DSM (see Supplementary Material). The relevance score (Equation 6) uses output from three score functions: GSZ (), word score (WS) () and weighted word score (WWS) as an input (Equations. 810). RegressionModel2  0:59  0:53  log trunk WS  0:02  WWS  0:0004  GSZ cluster ;whereEach score function in Equation 6 was tested for preprocessing before regression with simple function (X, X 2 , ffiffiffiffi X p ; log trunk X, etc.). These improved the performance of regression model (see Supplementary Material). GSZ, WS and WWS are defined below. The GSZ cluster evaluates the overlap of two sets of sequences: (A) the SSRL (i.e. the BLAST result list) and (B) the set of sequences in the database that have one of the clustered descriptions (functionally related sequences). GSZ analyses overlap in a weighted manner and takes the sum of regression score values (Equation 3) for sequences that are in the cluster and creates a Z-score normalization with the mean and STD estimates discussed in the Supplementary Material and in earlier publication (). This emphasizes clusters of descriptions that are common in SSRL but rare in the background. For WS we first calculate WS w for every unique word w that occurs in SSRL descriptions d:where S w is the subset of SSRL sequences that have the word w in the description and Bit is the BLAST bit score. The WS score used in Equation 6 is an average of WS w scores over the description d. This score function emphasises descriptions containing words that are frequently seen in the descriptions in SSRL. This kind of a word scoring scheme has proved to perform well in previous studies (). The WWS w for word w is derived from WS w , but is weighted by Jaccard Similarity Coefficient JSC w : JSC w  jA \ Bj jA [ Bj ;where A is the SSRL and B is the set of descriptions where the word w occurs in the whole database D. Note that A \ B is the subset of descriptions where the word w occurs in SSRL. WWS w is:This function emphasises words that are frequent in the SSRL and are not frequent in the background. The WWS score used in Equation 6 is a average of WWS w scores over the description d. The selected score functions differed in the way how they analyse the SSRL. Two score functions, GSZ and the WWS in Equation 6, do standard statistical testing against the background (i.e. whole database). Only WS does not consider background. Score functions can also be split into two other groups: GSZ uses a group of sequences with similar descriptions (description cluster) as input, whereas WS and WWS define first a score for each single word observed in descriptions and then combine the score of single words to generate the signal of whole description. Having selected the best cluster according to Equation 6, the PANNZER method selects one of the descriptions as a representative for the whole cluster. The representative description is the most frequent description in the cluster.
Scoring GO classesA second form of annotation is the prediction of GO classes. Here we use a method that resembles the description prediction above. First, the direct GO annotations as well as parent classes are collected from UniProt-GOA database for all the sequences in the SSRL. We collected different variables for SSRL and each GO class. These variables include the count of GO class members in the SSRL, the size of GO class in the whole database, the sum of RegressionModel1 scores within GO class members in SSRL, size of SSRL, size of whole database, etc. These in turn are used to define a various score values for each of GO classes. Earlier similar works () have selected a single score function to estimate the score value for each GO class. We decided to use a weighted sum of score functions also here. Again the motivation is to emphasize the common signal in different score functions and lessen the different noise signals. The weighted sum was obtained using sparse regression. We optimized the regression against weightedLin similarity (see GO semantic distances below). WeightedLin was used to test how similar the predicted GO class was to nearest correct GO class. In the final regression model, we excluded all terms that had negative correlation with predicted variable from the model. These terms create non-linear signal that causes non-monotonic behaviour in the model and they constitute a small subset of total signal. The training and evaluation datasets are described later. The regression model that we obtained was: RegressionModel3  0:36  0:01  logJSC  0:02  log mod GSZ  3:67  10 5   ffiffiffiffiffiffiffiffiffiffiffi jGOj p  0:007where log mod x  signx  logabsx  1 ; if absx > 1 x; ; if absx 1 (and jXj is the size of set X. JSC is the Jaccard Similarity Coefficient between the GO class hits in SSRL and GO class occurrences in the whole UniProtKB database and GSZ is a weighted version of hypergeometric Z-score. It is calculated for GO class members in the SSRL using the scores from RegressionModel1 as weights. GSZ is explained in the Supplementary Material and in previous publications (). In addition, Equation 11 uses various simple functions, like squareroot, log and modified log (log mod ) to improve the regression performance (see Supplementary Material for details).
GO semantic distancesThe optimization of regression models for GO classes required a similarity measure that estimates how close two GO classes,predicted and correct, are in the GO tree. The GO similarity methods give the strongest similarity when predicted and correct GO class are exactly the same and weakening similarity when two classes move away from each other in the GO structure (). We used two previously published GO semantic similarity measures, Lin-score () and weighted Lin-score () for regression model training and evaluation. We also used two modified versions of Lin-score and weighted Linscore, called Path Lin-score and weighted Path Lin-score. We also used a simple Jaccard similarity (Equation 9) that compared parental classes of two tested GO classes. GO semantic distances are discussed more in the Supplementary Material.
Training and evaluation datasets for description predictionIn the PANNZER project we used training datasets to find parameters for the sparse regression model and evaluation test sets to compare prediction accuracy. The test sets used in training and evaluation are separate and the training data were never used in evaluation. Sequences were selected from UniProtKB/SwissProt (downloaded November 14, 2012) to test and evaluation datasets so that: (i) they have high-quality annotation, (ii) they do not have considerable sequence similarity with each other and (iii) they do not have strong similarity in description line with each other. More details on this process are shown in the Supplementary Material. The evaluation dataset was further divided into the eukaryote and prokaryote datasets. Viruses, environmental samples and unclear taxonomic cases were excluded from the analysis. These evaluation datasets allow the analysis of methods at different parts of evolutionary tree. Our final evaluation datasets were 2954 prokaryotic sequences and 5115 eukaryotic sequences.
Training and evaluation dataset for GO predictionGO regression training and evaluation sets were created using the same rules as description test sets above. We first selected only the sequences that had a manually curated GO annotations (i.e. GO annotations with non-IEA evidence code). Second, we excluded sequences that had annotations only in very large GO classes. Large GO classes are the ones that have many members in the GOA database. Third, the sequences were filtered for mutual sequence similarities starting from randomly selected sequence. The final training set included 8003 sequences and final evaluation set had 80 027. More detailed description is represented in Supplementary Material. Methods were also evaluated without the GO annotations with ISS evidence code. However, qualitatively the differences between the methods stayed the same. The GO evaluation only considered GO annotations that were observed in the annotations of sequences in the BLAST results. Our scoring used only these GO annotations as True Positive set. Although this alters recall values, it does not affect the ordering of the methods.
Evaluation databasesSequence similarity searches were conducted against a UniProtKB () database from which the evaluation and training sets were removed. This imitates the situation with novel sequence that cannot be found from database. This also ensures that there is no circular logic in the test, where simple matching of the query with itself in the database would always lead to optimal end result. This was called the NOSELF evaluation database. This was also used in the GO evaluation task. With well-annotated sequences, like we had in our test sets, there is always the risk that annotations have already propagated to other sequence neighbours creating an annotation cloud around the query sequence. To ensure that we can test also sequences that would not have exactly matching description in their sequence neighbourhood we modified our evaluation database so that we selected first the sequence neighbourhood for each query sequence and removed every sequence from the neighbourhood that were at least 90% identical to query sequence and has an identical description annotation. This database version was called NOCLOUD. More details on the process can be found in the Supplementary Material.
Comparison to other description prediction methodsIn the description prediction we used two different evaluation test sets: prokaryote and eukaryote. The eukaryote test set was found to be more difficult to functionally annotate correctly than the prokaryote set. We did the prediction for the prokaryote test set with BLAST-based methods: Best BLAST Hit (BBH), Best Informative BLAST Hit (BIBH), Blannotator () and PANNZER using NOSELF and NOCLOUD databases. We also made the prediction using RAST (Rapid Annotation using Subsystem Technology) server () that uses FIGfam database () in functional annotation. It is noteworthy that the PANNZER tool is the only method from these that does statistical testing in prediction. The BIBH was derived by going through SSRL in best first order and removing following words from descriptions: 'hypothetical',shotgun', 'cdna' and 'family'. If after word removal there were no words left in description, the description was skipped and the procedure was repeated to the next description in SSRL until an informative description is found. RAST and Blannotator are designed for prokaryotic annotation. Therefore for the eukaryote dataset were predicted by using only BBH, BIBH and PANNZER using NOSELF and NOCLOUD databases.
Comparison to other GO prediction methodsWe show two different types of evaluations. One is the comparison with GO semantic similarities. This shows the performance of the generated regression models: How good they are at estimating the distance of GO classes observed in the SSRL from the correct GO classes. The other evaluation was based on classifier evaluation using Receiver Operating Characteristics (ROC) curves and Precision-Recall (PR) curves. These were also used to calculate Area Under Curve (AUC) values from ROC curves and maximum F-measure values from PR curves. Evaluation of GO classifications has one significant problem: Extremely varying GO class sizes. This results in a situation where naive prediction that simply ranks GO classes in their size order performs really well (). This problem can be corrected by (a) evaluating each GO class separately and combining them, (b) excluding largest GO classes and/or (c) weighting each GO class prediction with its Information Content. Information Content is negative log of probability of the class (IC  logp class ). It is used extensively in GO distances () and it was proposed for PR and ROC curves in the CAFA challenge. We used combination of b and c in our analysis. We excluded GO classes that were larger than 1/3 of the whole data from the analysis and we weighted the remaining classes with IC in the result analysis (Supplementary Material explains this task more in detail).
ResultsThe evaluation of the PANNZER method performance was conducted using description prediction and also prediction of GO classes. For the description prediction and the GO prediction we used evaluation test sets described in Section 2. We did the GO evaluation to estimate the performance improvement of our latest version of PANNZER against the PANNZER version that participated in the CAFA 2011 challenge.
Description predictionTextual annotation, or descriptions of protein functions in biological databases, provides a concise source of knowledge about protein function and subcellular location. The computational evaluation of textual annotations has been considered to be too difficult due to usage of free text in descriptions. Here we present one approach to perform computational evaluation of free text annotations. In this study we used the DSM (Equation 5) to calculate similarity between descriptions. The description pairs are divided into bins according to the DSM. In following results 'correct' (DSM > 0.7) includes the very similar to original annotations and 'incorrect' (DSM < 0.3) completely different descriptions. The bins between 'correct' and 'incorrect' (DSM 0.7  0.3) are intermediate bins where we cannot say for sure if the annotation means the same as original or not.shows examples at various DSM levels. We also show how these different categories distribute in the BLAST results (
Description prediction of prokaryotesThe first functional annotation prediction with BLAST-based methods was done against the NOSELF database and the second prediction was done against the NOCLOUD database. The results indicate that the PANNZER method is able to find remarkably more hits that fall into category 'correct' than any other competing method (). In NOSELF category PANNZER is able predict 524% more correct functional annotations than competing methods and in NOCLOUD category the improvement is between 6 and 16%. The PANNZER method also proved to be much faster than Blannotator. An average run time for a single query was 1.3 s with PANNZER and 39 s with Blannotator, making PANNZER about 30 times faster. It is notable that RAST annotation was done against FIGfam database where the self hits and propagation cloud was not removed. Therefore the RAST results are not directly comparable to other methods.
Description prediction of eukaryotesUsing BLAST-based methods the eukaryotic test set proved to be more difficult to annotate correctly than prokaryotic test set. Especially with the eukaryotes the PANNZER method outperforms BLAST-based methods clearly (). In case of eukaryotes against the NOSELF the PANNZER method predicts 37% more 'correct' annotations than BBH and 12% more than BIBH. In 'incorrect' annotations there are 44% less hits than with BBH and 14% less than with BIBH. When compared with NOCLOUD the differences grow even higher in favour of PANNZER.
GO predictionA previous version of PANNZER method participated in CAFA 2011 challenge that provided an independent large-scale blind testing of GO prediction methods. PANNZER was ranked in top three over 54 competing methods (). The differences in the prediction accuracy was very small between best performing methods; Jones-UCL (), Argot 2 () and PANNZER. Since the Jones-UCL and the Argot 2 have no publicly available stand-alone version, we were not able to compare new PANNZER against these methods directly using our NOSELF database. Therefore we decided to only evaluate our latest version of PANNZER against the version that participated in the CAFA 2011 challenge. We compare how results from new and old methods correlate with various GO semantic distances. The overall improvement of the PANNZER performance is between 28 and 47%, depending on which semantic similarity measure was used ().In addition, we show GO analysis with three separate score functions: hypergeometric P-value, Jaccard and GSZ. This compares how different enrichment scoring functions rank GO classes for class prediction. Jaccard and GSZ were inputs to our PANNZER model training, whereas hypergeometric P-value has been used before in the GO prediction (). We also include two reference methods that were also used in CAFA competition: Best BLAST and Naive prediction. Best BLAST selects the maximum Bit score reported for GO class members. Naive prediction reports simply GO classes in their size order starting from largest class. These test whether our regression outperforms single score functions and reference methods from CAFA competition. We also evaluated the GO-prediction using PR and ROC-curves. Here the GO evaluation dataset is divided into Biological Process, Cellular Component and Molecular Function GO categories. The latest version of PANNZER outperforms the old version in prediction accuracy especially in BP (Figs 4 and 5). Difference between PANNZER versions is clearer from. In addition the individual score functions show weaker performance, although the difference to GSZ is quite small. Reference methods show clearly weaker performance than other methods. Especially Best BLAST is even weaker than naive prediction. This again underlines the fact that straight analysis of SSRL without any summarization of hits is sub-optimal annotation method.
DiscussionAs the amount of newly submitted sequences grow rapidly in public databases, and the functional annotation is critical step before studying these sequences, we need more reliable methods for in silico functional annotation. The PANNZER method outperforms competing methods in functional annotation prediction accuracy and brings novel statistical testings to the analysis. In particular, the k-nearest neighbour clustering with statistical testings bring major advantages over traditionally used nearest neighbour method (e.g. Best BLAST Hit). Our results in description prediction show that the use of the nearest neighbour does not bring any advantage in functional annotation. It is remarkable how evenly 'correct' and 'incorrect' description hits are distributed over the BLAST result lists. In the standard BLAST (using default parameters) against the NOSELF database, the 'correct' hit count does not rise above 'incorrect' count in any index of the result list, including the best hit (). It seems that the probability of having correct annotation from the best hit is no different to any other hit in a BLAST result list. Free text description is the most comprehensive way to describe functionality of a protein and is required for every protein sequence that is submitted to a public sequence database. The current release of UniProtKB contains more than 1.5 million unique descriptions about protein functions and GO annotations that contain today 40 000 non-obsolete live terms. Despite a large fraction of synonymous descriptions, the difference is considerable. GO annotation suffers of biased usage of large and general GO terms which explains the unexpectedly good performance of the Naive GO prediction method (Figs 4 and 5). According to our results Naive method outperforms the Best BLAST Hit method clearly. This highlights the fact that closest neighbour-based methods should be avoided. Surprisingly description prediction has obtained recently very little attention in the bioinformatics community. This could be because the free text annotation is seen as an ill-defined problem without effective evaluation metrics and difficulties in handling synonyms and homonyms. To alleviate these shortcomings to some extent, we propose DSM as a new standard in description evaluation. Since descriptions and GOs are used in different contexts, both annotations are needed. DE annotations are used by the biologists andThe improvement of the PANNZER method compared with version that participated in CAFA 2011 challenge are shown. other non-computationally related researchers, whereas GO terms are frequently used in computational functional analysis and have become the standard in, e.g. enrichment analysis. PANNZER is the only tool to our knowledge that does both types of prediction.Functional annotation of uncharacterized proteins in an error-prone environment
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
P.Koskinen et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
