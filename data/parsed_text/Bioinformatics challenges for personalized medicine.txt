Motivation: Widespread availability of low-cost, full genome sequencing will introduce new challenges for bioinformatics. Results: This review outlines recent developments in sequencing technologies and genome analysis methods for application in personalized medicine. New methods are needed in four areas to realize the potential of personalized medicine: (i) processing large-scale robust genomic data; (ii) interpreting the functional effect and the impact of genomic variation; (iii) integrating systems data to relate complex genetic interactions with phenotypes; and (iv) translating these discoveries into medical practice.
INTRODUCTIONWe are on the verge of the genomic era: doctors and patients will have access to genetic data to customize medical treatment. Consumers can already get 500 0001 000 000 variant markers analyzed with associated trait information (), and soon full genome sequencing will cost less than $1000 (). One group has performed a complete clinical assessment of a patient using a personal genome (), and the 1000 Genomes Project is sequencing 1000 individuals (1000 Genomes). In the coming years, the bioinformatics world will be inundated with individual genomic data. This flood of data introduces significant challenges that the bioinformatics community needs to address. This review outlines the developments that led to these challenges, the previous work that can address them and the need for new methods to address them. The challenges fall into four main areas:(i) processing large-scale robust genomic data; (ii) interpreting the functional impacts of genomic variation; (iii) integrating data to relate complex interactions with phenotypes; and (iv) translating these discoveries into medical practices. * To whom correspondence should be addressed.
THE PROMISE OF PERSONALIZED MEDICINEIn the last decade, molecular science has made many advances to benefit medicine, including the Human Genome project, International HapMap project and genome wide association studies (GWASs) (). Single nucleotide polymorphisms (SNPs) are now recognized as the main cause of human genetic variability and are already a valuable resource for mapping complex genetic traits (). Thousands of DNA variants have been identified that are associated with diseases and traits (). By combining these genetic associations with phenotypes and drug response, personalized medicine will tailor treatments to the patients' specific genotype (). Although whole genome sequences are not used in regular practice today (), there are already many examples of personalized medicine in current practice. Chemotherapy medications such as trastuzumab and imatinib target  The Author(s) 2011. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.Page: 1742 17411748
G.H.Fernald et al.specific cancers (), a targeted pharmacogenetic dosing algorithm is used for warfarin (International Warfarin) and the incidence of adverse events is reduced by checking for susceptible genotypes for drugs like abacavir, carbamazepine and clozapine (). Despite all of these advances, many challenges need to be addressed to make personalized medicine a reality. Today, a patient's genetics are consulted only for a few diagnoses and treatment plans and only in certain medical centers. Even if doctors had access to their patients' genomes today, only a small percentage of the genome could even be used (). Many of the annotations come from association studies, which tend to identify variants with small effect sizes and have limited applications for healthcare (). By addressing the challenges outlined in this review, bioinformatics will create the tools to tailor medical care to each individual genome, rather than rely on blanket therapies ().
CHALLENGE 1: PROCESSING LARGE-SCALE ROBUST GENOMIC DATASequencing technologies are becoming affordable and are replacing the microarray-based genotyping methods, which were limited to interrogating regions of known variation (). Now a whole genome or a few dozen exomes can be sequenced in <2 weeks with an error rate of  1 error per 100 kb (). Even such low error rates can lead to a significant number of errors; a 3 GB human genome would have 30 000 erroneous variant calls. The error rate from these technologies is a source of significant challenges in applications, including discovering novel variants. Each newly sequenced genome is expected to have between 100 000 and 300 000 previously undiscovered SNPs and <1000 somatic mutations per generation (1000 Genomes). The number of expected mutations may decrease as new genomes are sequenced; however, such a high number of errors turns variant discovery into a 'needle in a haystack' problem. Whenever a novel variant is identified, it will still have to be verified due to this false positive rate. In addition, other classes of variation, such as short insertiondeletion variants (indels), as well as copy number variants (CNVs) and structural variants (SVs), are even more difficult to detect using high-throughput sequencing. New algorithms for calling indels, CNVs and SVs from read data will be crucial in detecting these types of variations for clinical applications. Even high-quality sequence reads must be placed into their genomic context to identify variants, which is an active area of research since, for example, different mapping and alignment algorithms often yield different results. Because de novo assembly () is slow and complicated by repetitive elements, sequences are usually mapped to a genomic reference sequence instead. Algorithms such as BLAST () or SmithWaterman () have been traditionally used, but their execution speed depends on the genome size. While individual queries may only take seconds per CPU, aligning 100 million of them would require more than 3 CPU years. As a result, new algorithms are being developed to address this problem. BLAT is similar to standard sequence alignment, but also incorporates an indexed version of the genome instead of linear search (). Many packages like BLAT have been optimized for the alignment of short reads by using hashing, prefix and suffix trees or other heuristics (). BWA, used for the 1000 Genomes Project, is highly accurate with <0.1% errors for simulated data and can map  7 GB of short reads per CPU day (). To achieve the standard 30X coverage would still require 13 CPU days and so is ideally performed on a cluster or by using a cloud computing environment (), which can be used for efficient computational analysis of secure clinical data. A remaining challenge for short read assemblers is reference sequence bias: reads that more closely resemble the reference sequence are more likely to successfully map as compared with reads that contain valid mismatches. Proper care must be taken to avoid errors in these alignments, and is discussed in a recent review (). There is an inherent trade-off in allowing mismatches: the program must allow for mismatches without resulting in false alignments. Reference sequence bias is important when making heterozygous SNP calls and when analyzing allelespecific expression using RNA-Seq data (). The problem is exacerbated with longer reads: allowing for one mismatch per read is acceptable for 35 bp reads, but insufficient for 100 bp reads. When the diploid sequence is known, reference sequence bias can be avoided by mapping the reads to both strands, as can be done when mapping RNA-Seq reads to a sequenced genome. An alternative approach is to use ambiguous base codes to avoid the requirement of storing redundant sequences, such as with MOSAIK, developed by the Marth Lab (Michael Stromberg, Boston University). Using this approach, a C/T SNP can be represented as Y. This representation increases the storage requirements: because the genome is often stored in a hashed data structure, the number of keys and mappings increases to accommodate the new codes. Another challenge is developing new methods for novel SNP discovery: while the calling of common variants can be aided by their presence in a database such as dbSNP, accurate detection of rare and novel variants will require increased confidence in the SNP call. De novo alignment methods require too much computation time to be feasible and reference alignment methods are biased. The challenge is to develop new algorithms that are computationally tractable and still avoid reference sequence bias. Finally, there is a pressing need to improve quality control metrics. We can judge mapping and SNP call qualities by the ratio of transition (purine/purine or pyrimidine/pyrimidine) substitutions to transversion (purine/pyrimidine) substitutions. These ratios were established during previous sequencing efforts and we expect to see similar ratios (22.1) for newly human genomes (). When working with genomes from families, we can estimate errors with the Mendelian inheritance error (MIE) rate: impossible combinations of inheritance most likely represent errors (). Transition/transversion ratio and MIE metrics are useful for measuring the quality of a dataset and are used by most large projects, such as the 1000 Genomes project (1000 Genomes). At the individual SNP level, we must rely on relative quality scores, so in order to confidently identify novel variants we must be verify them with an independent Page: 1743 17411748method. Variants can be validated with targeted resequencing or genotyping arrays. Alternatively, whole genome resequencing by an orthogonal sequencing platform can be performed, but is expensive and time consuming.
Bioinformatics challenges for personalized medicine
CHALLENGE 2: INTERPRETATION OF THE FUNCTIONAL EFFECT AND THE IMPACT OF GENOMIC VARIATIONAfter genomic data has been processed, the functional effect and the impact of the genetic variations must be analyzed. Genome-wide association studies (GWASs) have been used to assess the statistical associations of SNPs with many important common diseases (WTCC). These methods are providing new insights, but only a limited number of variants have been characterized, and understanding the functional relationship between associated variants and phenotypic traits has been difficult (). In the strictest definition, a SNP is a single nucleotide variant where the allele frequency in the human population is higher then 1%. In this review, we use the term SNP in a broader sense to also include rare variants that occur in a smaller fraction of the population. Important issues for predicting the impact of SNPs are data management, retrieval and quality control. During the last few years, the number of known SNPs has increased at an exponential rate (). The dbSNP database (The SwissVar is a database of manually annotated missense SNPs (mSNPs) and contains 56 000 mSNPs from >11 000 genes. Another important resource for SNP data is the Online Mendelian Inheritance in Man (OMIM) database () of human SNPs and their associations with Mendelian disorders. The PharmGKB database contains manually curated associations between genes and drugs and a catalog of genetic variations with known impact on drug response, including >40 very important pharmacogenes (VIPs) and over 3400 annotated drugresponse variants. The Catalogue of Somatic Mutations in Cancer (COSMIC) at the Sanger Institute stores 25 000 unique mutations somatic mutation data related to human cancer extracted from the literature. A selection of the most significant SNP data sources is reported in Supplementary Table S1. In the last few years, several computational methods have been developed to predict deleterious missense SNPs (). These methods have used different approaches such as empirical rules (), Hidden Markov Models (HMMs) (), Neural Networks (), Decision Trees (), Random Forests () and Support Vector Machines (). The prediction algorithms input features generally include amino acid sequence, protein structure and evolutionary information. The amino acid sequence features rely on the physicochemical properties of the mutated residues such as hydrophobicity, charge, polarity and bulkiness. Protein structural information describes the structural environment of the mutation and has been successfully used to predict the protein stability change upon mutation (). Some of the most important features for the prediction of the impact of missense SNPs are derived from evolutionary analysis: critical amino acids are often conserved in protein families and so changes at conserved positions tend to be deleterious. New algorithms that include knowledge-based information are being developed (). Methods based on evolutionary information for the prediction of mSNPs include SIFT () and PolyPhen (). SIFT scores the normalized probabilities for all possible substitutions using a multiple sequence alignment between homolog proteins, and PolyPhen predicts the impact of mSNPs using different sequence-based features and a position-specific independent counts (PSICs) matrix from multiple sequence alignment. The PANTHER algorithm () uses a library of protein family hidden Markov models to predict deleterious mutations. Recent work shows that 3D structural features improve the prediction of disease-related mSNPs (). Knowledgebased information has been used to increase the accuracy of prediction algorithms to over 80%. For example, SNPs&GO () is an algorithm based on functional information that takes in input log-odd scores calculated using Gene Ontology (GO) annotation terms. MutPred () evaluates the probabilities of gain or loss of structure and function upon mutations and predicts their impact using a Random Forest-based Page: 1744 17411748
G.H.Fernald et al.approach. Selected methods for the prediction of deleterious mSNPs are listed in Supplementary Table S2 and more details about mSNP predictors have been recently reviewed () Prediction methods do not provide any information about the pathophysiology of the diseases and so experimental tests are required to validate genetic predictions. Laboratory validation is expensive and time consuming and so there is a need for fast and accurate methods for gene prioritization. Currently, the most effective strategy uses the concept of similarity to genes that are linked to the biological process of interest (guilt-by-association). The input data for the available gene prioritization methods are derived from functional annotation, proteinprotein interaction (PPI) data, biological pathways and literature. The SUSPECT algorithm prioritizes genes by comparing sequence features, gene expression data, Interpro domains and functional terms (). ToppGene combines mouse phenotype data with human gene annotations and literature. MedSim uses functional information from human disease genes or proteins and their orthologs in mouse models (). ENDEAVOUR is trained on genes involved in a known biological process and ranks candidate genes after considering several genomic data sources (). G2D prioritization strategy is based on a combination of data mining on biomedical databases and sequence features (). PolySearch analyzes biomedical databases to build relationships between diseases, genes, mutations, drugs, pathways, tissues, organs and metabolites in humans (). MimMiner ranks phenotypes using text mining by comparing the human phenome and disease phenotypes (van). PhenoPred detects genedisease associations using the human PPI network, known genedisease associations, protein sequences and protein functional information at the molecular level (). GeneMANIA () generates hypotheses about gene function, analyzing gene lists and prioritizing genes for functional assays. The method takes in input genes from six organisms and analyzes them using information from different general and organism-specific functional genomics datasets. For more details about gene prioritizing tools, a recently published review () and the Gene Prioritization Portal provide comprehensive descriptions of available predictors. The methods for the analysis of SNPs are mainly limited to the prediction of the impact of missense SNPs. New methods are needed to evaluate the impact of insertion, deletion and synonymous SNPs. In addition, there is a need to detect functional regions in the genome so that the effect of intronic SNPs can be analyzed, such as those in promoter regions and splicing sites. For noncoding regions, conservation across species is more difficult to detect. Fortunately, with the fast growth of functionally annotated genomes our ability to predict the impact of non-coding variants will increase. For example, SNPs occurring in transcriptional motifs can affect transcription factor binding, which suggests functional consequences for variants in regulatory regions (). Recently, a method to identify possible genetic variations in regulatory regions (is-rSNP) has been developed (). Is-rSNP combines phylogenetic information and transcription factor binding site prediction to identify variation in candidate cisregulatory elements. The detection of variants affecting splicing site is also an important task. The Skippy algorithm () analyzes the genomic region surrounding the variant to predict severe effects on gene function through disruption of splicing. A more exhaustive description of the methods for the prediction of deleterious variants in non-coding has been recently published (). Last year, the first edition of the Critical Assessment of Genome Interpretation (CAGI) was organized to assess the available methods for predicting phenotypic impact of genomic variation and to stimulate future research. In the first year of CAGI (http://genomeinterpretation.org/), the organizers provided six different sets of data for six different tasks. The majority of the participating groups submitted predictions for just two classes of experiments related to the detection of disease-related and functionmodifying variants. A few groups submitted predictions for the other categories: evaluation of risky SNPs from GWAS studies, interpretation of the Personal Genome Project data, prediction of mutations to P53 function and the response of breast cancer cell lines to different drugs. Several available predictors performed well for disease and functional predictions and there were promising results in the other categories. In the future, competitions such as CAGI will improve the quality of the available prediction methods and will renew the challenge for the understanding of genomic variation data.
CHALLENGE 3: INTEGRATING SYSTEMS AND DATA TO CAPTURE COMPLEXITYGiven the complex phenotypes involved in personalized medicine, the simple 'one-SNP, one-phenotype' approach taken by most studies is insufficient. Most medically relevant phenotypes are thought to be the result of genegene and geneenvironment interactions (). For example, drug response often depends on multiple pharmacokinetic and pharmacodynamic interactions, which form a robust and tolerant system with highly polymorphic enzymes and many interaction partners (). As a result of this complexity, a drugresponse phenotype of interest is likely to depend on many genes and environmental factors. Basic GWAS approaches for pharmacogenomics have had some success, including studies of warfarin that have linked the majority of variation in response to just two genes, CYP2C9 and VKORC1 (). These and other studies of warfarin have even led to an improved dosing algorithm with improvements over the traditional clinical algorithm (International). Clopidogrel response has similarly been associated with variants of CYP2C19 (). Despite this success, there is debate over whether or not traditional techniques will be successful for pharmacogenomics. There is concern that pharmacogenomics GWAS themselves are susceptible to many limitations: insufficient sample size, selection biases for genetic variants, environmental interactions that may affect the outcome measures and multiple genegene interactions that may underlie unexplained effects (). These limitations become particularly difficult when researching rare events such as the pharmacogentics of adverse events. The methods for GWAS are designed for single marker associations and are known to have limitations in explaining the heritability of disease (). It is unlikely that Page: 1745 17411748
Bioinformatics challenges for personalized medicinethese same methods will do any better with pharmacogenetics. In fact, if these methods are parameterized for the multiple-marker associations necessary for pharmacogenetics, then they will suffer from the 'curse of dimensionality' and lose a significant amount of statistical power (). For example, to evaluate all combinations of two SNPs for 1 million SNPs in a genome requires examining nearly 500 billion possibilities. The challenge for bioinformatics is to address this complexity by developing methods that combine multiple data sources without losing statistical power. Several groups have already tried to deal with this kind of complexity in GWAS for disease (). Exhaustive search () and forward search () have both been applied; however, the former can still lose statistical power and the later may miss some associations. Model selection methods have been successful with disease and trait GWAS studies by using selection techniques to choose multifactorial models that balance the false positive rate, statistical power and computational requirements of the search (). Given the size of the genomic datasets, dimensionality reduction methods such as principal components analysis, information gain and multifactor dimensionality reduction will be essential to make complexity algorithms tractable (). Some of these methods have proven successful for finding multilocus associations with diseases such as hypertension and familial amyloid polyneuropathy type I (). Many more feature selection techniques for bioinformatics are classified and discussed in a recent review (). These methods can be very effective when dealing with large datasets; however, they do not integrate with any external knowledge sources or inform the biology behind the interactions. Systems biology and network approaches address to the problem of complexity by integrating molecular data at multiple levels of biology including genomes, transcriptomes, metabolomes, proteomes and functional and regulatory networks (). We can view a disease or a drugresponse phenotype as a global perturbation of networks from their stable state (). This approach integrates biological knowledge from networks to make inferences about what genes or combinations of genes and other biological markers are more likely to be associated. Combining disparate data sources can result in novel associations and provide insight into genegene and geneenvironment interactions. One group created a diseasegene network by combining the diseases and associated genes available in OMIM (). Analyzing this network showed that disease genes are often non-essential and not necessarily hub genes. The same group created a drugtarget network and integrated that network with a PPI network. The network shows that similar drugs cluster together, palliative and etiological drugs show different topologies, and newer and experimental drugs tend toward polypharmacology (). A global mapping of pharmacalogical space can be made using chemical structure, disease indication and protein sequence and can be used to make predictions of polypharmacology (). Another suggestion is to integrate epigenetic information to further our understanding of drug phenotypes (). Pathway and gene set methods can also be applied to GWAS, where a set of genes is identified that is suspected to be associated. These methods are similar to Gene Set Enrichment Analysis (GSEA) for microarray expression data (). Usually a standard statistical test is used to determine if a set of genes is associated (), but other more specialized metrics have been created. The SNP Ratio Test compares the number of SNPs in a pathway to permuted sets, and the Prioritizing Risk Pathways method combines pathway and genetic data into a single metric (). Many groups hypothesize that the integrative approach of systems biology will successfully link genomic measurements with clinical applications (). Indeed, one group has integrated chemical similarity metrics, pharmacogenomic interactions and PPI to predictive method for pharmacogenes (). Another group has used similarity of drug ligand sets to predict and validate novel 'off-target' interactions (). These systems approaches are encouraging, but bioinformaticians need to be careful of a few pitfalls as they proceed. Methods need to be based on high-quality data to avoid the 'garbage-in, garbageout' phenomenon, especially when one incorrect assumption can propagate through multiple data source and magnify the error. For example, transferring annotations based on similarity works sometimes, but could easily associate a paralog with an incorrect function. Chemical similarity poses the same risk; two similar molecules may behave very differently biochemically. Finally, assumptions must also be examined carefully; for example, a method that relates gene expression with drug targets must bear in mind that most drugs bind proteins, not DNA or RNA.
CHALLENGE 4: MAKING IT ALL CLINICALLY RELEVANTThe ultimate challenge for this research is to apply the results for improved patient care. Much of this research has yet to be translated to the clinic. In fact, many physicians are unprepared to incorporate personal genetic testing into their practice and it is unclear how to best apply research results to improve patient care (). One of the areas where bioinformatics can have the greatest clinical impact is in pharmacogenomics. Most pharmaceutical development addresses medical problems with a 'one drug fits all' approach. Genetic variation has been shown to influence drug selection, dosing and adverse events (), and the therapeutic benefits of taking a genetically tailored approach to drug development is now recognized (). One study found that a hypothetical pharmacogenetically driven clinical trial of the anticoagulant warfarin could save up to 60% of the cost and reduce possible adverse events (). There are already many examples of drugs which have retrospectively been found to have strong pharmacogenomic interactions, including thiopurines for cancer () and the anticoagulant clopiogrel (). A trial for using rosiglitazone, an approved Type II diabetes drug, for Alzheimer's disease is an early example of prospective application of pharmacogenomics. The hypothesis was that ApoE4 non-carriers would have a better response than ApoE4 carriers. Page: 1746 17411748
G.H.Fernald et al.The initial Phase II pharmacogenetic-based results appeared to show that non-ApoE4 carriers showed improvement over placebo (). A later study of ApoE4-stratified patients showed no significant benefits; however, the idea of prospective gene-based stratification for drug trials still holds future promise (). Prospective gene-stratification hypotheses need to be generated for future trials and will require new bioinformatics methods (). Since new drugs will not have any known gene interactions, tools for predicting drugtarget or druggene interactions will be essential (). Pharmacogenomics has already been successful in improving drug prescription and dosing. Most prescriptions are written with a 'one dose fits all' approach with adjustments based on gender, weight, liver and kidney functions or allergies. Some drugs have more laborious dosing calculations such as the anticoagulant warfarin (). Warfarin dosing is traditionally determined by a time-intensive 'guess and test'method, until the coagulation tests stabilize. Pharmacogenomics identified several SNPs affecting dosing, includingCYP2C9 and VKORC1 (). Similar studies have been applied to clopidogrel, tramadol, anti-psychotics and many other drugs (). Ultimately, pharmacogenomic prescription and dosing algorithms need to be accessible to physicians, like the new warfarin dosing algorithms from the International Warfarin Pharmacogenomic Consortium (IWPC) (). Moreover, the current state of medical practice needs to be updated to include routine pharmacogenetic testing, educating and training physicians in personalized medicine, and further clinical trials to prove the efficacy of pharmacogeneticbased prescriptions. Bioinformatics also translates discoveries to the clinic by disseminating discoveries through curated, searchable databases like PharmGKB, dbGaP, PacDB and FDA AERS (). A major bottleneck for these databases is manual curation of the data. Biologically and medically focused text mining algorithms can speed the collection of this structured data, such as methods that use sentence syntax and natural language processing to derive druggene and genegene interactions from scientific literature (). These databases and methods need to be developed and used carefully. All these data sources are susceptible to errors and so validation of data is essential, especially before the information is applied in the clinic. Finally, there are challenges and opportunities for bioinformatics to integrate with the electronic medical record (EMR) (). For example, the BioBank system at Vanderbilt links patient DNA with a deidentified EMRs to provide a rich research database for additional translational research in diseasegene and druggene associations (). Some health care companies and HMOs have also begun to collect genetic information from their patients. In order to even implement such genome-based systems, the medical infrastructure will have to shift from paper to electronic medical records, in order to be compatible with bioinformatics portals for data delivery and interpretation. Ultimately, bioinformatics needs to develop methods that interrogate the genome in the clinic and allow physicians to use personalized medicine in their daily practice.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
