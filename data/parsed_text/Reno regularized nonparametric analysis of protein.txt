Motivation: The reverse-phase protein lysate arrays have been used to quantify the relative expression levels of a protein in a number of cellular samples simultaneously. To avoid quantification bias due to mis-specification of commonly used parametric models, a nonparametric approach based on monotone response curves may be used. The existing methods, however, aggregate the protein concentration levels of replicates of each sample, and therefore fail to account for within-sample variability. Results: We propose a method of regularization on protein concentration estimation at the level of individual dilution series to account for within-sample or within-group variability. We use an efficient algorithm to optimize an approximate objective function, with a data-adaptive approach to choose the level of shrinkage. Simulation results show that the proposed method quantifies protein concentration levels well. We show through the analysis of protein lysate array data from cell lines of different cancer groups that accounting for within-sample variability leads to better statistical analysis.
INTRODUCTIONProtein microarray technologies [e.g.;; MacBeath and] have been developed to measure protein concentrations in a high-throughput fashion. Extensive reviews of this technology can be found in. A single nitrocellulose-coated array slide can measure concentrations of a common protein in hundreds of samples in the form of dilution series. The samples are hybridized and label-attached with primary and biotinylated secondary antibodies and the protein concentrations are then measured using streptavadin-linked labels that bind to the biotin. The final product of each array is an image file in which * To whom correspondence should be addressed. quantified spots represent the observed protein expression levels at various dilutions steps. Protein lysate array technology has shown its promise in a number of clinical studies [e.g.;. In particular, its applications to various cancer studies have been documented extensively; see, for example,;;;;. Various procedures to improve the analysis of protein lysate arrays have been proposed recently. For example,proposed antibody-mediated signal amplification to increase the sensitivity of this technology.introduced a variable slope normalization among arrays to help reduce loading bias and recover true correlation structures among proteins. In this article, we focus on the protein quantification problem of estimating the relative protein concentrations in the arrayed samples. In this area, the commercial analysis package MicroVigene (http://www.vigenetech.com/products.htm) estimates the protein expression level by fitting a four-parameter logistic model to each dilution series.used a linear model between the observed protein expressions (or at logarithmic scale) and the underlying concentration levels.modeled the mean of the observed expression level as a sigmoidal curve and estimated the model parameters via the non-linear least squares. Alternatively,modeled the serial dilution curve based on the Sips model (similar to the logistic model) to characterize the relationship between signals in successive dilution steps. These methods can be considered as parametric. On the contrary,proposed a non-parametric approach by assuming that the median of the observed protein expression is equal to a monotonically increasing function without a parametric form. This non-parametric method, contained in a recently developed statistical tool RPPanalyzer for analyzing protein lysate array data (), is highly dataadaptive without bias due to mis-specification of g. Simulation studies and real experiments have shown the advantage of the nonparametric approach in producing robust results in a variety of scenarios. However, the existing methods of estimation deteriorate in performance as the noise to signal ratio increases in the data. In addition, the non-parametric estimates, if used to estimate protein concentration at the level of individual dilution series, tend to be unstable due to data sparsity. Our work aims at improving the accuracy of protein level quantification over the non-parametric procedure described in Hu, by incorporating a method of regularization on estimates within each sample. We start with a general description of the problem. Let y ijl be the observed expression level for the j-th replicate of the i-th sample at the l-th dilution step (i = 1,...,m, j = 1, ,n i , and l = 1,...,t). Each replicate is a dilution series of t steps. The relationship between y ijl and the unobserved protein concentration level x ij (at the log 2 scale) can be modeled aswhere g is the protein-specific response curve, d l defines the corresponding dilution index at the l-thfor some smoothing parameter , subject to the constraint that g is a non-decreasing function. Note that x ij are identifiable only up to a constant, and the relative differences between x ij 's are of interest in protein concentration. As pointed out by Yang and He (2011), the complexity of this quantification problem comes from the dimension of x ij , which increases with the sample size. One assumption made in earlier work is that x ij are constant within the i-th sample. This assumption is practically useful because it reduces the dimensionality of the problem and ensures stable estimates. However, it could lead to bias in estimation, and more importantly, could mask or distort variability across replicates. We propose to allow x ij to vary with j but regularize the estimation problem by using a penalty term on within-sample variabilities. As a result, we shrink some of the replicate-level estimates to common values but allow within-sample variabilities to be retained in other samples as needed. As with other regularization methods in statistics, our proposed method aims to balance the bias-variance trade-off in a data-adaptive way. The rest of the article is organized as follows. We describe the new method, Regularized Nonparametric (Reno) analysis of lysate arrays, in Section 2. We demonstrate the performance of the new estimator through simulation studies in Section 3, and show how the proposed method leads to better significance testing in a real data example in Section 4. Some concluding remarks are given in Section 5.
METHODS
New objective functionWe formulate the problem of simultaneously estimating g and the vector x ={x ij : i = 1,...,m; j = 1,...,n i } as follows minwhere L(y,g(x)) =|yg(x)|, is the L 1 loss function measuring the discrepancy between the observed expression level y ijl and its fitted value, and V 1 and V 2 are penalty functions on g and x, respectively, with  1 and  2 to be determined. As in, we choose the penalty on gwhich leads to a quadratic spline solution of g (). The L 1 loss function is appealing for lysate data quantification, because the unknown function g(x) corresponds to the median protein expression level, and the relationship is equivariant under any monotone transformation of the response. The new penalty term V 2 (x) used in (3) aims to regularize the solution of a high-dimensional vector x. Empirical results show that the optimization problem without this penalty (that is,  2 = 0) is often ill-conditioned and leads to unstable results. The special case of  2 = corresponds to the assumption in earlier work that x ij = x i for all j = 1,...,n i. The results obtained under this setting mask the variability across replicates. In protein microarray experiments, it is often the case that the samples can be divided into m subgroups G 1 G m , and the protein concentration levels are similar within each subgroup. In this article, we focus on a common case where each subgroup contains biological or technical replicates of some kind. In our notation, all the replicates within the sample form a subgroup. Naturally, we use the following penalty on x,where x i = (x i1 ,.
..,x inSimultaneous optimization over g and x is numerically challenging, and we propose an approximation to (6) in the next subsection.
Modified objective functionA natural way to solve (6) is a block-wise coordinate descent algorithm that iterates between the following two steps: @BULLET Step 1: minimize (6) over g given x;@BULLET Step 2: minimize (6) over x given g. Iterative algorithms of this type have been widely adopted (). Step 1 reduces tosubject to the constrain that g is a non-decreasing function. This is the same problem considered by, and we use the same procedure as before. The solution of g is a quadratic spline, and it can be found via linear programming with the R package Cobs developed by He and Ng (1999). Given g,which can be optimized for each i separately. Still, the non-linear nature of g makes this problem harder. We propose to simplify the calculation by using an approximate loss function. Note that |yg(x)|=|g(g 1 (y))g(x)||g (x)| |x g 1 (y)|.Then for the i-th subgroup, we solve, in lieu of (8),where
Regularized Non-parametric Analysis of Protein Lysate Arrays@BULLET Step 0 (Initialization): following, we obtain initial values of x as crude estimates of x ij = x i by assuming g taking a sigmoidal form g(x) =  +/(1+2  x ). @BULLET Perform Step 1 to find an estimate of g from(7). @BULLET Given g from Step 1, solve (10), and then center x by making the median of x to be zero.@BULLET Iterate between the previous two steps until a stopping rule is triggered. A useful stopping rule is to check if the objective function in (3) changes by less than a pre-specified value for two consecutive solutions.After each iteration, we add a step to check whether the new estimates indeed decrease the original objective function. If not, we do a grid search in a neighborhood around the previous estimates to ensure the monotonic decreasing of the objective function. We also note that centering x in each step of the iteration helps identify g. In our empirical study, we found that the algorithm converges quickly, often within four to five iterations.
Selection of tuning parametersThe tuning parameter  1 is associated with the penalty for g in (6). We use the procedure as in. In this section, we consider a data-adaptive choice of  2. We follow the basic idea of cross-validation (CV), but the special structure of the lysate data renders the ordinary CV procedure ineffective. If a dilution series (y ij1 ,...,y ijt ) were taken as a case to be left out, we would have no validation data for that case. To accommodate the special structure here, we propose a t-fold cross-validation, where the t observations from each dilution series are assigned randomly to each of the t-folds. Each fold contains one observation from each dilution series. Let F k (k = 1,...,t) denote the set of indices (i,j,l) in the resulting k-th fold, andare the estimates of g and x ij based on data not in F k. The parameter  2 is chosen to minimize CV( 2 ).
SIMULATION STUDIESTo evaluate the performance of the proposed method, we conduct simulation studies to compare our procedure (Reno) with the following alternatives: @BULLET Indep: the procedure with  2 = 0, which is equivalent to estimating a different concentration level for each dilution series. @BULLET Same: the procedure with  2 =, which is equivalent to assuming that the replicates within each sample have exactly the same protein concentration levels. @BULLET Nonpa: the procedure with  2 =, but (8) is solved using the procedure of, instead of the approximation (10). We include this alternative procedure to evaluate the accuracy of our approximation. It turns out that Same is much faster than Nonpa without much sacrifice in accuracy.@BULLET Tabus: the parametric procedure based on sigmoid curve of@BULLET Oracle: the procedure with  2 = only for the samples with x ij = x i1 (j = 1,...,n i ), and  2 = 0 otherwise. This procedure requires the knowledge on the unknown quantities so it is used only as an Oracle in the simulation for comparison purposes.In the experiments, we simulate data of 300 dilution series, each of length t = 6, from m = 100 samples with n i = 3 replicates for each sample. The three replicates for each sample form a subgroup. The concentration levels of all the replicates for 60% of the samples are generated to be identical, whereas the remaining 40% of the samples have varying concentration levels from replicate to replicate. Throughout this section, we evaluate the estimation error bywhere g is the true response function. Note that |g(x)g(z)| |g (x)||x z|, so this criterion downweights the estimation error of x where |g (x)| is small, i.e. in the nearly flat regions of g. Obviously, accuracies in the nearly flat regions of g cannot be expected from any method. We choose Oracle as the benchmark method and report the relative error, the ratio of the error of each method to that of Oracle.
Experiment 1In our first experiment, the response curve is taken from two sigmoid curves in the form of g(x) =  +/(1+2  x ) with  = 3000 and  = 10 000, but  = 0.7 for positive x and  = 2.1 for negative x. For positive x, the noise is generated from a scaled t distribution with three degrees of freedom with the scale  = 6000(x +5) 1 ; for negative x, the noise follows the normal distribution N(0,600 2 ). We conducted 100 simulation trials, and the results are summarized in. For the first 20 datasets, the tuning parameter  2 was chosen adaptively by CV. The upper left panel ofis the boxplot of CV( 2 ) based on these 20 datasets. The value  * 2 = 1500 is a good choice for most datasets from this model.To save time in the simulation study, we used this fixed value for all other datasets. But we expect the results for Reno to be similar if the CV is done for each trial. The upper right panel ofis the boxplot of the relative errors of each competing method over 100 trials. It is clear that the proposed method Reno outperforms its competitors. We repeated the data generating process with two smaller error variances, where the SD is reduced by a factor of 2/3 and 1/2, respectively. The results are summarized in the lower half of. In all these cases, Tabus performed the worst due to model misspecification. The performances of Nonpa and Same are similar. When the SNR (signal-to-noise ratio) is low, even if many of the three replicates have different x ij values, the variance deduction from pooling information from the replicates to estimate a single protein concentration level more than offsets the bias. Thus, Nonpa and Same have smaller errors than Indep in the upper right panel of. When the SNR is higher, bias becomes dominating, so Indep has a smaller error than Nonpa and Same in the lower two panels of. It is worth noting that Reno consistently outperforms, and it even beats Oracle when the SNR is low. If the calibration curve g were known, Oracle would have the best performance. However, when g is unknown and the SNR is low, adaptively borrowing information across replicates can improve estimation accuracy of g and hence the quantification accuracy of x.
Experiment 2In the second experiment, we generated the response curve from a single sigmoidal curve with  = 1000,  = 4000 and  = 0.8. The errors were generated independently from normal distributionBoxplots for the estimation errors (relative to Oracle) on 100 datasets are displayed in the upper right panel (original data generating process,  2 = 1500), the lower left panel (with the error SD reduced by a factor of 2/3,  2 = 1000), and the lower right panel (with the error SD reduced by 1/2,N(0,500 2 ). The true x values and the rest of the set-up are the same as in Experiment 1. The result are now presented in. In this experiment, Indep is always inferior to others, confirming that estimating individual x ij for all replicates is usually not recommended. It is interesting to note that Same and Nonpa outperform Oracle when the SNR is small (the upper right panel of), but not in the other cases. Since the data are generated from a sigmoidal curve, the results of Tabus are quite decent, but Reno remains a top performer.
ANALYSIS WITH REAL DATAWe analyzed the protein lysate data fromwith the proposed Reno, as well as the non-parametric procedure Nonpa fromand the parametric procedure Tabus from. The data consist of the intensity measurements for 90 samples from 52 protein arrays. Each sample has three replicates that are diluted 2-fold six times. The fitted curves from Reno and Nonpa are very similar for all arrays, but the parametric curves look different on some datasets. This is because the curve estimated by Tabus is constrained to take the logistic shape, which may not fit the data well. Between Reno and Nonpa, we noticed that the estimated protein concentration levels x ij differ on a number of arrays. Those differences have an impact on subsequent statistical analysis, as shown in the example below. The samples in this study are cell lines from 12 different cancer groups. We use the data to find out whether protein concentration levels are significantly different among cancer groups. We conductedBoxplots for the estimation errors (relative to Oracle) on 100 datasets are displayed in the upper right panel (original data generating process,  2 = 800), the lower left panel (with the error SD reduced by a factor of 2/3,  2 = 500), and the lower right panel (with the error SD reduced by 1/2,
Regularized Non-parametric Analysis of Protein Lysate Arraysa two-sample t-test based on the average concentration levels z i = (x i1 +x i2 +x i3 )/3 by assuming that z i are normally distributed with mean  and variance  2 + 2 i , where  2 i is estimated by the sample variance of the replicates x i1 ,x i2 and x i3 , and  2 denotes the variance component that is homogenous for all samples from the same cancer group. For Nonpa,  2 i = 0, but for Reno,  2 i can be non-zero, and the MLE MLE MLE is a weighted average of z i 's. Samples with higher within-sample variation are down-weighted, because their estimated x ij values are less reliable. As will be shown in the examples below, the withinsample variation, which is available only with the Reno procedure, indeed provides valuable information in the statistical analysis. We focus on protein cdk7, for which the P-values for contrasting the lung cancer group with two other cancer groups (Colon orColon, Sarcoma and Lung cancer groups from Tabus (, 1st column), Nonpa (+, 2nd column) and Reno (@BULLET, last two columns). The estimated concentration levels from Reno are displayed twice, one is the sample average (one point for each sample, the third column) and the other is the raw data (three points for each sample, the last column). the data of four samples from the lung cancer group in(left panel). Sample 1 has higher expressions than the other samples, and more importantly, there is a visible within-sample variation for Sample 1, but not for the other samples. This information is reflected in the estimates of protein concentrations from Reno: 4.2, 1.9 and 1.7 for the three replicates. As a consequence, Sample 1 gets downweighted in the statistical analysis with the following effects: (i) it decreases the average of the lung cancer group, and therefore enlarges the mean difference between groups; and (ii) it reduces the group variance, making the t-test more powerful. This explains why the P-values from Reno are less than a third of the P-values from Nonpa. Note that Reno provides more accurate and robust uncertainty analysis than its competitors, but does not always provide smaller P-values. In some cases, Reno points to the opposite direction when Tabus and Nonpa report smaller P-values by ignoring within-sample variations. Results from two other example slides (proteins p19 and Zap70) listed indemonstrate this point.
DISCUSSIONIn this article, we propose a regularized estimation procedure in non-parametric analysis of the protein lysate array quantification, which enables us to consider replicate-specific quantities when the within-sample variability is significant. We use a simple approximation to the loss function so that the optimization can be carried out by linear programs. We propose a specialized CV method to select the tuning parameter that regulates the withinsample variability. Earlier methods of lysate array quantification have to aggregate the replicates in each sample to avoid unstable estimates, and in doing so, important information may get lost in the aggregation. We demonstrate through simulated and real data that the proposed method is helpful in providing additional information about within-sample variabilities, which has important implications in the subsequent statistical analysis of the lysate array data. Note that the estimated calibration curves from different methods are essentially the same, and the differences are in the concentration estimates of individual samples. Funding: National Institute of Health Grants R21CA129671, R01GM080503, R01CA158113 and NCI CA97007, and the National Science Foundation Awards DMS-0706818, DMS0800631 and DMS-1007396. Conflict of Interest: none declared.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
