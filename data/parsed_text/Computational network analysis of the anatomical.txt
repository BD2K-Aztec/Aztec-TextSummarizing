Motivation: The mammalian central nervous system (CNS) generates high-level behavior and cognitive functions. Elucidating the anatomical and genetic organizations in the CNS is a key step toward understanding the functional brain circuitry. The CNS contains an enormous number of cell types, each with unique gene expression patterns. Therefore, it is of central importance to capture the spatial expression patterns in the brain. Currently, genome-wide atlas of spatial expression patterns in the mouse brain has been made available, and the data are in the form of aligned 3D data arrays. The sheer volume and complexity of these data pose significant challenges for efficient computational analysis. Results: We employ data reduction and network modeling techniques to explore the anatomical and genetic organizations in the mouse brain. First, to reduce the volume of data, we propose to apply tensor factorization techniques to reduce the data volumes. This tensor formulation treats the stack of 3D volumes as a 4D data array, thereby preserving the mouse brain geometry. We then model the anatomical and genetic organizations as graphical models. To improve the robustness and efficiency of network modeling, we employ stable model selection and efficient sparsity-regularized formulation. Results on network modeling show that our efforts recover known interactions and predicts novel putative correlations. Availability: The complete results are available at the project website
INTRODUCTIONThe mammalian central nervous system (CNS) generates high-level control functions, and knowledge on the anatomical and genetic organizations in this system can elucidate the functional brain circuitry. The enormous complexity of this system is reflected in the large number of cell types, each with unique gene expression patterns. Therefore, it is of central importance to capture the anatomical localization of gene expressions in the brain. Recent advances in bioimaging technologies, such as the high-throughput in situ hybridization (ISH) technique, have made it possible to capture the spatial expression patterns in the adult mouse brain (). Consequently, genomic-scale expression atlases in the form of digital images have been produced at increasing speed and resolution. The marriage of image processing tools and advanced computational methods opens the door for unraveling the functional brain circuitry and the generation of high-level cognitive functions on top of it. The Allen Brain Atlas (ABA) () contains 3D atlas of gene expression in the adult mouse brain and is one of the most comprehensive datasets for spatial expression patterns in the mammalian CNS. It provides cellular resolution 3D expression patterns in the male, 56-day-old C57BL mouse brain. In this atlas, genome-wide coverage is available in sagitally oriented sections. In addition, coronal sections at a more refined scale are available for a set of about 4000 genes showing restricted expression patterns. The image data are generated by in situ hybridization using genespecific probes, followed by slide scanning, 3D image registration to the Allen Reference Atlas (ARA) () and expression segmentation (). This results in a set of spatially aligned 3D volumes of size 674158, one for each gene, that document the spatial expression patterns of genes in the mouse brain. Efficient and effective analysis of these highthroughput data can shed light on the global function of mammalian CNS (). On the other hand, the sheer volume and complexity of these data pose significant challenges for efficient computational analysis. Hence, computational understanding of these data is limited to unsupervised techniques, which cluster the brain regions into co-expressed groups (). In this article, we employ advanced computational techniques to model the anatomical and genetic organizations in the mouse brain as networks. First, to reduce the size of data and accelerate efficient analysis and storage, we propose to apply tensor factorization techniques to reduce the data volumes (). This tensor formulation treats the stack of 3D volumes as a 4D data array, thereby preserving the mouse brain geometry. Based on the reduced data, we model the anatomical and genetic organizations as graphical models in which each vertex represents a spatial location or a gene, and the edges between vertices encode the correlations between locations and genes (). To improve the efficiency of network modeling, we employ an approximate formulation for Gaussian graphical modeling, which involves a series of sparsity regularized regression problems (). The efficiency of this approximate formulation enables us to employ a robust estimation technique known as stability selection (), which estimate and combine multiple models based on resampling. We apply the data reduction and network modeling techniques to learn the anatomical and genetic networks underlying the mouse
S.Jibrain using the ABA expression volume data. Results show that the expression patterns of spatially adjacent voxels tend to correlate. We also observe that the expression patterns of certain brain structures are correlated to the patterns of a large number of other regions, some of which are spatially distant. In-depth analysis reveals that such correlation patterns recover existing knowledge on the brain functionality. Our efforts on genetic network modeling identify functionally related genes that act in a concerted manner in the mouse brain.
HIGH-ORDER FEATURE EXTRACTION VIA TENSOR FACTORIZATIONIn ABA, the ISH image series of each gene are aligned to the ARA. To faithfully capture the mouse brain geometry, a 3D grid is employed to divide the 3D ARA space into quadrats, and expression information within each quadrat is summarized. Specifically, an expression segmentation algorithm is employed to identify expressed cells, and then an expression energy value is computed from each voxel as a function of the intensity and density of expression within that voxel. These image processing steps convert each expression pattern into a 3D volume. To enable the application of matrix computation techniques such as the singular value decomposition (SVD), these volumes are usually converted to vectors and stacked into a data matrix (). However, such conversion fails to retain the spatial locality and other high-order information in the expression volumes. To overcome this limitation, we propose to treat the 3D volumes as 3D tensors and stack them together to form a 4D tensor. We then employ tensor factorization techniques to reduce the dimensionality of this 4D tensor along each mode, resulting in significant data compression. A key advantage of this tensor representation is that the associated tensor computation techniques, such as high-order SVD and low-rank tensor approximation, can be employed to compress the data without flattening the internal structure of the high-order data array. These techniques approximate the original tensor by a core tensor multiplied by a basis matrix along each mode. Hence, the core tensor and the set of basis matrices give a compact representation of the original tensor, and the core tensor captures the major information in the original tensor.
Background on tensorsTensors, also known as multidimensional matrices (), are higher order generalizations of vectors (first-order tensors) and matrices (second-order tensors). The order of a tensor is the number of indices, also known as modes or ways. In this article, tensors are denoted by boldface Euler script letters, e.g. X  R J 1 J 2 ...J N , and its elements are denoted as x j 1 ,j 2 ,...,j N , where 1  j n  J n for n = 1,...,N. As a generalization of matrix multiplication, the n-mode tensor-matrix product defines the multiplication of a tensor by a matrix in mode n (). The n-mode product of a tensor X  R J 1 J 2 ...J N with a matrix A  R IJn = (a ijn ) is denoted by X  n A. The result is a tensor of size J 1 ...J n1 I J n+1 ...J N defined elementwise asLet Y  R J 1 J 2 ...J N be another tensor of the same size as X. The scalar product of these two tensors is defined as:Based on the scalar product, the Frobenius norm of a tensor X can be defined as=The mode-n vectors of X are the J n-dimensional vectors obtained from X by varying index j n while keeping all other indices fixed. Tensors can be converted into matrices via a process known as unfolding (). Specifically, the mode-n unfolding of X yields a matrix X (n)  R Jn(J 1 J 2 ...J n1 J n+1 ...J N ) whose columns consist of the mode-n vectors of X .The mode-n rank of X , denoted as rank n (X ), is defined as the rank of the matrix obtained from mode-n unfolding of X : rank n (X ) = rank(X (n) ). Tensors have been used in a wide range of domains including microarray data analysis () and natural image modeling ().
Tensor factorizationHigh-order singular value decomposition (HOSVD) () is a generalization of the SVD for matrices. Given a tensor X  R J 1 J 2 ...J N , its HOSVD can be expressed aswhere S  R J 1 J 2 J N , and U (n)  R JnJn , for n = 1,...,N, are orthogonal matrices. In HOSVD, the basis matrices {U (n) } N n=1 are computed as the left singular matrices of the mode-n unfolding of X , and the core tensor can then be computed asGiven a tensor X  R J 1 J 2 J N , a rank-(R 1 ,...,R N ) factorization of X () is formulated as finding a tensorXtensor tensorX withfor 1  n  N such that the following cost function is minimized:It follows from this definition thatXthat thatX can be expressed asXwhere C  R R 1 R 2 R N is called the core tensor and V (n)  R JnRn (1  n  N) has orthonormal columns. When the basis matrices {V (n) } N n=1are given, the core tensor C can be readily computed as Lathauwer et al.Hence, the key to the low-rank tensor factorization problem is to compute the basis matrices. The factorization of a 3D tensor is illustrated in. One of the commonly used algorithms to compute the basis matrices is the alternating least squares (ALS) method (). In each iteration of this method, one of the basis matrices is optimized while all others are fixed. Specifically, whenare fixed, we first computeThen the columns of V (n) can be obtained as the first R n columns of the left singular matrix of (X n ) (n) , which is the mode-n unfolding of X n. In ALS, the basis matrices are usually initialized as the truncated basis
Computational network analysismatrices from HOSVD (). That is, V (n) is initialized as the first R n columns of U (n) , for n = 1,...,N. When the size of the tensor is very large and cannot fit into memory, an out-of-core algorithm can be applied by partitioning the tensor into blocks (). The advantages of tensor-based methods in comparison to matrix-based approaches have been addressed in the literature (). In summary, tensorbased methods have the following two major advantages: (i) tensor-based methods can be applied to large datasets for which matrix-based methods are too expensive to apply. For example, the size of the data array for genetic network modeling in this article is 3012674158. While the tensorbased method requires the SVD of three matrices of sizes 6767, 4141, and 5858, respectively, the matrix-based method requires the SVD of a matrix of size 3012159,326. (ii) Although matrix-based methods give the lowest reconstruction error due to the best low-rank approximation property of matrix SVD, tensor-based methods preserve the geometry of the highorder data array. In the literature, tensor-based and matrix-based methods have been compared in classification tasks (). Specifically, it has been shown that, though tensor-based methods give larger reconstruction error, they usually yield higher classification accuracy.
NETWORK CONSTRUCTION VIA SPARSE MODELINGThe 4D tensor of gene expression obtained from the ABA is factorized as described above. The core tensor retains most of the information in the original tensor while its size is significantly reduced. This data reduction step is critical for the subsequent efficient analysis. Based on the reduced data, we employ sparse graphical modeling approaches to construct the anatomical and genetic networks underlying the mouse brain.
A sparsity regularization formulationGaussian graphical models are a class of methods for modeling the relationships among a set of variables (). In this formulation, the d-dimensional variable x =[x 1 ,x 2 ,.
..,x d ] Tis assume to follow a multivariate Gaussian distribution x  N(,), where   R d and  R dd are the mean and covariance, respectively. The conditional dependency between pairs of variables can be encoded into a graphical model in which vertices represent variables and edges characterize the conditional dependency between variables. In particular, there is an edge between nodes corresponding to x i and x j if and only if these two variables are conditionally dependent given all other variables. This is equivalent to the saying that there exists an edge between nodes corresponding to x i and x j if and only if the (i,j)-th entry of the inverse covariance matrix (also known as concentration matrix) = 1 is non-zero (). This correspondence is illustrated in. Given a set of n observations y 1 ,y 2 ,...,y n , the concentration matrix can be estimated by maximizing the penalized log likelihood as follows ():where det is the determinant of , 0 represents that is positive definite, S denotes the empirical covariance matrix computed from data, and 1 is the 1-norm of , which is the sum of the absolute values of the entries of. The first two terms in Equation (8) are the log likelihood, and the last term is used to enforce that many entries of are set to zero, yielding a sparsely connected graph. This formulation has been used to model the gene networks in Arabidopsis thaliana (). The optimization problem in Equation (8) is convex and can be solved by several algorithms such as the interior point method () and the graphical lasso algorithm (). However, all these algorithms are computationally expensive and can only be applied to small-scale problems. For the modeling of mouse brain networks, we have thousands of genes and tens of thousands of voxels; hence, this formulation is not applicable.In Meinshausen and Bhlmann (2006), an approximate formulation is proposed to learn Gaussian graphical models by solving a series of sparse regression problems. Specifically, the conditional dependencies between x i and all other variables are learned by solving the following 1-norm penalized regression problem known as lasso ():is the data matrix obtained by removing the i-th data item. The conditional dependencies between x i and all other variables are obtained from the corresponding components in the weight vector w. Note that the regression of x i onto x j and that of x j onto x i may not give the same result. Hence, two simple schemes, based on logic operations or and and, are proposed to interpret the results. In the first scheme, two variables are considered to be conditionally dependent if either of them yields non-zero weight (). In the second scheme, they are considered as conditionally dependent if both of them give non-zero weights. The first scheme is employed in this work (). The pairwise relationships between all pairs of variables can be obtained by running the sparse regression problem in Equation (9) for each variable. A critical observation that leads to the efficiency of the formulation in Equation (9) is that it involves solving d independent lasso problems, one for each variable. The lasso problem can be solved very efficiently by many algorithms such as the accelerated gradient method (). It has been shown that this sparse regression formulation of Gaussian graphical modeling maximizes the pseudo likelihood () and is an approximation to the maximum likelihood scheme in Equation (8) (). In particular, the exact maximization of log likelihood involves solving the lasso problems iteratively as in the graphical lasso algorithm (), and the formulation in Equation (9) can be considered as a one-step approximation to the maximum likelihood scheme. We employ this approximate formulation to learn the mouse brain networks due to its efficiency.
Robust estimation via stability selectionThe regularization parameter  in Equation (9) controls the trade-off between the sparsity of solution and data fit. Specifically, when  is set to a very large value, most of the entries of w are set to zero. Hence, a challenge in practice is how to select the value for . Stability selection (Meinshausen and) addresses this problem by ideas similar to the ensemble learning methods widely used in machine learning (). In stability selection, we choose a set of  values denoted by , instead of a single  value. For each   , we compute the selection probability for each variable, which is defined as the probability of each variable been selected when randomly resampling from the data. Formally, let I be a random subsample of y 1 ,y 2 ,...,y n of size n/2 drawn without replacement. The selection probability for variable x i is defined as as asPage: 3296 32933299
S.Jiwhere A  (I) denotes the set of variables that have been selected when I is used as the sample and the regularization parameter is set to . Note that this definition of A  (I) is independent of the specific method used for variable selection. The probability in Equation (10) is with respect to both the random sampling and other sources of randomness such as that induced by the algorithm as we discuss below. For every variable x i , the stability path is given by the selection probabilitiesprobabilities probabilities x i , . It has been shown in Meinshausen and Bhlmann (2010) that 100 random resampling is sufficient to obtain accurate estimates. Based on the selection probabilities, stable variables can be defined. For a cutoff  thr with 0 < thr < 1 and a set of parameters , the set of stable variables are defined asSBy choosing the set of stable variables under the control of the cutoff  thr , we keep variables with a high selection probability and discard those with low selection probabilities. It has been show that the results of stability selection vary little for sensible choices of the cutoff  thr and the parameter set .It has also been shown that performance can be further improved if additional randomness is introduced into the lasso problem in Equation (9). In particular, we can randomize the amount of regularization for each variable by solving the following problem:where D i d ={1,...,i1,i+1,...,d}, c i are IID random variables inand   (0,1] is a user-specified weakness factor.
RESULTS AND DISCUSSION
Experimental setupIn this article, we use a set of expression volumes for 3012 genes documented in the coronal sections as in. This set of genes exhibit restricted expression patterns and thus are of high neurobiological interest. For anatomical network modeling, we only use the left hemisphere voxels, since only this part of the brain is annotated in ARA. This gives rise to a 4D tensor of size 3012 674133 in which the first index corresponds to genes, and the other three indices represent the rostralcaudal, dorsalventral and leftright spatial directions, respectively. In tensor factorization, we keep the dimensionality of the last three modes while reduce the dimensionality of the first mode, since we are interested in modeling the relationships among brain voxels. For genetic network modeling, we use the full volumes, and the size of our 4D tensor is 301267 4158. In this case, we keep the dimensionality of the first mode while reducing the dimensionality of the other three modes. The computational experiments were performed on a cluster consisting of 256 cores and 512 GB RAM. The lasso formulation was solved using the SLEP package (). We can determine the  value that enforces w to be a zero vector in Equation (9) (), and this  value is denoted as  max. Then we set ={0.1 max ,0.2 max ,...,0.9 max }. The selection probabilities were estimated on 100 random resampling, and the weakness factor  was set to 0.8. The sizes of reduced data were set to retain 90 and 80% of the original information for anatomical and genetic network modeling, respectively, based on the computational resource requirements. Specifically, the size of the reduced tensor is 179674133 in anatomical network modeling and is 3012 221319 in genetic network modeling.Each vertex is labeled with the ARA informatics ID of the brain structure, and the corresponding structure name is given in Supplementary Table S2.
Results on anatomical network modelingComputational modeling of the anatomical organization in the mouse brain yields a graph in 3D space in which the vertices represent brain regions, and the edges characterize the expression correlations between regions. The correlation patterns can be visualized by showing slices of the 3D brain network on 2D planes.shows one slice of the brain network along the coronal section. We can observe that most of the edges connect adjacent regions, showing that spatially adjacent regions tend to exhibit correlated expression patterns. Note that these correlation patterns are learned without knowing the spatial locations of voxels. Although most of the edges connect spatially adjacent regions, there are apparent exceptions. A slice-by-slice examination of the entire anatomical networks at multiple cutoffs reveal that the voxels annotated as dentate gyrus (DG) in the ARA are highly correlated to many voxels in distant regions as shown in. According to classical neuroanatomy, the DG plays an important role in learning and memory by processing and representing spatial information, and it has always been a topic of intense interest (). It has been shown that the DG receives multiple sensory inputs including vestibular, olfactory, visual, auditory and somatosensory from its upstream perirhinal cortex and entorhinal cortex. It plays the role of a gate or filter, blocking or filtering excitatory activity from the inputs and controlling the amount of excitation that is propagated to the downstream hippocampus (). A close examination ofshows that the correlation patterns are largely consistent with those classical results. A more quantitative analysis of the results show that the correlation patterns obtained solely based on gene expressions match well with the known functions of DG. In particular, the expression patterns of the DG is highly correlated to those of the cerebral cortex and the main olfactory bulb, which provide sensory inputs to DG. In addition, DG is highly correlated to the hippocampal region and the retrohippocampal region, propagating the filtered signals to its downstream regions. We also observe that the intra-DG correlations dominate, demonstrating again that most of the edges connect spatially adjacent regions. Besides the correlations with knownThe region with the largest number of connections corresponds to the brain structure dentate gyrus. functions, our modeling of the anatomical networks also identifies many new relationships with DG that are not known from classical anatomical studies. Based on the obtained networks in 3D space, a variety of network analysis and visualization techniques can be employed to analyze the anatomical organization in the mouse CNS. In, the K-means algorithm is used to cluster the brain voxels into groups based on dimensionality reduced expression data, and a metric known as the S index was employed to quantitatively characterize the correspondence of the clustering results with the classical anatomy reflected in the ARA annotations. Specifically, let R ={r 1 ,...,r N } be a partition of the set of brain voxels in which each r i comprises the set of indices of the voxels that map to that cluster (or anatomical label). The spatial overlap between a region from the ARA and the clustering result is defined as:
Computational network analysisthe P ij values that are computed over all pairs of ARA regions and cluster result, we can then derive a global scalar index of similarity between the two partitions. Sincewhere U ij = min{|r i |,|r j |} if X ij > 0 and 0 otherwise. Finally, the S index is defined as S = 14 ij W ij X ij (1X ij ). To compare our network modeling method with the K-means clustering, we apply the leading eigenvector community detection algorithm proposed byand treat each detected community as a cluster. Since different cutoff values  thr in the stability selection yield different graphs, we vary  thr from 0.5 to 0.85 and detect communities from each of the resulting graphs. We then run K-means with K equal to the number of communities so that the results are comparable. Since the results of K-means depend on the initialization, we run this algorithm 10 times and choose the one with the best result. We compute the S index for each case and report the results in. We can observe that the community detection results consistently give higher S index values, indicating that the structures of our anatomical networks are in higher accordance with the classical anatomy. We also plot the number of detected communities as the cutoff changes in. We can see that the number of communities lies approximately between 100 and 250, which is largely in correspondence with the number of structures in classical anatomy. Detailed results on community identification are provided in the Supplementary Material. The classical anatomy was created mainly based on brain functions. Since functions are mainly determined by gene expression, the expression patterns within anatomical structures should be more correlated than those across structures. To validate this hypothesis, we show the distribution of the edges within and across the anatomical structures when  thr = 0.5 in. We also show the number of edges within and across structures when the cutoff varies from 0.2 to 0.9. We can observe that the edges within structures dominate in all cases, indicating that the expression patterns within classical anatomy are highly correlated. We can also observe fromthat the proportion of edges within anatomical structures increases as the cutoff increases. This indicates that most of the cross-structure edges have relatively small selection probabilities, and they are removed as the cutoff increases. The ranked lists of regions in terms of the number of connections are provided in the Supplementary Material.Shows the distribution of the edges within and across the anatomical structures when  thr = 0.5. The rows and columns correspond to the structures annotated in the ARA. This matrix is normalized to the intervalrow by row, and hence it is not symmetric. Each row indicates the proportion of a particular structure's edges that connect to other structures. In particular, each entry (i,j) in the matrix represents the proportion of structure i's edges that connect to structure j. (B) Shows the number of edges within and between anatomical structures in ARA as the cutoff changes. between genes. Since genes involved in the same pathway usually exhibit similar expression patterns, correlated expression patterns may imply similar biological functions. We hence use Gene Ontology (GO) () to evaluate the functional relationships among tightly connected genes in the network. In particular, we consider a gene and its direct neighbors as a group () and evaluate the functional enrichment of each group using the hypergeometric distribution (). We apply Bonferroni correction for multiple hypothesis testing and consider GO terms with corrected P < 0.05 as statistically significant (). We vary the cutoff  thr and observe that most of the groups are annotated with at least one statistically significant GO term. In particular, when  thr = 0.5, there are 2702 groups annotated with at least one statistically significant GO term, and the average number of terms per group is 15. This indicates that most of the groups are associated with multiple enriched terms. It has been previously observed that the degrees of many biological networks follows a power-law distribution (). This indicates that there exists a small number of highly connected genes known as hubs. We vary the cutoff and observe that the set of highly connected genes are largely consistent (details provided in the Supplementary Material). We report the top 10 genes with the largest number of connections inwhen  thr = 0.8 and show slices of their expression patterns in the Supplementary Material. We can observe that all these groups are highly enriched with the biological function binding or protein binding, implicating that they are likely to encode transcription factors. Among these 10 genes, the APP encodes an integral membrane protein expressed in many tissues and concentrated in the synapses of neurons. Homologous proteins have been identified in other organisms such as Drosophila, C.elegans and all mammals. APP is best known for its association with the Alzheimer's disease, and mutations in critical regions of APP cause familial susceptibility to Alzheimer's disease. It would be interesting to investigate how the 'hubness' of APP is related to CNS disease.
Results on genetic network modeling
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
