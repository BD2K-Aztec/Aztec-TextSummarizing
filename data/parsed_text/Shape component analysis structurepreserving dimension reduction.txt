Motivation: Quantitative shape analysis is required by a wide range of biological studies across diverse scales, ranging from molecules to cells and organisms. In particular, high-throughput and systems-level studies of biological structures and functions have started to produce large volumes of complex high-dimensional shape data. Analysis and understanding of high-dimensional biological shape data require dimension-reduction techniques. Results: We have developed a technique for non-linear dimension reduction of 2D and 3D biological shape representations on their Riemannian spaces. A key feature of this technique is that it preserves distances between different shapes in an embedded low-dimensional shape space. We demonstrate an application of this technique by combining it with non-linear mean-shift clustering on the Riemannian spaces for unsupervised clustering of shapes of cellular organelles and proteins. Availability and implementation: Source code and data for reproducing results of this article are freely available at https://github.com/ccdlcmu/shape_component_analysis_Matlab. The implementation was made in MATLAB and supported on MS Windows, Linux and Mac OS.
IntroductionGeometrical shapes are a fundamental property of biological structures. Quantitative shape analysis is required by many biological studies across diverse scales. For example, at the molecular scale, quantitative analysis of shapes of proteins is essential for understanding their functions and interactions (). As another example, at the cellular scale, quantitative analysis of shapes of cells is essential for understanding their morphogenesis and migration (). Recently, high-throughput and systems-level biological studies have started to produce large volumes of complex biological shape data from structural analysis () or image analysis (D'). The biological shape data produced often have high dimensions, which pose a significant challenge for their analysis and understanding. Dimension reduction is an essential tool for analyzing and understanding high-dimensional data. A wide range of related techniques have been developed (). However, for effective dimension reduction of biological shape representations, it is crucial to take into account their specific structures and properties. Specifically, biological shapes are often represented by points on high-dimensional Riemannian spaces (). Differences between distinct shapes are best represented by their Riemannian distances rather than Euclidean distances, which are commonly used in dimension reduction. This can be seen from the example in, which shows that Riemannian distances better differentiate between different shapes. Indeed, non-linear Riemannian geometry of shape spaces is proposed as a tool of choice for characterizing geometric differences between shapes (). In this study, we described 2D shapes using their landmark representation (lack well-defined landmarks using their spherical harmonic representation (SHR) (). We developed a technique for nonlinear dimension reduction of these two shape representations on their Riemannian spaces. A key feature of our technique is that the non-linear distances between shapes are preserved in an embedded low-dimensional shape space. We demonstrate an application of our dimension reduction approach by combining it with non-linear mean-shift clustering on Riemannian spaces () for unsupervised clustering of shapes of mitochondria and proteins. Experimental results confirmed that the proposed dimension-reduction technique, when combined with mean-shift clustering, provided equivalent clustering performance but substantially reduced processing time, because the cost of computing the Riemannian distance between two shapes depends linearly on their dimension. The proposed dimension-reduction approach is general and can also be combined with other shape analysis techniques. In the remaining part of the article, we first outline the theory of shape space and then describe our dimension reduction technique and its integration with unsupervised mean-shift clustering on shape spaces. The overall shape analysis work flow is summarized in. We present experimental results on a variety of shape datasets, first on a generic 2D shape dataset, then on a 2D mitochondrial shape dataset and lastly on a 3D protein shape dataset.
Methods
Landmark representation of 2D biological shapesIn this study, we describe 2D biological shapes using their landmark representation (). Landmark representation can also be used to describe shapes of 3D or higher physical dimensions as long as landmark points are specified. A shape of physical dimension N is usually represented by a sequence of its D landmark points. In the case of a planar object, its 2D shape can be represented by landmarks along its contour. This ordered sequence of landmark points is referred to as a configuration. A configuration can be mathematically represented by a N-by-D real matrix Y, where Y k,j records kth coordinate of the jth landmark point (). The pre-shape Z of a configuration Y has the location and scale information removed. Typically, it is determined by setting Y to be zero-mean in centroid and unit length in size. The space of pre-shapes can be defined aswhere the Frobenius norm jjjj F is the square root of the sum of the absolute squares of elements in a matrix. This space is a high-dimensional hyper-sphere in the Euclidean space. While a pre-shape is invariant under the translation and scaling of the original configuration, a shape is invariant under translation, scaling and rotation. The N-dimensional shape space R D N is defined as the collection of equivalent configurations under translation, scaling and rotation.
Riemannian geometry of the shape spaceThe Riemannian geometry of the N-dimensional shape space can be explicitly formulated. Here, we list several related properties that will be used in the remaining part of the article. For ease of implementation, we represent the shape space as a submanifold embedded in R ND with a lifted tangent space. To define the Riemannian geometry of the shape space, we need to specify: (1) a mathematically defined set containing all possible shapes and (2) the pairwise distance between two shapes. In the theory of landmark representation, a shape is defined as the equivalent class of a pre-shape Z up to rotations (). This equivalent class can be represented by Z  fOZ : O 2 SONg;where SO(N) is the group of N-dimensional rotation matrices. The N-dimensional shape space R D N is a Riemannian manifold containing all possible shapesand equipped with a distance q(,). The Riemannian distance is given by qZ 0 ; Z 1   cos 1  max O2SON hOZ 0 ; Z 1 i:Note that this distance is equivalent to the L 2 distance q F after two shapes are rotationally aligned, that is q F Z 0 ; Z 1   minFrom the computational perspective, the structure of the tangent space is crucial for developing efficient algorithms. Here, we introduce three). A representative of each shape group is shown on the top and to the left of the distance map. (a) Riemannian distances between different selected shapes; (b) Euclidean distances between different selected shapes, calculated using elliptic Fourier descriptors (). Each distance is normalized and color codedconcepts: the tangent space of a shape space, the Exponential map and the Log map. Roughly speaking, the tangent space contains information of a function's gradient on a Riemannian manifold, while the Exponential and Log maps are used to exploit the direction of a gradient for searching extrema of the function. The lifted tangent space atis represented by H Z R D N   fX 2 R ND jhZ; Xi  0; XZ T  ZX T g:Given an equivalent classand a tangent vector v 2 H Z R D N , the exponential map on the shape space is given by Exp Z v  Z cos kvk F   v kvk F sinkvk F  :Conversely, when two pre-shapes Z 0 and Z 1 and a rotation O 2 SON satisfy OZ 1 Z T 0 being symmetric and positive definite, we obtain the (horizontally lifted) Log map, which is Log Z0 Z 1   s 0 sins 0  O T Z 1  Z 0 coss 0 ;where s 0  qZ 0 ; Z 1 . Lastly, we note that the Riemannian distance q(,) and the Log map can be computed using polar decomposition () as follows:1. Given two pre-shapes Z 0 and Z 1 , compute the singular value decomposition of their covariance matrixHere, S is a diagonal matrix formed by singular values and U and V are two orthonormal matrices.For 2D shapes, explicit formulas for the Riemannian distance and the Log map have been derived by representing a 2  N matrix as a 1  N complex vector (). This allows a fast implementation without calculating the singular value decomposition.
Dimension reduction on the shape spaceIn this section, we define a mapping that projects a high-dimensional shape space into another low-dimensional shape space while the pairwise distance is preserved in the embedded low-dimensional shape space, as defined in Equation (7) or Equation (11). This allows us to adapt, for example, the mean-shift algorithm on the shape space with substantially lower dimensionality.
Low-dimensional shape space and projection errorThe essence of principal component analysis is to find the lowdimensional subspace that minimizes projection error in the original Euclidean space. Analogous to this concept, we first define embedded low-dimensional shape spaces and their corresponding projection errors. Given a D-by-r matrix R satisfying R T R  I r , we define the embedded r-dimensional shape space induced by R aswhich is the image of a continuous functionFor every shape Z 2 R D N , we propose to use the cosine of the Riemannian distance as the similarity metric to assess the performance of representingin the embedded shape space. That is, we define the similarity measurement as EZ; R max M2R nr ; jjMjj F 1 cosqZ; MR T   max M2R nr ; jjMjj F 1; O2SON hOZ; MR T i  jjZRjj F :This similarity measurement is continuous inand invariant under left multiplication of SO(N) and hence its well-definiteness is guaranteed by the universal property of the quotient (). Note that this similarity measure also reflects the extent of information of a shape data point that is preserved under the projection. And the set of zero similarity scores is an analogy to the set that is perpendicular to a Euclidean subspace. Thus, an embedded submanifold's accuracy to represent a given shape dataset can be easily checked by the similarity scores of individual shape data points.
Structure-preserving dimension reductionNext we describe the desired mapping. To provide a strong link to the implementation, we consider the shape space R D N and its tangent space embedded in R ND as described in Section 2.2. The following statement is the cornerstone of our proposed method.
Theorem 1:Given a D-by-r matrix R satisfying R T R  I r , the mappingis well defined. Furthermore, distance is preserved under T r in the following embedded r-dimensional shape space induced by R fMR T  : M 2 R Nr ; jjMjj F  1g:Proof:The well-definiteness is followed by verifying that the mapping is continuous and satisfies the universal property of the quotient. The preserving of pairwise distance is ensured since the inner product is preserved in this subset, i.e.Next we address the question of determining the optimal basis matrix R from a given dataset.
Determining the optimal basis matrixGiven pre-shapes z i , we propose to determine the optimal basis matrix R by solving the following optimization problem:Note that the optimal solution is the first r eigenvectors of the coThe solution to this optimization formulation is similar to the one of 2D principal component analysis (), though we derived this optimization problem from a different perspective. We henceforth refer to this method as shape component analysis (SCA). The overall procedure is summarized in Algorithm 1.
Shape component analysisAlgorithm 1. Shape component analysis 1: Given pre-shapes z j ; j  1;. .. ; n 2: specify the reduced dimension r 3: C  X j z T j z j 4: Compute the first r eigenvectors of C 5: List the first r eigenvectors in columns of a matrix R 6: z $ j z j R=jjz j Rjj F 7: Output z $ j for mean-shift clustering
SHR of 3D biological shapesLandmark representation can also be used to describe 3D biological shapes, as long as ordered sequences of landmark points are available. However, well-defined landmarks sometimes are lacking in biological studies. In such cases, 3D biological shapes are often represented by unordered sets of surface mesh vertices. But the dimension-reduction technique developed previously for landmark representation can no longer be directly applied. To solve this problem, we adapted SHR to transform unordered sets of mesh vertices of 3D surfaces into an ordered sequence of coefficients in the frequency domain. This approach allows a systematic treatment of SHR similar to the landmark representation.where c l m  c x m;l ; c y m;l ; c z m;l  T. Each coefficient is calculated by the integration with a specific spherical harmonic, for example,xh; /Y l m h; /dX:In practice, the infinite series is truncated by limiting l to 0 l L: Detailed explanation of SHR can be found inNext we would like to point out that the landmark representation and SHR share the same Riemannian geometry. As mentioned in Section 2.2, a Riemannian geometry is specified by the set containing all elements and the pairwise distance between any two elements. We shall define the SHR 'shapes' and show that they (1) form a structurally equivalent set and (2) have the pairwise distance as landmark shapes do. As in the landmark representation, a truncated series of spherical harmonic coefficients can be arranged in an ordered array of coefficients. Such an ordered array is represented as a 3-by-2L 2 matrix whose elements record the real and imaginary parts of spherical harmonic coefficients. To define the SHR 'pre-shape', we propose to 1. Set the coefficient c 0 0 to be 0 to make SHR invariant of translation. This operation will set a closed surface to be centered at the origin sinceis the centroid of the surface. 2. Normalize C $ to unit norm to make SHR invariant of scaling.We denote this normalized matrix as C $ to represent SHR preshape. Clearly, SHR pre-shapes reside on a high-dimensional hyper-sphere as the landmark pre-shapes. Furthermore, the SHR shape can be defined as the equivalent set of a SHR pre-shape up to rotations, i.e.Note that this set is also the equivalent set of a closed surface since rotating the surface is equivalent to applying a rotation matrix to its coefficients array. This can be seen fromSo far we have shown that the space of SHR shapes can be represented as the collection of equivalent sets in a hyper-sphere. The pairwise distance of SHR shapes can be defined as the metric between landmark shapes. The key idea behind the Riemannian distance is to capture the minimum L 2 distance between two rotationally aligned objects. For example, given two closed surfaces, we would computefor comparing how different these two surfaces are. Because of the orthonormality of the spherical harmonics, this quantity can be written in terms of SHR coefficient matrices, i.e.Note that this is the same distance used in landmark representation in Equation (3). Therefore, SHR shapes and landmark shapes share the same pairwise distance. Overall, we have shown that SHR shapes and landmark shapes share the same defining set and the same pairwise distance and hence the same Riemannian geometry. Since our dimensional reduction technique in the previous section and the clustering algorithm presented in the following section are designed according to the Riemannian geometry of shape spaces, SHR coefficient arrays can be used as inputs to our algorithm once they are transformed properly. The overall procedure of transforming SHR coefficient arrays to their shape representations is summarized in Algorithm 2.Algorithm 2. Transformation of an SHR into an SHR shape 1: Given SHR C 2:5: Output C $ as a pre-shape for subsequent analysis
Mean-shift clustering on a Riemannian manifoldThe mean-shift clustering algorithm is a peak-finding algorithm that searches for representatives supported by the majority of data (). A mean-shift clustering algorithm defines modes as local maxima of the underlying probability density distribution estimated by a kernel density estimation and iteratively searches for local maxima using a gradientwhere z i are pre-shapes given, h is the bandwidth of the kernel function g(x)  e x/2. Convergence of this algorithm is guaranteed. We implemented mean-shift clustering on the shape space following the procedure shown in Algorithm 3. With the proposed SCA, meanshift clustering can be performed on the shape space of substantially lower dimensionality without any change in procedure.Algorithm 3. Mean-shift clustering on the shape space 1: Given pre-shapes z i ; i  1;. .. ; n 2: for i  1;. .. ; n do 3: x z i 4: while jjm h xjj < e do 5: for j  1;. .. ; n do 6: U j ; S j ; V j   svdz j x T  7: O j  detV j U T j V j U T j8: q j  cos 1 trS j  9: Log j  s0 sins0 O j z j  xcoss 0  10: end for 11:
Results and discussionWe tested our dimension reduction algorithm on three different shape datasets. All computation was performed on a desktop workstation (2  Intel Xeon E5503 2.00 GHz and 8GB RAM).
Application
Results: geometric approximationWe tested whether our dimension reduction technique could reliably preserve distances between different shapes under reduced dimensions. We first calculated the pairwise Riemannian distances between different shapes and used them as the ground truth. We then calculated Riemannian distance between shapes under different levels of dimension reduction (). We report the approximation accuracy under dimension reduction in root mean square error (RMSE) of the Riemannian distances compared with the ground truth. For comparison, we also calculated the pairwise Euclidean distance on the tangent space of the mean shape. This allowed us to compare our dimension-reduction technique to the principal geodesic analysis (PGA) technique (), which is based on the Euclidean distance on the tangent space. Furthermore, we calculated the level of distortion under reduced dimension as the normalized error of the distance matrix M compared with the ground truth distance matrix M truth , i.e.where jj  jj F is the Frobenius norm. The results were summarized in. There was a non-vanishing distortion in pairwise distances when PGA and the Euclidean distance were used. In contrast, SCA consistently reduced distortion when the final reduced dimension was increased. In terms of approximation errors, SCA was comparable with PGA under higher final reduced dimensions.
Results: shape clusteringTo further assess the performance of our dimension-reduction technique, we combine it with mean-shift clustering and compare it against four other methods. The following is a list of all the methods we tested: 1. Riemannian mean-shift clustering (RMS). 2. RMS with shape component analysis (RMSSCA). 3. Mean-shift clustering on the tangent space, using Euclidean distance (tMS). This method was chosen to compare the difference between using Riemannian and Euclidean distances for shape clustering.) as inputs. This method was chosen to compare the performance of using rotation-invariant descriptors in shape clustering. 5. Mean-shift clustering with features obtained from Laplacian eigenmap () (MSLAP). This method was chosen to compare the performance of a common strategy of 'flattening the non-linear manifold.' We implemented Laplacian eigenmap with 5-nearest neighbors to project the Riemannian manifold into a 10D Euclidean space, where the mean-shift clustering was performed to find clusters. For all algorithms, clusters were first identified and then used to classify each data point using one-nearest-neighbor classification. The final clustering results were evaluated by three performance metrics (), including:Here, a j  #fg j g; b k  #fc k g; n j;k  #; fc k \ g j g, c k and g j are the kth and jth set of members from clustering and ground truth labeling, respectively, and # denotes the number of elements in a set. In some cases, these performance metrics could be biased by the number of clusters. To avoid this problem, we adjusted parameters so that each algorithm generated seven to eight clusters. Performance metrics for all the methods tested were summarized in. The results showed that algorithms using Riemannian distance such as RMS and RMSSCA consistently performed better than algorithms using Euclidean distance such as tMS and MSFD. MSLAP performed the worst among all the algorithms. This confirmed the merit of using non-linear similarity in clustering. Although RMSSCA provided similar clustering performance as RMS, its computational time was significantly reduced, by a factor of $3.
Application II: 2D mitochondrial shapes3.2.1 Experiment design and data preprocessing. We further tested our SCA algorithm on a mitochondrial shape dataset. Mitochondria within segmental nerves of dissected Drosophila 3rd instar larvae were visualized by fluorescence live imaging. Images were collected on a Nikon Ti-E inverted microscope at 5 frames per second, under a NA of 1.41 and a 100  magnification. Because of the constraint imposed by the axon geometry, shapes of axonal mitochondria could be represented in 2D instead of 3D. An adaptive active-mask algorithm was used to segment mitochondria (). Individual mitochondria were tracked as in. The boundary of each segmented mitochondrion was fitted by a cubic spline. Then a configuration of 200 semi-landmark points was sampled from the fitted spline.
Results: dimension-reduction and shape clusteringA total of 4000 configurations were collected from 800 mitochondria with five repeats sampled at different time points.summarized the performance of SCA and PGA on representing mitochondria morphology. The shape groups identified by the proposed algorithm are shown in. The original mean-shift algorithm detected 11 diverse shape clusters. Mean-shift with dimension-reduction provided similar clustering results by identifying eight shape clusters, but the run time was significantly reduced. While the original mean-shift algorithm took 20 756 s to finish the. Clustering of mitochondrial shapes without and with dimension reduction. Red: examples of mitochondrial shapes (selected randomly from 4000 shapes). Blue and black: shape clusters identified by RMS and RMSSCA, respectively computation, mean-shift with dimension reduction took only 1390 s in total (480 s in dimensional reduction and 910 s in mean-shift clustering, dimension reduced from 2  200 to 2  8), a reduction of computation time by a factor of $15. Functions of mitochondria depend critically on their shapes (). The dimension-reduction technique developed here provides a tool for efficient and multi-resolution representation and analysis of mitochondrial shapes. Combined with unsupervised mean-shift clustering, it supports efficient and unbiased identification of different types of mitochondrial shapes. The identified shape types provide the foundation for further investigation of their cellular functions.
Application III: 3D protein surfaces
Experiment designWe also tested our algorithm on a 3D protein shape dataset. We selected three series of protein structures from Database of Macromolecular Movements (http://www.molmovdb.org/), including insulin, Ca 2 sensor S100 and small G-protein Arf6. These three proteins were selected for their distinct geometry (). Insulin tends to have an ellipsoidal shape, Arf6 is more spherical overall, whereas S100 is approximately Y-shaped with a blunt end. Snapshots of the motion sequences of these three proteins were used as examples of each protein class. Overall, 20 examples were collected from each protein class.
Data preprocessingSurfaces of the selected proteins were generated using Gaussian kernel functions as in. A multi-level summation of Gaussian kernel functions was employed to generate implicit models from atomic resolution data of the selected proteins. A unique strength of this method is that it allows local resolution control on protein surfaces. Parameters were chosen manually to ensure a genus-zero surface. After triangular meshing of the protein surfaces and output of mesh vertices, spherical harmonics parameterization was computed using SPHARM-MAT software (). We computed spherical harmonics up to order 31 to ensure a 0.2-A  accuracy in approximating original molecule surfaces. In this way, each molecular surface was represented by a matrix of dimension 3  2048.
Results: dimension reductionFirst we tested the performance of our algorithm in approximating protein surfaces under reduced dimensions. SCA was performed on the set of 60 protein molecule surfaces and approximations of 10, 20 and 40 dimensions were generated (). To evaluate the level of surface distortion, we calculated three metrics: surface area distortion, normalized RMSE in spherical harmonic coefficients and normalized RMSE in coordinates of landmark points. Results were summarized in. In this case, SCA was able to approximate original surfaces with 95% accuracy using 3  40 real matrices (, rightmost column). Compared with the original 3  2048 coefficient arrays, the dimension of shapes was reduced by a factor of $51.
Results: shape clusteringNext we tested performance of the proposed algorithm in clustering molecule surfaces. The following is a summary of all the methods we tested: 1. Riemannian mean-shift clustering with dimension reduction (RMSSCA) and without dimension reduction (RMS). 2. Mean-shift clustering with 31 rotation invariant spherical harmonic features () as inputs (MSSH). From the 31D feature space, principal component analysis was used to select a 6D subspace that account for 99% variance of the data for mean-shift clustering (MSSH  PCA). 3. Mean-shift clustering with features obtained from Laplacian eigenmap () (MSLAP). Laplacian eigenmap was implemented as in the case of 2D shape clustering forNormalized errors in surface area, normalized RMSE in spherical coefficients and normalized RMSE in coordinates of landmark points on the molecule surface are reported. All numbers are reported in percentages.
Shape component analysisexamining the performance of a common strategy of 'flattening the nonlinear manifold.' As discussed in Section 3.1, metrics including purity, mutual information and adjusted Rand index were used to evaluate clustering performance. The results were summarized in. Overall, Riemannian mean-shift clustering provided the best performance. RMSSCA provided similar performance as RMS, but reduced the computation time by a factor of $5 compared with RMS. Among these algorithms, only RMS and RMSSCA provide backward projection to the original space and thus their clustering results can be visualized. In, clustering results via RMSSCA under various reduced dimensions are shown. The three basic structures (ellipsoidal, spherical and Y-shape) were detected under a dimension of 10. As the reduced dimension was increased to 20 and then 40, more details of the protein surfaces were captured by RMSSCA and resulted in more detected shape clusters (). Overall, the dimension-reduction technique developed here provides a tool for efficient and multi-resolution representation and analysis of 3D proteins shapes. Combined with unsupervised mean-shift clustering, it supports efficient and unbiased classification of proteins shapes and identification of different configuration modes of proteins in their movement.
DiscussionIn this study, we proposed a technique for dimension-reduction on the Riemannian manifold of 2D and 3D biological shapes. A key advantage of this technique is that it preserves distances between different shapes in an embedded low-dimensional shape space. We showed that although SHR of 3D shapes differs from the landmark representation of 2D shapes, they share the same Riemannian geometry and thus can be processed using the same dimension-reduction technique. We verified the proposed technique on datasets of 2D and 3D shapes. Specifically, we demonstrated an application of the technique by combining it with non-linear mean-shift clustering for unsupervised classification of biological shapes. Our approach is general and provides a tool for analyzing and understanding large sets of high-dimensional shape data. It can also be integrated with shape analysis techniques other than mean-shift clustering. Our approach is similar to PGA () in that both aim for dimension-reduction of shape representations. PGA relies on principal component analysis on the tangent space of the mean shape. In comparison, our approach maps shapes directly from a high-dimensional shape space to a low-dimensional shape space without making use of the tangent space. In this way, our approach better preserves distances between different shapes by causing less distortion and therefore better retains Riemannian geometry in the dimension-reduced space (Tables 1 and 3). However, our approach also has its limitations. In its current form, it can only handle genus-zero surfaces and does not support local or adaptive approximation of shape features.
FundingThis work was supported by the National Science Foundation [DBITwenty examples from each three shape clusters were used for testing in the 3D case.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
H.-C.Lee et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
