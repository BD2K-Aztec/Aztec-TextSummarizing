Motivation: The detection of positive selection is widely used to study gene and genome evolution, but its application remains limited by the high computational cost of existing implementations. We present a series of computational optimizations for more efficient estimation of the likelihood function on large-scale phylogenetic problems. We illustrate our approach using the branch-site model of codon evolution. Results: We introduce novel optimization techniques that substantially outperform both CodeML from the PAML package and our previously optimized sequential version SlimCodeML. These techniques can also be applied to other likelihood-based phylogeny software. Our implementation scales well for large numbers of codons and/or species. It can therefore analyse substantially larger datasets than CodeML. We evaluated FastCodeML on different platforms and measured average sequential speedups of FastCodeML (single-threaded) versus CodeML of up to 5.8, average speedups of FastCodeML (multi-threaded) versus CodeML on a single node (shared memory) of up to 36.9 for 12 CPU cores, and average speedups of the distributed FastCodeML versus CodeML of up to 170.9 on eight nodes (96 CPU cores in total).
INTRODUCTIONThe development of evolutionary models has a long tradition in phylogenetics, and recent advances have enhanced our understanding of the molecular mechanisms involved. At the heart of these advances is the democratization of the use of the likelihood framework, which was made possible by algorithmic developments () and the wide availability of powerful computing platforms. The surge of genomic data is, however, pushing the limits of current implementations [e.g. ()] and demands for the developments of better and more efficient ways to compute the phylogenetic likelihood function (PLF). The development of codon models is a good example to illustrate these current challenges and the benefits that can be reached by improving the efficiency of current likelihood calculations (). There are clear advantages to use codon models in phylogenetics (), but these are currently not widely used because of the large computational burdens involved (). Further, the detection of positive selection has been facilitated by the development of new codon models. However, their application to genome-scale data comprising a large number of species, or individuals in the case of population genomic studies, remains challenging. Thus, there exists an urgent need for improved implementations and novel optimization techniques to analyse emerging genomic datasets (). The prevalent approach for detecting positive selection in protein-coding genes is to use Markov models of codon substitution to estimate the ratio of non-synonymous to synonymous changes along the branches of a phylogenetic tree (). The branch-site model (BSM)(Section 8.4);allows to detect positive selection that affects a subset of codon sites for a subset of branches in a phylogenetic tree. This model is particularly useful to perform interspecific comparisons and is probably the most widely used approach for this specific purpose. The test compares a model that assumes positive selection on one branch or on a set of a priori specified branches (hypothesis H 1 ) with a null model that does not incorporate positive selection (hypothesis H 0 ). If the test is significant, the Bayes Empirical Bayes (BEB) method is used to compute the posterior probability of each particular codon to evolve under positive selection along the specified branches (). In CodeML, the test is usually applied iteratively and independently to each branch of a given phylogenetic tree (). This approach is compute bound, and although alternatives have recently been proposed, the limiting factor of such analyses *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.  The Author 2014. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/bync/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com still lies with the repeated calls to compute the PLF. For example, the estimation of positive selection on a large genomic vertebrate dataset (shows the enormous computational requirements of such analyses [approx. 100 CPU years for each release of the Selectome database (. As a consequence, large gene trees, comprising more than 100 sequences, are usually excluded and faster implementations of the BSM are urgently needed. This clearly illustrates the need to further optimize current software and to develop more efficient computational approaches for maximum likelihood inference on phylogenetic trees. Several recent studies introduced techniques for efficiently computing positive selection on the branches of a phylogenetic tree. One idea is to use stochastic mapping to count substitutions along the branches of a tree and thereby derive dN/dS ratios (). While this approach is fast, it is computationally distinct. Alternatively, new models have been proposed to avoid the likelihood ratio test (LRT) estimation of positive selection for all branches of the tree. Instead, branch assignments are considered as a random effect within a mixed effect framework (). Their model notably differs from the BSM () in that putative positive selection is not optimized on a priori defined branches, but over a subset of branches which are determined by the software. This technique reduces the computational cost of the test, but the accuracy and robustness of this new model is not yet fully characterized. Moreover, the authors introduced solutions for parallelizing BSM computations, but the parallel approach is not discussed in their article. The bottleneck in efficiency of phylogenetic software is commonly the PLF, as the majority of runtime is spent here. In (Stamatakis, 2011, p.2), the PLF is reported to consume 495% of total execution time in maximum likelihood and Bayesian tools for phylogenetic tree reconstruction. Although this was estimated when searching for the best tree topology, which is a key component of phylogenetic computations but not the focus of this article, the PLF is still the core element in all phylogenetic applications using maximum likelihood. All these areas would therefore benefit from an optimized PLF. Recent discussions have proposed to use data augmentation strategies to speed up the likelihood calculations by using heuristics to simplify the estimation of the conditional vectors at each node (). However, there are still opportunities for improving the PLF with respect to sequential efficiency and parallelization techniques. Our main objective is therefore to propose methodological and algorithmic improvements and parallelization strategies to compute the PLF without modifying the underlying evolutionary model. Our optimizations and parallelizations yield substantial speedups in the likelihood computations. Hence, we can apply the BSM to large trees of several hundreds of sequences and obtain results in feasible times. These computational optimizations are thus of broad applicability to further likelihoodbased phylogenetic software, including but not limited to nucleotide-and amino acid-based phylogenetic analyses in both the maximum likelihood and Bayesian frameworks ().
Number of elementary tree operationsIn the BSM framework, four site classes 0, 1, 2a and 2b are applied to model combinations of purifying selection, neutral evolution, and positive selection on foreground and background branches. When computing hypotheses H 0 and H 1 , each site class has its distinct proportion according to its contribution to the overall likelihood (cf. the supplementary material for an introduction to the BSM). These proportions only depend on the two parameters p 0 and p 1 ; each site class has a specific ! value for its selective pressure in the foreground and in the background. ! 0 is in the interval (0,1), ! 1  1 and either ! 2 41 (foreground for H 1 ) or ! 2  1 (foreground for H 0 ). Q f0, 1, 2g corresponds to ! f0, 1, 2g , respectively. Computing the likelihood requires computing the transition probabilities for a given branch length t by computing the matrix exponential P t  e Qt  e St , where Q is the instantaneous substitution rate matrix, S is the symmetric codon substitution matrix and  is the diagonal matrix of codon frequencies. The resulting probability matrix P t is used to update the corresponding conditional probability vector (CPV) w, that is, w 0  P t w. Each CPV models the site-wise transition between 61 codon states (universal genetic code) along each branch of the phylogenetic tree. This operation is applied to all sites of the multiple sequence alignment (MSA) and to all nodes of the tree by means of a post-order tree traversal. The CPU-intensive computation of the CPV entails the following three computational kernels that operate on real dense matrices (similar to SlimCodeML, see Section 2.1.2): (i) eigendecomposition of a symmetric matrix [see, e.g. (, (ii) multiplication of a matrix by its transpose (resulting in a symmetric matrix) and (iii) multiplication of a symmetric matrix by a vector.
How many decompositions?To compute e Qt we need to decompose Q for each distinct combination of parameters (transition to transversion rate), j and !. The j are constant over site classes and parameter optimization steps; may change at each parameter optimization step (but is constant over site classes); ! varies among optimization steps and site classes. For each distinct value of !, Q is distinct and therefore needs to be decomposed separately. There are three distinct ! values over all site classes; hence, we need to decompose three Q matrices in the first parameter optimization step. For subsequent steps, ! 1  1 remains constant, but Q 1 may change because of a new value. The total number of Q decompositions does not depend on the number of branches in the tree nor on the number of sites in the MSA. In the general case, the number of Q matrices depends on the number of unique substitution matrices in the model, which can be large in mixture models [e.g. (. With respect to other evolutionary models, similar optimizations may be applicable.
How many matrixmatrix multiplications? P t has to be computed for each combination of Q and t. For our case of binary trees, the number of branches in the phylogeny equals 2n2 where n is the number of extant taxa. For each distinct Q, branches have to be computed separately. The BSM applies Q 0 and Q 1 to each branch, but Q 2 only to foreground branches. In other words, P t has to be computed for all branches using Q 0 and Q 1 (site classes 0 and 1), and in addition on the foreground branch(es) by using Q 2 (site classes 2a and 2b). Therefore, we need to compute P t 2m  l times for m branches in the phylogeny and l foreground branches; this yields 2  2n  2  1  4n  3 branches when using a single foreground branch. Overall, we need to compute 17 distinct P matrices in our example 1. This matrixmatrix multiplication is also applied in further evolutionary models based on substitution matrices.
How many matrixvector computations? In a straightforward approach, each CPV is computed along each branch for all sites and all site classes. In our example this makes 8  4  2  64 CPV computations. If a CPV connected to a leaf is computed on 'clean' data [no ambiguity symbols in MSA (, the CPV at the leaf only contains a single 1 (0 elsewhere). In this case, computing the resulting CPV simplifies to selecting the corresponding column of the P matrix. In the general case, an upper limit of the number of involved matrixvector multiplications per site class is the number of branches in the phylogeny  the number of sites in the MSA. Certainly, this number can be decreased depending on similarities in the codons as discussed in Section 2.1.1 ('subtrees reuse'). Likewise, this step is important to all other evolutionary models based on substitution matrices. Further computational savings are possible. In this context, we refer to a 'subtree' as a connected part of the phylogeny where at least one node is a leaf. Whenever a particular branch of a single site applies the same P and all other CPVs of its subtree match, the particular CPV has a 'twin' in another site class and needs to be computed only once. In, such matching CPVs are identified by matching indexes. For example, CPV23 appears in site class 1 and in site class 2b, as also CPV20 and CPV21 have twins, and they pairwise apply matching P matrices (here, all based on Q 1 ). These redundancies are caused by matching ! 0 values for site classes 0 and 2a and by matching ! 1 values for site classes 1 and 2b. In our example, this means that only 40 out of 64 (62.5%) CPVs have distinct values and will hence have to be computed. CPVs are computed recursively via a postorder traversal propagating from the leaves towards the root (). Hence, for the BSM in general, the number of distinct CPVs depends on the location of the foreground branch in the tree (the closer to the root, the less CPV computations are required).
IMPROVEMENTSHere we discuss optimization techniques that we propose. Note that we have not added any heuristics, and each of the following improvements is supposed to be beneficial independent of the number of species and independent of the number of alignment sites. Specific implementation issues are described along with each optimization technique.
Sequential improvementsbe advantageous to implement the second approach, where the additional cost for storing or linking site patterns is compensated by a faster lookup. In FastCodeML, we identify reusable subtree patterns in a preprocessing step and tag each node with the codon sequence identified by the subtree rooted in this node. Subsequently, a lookup of these tags for all sites with identical subtrees is done. Once identified, the CPV that can be re-used is linked via a pointer in the reusing tree, that is, this saves the costs of computing this particular CPV. The unused subtree can be freed to reduce memory consumption. In the example of, computing the two CPVs incident to two leaves in box  and the CPV at  are redundant, because both codon sites feature an identical subtree: all involved CPVs match. Thus three CPV computations can be saved. Related techniques for extending pattern detection and re-use in the MSA to the subtree level have already been proposed (). However, they focus on detecting patterns and avoiding redundant likelihood computations on trees whose topologies change in the course of ML tree search. For dynamically changing trees, a trade-off between the pattern detection and memory storage costs and the amount of saved computations needs to be achieved. To reduce the cost of pattern detection, the initial implementation of the Subtree Equality Vector (SEV) technique () only considered subtree patterns that contained a single identical character. The book keeping was subsequently further simplified to sites consisting entirely of gaps (). In Kosakovsky, the authors suggest to sort nucleotide-based MSAs by site similarity to avoid redundant computations. This approach minimizes memory consumption, as only a subset of sites needs to be kept in memory. However, this incurs additional costs for rearranging the sites in order to maximize the number of lookups from neighboring sites. The memory consumption for our application scenario (Selectome database updates) does not represent a limiting factor. Hence, all CPVs can be kept in memory, avoiding the expensive reordering of sites. However, especially for memoryintensive approaches, it may be more effective to keep only a subset of all CPVs in memory and consider site sorting.
New matrix exponential and CPV computationwhere ^ Y   1=2 Xe t=2 : 2Note that ^ Y ^ Y > is by construction a symmetric matrix, whereas  1=2 YY >  1=2 is generally asymmetric. The advantage of this modification is that the symmetry reduces the number of necessary matrix memory accesses by approx. 50% (). This technique has been implemented in FastCodeML.
LRT optimizationWhen optimizing parameter values for H 0 and H 1 one after the other, one can save on parameter optimization steps. Each step in the parameter optimization procedure improves the associated lnL of the tree until convergence has been reached. In this discussion, the optimizer may modify all parameter values at each single step. One can either (i) optimize H 0 first with high accuracy and iteratively improve H 1 afterwards: once 2lnLH 1   lnLH 0  becomes larger than  2 1  1 1  , the parameter optimization for H 1 can be stopped because the LRT is already significant. This potentially saves optimization steps for H 1. Or we can (ii) optimize H 1 first, then proceed analogously: the parameters of H 0 are optimized until 2lnLH 1   lnLH 0  becomes smaller than  2 1  1 1  . In general, a significant LRT (i.e. detecting positive selection) is a relatively rare event (). Strategy (i) saves optimization steps if positive selection occurs; strategy (ii) saves optimization steps if not. Consequently, without prior knowledge of the frequency of occurrence of positive selection in the MSA at hand, strategy (ii) (implemented in FastCodeML) will yield larger savings. If the LRT is significant, a BEB is applied to identify the sites under positive selection. Otherwise, FastCodeML does not execute the BEB, in contrast to CodeML. In the general case, this optimization is applicable if different models are compared, where each of them is optimized iteratively.. Subtrees reuse strategy depicted for two (not necessarily neighboring) sites in the MSA; in (a) subtree (1) contains identical codons for both sites; consequently, in (b) the CPVs for both sites are identical and need to be computed only once (dotted line)
ParallelizationWhile the parallelization of ML-based nucleotide-protein-and codon models has already been addressed () (e.g. RAxML, IQPNNI, HyPhy), it has mostly been in the context of tree topology optimization, and not for the likelihood itself. The main challenge in parallelizing ML-based phylogeny computations comes from the tree structure that leads to an irregular domain decomposition (). An efficient parallelization of the BSM is even more challenging due to its site classes and dependencies in between. Our implementation optimizes simultaneously all the parameters. The maximizer acts as an impenetrable boundary for parallelization, and we distinguish parallelization 'above' (coarsegrain) and 'within' (fine-grain) this boundary (cf. supplementary material,2.2.1 Coarse-grain parallelization: Gene-wise parallelization. Because distinct genes typically have different evolutionary histories with distinct branch lengths and evolutionary parameters, phylogenies for genes are commonly estimated independently for each gene. Consequently, single genes cannot be concatenated into multi-gene alignments to attain high scalability by means of a fine-grain parallelization of the likelihood function [see, e.g. (. Here we test for selection independently (gene-wise), these analyses can be carried out in an embarrassingly parallel way [see, e.g. (. Foreground branch parallelization. A further BSM parallelization option is the simultaneous analysis of distinct foreground branches. This is possible because we want to test for positive selection on each branch of a given phylogeny. Thus, the 2n  3 tests for positive selection, where n is the number of taxa, can be conducted in parallel by duplicating the tree data structure and CPVs. Under this parallelization strategy, a dedicated master process broadcasts all model parameters, tree topologies and branch lengths to all worker nodes. The workers then conduct the tests independently of each other on different foreground branches of the same tree. Afterwards, the worker nodes return the estimated parameter values and the lnL scores to the master process. We implemented this approach using MPI (). The foreground-branch based parallelization can be combined with a site-wise fine-grain parallelization of the per-tree likelihood computations (Section 2.2.2) into a hybrid parallelization scheme. Hypotheses parallelization. Note that for each foreground branch, hypotheses H 0 and H 1 can be computed independently and simultaneously, thus increasing the degree of parallelism. However, the simultaneous computation of H 0 and H 1 prevents us from using the aforementioned LRT optimization (Section 2.1.3). Although the LRT and the subsequent BEB must be computed after H 0 and H 1 , they can be parallelized between different foreground branch computations. This parallelization strategy can be applied whenever two evolutionary models are compared. It is implemented in FastCodeML via the same master-worker scheme. 2.2.2 Fine-grain parallelization: Site-wise parallelization. A common way to parallelize likelihood computations on shared memory architectures is by parallelizing over the sites of the MSA. This site-wise parallelization can be implemented using OpenMP or POSIX Threads. MPI-based implementations exist but focus on large MSAs that are outside the scope of this article. However, while our subtree patterns re-use scheme (Section 2.1.1) reduces the number of computations along the branches, it poses a load balance challenge: (i) a particular CPV for a site can only be computed after the site whose results it reuses (i.e. data dependency) has been computed and (ii) a site that reuses a previously computed CPV exhibits a smaller workload which leads to load imbalance. The load balancing strategy we use in FastCodeML subdivides the alignment sites into groups such that each group exclusively reuses subtrees from the previous groups (). Each group is assigned a rank value starting from zero. CPVs from groups with lower rank values can potentially be reused. The first group does not reuse any subtree. All subtrees of a group can be parallelized, because they are independent of each other. The groups are then computed sequentially in order of rank. To balance the load for each group, subtrees can be moved to higher ranked groups. To increase parallelism, the trees of each group are replicated for each site class that should be computed until no lower rank group depends on it. The parallelization inside each group has been implemented using OpenMP. This site-wise parallelization strategy including load balancing can likewise be applied to nucleotide-or protein-based MSAs. The parallel performance may vary due to different computational load per site.
Implementation
EVALUATIONWe measure median runtimes of 10 individual runs for each evaluation (three on the large scale analysis in Section 3.5). Speedup values are determined by S  T1 T2 , where T 1 is theLoad balancing strategy: the sites of the tree are grouped so that each group depends only on groups at its left (continuous lines). A tree can be moved to a group to its right (dashed line) only if it has no dependencies from other trees in intermediate groups 1133 FastCodeML runtime (elapsed time, wall-clock time) of the reference execution and T 2 the runtime of the execution to be evaluated on the same dataset; for a relative speedup T 1 and T 2 denominate runtimes of the same executable, while for the absolute speedup T 1 is strictly the original CodeML. Initial branch lengths were read from file, while model parameters are initialized randomly. Memory consumption of CodeML, SlimCodeML and FastCodeML for these datasets is not a limiting factor and therefore not performance critical. Although a single executable can be used for all subsequent evaluations, we built sequential, OpenMP parallelized, MPI parallelized and hybrid executables separately. A summary of the platforms used can be found in the supplementary material.
Datasets
AccuracyInwe analyse the accuracy of FastCodeml with respect to lnLs and LRT scores. We use SlimCodeML as a proxy for good accuracy, as it gives very similar results as CodeML (), which is the established gold standard. We note that the accuracy of computed lnLs is much higher than typically required to discriminate between significant and insignificant LRTs.
Sequential runtimesSequential speedups of FastCodeML (single-threaded) versus CodeML and SlimCodeML for five datasets (H 0 and H 1 , respectively) on platform Macpro (cf. supplementary material) are depicted in; here, FastCodeML includes the following improvements: faster matrix exponentiation (Section 2.1.2) and subtrees reuse (Section 2.1.1). LRT optimization (Section 2.1.3) is not considered, as either H 0 or H 1 is computed per run. We observe speedups of FastCodeML versus CodeML ranging from 2.6 to 5.8. The sequential FastCodeML is significantly faster than both CodeML and SlimCodeML on all five datasets.shows the scaling of FastCodeML on a site-wise (OpenMP based) parallelizationslightly worse than without subtrees reuse, absolute runtimes on this particular platform and dataset suggest to enable subtrees reuse on 111 cores but not on 12. The worse scaling of subtrees reuse is presumably caused by load imbalance. Due to differences in the sequential performance of subtrees reuse, we also expect the performance of parallel subtrees reuse to vary with different datasets. In general, the effectiveness of parallel subtrees reuse is a trade-off between the number of redundant branches versus the data dependencies introduced.depicts the relative scaling of FastCodeML on a foreground-branch based parallelization strategy. The evaluation has been done for dataset D3 on 17 worker nodes (single thread per node). Due to the masterworker scheme used, performance gains are observed for two or more worker nodes. The analysis is done for all possible 22 foreground branches, where the runtime for CodeML is measured only on a single foreground branch but multiplied by 22; running CodeML on all foreground branches is expected to consume more than a day. We observe relative speedups of up to 5.9 on 7 worker nodes, which corresponds to absolute speedups from 3.3 to 19.4. In general, the relative speedup for foreground branch-based parallelizations benefits from a high ratio of foreground branches to available nodes, as the workload can more easily be divided into balanced parts.cores per node and by reusing subtrees (Section 2.1.1). We analysed D6 for H 0 and H 1 running FastCodeML (multithreading) on 12 CPU cores and determined average runtimes of three test runs. The average runtime of FastCodeML on dataset D6 is 21.9 h for H 0 and 31.9 h for H 1. Due to time restrictions, we evaluated only a single iteration of CodeML for D6 which took 2.2 h on H 0 (367 iteration steps) and 2.3 h on H 1 (426 iteration steps) on the same platform. As we apply the same parameter optimization codes, we use the average number of optimization steps of FastCodeML on dataset D6 for the following speedup metric: we extrapolate that CodeML would have finished executing in approximately 2:2  367  807:4 h (i.e. ca. 33.6 days) for H 0 and 2:3  426  979:8 h (i.e. ca. 40.8 days) for H 1. The estimated speedups comparing the single threaded CodeML with FastCodeML running in 12 threads is thus 36.9 for H 0 and 30.7 for H 1. In this example, the LRT optimization saves 268 optimization steps for H 1 (63%).
Parallel runtimes
Site-wise parallelization
Foreground branch-based parallelization
Hybrid parallelization
CONCLUSIONSWe introduced here three sequential code optimizations: an improved matrix exponential, subtrees reuse and LRT optimization.We observed significant speedups versus both CodeML and our previous version SlimCodeML, and the first two optimizations can be used in various likelihood computations in phylogenetics. Moreover, we present a parallelization strategy that uses a finegrain and a coarse-grain approach. Overall, our improvements allow for testing selection on phylogenetic trees which exceed the possibilities of the original CodeML software; this is crucial to tackle the genomic data avalanche. The discussed improvements are motivated by the branch-site model but can, due to the likelihood framework, be extended to nucleotide-and amino acid-based MSAs as well as Bayesian approaches. We briefly identified such opportunities where applicable, but an extensive discussion is subject to future work. The optimization of the likelihood surface for phylogenetics problems is complex and we have started experimenting with the alternative parameter optimizers available in NLopt (http://abinitio.mit.edu/wiki/index.php/NLopt). It may be interesting to compare different implementations of the BroydenFletcher GoldfardShanno (BFGS) optimization method, but a deeper investigation of the global and derivative-free optimizers is needed to better understand the potential solutions to find the maximum likelihood estimator for complex evolutionary models. In a future version the dependencies between nodes could be modelled as a directed acyclic graph and the parallelism be based on a dataflow model () to study and potentially further improve parallel performance. Moreover, the site classes could be included into the dependency graph. This way a more fine-grained parallelism could be achieved. Increasing the parallel performance becomes crucial with the trend of more parallelism in future computer platforms ().
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
M.Valle et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
.1.1 Subtrees reuse The per-site likelihoods for a MSA are independent of each other and can thus be computed in an arbitrary order. If two or more sites of the MSA are identical, it suffices to only compute the logarithmic likelihood (lnL) on one site and multiply it by the number of identical sites to obtain the total lnL. This technique is used in most likelihood-based software, but there are further redundant computations caused by re-occurring patterns in the MSA. In each subtree, there is a potential to economize CPV computations for different sites of the MSA. If the same state appears at two or more sites of a sequence, all occurrences yield identical CPVs at the particular leaf. If the patterns of the sub-alignment induced by a subtree match are identical for two or more sites, the corresponding CPVs for the two sites are also identical. However, identical patterns in the sub-alignments induced by a subtree need to be identified first. The identification of such identical patterns in sub-alignments can be done, e.g. by searching (i) sequentially or (ii) using a symbol table (Sedgewick and Wayne, 2011, p.361). In the latter case, the key is the index of the CPV within the tree, and the value associated with the key is its CPV. In the straightforward approach (i), there are no costs on storing values, but up to m  1 lookups for a matching subpattern, where m is the length of the MSA. For huge MSAs, it may Fig. 1. Analysis on how many elementary subtree computations are necessary in the branch-site model; CPVm correspond to m distinct conditional probability vectors, where matching m need to be computed only once; Q 0, 1, 2 f g identify three distinct Q matrices for distinct ! 0, 1, 2 f g values 1131 FastCodeML at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
