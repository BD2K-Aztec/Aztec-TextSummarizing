Motivation: Counting the frequencies of k-mers in read libraries is often a first step in the analysis of high-throughput sequencing data. Infrequent k-mers are assumed to be a result of sequencing errors. The frequent k-mers constitute a reduced but error-free representation of the experiment, which can inform read error correction or serve as the input to de novo assembly methods. Ideally, the memory requirement for counting should be linear in the number of frequent k-mers and not in the, typically much larger, total number of k-mers in the read library. Results: We present a novel method that balances time, space and accuracy requirements to efficiently extract frequent k-mers even for high-coverage libraries and large genomes such as human. Our method is designed to minimize cache misses in a cache-efficient manner by using a pattern-blocked Bloom filter to remove infrequent k-mers from consideration in combination with a novel sort-and-compact scheme, instead of a hash, for the actual counting. Although this increases theoretical complexity, the savings in cache misses reduce the empirical running times. A variant of method can resort to a counting Bloom filter for even larger savings in memory at the expense of false-negative rates in addition to the false-positive rates common to all Bloom filter-based approaches. A comparison with the state-of-the-art shows reduced memory requirements and running times. Availability and implementation: The tools are freely available for download at
INTRODUCTIONK-mers play an important role in many methods in bioinformatics because they are at the core of the de Bruijn graph structure () that underlies many of today's popular de novo assemblers (). They are also used in assemblers based on the overlaplayout-consensus paradigm like Celera () and Arachne () as seeds to find overlap between reads. Several read correction tools () use k-mer frequencies for error correction. Their main motivation for counting k-mers is to filter out or correct sequencing errors by relying on k-mers that appear multiple times and can thus be assumed to reflect the true sequence of the donor genome. In contrast, k-mers that appear only once are assumed to contain sequencing errors. Melsted and Pritchard (2011) and Marcais and Kingsford (2011) make a more detailed compelling argument about the importance of k-mer counting. In a genome of size g, we expect up to g unique k-mers. This number can be smaller because of repeated regions (which produce the same k-mers) and small k, as smaller k-mers are less likely to be unique, but is usually close to g for reasonable values of k. However, depending on the amount of sequencing errors, the total number of k-mers in the read library can be substantially larger than g. For example, in the DM dataset (), the total number of 31-mers is $289.20 M, whereas the number of 31-mers occurring at least twice is $131.82 M. The size of the genome is 122 Mb (megabase pairs). This is not surprising because one base call error in a read can introduce up to k false kmers. Consequently, counting the frequency of all k-mers, as done by Jellyfish (), which is limited to k 31, requires O(N) space where N is the number of k-mers in the read library. This makes the problem of k-mer frequency counting time and memory intensive for large read libraries like human. We encounter similar problems for large libraries while using Khmer (), which uses a Bloom filter-based () approach for counting frequencies of all k-mers. Ideally, the frequent k-mer identifier should use O(n) space where n is the number of frequent k-mers (n ( N). The approach taken by BFCounter () achieves something close to this optimum by ignoring the infrequent k-mers with a Bloom filter and explicitly storing only frequent k-mers. This makes BFCounter more memory-efficient compared with Jellyfish. However, the running time of BFCounter is large for two reasons. First, it is not multi-threaded. Second, both the Bloom filter and the hash table used for counting incur frequent cache misses. The latter has recently been identified as a major obstacle to achieving high performance on modern architectures, motivating the development of cache-oblivious algorithms and data structures (), which optimize the cache behavior without relying on information of cache layout and sizes. Additionally, BFCounter is also limited to a count range of 0255, which will often be exceeded in single-cell experiments because of the large local coverage produced by whole genome amplification artifacts. A different approach is taken by DSK () to improve memory efficiency. DSK makes many passes over the read file and uses temporary disk space to trade off the memory requirement. Althoughclaimed DSK to be faster than BFCounter, on our machine *To whom correspondence should be addressed. using an 18 TB Raid-6 storage system; DSK required more wallclock time compared with BFCounter. Therefore, we consider DSK without dedicated high-performance disks, e.g. solid state, and BFCounter to be too slow for practical use on large datasets. A disk-based sorting and compaction approach is taken by KMC (), which was published very recently, and it is capable of counting k-mers of large read libraries with a limited amount of memory. However, in our test environment, we found it to be slower than the method described here. We present a novel approach that reduces the memory footprint to accommodate large genomes and high-coverage libraries. One of our tools (scTurtle) can report frequent 31mers with counts (with a very low false-positive rate) from a human read set with 135.3 Gb using 109 GB of memory in 52 h using 19 worker threads. Like BFCounter, our approach also uses a Bloom filter to screen out k-mers with frequency one (with a small false-positive rate), but in contrast to BFCounter, we use a pattern-blocked Bloom filter (). The expected number of cache misses for each inquiry/update in such a Bloom filter is one. The frequency of the remaining k-mers is counted with a novel sorting and compaction-based algorithm. Our compaction step is similar to run-length encoding (). Note that this is similar to the strategy of KMC, which was developed as a concurrent and independent work. Though the complexity of sorting in our compression step is On log n, it has sequential and localized memory access that helps in avoiding cache misses and will run faster than an O(n) algorithm that has O(n) cache misses as long as log n is much smaller than the penalty issued by a cache miss. For larger datasets, where O(n) space is not available, the aforementioned method will fail. We show that it is possible to get a reasonable approximate solution to this problem by accepting small false-positive and false-negative rates. The method is based on a counting Bloom filter implementation. The error rates can be made arbitrarily small by making the Bloom filter larger. Because the count is not maintained in this method, it reports only the k-mers seen more than once (with a small false-positive and false-negative rate), but not their frequency. We call the first tool scTurtle and the second one cTurtle.
METHODS
scTurtle2.1.1 Outline By a k-mer, we always refer to a k-mer and/or its reverse complement. Our objective is to separate the frequent k-mers from the infrequent ones and count the frequencies of the frequent k-mers. To achieve this, first, we use a Bloom filter to identify the k-mers that were seen at least twice (with a small false-positive rate). To count the frequency of these k-mers, we use an array of items containing a k-mer and its count. These are the two main components of our tool. Once the counts are computed, we can output the k-mers having a frequency greater than the chosen cutoff. For the sake of cache efficiency, the Bloom filter is implemented as a pattern-blocked Bloom filter (). It localizes the bits set for an item to a few consecutive bytes (block) and thus reduces cache misses. The basic idea is as follows: when a k-mer is seen, the Bloom filter is checked to decide whether it has been seen before. If that is the case, we store the k-mer in the array with a count of 1. When the number of items in the array crosses a threshold, it is sorted in place, and a linear pass is made, compressing items with the same k-mer (which lie in consecutive positions of the sorted array) to one item. The counts add up to reflect the total number of times a k-mer was seen. Note that this strategy is similar to run-length encoding () of the items. Our benchmarking () shows that this simple approach of storing items and their frequencies is faster than a hash tablebased implementation. An outline of the algorithm is given in Algorithm 1. More details are provided in the following subsections. Note that the improved efficiency of sort and compaction also suggests that it can speed up the k-mer counting step for all de Bruijn graph-based assemblers where k-mer counting is required for building the graph. We found that ABySS () and SPAdes () require 3660 and 2144 s, respectively, for k-mer counting on the DM dataset (see). But the sort and compaction method takes only 523.41 s. We provide a single-threaded preliminary tool called aTurtle that implements this method for counting all k-mers and their frequencies.Algorithm 1 scTurtle outline 1. Let S be the stream of k-mers coming from the read library, BF be the Bloom filter, A be the array to store k-mers with counts and t be the threshold when we apply sorting and compaction. 2. for all k-mer 2 S do 3. if k-mer present in BF then 4. Add k-mer to A 5. end if 6. if jAj ! t then 7. Apply sorting and compaction on A 8. end if 9. end for 10. Apply sorting and compaction on A. 11. Report all k-mers in A with their counts as frequent k-mers and their counts. 2.1.2 k-mer extraction and bit encoding For space efficiency, kmers are stored in a bit-encoded form where 2-bits represent a nucleotide. This is possible because k-mers are extracted out of reads by splitting them on 'N's (ambiguous base calls) and hence contain only A, C, G and T. Because we consider a k-mer and its reverse complement to be two representations of the same object, whenever we see a k-mer, we also compute the bit representation of the reverse complement and take the numerically smaller value as the unique representative of the k-mer/reverse complement pair.Note: Jellyfish is a highly optimized hash table-based implementation for the k-mer counting problem. We also compare against general purpose tools that uses Google sparse/dense hash maps for storing k-mers and their counts.
Identificationof frequent k-mers with pattern-blocked Bloom filter A Bloom filter is a space-efficient probabilistic data structure, which, given an item, can identify whether this item was seen before with some prescribed, small false-positive rate. We use this property of the Bloom filter to identify k-mers that were seen at least twice. An ordinary Bloom filter works as follows: a large bit-array (B) of size L is initialized to 0. Given an item x, k hash values (h 1 , h 2 ,. .. , h k ) using kindependent hash functions {within the range 0, L  1} are computed. We now check all the bits Bh 1 ,. .. , Bh k . If they are all set to 1, with high probability, this item has been seen at least once before. If not, it is certainly the first appearance of this item, and we set all of Bh 1 ,. .. , Bh k  to one. For all subsequent appearance(s) of this item, the Bloom filter will report that it has been seen at least once before. In this way, the Bloom filter helps us to identify frequent k-mers. Note that if the bit locations are randomly distributed, because of the large size of the Bloom filter, each bit inspection and update is likely to incur one cache miss. Thus, the total number of cache misses per item would be k. On the contrary, if the bit locations are localized to a few consecutive bytes (a block), each item lookup/update will have a small number of cache misses. This can be done by restricting h 1 ,. .. , h k to the range h 1 x, h 1 x  b where b is a small integer. The bit pattern for each item can also be precomputed. This is called the pattern-blocked Bloom filter.observe that the increase in false-positive rate because of this localization and precomputed patterns can be countered by increasing L by a few percent. To summarize, we first select a block for an item (using a hash function), select h 1 5h 2 5. .. 5h k from a set of precomputed random numbers such that all of them lie within the block and update/inquire them sequentially. Note that Bloom filters are widely used in many applications like assembly (), and we believe using a more optimized version of this data structure (like the pattern-blocked Bloom filter) will benefit such applications.
Counting frequencies with sorting and compaction Our nextobjective is to count the frequencies of the frequent k-mers. The basic idea is to store the frequent k-mers in an array A of size4n, where n is the number of frequent items. When this array fills up, we sort the items by the k-mer values. This places the items with the same k-mer next to each other in the array. Now, by making a linear traversal of the array, we can replace multiple items with the same k-mer with one item where a count field represents how many items were merged, which is equal to how many times this k-mer was seen; see. Note that this is similar to run-length encoding. Here is a toy example: say A  .. . , i, 1, .. . ,. .. , i, 1,. .. .. . , i, 1. After sorting A  .. . ,. .. , i, 1, i, 1, i, 1,. .. .. . and compressing results in A  .. . , i, 3,. . .. We have to repeat these steps until we have seen all items. To reduce the number of times, we sort the complete array, and we apply the following strategies. We select a threshold n5t5jAj. We start with an unsorted k-mer array. It is sorted and compacted (Phase-0 Sorted and Compacted array or Phase-0 SAC). We progress in phases as follows. At phase i, a certain number of items in the beginning of the array are already sorted and compressed [Phase-(i1) SAC]. The new incoming k-mers are stored as unsorted items in the empty part of the array. Let m be the total number of items in the array. When m4t, we sort the unsorted items. Many of these k-mers are expected to exist in Phase-(i1) SAC. We make a linear traversal of the array replacing k-mers present in both Phase-(i1) SAC and the newly sorted part with one item in Phase-(i1) SAC. The k-mers not present in Phase-(i  1) SAC are represented with one item in the newly sorted part. The counts are added up to reflect the total number of times a k-mer was seen. This takes O(m) time. Note that this compaction has sequential and localized memory access, which makes it cache-efficient. After a few such compaction steps, m4t, and we sort and compress all the items in the array to produce Phase-i SAC. By repeatedly applying this mechanism on the frequent items, we ultimately get the list of frequent k-mers with their counts decremented by 1. This is due to the fact that when inserted into the array for the first time, an item was seen at least twice unless it is a false-positive finding. To offset this, we simply add 1 to all counts before writing them out to a file.
ParallelizationWe implemented a one-producer, multiple-consumer model with a pool of buffers. The producer extracts k-mers from the reads and distributes them among the consumers. Each consumer has its own Bloom filter. Because a k-mer should always pass through the same Bloom filter, we distribute the k-mers to the consumers using the modulo operation, which is one of the cheapest hash functions available. Because modulo a prime number shows better hash properties compared with non-primes, it is recommended that one uses a prime (or at least an odd) number of threads because this spreads out the k-mers more evenly among the consumers, which is helpful for speeding up the parallelization. The k-mers are stored in buffers, and only when the buffers fill up are they transferred to the consumer. Because consumers consume the kmers at an uneven rate, having the same fixed buffer size for all consumers may cause the producer to block if the buffer for a busy consumer fills up. To reduce such blocking, we have a pool of buffers, and the number of buffers is more than the number of consumers. If a consumer is taking longer to consume its items, the producer has extra buffers to store its k-mers in. This improves the speedup. With many consumers (usually413), the producer becomes the bottleneck. Therefore, it is important to make the producer more efficient. The two most expensive parts of the producer are converting reads to k-mers and the modulo operation required to determine which consumer handles a particular k-mer. Modern computers support SSE () instructions that operate on 128-bit registers and can perform arithmetic/logic operations in parallel on multiple variables. We used Streaming SIMD Extensions (SSE) instructions for speeding up bit encoding of k-mers. It is also possible to design approximate modulo functions that execute much faster than regular modulo instruction for some numbers (e.g. 5, 7, 9, 10, 63) (). But each of these functions has to be custom designed. If we restrict the number of consumers to the numbers that have efficient modulo function, it is possible to improve the producer's running time even further.
Running time analysis We first analyze the sort and compressalgorithm. Let the total number of frequent k-mers (those with frequency !2) be N, and let n be the number of distinct frequent k-mers. We use an xn, x41, sized array A for storing the frequent k-mers and their counts. First, consider the following simplified version of our algorithm: x  1n new items are loaded into the array, and they are sorted and compacted. Because there are n distinct k-mers, at least xn  n  x  1n locations will be empty after sorting and compaction. We again load x  1n items and perform sorting and compaction. We iterate until all N items have been seen. Each iteration takes Oxn log xn  xn time, and we have at most N=x  1n such iterations. Thus, the total time required is:As discussed earlier, to reduce the number of times sorting is performed, which is more expensive than compaction, we implemented a modified version of the aforementioned method, which delays sorting at the expanse of more compactions. Our benchmarking shows this to be faster than the naive method. The algorithm we implemented progresses in phases as follows. At the beginning of phase i, the array is filled up with unsorted elements. They are sorted and compactedThe total cost of a lazy compaction is therefore upper bounded by Oxn log xn  xn. This again creates empty locations at the end of the array, which allows us to perform another round of lazy compression. We assume that the incoming items are uniformly distributed, and every lazy compaction stage reduces the size of the empty part by an approximately constant fraction 1/c. Therefore, on average, we expect to have c lazy compaction stages. This completes Phase-i, the expected cost of which is upper bounded by:Oxn log xn  xn  cxn log xn  xn  Oc  1xn log xn  xnTo compute how many phases are expected to consume all N items, we observe that, at every phase, the lazy compaction steps consume a total of at least x  1nf1  1  1 c   1  2 c  . ..  1  c1 c g  x  1 nc  1=2 items. So, on average, each phase consumes at least c  1nx  1=2 items, and hence the expected number of phases is at most 2N=nc  1x  1. Therefore, the total expected cost would be: 2N c  1nx  1 O xnc  1 log xn  xnc  1    2x x  1 O N log xn  N   O x x  1 N log N  N Note that we obtained the same expression for the naive version of sorting and compaction. It is surprising that this expression is independent of c. As an intuitive explanation, observe that more lazy compactions within a phase result in more items being consumed by a phase, which in turn decreases the number of phases. This inverse relationship between c and the number of phases makes the running time independent of c. We found the naive version to be slower than the implemented version in empirical tests and therefore believe our bound to be an acceptable approximation.We now analyze the performance of sorting and compaction-based strategy against a hash table-based strategy for counting frequency of items. Let p be the cache miss penalty, h be the hashing cost, s be the comparison and swapping cost for sort and compress and b be the number of items that fits in the cache. The cost of frequency counting in the hash-based method will be (p  h)N because each hash update incurs one cache miss. For sorting and compress, we will have one cache miss for every b operation, and thus the cost for sorting and compaction will be p=b  saN log N  N, where a  x x1. To compute the value of N for which sorting and compaction will be faster than a hashbased method, we write:Let a comparison and swap be one unit of work. A conservative set of values like s  1, p  160 (), h  8, b  256 (assuming 8 byte items and 2 KB cache), a  2 results in N 2 50. Therefore, for a large range of values of N, with a fast and moderate-sized cache, the sorting and compaction-based method would run faster than a hashbased method. Because every observed k-mer has to go through the Bloom filter, the time required in the Bloom filter is O(M) where M is the total number of k-mers in the read library. Thus, the total running time that includes the Bloom filter checks and the sorting and compression of the frequent items is OM  ON log N  N. Our measurements on the datasets used show that the total time is dominated by the Bloom filter updates [i.e. OM4ON log N  N].
cTurtleWhen there are so many frequent k-mers that keeping track of the k-mers and their counts explicitly is infeasible, we can obtain an approximate set of frequent k-mers by using a counting Bloom filter. Note that the number of bits required for a Bloom filter for n items is O(n), but the constants are small. For example, it may be shown that for a 1% falsepositive rate, the Bloom filter size is recommended to be $9.6n bits (). On the other hand, with a k-mer size of 32 and counter size of 1 byte, the memory required by a naive method that explicitly keeps track of the k-mers and their count is at least 9n bytes or 72n bits. With data compression techniques like prefix of k-mers being implied from the context of the data structure as in Jellyfish and KMC, this is 59n bytes but, from the memory comparison between Jellyfish and cTurtle presented in, we believe it still remains considerably higher than the 9.6n bits required by the Bloom filter. The basic idea of our counting Bloom filter is to set k bits in the Bloom filter when we see an item for the first time. When seen for the second time, the item is identified as a frequent k-mer and written to the disk. To record this writing, k 0 more bits are set in the Bloom filter. For all subsequent sightings of this item, we find the (k  k 0 ) bits set and know that this is a frequent k-mer that has already been recorded. For cache efficiency, we implement the counting Bloom filter as a pattern-blocked counting Bloom filter as follows. We take a larger Bloom filter (B) of size L. When an item x is seen, k values (h 1 , h 2 ,. .. , h k ) within the range hx, hx  b, where h(x) is a hash function and b is the block size, are computed using precomputed patterns. If this is the first appearance of x, with high probability, not all of the bits Bh 1 ,. .. , Bh k  are set to 1, and so we set all of them. When we see the same item again, we will find all of Bh 1 ,. .. , Bh k  set to 1. We then compute another set of locations (h k1 , h k2 ,. .. , h kk 0 ) within the range hx  b, hx  2b using precomputed patterns. Again, with high probability, not all of Bh k1 ,. .. , Bh kk 0  are set to 1, and so we set all of them. At the same time, we write this k-mer to the disk as a frequent k-mer. For all subsequent observations of this k-mer, we will find all of Bh 1 ,. .. , Bh kk 0  set to 1 and will avoid writing it to the disk. Note that a false-positive rate in the second stage means that we do not write the k-mer out to file and thus have a false-negative rate.Currently, cTurtle reports k-mers with frequency 41. But this strategy can be easily adopted to report k-mers of frequency greater than c41. We argue that for most libraries with reasonable uniform coverage, c  1 is sufficient. Let C be the average nucleotide coverage of a read library with read length R. Then, the average k-mer coverage is C k  CRk1 R (). Suppose we have an erroneous k-mer with one error. The probability that the same error will be reproduced is 1 3k where 1/k is the probability of choosing the same position, and 1/3 is the probability of making the same base call error. Therefore, the expected frequency of that erroneous k-mer is 1  Ck1 3k. For R  100 and k  31, this expression is 1  0.0075C. Therefore, we need C4132:85 at a location for an erroneous 31-mer to have a frequency42. Because most large libraries are sequenced at a much lower depth (560x), such high coverage is unlikely except for exactly repeated regions, and therefore our choice of frequency cutoff will provide a reasonable set of reliable kmers. However, this does not hold for single-cell libraries, which exhibit uneven coverage (). Note that frequent k-mers are considered reliable only for uniform coverage libraries, and thus singlecell libraries are excluded from our consideration. The parallelization strategy is the same as that for scTurtle.
COMPARISONS WITH K-MER COUNTERSThe datasets we use to benchmark our methods are presented in) is the fastest open-source k-mer counter. DSK () is also memory-efficient but is slow. Khmer () and BFCounter () use Bloom filter-based methods for reducing memory requirements. We have a similar strategy for memory reduction but achieve a much better computational efficiency. We decided not to report times for any tool that required 410 h of wall-clock time, and therefore some data are missing in. KMC was able to perform k-mer counting for all the datasets but was slower than Turtle for the larger datasets. Note that for the sake of comparison, we allowed KMC to use the same amount of memory that Turtle used, but it is capable of performing the computation with smaller amount of memory. Unexpectedly, on large datasets (ZM and HS), BFCounter required more memory than scTurtle (for 128 versus 109 GB). We suspect this is due to the memory overhead required to reduce collusions in the hash table used for storing frequent kmers, which we avoid using in our sort and compaction algorithm.claimed DSK to be faster than BFCounter, but on our machine, which had an 18 TB Raid-6 storage system of 2 TB SATA disks, it proved to be slower (1591 versus 1012 min for the GG dataset).reported performance using more efficient storage systems (solid-state disks). This might explain DSK's poor performance in our experiments. The detailed results are presented infor multithreaded Khmer, KMC, scTurtle and cTurtle. Because BFCounter (single threaded) and DSK (4 threads) do not allow a variable number of threads, we present their results separately in.Jellyfish's () performance was inconsistent on the AMD machine (details not shown). For example, while running with 17 worker threads, it started with a near-perfect central processing unit (CPU) utilization of $1700% but steadily declined to $100%, resulting in an average CPU utilization of only 290%. The computation required 16 h and 56 min of wall-clock time and 238 GB of memory. This is inconsistent with Jellyfish's performance on Intel machines reported by. Therefore, to compare our tools with Jellyfish, we ran additional experiments on the Intel machine.presents the wall-clock times for Jellyfish, KMC, scTurtle and cTurtle run with 19 worker threads on the Intel machine for all the datasets. OnNote: The input of a run is a single read file (FASTA or FASTQ format), and the output is a text file containing k-mers and their frequencies (FASTA or tab delimited format). The k-mer size is 31. Each tool was run six times with 19 worker threads, and the average was reported.this machine, Jellyfish's count step had a .5% for 19 worker threads. We found KMC to be the fastest tool for the small datasets (DM and GG), but for the two large datasets ZM and HS, Jellyfish and cTurtles, respectively, were the fastest. Jellyfish had the highest memory requirements for all datasets. To support our claim that the wall-clock time (and therefore parallelization) may be improved by speeding up the producer, we made special versions of scTurtle and cTurtle, which use 31 worker threads and a fast approximate modulus-31 function. For the largest library tested (HS), on average, the special version of scTurtle (counting only) produces frequent 31-mers in $73 min compared with $87 min by the regular version (a 19% speedup). As we use 64-bit integers for storing k-mers of length 32 and less and 128-bit integers for storing k-mers of length in the range 33 64, the memory requirement for larger k-mers was also investigated. Again, for the largest dataset tested (HS), we found that scTurtle's memory requirement increased from 109 GB for 05k 31 to 172 for 32 k 64 (a 58% increase). Note that the Turtles require less memory for up to 64-mers than Jellyfish for 31-mers. Detailed results of all the datasets for the Turtles are presented in. We also examined the error rates for our tools and BFCounter. Note that, just like BFCounter, scTurtle has falsepositive rates only, and cTurtle has both false-positive and falsenegative rates. We investigated these rates for the two small datasets (see) and found error rates for all tools to be 51%. For the large datasets, because of memory requirements, we could not get the exact counts for all k-mers and therefore could not compute these rates. The error rates also increase if the expected number of frequent k-mers is underestimated. As discussed in the introduction, for a genome of size g, we expect to observe approximately g frequent k-mers in the read library. To get an understanding of how the underestimation of g drives up the error rates, we tested the DM dataset with expected number of frequent k-mers to be g, 0.9g and 0.85g and found the false-positive rates to be 1.87, 1.99 and 2%, respectively. However, the true number of frequent k-mers in the DM library is % 1:07g. Therefore, we recommend setting this parameter to % 1:1g. Software versions, commands and parameters used for producing the results presented in this article are provided in Supplementary Materials.
CONCLUSIONIdentifying correct k-mers out of the k-mer spectrum of a read library is an important step in many methods in bioinformatics. Usually, this distinction is made by the frequency of the k-mers. Fast tools for counting k-mer frequencies exist, but for large read libraries, they may demand a significant amount of memory, which can make the problem computationally unsolvable on machines with moderate amounts of memory resource ( 128 GB or even with 256 GB for large datasets). Simple memory-efficient methods, on the other hand, can be timeconsuming. Unfortunately, there is no single tool that achieves a reasonable compromise between memory and time. Here we present a set of tools that make some compromises and simultaneously achieve memory and time requirements that are matching the current state-of-the-art in both aspects. With our first tool (scTurtle), we achieve memory efficiency by filtering k-mers of frequency one with a Bloom filter. Our pattern-blocked Bloom filter implementation is more time-efficient compared with a regular Bloom filter. We present a novel strategy based on sorting and compaction for storing frequent kmers and their counts. Because of its sequential memory access pattern, our algorithm is cache-efficient and achieves good running time. However, because of the Bloom filters, we incur a small false-positive rate. The second tool (cTurtle) is designed to be more memory-efficient at the cost of giving up the frequency values and allowing both false-positive and false-negative rates. The implementationNote: The tools ran with fast mod and 31 worker threads. Each reported number is an average of five runs.Note: For the large datasets, because of memory constraints, the exact counts for all k-mers could not be obtained, and therefore, these rates could not be computed. is based on a counting Bloom filter that keeps track of whether a k-mer was observed and whether it has been stored in external media. This tool does not report the frequency count of the kmers. Both tools allow a k-mer size of up to 64. They also allow the user to decide how much memory should be consumed. Of course, there is a minimum memory requirement for each dataset, and the amount of memory directly influences the running time and error rate. However, we believe, with the proper compromises, the approximate frequent k-mer extraction problem is now computationally feasible for large read libraries within reasonable wall-clock time using a moderate amount of memory.
The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Turtle at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
R.S.Roy et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
7 9 1 1 1 3 1 5 1 7 1 9 DM
Note: The input of a run is a single read file (FASTA or FASTQ format), and the output is a text file containing k-mers and their frequencies (FASTA or tab delimited format). Khmer does not provide a tool for dumping. Therefore, its run times are for counting only. Each reported number is an average of 5 runs. Runs requiring410 h were not reported. The k-mer size is 31. Recall that KMC, scTurtle and Khmer report k-mers and their counts, whereas cTurtle only reports the k-mers with count 41.
