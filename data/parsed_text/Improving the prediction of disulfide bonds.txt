Motivation: Disulfide bonds stabilize protein structures and play relevant roles in their functions. Their formation requires an oxidizing environment and their stability is consequently depending on the redox ambient potential, which may differ according to the subcellular compartment. Several methods are available to predict cysteine-bonding state and connectivity patterns. However, none of them takes into consideration the relevance of protein subcellular localization. Results: Here we develop DISLOCATE, a two-step method based on machine learning models for predicting both the bonding state and the connectivity patterns of cysteine residues in a protein chain. We find that the inclusion of protein subcellular localization improves the performance of these predictive steps by 3 and 2 percentage points, respectively. When compared with previously developed methods for predicting disulfide bonds from sequence, DISLOCATE improves the overall performance by more than 10 percentage points. Availability: The method and the dataset are available at the Web page
INTRODUCTIONThe formation of disulfide bonds between cysteine residues is essential for folding, stability and maturation of many proteins (). Predicting which cysteines in a protein sequence form disulfide bonds plays a relevant role in protein structural and functional annotation (). Several computational methods are available, which can be grouped as: (i) methods that predict the disulfide-bonding state (); (ii) methods that predict the connectivity patterns, assuming that the cysteine-bonding state is known (); (iii) methods that compute both features (). * To whom correspondence should be addressed.Proteins that contain disulfide bonds are rarely found in the cytoplasm and are routinely secreted (). Disulfide bond formation in Eukaryotes happens in the lumen of the endoplasmic reticulum (). These experimental studies show that the localization in the different cell compartments plays a relevant role in disulfide bond generation and stabilization. However, to the best of our knowledge none of the methods developed so far has actually exploited information on the subcellular localization of proteins. Protein datasets with experimentally known subcellular localization are available as well. Several efficient prediction methods were developed to predict subcellular localization (). Here, we propose DISLOCATE, a novel twostage method for disulfide bond prediction in Eukaryotes based on machine learning approaches. We show that the inclusion of protein subcellular localization improves the performance of disulfide bond prediction methods. This improvement is noticeable also when the subcellular localization is predicted with BaCelLo ().
MATERIAL AND METHODS
DatasetsFrom PDB (release May 2010), we extracted 1797 eukaryotic protein structures with resolution <2.5  with at least two cysteine residues and global pairwise sequence similarity <25%. We refer to this dataset as PDBCYS: it includes 7619 free and 3194 bonded cysteines. Since the selected proteins contained some measure of sequence similarity, we clustered the remaining chains using a local sequence similarity score. First, we ran a BLAST sequence search using all the proteins of the set versus themselves. Then, for each pair of proteins we selected the higher bidirectional (say p1 versus p2 or p2 versus p1) sequence identity as reported in the BLAST output. We subsequently treated the proteins as a node of a graph and assigned an edge between two nodes only where local sequence identity between the corresponding protein sequences was >25%. In addition, we computed the connected components of the graph and treated each group of nodes as a protein cluster. Finally, the clusters were grouped in 20 disjoint sets used to train and test the method. For sake of comparison, we also adopted the same procedure on the SPX-set ().
PDBCYS subcellular localizationFor each protein in PDBCYS, we extracted from the corresponding UniProt file the annotated subcellular localization, considering five different macro compartments: chloroplast, cytoplasm, mitochondrion, nucleus and secreted.The distribution of the different proteins in the various compartments is reported in. The 62% of the PDBCYS proteins (1121 chains including 4894 free and 1598 bonded cysteines) is endowed with subcellular localization. To predict subcellular localization, we adopted a cross-validation version of BaCelLo () also aiming at preventing an overestimation of the predictive contribution.
Protein structure and function
Predicting disulfide-bonding stateDisulfide-bonding state of cysteines is predicted with GrammaticalRestrained Hidden Conditional Random Fields (GRHCRFs). GRHRCFs have been recently introduced as a promising framework for solving sequence labeling tasks (). Here, for the sake of clarity, we introduce GRHCRFs starting from linear conditional random fields (CRFs). Linear CRFs can also be seen as discriminative versions of Hidden Markov Models (HMMs) and we will describe them applying this approach ().
From HMMs to CRFs. A HMMis defined by its transition (a s,t ) and emission (e s (c)) probabilities (). Given an observed sequence of symbols X ={x 1 ,...,x L } and a sequence of states Y ={y 1 ,...,y L }, the joint probability of X and Y can be computed by the HMM as:where the variable j runs over the length of the sequence X and an explicit begin state (y 0 ) is indicated by the index of position 0. Here, we adopt the convention of using uppercase letters for the entire sequences (X and Y ) and lowercase letters for the single elements (x i and y j ). The Equation (1) can be rewritten in an exponential form by introducing new variables:  s,t = log(a s,t ) and  s (c) = log(e s (c)):Taking into consideration Kroneker's deltas ((a,b) = 1 if a = b, 0 otherwise), p(Y ,X) can be rewritten as follows:where f s,t (y j1 ,y j ) = (s,y j1 )(t,y j ) and g s,c (y j ,x j ) = (s,y j )(c,x j ) are feature functions, respectively, defined over (state, state) and (state, symbol) pairs. A step forward to the CRF is to 'relax' the assumption that  s,t and  s (c) are log-probabilities, assigning them arbitrary values. However, in order to maintain the meaning of joint probability, a 'global' normalization factor (Z)is needed, such as:The notation is simplified by introducing the so-called potential functions  j (). In spite of the additional flexibility of  s,t and  s (c), it can be shown that p(Y ,X) describes exactly the HMM class (). Generative models as these model both the sequence of states Y and the observed sequence of symbols X. The last step toward linear CRFs is to write the conditional distribution p(Y |X) using the previous definition of p(Y ,X) as:where the normalization factor over all possible sequences of labels Y is usually referred to as a partition function Z(X). The discriminative nature of CRFs (as the conditional probability p(Y |X) is directly modeled) offers several advantages over generative approaches such as HMMs, including the relaxation of the strong independence assumptions implied in HMMs (). Finally, linear CRFs can further relax the definition of the functions making them dependent on the sequenceWith this notation, for instance, we can have feature functions that take into consideration a window around the j-th position or global sequence descriptors such as the subcellular localization.
From CRFs to GRHCRFs. Oneof the problems with linear CRFs is the fact that the set of observed labels {y j } coincides with the set of states {s j }. However, in order to identify biological meaningful predictions, the observed sequence of label Y can be considered as generated by an automaton with several different states, sharing the same type of label. For instance, the automaton presented indefines the simplest disulfide grammar with four states, but the observed sequences of labels contain only two symbols: free (F) and bonded (B). This makes the model cumbersome to treat because, in order to map the automaton into the observed sequence, a new artificial (and unambiguous) relabeling of the observed sequence has to be created (for instance, the sequence of observed labels Y = FBFB must be converted in Fe, Bo, Fo, Be). Any time that a new model is tested, a new artificial sequence of labels must be generated. Furthermore, grammatical rules such as forbidden transitions must be learned from the examples. This may lead to erroneous predictions if the rules are not sufficiently represented into the Page: 2226 22242230
C.Savojardo et al.training examples. Alternatively, the rules can be hard-coded in the source code (and have to be consequently adapted when the grammar changes) by setting the corresponding transitions to . To overcome these limitations, we introduced the GRHCRFs (). GRHCRFs decouples the observed sequence of labels from the set of states introducing a hidden set of variables. Like HMMs, GRHCRFs can be represented through a finite state machine (FSM) with some missing transitions between states. The structure of the FSM is determined by the specific grammar used for the problem at hand. In order to better generalize, GRHCRFs (as well as HMMs) define a one-to-many mapping between labels and FSM states and, at the same time, restrict the accepted predictions to only those that correspond to an allowed path in the FSM. A function (s) = y, is defined to map each state s to a given observed label y. The potential functions  j for each sequence position j are defined as:The  j are defined similarly to the CRF potential functions with the added constraints:that ensure that the only valid path of the FSM is considered. The probability of a sequence of labels Y given an observation sequence X is obtained as:where Z(Y ,X) and Z(X) are normalization factors defined as:that can be computed using the forwardbackward procedure ().} associated to each feature are learned by maximizing log-likelihood over training datawhere the last term is a Gaussian prior regularizer (). The maximization is carried out using the Limited memory BroydenFletcher GoldfarbShanno (L-BFGS) quasi-Newton optimization algorithm (). As far as expressiveness is concerned, linear CRFs and GRHCRFs are in many instances theoretically equivalent. However, the decoupling between states and observed labels allows the GRHCRFs to model ambiguous conditions: the sequence of observed labels can be associated with several different paths on the FSA. In these cases, GRHCRFs can exploit the ambiguity summing over all possible solutions and obtaining better performance (). However, in the case of the FSA of, both models are theoretically similar, and GRHCRFs collapses to 'constrained CRFs'. Nonetheless, GRHCRFs are simpler to deal with when grammar rules are introduced.
Bonding state prediction with GRHCRFs. Forthe bonding state prediction, we adopted the automaton described in. The arrows represent the allowed transitions, while the B and F circles, respectively, represent the bonding and non-bonding cysteine states. The labels 'e' (even) and 'o' (odd) indicate the number of cysteines in the bonding state so far processed. The path can end only from an e-label state. This guarantees that only correct even predictions are assigned when considering intra-chain disulfide bonds. To assign the bonding state, we encoded each cysteine with a 'local vector' representing the sequence nearest neighborhood. The vector is computed starting from the Position Specific Scoring Matrix (PSSM) as internally computed by PSI-BLAST using BLOSUM62 (). The input vector represents each cysteine in the protein sequence and its neighborhoods, by defining a window of size w = 2k +1 centered on each cysteine. The encoding vector consists in 20w components, where 20 is the number of residue types. We supplemented the local encoding (PSSM) with the piece of information provided by the subcellular localization (PSSM + SL) as obtained by the BaCelLo predictor in cross-validation. On a more practical level, for each cysteine in position j and for each state s, the GRHCRFs defines the following state features: @BULLET g s (x j+k) for k in {w/2,...,w/2} and a in {Residue Alphabet} and @BULLET g s (o) for o in {Global Features} where w is the width of the window around the cysteine of position j, and the set of global features can be 1 or 0 depending on the different types of subcellular localizations. The functions g s (x j+k) are weighted by the corresponding position (j +k) and residue type (a) values extracted from the PSSM.
Details on the employment of BaCelLo. The predictions provided by theBaCelLo have been integrated into the input vector of our method. As training dataset for BaCelLo, we employed its original training set described in. In order to make this procedure as fair as possible, we proceeded as follows: each protein in our dataset has been aligned using BLAST against the BaCelLo training set and the relevant hits with an e < 1e-3 for that protein have been identified. Then, the hits found have been removed from the BaCelLo training set and the predictor has been retrained on this reduced set. Finally, the protein subcellular localization has been predicted using the re-trained BaCelLo predictor. This guarantees that each protein has been processed with a training set that does not contain homologous proteins.
Predicting connectivity patternsOnce the cysteine-bonding state is assigned, we predict the connectivity pattern of the subsets of proteins that contain at least a pair of cysteines in the bonding state. The connectivity pattern is assigned by applying a support vector regression (SVR) approach (similarly to). The SVR predictions of each possible pair of cysteines is used as edge weight and the EdmondGabow algorithm is adopted to predict the most probable disulfide pattern (). In order to evaluate SVR, we use the same 20-fold cross-validation procedure described above, considering only proteins with at least two disulfide bridges. SVRs were trained using an input encoding based on global and local information. The global information (that does not depend on each particular cysteine pair) is defined by the Normalized Protein Length (one real value), the Protein Molecular Weight (one real value) and the protein amino acid composition (20 real values). The local pairwise encoding (that depends on each particular cysteine pair) consists of the following descriptors: @BULLET two PSSM-based windows centered into the cysteines forming the pairs. We used a window of length 13, the one that performed better among the several different-size windows we tested. With this choice, we ended up with a vector of 13202 = 520 components; @BULLET the relative order of the cysteines. This feature is encoded with 2 real values that represent the normalized relative order of a cysteines pair.
Page: 2227 22242230
Protein structure and functionGiven a protein with n cysteines (C 1 ,C 2 ,...,C n ), the corresponding normalized ordered list of cysteines is given by (1/n, 2/n, , n/n). For each pair of cysteines, the corresponding values are then taken from the list (e.g. the pair (C 1 ,C 4 ) is encoded as (1/n,4/n)); @BULLET the cysteine separation distance. This feature is encoded with 1 real value that represents the log-cysteine sequence separation computed as SEP(C i ,C j ) = log(|j i|) where i and j are sequence positions of cysteines Ci and Cj, respectively. Finally, we provided to the SVR an input vector of 545 components based on all features described above. For the SVR implementation, we used the libsvm package (http://www.csie.ntu.edu.tw/~cjlin/libsvm) with a RBF kernel.
Measuring scoring efficiencyHere Tp, Tn, Fp and Fn are, respectively, true positives, true negatives, false positives and false negatives with respect to the disulfide-bonding state class. The disulfide-bonding state predictions are evaluated using the following indices: @BULLET Q 2 or accuracy that evaluates the number of correctly predicted cysteines divided by the total number of cysteines:@BULLET Precision (Pr) of the disulfide-bonding state class that is the number of correctly predicted cysteines divided by the total number of predicted cysteines in the positive class:@BULLET Recall (Rc) of the disulfide-bonding state class is the number of correctly predicted cysteines divided by the total number of observed bonded cysteines:@BULLET F 1 , defined as the harmonic mean of Pr and Rc:@BULLET Matthews Correlation Coefficient (CC) defined as follows:@BULLET Q prot is the number of correctly predicted proteins N cp divided by the total number of proteins N p :When we score the connectivity pattern prediction, we also compute the following indices: @BULLET P b is the number of correctly predicted bonds N c divided by the total number of predicted bridges N p :@BULLET R b is the number of correctly predicted bonds N c divided by the number of observed bonds N b :@BULLET Q p is the number of correctly predicted disulfide patterns N pat divided over the total number of proteins N p :
RESULTS AND DISCUSSION
Model selection procedureBoth CRF and SVR depend on hyperparameters that need to be adjusted ( 2 in the regularization term of CRF,  and C in SVR). Furthermore, both in the bonding state prediction and in the disulfide connectivity prediction, part of the input is based on a window of flanking residues centered on the cysteines. The size of these windows needs to be set as well. All these parameters have been chosen by performing a crossvalidation procedure. The dataset was first divided into a number of balanced sets as described in Section 2.1. Using this data split, we selected the parameters by averaging the best values obtained for each training set (in many cases, they are the same). With this procedure, we ended up with the following training-based parameters:  2 = 0.05 and w = 15 for the CRF and  = 0.0625, C = 11.31 and w = 13 for the SVR.
Predicting the disulfide-bonding state: the role of subcellular localizationWe applied the GRHCRF method for the prediction of the disulfide-bonding states of cysteines with a 20-fold cross-validation procedure. The best results (with input window w = 15) are reported in. For sake of comparison (and using the same input encoding), we also trained CRF models based on the same FSA of, using artificial sequences of labels derived from a FSA parsing. However, in order to highlight the effect of the grammatical constraints, CRF models inwere not provided with hardcoded forbidden transitions ( st ) set to . The results show that the grammatical rules are not easy to be acquired from the examples, since the GRHCRF models outperform the CRF ones in. As mentioned above, in the simple case of the FSM of, GRHCRFs collapse to linear CRFs when both label and grammatical constraints are taken into account. From Table 2, it is also evident that subcellular localization plays a significant role in predicting the cysteine-bonding state. In particular, Q prot (accuracy per protein) and CC (Matthews correlation coefficient) scores indicate that information derived from the observed subcellular localization (OSL) increases up by five percentage points the method performance. The improvement is slightly lower when the predicted subcellular localization (PSL) is included in the input vector, indicating that BaCelLo is endowed with a high prediction score. It is worth mentioning that in PDBCYS the trivial cases are not present (chains that contain a single cysteine).Relative errors for each index in terms of percentage are reported within parenthesis. SVR description and index definition are provided in Section 2.#B, number of bonds. PSSM, input with PSSM. PSSM + PSL, input that add the predicted subcellular localization. Relative errors for each index in terms of percentage are reported within parenthesis. For index definition see Section 2.
Page: 2228 22242230
C.Savojardo et al.
Predicting connectivity patternsTo train and test the predictor of connectivity patterns based on SVR, we adopted the same 20-fold partition of the dataset after removing chains that contained less than two disulfide bonds per structure (). Here, we assume a perfect knowledge of the disulfide-bonding state of cysteines (). The SVR aims to predict the connectivity pattern that gets increasingly complex as the number of disulfide bonds increases (). The procedure does not restrict the prediction to the connectivity patterns that are present in the dataset and allows prediction of neverseen-before patterns (the restricted procedure can be implemented as well, improving the method performance, Singh 2008;).
DISLOCATE: the integrated predictor of cysteine bonds in proteins considering subcellular localizationThe prediction of subcellular localization, of cysteine-bonding states and of their topology, is then integrated into DISLOCATE, that takes a protein sequence as input. To evaluate DISLOCATE, both wrong disulfide state predictions and wrong connectivity assignments are taken into account when scoring the performance. Subcellular localization is considered as an input added feature. Values reported inare obtained with a cross-validation procedure and as a function of the number of known disulfide bonds in the protein chain. It is patent that information on subcellular localization, albeit predicted, increases DISLOCATE performance for proteins with up to four disulfide bridges.In Table 5, we report its accuracy as a function of the number of cysteines in the proteins (up to 10 cysteines), independently of the observed or predicted bonding state. Data show that when the predicted subcellular localization is added (PSL), the performance of the method increases. It is worth mentioning that when accuracy is scored as a function of the number of cysteines (), the vast majority of the protein sequences contain only free cysteines, resulting in higher Q p values if compared to the case that consider only proteins with disulfide bonds ().
Comparison with other methodsOur method exploits protein subcellular localization in Eukaryotes according to the computations resulting by a cross-validated version of BaCelLo (). However, for sake of comparison, we benchmarked DISLOCATE with other methods that were tested on SPX-(). This dataset comprises 51% of proteins from Prokaryotes (out of 2547 protein structures). Unfortunately, it is not possible to compare the performance of the two separate steps (disulfide-bonding state and connectivity pattern predictions) with the available methods on a single dataset, since in the literature this information is not reported. In particular, for the prediction of the connectivity patterns most of the approaches adopt the highly redundant SP39 dataset (), where the sequence homology (if not properly handled) can mask the real performance (Supplementary). With this aim, here we retrain DISLOCATE (with the same parameter selected for PDBCYS) using a 10-fold cross-validation procedure. These 10 subsets were selected in order to prevent sequences with local similarity >25% from being extracted from two different sets (the cross-validation folds are available on the DISLOCATE Web page). BaCelLo predictions are obtained using the cross-validation procedure described above (Section 2.3.4). DISLOCATE results are shown inwithout and with the added subcellular localization feature (first and second groups of columns, respectively). Our data are compared with the scoring indices of state-of-the-art predictors as derived from the literature (). The higher DISLOCATE overall accuracy is due to the fact that the vast Page: 2229 22242230
Protein structure and function
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
CONCLUSIONS In this article, we present a new two-step predictor of disulfide bonds based on a newly developed machine learning model (Fariselli et al., 2009) and taking protein sequence as input. We show that the inclusion of protein subcellular localization improves its performance, indicating that this piece of biological information is relevant for the classification of the bonding state of cysteine residues. We also show that the method matches up to with the available state-of-the-art predictors.
