Motivation: In biomedical research a growing number of platforms and technologies are used to measure diverse but related information, and the task of clustering a set of objects based on multiple sources of data arises in several applications. Most current approaches to multi-source clustering either independently determine a separate clustering for each data source or determine a single 'joint' clustering for all data sources. There is a need for more flexible approaches that simultaneously model the dependence and the heterogeneity of the data sources. Results: We propose an integrative statistical model that permits a separate clustering of the objects for each data source. These separate clusterings adhere loosely to an overall consensus clustering, and hence they are not independent. We describe a computationally scal-able Bayesian framework for simultaneous estimation of both the consensus clustering and the source-specific clusterings. We demonstrate that this flexible approach is more robust than joint clustering of all data sources, and is more powerful than clustering each data source independently. We present an application to subtype identification of breast cancer tumor samples using publicly available data from The Cancer Genome Atlas. Availability: R code with instructions and examples is available at
INTRODUCTION
MotivationSeveral fields of research now analyze multisource data (also called multimodal data), in which multiple heterogeneous datasets describe a common set of objects. Each dataset represents a distinct mode of measurement or domain. While the methodology described in this article is broadly applicable, our primary motivation is the integrated analysis of heterogeneous biomedical data. The diversity of platforms and technologies that are used to collect genomic data, in particular, is expanding rapidly. Often multiple types of genomic data, measuring various biological components, are collected for a common set of samples. For example, The Cancer Genome Atlas (TCGA) is a large-scale collaborative effort to collect and catalog data from several genomic technologies. The integrative analysis of data from these disparate sources provides a more comprehensive understanding of cancer genetics and molecular biology. Separate analyses of each data source may lack power and will not capture intersource associations. At the other extreme, a joint analysis that ignores the heterogeneity of the data may not capture important features that are specific to each data source. Exploratory methods that simultaneously model shared features and features that are specific to each data source have recently been developed as flexible alternatives (). The demand for such integrative methods motivates a dynamic area of statistics and bioinformatics. This article concerns integrative clustering. Clustering is a widely used exploratory tool to identify similar groups of objects (for example, clinically relevant disease subtypes). Hundreds of general algorithms to perform clustering have been proposed. However, our work is motivated by the need for an integrative clustering method that is computationally scalable and robust to the unique features of each data source. In Section 3.3, we apply our integrative clustering method to mRNA expression, DNA methylation, microRNA expression and proteomic data from TCGA for a common set of breast cancer tumor samples. These four data sources represent different but highly related and dependent biological components. Moreover, breast cancer tumors are recognized to have important distinctions that are present across several diverse genomic and molecular variables. A fully integrative clustering approach is necessary to effectively combine the discriminatory power of each data source.
Related workMost applications of clustering multisource data follow one of two general approaches:(1) Clustering of each data source separately, potentially followed by a post hoc integration of these separate clusterings.(2) Combining all data sources to determine a single 'joint' clustering.Under approach (1), the level of agreement between the separate clusterings may be measured by the adjusted Rand index () or a similar statistic. Furthermore, consensus clustering (also called ensemble clustering) can be used to determine an overall partition of the objects that agrees the most with the source-specific clusterings. Several objective *To whom correspondence should be addressed functions and algorithms to perform consensus clustering have been proposed [for a survey see. Most of these methods do not inherently model uncertainty, and statistical models assume that the separate clusterings are known in advance (). Consensus clustering is most commonly used to combine multiple clustering algorithms, or multiple realizations of the same clustering algorithm, on a single dataset. Consensus clustering has also been used to integrate multisource biomedical data (Cancer Genome Atlas Network, 2012). Such an approach is attractive in that it models source-specific features, yet still determines an overall clustering, which is often of practical interest. However, the two stage process of performing entirely separate clusterings followed by post hoc integration limits the power to identify and exploit shared structure (see Section 3.2 for an illustration of this phenomenon). Approach (2) effectively exploits shared structure, at the expense of failing to recognize features that are specific to each data source. Within a model-based statistical framework, one can find the clustering that maximizes a joint likelihood. Assuming that each source is conditionally independent given the clustering, the joint likelihood is the product of the likelihood functions for each data source. This approach is used byin the context of integrating gene expression and DNA methylation data. The iCluster method () performs clustering by first fitting a Gaussian latent factor model to the joint likelihood; clusters are then determined by K-means clustering of the factor scores. Rey and Roth (2012) propose a dependency-seeking model in which the goal is to find a clustering that accounts for associations across the data sources. More flexible methods allow for separate but dependent source clusterings. Dependent models have been used to simultaneously cluster gene expression and proteomic data (), gene expression and transcription factor binding data () and gene expression and copy number data (describe a more general dependence model for two or more data sources. Their approach, called Multiple Dataset Integration (MDI), uses a statistical framework to cluster each data source while simultaneously modeling the pairwise dependence between clusterings.use MDI to integrate gene expression, methylation, microRNA and copy number data for glioblastoma tumor samples from TCGA. The pairwise dependence model does not explicitly model adherence to an overall clustering, which is often of practical interest.
METHODS
Finite Dirichlet mixture modelsHere we briefly describe the finite Dirichlet mixture model for clustering a single dataset, with the purpose of laying the groundwork for the integrative model given in Section 2.2. Given data X n for N objects (n  1,. .. , N), the goal is to partition these objects into at most K clusters. Typically X n is a multidimensional vector, but we present the model in sufficient generality to allow for more complex data structures. Let fX n j define a probability model for X n given parameter(s). For example, f may be a Gaussian density defined by the mean and variance  , 2 . Each X n is drawn independently from a mixture distribution with K components, specified by the parameters 1 ,. .. , K. Let C n 2 f1,. .. , Kg represent the component corresponding to X n , and k be the probability that an arbitrary object belongs to cluster k: k  PC n  k: Then, the generative model is X n $ fj k  with probability k :Under a Bayesian framework, one can put a prior distribution on    1 ,. .. , K  and the parameter set    1 ,. .. , K . It is natural to use a Dirichlet prior distribution for . Standard computational methods such as Gibbs sampling can then be used to approximate the posterior distribution for ,  and C  C 1 ,. .. , C N . The Dirichlet prior is characterized by a K-dimensional concentration parameter of positive reals. Low prior concentration (for example, k 1) will allow some of the estimated k to be small, and therefore N objects may not represent all K clusters. Letting K ! 1 gives a Dirichlet process.
Integrative modelWe extend the Dirichlet mixture model to accommodate data from M sources X 1 ,. .. , X M. Each data source is available for a common set of N objects, where X mn represents data m for object n. Each data source requires a probability model f m X n j m  parametrized by m. Under the general framework presented here, each X m may have disparate structure. For example, X 1n may give an image where f 1 defines the spectral density for a Gaussian random field, while X 2n may give a categorical vector where f 2 defines a multivariate probability mass function. We assume there is a separate clustering of the objects for each data source, but that these adhere loosely to an overall clustering. Formally, each X mn n  1,. .. , N is drawn independently from a K-component mixture distribution specified by the parameters m1 ,. .. , mK. Let L mn 2 f1,. .. , Kg represent the component corresponding to X mn. Furthermore, let C n 2 f1,. .. , Kg represent the overall mixture component for object n. The source-specific clusterings L m  L m1 ,. .. , L mN  are dependent on the overall clustering C  C 1 ,. .. , C N :where m adjusts the dependence function. The data X m are independent of C conditional on the source-specific clustering L m. Hence, C serves only to unify L 1 ,. .. , L M. The conditional model isThroughout this article, we assume has the simple formwhere m 2  1 K , 1 controls the adherence of data source m to the overall clustering. More simply m is the probability that L mn  C n. So, if m  1, then L m  C. The m are estimated from the data together with C and L 1 ,. .. , L m. In practice we estimate each m separately, or assume that 1 . ..  M and hence each data source adheres equally to the overall clustering. The latter is favored when M  2 for identifiability reasons. More complex models that permit dependence of the m s are also potentially useful. Let k be the probability that an object belongs to the overall cluster k: k  PC n  k:We assume a Dirichlet() prior distribution for    1 ,. .. , K . The probability that an object belongs to a given source-specific cluster follows directly:Moreover, a simple application of Bayes rule gives the conditional distribution of C:where is defined as in (1). The number of possible clusters K is the same for L 1 ,. .. , L M and C. The link function naturally aligns the cluster labels, as cases in which the clusterings are not well aligned (a permutation of the labels would give better agreement) will have low posterior probability. The number of clusters that are actually represented may vary, and generally the sourcespecific clusterings L m will represent more clusters than C, rather than vice versa. This follows from Equation (2) and is illustrated in Section 2 of the Supplementary Material. Intuitively if object n is not allocated to any overall cluster in data source m (i.e. L mn = 2C), then X mn does not conform well to any overall pattern in the data.summarizes the mathematical notation used for the integrative model.
Marginal formsIntegrating over the overall clustering C gives the joint marginal distribution of L 1 ,. .. , L M :Under the assumption that 1 . ..  M the model simplifies:where t k is the number of clusters equal to k and U  K11 11 ! 1. This marginal form facilitates comparison with the MDI method for dependent clustering. In the MDI model ij 40 control the strength of association between the clusterings L i and L j :where ~ mk  PL mn  k. For K  2 and ~ 1  ~ 2 , it is straightforward to show that (4) and (5) are functionally equivalent under a parameter substitution (see Section 3 of the Supplementary Material). There is no such general equivalence between the models for K42 or M42, regardless of restrictions on ~  and . This is not surprising, as MDI gives a general model of pairwise dependence between clusterings rather than a model of adherence to an overall clustering.
EstimationHere we present a general Bayesian framework for estimation of the integrative clustering model. We use a Gibbs sampling procedure to approximate the posterior distribution for the parameters introduced in Section 2.2. The algorithm is general in that we do not assume any specific form for the f m and the parameters mk. We use conjugate prior distributions for m ,  and (if possible) mk ., the Betaa m , b m  distribution truncated below by 1 K. By default we choose a m  b m  1, so that the prior for m is uniformly distributed between 1 K and 1.  $ Dirichlet 0 . By default we choose 0  1, 1,. .. , 1, so that the prior for  is uniformly distributed on the standard M  1-simplex.The mk have prior distribution p m. In practice, one should choose p m so that sampling from the conditional posterior p m  mk jX m , L m  is feasible.Markov chain Monte Carlo (MCMC) proceeds by iteratively sampling from the following conditional posterior distributions:jC $ Dirichlet 0  , where k is the number of samples allocated to cluster k in C.This algorithm can be suitably modified under the assumption that 1 . ..  M (see Section 1.2 of the Supplementary Material). Each sampling iteration produces a different realization of the clusterings C, L 1 ,    , L m , and together these samples approximate the posterior distribution for the overall and source-specific clusterings. However, a point estimate may be desired for each of C, L 1 ,    , L m to facilitate interpretation of the clusters. In this respect, methods that aggregate over the MCMC iterations to produce a single clustering, such as that described in Dahl (2006), can be used. It is possible to derive a similar sampling procedure using only the marginal form for the source-specific clusterings given in Equation (3). However, the overall clustering C is also of interest in most applications. Furthermore, incorporating C into the algorithm can actually improve computational efficiency dramatically, especially if M is large. As presented, each MCMC iteration can be completed in O(MNK) operations. If the full joint marginal distribution of L 1 ,. .. , L M is used the computational burden increases exponentially with M (this presents a bottleneck for the MDI method). For each iteration, C n is determined randomly from a distribution that gives higher probability to clusters that are prevalent in fL 1n ,. .. , L mn g. In this sense, C is determined by a random consensus clustering of the source-specific clusterings. Hence, we refer to this approach as Bayesian consensus clustering (BCC). BCC differs from traditional consensus clustering in three key aspects.(1) Both the source-specific clusterings and the consensus clustering are modeled in a statistical way that allows for uncertainty in all parameters.(2) The source-specific clusterings and the consensus clustering are estimated simultaneously, rather than in two stages. This permitsWe have developed software for the R environment for statistical computing (R Development Core Team, 2012) to perform BCC on multivariate continuous data using a Normal-Gamma conjugate prior distribution for cluster-specific means and variances. Full computational details for this implementation are given in Section 1.1 of the Supplementary Material. This software is open source and may be modified for use with alternative likelihood models (e.g. for categorical or functional data).
Choice of KOne can infer the number of clusters in the model by specifying a large value for the maximum number of clusters K, for example K  N. The number of clusters realized in C and the L m may still be small. However, we find that this is not the case for high-dimensional structured data such as that used for the genomics application in Section 3.3. The model tends to select a large number of clusters even if the Dirichlet prior concentration parameters 0 are small. The number of clusters realized using a Dirichlet process increases with the sample size; hence, if the number of mixture component is indeed finite, the estimated number of clusters is inconsistent as N ! 1 (). This is undesirable for exploratory applications in which the goal is to identify a small number of interpretable clusters. Alternatively, we consider a heuristic approach that selects the value of K that gives maximum adherence to an overall clustering. For each K, the estimated adherence parameters m 2  1 K , 1 are mapped to the unit interval by the linear transformationWe then select the value of K that results in the highest mean adjusted adherenceThis approach will generally select a small number of clusters that reveal shared structure across the data sources.
RESULTS
Accuracy of ^We find that with reasonable signal the m can generally be estimated with accuracy and without substantial bias. To illustrate, we generate simulated datasets X 1 : 1  200 and X 2 : 1  200 as follows:(1) Let C define two clusters, where C n  1 for n 2 f1,. .. , 100g and C n  2 for n 2 f101,. .. , 200g.(2) Draw from a Uniform(0.5,1) distribution.(4) For m  1, 2, draw values X mn from a Normal(1.5,1) distribution if L mn  1 and from a Normal1:5, 1 distribution if L mn  2.We generate 100 realizations of the above simulation, and estimate the model via BCC for each realization. We assume 1  2 in our estimation and use a uniform prior; further computational details are given in Section 4 of the Supplementary Material.displays ^ , the best estimate for both 1 and 2 , versus the true for each realization. The point estimate displayed is the mean over MCMC draws, and we also display a 95% credible interval based on the 2.597.5 percentiles of the MCMC draws. The estimated ^ are generally close to the true , and the credible interval contains the true value in 91 of 100 simulations. See Section 4 of the Supplementary Material for a more detailed study, including a simulation illustrating the effect of the prior distribution on ^ .
Clustering accuracyTo illustrate the flexibility and advantages of BCC in terms of clustering accuracy, we generate simulated data sources X 1 and X 2 as in Section 3.1 but with Normal(1,1) and Normal(1,1) as our mixture distributions. Hence, the signal distinguishing the two clusters is weak enough so that there is substantial overlap within each simulated data source. We generate 100 simulations and compare the results for four model-based clustering approaches:(1) Separate clustering, in which a finite Dirichlet mixture model is used to determine a clustering separately for X 1 and X 2 .(2) Joint clustering, in which a finite Dirichlet mixture model is used to determine a single clustering for the concatenated data X 0(3) Dependent clustering, in which we model the pairwise dependence between each data source, in the spirit of MDI.(4) Bayesian consensus clustering.The full implementation details for each method are given in Section 5 of the Supplementary Material.. Estimated ^ versus true for 100 randomly generated simulations. For each simulation, the mean value ^ is shown with a 95% credible interval
Bayesian consensus clusteringWe consider the relative error for each model in terms of the average number of incorrect cluster assignments:where 1 is the indicator function. For joint clustering, the source clusters b L m are identical. For separate and dependent clustering, we determine an overall clustering by maximizing the posterior expected adjusted Rand index () of the source clusterings. The relative error for each clustering method with M  2 and M  3 sources is shown in. Smooth curves are fit to the results for each method using LOESS local regression () and display the relative clustering error for each method as a function of. Not surprisingly, joint clustering performs well for % 1 (perfect agreement) and separate clustering performs well when % 0:5 (no relationship). BCC and dependent clustering learn the level of cluster agreement, and hence serve as a flexible bridge between these two extremes. Dependent clustering does not perform as well with M  3 sources, as the pairwise dependence model does not assume an overall clustering and therefore has less power to learn the underlying structure for M42.
Application to genomic dataWe apply BCC to multisource genomic data on breast cancer tumor samples from TCGA. For a common set of 348 tumor samples, our full dataset includes RNA gene expression (GE) data for 645 genes. DNA methylation (ME) data for 574 probes. miRNA expression (miRNA) data for 423 miRNAs. Reverse phase protein array (RPPA) data for 171 proteins.These four data sources are measured on different platforms and represent different biological components. However, they all represent genomic data for the same sample set and it is reasonable to expect some shared structure. These data are publicly available from the TCGA Data Portal. See http://people.duke. edu/%7Eel113/software.html for R code to completely reproduce the following analysis, including instructions on how to download and process these data from the TCGA Data Portal. Breast cancer is a heterogeneous disease and is therefore a natural candidate for clustering. Previous studies have foundanywhere from 2 () to 10 () distinct clusters based on a variety of characteristics. In particular, 4 comprehensive sample subtypes were previously identified based on a multisource consensus clustering of the TCGA data (). These correspond closely to the well-known molecular subtypes Basal, Luminal A, Luminal B and HER2. These subtypes were shown to be clinically relevant, as they may be used for more targeted therapies and prognosis. We use the heuristic described in Section 2.5 to select the number of clusters for BCC, with intent to determine a clustering that is well-represented across the four genomic data sources. We select K  3 clusters, and posterior probability estimates were converted to hard clusterings via Dahl (2006) to facilitate comparison and visualization.shows a matching matrix comparing the overall clustering C with the comprehensive subtypes defined by TCGA, as well as summary data for the BCC clusters. The TCGA and BCC clusters show different structure but are not independent (P-value 50:01; Fisher's exact test). BCC cluster 1 corresponds to the Basal subtype, which is characterized by basal-like expression and a relatively poor clinical prognosis. BCC cluster 2 is primarily a subset of the Luminal A samples, which are genomically and clinically heterogeneous. DNA copy number alterations, in particular, are a source of diversity for Luminal A.). For comparison, those Luminal A samples that were not included in Cluster 2 had a substantially higher average FGA of 0:17 AE 0:02. Cluster 3 primarily includes those samples that are receptor (estrogen and/or progesterone) positive and have higher FGA. These results suggest that copy number variation may contribute to breast tumor heterogeneity across several genomic sources.provides a point-cloud view of each dataset given by a scatter plot of the first two principal components. The overall and source-specific cluster index is shown for each sample, as well as a point estimate and $95% credible interval for the adherence parameter. The GE data has by far the highest adherence to the overall clustering (  0:91); this makes biological sense, as RNA expression is thought to have a direct causal relationship with each of the other three data sources. The four data sources show different sample structure, and the sourcespecific clusters are more well-distinguished than the overall clusters in each plot. However, the overall clusters are clearly represented to some degree in all four plots. Hence, the flexible, yet integrative, approach of BCC seems justified for these data. Further details regarding the above analysis are given in Section 6 of the Supplementary Material. These include the prior specifications for the model, charts that illustrate mixing over the MCMC draws, a comparison of the source-specific clusterings L mn to source-specific subtypes defined by TCGA, clustering heatmaps for each data source and short-term survival curves for each overall cluster.
DISCUSSIONThis work was motivated by the perceived need for a general, flexible and computationally scalable approach to clustering multisource biomedical data. We propose BCC, which models both an overall clustering and a clustering specific to each data source. We view BCC as a form of consensus clustering, with advantages over traditional methods in terms of modeling uncertainty and the ability to borrow information across sources. The BCC model assumes a simple and general dependence between data sources. When an overall clustering is not sought, or when such a clustering does not make sense as an assumption, a more general model of cluster dependence (such as MDI) may be more appropriate. Furthermore, a context-specific approach may be necessary when more is known about the underlying dependence of the data. For example, Nguyen and Gelfand (2011) exploit functional covariance models for timecourse data to determine overall and time-specific clusters. Our implementation of BCC assumes the data are normally distributed with cluster-specific mean and variance parameters. It is straightforward to extend this approach to more complex clustering models. In particular, models that assume clusters exist on a sparse feature set () or allow for more general covariance structure () are growing in popularity. While we focus on multisource biomedical data, the applications of BCC are potentially widespread. In addition to multisource data, BCC may be used to compare clusterings from different statistical models for a single homogeneous dataset.Note: Summary data includes 5-year survival probabilities using the KaplanMeier estimator, with 95% confidence interval; mean fraction of the genome altered (FGA) using threshold T  0:5, with 95% confidence interval; receptor status for estrogen (ER), progesteron (PR) and human epidermal growth factor 2 (HER2); and copy number status for amplification at sites 8p11 and 8q23 and deletion at sites 5q13 and 16q23.
Bayesian consensus clusteringFunding: National Institute of Environmental Health Sciences (NIEHS) (R01-ES017436). Conflict of Interest: none declared.
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Bayesian consensus clustering at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
