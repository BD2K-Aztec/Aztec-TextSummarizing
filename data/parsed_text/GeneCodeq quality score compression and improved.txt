Motivation: The exponential reduction in cost of genome sequencing has resulted in a rapid growth of genomic data. Most of the entropy of short read data lies not in the sequence of read bases themselves but in their Quality Scoresâ€”the confidence measurement that each base has been sequenced correctly. Lossless compression methods are now close to their theoretical limits and hence there is a need for lossy methods that further reduce the complexity of these data without impacting downstream analyses. Results: We here propose GeneCodeq, a Bayesian method inspired by coding theory for adjusting quality scores to improve the compressibility of quality scores without adversely impacting geno-typing accuracy. Our model leverages a corpus of k-mers to reduce the entropy of the quality scores and thereby the compressibility of these data (in FASTQ or SAM/BAM/CRAM files), resulting in compression ratios that significantly exceeds those of other methods. Our approach can also be combined with existing lossy compression schemes to further reduce entropy and allows the user to specify a reference panel of expected sequence variations to improve the model accuracy. In addition to extensive empirical evaluation, we also derive novel theoretical insights that explain the empirical performance and pitfalls of corpus-based quality score compression schemes in general. Finally, we show that as a positive side effect of compression, the model can lead to improved genotyping accuracy. Availability and implementation: GeneCodeq is available at:
IntroductionOver the past decade, unprecedented advances in next generation sequencing (NGS) technologies have led to a dramatic reduction in sequencing cost () and much faster data production rates (). These technological advances are fostering increasingly wide-ranging applications in biotechnology, public healthcare () and personalized medicine (). Furthermore, as genomic sequencing data has grown exponentially, they have outpaced advances in some information technologies that they indirectly rely on: computing power and storage (). In particular, genome sequencing results in a large storage footprint for each genome. Thus, storing and transferring raw sequencing information is becoming prohibitively expensive () and effective compression schemes of raw sequencing data are indispensable.The majority of NGS data output consists of read sequence information whereby each nucleotide base is associated with a sequence confidence level (also referred to as quality score), produced by the base caller (). The Phred scale () Q  10log 10 P   encodes with integer Quality Score Q an estimate of the probability P that the base has been called incorrectly. This information is used both for the quality control of raw read data and for downstream processing, including genome assembly, read mapping and genotyping (). In compressed genomic datasets, quality score values take up the dominating share. For example, when compared to the read sequence data, quality scores of Illumina reads take at least 2.3 more storage, although this can be an even higher ratio when using more aggressive sequence compression (). Quality scores are more difficult to compress due to a larger alphabet (6394 in original form) and intrinsically have a higher entropy (). With lossless compression algorithms and entropy encoders reaching their theoretical limits and delivering only moderate compression ratios (), there is a growing interest to develop lossy compression schemes to improve compressibility further. Quantizing quality scores (i.e. reducing the alphabet size) is the most basic approach to improve compressibility in a lossy manner. One such approach of reducing all quality values to eight levels (bins) () has become a widely used standard for the Illumina platform and is enabled by default on the most recent machines (). Another approach called P) involves local quantization so that a representative quality score replaces a contiguous set of quality scores that are within a fixed distance of the representative score. Similarly the R) scheme replaces contiguous quality scores that are within a fixed relative distance of a representative score. Other lossy approaches improve compressibility and preserve higher fidelity by minimizing a distortion metric such as meansquared-error or L1-based errors (Qualcomp and QVZ) (). However, adoption of lossy compression schemes for quality scores has been slow due to concerns about adverse effects on downstream analyses, in particular genotyping accuracy (). However, there are also reports that compression schemes such as P-Block, R-Block, QVZ and Qualcomp can, under some circumstances, lead to a slight improvement in genotyping accuracy. A number of more recent approaches utilize the sequence data itself to guide the quality score compression. Quartz achieves this using a reference corpus built from frequent 32-mers across reads from individuals sequenced in the 1000 Genomes Project (). Read base pairs that match any one 32-mer in the corpus (up to one allowed mismatch per 32-mer) have their quality score set to a fixed high value. This 'sparsification' of quality scores reduces entropy, thus improving quality score compressibility. A different approach, Leon, () utilizes the dataset itself for building its set of k-mers, and to generate a reference probabilistic de Brujin Graph. In this case, bases in a read that have enough highly frequent k-mers covering it within the dataset are set to a fixed high-value quality score. Both methods were reported to improve genotyping accuracy. In this article we present GeneCodeq, a lossy compression scheme that is inspired by coding theory () and Bayesian inference. Uniquely, GeneCodeq uses a statistical approach to objectively reason about the compressibility of quality scores. Briefly, our model estimates the posterior probability of a sequencing error given the evidence of the full read, including quality scores, together with information from a reference corpus. As a result, the posterior estimates of most quality scores are boosted above a saturation point of the Phred scheme, corresponding to very high confidence. This approach results both in a significant reduction in entropy and better genotyping accuracy when compared to existing methods.
MethodsGeneCodeq uses a basic statistical model for mutations and sequencing errors within the Bayesian framework. These processes are not modelled in full detail. Instead, this simplified generative model lends itself to an effective algorithm to estimate the probability of a sequencing error (seefor an overview). During sequencing, the sample genome is fragmented and individual reads are randomly selected and sequenced. If the set of all possible fragments was known a priori, these could be regarded as codewords transmitted over a noisy channel (sequencing). Given an observed sequence read, standard principles from coding theory could be applied to infer the likelihoods for each codeword and thereby accurately estimate the probability of sequencing error for each base. In real sequencing, we do not have access to the set of true fragments. Instead, GeneCodeq uses the reference genome as a source of possible read fragments of a defined length k (source k-mers). The mismatch between the reference and the true sample genome is then modelled as an additional source of error in the transmission process. Importantly, this additional error probability tends to be low as most reads can be well explained by the reference corpus. Hence, a reference corpus is a suitable proxy for the true sequence and again principles from coding theory can be used to estimate the posterior probability of a sequencing error.(b). Illustration of our approach to model sequencing as a noisy transmission problem. (a) The source genome is observed. The source genome is uniformly randomly fragmented into a set of k-mers. A k-mer (sequence read) is selected at random for transmission over the noisy channel (the sequencer), resulting in an observed read and corresponding quality scores. The posterior probability of a sequencing error can be estimated for each base, given the read, quality scores and the set of possible source k-mers. (b) The source genome is not observed. The reference genome is used as a proxy for the sample genome, and split into a set of all possible k-mers (a corpus). The noisy channel now introduces both mutation and sequencing errors. The posterior probability of sequencing error can be calculated as in (a) but also taking into account the probability of mutation in the noisy channelWhen applying this approach to NGS, we consider both the sequence of the received read as well as the quality scores for each base. We use these additional data as additional evidence for the likelihood of a sequencing error. This estimate is then refined in the light of all the remaining evidence, which includes the sequence context of the entire read and its quality scores as well as the read k-mer corpus, which is derived from the reference.
Relationship to coding theoryThe model that underlies GeneCodeq is related to coding theory. In the general case of transmitting binary data over a noisy medium, forward Error Correction () can be used to correct multiple bit errors. The key idea is that outgoing data is encoded using a dictionary of carefully constructed binary strings called codewords, ideally accounting for the specifics of the noisy channel. When a particular codeword is transmitted, the original transmitted codeword can be recovered with high probability, even in the presence of bit errors. The key component to enable accurate decoding is to define a dictionary of error tolerant codewords. For example, if the Hamming distance is used to determine the likelihood that the signal on the other end corresponds to one codeword versus another, it is desirable to define a dictionary where individual codewords have maximal pairwise Hamming distances to each other (). In NGS we also have the objective to identity errors and recover the true source sequence. However, unlike applications in Coding Theory, the codewords are not constructed but instead defined by a corpus, derived from e.g. a reference or sample genome (). Nonetheless, we can use the analytical framework of Coding Theory to solve the decoding task. Although this approach could also be used to uncover the most likely true sequence, we are not interested in error correction. Instead, GeneCodeq uses the noisy channel model to estimate the likelihood of sequencing error and in particular to identify bases where errors are unlikely. Intuitively, the ability to reject a sequencing error is strongly affected by the Hamming Distance between the source k-mer and its Hamming neighbourhood in the dictionary. Those k-mers which have high Hamming Distance to all other k-mers are easier to identify, which in turn informs the probability of a sequence error.
GeneCodeq modelGeneCodeq assumes that an unknown Sample Genome (Z) is generated from a known Reference Genome (C) with per base mutation rate m. We use a simple mutational model that assumes that each mutation event is independent and identically distributed, with a value for m informed by GWAS studies (see Supplementary Materials for more details). It is well known that mutation rates vary considerably across the genome. More complex mutational models could be straightforwardly incorporated in the model, and this would be an area worthy of future exploration. Using the reference genome, we generate a corpus of N k-mers that forms a dictionary of codewords C  C 1 ;. .. ; C N  with alphabet size four (i.e. A, C, G or T), where C i;j denotes base j of the ith codeword. The corresponding unobserved sequences in the sample genome is denoted as Z i , where the base Z i;j differs from C i;j due to mutations. We purposefully model each read independently from other reads, which simplifies the overall model and is consistent with the notation of independent sampling of reads (or fragments) from the source genome. Hence, the model can be fully described when considering a single observed read. Let k denote the total number of bases in the observed read R with R j denoting the jth base. Additionally, we observe information about the sequence uncertainty for each base   1 ;. .. ; k , which are derived from the corresponding quality scores. Our objective is to infer the posterior probability of sequencing error at each base in order to then adjust the corresponding quality scores. To achieve this, we start with a model that assumes that we know which particular source codeword C w , for w 2 1;. .. ; N gave rise to the observed read R. We denote the presence of a sequence error with Boolean variable E j (i.e. where R j is not identical to the true sample sequence Z w;j ). In this model, we represent the mutational error as Pr Z w;j jC w;j ; m   and the error process of sequencing as Pr R j jZ w;j ; E j   . The joint probability of the observed read and the true uncorrupted sample sequence factorize, which reflects the assumption that mutations and sequence errors are independent:The prior probability of a sequencing error based on the quality score j is given by the Bernoulli distribution PrNote that while the distribution for mutational errors is identical and independent across base pairs, the sequencing error is independent across base pairs but not identically distributed, as the model accounts for base pair specific differences in quality scores j. The derivation in Equation (1) assumes that the true source codeword C w is known. In principal, one would need to marginalize over all possible codewords. In order to retain computational efficiency, we use a greedy search method to limit the space of codewords to be considered (see Section 2.3). For more details and illustration diagrams on this model, please consult the section on GeneCodeq Methodology in the Supplementary Material. Unlike Leon, our approach does not attempt to identify the true sequence variants Z w in the process. Although the integration of variant calling and compression of quality scores may have practical advantages, there is the risk of circularity when reproducing the compressed read data. When it comes to sequencing errors, we also assume the errors on one base are independent from other bases, given their observed quality scores. This assumption is accurate if the quality scores internally reflect the known biases of read error, which for some platforms such as Illumina tend to accumulate at reads ends. In principle, it would be straightforward to include more complex error models that account for the relative biases of specific sequencing platforms.
Statistical detailsWe here provide the core statistical elements. For a complete derivation, please consult the Supplementary Material. We again start by assuming that true source codeword C w for the read R was known. For brevity, we will denote this source codeword by S  C w. As mentioned, we are not interested in explicitly recovering the true sample genome Z w and hence, we collapse both the mutational error and the sequencing error process, resulting in the following conditional distribution, which factorizes across base pairs:Pr R j jS j   with a per base pair distributionPr R j jS j    Given a particular R j , j and S j , the probability of sequencing error E j follows from Equation (3) as (see derivation in Supplementary Material):Then since S is unknown, we can determine the probability across all possible S 2 C 1 ;. .. ; C N  by utilizing the rest of the read and sequencing error rates (i.e. quality scores):The posterior probability of the source codeword originating from C i then follows from Bayes theorem:It is worth noting that the posterior probability of assigning the read to a particular source codeword accounts for the full information within the read and does not factorize across base pairs. Assuming that reads can originate from any codeword in the corpus with equal probability, then Pr S  C i j; C    1 N. Please see the Supplementary Material for how this approach appropriately handles the case of duplicate k-mers/codewords, as are typically found in real genomes, by allowing duplicate codewords in the corpus. Expanding the denominator in Equation (6) and eliminating common terms, this yields:Combining Equation (5) with Equation (7) gives the posterior probability of sequencing error that can be calculated from all the knowns R, and C:With $3.2 billion codewords, this calculation is resource intensive when completed by brute force, making it rather impractical. However, an upper-bound estimate can be found much faster by recognizing that the contribution of reference codewords decreases exponentially according to their Hamming distance from the read. Let L be the set of local indices s.t. 8i 2 L; jC i  Rj < B 8i 6 2 L; jC i  Rj ! B for some Hamming Distance B (see Methodology in Supplementary Material). This means we separate codewords into local and background codewords according to distance, with the bulk of codewords being in the background set. Because the contribution of each background codeword is small, we can then try to estimate the contribution of background codewords without directly calculating it. Let b be an upper-bound estimate, such that:Let l denote the probability of a base change (whether due to mutation or sequencing). To obtain an upper-bound estimate b for the background contribution, we assume each of the background codewords is at distance B, which produces the worst-case contribution. The average probability of these codewords Pr RjS  C i  is then scaled to encompass all N codewords in the corpus, and not just a subset of background codewords, leading to a value that is typically a very large overestimate and thus conservative in practice (see Supplementary Material):The above assumes that l, the probability of a base change (whether due to mutation or sequencing), is constant for each base position. A more accurate calculation uses a per base probability l i where the B mismatches are uniformly distributed over the bases in the read. An approach based on dynamic programming is used to calculate a more accurate background contribution b, and this is what is used for the results presented with this work (see section Calculating the value of b in Supplementary Material). Therefore, an upper-bound estimate of each base's read error can be represented with:where / j j 3m   3j3m4mj b. Let P j denote the RHS of the inequality in Equation (10). This upper bound P j yields a lower-bound for the associated quality score for each base, which we encode using the Phred scheme:where Q j is the corresponding Phred quality score for upper bound error probability P j. This lower bound is dependent on the depth of the codeword search, where a deeper search can result in a higher value for the bound. In the limiting case, where the search covers all codewords, this bound becomes equal to the actual quality score for the posterior probability. In other words, the usage of the lower bound leads to a conservative approach. The posterior probability of a sequencing error is by construction biased upwards and hence the boosting of quality scores is conservative. In practice, this means that increased depth of the codeword search may lead to additional compressibility at the price of higher computational cost. However, the approximate nature of the algorithm will not falsely boost quality score values. Furthermore, the inequality in Equation (10) is only informative for replacing the observed quality score with a higher value, and is uninformative for replacing it with a lower value. Note that the use of Hamming-distance for the search means that the presence of an indel in a read may result in large Hamming distances from the reference corpus, yielding no results up to a reasonable search distance. In this case, GeneCodeq will be unable to calculate a better posterior probability of sequencing error, leaving the read untouched. In principle, the approach could potentially be extended by future work to use edit-distance instead, which enables the accounting of insertions and deletions from both mutations as well as sequencing errors. Further details of how such an approach might work is explored in the Edit Distance section of the Supplementary Material.
Codeword searchRather than calculate the contribution of every single possible codeword in the corpus, the strategy adopted is to search for all the closest codewords to the read, and use a conservative approximation for the likelihood of all the other remaining (background) codewords. Thus, unlike read-alignment against a reference, we are not reconstructing where the read can be positioned in the reference genome, but rather in the distribution of likelihoods for the highest likelihood matches to reference codewords. To find all codewords with up to M mismatches from the read, the read is divided into M  1 slots (see). Based on the Pigeonhole principle, for any codeword up to Hamming distance M away, there must be at least one slot which does not contain a mismatch (i.e. is an exact match). All slots are searched for all matching codewords. If a particular slot, for example, is a n-mer (where n < k), a search is made to find all codewords that contain that particular n-mer (this is the Slot LU operation in). The union of searches across the slots is then guaranteed to contain at least all those codewords within the desired Hamming distance M, however it can also contain codewords that are greater than this distance. Filtering is used to discard codewords that are greater than distance M, yielding the desired search results. The per-slot search can be done by indexing the reference sequence/corpus according to overlapping n-mers (e.g. if the corpus contains ACGGCTAC at some position 1004 then a 6-mer index for that would store position 1004 at index ACGGCT and position 1005 at index CGGCTA and position 1006 at index GGCTAC). For each slot, the set of possible matching codewords is then easily determined by looking up the index for match positions in the corpus. It can be noted that the larger the slot width, the more specific the slot search (Slot LU in), and the fewer the possible candidate codewords that need to be examined. A basic mechanism for conducting this search is to generate an index of all possible matches for a fixed slot width of 12 bases. Searching with a larger slot width, such as 15 can then be conducted by taking the intersection of the results of two overlapping searches at width 12. Smaller slot width searches would allow greater search distance, enabling further quality scores boosting, but at a computational cost. For further details of how the Codeword Search is implemented, please see Supplementary Material.
Implementation details and preprocessing
Read preprocessingReads are known to frequently have poor quality regions at the head or tail of the read. GeneCodeq operates on a trimmed version of the read. The preprocessing step trims the head and tail of the read for quality scores less than or equal to the threshold value (default of 10). This potentially shorter read is thus guaranteed to begin and end with a quality score above the threshold.
Search parametersA fixed slot width W lets one search a maximum distance b n W  1c for read length n. One strategy is to always search to this distance, however it can be faster to try searching with a lower distance first. A simple heuristic for balancing between speed and depth was used in GeneCodeq, which is described in detail in the Supplementary Material.
Quality score quantizationIf the lower-bound estimate of the quality score (i.e. boosted quality score), given by the posterior probability calculation, is higher than the original quality score, then any intermediate value between the two quality scores is also a better estimate of the probability of sequencing error than the original quality score is. Boosted quality scores may be improved to such an extent that they represent negligible error. Such quality scores can be constrained to a maximum saturation value S (e.g. Phred value 40). Those quality scores that are boosted from a value x to a non-saturation value y < S may instead of recording the value y, use a quantized value y 0  Q y   (e.g. based on the Illumina 8-bin quantization values) provided y 0 > x. This means that the resultant quality scores are conservatively quantized with the boosting, leading to higher compression ratios. For example, if a quality score is boosted from 32 to 43, instead of recording 43, we quantize to 40. In the results here we used the Illumina 8-bin quantization levels, so that whenever a quality score was improved enough to exceed a quantization level, it would be given the highest quantized score instead. This is consistent with our primary aim of improving the compressibility of quality scores, while preserving genotyping accuracy.
Analysis pipelineThe following steps comprise the analysis for each read: 1. trim head and tail of read to remove bases with low quality scores 2. conduct a neighbour search of codewords for the remaining read at a suitable Hamming distance 3. use Equation (10) to determine the posterior probabilities of sequencing error for each base 4. calculate a new quality score per base, quantizing based on Illumina 8-bin quantization levels 5. for each base, if the new quality score is better than original quality score, replace the quality score with the new quality score
Accounting for known sequence variants in the reference corpusWe can add support for variants on top of a reference corpus by allowing them to be matched directly in addition to the original reference codeword. For example, if a read has a mismatch against the closest codewords from the corpus and any of those mismatches are covered by known variants, then those variants are also temporarily k. Codeword search in GeneCodeq. A codeword search of up to Hamming Distance M first divides the read into M  1 slots, each of which are looked up in an index of the corpus (Slot LU) to find all matches to that slot. The results are merged into a union of candidate matches, and filtered according to distance to return only those up to distance M GeneCodeq: quality score compression in a Bayesian frameworktreated as part of the corpus for the purposes of that read and are added to the search result. Variant calling tools report many spurious variants (false positives) that may simply be the result of sequencing errors or other uncertainties/problems in the pipeline. To guard against these, we only include common variants that are present with a minimum variant frequency of h in the reference population, set to try to exclude 99.99% of spurious variants. See Supplementary Material for more details.
Note on indelsReads with indel variants (base insertion or deletion) that are not present in the corpus are likely to result in large Hamming distances to the rest of the corpus. This makes it unlikely for such reads to be affected by GeneCodeq, preserving their original quality scores. For future work, using Edit distance instead of Hamming distance as a metric thus presents an opportunity for further gains in compression. It could allow GeneCodeq to also improve genotyping accuracy and compression on sequence data that has a higher indel error rate such as those from Ion Torrent and Pacific Biotech sequencers. Further details of how such an approach might work is explored in the Supplementary Material.
Analysis of quartzThe Quartz approach uses a corpus of 32-mers in order to sparsify the quality scores of matching 32-mers within each read. Although there are similarities to GeneCodeq, Quartz is based on a heuristic approach and hence the assumptions made are less transparent. Using the same modelling framework as used to derive GeneCodeq, one can provide a better assessment of Quartz to see under what circumstances it can improve compressibility while preserving genotyping accuracy, and under what circumstances it can degrade genotyping accuracy and fail to improve compressibility. See Supplementary Material for more details.
ResultsWe've used sequences from NA12878 for evaluation purposes due to availability of high quality trio-validated SNP calls to validate genotyping accuracy. Unmapped raw reads were obtained for two datasets: @BULLET SRR622461, a 1000 Genomes Project (1000 Genomes) dataset with 18.3 gigabases at 5 coverage. @BULLET NA12878J, a public dataset from the Garvan Institute with 122.6 gigabases at 30 coverage (see Supplementary Materials for details). Resulting variant calls from genotyping were compared to the Illumina Platinum set (http://www.illumina.com/platinumgenomes), which served as a gold standard. In the Supplementary Material we also provide results for additional datasets. The raw FASTQ files were processed with GeneCodeq, Quartz (), R), P-Block (C), QVZ (), Qualcomp (), Leon () and Illumina 8-bin quantization (). We used the GATK best practices workflow and the samtools recommended workflow, with and without BaseQuality-Score-Recalibration (BQSR). In addition to running GeneCodeq in its default mode, we also explored its behaviour with a reference-only corpus (see Section 3.2 for more details), as well as in combination with other approachesIllumina 8-bin, P-Block and R-Block (see Section 3.4 for more details). For comparison, Quartz was also run with a reference-only corpus, and in combination with Illumina 8-bin quantization. We found that the BQSR stage produces a greater variable change to genotyping accuracy than most of the lossy compression algorithms do alone (pre-BQSR). This makes it difficult to separate and properly assess the impact of lossy compression alone on postBQSR results. On the lossless dataset, applying BQSR resulted in a significant drop in genotyping accuracy. Combinations of BQSR with lossy compression also resulted in lower genotyping accuracy versus the original across almost all methods. However, since BQSR had a large variable impact, sometimes the lossy compressed postBQSR results appeared better than the lossless post-BQSR results, skewing relative comparisons. There are strong indications that this is due to flaws with BQSR itself rather than due to the lossy compression. A detailed discussion, included additional results, can be found in the Supplementary Material. The improvements with GeneCodeq are seen across each combination of workflows, for brevity the main results shown here are with the GATK best practices workflow excluding BQSR. Similar results are seen with the samtools recommended workflow, which can be found in the Supplementary Material, along with BQSR results. Variant calls that resulted from the datasets and produced using these workflows, were ranked by their confidence levels (quality of the variant call) to generate receiver operating characteristic (ROC) curves. This approach avoids choosing a specific quality threshold and hence is well suited to compare alternative methods (see alsowhere a similar approach is used). However, if two ROC curves have different domains (i.e. a different set of variants for the x-axis) then they are not directly comparable to one another. For the results here and in the supplementary material, the ROC curves and area under the curve (AUC) were calculated with a common domain. As additional quality metrics, we also report precision, recall and F-score metrics. See Supplementary Material for more details on the evaluation approach for all these metrics. To estimate the impact in compressibility, both the raw and modified quality scores were compressed with bzip2 (http://www. bzip.org), unless a custom entropy coder was already integrated into the lossy compression (i.e. QVZ, Qualcomp and Leon). We chose bzip2 because under its default options it consistently yielded superior compression numbers compared to 7zip (http://www.7-zip.org) and gzip (http://www.gzip.org). We note, however, that 7zip in its PPMd mode can perform even better, and these results are included in the Supplementary Material. This includes significantly better 7zip re-compressed results for Leon which ordinarily uses the poorer performing zlib for its internal compression.reports quality score compression rates and genotyping accuracy metrics for these approaches.shows a scatter plot summarizing the best results in, depicting the genotyping accuracy and compressibility.
Non-corpus based approachesThe non-corpus based approaches with Illumina 8-bin, P-Block, RBlock, QVZ and Qualcomp consistently resulted in poorer genotyping performance compared to the raw data. For each of the parameterizable approaches P-Block, R-Block, QVZ and Qualcomp, we observed that at higher compression ratios, the genotyping AUC and F-scores degraded, demonstrating a tradeoff between compression and genotyping accuracy. With these non-corpus approaches, we note that AUC is typically lower than when using the raw uncompressed data, whereas other metrics such as Precision for Illumina 8-bin, P-Block, R-Block and Recall for QVZ and Qualcomp attest a marginal improvement. This suggests that these approaches are at a different tradeoff point between Precision and Recall along the curve, rather than resulting in an actual improvement. This supports using AUC and F-score, that account for these tradeoffs, as the main comparison metrics.
Impact of the reference corpus on quartz and GeneCodeqAn important experimental consideration is the specific reference corpus that is used for evaluation. For example, Quartz utilizes a corpus that is built from reads extracted across samples from the 1000 Genomes Project. It implicitly includes multiple known variants within that corpus that a variant calling pipeline may not be aware of. This extra information can directly improve the actual genotyping accuracy. It is not so surprising that a particular variant has improved calling characteristics when that variant is found to be present in its corpus and its sequencing uncertainty is consequently reduced. In order to assess the impact of additional variant information on genotyping accuracy and compression, we generated a new corpus for Quartz that excludes all variant information. This was built from the set of all 32-mers in the Human Reference Genome (from valid human chromosomes in hs37d5 (Reference Assemblies at https://browser.cghub.ucsc.edu/help/assemblies/). The results can then be compared to GeneCodeq when the model is also run with a corpus derived from the same Human Reference Genome, excluding any variant data. As can be seen inthere is a minor improvement in the ROC AUC for GeneCodeq (ref) compared with the raw sequence data (AUC 0.7784 versus 0.7782), whereas Quartz (ref) resulted in reduced genotyping accuracy (AUC 0.7764 versus 0.7782). The corresponding ROC curves can be found in Supplementary ma terial. Both approaches result in almost identical improvements in Fscore. Thus given the same corpus information, GeneCodeq appears to perform slightly better than Quartz in genotyping accuracy. GeneCodeq also performed a factor of 1.16 better in compressibility than Quartz ().
Impact of known variants on quartz and GeneCodeqTo explore the benefits of incorporating known sequence variants, we built a corpus of variants from the 1000 Genomes Project (see. Performance evaluation for a selection of methods applied to the SRR622461 benchmark dataset. Scatter plots show the tradeoffs between genotyping accuracy and compression ratio. To represent genotyping accuracy, the left plot considers ROC AUC whereas the right plot shows F-score (using the Illumina Platinum set of variants of NA12878 as a gold standard). The horizontal dotted lines correspond to the genotyping accuracy of the raw dataset, with methods above this threshold improving genotyping accuracy. Arrows correspond to the result of applying GeneCodeq compression after a particular version of the dataset (raw original dataset or 8-bin Illumina quantized). Notably, while the quantized versions of the dataset results in reduced genotyping accuracy, this loss is recovered when applying GeneCodeq, both when considering AUC or F-score metrics. Note PB2, PB4 and RB115 denote P-Block at p  2, at p  4, and R-Block at t  1.15 respectively. When comparing compression ratios, AUC and F-score, higher values are better. For full results, please seeAUC is the area under the curve of a ROC plot. The lowest bits/QS is highlighted. Where genotyping metrics are preserved or improved compared to the original, they are highlighted. Where both AUC and F-score genotyping metrics are improved, the approach is also highlighted. Note * denotes custom compression, '(ref)' denotes the use of a reference corpus with no variants. GC denotes GeneCodeq. The 8-bin denotes Illumina 8-binning. Qualcomp was run with one cluster. Genotyping was performed using the Broad Institute recommended pipeline excluding BQSR. Higher AUC, Precision, Recall and F-score measures are better. Lower bits/QS are better.
GeneCodeq: quality score compression in a Bayesian frameworkSupplementary Material for details) and present the comparison here. Generally, we observed that both Quartz and GeneCodeq improved genotyping accuracy compared to the raw data when including sequencing variants (). Additionally, we observed that GeneCodeq exhibits a larger improvement when compared to Quartz. When variants are included as part of the corpus, we see that Quartz and GeneCodeq both improved in their AUC, and marginally improved in F-score compared to the reference-only corpus. The change in AUC in both cases is significantly more than when applying the reference-only corpus. This suggests that for both approaches, Quartz and GeneCodeq, the main factor that contributes to improved genotyping accuracy is information about known sequence variants. With this dataset, only the Quartz and GeneCodeq approaches result in better genotyping accuracy, and of these GeneCodeq has both superior compression and genotyping accuracy. Looking just at the compression numbers overall, we see that compression for GeneCodeq is a factor of 1.24 better than for Quartz.
Combining GeneCodeq with non-corpus lossy compressionSince GeneCodeq leaves some quality scores untouched, the bulk of the remaining entropy lies in those bases or reads where there is insufficient evidence to saturate them, and indeed some reads are completely untouched as a result. This means that the principle of GeneCodeq is complementary to most (non-corpus based) lossy compression schemes, suggesting that combinations of different schemes may offer further improvements. When applying GeneCodeq after Illumina 8-binning (il8b  GeneCodeq), we observe a markedly low entropy of 0.163 bits per quality score under bzip2 while retaining a higher AUC and F-score compared to the raw dataset. We also explored combining Quartz with Illumina 8-binning, which although improved Quartz compression, degraded both AUC and F-score metrics to be worse than that of the raw sequence data. When combining GeneCodeq with the P-Block scheme, we see that improvements in AUC and F-score can be retained at even higher compression ratios, with compression of 10:1 for the combination of P-Block with p  4 followed by GeneCodeq. This combination is a factor of 2.6 better in bits/QS compared to Quartz while still having superior AUC, F-score, Precision and Recall values compared to the raw dataset.
Full sequence data compressionIt is instructive to see how well the full sequence data (i.e. both sequence data and quality scores) is compressed using standard sequence compression approaches as a result of this reduction in quality-score entropy. Taking the datasets referred above, and looking at file sizes of standard compression formats for sequence data (namely gzip, BAM and CRAM) we can explore the impact of GeneCodeq on these complete datasets.shows file sizes for these different approaches, and we observe that the combination of CRAM with GeneCodeq is a factor of 3.64.6 times smaller than the gzip-compressed FASTQ files of the raw dataset, and under half the size of the CRAM file for the raw dataset. The combination of GeneCodeq with P-Block demonstrates how further size reduction can be realized.
Throughput and memory consumptionSingle-threaded throughput on an IntelV R i7-4790K CPU was 5.57MiB/s with up to 24GiB of memory consumption on a sample human FASTQ file. Since reads are processed independently, the overall throughput can be trivially improved by using a multithreaded implementation of the GeneCodeq algorithm without further memory consumption. For further details please consult the Throughput and Memory consumption section of the Supplementary Material.We see significant compression benefits when applying GeneCodeq for gzip-compressed FASTQ, BAM and CRAM formats. Note that the BAM and CRAM formats also include some additional tags from alignment and indel realignment. and discussions on variant calling pipelines, and Yun William Yu for his help with reproducing results from Quartz.
FundingThis research was conducted under a R&D Smart Grant from Innovate UK. Conflict of Interest: Daniel Greenfield and Alban Rrustemi were employees of Fonleap Ltd during this research.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
m   1  j    1 3 m j if R j  S j 1 3 1  m   j  1 3 m 1  j    2 9 m j if R j 6  S j : 8 > > < > > : (3) GeneCodeq: quality score compression in a Bayesian framework
Discussion We have described a new method for adjusting base quality scores, which is inspired by coding theory. Our approach leverages available evidence under a Bayesian statistical model to infer posterior estimates of sequencing errors using a known reference corpus and a noisy channel transmission model. This approach requires a suitable corpus either derived from a reference or existing sample(s) in order to apply the statistical model. Given such a corpus, we find that this approach yields significantly higher compression and improves overall genotyping accuracy when compared to previous corpus-based compression methods such as Quartz, as well as to all other existing lossy compression approaches. Indeed when combined with the PBlock lossy compression approach, for the SRR622461 dataset we see quality score compression ratios of 10:1 whilst preserving the genotyping accuracy of the raw dataset. Other combinations with GeneCodeq may also be interesting to explore in future. Notably, even on datasets that have been Illumina 8-bin quantized (also see Supplementary Material), we find that applying GeneCodeq can result in improved AUC, Precision and F-score results indicating better genotyping accuracy compared to the raw dataset. This means that GeneCodeq is well suited to process and possibly enhance sequencing datasets generated by machines such as Illumina's HiSeq X that use this quantization by default. However, we note that, unsurprisingly, the largest gains in genotyping accuracy arise as a result of utilizing a corpus of known variants extracted from the 1000 Genomes Project (carefully excluding the test samples from the corpus set). When used with a corpus containing only reference genome information without variants, we note that genotyping accuracy may still be improved, albeit only slightly, compared to the raw dataset. This means that while the approach preserves genotyping accuracy for variants not in the corpus (such as rare variants), it works significantly better when it knows about potential variants. The Supplementary Material includes results from other datasets and genotyping pipelines also demonstrating considerable gains in compression whilst preserving genotyping accuracy with GeneCodeq.
