Motivation: Transmembrane β-barrels (TMBBs) are extremely important proteins that play key roles in several cell functions. They cross the lipid bilayer with β-barrel structures. TMBBs are presently found in the outer membranes of Gram-negative bacteria and of mitochondria and chloroplasts. Loop exposure outside the bacterial cell membranes makes TMBBs important targets for vaccine or drug therapies. In genomes, they are not highly represented and are difficult to identify with experimental approaches. Several computational methods have been developed to discriminate TMBBs from other types of proteins. However, the best performing approaches have a high fraction of false positive predictions. Results: In this article, we introduce a new machine learning approach for TMBB detection based on N-to-1 Extreme Learning Machines that significantly outperforms previous methods achieving a Matthews correlation coefficient of 0.82, a probability of correct prediction of 0.92 and a sensitivity of 0.73. Availability: The method and the cross-validation sets are available at the web page
INTRODUCTIONTransmembrane -barrel (TMBB) proteins cross the lipid bilayer with a series of -strands arranged in a cylindrical geometry and forming a structure that resembles a barrel (). TMBBs can be membrane anchors or membrane-bound enzymes endowed with functions relevant to the entire cell metabolism and including active ion transport, passive nutrient intake and defense against attack proteins (). While all living organisms have transmembrane proteins organized into all -helical bundles, TMBBs are presently found in the outer membranes of Gram-negative bacteria, mitochondria and chloroplasts. TMBBs are estimated to be encoded by as many as 23% of the genes in Gramnegative bacteria (). However, very few TMBB structures are available at atomic resolution from Gram-negative organismsSeveral computational methods have been developed to predict TMBBs from protein sequences. Two prediction problems can be addressed: (i) the prediction of protein topology (both strand localization along the protein sequence and orientation of the loops with respect to the membrane plane); and (ii) TMBB detection in genomes. When the interest focuses on the prediction of the protein topology, it has been shown () that the best performing methods are based on hidden Markov models () or Grammatical Restrained Hidden Conditional Random Fields (). The detection of TMMBs in a set of proteins, as large as the whole genome, is more difficult due to the cryptic nature of the TMBB structure as compared with that of the all -membrane proteins (). Computational methods for TMBB detection can identify candidate genes in order to perform experimental validations. To address this task, a wide variety of algorithms has been developed including: approaches based on sequence homology detection (), machine learning () and physicochemical properties of TMBBs (). In this article, we tackle the problem of TMBB detection and significantly improve over existing algorithms taking advantage of previous works. In particular, we exploit the effort made byin generating a reliable dataset and defining a thorough comparison with previous methods. Furthermore, we refine the approach of Pollastri and co-workers () by incorporating their N-to-1 method in an Extreme Learning Machine (ELM) framework ().
N-TO-1 ELMS
N-to-1 neural networksRecently, a new formulation of single hidden layer feedforward neural networks (SLFNs) aimed at encoding an entire protein sequence into a single object has been described by Pollastri and co-workers (). The basic idea is to encode in a single hidden layer of a neural network the piecewise information defined by all the segments generated by a sliding window over the protein sequence. In a more formal way, given a protein sequence of length N and a non-linear sigmoid activation function , it is possible to map the entire input sequence of length N into the single hidden layer vector H by 'enrolling' an input sliding window
C.Savojardo et al.protein residue) as:where H h is the h-th element of the hidden neuron vector H, W h is the weight vector that connects the j-th input window X j with the h-th hidden neuron and the brackets '<>' define the dot product of the two vectors (here we implicitly include in the weight matrix W also the bias thresholds of the hidden neurons). For sake of simplicity we set k equal to 1/N. With this choice H represents the average hidden layer of the entire sequence of length N and it becomes independent of the sequence length. For each protein of length N, the hidden layer encodes the information of N input vectors ( X j ), obtained by sliding a symmetric window of an odd length (L = 2n+1) along the protein sequence, one residue at a time. The encoding vector X j consists of 20 * L components, where 20 is the number of residue types. X j is computed starting from the position-specific score matrix (PSSM) as internally computed by PSI-BLAST (). When the n most terminal residues (N-and C-termini) are encoded, the empty positions are set with zero values. For each protein we generate a PSSM by aligning its sequence against the Uniref90 dataset (www.uniprot.org/help/uniref). The final network output (used to assign the prediction) for a given protein is obtained by considering H as a normal hidden layer:where  is the weight matrix that connects the hidden layer H with the neural network output vector O. Equations (1)and(2) allow to encode N different input windows into a single hidden layer, making it possible to treat a protein sequence as a single object (and independently of its length). However, differently from Pollastri and co-workers (), we take advantage of an ELM approach to set the network junctions ().
ELMsThe main idea of our approach is to couple the N-to-1 encoding with an ELM approach (). One interesting theoretical point about neural networks with ELM is the fact that differently from many other learning methods their universal approximation capability has been proved (). In the ELM context, the first layer of weights (W ) is set with random values. Only the second layer () is linearly trained. The first layer generates a non-linear 'random' encoding of the entire protein by mapping in a feature space for each sliding window. Once the N-to-1 random-enrolling is generated, the basic ELM machinery is used to train the  learnable parameters. Differently from what commonly thought, the input weights of SLFNs do not need to be learned (). Since in ELM the hidden layer does not need to be tuned and the hidden layer parameters can be fixed, the output weights can then be resolved using a least-square method. In practice, taking advantage of the fact that only the output layer () is trainable, the non-linearity of Equation (2) can be removed and  values can be obtained by the fitting the targets as:Page: 3125 31233128
Improving the detection of TMBB chainsSince the set contains a significant amount of sequence similarity, we clustered chains by local similarity (computed with BLAST) and we created 10 different subsets that internally confine sequence homology. By this sequence identity between pairs of proteins belonging to two different subsets is <25% (this is both for TMBB and non-TMBB proteins).
Measuring the performanceHere TP, TN, FP and FN are, respectively, the true positives, the true negatives, the false positives and the false negatives with respect to the TMBB class. Performances are evaluated adopting the following scoring indexes: @BULLET Accuracy (AC) evaluates the number of correctly predicted TMBB proteins divided by the total number of proteins:@BULLET F1is defined as the harmonic mean of PPV and SN:@BULLET The Matthews correlation coefficient (MCC) is:
Model selection procedureELM models depend on some hyper-parameters (i.e. random initial weights, number of hidden neurons, the input window) that can influence the method performance and need to be optimized. For this and in order to avoid over-fitting, we carried out a 10-fold cross-validation procedure on the training set, based on a distinction among validation and test sets. In particular, given the 10 non-homologous subsets generated as described above from NRPDB, we trained on the 10 different learning sets as follows. Eight subsets at a time are adopted for training and the remaining two are used for validation and testing. Furthermore, in order to cope with the stochastic nature of ELMs, for eachwindow/hidden-neuron pair, we generated five random sets of weights (W ). Scoring values for model selection are obtained by averaging on the 10 different 'validation' subsets. More formally, if T i is the i-th test set (i = 0 ... 9), the corresponding validation set V i and training set L i are defined as V i = T (i+1) mod 10 and L i ={T (i+2) mod 10 ,T (i+3) mod 10 , ...T (i+9) mod 10 }, where (k)mod j is the modulo operation that computes the remainder of division of k by j. For the ensemble selection we adopted an exhaustive procedure based on MCC scores computed on the validation sets. First, we selected the N-to-1 ELM models whose MCC scored 0.77. Then for each combination of these models, we defined an 'ensemble' by averaging their predictions. Finally, the best performing ensemble on the validation sets was retained. The final performance was assessed on the test sets without any further hyper parameter tuning.
RESULTS AND DISCUSSION
Performance of the ELM-based predictorThe aim of this article is to generate a reliable method for finding new TMBB proteins that do not share sequence similarity with those of our training sets. In order to properly train the method, it is necessary that sequence similarity between training and testing proteins is kept at a minimum (<25%). For this reason, we split NRPDB in 10 different subsets so that any pair of sequences selected from two different subsets does not have a local sequence similarity 25%. We performed a cross-validation procedure on the 10 different subsets distinguishing among validation and test sets as described in Section 3.3. Different ELM models, differing from each other in the number of neurons of the hidden layer and in the dimension of the sliding window, are evaluated. We report the MCC scores for each tested model on the validation subsets in. Each MCC score is obtained averaging over five different random initializations of the first layer of weights (W ). It appears that MCC values are >0.70 for nearly any input window in the range of 8001600 hidden neurons. In ELM, the first layer of weights is randomly set with values that are independent of the training examples (so that in our case the Page: 3126 31233128number of hidden neurons is the number of parameters to set). For sake of comparison, a classical network with an input window of seven residues (and the same encoding of ELM) supplied with only 20 neurons in the hidden layers needs to train >2800 parameters. The number of training examples in our application is one order of magnitude higher than the number of optimal hidden neurons, ensuring that training can be properly accomplished. Moreover, the minimal norm associated to the ELM training (see Section 2.2) strongly reduces the over-fitting problems. Inwe also list (within parentheses) values of MCC standard deviations (SDs) for all the window/hidden neuron combinations. The small variations around the average obtained indicate that the method is quite robust and insensitive to the random initialization. Notably, this is true also for a single residue long sliding window, indicating that N-to-1 ELMs are able to detect significant differences between TMBBs and other proteins even encoding a single residue at a time in the hidden layer. Among the four models that obtained an average MCC score 0.77 in, we computed all their possible ensembles as:
C.Savojardo et al.where O x is the output of the x-th selected model. The models for the ensemble were selected on the validation sets, using the criterion described above (see Section 3.3). After an exhaustive search we ended up with an ensemble of two models that on the validation sets achieved an MCC score of 0.83. The two models singled out for the ensemble have both 1600 neurons in the hidden layer and input windows of 7 and 11 residues, respectively. The selected ensemble (ELME), together with its constituent models is then evaluated on the test sets (). Although scores obtained on the test sets are slightly lower than those computed using the validation sets (comparewith), ELME performances are still very high achieving an MCC score of 0.82 with SD equal to 0.02 ().
Comparison with previous methodsFreeman and Wimley (2010) thoroughly compare performances of their algorithms with available methods (also based on machine learning approaches) on their newly generated dataset (NRPDB comprising 14 238 proteins, of which 48 are TMBBs). Thanks to their effort, here we can compare our results with their best methods under the same condition: we set a predictive threshold so that 46 or 37 of 48 true TMBBs are considered positive predictions. Inwe compare our ELME with the two best algorithms obtained by Freeman and Wimley (2010). In particular, they defineda new algorithm (FW in) including several improvements over a previously published implementation () and a filtered version based on a mean randomized score (MRS in). Scores infor FW and MRS were taken from Freeman and Wimley (2010). In that paper, FM and MRS performances were computed using some of the TMBB proteins included in the test sets to fit the main algorithm parameters (TMBB amino acid abundance values). Although NRPDB includes sequences that were used to generate abundance values for the FW algorithm, the authors explained that this was not a problem since their statistical approach was not readily subject to over-fitting due to the nature of the method and to the low number of parameters of the FW algorithm (120) (). Machine learning approaches, however, need cross-validation to prove the robustness of the learning process and for this reason we adopt a 10-fold crossvalidation procedure based on a distinction among validation and testing sets. Results reported infor ELME were obtained using the 10-fold cross-validation procedure by averaging the scores of the 10 predictions (two models with five different initial random weights) obtained for each protein on its test sets and by adopting a predictive threshold to match the true positive values of FW and MRS. ELME outperforms both methods () when the number of true positives is the same. In either case with ELME the number of false positives is drastically reduced and the PPV and MCC values are significantly improved. The performance of ELME can be also evaluated in comparison with other available methods under the conditions set by, namely after matching the sensitivity values of the different algorithms based on machine learning (kNN,) and pattern recognition (BOMP,) (). Also in this benchmark, ELME achieves the highest scores.
Genomic analysisOne of the main purposes of our N-to-1 ELM algorithm was to sort out TMBB proteins in genomic databases. We evaluated our method on the genome of Escherichia coli (K12 strain), which is one of the most comprehensively annotated genomes available. From UniProt we extracted the proteins from E.coli K12, which were not annotated as hypothetical or putative. Similarly of what has been done before by other authors, we did not consider proteins that were shorter than 60 or longer than 4000 residues ().For the legend seeWith this selection we ended up with 30 TMBBs and 2515 nonTMBBs. Since there is some degree of similarity between the E.coli proteins and those in NRPDB, we adopted the following procedure to asses the genomic predictions: @BULLET for each E.coli protein q, we run a BLAST search of q against NRPDB, @BULLET we extracted the most similar sequence p from NRPDB (the highest BLAST hit of q in NRPDB), @BULLET we selected the 1-to-N ELM models generated by the 10-fold cross-validation on NRPDB, from which p belonged to the 'test set', @BULLET we predicted q with those models (whose parameters and hyperparameters were chosen in the validation set).With this procedure, we simulated the case of a genomic analysis on never-seen before proteins. The results on E.coli set are reported in, where it appears that the N-to-1 ELME is more efficient in analyzing the E.coli annotated proteins than in scoring over NRPDB (compare Tables 2 and 5).
CONCLUSIONSIn this article, we present a new algorithm for the detection of TMBB proteins starting from their sequence. The main novelties of our work are of two kinds: methodological and applicative. First of all, here we introduce a new very powerful and general method that combines two ideas: an N-to-1 whole protein encoding in a single hidden layer () and an ELM approach (). Our new method is here applied to the detection of TMBB proteins. However, N-to-1 ELM models are very general and can be profitably applied to address other problems of computational biology. In this article, we also show that the N-to-1 ELM approach outperforms previously developed methods of TMBB detection, including the most recent one (). Differently from methods based on biochemical principles, the parameters of an N-to-1 ELM do not have an immediate physicochemical description. This can be a disadvantage when for the task at hand, a clear interpretation of the method parameter is needed. However, when the main goal is achieving the best predictive performance, a bottom up approach, such as learning rules from data mining, can be advantageous.
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
where T =[ 1 (t 1 ,), 1 (t 2 ),...  1 (t N ),] T is the matrix of transformed labels of protein classification by means of the inverse of the activation function  1 (x) = log[(1x)/x]. In order to prevent overflow, classification labels [1 or 0 for x in  1 (x)] are substituted with real values (for instance, 0.99 for 1 and 0.01 for 0). Our training algorithm can be stated as follows. Given a training set of protein sequences {P i } with their corresponding classification labels {t i }, the learning algorithm proceeds as: Step 1. Set the number of hidden neurons. Step 2. Assign randomly the input weights W. Step 3. For each protein P i generate a vector H i using the N-to-1 encoding of Equation (1) to obtain the final H matrix. Step 4. Calculate the output weights  as:  = H 1 T (4) where T =[ 1 (t 1 ,), 1 (t 2 ),...  1 (t N ),] T is as in Equation (3) and H 1 is the MoorePenrose generalized inverse of the H matrix (Huang, 2003). Differently from traditional learning algorithms, ELM not only tends to reach the smallest training error but also the smallest norm of output weights (Huang et al., 2006a). This implies that the H 1 choice minimizes both: MinimizeT H Minimize (5) In analogy with Support Vector Machines, minimizing the norm of the output weight |||| is similar to maximizing the distance of the separating margins of the two different classes in the ELM feature space 2/|||| (Huang et al., 2010). Another invaluable advantage of ELMs with respect to the classical gradient-based algorithms is their speed. Published papers addressed this issue although on problems different from the one we address here and reported that accuracy is somewhat similar (in some cases slightly lower) when back-propagation is compared to ELM (Alhamdoosh et al., 2010; Haung, 2006). For our application, the back-propagation version is at least 30 times slower with more hyper-parameters (learning rate, training cycles, etc.) to set (data not shown). The ELM speed during the learning phase allows to test a larger number of different models. In our current implementation, the random weights (W ) implicitly contain the threshold biases (set as uniform random values). On the contrary the weights of the second layers () do not contain biases, since it has been demonstrated that the hyper-plane crossing the origin is sufficient for universal function approximation (Huang and Chen, 2007, 2008; Huang et al., 2006b). Once a random set of weights is generated (W ) and the trainable parameters () are set according to Equation (4), the predictions of a set of unknown proteins (or of the validation/test sets) are assigned using Equations (1) and (2). 3 DATASET AND MEASURES OF PERFORMANCE 3.1 Dataset To train and test our method we rely on the dataset generated by Freeman and Wimley (2010). This dataset (NRPDB) consists of 14 238 chains derived from PDB, 48 of which are TMBBs from Gram-negative bacteria while 14 190 are non-TMBB proteins.
