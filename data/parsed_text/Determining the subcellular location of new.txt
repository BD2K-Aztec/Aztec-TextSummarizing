Motivation: Evaluation of previous systems for automated determination of subcellular location from microscope images has been done using datasets in which each location class consisted of multiple images of the same representative protein. Here, we frame a more challenging and useful problem where previously unseen proteins are to be classified. Results: Using CD-tagging, we generated two new image datasets for evaluation of this problem, which contain several different proteins for each location class. Evaluation of previous methods on these new datasets showed that it is much harder to train a classifier that generalizes across different proteins than one that simply recognizes a protein it was trained on. We therefore developed and evaluated additional approaches, incorporating novel modifications of local features techniques. These extended the notion of local features to exploit both the protein image and any reference markers that were imaged in parallel. With these, we obtained a large accuracy improvement in our new datasets over existing methods. Additionally, these features help achieve classification improvements for other previously studied datasets. Availability: The datasets are available for download at http://murphy lab.web.cmu.edu/data/. The software was written in Python and Cþþ and is available under an open-source license at http://murphylab. web.cmu.edu/software/. The code is split into a library, which can be easily reused for other data and a small driver script for reproducing all results presented here. A step-by-step tutorial on applying the methods to new datasets is also available at that address.
INTRODUCTIONGeneration of images of cells and tissues is increasingly easy. With the advent of automated microscopes, the capability for data generation has out-stripped the capability for visual data analysis. This has led to extensive work on automated methods for interpreting microscope images. The problem of classification of subcellular patterns has received particular attention, and a number of datasets and classifiers have been described. These datasets typically feature one different protein for each class of interest, with multiple images for the same tagged protein. On these datasets, better than human performance has been reported (). This previous work implicitly assumed that results obtained in those datasets can be generalized to the problem of classifying previously unseen proteins. In this work, we test this assumption using two new datasets where there are multiple proteins in each location class (and multiple images per protein). These datasets were created using NIH 3T3 cell lines expressing green fluorescent protein (GFP)-tagged proteins created by CD-tagging (). We tested classifiers using a cross-validation protocol whereby images from the same protein are never present in both the training and testing sets. This is a stricter proxy for cross-protein generalization than randomizing by image, and guards against the possibility that learning is based on properties of the tagging method (e.g. intensity) or too specific to the protein in question (e.g. a particular subpattern of an organelle). With this protocol and existing methods, generalization accuracy was only 60% for our new datasets. We therefore investigated whether improved generalization could be obtained using alternative feature representations of the images. Many previous systems use image-level features such as texture features (), but some specialized features for cell images have also been proposed (), including features for single-cell regions [in fact, historically, classification on cell-segmented images was reported first (In the computer vision literature, local features, such as the scale-invariant feature transform, introduced by Lowe (1999), have shown good results in many settings. They have not been widely used in bioimage analysis [there are a few uses of patchbased methods, a basic form of these features (. Object-level features, which can be seen as a form of local features, were used for subcellular location unmixing, both in supervised and unsupervised modes (). Local features, as presented in the literature, are generally defined on a gray-scale image and do not take advantage of the multiple image channels frequently acquired by fluorescent microscopy. There is some work on natural scene color images (van de), but it does not directly apply to fluorescence microscopy images for analyzing subcellular patterns where one channel is privileged (depicting the protein distribution of interest) and others serve as references. Naturally, the simplest protocol is to ignore all but the primary channel. However, the use of a reference channel can provide additional important information, particularly at the local level. For example, we could distinguish between two vesicle classes that appear similar in the primary (protein) channel but differ in distance from the nucleus because the region containing vesicles will appear differently in the reference nuclear channel. We present a simple protocol to take these reference channels into account. Using these features, we obtain a large accuracy gain on our datasets. We also use other datasets to further validate the value of the features and find that they lead to good results in all tested datasets. widefield confocalon the left, and nucleoli on the right), second row of confocal images (cytoplasmic pattern on the left, mitochondrial pattern on the right). Images are false color: the red channel shows the nuclear marker Hoechst, the green channel is the GFP-tagged protein. Images shown are the first image in their classes and have not been manually chosen. The widefield images were automatically acquired and the quality is lower than if they had been manually acquired. Images have been contrast stretched for publication
DATASETS
RandTag datasetsTwo datasets are introduced in this article, both from the RandTag (RT) project (). The first dataset consists of widefield images, the second of confocal images. The widefield images were collected with an automated microscope. Therefore, the quality of the images is variable. As a preprocessing step, images that are completely out-of-focus or empty of cells were removed. The confocal images were acquired manually and are of higher quality. Examples are shown in. These examples were not chosen as particularly pleasant looking, but are representative of the images in the dataset. The images were labeled by three experts (The experts were L.P.C., E.O.H. and E.G.A. for the widefield dataset, and L.P.C., E.G.A. and A.N. for the confocal dataset.) using a protocol where the experts first labeled the images independently and were then given an opportunity to change their minds given the other labelings. Only images where all experts agreed after this second step were retained.shows summary statistics for these two datasets. The two datasets contain multiple images of the same protein, and multiple proteins per location class. Most other subcellular location datasets contain multiple images per protein but only one protein for each location class (the exception is the Locate database).
Other datasetsWe present the main properties of the datasets in. All are publicly available.
Murphy Lab 2D HeLa DatasetThe Murphy Lab 2D HeLa dataset is by now a benchmark in the field, used by many researchers (). The dataset contains approximately 100 images collected by widefield fluorescence microscopy (with nearest neighbor deconvolution) for each of 10 subcellular patterns.obtained the best reported results on this dataset, 96% accuracy, using a combination of texture and other features.
Locate endogenous and Locate transfectedThese images were collected by widefield microscopy to detect 10 endogenous proteins or 11 transfected proteins (). Each dataset contains approximately 50 images for each subcellular patterns.The images were manually annotated and most proteins are labeled with more than one location. We are not aware of previous work in automatic classification of these images.
Locate Confocal
Image Informatics and Computational Biology Unit (IICBU) 2008 Benchmark The IICBU 2008 collectionof datasets includes several collections of bioimages with different properties, which was intended for testing computer vision algorithms () (The datasets are available at http://ome. grc.nia.nih.gov/iicbu2008.). We used the fluorescent microscopy datasets (the collection includes other modalities). This collection includes the HeLa 2D dataset, but it includes a version without dna channel. Our experiments were on the original, two channel, dataset.
Human Protein AtlasThe Human Protein Atlas (HPA) contains a collection of confocal images of immuno-stained proteins in human cells, with visual annotation (). We used those images where the visual annotation is to a single location class ().
MATERIALS AND METHODS
SURF-RefSpeeded-Up Robust Features (SURF) are calculated by a two pass algorithm. The first pass detects interest points by using an approximate Gaussian blob detector. These interest points are localized in both space (i.e. at a specific pixel location) and scale (i.e. they have an automatically determined size). The second pass computes 64 descriptors at each interest point.
Determining the subcellular location of new proteinsSURF works on a single channel (a gray-scale image), while bioimages are frequently multichannel: in addition to the primary channel, one or more reference channels are often acquired in parallel. Typically the primary channel is a protein image and a nuclear marker is used as a reference. SURF as presented in the literature can only be applied to the primary channel, discarding valuable information. The protocol to incorporate the reference information is as follows: run the point detection on the primary channel and compute feature descriptors on both channels independently. The feature descriptor for each point is then the concatenation of both descriptors.
Baseline feature sets As a baselinefeature set, we used a global feature set, which includes Haralick texture features (), parameter-free Threshold Adjacency Statistics (), object and skeleton features (), and overlap features ().
ClassificationComputing local features leads to several hundred descriptor vectors per image. To use these in classification, we clustered the descriptor vectors. This process assigns each descriptor to a cluster index. We represent an image as a normalized histogram of membership in the various clusters (). This is known as the ''bag of visual words'' model. The first step is to obtain a set of k centroids, using k-means. This algorithm takes two parameters: k, the number of clusters; and an initial set of centroids. This is implemented by setting the random number generator seed to different values and randomly selecting elements. For efficiency, centroids were obtained from a fraction (1/16th) of the data. All feature vectors are then assigned to the closest centroid. The resulting histogram can then be used with a standard support vector machine (SVM) classifier.provides an overview of the method. Asshows, there is a large variation in accuracy for different choices of the random seed even for the same value of k: the difference between the highest scoring and the lowest can be as high as six percentage points. Furthermore, asshows, the typical solution of minimizing the value of the Akaike information criterion (AIC) introduced by Akaike (1974), will not necessarily lead to a high accuracy. In fact, high AIC leads to high accuracy. Given the results inand 4, we used k  n=4 clusters, where n is the number of images in the training set. For the RTwidefield dataset shown in thethis corresponds to circa 310 clusters. Supplementalrepeats the calculation for the other datasets and confirms the value of this rule. We used a different random initialization for each point. The models learned are SVM based after feature normalization and selection using stepwise discriminant analysis (Jennrich, 1977a, b). A radial basis function kernel is used for the SVM, and an inner loop of cross-validation is used to select the hyperparameters. For the Locate database, which is a multilabel dataset, we used a separate classifier per label; for all other datasets, we used the ''one versus one'' strategy to convert binary classification into multiclass learning (These are the default settingsfor the milk Python machine learning library used in this work, no settings were changed or tuned).
Significance computationFor the measurement of statistical significance, we used a Bayesian approach. Given a dataset of size n, on which two algorithms correctly classify c 0 and c 1 elements, respectively, we assume that each algorithm has an underlying accuracy of r i and compute the Pr 0 4r 1 jc 0 , c 1 , n, the probability that the first algorithm is better than the second one. We also assume that the performance of the algorithms is independent,and computeTo be able to numerically obtain a value for (2), we model the accuracy of each classifier with a binomial distribution:In this framework, higher values are better, which is the opposite of the traditional statistical practice. Therefore, we report 1  Pr 0 4r 1 jc 0 , c 1 , n as a significance value. If the assumptions (1) and (3) are accepted, this significance value is the probability of making a Type I error (i.e. erroneously rejecting the null hypothesis that r 0 r 1 ). The Locate database needs to be handled differently as its proteins are annotated with multiple labels. The system we built learns a binary classifier for each label and, at evaluation time, outputs all the labels whose corresponding binary classifier returned a positive label. Each binary classifier was learned independently. For evaluation, the above framework is not directly applicable and we measured and report the F 1 score.
Cross-validationAll results were obtained using cross-validation. Ten-folds were used, except in the cases where the smallest class had less than 10 objects. In that case, the number of folds was set to the minimum class size. When handling the RandTag datasets with multiple images of the same protein, we can perform cross-validation in two ways:(1) Per image, in which we group the images into 10-folds without taking the depicted protein into account.(2) Per protein, in which we group the proteins into 10-folds. In this setting, there were never any images in training and testing from the same protein. Accuracy is still reported on a per-image level (the fraction of images that were correctly classified).
Software All software presented was developed in Python andC and incorporates code from dlib (Dlib's webpage is at http://www.dlib.net) by David King and LIBSVM by Chang and Lin (2001) for feature computation and classification, respectively. The software is designed to be easily reused in new datasets ().
RESULTS
Generalization to new proteinsAs described above, the RandTag datasets have images from several proteins in each dataset. Cross-validation over proteins is a stricter test of generalization capabilities and it was expected that it would lead to lower accuracies than the cross-validation over images (where training and test sets have different images of the same labeled protein).shows that the resulting difference in accuracy is large: a drop of 22 percentage points (8462%). Even with multiple proteins per class, having examples from the same protein in training and testing results in high measured accuracies. These values (9188%) are close to what is typically reported in subcellular location problems. However, when results are evaluated using the stricter cross-validation protocol, accuracy values are much lower, circa 60% for the baseline results. The differences in results with the two forms of cross-validation are statistically significant (at the 10 51 and 10 10 levels, for the widefield and confocal datasets, respectively). The HPA dataset also contains multiple proteins per class and a small number of images of each protein, often only two.Note: Shown are accuracies, in percentage, obtained either with per-protein or per-image cross-validation on two feature sets.
Determining the subcellular location of new proteinsTherefore, we used 2-fold cross-validation. The results in this dataset confirm that performing cross-validation per protein results in lower accuracy than cross-validating per image. The difference of 12 percentage points is significant at the 10 7 level.
SURF-refTable 4 summarizes the results obtained. On five of the datasets, using SURF variations shows a statistically significant improvement over the baselines used. On the other datasets, the results are not statistically distinguishable from the baseline. The worst results are obtained in the RNAi dataset, where local features alone perform much worse (significant at the 2:5  10 8 level). However, once the baseline is added, the results are indistinguishable from the baseline. Therefore, we recommend the use of all features combined.
Computational CostsSURF-ref is efficient in terms of computational time. On average, our implementation requires 7 s per image for both interest point detection and feature descriptor computation. Images in this case are 768  1024 pixels large. As part of interest point detection, each point is ranked according to a metric of how strongly it matches the approximate filter usedsee the original SURF article for details (). For large datasets, the computed feature data can be overwhelming. Therefore, we limited the number of interest points per image to 1024 (which are the 1024 highest matches according to the metric alluded to above). The traditional SURF consists of 64 descriptor values. In addition, we save the location, scale, angle and match strength for 70 floating point values per interest point.
DISCUSSIONThis work frames the subcellular location problem as recognizing different proteins in the same class. While this may have been implicit in previous work, it was not directly tested by datasets with a single representative protein per location class. We introduce two new datasets, which contain multiple proteins per class (and multiple images per protein). We observed that when cross-validation was performed over proteins, the resulting accuracy was much lower than when it was performed over images (where it is comparable with other datasets). This is intuitive as it is an easier problem to recognize proteins that are in the training set than proteins that are only in the same class (in particular, in the first case, it is possible that the system distinguishes the proteins by artifacts of the tagging or variation in subpatterns). Our data show that it is incorrect to assume that the high accuracy values obtained in datasets composed of multiple images of the same protein imply that the system would generalize well to other proteins in the same location class. Our datasets are publicly available. There is still a lot of room for improvement in accuracy and we hope that other researchers will test their methods on this harder problem. We also introduce a new methodology for classification of subcellular location patterns, which is based on interest point detection and local feature analysis. We developed a protocol to integrate the information in reference channels (which are typically acquired in parallel to the protein of interest). We implemented this method based on SURF, but the protocol is a generic method and could be applied to other local feature sets, such as scale-invariant feature transform () or any combination of detector and descriptor. On our new datasets, these methods performed better than the traditional whole-field features by 10 percentage points (a difference that is highly statistically significant). We tested these features on traditional datasets as well. On these, the baseline methods already perform well and there was less room for improvement. In four cases, the results are statistically indistinguishable from the baseline. It should be noted, though, that in no dataset did we observe that adding the local features lead to a statistically distinguishable worse outcome. These features have the further advantage that they are computed on the raw images without any pre-processing such as background correction or contrast enhancement. No tuning is necessary for adapting to new datasets and it is flexible for application to large datasets. Therefore, we recommend that local features with reference information be added to the standard toolkit for bioimage classification.
ACKNOWLEDGEMENTSWe thank the HPA project team, especially Emma Lundberg, for providing the high-resolution confocal microscopy images used for the HPA dataset, and Jieyue Li for preparing this dataset for computational analysis.
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
L.P.Coelho et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
