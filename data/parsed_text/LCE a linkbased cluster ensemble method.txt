Motivation: It is far from trivial to select the most effective clustering method and its parameterization, for a particular set of gene expression data, because there are a very large number of possibilities. Although many researchers still prefer to use hierarchical clustering in one form or another, this is often sub-optimal. Cluster ensemble research solves this problem by automatically combining multiple data partitions from different clusterings to improve both the robustness and quality of the clustering result. However, many existing ensemble techniques use an association matrix to summarize sample-cluster co-occurrence statistics, and relations within an ensemble are encapsulated only at coarse level, while those existing among clusters are completely neglected. Discovering these missing associations may greatly extend the capability of the ensemble methodology for microarray data clustering. Results: The link-based cluster ensemble (LCE) method, presented here, implements these ideas and demonstrates outstanding performance. Experiment results on real gene expression and synthetic datasets indicate that LCE: (i) usually outperforms the existing cluster ensemble algorithms in individual tests and, overall, is clearly class-leading; (ii) generates excellent, robust performance across different types of data, especially with the presence of noise and imbalanced data clusters; (iii) provides a high-level data matrix that is applicable to many numerical clustering techniques; and (iv) is computationally efficient for large datasets and gene clustering.
INTRODUCTIONThe use of clustering is vital both for visualizing and extracting useful information from microarray data. However, different algorithms (or even the same algorithm with different parameters) often provide distinct clusterings. As a result, it is extremely difficult for users to decide which algorithm and parameters will be optimal for a given set of datathis is because no single-pass/simple * To whom correspondence should be addressed. clustering algorithm can perform the best for all datasets (), and discovering all types of cluster shapes and structures presented in data is impossible for any known clustering algorithm (). Clinical researchers commonly use simple clustering methods, such as agglomerative hierarchical and k-means () to cluster cancer microarray samples, despite the advent of several new techniques that capitalize on the inherent characteristics of gene expression data (noise and high dimensionality) to improve clustering quality (e.g.). desays, this is because the use of such methods is difficult for non-expert users. Recently, cluster ensembles or consensus clusterings have emerged as simple, effective, one-stop methods for improving the robustness and quality of clustering results. Cluster ensembles combine multiple clustering decisions (referred to as 'base clusterings' or 'ensemble members') where the base clusterings contain diversity in their choice of clusters by:(i) using a single clustering algorithm with random parameter initializations (); (ii) employing multiple clustering algorithms (); (iii) selecting a random number of clusters (); (iv) using different subsets of gene (); or (v) using data sampling techniques (). Most existing methods compare cluster associations between each of the N samples in the dataset to produce an N N pairwise similarity matrix [i.e. consensus (), agreement () and co-association () matrices], to which a consensus function (e.g. agglomerative hierarchical clustering) is applied to acquire the final data partition. With the ensemble of two base clusterings ={ 1 , 2 } and five samples (x 1 ,...x 5 ) that is given in, the corresponding similarity matrix is shown in. An alternative approach (; Strehl and) to pairwise similarity methods makes use of an N P binary cluster-association matrix (BM) (where P denotes the number of clusters in an ensemble).shows the example of such matrix that is generated from the ensemble of. Despite reported success and efficiency, these methods generate the ultimate clustering result based on incomplete information of a cluster ensemble. The underlying association matrix presents samplecluster relations at a coarse level and completely ignores the relations among clusters (). As a result, the
N.Iam-on et al.the corresponding pairwise similarity matrix and (c) BM, respectively. performance of such techniques may consequently be degraded as many matrix entries are left unknown, each presented with zero. In response, we present a new methodthe LCEfor clustering data. It significantly extends the hybrid bipartite graph formulation (HBGF) technique (), by applying a graphbased consensus function to an improved cluster association matrix, instead of the conventional BM. This article explores its application to the problem of clustering cancer microarray samples, and is shown to refine the cluster-association matrix, as well as reducing the number of unknown entries and, therefore, increasing accuracy; moreover, it can easily replace or augment a researcher's existing clustering tools.
METHODSThe proposed LCE methodology is illustrated in. It includes three major steps: (i) creating M base clusterings to form a cluster ensemble; (ii) creating a refined cluster-association matrix (RM) using a link-based similarity algorithm (Weighted Connected-Triples, WCT); and (iii) generating the final data partition by exploiting the spectral graph partitioning (SPEC) technique as a consensus function. This framework is similar to that of HBGF (), except the second step that is introduced for constructing a refined information matrix. As compared to HBGF that is based on the BM, LCE may enhance effectiveness of the former using a more informative RM.
Creating a cluster ensembleLet X ={x 1 ,...,x N } be a set of N samples and let ={ 1 ,..., M } be a cluster ensemble with M base clustering results. Each base clustering returns a set ofwhere k i is the number of clusters in the i-th clustering. As in many previous studies (), the k-means clustering algorithm is used to generate base clusterings, each with random initialization of cluster centers. Euclidean distance is used to measure the dissimilarity between two samples unless stated otherwise. For each base clustering, there are two schemes of selecting the number of clusters: Fixed-k (k =  N, where N is the number of samples) and Random-k (k  {2,...,  N}). To create diversity in an ensemble, k should be greater than the expected number of clusters and the common rule-of-thumb is k =  N (). Note that the quality of the. The LCE framework: (i) a cluster ensemble ={ 1 ,..., M } is created from M base clusterings; (ii) a refined cluster-association matrix (RM) is then generated from the ensemble using the WCT algorithm; and (iii) a final clustering result ( * ) is produced by a consensus function of the spectral graph partitioning (SPEC). final clustering is directly subjected to the diversity among base clusterings (). Another alternative to generate diversity within an ensemble is to exploit a number of different data partitions. To this extent, the cluster ensemble is also established on various data subspaces. Similar to the study of, for a given N d dataset of N samples and d genes, an N q data subspace (where q < d) is generated by q = q min ++(q max q min )here  is a uniform random variable, q min and q max are the lower and upper bounds of the generated subspace, respectively. In particular, q min and q max are set to 0.75d and 0.85d. A gene is selected one by one from the pool of d genes, until the collection of q is obtained. The index of each selected gene is determined as follows, where h denotes the h-th gene in the pool of d genes and  [0,1) is a uniform random variable.
Generating a refined cluster-association matrix (RM)In particular to HBGF, BM has been used to summarized information presented in an ensemble. Each entry in this matrix BM(x i ,C j ) {0,1} represents a crisp association degree between sample x i  X and cluster C j . According to, which shows an example of cluster ensemble and the corresponding BM, a large number of entries in the BM are unknown, each presented with 0. Intuitively, this may limit the quality of a data partition generated by any consensus function. These conditions occur when relations between different clusters of a base clustering are originally assumed to be nil. It is important to note that each sample can associate (to a certain degree within) to several clusters of any particular clustering, at the same time. These hidden or unknown associations can be estimated upon the similarity among clusters, discovered from a link network of clusters. Based on this insight, the refined cluster-association matrix (RM) is put forward as the enhanced variation of the original BM. Its aim is to approximate value of unknown associations ('0') from known ones ('1'), whose association degrees are preserved within the RM (i.e. BM(x i ,cl) = 1  RM(x i ,cl) = 1). For each clustering  t ,t = 1...M and their corresponding clusters C t 1 ,...,C t kt (where k t is the number of clusters in the clustering  t ), the association degree RM(x i ,cl) that sample x i  X has with each cluster cl {C t 1 ,...,C t kt } is estimated as follows:where C t * (x i) is a cluster label (corresponding to a particular cluster of the clustering  t ) to which the sample x i belongs. In addition, sim(C x ,C y ) not appropriate for re-scaling associations within the RM. In fact, such local normalization will significantly distort the true semantics of known associations ('1'), such that their magnitudes become dissimilar, different from one clustering to another. According to our empirical investigation, the quality of RM is usually higher than other soft, fuzzy-like variations of the BM, which can be obtained from sample-to-cluster distances or a fuzzy cluster ensemble. See Supplementary Section 8.1 for details of such methods and associated experimental results.
WCT: a link-based similarity algorithmGiven a cluster ensemble of data samples X, a weighted graph G = (V ,W ) can be constructed, where V is the set of vertices each representing a cluster and W is a set of weighted edges between clusters. Formally, the weight assigned to the edge w xy  W , that connects clusters C x ,C y  V , is estimated bywhere L z  X denotes the set of samples belonging to cluster C z  V .shows the network of clusters that is generated from the example given in. Note that circle nodes represent clusters and edges exist only when the corresponding weights are non-zero. Given this network formalism, the new WCT algorithm is introduced to disclose the similarity between any pair of clusters. It extends the Connected-Triple method () that has been originally developed to identify ambiguous author names within publication databases. In particular, the similarity of any C x ,C y  V can be estimated by counting the number of Connected-Triples (i.e. triples) they are part of. Formally, a triple, Triple = (V Triple ,W Triple ), is a subgraph of G containing three verticesV and two non-zero edges W Triple ={w xk ,w yk }W , with w xy = 0. An example of triple within the network ofis shown in. This simple counting might be sufficient for any indivisible object, e.g. name or sample. However, to evaluate the similarity between clusters, it is important to realize and take into account the composite characteristic of a cluster (i.e. shared members). Inspired by this idea, the WCT measure of clusters C x ,C y  V with respect to each triple C k  V , is estimated aswhere w xk ,w yk  W are weights of the edges connecting clusters C x and C k , and clusters C y and C k , respectively. The count of all triples (1...q) between clusters C x and C y can be calculated as follows:Then, the similarity between clusters C x and C y can be estimated bywhere WCT max is the maximum WCT pq value of any two clusters C p ,C q  V and DC  (0,1) is a constant decay factor (i.e. confidence level of accepting two non-identical clusters as being similar). Following the example shown inand 3, the discovered link-based similarities/relations and the resulting RM are presented in.
Applying a consensus function to RMHaving obtained a refined cluster-association matrix (RM) with the aforementioned link-based similarity algorithm, a graph-based partitioning method is exploited to obtain the final clustering. This consensus function requires the underlying matrix to be initially transformed into a weighted bipartite graph. Formally, given an RM representing associations between N samples and P clusters in an ensemble , a weighted bipartite graphGiven such graph, the spectral graph partitioning (SPEC) method similar to that ofis applied to generate a final data partition. This is a powerful method for decomposing an undirected graph, with good performance being exhibited in many application areas, including protein modelling, information retrieval and identification of densely connected on-line hypertextual regions (). Principally, given a graph G = (V ,W ), SPEC first finds the K largest eigenvectors u 1 ,...,u K of W , which are used to formed another matrix U (i.e. U =[u 1 ,), whose rows are then normalized to have unit length. By considering the row of U as K-dimensional embedding of the graph vertices, SPEC applies k-means to these embedded points in order to acquire the final clustering result. Further details of SPEC can be found in Supplementary Section 1.
Experiment designThe experiments set out to investigate the performance of LCE compared to a number of different simple/standard clustering algorithms and state-ofthe-art cluster ensemble methods, over real gene expression and synthetic datasets. The compared techniques include: (i) HBGF that is the baseline model of LCE; (ii) four simple clustering techniques that are usually used by clinical researchers to analyse microarray dataPage: 1516 15131519
N.Iam-on et al.where CO(x i ,x j ) represents the similarity measure between samples x i ,x j  X. In addition, S m (x i ,x j ) = 1 if C m (x i ) = C m (x j ), and S m (x i ,x j ) = 0 otherwise. Note that C m (x i ) denotes the cluster label of the m-th clustering to which a sample x i  X belongs. Since co-association matrix (CO) is a similarity matrix, any similaritybased clustering algorithm (referred to as 'consensus function') can be applied to this matrix to yield the final partition  * (). Among several existing similarity-based techniques, the most well-known is agglomerative hierarchical clustering algorithm. Specifically to the problem of clustering cancer samples, MULTI-K () and CC HC () methods make use the SL and AL agglomerative hierarchical clusterings as consensus functions, respectively. In addition, to obtain  * , the GCC approach () transforms the CO matrix into a graph of samples to which the normalized cut algorithm (of the bipartite graph that is generated from the BM. There is no edge connecting vertices of the same object type, and the weight of an edge between any data point and cluster is either 1 (when the sample belongs to the cluster) or 0 (otherwise). SPEC () is exploited to obtain the final clustering result from this graph. This effectively allows the quality of the two cluster-association matrices (i.e. BM and RM) to be compared. CSPA creates a similarity graph, where vertices represent samples and edges' weight represent similarity scores obtained from the CO matrix. Afterwards, a graph partitioning algorithm called METIS () is used to partition the similarity graph into K clusters. HGPA constructs a hyper-graph, where vertices represent samples and the same-weighted hyper-edges represent clusters in the ensemble. Then, HMETIS () is applied to partition the underlying hyper-graph into K parts. MCLA creates a graph where each vertex corresponds to each cluster in the ensemble and each edge's weight between any two cluster vertices is computed using the binary Jaccard measure. METIS is also employed to partition the meta-level graph into K meta-clusters. The final clustering is produced by assigning each sample to the meta-cluster with which it is most frequently associated. Note that the performance of SL, CL, AL and KM are always assessed over the original data, without using any information of cluster ensemble.The effectiveness of LCE and other cluster ensemble methods with different ensemble sizes and types are also empirically examined. Details of gene expression datasets and experiment setting are presented below.
Real gene expression datasetsThis evaluation is based on real gene expression data, obtained from nine published microarray studies, and summarized in. The experiments were conducted over filtered datasets as given in the empirical study of de, where uninformative genes are removed for a better quality of clustering result. Details of the types of datasets, data preprocessing and the gene selection method are given in Supplementary Sections 2.12.2. To rigourously evaluate the robustness of LCE and its compared techniques, they are also assessed on both simulated gene expression data (with noise and imbalanced clusters) and geometrically complicated datasets (see Supplementary Sections 34 for data descriptions).
Experiment settingThe proposed LCE method and its competitors are evaluated, using the experiment setting illustrated below. @BULLET Each cluster ensemble method is evaluated over four different types of ensemble: (i) Fixed-k with full-space data (with d genes), (ii) Fixed-k with subspace data (with q genes), (iii) Random-k with full-space data and (iv) Random-k with subspace data, respectively.@BULLET An ensemble size (M) of only 10 base clusterings was used. @BULLET To generate a refined cluster-association matrix (RM), the constant decay factor (DC) of 0.9 is exploited with the underlying link-based similarity algorithm (i.e. WCT). @BULLET For a comparison purpose, as in Fern and Brodley (2004) and, each clustering method divides data points into a partition of K (the number of true classes for each dataset, known as 'gold standard') clusters, which is then evaluated against the corresponding true partition using a set of well-known evaluation indices. Note that, true classes are known for all datasets but are absolutely not used in any way by the cluster ensemble process; they are only used to evaluate the quality of the clustering results after clustering is complete. This assessment framework has been successfully adopted in de@BULLET The current research follows several previous studies () that focus on clustering samples of a given microarray data into known groups, i.e. class prediction. In particular to these methods, the quality of data partition  * generated by a clustering technique is directly compared against the Page: 1517 15131519known partition (i.e. class labels), using external validity indices such as Adjusted Rand (AR;), Normalized Mutual Information (NMI;) and Classification Accuracy (CA;). These specific indices are exploited for evaluating the performance of the proposed LCE method, against several other clustering techniques. The limitation of this evaluation is that the capability of examined methods for 'class discovery' has not been reviewed. Unlike the task of class prediction, the quality of data partition is determined by a structural properties of clusters, e.g. a compactness of samples in a cluster and a distance between clusters. To this extent, an initial study regarding of LCE for the task of class discovery is provided in Supplementary Section 8.2. In addition, the analysis of gene domain is another prominent research, in which LCE may prove to be useful. In particular, a better quality assessment should make use of a validity index that takes into account known gene functions, instead of simple external or internal indices mentioned earlier. Here, the performance of a given clustering algorithm is justified in terms of its ability to produce biologically meaningful clusters using a reference set of functional classes, which can be obtained from prior biological knowledge specific to a microarray study or may be formed using the growing databases of Gene Ontologies.
LCE method for improved gene expression data analysis@BULLET The quality of each cluster ensemble method with respect to a specific ensemble setting is generalized as the average of 50 runs.
RESULTSThe results 1 with real gene expression data are summarized in, where each investigated clustering method is represented with its average validity measure across all validity indices, datasets and ensemble types. It is clear that LCE regularly performs better than any of these clustering methods. It also enhances the performance of KM, which is used as base clusterings. In particular, HBGF is apparently less effective than LCE. This information suggests that the quality of the refined cluster-association matrix (RM) is superior than the original BM counterpart. See the full results with real gene expression datasets in Supplementary Section 2.3. Following the study of, to rigourously evaluate the quality of investigated clustering techniques, the number of times that one method is significantly better and worse (to 95% confidence level) than the others are assessed across all experiment settings. Let X C (i,) be the average value of validity index C  {CA, NMI, AR} across n runs (n = 50 in this evaluation) for a cluster ensemble method i  CM (CM is a set of 12 experimented clustering methods), on a specific experimentsetting   ST (ST is a set of 40 unique combination of four ensemble types and ten real gene expression datasets). The 95% CI,. Note that S C (i,) denotes the SD of the validity index C across n runs for a clustering method i and an experiment setting . In addition, multiple runs of any setting   ST are different and independenteach with a unique ensemble that is generated by randomly selected parameters, and possibly dissimilar gene subsets. The number of times that one method i  CM is significantly better than its competitors, B C (i) (in accordance with the validity criterion C, across all experiment settings), can be defined asSimilarly, the number of times that one method i  CM is significantly worse than its competitors, W C (i), in accordance with the validity criterion C, can be computed asUsing the aforementioned assessment formalism,illustrates for each method i  CM the statistics of total performance. The results shown in this figure indicate that LCE is more effective than other clustering techniques included in this experiment. Since SL and AL do not perform well over the examined datasets, MULTI-K and CC HC that use the former and latter as a consensus function, respectively, are less accurate than other cluster ensemble methods and KM. However, their performance may improve with an ensemble that is much larger than the one investigated herein (i.e. M >> 10). In addition to this evaluation scheme, a further performance analysis with a paired t-test is discussed in Supplementary Section 2.4. Another important investigation is on the subject of relations between performance of experimented cluster ensemble methods and different types of ensemble being explored in the present evaluation.shows the average validity measures of different cluster ensemble methods across all validity indices and real gene expression datasets. For each method, its performance with Page: 1518 15131519,0.2,...,0.9} and the performance of the LCE method (the averages of CA, NMI and AR over 10 real gene expression datasets and 4 ensemble types), whose values are presented in X-axis and Y-axis, respectively. Note that the averaged performance of other cluster ensemble methods are also included for a comparison purpose. four ensemble types (Full-space + Fixed-k, Full-space + Random-k, Subspace + Fixed-k, and Subspace + Random-k) are compared. It is clear that LCE is more effective than other cluster ensemble techniques over all ensemble types, with its best performance being generated from a 'Full-space + Fixed-k' ensemble. Most methods work better with Full-space ensembles, as compared to Subspace alternatives. Unlike LCE, GCC and other graph-based techniques that usually produce a superior performance with a Fixed-k ensemble type, MULTI-K and CC HC are best when coupled with Random-k ensembles. Although the results are impressive, on several datasets, it is important to ensure they are obtainable in a wide range of conditions. To this end the LCE algorithm's response was examined to perturbations in its parameters, and by investigating its time and space complexity.
N.Iam-on et al.
Parameter analysisThe parameter that has any effect on the results of LCE is DC [see. With the ensemble size of 10, we varied this value from 0.1 through 0.9, in steps of 0.1, for three validity measures, and obtained the results in. Thisshows that the results are robust, and do not depend strongly on any particulary value of DC. This makes it easy for users to obtain high-quality, reliable results when using LCE, particularly since values of DC near 0.7 generally produce the best results. Although there is variation in response across the DC values, the performance of LCE is always better than any of the other cluster ensemble methods included in this assessment. Another important parameter that may determine the quality of a cluster ensemble technique, is the ensemble size (M). Intuitively, the larger an ensemble is, the better the performance becomes.According toin which DC = 0.9, this heuristic is applicable to LCE, where its validity measures (averages of CA, NMI and AR across all experimental settings) gradually incline to the increasing value of M {10,20,...,100}. Furthermore, LCE performs better than its competitors with all different ensemble sizes. Note that a bigger ensemble leads to an improved accuracy, but with the tradeoff of run timebut, again, even the worst results for LCE are better than the best results of the other methods. These findings regarding the relation between LCE and its parameters have also been observed when both DC and M are simultaneously analysed (see details in Supplementary Section 7).
Complexity analysisThe space and time complexity of creating a refined clusterassociation matrix (RM) are O(P 2 +NP) and O(P 2 l +NP), where N is the number of samples, P denotes the number of all clusters in an ensemble and l represents the average number of neighbours connecting to one cluster in a link network of clusters. For each entry (corresponding to clusters C x ,C y  ) in the P P matrix of cluster similarity, WCT searches through l neighbors of C x (or C y ) to identify connected triples. Following this, the RM of size N P is created using the aforementioned similarity matrix. As a result, LCE is computationally efficient with the time complexity generally converging to O(N). Please consult Supplementary Section 6 for details of the scalability test.
Additional utilization of RM with simple clusteringsBesides its current utilization through the formation of a weighted bipartite graph, the RM can also be regarded as a 'high-level' data matrix to which any simple clustering algorithm can be directly applied. Promising results have been obtained from the exploitation of six simple clustering techniques with RM: RM + SL, RM + CL, RM + AL, RM + KM, RM + Partitioning Around Medoids and RM + spectral clustering, respectively (see detailed results in Supplementary Section 5).
CONCLUSIONA new LCE method has been introduced for clustering gene expression data samples that has greatly improved accuracy and efficiency. The performance of LCE is usually superior than existing graph-based ensemble techniques, and those that are particularly developed for gene data analysis. LCE is highly effective over real gene expression datasets and synthetic data collections (with Page: 1519 15131519
LCE method for improved gene expression data analysisthe presence of noise and non-equal-size clusters). Unlike existing pairwise similarity based counterparts, LCE is efficient for clustering large-size datasets, including the clustering of genes. Specifically, the refined cluster-association matrix (RM) used by LCE is able to recover and account for unknown entries in the original BM counterpart, and hence, delivers a superior clustering performance. With its consistent performance over settings of parameter, ensemble type and size, LCE also proves to be a user-friendly data analysis tool, especially for non-expert users.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org 1513 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from [15:46 21/5/2010 Bioinformatics-btq226.tex]
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
This section only contains a summary of our empirical evaluation over real gene expression data. For more detailed results and experiments with other data collections, please see Supplementary Sections 2-4.
