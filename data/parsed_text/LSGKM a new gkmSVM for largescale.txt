gkm-SVM is a sequence-based method for predicting and detecting the regulatory vocabulary encoded in functional DNA elements, and is a commonly used tool for studying gene regulatory mechanisms. Here we introduce new software, LS-GKM, which removes several limitations of our previous releases, enabling training on much larger scale (LS) datasets. LS-GKM also provides additional advanced gapped k-mer based kernel functions. With these improvements, LS-GKM achieves considerably higher accuracy than the original gkm-SVM. Availability and implementation: C/C þþ source codes and related scripts are freely available from http://github.com/Dongwon-Lee/lsgkm/, and supported on Linux and Mac OS X. Contact: dwlee@jhu.edu Supplementary information: Supplementary data are available at Bioinformatics online. We have previously introduced a sequence-based method, kmer-SVM (Fletez-Brant et al., 2013; Lee et al., 2011) to predict regulatory elements from DNA sequence and epigenetic data using support vector machines (SVM) (Vapnik, 1995). It has been successfully applied to studies of regulatory elements in different cellular contexts (Gorkin et al., 2012; Pimkin et al., 2014), and further improved by using gapped k-mers as new features (gkm-SVM (Ghandi et al., 2014)). We have also recently demonstrated its ability to predict regulatory sequence variants (Lee et al., 2015). Since then, gkm-SVM has gained increasing attention (Setty and Leslie, 2015; Zhou and Troyanskaya, 2015). Our general strategy is to build an SVM classifier that distinguishes regulatory sequences from non-regulatory genomic sequences in the k-mer or gapped k-mer frequency feature vector space. Training of SVM involves evaluation of a kernel matrix (or Gram matrix), defined as an n-by-n matrix of all possible inner products (or kernel functions) between a set of vectors of n training examples. However, as n increases, direct computation of the kernel matrix quickly becomes impractical with gapped k-mers as features. To resolve this issue, gkm-SVM employs an efficient algorithm that calculates a full kernel matrix with a runtime that linearly scales with n instead of n 2. An SVM classifier is then trained using the pre-calculated kernel matrix and standard SVM training methods. Yet, the full kernel matrix evaluation required in the original implementation has hindered optimal training of gkm-SVM on larger datasets because it needs substantial memory resources proportional to n 2. Sub-sampling strategies to circumvent this issue can be helpful (Ghandi et al., 2014), but training on smaller datasets may yield sub-optimal SVM classifiers. To tackle this problem, I have developed an improved software, LS-GKM, by implementing gapped k-mer kernel (gkm-kernel) functions within the LIBSVM framework (Chang and Lin, 2011). Most SVM tools such as LIBSVM utilize decomposition methods for SVM training. It iteratively finds and solves a small subset SVM problem that only needs a partial kernel matrix. For example, LIBSVM evaluates just two columns of the kernel matrix in each of the problem solving steps in its sequential minimal optimization algorithm (Fan et al., 2005). Therefore, replacing the LIBSVM kernel routines with the gkm-kernel functions can essentially solve the memory resource issue and, consequently, allows us to train SVM on much larger datasets. To this end, I adopted and modified the original gkm-kernel algorithm, substituting for the original LIBSVM kernels so that it can efficiently evaluate one column of the kernel matrix in the same manner as the original gkm-SVM does for the full matrix. Multi-thread functionality is also implemented in LS-GKM for further speed-up (Supplementary Methods for more details). I first compared runtime and memory usage of the new software to the original gkm-SVM by varying the training set size n (Supplementary Methods, Fig. S1). As expected, our previous method exhibits quadratic growth of memory usage as n increases. gkmtrain (the new SVM training module of LS-GKM) with the large Downloaded from cache (8 Gb) also exhibits quadratic memory expansion when n < 60 000. However, the overall memory usage is much less than the original method ($20%). Moreover, once the cache is full, the memory only linearly increases. Regarding runtime, the original method initially shows better computational efficiency than gkmtrain with the default setting (1 thread, 100 Mb). However, with either the large cache or the four threads, the new program can run faster than the original one. Furthermore, it can run almost 5Â faster when both options are used. Most notably, we can now regularly train LS-GKM on much larger datasets with reasonable time and memory. In addition to the integration of kernel functions into LIBSVM, three new kernel functions have been developed. Analogous to the basic RBF kernel, gkmrbf-kernel is defined as the radial basis function , in the space of gapped k-mer frequency vectors (Supplementary Methods). The second option, denoted as center weighted gkm-kernel or wgkm-kernel, is inspired by the observation that most ChIP-seq and DNaseI-seq signals are concentrated in the central regions within peaks. In this new kernel, the gapped k-mers are differentially weighted based on their distances from the center of the peak (Supplementary Methods, Fig. S1). The last kernel, wgkmrbf-kernel, is the combination of the previous two. To demonstrate the utility of LS-GKM, I assessed how much classification accuracy can be improved by LS-GKM for predicting regulatory elements. Uniformly processed 322 ENCODE ChIP-seq datasets (The ENCODE Project Consortium, 2012) containing at least 5000 regions were considered, and standard training and test procedures developed in the previous studies (Ghandi et al., 2014; Lee et al., 2011) were applied with some modifications (Supplementary Methods). First, the new models trained on the whole datasets exhibit considerably better AUC than the models trained on the sub-sampled sets (n ¼ 10 000), especially when the training set is large (n > 60 000) (Supplementary Methods, Fig.S2A). A grid search of the C parameter on selected datasets confirms that this result is not an artifact caused by a sub-optimal choice of C (Supplementary Table S1). In fact, our default value (C ¼ 1) was optimal or near optimal in almost all cases we tested. Second, the model trained with gkmrbf-kernel further increases the AUC as compared to the original gkm-kernel in every case, but the improvement is marginal (Supplementary Methods, Fig.S2B). This result implies that the advantage of using non-linear decision boundaries is limited with gkm-kernel. Third, wgkm-kernel can also significantly improve the AUC, when the datasets already exhibit AUC values >0.9 (Supplementary Methods, Fig.S2C). Closer investigation reveals that most of the less predictive datasets (AUC < 0.9) are ChIP-seq on Pol2 and its related factors (TAF1, TAF7 and TBP). This suggests that many of these peaks may represent transient binding of the factors and, thus, contain less predictive sequence features. Fourth, similar to the gkmrbf-kernel, wgkmrbf-kernel marginally improves AUC when compared to wgkm-kernel (Supplementary Methods, Fig.S2D). Note that, in some cases such as Pol2 ChIP-seq, the best AUCs are achieved by gkmrbf-kernel not by wgkmrbf-kernel. Therefore, for the final comparison, the better kernels were chosen based on classification performance with independent training and evaluation (Supplementary Methods, Fig. S3). Supplementary Figure S4 compares the baseline AUCs (trained on 10 000 regions with gkm-kernel) from the original gkm-SVM to the best AUCs achieved by LS-GKM. The average gain of AUC is significant (0.912 ! 0.941). If Pol2 and the related ChIP-seq datasets are removed, the average of the best AUCs is remarkably high, 0.960. To determine whether LS-GKM can also improve deltaSVM, a major application of gkm-SVM for predicting regulatory sequence variants (Lee et al., 2015), the dsQTL test set was reanalyzed using the new LS-GKM models trained on a larger GM12878 DHS data-set (Supplementary Methods, Fig. S5). The Precision Recall curves show that the new models consistently outperform the original model. However, no further improvement is achieved with new kernels, suggesting that larger datasets primarily contribute to the improvement of deltaSVM accuracy. In this study, I have presented new and improved software, LS-GKM, which offers several new functionalities and considerably improves the classification accuracy on predicting regulatory elements. We strongly encourage all users of our software to train models on the largest datasets available, which can produce significantly more accurate predictions. In this regard LS-GKM should improve the performance considerably, and in combination with its enhanced functions, LS-GKM is expected to significantly contribute to our understanding of gene regulation.
FundingThis work has been supported by NIH grants HL128782, HL086694 and GM104469. Conflict of Interest: none declared.. LS-GKM is highly scalable in comparison to gkm-SVM. The maximum memory usage (A) and runtimes (B) are compared as training set sizes vary. Eight sub-sampled sets from H1hescCtcf (5k, 10k, 15k, 20k, 25k, 30k, 60k and 90k) were evaluated. All SVM trainings were done on a single machine equipped with Intel Core i7 processor (2.8 GHz, 4 cores) and 16 GB RAM for equivalent comparison
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
D.Lee at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
