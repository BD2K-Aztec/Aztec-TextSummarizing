Motivation: Measurements are commonly taken from two phenotypes to build a classifier, where the number of data points from each class is predetermined, not random. In this 'separate sampling' scenario, the data cannot be used to estimate the class prior probabilities. Moreover, predetermined class sizes can severely degrade classifier performance, even for large samples. Results: We employ simulations using both synthetic and real data to show the detrimental effect of separate sampling on a variety of classification rules. We establish propositions related to the effect on the expected classifier error owing to a sampling ratio different from the population class ratio. From these we derive a sample-based mini-max sampling ratio and provide an algorithm for approximating it from the data. We also extend to arbitrary distributions the classical population based Anderson linear discriminant analysis minimax sampling ratio derived from the discriminant form of the Bayes classifier. Availability: All the codes for synthetic data and real data examples are written in MATLAB. A function called mmratio, whose output is an approximation of the minimax sampling ratio of a given dataset, is also written in MATLAB.
INTRODUCTIONThe medical community is being confronted with serious problems of reproducibility in the development of biomarkers. The issue has been highlighted by a recent report regarding comments by Janet Woodcock, FDA drug division head. The report states, 'Based on conversations Woodcock has had with genomics researchers, she estimated that as much as 75 percent of published biomarker associations are not replicable. ''This poses a huge challenge for industry in biomarker identification and diagnostics development,'' she said ().' Many issues affect reproducibility, including the measurement platform, specimen handling, data normalization and sample compatibility between the original and subsequent studies. These matters concern experimental procedures and are not our concern here; rather, we are interested in the methodology for designing classifiers. One issue in this regard is the impact of inaccurate error estimation owing to small samples. This has been previously quantified (). Here we are interested in a different problem, one that will confront us even if we have large samples and perfect error estimation: the effect of having predetermined sample sizes so that sampling is not random. In classification studies it is typically a tacit assumption that sampling is random; indeed, it is commonplace for this assumption to be made throughout a text on classification. For instance, Devroye et al. declare on page 2 of their text that all sampling is random (). The assumption is so pervasive that it can be applied without mention. With regard to the problem at hand,state, 'In typical supervised pattern classification problems, the estimation of the prior probabilities presents no serious difficulties.' But, in fact, there are often serious difficulties. Under the assumption of random sampling, the data set,}, is drawn independently from a fixed distribution of feature-label pairs, (X, Y); in particular, this means that if a sample of size n is drawn for a binary classification problem, then the numbers of sample points, n 0 and n 1 , in classes 0 and 1, respectively, are random variables such that n 0  n 1  n. An immediate consequence of the random-sampling assumption is that the prior probability c  Pr(Y  0) can be consistently estimated by the sampling ratio, namely, by ^ c  n0 n. Consistency is nothing but Bernoulli's weak law of large numbers, namely, n0 n ! c in probability. Thus, if the sample is large, we can expect the sampling ratio to be close to the prior probability. Suppose the sampling is not random, in the sense that the ratios n0 n and n1 n are chosen prior to sampling. In this 'separate (stratified) sampling' case, S n  S n0 [ S n1 , where the sample points in S n0 and S n1 are selected randomly from  0 and  1 but, given n, the individual class counts n 0 and n 1 are not random. Then, in effect, we have no sensible estimate of c. One could let ^ c  n0 n , but there would be no reason to do so. Since our aim is to use the data to train a classifier, does the inability to consistently estimate c matter? Clearly in the case of linear discriminant analysis (LDA) it does, since the LDA classifier is defined by n x  1 if D samp x 0 and n x  0 if D samp x40, whereand ^ l 0 and ^ l 1 are the sample means of the class-conditional populations  0 and  1 , respectively, and ^ D is the pooled sample covariance matrix. The rationale for the LDA discriminant is that the estimators converge to the population parameters *To whom correspondence should be addressed. as n ! 1, in which case the resulting discriminant, D Bayes x, defines the Bayes (optimal) classifier in the two-class Gaussian model with common covariance matrix. It is obvious from Equation (1) that an estimate of c is required for LDA and a bad choice of ^ c will negatively impact the classifier. This fact, which is a consequence of separate sampling, has long been recognized (). The situation is less transparent with model-free classification rules such as support vector machines. In this article we use simulation to study the effect of separate sampling on several different classification rules, where the role of c does not appear explicitly in classifier learning. We generate separate samples with different ratios r  n0 n and consider the expected error, E" n jr, of the designed classifier, given r, where the error of classifier n is defined by " n  Pr n X 6  Y, the probability of misclassification. We will see that the penalty for separate sampling without knowledge of c can be severe. With random (or, 'mixed') sampling, rather than being fixed prior to sampling, r is a sample-dependent random variable. In this case, E" n jr denotes the expectation of the error conditioned on r and the expected classification error is given by E" n   E r E" n jr, where the outer expectation is relative to the distribution of r. The classifier error is likely to be smaller when the sampling ratio r is close to c. Hence, if one happens to fix r sufficiently close to c, then E" n jr5 E" n . Because r ! c in probability as n ! 1 for mixed sampling, as n gets larger the distribution of r gets more tightly concentrated around c, so that the distribution of E" n jr (as function of r) gets more tightly packed around E" n , which in turn means that to have E" n jr5 E" n  one must choose r very close to c. To illustrate this phenomenon, consider 2D Gaussian class-conditional densities with means at (0.3, 0.3) and (0.8, 0.8), possessing common covariance matrix 2 I, where I is the identity matrix and 2  0:4, and with c  0:6: For this model, the Bayes error is " Bayes  0:27.shows the difference E" n   E" n jr for different values of r and different sample sizes when using LDA. If r  0:6, then E" n jr5 E" n  for all n. If r  0:7, which is fairly close to c  0:6, then E" n jr5 E" n  for n 25. Notice the lack of symmetry, both 0.5 and 0.7 being equally close to c. This should not be surprising because we should not expect the distribution of E" n jr to be symmetric. Let us examinefrom the practitioner's perspective. Suppose that cost limits the sample to a given size n. If the sample is random, then the expected error of the designed classifier will be E" n , which is unknown since the feature-label distribution is unknown. Consider three cases: (i) if c is accurately known from existing population statistics regarding the two classes, say BRCA1 and BRCA2 breast cancer, then no matter what the sample size, it is best to do separate sampling with n 0 % cn; (ii) if c is approximately known, meaning that the practitioner believes that c is close to c 0 , then, for small n, it may be best, or at least acceptable, to do separate sampling with n 0 % c 0 n, and the results will likely still be acceptable for large n, though not as good as with random sampling; (iii) if the practitioner has no idea what c is, then sampling must be random because the penalty for separate sampling can be very large. While, at this point, these comments refer specially to, which is for LDA, a salient point to be made in this article is that they are quite general and, moreover, can be extended to the commonplace separate sampling situation where one cannot choose n 0 and n 1. Why is all of this a major issue for bioinformatics? Simply put separate sampling is ubiquitous in bioinformatics, in particular, with genomic classification, where a standard approach is to take tissue samples from two classes, say, different types of cancer or different stages of cancer, for which the number of specimens in each class is not chosen randomly, and then to design a classifier. The Supplementary Material lists 20 published studies using separate sampling. In each case we give the classification problem, sample sizes, classification rule and error estimator. Even if an error estimate is exact for the problem at handthat is, for the sampling ratio represented by the datawhat does it mean relative to the classification error for future observations (say, patients)? That depends on the true prior probabilities, which we do not know.
SYSTEMS AND METHODS
Effect of sampling ratiosynthetic dataWe employ simulation to study the effect of the sampling ratio for different classification rules using a general model based on multivariate Gaussian distributions with a blocked covariance structure. This model conforms to the setting where blocks represent correlated gene groups, say common pathways, and between-block correlation is negligible (). The model has several parameters that can generate a battery of covariance matrices. For example, a 4-block covariance matrix with block size 3 has the structurein which 2 y is the variance of each variable and the i , i 2 f1, 2, 3, 4g, are the correlation coefficients inside blocks. We consider both identical and unequal covariance matrices. We assume common correlation coefficient, i  , i 2 f1, 2, 3, 4g. A typical microarray or next-gen RNA sequencing () experiment yields expressions for thousands of genes, but a small number of sample points, typically 5200. Therefore, data-based feature selection is typically employed; however, since our sole aim is to study the effect of the ratio r on the expected true error, we do not consider feature selection and assume a model containing a reasonable number, D, of features (which is equivalent to assuming that a set of D genes has been chosen by the researcher based on prior biological knowledge). We let D  15. Two covariance matrix settings are considered: identical covariance matrices, 2 0  2 1  0:4, and unequal covariance matrices, 2 0  0:4, 2 1  1:6, with block size l  5 and correlation coefficient  0:8 corresponding to tight correlation within a block. The parameter settings are summarized in. Seven classification rules are considered: 3-nearest neighbor (3NN), 5nearest neighbor (5NN), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), linear support vector machine (L-SVM), radial basis function SVM (RBF-SVM) and decision tree (DT). The SVM classifiers are trained from the package LibSVM written in MATLAB (). A decision tree classifier is trained using the MATLAB classregtree function.
Effect of sampling ratioreal dataFour microarray real datasets are used: pediatric acute lymphoblastic leukemia (ALL) (), acute myeloid leukemia (AML) (), multiple myeloma () and breast cancer (). We follow the data preparation instructions reported in the cited articles. The properties of these datasets are summarized in. The right-most column incontains the initial feature size, number of sample points in classes 0 and 1, respectively, from left to right. The Supplementary Material provides detailed descriptions of these datasets. The same classification rules as those used for the synthetic data are applied to the real data. T-test feature selection is used to reduce the original set of genes down to D  15.
Holdout error estimationBecause we are going to use real data, we wish to use holdout error estimation; however, the standard holdout procedure, which is unbiased with random sampling, become biased, perhaps severely so, with separate sampling. Therefore, we redefine holdout for separate sampling.The true error of a designed classifier n is given by " n  Pr n X 6  Y  c Pr n X 6  0jY  0  1  c Pr n X 6  1jY  1  c" 0 n  1  c" 1 n : 4Relative to a random sample, S n , the expected true error isFor standard holdout error estimation, the sample is split into t points (the training set) to train the classifier and m points (the test set) to estimate the error, where in this scenario the notation indicates that the total sample size is n  t  m. Let S t , S m , S m0 , and S m1 denote the set of training data, the full set of test data, the class-0 test points, and the class1 test points, respectively. The holdout estimator iswhere ^ " 0 and ^ " 1 denote the holdout estimators of " 0 n and " 1 n. Taking expectations in (6) yieldsBecause the test data are independent from the training data, the holdout estimator is unbiased given the training data, which means that E Sn  ^ "jS t   " n : Taking the expectation relative to the training data yields E Sn  ^ "  E St E Sn  ^ "jS t   E Sn " n : Similar expressions apply to ^, and ^ " 1 are unbiased estimators of " n , " 0 n , and " 1 n , respectively, and Expression (7) corresponds term by term to (5). With separate sampling, taking expectations in (6) yieldsbecause the ratio m0 m is fixed. Hence, ^ " is not unbiased. The bias depends on the difference between c and m0 m : If c is known, then the holdout estimator can be redefined asfor both random and separate sampling. In both cases it is unbiased: taking expectations on the right-hand side of (9) yields the right-hand) Absent an independent estimate of c, use of (6) for separate sampling is unacceptable because it is biased and the extent of the bias is unknown. We use (9) for our real-data examples.
ImplementationFor a given synthetic model parameter setting, sample size n, ratio r and classification rule , we approximate the expected true error rate E Sn " n cjr via Monte-Carlo (MC) simulation. Each repetition of the MC simulation is depicted in(a). The first set of experiments is done using the flowchart in(a). In these experiments, the sample size, n, is fixed but the class sample sizes vary according to the sampling ratio r. Samples S n are generated using the model determined by l 0 , D 0  and l 1 , D 1  described inin accordance with r and n. Assuming a classification rule , a classifier is trained. The last stage in(a) is the true error computation for the designed classifiers, which is also done via MC with 10 000 repetitions. The whole procedure is repeated 5000 times. The real datasets inare sufficiently large to be divided for training and testing and we use holdout error estimation, as previously described for separate sampling. The procedure is graphically illustrated in(b). Fixing the total sample size n, assuming different values for the parameter r, we choose n 0  rn  m AE  points from class 0 and n 1  n  m  n 0 from class 1, where a d e is the smallest integer greater than or equal to a. The remaining data points are used for holdout estimation. The expected holdout estimate, E ^ " n cjr, is computed via MC approximation. The process is repeated 3000 times.
RESULTS AND DISCUSSIONThe full set of results appears in the Supplementary Material. Herein, we provide some results covering a variety of cases. We show results of four classification rules: 3NN, 5NN, L-SVM and RBF-SVM. Results for the synthetic examples with dimension 15 are shown. Also, we only provide the results for n  100 and n  80, for the synthetic and real datasets, respectively. For the real datasets, a two-sample t-test is used to reduce the dimensions into D  15.
Expected true errorThe expected true errors for synthetic data with common covariance matrix are given in(a)-(d), where each plot gives the expected true error versus the parameter r for different class prior probabilities, i.e. c 2 f0:3, 0:4, 0:5, 0:6, 0:7g: Similar behavior is observed, regardless of classification rule. There is, however, different sensitivsities of different rules to the sampling ratio r. Results for the second model with different covariance matrices are shown in(e)(h). In this case, there is more varied behavior among the classification rules. Note the point in each figure where the curves cross. This point corresponds to the minimax solution and will be discussed in detail when we analyze the properties of the curves in a later subsection. For equal covariance matrices, the point is close to 0.5 for all the classification rules; however, it can be far from 0.5 for unequal covarinace matrices, depending on the classification rule.(d)(p) show results for two real datasets, where the performance is assessed via holdout error estimation. Each plot includes the expected holdout error estimate versus r for five values of c. In contrast to(a)(h), the curves in(d)(p) are not smooth, which is a result of discrete error estimation. Nonetheless, there still is a crossing point. The solid vertical lines in(i)(p) are fixed on the initial sampling ratios.
Properties of the error curvesThe most obvious characteristic of the error curves is that in each figure they appear to cross at a single value of r. We will now examine this phenomenon. We must be careful because the figures show continuous curves but r is a discrete variable. Hence, we will have to carefully define what it means to 'cross'. According to the standard definition, a classification rule is said to be 'smart' if the expected value of the error is monotonically decreasing as a function of sample size for all feature-label distributions (). This is in accord with the intuition (not always correct) that more data cannot hurt classifier design. We adapt the notion of smartness to the present circumstances by defining a classification rule to be 'class-wise smart' relative to a family of feature-label distributions f c x, y  cfxj0 1  cfxj1, c 2 0, 1, if, for all c 2 0, 1 and r 2 4r 1 , E" 0 n jr 2  E" 0 n jr 1  and E" 1 n jr 2  ! E" 1 n jr 1 : Intuitively, r 2 4r 1 means that there are more data available from class 0 when designing the classifier when conditioning on r 2 than when conditioning on r 1 , so that one would intuitively expect that the class-0 error when conditioning on r 2 is not greater than when conditioning on r 1 , which is what is stated by the first inequality. The situation reverses relative to the class-1 error and that is what is stated by the second inequality. A classification rule is 'strictly class-wise smart' if r 2 4r 1 implies E" 0 n jr 2 5E" 0 n jr 1  and E" 1 n jr 2 4E" 1 n jr 1 : Inwe observe that, if c 2 4c 1 , then for sufficiently small r, E" n c 2 jr4E" n c 1 jr, where the notation E" n cjrFor small r, the preponderance of data for classifier design is in class 1 with very little data in class 0. Hence, the class-0 error tends to be greater than the class-1 error, so that E" n c 2 jr  E" n c 1 jr  c 2  c 1 E" 0 n jr  E" 1 n jr40: 11. The first and the second rows show the expected true error rates of four classification rules when covariance matrices are identical and unequal, respectively. In these plots n 0  n 1  100 is fixed, where n 0 and n 1 are chosen according to the ratio r. The third and the fourth rows show the expected holdout error estimate of the datasets () and (), respectively, where n 0  80r d e and n 1  80  n 0 points are randomly selected from classes 0 and 1, respectively, as the training set. The rest of points are held out for error estimation computed via (9)Analogously, for sufficient large r, E" n c 2 jr  E" n c 1 jr50: 12We shall assume that whatever classification rule and featurelabel distribution we are considering, (11) and (12) hold for sufficiently small and sufficiently large r, respectively. The next lemma, whose proof is in the Supplementary Material, states a fundamental property of the error curves. LEMMA 3.2.1 If a classification rule is strictly class-wise smart relative to the family ff c x, yg, then, for c 2 4c 1 , E" n c 2 jr E" n c 1 jr is a strictly decreasing function of r.If we only assume class-wise smart, then E" n c 2 jr E" n c 1 jr would only be sure to be a decreasing function of r. The next lemma, whose proof is in the Supplementary Material, shows that constraining c results in a corresponding constraining of the expected error.To ease notation, we will say that E" n c 2 jr is between E" n c 1 jr and E" n c 3 jr if either E" n c 3 jr ! E" n c 2 jr ! E" n c 1 jr or E" n c 3 jr E" n c 2 jr E" n c 1 jr: The salient proposition concerning the error curves involves a strictly decreasing function g(r) of r that is positive for sufficiently small values of r and negative for sufficiently large values of r. Since r is a discrete variable in 0, 1, we have a sequence of values 05r 1 5. .. 5r n1 51: If g were continuous, then there would exist a unique value r  such that gr    0, gr40 for r5r  , and gr50 for r4r . But since g is discrete, this basic proposition is slightly altered. Rather, there are two possibilities: (i) there exists a unique value r   r j for some value j such that gr    0, gr40 for r5r  , and gr50 for r4r  or (ii) there is a unique value r j such that gr40 for r r j and gr50 for r ! r j1 : In the second case, we select a point r  2 r j , r j1 , say, the mid-point, and then we have gr40 for r5r  and gr50 for r4r  , as in the first case. In the next theorem, whose proof is in the Supplementary Material, we will be interested in a 'unique' point r  2 0, 1: For the second case, we interpret this to mean that there is a unique interval r j , r j1  and r  is the selected point in that interval. Given the preceding discrete interpretation, we shall say that a function pr 'crosses' function q(r) at r  if pr ! qr for r5r  and if pr qr for r4r  :This is precisely the theorem we want because it means that all error curves cross at r  : In the error curves of, we observe that r  provides a minimax value; that is, r mm  r  yields the minimum value of E" n cjr when taking the maximum error over all values of E" n cjr for r 2 0, 1 :where we must keep in mind that r 2 R  fr 1 ,. .. , r n1 g is a discrete variable. The next theorem, proven in the Supplementary Material, formalizes this observation. THEOREM 3.2.4 Consider a classification rule that is strictly class-wise smart relative to ff c x, yg and let r mm be the minimax value defined by (13). If Theorem 3.2.3 yields a unique point r   r j , then r mm  r j ; otherwise, if Theorem 3.2.3 yields an interval r j , r j1 , then either r j or r j1 is the minimax ratio, determined by
Practical implications of the error curvesRecall the practical implications we drew regardingin the Section 1: (i) if c is known, then do separate sampling with n 0  cn; where the equal sign means 'as close to cn as possible'; (ii) if c % c 0 , then for small n do separate sampling with n 0  c 0 n; (iii) if one has no idea regarding the value of c, then sampling must be random. Looking at the curves in(and similar figures in the Supplementary Material), we see that the curve for c has its minimum value at r  c or r  c 0 % c and, in the latter case, E" n cjr % E" n c 0 jr: Hence, the first two recommendations hold for the other classification rules examined. Going beyond the case where c is known or approximately known, consider the third implication, where one has no good idea concerning the value of c. Then the minimax r mm is an option. Its suitability depends upon the classification rule and feature-label distribution. As we can see from, except for extreme values of c, E" n cjr mm  tends not to be too much greater than E" n cjc: Of course, there is a practical problem: while we may well know the classification rule, we will not know the feature-label distribution.
Algorithm to approximate r mmAlgorithm 1 provides an iterative algorithm for approximating r mm when the feature-label distribution is unknown. The procedure is an empirical illustration of Theorem 3.2.4, which requires E" n cjr, which now needs to be approximated to approximate r mm : Algorithm 1 uses holdout error estimation. The expectation of this error estimate is taken by iterative random sampling from the dataset. Here we give a brief overview of the algorithm. The inputs to the algorithm are: dataset denoted by S N , classification rule, number of points to be held out for error estimation from classes 0 and 1, denoted, respectively, by n 0 test , and n 1 test and number of iterations, MaxIters, for computing the expected holdout error estimate. The maximum number of points after holding out test sample points is N new  N  n 0 test  n 1 test , denoted class-wise as N 0 new and N 1 new : The algorithm searches over possible values for r, from 0 to 1, until a stopping criterion is met. Suppose we fix the total sample size n. Then, considering the first extreme case, r  0, we need to have at least n points in class 1 to draw sample points from, randomly, i.e. N 1 new ! n: On the other hand, when r  1, we similarly should have N 0 new ! n: Hence, we should have n minfN 0 new , N 1 new g, whereby we set n  minfN 0 new , N 1 new g:The algorithm's search criterion is based on Theorem 3.2.4: in a 'while loop' over an increasing sequence of the ratios r, the algorithm computes the estimated slope of the expected error (as a function of c), this being slope new  E ^ " 0 n jr  E ^ " 1 n jr (line 22 of the algorithm), obtained by plugging the error estimates (lines 721 of the algorithm) into the unknown slope formula E" 0 n jr  E" 1 n jr: Because the classification rule is strictly classwise smart, for sufficiently small r, the slope is positive, and it becomes negative for sufficiently large r (refer to Supplementary Material file for further explanation). Once a point is reached at which the sign of the slope becomes non-positive, the 'while loop' stops increasing r. Thereafter, the three different possibilities given by Theorem 3.2.4 are checked, in lines 2632, and finally a single r mm is returned. Although the returned minimax ratio is only computed for sample size n defined above, the class-sizes can still be conservatively adjusted per r mm in the dataset S N because, for a ratio given by the algorithm, if one increases the sample size, then in the worst case the error is as large as the minimax value returned by the algorithm.
Adjusting sample sizesConsider the common situation in which n 0 and n 1 have been determined beforehand, but suppose one knows c. The curves ofstill apply but we are not free to choose n 0 and n 1 , so that we cannot choose n 0  cn. Nevertheless, we desire the training data to be apportioned according to c and we want to use as much data as is possible. These conditions mean that for training we want class sample sizes m 0 and m 1 such that m  m 0  m 1 is maximized given the constraints m 0  cm, m 1  1  cm, m 0 n 0 , and m 1 n 1 : The solution is to let m  minf n0 c , n1 1c g AE  : To see the effect of adjusting sample sizes, we consider the difference, r, c  E" n cjr  E" m cjc, between the expected true errors of two cases, n being the original sample size and m the adjusted sample size. When the sampling ratio is r and the true prior probability is c, r, c can be interpreted as the penalty incurred.shows r, c for L-SVM and RBFSVM for the equal covariance model described in. The result for the case with unequal covariance matrices can be found in the Supplementary Material. The two parameters r and c take values from 0.06 to 0.94 with the step size of 0.04. As expected, as jr  cj increases, r, c significantly increases. When r % c, r, c % 0, which is always the minimum. The figure shows that except when r is very close to c, r, c40, meaning that, even though m5n, a correct sampling ratio more than compensates for the loss of data due to subsampling.
Population-based minimax theoryThe minimax value in the error curves ofdepends on the sampling distribution and results from the fact that E" n cjr is minimized over c for a single value r  : In Anderson (1951), a population-based minimax approach was taken to arrive at a 'best' choice for ^ c in (1) in the Gaussian model with common covariance matrix under separate sampling. Here we extend the population-based mimimax approach to arrive at much more general solution than that given by Anderson. It is based upon the fact that the Bayes classifier can be determined via a discriminant involving the class-conditional densities. Anderson also utilized the Bayes classifier in his analysis but he restricted it to the Gaussian model with common covariance matrix, in which case the Bayes classifier is given by LDA using the actual parameters rather than their estimates as in (1). Given the class-conditional distributions and prior probabilities, the Bayes classifier, Bayes , is determined by the discriminantwhere Bayes x  1 if D Bayes x 0 and Bayes x  0 if D Bayes x40: The regions assigned to the two classes are R 1  fx : D Bayes x 0g and R 0  fx : D Bayes x40g: If c is unknown and replaced by ^ c, then the discriminant becomeswhich defines the classifier ^ c , with corresponding class regions R 1  ^ c  fx : D prior x 0g and R 0  ^ c  fx : D prior x40g. The error associated with ^ c isfor y 2 f0, 1g: R 1  ^ c and R 0  ^ c are strictly increasing and decreasing, respectively, for increasing values of log 1 ^ c ^ c : Hence, if the conditional densities are strictly positive, then " 1  ^ c and " 0  ^ c are strictly decreasing and increasing, respectively, for increasing values of log 1 ^ c ^ c : The minimax choice selects the value of ^ c that yields the minimum value of the error " ^ c, c when taking the maximum error over all values of c 2 0, 1 :We state a lemma and a theorem, whose proofs are given in the Supplementary Material that can be used to find minimax solutions. LEMMA 3.6.1 If the class-conditional distributions are strictly positive and " y  ^ c, y  0, 1, is a continuous function of ^ c, then there exists a unique point ^ c mm such that " 0  ^ c mm   " 1  ^ c mm  and this point corresponds to the minimax solution defined in (19). THEOREM 3.6.2 If the class-conditional distributions are strictly positive and " y  ^ c, y  0, 1, is a continuous function of ^ c, then the minimax solution for the discriminant in (16) is the value of c that gives rise to the maximum Bayes error for the discrimination problem of (15). To apply the lemma to the Gaussian model with common covariance matrix, note that the discriminant takes the formand the error of the classifier induced by D prior is given bywhere is the standard normal cumulative distribution function. It is immediate that for ^ c  0:5, " 0  ^ c  " 1  ^ c: Hence, ^ c mm  0:5, which is precisely Anderson's result for this special case. To illustrate the effect of the underlying feature-label distribution on ^ c mm , we consider a situation similar to that used for, except that we allow unequal covariance matrices.contains Bayes-error curves as functions of c for different covariance models. It shows that, except for a common covariance matrix, ^ c mm 6  0:5, ^ c mm being the value of c at which the curve attains its maximum. The curve for equal covariance matrices is constructed analytically; for other values, MC simulation is employed. Obviously, if ^ c mm  c, then the minimax value will perform well. But what happens when ^ c mm 6  c? In fact, the minimax value can work well so long at it is close to the true value, how close depending on the particulars of the problem. For a Gaussian model with common covariance matrix, as used for, we consider LDA with random sampling under three scenarios: (i) known c, (ii) minimax ^ c mm and (iii) the maximumlikelihood estimate ^ c ml  n0 n :shows the expected errors (MC estimates) as a function of c for n  20 and 80. In all cases, known c is the best. When the sample is small, ^ c mm outperforms(b). The parameter r, c for classification rules, L-SVM and RBFSVM trained on the sample data with size n  100 from the common covariance matrix model described in. Bayes error as a function of class prior probability c for different settings with multivariate Gaussian distributions with 0  0:3, 0:3 and 1  0:8, 0:8 ^ c ml for a fairly wide range of c, but this advantage disappears rapidly as n grows. The reason for this behavior is the difficulty of estimating c by ^ c ml for small samples. All curves show that when c is large or small the minimax solution gives poor results. Let us close this section by noting that the Bayes classifier is intrinsic to the feature-label distribution and, since the minimax choice depends only on the form of the Bayes classifier, it is independent of any particular classification rule.
Concluding remarksWe have shown, via simulations on both synthetic and real examples, that separate sampling with an inappropriate sampling ratio can significantly degrade classification accuracy for classification rules that do not use an explicit estimate of the prior probability. We have demonstrated some fundamental properties of the expected-error curves, developed the minimax samplebased theory for those curves, proposed an algorithm to approximate the minimax value in practice and extended the classical Anderson minimax theory for prior probabilities. We have provided heuristics on how to proceed when the prior probability is known (or known within a small range) and we have proposed a subsampling methodology to implement these heuristics when the class sample sizes are predetermined. Given the ubiquity of separate sampling in biomedicine, it would behoove the medical community to record incidence rates of patient sub-types (population statistics), so that very accurate estimates of class prior probabilities would be available. While this would certainly incur some cost, that cost would be minuscule compared to the costs incurred by the irreproducibility of classification studies. Conflict of Interest: none declared.
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
M.S.Esfahani and E.R.Dougherty at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Effect of separate sampling on classification accuracy at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
