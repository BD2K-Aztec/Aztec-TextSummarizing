Motivation: Accurately predicting the binding affinities of large sets of diverse proteinâ€“ligand complexes is an extremely challenging task. The scoring functions that attempt such computational prediction are essential for analysing the outputs of molecular docking, which in turn is an important technique for drug discovery, chemical biology and structural biology. Each scoring function assumes a predetermined theory-inspired functional form for the relationship between the variables that characterize the complex, which also include parameters fitted to experimental or simulation data and its predicted binding affinity. The inherent problem of this rigid approach is that it leads to poor predictivity for those complexes that do not conform to the modelling assumptions. Moreover, resampling strategies, such as cross-validation or bootstrapping, are still not systematically used to guard against the overfitting of calibration data in parameter estimation for scoring functions. Results: We propose a novel scoring function (RF-Score) that circumvents the need for problematic modelling assumptions via non-parametric machine learning. In particular, Random Forest was used to implicitly capture binding effects that are hard to model explicitly. RF-Score is compared with the state of the art on the demanding PDBbind benchmark. Results show that RF-Score is a very competitive scoring function. Importantly, RF-Score's performance was shown to improve dramatically with training set size and hence the future availability of more high-quality structural and interaction data is expected to lead to improved versions of RF-Score.
INTRODUCTIONMolecular docking is a computational technique that aims to predict whether and how a particular small molecule will stably bind to a target protein. It is an important component of many drug discovery projects when the structure of the protein is available. Although it is primarily used as a virtual screening tool, and subsequently for lead optimization purposes, there are also applications in target identification (). Beyond drug discovery, these bioactive molecules can be used as chemical probes to study the biochemical role of a particular target (). Furthermore, this technique can also be applied to a range of structural bioinformatics problems, such as protein function prediction (). Molecular docking has two stages: docking molecules into the target's binding site (pose identification), and predicting how strongly the docked conformation binds to the target (scoring). Whereas there are many relatively robust and accurate algorithms for pose identification, the imperfections of current scoring functions continue to be a major limiting factor for the reliability of docking (). Indeed, accurately predicting the binding affinities of large sets of diverse proteinligand complexes remains one of the most important and difficult unsolved problems in computational biomolecular science. Scoring functions are typically classified into three groups: force field, knowledge-based and empirical. Force field scoring functions parameterize the potential energy of a complex as a sum of energy terms arising from bonded and non-bonded interactions (). The functional form of each of these terms is characteristic of the particular force field, which in turn contains a number of parameters that are estimated from experimental data and computer simulations. These force fields were designed to model intermolecular potential energies, and thus do not account for entropy (). Knowledge-based scoring functions use the 3D co-ordinates of a large set of proteinligand complexes as a knowledge base. In this way, a putative proteinligand complex can be assessed on the basis of how similar its features are to those in the knowledge base. The features used are often the distributions of atomatom distances between protein and ligand in the complex. Features commonly observed in the knowledge base score favourably, whereas less frequently observed features score unfavourably. When these contributions are summed over all pairs of atoms in the complex, the resulting score is converted into a pseudo-energy function, typically through a reverse Boltzmann procedure, in order to provide an estimate of the binding affinity (e.g.). Some knowledgebased scoring functions now include parameters that are fitted to experimental binding affinities (e.g.) or introduce Information Theory-driven improvements as well as
P.J.Ballester and J.B.O.Mitchellexplicit solvent models (). Lastly, empirical scoring functions calculate the free energy of binding as a sum of contributing terms, each identified with a physicochemically distinct contribution to the binding free energy such as: hydrogen bonding, hydrophobic interactions, van der Waals interactions and the ligand's conformational entropy. Each of these terms is multiplied by a coefficient and the resulting parameters are estimated from binding affinities. In addition to scoring functions, there are other computational techniques, such as those based on molecular dynamics simulations, that provide a more accurate prediction of binding affinity. However, these expensive calculations remain impractical for the evaluation of large numbers of proteinligand complexes and are currently typically limited to family-specific simulations (). Scoring functions do not fully account for a number of physical processes that are important for molecular recognition, which in turn limits their ability to select and rank order small molecules by computed binding affinities. It is generally believed () that the two major sources of error in scoring functions are their limited description of protein flexibility and the implicit treatment of solvent. In addition to these enabling simplifications, there is an important issue that has received little attention so far. Each scoring function assumes a predetermined theory-inspired functional form for the relationship between the variables that characterize the complex, which also include a set of parameters that are fitted to experimental or simulation data, and its predicted binding affinity. The inherent problem of this rigid approach is that it leads to poor predictivity in those complexes that do not conform to the modelling assumptions. For instance, the van der Waals potential energy of non-bonded interactions in a complex is often modelled by a Lennard-Jones 12-6 function with parameters calibrated with experimental data. However, there could be many cases for which this particular functional form is not sufficiently accurate. Clearly, there is no strong theoretical reason to support the use of the r 12 repulsive term. Furthermore, while the r 6 attractive term can be shown to arise as a result of dispersion interactions between two isolated atoms, this does not include the significant higher order contributions to the dispersion energy, as well as the many-body effects that are present in proteinligand interactions (). Moreover, resampling strategies, such as cross-validation or bootstrapping, are still not systematically used to guard against the overfitting of calibration data in parameter estimation for scoring functions (). As an alternative to modelling assumptions in scoring functions, non-parametric machine learning can be used to implicitly capture binding effects that are hard to model explicitly. By not imposing any particular functional form for the scoring function, any possible kind of interaction can be directly inferred from experimental data. The first study of this kind that we are aware of () was based on the distance-dependent interaction frequencies between a set of predefined atom types observed in two separate modestly sized datasets. Kernel Partial Least Squares was trained on these data, and finally validated against several small external test sets (6 or 10 compounds). This study was a valuable proof-of-concept that machine learning can produce useful scoring functions. More recently (), support vector regression (SVR) was applied to produce family-specific scoring functions for five different proteinligand systems using datasets ranging from 26 to 72 complexes. Excellent correlation coefficients on the cross-validation data partitions were obtained. Importantly for the interpretability of data, Inductive Logic Programming was used in combination with SVR to derive a set of quantitative rules that can be used for hypothesis generation in drug lead optimization. In contrast to machine learning-based scoring functions, there has been much more research on machine learning approaches to Quantitative StructureActivity Relationships (QSAR). However, QSAR bioactivity predictions are exclusively based on ligand molecule properties. Hence, unlike scoring functions, QSAR performance is inherently limited by the fact that the information from the protein structure is not exploited. Here we present the first application of Random Forests (RFs) () to predicting proteinligand binding affinity. This machine learning technique has been previously applied to a number of related problems such as predicting proteinprotein interactions (), glycosylation sites () or binary classifying docking poses in target-specific studies (). The latter study also shares our ultimate goal of identifying bioactive molecules, although their approach is fundamentally different from ours in that it was not designed to deal with diverse proteinligand complexes and it does not exploit binding affinity data. RF is based on an ensemble of decision trees generated from bootstrap samples of training data, with predictions calculated by consensus over all trees. RF does not assume any a priori relationship between the descriptors that characterize the complex and binding data, and thus should be sufficiently flexible to account for the wide variety of binding mechanisms observed across diverse proteinligand complexes. RF is particularly suited for this task, as it has been shown () to perform very well in non-linear regression. In addition, RF can be also used to estimate variable importance as a way to identify those proteinligand contacts that contribute the most to the binding affinity prediction across known complexes. Lastly, the availability of substantially more data suggests that machine learning should now be an even more fruitful approach, leading to scoring functions with greater generality and prediction accuracy. The rest of the article is arranged as follows. Section 2 describes the benchmark used to validate scoring functions. Section 3 presents the scoring functions and experimental setup used in this study, with particular attention to RF. In Section 4, we will construct and study a RF-based scoring function (RF-Score). Lastly, in Section 5, we will present our conclusions as well as outline the future prospects of this promising class of scoring functions.
MATERIALS
Validation using the PDBbind benchmarkA number of studies (e.g.) have validated scoring functions based on their ability to predict the binding affinities of diverse proteinligand complexes. Indeed, since current algorithms are generally able to find poses that are close to the co-crystallized ligand, it makes sense to focus on the much harder scoring task so that the intrinsic properties of scoring functions are studied in isolation. Otherwise, confounding factors present in alternative enrichment validations such as the docking algorithm adopted, the particular target considered or the composition of the ligand and decoy sets could even lead to contradictory conclusions Page: 1171 11691175
Predicting proteinligand binding affinity(). Consequently, this approach permits a reliable assessment of the proposed function for re-scoring purposes. The PDBbind benchmark () is an excellent choice for validating generic scoring functions. It is based on the 2007 version of the PDBbind database (), which contains a particularly diverse collection of proteinligand complexes, assembled through a systematic mining of the entire Protein Data Bank (PDB;). The first step was to identify all the crystal structures formed exclusively by protein and ligand molecules. This excluded proteinprotein and proteinnucleic acid complexes, but not oligopeptide ligands as they do not normally form stable secondary structures by themselves and therefore may be considered as common organic molecules. Secondly, Wang et al. collected binding affinity data for these complexes from the literature. Emphasis was placed on reliability, as the PDBbind curators manually reviewed all binding affinities from the corresponding primary journal reference in the PDB. In order to generate a refined set suitable for validating scoring functions, the following conditions were additionally imposed by the curators. First, only complete and binary complex structures with a resolution of 2.5  or better were considered. Second, complexes were required to be non-covalently bound and without serious steric clashes. Third, only high-quality binding data were included. In particular, only complexes with known dissociation constants (K d ) or inhibition constants (K i ) were considered, leaving those complexes with assay-dependent IC 50 measurements out of the refined set. Also, because not all molecular modelling software can handle ligands with uncommon elements, only complexes with ligand molecules containing just the common heavy atoms (C, N, O, F, P, S, Cl, Br, I) were considered. In the 2007 PDBbind release, this process led to a refined set of 1300 proteinligand complexes with their corresponding binding affinities. Still, the refined set contains a higher proportion of complexes belonging to protein families that are overrepresented in the PDB. This is detrimental to the goal of identifying those generic scoring functions that will perform best over all known protein families. In order to minimize this bias, a core set was generated by clustering the refined set according to BLAST sequence similarity (a total of 65 clusters were obtained using a 90% similarity cutoff). For each cluster, the three complexes with the highest, median and lowest binding affinity were selected, so that the resulting set had a broad and fairly uniform binding affinity coverage. By construction, this core set is a large, diverse, unbiased, reliable and high-quality set of proteinligand complexes suitable for validating scoring functions. The PDBbind benchmark essentially consists of testing the predictions of scoring functions on the 2007 core set, which comprises 195 diverse complexes with measured binding affinities spanning more than 12 orders of magnitude.
METHODS
Intermolecular interaction featuresMachine learning-based regression techniques can be used to learn the non-linear relationship between the structure of the proteinligand complex and its binding affinity. This requires the characterization of each structure as a set of features relevant for binding affinity. In this work, each feature comprises the number of occurrences of a particular protein ligand atom type pair interacting within a certain distance range. Our main criterion for the selection of atom types was to generate features that are as dense as possible, while considering all the heavy atoms commonly observed in PDB complexes. As the number of proteinligand contacts is constant for a particular complex, the more atom types are considered the sparser the resulting features will be. Therefore, a minimal set of atom types was selected by considering atomic number only. Furthermore, a smaller set of intermolecular features has the additional advantage of leading to computationally faster scoring functions. However, this simple representation has the drawback of not allowing a direct interpretation in terms of which intermolecular interactions contribute the most to binding in a particular complex. More easily interpretable features would arise from the additional consideration of the atom's hybridization state and bonded neighbours. This is out of the scope of the present work, but it will be studied in detail in the future. Here we consider nine common elemental atom types for both the protein P and the ligand L:The occurrence count for a particular ji atom type pair is defined as:where d kl is the Euclidean distance between k-th protein atom of type j and the l-th ligand atom of type i calculated from the PDBbind structure; K j is the total number of protein atoms of type j and L i is the total number of ligand atoms of type i in the considered complex; Z is a function that returns the atomic number of an element and it is used to rename the feature with a mnemonic denomination; is the Heaviside step function that counts contacts within a d cutoff = 12  neighbourhood of the given ligand atom. For example, x 7,8 is the number of occurrences of protein nitrogen hypothetically interacting with a ligand oxygen within a 12  neighbourhood. This cut-off distance was suggested in PMF () as sufficiently large to implicitly capture solvation effects, although no claim about the optimality of this choice is made. This representation leads to a total of 81 features, of which 45 are necessarily zero across PDBbind complexes due to the lack of proteinogenic amino acids with F, P, Cl, Br and I atoms. Therefore, each complex will be characterized by a vector with 36 features:x 16,6 , .
.. ,x 16,53 )  36On the other hand, the binding affinities uniformly spanned many orders of magnitude and are hence log-transformed. We merge K d and K i measurements in a single binding constant K, as this increments the amount of data that can be used to train the machine learning algorithm and preliminary tests showed no significant performance gain from making such a distinction (data not shown). By applying this process to a group of N complexes, the following pre-processed dataset will be obtained:
RFs for regressionRF is an ensemble of many different decision trees randomly generated from the same training data. RF trains its constituent trees using the CART algorithm (). As the learning ability of an ensemble of trees improves with the diversity of the trees (), RF promotes diverse trees by introducing the following modifications in tree training. First, instead of using the same data, RF grows each tree without pruning from a bootstrap sample of the training data (i.e. a new set of Ncomplexes is randomly selected with replacement from the N training complexes, so that each tree grows to learn a closely related but slightly different version of the training data). Second, instead of using all features, RF selects the best split at each node of the tree from a typically small number (m try ) of randomly chosen features. This subset changes at each node, but the same value of m try is used for every node of each of the P trees in the ensemble. RF performance does not vary significantly with P beyond a certain threshold
P.J.Ballester and J.B.O.Mitchell(e.g.) and thus we subscribe to the common practice of using P = 500 as a sufficiently large number of trees. In contrast, m try has some influence on performance and thus constitutes the only tuning parameter of the RF algorithm. In regression problems, the RF prediction is made by averaging the individual predictions T p of all the trees in the forest. Thus, in our case, the binding affinity of a given complex x (n) is predicted by RF as:The performance of each tree on predicting out-of-bag (OOB) data, that is complexes not selected in the bootstrap sample and thus not used to grow that tree, gives an internal validation of RF. OOB is a fast resampling strategy carried out in parallel to RF training that yields estimates of prediction accuracy that are very similar to those derived from more computationally expensive k-fold cross-validations (). The mean square error (MSE) expressed in terms of the OOB samples is:comprises the indices of those complexes that were not used for training the p-th regression tree and |I OOB p | is the cardinal of such set. Possible m try values cover all the feature subset sizes up to the number of features ({2,  ,36} in our case), which gives rise to a family of 35 RF models. It is expected that the m try value with best internal validation on OOB data, i.e. data not used for training, will also provide the best generalization to independent test datasets. Thus, the selected RF predictor is:RF has also a built-in tool to measure the importance of individual features across the training set based on the process of 'noising up'. For each feature, this consists of randomly permuting its values across OOB samples for the current tree and evaluating the MSE of these perturbed data (MSE OOB j). The higher the increase in error (MSE OOB jMSE OOB ), the more important the j-th feature will be for binding affinity prediction.
Scoring functions for comparative assessmentA comparative assessment of 16 well-established scoring functions, implemented in mainstream commercial software or released by academic research groups, was very recently carried out (). In our study, we will be using these scoring functions to assess the performance of RF-Score relative to the state of the art. Five scoring functions in the Discovery Studio software version 2.0 (): LigScore (), PLP (), PMF (), Jain () and LUDI (). Five scoring functions (D-Score, PMF-Score, G-Score, ChemScore and F-Score) in the SYBYL software version 7.2 (). GlideScore () in the Schrdinger software version 8.0 (). Three scoring functions in the GOLD software version 3.2 (): GoldScore, ChemScore () and ASP (). In addition, two stand-alone scoring functions released by academic groups, that is, DrugScore () and X-Score version 1.2 (). Several of these scoring functions have different versions or multiple options, including LigScore (LigScore1 and LigScore2); PLP (PLP1 and PLP2) and LUDI (LUDI1, LUDI2 and LUDI3) in Discovery Studio; GlideScore (GlideScore-SP and GlideScore-XP) in the Schrdinger software; DrugScore (Drug-Score PDB and DrugScore CSD ); and X-Score (HPScore, HMScore and HSScore). However, for the sake of practicality, only the version/option of each scoring function that performed best on the PDBbind benchmark was considered in. We will also restrict our scope here to the best version/option of each scoring function, as listed in Supplementary Table S1 (Appendix A3 in Supplementary Material).
RESULTS AND DISCUSSION
Building RF-ScoreThe process of training RF to provide a new scoring function (RF-Score) starts by separating the 195 complexes of the core set from the remaining 1105 complexes in the refined set. The former constitutes the test set of the PDBbind benchmark, while the latter is used here as training data. Consequently, the training and test sets do not have complexes in common. Next, each of these sets is preprocessed, as explained in Section 3.1 and implemented in the C code provided in the Supplementary Material. Thereafter, the protocol detailed in Section 3.2 is followed using the training dataset only (this is implemented in the R code provided in the Supplementary Material). As a result, it was found that the RF model with the best generalization to internal validation data corresponded to m best = 5, which obtained an error of RMSE OOB = 1.52 (square root of the MSE OOB ). RF-Score is therefore defined as:RF-Score reproduces the training data with very high accuracy.shows the correlation between measured and predicted binding affinities. This is quantified through Pearson's correlation coefficient (R), defined as the ratio of the covariance of both variables over the product of their standard deviations (SDs). In this training set, R = 0.953, indicating a very high linear dependence between these variables over the training data. Another commonly reported performance measure is the root mean square error (RMSE):Indeed, RMSE is practically the same as the SD used elsewhere (e.g.), specially for large sets such as this (with N = 1105, both RMSE and SD are 0.74 log K units on the training set). The performance on the OOB samples (R OOB = 0.699 and RMSE OOB = 1.52) is a more realistic and useful estimation of RF-Score's predictive accuracy, since merely fitting the training set does not constitute prediction.shows the increase in error
Predicting proteinligand binding affinity
RF-Score on the PDBbind benchmarkRF-Score is next tested on an independent external test set. This constitutes a real-world application of the developed scoring function, where the goal is to predict the binding affinity of a diverse set of proteinligand complexes not used for training/calibration, feature/descriptor selection or model selection. RF-Score predicts binding affinity for test complexes with high accuracy (R = 0.776, RMSE = 1.58 log K units; see). The OOB estimates are close to the performance obtained on the test set, which further supports the usefulness of this validation approach. There is also the question of how much of the predictive ability of RF-Score is due to learning the true relationship between the atomic-level description of structures and their binding affinities. To investigate this, we destroyed any such relationship in the training set by performing a random permutation of y-data (binding affinities), while leaving the intermolecular features untouched. Thereafter, the training process in Section 3.2 was carried out again with this modified data and the resulting RF-Score function used to predict the test set. Over 10 independent trials, performance on the test set was on average R =0.018 with standard deviation S R = 0.095 (average RMSE = 2.42 with S RMSE = 0.04). These results demonstrate the negligible contribution of chance correlation to RF-Score's prediction ability. Such y-scrambling validation is very useful in the validation of QSAR studies (e.g.), where an optimal set of features is selected over a very large poolR, R s and RMSE are evaluated over the test set. R s is the Spearman's rank-correlation coefficient, which measures here the ability of a scoring function to predict the correct ranking of complexes according to binding affinity. RMSE = RMSE  RMSE OOB of not always relevant molecular (ligand) descriptors and thus the likelihood of chance correlation is much higher. Importantly for the resulting prediction's accuracy and generality, we were able to train and validate RF-Score with an unusually large and diverse set of high-quality data. This was possible because RF is sufficiently flexible to effectively assimilate large volumes of training data. We have trained and validated RF-Score with randomly chosen differently sized subsets of the training data (see). Results show that RF-Score's performance on the test set improves dramatically with increasing training set size (N train ). This strongly suggests that ongoing efforts to compile and curate additional experimental data will be of great importance in improving generic scoring functions further. Also, as expected, the RMSE OOB generalization estimate becomes more accurate, i.e. closer to RMSE on the test set, as the training set, and thus the validation set, grows. This is reflected in the RMSE values.
Comparison with the state of the artA wide selection of scoring functions has very recently been tested against the PDBbind benchmark (). These scoring functions are listed in Section 3.3, with references to their original papers.functions along with that obtained in the previous section by RFScore. Results show that RF-Score obtains the best performance among the tested scoring functions on this benchmark. The performance results for the other 16 scoring functions shown inwere extracted from. This procedure has a number of advantages. First, it ensured that all scoring functions are objectively compared on the same test set under the same conditions. Like Cheng et al., we consider that a fair comparison of scoring functions requires a common benchmark. Second, by using an existing benchmark, the danger of constructing a benchmark complementary to our own scoring function is avoided. The latter would lead to unrealistically high performance and thus to poor generalization to other test datasets. Third, the results reported incorrespond to the version/option of each scoring function that performed best on the PDBbind benchmark. Most importantly, thanks to the team maintaining the PDBbind database, future scoring functions can be unambiguously incorporated into this comparative assessment. Moreover, the free availability of RF-Score codes permits the reproduction of our results and facilitates application of RF-Score to other sets of proteinligand complexes. Lastly, it could be argued that RF-Score's performance is somehow artificially enhanced by its training set being related to the test set by the non-redundant sampling explained in Section 2.1. The rationale would be that the other scoring functions could have used training sets chosen without any reference to the test set. Actually, unlike RF-Score, top scoring functions such as X-Score::HMScore, DrugScore CSD , SYBYL::ChemScore and DS::PLP1 have a number of training complexes in common with this test set (). In order to investigate whether these overlaps could provide scoring functions with an advantage, the second best performing function in, X-Score::HMScore, was recalibrated by its authors using exactly the same 1105 training complexes as RF-Score in Section 4.1 (i.e. ensuring that training and test sets have no complexes in common). This gave rise to X-Score::HMScore v1.3, which obtained practically the same performance as v1.2 in Table 2 (R = 0.649 versus R = 0.644). Since RF-Score and XScore::HMScore v1.3 used exactly the same training set and were tested on exactly the same test set, this result also means that all the performance gain (R = 0.776 versus R = 0.649) is guaranteed to come from the scoring function characteristics, ruling out any influence of using different training sets on performance. Additional experiments exploring the effects of varying the training and test sets are included in the Supplementary Material.
P.J.Ballester and J.B.O.Mitchell
CONCLUSIONSWe have presented a new scoring function called RF-Score. RF-Score was constructed in an entirely data-driven manner by circumventing the need for problematic modelling assumptions via non-parametric machine learning. RF-Score has been shown to be particularly effective as a re-scoring function and can be used for virtual screening and lead optimization purposes. It is very encouraging that this initial version has already obtained a high correlation with measured binding affinities on such a diverse test set. In the future, we plan to study the use of distance-dependent features, which could result in further performance improvements given that the strength of intermolecular interactions naturally depends on atomic separation. Also, less coarse atom types will be investigated by considering the atom's hybridization state and bonding environment. This will enhance the interpretability of features in terms of the intermolecular interactions. Admittedly, interpretability is currently a drawback of this approach. However, it is important to realize that, although the terms comprising modelbased scoring functions provide a description of proteinligand binding, such a description is only as good as the accuracy of the scoring function. Lastly, machine learning-based scoring functions constitute an effective way to assimilate the fast growing volume of high-quality structural and interaction data in the public domain and are expected to lead to more accurate and general predictions of binding affinity.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org 1169 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
