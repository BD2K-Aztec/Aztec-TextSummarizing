Motivation: Alignment of similar whole genomes is often performed using anchors given by the maximal exact matches (MEMs) between their sequences. In spite of significant amount of research on this problem, the computation of MEMs for large genomes remains a challenging problem. The leading current algorithms employ full text indexes, the sparse suffix array giving the best results. Still, their memory requirements are high, the parallelization is not very efficient, and they cannot handle very large genomes. Results: We present a new algorithm, efficient computation of MEMs (E-MEM) that does not use full text indexes. Our algorithm uses much less space and is highly amenable to parallelization. It can compute all MEMs of minimum length 100 between the whole human and mouse genomes on a 12 core machine in 10 min and 2 GB of memory; the required memory can be as low as 600 MB. It can run efficiently gen-omes of any size. Extensive testing and comparison with currently best algorithms is provided. Availability and implementation: The source code of E-MEM is freely available at:
INTRODUCTIONMaximal exact matches (MEMs) are exact matches between two sequences that cannot be extended either way without introducing mismatches. MEM computation is a fundamental problem in stringology () and has important applications in sequence alignment. Closely related genomes are often aligned by using local similarities as anchors () and sufficiently long MEMs have been quite successfully used in this respect (). A theoretically optimal solution, in linear time and space, for the MEM computation problem is easily obtained using suffix trees (). However, suffix trees require large memory and practical implementations use highly engineered suffix trees (). Suffix arrays were introduced by Manber and Myers (1993) as a space-efficient alternative to suffix trees and have replaced them in most applications. Enhanced suffix arrays were shown to solve all problems suffix trees could solve with the same theoretical complexity (). Suffix arrays still use a significant amount of memory, especially with the additional tables required to match the complexity of suffix trees (). When whole genomes are aligned, the memory required by the computation of all MEMs may become prohibitively high. The large popularity of whole-genome alignment programs, most notably that of the MUMmer software (), attracted a lot of attention to the MEM computation problem, with the purpose of enabling the alignment of larger genomes within reasonable amount of memory. For example, one of the most reliable such programs, Vmatch (), uses enhanced suffix arrays and its memory usage is very high. The idea of sparseness has been already used for suffix trees (K arkk ainen and) and it has been successfully employed for suffix arrays byin their sparseMEM program. Their approach relies on indexing only every kth suffix of the given genome sequence (k is called sparseness factor) and is able to find MEMs faster and using less memory than previous approaches. It can serve as a drop-in replacement for the MUMmer3 software package (). The approach of sparseMEM has been enhanced bywith a sparse child array for large sparseness factors and implemented in their essaMEM software. Our tests show that essaMEM is currently the best program for MEM computation in large genomes. Compressed indexes () have been used as well.developed backwardMEM that uses a backward search method over a compressed suffix array. Recently, Fernandes and Freitas (2013) employed in slaMEM a new sampled representation of the longest common prefix (LCP) array that works with the backward search method of the FM-Index (). In spite of these advances, MEM computation remains a challenging problem for large genomes. The memory requirements of the current approaches remain high and very large genomes cannot be effectively handled. We present a new algorithm, E-MEM that targets large genome sequences. E-MEM does not use full text indexes. Instead, hash tables are efficiently used in combination with several ideas to speed up the search. Our algorithm uses much less space and is highly amenable to parallelization. For example, it can compute all MEMs of *To whom correspondence should be addressed.  The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com minimum length 100 between the whole human and mouse genomes on a 12-core machine in 10 min and 2 GB of memory; the required memory can be as low as 600 MB. It can run efficiently on genomes of any size. Extensive testing and comparison with currently best algorithms is provided. We have used for comparison traditional datasets, such as whole human versus mouse genomes and whole human versus chimp genomes, but also introduced a new test where two species of wheat, Triticum aestivum and Triticum durum are used. It turns out that only E-MEM and Vmatch could handle these genomes. However, Vmatch requires 57 GB whereas E-MEM can use less than 1 GB while being also faster. Our E-MEM software is implemented in C++ and OpenMP, is freely available, and can be used as a stand-alone program or as a drop-in replacement for the MUMmer3 software package ().
METHODSAssume we have two genomes, the reference R, of length jRj=n, and the query Q, of length jQj=m. The character of R at position i is Ri and the substring of R starting at position i and ending at j is denoted Ri::j; we have also that R=R1::n. A k-mer is a string of length k. A maximal exact match (MEM) between R and Q is a match that cannot be extended on either side. Formally, it is a quadruple b r ; e r ; b q ; e q  such that Rb r ::e r =Qb q ::e q  and Rb r  1 6  Qb q  1; Re r +1 6  Qe q +1, if defined; e r  b r +1 is the length of the MEM. The MEM finding problem is: given two sequences R and Q and an integer L, find all MEMs of length at least L between R and Q.
Hashing the referenceThe starting idea of our approach is that, for any MEM b r ; e r ; b q ; e q  of length L or more between R and Q, there must be a k-mer in R starting at a position that is a multiple of L  k+1 that is completely contained in the MEM, that is, b r jL  k+1 e r  k+1, for some j ! 1. That means, it is sufficient to index all k-mers in the reference that start at positions that are multiples of L  k+1, which significantly reduces the required memory. However, all k-mers of the query genome have to be processed and this must be done very efficiently. Each genome is encoded using two bits per nucleotide and then stored as an array of unsigned 64-bit integers, that is, as blocks of 32 nucleotides. We use k-mers of size 28 or less for technical reasons described below. Each k-mer starting at a position that is a multiple of L  k+1 is computed by a bit-and operation between a mask of 2k 1's and the appropriate elements of the array storing R. All k-mers are hashed using double hashing. In the hash table, each entry stores the value of the k-mer and a list of all positions where the k-mer occurs in R. All numbers are unsigned 64-bit integers, thus able to handle genomes of any size.
Searching the queryAs mentioned above, all k-mers in Q need to be considered. However, they are not indexed but simply computed from Q and searched for in the hash table we built for R. Every time a k-mer of Q is found in the hash table, all positions at which it occurs in R are investigated for possible extension. If the extension has length L or more, then it is reported as a MEM. This way all MEMs are found. The idea is now clear but a straightforward implementation is very slow. First, we need a fast way to compute the k-mers of Q. This is done by bit operations; we slide a 64-bit window across Q and each k-mer is computed by a bit-and between the window and a mask of 2k 1's. The window is then updated by shifting its content two bits. Every four shifts, another byte is added, for that reason the k-mer size has to be at most 28. Second, we need to check each extension very fast. Recall that the genome sequences are stored in arrays of 64 bit blocks. The extension is performed in such a way that two 64-bit blocks are compared using only very few bit operations. Third, some MEMs may be rediscovered many times, thus decreasing the speed. In order to avoid MEM rediscovery, we use a data structure, CurrMEMs, that stores the current MEMs. Given the current position j in Q, the current MEMs are those whose query part covers j, that is, all MEMs b r ; e r ; b q ; e q  such that b q j e q. Any new MEM found is added to CurrMEMs. When the current position j moves past e q , the corresponding MEM is removed from CurrMEMs. MEM rediscovery verification is done as follows. Any time a k-mer w at position j in Q is found in the hash table of R, then, for each position ' in the list of w in the hash table, we check first whether the k-mer pair j; j+k  1; '; '+k  1 lies inside an already discovered MEM. For each current MEM, b r ; e r ; b q ; e q , we need only check whether j  b q ='  b r. In practice, the number of current MEMs is never very high and thus the algorithm advances very fast through Q.
Unknown basesGenome sequences may contain unknown bases, denoted by N. Our twobit encoding does not allow storing N's so we replace them with random bases. Therefore, we need to eliminate any accidental matches between randomly replaced N's and real bases. Storing the positions of N's would be too space consuming, so we store them as blocks of N's, recording only their beginning and end. During the MEMs discovery process, we check whether any intersection with a block of N's occurred in R or Q. For Q this is very simple, since we know, for the current position in Q, where the closest blocks of N's on both sides are. For R, we need to verify, for each MEM found, that no N position has been included.
Reducing the memory furtherSo far the algorithm works very well, with little space and time. However, we need to store both sequences R and Q to enable comparison, which gives a lower bound for the required peak memory. In order to avoid this problem and reduce the memory further, we split each of R and Q sequences into D ! 1 subsequences each; D is called division factor. However, we cannot simply cut them into D equal subsequences since this may prevent us from finding some MEMs straddling the borders. The D sequences must overlap. For R (similarly for Q), each subsequence has length '= 1 D jRj+D  1L and the overlap between consecutive subsequences is L  1. This way, any MEM must have at least L bases within a single subsequence and will be discovered. Since we build hash tables on the reference, as explained above, we consider the subsequences of R in order, one at the time, build its hash table, then, for each subsequence of Q, find all MEMs shared by the two subsequences. The time increases since we process each subsequence of Q multiple times, but the space decreases very much with D. A different problem that appears now is that we may discover only parts of the MEMs straddling borders. That is, some of the matches that reach the borders of the subsequences may not be maximal. This happens because we load into memory only one subsequence from each genome, and hence no match can be extended past the end of either subsequence. Matches reaching the ends of the subsequences are stored together in a data structure, MEMext, and processed at the end. If some of these are parts of the same, larger, MEM, then they are merged accordingly.
ParallelismThe above algorithm can be easily parallelized on any number of cores. Each subsequence of the query genome is divided equally into a number of pieces equal to the number of processors. Each processor will then execute the algorithm we have described independently of the others. MEM's straddling these new borders may be discovered more than once and duplicates are removed at the end. Also, each thread has its own CurrMEMs data structure that it has to compute from scratch. Nevertheless, a speed up of up to six times faster is obtained on the 12-core machine that we used for testing.
Very large number of MEMsFor large genomes that are highly similar, the number of MEMs can be very large. For example, for human and chimp, the number of MEMs is well over a 100 million. Keeping all these MEMs in memory at the same time will reverse all the memory reductions we obtained so far, not to mention a significant amount of time necessary to sort them. To avoid these problems, we store the MEMs in files, sorted according to their starting positions in the query genome. At the end, each file is sorted, duplicates are removed, and MEMs are output in the right order.input: two sequences R and Q and a minimum MEM length L output: all MEMs of length at least L between R and Q 1. choose a division factor
Pseudocode7. while ' jR i j  k+1 do 8. hash the k-mer at position ' of R i 9. ' '+L  k+1 10. for j from 1 to D do 11. encode Q j 12. for ' from 1 to jQ j j  k+1 do 13. get the k-mer q at position ' in Q j 14. search for k-mers r = q in the hash table of R i 15. for each occurrence of r with extension ! L do 16. check CurrMEMs 17. if ((q, r) discovers a new MEM) then 18. if (MEM at ends of R i or Q j ) then
RESULTSWe have compared E-MEM with several programs, including the top ones: essaMEM (), slaMEM (), sparseMEM (), and Vmatch (). We have also tested backwardMEM () and MUMmer () but they could not run any of our large tests. The genomes involved in the tests are given in, where we give also their length and number of sequences included in each fasta file. Download information for each genome sequence as well as for the source code of the programs tested is given in the Supplementary Material. Note that the wheat genome has $17 GB, however the available sequence has only 4.3 GB. We performed three tests. The first two are classical problems of large genome alignment: human versus mouse, human versus chimp. We have added two larger genomes of two wheat species: common wheat versus durum wheat. The results are presented in Tables 24, respectively. In each table, we include the time andNote: A dash means the program could not run that test, that is, in serial mode, the output was incorrect or empty, whereas the parallel mode was not supported.of all programs were run; details are given in the Supplementary Material. Since we are trying to reduce both time and memory, a trade off is obtained and the results are better seen when time is plotted against memory. We show these plots in, 2 and 3. For the first two tests, we give two plots for clarity, one for the serial mode, the other for the parallel.
Human versus mouseIn our first test, the whole human and mouse genomes have 537 491 MEMs of minimum length 100. E-MEM produces much better results than the other programs; see. In serial mode, the smallest memory obtained by the other programs was that of slaMEM, with 3.5 GB. However, slaMEM took 17 h to complete the test. Therefore, the best performance comes from essaMEM, which can use as low as 4 GB of memory, but complete the test in 2.5 h; essaMEM can complete the test in little over one hour but at the cost of increasing the memory to 19 GB. E-MEM can run within as little as 623MB and complete in less than two hours or in half an hour and 4 GB of memory. E-MEM has the best speed up in parallel, between 5 and 6 times on our 12-core machine. The speed up of essaMEM varies between 1.3 and 4.5 times. We note that sparseMEM has speed up comparable to that of E-MEM but the running time in serial mode is one order of magnitude higher. The best result from the competing programs comes again from essaMEM, that has been consistently outperforming the other ones: 23 min and 6 GB or 1 h and 5.4 GB. E-MEM can complete the test in 6 min and 4.7 GB or 10 minutes and less than 2 GB. The time and space are plotted against each other in. Programs closer to origin are faster and require less memory. The left plot is for serial mode and the right for parallel. The area is very large in the serial plot, due to the large memory requirements of Vmatch and long running times required by slaMEM and sparseMEM. Since the two programs do not run in parallel, the plot giving the time/space values for the parallel testing gives a more clear picture. Only essaMEM, sparseMEM, and E-MEM could run in parallel. The memory required for the same sparseness factor by essaMEM is slightly smaller than that of sparseMEM while essaMEM is much faster than sparseMEM. All three programs trade time for space but the results of E-MEM are much closer to origin than the rest; see.
Human versus chimpFor our second test, human versus chimp, there are 132 368 058 MEMs of minimum length 100 to be found. This slows down. The whole picture, given inis similar with the one for the first test, human versus mouse () except that the difference between the running times is not as large.
Two wheat speciesIn our last test, we used two wheat genomes that are larger than the mammalian ones used in the previous two tests. Only E-MEM and Vmatch could run this test and the results are presented inand plotted in. Note that this figure is slightly different thanin that both serial and parallel results are presented in the same plot. E-MEM could run this test efficiently; in serial mode it can use less than 1GB of memory and in parallel it requires only 16 min and 2GB of memory.
CONCLUSIONE-MEM provides a practical solution for finding MEMs between arbitrarily large genomes. It uses much less memory than the currently available programs. It is freely available and it can be used as a stand-alone program or as a drop-in replacement for the MUMmer3 software package (). MEMs are good anchors for closely related genomes. Otherwise, approximate matches are more suitable. The approach of E-MEM can be generalized to work with spaced seeds () inorder to search efficiently for approximate matches. Highly sensitive multiple spaced seeds () of large weight are necessary and they can be designed using the approach of Ilie and Ilie (2007) by the SpEED program (). The exact matching procedure of index-based algorithms is not well suited for finding approximate matchings.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
N.Khiste and L.Ilie at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Efficient computation of maximal exact matches at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
