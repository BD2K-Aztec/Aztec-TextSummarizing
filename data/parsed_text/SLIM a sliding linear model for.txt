Motivation: The pre-estimate of the proportion of null hypotheses (π 0) plays a critical role in controlling false discovery rate (FDR) in multiple hypothesis testing. However, hidden complex dependence structures of many genomics datasets distort the distribution of p-values, rendering existing π 0 estimators less effective. Results: From the basic non-linear model of the q-value method, we developed a simple linear algorithm to probe local dependence blocks. We uncovered a non-static relationship between tests' p-values and their corresponding q-values that is influenced by data structure and π 0. Using an optimization framework, these findings were exploited to devise a Sliding Linear Model (SLIM) to more reliably estimate π 0 under dependence. When tested on a number of simulation datasets with varying data dependence structures and on microarray data, SLIM was found to be robust in estimating π 0 against dependence. The accuracy of its π 0 estimation suggests that SLIM can be used as a stand-alone tool for prediction of significant tests. Availability: The R code of the proposed method is available at
INTRODUCTIONExperiments in the 'omics' fields often involve hundreds to thousands of dependent variables, such as those encountered in genetic linkage (), gene expression () and metabolic profiling (). Multiple testing correction is necessary in order to identify statistically significant variables for subsequent analyses, without a flood of false positives called by chance. The false discovery rate (FDR) approach () and its many variants, including the positive FDR (pFDR)-based q-value statistic (), have been widely used for false discovery control in multiple hypothesis testing. The q-value represents the minimum pFDR that can occur for any possible  greater than or equal to a p-value point. Given a set of p-values ranked in an increasing order, p i ,i = 1,2,...,m (m is the total number of tests), * To whom correspondence should be addressed. the q-value is calculated as:This formula indicates that  0 is the only unknown parameter to be pre-estimated. The accuracy of the  0 estimate directly affects the q-value calculation and the optimal control of FDR. A reliable  0 estimate also provides a simple prediction of the number of genes that are differentially expressed under the experimental conditions in gene expression analysis. A widely used method is the -estimator (), i.e.where   (0,1) is a pre-chosen cutoff and #{p i >} is the number of p-values greater than . Considering the non-linear relationship between  0 () and , we refer to this estimator as the non-linear model. The underlying assumption is that the largest p-values are most likely to come from a uniform distribution of null features in the range (0,1). In practice, there is a bias versus variance tradeoff for choosing an optimal  for the estimation of  0 (). To balance this trade-off, Storey and Tibshirani (2003) used a natural cubic spline smoothing (CSS) method to fit the nonlinear relationship across a range of , as implemented in the QVALUE software.proposed an average estimate (AE) method, which takes advantage of multiple non-linear estimators to reduce the  0 estimation variance. Markitsis and Lai (2010) recently proposed a censored beta mixture model (CBMM), based upon the beta-uniform mixture (BUM) method of Pounds and, to approximate the p-value distribution. Both the original and the q-value-based FDR procedures were developed for independent test statistics. Although these methods can be applied to weakly dependent data (), they are unreliable for datasets with inherent multiplicity and complex dependence (). The BUM-based methods also cannot effectively handle irregular p-value distribution (). To specifically handle multiple testing under dependence, Efron (2007a) developed an empirical Bayesian framework, called Locfdr, to remedy the effects of data correlation. However, Locfdr is only applicable to data with a large (0.9)  0 (). We have developed a linear  0 estimator from the non-linear method of Storey (2002) to explore local properties of p-value distributions as a means to better capture data dependence. When applied to data with a uniform null p-value distribution, the slope and intercept of the linear model reflect the proportions of null
H.-Q.Wang et al.( 0 ) and alternative hypotheses ( 1 ), respectively. Based on this framework, we devised a sliding linear model (SLIM) for estimating  0 that can be applied over a broader range of p-value distributions. An interrelationship between p-values and q-values was observed and exploited to optimize SLIM. We compared the performance of SLIM with that of four other methods on three types of simulated datasets mimicking various p-value distribution scenarios and data dependence structures, as well as on one Populus microarray experiment. The results show that SLIM performs better than the previous methods under the conditions examined.
GENERATION OF SIMULATION DATASETSConsidering that dependence often leads to a distorted p-value distribution, we generated two simulated datasets with uniform and non-uniform null p-values, referred to as uniform and non-uniform datasets, respectively, to investigate the effect of various p-value distributions on  0 estimation. The third simulated dataset mimics microarray gene expression data by explicitly adding dependence structures.
Uniform and non-uniform datasetsThe uniform and non-uniform datasets were produced using a procedure modified from Storey (2002). For the former, we set m = 10 000, and generated m 0 null hypotheses from a normal distribution with mean  0 = 0 and SD  = 1, as well as m(1 0 ) alternative hypotheses from a normal distribution with  1 = 5 and  = 1. We varied  0 among 0.10.9 to track estimation performance. The non-uniform datasets were similarly generated except that the null hypotheses comprised a mixture of two normal distributions: one with  0 =1 and  = 1 and the other,  0 = 1 and  = 1. The mixture coefficient was set to  = 0.5. The p-values for each dataset were calculated assuming a normal null distribution N(0,1).
Gene expression simulation datasetsWe followed the procedure ofto produce the microarray gene expression simulation data, consisting of two experimental conditions with sample sizes of n 1 and n 2 , and G =10 000 gene probes. To add hidden dependence structures, a correlation background X[Gn(n = n 1 +n 2 )] was generated by (i) randomly selecting clump size m from {1,2,...,100} and clump-wise correlation  from U(0.5,1). For a given (m,) pair, we (ii) generated noise vectors e j of dimension m1 from(iii) set x j = +diag()e j as the background expression values of the m genes in the clump at sample j = 1, 2, ..., n = n 1 +n 2 , where  is an m  1 vector of elements  g  1000 2 5 and  is an m  1 vector of elementsg , and  0 and  1 are two constants for all G genes. In this exercise, we set  0 =5,  1 = 2 and n 1 = n 2 = 6. Differential expression data were generated by setting  1 G as the number of genes differentially expressed between the two conditions, with one half upregulated and the other half downregulated. Steps (i) through (iii) were repeated to form a correlation background for the  1 G regulated genes in the n samples. We then added (or subtracted) a term 2 1/2  g  g to (from) the samples of one of the two conditions, where the coefficient  g was sampled from a uniform distribution U(5,10) such that the true expression ratio is 1+2 1/2 e  0 /2  g  UFinally, we randomly replaced  1 G rows of the background X with the  1 G differentially expressed genes, and varied  0 between 0.1 and 0.9 to simulate various data configurations, with 1000 iterations each. For each 'gene', we calculated the p-value for differential expression between the two conditions using the moderated t-statistic ().
THE LINEAR MODEL-BASED FRAMEWORK FOR  0 ESTIMATIONLet #{p i  } be the number of hypotheses with p-values less than a cutoff , we transform the Storey's non-linear model to:Next, let y and x substitute #{p i  } and m, respectively, and Equation (3) can simply be rewritten as:where  1 = 1 0 is just the proportion of the alternative hypotheses. Equation(4) clearly reveals that the total number of hypotheses called significant with a p-value cutoff  comprises two portions: one associated with null hypotheses (false positive),  0 x [or (m) 0 ], and the other with alternative hypotheses (true positive), m 1. Let  = y m be the proportion of hypotheses called significant by , Equation (4) then becomes:The associated (,) plot actually represents the cumulative probability distribution (CPD) of p-values (). Intuitively, from Equation (5) one may linearly fit the CPD of p-values over a proper range, and calculate the slope of this fitting line as the estimated  0. Without loss of generality, given a range of ,, the  0 estimation can be written as:
Sliding linear model estimation of  0where  s and  e represent the cumulative probabilities at  s and  e , respectively, and 0.05 is set according to the conventional p-value cutoff for statistical significance. For data with a uniform p-value distribution, Equation (6) may be applied directly for  0 estimation. This is shown in, using the uniform simulation dataset. We obtained a linear fitting curve via Equation (6) over a range  [0.7,1] (), and a nonlinear fitting curve using the CSS algorithm () with default parameters (). Given a true  0 of 0.4, Equation (6) takes advantage of the data linearity on the right side of the (,) plot to yield a   0 = 0.4089, which is more accurate thanthan than 0 = 0.4219 generated by CSS. Large variation of  0 () at  [0.8,1] makes fitting the non-linear model more error prone. Equation(6) only uses the partial information in, e.g.in, for the  0 estimation, and will be inadequate to handle datasets with non-uniform p-value distributions. In order to retain as much information as possible about the null hypotheses' p-value distribution, we devised the following strategy for the estimation of  0. We first divide the (,) plot into a series of -segments (S). Thus, n segments can be formed as:We then linearly regress  by  using Equation (6) for each segment and obtain n local estimates ofof of i,2,...,n, in accordance with Equation (7). In view of the overall null p-value distribution being a mixture of local distributions, we estimate  0 as:where D represents the cumulative distribution function ofof of i 0 , D 1 represents its inverse (i.e. quantile) function, and 0    1 represents a given quantile point. The selection of an appropriate value of  will be addressed in Section 4.2. This  0 estimator, which we termed the sliding linear model (SLIM), considers the collective effect of null p-values over the majority of their distribution range. Therefore, SLIM should be able to deal effectively with null p-values having a complex distribution pattern.
PARAMETERS OF THE SLIM ESTIMATORIn this section, we first consider some properties of the q-value and their implications for the development of SLIM. We then discuss the optimization of SLIM parameters for  0 estimation.
Properties of q-valuesBased on Equation (1), the CPD curves of p-values and q-values in the (,) plot intersect at  = p z , where z =  0 m. This is illustrated inand B (gray solid arrows) using the uniform simulation datasets with various  0 scenarios. We observe that the q-value of a given test is larger than p-value (i.e.This non-static relationship suggests that the common use of an arbitrary q-value cutoff, often at the same level as the p-value or FDR cutoff, is not universally appropriate. Given  0 , let p max be the maximum p-value among the truly significant (as opposed to called significant) tests; we infer that there exists a corresponding, hidden maximum FDR, denoted by FDR max. That is, no false negatives are encountered at p max , and all errorsare due to false positives, estimated p max  0. The FDR max thus can be written as:Taking p max = 0.05 as an example, we can calculate the FDR max by Equation(9) for different  0 scenarios (Supplementary). Since the rank of p max is  1 m+p max  0 m, the q-value at p max becomes q = FDR max according to Equation (1). Therefore, the FDR max can be taken as the maximum q-value among all alternative tests. This means that the two hypothesis testing criteria, p-value and q-value, will call the same set of tests significant at their respective cutoffs, p max and FDR max , because the q-value procedure does not change the order of hypotheses ranked by p-values. Let L be the absolute difference between the fractions of tests called significant by the two (p-value and q-value) methods; we have L = 0 at the true value of  0. Supplementarygives a geometric interpretation of the relationship in the (,)
H.-Q.Wang et al.plot.and B illustrates the relationship for uniform data: at  = FDR max , the CPD of q-values has a  intercept same as that of the CPD of p-values at  = p max (green horizontal dashed lines). For non-uniform data, the irregular distribution of null pvalues can skew this relationship, as shown for three  0 scenarios at p max = 0.05, wher  0 at the minimum L deviated from the true values (). However, given a proper p max (e.g. 0.03, 0.02 and 0.01 for the three  0 scenarios, respectively), the relationship held again, and the minimum L = 0 gave accurate  0 estimates (). Extensive testing showed that L always reached a minimum around the true  0 over a range of p max (Supplementary). Together, the data suggest that the observed relationship between q-and p-values holds irrespective of data structure, and can be exploited to guide the parameter tuning of SLIM.
Parameter tuning of the SLIM estimatorThe SLIM depends on three parameters:  1 , n and .  1 specifies the lower boundary of the first sliding segment used for  0 estimation across the  axis of the (,) plot. Considering that the smaller p-values are most likely to come from alternative hypotheses, a small  1 is preferred to maximize the range of data coverage while minimizing the influence of true positives on  0 estimation. Analysis with various simulation datasets and  0 scenarios showed that the mean relative errors (MREs) of  0 estimates reached a minimum at  1 < 0.2 (Supplementary). We therefore set  1 = 0.1 as default. The parameter n specifies the number of -segments, and influences how the distribution of null p-values is estimated. For data encompassing a complex p-value distribution, a sufficiently large n is necessary to capture clusters of similar p-values. However, an overly large n may lead to smaller segments with abrupt changes in slope and increase  0 estimation errors. We have found n = 10 to be generally robust against a range of data scenarios. The quantile parameter  captures the collective effect of the n local  0 estimates from the sliding segments, thus playing a crucial role in the global  0 estimate by SLIM. Following the relationship between p-and q-values, the selection of a proper  can be solved as an optimization problem, i.e.wherewhere where  q = #{q i < FDR max }/m andand and  p = #{p i < p max }/m represent the fraction of tests called significant by q-values and p-values, respectively, underunder under  0 = D 1 (). Equation (10) aims to choose the optimal  by minimizing the difference L betwee   q andand and  p at a given p max to approximate the global  0. We devised an searching procedure to solve the optimization problem. We first form a candidate set of , A ={ i = 1 B i,i = 1,2,...,B}, from which a proper  will be selected. To gain sufficient granularity, we set B to be no less than 100. For each  i , we then calculatand the difference L betwee   q andand and  p using  0 =    0 and a given p max (see below). Finally, thethe the with the minimum L is chosen as the final value of . We examined the effect of varying p max on  0 estimation using simulated uniform, non-uniform and gene expression data. Although the results varied depending on data structure and  0 scenario, too large (>0.6) and too small (<0.01) a p max tended to give rise to large MREs ofof of 0 , based on 1000 random iterations (Supplementary). In most cases, the MRE was less than 0.1 for a p max between 0.01 and 0.1, suggesting that SLIM is relatively insensitive to p max (i.e. a true p max is not necessary in practice, since it is an unknown property). We recommend setting p max at 0.05 as default. For further optimization, users may wish to test multiple p max and examine the CPD curves of p-values and q-values in the resultant (,) plots, as illustrated in, in order to select an optimal p max. Overall, we transform the  optimization problem into two user-definable and insensitive parameters B and p max , thereby simplifying the implementation of SLIM. A procedural framework for SLIM is depicted in Supplementary.
SIMULATION EXPERIMENTSWe applied SLIM to the three types of simulation datasets from Section 2 in comparison with four other methods: CSS (), AE (), CBMM () and Locfdr (Efron, 2007a).summarizes the mean errors and SD of  0 estimates for each of the five methods across nine  0 scenarios on the uniform and nonuniform data. The results show that SLIM achieved overall lower mean errors for  0 estimation than the other four methods, especially for the non-uniform data. CBMM also worked well, but exhibited a decreasing accuracy as  0 increases. As reported by, Locfdr is applicable only to datasets with a large  0 , and it was indeed most accurate for the non-uniform data with  0 = 0.9 (). CSS and AE worked reasonably well with uniform data,
Uniform and non-uniform simulation data
Sliding linear model estimation of  0but lost their accuracy for non-uniform data, suggesting their general deficiency in dealing with complex p-value distributions.also reports the range of  determined by the optimization scheme of Equation (10):   was around the mid-range (0.3 to 0.8) for the uniform data, and trended toward larger values (0.7 to 0.9) for the non-uniform data. Based on one of the non-uniform datasets ( 0 = 0.8), we compared the computation cost of SLIM and three other previous methods (CSS did not work for this dataset). The CPU time (in second) was 1.1 (SLIM), 13.00 (AE), 0.17 (Locfdr) and 0.50 (CBMM) using an intelCore2 Duo 3 GHz processor with 3 GB of RAM and the Microsoft Windows XP operating system. The result suggests that SLIM is not computationally demanding.
Gene expression simulation data with hidden dependence structuresThe simulated gene expression analysis showed that SLIM outperformed the other four methods based on the mean errors and SDs ofof of 0 (). We calculated the number of alternative hypotheses predicted by each method, i.e. (11 1 0 )10 000, as a proxy of the number of differentially expressed (DE) genes for each  0 scenario. Because the true null and alternative hypotheses are known, the false positive rate (FPR), false negative rate (FNR), FDR and accuracy of each method can be evaluated. SLIM achieved anoverall low FPR, FNR and FDR (). The ability of SLIM to balance between false positive and false negative errors led to an overall high accuracy (0.99). The previous methods suffered from various trade-offs. For instance, CBMM generated the lowest FNR in most scenarios at the expense of higher FPR and FDR, leading to reduced accuracy compared with SLIM. Locfdr was effective for large  0 scenarios, producing a low level of FPR and FDR in these cases, but at the expense of higher FNR and an overall low accuracy. These results demonstrate the power and robustness of SLIM in coping with data dependence structures. We also observed that the  1 (= 1 0 ) estimate of SLIM may be used as a cutoff for DE gene selection with sufficient FDR control and a high level of accuracy (). In practice, the FDR max of the maximum p-value among the (11 1 0 )m DE genes may be taken as the FDR for these DE genes. For the simulation data, the FDR max values of SLIM-called DE genes were found to be very close to the actual FDR (most of the differences were around 0.001). To illustrate the influence of hidden dependence structures on the  0 estimation, representative p-value distributions of the three types of simulation data were shown in histograms and (,) plots. Compared with the uniform data (), the non-uniform data have a skewed distribution of null hypotheses (), which complicates  0 estimation. The addition of dependence structures (G) led to various irregular p-value distribution patterns. These differences in histograms are reflected in the CPD curves of p-values in the (,) plot (). The non-uniform data show sinusoidal deviations from the straight line generated by the uniform data, while the gene expression simulation datasets are intermediate in shape between those for the uniform and non-uniform data. These comparisons help explain why  0 estimation methods developed on the assumptions of data independence and uniformity do not work as effectively for datasets containing dependence structures.
APPLICATION TO REAL-WORLD GENE EXPRESSION DATASETSTo test SLIM in practice, we examined two Affymetrix microarray datasets from a Populus stress response experiment (GEO no. GSE14515,). The experiment monitored gene
Sliding linear model estimation of  0can further reduce the list of DE genes, if deemed necessary, by other criteria, such as q-value, FDR or fold-change cutoffs.
DISCUSSIONAn important issue in multiple hypothesis testing is how to deal with the dependencies hidden among thousands of tests. Efron (2007b) has shown that correlation among variables considerably changes the theoretical null distribution patterns. We also observed that dependence structures lead to distorted distributions of null p-values, and this likely underlies the relatively large  0 estimation errors by methods developed under the assumption of data independence. The Locfdr approach () was specifically designed to handle data containing dependence structures, but it is only applicable when  0 is large. CBMM uses a censored beta-uniform mixture model to fit the distorted p-value distribution, alleviating to some degree the difficulty caused by dependence. SLIM is based on a linear model transformed from the non-linear  estimator (). The superior performance of SLIM can be ascribed to its data partitioning and optimization schemes. SLIM uses a sliding linear model to partition data into local dependence blocks. This reduces data complexity, while enabling SLIM to utilize information from a broader range of p-value distribution for  0 estimation. Using simulated data, we uncovered a non-static relationship between p-values and q-values of a given set of tests that is influenced by data structure and  0 scenarios. SLIM employs an optimization scheme to explicitly exploit this relationship by minimizing the difference (L) between the fractions of tests called significant by the p-value and q-value methods. The optimization scheme is particularly important to balance between positive and negative errors, thereby achieving FDR control. Thus, SLIM effectively handles hidden dependence without the need to empirically adjust the null p-value distributions. The selection of a proper q-value cutoff in multiple hypothesis testing is not trivial, especially given the dependence of the q-value calculation on the  0 estimation. Recalling that the number of significant tests in a given experiment is simply  1 m, we argue that an accurate estimation of  0 can serve as an alternative to q-value-based significance testing. Using simulated data, SLIM was shown to outperform the other methods, achieving the lowest FDR overall, accompanied by the highest degree of accuracy in declaring significant tests. This suggests that SLIM can be used as a stand-alone tool in multiple testing for determination of significant tests. In summary, SLIM is a robust estimator especially suited for datasets with non-uniform p-value distribution patterns due to data dependence. SLIM is computationally efficient and easy to implement. It requires four user-selected parameters, n,  1 , p max and B (the latter two are proxies of the quantile parameter ). We recommend n = 10,  1 = 0.1, B = 100 and p max = 0.05 as the default settings. Users may wish to test a higher B to ensure sufficient granularity, and a range of p max for optimal  selection and  0 estimate. In addition to microarray analysis, SLIM has been applied to metabolite profiling analysis in our laboratory, and should be applicable to a wide range of experiments
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
