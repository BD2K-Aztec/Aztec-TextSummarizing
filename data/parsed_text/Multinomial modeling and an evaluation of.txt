Motivation: A principal objective of pharmacovigilance is to detect adverse drug reactions that are unknown or novel in terms of their clinical severity or frequency. One method is through inspection of spontaneous reporting system databases, which consist of millions of reports of patients experiencing adverse effects while taking one or more drugs. For such large databases, there is an increasing need for quantitative and automated screening tools to assist drug safety professionals in identifying drugâ€“event combinations (DECs) worthy of further investigation. Existing algorithms can effectively identify problematic DECs when the frequencies are high. However these algorithms perform differently for low-frequency DECs. Results: In this work, we provide a method based on the multinomial distribution that identifies signals of disproportionate reporting, especially for low-frequency combinations. In addition, we comprehensively compare the performance of commonly used algorithms with the new approach. Simulation results demonstrate the advantages of the proposed method, and analysis of the Adverse Event Reporting System data shows that the proposed method can help detect interesting signals. Furthermore, we suggest that these methods be used to identify DECs that occur significantly less frequently than expected, thus identifying potential alternative indications for these drugs. We provide an empirical example that demonstrates the importance of exploring underexpected DECs. Availability: Code to implement the proposed method is available in R on request from the corresponding authors.
INTRODUCTIONOver the past 50 years, pharmacovigilance has evolved from a small limited-scale data collection and evaluation process involving scientific rationale and debate () to a large world-wide systematic data collection process, which now additionally includes extensive statistical analysis (). As data collection and statistical evaluation of adverse events have become more systematic, the ultimate goal has remained the same: to identify, as soon as possible, drugadverse event combinations (DECs) that pose a significant risk to the population (). Although there has been a manyfold improvement in collecting spontaneous reports, the penultimate goal is extremely difficult to achieve. For one, spontaneous reporting systems (SRSs) are far from perfect and contain both error and bias. Known sources of error include incorrect association between drugs and events, over-reporting (multiple reports for the same incident) and under-reporting (events that are never reported) (). In addition, SRS databases are subject to reporting bias [over-reporting of DECs on publicity of a suspected association (. Furthermore, SRS databases lack exposure information, thus implying that the event reporting rates derived from databases can only be considered as relative. Yet, despite the noise and bias present in SRS databases, it is still possible to use this data to identify many unusually large reporting frequencies, which are also known as signals of disproportionate reporting (SDRs) (). However, it is extremely important to realize that these SDRs, at best, indicate associations that are potentially causal and that should be scientifically evaluated. Moreover, an SDR represents a numerical output devoid of clinical context and is not equivalent to a signal of suspected causality (). In addition, SRS data are more akin to a census, rather than an unbiased sample from an underlying true population of adverse event reports. Therefore 'estimates' and corresponding 'confidence intervals' of SDRs stemming from any data-mining algorithm (DMA) should really be viewed as 'pseudo-estimates' and 'pseudo-intervals'. Many statistical approaches, also known as DMAs, have been applied and developed to find SDRs, especially in the less-thandesirable conditions that SRS databases present. DMAs fall into two categories: traditional (or frequentist) methods and Bayesian methods (). Each of these approaches provides distinct advantages and inherent disadvantages (). The primary advantages of frequentist approaches are that they are simple to compute, are easy to interpret and have higher sensitivity than current Bayesian methods when comparing common implementations. However, the underlying model assumptions fail for low-count drugadverse *To whom correspondence should be addressed.  The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com event combinations, which represent a majority of DECs in SRS databases. Under these conditions, frequentist signal detection methods can become unstable (i.e. the increased detection of signals is accompanied by an increased detection of noise) and unreliable. Bayesian methods, in contrast, attempt to stabilize the resulting ratio metrics for low-count DECs via shrinkage. However, these approaches have been shown to be less sensitive for detecting SDRs for low-count combinations, thus implying that they can overshrink (). Bayesian approaches are also less intuitive and more computationally intensive than frequentist approaches. Regardless of approach, no one method has been shown to be superior to others at identifying unusual DECs (), and the lack of both uniformly accepted gold standards of causality and a calculus of costs and utilities associated with correct and incorrect classifications in pharmacovigilance complicates head-to-head comparisons. This should not be surprising, given the inherent messiness of the data. Furthermore, it has been shown that frequentist and Bayesian approaches produce similar results for higher-count DECs (). Finally, neither frequentist nor Bayesian approaches optimally handle low-count DECs, which is where pharmacovigilance desires to first identify signal. In this work, we introduce a method based on a multinomial model for estimating the degree of interaction between a drug and an adverse event. The resulting score is standardized using a nonparametric approach, which avoids the asymptotic pitfalls that frequentist parametric methods must assume in low-count cases. Furthermore, we compare the multinomial approach and common DMAs on the metrics of shrinkage and scoring of DECs. These comprehensive results can enable the practitioner to better assess the relevance of DECs. Last, we show that these methods can be used to identify DECs that occur significantly less frequently than expected. Although traditional pharmacovigilance ignores this direction of the test, we advocate that this direction should not be ignored because these results may suggest potential alternative indications of medicines. Specifically, large negative scores could imply that a drug may be beneficial for a specific event. This possibility is accommodated by a recently proposed definition of signal (). Our work is organized as follows: in Section 2, we review several DMAs and explain the caveats of each. In addition, we propose an empirical approach based on a multinomial model and illustrate how this approach avoids the pitfalls of frequentist approaches. In Section 3, we compare each method with the proposed method on a subset of the Adverse Event Reporting System (AERS) data. Then in Section 3.2, we show how these signal detection methods can be used to identify potential alternative indications for individual drugs or classes of drugs. Finally, we summarize and discuss these results in Section 4.
METHODSMany authors have used one or more DMAs to identify signal in SRS databases (), whereas others have contrasted performance of DMAs at identifying known signal (). vanprovided an extensive comparison of the performance of several DMAs on identifying signal and concluded that common DMAs perform similarly at identifying DEC signal when there are at least four reports for the combination. In this section, we will further explore why common DMAs perform similarly when there are sufficient numbers of reports. However, this analysis did not include one of the commonly used algorithms, the Multi-item Gamma Poisson Shrinker (GPS). In a later section, we will extend on the results presented by van, including the results from our proposed approach.
Existing signal detection methodsTo begin, we will outline the general problem and corresponding notation, and we will define common DMAs and provide references for more in-depth information. First, given a specific drug and adverse effect combination, let a represent the number of reports for the drug and adverse event combination, let b be the number of reports for individuals taking the drug but having other adverse events, let c be the number of reports for individuals with the adverse event but not taking the drug and let d be the number of reports excluding the drug and adverse event. Then the particular drugadverse event combination can be represented by the following 2  2Existing algorithms for detecting signal are based on the association between drugs and events of the above 2  2 table. Common frequentist methods include the reporting odds ratio (ROR) and the proportional reporting ratio (PRR) (), and are often viewed on the log scale log ROR  log ad bc log PRR   log a c  d   c a  b   Although these two methods are easy to use and interpret, there are some practical constraints that occur for many DECs. Specifically, for ROR to be defined, b and c must be greater than zero; similarly, for PRR to be defined, a  b and c must be greater than zero. To determine the significance of the magnitude of any of the above measures of association, each signal should be evaluated relative to its standard error. The corresponding asymptotic estimates of variability for these methods areand depend on sufficient cell counts to be reliable (). It is easy to see that a, b, c and d must be greater than zero for SE log ROR    to be defined; a and c must be greater than zero for SE log PRR    to be defined.Bayesian methods do not suffer from these kinds of constraints and include the GPS and Bayesian Confidence Propagation Neural Network (BCPNN) (), and are based on estimating the information criterionThe posterior expectation of the IC via BCPNN iswhere  n  2 a  b  1  n  2 a  c  1 , and the corresponding estimate of variability isOne can see from the above formulas that E IC   and SE IC   are defined when a, b, c or d are zero, which is a distinct advantage of this method, especially when trying to detect signal in sparse combinations. When log ROR   and log PRR   are defined, we can use the above formulas to compare the estimates of variability and their corresponding estimates of variability under common scenarios. For a majority of DECs in SRS databases, a ( b, c ( d % n. Under this scenario, a  b % b, a  c % c and n  x % n, where x is a small integer. Applying these approximations, one can show the following:and(2) appear to be similar in value, they are not when a ( b, c ( d. In fact, as E(a) decreases, E(IC) decreases more rapidly than log ROR  or log PRR  owing to the additional 1 in the denominator of (2). When a is small, E a   is usually much smaller than 1, which produces large values of log ROR   and log PRR   and a much smaller value of E IC  : Therefore, the BCPNN method is less sensitive to pharmacovigilance signal for low-count DECs, which often occurs for more recently approved drugs or for drugs that have low exposure to the populationtwo key scenarios where pharmacovigilance desires to identify signal. When a ( b, c ( d % n and using the above approximations, one can show that the estimates of variability arewhere is on the order of max b 1 , c 1   : Hence, the estimates of variability are similar under this common DEC scenario and depend on the magnitude of a. In a small number of cases, a4b, and b is small (possibly close to 0). Under this condition,where is on the order of c 1 : In the small number of cases when a4b, the standard errors depend more strongly on the magnitude of b. Alternatively, when a4b,The error for PRR thus depends on the magnitude of a 2. Because a is greater than b, SE log PRR     will be smaller than SE log ROR     and SE IC  . This implies that the standardized PRR will be large when a4b and b approaches zero. The mathematical setup of ROR, PRR and BCPNN make it easy to identify scenarios that cause these methods to yield different signal and noise values. GPS, in contrast, does not nicely fit into a form that allows for the same kind of succinct mathematical comparison. Below, we provide the forms of the signal and error for GPS, which we will then use to make an empirical comparison across these methods in Section 3. The GPS method assumes a Poisson distribution for the observed counts for the i-th drug and the j-th adverse event, a ij $ Poisson ij  E ij , where E ij  dij bijcij is the expected counts for drug i and event j if there is no association. The parameter ij characterizes the association between drug i and adverse event j. ij larger than 1 means there is positive association, and ij smaller than 1 means there is negative association. The GPS method takes a Bayesian framework, which models the parameter ij as a random variable, with prior distribution  ij  as a mixed gamma distributionfor i  1,. .. , I and j  1,. .. , J, where Gaj,   1 e  is the gamma distribution with parameter and. Given the distribution of the observed counts and the prior distribution of the parameter, we can compute the posterior distribution for the parameter ijwhere ~ Q is the posterior probability that the parameter ij follows the first gamma distribution. We havewhere NBaj, p is the negative binomial distribution NBaj, p  C n1 n p n 1  p 1 Then the GPS score is the mean value of ij divided by the standard error of ij :To summarize, although ROR and PRR are easy to compute and interpret, their usefulness is somewhat limited. Specifically, cell counts must be sufficiently large for these estimates of independence and corresponding variability to be stable for rare DECs. BCPNN does not suffer from these constraints, but shrinks the expected IC more harshly as the expected value of a decreases. This means that BCPNN will not be able to detect signal for drugs that have low exposure to the population. In the following section, we propose an empirical nonparametric method for estimating variability that can yield more stable results than the above historical methods. Furthermore, it retains the advantages of each of the above methods while avoiding the disadvantages.
Multinomial modeling of SRS databasesFor notational simplicity, letThen the probability of the ij th DEC can be represented using the saturated model: IC. In (3), i, j represents the effect not accounted for by the i-th drug or j-th event. This residual information can also be viewed as the effect due to the interaction between the i-th drug and j-th event. If the ij-th DEC does not interact, then i, j 0; however, if i, j 6  0, the combination of drug i and event j has a synergistic or antagonistic effect on the probability of the ij-th DEC. To understand the significance of the magnitude of i, j , we must first understand the variation of this measure. Assume that drugs (rows) and adverse events (columns) are independent and that the counts from an I  J table follow a multinomial distribution:Then it can be shown that given the marginal probability P i,  and P , j , the standard error of i, j is approximately(see Supplementary Material for derivation), and increases as the marginal probabilities of either drug or event decrease, which usually occurs for extremely low-count DECs. Although this approximation works well when all expected cell counts are large, it overestimates the true variation when expected cell counts become small because it is conditioned on the expected counts. Specifically, as P i,  and/or P , j become small, the standard error increases rapidly. A better estimate of variation when cell counts are small can be obtained from a simulated independent table by conditioning on observed counts. Let fN Ind 1, 1 ,. .. , N Ind i, j ,. .. , N Ind I, J g be the counts from an independent multinomial distribution. fN Ind 1, 1 ,. .. , N Ind i, j ,. .. , N Ind I, J g $ MultinomialP 1, 1 ,. .. , P i, j ,. .. , P I, J , N whare P i,j  P i, * P ,j The saturated model (3) is then applied to the simulated independent table, and i, j is computed for all i and j. The i, j s estimated from this simulated data provide a null distribution or reference range under the assumption of no interaction between drug and event, and can be used to better assess relative magnitude of observed i, j s. Now consider estimating ij variation by conditioning on the observed counts from the simulated independent table as follows:This approach avoids using the marginal distribution and instead calculates the standard error based on the observed counts, thus making the standard error estimate more stable, especially in very sparse datasets. As a result of this approach, DECs having the same observed counts will have the same standard error estimate.compares the theoretical and empirical standard errors for a real dataset (see Section 3 for description). For DECs with count greater than five, the standard errors are nearly identical. However, for combinations with five or fewer occurrences, the empirical standard error is less than the theoretical standard error and is a better reflection of the true variation.
Detecting signalFor any drug and adverse event, consider estimating the relative significance of ij by standardizing ij with its standard errorThe threshold of significance for standardized gamma (SG) is chosen based on the null distribution of  i, j estimated from an independent table. Let T be the threshold for SG,where is the user-specified level, and quantileC,  is the-th quantile of the vector C. This means under independence that 1  percentage of all the drugadverse event combinations will exceed the threshold T .highlights the top 1% of DECs for a real dataset (see Section 3 for description).
RESULTS
Method comparisonTo better understand how each method performs, standardized PRR, BCPNN, GPS and SG are applied to a real datasetextracted from the Food and Drug Administration's AERS database. We used a commercially available version from Oracle Health Sciences in which the duplicated records of reports are removed and the drug names are standardized. This enables us to calculate accurate counts of reports per DEC. To form the dataset, we extracted all reports with adverse events in the cardiac preferred term category from 1968 to 2009Q2, as cardiac is one of the major categories in drug-induced adverse events. We did not use age, gender or year of report as stratification variables for any of the methods, which is common practice for pharmacovigilance problems. Instead, the comparisons within this text are based on the aggregated data to enable a direct scoring comparison among methods. Computing was done in R, and the PhViD package was used to compute BCPNN and GPS scores. The BCPNN and GPS implementations in PhViD are close representations of the commercially available versions of these algorithms. Code to compute SG scores is available on request from the corresponding author. Consider, which illustrates the standardized PRR, BCPNN, GPS and SG values for the DECs with counts less than 50. This figure confirms the characteristics noted in Section 2. Specifically, for the DECs with small counts (a 10), the standardized PRR method tends to generate a number of large scores, whereas the BCPNN method tends to more strongly shrink scores. In the case where b is 0, the PRR standard errors are small (on the order of c 1=2 ), which inflates the standardized score for PRR (4100). In contrast, when a is small, BCPNN method shrinks the scores toward the prior information that is 0. If we use the 99th quantile as the threshold, many DECs with counts less than 10 will be detected as significant by the standardized PRR method only because those drugs do not have much exposure (e.g. b is small). Any DECs with counts less than 10 will not be detected as significant by the BCPNN method only because they do not have enough observed counts (a 10). For GPS and SG, the distribution of the scores is more homogeneous across all observed counts. This enables a universal threshold for identifying interesting DECs regardless of observed counts.further highlights the contrasts among methods for low-count DECs. Standardized PRR inflates the scores when b equals 0 and has good agreement with SG otherwise. BCPNN shrinks scores toward 0 as a approaches 0, but has better agreement with SG as a increases. SG and GPS have most agreement, but differ slightly for counts between 1 and 4. We further compared GPS and SG using concordance correlation and found a coefficient of 0.98, indicating that SG should be comparable with one widely used algorithm, GPS, for signal detection in pharmacovigilance. We also looked at the signal identification capabilities of SG in a specific adverse event example of drug-induced immune thrombocytopenia (DITP). DITP can be a serious adverse risk, which has been associated with a number of drugs (Al). To evaluate DITP, SG scores were generated for all DECs at the higher-level term (HLT) level. Count data for all individual reports were extracted from the cumulative AERS 2011Q2 release of Oracle Health Sciences Empirica Signal database. To generate the HLT counts, individual reports mapping to multiple terms were collapsed to single counts at the corresponding HLT-level term. HLT drugevent counts were used to generate SG scores as described above. Using case report data from Reese et al., which provides drug-specific level-of-evidence categories in the Supplementary Table S1 (), 70 drugs with published case reports for thrombocytopenia were identified and an additional 1164 drugs with no case reports or where the case report clinical data did not support evidence for a causal association with thrombocytopenia were also identified. The average SG score for drugs with positive and negative case reports were compared. The average SG score for the positive drugs was 10.87 and for the negative drugs was 0.76, with a Student's t-test P-value of 0.013. Non-parametric analyses were also significant.
Negative gammaTraditional pharmacovigilance approaches, as described above, desire to detect adverse events that are a detriment to the population. In addition, these methods can also be used to detect DECs that have a significant negative association (e.g. events. Scatter plots of SG versus standardized PRR, BCPNN and GPS scores for counts between 1 and 100. Grey dots represent counts between 1 and 10 and black pluses represent counts between 11 and 100 that occur much less frequently than expected). This information can be used to hypothesize about possible alternative indications for existing single medicines or to identify classes of drugs that have potentially beneficial effects. To evaluate drugs for potential alternative indications, the above SG scores were used; in addition, Z-scores were generated for all drugs in a given event. Z-scores were then converted to cumulative probability P-values. An examination of the resulting SG scores revealed that two non-steroidal anti-inflammatory drugs have very low scores for mood-related events. For example, aspirin has an SG score of 14.48 for 'Anxiety symptoms' (). SG scores for aspirin were also very low for a number of other mood-related events. Another Cox-2 inhibitor, diclofenac, had similarly low SGs for these events. The potential value of Cox-2 inhibitors in the treatment of mood disorders has been evaluated by a number of groups (). Aspirin inhibits Cox-2, which in turn participates in the metabolism of prostaglandins and arachidonic acids (). Arachidonic acid is known to play an important part in nerve signal transmission, and alterations in this pathway are thought to be involved in bipolar disorder (). Individuals suffering from bipolar disorder have severe mood swings and often need to take lithium to stabilize their mood ().have reported that aspirin coadministered with lithium can result in a statistically significant reduction in frequency of emotional incidents. Other workers have found that aspirin can shorten the time to onset of effectiveness of antidepressant compounds (). Together, these results demonstrate that aspirin can have dramatic mood effects in some situations. Recently, dopaminergic compounds have resurfaced as potential therapeutics in the treatment of leukemia (). Although the mechanism behind this effect has not been worked out, it may be related to dopamine's known modifying effects on prolactin production, which, in turn, has been shown to be linked to leukemia (). To determine whether negative gamma scores could also be used to identify drug-class alternative indications, a set of dopaminergic agents was identified from MESH using the query 'Dopamine Agents', and drug names with exact matches to the identified compounds were flagged. Of 3056 compounds, 55 were classified as dopaminergic by this method. The average SD scores for the leukemia-related events were calculated and compared for all dopaminergic compounds versus all remaining compounds. For every leukemia event at the HLT level, the average SD score for the dopaminergic agents was statistically lower than that for the remaining compounds. Statistical significance was measured by the Student's t-test (). The results of this analysis demonstrate that negative gammas can be used to identify potential new drug-class indications. We have also used this approach to reconfirm the widely reported melanoma-protective effects of some non-steroidal anti-inflammatory drugs (data not shown).Although the mathematical comparisons and contrasts presented in Section 2 are true regardless of application, the research presented in this work does have a few limitations from the pharmacovigilance perspective, and opportunities for future research exist. First, we did not use a gold standard database containing accepted drugevent signal and adverse event terminology. Therefore, we cannot use the results above to make a declarative statement about the superiority of any method at finding this kind of signal. Second, the ability to stratify the analysis by age, gender and year of report is common for standard real-world pharmacovigilance work. The SG approach as presented above could be used in this context by applying the method to stratified versions of the data. Alternatively, a more complex model could be derived accounting for these factors. A third limitation is that our method comparison analysis was limited to the subset of cardiovascular events. In real-world pharmacovigilance, global database screening is performed on the entire database, possibly minus certain sources of reports that are considered not truly spontaneous (e.g. study report). In addition, we used an early implementation of the BCPNN, and the results we obtained may therefore not apply to current implementations that have, in effect, been better calibrated for lowfrequency DECs (). Furthermore, we did not take into account the impact of the adverse event terminology structure used to memorialize adverse events in spontaneous reporting databases (). The Medical Dictionary for Regulatory Affairs (MedDRA), which is used to code the adverse event data, is a hypergranular hierarchical thesaurus in which a medical concept may be fragmented in the database across multiple conceptually similar but literally distinct preferred terms. We did not know the extent to which this neutralizes some or all of the performance differentials observed. For example, if one method highlighted an association between a drug and the cardiac preferred term cardiomyopathy and another did not, or one highlighted it earlier than another, the clinical significance of this difference would depend on the simultaneous results of both methods with related terms such as myocarditis. If the method that failed to highlight cardiomyopathy did highlight myocarditis, and this represents variability in coding a single concept, then the differences from a practical perspective are not significant. Ultimately, semantic search approaches may lessen these biases by allowing many related event terms to be coalesced into a single event category. Several groups have begun using methods to explore ontology models, and more recently to analyze the MedDRA ontology (). However, a full exploration of this methodology is beyond the scope of this article. Regardless of these caveats, we believe that the SG approach represents a new and freely accessible methodology for the pharmacovigilance and research communities. The open nature of the SG method offers substantial opportunities for further exploration. We present one such opportunity in the potential utility of using negative SG scores to identify alternative indications for existing drugs and drug classes.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
K.Johnson et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Multinomial modeling at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
DISCUSSION AND CONCLUSION The use of the SG score based on a multinomial model and the empirical non-parametric calculation of variance produced results that were similar to those of well-accepted Bayesian shrinkage-based methods, with some potentially important differences. Namely, the magnitude of the shrinkage was less than GPS for cardiovascular DECs with one to four reports in an authentic dataset. As the goal of pharmacovigilance is the detection of novel adverse events in the most expeditious manner, with minimal patient exposure, the observed differences at low reporting frequencies could be significant, especially as some shrinkagebased methods such as GPS may miss signals absolutely or relatively in terms of timingin other words, some credible signals may be shrunk along with noise. SG scores may therefore provide an option for pharmacovigilance organizations to obtain similar results with a methodology that is arguably more transparent and easily understood by the broader range of drug safety specialists. Although the historical pharmacovigilance use of SRS databases has focused on finding DECs that occur unusually frequently, this work showcases an alternative non-traditional use of these kinds of databases. As illustrated with the HLT subset of events, the SG approach can identify signals that occur much less frequently than expected. Although this kind of information does not replace results that could be obtained from a controlled clinical study, it does provide a rich source of hypotheses for organizations seeking to explore alternative treatments via currently approved medicines.
