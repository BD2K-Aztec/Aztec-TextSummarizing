Motivation: The association between two blocks of 'omics' data brings challenging issues in computational biology due to their size and complexity. Here, we focus on a class of multivariate statistical methods called partial least square (PLS). Sparse version of PLS (sPLS) operates integration of two datasets while simultaneously selecting the contributing variables. However, these methods do not take into account the important structural or group effects due to the relationship between markers among biological pathways. Hence, considering the predefined groups of markers (e.g. genesets), this could improve the relevance and the efficacy of the PLS approach. Results: We propose two PLS extensions called group PLS (gPLS) and sparse gPLS (sgPLS). Our algorithm enables to study the relationship between two different types of omics data (e.g. SNP and gene expression) or between an omics dataset and multivariate phenotypes (e.g. cytokine secretion). We demonstrate the good performance of gPLS and sgPLS compared with the sPLS in the context of grouped data. Then, these methods are compared through an HIV therapeutic vaccine trial. Our approaches provide parsimonious models to reveal the relationship between gene abundance and the immunological response to the vaccine. Availability and implementation: The approach is implemented in a comprehensive R package called sgPLS available on the CRAN.
IntroductionRecent advances in high-throughput 'omics' technologies enable quantitative measurements of expression or abundance of biological molecules of a whole biological system. Various popular 'omics' platforms in systems biology include transcriptomics, proteomics, cytomics and metabolomics. The integration of multi-layer information is required to fully unravel the complexities of a biological system, as each functional level is hypothesized to be related to each other (). Furthermore, multi-layer information is increasingly available such as in standard clinical trials. As an example, the evaluation of vaccines in phase I/II trials incorporates various measurements of the cell counts (tens of population of interest), of the cell functionality by many ways including the production of cytokines (intra and extracellular) and of the gene expression (). The integration of omics data is a challenging task. First, the high dimensionality of the data, i.e. the large number of measured biological entities (tens of thousands) makes it very difficult toobtain a good overview or understanding of the system under study. The noisy characteristics of such high-throughput data require a filtering process to be able to identify a clear signal. Second, because of experimental or financial constraints, the small number of samples or patients (typically < 50) makes the statistical inference difficult and argue for using the maximum amount of available information. Third, the integration of heterogeneous data also represents an analytical and numerical challenge to try to find common patterns in data from different origins. In recent years, several statistical integrative approaches have been proposed in the literature to combine two blocks of omics data, often in an unsupervised framework. These approaches aim at selecting correlated biological entities from two datasets () or more (). This abundant literature clearly illustrates that the integrative analysis of two datasets poses significant statistical challenges to deal with the high dimensionality of the data. In particular, sparse partial least squares (sPLSs), using a L 1 penalty, has been developed for that purpose. With sPLS, it has been demonstrated that the integrative analysis of large scale omics datasets could generate new knowledge not accessible by the analysis of a single data type alone (). Moreover, the biological relevance of the approach has been demonstrated in recent studies (). However, group structures often existing within such data have not yet been accounted for in these analyses. For example, genes within the same pathway have similar functions and act together in regulating a biological system. These genes can add up to have a larger effect and therefore can be detected as a group [i.e. at a pathway or gene set level (. This has been increasingly used thank to geneset enrichment analysis approaches (). Considering a group of features instead of individual features has been found to be effective for biomarker identification ().proposed group lasso for group variables selection.extended it to logistic regression.modified group lasso to solve the non-orthonormal matrices problem. Although group lasso penalty can increase the power for variable selection, it requires a strong group-sparsity () and cannot yield sparsity within a group.proposed a supervised group lasso which selects both significant gene clusters and significant genes within clusters for logistic binary classification and Cox survival analysis.proposed a sparse group lasso penalty by combining an L 1 penalty with group lasso to yield sparsity at both the group and individual feature level.applied it to genomic feature identification.developed a sparse group-subgroup Lasso to accommodate selecting important groups, subgroups and individual predictors. In a regression context with a multivariate response variable,have recently proposed a multivariate sparse group lasso. Also some work has been reported to incorporate 'group effect' into a conventional canonical correlation analysis (CCA) model.studied structure-based CCA and proposed treebased and network-based CCA (). Chen and Liu (2012) incorporated group effect into an association study of nutrient intake with human gut microbiome (). Both papers show an improvement when incorporating group effect; however, a priori knowledge of group structure is needed and only the group effect of one type of data is discussed. More recently,developed a more general group sparse CCA method, which have been illustrated on genomics datasets (human gliomas data and NCI60 data).proposed a general penalized matrix decomposition (PMD) approach, which include sparse principal components and CCA. Sparsity have been realized by introducing different penalties forms such as L 1 penalty or fused lasso penalty to get smooth results in the context of ordered features. Based on generalized least square matrix decomposition, Allen et al.(2014) develop fast computational algorithms for generalized principal component analysis (PCA) and sparse PCA. In the same idea, a regularized PLS (RPLS) approach is proposed byto take into account the correlations between adjacent variables. However, none of the PMD and RPLS approaches have introduced group and sparse group lasso penalty. Here, we develop in a more general framework a group PLS (gPLS) method and a sparse gPLS (sgPLS) method (see also L fstedt et al. 2014). Both methods focus on sub-matrices decomposition taking into account the group structures. They could be used in 'regression' mode or in 'canonical mode'. The gPLS model aims at performing selection at group level while sgPLS enables selection at both group and single feature levels. Both irrelevant groups of features and individual features in the remaining groups will be simultaneously discarded with sgPLS. Our article is organized as follows. The model and algorithm for group and sgPLS are described in Section 2 after introducing the main steps of sPLS. We also present our extension in a context of PLS discriminant analysis. The performances of our approaches are compared with sPLS via a simulation study in Section 3. This section also contains an illustration of our method with an HIV vaccine study. The results are compared with the one obtained by applying the multivariate sparse group lasso recently proposed by).
Methods
NotationsLet X and Z be two data matrices containing n observations (rows) of p predictors (gene expression) and q variables (cytokine secretion), respectively. The soft thresholding function is g soft x; k  signxjxj  k  , where a   maxa; 0. The Frobenius norm is denoted jj  jj F , while the Euclidean vector norm is jj  jj 2 and the L 1 vector norm is jj  jj 1 .
PLS and sPLS for integrative analysis2.2.1 Partial least square PLS () is a well-known exploratory approach that was initially applied in chemometrics. It is particularly useful for analyzing noisy, collinear, even incomplete highly dimensional data; see Boulesteix and Strimmer (2007) for a review. It performs successive matrix decompositions of X and Z into new variables (called component scores or latent variables), denoted by n 1 ;. .. ; n H for the X-scores and x 1 ;. .. ; x H for the Z-scores. These scores should be few in number (H small) and orthogonal to each other within each dataset. They are estimated as linear combinations of the original variables in X and Z, with their weight coefficients stored in the associated so-called loading vectors u h and v h ; h  1;. .. ; H. In a matrix representation, we havewhere F X and F Z are the residual matrices and where the h  1th columns of C and E contain, respectively, the coefficients from the simple regressions of each column of the current deflated matrices X h  X h1 n h c T h and Z h  Z h1  x h e T h onto the score vectors n h1 and x h1. PLS relates both matrices by maximizing the covariance between each pair of scores n h  X h1 u h ; x h  Z h1 v h : argmax jju h jj 2 jjv h jj 2 1 CovX h u h ; Z h v h ; h  1;. .. ; H:This PLS form is often referred to as 'PLS mode A' in the literature () where, similarly to CCA, the relationship between the two datasets is symmetric. A variant is an asymmetric way ('PLS2', Wegelin, 2000;) of deflating Z and in this case the model consequently differs: Z  ND T  F Z , where F Z is a residual matrix and where the h  1th column in D contains the coefficients from the local regressions of each column of the current deflated matrix Z h  Z h1  n h d T h onto the score vector n h1 .
Sparse PLSThe sPLS enables variable selection from both sets by including L 1 penalizations on both u h and v h simultaneously in (1), which is solved with a Lagrangian form (see L). The result is a subset of correlated variables from both X and Z indicated through the non-zero elements of the loading vectors u h and v h , respectively (for each PLS dimension h) and a set of score vectors n h ; x h  which are useful for graphical representations. Let us consider the singular value decomposition of the r-rank matrix M  X T Z: M  UDV T ; where U  u 1 ;. .. ; u r  : p  r and V  v 1 ;. .. ; v r  : q  r are orthonormal and where D is a diagonal matrix containing the singular values d k. The column vectors of U and V are the PLS loadings of X and Z. It is worth noting that u 1 ; v 1  is also solution of (1) when h  1. Moreover, by EckartYoung's theorem, these two vectors can also be obtained by first solving (for ~ u and ~ v, without a norm constraint) the minimization problem minfollowed by a norming step of the vectors found. We thus haveThis is equivalent to solve minfollowed by norming ~ v (respectively, ~ u). To obtain sPLS loadings, Lfollowed this idea, similar to the one implemented by Shen and Huang (2008) to develop sparse PCA. In sPLS, one tries to optimize minusing an iterative algorithm (see Supplementary Material) in which at each iteration, u h (respectively, v h ) is alternatively fixed, while v h (respectively, u h ) is constrained to be of unit-norm and where M h  m ij;h  i;j  X T h Y h ; u h  u i;h  i and v h  v j;h  j ; h  1;. .. ; H. The penalizations P k 1;h u h   P p i1 2k h 1 ju i;h j and P k 2;h v h   P q j1 2k h 2 jv j;h j are introduced to penalize the loading vectors u h and v h. This procedure leads to normed sparse loading vectors.
Group PLS and sparse group PLS2.3.1 Group PLS Let us consider that both matrices X and Z can be divided, respectively, into K and L sub-matrices (groups) X k : n  p k and Z l : n  q l , where p k (respectively, q l ) is the number of covariates in group k (respectively, l). For example, for gene expression data, these sub-matrices may be gene pathways or factor level indicators in categorical data. The aim is to select only a few groups of X which are related to a few groups of Z. For each dimension h, following the same idea as in (), we propose to use group lasso penalties in the optimization problem (2):where u k (respectively, v l ) is the loading vector associated to the kth (respectively, lth) block. The subscript h has been removed to improve readability. The minimization criterion (2) can thus be rewritten aswhere M k;l  X k Z l T. Next, we discuss optimization over u for a fixed v. The minimization criterion (3) can be rewritten aswhere M k;  X k Z T. Therefore, we can optimize over group wise components of u separately. The first term in the above equation expands as traceM k; M k; T   2traceu k v T M k; T   traceu k u k T : Hence, the optimal u k minimizes: traceu k u k T   2traceu k v T M k; T   k 1 ffiffiffiffiffi p k p jju k jj 2 :The objective function is convex, so the optimal solution is characterized by subgradient equations. For group k, u k must satisfywhere h is the subgradient of jju k jj 2 evaluated at u k. So, h  u k jju k jj 2 if u k 6  0; 2 fh : jjhjj 2 1g if u k  0:We can see that subgradient Equation (4) is satisfied with u k  0 ifFor u k 6  0, Equation (4) givesCombining Equations (5) and(6), we find:In the same vein, optimization over v for a fixed u is also obtained by optimizing over group wise components:Sparse group PLSThe iterative procedure for gPLS is similar to that of sPLS. Only the steps (1)and (2) in 2.c. of the algorithm (see Supplementary Material) are modified: 1. For k in 1;. .. ; K: apply (7) norm u new 2. For l in 1;. .. ; L: apply (8) norm v new 2.3.2 Sparse group PLS One potential drawback of gPLS is to include a group in the model when all loadings in that group are non-zero. However, sometimes we would like to combine both sparsity of groups and within each group. For example, if the predictor matrix contains genes, we might be interested in identifying particularly important genes in pathways of interest. To achieve this, in the same spirit as, we introduce sparse group lasso penalties in the optimization problem (2):Next, we discuss optimization over u for a fixed v. The minimization criterion can be rewritten asTherefore, we can optimize over group wise components of u separately. Hence, the optimal u k minimizes:Using similar tools as before, we define the subgradient equations:where h is the subgradient of jju k jj 1 evaluated at u k. So,2 fc j : jc j j 1g if u k j  0:We can see that subgradient Equation (9) is satisfied with u k  0 ifwhere the thresholding function g soft ; k is applied to the vector M k; v componentwise. The subgradient equations can also give insight into the sparsity within a group which is at least partially nonzero. For u k 6  0, the subgradient conditions for a particular uThis is satisfied for u k j  0 ifFor u k j 6  0, we find by that uTaking the L 2 norm of both sides of (13), we getBy substituting (14) into (13), we findIn the same vein, optimization over v for a fixed u is also obtained by optimizing over group wise components. We find that v l  0 ifotherwise. The iterative procedure for sgPLS is similar to that of sPLS. Only the steps (1)and(2) in 2.c. of the algorithm (see Supplementary Material) are modified: @BULLET For k in 1;. .. ; K: if (10) is true u new  0 else apply (15) norm u new @BULLET For l in 1;. .. ; L: apply (16) is true v new  0 else apply (17) norm v new
RemarkCalibration of the different tuning parameters is discussed in the Supplementary Material.
Extension to discriminant analysis of one datasetPLSs has often been used for discrimination problems () by recoding the qualitative response as a dummy block matrix Y : n  g indicating the group of each sample (g being the number of groups). Barker and Rayens (2003) give some theoretical justification for this approach. One can also directly apply PLS regression on the data as if Y was a continuous matrix (from now on called PLS-DA). A sparse version has been proposed by L Cao et al. (2011a, b) using the Lagrangian form of PLS-DA to include a L 1 constraint. Our algorithm for the gPLS-DA is obtained by replacing Z with Y and by using only a group penalty on the loading related to the X matrix (k 2  0 in Equation 3). Algorithm for sgPLS-DA is also obtained by replacing Z with Y and by using only penalties on the loading related to the X matrix (k 2  0 and a 2  0 in Equation 3). The tuning parameters (k h 1 and a h 1 ) are calibrated sequentially by evaluating the classification error rate using leave-one-out or k-folds cross-validation.
Results and discussionA simulation study is performed to demonstrate the good performance of gPLS and sgPLS when compared with sPLS. Then, the three methods are applied on an HIV vaccine evaluation study.
Simulation studyDifferent simulation studies are performed to investigate the detection power of gPLS and sgPLS when group effect exists. We compare the results with the popular sPLS method under several conditions such as different values of the standard deviation of noise, the number of groups associated with the multivariate responses and different sample sizes. We focus on the regression mode of PLS. For all cases described below, we generated p  400 variables stored in the dataset X, which was divided into G X groups. We considered two situations for the q variables of dataset Z: (i) q  10 variables which are not divided into groups and (ii) q  500 variables which are divided into G Z groups. The link between X and Z is specified by the following models:where the n  H matrix N contains H latent variables n 1 ;. .. ; n H. The components of these vectors have all been independently generated from a standard normal distribution. The rows of the residual matrix F X (respectively, F Z ) have been generated from a multivariate normal distribution with zero mean l X (respectively, l Z ) and covariance matrix R X  rI p (respectively, R Z  rI q ). The orthogonal matrix of regression coefficients C  C 1 ;. .. ; C H  enables us to specify the 'true' (i.e. active) X-variables linked to the response Z-variables. The p-vector C j  c 1 j ;. .. ; c p j  T includes non-zero values c i j 6  0 if the corresponding variable X j (jth column of X) is a true variable (i.e. associated to one of the latent variables n h ) and zero values otherwise. The matrix orthogonal D  D 1 ;. .. ; D H  enables us to specify the association of the response variables to each latent variable. All components of the column vectors D 1 ;. .. ; D H have been independently generated from a uniform distribution U0:5; 2 when the dataset Z is not divided into groups. When the dataset Z is divided into groups, the column vectors D 1 ;. .. ; D H include non-zero values d i j 6  0 if the corresponding variable Z j (jth column of Z) is a true variable and zero values otherwise. Matrix C (respectively, D) should be made orthogonal. To understand and compare the behavior of all methods, we used an optimal value for tuning parameters. Thus, the sPLS tuning parameter is set to the number of true variables, and the gPLS and sgPLS tuning parameter relative to k is set to the number of active groups. The mixing parameter a in the sgPLS approach is selected using a 5-folds crossvalidation on a grid of 15 values between 0.05 and 0.95 since it is not possible to set an optimal value for a. We evaluate the performance of each method by presenting the true-positive rate (TPR) which reflects the number of correctly identified true variables and the total discordance (TD) which is the number of incorrectly identified variables. These measures are defined aswhere TP, FP and FN are the number of true positives, false positives and false negatives, respectively.
Recovering the signalThe first simulation corresponds to the case when the q  10 variables from dataset Z have not been divided into groups. The sample size is n  100 and we consider a model with only one latent variable n 1. Dataset X is divided into G X  20 groups of 20 variables. Among them, only 4 groups each containing 15 true variables and 5 noise variables are associated to the response variables Z. We set the p-vector C 1 to have 15 1s, 30-1s and 15 1.5s, the other components being set to 0. All 15 non-zero coefficients are assigned randomly into one group along with the remaining 5 zero coefficients corresponding to noise variables (see top left panel of Supplementary). In the top four panels of Supplementary, the standard deviation r has been set to 0.5. A standard deviation r  2:5 is set for the other panels. Supplementarydisplays the results of the loading vectors u 1 recovered by sPLS, gPLS and sgPLS. Notice that sPLS and sgPLS recover satisfactorily the true u 1 when the noise is small (r  0:5). However, when r  2:5, sPLS selects more noise variables than the true u 1 and misses out some true variables while sgPLS still works well. For both situations, gPLS recovers all groups but gives false positives within the groups. We arrive at the same conclusion when the q  500 variables of Z are divided into G Y  25 groups (see Supplementary Figs S2S5). In this situation, we consider a model with two latent variables n 1 and n 2. The vector C 2 is chosen in the same way as previously described for C 1. The two columns of D are q-vectors containing 15-1s, 15-1.5s and 30 1s and the rest are 0 s. For a small standard deviation r  0:5 (Supplementary Figs S2S3), all methods perform well to recover the loading vectors u 1 and v 1 related to the first component and the loading vectors u 2 and v 2 related to the second component. However, gPLS gives false positives within the groups. For a high standard deviation r  2:5 (Supplementary Figs S4S5), sPLS selects more noise variables, while sgPLS still performs well by selecting only relevant variables into the true groups.
Effect of noiseWe investigate the performance of our methods with respect to noise. This simulation corresponds to the situation when the q  10 variables from dataset Z have not been divided into groups and when one latent variable has been generated. We use a sequence of 10 equispaced standard deviation values r, from 1.5 to 3. Supplementarypresents the average of TPR and TD over 50 replications for each method. For all noise levels, we found that gPLS and sgPLS manage to recover the true groups. Thus, TD for gPLS is equal to 20 as each of the 4 true groups contains 5 noise variables (FP). However, for high values of noise, the sparsity introduced within the group by sgPLS misses out some true variables. Note that sPLS also misses out true variables when the noise is increased and gets the highest TD due to the high number of false positives selected. We present in Supplementarythe average, over 50 replications, of the signal recovered (original loading u 1 ) by the three methods when r  3. This figure highlights the number of false positives selected by sPLS.
Effect of the number of true variables in the groupWe investigate the performance of our methods when the number of true variables within each group is varied. We are still in the framework of one latent variable generated and only the p  400 variablespresents the average results over 50 replications. When true variables are distributed in 4 and 5 groups, both gPLS and sgPLS give a much higher value of TPR and a lower value of TD than sPLS. When the true variables are more sparsely distributed into different groups (number of groups increased), gPLS still gives high TPR but TD is increased. Note that sgPLS is the best method with high TPR and low TD values.
Effect of sample sizeWe investigate the performance of our methods when the sample size is increased by steps of 50 from n  50 to n  500. Dataset X contains G X  40 groups, each with the same group size of 10. Among the p  400 variables in dataset X, 60 are true variables, which are distributed evenly into 6 groups. This clearly advantages gPLS. Supplementarypresents the average results over 50 replications of TPR and TD with respect to different sample sizes. It can be seen that gPLS gives better results than the two other methods by finding the true groups whatever the sample size. For both sPLS and sgPLS, TPR increases while TD decreases when sample size increases. The TPR of sPLS is most of the time lower than that of sgPLS. Moreover, the TD of sPLS is higher than both those of gPLS and sgPLS.
Effect of the dimension pWe investigate the performance of our methods when the dimension p of X is increasing (p 2 f500; 1000; 2000; 3000; 4000g).Dataset X contains G X  p=50 groups, each with the same group size of 50. Among the G X groups, p=100 groups contains 35 true variables. This simulation corresponds to the situation when the q  10 variables from dataset Z have not been divided into groups and when one latent variable has been generated. We set different sample size n  25; 50 and 100. The standard deviation of the noise is r  3:5. Supplementarypresents the average results over 50 replications of TPR, false-positive rate (FPR) and TD for the different values of p and for different sample sizes. In this simulation setting gPLS manages to find the true groups whatever the dimension p and the sample size. As 15 noise variables are included in each true group, gPLS method gets higher FPR and TD than sgPLS and sPLS. Almost every time, sgPLS outperforms sPLS by getting higher TPR and lower TD and FPR.
Application for an HIV vaccine evaluation study
Description of the studyThe method has been applied to an HIV vaccine trial: the DALIA trial (). In this trial, 19 HIV-infected patients have been included for an evaluation of the safety and the immunogenicity of a dendritic-cell-based vaccine. The vaccine was injected on weeks 0, 4, 8 and 12, while patients received an antiretroviral therapy. An interruption of the antiretrovirals was performed at week 24 and the treatment was resumed if the CD4  T cell count dropped below 350 cells/ ^ AlL. After vaccination, a deep evaluation of the immune response was performed at week 16 while repeated measurements of the main immune markers and gene expression were performed every 4 weeks until the end of the trial at 48 weeks.The analyses of the immune markers showed a strong immunological response especially among the CD4  T cell populations with an increase of cells producing various cytokines. After the antiretroviral treatment interruption, there was a strong viral replication that was heterogeneous among the study population and the immune response appeared to be associated with the observed maximum value of viral load during the rebound (). One of the question that follows these results was whether the immune response and its impact on the viral dynamics could be explained and predicted with the observation of the change of gene expression during vaccination. To answer this question, we first analyzed the repeated measurements of gene abundance performed by microarrays (Illumina HumanHT-12 v4). Because of the sparsity of the measurements, only geneset analyses allow to detect significant changes over time of group of genes. Using previously defined group of genes, so called modules (), we reported a significant change of gene expression among 69 modules over time before antiretroviral treatment interruption. Then, we asked how the gene abundance of the 5399 genes (p) from these 69 modules as measured at week 16 correlated with immune markers measured at the same time point. The immune markers were either direct measurement of cytokine concentrations on supernatants (IL21, IL2, IL13, IFNg) or a combination of markers as measured by multiplex and intracellular staining (Luminex score, TH1 score, CD4 polyfunctionnality). Seven (q) different immune markers have been used in the current application. Measurements and calculations are detailed elsewhere (). Of note, the modules as defined inwere not overlapping: each gene contributed to only one module. To use the present approach with overlapping groups of genes (such as in Gene Ontology), an extension in the spirit ofwould be required.
ResultsThe selection process of the genes according to the mean square error of prediction criteria for each method is shown in Supplementary Figures S11S13. Supplementaryshows the cumulative percentage of variation of the responses variables explained by the components according to the method. With three components, more than 80% of the variance could be explained. The sgPLS methods selected slightly more genes than the sPLS (respectively, 487 and 420 genes selected), but sgPLS selected fewer modules than the sPLS (respectively, 21 and 64 groups of genes selected). Of note, all the 21 groups of genes selected by the sgPLS were included in those selected by the sPLS method. sgPLS selected slightly more modules than gPLS (4 more, 14/21 in common). However, gPLS led to more genes selected than sgPLS (944). Supplementarydisplays Venn diagrams of the selected sets by the three methods, both at the gene and module levels. Therefore, in this context of hierarchical data, sgPLS ends up being more parsimonious than sPLS in term of modules and than gPLS in term of genes by taking the most predictive genes within a selected module (see Supplementary). Such results are consistent with those of the simulation study. The selection of the most predictive genes inside a module improves the prediction capacity and does not avoid any interpretation of the biological significance of the remaining genes inside a module based on the biological knowledge available for this module. Interestingly, the modules commonly selected by the three approaches were the most biologically relevant ones, such as the modules associated to inflammation (M3.2, M4.13, M4.2, M4.6, M7.1) and cellular responses (M3.6 cytotoxic/NK cell, 4.15 on T cells). A significant number of additional modules have been selected by sPLS but not by the other methods. Although, some are biologically sound (e.g. M3.1 Erythrocytes), many were not annotated or are unrelated to the immune system making the hypothesis of false signals likely. Regarding the modules differentially selected by sgPLS and gPLS, three modules were selected by gPLS but not by sgPLS (M4.11 Plasma cells and undetermined modules 4.8 and 7.35), whereas seven modules were selected by sgPLS and not by gPLS (M3.5 and M4.7 Cell cycle, M4.1 T cell, M5.1 and M5.7 inflammation, M6.7 and M5.2 undetermined). Clearly, the selection by sgPLS sounds more biologically relevant as M4.1 is known to be associated to the other T-cell module M4.15 that was selected by both methods (see www.biir.net/public_wikis/module_annotation/ V2_Trial_8_Modules). This is the same for the inflammatory modules M5.1 and M5.7 that are known to be related. In regards of the module M4.11 (plasma cells), we have already notice that its selection was not robust in bootstrap analyses performed with sPLS (data not shown). Therefore, in this application, the sgPLS approach led to a parsimonious selection of modules and genes that sound very relevant biologically and is in agreement with the simulation results. An illustration of the results is shown through the correlation matrix in Supplementaryaccording to the genes selected in common by the three methods. Genes and modules negatively or positively associated to the immune response appeared very clearly. As expected, inflammatory modules were negatively correlated to the immune response whereas modules related to the cellular immune response (M3.6, M4.15) were positively correlated to the immune response. Stability of the module selection has been assessed for sPLS, gPLS and sgPLS (see Supplementary) in the spirit of, Meinshausen and B hlmann (2010) and L). It highlights the insights gained from incorporating the grouping structure into the analysis, comforting the above biological conclusions. Also, it reveals the instability of the selection of the module 4.11 by sPLS and gPLS, module that was not selected by sgPLS. Furthermore, we compared the proposed novel approach with the multivariate lasso implemented in glmnet R package () and a multivariate (sparse) group lasso proposed by. The two alternative approaches selected less modules and genes but the modules selected were most often the same across the different methods (see Supplementary Materials, Section S1.7).
FundingThis research was supported by the NSERC of Canada (to P.L.d.M.). The second author also thanks the GENES. Conflict of Interest: none declared.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
B.Liquet et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
