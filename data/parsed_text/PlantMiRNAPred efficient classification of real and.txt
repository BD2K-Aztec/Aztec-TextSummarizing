Motivation: MicroRNAs (miRNAs) are a set of short (21â€“24 nt) non-coding RNAs that play significant roles as post-transcriptional regulators in animals and plants. While some existing methods use comparative genomic approaches to identify plant precursor miRNAs (pre-miRNAs), others are based on the complementarity characteristics between miRNAs and their target mRNAs sequences. However, they can only identify the homologous miRNAs or the limited complementary miRNAs. Furthermore, since the plant pre-miRNAs are quite different from the animal pre-miRNAs, all the ab initio methods for animals cannot be applied to plants. Therefore, it is essential to develop a method based on machine learning to classify real plant pre-miRNAs and pseudo genome hairpins. Results: A novel classification method based on support vector machine (SVM) is proposed specifically for predicting plant pre-miRNAs. To make efficient prediction, we extract the pseudo hairpin sequences from the protein coding sequences of Arabidopsis thaliana and Glycine max, respectively. These pseudo pre-miRNAs are extracted in this study for the first time. A set of informative features are selected to improve the classification accuracy. The training samples are selected according to their distributions in the high-dimensional sample space. Our classifier PlantMiRNAPred achieves >90% accuracy on the plant datasets from eight plant species, including A.thaliana, Oryza sativa, Populus trichocarpa, Physcomitrella patens, Medicago truncatula, Sorghum bicolor, Zea mays and G.max. The superior performance of the proposed classifier can be attributed to the extracted plant pseudo pre-miRNAs, the selected training dataset and the carefully selected features. The ability of PlantMiRNAPred to discern real and pseudo pre-miRNAs provides a viable method for discovering new non-homologous plant pre-miRNAs. Availability: The web service of PlantMiRNAPred, the training datasets, the testing datasets and the selected features are freely available at http://nclab.hit.edu.cn/PlantMiRNAPred/.
INTRODUCTIONDerived from hairpin precursors (pre-miRNAs), mature microRNAs (miRNAs) are non-coding RNAs that play important roles in gene regulation by targeting mRNAs for cleavage or translational repression (). MiRNAs are involved in many important biological processes including plant development, signal transduction and protein degradation (). However, systematically detecting miRNAs from a genome by experimental techniques is difficult (). As an alternative, the computational prediction methods are used to analyze the genomic DNA and to obtain the putative candidates for experimental verification. Since many miRNAs are evolutionarily conserved in multiple species, methods that use comparative genomics to identify putative miRNAs have been presented. MirFinder () predicted the potential miRNA of in the Arabidopsis thaliana genome. It is based on the conservation of short sequences between the genomes of Arabidopsis and Oryza sativa. MicroHARVESTOR () can identify candidate plant miRNA homologs for a given query miRNA. The approach is based on a sequence similarity search step followed by a set of structural filters. The miRNAs of Vigna unguiculata are identified through homology alignment of highly conserved miRNAs in multiple species (). Although these methods are effective in identifying new conserved miRNAs, they cannot discover novel miRNAs with less homology. On the other hand, since miRNAs in plants often have near perfect matches to their target mRNAs, methods that are based on the complementarity characteristics have also been proposed. MIRcheck (Jones) searches for the new miRNAs whose target sites are conserved in A.thaliana and O.sativa. FindMiRNA () predicts potential Arabidopsis miRNAs from candidate pre-miRNAs that have corresponding target sites within transcripts. In order to decrease the number of miRNA candidates, the predicted candidates are required to have orthologs in rice. MiMatcher () also predicts the plant miRNAs through exploiting the complementarity of plant miRNAs. However, since the candidates that are complementary to target mRNAs are enormous within intergenic regions and introns, rigorous criteria such as conservation among multiple species are introduced to significantly reduce thePage: 1369 13681376
PlantMiRNAPred
numberof candidates. This practice also considerable reduces the chance of discovering new miRNAs. As an alternative, the ab initio methods have been developed to distinguish real pre-miRNAs from pseudo pre-miRNAs. The real pre-miRNAs and pseudo pre-miRNAs are used to train the classification models including support vector machines (SVM) (), probabilistic co-learning model (), nave Bayes (), random forest () and kernel density estimation (). These trained classification models can be then used to classify real pre-miRNAs from pseudo pre-miRNAs. However, they all are trained by the human pre-miRNAs and pseudo pre-miRNAs, and mainly used to identify human pre-miRNAs. Triplet-SVM () is the only one that has been tested on the premiRNAs from A.thaliana and O.sativa. As the plant pre-miRNAs differ greatly from the animal pre-miRNAs, triplet-SVM cannot achieve high classification accuracy for the plant species including A.thaliana and O.sativa. Since nearly all the pre-miRNAs in plants and animals have the stemloop hairpin structures, this characteristic is widely used as an important feature in the ab initio methods. However, the plant premiRNAs usually have more complex secondary structures than the animal pre-miRNAs and the structures have not been considered in existing methods. We propose a computational classification approach that considers the unique characteristics of plant premiRNAs. To construct a comprehensive training dataset, the new pseudo plant pre-miRNAs are extracted from the protein coding regions of the A.thaliana and Glycine max genomes, based on which an efficient SVM classifier is constructed to classify the real/pseudo plant pre-miRNAs.
METHODS
Features of plant pre-miRNAsRecent research indicates that pre-miRNAs in animals and plants have many features in both primary sequence and secondary structure. These features can be used to classify real plant pre-miRNAs and pseudo hairpins with an ab initio method. miPred () extracted 29 global and intrinsic folding features from human real and pseudo pre-miRNAs. These features include (i) 17 base composition variables: 16 dinucleotide frequencies, that is %XY where X,Y {A,C,G,U} and %G+C content; (ii) six folding measures: adjusted base pairing propensity dP (), adjusted minimum free energy (MFE) of folding denoted as dG (), adjusted base pair distance dD (), adjusted Shannon entropy dQ (), MFE Index 1 MFEI 1 () and MFE Index 2 MFEI 2 ; (iii) one topological descriptor which is the degree of compactness dF (and %(GU)/n_stems, where n_stems is the number of stems in the secondary structure. The plant pre-miRNAs have more diversities than the animal premiRNAs. First, the pre-miRNAs in animals are typically 6080 nt (), whereas the length of pre-miRNAs in plants ranges fromPage: 1370 13681376In total, 115 features are obtained from each real/pseudo plant premiRNAs. They include redundant features that do not contribute to classification. The algorithm based on graph is presented to eliminate the redundant features. In the end, the discriminative feature subset is selected to achieve the best classification accuracy.
P.Xuan et al.
SVMDue to the excellent generalization ability of SVM, we use it to classify real/pseudo plant pre-miRNAs with high-dimensional (115-dimensional) feature vectors. Given a training dataset S, each x i  S (i = 1, ...,N) is a feature vector of real/pseudo pre-miRNA with corresponding labels z i (z i =+1 or 1, real pre-miRNA or pseudo pre-miRNA, respectively). SVM constructs a decision function (classification of unknown sequence x with stemloop structure),where  i is the coefficient to be learned (0   i  C) and k is a kernel function. In our study, a radial basis function (RBF) kernel is used, where the parameter  determines the similarity level of the features so that the classifier becomes optimal. k(x,x i ) = exp( x x i )The penalty parameter C and the RBF kernel parameter  are tuned based on the training dataset using the grid search strategy in libSVM (version 2.9).
Classification based on SVMBoth the features of real/pseudo pre-miRNAs and the training samples are important for constructing an efficient SVM classifier. As shown in, we propose a classification method based on SVM. All the 2043 plant pre-miRNAs from miRBase 14 (covers 29 plant species) are collected as positive datasets. The homologous pre-miRNAs among different species are gathered into the same miRNA gene family by RFam (). Most of the pre-miRNAs in the same families are similar. Therefore, the representative pre-miRNAs are selected from the 128 plant miRNA families of miRBase 14 (including 1612 real pre-miRNAs), as the positive training samples. Since the remaining 431 pre-miRNAs do not belong to any of miRNA families, they are used as the positive training samples. The negative dataset consists of the 17 groups. Each group is composed of pseudo premiRNAs from the genome segments of A.thaliana and G.max (see Section 3 for details). (i) The 115 features are extracted from the primary sequence and secondary structure of pre-miRNAs and their stems. (ii) The redundant features are eliminated and the informative feature subset is selected through calculating the information gain of features and the similarity between any two features. (iii) The positive/negative training samples are selected according to their distribution in the high-dimensional sample space, the size of each family and the size of each negative sample group. (iv) An SVMbased classifier named PlantMiRNAPred is trained by using these samples to classify real pre-miRNAs and pseudo pre-miRNAs. The feature selection and sample selection modules are implemented in Java. The web service of classifying plant pre-miRNAs is developed in PHP on the Linux platform.
Feature subset selectionFeature selection aims to select a group of informative features which can retain most information of original data and distinguish each sample in the dataset. The feature selection method considers information gain and feature redundancy. Information gain: since all the features of pre-miRNAs are discrete, the discrimination ability of a feature is measured by information gain based on Shannon entropy. Suppose a feature of pre-miRNAs is x and the entropy of x is denoted as H(x). When the value of feature y is given, the conditional entropy is H(x|y). IG(c,x) is the information gain of feature x relative to the class attribute c (). Since classification of real or pseudo premiRNAs is binary classification problem, c is assigned to 1 (real pre-miRNA)Suppose that the complete feature set is X ={x 1 ,x 2 , ... ,x 115 } and the class attribute is c. The values of 115 features are obtained from each real premiRNA (1906 real pre-miRNAs in total) and c is set to 1. Also, the values of 115 features are obtained from each pseudo hairpin (2122 pseudo hairpins in total) and c is set to 1. The information gain of feature x i (1  i  115) is calculated and denoted as IG(c,x i ). The features with greater information gain are given higher preference. However, the 115 features still include redundant features, inclusion of which will not improve the classification accuracy. To identify the redundant features, the feature similarity is used to measure the similarity between two features. Feature similarity: let Sim(x,y) represent the similarity between features x and y and it is defined as,where IG(x,y) denote the information gain of y respect to x. Sim(x,y) ranges from 0 to 1. Sim(x,y) equal to 0 means that x and y are completely irrelevant. Sim(x,y) equal to 1 means that x and y are completely relevant. When Sim(x,y) is greater than a threshold , x or y is a redundant feature. Keeping both features does not improve the classification performance. In such situation, the feature with greater information gain is kept and the other one is dropped. In order to effectively eliminate the redundant features when multiple features are correlated, a redundant feature graph G is constructed. We propose an algorithm based on graph. Suppose that the redundant feature graph is G = (V ,E). Each node v i (v i  V ) represents the feature x i. The weight of node v i is the IG(c,x i ). If the similarity between two nodes v i and v j is more than the threshold ( = 0.49), x i or x j is redundant. Then a new edge is added to connect the two nodes. The weight of the edge between v i and v j is Sim(x i ,x j ). Here,  is determined by the experiments and the prior experience (). The process of eliminating redundant features is illustrated by an example. As shown in, a redundant feature graph G consists of eight features, including x 1 ,x 2 , ... and x 8. Suppose that groups of redundant features exist, where features within a group are redundant to one another but are independent to the remaining features. If we assume that there are three groups, then the graph G is composed of three subgraphs: SG 1 , SG 2 and SG 3. Feature selection weight (FSW) of each feature is first calculated. Suppose that k edges are adjacent to v i. FSW of v i is defined as FSW vi = Sum of weights of the k edges (that are connected with v i ) + weight of v i. The feature node v x with the most FSW is selected. The nodes adjacent to v x are the redundant features and should be deleted. In the subgraph SG 2 , FSW of x 3 = (0.71+0.51)+0.42 = 1.64. FSWs of x 4 , x 5 , x 7 and x 8 are 1.48, 0.8, 1.52 and 1.15, respectively. Therefore, x 3 is selected, and the adjacent x 5 and x 7 are deleted. The bolded nodes inare the selected feature nodes. Next, the FSWs of the remaining nodes x 4 and x 8 in the current SG 2 are calculated again. Since x 8 has greater FSW, it is selected. In the same way, in SG 1 , x 1 is selected and x 2 is deleted. In SG 3 , x 6 is an independent node. As no other node can represent the independent node, x 6 is also selected. At last, a feature subset {x 1 ,x 3 ,x 6 ,x 8 } is obtained and the redundant features x 2 , x 4 , x 5 and x 7 are eliminated. In addition, if two nodes (v y and v z ) are with the maximum FSW in a subgraph, the node weights of v y and v z are compared and the node with greater weight is selected. The algorithm of eliminating redundant features is described in. The proposed algorithm is applied to eliminating redundant features among the 115 features. We found that two pairs of attributes are strongly correlated: dQ versus dD, and dG versus NEFE. This observation is consistent with the result in miPred, which indirectly confirms the result of eliminating features. In addition, we found two new strongly correlated pairs of attributes: dH versus dS, and dH/L versus dS/L, and dQ, dG, dH and dH/L are selected due to their higher selection weights.Initially, the 115 features are categorized into three feature subsets: (i) primary sequence-related feature subset S 1 = {%G+C,%XY |X,Y {A,C,G,U}} (17 features); (ii) secondary structurerelated feature subset S 2 = {"A(((","U","A(((_S",,"U_S"} (64 features); (iii) energy-and thermodynamics-related feature subset S 3 = {dP, dG,  , zF, MFEI 5 , MFEI 6 , Avg_mis_num} (34 features). Supplementaryillustrates the name and the classification of 115 features. After eliminating the redundant features, the three subsets are denoted as S 1 , S 2 , and S 3. For each subset, the remaining features are sorted by information gain in descending order. The features with information Page: 1372 13681376
P.Xuan et al.gain greater than a threshold  ( 1 = 0.136,  2 = 0.083,  3 = 0.0159) are selected for the classification.  is determined by the experiments and the prior experience (). In the end, a total of 68 features are selected and listed in Section 3.
Training samples selectionThe classification performance of SVM is highly dependent on the selection of training dataset. First, we noted that the real pre-miRNAs from the same species and the same miRNA families are highly similar to one another. These redundant positive samples should be removed from training samples to avoid over-fitting. Secondly, current research has shown that training an SVM classifier with an imbalance positive and negative dataset would result in poor classification performance with respect to the minority class (). So we select the appropriate proportion of representative real/pseudo pre-miRNAs to construct positive/negative training dataset. The sample selection method (miSampleSelection) selects the positive/negative training samples according to the sample distribution in the positive families/negative groups. As shown in, the real pre-miRNAs are selected based on the sample distribution in the 128 families. Since 68 informative features have been selected, each sample is denoted as a 68-dimensional feature vector. Suppose the feature vector of a sample is v and there are M (M = 128) families. The vector set of central points is C ={c 1 ,c 2 ,...,c M }, where c i represents the feature vector of the central point in the i-th family. The sample selection process of the i-th family is as follows.(1) Assume that the number of samples in the i-th family is N i. v k is the feature vector corresponding to the k-th sample. c i is then calculated as,(2) The distance between the k-th sample (real pre-miRNA) v k and the central point c i is denoted as d v k c i. v t k means the transpose of vector v k. Then, the radius of the i-th family is r i , where(3) Suppose that the selection rate of sample space is 1/n. That is, N i /n samples in the i-th family are selected. The number of the selected samples is denoted as(4) Suppose that c i is the center of a circle, draw two circles with radius 0r i and (1/P i )r i , respectively. The region between these two circles is denoted as A 0. The degree of coverage for each sample s in A 0 is calculated and denoted as C(s). C(s) represents the number of samples in A 0 whose nearest neighbor sample is s. The sample s with the greatest C(s) value is selected as a training sample.(5) We set (1/P i )r i as the step length and compute the degree of sample coverage in the region A k between two circles with the radius, respectively. The sample in A k with the largest degree of coverage is selected.The positive training dataset is composed of the samples selected from the 128 families. For 431 pre-miRNAs that do not belong to any of miRNA families, they are added into the positive training dataset. The process of selecting negative training samples is similar. The negative samples are composed of pseudo hairpins grouped by length. There are 17 groups in total, where the 60nt_Group refers to pseudo hairpins of length from 60 to 69 nt, the 220nt_Group refers to pseudo hairpins of length from 220 to 229 nt, etc. Seventeen groups of the negative dataset correspond to the families of the positive dataset. The negative training samples are selected in the same way as that of the positive samples.
RESULTS AND DISCUSSION
Data collectionA classifier of pre-miRNAs should distinguish real plant premiRNAs from pseudo plant hairpins. The positive dataset is composed of known plant pre-miRNAs, while the negative dataset is composed of both pseudo A.thaliana hairpins and pseudo G.max hairpins. Positive dataset: there are 2043 known plant pre-miRNAs in the miRNA database miRBase 14 (http://www.mirbase.org/). Rfam (http://rfam.janelia.org/) grouped the available real pre-miRNAs into a set of families by means of multiple sequence alignments. A miRNA gene family is composed of the homologous pre-miRNAs from different species. One thousand six hundred and twelve premiRNAs belong to 128 miRNA families and 431 pre-miRNAs do not belong to any of miRNA families. Two thousand forty-three real pre-miRNAs are chosen as the positive sample dataset., the top 22 families consist of 1066 plant pre-miRNAs. Supplementaryshows the pre-miRNAs distribution and the species distribution in all the 128 plant families. Each family contains at least two pre-miRNAs and covers at least one plant species. After eliminating the special sequences with complex secondary structures, 1906 real pre-miRNAs remain in the positive dataset. Negative dataset: the complete genome sequence of A.thaliana was released in 2000 (AGI, 2000). The sequences of 20 chromosomes in G.max genome are released in 2010 (). Arabidopsis thaliana is a typically model plant and G.max is one of the most important crop. The negative samples are extracted from these two species. Since almost all reported miRNAs are located in the untranslated regions or intergenic regions, the pseudo hairpins are extracted from protein coding sequences (CDSs) of A.thaliana and G.max. The CDSs of A.thaliana and G.max are downloaded from the plant database Phytozome 6 (http://www.phytozome.net/). The ratios of pre-miRNAs with different length in the 2043 real pre-miRNAs are listed in. It is found that most of known plant pre-miRNAs in length ranges from 60 to 220 nt. Thus, a sliding window of width ranging from 60 to 220 nt is used to scan the CDSs to produce sequence segments. The secondary structures of the sequence segments are predicted by RNAfold from the Viennapackage (). The sequence segments should be folded into stemloop structures. Further, they should satisfy three criteria on the number of base pairs in hairpins, %G+C and MFEI. The criteria are determined by observing real plant pre-miRNAs in length among 6069 nt, 7079 nt, etc., till 220229 nt. For instance, the criteria for selecting the pseudo miRNAs in length from 60 to 69 nt are: minimum of 19 base pairings in hairpin structure, %G+C
>0.242 and <0.825 and MFEI >0.522 and <1.39. The lengthof the sliding windows changes from 60 to 69 randomly. Supplementarylisted all the criteria for different lengths. Therefore, the extracted pseudo pre-miRNAs are similar to the real pre-miRNAs. The negative samples (pseudo pre-miRNAs) are collected according to the proportion of the real pre-miRNAs of different lengths. For example, suppose the ratio of real pre-miRNAs in length 7079, 8089, 9099 and 100109 nt is 0.02:0.08:0.12:0.20. Then the negative samples in different length are added into the negative dataset in corresponding proportion. In total, 2122 pseudo pre-miRNAs are collected as negative dataset. Positive and negative training dataset: the 980 real pre-miRNAs and 980 pseudo pre-miRNAs are selected by the sample selection algorithm. The final training dataset includes a total ofpre-miRNAs and 83 G.max (gma) pre-miRNAs are used to construct the osa dataset, ptc dataset, ppt dataset, mtr dataset, sbi dataset, zma dataset and gma dataset, respectively. The remaining 1142 pseudo pre-miRNAs from 2122 pseudo pre-miRNAs (excluding the 980 pseudo pre-miRNAs) are used as 1142 negative testing dataset. The 191 A.lyrata (updated aly dataset) and 118 G.max pre-miRNAs (updated gma dataset) were newly reported by miRBase 1516 when this work was nearly completed.
Evaluation methodThe informative feature subset and the training samples were used to construct the classifier PlantMiRNAPred. The prediction result of PlantMiRNAPred can be either one of the following four outcomes: true positive (TP), false positive (FP), true negative (TN) and false negative (FN). The sensitivity (SE), specificity (SP), geometric mean (Gm) and total prediction accuracy (Acc) for assessment of the prediction system are as follows,where SE is the proportion of the positive samples (real premiRNAs) correctly classified, and SP is the proportion of the negative samples (pseudo pre-miRNAs) correctly classified.
Feature subset selection resultThe selected 68 informative features by feature selection process and the corresponding information gain are listed in. They are ranked by their normalized information gain. It has been well studied that the stemloop structures of plant pre-miRNAs is thermodynamically stable. Most of the selected features are related to the thermodynamic stability of the secondary structures. It indirectly confirms the effectiveness of the selected features. There are some features with suffix _S and three new features (MFEI 5 , MFEI 6 , Avg_mis_num) in the selected feature subset. It shows the significance of extracting the new features for the stems. In addition, some features and the corresponding features obtained from stems appear in pairs, such as A((( and A(((_S, U((( and U(((_S, etc. It indicates that there is an obvious difference between two features in any pair of the features listed above. Also, both features are important for the classification of real/pseudo pre-miRNAs. In order to validate the efficiency of the feature selection method, we tested the classification accuracies of 68 features, 80 features (containing no features of stems), 51 features (containing no structural features) and all 115 features, respectively. For each feature subset, 980 real pre-miRNAs and 980 pseudo pre-miRNAs were selected by the sample selection method to train an SVM classifier. These four SVM classifiers were tested by performing five-fold cross-validation. With five-fold cross-validation, all premiRNAs in the training dataset were divided into five equal subsets,four of which were used for training the classifier, while the left out subset was used for validation. We performed 10 repeated evaluations for each testing dataset and averaged the results. The classification results are summarized in. The classification performances of 80 features and 51 features are worse than that of 115 features. It indicates that the stem-related features and the structural features are absolutely necessary. Obviously, the classifier trained by the selected 68 features achieves the best classification performance. It shows the importance of feature selection during construction of the efficient classifier.
Training sample selection resultThe 980 positive samples and 980 negative samples with 68 features were selected by our sample selection method miSampleSelection to construct the classifier PlantMiRNAPred. Moreover, the equal number of real/pseudo pre-miRNAs were randomly selected from the positive/negative dataset, referred to as 1960 random dataset. The performance of PlantMiRNAPred was compared with the classifier trained with 1960 random dataset. As shown in, five-fold cross-validation was performed on each training dataset. The classifier trained by 1960 training dataset achieves much higher sensitivity and specificity. It demonstrates that miSampleSelection is effective for improving the classification accuracy. In addition, the classifier which was trained by the 1960 random dataset achieves excellent classification accuracy. It further confirms that the selected 68 features are sufficient to ensure the classification performance.
Comparison with triplet-SVM and microPredTriplet-SVM is the only ab initio method that has been tested with the pre-miRNAs from A.thaliana and O.sativa. Therefore, we compared PlantMiRNAPred with triplet-SVM. The program of triplet-SVM was downloaded from Xue's web site (http://bioinfo.au.tsinghua.edu.cn/mirnasvm/). The eight testing datasets composed of known pre-miRNAs from eight species were tested to evaluate the ability of identifying the real pre-miRNAs.The 1142 negative testing dataset was tested to evaluate the ability of identifying the pseudo hairpins. The Updated dataset was also tested to observe the ability of discovering new plant pre-miRNAs. We performed evaluations for all the testing datasets and illustrated the results in the Table 4. PlantMiRNAPred is nearly 18% better than triplet-SVM in overall accuracy. SE increased by 22.19% and SP increased by 12.25% on average. As many plant pre-miRNAs contain multiple loops, triplet-SVM cannot classify them correctly. Almost all the plant pre-miRNAs with multiple loops in the testing dataset are classified by PlantMiRNAPred correctly. This indicates that our method is sensitive enough to identify pre-miRNAs with multi-loops. MicroPred is more similar to our approach as it uses the same 48 features to classify pre-miRNAs. However, it was originally developed for human pre-miRNAs. The program of microPred can be downloaded from the web site (http://web.comlab.ox.ac .uk/people/manohara.rukshan.batuwita/microPred.htm). In order to compare with microPred, the classification model of microPred was changed according to the plant pre-miRNAs datasets. As shown in, PlantMiRNAPred is nearly 5% better than microPred in overall accuracy. SE increased by 5.19% on average and SP increased by 4.98%. The improvement is mainly due to the additional 32 structural features extracted from the plant pre-miRNAs and the 35 features extracted from the stems. Twenty-three of 397 O.sativa-positive samples, 19 of 233 P.patens-positive samples, two of 131 S.bicolor-positive samples are classified as pseudo pre-miRNAs. However, in miRBase 16, 9 of 23 O.sativa-positive samples, 8 of 19 P.patens-positive samples,
Page: 1375 13681376
PlantMiRNAPred
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
to >400 nt (Smalheiser and Torvik, 2005). Secondly, the molecular characteristics of plant pre-miRNAs are also different from the animal pre-miRNAs. The former have great varieties in secondary structures. For instance, like Homo sapiens miR-95, the central loops of the pre-miRNAs in animals are typically 320 nt in length (Nam et al., 2005). The loops of plant pre-miRNAs have great varieties in length, such as the loop in A.thaliana miR166c. This is shown in Figure 1. Further, some plant premiRNAs contain the big bugles, e.g. G.max miR166b in Figure 2b. Moreover, there are big unmatched parts in some plant pre-miRNAs, e.g. Physcomitrella patens miR166i in Figure 2c. Stems of plant pre-miRNAs are relatively stable and conserved. Central loops, big bugles and big unmatched parts have great diversities and are not conserved. Therefore, they are removed to obtain the stems. Two novel MFErelated features are proposed and calculated for stems, since they are more stable. (i) MFE Index 5: MFEI 5 = MFE/%G+C_S, where %G+C_S is the GC content in the stems. (ii) MFE Index 6: MFEI 6 = MFE/stem_tot_bases, where stem_tot_bases is the number of base pairs in the stems. (iii) Average number of mismatches per 21-nt window: Avg_mis_num = tot_mismatches/n_21nts, where tot_mismatches is the total number of mismatches in the 21-nt sliding windows (which is roughly the length of a mature miRNA region and naturally has fewer than four successive mismatches) and n_21nts is the number of sliding windows in a stem. For the 48 existing features and 3 new features, the 17 dinucleotide frequencies features describe the sequential characteristic. The remaining 31 features are mainly related to the thermodynamics and stability of the secondary structures of the hairpins. The current research indicates that the structural characteristic is also significant for distinguishing the hairpins of real/pseudo pre-miRNAs. Therefore, 32 structured triplet composition features [frequencies of "U(((","A((.", etc., which were defined in tripletSVM (Xue et al., 2005)] are extracted from the pre-miRNAs. Since nearly all mature miRNAs are located in stems, these 32 features are extracted again from stems and denoted as "U(((_S", "A((._S", etc. The transformation of secondary structure of a pre-miRNA is shown in Figure 2. First, loops, big bugles and big unmatched parts of pre-miRNAs are removed, in order to capture the features of stable stems. Then, the 5-arm and 3-arm are connected by a linker sequence, 'LLLLLLLL'. It is helpful to unify the length of loops in all the real/pseudo pre-miRNAs. Since 'L' is not an RNA nucleotide, it does not match with any nucleotide and prevents nucleotides in 5-arm and 3-arm from binding with sequencespecific linker sequences. Finally, the three new features (MFEI 5 , MFEI 6 and Avg_mis_num) and the 32 structure-related features are extracted from the linked stems.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from [15:38 19/4/2011 Bioinformatics-btr153.tex] Page: 1374 13681376
of 2 S.bicolor-positive samples are obtained by computational identification method. They are not verified by biology experiments. Despite this, the accuracies of the three testing datasets are 96.39, 95.11 and 100%, respectively. Most of the new reported pre-miRNAs in miRBase 1516 was correctly predicted by PlantMiRNAPred with an average accuracy of 98.11%. This shows that PlantmiRNAPred is powerful in discovering novel pre-miRNAs. In the two updated datasets, 109 of 118 G.max pre-miRNAs and 74 of 193 A.lyrata are lineagespecific pre-miRNAs. Therefore, PlantMiRNAPred is also shown to be able to achieve high performance in classifying lineage-specific pre-miRNAs. In addition, 11 918 inverted repeats were also extracted from the Gm08 (the eighth chromosome of G.max genome) by EINVERTED (Rice et al., 2000). One thousand inverted repeats (including eight real pre-miRNAs) were selected according to the proportion of the real pre-miRNAs of different lengths. Thirty-seven of 1000 are classified by PlantMiRNAPred as putative real pre-miRNAs, covering eight real pre-miRNAs. The FP rate of PlantMiRNAPred is 2.9%. MicroPred classified 89 inverted repeats as real pre-miRNAs, covering eight real pre-miRNAs. The FP rate of MicroPred is 8.1%. Triplet-SVM classified 184 inverted repeats as real pre-miRNAs, covering six real pre-miRNAs. The FP rate of triplet-SVM is 17.8%. It indicates that PlantMiRNAPred is more sensitive to the inverted repeats from the genome. 4 CONCLUSION A new ab initio classifier (PlantMiRNAPred) was developed for predicting plant pre-miRNAs. We demonstrated the importance of careful feature extraction, feature selection and training sample selection in achieving efficient and effective classification result. Particularly, according to the characteristics of plant pre-miRNAs, 115 features were extracted to distinguish the hairpins of real premiRNAs and pseudo pre-miRNAs. After eliminating redundant features, 68 informative features were selected. Each real/pseudo pre-miRNA was mapped into the 68-dimensional space. 1960 positive/negative representative samples were selected as the training dataset. PlantMiRNAPred has been compared with the existing premiRNA classification methods, triplet-SVM and microPred. The results demonstrated that PlantMiRNAPred has higher classification performance. Further analysis indicated that the improvement of classification accuracy was due to the informative features and the representative training samples. PlantMiRNAPred will be useful in generating effective hypothesis for subsequent biological testing.
