Motivation: The question of how to best use information from known associated variants when conducting disease association studies has yet to be answered. Some studies compute a marginal P-value for each Several Nucleotide Polymorphisms independently, ignoring previously discovered variants. Other studies include known variants as covariates in logistic regression, but a weakness of this standard conditioning strategy is that it does not account for disease prevalence and non-random ascertainment, which can induce a correlation structure between candidate variants and known associated variants even if the variants lie on different chromosomes. Here, we propose a new conditioning approach, which is based in part on the classical technique of liability threshold modeling. Roughly, this method estimates model parameters for each known variant while accounting for the published disease prevalence from the epidemiological literature. Results: We show via simulation and application to empirical datasets that our approach outperforms both the no conditioning strategy and the standard conditioning strategy, with a properly controlled false-positive rate. Furthermore, in multiple data sets involving diseases of low prevalence, standard conditioning produces a severe drop in test statistics whereas our approach generally performs as well or better than no conditioning. Our approach may substantially improve disease gene discovery for diseases with many known risk variants. Availability: LTSOFT software is available online
INTRODUCTIONThe NHGRI catalog of Published Genome Wide Association Studies (GWAS) () lists thousands of single nucleotide polymorphisms (SNPs) associated with several hundred complex phenotypes. However, it is currently unknown how to optimally use these discovered SNPs when conducting additional GWAS. Typically, known variants are ignored and SNPs are tested independently for association via logistic regression for case control phenotypes and linear regression for quantitative phenotypes (). Occasionally, known variants are used as covariates in the regression models to determine additional signals exist in the data beyond those already discovered, as in recent studies of Type 2 diabetes (). We show that for standard casecontrol studies neither one of these strategies, testing SNPs marginally or standard conditioning on associated variants, is optimally powered to discover new loci. Surprisingly, standard conditioning will often dramatically decrease power (). For example, in the Welcome Trust Case Control Consortium (WTCCC), Type 1 diabetes (T1D) dataset (WTCCC, 2007b), conditioning on a known variant on Chromosome 6 decreases the one degree of freedom (df)  2 statistic from a logistic regression likelihood ratio test by an average of 27% at independent known associated variants on entirely different chromosomes relative to the same test without conditioning on the
N.Zaitlen et al.Chromosome 6 variant. However, we find that if used properly, known variants can substantially improve study power and therefore represent an important resource in conducting future GWAS. In this work, we thoroughly examine the use of known associated variants in the analysis of GWAS and their effects on SNPs that are completely unlinked to the known variants (i.e. on different chromosomes or distant loci). Previously, (showed that in the case of logistic regression, including a covariate increased power when it was uncorrelated to the random variable being tested, but decreased power when it was correlated. Our extensive simulations and analysis of real gene expression and casecontrol data indicate that for randomly ascertained individuals such as those in a cross-sectional study, the practice of standard conditioning on known variants does indeed improve the power to discover new variants (), with larger gains in power as the fraction of variance explained by the conditioned SNPs increases. However, in a balanced casecontrol study in which an equal number of cases and controls are ascertained based on disease status, standard conditioning on known variants significantly decreases power when the disease prevalence is low. This power loss is due to an induced non-independence between associated variants in casecontrol datasets. That is, SNPs that were completely uncorrelated in the population become correlated when individuals are collected in a casecontrol study design, and as predicted by, there is a corresponding loss in power due to this correlation. This is true regardless of whether the data are generated under a liability threshold model of disease or the logit model of disease assumed by logistic regression. We show that the effect of standard conditioning on known variants is a function of prevalence, sample ascertainment, and the total phenotypic variance explained by the known variants. We give full analytic derivations of the non-centrality parameter of the conditioned and unconditioned tests detailing the scenarios when each improves or diminishes power. To address this power loss in the casecontrol setting, we develop a new statistic, called LTSCORE, based on the liability threshold model (). LTSCORE properly accounts for study design and disease prevalence while still leveraging the known associated SNPs. The basis for the improvement of our statistic is the incorporation of external prevalence information, which is readily available. The liability threshold model models individuals as having an unobserved continuous phenotype called the liability (). Cases are individuals whose liability exceeds some threshold while all other individuals are controls. We compute the posterior mean of the residual of the liability given an individual's disease status, the disease prevalence and the known associated variants. This posterior mean is then treated as a continuous phenotype and tested for association via linear regression while easily incorporating covariates such as principal components () (see Supplementary Material). The crucial distinction between our approach and previous applications of liability threshold modeling () is that we incorporate ascertainment strategy and disease prevalence, which is the source of the power loss for logistic regression with covariates when estimating the parameters of the model. We show that accounting for ascertainment can also be done in a relative risk framework, but the liability threshold approach is more versatile. In practice, our disease model changes dichotomous phenotypes to continuous ones. Cases are assigned positive-valued phenotypes and controls negative-valued phenotypes. Individuals carrying a smaller number of risk alleles are given a larger phenotype. The size of these shifts are a function of SNP effect size and disease prevalence, which is not accounted for in standard logistic regression. Our approach, unlike standard logistic regression, does not suffer any loss of power when the disease prevalence is low. This is not an issue with the logit model, which may also be adapted to account for ascertainment, but with the commonly used approach of adding genetic covariates to standard logistic regression in ascertained data, without accounting for disease prevalence (see Section 4). Results on empirical data, including a large Type 2 diabetes (T2D) casecontrol study and the (WTCCC, 2007b) T1D, Rheumatoid Arthritis (RA), and T2D GWAS, demonstrate the pitfalls of using logistic regression with covariates as well as the power gains of LTSCORE when compared with both logistic regression with and without covariates. The gain in power is a function of prevalence and total variance explained by the known SNPs. Our method matches or outperforms conditioned or unconditioned linear or logistic regression for nearly all values of prevalence or ascertainment examined. Its performance relative to these methods will continue to increase as more variance in disease risk is explained by risk variants that are identified. We release a software package implementing LTSCORE for use in future association studies.
METHODSGiven a normally distributed continuous phenotype Y or a casecontrol phenotype Z, we want to test candidate SNP s 0 for association with the phenotype. There are K independent SNPs s 1 ,...,s K with genotypes g 1 ,...,g K and minor allele frequencies p 1 ,...,p K known to be associated with the phenotype and in complete linkage equilibrium (e.g. on different chromosomes) with SNP s 0. SNP s 0 has genotypes g 0 and minor allele frequency p 0. In this work, we explore three classes of statistical tests of association: NOCOND, STDCOND and LTSCORE. NOCOND-log is logistic regression of the genotypes g 0 against the phenotypes without conditioning on any known genetic covariates. STDCOND-log is logistic regression where the genotypes g 1 ,...,g K are included as covariates. LTSCORE is linear regression applied to the posterior mean of the residual of the liability threshold model described below. NOCOND-lin and STDCOND-lin refer to linear instead of logistic regression. Each test generates a  2 one df test statistic by performing a likelihood ratio test. Under the alternate hypothesis the effect size of s 0 is a free parameter and under the null hypothesis the effect size of s 0 is fixed at 0. The details of logistic and linear regression models are described in (). For reasons of simplicity, the derivations below all use linear instead of logistic regression. Linear regression is commonly used in place of logistic regression in association studies (). Furthermore, we perform simulations and experiments under both linear and logistic regression frameworks to demonstrate that the theory described below holds under both models in practice (see Section 3). The extension of these tests to recessive and dominant models is straightforward. LTSCORE is publicly available in the LTSOFT software package.
Randomly ascertained casecontrol phenotypesWe begin with the case of cross-sectional dichotomous phenotypes (see Supplementary Material for Continuous phenotypes). We create a dichotomous phenotype Z under a liability threshold model (
Case-control with known risk variantsotherwise. We consider the simple case of conditioning on one SNP i.e. K = 1. In this case, the non-centrality parameter of NOCOND-lin is N * corr(g 0 ,Z) 2 .The non-centrality parameter of STDCOND-lin iswhere corr(g 0 ,Z) is the correlation between the genotypes g 0 and the phenotypes Z. The full details of the derivation are given in Supplementary Material S1. The non-centrality parameter increases in proportion to the inverse of (1fraction of variance explained by s 1 ). That is, as the known variants explain more of the phenotype, the greater our power to discover new variants by conditioning in randomly ascertained study designs (). The non-centrality parameter for STDCOND-lin is also larger than that of NOCOND-lin in the case of randomly ascertained continuous phenotypes (see Supplementary Material).
Non-randomly ascertained casecontrol phenotypesA key assumption in the derivations above is that candidates SNP s 0 and SNP s 1 are independent. However, in an ascertained casecontrol study, especially one for a disease of low-prevalence this assumption no longer holds. That is, candidate, SNP s 0 and SNP s 1 that are independent in the population will become correlated in the study cohort. Consider the extreme example of drawing cases from one tail of Y and controls from the other. Under both a logit and liability threshold model of disease, controls will have relatively fewer copies of g 0 and g 1 , while the cases will have relatively more, and so in the study, g 0 and g 1 will be correlated. This exaggerated example provides the intuition for why we see correlation in the ascertained study as we are drawing all of our cases from an extreme tail of an underlying distribution. As shown in Section 3 below, this correlation and the corresponding effects of conditioning exist under both the liability threshold model of disease and the logit model assumed by logistic regression. The covariance between g 0 ,g 1 in the case of haploid individuals (this is easily extended to the diploid case) is cov(g 0 ,The expectation of the product of g 0 * g 1 iswhere F S is the frequency of cases in the study and F is the frequency of cases in the population. In the case of random ascertainment, F S and F are the same and so the covariance will be 0. However, in a disease of low prevalence, F S and F will be different and so s 0 ,s 1 will be correlated in the study due to ascertainment-induced correlation. When we test SNP s 0 marginally (NOCOND-lin), the non centrality parameter isWhen we test s 0 conditioned on s 1 (STDCOND-lin), the non-centrality parameter iswhere X are the genotypes of the K known SNP, and  is a vector of the effects size  1 ,... K. To use both the prevalence information and the effects of the known associated variants s 1 ,...,s K when testing a new candidate SNP s 0 , we compute the posterior mean of the residual of the liability given the genotypes of the known variants X , their effect sizes , the disease prevalence F and the casecontrolwhere  2 e is 1var(X ) the residual variance of  after subtracting the variance from the known SNPs. The prevalence-aware liability threshold based statistic is then computed by running standard linear regression between the genotypes of the new SNP s 0 and the posterior mean of the residual of the liability of each individual as calculated above. Although
N.Zaitlen et al.the posterior mean is not normally distributed, the use of linear regression in place of logistic regression is common practice in association studies (). Intuitively, the above integrals have the following effect. Cases without risk alleles at other loci are assigned more extreme phenotypes than cases with risk alleles at other loci (and analogously for controls). Consider a case with no risk alleles at any of the known associated variants. To exceed the liability threshold, such an individual will require a large value relative to a case with many risk alleles at the known associated variants. Another implication of this model (as well as the relative risk model) is that the odds ratio at s 0 will be higher when computed with cases having no known risk alleles (). For fixed effect sizes , as the prevalence of the disease approaches 0, the computation of Eis dominated by the threshold m. All of the case individuals will have approximately the same value of, and all of the controls will have approximately the same value of E). Since the LTSCORE statistic is linear regression applied to E, it is equivalent to the marginal test NOCOND-lin in this case of near 0 prevalence. The liability threshold model is not the only model of disease and we also derive a prevalence aware statistic from the relative risk model of disease (RRCOND)(). The RRCOND model is presented in Supplementary Material S1, but we primarily focus on the LTSCORE because the relative risk model does not easily handle non-SNP covariates such as principal components.
Estimating  using published prevalencewhere p + 1 and p  1 are the observed frequencies of s 1 in the cases and controls. Given an estimated effect sizwhere (x,y,z) is the cumulative normal distribution evaluated at x, with mean y and variance z. Thenand similarly for g 1 = 1,2. Finally, we compute the frequency of s 1 in the cases givengiven given 1 andpand andp 1 aspand similarly for controls. Using these frequencies, we can compute the squared error between the observed and expected frequencies in the cases and controlsWe perform a binary search for 10 iterations to identify th  1 that minimizes S e. For multiple known SNPs, th  i are estimated independently and combined, and only one associated SNP from any locus can be used.
RESULTSThe theory presented in Section 2 above modeled casecontrol phenotypes under a liability threshold model and estimated the power of linear regression with no covariates (NOCOND-lin), linear regression conditioned on known variants (STDCOND-lin) and our liability threshold model-based LTSCORE, under various ascertainment scenarios. Here, we examine the relative benefits of the three classes of statistical tests NOCOND, STDCOND and LTSCORE over simulated and real data. For NOCOND and STDCOND, we conduct most of our analyses using the logistic regression versions NOCOND-log and STDCOND-log, but we have verified that NOCOND-lin and STDCOND-lin produce very similar results (see below). There are many equivalencies between the logit model, the liability threshold model and the multiplicative relative risk model (). To be maximally conservative and to demonstrate that the results derived in Methods section hold for different disease models, we simulate our case control phenotypes under a logit model. This prevents our method from having an unfair advantage due to testing the same model that generated the data. As shown below similar results were obtained when using linear instead of logistic regression and the liability threshold model instead of the logit model. LTSCORE computes posterior mean of the residual of the liability, using liability threshold model parameters that account for disease prevalence and study design, and then uses posterior mean as input to linear regression (see Section 2). The LTSCORE parameters are estimated from published disease prevalence data. This external information, unavailable to either NOCOND-log or STDCOND-log, is the basis of the improvement of LTSCORE. We are interested in the effects of known associated SNPs on association tests for undiscovered SNPs that are in complete linkage equilibrium (e.g. those on completely different chromosomes) with the known associated SNPs in the population. In both the simulated and real datasets below, we never condition on SNPs that are in LD with the candidate SNP. The derivations above assumed a liability threshold model of disease. However, both the STDCONDlog and NOCOND-log tests assume a logit model of disease as they are applications of logistic regression. We compare the the performance of the methods by measuring the ratio of the average  2 test-statistics produced by each method. This has a natural interpretation of the increase in sample size needed to obtain the equivalent power (). For example, if LTSCORE gives 10% increase in test-statistic over STDCONDlog, this corresponds to adding 10% more individuals to a study analyzed with STDCOND-log to achieve the power of the original study analyzed by LTSCORE.
Simulated datasets
Randomly ascertained casecontrol phenotypesTo examine the effect of conditioning in randomly ascertained (crosssectional) casecontrol phenotypes, we generated casecontrol data from a logit model P(Disease) = e g 0 +X +z 1+e g 0 +X +z. The affine term z determines the prevalence F of the disease in the population. To test the effects of conditioning we tested candidate SNP s 0 under NOCOND-log, STDCOND-log and LTSCORE. We ran 5000 simulations of 1000 cases and 1000 controls. In each simulation, there was one candidate SNP with effect size  and one known variant with effect size . The fraction of variance explained with K SNPs of effect size / (K) is the same as the fraction of variance explained by one SNP with effect size . LTSCORE with K SNPs of effect size / (K) produced equivalent results to using LTSCORE with one SNP of effect size  (see Supplementary Material) and so we chose to use one SNP for simplicity. The genotypes were generated as random draws from
Case-control with known risk variantsa binomial distribution for each simulation. We examined a range of known variant effect sizes , a fixed candidate SNP effect size  = 0.35, minor allele frequencies p 0 = p 1 = 0.2 and z (the affine term in the logit model) corresponding to a prevalence of F = 50%. The results are presented in. STDCONDlog always improves on NOCOND-log and the improvement is a function of the total variance explained by the known variants. LTSCORE assumes the data were generated with a liability threshold model. Despite generating data under a logit model, LTSCORE and STDCOND-log perform similarly. Reducing the prevalence F (the fraction of case individuals in the population) decreases the number of cases and increases the number of controls, but both LTSCORE and STDCOND-log still outperform NOCONDlog. Results for the liability threshold-based simulation are presented in Supplementary(a) and are similar to those presented in(a) Standard conditioning also improves power for randomly ascertained continuous phenotypes in simulations (see Supplementary Material and).
Non-randomly ascertained casecontrol phenotypesWe have seen that STDCOND-log improves the power to detect new variants at independent loci relative to NOCOND-log. Surprisingly, in a balanced casecontrol study, this is not always the case and STDCOND-log often significantly decreases the power to detect new loci. The reason for this reduction in power is the nonrandom ascertainment of the samples which induces a correlation between all the causal variants. The strength of the correlation between associated variants is a function of disease prevalence. The STDCOND-log test on any set of associated variants will not only remove their signal but also some of the signal from the SNP being tested. We simulated a low-prevalence casecontrol phenotype under a logit model as in the randomly ascertained experiments described above with F = 0.1%,  = 2.0 and  = 2.0. We then sampled 1000 cases and 1000 controls and measured the correlation between candidate SNP s 0 and SNP s 1. The average correlation in 5000 simulations was r 2 = 0.11 ( 2 = 220 via Armitage trend test). We used an extreme  to demonstrate the effect with a small number of simulations. To examine the relative behaviors of the three classes of tests in casecontrol data, we simulated a casecontrol phenotype under a logit model as in the randomly ascertained experiments described above with F = 4.0%,  = 0.35 and minor allele frequency MAF= 0.20 for both SNPs. We then sampled 1000 cases and 1000 controls. The results are presented in(b). When  is small, LTSCORE is nearly identical to NOCOND-log losing 0.5% in the worst case. The improvement of LTSCORE relative to NOCOND-log increases as the known variant  explains more the population phenotypic variance. STDCOND-log decreases in performance relative to NOCOND-log until the known variant explains at least 35% of the population phenotypic variance, at which point STDCOND-log starts to improve. However, even after the known variant explains 50% of the in study phenotypic variation, STDCOND-log achieves only 96.8% of the NOCOND-log statistic. Note that the results presented in(b) refer to the fraction of study variance not population variance explained. Because of the ascertainment strategy, there is a significant difference between the effect sizes of the SNPs in the study and their effect size in the population. In the population, only 4.0% of individuals are cases, while in the study, 50% of individuals are cases. This skew causes thevariance explained by a SNP in the population to be much smaller than the variance explained in the study (). Results for the liability threshold based simulation are similar and presented in Supplementary. We repeated the experiments for Figure1a and b replacing logistic regression with linear regression and found nearly identical results, Supplementaryand b. We conclude that replacing linear with logistic regression makes little difference in this context and use only logistic regression for the remaining experiments (). To examine the effects of prevalence on the three tests, we fixed  = 1.5,  = 0.35 MAF = 0.2 and varied the disease prevalence F under the same model as above. The results presented inshow that the LTSCORE always outperforms STDCOND-log. STDCOND-log reduces power compared with the NOCOND-log test when the prevalence is low. However, as the prevalence increases, the study becomes more like a randomly ascertained study and the STDCOND-log test performance increases above the NOCOND-log test. LTSCORE is slightly (<2%) worse than NOCOND-log for very low-prevalence (0.1%) disease and improves as the prevalence increases. This modest loss in power is removed when the data are generated under a liability threshold model (see Supplementary). In this case LTSCORE always outperforms or matches NOCOND-log and STDCOND-log. It is unknown which model better represents the truth about disease. We tested the sensitivity of our model to the misspecification of the prevalence F by generating data under the same model as above for a disease with true prevalence of 3%. We tested under our LTSCORE model for a range of 'estimated' prevalences. We repeated the simulation 5000 times, with 1000 cases and 1000 controls. The results are presented in Supplementary. Changing the estimated prevalence between 1% and 5% had a minimal effect and the performance in this case was greater than either the NOCOND-log or STDCOND-log tests. The power was greater than NOCOND-log until the specified prevalence was greater than twice the true prevalence. The maximum power was not attained at the true prevalence and we believe this is because the disease model tested (liability threshold) is different than the disease modelcontrol phenotypes from a logit model, as a function of disease prevalence. Under low-disease prevalence, there is an induced correlation between associated variants causing a sever loss of power for the STDCOND-log test relative to the NOCOND-log test. As the prevalence increases, the design is more like a randomly ascertained study and the STDCOND-log test outperforms the NOCOND-log test. The LTSCORE always outperforms STDCOND-log. For low-prevalence disease, LTSCORE is slightly worse than NOCOND-log and improves as the prevalence increases used to generate the data (logit). Results for the liability thresholdbased simulation are presented in Supplementaryand in this case, the maximum is attained at the true prevalence. To examine the behavior of the tests under the null, we repeated the experiments for a range of prevalences F = 0.01,0.03,0.05,0.1 and setting the effect size  = 0 and keeping  = 1.5. For each prevalence, we generated 1000 cases and 1000 controls 1000000 times. All three tests were well behaved maintaining a false positive rate of 0.050 as desired. To inform researchers about the potential gains available in their casecontrol datasets, we include the average  2 test statistics for all three tests for a range of realistic disease parameters in Supplementary Table S8.
Real data sets
Non-randomly ascertained datasetsfor low-prevalence disease (T1D, RA) We begin with an analysis of low-prevalence casecontrol phenotypes (see Supplementary Material for real continuous phenotypes). We examined the performance of the NOCOND-log, STDCOND-log and LTSCORE statistics on the WTCCC T1D and RA datasets (WTCCC, 2007b). There were 1924 and 1860 cases for RA and T1D respectively, and the same set of 2938 controls for the two datasets. For T1D, we used a prevalence of 0.125% () and HLA SNP rs9273363 from Chromosome 6 as the known variant which explained 12.4% phenotypic variation () in the study. For RA, we used a prevalence of 1% () and HLA SNP rs6457620 from Chromosome 6 as the known variants which explained 7.1% phenotypic variation in the study. We filtered out all SNPs with MAF <5% and applied the NOCOND-log, STDCOND-log and LTSCORE, tests to all SNPs not found on Chromosome 6. Although the WTCCC studies identified a relatively small number of risk loci due to limited sample size, for T1D and RA this includes HLA, a locus of large effect. The prevalences of T1D and RA are low so the expected improvement of LTSCORE relative to STDCONDlog is not expected to be large (see Section 3.1). However, these datasets demonstrate the potential for a severe loss in power of using STDCOND-log and that LTSCORE is well behaved for lowprevalence diseases. Indeed, for T1D, there was a greater than 27% drop in test statistic using STDCOND-log relative to NOCONDlog and a 4% increase using LTSCORE relative to NOCOND-log as measured by the average change in test statistic at all published GWAS variants according to the NHGRI () (see Supplementary Tables S1S8). The QQ plots of NOCOND-log, STDCOND-log and LTSCORE are shown inand b and serve as one means of assessing the relative performance of the methods. The significant SNPs lie at the tail of the distribution and methods with larger values at the tail are better powered. All of the test statistics had a similar  GC and all were genomic control (GC) corrected before analysis (). On the RA dataset for example the  GC values were 1.046, 1.047 and 1.041, for the NOCOND-log, STDCONDlog and LTSCORE tests, respectively. It is clear that STDCOND-log reduces the  2 test statistic relative to NOCOND-log and LTSCORE in T1D () and RA (). The reduction in T1D is the most dramatic because it has a very low-prevalence and the SNPs explain a larger fraction of the variance. As another means of assessing the relative performance of the methods, we look at the test statistics of known associated variants published in the NHGRI catalog (). When the known associated variant was missing from the dataset, we used the best tag as measured by r 2 , removing any SNP where the best tag had r 2 < 0.5. The results are presented in Supplementary Tables S1,S2 and are analogous to the QQ plot results. STDCOND-log performs poorly for T1D and RA with a reduction in the sum of test statistics of roughly 27% in T1D equivalent to removing 27% of the individuals from the study (). On the other hand, LTSCORE has slightly larger sum  2 test statistics relative to NOCOND-log. We simulated 1000 casecontrol studies with effect sizes, prevalences and sample sizes matching the WTCCC studies. We generated the data under a liability threshold model and found expected gains for both studies close to 2% relative to NOCOND-log.
Non-randomly ascertained datasets for high-prevalence disease (T2D)We examined the performance of the NOCONDlog, STDCOND-log and LTSCORE statistics over of 6142 cases and 7403 controls genotyped at 19 known associated SNPs from the Multiethnic Cohort (MEC) (Americans, Native Hawaiians, and European Americans) () and used a prevalence of 9% (). Unfortunately, the known associated variants together explain onlyThe tail of the plots serves as an empirical measure of improvement. In T1D (a), the LTSCORE outperforms the NOCOND-log test and the STDCOND-log test suffers significant power loss. In RA (b) the LTSCORE matches the performance of the NOCOND-log test and again the STDCOND-log test suffers significant power loss. In T2D (c) and T2D+CONTROLS (d) LTSCORE and NOCOND-log perform similarly. STDCOND-log improves significantly with the addition of controls, which mimics a randomly ascertained design 4% phenotypic variation in the study. We simulated 1000 datasets with the same sample size, a disease prevalence of 9%, and a known associated variant that accounted for 4% of the phenotypic variation. For an SNP with a minor allele frequency of 20% and an effect size on the liability scale of 0.05 (corresponding to 1.6% of the variance on the liability scale), the average improvement of LTSCORE was 3% with a standard error of 10% in the simulations. Using many SNPs of small effect produced, the same result as one SNP of large effect. The results on the MEC data are shown in Supplementarywith LTSCORE slightly outperforming NOCOND-log, but not significantly different from STDCOND-log. The variance of the expected improvement is large and this improvement is within the expected range. As expected, the relative performance of STDCOND-log in this high-prevalence disease is much better than it was in T1D and RA. We examined the relative performance of NOCOND-log, LTSCORE and STDCOND-log in the WTCCC T2D study with 1924 cases. We used the 2938 controls in the original study and we created a large control set (+CONTROLS) for T2D containing individuals in all other diseases with a sample size of 14255. We note that the use of cases from other diseases as shared controls is commonplace in WTCCC and other studies (WTCCC, 2007a, b). The known variants explained 1.42% and 0.64% in the original study and T2D+CONTROLS respectively. The results are shown in Supplementary Tables S5 and S6. The expected improvement is even smaller than in the MEC study above as a smaller fraction of the variance is explained and LTSCORE performed slightly worse than STDCOND-log but within the range predicted by simulations (1  6%). The performance of STDCOND-log is affected by the addition of controls as this simulates the properties of random ascertainment where STDCOND-log is expected to perform better. In the original study NOCOND-log had an 8% higher sum of test statistics than STDCOND-log, while in the T2D+CONTROLS study, this was reduced to 2%. The QQ plots of NOCOND-log, STDCOND-log and LTSCORE are shown inand d. STDCOND-log reduces the  2 test statistic relative to NOCOND-log and LTSCORE in T2D 3(c). In the case of T2D+CONTROLS, the large number of controls create a study that is more similar to random ascertainment. As expected, STDCOND-log improves over NOCOND-log in this case as shown in. The LTSCORE method performs well in all instances, matching or outperforming each of the other tests.
Case-control with known risk variants
DISCUSSIONWe have shown that the practice of standard conditioning on known associated variants does not account for study design and disease prevalence potentially leading to significant power loss. This power loss is due to the induced correlation between associated variants in casecontrol studies. The phenomenon of higher odds ratios in cases with fewer risk alleles at other loci than in cases with more risk alleles at other loci can be viewed as a gene gene interaction (). This is a statistical, rather than biological, interaction. By properly modeling the ascertainment and prevalence while still leveraging known associated variants, our LTSCORE statistic improves study power relative to NOCONDlog and STDCOND-log tests in casecontrol studies of mid-to-low prevalence diseases. This increase in power is a function of the total phenotypic variance explained by known variants and disease prevalence. The datasets examined here had either a low-prevalence or a small fraction of the variance explained and therefore we did not expect a large improvement. However, as more associated variants are discovered, the performance of LTSCORE will increase giving rise to power gains as a function of covariate effect size and disease prevalence. This approach can also be applied to clinical covariates, and in this case, an average power gain of >17% was achieved (unpublished data). We have verified that results similar to Supplementaryare obtained when comparing genetic + clinical covariates to clinical covariates only (see Supplementary). However, conditioning on clinical covariates is a fundamentally different problem, both because a different parameter estimation method is needed and because with clinical covariates, it is often the case that samples are non-randomly ascertained for covariate value as well as casecontrol status. A recent T2D meta-analysis () uses the standard conditioning statistic and shows a significant gain in power. Their ratio of cases to controls is closer to a randomly ascertained study and in this case we expect STDCOND-log to outperform NOCOND-log and increase power. In addition to their beneficial study design, some of the conditioned variants are proximal to the new discoveries. Both of the elements serve to improve the power of standard conditioning. () also examine the potential benefits of genomewide conditioning in T2D. However, we believe the use of our LTSCORE statistic on these data could improve the power further
N.Zaitlen et al.by accounting for prevalence and ascertainment. In a recent metaanalysis of GWAS height data (), a randomly ascertained continuous phenotype, standard conditioning revealed no new associated variants. This is due to the nature of their study design and not a contradiction of our results (see Supplementary Material S1). In their landmark T1D paper,find a correlation between disease risk computed from HLA SNPs and disease risk computed from SNPs in the rest of the genome. They suggest that this is due to a departure from a multiplicative model of disease. However, this effect may also be explained from the non-independence of the genotypes that we described in case control studies. That is, some or all of the effect that was described (correlation between MHC major histocompatibility complex risk score and non-MHC risk score) may be due to ascertainmentinduced correlation. We caution that in tests for epistatic interaction (), this induced correlation could give rise to a spurious signal of epistatic interaction at true (marginally) associated variants. Adjustment for informative covariates is not unique to genetics and the problem of estimation from casecontrol data has received considerable attention in the epidemiological literature. It is well known that regressing or stratifying on a covariate which is related to disease but not exposure of interest causes a reduction in power unless one matches on the covariate when sampling controls (). We derive this power loss in terms of the liability threshold model. (shows the reduction in power under a logit model for any correlated covariate (i.e. not just due to ascertainment). Although we focus on adapting the liability threshold model to incorporate prevalence information, it may be possible to achieve the same result in a logistic framework. For example, if there is only one known variant, one could construct a 222 table of casecontrol status, candidate SNP s 0 and known covariate s 1. Much larger tables would be required as the number of known variants increased. We recently proposed () a weighted logistic regression method (IPW) in the case of conditioning on environmental variables in casecontrol studies. Rose and van der Laan (2008) also offer an efficient estimator for casecontrol studies to account for ascertainment-induced biases. However, the focus of these works is obtaining an unbiased estimate of effect size while our concern is power (and a valid test under the null). In the case of genetic association studies, the effect sizes are generally small and the emphasis of the community is on discovery as opposed to effect size estimation. In the case of IPW, unbiased effect sizes are indeed obtained, but it under-performed relative to STDCOND-log, NOCOND-log and LTSCORE in simulations so is not considered. If the objective is to obtain unbiased effect sizes, IPW is recommended over LTSCORE. Note that the basis for the improvement of LTSCORE is the published prevalence data and not published SNP effect sizes. It is not equivalent to using STDCONDlog with an offset, which will perform similarly to STDCOND-log in the presence of ascertainment. Including an explicit interaction term in the logistic model introduces an extra df reducing the overall power. Although this paper focuses exclusively on the use of conditioning to discover new loci that are completely unlinked to the known variants, conditioning is also a widely used tool for SNPs in the same locus. In this case, the purpose is to perform fine-mapping and better understand the genetic architecture of the known associated locus (). Therefore, any drop in power due to induced correlation should not prevent researchers from using conditioning in this same-locus context. LTSCORE may improve fine-mapping efforts in some situations (see Supplementary Material). A discussion of usage and meta-analysis is given in the Supplementary Material.
The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
var(Z) 2 1 , (6) where  0 , 1 are the expected SNP effect sizes in the study (as opposed to  0 , 1 the effect sizes in the population). The full details of the derivation are given in Supplementary Material S1. In the marginal case (NOCOND-lin), the shared signal of s 0 and s 1 is added to the non-centrality parameter. This implies that the power to detect s 0 in the marginal case is greater if there exists another SNP s 1 that explains a significant fraction of the variance. In the conditioned case (STDCOND-lin), the numerator is decreased because the shared signal of s 0 and s 1 is conditioned out. However, the denominator is also smaller since the variance of Z conditioned on s 1 is smaller than the unconditioned variance of Z. The power of STDCOND-lin relative to NOCOND-lin is therefore a function of effect size, prevalence and ascertainment. Yang and colleagues (Yang et al., 2010) provide alternative derivations of the non-centrality parameter in the unconditioned case for both quantitative and casecontrol phenotypes based on the liability threshold model. In the case of non-randomly ascertained quantitative phenotypes, two associated variants s 0 and s 1 that are independent in the population will be correlated in the study for the reasons given above. We do not consider this case in detail in this work but note that in many cases, STDCOND-lin will reduce power significantly and we therefore caution against this statistic for non-randomly ascertained quantitative phenotypes. 2.3 LTSCORE statistic We model a casecontrol phenotype as arising from an underlying normally distributed phenotype  =m+;  N (0,1) (7) called the liability (Falconer, 1967). Cases are those individuals with   0 and controls are those individuals with  <0. There is a relationship between this liability scale and the relative risk model of disease described in detail previously in Wray et al. (2010) and Yang et al. (2010). If F is the prevalence of the disease in the population then m = 1 (1F), where 1 (x) is the inverse of the cumulative normal distribution function with mean 0 and variance 1 evaluated at x, so that the expected proportion of individuals with   0 is F. A SNP s 1 associated with the disease and having mean adjusted genotypes g 1 {02p,12p,22p} is incorporated into the model as  =m+ 1 g 1 +, where  N (0, 1var( 1 * g 1 )) so that the total variance of  is 1. Given a casecontrol study where SNP s 1 has frequency p + 1 in the cases and frequency p  1 in the controls, we estimate  1 via a method (described below) that relies on published prevalence data for the disease. This prevalence represents a source of external data not available to STDCOND-lin. The estimation procedure is repeated for independent known associated SNPs s 2 ,...,s K giving a final model
