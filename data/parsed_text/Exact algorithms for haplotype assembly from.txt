Motivation: Haplotypes play a crucial role in genetic analysis and have many applications such as gene disease diagnoses, association studies, ancestry inference and so forth. The development of DNA sequencing technologies makes it possible to obtain haplotypes from a set of aligned reads originated from both copies of a chromosome of a single individual. This approach is often known as haplotype assembly. Exact algorithms that can give optimal solutions to the haplotype assembly problem are highly demanded. Unfortunately, previous algorithms for this problem either fail to output optimal solutions or take too long time even executed on a PC cluster. Results: We develop an approach to finding optimal solutions for the haplotype assembly problem under the minimum-error-correction (MEC) model. Most of the previous approaches assume that the columns in the input matrix correspond to (putative) heterozygous sites. This all-heterozygous assumption is correct for most columns, but it may be incorrect for a small number of columns. In this article, we consider the MEC model with or without the all-heterozygous assumption. In our approach, we first use new methods to decompose the input read matrix into small independent blocks and then model the problem for each block as an integer linear programming problem, which is then solved by an integer linear programming solver. We have tested our program on a single PC [a Linux (x64) desktop PC with i7-3960X CPU], using the filtered HuRef and the NA 12878 data-sets (after applying some variant calling methods). With the all-hetero-zygous assumption, our approach can optimally solve the whole HuRef data set within a total time of 31 h (26 h for the most difficult block of the 15th chromosome and only 5 h for the other blocks). To our knowledge, this is the first time that MEC optimal solutions are completely obtained for the filtered HuRef dataset. Moreover, in the general case (without the all-heterozygous assumption), for the HuRef dataset our approach can optimally solve all the chromosomes except the most difficult block in chromosome 15 within a total time of 12 days. For both of the HuRef and NA12878 datasets, the optimal costs in the general case are sometimes much smaller than those in the all-heterozygous case. This implies that some columns in the input matrix (after applying certain variant calling methods) still correspond to false-heterozygous sites. Availability: Our program, the optimal solutions found for the HuRef dataset available at http://rnc.r.dendai.ac.jp/hapAssembly.html. Contact:
INTRODUCTIONA haplotype is the sequence of SNPs in each of the two copies of a given chromosome in a diploid organism. Haplotypes are crucial for genetic analysis and have many applications such as gene disease diagnoses, association studies, ancestry inference, drug design and so forth (). Traditional approaches to obtaining haplotypes are based on genotype data from a set of individuals. The genotype data tell the status of both alleles at a position without distinguishing which one is on a specific copy of the chromosome. This approach is generally known as haplotype phasing. One can use various algorithms to infer the haplotypes (). A drawback of this approach lies in its weakness in identifying rare and novel SNPs (). Besides, it is hard to verify whether the inferred haplotype is completely correct. With the development of high-throughput sequencing technologies, an alternative way to obtain the haplotypes for an individual is to combine sequence fragments, which is known as haplotype assembly (). Given a set of aligned reads sequenced from the two copies of a given chromosome of a single individual, the goal of haplotype assembly is to correctly determine two haplotypes, each of which corresponding to one of the two copies of the chromosome. The haplotype assembly problem was first introduced by. Basically, when reads contain errors, the reads cannot be partitioned perfectly into two disjoint sets each of which consists of non-conflicting reads. To deal with errors when looking for the best reconstruction of haplotypes, one has to fix an objective function for evaluating candidate haplotypes. For this purpose, various functions such as Minimum Fragment Removal, Minimum SNP Removal, Longest Haplotype Reconstruction, Minimum Error Correction (MEC), Minimum Implicit SNP Removal and Minimum Implicit Fragment Removal have been subsequently proposed (). Recently,proposed the Minimum Weighted Edge Removal function, whereasproposed the Maximum Fragments Cut function. Of special interest among the proposed functions is *To whom correspondence should be addressed. MEC, which aims at minimizing the total number of conflicts (errors) between the reads and the constructed haplotypes h 1 , h 2 . The problem of minimizing MEC is NP hard (). For this problem,presented an exact algorithm based on the branch-andbound method and a genetic algorithm. A weighted version of this problem is considered by. In the remainder of this article, we only consider the problem of minimizing MEC.presented the first diploid genome sequence of an individual human, J. Craig Venter, using Sanger sequencing technology. They also designed a greedy heuristic method that concatenates the reads with minimum conflicts. Their method is fast but not accurate when errors appear in reads.developed a software package (named HapCUT), and their algorithm tries to minimize the MEC score of the reconstructed haplotypes by iteratively computing maxcuts in graphs derived from the sequenced fragments.designed a Markov chain Monte Carlo algorithm (named HASH). Both HASH and HapCut work well in practice, but there is no guarantee of finding optimal haplotypes. Recently,proposed a dynamic programming algorithm for the problem that runs in time O2 k mn, where k is the length of the longest read, m is the number of reads and n is the total number of SNPs in the haplotypes. Their experiments show that their algorithm works well when k 15. On the other hand, when k is large, they model the problem as a MaxSAT problem, which is then solved by a MaxSAT solver. To compare their MaxSAT approach with the previous methods, they use the filtered HuRef dataset fromover 22 chromosomes. Via experiments, they show that their program can construct better haplotypes than the previous programs by. It is worth pointing out that to solve the problem for the 22 chromosomes, their program takes a total time of $15h on a PC cluster. Moreover, their program does not solve the problem exactly because it excludes certain reads (3725 reads in total) from consideration. Furthermore, their program fails to find optimal haplotypes for a total of eight blocks of the 22 chromosomes. In this article, we develop a new approach to optimally solving the problem. In our approach, we first use new methods to decompose the input read matrix into small independent blocks and then model the problem for each block as an integer linear programming (ILP) problem, which is then solved by an ILP solver [such as CPLEX (IBM ILOG CPLEX Optimizer) and GLPK(GNU Linear Programming Kit)]. We have tested our program on a single PC [namely, a Linux (x64) desktop PC with i7-3960X CPU], using the filtered HuRef dataset. Our experimental results show that our program can optimally solve all the chromosomes within a total time of 31 h (26 h for the most difficult block of the 15th chromosome and only 5 h for the other blocks). To our knowledge, this is the first time that optimal haplotypes under the MEC model are completely obtained for the filtered HuRef dataset. Moreover, to find almost optimal haplotypes within much shorter time for the difficult blocks, we propose several powerful heuristic methods.have generalized the problem by removing the all-heterozygous assumption to handle the existence of a small number of homozygous sizes in the solution. The generalized problem is much harder because it allows many more candidate haplotypes. Nevertheless, we develop a program that can optimally solve the generalized problem for all the 22 chromosomes of the filtered HuRef dataset except the most difficult block of the 15th chromosome within a total time of 12 days. As far as we know, this is the first strike on computing optimal solutions for the HuRef dataset without the all-heterozygous assumption. Moreover, to find almost optimal haplotypes within much shorter time for the difficult blocks, we propose several powerful heuristic methods. Via experiments with the simulated dataset of Geraci (2010), we show that an optimal solution for the general case of the problem achieves a better reconstruction rate than an optimal solution for the all-heterozygous case of the problem.
METHODS
The haplotype assembly problemFor convenience, we define a ternary string to be a string whose characters each belong to f0, 1, g. The extended Hamming distance between two ternary strings s and t, denoted by ds, t, is the total number of positions p at which both characters of s and t belong to f0, 1g, but they are different. Two binary strings h and h 0 of the same length are complementary if the bit of h at every position is different from the bit of h 0 at the same position. In the haplotype assembly problem, we are given a matrix of X whose entries each belong to f0, 1, g (i.e. each row of X is a ternary string). Supplementary Table S1 shows an example X. Such an X is constructed from a given reference genome sequence and the set of reads containing sequence from both chromosomes, by aligning all the reads to the reference genome and then applying some SNP/variant calling methods. See Section 2 of the Supplementary Material for details. Each row of X corresponds to a read, whereas each column corresponds to an SNP site. The first (last, respectively) entry of a read that is not a '' is called the start (end, respectively) position of the read. There can exist ''s between the start and the end positions of a read. Such ''s are called gaps of the read because they either correspond to missing data or serve as gaps to connect disjoint parts of the read. If a read has no gap, it is called a gapless read; otherwise, it is called a gapped read. Moreover, if the gaps of a gapped read appear consecutively, then it is called a paired-end read; otherwise, it is called a multi-gapped read. The length of a read is j e  j s  1, where j e and j s are the end and the start positions of the read, respectively. Given X, we want to compute the unknown haplotypes, which are an unordered pair H  h, h 0  of binary strings each of length n, where n is the number of columns in X. Such a pair is called a solution of X. As the haplotypes are unknown and there are many solutions for them, we need a criterion for evaluating solutions. The MEC score is such a criterion and is defined as follows: Given X, the MEC score of a solution H  h, h 0  of X is P m i1 min  dr i , h, dr i , h 0   , where m is the number of rows in X and r i is the ith row of X. Given X, the haplotype assembly problem asks for a solution of X whose MEC score is minimized. Such a solution is called an optimal solution of X. For convenience, we say that a row of X is useless if every entry in the row is a '' and is useful otherwise. Clearly, the removal of useless rows from X does not change the problem. Moreover, we say that a column of X is monotone if 0 or 1 does not appear in the column. Obviously, if the jth column of X contains no 0's (1's, respectively), then X has an optimal solution h, h 0  such that the jth bits of h and h 0 are 1's (0's, respectively). Recall that a diploid organism has two alleles at each position. An SNP site is homozygous if the two alleles at this site are identical; otherwise, it is heterozygous. When the reads contain no errors, a homozygous SNP site corresponds to a monotone column in X, whereas a heterozygous SNP site corresponds to a non-monotone column in X. However, when reads contain errors at a homozygous SNP site, the corresponding column can also be non-monotone. Therefore, we may hereafter make the following assumptions: A1. No row of X is useless. A2. No column of X is monotone. A3. No column of X contains more 1's than 0's.We make Assumption A3 only for a technical reason. If the jth column of X contains more 1's than 0's, then we can modify the column by flipping 1's and 0's so that each solution h, h 0  of the modified X can be transformed back to a solution of the original X with the same MEC score by flipping the jth bits of h and h 0. Therefore, it does not matter to instead assume that no column of X contains more 0's than 1's. Assumption A2 ensures that all the columns of X contain both 0's and 1's. However, some originally homozygous sites may also be included in X owing to errors in the reads. Given a solution h, h 0  of X, we can obtain a bipartition H, H 0  of the rows in X as follows. Each row r i of X with dr i , h dr i , h 0  belongs to H, whereas each row r i of X with dr i , h4dr i , h 0  belongs to H 0. Consider an arbitrary integer j 2 f1, 2,. .. , ng. Let h j, 0 (h j, 1 , respectively) be the number of rows r i in H such that the entry in the jth column of r i is a 0 (1, respectively). Similarly, let h 0 j, 0 (h 0 j, 1 , respectively) be the number of rows r i in H 0 such that the entry in the jth column of r i is a 0 (1, respectively). We define the contribution of the jth column of X to the MEC score of h, h 0  as follows: Let h j and h 0 j be the jth bit of h and h 0 , respectively. If h j h 0 j is 00 (11, 01 or 10, respectively), then the contribution is h j, 1  h 0 j, 1 (h j, 0  h 0 j, 0 , h j, 1  h 0 j, 0 or h j, 0  h 0 j, 1 , respectively). The MEC score of h, h 0  is the total contribution of the columns of X.
The all-heterozygous caseRecall that each column of X is not monotone based on Assumption A2. When there is no error, every non-monotone column in X should correspond to a heterozygous site in the solution. When errors occur, it is still true that most of the columns in X correspond to heterozygous sites in the solution. This motivates researchers to assume that all sites corresponding to the columns in X are heterozygous, i.e. we should search for solutions h, h 0  of X such that h and h 0 are complementary (). In this subsection, we show how to solve this case by reductions and ILP. For each pair of integers j 1 , j 2  with 1 j 1 j 2 n, we use Xj 1 , j 2  to denote the submatrix of X consisting of the j 1 th, j 1  1st ,. .. , j 2 th columns of X. For brevity, we simply use Xj to denote Xj, j. Block reduction: Like most of the existing methods for the problem, we split the whole chromosome into a set of column-disjoint blocks such that no read starts and ends at different blocks. This reduction can be done in time linear in the total length of reads in X. Many rows of the blocks are useless and can be removed immediately. A singular block is a useful block consisting of a single column. Singular blocks can be ignored because the problem for them is trivially solved. Block decomposition: Suppose that B is a block of X with ' columns.Consider an integer j with 15j5'. If there is no read r in B such that j is greater than the start position of r but less than the end position of r, then we say that Bj is a splittable column. Suppose that the splittable columns of B are Bj 1 , Bj 2  ,. .. , Bj k . Then, the submatrices B1, j 1 , Bj 1 , j 2 , Bj 2 , j 3  ,. .. , Bj k1 , j k , Bj k , ' are called unsplittable blocks of X. Many rows of these blocks are useless and can be removed immediately. An important observation is that we can solve the problems for these blocks independently (say, in parallel) and then concatenate their optimal solutions h 1 , h 0 1 , h 2 , h 0 2  ,. .. , h k1 , h 0 k1  into an optimal solution h, h 0  of B as follows. Consider an integer i with 1 i k. As h i and h 0 i are complementary, one of them ends with a 0 and the other ends with a 1. Similarly, one of h i1 and h 0 i1 starts with a 0 and the other starts with a 1. Therefore, the last character of h i is the same as the first character of h i1 or h 0 i1. We assume that the last character of h i is the same as the first character of h i1 ; the other case is similar. Then, we can concatenate h i , h 0 i  and h i1 , h 0 i1  by (1) deleting the last character of h i and then appending h i1 to h i and (2) deleting the last character of h 0 i and then appending h 0 i1 to h 0 i. This shows that we can reduce the original problem for B to the smaller problems for B1, j 1 , Bj 1 , j 2 , Bj 2 , j 3  ,. .. , Bj k1 , j k , Bj k , '. We call this reduction block decomposition. It can be done in time linear in the total length of reads in B. Singleton removal: Suppose that C is an unsplittable block of X. A row in C is singular if the start and the end positions of the read corresponding to the row are the same. Obviously, the removal of a singular row from C does not change the MEC score of an optimal solution of C. Therefore, we can reduce the problem for C to a smaller problem by removing all singular rows from C. This type of reduction is called singleton removal and can be done in time linear in the total length of reads in C. Duplicate removal: Suppose that C is an unsplittable block of X. Duplicate removal is the reduction that repeats modifying C as follows, until no two rows or columns of C are identical:(1) Merge identical rows into a single row and memorize the original multiplicity of the row. (Of course, if a row is originally identical to no other row, then its multiplicity is 1.)(2) Merge identical columns into a single column and memorize the original multiplicity of the column. (Of course, if a column is originally identical to no other column, then its multiplicity is 1.)Duplicate removal can be done in OL log k time, where k is the number of reads in C and L is the total length of reads in C. Solving reduced-blocks via ILP: A reduced block of X is an unsplittable block of X without singular rows, identical rows or identical columns. Suppose that D is a reduced block of X. To solve the problem for D, we formulate the problem as an ILP problem and then solve it using CPLEX of IBM, which is a freely available ILP solver for academic research. Let p (q, respectively) be the number of rows (columns, respectively) of D. For each integer j with 1 j q, let c j be the multiplicity of Dj. Similarly, for each integer i with 1 i p, let w i be the multiplicity of the ith row of D, and J i, 0 (J i, 1 , respectively) be the set of integers j 2 f1, 2,. .. , qg such that the ith entry in Dj is a 0 (1, respectively). As we want to compute an optimal pair h, h 0  of complementary haplotypes for D, we introduce a binary variable x j for Dj whose value is supposed to be 1 if and only if the jth bit of h is a 1 (and hence the jth bit of h 0 is a 0). Moreover, we introduce a binary variable y i for the ith row of D whose value is supposed to be 1 if and only if the read corresponding to r i is aligned to h, where r i is the ith row of D. Then, the problem of finding an optimal pair h, h 0  of complementary haplotypes for D becomes the following integer programming problem:Subject to 8 1 i p y i 2 f0, 1g 8 1 j q x j 2 f0, 1gThe aforementioned integer programming problem is not linear because it contains quadratic terms such as x j y i. Therefore, for each pair i, j with 1 i p and 1 j q, we introduce a binary variable t i, j (for replacing x j y i ) and add the following three constraints to ensure that t i, j  y i x j :The resulting ILP formulation is given in the Supplementary Material. It is easy to construct the ILP from D in O(Lp) time, where L is the total length of reads in D. A powerful heuristic for hard blocks: Let C be an unsplittable block of X. Our experiments will show that performing singleton and duplicate removal on C usually yields a reduced block D of X such that D is small enough that the aforementioned ILP problem for D can be solved by CPLEX within several seconds on a single PC. However, although rare, it is possible that the aforementioned ILP problem for D cannot be solved by CPLEX within several hours. In this case, we can solve the problem for C heuristically as follows.(1) First, we choose an integer j with 1 j5q, where q is the number of columns in C. We refer to j as the cut position. The choice of the cut position is important, and we will detail how to choose it later.(2) Then, we perform singleton and duplicate removal on C1, j and Cj  1, q and further use CPLEX to solve the smaller problems for the two resulting reduced blocks independently (say, in parallel) to obtain optimal solutions h j, 1 , h 0 j, 1  and h j, 2 , h 0 j, 2  for C1, j and Cj  1, q. Let s j, 1 and s j, 2 be the MEC scores of the two solutions. Obviously, s j, 1  s j, 2 is a lower bound on the MEC score of an optimal solution of C.(3) We concatenate the solutions obtained in Step 2 to obtain two solutions h j, 1 h j, 2 , h 0 j, 1 h 0 j, 2  and h j, 1 h 0 j, 2 , h 0 j, 1 h j, 2  of C. Unfortunately, it is often the case that neither solution has a MEC score close to the lower bound s j, 1  s j, 2. Therefore, we refer to them as the raw solutions for C associated with j and will design a method for refining them. Finally, among the two refined solutions, we output the one with the smaller MEC score. For choosing the cut position, we propose the following two methods: C1. For each j 2 f1,. .. , q  1g, we use an approximation algorithm to estimate the MEC score s 0 j, 1 (s 0 j, 2 , respectively) of an optimal solution of C1, j (Cj  1, q, respectively). We measure the quality of j by js 0 j, 1  s 0 j, 2 j. By trying all j 2 f1,. .. , q  1g, we can find the j with the best quality. The intuition behind this choice of the cut position is that if s 0 j, 1 and s 0 j, 2 are close, then it takes roughly the same amount of time to solve the problems for C1, j and Cj  1, q. Thus, we refer to this choice as the balanced choice of the cut position. Basically, this choice aims at shortening the running time.have recently designed an approximation algorithm for estimating the MEC score. C2. Let p be the number of rows in C. For each j 2 f1,. .. , q  1g, we find the set R j of integers i 2 f1,. .. , pg such that f i j5t i , where f i and t i are the start and the end positions of the read corresponding to the ith row of C, respectively. For each i 2 R j , we also calculateexample, if C is the unsplittable block in the bottom of the matrix inin the Supplementary Material, then R 2  f3, 4g, e 3, 2  2, e 4, 2  1 and e 2  3. It is not hard to see that the MEC score of each raw solution of C associated with j does not exceed e j  s j, 1  s j, 2 , where s j, 1 and s j, 2 are the MEC scores of optimal solutions of C1, j and Cj  1, q, respectively. Thus, e j  s j, 1  s j, 2 is an upper bound on the MEC score of an optimal solution of C. Also recall that s j, 1  s j, 2 is a lower bound on the MEC score of an optimal solution of C. Hence, the smaller e j is, the closer the two bounds are. Therefore, by trying all j 2 f1,. .. , q  1g, we can find the integer j that minimizes e j. Unfortunately, such a j is usually either close to 1 or close to q and hence either the problem for C1, j or the problem for Cj  1, q remains hard to solve. Therefore, instead of trying all j 2 f1,. .. , q  1g, we choose a real number " with 0550:5 (say, 0.1) and only try all j 2 fq,. .. , 1  qg to find the integer j with the smallest e j. We refer to this choice as the unbalanced choice of the cut position. For the purpose of refining a raw solution h, h 0  of C associated with the cut position j, we design a subroutine (named Majority), which repeats the following steps until h, h 0  does not change any more.(a) For each read i 2 f1,. .. , pg, we compute dr i , h and dr i , h 0 , where r i is the ith row of C.If dr i , h dr i , h 0 , we align r i to h; otherwise, we align r i to h 0. In this way, each row is aligned to one of h and h 0. Let S (S 0 , respectively) be the set of rows aligned to h (h 0 , respectively).(b) For each j 2 f1,. .. , qg, we compute the total number j, 0 ( j, 1 , respectively) of rows r i 2 S such that the jth entry in r i is a 0 (1, respectively), and we also compute the total number 0 j, 0 ( 0 j, 1 , respectively) of rows r i 2 S 0 such that the jth entry in r i is a 0 (1, respectively). Let b j 2 f0, 1g be the jth bit of h. Note that 1  b j is the jth bit of h 0. Also note that Cj contributes j, 1bj  0 j, bj to the MEC score of h, h 0 . Similarly, if we flip the jth bits of h and h 0 , then Cj contributes j, bj  0 j, 1bj to the MEC score of h, h 0 . Thus, we check whether j, 1bj  0 j, bj 4 j, bj  0 j, 1bj. If this inequality holds, then we flip the jth bits of h and h 0. Majority usually refines a raw solution h, h 0  of C associated with the cut position j so that its MEC score becomes significantly smaller. To enhance Majority, we propose the following random-sampling approach. First, we choose a large enough number , say  100  s, where s is the current MEC score of h, h 0 . Then, we repeat the following three steps times:(1) Calculate the current MEC score s of h, h 0  and select (not necessarily distinct) integers j 1 , j 2 ,. .. , j each uniformly at random from f1, 2,. .. , qg, where  0:1s b c.(2) Let j 0 1 , j 0 2 ,. .. , j 0 ' be the distinct integers among j 1 , j 2,. .. , j. Obtain a new solution  ^ h, ^ h 0  of C by flipping the j 0 i th bits of h and h 0 for all j 0 i with 1 i '.(3) Call Majority to refine the solution  ^ h, ^ h 0 . If the MEC score of the refined  ^ h, ^ h 0  is smaller than s, then update h, h 0  to  ^ h, ^ h 0 .
The general caseIdeally, the input matrix X should only contain heterozygous sites for the specific individual. However, accurately identifying the set of heterozygous sites of an individual is extremely difficult. Most of the existing methods choose non-monotone columns to form X. This is based on the observation that when each read contains no errors, a non-monotone column corresponds to a heterozygous site and a monotone column corresponds to a homozygous site. However, in practice, reads may contain a (small) number of errors. Thus, some homozygous sites may also lead to non-monotone columns, and those non-monotone columns corresponding to homozygous sites are also included in the input matrix X. To deal with this problem, we have to consider the general case where each site in the solution h, h 0  can be either homozygous or heterozygous. See the Supplementary Material for details. In this case, neither block decomposition nor singleton removal is applicable anymore because some sites corresponding to the columns of X may be homozygous, and hence an optimal solution h, h 0  of X may not be complementary. For each j 2 f1, 2,. .. , ng, we say that the jth column of X is intrinsically heterozygous if for every optimal solution h, h 0  of X, we can modify the jth bits of h and h 0 without losing the optimality so that the jth bit of h is different from that of h 0. An intrinsically heterozygous column is useful because as in the all-heterozygous case, it can be used to perform singleton removal and is also possibly a splittable column for block decomposition. We next consider how to find heterozygous columns. Finding intrinsically heterozygous columns: For each j 2 f1, 2,. .. , ng, we compute the total number j, 0 ( j, 1 , respectively) of rows r i in X such that the entry in the jth column of r i is a 0 (1, respectively), and further compute the number s j, 0 ( s j, 1 , respectively) of non-singular rows r i in X such that the jth entry in r i is a 0 (1, respectively). LEMMA 1. Suppose that j 2 f1, 2,. .. , ng satisfies minf j, 0 , j, 1 g ! sj,0 sj,1 2 j k . Then, Xj is intrinsically heterozygous. PROOF. See Section 5 of the Supplementary Material. For example, LEMMA 1 ensures that the first six columns of the matrix in Supplementary Table S2 are intrinsically heterozygous. Block reduction: This is same as that in the all-heterozygous case. Block decomposition: This is almost the same as that in the all-heterozygous case. The only difference is in the definition of a splittable column of a block B. Specifically, the new definition of a splittable column Bj of B requires that the jth column of B be intrinsically heterozygous. Singleton removal: This is almost the same as that in the allheterozygous case. The only difference is that instead of removing all singular rows, we only remove those singular rows r i such that the read corresponding to r i starts and ends at an intrinsically heterozygous column. Duplicate removal: This is the same as that in the all-heterozygous case. Solving reduced-blocks via ILP: Similar to the all-heterozygous case, we can obtain an ILP formulation for the general case. The only difference is that for each column Dj of D that is not known to be intrinsically heterozygous, we need to introduce two binary variables x j and z j (instead of one for the all-heterozygous case) such that the value of x j (z j , respectively) is supposed to be 1 if and only if the jth bit of h (h 0 , respectively) is a 1. For lack of space, we detail the ILP formulation in the Supplementary Material. It is worth pointing out that Wu et al. (2009) has also given an ILP formulation for the problem. However, their ILP formulation contains non-binary variables. Moreover, finding intrinsically heterozygous columns and merging identical columns enable us to use fewer variables than theirs. Consequently, ours can be solved within shorter time. A powerful heuristic for hard blocks: This is almost the same as that in the all-heterozygous case. There are only two differences. The first is in Step (b) of the Majority subroutine, which should be modified as follows.(b) For each j 2 f1,. .. , qg, we compute the total number j, 0 ( j, 1 , respectively) of rows r i 2 S such that the entry in the jth column of r i is a 0 (1, respectively), and we also compute the total number 0 j, 0 ( 0 j, 1 , respectively) of rows r i 2 S 0 such that the entry in the jth column of r i is a 0 (1, respectively). Let b j 2 f0, 1g (b 0 j , respectively) be the jth bit of h (h 0 , respectively). If j, 1bj 4 j, bj , then we flip the jth bit of h., then we flip the jth bit of h 0 .The other difference is in the random-sampling approach, whose last two steps should be modified as follows.If the j 0 i th column of D is intrinsically heterozygous, then flip the j 0 i th bits of h and h 0 ; otherwise, choose one of h and h 0 uniformly at random and flip its j 0 i th bit.(2) Call the modified Majority subroutine to refine  ^ h, ^ h 0 . If the MEC score of the refined  ^ h, ^ h 0  is smaller than s, then update h, h 0  toAnother powerful heuristic for hard blocks: The ILP formulation of the all-heterozygous case has fewer variables and constraints than that of the general case and hence can usually be solved within much shorter time. Therefore, a natural heuristic for hard reduced-blocks D in the general case is as follows. First, we solve the problem for D by assuming that all columns of D are heterozygous. This yields a raw solution h, h 0  of D. We then use the modified Majority subroutine and the random-sampling approach to refine h, h 0 . Our experiments will show that this heuristic often finds a solution of D whose MEC score is extremely close to optimal. A drawback of this heuristic is that it does not give any lower bound on the MEC score of an optimal solution of D.
RESULTS AND DISCUSSIONTo evaluate our methods empirically, we run our program on a Linux (x64) desktop PC with i7-3960X CPU and 31.4GiB RAM. In our experiments, we use three datasets.
The filtered HuRef datasetThe filtered HuRef dataset over 22 chromosomes is generated by. Some simple variant calling method has been applied to form the matrices in the dataset (). The dataset has been used to compare previous methods with each other (). This dataset is known to be hard to solve. Indeed, He et al. (2010)'s program takes 15 h on a PC cluster to only roughly solve the all-heterozygous case of the problem for the dataset. In particular, their program excludes multi-gapped reads (3725 in total) and fails to solve a total of eight hard blocks. As each of the 22 chromosomes has a large number of SNP sites and a huge number of reads, it is effective to cut each of them into as many smaller independent blocks as possible. Block reduction has been used for this purpose in previous studies (). In contrast, to our knowledge, block decomposition has not been used for this purpose before. For the all-heterozygous (general, respectively) case, Supplementary(Supplementary, respectively) summarizes the numbers of non-singular blocks of the 22 chromosomes obtained by block reduction only and by both block reduction and block decomposition, respectively. As can be seen from the tables, block decomposition enables us to cut the nonsingular blocks (obtained by block reduction only) of a chromosome into many smaller blocks. When we use CPLEX to solve each reduced-block in the allheterozygous (general, respectively) case of the problem, we set a time limit of 10 (20, respectively) min. As the result, CPLEX fails to solve only three (seven, respectively) reduced-blocks optimally within the time limit. The three (seven, respectively) blocks are shown in the first (second, respectively) part of Supplementary. As they are hard, we use the (first) heuristic detailed in Section 2.2 (Section 2.3, respectively) to find heuristic solutions for them. The results are summarized in the same table from which we can see that our heuristics can find solutions extremely close to optimal for hard reduced-blocks. Excluding these hard blocks, our program can solve the all-heterozygous (general, respectively) case of the problem for the 22 chromosomes within a total of 3 h (5, respectively) on the PC. The running times and the optimal MEC scores for the 22 chromosomes for the all-heterozygous case and the general case are summarized in, where one can see that the scores for the general case are significantly smaller than those for the all-heterozygous case. This indicates that the number of homozygous sites included in the input matrix cannot be ignored. In Table 1, we also compare our program with HapCUT (). It is worth noting that HapCUT uses randomness and hence different runs of HapCUT on the same input may generate different outputs. Therefore, for each chromosome in the HuRef dataset, we ran HapCUT 10 times, where each run was given the default maximum number (namely, 100) of iterations. Consequently, for each chromosome in the dataset, the running time of HapCUT inis the total time of the 10 runs, whereas the score of HapCUT in the table is the best score among the 10 runs. Clearly, our program spends less time than HapCUT to give optimal solutions. We also use the second heuristic in Section 2.3 to solve the general case of the problem for the seven hard reduced-blocks. The results are summarized infrom which we can see that the second heuristic can find solutions close to optimal for the hard reduced-blocks within shorter time than the first heuristic, but cannot find a lower bound on the optimal MEC score.
Simulated datasetsWe use part of the simulated datasets of Geraci (2010). To generate a read matrix, three parameters ', c and e are used, where ' is the number of SNPs, c is the coverage and e is the error rate. Intuitively speaking, the larger c (e, respectively) is, the more reads we have in the matrix (the larger optimal MEC score we have for the matrix, respectively). The values for ', c and e are chosen from f100, 350, 700g, f3, 5, 8, 10g and f0%, 10%, 20%, 30%g, respectively. For each combination of the three parameters, 100 read matrices are generated. A merit of each generated read matrix X is that we know its true solution h 1 , h 2 . To evaluate the quality of a solution  ^ h 1 , ^ h 2  of X (returned by a program), the reconstruction rate of the solution is defined to be, where d is the Hamming-distance function. Intuitively, the larger the reconstruction rate is, the better the solution is. To compare the all-heterozygous and the general cases, we use the datasets of Geraci (2010) for those combinations ', c, e with ' 2 f100, 350g, c 2 f3, 5, 8, 10g and e 2 f0%, 10%g. It turns out that when e  0%, our exact program for both cases can find the correct solution. On the other hand, when e  10%, the solutions for the two cases found by our exact program look different (cf.). In particular, the average reconstruction rate achieved by our exact program for the general case is better than the best reconstruction rate reported by, but the average reconstruction rate achieved by our exact program for the all-heterozygous case is worse. Moreover, the larger c is, the worse the average reconstruction rate achieved by our exact program for the all-heterozygous case is. Consider a homozygous site p. For a fixed error rate e, with the increase of coverage at p, the chance that errors exist for reads at site p increases and so does the chance that p corresponds to a non-monotone column. Thus, the number of columns corresponding to homozygous sites in X increases. Therefore, the larger c is, the worse the average reconstruction rate achieved by the exact program for the all-heterozygous case is. This is the reason why we have to consider the general case. Contrary to the all-heterozygous case, the general case can handle such 'false-heterozygous' sites correctly, and thus the reconstruction rate increases with more coverage for the general case. The running time of our program is also related to the coverage. With the increase of coverage, the number of errors increases and so does the MEC score. As the result, the running time increases accordingly as shown in. To see how good our heuristics for the general case are, we use the datasets of Geraci (2010) for those combinations', c, e with ' 2 f100, 350g, c 2 f3, 5, 8, 10g and e  10%. For simplicity, we always choose the middle position as the cut position when experimenting with the first heuristic. The experimental results are summarized in. From the table, we can see that for the tested dataset, the average reconstruction rate achieved by our first (second, respectively) heuristic for the general case is better than the best reconstruction rate reported by Geraci (2010) when c ! 8 (c 5, respectively). It also turns out that for the tested dataset, the first heuristic is faster than the second. Furthermore, for those instances with c ! 5 in the tested dataset, our second heuristic always finds optimal solutions, but its average reconstruction rate is different from that of our exact program for the general case. Consequently, the optimal solutions found by our second heuristic can be different from the optimal solutions found by our exact program for the general case.
The NA12878 datasetThis real dataset contains the whole-genome fosmid sequence data for an individual NA12878 (). It is obtained by using fosmid pool-based next generation sequencing, which allows genome-wide generation of haploid DNA segments significantly larger than other standard shotgun sequencing technologies. We use the NA12878 dataset). Our experimental results are summarized in Supplementary Table S11 from which one can see that our exact program can finish within 3 min for each of the 22 chromosomes in the dataset and is much faster than He et al. (2010)'s program. Besides, we can see that the optimal costs in the general case are smaller than those in the all-heterozygous case for all the 22 chromosomes in Supplementary Table S11. This implies that there are still some false-heterozygous columns in the filtered matrices.
DiscussionMultiple optimal solutions may exist for both the all-heterozygous and the general cases. If there is no error in the reads, the optimal solution is unique. If the error rate in reads is low, the chance that the optimal solution is unique is high. However, if the error rate is high, many optimal solutions may exist. In general, enumerating all optimal solutions are much harder and takes much longer time than computing a single optimal solution. From the experiments on simulated datasets, we can see that the single optimal solution found by our exact algorithm can achieve better reconstruction rate than the previously known heuristics in most cases. Still, it remains an open problem to handle multiple optimal solutions.. Evaluating our exact program using the simulated dataset of Geraci (2010) for those combinations ', c, e with ' 2 f100, 350g, c 2 f3, 5, 8, 10g and e  10%, where column 'prev RR' shows the best average reconstruction rate reported by Geraci (2010) and columns 'org score', 'score', 'time' and 'RR' show the average MEC score of the correct solution, the average MEC score of the solution found by our exact program, the average time (in minutes) taken by our exact program, and the average reconstruction rate of our program over the 100 instances in the dataset for a particular combination ', c, 10%, respectively. Evaluating our heuristics for the general case using the simulated dataset of Geraci (2010) for those combinations ', c, e with ' 2 f100, 350g, c 2 f3, 5, 8, 10g and e  10%, where the columns mean the same as in
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Z-Z.Chen et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Exact algorithms for haplotype assembly at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
