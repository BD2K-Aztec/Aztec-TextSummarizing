Motivation: The abundance of biomedical literature has attracted significant interest in novel methods to automatically extract biomedical relations from the literature. Until recently, most research was focused on extracting binary relations such as protein–protein interactions and drug–disease relations. However, these binary relations cannot fully represent the original biomedical data. Therefore, there is a need for methods that can extract fine-grained and complex relations known as biomedical events. Results: In this article we propose a novel method to extract biomed-ical events from text. Our method consists of two phases. In the first phase, training data are mapped into structured representations. Based on that, templates are used to extract rules automatically. In the second phase, extraction methods are developed to process the obtained rules. When evaluated against the Genia event extraction abstract and full-text test datasets (Task 1), we obtain results with F-scores of 52.34 and 53.34, respectively, which are comparable to the state-of-the-art systems. Furthermore, our system achieves superior performance in terms of computational efficiency. Availability: Our source code is available for academic use at
INTRODUCTIONAutomatic relation extraction is an important and established way to extract information from biomedical literature (). Many relation extraction approaches have been proposed to extract binary relations between biomedical entities from text such as proteinprotein interactions (), drugdrug interactions () and causal relations on drug resistance (). However, such binary relations cannot fully represent the biological meaning of the original relations (). As a consequence, there is a need to have a better representation that can exemplify complex relations extracted from text. Recently, this shortcoming has been addressed in the BioNLP'09 Shared Task () by introducing the biomedical event extraction task, which aims to extract complex and nested events from text. In this shared task, an event is defined as follows: each event consists of a trigger, a type and one or more arguments. Depending on the event type, an argument can either be a protein or another eventfor more details] as illustrated in.shows examples of three event types where a simple event (E1) has one protein as the argument, a binding event (E2) has two proteins as the arguments and a regulatory event (E3) has two events as the arguments. Several event extraction approaches have been proposed to the BioNLP'09 and BioNLP'11 challenges (). In general, to extract events from a given sentence, two steps need to be carried out: identifying event triggers and determining their arguments. First, the sentence is typically preprocessed and converted into structured representations such as dependency parse tree and candidate event triggers are often pre-selected using a dictionary. Next, the candidate event triggers and the parse tree are used as the input for the event extraction process. The proposed approaches to extract events can be divided into two main groups, namely, rule-based and machine learning (ML)-based approaches. Rule-based event extraction systems consist of a set of rules that are manually defined or generated from training data. To extract events from text, each pre-selected candidate trigger is first validated to determine its event type. Next, the defined rules are often applied to the parse tree which contains that trigger to find relations between the trigger and its arguments (). Evaluation results from the BioNLP'09 and BioNLP'11 show that rule-based systems achieve high precision but low recall. In particular, the system proposed byachieves the highest precision on the BioNLP'09 event extraction track. ML-based systems model event extraction tasks as a classification problem. In these systems, pre-selected candidate event triggers and the arguments attached to them are classified as true event triggers or not. Extraction methods proposed in the BioNLP'09 exploit various learning algorithms and feature sets to leverage the system performance (). Methods proposed after the BioNLP'09 can be divided into two groups based on how event triggers and arguments are determined. The first ML-based group consists of systems that adopt the pipeline and feature sets proposed by Bjorneand later improved by, in which the evaluation of the candidate triggers and the determination of their arguments are carried out independently. For this type of approach, errors made in the trigger detectiongood results in terms of F-scores on both BioNLP'09 and BioNLP'11 challenges. Apart from these two main approaches, there are a few systems that use different methods. Morapropose a hybrid method that combines both rule-and ML-based approaches.use a system using sub-graph matching.use case-based reasoning system.introduce a system using dependency graphs by extending the function of an existing dependency parser. However, the performance of these systems is not comparable to the state-of-the-art ML-based systems. In this article we introduce a novel rule-based method to extract biomedical events from text. Our method differs in many ways from the existing methods. First, we define a new structured representation to express the relations between event triggers and their arguments. Next, we transform the input text into this structured representation using a shallow parser. We then map the annotated events from the training data into these structured representations, based on that rules are extracted automatically by using predefined templates. The obtained rules are unified to make them more flexible. Finally, we develop event extraction algorithms to apply these unified rules. By using the structured representation to decompose the nested and complex structures of events into simple syntactic layers, we can use simple methods to learn and apply extraction rules. When evaluated on the BioNLP'11 test set, our system achieves an impressive performance in terms of precision and computational times. To the best of our knowledge, this is the first system of its kind to extract biomedical events from text.
METHODSOur method to event extraction consists of two phases: a learning phase and an extraction phase. In the learning phase, the system learns rules to extract events from training data. In the extraction phase, the system applies rules learned in the previous phase to extract events from unseen text. A text preprocessing step is used in both phases to convert unstructured text into structured representations. In the following sections, we present these steps in more detail.
Structured representationWe define the following syntactic layers that form a structured representation to express the structures of biomedical events: Chunk: is the base syntactic layer that can express an event (see). A chunk consists of one or more tokens, taken from the output of a shallow parser (see Section 2.5 for more detail). Four chunk types that are used to express events are as follows: adjective, noun, verb and preposition. Phrase: consists of a list of chunks, connecting by preposition chunks (see). A phrase can express one or more events where the event triggers and their arguments should belong to different chunks. To reduce the variants when learning rules and to simplify the event extraction process, all noun chunks that are in the coordinative form are merged into one noun chunk. For example, the following chunks:Clause: consists of a verb chunk that connects with a left child and a right child (see). A child of a clause is a phrase but it can also be a chunk. A clause can express one or more events where the event trigger belongs to the verb chunk and its arguments belong to the clause's children.Merged phrase: is a special noun phrase obtained by merging the verb chunk of a clause with its children. The merged phrase is used to express an event where its trigger and its arguments belong to the left child and the right child of a clause. For example, the binding event E2 inneeds a merged phrase to be expressed.Relation between layers: An upper layer can query its direct lower layer to find arguments. The returned values of the query can be a list of proteins, a list of events or an empty list. For example, a phrase can query its right chunks. A clause can query its left and right children. Representing biomedical events: with this structured representation, we can express most of biomedical events from the training data. Here, both event triggers and their arguments must belong to the same sentence.presents the events fromusing the structured representations.
Learning rulesTo learn extraction rules from structured representations, we define a list of syntactic and semantic features to capture the underlying rules that govern the relations between event triggers and their arguments whenhas two events as the arguments whereas the event E2 inhas one protein and one event as the arguments. Structured representations for the events inexpressed by structured representations. These features are divided into three groups that correspond to three syntactic layers: chunk, phrase and clause. The descriptions of these features are as follows.
Features for events expressed in a chunk layer:Frequency: counts frequency of an event trigger when expressed in chunk form. Part-of-speech (POS): indicates which syntactic form (e.g. NN, ADJ) of an event trigger is used in the chunk form.
Features for events expressedin the phrase layer:Distance: measures the distance (by chunk) from the chunk that contains an event trigger to the chunk containing its 'theme'. Preposition2: is used for binding and regulatory events. It indicates which preposition is used to connect the chunk that contains the 'theme' to the chunk that contains the second arguments (i.e. theme2 for binding event or cause for regulatory event) of an event. For example, the Preposition2 of the binding event:of [NP PRO1] tois to. Distance2: is used for binding and regulatory events. It measures the distance (by chunk) from the chunk that contains an event trigger to the chunk that contains its second argument.Cause order: is used for regulatory events. It indicates the relative position between the theme and the cause of an event. For example, consider a regulatory event: effect of PRO1 on PRO2 where PRO1 is the cause. This feature helps determining the argument which is closer to the trigger is a theme or a cause.Theme position: is used for binding events. It indicates the position of the theme is on the left or on the right of an event trigger. For examples, PRO1 binding to PRO2, interaction between PRO1 and PRO2.Theme 2 position: is similar to theme position but for the second argument.
Features for events expressedin the clause layer:Passive: indicates whether the clause in the 'active' or the 'passive' form.Theme position: indicates the theme is on the left child or on the right child when the clause in the active form. Distance, Distance2: these features are similar to those of phrase form.Beside these features, we use the following features to determine the number of arguments and argument types for binding and regulatory events.
Specific feature for binding events:Theme2Count: counts frequency of an event trigger having theme2.
Specific features for regulatory events:ThemePro: counts frequency of an event trigger having theme as a protein. ThemeEvent: counts frequency of an event trigger having theme as an event. CausePro: counts frequency of an event trigger having cause as a protein. CauseEvent: counts frequency of an event trigger having cause as an event.To learn extraction rules, we create three templates using the above features where each template corresponds to a layer (i.e. chunk, phrase). Next, we map each annotated event from the training data into a suitable layer. The extraction rule for each event is then extracted using a corresponding template. The learning procedure is shown in Algorithm 1. The text conversion step used in this algorithm is described in Section 2.5. During this learning process, some events are skipped if their event triggers do not belong to a dictionary that is prepared based on single-word event triggers collected from the training and development datasets. In addition, events are skipped if they cannot be mapped into the structured representation.
Rule combinationFor each event trigger and its specific POS tag, we may obtain one to three sets of extraction rules which correspond to three layers: chunk, phrase and clause. For example, the 'binding' trigger has three POS tags: ADJ, NN and VBG and therefore it may possess up to nine sets of extraction rules. Examples of extraction rules of the binding trigger and its POS tag NN, which learned from the phrase layer, are shown in. The first row of this table is an extraction rule that learned from binding events such as the binding event E2 in. To simplify the event extraction step and to make the extraction rules generalize well, we combine the extraction rules for phrase and clause layer of each event trigger and its specific POS tag to generate a unified rule. For consequent process, each unified rule is represented as a decision table. These decision tables are then used in the extraction step to determine event arguments. For example,Input: Sentence S, list of events E belongs to S, a dictionary D, an empty hashtable H for each event type, template T Output: List of rules for each event type.(1) Convert S into structured representation R(2) For e 2 E
(3)If event trigger of e in D
(4)If e can be mapped to layer r 2 R
(5)Fill t 2 T with features extracted from rGenerate key k for tUpdate entry in HElse
(10)Store k into H
(11)Return H confidence score based on the frequency of that event trigger being extracted in a layer (e.g. chunk, phrase) divided by frequency of that event trigger being found in the training data. These specific feature values such as Theme2Count and ThemePro for binding and regulatory events are then normalized based on the frequencies of the combined rules. Since simple, binding and regulatory events have different argument types and each event class has two sets of extraction rules that need to be combined (chunk layer has only one extraction rule), we need six variants of the decision tables to represent these unified rules. However, the procedures that are used to combine these rules are similar. Here, we use a simple voting scheme based on frequencies to select the rules that are more likely reliable and remove those that contradict to the selected ones. An example of such rule combination algorithm is shown in Algorithm 2.
Event extractionIn this section we present algorithms to extract events from an input sentence. First, the sentence is preprocessed to detect candidate event triggers and is converted into structured representations. Candidate event triggers are detected using the same dictionary as the one used in the learning rules step. Next, we evaluate each candidate trigger and determine its arguments by using a decision table. For each candidate trigger, its decision table is retrieved using a key which consists of the trigger, its POS, the layer containing it and its event type. For example, if the binding trigger belongs to a phrase and it has POS tag NN, then its retrieval key is binding_NN_Phrase_Binding. We model the process of extracting events from a sentence as a pairing task where each protein may be paired with a trigger on the left or on the right of that protein based on a given rule. For example, consider the sentence in: RFLAT-1activates RANTES gene expression, with the flat representation, it is not clear whether RANTES should pair with the 'activates' trigger or with the 'expression' trigger. However, when mapped into structured representation as show in, the decision can easily be made based on the syntactic layer. In this case, pairing RANTES with the expression trigger should have a higher priority than pairing it with the activates trigger. Based on the structured representation, our event extraction procedure consists of three steps: extract events from chunk, from phrase and from clause. At each layer, the extracted events can be used by its direct upper layer as arguments. The event extraction algorithm is shown in Algorithm 3.
Text preprocessingThe text preprocessing step consists of splitting sentences, replacing protein names with place-holders, tokenizing words, POS tagging, parsing sentences with a shallow parser and converting the output of the parser into structured representations. To split the input text (e.g. title, abstract, paragraph) into single sentences, we use the LingPipe sentence splitter (http://alias-i.com/lingpipe/). Sentences that do not contain protein names are skipped. We replace protein/gene names with a place-holder, e.g. PRO i (i is the index of the i th protein/gene in the text) to prevent the parser from segmenting multiple-word protein names and for subsequent processing. Each sentence is then tokenized and tagged with the LingPipe POS tagger. Finally, these outputs (tokens and their POS tags) are parsed with the OpenNLP shallow parser (http://incubator.apache.org/opennlp/) to produce chunks.the variants of the structured representations, coordinative noun chunks are merged into one noun chunk as mentioned in Section 2.1. Furthermore, if there is an adjective chunk that immediately follows a verb chunk, we merge that adjective chunk into the verb chunk. For example, the following chunks [VB is]are merged into; the merged chunk is considered as a verb chunk.
Converting
RESULTS AND DISCUSSION
DatasetsWe use the Genia Event Extraction datasets provided by the BioNLP'11 (https://sites.google.com/site/bionlpst/ home/geniaevent-extraction-genia) to evaluate our extraction method.(1) For e 2 E(2) Retrieve rules L of e in R(3) Split L into two groups G1, and G2 based on passive feature(4) Sort G1 and G2 based on frequency(5) Select active direction dr 1 , dr 2 for G1 and G2.(6) For i  1 to 2(7)For g 2 G iRemove r 2 g if r active direction 6  dr i active directionSet distance d1, d2 for G iCalculate ThemPro, ThemeEvent, CausePro, CauseEventThe datasets include training, development and test datasets and each dataset consists of two parts, an abstract dataset and a full-text dataset. For training and development datasets, a list of proteins/genes and annotated events is given. For the test set, only a list of proteins/genes is given. Statistics of the datasets are shown in.
Evaluation settingsWe use both training and development datasets for building the dictionary and for learning extraction rules, which are then used to extract biomedical events from the test dataset (Task 1). The extracted events are submitted to the online evaluation system (https://sites.google.com/site/bionlpst/home) to evaluate the results. To obtain realistic results, we do not apply any tuning techniques to optimize for F-score of the test dataset. We set the thresholds for dictionary entries and confidence scores of the extraction rules to 0.1 and 0.03, respectively. These threshold values are determined empirically based on the development dataset.shows the results of our extraction method evaluated on the test dataset using the 'Approximate Span/Approximate Recursive matching' criteria. We present the evaluation results of the abstract and full-text datasets in parallel to easily analyze the results of both types of text. The data show that our system performs well on both abstract and full-text datasets. In particular, it achieves the best results on simple events (SVT-TOTAL), followed by binding events and regulatory events (REGTOTAL). Overall, the results on the full-text dataset are better than on the abstract dataset. The results on simple and binding events in the full-text dataset are significantly better than in the abstract dataset with 810 F-score points higher, whereas the results on the regulatory events in the abstract dataset are slightly better than in the full-text dataset. Tables 5 and 6 present comparison results of our system and the top four systems that participated in the BioNLP'11 challenge. To study the results of each system in more details, we also
Event extraction
Algorithm 3 Extracting biomedical events from a sentenceInput: Sentence S, a list of proteins P, a list of extraction rules R, a dictionary D, a list of candidate triggers G. Output: list of extracted events. Sub functions isChunk(trigger g, protein p): return True if g and p can form an event queryArgs(layer l, trigger g, rule r): query l for a list of protein/events based on the data of r and g. formEvent(trigger g, List L1, List L2): form events based on g, L1, and L2 hasCause(trigger g, rule r): return True if has Cause/Theme2 inRange(trigger g, rule r, trigger g2): return True if events formed by g2 can be used as arguments for g. ChunkExtraction(List G, Structure T) / / extract events from chunk layer.(1) For g 2 G (2) Retrieve chunk rule r of g in R (3) Locate chunk l 2 T containing g (4) List L1  queryArgs(l, g, r) (5) If r 6  null and isChunk(g, p 2 L1)PhraseExtraction(Trigger g, List G, Structure T) / / extract events from phrase layer(1) Repeat (2) List L1  null; (3) List L2  null; (4) g  getNext(G) / / get next trigger (5) Retrieve phrase rule r of g in R (6) Locate phase l 2 T containing g (7) L1  queryArgs(l,g,r) / / theme (8) If r 6  null and L1  nullIf inRange(g,r,g2)ClauseExtraction(List G, Structure T) / / extract events from clause layer(1) For g 2 G (2) List L1  null; / / theme (3) List L2  null; / /cause / theme 2 (4) Retrieve clause rule r of g in R (5) Locate clause l 2 T containing gIf r 6  null and L1 6  null(1) Convert s into structured representation T (2) Map p2 P into chunks 2 Tpresent the evaluation results on each dataset separately, whereshows the results on the abstract test dataset andshows the results on the full-text test dataset.shows that our system achieves good results compared to the best system on simple (SVT) and binding (BIND) events. In fact, it performs slightly better on simple events than those systems. However, the performance on regulatory (REG) events is lower than the best system, with a gap of 12 F-score points. Overall, on the abstract dataset, the Riedel and Andrew (2011) system is the best since their system yields the highest F-score, whereas our system yields good results in terms of precision and the BjorneBjorne and Salakoski (2011) system achieves good results in terms of recall. With the inclusion of full-text documents in the test dataset, the BioNLP'11 brings more challenges to the event extraction task than the BioNLP'09 does. This requires the adaption ability of each system since the structure and content of biomedical abstracts and full-text bodies are different (). The results inshow that all systems yield better F-scores on the simple events in the full-text dataset than in the abstract dataset. Our system still leads on this type of events. Interestingly, while the other systems drop the performance on the binding events, our system gains 11 F-score points, from 49.11% for the abstract dataset to 60.45% for this dataset. This yields the best result on binding events reported so far. An analysis of the results on the binding events of these systems (data not shown) showed that while two rule-based systems achieve nearly the same precision on the binding events in both datasets (e.g. 49.76% versus 49.38% for Kilicoglu's system; 64.93% versus 65.34% for our system), three ML-based systems drop precision significantly (e.g. 60.89% versus 47.62 for Riedel's system; 50% versus 31.76% for BjorneBjorne's system; 44.51% versus 32.77% for Quirk's system). This might be due to over-fitting since the number of binding events which is available for training in the abstract dataset is much higher than in the full-text dataset (881 events vs. 101 events). This implies that, for binding events, the aforementioned rule-based systems generalize better than their counterpart ML-based systems. This finding was also pointed out by. For regulatory events, our system drops the performance on this dataset but the gap of results between our system and the best system on these event classes is reduced to 6 F-score points. Overall, our system outperforms the best system of the BioNLP'11 on full-text dataset in terms of both precision and F-score.
Computational performanceWhen the system is applied to large-scale extractions such as the whole PubMed database or used in QuestionAnswer systems as envisioned by, then computational resources required to run the system should be taken into account. Despite this obvious fact, only few systems report on the computational time needed to run their systems. Riedel and McCallum (2011) report that their system needs from 60 to 297 ms, depending on the learning models, to extract events from a sentence. However, these numbers do not include the parsing times and feature extraction times. Bjornereport in their large-scale experiment that, on average, their system needs 954 ms to parse a sentence and needs 486 ms to extract events from that sentence. Sinceuse the same parser as Bjorne, we assume that the parsing times of the two systems are equivalent. Therefore, both systems need from 1040 to 1400 ms to extract events from a sentence. In contrast, our system needs 6 ms to do so. Details of the computational times are shown in. In general, it is not straightforward to directly compare the computational times between systems due to the differences inhardware as well as other factors, e.g. the length of sentences and the number of events per sentence. These differences are shown inwhere the text processing times and event extraction times vary on two datasets even though they run on the same system. However, it is apparent that our system outperforms the mentioned systems with 150-fold faster in terms of computational time.
Performance analysisThere are some issues that affect our system performance, mostly in terms of recall. In the following section we address these issues and discuss possible directions to improve the overall performance. First, the use of the dictionary to detect candidate event triggers and to disambiguate event triggers affects the performance considerably. We found that raising the threshold of the dictionary entries would increase precision for some event classes but this would also decrease recall of the others. There are many cases where event triggers may belong to more than one event class and they cannot be determined by simply increasing the dictionary threshold. This problem was also discussed by. Furthermore, our extraction algorithm relies on both candidate triggers and proteins to extract events. If some candidate triggers are filtered by the threshold, then the procedure used to find event arguments fails to return a desired proteins/events list. Another factor that affects the system performance is the rule combination step. While combining extraction rules definitely simplify the event extraction method, it also causes the loss of information. As shown in Algorithm 2 (line 8), during the combination process, we remove some rules that contradict to the selected rules. This increases precision but decreases the recall of the system. Furthermore, the loss of information is clearly visible in the case of binding and regulatory events. For example, the CausePro, and CauseEvent values in the decision table of an event trigger can provide statistical data for that event trigger such as indicating how often that trigger may have a cause as a protein or as an event. However, these data do not indicate in which particular case that trigger has a cause. When analyzing 50 false-positive regulatory events obtained from the developing dataset, we found that of 22 cases due to wrong event classes (e.g. Pos_Regulation vs. Regulation) and of 28 cases due to wrong number of arguments (e.g. with or without causes) or wrong argument types (e.g. protein vs. event). Therefore, to reduce the false-positive regulatory events, we need a better strategy such as adding more specific features for these event classes or modify the current extraction algorithm to retain more rules.
CONCLUSIONIn this article we have proposed a novel rule-based method to extract biomedical events from text. Our core method to event extraction is the use of a structured representation to decompose nested and complex events into syntactic layers. This representation not only allows us to simplify the learning and extraction phases but also it requires less syntactic analysis of input sentences. The evaluation results show that our system performs well on both abstract and full-text datasets. Furthermore, it achieves superior performance in terms of computational efficiency. It is clearly suited to large-scale experiments. Our event extraction method is simpler than the existing ML-based approaches. It is also more robust than the previously proposed rule-based approaches since it learns rules automatically from training data. Its simplicity and robustness have been proven by the performance on simple and binding events. Its structured representation is generic and is capable of representing any relation types. The proposed feature sets based on the structured representation mainly consist of generic features and a few specific features. Therefore, it is suited to extract many types of relations. If needed, its specific features can be easily adapted to any new domain.
The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Extracting biomedical events from literature at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Q.C.Bui and P.M.A. Sloot at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
