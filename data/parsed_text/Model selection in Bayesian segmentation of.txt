Motivation: The analysis of multiple sequence alignments is allowing researchers to glean valuable insights into evolution, as well as identify genomic regions that may be functional, or discover novel classes of functional elements. Understanding the distribution of conservation levels that constitutes the evolutionary landscape is crucial to distinguishing functional regions from non-functional. Recent evidence suggests that a binary classification of evolutionary rates is inappropriate for this purpose and finds only highly conserved functional elements. Given that the distribution of evolutionary rates is multi-modal, determining the number of modes is of paramount concern. Through simulation, we evaluate the performance of a number of information criterion approaches derived from MCMC simulations in determining the dimension of a model. Results: We utilize a deviance information criterion (DIC) approximation that is more robust than the approximations from other information criteria, and show our information criteria approximations do not produce superfluous modes when estimating conservation distributions under a variety of circumstances. We analyse the distribution of conservation for a multiple alignment comprising four primate species and mouse, and repeat this on two additional multiple alignments of similar species. We find evidence of six distinct classes of evolutionary rates that appear to be robust to the species used. Availability: Source code and data are available at
INTRODUCTIONMuch attention has been given to delineating protein-coding regions, believed to account for  1.2% of the human genome (). However, it is the extensive non-protein-coding regions that remain a largely untapped resource for the identification of novel classes of functional elements. Recent studies have indicated that there exists a vast amount of functionality in the non-coding regions (; Pheasant and * To whom correspondence should be addressed.). The task of delineating and identifying the functional elements is hindered by the lack of understanding of the constraints shaping such elements. Studying rates of evolution in genomes is also key to understanding genomic function. Humanmouse and humandog sequence comparisons suggest that  5% of the mammalian genome is subject to purifying evolutionary selection (). This figure is supported by the recent finding that 5% of bases are confidently predicted as being under evolutionary constraint in mammals by two out of three algorithms employed in the ENCODE project analysis () and in analyses of large alignments of mammals (). What remains unclear is the relationship between the rate at which a functional element evolves and its functional type. It is understood that conserved sequences are likely to harbour functional elements: the length of evolutionary time the segments have remained unchanged is evidence of biological or functional significance. It should be stressed, however, that the amount of conserved sequence is a lower bound on the amount of functional sequence, since lack of evolutionary constraint on a sequence does not imply lack of function (). Indeed, the ENCODE pilot study found many functional elements that are seemingly unconstrained across mammalian evolution. There is a strong demand for the development of more sophisticated bioinformatic approaches to studying distributions of evolutionary rates. Current approaches such as the simple per cent identity within a sliding window () and the two state phylo-Hidden Markov Model () hide a multitude of sins: most importantly, the implicit assumption of a dichotomy of evolutionary rates: conserved and non-conserved. Conservation estimates are based on a continuous model of a known distribution of evolutionary rates for ancient repeat (AR) sequences, assumed to be neutrally evolving, and an unknown distribution for functional sequences. This unknown distribution is estimated through the computationally efficient sliding window technique; however, results are affected by window size. It is assumed that the conserved fraction is the difference, estimated by mixture decomposition, between the level of similarity across all windows in the alignable fraction of the genome versus that in AR sequences or other supposedly neutrally evolving sequences. This fundamental assumption of a dichotomy of conservation levels does not adequately capture the complexity of the evolutionary rate distribution and may lead to invalid estimates of
Model selection of multiple DNA alignmentsthe amount of functional sequence. The knowledge of variation in mutation rates along a sequence has been acknowledged for some timeand references therein for an overview]. The discrete-gamma model () was developed to account for rate variation when inferring phylogenetic relationships from molecular sequences. This model allows a variable number of substitution rate classes to approximate a gamma distribution of rates for computational efficiency; however, the underlying distribution of rates is still considered to be a single univariate gamma distribution. Recently, by using a more sensitive statistical approach, the authors showed that a multimodal distribution of evolutionary rates is more appropriate, and one can no longer confidently separate conserved from non-conserved sequences using existing approaches (). This discovery was based on a method of estimating distributions of evolutionary rates using a Bayesian multiple changepoint model (): a genomic segmentation technique that simultaneously segments a genome and classifies the segments, with computational speed that enables chromosome or even genome-wide analysis. The model, denoted here as the Bayesian Segmentation and Classification Model (BSCM), has been used with success at analysing pairwise () and multiple () alignments. In summary, the algorithm takes as input a sequence of characters (which may represent pair-wise or multiple alignments) and output positions that delineate homogeneous segments, the number of which is unknown a priori. Each segment is also classified based on the probability of observing each of the constituting input characters. Most importantly, the algorithm allows for an unknown number of segment classes; a facet of the model recently exploited for finding multiple classes of evolutionary rates in animal genomes (). The optimal number of classes is determined by fitting a number of models differing only in the number of classes, and calculating an information criterion for each model, based on a tradeoff between model fit and model complexity. The model with the smallest information criterion is generally considered the optimal; however, in practice, the model for which the majority of reduction in information criterion has occurred is chosen for further investigation, as there may not be a true optimum. The ability to take advantage of already sampled MCMC values is key to efficient analysis of these highly complex hierarchical models. Approximations to information criteria such as the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) have been used previously (); however, the behaviour of these approximations under different circumstances has not been investigated. The problem of generating an effective tool for model comparison is a challenging statistical problem, especially for computationally demanding hierarchical models. We consider the problem of selecting the optimal number of classes for the BSCM through information criteria and an approximation to the Deviance Information Criterion (DIC) for the BSCM is also presented. The behaviour and capabilities of three information criteria are explored through simulated data, examining the potential of the chosen model to detect superfluous changepoints and mixture modes and also the effect of sequence size on the resulting optimal model. Recent advances in DNA sequencing have lead to the sequencing of a number of species that are closely related to humans. We apply the BSCM to a sequence derived from the region corresponding to human chromosome 21 from a multiple alignment of four primates and mouse using the most recent assemblies. We explore the problem of identifying the optimal number of evolutionary rate classes for this input sequence.
APPROACH
The BSCMA full description of this model can be found inor the Supplementary Material in. A brief summary of the model is as follows: each alignment position is reduced to a single character, summarizing some feature of interest, with examples including GC content, match/mismatch for pairwise alignments and parsimony score () for multiple alignments. A sequence of such characters (of dimension D) defines the input sequence (S), assumed to be possible to partitioned into a series of homogeneous segments of length L, defined by a vector of changepoint positions, denoted by A. A uniform prior distribution is assumed for the locations of the changepoints, some of which are fixed, based on the blocks in the input alignment and a variable number, k, are estimated. The characters within a segment are assumed independent and drawn from a multinomial distribution with parameters, each drawn from one of T Dirichlet () distributions with uniformly sampled mixture proportions , . Uniform prior distributions are placed on the mean and SD of the Dirichlet parameters. The posterior density function for the parameters of a BSCM is given as:where m ij is the number of occurrences of character j in segment i and,The generalized Gibbs sampler (GGS) is used to sample from this varying dimensional space: it allows the number of changepoints to vary. However, there is another dimension in this model that can varythe number of classes T. It is possible to utilize the GGS again to allow the model dimension to vary in T , but it is unclear whether this approach would be advantageous. The availability of cluster computers makes information criterion methods of determining the best model an attractive choice. Typically, in this approach the number of classes is assumed known and the model parameters are then estimated for a range of numbers of classes. Each fitted model is then assessed in terms of model fit and model complexity, and the most parsimonious model that explains the data is chosen. There are a number of different choices for measuring model fit and complexity; in the following section, we give a brief overview of this problem in a general setting and provide a summary of how these measures are used in the BSCM.
AICThe AIC () is an estimate of the KullbackLeibler distance between the hypothetical 'true' and estimated distributions.
Page: 606 604610
C.Oldmeadow and J.M.KeithIt is defined as: AIC = 2lnf (y|y| y|)+2dwhere y is the observed data, d is the number of free parameters and and and the maximum likelihood estimate (MLE) for the model parameters . Strictly speaking, the AIC is not appropriate for mixture models, since its derivation relies on asymptotic theory of the MLE and the breakdown of regularity conditions namely, parameter values under the null hypothesis are on the boundary of the parameter space (). Nevertheless, it still is used with success in practice (). The AIC tends to penalize models with larger numbers of parameters less than other information criteria. An approximation to Equation (3) described in Keith et al.This approximation has the advantage that it makes use of MCMC sampled values, with 2lnf (y|) being the posterior mean deviance, rather than the optimized deviance. The use of the posterior mean deviance as a measure of model fit has been adopted by many authors (). For the BSCM, the advantage is that the criterion can be estimated using the already sampled segmentations, and is defined as:where  k is the average number of changepoints over the set of segmentations sampled by MCMC.
BICThe BIC is an information criterion derived from a Bayesian approach to model selection (). It has been shown to be consistent in estimating the number of components for a finite mixture model (). The BIC is defined as: BIC = 2lnf (y|y| y|)+d lnn, wher  is the maximum likelihood solution. The BIC tends to favour models with fewer parameters than the AIC since the BIC penalizes parameters more heavily. An approximation to the BIC that makes use of posterior samples is defined for the BSCM inas:
DICThe DIC () can be seen as a Bayesian version of the AIC and was originally developed as a model selection tool in generalized linear models. It takes on a similar form to the AIC; however, the complexity penalty is based on the number of effective model parameters, and not the actual number of free parameters as in AIC. The complexity measure proposed by, p D , is known as the effective number of parameters (or effective dimension) and is defined as the difference of the posterior mean deviance and the deviance at the posterior estimates of the parameters. Providing the posterior distribution can be reasonably approximated by a multivariate normal distribution,noted that the model Deviance, D() = 2lnf (y|) can be approximated by:where  2 p is a chi-squared distribution with p degrees of freedom, p is in this case the number of model parameters. By taking expectations of both sides of 4, the formula for the effective dimension is given as:) andand and is an estimate of , often the posterior mean, median or mode. The DIC is thus defined as:The DIC in Equation (6) can be seen as analogous to the AIC in Equation (3), with two essential differences. One, the plug-in deviance in the AIC is calculated at maximum-likelihood estimates of parameters, whereas the plug-in deviance in the DIC is calculated at posterior means (or, alternatively, posterior medians or posterior modes). Two, the penalty function for model complexity in the AIC is determined by the nominal number of parameters in the model, whereas the penalty function for model complexity in the DIC is determined by an estimate of the effective number of parameters in the model. The nominal number of parameters assumes that parameters are independent, whereas the effective number of parameters allows for non-zero covariance among parameters. Furthermore, the nominal number of parameters can be difficult to ascertain, especially in hierarchical models, whereas the effective number of parameters is estimated from the data. The problem for missing data models such as the mixture model is that p D cannot be intrinsically defined. The use of DIC in missing data problems has been considered (). In these models, the parameters  may not always be identifiable [a problem known as label switching (] and using the posterior mean forfor for = Ecan be a poor estimate. A more relevant choice is the posterior median or mode. For the Bayesian segmentation model, the set over which we are sampling is large. Therefore, finding the posterior mode is problematic and techniques such as simulated annealing () can be used. Another problem with p D is that it can be negative if the posterior distribution is very different from the normal distribution, hence f (  ) does not provide a very good estimate of . Also it is possible to get negative p D , if the sampling distribution is non-log concave and there is strong prior-data conflict (). An alternative to p D has been tentatively proposed by various sources) and discussed in. If instead of taking expectations of Equation (4), the variance is taken resulting in a different estimator of the effective dimension size. This estimate has been termed p V and is simply:Although it has been suggested that this version tends to substantially overestimate the effective number of parameters for normal random effect models. Page: 607 604610
Model selection of multiple DNA alignmentsThroughout this article, we propose the term DICV to signify the use of p V in the DIC.
METHODSWe simulated three sequences using a four-character alphabet, each sequence consisted of approximately 110 6 characters with only fixed changepoints. The positions of the fixed changepoints were determined using 1000 randomly sampled block lengths taken from the analysis of a parsimony sequence derived from an alignment of four mammals in a previous study by the authors (). The multinomial probabilities for each segment were drawn from a Dirichlet distribution with parameters set according to three scenarios:(1) A single class distribution, with parameters set to those of the most abundant class identified in the mammalian analysis (). For each block, we sampled multinomial probabilities from a Dirichlet (854.29, 553.71, 25.87, 0.09) distribution.(2) A three-class distribution, with parameters chosen as the posterior means from the three-class distribution identified in the analysis of Ancient Repeat sequence from the mammalian alignment (). Specifically:  ={0.07,0.20,0.73},To determine sensitivity to input sequence size, we simulated a sequence of approximately 110 7 characters using the same scheme as described above. All models were run for a period of 20 000 iterations (a single iteration defined as a full sweep over all parameters), at which point convergence was determined by inspecting the trace plots of the model parameters, and each of the the cumulative information criteria were inspected to determine if the chains were stationary. The expected proportion of no mutations against the mixture proportions were plotted to ensure no systematic drift. The MCMC chains were then set to run for another 5000 samples, post burn-in. We also ran the changepoint algorithm on a sequence of parsimony scores derived from a multiple DNA sequence alignment of four primates: Human (hg19), Chimp (panTro2), Gorilla (gorGor1) and Orangutang (ponAbe2) and Mouse(mm9). These data were obtained by extracting alignment blocks containing all five species from the 45-multi-species alignment (). Only blocks containing DNA sequence from standard chromosomes for all five species were retained. Further filtering included the removal of alignment columns that contained no-call or insertiondeletion characters. Parsimony scores were calculated for the filtered alignment columns using the method of. We ran the changepoint algorithm over a range of class numbers (1..10). Finally, we repeated the above analysis on an additional two multiple alignments:(1) Human (hg19) ,Gorilla (gorGor1), Rhesus (rheMac2), Tarsier (tarSyr1), Mouse (mm9) and(2) Human (hg19), Tarsier (tarSyr1), Bushbaby (otoGar1), Tree shrew (tupBel1), Mouse (mm9).
DISCUSSION
SimulationUpon first inspection of results from the one-class simulation (), we would choose the 10-class model as being superior using AIC or BIC. However, it is clear inthat the 10-class model was the worst fit (the size of the numbers is proportional to the deviance), and that model resulted in a smaller number of changepoints than for other models. This would suggest that both AIC and BIC have been conservative and placed a heavy weighting for the complexity penalty. The DICV, however, appears to put more weighting for model fit (or less penalty on model complexity)the one-class model has the minimum DICV. It should be noted that a degree of discretion should be used when choosing the best model, as inspection of the estimated mixture proportions tells another story. The mixture proportions for each fitted model all exhibit of a single dominant class, with the remainder of classes having much smaller weights (<0.0003%) (with the exception of the fiveclass model, in which case there are two majority classes with similar Dirichlet parameters). This would suggest that one should choose the one-class model as the superior model, as indicated by DICV and as known to be correct. The median posterior number of changepoints for the one-class model was 10 373; this is a slightly larger increase in the true number of changepoints (9892). The posterior median Dirichlet parameters for the best model werwer  = (762.8,494.1,23.1,0.07). Parameter estimates for the 110 7 character sequence are even closer to the true parameter values and given in Supplementary. For the higher order simulated data (Scenarios 2 and 3), we found few differences among the information criteria. The model with the minimum information criterion was the true model for the three-class Page: 608 604610simulated data (). For the simulated seven-class sequence, it appears as though the majority of reduction in all criteria had occurred by the sixth class (). Since there is no further reduction in information criterion or mean number of changepoints beyond the seven-class model, we would choose this model as optimal and indeed this was the true model. Note, a choice based on information criterion alone would be the six-class model, which turns out to be conservative as it is fewer classes than were simulated.For the seven-class data, the AIC and DICV behaved similarly. The number of estimated changepoints for fitted models with 7,8,9 and 10 classes were essentially the same, and both AIC and DICV assigned essentially the same rankings to the models. However, the effect of penalty on higher class numbers could be seen for BIC: higher information criterion values were assigned in BIC for models of higher dimension. This is consistent with BIC being a conservative measure placing a higher weight on model complexity; the complexity term [  k +c(a+1)] is multiplied by 2 in AIC, whereas for BIC the multiplicative factor is lnn.
C.Oldmeadow and J.M.Keith
Segmenting primate genomesFor an alignment of five DNA sequences, the only possible parsimony scores given the tree relating the species are {0,1,2,3}. The parsimony score input sequence was 61 656 42 characters in length and there were 109 271 fixed changepoints corresponding to alignment boundaries. The proportion of each of the four characters in this input sequence were 62.35, 36.11, 1.49 and 0.04%, respectively. We calculated the three information criteria for each of the models, and plotted these against the posterior mean number of changepoints (). It is clear that a minimum value of all three information criteria is reached at six classes, and for more classes it can be seen that there are much smaller reductions in the posterior mean number of changepoints. It could be argued that the majority of reduction in all information criteria had occurred by the four-class model; however, the mixture proportions for the six-class model are all appreciable ( ={0.034,0.064,0.451,0.035,0.317,0.097}), which suggests the additional classes may be real.shows fitted densities of a proxy for conservation level for the five species alignment. The proxy is the proportion of columns in which no mutations occur, parametrized by  0. The distributions of conservation levels are mixtures of beta Page: 609 604610distributions, with the mixture proportions estimated by taking posterior sample means and beta parameters estimated using the posterior medians (the medians were used in this case because a small number of large values had large effects on the means). The figure shows clearly that the modes are all well separated and correspond to class of sequence evolving at different rates among the five species.The analyses of the two additional multiple alignments were overwhelmingly similar, with the same optimal number of classes chosen. This is remarkable considering the difference in size of the input sequences for the three multiple alignment. A plot of the fitted conservation densities for all three alignments and the alignment of four species fromis in Supplementary. This shows the clear consistency in the location and relative proportions of the six classes of evolutionary rate. Of particular interest is the most slowly evolving class that occurs in all four alignments. We speculate this class could represent a highly conserved group of functional elements in primates, and is the focus of further research. The median length of the intervals between adjacent changepoints is given in Supplementaryfor each of the identified classes for each analysis.
Model selection of multiple DNA alignments
CONCLUSIONWe have presented an overview of information criterion approaches to model selection in genomic segmentation, and presented three methods of approximating information criterion using posterior samples. We conclude that the approximate information criteria are reasonable for model selection. In the simulation study, we successfully chose the true number of components for all three scenarios, differing in number of components and also the underlying Dirichlet parameters. Although, these criteria are very blunt instruments, used on their own they may result in incorrect model choice; however, a better choice can be made when these criteria are used in conjunction with an examination of the other model parameters such as the mixture proportions, model deviance and number of changepoints. Hence, it is advised that they be used with care and intelligence. Another important result is that the information criterion approach to model selection does not inflate the number of modes; if anything it is a conservative estimate of the number of modes. We applied the information criterion to determine the number of classes for an alignment of four primates and mouse and found six wellresolved classes. A previous analysis () using a parsimony score mapping of columns from an alignment of three primates (human, chimp, Rhesus-monkey) and mouse found evidence of seven classes, and moreover a comparison ofto Figure 5c insuggests that many of the classes identified in the two analyses correspond. The fact that such similar results were obtained is particularly intriguing because the estimated divergence times for the four primates (human, chimp, gorilla, orangutan) are much less than those of the three primates, yet there is still substantial evidence that there are multiple wellresolved classes of evolutionary rates. Hence, our methods are applicable to quite closely related species provided that a suitable outgroup is included. Moreover, the results from the simulation study increase our confidence in the results reported in. We have shown that the use of fixed changepoints does not result in an over-inflated estimate of the number of non-fixed changepoints, hence modelling natural boundaries such as alignment block boundaries as fixed changepoints apparently has no detectable influence. There is still much work needed in the area of model selection and its application to these models. While information criterion approaches to estimating the number of classes in the BSCM appear to be justified, they are computationally expensive, even with high powered clusters of computers. Further investigation would be warranted in applying a GGS scheme to sample over the model space, and this is a direction of future research. The ultimate goal of genomic segmentation is to attribute a specific functional class to each of the discovered segment classes; however, at this stage results suggest that the classes contain mixtures of functional classes (). Multiple sources of data indicative of function can be incorporated into the model by defining a suitable input sequence. In preliminary work by the authors, we have used these methods to identify a large number of classes when other information is accounted for in the input sequence, such as GC content, frequency of indels and also mutation types. This has led to the discovery of many novel functional elements in regions once thought of as largely not functional. The work presented here shows that we can be confident that any classes identified through changepoint analysis are most likely real classes and not artefacts of the data, and the information criteria should be used with a degree of caution. Our results also show the model is not influenced by the number of fixed changepoints or the number of segment classes underlying the input sequence. Funding: Australian Research Council (grants DP0879308 and DP1095849).
Conflict of Interest: none declared.
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
