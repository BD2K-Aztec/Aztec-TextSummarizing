Motivation: Second-generation sequencing technology has reinvigorated research using expression data, and clustering such data remains a significant challenge, with much larger datasets and with different error profiles. Algorithms that rely on all-versus-all comparison of sequences are not practical for large datasets. Results: We introduce a new filter for string similarity which has the potential to eliminate the need for all-versus-all comparison in clustering of expression data and other similar tasks. Our filter is based on multiple long exact matches between the two strings, with the additional constraint that these matches must be sufficiently far apart. We give details of its efficient implementation using modified suffix arrays. We demonstrate its efficiency by presenting our new expression clustering tool, wcd-express, which uses this heuristic. We compare it to other current tools and show that it is very competitive both with respect to quality and run time. Availability: Source code and binaries available under GPL at
INTRODUCTIONThe clustering of expressed sequence tags (ESTs) and other gene expression data continues to be a major challenge in bioinformatics. The emergence of new sequencing technologies such as pyrosequencing, collectively referred to as second-generation sequencing (), has recently reinvigorated studies using expression data. Second-generation sequencing provides the opportunity to study the transcriptomes of organisms for which good quality genomes are not known. However, new computational challenges have emerged, with much larger datasets, shorter sequence length and new error profiles (). In expression clustering, we start with a large set of cDNA sequences, typically 10 5 or more, which have been derived from transcriptomic data in a laboratory process (commonly, these sequences are referred to as ESTs). The goal is to find a partitional clustering such that sequences derived from the same gene are * To whom correspondence should be addressed. members of the same cluster. Expression clustering can broadly be divided into two classes: (i) clustering for which a reference genome is known (supervised clustering) and (ii) clustering for which a reference genome is not known (also called ab initio or de novo clustering). In this article, we focus on the latter class. Typically, single linkage clustering is used for expression data: if two sequences are similar, their clusters are merged. Within this approach, different similarity measures can be used. Traditionally, edit distance/alignment has been used to define similarity between sequences. However, alignment-free measures are increasingly being adopted, such as q-gram distance () or d 2 (). These define similarity between sequences with respect to the multiplicity of substrings (subwords) of a fixed, usually small, length. Because of effects such as alternative splicing, in expression clustering typically a local similarity of a predefined length is sought. For two sequences of length m to be regarded as similar, it suffices to find a pair of similar windows. Using subword-based measures, it is possible to compute the maximum similarity between all pairs of windows of a fixed length in time O(m 2 ) [similarly, computation of an optimal local alignment score takes O(m 2 ) time]. EST clustering algorithms that use subword-based distance measures rather than alignment methods have proved successful (). However, with the new and much larger datasets, computation time is still an issue: given n EST sequences, with average length m, computing all pairwise similarities requires (n 2 m 2 ) time. For real datasets this is prohibitive, at least without massive parallelism. Much work has gone into breaking these complexity limits. Filtering heuristics have been very successful. They test two strings in linear time to see whether they are likely to be similar, before a more expensive comparison is done. In practice, these heuristics have sped up clustering by orders of magnitude. However, the algorithms still remain quadratic in the number of sequences. In this article, we introduce the KABOOM filter, which greatly reduces the number of candidate pairs without compromising on clustering quality. This heuristic passes a pair of sequences if they share a given number of common words (substrings) of a given length, occurring at least a given distance apart. We also give details of its efficient implementation, which uses a modified suffix array. Contribution: our contribution is 2-fold:(1) We introduce a new heuristic filter for sequence similarity.Page: 3349 33483355
KABOOM(2) We demonstrate a method which, at least for real-world data, implements this heuristic and eliminates the need for all-versus-all comparison.The computation of our heuristic runs in time which depends on the multiplicities of words of the given length within the set to be clustered. Although in the worst case, the computation of our heuristic could take quadratic time in n, the number of sequences, the cost on real-life data is significantly subquadratic. On the other hand, the filtration rate of the KABOOM heuristic is impressive: on all our experimental data, the number of pairwise comparisons was reduced to far below 1%. We present extensive experimentation results which show how the KABOOM heuristic has sped up our previous EST clustering tool, wcd (), without compromising clustering quality. wcd uses the subword based dissimilarity measure d 2 to cluster sequences, shown to be competitive with or even superior to many existing tools, both with respect to running time and clustering quality (), where we reported comparison studies with ESTate (), xsact (), PaCE () and the assembler tool CAP3 (). Here, we compare our new tool, called wcd-express, to wcd and to two other EST clustering tools, TGICL () and the recent tool PEACE (), which claims to have better performance than wcd. wcd-express outperforms them both with respect to run time and produces at least as good quality. We show applicability to Sanger style data as well as to 454 and Illumina data (second-generation sequencing). We believe that the new filter has the potential to achieve similar speedups for other tools which rely on pairwise comparison of a set of sequences. Related work: EST clustering tools based on common words include PaCE (), xsact (), QUASAR (), more recently our tool wcd () and PEACE (). Several of these use suffix trees or suffix arrays for finding common words. PaCE and wcd both explore a heuristic of finding candidate matches based on a shared common word to avoid all-versus-all comparison. The basic idea is to choose a length k and to do a full comparison only on those pairs of sequences that share at least one word of length k. These pairs can be found efficiently using a suffix array or suffix tree, which allows to identify all sequences that contain a given k-word. The KABOOM heuristic adapts this idea to sequence pairs which share several exact matches. Our approach is closest to that of xsact (), which also uses multiple common words as a criterion for similarity. Unlike in KABOOM, xsact uses variable size common words, which must occur in the same order in both strings. However, the biggest difference is in the implementation. The authors only report results on one fairly small dataset; in our earlier work, we found the tool was very slow on large sets, and it did not scale (). This article shows how a related approach can be efficiently implemented and integrated in a well-established EST clustering system.
PRELIMINARIESWe start with some formal definitions. A string (or word or sequence) over a finite alphabet is a finite sequence s = s 1 ...s n of characters from ={A,C,G,T }. * is the set of all strings over. For a string s = s 1 ...s n  * , we denote by |s| its length n. For two strings s,w  * , where s = s 1 ...s n and w = w 1 ...w k , w is a substring, or subword of s, denoted w s, if there exists an index 1  i  n such that s i ...s i+k1 = w 1 ...w k. Such an index i is called an occurrence of w in s. A string of length k is also referred to as a k-word.
Clustering based on pairwise string similarityGiven a set S of strings over {A,C,G,T } (sequences derived from expression data), we want to find a clustering, i.e. a partition, of S such that two sequences end up in the same cluster if and only if they have been derived from the same gene. Since it is difficult to detect when two sequences have been derived from the same gene, we formally state our problem as: Problem statement (Expression Clustering). Given a set S of sequences over the alphabet ={A,C,G,T }, compute a clustering C ={C 1 ,...,C r } of S such that (i) r i=1 C i = S, and for i = j, C i C j = ; and (ii) for all s,t  S, if s and t are similar according to the given similarity or dissimilarity measure, then s and t are in the same cluster. This is the definition for ab initio, partitional, single-linkage clustering. For definitions of other types of clustering, see e.g. (). Condition (i) states that C is a partition of S. Dissimilarity measures commonly used for string comparison in EST clustering include the edit distance (), q-gram distance () and d 2 (). Usually, one decides on a threshold   R + , and then two sequences s,t are said to be similar if d(s,t)  , where d is the dissimilarity measure. In our tool, we use the dissimilarity measure d 2. We denote by mult(w,s) the number of occurrences of the word w in s. Definition 2.1 (Windowed d 2 ). Let s,t  * , Let q (the word size) and  (the window length) be two positive integers. Then d 2 (s,t) = min{ w q mult(w,s )mult(w,t ) 2 : s s,t t,|s |=|t |=}.For a string x, let mult q (x) denote the vector of length || q containing the multiplicities of all q-words in x, for some fixed enumeration of q. If |s|=|t|=, then d 2 (s,t) is the Euclidean distance squared (hence the name) of mult q (s) and mult q (t). In general, the two sequences s and t need not have the same length. When at least one of the two strings s,t is shorter than , then the parameters  and  are scaled accordingly. The measure d 2 has been shown to be well suited for Sanger-style expression clustering (), using q = 6 as word size. This article and that of () are the only ones to have examined the use of d 2 with second-generation data. A closely related dissimilarity measure employed for EST clustering is the q-gram distance (), defined as d qgr (s,t) = w q |mult(w,s)mult(w,t)|.
Filters based on common wordsBecause edit distance or windowed d 2 is quadratic in the length of the two strings, often filters are used: a similarity relation that the pair is first checked for before d(s,t) is computed. Our previous tool, wcd, uses the H-filter. It passes two words s,t if they have a certain number of qwords in common. For reasons
S.Hazelhurst and Zs.Liptkof efficient implementation, the word count is asymmetric: if a word appears x times in s and y times in t, then we add y to the count. Definition 2.2 ((H,q)-Similarity). Let H,q be positive integers. For s,t  * , we say that t is (H,q)-similar to s if it has at least H occurrences of the q-words that are substrings of s [i.e. if ws,|w|=q mult(w,Typically, q  6, H  70. This similarity relation is asymmetric, as the common substrings of s and t are counted with their multiplicity in t (and not in s). For the right choice of H, the (H,q)-filter is a true, non-heuristic filter for d 2 , as stated in the following lemma. The proof can be found in the Supplementary Material. Lemma 1. Let q, and  be given. Set H = q+1/2. If d 2 q (s,t)  , then s is (H,q)-similar to t, t is (H,q)-similar to s.This filter has probably good performance. Its computation is linear in the length of s and t; however, every pair has to be inspected separately. Therefore, this filter has (n 2 m) run time. In an attempt to avoid an all-versus-all comparison,use variants of the k-word similarity filter: Definition 2.3 (k-Word Similarity). Let k be a positive integer. For s,t  * , we say that s and t are k-word-similar if they have at least one substring of length k in common.The k-word filter can be implemented efficiently using a suffix array. Its fundamental problem is the choice of k. If k is too large, then many pairs with high similarity will be missed; if k is too small, then a quadratic number of candidate pairs will be found. The heuristic is particularly sensitive to the sequence error rateas it increases, the largest common word that two sequences must share in order to approximately match becomes smaller. In our previous work, we found setting k = 27 was reasonably effective for many datasets. But the approach was fragile: for some datasets, k = 27 was too big (particularly for shorter sequences) or too small (particularly for sequences with repeats). We now introduce the KABOOM filter, which is an extension of the k-word filter, and combines its efficient implementation with the good filtering properties of the H-filter.
The KABOOM filterThe idea behind the (k,,)-multiword filter (pronounced KABOOM) is to generalize the k-word approach: rather than requiring that two sequences share a relatively long common word, we require that they share several shorter common words. We claim that this has two advantages (which we substantiate through experimentation later): using multiple words allows us to test for longer regions of high similarity rather than short regions of exact similarity; and this is more likely to be biologically relevant. While we present the KABOOM filter in the context of using the d 2 dissimilarity measure and the wcd tool, the filter could in principle be used by any clustering algorithm. The KABOOM filter defines a pair of sequences to be similar if they share a fixed number  of common words (substrings) of a fixed length k, and in addition, the first and the last must occur at least a fixed distance  apart. This last condition is introduced in order to avoid too many overlapping matches. We shall count the substrings with multiplicities, i.e. if s has x occurrences of w and t has y, then we shall add xy to our count. [This count is known as D 2 in the literature (see e.g.), not to be confused with d 2 ]. Note that k refers to the word length used in the KABOOM filter, while q is the length of the word used in the computation of d 2 : typically k > q. Definition 2.4 ((k,,)-Multiword Similarity). Let k,, be positive integers. For s,t  * , let Common(s,t) ={(i,j) | s i .
..s i+k1 = t j ...t j+k1 }be the set of pairs of occurrences of common k-words of s and t. We say that s and t are (k,,)-similar if(1) |Common(s,t)|;Experiments showed that for Sanger-style data, conservative values of these parameters are k = 16,  = 3,  = 45 (for d 2-threshold of  = 40), while for 454-type data, conservative values are k = 16,  = 3,  = 16 (for  = 60): even though we do not have a theoretical guarantee, with these parameters, the filter produces negligibly few false negatives, i.e. pairs (s,t) which are not (k,,)-similar but whose d 2 score is below the threshold .
Symmetric and asymmetric implementation:if we drop requirement (3), we get an asymmetric KABOOM-heuristic requiring distance  only in one of the two sequences.
ALGORITHMIn this section, we detail our algorithm for finding all pairs of sequences that are (k,,)-similar. The suffix array of a string s is an array of length |s| which lists the indices i according to the lexicographical order of the suffixes starting at position i (). By the properties of the suffix array, for any non-empty substring w s, all occurrences of w are listed contiguously in the suffix array, as all suffixes prefixed by w are contiguous in the lexicographic order. Let sa be the suffix array of the sequence data. Fix k > 0. A k-block of sa is a maximal subarray of the suffix array where the first k characters of the corresponding entries in the text are equal. We define the modified suffix array to be the array sa such that the indices within each k-block of the suffix array sa are reordered in ascending order, and invs to be the inverse mapping of sa , i.e. invs[sa] = i. The modified suffix array sa has the following property, whose proof is immediate. Lemma 2. For each w with |w|=k, and every pair of occurrencesAs a small example, consider a set of seven sequences: aaa, aacggt, gttaaagt, tcggt, gttat, cgg and acggt. Let k = 3. The sequences are concatenated together (the @ symbol represents a sequence break character) and the suffix array constructed.gives a detail (lines 1526) of the suffix array sa (left), the suffix that starts at the corresponding position in the text (centre) and the modified suffix array sa. The Supplementary Material contains the fullEach k-block is ruled-off: e.g. rows 1720 and 2325 are kblocks. For clarity, we show all 3-words of the text; however, in the implementation, only those are considered which are substrings of one of the sequences s i , i.e. those that do not contain @. The k-block in our example from positions 17 to 20 inclusive corresponds to all occurrences of the k-word cgg. The array sa stores the elements of this k-block, 32,37,6 and 21, according to the lexicographic order of the corresponding suffixes, while the column sa shows the entries within the block sorted. We create the modified suffix array from the suffix array by scanning through it and re-ordering it, but it could probably be done more efficiently by modifying a suffix array generation algorithm.
The algorithmAn outline of the modified clustering algorithm is shown below. Detailed pseudo-code can be found in Algorithm 1. The main loop of the algorithm considers each sequence s i in turn. While processing sequence s i , we record each sequence s j containing common kwords with s i , where j > i. For each such sequence s j , we compute count:=|Common(s i ,s j )|, as well as the leftmost and rightmost occurrences of such words in s i and in s j. The variables lindIand rindI(lindJand rindJ) store the current leftmost and rightmost positions in s i (s j ) of all common k-word of s i and s j .
for allIn more detail: when processing sequence s i , we consider each (overlapping) k-word in s i. Let w be a k-word in sequence s i that starts at position r. Using the inverse of the modified suffix array invs, we find where this word occurs in sa , say at position p = invs. Then we find all occurrences of w in other sequences s j , where j > i, by looking at each entry p > p within the same k-block. By Lemma 2, this gives all occurrences to the right of the current occurrence of w. With this information, we update our records of which sequences s j share a k-word with s i , and also where, in both s i and s j , the leftmost and the rightmost of these common k-words are. Once all k-words in sequence s i have been processed, we know which sequences s j , for j > i, share a k-word with s i , how many kwords are shared and the various leftmost and rightmost positions. Sequence s i is a candidate sequence with each of those sequences satisfying the condition of the KABOOM filter. A small example is given in the Supplementary Material. wcd-express then compares each candidate pair first with the heuristics described by Hazelhurst (2008) and then, if necessary, using d 2. However, any dissimilarity function (such as edit distance) could be used. Note that every pair (s i ,s j ), i < j, is considered only once, namely in the iteration for s i .,lindJ); rindJ max(sa',rindJ); count++; {count: # common words of s i and s j } p++; r++; {compute d 2 (s i ,s j ) for j passing filtering phase} for all j  M do if (i,j) are (k,,) similar then if d 2 (s i ,s j )   {distance of s i and s j is small} then merge(cluster(i),cluster(j)); reset lindI,rindI,lindJ,rindJ,count,M;
Analysis of algorithmSuppose there are n sequences of average length m in set S. For Sanger-style sequencing m may be between 300 and 700. For real datasets n is unlikely to be <10 4 and may be as large as 10 6 (so m < n, and in large datasets m 2 < n). Let k be the word length used in the suffix array algorithm step. Typical values of k will be in the range 1220. We make the following definitions:The previous version of wcd needs (mn 2 ) time for the heuristics and (cm 2 ) time for computing the d 2 or edit distance of those pairs of sequences that have been passed by the heuristics. In wcdexpress, in the KABOOM-step, for each word w in the file, we do O( w ) work. Each distinct word w appears  w times, so the total work done is O( |w|=k  w 2 ) = O(Emn). Another way of looking at this is that there are mn (non-distinct) words, and for each we do E work on average. wcd-express then applies all of wcd's heuristics on the pairs passed, which run in linear time. Thus, the total amount of work done by wcd-express is (Emn+c K m+c X m 2 ). Since wcd-express applies all of wcd's heuristics, c X  c. In practice, c  O(n). Thus, provided E < n, wcd-express will run faster than wcd. However, since the constant factors due to memory behaviour are likely to be substantially larger, for practical purposes, it is important that E n. The experimental results reported later demonstrate that in practice wcd-express runs substantially faster than wcd and that indeed E n. For the generation of the suffix array, any of a large number of suffix array construction algorithms can be used, several of which run in linear time. In our current implementation, we use the mkESA tool (), which employs the DeepShallow algorithm (), one of several super-linear algorithms that have been shown to perform better in practice. See () for a survey of suffix array construction algorithms. wcd-express requires substantial amount of RAM to store both the suffix array and its inverse: (mnlogmn) amount of RAM.
IMPLEMENTATIONThe algorithm is implemented as an extension to the wcd clustering tool (). The code is implemented in C. Pre-processing: pre-processing requires building a suffix array of the data file and its reverse complement. We use the mkESA tool () to create the suffix array, which suited our needs well and performed excellently. Clustering: the wcd-express program performs clustering as presented in Algorithm 1. However, it is important to note that the call to the d 2 algorithm is preceded by the use of filtering heuristics described in;
RESULTSThis section compares wcd-express to the previous version of wcd, and to two other systems, TGICL () and PEACE (), evaluating the impact of the KABOOM heuristic and the overall performance of wcd-express. All experiments were done on an Intel E5335 (2 GHz; dual quadcore processor with 4 MB of L2 cache per processor and 16 GB of RAM; single thread; Scientific Linux 5.4, gcc 4.1.2, O-2 for wcd-express and wcd). We used the asymmetric implementation of wcd-express, which initial experimentation showed was slightly faster (but which is controlled by a compiler-switch).Experimental results on different sets of EST datasets. # seqs is the number of sequences in thousands; size is the number of megabases. E is the average square of the frequency of the distinct words;  is the ratio of the number of distinct words to the total number of words. wcd-express is the time our new algorithm takes including pre-processing; wcd is the time our previous version of wcd takes. All times in seconds are rounded to the nearest second. For both versions of wcd, the same parameters were used. The sensitivity of wcd-express with respect to wcd is over 0.999 in all cases.
DataThe experiments use both real and synthetic Sanger-style and second-generation data. The data are described in the Supplementary Material. In summary, A686904 is a set of 686904 Arabidopsis thaliana sequences from GenBank, while other datasets of the form Ax are subsets of this; ecHuman is the EasyCluster reference set (); chlamy a set of Chlamydomonas reinhardtii sequences used to test PEACE (). The C-series is a set of synthetic data files, generated using the ESTsim tool (); the metasim files are synthetic files simulating 454 and Illumina style sequences using the tool MetaSim ().shows the improvement in running time from wcd to wcdexpress, gained by adding the KABOOM filter. We use a number of Sanger-style datasets using the same parameters for both tools. The running times for wcd-express include the generation of the suffix array. In all cases, there is a large speed-up, and the speed-up is larger with larger datasets. Recall from Section 3.2 that the success of the algorithm depends on how E compares to the number of sequences. As can be seen from, on all our datasets E is two to three orders of magnitude smaller than n. Next, we explore the impact of the KABOOM filter with different word sizes on the quality of the clustering and running times.shows the time taken and quality of the clustering of different datasets as the suffix word length k changes, on different data files (both real and synthetic). H is the parameter of the (H,q)-filter (Section 2.2). For quality, we measure sensitivity of the clustering with respect to a base case of suffix array word length of 10 (a very conservative value). The other clustering parameters are chosen extremely conservatively to avoid masking too aggressive values of other parameters. These results show that choosing a suffix word length of 16 does not significantly decrease sensitivity but that for word length at least 12 the running time improves dramatically.
Impact of the KABOOM filter
Quality comparison with other toolsGiven a clustering C of a set S, quality is measured using validity indices, which compare C to a known clustering D. These give a numerical value in the range(1 being best value), in terms of the number of pairs (x,y) which are put into the same cluster by both C and Dcompares the running times and quality of wcd-express and PEACE on several datasets with known clusterings (the first three are Sanger-style data, the others second-generation). wcdexpress and PEACE have very similar quality scores, and wcdexpress consistently outperforms PEACE with respect to run-time. A complication in comparison is that PEACE filters out lowcomplexity sequences in a pre-processing step, while the wcd tools do not. Therefore, forwe adopt a pre-step before calling wcd-express of filtering out the same sequences that PEACE does. Note that our results differ from those quoted inbecause we adopt a different methodology for dealing with the filtered sequences. See the Supplementary Material for a full discussion. Since TGICL incorporates filtering into its clustering step in a different way, a direct comparison with wcd-express was not possible. However, we note that () report a comparison of TGICL with PEACE and with wcd and found the clustering quality to be competitive. The clusterings computed by wcd-express and wcd are essentially the same. Note that the scores for 454 and Illumina data indiffer from those reported in (). The reason is that the default parameters of wcd were optimised for Sanger-style data, while for short read sequences, other values are appropriate. (For details see Supplementary Material.) As shown in our earlier work, the choice of parameters has a very important effect on the quality of the results (). In most published work, tools are compared based on one set of parameters. This kind of study is limited, since it only proves that with one set of parameters one tool performs better or worse than another tool with its own set of parameters.We stress that it is unlikely that any tool has a set of universally optimal parameters. The range of different sequencing technologies and quality of data means that different modelling parameters will be required to cluster optimally. This makes computational performance more important. A fast tool is very helpful since it gives an experimenter the ability to cluster the same dataset with different parameters and investigate the stability of the clustering with respect to the various parameters. We also ran tests that showed that for reasonable values of k, very high levels of sensitivity can be obtained. For Sanger-style data with k = 16, even for an aggressive  = 60, wcd-express gives less than a 0.1% FN rate. For 454 data, with k = 14, and aggressive clustering values we get less than a 0.5% FN rate. As expected, performance declines with shorter sequence length; however, second-generation sequence length is increasing as the technology improves.
Computational cost comparisons with other toolsMemory usage: the use of the suffix array and inverse creates significant memory requirements for wcd-express. The current implementation requires 25 bytes of RAM per byte of input so that a 200 MB input file requires 5 GB of RAM to run effectively. This makes wcd-express much more expensive than wcd (which required 0.3 bytes of RAM per byte of input). TGICL's memory footprint is very small as the input file is broken into chunks. PEACE's memory usage pattern varies. PEACE's usage is 210 times less than wcd-express's, with the difference diminishing on larger data (2.1 GB for the 200 MB input file mentioned above). A positive feature of wcd-express's memory usage is that memory is allocated as the data are read inthus even on a long run, usage will be known after a minute or so. PEACE's memory usage, on the other hand, may increase throughout its execution. The Supplementary Material presents experimental evidence and discusses memory use in more detail.
Running times: the tables and figure below show computational costs of wcd-express, PEACE and TGICL on a variety of data. As discussed, TGICL and PEACE do filtering while wcd-express does not. The Supplementary Material shows that this has a profound effect on quality and performance. If tool A filters and B does not, it may appear that A is faster than B, or vice-versa when the root cause is that the data to be clustered are effectively very different. In the experiments below, we adopt a filtering pre-step for wcd-express.To be fair, we do this differently in the comparisons with TGICL and PEACE, as these tools filter differently (see Supplementary Material for details). We emphasize that the times reported for wcd-express take into account all pre-processing time, including filtering and construction of the suffix array. In, we show how the running times scale as datasets grow larger. The Arabidopsis data is a set of real ESTs.shows the difference between wcd-express and TGICL on subsets of Human 454 ESTs of different sizes. See the Supplementary Material for additional experimentation.shows performance of wcd-express on a range of different datasets. In summary, the results show that wcdexpress outperforms PEACE, and for very large datasets does so substantially. wcd-express is also much faster than TGICL. Additional experimentation and more detailed results (including pre-processing costs) are presented in the Supplementary Material.
S.Hazelhurst and Zs.Liptk
CONCLUSION AND FUTURE RESEARCHThis article has introduced a new algorithm for finding candidate pairs for clustering gene expression data. The idea is that two sequences are candidates for comparison if they share  many common k-words, where the leftmost and rightmost words start at least  away from each other. This heuristic can be implemented very efficiently using a modified suffix array and its inverse. The experimental results show that the algorithm is very effective. For reasonable values of word length, substantial improvement in compute performance is achieved without degradation in quality of clustering. There are a number of areas for future work: Improvement of the algorithm implementation: there is scope for improving the current implementation, using less memory and improving cache behaviour. We currently have an experimental version of our tool that uses about half the amount of memory, with an approximate 15% run-time penalty. Parallelization: at this point, wcd-express is not parallelized (wcd has both pthreads and MPI parallelization). Parallelization should be straightforward, though given the overall cost of pre-processing, for large-scale parallelization, suffix array construction and sequence filtering must both be parallelized. A number of implementations of parallel suffix array construction are available, including in mkESA, which can be used directly. Improving clustering quality: the experiments show that the quality of clustering is very dependent on parameters used. The most important lesson is that in practice, bioinformatists should run their
Page: 3355 33483355
KABOOMtools several times with different parameters to evaluate the stability of their results. Work is needed to separate out the effect of the parameters used and the underlying algorithms. This is also likely to be affected by the error models of the underlying sequencing technology (e.g. the homopolymer problem for 454 data).
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
