Motivation: High-dimensional data such as microarrays have created new challenges to traditional statistical methods. One such example is on class prediction with high-dimension, low-sample size data. Due to the small sample size, the sample mean estimates are usually unreliable. As a consequence, the performance of the class prediction methods using the sample mean may also be unsatisfactory. To obtain more accurate estimation of parameters some statistical methods, such as regularizations through shrinkage, are often desired. Results: In this article, we investigate the family of shrinkage estimators for the mean value under the quadratic loss function. The optimal shrinkage parameter is proposed under the scenario when the sample size is fixed and the dimension is large. We then construct a shrinkage-based diagonal discriminant rule by replacing the sample mean by the proposed shrinkage mean. Finally, we demonstrate via simulation studies and real data analysis that the proposed shrinkage-based rule outperforms its original competitor in a wide range of settings.
INTRODUCTIONWith the advent of high-throughput technologies, learning highdimensional complex models is critical in many disciplines such as biology, genetics, epidemiology, geology, ecology, neurology and engineering. One such example is microarray data, where the expression levels of thousands of genes are measured simultaneously from each sample. Due to the cost and/or other experimental difficulties such as the availabilities of biological materials, it is common that high-throughput data are collected only in a limited number of samples. They are referred to as high-dimensional data with small sample size, or 'large G small n' data, where G is the number of dimensions and n is the sample size. Highdimensional data pose many challenges to traditional statistics methods. Specifically, due to the small n, there are more uncertainties associated with standard estimations of parameters such as the mean and variance estimations. As a consequence, statistical analyses based on such parameter estimation are usually unreliable. To obtain * To whom correspondence should be addressed. more accurate estimation of parameters some statistical methods, such as regularizations through shrinkage, may yield better results. Shrinkage-based methods have been proposed in recent years to improve the variance estimation for 'large G small n' data. See for example,, Storey and, Wright and Simon (2003), Smyth (2004),, Tong and, Opgen-Rhein and Strimmer (2007) and, among many others. In contrast to the advances on variance estimation, little attention has been paid to improving the mean estimation for high-dimensional data until recently (). In this article, we investigate the family of shrinkage estimators for the mean value which is tailored to the high-dimensional data such as microarrays. Specifically, we will propose the optimal shrinkage parameter under the quadratic loss function for the data when G tends to be infinite. Class prediction with high-dimensional data has been recognized as a very important problem and received much attention in different fields such as genomics, proteomics, brain images, medicine and machine learning. For high-dimensional data with small sample sizes, it is known that the traditional classification methods such as the linear discriminant analysis are not applicable as the sample covariance is going to singular when G is greater than n. To overcome the singularity problem,introduced two diagonalized discriminant rules, the diagonal linear discriminant analysis (DLDA) and the diagonal quadratic discriminant analysis (DQDA). When the sample size is small, DLDA performed remarkably well compared with more sophisticated classifiers in terms of both accuracy and stability (). In addition, DLDA is easy to implement and is not sensitive to the number of predictor variables. Though DLDA performed well for high-dimensional small sample size data, there is still room to improve it (). In particular, we notice that the mean estimation (the sample mean) in DLDA will be unreliable when the sample size is not sufficiently large. As a consequence, the performance of DLDA may also be unsatisfactory. With this insight, we propose in this article an improved version of DLDA, which replaces the sample mean by the optimal shrinkage estimator. We expect that the proposed shrinkage-based DLDA will improve the classification accuracy in practice. The remainder of the article is organized as follows. In Section 2, we investigate the family of shrinkage estimators for the mean value under the quadratic loss function. With the nature of highdimensional data, we assume that the variances are unequal and unknown. Under regularity conditions, we discuss the choices of thePage: 532 531537
T.Tong et al.shrinkage parameter and then evaluate their practical performance via numerical studies. In Section 3, we construct a shrinkage-based diagonal discriminant rule by replacing the sample mean by the proposed shrinkage mean. Simulation studies will also be conducted to evaluate its performance over its original competitor. In Section 4, we use the leukemia data to demonstrate that the proposed method is widely applicable and performs well. Finally, we conclude the article in Section 5 with a brief discussion.
IMPROVED MEAN ESTIMATION,...,n, be independent G-dimensional vectors normally distributed with mean  = ( 1 ,..., G ) T and covariance matrix. Let  X = n j=1 X j /n be the sample mean. Let for any invertible matrix. In this section, we consider estimating  with respect to the quadratic loss function.
MotivationShrinkage estimation of means has a long history starting with the seminal paper of James andwher  2 0 is the pooled estimator of  2. More generally, when is non-diagonal and unknown, the James-Stein type estimator has the form ()is the pooled sample covariance matrix. To guarante non-singular, we require that n > G. Note that this is not the case for high-dimensional data where G can be much larger than n. Therefore, the existing shrinkage methods for estimating  break down and cannot apply to the high-dimensional data directly.
Proposed mean estimatorTo overcome the singularity problem, we assume is diagonal and denote it by D = diag( 2 1 ,..., 2 G ). As mentioned in, most existing shrinkage estimators of  in the literature required the variances  2 i , i = 1,...,G, to be either equal or unequal but known. When  2 i =  2 for all i, the problem reduces to D =  2 I with  2 unknown (). When the  2 i are unequal but known, the problem reduces to the case considered in. In this article, we consider the assumption of  2 i being both unequal and unknown. Note that the same assumption has been commonly used in the recent literature,where i = 1,...,G, j = 1,...,n, N() is the standard normal distribution, and Inv- 2 () is the scaled inverse chi-squared distribution with unknown hyperparameters ( 0 , 0 , 2 0 ). The joint prior density of  i and  2 i is given asbe the sample mean and the sample variance for gene i, respectively. Let). By, the posterior distribution of ( i , 2 i ) can be represented asThe posterior mean estimator of  i is then,...,G.Note that  0 in (1) is an unknown parameter. We propose to estimate n/ 0 +1 by the form of  X 2 S /r, where r is a shrinkage parameter to be tuned. We then have the following estimator for ,Under the quadratic loss functionshowed that the estimator (2) dominates  X when 0 < r < 2(G2)/(n+1).
Optimal shrinkage parameterIn this section, we propose the optimal shrinkage parameter of r > 0 within the family of estimator (2). In Appendix A, we show that the optimal shrinkage parameter is given asPage: 533 531537
Improved mean estimationNote that for high-dimensional data, n is usually small but G is large. We have the follow asymptotic result. (n3)/(n1) as G .The proof of Lemma 1 is given in Appendix B. By Lemma 1, for high-dimensional data, we hav r opt and the optimal shrinkage estimator isLetrLet Letr = (G2)/(n+1) be the middle point of the range 0 < r < 2(G2)/(n+1) in. It is clear that when n is large,  r opt andrand andr are asymptotically equivalent. While for highdimensional data with small sample sizes, the difference between these two estimators can be large. For instance, when n = 5, the rati r opt / r is given as 2.4; and when n = 6, the ratio is 1.9. The practical performance of these two estimators will be studied in the next section.
EvaluationThe first simulation is to evaluate the performance ofof of). Assume that G = 100. We consider four different values of n, ranging from 5, 10, 20 to 50, to represent different levels of sample sizes. We draw  2 i from the scaled chisquare distribution  2 n1 /(n1), and  i from N(0, 2 ), where  = 0.2, 0.6 or 1 representing different levels of mean heterogeneity. For each gene i, we simulate n observations from the normal distribution N( i , 2 i ). We repeat the process 5000 times for each setting and report the simulated average risk AR = 5000 k=1 L(for the four estimators, respectively. Under the quadratic loss function (3), the sample mean  X has a constant risk at 1, as reported in the simulations. The standard errors of these average risks are all around 0.14/  5000 = 0.0020. Therefore, the improvements of the three shrinkage estimators over  X are all statistically significant. We observe thatthat that( r opt ) has a smaller average risk than bothboth both JS andand and( r) in most settings, especially when the sample size is small. In addition, the improvement ofof of( r opt ) over  X increases when the mean heterogeneity decreases, especially when  is near zero. We also observe that the improvements of the shrinkage estimators over the sample mean become smaller when n becomes larger. This indicates that for the large sample size scenario, it is no longer necessary to borrow information from other genes to improving the mean estimation. Note that we have assumed the grand mean to be zero in the above simulation. Note that when the grand mean has a shift from zero, the term G i=1  2 i / 2 i will tend to be larger so that the improvement ofof of( r opt ) over  X will be diminished correspondingly. In such situations, Lindley (1962) suggested to apply the shrinkage method to the deviations X ij   X  rather than to the original observations X ij , which leads to the following variation of the estimator (5),is a vector of size G with common values. To evaluate the performance of the Lindley-type estimator (6), we simulate  i from N( 0 ,0.5 2 ) for three different values of  0 : 0, 1 and 2. All other settings remain the same as before. We repeat 5000 simulations for each setting and report the average risks of both both both( r opt ) andand and( r opt ) in. We observe that the two shrinkage estimators perform similarly when  0 = 0. When  0 is away from zero,  ( r opt ) may perform unsatisfactory while the performance ofof of( r opt ) remains the same. This indicates that the Lindley-type estimator is robust to the shift of the grand mean. In the remainder of the article, the estimator (6) will be adopted to estimate the mean value unless otherwise specified.
IMPROVED DIAGONAL DISCRIMINANT ANALYSISAs mentioned in Section 1, class prediction with high-dimensional data is an important problem in high dimensional data analysis. The objective of class prediction is to assign a new observation to one of the K classes based on its given profile. For ease of notation, we define the class labels to be integers ranging from 1 to K with n k observations belonging to class k,k, respectively. Let N = n 1 ++n K be the total number of observations. For high-dimensional data such as microarrays, it is common that the dimension is much larger than the sample size. As a consequence, traditional classification methods such as the linear discriminant analysis are likely to be inapplicable to such high-dimensional data directly. For instance, the sample covariance matrix is singular when G is larger than N. To overcome the singularity problem,introduced DLDA and DQDA that ignored the correlations among genes. Specifically, under the assumption that k = for all k, DLDA classifies a new observation y = (y 1 ,...,y G ) to class k which minimizes the following discriminant scorscoris the sample mean of class k,is the pooled sample covariance matrix withkwith withk), andand and k = n k /N are the prior probabilities of which the next new observation is coming from class k. DLDA is also called a 'naive Bayes' classifier as it arises in a Bayesian setting ().We refer to it as shrinkage-mean-based DLDA (SmDLDA). Similarly, we can propose a shrinkage-mean-based version for DQDA as well. The behavior of the proposed SmDLDA will be studied in the next section under various scenarios.
Simulated studyIn this section, we conduct simulations to compare the performance of SmDLDA with DLDA. We consider a binary classification with simulate data from multivariate normal distributions MVNThe first simulation study considers an identity covariance matrix, i.e. = I G. To differentiate the two classes, without loss of generality we assume the first d, 0 d  G, components of  1 and  2 are the same and the left ones are different. Specifically, we letis a random sample of size Gd from the uniform distribution U(0,0.5). We consider two choices of G (50 and 200), and for each G, we consider six different d values at 0, 0.1G, , 0.5G, respectively. For each simulation, we generate a training set of size n k for each class k under the simulation setting described above, and a test set of size 5n k under the same setting to assess the misclassification rate. The overall misclassification rate is calculated by the percentage of misclassified observations over the total numberof observations in the test sets. Finally, we repeat the procedure 1000 times and report the average misclassification rates infor n 1 = n 2 = 10 and 20, respectively. It is evident that SmDLDA has a smaller misclassification rate than DLDA in all settings. To evaluate and compare the performance of DLDA and SmDLDA under more realistic situations, we conduct here another simulation study in the case where the observations are correlated. Similarly as in, we use the following block diagonal structure to mimic the true covariance matrix,where each diagonal block has the following auto-regressive structureWe consider five different values of the correlation coefficient , ranging between 0, 0.2, 0.4, 0.6 and 0.8, to represent different levels of dependence. Note that  = 0 corresponds to the independent situation in the previous simulation. All other settings are the same as before except that we set d = 0.1G in this new simulation. We repeat the procedure 1000 times and report the averagemisclassification rates infor both DLDA and SmDLDA. Once again, SmDLDA outperforms DLDA in most situations. The comparison result is more evident when the correlation coefficient  is not large. Though we have restricted to a balanced binary classification with n 1 = n 2 due to the page limitation, extensive simulations (not shown) indicate that the above comparative conclusions remain the same for unbalanced designs as well as for other simulation settings, including the multiclass comparison problems.
APPLICATION TO LEUKEMIA DATAIn this section, we apply the proposed discriminant rule, SmDLDA, to the leukemia data of. The dataset is available in the website of www.bioconductor.org. By following the same pre-processing steps (thresholding, filtering and logarithm transformation) as described in, we end up with a gene expression dataset with a total of 3571 genes for 47 acute lymphoblastic leukemia (ALL) patients and 25 acute myeloid leukemia (AML) patients. We further standardize the dataset so that each array has mean zero and variance one across genes. Note that in general, the shrinkage methods work well for dense data rather than sparse data. Thus, to better reveal the practical performance of SmDLDA we perform a preliminary screen of informative features before the case study. Different screen methods are available in the literature so as to identify biologically significant gene functional groups or pathways via Gene Ontology annotations (). In this study for simplicity, we will not carry out the screen by integrating the real biological knowledge, but instead, will perform it based on the ratio of the between-group to within-group sums of squares as in. Note that the gene selection can also be based on other proposals, see forexample, Bayesian variable selection (), analysis of variance () and independent component analysis (). Let ALL be class 1 and AML be class 2. The ratio for gene j is given aswhere  X ..j is the averaged expression values across all samples and  X k.j is that across samples belonging to class k. We select the top G genes (50 and 200) with the largest BW ratios for further study. To assess the misclassification rates for both SmDLDA and DLDA, we randomly divide the total 72 samples into training sets and test sets. We let the training set size for each class ranging from 5, 10, 15 to 20, respectively. The remaining samples are then used as the test sets. Recall that the improvement of shrinkage is inversely proportional to the mean heterogeneity (Section 4). To better improve the performance of the shrinkage-based rule, we propose an adaptive procedure that aims to reduce the possibly large level of mean heterogeneity that may appear in real data. Specifically, we shrink the distances between the class centroids and the overall centroids rather than to shrink the class centroids directly. This is a similar idea as in. Finally, for each setting, we repeat the procedure 1000 times and report the average misclassification rates in. Similarly as in the simulation studies, it is evident again that SmDLDA outperforms DLDA in all settings.
DISCUSSIONIn this article, we proposed an optimal shrinkage estimator for the mean value under the 'large G small n' scenario. We then applied the proposed shrinkage estimator to high-dimensional classification problem by constructing a shrinkage-based diagonal discriminant rule. Its improvement over the original competitor was demonstrated through both simulations and real data analysis. Though the independence assumption in this article is popular in the literature (), it is unlikely to be true in practice and so certain remedy might be necessary for a further improvement when additional information is available. Langaas Page: 536 531537 T.suggested that the clumpy dependence is a likely form of dependence, where the clumpy dependence means that the genes are dependent within groups and independent among groups. Inspired by that, one natural extension would be to propose new shrinkage estimators for the mean value under the clumpy dependence structure. To avoid the singularity problem, we might need to assume that the largest group size is not larger than the number of samples. Another future work is to examine if the proposed optimal shrinkage estimator has any good in its own right, or if it can be further improved by its positive-part estimator. Finally, we note that the proposed SmDLDA in this article is a shrinkage-mean-only-based DLDA, whereas inthe authors proposed a shrinkage-variance-only-based DLDA. As both the mean and variance estimations are crucial in the statistical analysis, further research might be needed to develop new classification rules that shrink both the mean value and the variance. Possible approaches can be either by plugging-in the existing shrinkage estimators, respectively, or by proposing new shrinkage estimators for the mean value and variances simultaneously.Finally, by minimizing the above quantity, we have
APPENDIX B B1. PROOF OF LEMMANoting that f (x) = 1/x is a continuous function in (0,), it suffices to show that   n as G . By, the following strong law holds under the condition sup mFurther, we have EZ 1 = 1/(n3) for any n  4. This proves the lemma by noting that A G ==
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
