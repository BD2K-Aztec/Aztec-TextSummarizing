Motivation: Several applications in bioinformatics, such as genome assemblers and error corrections methods, rely on counting and keeping track of k-mers (substrings of length k). Histograms of k-mer frequencies can give valuable insight into the underlying distribution and indicate the error rate and genome size sampled in the sequencing experiment. Results: We present KmerStream, a streaming algorithm for estimating the number of distinct k-mers present in high-throughput sequen-cing data. The algorithm runs in time linear in the size of the input and the space requirement are logarithmic in the size of the input. We derive a simple model that allows us to estimate the error rate of the sequencing experiment, as well as the genome size, using only the aggregate statistics reported by KmerStream. As an application we show how KmerStream can be used to compute the error rate of a DNA sequencing experiment. We run KmerStream on a set of 2656 whole genome sequenced individuals and compare the error rate to quality values reported by the sequencing equipment. We discover that while the quality values alone are largely reliable as a predictor of error rate, there is considerable variability in the error rates between sequencing runs, even when accounting for reported quality values.
INTRODUCTIONk-mers are one of the most fundamental objects used when analyzing DNA sequencing data. Many assembly algorithms () start by constructing a de Bruijn graph, a graph containing all k-mers for some fixed k. Some of the most commonly used algorithms for aligning reads to a reference genome start by finding short exact matches of a fixed length k (commonly referred to as a seed); an index of all k-mers is constructed and from this index an initial alignment of a part of the read is found.In this work, we focus on the problem of estimating the number of distinct k-mers in a set of sequencing reads. This problem is related to k-mer counting, in the sense that if we can solve k-mer counting we can report the number of distinct k-mers. However, the methods we develop are an order of magnitude faster and use only a fraction of the memory compared with current k-mer counting software. We develop streaming algorithms to solve this problem, a framework first proposed by. A similar approach is used by KmerGenie () which samples kmers to approximate the frequency histogram of k-mer occurrences. Sequencing errors cause considerable problems for k-mer based algorithms for both assembly and read mapping; in assembly these may cause the assembly to become disconnected and increases both the memory usage and computational time. In read mapping it may lead to increased computational overhead and the true location of the read not being found. The removal or correction () of erroneous bases and erroneous fragments is therefore a common preprocessing step in the analysis of DNA sequences, in particular when doing de novo assembly or read mapping to a reference assembly. Several programs () use k-mers to explicitly fix or remove sequencing errors in the dataset and it is recommended to use them prior to assembly (). A number of software programs have been written for quality control of DNA sequencing data, including FastQC (http:// www.bioinformatics.babraham.ac.uk/projects/fastqc/). FastQC has a number of functionalities useful for quality control, including giving a distribution of the quality values assigned by the sequencer, quality distribution by position, N content and GC content, identifying overrepresented k-mers and sequence length distribution. However, the k-mers identified tend to be short, 6 basepairs by default, and although they are useful for identifying contaminants they are not unique enough in the underlying genome to be useful for assessing the sequencing error rate, independent of what is given by the DNA sequencers. The problem of k-mer counting for high throughput sequencing data sets has been well studied (), given a set of reads report the coverage of each k-mer, i.e. how many reads contain that k-mer. Current methods for obtaining aggregate statistics of k-mer data are based on keeping track of all k-mers in a set ofMuch work has been done on reducing memory requirements, based on exact or approximately correct methods of keeping track of a large set of k-mers, this work includes using succinct set representations () or probabilistic encodings such as Bloom filters (), whereas recent advances have focused on more speed (). Although the impact on memory usage is considerable, compared to previous approaches, these methods require storing all k-mers, explicitly or implicitly, in memory. Thus the amount of resources will grow linearly with the input size. Many methods also rely on having access to all the reads for multiple passes over the data or require additional disk space for storing intermediate results. Thus all of the above methods suffer from 'the curse of deep sequencing' () in which more sequencing can overwhelm the program in terms of memory usage and the algorithms simply fail to make use of increased amounts of data.
ContributionThe main contribution of this article is a streaming algorithm for estimating efficiently the number of k-mers that occur exactly once in a data set, taking care of identifying the k-mer with its reverse complement. The time requirement for this algorithm is only a constant factor times the time taken to read the data and requires space that is only logarithmic in the size of the dataset. This method can be extended to give an estimate of the k-mer abundance histogram. Additionally our algorithm reports frequency moments of the k-mer count, which are aggregate statistics of the histogram. For experimental validation we show the results of running KmerStream on reads from a single lane of PhiX control sequences. We show the distribution accuracy of the estimator compared to the accurate k-mer counts and that the estimators are almost always within the approximation levels guaranteed. Additionally we formulate a simple error model, both from the estimates KmerStream produces as well as the true k-mer counts. Both error estimates are then compared to the true k-mer error rate obtained from mapping k-mers to the reference genome of PhiX174. The results in Section 3.2 show that our simple error model underestimates the true error rate, however only by a few percent on average. As a simple application of our method we use it to estimate the error rate in a sequencing dataset, conditioned on the per basepair quality value given by the sequencing equipment. Each basepair in the read is assigned a quality value on PHRED scale, an assessment of the probability that the basecall is correct. It is up to the sequencing equipment, and the basecalling method used, to assess this probability from raw data. Unlike most other methods designed for estimating error of sequencing data, our method does not require the mapping of the data to a reference genome as a preprocessing step. The low computational overhead of our method makes it suitable to identify quality issues early on in the analysis pipeline. The method can therefore be used as a filtering step to determine the quality of a run, quickly determining errors before the read mapping or assembly stage. We ran the method on a set of 2656 whole genome sequenced individuals. Our results suggest that the error probabilities given by the Illumina HiSeq machines are largely accurate. Our results also show that once we have conditioned on an error probability given by the sequencing equipment there is considerable variance in the sequencing error, suggesting that sequencing error needs to be reassessed on a per sample basis.
BackgroundA common task is to generate the abundance histogram for the k-mers. Generating the exact histogram requires storing a large number of k-mers in memory and can be done with k-mer-counting tools such as Jellyfish (MarcaisMarcais and Kingsford, 2011) or BFCounter (). Counting all k-mers in high-throughput sequencing datasets requires tens, or even hundreds of gigabytes of memory, whereas the method we propose requires less than 10 MB. Recently, a method was proposed that generates an approximation of the histogram based on sampling k-mers (), this method however does not have a guaranteed error rate associated with it. We propose to not consider the exact histogram itself, but to compute statistics based on the histogram counts, which can be found using less memory and more speed than current methods by using streaming algorithms. We define f i to be the number of distinct k-mers that appear i times in the set of reads. The histogram is then simply a table of the f i values. One of the key statistics we are interested in is f 1 , the number of singleton k-mers, i.e. k-mers that appear exactly once in the set of reads. Previous studies () have shown that when a genome is being sequenced at relatively high coverage the majority of singleton k-mers do not come from the genome, and are generated from sequencing errors. Also, the number of singleton k-mers grows with increased coverage, whereas the number of k-mers from the genome will approach a fixed number as coverage increases. Given the frequencies f i of the histogram, we define the k-th frequency moment, F k , asIn addition to the number of singleton k-mers f 1 , we are interested in three moments; F 0 , F 1 and F 2. F 0 is the number of distinct k-mers in the set of reads and F 1 is simply the number of k-mers in the reads, counted with repetition. The second moment, F 2 , puts a higher weight on the number of k-mers that have high abundance values. For each of the frequency moments streaming algorithms have been developed, that can estimate their value to within a factor of 1 AE " with high probability using only O " 2  log N   memory, where N is the number of distinct k-mers. It should be noted that estimating the frequency is solved in the general setting of counting arbitrary items in a stream, but for the remainder of the paper we will focus exclusively on the k-mer counting case. Estimating the second moment, F 2 , was first solved in the seminal work of Alon, Matias and Szegedy (). This article also formalized the framework and popularized the research field. The first rigorous estimator for the number of distinct elements, F 0 is from (), although earlier work from () applies as well. The first moment, F 1 , is easiest to construct as we can maintain a single counter that is incremented once for each k-mer.
Estimating F 0 and f 1To compute the f 1 estimator we use a hashing approach similar to the approach of Bar. The high level idea of the algorithm is to sample the stream at different rates, and afterwards select the sampling rate that gives the best result. For each k-mer a we compute the hash value h(a) and find the least significant 1 when the value is written in binary, e.g. if ha=0110100 2 then the least significant 1 is in the third position. In the special case when h(a) = 0 we define the position to be the number of bits used to represent h(a) (64 in our experiments). If the hash value ends in w zeros, then 2 w divides h(a) and the binary representation of h(a) ends in 1, followed by w zeroes. Assuming the hash values are random half the k-mers will have w = 0, a quarter will have w = 1, etc. In order to estimate the number of distinct k-merswe use as our data structure a list of arrays, T, one for each potential value of w, we say that the array T w is at level w, and we say that all k-mers with value w hash to this level. Each array has a fixed size R that is only dependent on the error parameters ", and each element in the array is a 2-bit number storing values from 0 to 3. When processing a k-mer a we look into the array T w. Conditioned on the value of w, the w + 1 least significant bits are no longer random as they must end in a 1, followed by w zeros. We discard the lowest w + 1 bits by dividing by 2 w+1 and use the result of the division, modulo the size of the array, as an index into the array T w and increase the counter there. As we are only interested in F 0 and f 1 , in the application presented here we limit the counter to two bits and if the counter is at 3, we do nothing. An extension of this work would be to allow the counter to count more bits, the algorithm could then be used to estimate f i for general i. When the number of k-mers is much larger than R the array in T 1 will almost certainly be full, i.e. all the values will be at 3. This indicates that sampling at a rate of one half is too small, and so we should look at lower sampling rates. Algorithm 1 estimates f 1 by relating it to the probability that a counter in an array has value 0 or value 1, respectively. To get a good estimate for those probabilities we require that the number of distinct k-mers that hash to the array is roughly of the same size as the array itself. Thus after we have processed all the k-mers we can find the most appropriate array T w to use. In our analysis we ignore the possibility of two distinct k-mers having the same hash value, as we use 64-bit hash values and a pairwise random hash function we expect to have less than 10 collisions for 10 billion distinct k-mers. If we look at a fixed level w, the probability of a counter being 0 depends only on the number of distinct k-mers that hashed to this level, N w , and the size of the array, R. This probability is p 0 =1  1 R  Nw , as the probability of each k-mer hashing to this counter is 1 R. Note that the multiplicity of the k-mer does not factor in here, since each k-mer will always hash to the same counter every time it appears. If we can estimate this p 0 accurately, then we can solve for N w to get an approximation on the number of distinct k-mers that hash to the level w. SolvingSince the sampling rate is 1 2 w we can use the estimate for N w to approximate the number of distinct k-mers. Our estimator for p 0 is ^ p 0 = t0 R , where t 0 is the number of counters with value 0 in the array T w. The estimator of the number of distinct k-mers in the dataset is thenAlthough this would work for any value of w, i.e. all values of w have the same expectation, different values of w have different variance. If we use w = 1, then nearly all values in T w will be non-zero, giving a poor point estimate of p 0. Similarly if w is too high, so that only a handful of elements map to T w the estimate for p 0 will be poor. In general we want our estimates to be bounded away from 0 and 1, to this end we select the level w  where the fraction of empty counters is as close to 50% as possible. By thinning the stream of hash values we can adaptively sample at different rates and decide on the best rate to use after we have seen all the data when we have to report the answer. Algorithm 1 Streaming algorithm for counting k-mers function UPDATE(a) z h(a) w Number of trailing zeros of z iUp to this point we have followed the work of Barfor estimating F 0 , the estimator here is identical to the one derived in Theorem 2 of (). We now show how to extend these results to obtain an accurate estimate of f 1. We define a singleton to be a k-mer that occurs once and let x 1 be the number of singletons that hash to level w. We note that each counter with a value of 1 has to be the result of a singleton k-mer hashing to this counter, since for non-singleton k-mers the counter would have been increased at least twice. On the other hand some singleton k-mers will hash to the same counter as some other k-mer so we must find a way to relate x 1 to some probability we can compute. The probability that a counter contains the value 1 isThis is seen by choosing the one singleton k-mer that should hash to the counter from the x 1 possible candidates. The chosen singleton hashes to the counter with probability 1 R and all the other N w  1 k-mers must not hash to the counter with probability 1  1 R  Nw1. Note that from(2) we see that the part 1  1 R  Nw1 is equal to p0Thus we haveWe estimate p 1 in the same fashion as p 0 , namely let t 1 be the number of counters in T w equal to 1 and set ^As t 0 and t 1 follow a binomial distribution with R trials and probabilities p 0 and p 1 , respectively, we can expect the error in the estimates of the probabilities, ^ p 0 and ^ p 1 , to be on the order of 1 ffiffi ffiWe note that this method can be extended to obtain estimates of higher frequencies, f i for i ! 2. As an example for f 2 , we note that if a counter has the value 2, this can only be obtained from two independent singletons mapping to the counter or one k-mer with frequency 2. The first happens with probabilitythe latter with probwhere x 1 and x 2 are the number of k-mers with frequency 1 and 2 that map to a level. Given an estimate for N and x 1 we can solve to obtain an estimate for x 2. This scheme could be generalized for higher values of i to obtain estimates of f i , although obtaining a guarantee on the error rate is left as an open problem.
Estimation of k-mer error ratesWe can use our results to estimate error rates of a genomic sequencing experiment. For this we will focus solely on the k-mer error rate, i.e. given a k-mer from a read what is the probability that the k-mer did not originate from the DNA sequenced. This rate is higher than the basepair error rate, since we require all of the k basepairs to be intact. If the sequencing errors in a read were independently distributed, it would be easy to convert the k-mer error rate to a basepair error rate. However sequencing errors are not independent, generally the ends of the reads have higher error rates and sequencing errors can come in batches. Thus we will focus on the k-mer error rate in this discussion and note that converting it to a basepair error rate assuming independence will lead to an underestimate of the true basepair error rate. We use a simple generative model where total number of sampled k-mers with repetition, namely F 1 , is given. Each k-mer comes from a random position in the genome of size G, and with probability " k it contains a sequencing error and is error-free with probability 1  " k. To account for repeated errors, we further assume that each erroneous k-mer is produced by sampling a random basepair in the reference k-mer and changing the base to one of the three other nucleotides, so that it does not match the reference. For each true k-mer there are 3k possible error k-mers and Hamming distance 1. The number of k-mers sampled at each location follows a Poisson distribution with mean and so the coverage of an errorfree k-mer is Poi1  " k . For a fixed erroneous k-mer the coverage follows a Poi "k 3k , assuming that there is only one possible position in the genome that could produce this error. Our model has three parameters, G; and, " k. To obtain estimates for these parameters we require three statistics, namely f 1 , F 1 and, F 0. Given the Poisson distributions of the coverages and the number of possible k-mers G for the error-free and G  3  k for the single error k-mers, we can derive the following equationsWe can isolate from(8) and G from(7) to obtainSimilarly we derive by canceling the common factor G from(9) using (8)(11) we can solve for and " k numerically to obtain an estimate of the coverage, k-mer error rate and finally the number of k-mers present in the genome. We make a couple of observations about this model. First, the assumption that all error k-mers are solely due to single nucleotide differences causes us to slightly underestimate the true error rate. Second, a sequencing error in one k-mer may not be detected as an error because the error k-mer also occurs as a true k-mer in the genome. As we have chosen k = 31 then in random DNA the probability that a error k-mer also occurs in the human genome is less than 10 9. Third, the same error k-mer can occur in two distinct reads by chance given high enough coverage, this is modeled in our k-mer distribution model via the Poisson variables. Finally, our model ignores copy numbers in a diploid genome, either single copy from heterozygous SNPs or multi-copy from repetitive regions. In Section 3.3, we give the fraction of k-mers by copy number in the Human Genome. It has been observed that in practice sequencing reads are not randomly sampled from the genome and the true distribution is not a Poisson distribution. In particular, the coverage of genomic regions is known to be dependent on the GC rate of the basepairs ().have shown that the error rate is site specific, i.e. that sequencing error rate varies with the location being considered. Nonetheless we show in Section 3.2 that this simplified model gives a good estimate of the k-mer error rate using only three statistics from the k-mer distribution, rather than the entire histogram.
Implementation detailsThe input to KmerStream is a collection of short reads S. For each read s 2 S we generate all substrings of length k in s, denoted as k-mers. When reads contain other characters than ACGT, such as N, we split the read on the non-ACGT characters and consider all k-mers in the split reads. For each k-mer we also consider the reverse complement of that k-mer, and in general make no distinction between the two when counting. We considered all k-mers that only contained basepairs that had a q-value above a fixed threshold. Hence, if a read contains a basepair with a q-score less than the given threshold only k-mers to the left and to the right of the basepair were considered. Our computations were run for q-score thresholds of 0, 13, 20 and 30, corresponding to basepair error rates of 1, 0.05, 0.01 and 0.001, respectively. As an example, when using a q-score threshold of 30 then, according to the annotation given by the sequencing equipment, all basepairs considered should have an error rate of less than 0.001 or 0.1%. As many of the basepairs will have even lower annotated error rates, then if the annotation of the manufacturer is correct, the average error rate should be even lower. The software is implemented in C++ and can read FASTQ, compressed FASTQ and BAM files. The user can select the k-mer size used, the error rate " and additionally it allows for filtering k-mers based on quality scores. For the quality filtering, all bases in a read that do not meet the cutoff are discarded, thus only k-mers where all the bases in the k-mer have good quality values are kept.
Comparison to KmerGenieWe compare our software, KmerStream, to KmerGenie in terms of running time and memory usage. As input we used the H. Sapiens chr 14 (36M reads) and B. Impatiens (303 M reads), datasets from GAGE (). Both software were run for 4 values of k simultaneously, k=31; 47; 63; 79 using four threads for parallelism. As a comparison running a fast k-mer-counter, scTurtle () took 3594s using eight cores and 26 G of memory, for a single value of k = 31 for B. Impatiens. Our experiments show that KmerStream is at least 3.5 times as fast as KmerGenie and the memory usage is an order of magnitude better, see. It should be noted that the time to read the H. Sapiens dataset was 63 s, and 1085 s for the compressed B. Impatiens dataset. Our program is therefore only about two times slower than the time it takes to read the data. This comparison also shows that KmerStream is an order of magnitude faster than full blown k-mer counting, when it comes to exploratory analysis.
PhiX174 ValidationTo validate the accuracy of the k-mer error rate model proposed in Section 2.3, we used sequencing reads obtained from a PhiX control lane. We chose to use PhiX for this experiment as its genome is completely known and short enough for easily classifying k-mers as belonging to the genome or not. The sequencing data from the control has 363M reads, while the genome size of PhiX174 is only 5386 nucleotides. This high coverage allows us to partition the data and repeat the estimation process on a real dataset, rather than working with simulated data. Due to the small genome size, reads were sampled so that the per basepair coverage would be 30-fold. This was necessary to scale down the coverage to values that better correspond to a normal sequencing coverage used for whole genome sequencing. This was repeated 1000 times and each computational experiment run independently. For each input we used k = 31 and classified all k-mers in reads as true or false, depending on whether they appeared in the reference sequence or not. Separate frequency histograms were generated for both classes of k-mers. We then ran KmerStream for each input obtaining estimates for f 1 , F 1 and F 0. The top row ofshows the distribution of relative accuracy for f 1 and F 0 respectively. The program was run with a default error guarantee of 2% for F 0 , since for this input f 1 =F 0 is about 0.5 this results in a 4% error guarantee of f 1. From the distribution of(top row), we see that the vast majority of the experiments have estimates within the confidence interval specified. The error rate model was fitted using both the true values for the statistics as well as the estimated values from KmerStream.The true k-mer error rate was obtained by matching k-mers to the PhiX174 reference genome. Given the number of caveats listed when deriving our simplified model, as well as obtaining estimates from only three key statistics, we are pleasantly surprised to see how well the model fits the actual results. Based on the true values for the k-mer statistics we see from(bottom row, right) that our model underestimates the k-mer error rate by 4% on average. When using the estimated values from KmerStream,(bottom row, left) shows that we underestimate the error rate by 4% on average, same as using the true k-mer statistics, but we observe an increase in the variance of the accuracy of our estimate. This increase in variance is primarily because of the underlying variance in our estimate of the k-mer statistics. Of course the KmerStream method is much more efficient than obtaining the accurate counts, both in terms of runtime and memory. Based on these results we can safely interpret the results of the k-mer error model, at least up to a factor of 0.9 to 1.0.
Unique k-mers in the human genomeThe model for estimating the k-mer error rate does not take into account k-mers that have a high repeat count, due to repetitive regions, systematic errors in sequencing or heterozygous SNPs. To detemine the number of k-mers having high repeat count, we constructed the diploid genome of a single individual. As input we used Human reference genome build 36, genotypes called in that individual as determined using the GATK () genotype caller and the assignment of these genotypes to haplotypes using long range phasing (). From this information we constructed two copies of each chromosome. Using Jellyfish (Marcais) we counted a total of 2.552G 31-mers in this diploid genome and observe that 4.9% of the 31-mers only occur once in the diploid genome, indicating that they overlap a heterozygous polymorphic region. Additionally, 91.7% of the 31-mers occur twice, indicating that the region is non-polymorphic, 0.1% occur three times, 1.9% of the 31-mers occur four times and 1.4% occur more than four times. In total only 3.4% of k-mers are repeated in the genome, suggesting that our assumptions that most k-mers occur uniquely in the genome is largely correct.
Error ratesTo estimate the k-mer error rates we ran our algorithm on a data set of 2656 whole-genome sequenced individuals, using Illumina HiSeq sequencers. These individuals had an average coverage of 15.9 and a minimum coverage of 6. All of these data are sequenced under the same conditions at the same laboratory and have already undergone a number of quality control procedures (). We expect these data to have comparable error characteristics.shows a histogram of the average per k-mer error rate for different q-value thresholds and inwe give critical values of the distribution of the read error rate. We observe that without any filtering on q-values, on average 5.9% of the k-mers are estimated to contain errors. This number varies considerably between samples and 5% of the samples have an estimated error rate of 11.5% or higher, while 5% of the samples have an estimated error rate of 3.7% or lower. We observe that the error rate decreases with a higher q-value threshold. We observe a marked uniform decline in the fraction of error k-mers when the threshold is increased from 0 to 13, an average reduction of 80%. A smaller, but clear, decrease is observed when the q-value threshold is increased from 13 to 20, 25% on average. However when comparing increasing the threshold from 20 to 30 we observed an average change of 0.9%, but a median of 9%. Additionally this decrease in the number of error k-mers is not uniform and in 44% of all individuals the estimated error rate increases when the q-value threshold is increased from 20 to 30, i.e. using q-value thresholds larger than 20 does not necessarily give one lower error rate k-mers. Furthermore the number of k-mers, i.e. the coverage of the data set was reduced by 25% on average by increasing the q-value threshold from 20 to 30. In Table 2, we consider only the k-mers that are removed when the error threshold is increased We observe that when increasing the q-value threshold from 0 to 13, on average an estimated 96% of the k-mers removed are singleton k-mers. This suggests that unless the algorithm being used for analysis is highly robust to errors, then using a q-value threshold of 13 will increase the signal-to-noise ratio of the data, without losing coverage. On average of 85% of all k-mers in our dataset have q-value at all bases greater than 13, indicating that not considering k-mers with a q-value threshold less than 13 has limited impact on the number of k-mers being considered while it has a large impact on the fraction of k-mers that are error free. When we increase the q-value threshold from 13 to 20 an average of 91% of the k-mers removed are singleton k-mers and when increasing the threshold from 20 to 30 an average of 54% of the k-mers removed are singleton k-mers.
DISCUSSIONThe amount of data being gathered with modern sequencing methods continues to grow at a faster rate than our ability to analyze and store the data. An alternative view to the current state of the art is to consider technologies that sequence DNA 'on the fly'. In this case, the sequencing machine does not store all the results, but rather transmits the sequence reads as they aregenerated. Technologies that fit this framework have been proposed () and are currently in development, such as technologies from Oxford Nanopore, but technical details are limited at this point in time. Regardless of the exact technologies used, this new sequencing paradigm opens up new opportunities for online or streaming analysis of the data, where we bypass the storage requirements, and simply plug the sequencing directly into the analysis. One benefit of the algorithms we have developed is as follows; F 0  f 1 is a crude estimate of the number of k-mers that have been sequenced at least twice, when this number goes above a certain fraction of the genome size we can decide to stop sequencing. Another benefit is that when the error rate goes above some threshold we can decide to stop the experiment immediately, not wasting our time on failed experiments. The method presented here can be particularly useful when used for a species that has not been previously sequenced, allowing us to get an estimate the coverage of this genome while sequencing prior to assembly. When we condition on the error rate given by Illumina we see considerable variability in the error rate between individuals. Hence, it is not advisable to use the error rates in a model without considering differences between individuals. Our results show that although the base pair quality values given by the sequencing equipment are largely correct, there appears to be a considerable sample-dependent difference in the error rate conditioned on the base pair quality rate reported by the manufacturer. Our recommendation based on the results of sequencing 2656 individuals is to estimate both the number of k-mers F 0 as well as the coverage and k-mer error rate for multiple q-value thresholds and decide on a case by case basis.Conflict of interest: None declared.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
METHOD We start by giving the background behind our work, we then present our algorithm for estimating the number of unique k-mers. Finally we show how this estimate can be used to estimate the error rate in a sequencing experiment, independent of the error rate reported by the sequencing equipment.
P.Melsted and B.V.Halld orsson at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Streaming algorithms for k-mer abundance estimation at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
RESULTS We start by showing that our method is both faster and uses less memory than a previously presented methods. In order to validate our method for estimating the sequencing error rate, we first compare our method to an exact method on a known fully sequenced organism. We then estimate the effect that we repeated k-mers in the human genome could have on our error rate estimate. Finally, we apply the method to estimate error rate in a set of 2656 whole genome sequenced individuals. The scripts for the software comparisons and detailed version information are in the supplement.
