Motivation: RNA-seq technology has been widely adopted as an attractive alternative to microarray-based methods to study global gene expression. However, robust statistical tools to analyze these complex datasets are still lacking. By grouping genes with similar expression profiles across treatments, cluster analysis provides insight into gene functions and networks, and hence is an important technique for RNA-seq data analysis. Results: In this manuscript, we derive clustering algorithms based on appropriate probability models for RNA-seq data. An expectation-maximization algorithm and another two stochastic versions of expectation-maximization algorithms are described. In addition, a strategy for initialization based on likelihood is proposed to improve the clustering algorithms. Moreover, we present a model-based hybrid-hierarchical clustering method to generate a tree structure that allows visualization of relationships among clusters as well as flexibility of choosing the number of clusters. Results from both simulation studies and analysis of a maize RNA-seq dataset show that our proposed methods provide better clustering results than alternative methods such as the K-means algorithm and hierarchical clustering methods that are not based on probability models. Availability and implementation: An R package, MBCluster.Seq, has been developed to implement our proposed algorithms. This R package provides fast computation and is publicly available at http://www. r
INTRODUCTIONNext-generation sequencing (NGS) technologies have revolutionized studies of genome structure, gene expression and epigenetics (). One important application of NGS technologies is in the study of gene expression by measuring messenger RNA levels for all genes in a sample. This technology is called RNA-seq, and several reviews have described this nascent technology (). Here we briefly describe how RNA-seq data can be generated. The complete set of messenger RNA molecules are first extracted from a sample and converted to a library of short complementary DNA fragments. Then these fragments are sequenced simultaneously by NGS technology. The resulting millions of short sequences, which are commonly called reads, are then aligned to a reference genome or reference transcripts. Gene expression is measured by the enumeration of reads mapped to each gene where the gene can be defined as a collection of exons or other appropriate definitions given the context of a study (). The resulting RNA-seq data are essentially digital signals that can be used to quantify levels of gene expression (). This differs from microarray technologies that measure gene expression by fluorescence intensities detected from hybridized samples. Inescapable factors such as cross-hybridization, secondary structure of the DNA and technical challenges associated with fluorescent detection used in microarray analysis limit both the sensitivity and dynamic range. Compared with microarray technologies, NGS technologies permit quantitative measures of gene expression over a much larger dynamic. These advantages have rapidly accelerated the adoption of the NGS technologies in studies of gene expression and present new challenges to data analysis. In the pioneering studies using RNA-seq, only two treatment groups were analyzed (). More recently, RNA-seq experiments that examined multiple treatment groups have been published. For example,carefully selected a developing leaf from a corn plant that captures multiple stages of photosynthetic differentiation. They exploited Illumina sequencing technologies to profile gene expression from four representative sections of the leaf blade. One major goal of this study was to survey gene expression profiles along different developmental stages to gain understanding of the transcriptional network associated with the development of C4 photosynthesis. In this endeavor, cluster analysis is an important tool as it often reveals groups of genes with similar expression patterns, where genes within such groups tend to be functionally related.took a heuristic approach by applying the K-means algorithm to partition log-transformed data for the differentially expressed genes. The K-means algorithm starts from an initial partition of the objects (genes) and proceeds by iteratively calculating the centers (means) of clusters and reassigning each object to the closest cluster according to some measurement of distance such as Euclidean distance. This iteration continues until no more reassignments take place. Although this heuristic approach is easy to implement, its *To whom correspondence should be addressed  The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com performance was not evaluated for RNA-seq data analysis. Studies of clustering algorithms with microarray data revealed that heuristic algorithms performed worse than model-based algorithms (). Surprisingly, there has been few published statistical research to examine cluster analysis of RNA-seq data, although it is urgently needed due to the huge amount of data being generated. Model-based algorithms for microarray data are based on finite mixture of normal distributions and cannot be directly applied to RNA-seq data that are discrete counts and often skewed. RNA-seq data have been modeled using Poisson () or negative binomial (NB) distributions (). Witten (2011) describes a hierarchical clustering method to cluster samples (experimental units) based on the RNA-seq data of all genes within each sample using Poisson model and dissimilarity measure based on likelihood ratio statistics. Often the case, as in, clustering gene expression profiles is of interest. In this article, we aim to cluster genes based on the differential expression patterns across treatments using modelbased statistical methods. In other words, we are interested in grouping genes that share the same or similar expression foldchanges with respect to the mean expression level across all treatments. To do this, we derive model-based clustering algorithms for cluster genes based on either Poisson or NB models for RNA-seq data, and we evaluate the performance of the modelbased approach and heuristic algorithms including the K-means method to cluster genes. We describe the Poisson and NB distributions in Section 2 and show how our model-based clustering method handles both probability models in a unified fashion. We present an expectation-maximization (EM) algorithm for estimating the model parameters and cluster membership in Section 3.1. In addition, a model-based initialization algorithm is proposed in Section 3.2 to reduce the dependence on the initialization. We also describe two stochastic versions of EM algorithms in Section 3.3 that are intended to reduce the chance of being trapped at local solutions. A model-based hierarchical algorithm is proposed in Section 3.4 to generate a hierarchical structure of the clusters and allow more flexibility of choosing cluster numbers. In Section 4, we simulate data and compare the proposed method with others using three commonly used criteria: sensitivity, specificity and mutual information (MI) (). In Section 5, we apply the model-based method to the data fromand evaluate our results by comparing the clusters with gene annotations. We summarize in Section 6 that our results from extensive simulation studies and an analysis of an RNA-seq dataset all show that our proposed method outperforms alternative methods, namely, the K-means algorithm and self-organizing map (SOM) ().
MODELLet N gij denote the count of reads mapped to gene g for replicate j of treatment i for g  1,    , G; i  1,    , I; j  1,    , n i , where G is the total number of genes of interest, I is the number of treatment groups and n i is the number of replicates for treatment i. Two discrete probability distributions have been proposed to model RNA-seq data. The Poisson distribution has been shown to be appropriate for the RNA-seq data when only technical replicates are included (). When there are biological replicates, RNA-seq data may exhibit more variability than expected with a Poisson distribution, i.e. the overdispersion phenomenon (). The NB model proposed by Robinson and Smyth (2008) originally for serial analysis of gene expression data allows overdispersion and has been applied to RNA-seq data analysis (). We consider both distributions in this article.
Poisson distributionSuppose N gij follows a Poisson distribution with mean l gij that is parameterized as follows: log l gij  s gij  g  gi 1 with P I i1 gi  0. The offset term s gij is a normalization factor that may depend on the gene length and library of a sample such as the total number of mapped reads of a library. Once estimated from data, the normalization factor is often treated as known in the model (). The parameter g represents the geometric mean expression level of gene g across all treatments; gi measures the expression level of gene g in treatment i relative to the overall mean expression. To cluster gene expression profiles, we are interested in clustering the vectors g   g1 ,    , gI  for all G genes.
Negative binomial distributionFor the NB model, we adopt the parametrization inby modeling the variance as VarN gij   l gij  g l 2 gij
2where l gij is the same as in (1) and g is a dispersion parameter. Compared with Poisson model, an extra parameter, g , is introduced for each gene. Robinson and Smyth (2008) described several methods to estimate g. In this article, we estimate g by the quasi-likelihood method. To simplify the algorithm, we treat g as known on its estimation because our numerical studies showed this strategy produced similar clustering results to those based on the true g values (see Section 4.3). With this strategy, the unknown parameters are the same for the Poisson and NB models, and thus we denote the likelihood function for both models by fN g j g , g  for gene g where N g  fN gij g.
MODEL-BASED CLUSTERINGModel-based clustering methods assume that data are generated by a mixture of probability distributions where each component corresponds to one cluster. Extensive research has been done in modelbased clustering with multivariate normal mixture distributions. See, for example, Fraley and Raftery (2002) for an excellent review. In this section, we describe model-based clustering for RNA-seq data with the probability models introduced in Section 2. The algorithms described later in the text aim to cluster gene expression profiles, which is desired in practical application.Consequently, genes within the same cluster have similar expression profiles (denoted by g in our notation), but may have different overall mean expression levels (indicated by g ). However, it is straightforward to make changes in the algorithm if the goal is to cluster according to both the overall expression levels and the expression profiles, g  g. Suppose there are K clusters and let k   k1 ,    , kI  denote the center of cluster k with P I i1 ki  0 for k  1,    , K. The likelihood of the mixture model for gene g isis the likelihood if gene g belongs to the kth cluster and p k is the mixing proportion with p k ! 0 and P K k1 p k  1. The likelihood function can be based on a Poisson model or NB model as described in Section 2. Taking all genes together, the likelihood is as follows:Note that we assume independence among genes, which is likely not true in real situations. However, it is difficult, or impossible, to model and estimate the correlation among tens of thousands of genes with only several replicates and no prior knowledge about the relationship among genes. Thus, for simplicity, we take the independence assumption as in previous model-based cluster analysis for microarray studies ().
Model-based clustering with the expectation-maximization algorithm (MB-EM)The EM algorithm has been widely applied to model-based clustering with multivariate normal mixture distributions (). McLachlan (1997) describes an EM algorithm to fit overdispersed univariate count data in Poisson regression and logistic regression setting. Here, we derive an EM algorithm (Algorithm 1) for clustering RNA-seq gene expression profile with a mixture of Poisson or NB models. Let Z gk  1 if gene g belongs to the kth cluster and Z gk  0 otherwise. The EM algorithm views the cluster memberships Z  fZ gk : g  1,    , G; k  1,    , Kg as missing data and proceeds by iteratively calculating the conditional expectations of Z and updating the estimates for model parameters until convergence:Algorithm 1: MB-EM Algorithm.(i) Initialization: Set p 1 k according to prior knowledge about the cluster size. If no such information is available, let p 1 k  1=K for k  1,    , K. Choose K vectors 1 1 ,    , 1 K with P I i1 1 ki  0 for k  1,    , K as the initial set of cluster centers. See Algorithm 2 for one way to choose these 1 k. Obtain the initial values of 1  f 1 gk : g  1,    , G; k  1,    , Kg by maximizing fN g j gk , 1 k  with respect to gk for each combination of gene g and cluster k.(ii) E-step: Calculate the conditional expectation of Z gk given data and parameters estimated from the mth step(iii) M-step: Update the parameter estimates bywhere ^ Z m gk is obtained from from step (ii).(iv) Return to step (ii) or stop the iteration if change of the total log-likelihood is small.(v) For each g  1,    , G, assign gene g to cluster k if k  argmax l ^ Z gl , where ^ Z gl is obtained after the convergence of aforementioned steps.Note that Algorithm 1 not only assigns gene g to cluster k but also provides a measure of the uncertainty in the assignment by 1  ^ Z gk. If clustering based on g  g is preferred, then we do not estimate gk but estimate k together with k and corresponding calculations in step (i)(iii) can be easily modified.
InitializationIt is well known that initialization of the cluster centers impacts both the speed of convergence and the outputs of the EM algorithm (). To tackle this problem,proposed to pick the initial cluster centers from observations in a specific way such that they are well separated from each other with respect to some distance measure. Following this idea, rather than choosing K genes uniformly at random from all genes and using their expression profiles as the initial cluster centers, we only choose one cluster center uniformly at random and then set the additional centers gradually by selecting genes based on the distance between each gene and each of the selected centers. Here, the distance is measured by likelihood function.Algorithm 2: Model-based Initialization for Cluster Centers.(i) Choose one gene randomly from all genes, and set the initial center for cluster 1, 1selected cluster center 1 l by d gl  log max g2R,for g  1,    , G; l  1,    , m. Then randomly select a gene with probability q g  d 2 g = P G g 0 1 d 2 g 0 for d g  min fd g1 ,    , d gm g and set a new center 1 m1 as the MLE of g for the selected gene in this step. (iii) Repeat step (ii) until K cluster centers are obtained.By the definitions of d g and q g in step (ii) of Algorithm 2, a gene is more likely to be selected if it is far away from all existing centers. Hence the K centers chosen by this algorithm are expected to be separated better than a set of centers that are randomly selected. Our simulation study shows that this algorithm improves the performance of EM algorithm (Section 4.4).
Other algorithms for model-based clusteringThe EM algorithm does not guarantee global optimal solutions. Several stochastic algorithms have been proposed to reduce the risk of being trapped in local solutions. We describe two in this subsection and will examine their performances in our analysis. Both algorithms modify Equation (4) to calculate ^ Z m gk in step (ii) of Algorithm 1.(a) According to the deterministic annealing (DA) algorithm described in Rose (1998), the cluster in the m th iteration step is updated by(b) The classification expectation maximization (CEM) algorithm with simulated annealing (SA) proposed byupdates the estimate of Z gk byBoth algorithms use the annealing procedure with a sequence of preselected annealing rates ('temperatures', m ) decreasing to zero from a positive number. Apparently, when fixing m  1, both algorithm updates the values of ^ Z m gk the same way as the EM algorithm. Hence, Algorithm 1 can be viewed as a special case with a constant annealing rate m 1. As m ! 1, we always get ^ Z m gk  p k for DA algorithm and 1=K for SA algorithm, which means that genes are assigned to each cluster totally randomly. On the other hand, as m ! 0 the randomness is gradually lost and we finally get Z gk  0 or 1, i.e. a hard cluster solution. Hence, m determines the amount of randomness added in each step while searching for solutions. To apply these algorithms, we follow the suggestions of Rose (1998) and use m1  0:9 m with 1  2. For the SA algorithm proposed in, another difference from the EM algorithm (Algorithm 1) is that, before updating parameter values in the M-step, each gene is assigned to a cluster based on one random draw from a multinomial distribution with probabilities ^ Z m gk as calculated by Equation (6).
Model-Based Hybrid-Hierarchical Clustering AlgorithmSo far, we have assumed that the number of clusters, K, is predetermined. For a real data analysis, this quantity often needs to be estimated. There are different methods that can be applied to estimating K. For instance, choose the K that minimizes the Akaike information criterion (AIC) for the mixture model. Alternatively, instead of choosing a single value of K for the clustering analysis, we can build a hierarchical tree of clusters. The hierarchical structure of the clusters provides information about the relationships of clusters and allows flexibility of obtaining different number of clusters by cutting the tree at different levels. There can be tens of thousands of genes from RNA-seq data, and treating each gene as the smallest cluster at the bottom of the tree requires intensive computation. To speed up the calculation, we propose to use agglomerative (bottom-up) strategy starting with K 0 clusters, where K 0 is a number relatively large to allow enough resolution but far less than the number of genes, G. The initial K 0 clusters can be obtained by the model-based clustering algorithms described in the previous subsections. In each of the following steps, two clusters are merged if the 'distance' between them is the smallest among all possible pairs. Finally after K 0  1 steps, all genes belong to a single cluster and the hierarchical tree is built up. Such an algorithm has been called hybrid-hierarchical (HH) clustering algorithm (). Here, the term 'hybrid' is used to point out that the HH algorithm combines the starting steps that obtain K 0 clusters using non-hierarchical methods and the merging steps that are similar to ordinary hierarchical clustering. After the mth (0 m5K 0 ) merging step, we denote theand calculate the distance between two clusters, say G k and G l , by Equation:where k g and k maximize the likelihood f N g j g , k , and kl is the center of the cluster formed by merging G k and G l. This distance is the reduction of total log-likelihood from before to after the mergence. Obviously, merging clusters with the minimal distance defined in (7) aims to achieve the maximum log-likelihood in each step ().
SIMULATION STUDYWe conducted simulation studies to compare model-based clustering methods with other methods, including K-means and SOM, which have been popularly used in microarray data analysis and could also be applied to analyzing RNA-seq data. We first describe how data were generated in Section 4.1 and present the criteria used to evaluate the clustering performance in Section 4.2. Then we check the validity of treating the estimated dispersion parameter g as known for NB models in Section 4.3 and evaluate the model-based initialization algorithm (Algorithm 2) versus random initialization in Section 4.4. Finally, in Section 4.5, we compare our proposed algorithms with others.
Data simulationWe considered an experiment with three treatment groups and three replicates for each treatment group. This is a case easily encountered in real data analysis. Suppose that there were K  7 different expression patterns across three treatments and the cluster centers were characterized by k  k , where determined the magnitude of gene expression changes across treatments and k   k1 , k2 , k3  described the pattern of changes for cluster k, for k  1,    , K. A larger means larger distances between the centers and better separation of clusters. The distinct profiles characterized by  k1 , k2 , k3  are listed as follows:For the first cluster, the expression of genes increases from the first treatment group to the second one and increases further for the third treatment group. For the second cluster, the expression increases from first treatment group to the second one but then decreases for the third group. Note that the last cluster has a mean profile identically zero and this cluster corresponds to the group of genes that are non-differentially expressed across treatments. Although only identified differentially expressed genes are typically included in the cluster analysis, there could be false positives on the list of identified genes. For the simulation study, we included this cluster of non-differentially expressed genes to make our simulation more general and did not expect this to affect the relative ranking of the evaluated methods. RNA-seq data for G  10 000 genes were simulated for each dataset according to the following regime. For each g  1,    , G, Z 0 g  fZ 0 gk : k  1,    , 7g was drawn independently from a multinomial distribution with equal probabilities, where Z 0 gk  1 means gene g belongs to cluster k and Z 0 gk  0 otherwise. Given Z 0 gk  1, the gene expression profile was simulated according to g  k  g , where k  k as described earlier in the text and g   g1 , g2 , g3  added fluctuation around cluster center k specifically for gene g. We sampled gi for i  1, 2, 3 from N0, 0:2 2 , where controlled the level of fluctuation relative to the cluster center k. The overall mean expression level g was drawn from N4, 1, where controlled the magnitude of average expression level. The dispersion parameter g was simulated from Gamma0:75, 2, where Gamma0:75, 2 is a gamma distribution with mean 0:75=2 and variance 0:75=2 2. Changing the value of allowed different levels of dispersion. Specially,  0 corresponds to the Poisson model, which is the limiting case of NB model as the dispersion approaches zero. The normalization factor s gij was generated from N0, 1. Given these parameters, the gene expression count N gij was generated from the NB model with expectation exps gij  g  gi  and dispersion g. Once the dataset was simulated, we treated all parameters except s gij as unknown to resemble a real experiment. The values of , , and were varied to create different simulation settings, and 100 datasets were independently simulated for each setting. To test the robustness of our model, we also simulated data according to a generalized linear mixed model (GLMM)Here, we added a random effect gij to the expected expression, where gij is specific for each combination of gene and sample. gij was drawn from a normal distribution N0, 0:1 2 . With this GLMM model, we have overdispersed data compared with the NB model that we assume in (3). The results based on data simulated from both models [GLMM and the NB model with expectation exps gij  g  gi ] are similar, and our conclusions are the same. So we only present the results based on the NB model.
Assessment of performanceWe assessed the performances of different clustering approaches by comparing the resulting partitions with the original partition of genes defined by Z 0  fZ 0 g : g  1,    , 10000g. A better performance is indicated by more agreement between the two partitions. The following three statistics were used to evaluate the agreement. For all the three statistics, higher values indicate better performance.(1) Pairwise sensitivity: the proportion of pairs of genes (objects) that are clustered together among all pairs that had the same original assignment ().(2) Pairwise specificity: the proportion of pairs of genes (objects) that are clustered to different groups among all pairs that had different original assignment ().Normalized mutual information (NMI): MI is used in information theory to measure the amount of information one random variable contains about another, or equivalently, the reduction in the uncertainty of one due to the knowledge of the other. Here, MI is used to quantify the shared information between the true partition and the clustering result. See Strehl andfor the explicit formula for calculation using the contingency table formed by the two partitions. MI value is high if there is strong dependence (more shared information) between the two partitions, and is close to zero otherwise. Because there is no upper bound for MI, its normalized version ranging from 0 to 1 is often desirable for easier comparison ().
Validation of estimating dispersion parametersWe estimated the dispersion parameters g and treated them as if they were true values when applying the model-based clustering algorithms. However, it is challenging to obtain good estimates of dispersion parameters due to the small number of replicates inModel-based clustering for RNA-seq data RNA-seq data. To examine the impact of the estimated parameters on cluster analysis, we compared the model-based clustering methods using estimated values for g versus that using the input (true) values used to simulate the counts.plots the values of sensitivity, specificity and NMI for different clustering approaches over a range of values used to simulate RNA-seq data, whereas other parameters , and were fixed at 1. As shown in, when K  7 and at the same level of , the MB-EM algorithms using true and estimated dispersions perform indistinguishably as shown in. In practice, the true number of clusters is unknown, and we might apply a different number in cluster analysis, say K  10. Still, the clustering results from using true and estimated dispersions are almost the same. We also varied parameters , and one at a time while keeping others fixed at 1 to generate RNA-seq datasets. The difference between using true and estimated dispersions were small at most of the parameter settings (see Supplementary). Consequently, all results presented later were obtained using estimated dispersion parameters just like how we analyze real data. It is worth pointing out that we cannot conclude that the results for K  10 are better than that for K  7, though the specificity scores for the former are higher. Comparing the sensitivity or specificity scores is not meaningful when the numbers of clusters are different. For an extreme example, the sensitivity will always be 1 when K  1 because all gene pairs that had the same original assignment will be clustered together. Similarly, when choosing K as high as 10 000, the specificities will always be 1.
Comparison of initialization algorithmsIn, we compared the initialization effects on the MBEM clustering results. Our proposed model-based algorithm (Algorithm 2) and random initialization were examined. Though initialization using true cluster centers is not applicable in practice, we also included it in the comparison as a gold standard to evaluate the other two initialization methods.clearly illustrates that the model-based initialization performs much better than random initialization by giving higher evaluation statistics for all parameter settings in simulation. In many cases, the model-based approach generated results similar to those when the true cluster centers were applied for initialization. Results for other simulation settings are presented in Supplementary.
Comparison of MB cluster algorithms with othersWe proposed EM algorithm (Algorithm 1) to perform modelbased clustering. However, it is possible that the resulting partition from EM algorithm is not a global optimum. Hence, two stochastic versions, DA and SA algorithms, are described in section 3.3 to reduce such risk. In this section, we compare these slightly differing algorithms, whereas all three were initialized with the same set of cluster centers chosen by Algorithm 2. First, we did cluster analysis with the true number of clusters, K  7.and Supplementarysuggest that all three algorithms perform almost the same. We also analyzed the same datasets with K  10 (and Supplementary). Interestingly, Supplementaryshows that the SA algorithm typically achieves the highest sensitivity, whereas the DA algorithm gains in terms of specificity. If practitioners are more interested in sensitivity, getting groups of genes with similar profiles, then the SA algorithm is recommended. If separating genes with different profiles is more of interest, then DA algorithm can be applied. We also compared the proposed algorithms with K-means and SOM, two methods that have been popularly applied to microarray analysis and can potentially be applied for RNA-seq data. To cluster gene expression profiles, K-means and SOM were applied to cluster the MLEs obtained based on the NB model, i.e. the mean profile of normalized RNA-seq data across replicates for each gene. Plots inand d and Supplementary Figures S3 and S4 show that, evaluated by all three criteria, the model-based algorithms perform obviously better than K-means and even better than SOM. Note that our simulation settings include Poisson model, which is a special case when the dispersion parameter is set to be zero. We also did more simulations with Poisson model and the results are similar to what are shown here.
Choosing the number of clustersOne important question in the implementation of model-based cluster analysis for real data is to choose the number of clusters, K. Here, we evaluated the AIC. For given K, we can calculate the likelihood L by (3) and the AIC by 2log L  n p , where n p  GK  1  KI  1 is the number of parameters in the model. A low value of AIC indicates a better clustering result. As shown in, the AIC identified the true number of clusters being optimal.studied the maize leaf transcriptome using Illumina Genome Analyzer 2. The dataset quantifies transcript abundance of four sections along a leaf developmental gradient, with two biological replicates for each section. Using generalized linear model analysis based on NB distribution, we found that 12 631 genes were differentially expressed across the four sections.normalized the count data by calculating the values of reads per kilobase of exon model per million mapped reads (RPKM), a popular quantification method proposed by. In this section, on log-transform and mean-center the RPKM values for each gene, we obtained the log fold change estimates of the expressions relative to the average expression of each gene. To these log fold change estimates, we applied both the K-means, which has been used in, and the SOM algorithms. We also present results from the model-based clustering algorithms for the untransformed count data based on NB model. One advantage of the model-based approaches is that the Poisson or NB model can handle genes with low counts easily. When sequencing depth is low, there may be many genes with low counts or zero counts in some replicates or treatment groups. However, this will induce problems in the log-transformation, which is typically done before applying K-means method. The following numerical results also show that our proposed method provides better clusters than both K-means and SOM algorithms.As we expect that the genes within the same functional category have correlated expression patterns and thus more likely to be grouped together, a clustering result can be evaluated by checking its concordance with the functional categories. Gene annotations were obtained from Mapman as described in. Excluding categories that contain 55 or 4500 genes, we ended with 306 non-overlapping categories with a total of 5002 genes. Because these annotations are independent to the clustering processes, the evaluation is not biased toward any clustering method and data model. We first used K  100 to cluster genes using both our modelbased method and the K-means method. The reason that we chose K  100 is because we presume that more clusters can give better resolution of expression trends to the grouped genes with the 306 Mapman categories. We are interested in genes that show monotonic expression profiles along the leaf gradient, and we found that genes in clusters 14, 18 and 21, which are the three biggest clusters resulting from our model-based method, show a monotonic decreasing pattern from base to tip, which may help us to discover the biology that distinguishes base from other sections (Supplementary Table in excel file). We found that 23 genes in cell wall functional category according to Mapman annotation are grouped into cluster 21. However, these genes are scattered around different clusters obtained from the K-means method. The cell wall functional category totally includes 165 genes. We noticed that in model-based method, the cell wall related genes are enriched in cluster 14 (15 genes in cluster 14) and 18 (15 genes in cluster 18), in addition to cluster 21 (23 genes in cluster 21), which all represent the higher gene expression in base. However, these genes were scattered into 23 clusters obtained from K-means method, and there is no cluster identified by K-means that includes 410 genes from this gene category. Only by looking at these three clusters from model-based method, we can clearly conclude that there was an active cell wall metabolism at the basal part of developing leaf, which is not easy to detect using the K-means method. In addition, cell organization and DNA synthesis/chromatin structure pathways were also enriched in cluster 21 in model-based method, which suggested active cell construction and DNA replication in the leaf base, and this is consistent with the active cell wall metabolism in the basal part of leaf. All these biological events were easily identified by the model-based method, but not the K-means method. To obtain a more quantitative analysis, we measured the concordance between clustering results and gene functional categories by NMI. We performed cluster analysis with K  10, 15, 20,    , 100 clusters for all five methods, including SOM, K-means and the three model-based algorithms.shows that the model-based algorithms outperform SOM and K-means for all K values in terms of NMI. We then applied the HH clustering as described in Section 3.4, starting from K 0  100 clusters obtained using the corresponding distance measures. We also applied hierarchical clustering using average linkage based on Euclidean distance, Pearson correlation and the adjacency (similarity) function in weighted gene co-expression network analysis (WGCNA) proposed by Zhang and Horvath (2005). Our proposed HH method generated higher NMI scores () than the other three hierarchical methods. Examples of the clustering results for K  20 and hierarchical structures for the model-based hybrid-hierarchical clustering algorithm (MB-HH) clusters are plotted inand Supplementary Figures S5 and S6. These plots show that the EM algorithms result in much cleaner expression patterns than the clusters obtained from either K-means or SOM algorithm. We also used the AIC criterion based on NB models, similarly as in Section 4.6, to decide the number of clusters. We found K  15 is the optimal number of clusters by AIC ().
REAL DATA ANALYSIS
DISCUSSIONIn this article, we derived clustering algorithms based on finite mixture of Poisson or NB models. We proposed an EM algorithm with model-based initialization, and show this initialization method greatly improves the performance of the EM clustering. Compared with heuristic algorithms such as K-means method, our method has the following advantages: First, we build our approach of clustering RNA-seq data based on more appropriate probabilistic models such as Poisson and NB distributions. Owing to the nature of RNA-seq technology, the observed count data are discrete and skewed. Poisson model has been shown to fit well to data without biological replicates () and NB model to data with biological replicates (Anders and(b)
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Y.Si et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
, to be the maximum likelihood estimate (MLE) of g of the selected gene. (ii) Given m center(s), 1 1 ,    , 1 m for 1 m5K, selected from previous steps, calculate the measure of the distance, d gl , between each gene g and each previously
Model-based clustering for RNA-seq data at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Y.Si et al.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from Huber, 2010). Second, we demonstrated through both simulation studies and real data analysis that our proposed algorithms outperformed heuristic methods such as K-means and SOM, which have been popularly applied to cluster gene expressions from microarray and can also be applied to RNA-seq data. Third, we propose the MB-HH that allows flexibility in applying our method. Finally, our method provides a unified way to select the number of clusters. Using our models, we can evaluate the model selection criterion, AIC, and decide the number of clusters to use. Although our method is illustrated with analysis of data from completely randomized design, other more complex designs can be handled by appropriately modifying our model (1) and likelihood (3). Funding: This research was supported in part by the National Science Foundation (NSF) Grants (No. IOS-0701736 and IOS1127017). Conflict of Interest: none declared.
