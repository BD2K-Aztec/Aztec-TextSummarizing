MEGAHIT is a NGS de novo assembler for assembling large and complex metagenomics data in a time-and cost-efficient manner. It finished assembling a soil metagenomics dataset with 252 Gbps in 44.1 and 99.6 h on a single computing node with and without a graphics processing unit, respectively. MEGAHIT assembles the data as a whole, i.e. no pre-processing like partitioning and normalization was needed. When compared with previous methods on assembling the soil data, MEGAHIT generated a three-time larger assembly, with longer contig N50 and average contig length; furthermore, 55.8% of the reads were aligned to the assembly, giving a fourfold improvement. Availability and implementation: The source code of MEGAHIT is freely available at https://github. com/voutcn/megahit under GPLv3
IntroductionNext generation sequencing technologies have offered new opportunities to study metagenomics and understand various microbial communities such as human guts, rumen and soil. Due to the lack of reference genomes, de novo assembly of metagenomics data (short reads) is a beneficial and almost inevitable step for metagenomics analysis (). This step is, however, constrained by the heavy requirement of computational resources, especially for large and complex datasets encountered in environmental metagenomics (). The soil metagenomics dataset recently published by Howe et al. comprises 252 Gbp even after trimming low quality bases. The dataset was successfully assembled with preprocessing steps including partitioning and digital normalization. At present no de novo assembler can assemble the data as a whole using a feasible amount of computer memory. Estimated memory requirement for SOAPdenovo2 () and IDBA-UD () to assemble the soil data is at least 4 TB. As the volume of metagenomics data keeps growing, we are motivated to develop MEGAHIT, an assembler that can assemble large and complex metagenomics data in a time-and cost-efficient manner, especially on a single-node server (current maximum memory capacity 768 GB for a 2-socket server).
MethodsMEGAHIT makes use of succinct de Bruijn graphs (SdBG;), which are compressed representation of de Bruijn graphs. A SdBG encodes a graph with m edges in O(m) bits, and supports O(1) time traversal from a vertex to its neighbors. Our implementation has added a bit-vector of length m to mark the validityof each edge (so as to support dynamic removal of edges efficiently), and an auxiliary vector of 2kt bits (where k is the k-mer size and t is the number of zero-indegree vertices) to store the sequence of zeroindegree vertices to ensure the graph being lossless. Despite its advantages, constructing a SdBG efficiently is nontrivial. MEGAHIT is rooted in a fast parallel algorithm for SdBG construction; the bottleneck is sorting a set of (k1)-mers that are the edges of an SdBG in reverse lexicographical order of their length-k prefixes (k-mers). MEGAHIT exploits the parallelism of a graphics processing unit (GPU, CUDA-enabled) by adapting the recent BWT-construction algorithm CX1 (), which takes advantage of a GPU to sort the suffices of a set of reads very efficiently. Limited by the relatively small size of GPU's on-board memory, we adopt a block-wise strategy that partitions the k-mers according to their length-l prefix (l  8 in our implementation). The k-mers in consecutive partitions that fit within the GPU memory are sorted together. Leveraging the parallelism of GPU, MEGAHIT speeds up the construction by 35 times over its CPU-only counterpart. Notably, sequencing error is problematic, because a single base of sequencing error leads to k erroneous k-mer singletons, which increases the memory consumption of MEGAHIT significantly. To cope with the problem, before graph construction, all (k  1)-mers from the input reads are sorted and counted, and only (k  1)-mers that appear at least d (2 by default) times are kept as solid-kmer. This method removes many spurious edges, but may be risky for metagenomics assembly since many low-abundance species may have been sequenced at very low depth. Thus we introduce a mercykmer strategy to recover these low-depth edges. Given two solid (k  1)-mers x and y from the same read, where x has no outdegree and y has no indegree. If all (k  1)-mers between x and y in that read are not solid, they will be added to the de Bruijn graph as mercy-kmers. Mercy-kmers strengthen the contiguity of low-depth regions. Without this approach, many authentic low-depth edges would be incorrectly identified as tips and removed. Based on SdBG, we implemented a multiple k-mer size strategy in MEGAHIT (). The method iteratively builds multiple SdBGs from a small k to a large k. While a small k-mer size is favourable for filtering erroneous edges and filling gaps in lowcoverage regions, a large k-mer size is useful for resolving repeats. In each iteration, MEGAHIT cleans potentially erroneous edges by removing tips, merging bubbles and removing low local coverage edges. The last approach is especially useful for metagenomics, which suffers from non-uniform sequencing depths. The overall workflow of MEGAHIT is shown in.compares the performance of MEGAHIT with SPAdes () on three subsets (100-fold, 20-fold and 10-fold) of an E.coli MG1655 dataset. QUAST () was used to evaluate the assembled contigs (). MEGAHIT (CPU version) is six times faster than SPAdes, and performs well even on the low-coverage subset. To evaluate the performance on large scale metagenomics data, we assembled an Iowa prairie soil metagenomics dataset that comprises 3.3 billion reads totaling 252 billion base-pairs () using MEGAHIT and Minia, another memory-efficient assembler (). The assembly conducted by Howe et al. was included for comparison (). On a server with 384 GB memory, MEGAHIT took 44.1 h, $7 times faster than Minia. It reached peak memory consumption at 345 GB during k-mer counting and SdBG construction; this matches the expectation since MEGAHIT's sorting module automatically adjusts to fully utilize all available memory in a server. Notably, MEGAHITcan assemble this dataset with as little as 260 GB memory, using 55.3 h (Supplementary Section 4). To be consistent with Howe's analysis, we only considered contigs ! 300 bp for further analysis. The contigs produced by MEGAHIT had a total size at least three times larger than by other methods, and achieved better statistics on N50, average length, and the number of long contigs (length ! 1000 bp). Thus MEGAHIT gives better assembly contiguity. Raw reads were aligned back to the assembled contigs using Bowtie2 (). As shown in, MEGAHIT gets > 4 times more reads mapped and 56 times more read pairs properly aligned. 37% of distinct 17mers appeared ! 2 in the assembly, which might imply that MEGAHIT did a better job in recovering low-abundance subspecies in ultra-diversified metagenomics (Supplementary).
Results
ConclusionsMEGAHIT enables an efficient assembly of large and complex metagenomics data on a single server, while giving better completeness and contiguity. MEGAHIT is available in both CPU-only and GPU-accelerated versions. With GPU, the assembly time of the soil dataset is shortened from 4 days to less than 2 days.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
D.Li et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
