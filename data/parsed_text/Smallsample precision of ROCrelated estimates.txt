Motivation: The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics? Results: Through a simulation study using data models and analysis of real microarray data, we show that (i) for small samples the root mean square differences of the estimated and true metrics are considerable; (ii) even for large samples, there is only weak correlation between the true and estimated metrics; and (iii) generally, there is weak regression of the true metric on the estimated metric. For classification rules, we consider linear discriminant analysis, linear support vector machine (SVM) and radial basis function SVM. For error estimation, we consider resubstitution, three kinds of cross-validation and bootstrap. Using resampling, we show the unreliability of some published ROC results. Availability: Companion web site at http://compbio.tgen.org/ paper_supp/ROC/roc.html Contact:
INTRODUCTIONHigh-throughput technologies, such as those based on microarrays or 'Next-Generation' sequencing, make it possible to generate data on large numbers of genes, transcripts or proteins simultaneously in biological samples. Typical variables assessed include mutations, DNA copy number, DNA methylation, mRNA expression, microRNA expression, protein expression and post-translational modifications. A central goal of current biomedical research is to use those molecular profiles to identify biomarkers or multi-gene biosignatures for 'personalization' of medicinethat is, to use them for the full range of medical management choicesin disease risk assessment, sub-classification of disease, early diagnosis, prognosis, choice of optimal therapy, evaluation of response to therapy and/or identification of relapse. * To whom correspondence should be addressed.The profile data are used to develop univariate or multivariate predictors of biologically or medically interesting outcomes. Often, the aim is to develop a binary classifier, for example, diseased versus normal, disease subtype 1 versus disease subtype 2, response versus non-response to a drug, or 5-year survival versus death. A large literature has developed on such classifiers, but the recurring question is, 'How accurate are their predictions and classifications?' This question is supposed to be answered by the error rate; however, recent Monte Carlo simulations have shown large uncertainty in the error estimates. In the presence of high-dimensional feature spaces and small samples, a ubiquitous situation with high-throughput technologies, resampling error estimation methods, for example, cross-validation (CV), suffer from high-deviation variance, that is, the variance of the difference between the true and estimated errors is large (Braga)for an early criticism of. Moreover, there tends to be a lack of correlation and regression between the true and estimated errors, to the extent that the regression line of the true error on the estimated error is nearly horizontal (). These Monte Carlo studies have been supported by analytical studies in the case of the discrete histogram rule (Braga) and linear discriminant analysis (LDA;). For assessment of binary classifiers, in addition to the error rate, a favorite analytical tool is the receiver operator characteristic (ROC) representation ()for instance, with regard to gene-expression profiling in cancer, seeon the companion web site (http://compbio.tgen.org/paper_supp/ROC/ roc.html). An ROC curve is formulated by plotting the sensitivity and specificity of the classifier against each other as a function of some threshold criterion, for example, based on a biomarker or biosignature. The resulting ROC curve presents graphically the trade-off between false positives (FP) and false negatives (FN) in the classification process. The area under the ROC curve provides a scalar parameter that reflects the overall quality of the classifier. A natural question is whether parameters associated with ROC curves, such as the area under the curve (AUC), would suffer the same degree of uncertainty as discovered in the previous analyses of classifier error. Accordingly, we have established the computational machinery to address this question for both simulated and real datasets, and have performed a variety of analyses based on different predictive algorithms and methods of validation. We have analyzed the effect of sample size and the effect of an unbalance in the number of cases per class. That type of imbalance is common in biological datasets.
Small-sample precision of ROC-related estimatesAlthough ROC curves are insensitive to changes in class proportion, we show here that such imbalances have considerable impact on the estimation of both error rate and AUC. Through the simulations on both synthetic and real data, we identify how the training set size and class disproportion affect the performances of the different metrics. In particular, we show the unreliability of ROC performance metric estimations in small-sample settings.
SYSTEMS AND METHODS
ROC curvesConsider a two-class problem defined by the feature-label distribution F and a sample S ={(x 1 ,y 1 ),...,(x N ,y N )} of N examples drawn from F. An example is a pair (x,y), where x is a d-dimensional vector and y {0,1} is the class. A classification rule is used to design a discriminant S : R d  R from S. The output of S is a probability or a score that reflects the degree of uncertainty with which an example is assigned to a class. A binary classifier S,T is derived from S via a threshold T according to S,T (x) = 0 if S (x) > T and S,T (x) = 1 otherwise.Given an example x, there are four possibilities when comparing the class predicted by S,T (x) to its true class y: true positive (TP) y = 0 and S,T (x) = 0; FN y = 0 and S,T (x) = 1; FP y = 1 and S,T (x) = 0; true negative (TN) y = 1 and S,T (x) = 1. From these four possibilities, we can define three performance metrics; classifier error, ERR = (FP+FN)/N; true positive rate, TPR = TP/(TP+FN); and false positive rate, FPR = FP/(TN+FP). An ROC graph is a 2D graph in which the x-axis represents the FPR and y-axis represents the TPR. The point (0,1) represents perfect classification: no negatives classified as positives and all positives classified as positive. On the diagonal, the points (0,0) and (1,1) correspond to all examples being assigned to the negative class and to the positive class, respectively. The performance of a classifier S,T for a fixed threshold T is represented by a single point in ROC space. If the decision threshold T is allowed to vary, then the performance of the discriminant S is a variable depending on T and is represented by a curve in ROC space. A common metric to estimate the performance of a classifier independently of the decision threshold is the Area Under the Curve (AUC). AUC , AUC = 1 corresponds to the perfect classifier, for which the ROC curve goes directly from point (0,0) to (0,1) and then to (1,1), AUC = 0 corresponds to the classifier assigning all examples to the wrong class, and the ROC curve that follows the diagonal line has AUC = 0.5. A direct method to compute the AUC is to construct the ROC curve and then measure the AUC. If there are M test examples, then we obtain up to M +1 points in the ROC space with which to draw the curve. Accordingly, the AUC can be estimated by applying a rectangle or trapezoid area on each point. However, an alternative of AUC computation has been proposed in (), where it is shown that the AUC corresponds to the probability that an example from the positive class has a higher classifier output S (x) than an example from the negative class. In their procedure, the examples are sorted in increasing order according to the values S (x) and the AUC is computed by the following formula:where n 0 and n 1 are the numbers of examples of the positive and negative classes, respectively, in the test set, and S 0 is the sum of ranks of examples in the positive class.
Models for synthetic dataWe have performed a set offor the negative class, where the elements of A are evenly drawn from [0.5,1.5] and fixed throughout so as not to confound model variability with error estimation. An irrelevant feature follows the same normal distribution, N(0, 0 ), for both classes. Inside a class, all relevant features have a common variance. Two covariance matrix structures are considered: (i) is the identity matrix I in which the features are uncorrelated and the classconditional densities are spherical Gaussian. (ii) is a block-structured matrix in which the features are equally divided into blocks of size 4: features from different blocks are uncorrelated and every two features within the same block have a common correlation coefficient  = 0.8. In the linear models, the variances of covariance matrices of the two classes are equal,  1 =  0 = 1.8; in the non-linear models, the variances of covariance matrices are different, with  0 =  1 /1.5 = 1.4. With these model characteristics, we generate training and test sets with N and 10 000 examples, respectively, containing 20 relevant features and 180 irrelevant features. The large test set is used to compute the true metrics. The number of training examples (N) and class prior probabilities (p 0 for class 0, and p 1 = 1p 0 for Class 1) of the two classes are parameters of the dataset, with N varying from 50 to 1000, and p 0 from 0.2 to 0.8.
Classification rulesWe consider three classification rules: LDA, linear support vector machine (SVM) and radial basis function SVM (RBF-SVM). The output of an LDA classifier is readily transformed into the posterior probability of the positive class, so the usual decision threshold is 0.5. The output of an SVM is a score that represents the distance of the example from the separating hyperplane and the sign of the score defines the predicted class. The usual decision threshold of an SVM is 0. The RBF-SVM is in general a non-linear classifier, although linear SVM can be viewed as a special form of it.
IMPLEMENTATIONOur simulation study uses the following protocol:(i) A training set S tr and a test set S ts are generated. For the synthetic data, examples are sampled from the distribution determined by the model, N examples for S tr and 10 000 examples for S ts. For the microarray data, the examples are randomly separated into training and test sets with N = 50 examples for the training set and the remaining for the test set.(ii) To design a classifier based on a dataset S: first apply t-test to S and select 10 best features based on the t-test statistics; then build the classifier S from the reduced set. For classification, the decision threshold T is as defined in Section 2.3.(iii) For true performance:(a) Based on training data S tr , build the classifier Str .(b) Apply Str to test data S ts. For each example x, compute class prediction Str ,T (x) and classifier output Str (x).(c) Based on class predictions and true labels, compute true error rate, TPR and FPR.(d) Sort Str (x), then compute true AUC according to Equation (1).(iv) For estimated performance, consider the following estimators:(a) resubstitution:((2) Compute estimated error rate, TPR and FPR with class predictions, and estimated AUC with sorted classifier outputs.(b) k-fold CV:Page: 824 822830
B.Hanczar et al.(1) Randomly partition the training data into k folds S(i) tr , i = 1,...,k. For each fold S(i) tr , based on the remaining data in the training set S tr \S(i) tr , build the classifier Str \S (i) tr
. ApplyStr \S (i) tr
to fold S(i) tr to generate each example's class prediction and classifier output.(2) Collect the class predictions and classifier outputs from all folds. Compute estimated error rate, TPR, FPR and AUC.(3) For leave-one-out (LOO), set k to the training sample size; for 10-fold CV (10CV), set k = 10; for 10CV with 10 repetitions (10CV10), repeat Step (1) for 10 times before entering Step (2).(c) .632 bootstrap (BOOT):(1) Form a bootstrap sample S * of size N by drawing with replacement from S tr. Using S * , build the classifier S * . Apply S * to the examples that are in S tr but not in S * .(2) Repeat the above step 100 times, collect class predictions and classifier outputs from all repetitions, compute estimated error rate, TPR, FPR and AUC, and denote them as  0 , TPR 0 , FPR 0 and AUC 0 , respectively.(3) Obtain the resubstitution estimate of error rate, TPR, FPR and AUC, and denote them as  resub , TPR resub , FPR resub and AUC resub , respectively.Replace  in above equation with TPR, FPR and AUC to obtain the .632 bootstrap estimation of TPR, FPR and AUC, respectively.(v) Repeat the above procedure for 5000 times and collect all results.
RESULTS AND DISCUSSIONIn this article, we discuss representative results with the full set of results being given on the companion web site. For the synthetic data, we restrict ourselves here to the linear SVM and CV error estimation for the linear model with uncorrelated data, unless specifically indicated. For real data, again we demonstrate only the results of linear SVM with CV error estimation, unless specifically indicated.
Results for synthetic dataFigure 1 presents the deviation distributions (true minus estimated metric) for classifier error, AUC, TPR and FPR, with N = 100 and p 0 = 0.5,0.7 and for five error estimators. As expected, leave-oneout is practically unbiased but has the largest deviation variance. Generally, for this case, bootstrap and the CV estimators are nearly unbiased. Bootstrap has the smallest deviation variance except for resubstitution, which suffers from severe bias. However, one must be careful about generalizing from the bootstrap bias results, since .632 bootstrap is known to have substantial bias for certain models and classifiers.shows scatter plots comparing the true and estimated values of the four metrics for N = 50,100,200 (500 and 1000 are on the companion web site) and p 0 = 0.5,0.7. The estimated and true values are on the x-and y-axis, respectively, and the small triangles indicate the mean true and mean estimated values. The black line shows the linear regression for the true error on the estimated error. The lack of error regression and wide dispersion for small N is consistent with what we have previously reportedfor error estimation (). Of interest here, the dispersion is worse for the AUC than for the classifier error and that it is worse for the unbalanced prior (p 0 = 0.7) than the balanced prior (p 0 = 0.5). Note that the TPR variance is particularly bad for the unbalanced prior, a finding common throughout this study. There Page: 825 822830
Small-sample precision of ROC-related estimatesError rates area under curve true positive rate false positive rateis also little regression for the true AUC on the estimated AUC. On the companion web site, it can be seen that AUC regression generally does not improve for large sample sizes; however, the variance decreases greatly for N = 500,1000. Hence, estimation is good, even with a lack of regression.shows the root mean square (RMS) error and the correlation between the true and estimated metrics as functions of the class prior probabilities (on the x-axis) for N = 50,100,200,500,1000. RMS of the first row is defined bywhere  tru and  est are the true and estimated metrics. The second row in the figure shows the correlation between true and estimated metrics. The RMS is strongly negatively correlated with the training set size, RMS decreases as N increases. The prior probability also impacts the RMS. The classifier error decreases slightly and the AUC increases slightly with imbalance between the classes. Relative to the prior probability, RMS for the TPR increases substantially and RMS for the FPR decreases substantially for increasing prior probability. In all cases, sensitivity to the prior is substantial for small samples and decreases with increasing N. RMS can be decomposed into the bias and deviation variance asshows that the bias is small for the CV estimator being considered, so that both classifier error and AUC imprecision result from the deviation variance. The deviation variance can be further decomposed into the variances of the true and estimated metrics, along with the correlation, , between true and estimated metrics:According to, the correlation is typically not large and cannot offset the  2 est + 2 tru term in the deviation variance for the classifier error or the AUC. In sum, the AUC is poorly estimated for small samples, particularly so for N  100. The effect of imprecise estimation is observable in the ROC curves themselves.shows ROC-related curves for LDA classification and the non-linear uncorrelated model.The left-and right-hand columns show results for p 0 = 0.5 and p 0 = 0.7, respectively, and N = 50,100 and 200. The cross-and circle-marked curves correspond to using LOO error estimation and the true error, respectively. The solid lines are the mean ROC curves and the upper and lower dashed lines are the 95% confidence interval. The circle-marked dashed lines represent the variation associated with computing ROC curves from samples, that is, constructing the curves from sample-based TPRs and FPRs. The extra variance represented by the cross-marked dashed lines results from using the estimated TPR and estimated FPR instead of the true TPR and true FPR. For N  100, this extra variance is substantial.
Analytic representation of estimated AUC varianceWe have observed that the variance of the estimated AUC often exceeds the variance of the estimated error in the simulations. Although we cannot prove a general theorem to that effect, we can give an analytic proof for a special case. For simplicity, we consider the one-split trainingtesting scheme, but the conclusion can easily be extended to many other schemes. For the feature vector X and binary label variable Y , let the two class-conditional distributions be identical, the corresponding cumulative distribution functions (CDFs) be continuous over R, and the prior class probabilities be given by P{Y = 0}=p and P{Y = 1}=1p. The Bayes error is min(p,1p). For AUC estimation, the classification rule adopted will yield a discriminant S : R from the training data. Let the CDFs of S (X|Y = 0) and S (X|Y = 1) be continuous over. Assume there are altogether n testing sample points x 1 ,...,x n , with n 0 = pn points from class 0 and n 1 = (1p)n points from Class 1. The AUC is computed according to Equation (1). Owing to the continuity assumption,. Hence, the ranking order is unique with probability 1. Since the class-conditional distributions are identical, S (X|Y = 0) and S (X|Y = 1). Thus, the rank-sum S 0 follows the same distribution of the null-hypothesis in the MannWhitney test (), whose variance has been shown to be n 0 n 1 (n 0 +n 1 +1)/12. Hence, the variance of the estimated AUC iswhere we use p(1p)  1/4. It is well known that the variance of the estimated error in this testing scenario is bounded according to  2 est-ERR  1 4n (), which is smaller than 1 3n. The same argument applies to k-fold CV and many other resampling-based error estimation schemes, as long as the data partitioning is stratified so that the testing sample points are represented in the same proportion as the training data for every fold/resampling. For non-trivial distributions, where S (X|Y = 0) = S (X|Y = 1), the distribution of the rank-sum is generally unknown. Moreover, the variance of the true AUC and true error rate, which are functions of sample size and classification rule, are rarely known. Hence, we depend on simulation.
Results for microarray dataWe present two sets of experiments based on real microarray datasets. In the first experiment, we apply a hold-out-based Page: 827 822830
Small-sample precision of ROC-related estimatesprior = 0.5 prior = 0., LDA classifier, LOO. The x-axis is the FPR and the y-axis is the TPR. The curves with cross marks and curves with circle mark correspond to using LOO error estimation and the true error, respectively. The solid lines are the mean ROC curves and the upper and lower dashed lines are the 95% confidence interval. scheme on two existing cancer datasets to compare the true and estimated metrics and confirm the conclusions drawn from artificial data simulations. In the second experiment, we reproduce the experiments in two publications to verify the imprecise estimation observable in ROC plots. In the first set of experiments, we use microarray data from two published sources: breast cancer (van de) and lung cancer () studies. The breast cancer dataset includes 295 patients, 115 belonging to the good-prognosis class and 180 belonging to the poor-prognosis class, with prior probabilities 0.39 and 0.61, respectively. The lung cancer dataset contains 203 tumor samples, 139 being adenocarcinoma and 64 being of some other type of tumor, the prior probabilities being 0.68 and 0.32, respectively. We have reduced the two datasets to a selection of the 2000 genes with highest variance. For each iteration of our procedure, the datasets are divided into a training set and a test set. The training set is formed by 50 examples drawn without replacement from the dataset. The examples not drawn are used as the test set. Note that the training sets are not fully independent. Since they are all drawn from the same dataset, there is an overlap between the training sets; however, for a training set size of 50 out of a pool of 295 or 203, the amount of overlap between the training sets is small. The average size of the overlap is about 8 examples for the breast cancer dataset and 12 examples for the lung cancer dataset. The dependence among the samples is, therefore, not expected to have a large impact on the results (see Braga-Neto and Dougherty, 2004, for a discussion of this issue). We apply on these data the same protocol used for artificial data and detailed in Section 3. The results allow us to compare the true values of the metrics to the estimated metrics.shows the scatter plots for the breast and lung cancer microarray dataset with the linear SVM. As for the synthetic data, there is very little regression, wide dispersion, and the AUC dispersion is comparable or even greater than that for the classifier error. The RMS of the lung cancer dataset is smaller and there is less variance because the classification is easier (e.g. the error is low and the AUC is high).gives the RMS values and the correlation coefficients, where the latter are very small. In the second set of experiments, we demonstrate the large variance observable in ROC plots in real data cases. We use data in two published studies and repeat the same classification scheme multiple times with the only variance being the randomness in data partitioning. The first dataset is from a study on the prediction of Parkinson's disease based on gene expression from blood samples (). The authors have identified a set of biomarkers, constructed a classifier and validated their results with an ROC analysis of the classifier. The dataset contains 105 subjects, 50 at early stages of Parkinson's disease, 22 healthy and 33 having another brain disease. The classification task is to detect only the Parkinson's disease, so the prior of the problem is 0.52. The authors use an algorithm that selects the best genes based on the Pearson's correlation between their expression level and the class label. Then a template of each class is formed from the mean expression of the selected genes. The classification outcome is determined by the risk score, which is defined as its correlation with the Parkinson's disease template minus its correlation with the non-Parkinson disease template. To evaluate the performance of this classifier, the original dataset is randomly divided into a training set (66 examples) and a test set (39 examples). The training set is used for gene selection and classifier construction, and the test set for evaluation. The authors support the validity of the identified biomakers using the classifier ROC curves. These ROC curves are computed using both the test set and an LOO procedure. In our experiment, we apply the same scheme of gene selection, classification and evaluation as in the original study; however, we run this procedure 100 times to estimate the variance of the results.shows the average curve for the 100 ROC curves computed by LOO (solid line with cross marks) and the test set (solid line with circle marks). The dashed lines represent the 95% confidence intervals. These represent a kind of internal confidence bounds relative to the sample. We see that the results of our experiments are more pessimistic than the ones in the original paper. The AUC for LOO is 0.561 with confidence interval. The AUC for the test set is 0.536 with confidence interval. Our ROC curves are much more closer to the axis y = x. The confidence intervals are wide and include the axis y = x. In these conditions it is not possible to validate the genes, identified by the methodology, asPage: 828 822830
B.Hanczar et al.
Breast cancer datasetLung cancer dataset Error AUC TPR FPR. The scatter plots of performance measures for breast and lung cancer dataset: linear SVM, 10CV. The x-axis is the estimated performance, whereas the y-axis is the true performance.predictors of Parkinson's disease. The situation is even worse than what is depicted here because the confidence intervals are internal to the sample. This only accounts for resampling variation, not the variance across samples, which would have to be included to obtain the full variance (Braga). The second dataset is from a study of the loss of phosphatase and tensin homolog (PTEN) associated with the presence of solid tumor. The authors develop and validate a microarray gene expression signature for immunohistochemistry (IHC) detectable PTEN loss in breast cancer (). The data contain 105 examples, 70 being IHC negative and 35 being IHC positive. The genes set is reduced to genes containing <20% of missing values, thereby resulting in 16 039 genes. The best discriminant genes are selected by a MannWhitney test and the classifier is constructed from a linear SVM. The performance of the classifier is estimated by 3-fold 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 FPR TPR. ROC curves for the Parkinson's disease dataset. Lines with cross marks are the ROC curves by LOO and lines with circle marks are by the test set. The solid lines represent the average ROC, whereas the dashed lines the 95% confidence intervals. CV with 10 repetitions. The AUC of the final classifier is estimated to be 0.758. We use the same procedure as described in the original study, except the feature size in classifier design, which was not clearly described in the original publication. In our experiment, the final feature size is fixed as 100. We compute the ROC curve and AUC of the obtained classifier. We also estimate the internal variance of the ROC curve and AUC via repetitions of the procedure.axis y = x. This means that, even if the ROC curve is above the axis y = x, we cannot reject the hypothesis that the classifier is meaningless. The AUC of the classifier is estimated to 0.642 and its 95% confidence interval is. Note that the AUC of the original published classifier (AUC = 0.758) is included in our confidence interval, as is a random guess (AUC = 0.5). As with the Parkinson's data, the situation is worse because the confidence internal only reflects internal variance from resampling within the sample, not the full variance. These experiments demonstrate that there are insufficient examples in the microarray datasets to draw ROC-based conclusions with acceptable precision.
Concluding remarksThis article and several preceding it have shown that even for synthetic data of a simple type, two Gaussian distributions, it is difficult to find good feature sets () and difficult to identify the error rate of a classifier composed of the features that one does find with a small sample. The essential reason is that one cannot make sufficiently good estimates of predictive error at any of the steps required to select features or to characterize the error of the classifier finally developed (Braga). Here, we have demonstrated that small sample size leads to large inaccuracies in the estimated validation parameters associated with ROC analysis, even from well-behaved distributions. A previous study () applied permutation P tests to the AUC and obtained good P-values. There is no contradiction here because permutation P tests, when applied to classification, are essentially unrelated to classifier performance. Specifically, the P-values have virtually no regression with the error estimates (). A procedure for error estimation cannot provide more information than that exists in the distribution of samples with which it is presented: if that distribution is a poor estimate of the actual distribution, then the error estimate will be poor as well. In the case of actual biological samples, it can be seen that the differences between true and estimated error are even largerconsiderably largerthan for a similarly small sample of well-distributed synthetic data. ROC curves must be used with extreme caution unless one has a very large sample. In other cases, it would be nice to have some simple rule of thumb to determine if a sample is sufficiently large for the problem at hand; however, since in practice there is only a single sample available, no simple solution is possible. Nonetheless, an experimenter can take some precautions. First, one could use the kind of model-based analysis done in the present article. This would not reflect the actual population but it would allow the kind of confidence analysis demonstrated in. This could be performed using a model developed for the specific technology being used or, lacking the availability of such a model, a Gaussian model like the one employed herein. It can be expected that the true biological population is less well behaved than the model so that the resulting confidence bounds could be taken as a performance floor for determining a sufficient sample size. A second approach would be to use the internal variance of the AUC if a resampling procedure has been employed, as in the PTEN example (see Appendix A for a description of the internal variance). Again, this would provide a floor because it only provides an estimate of the internal variance, not the full variance of the AUC. Neither method is perfect, but certainly if the 95% confidence interval contains the line y = x in either case, then the sample size is insufficient. If forced to choose between the two approaches, we would choose the model-based approach because often the internal variance is much less than the full variance, so the resampling approach may be more optimistic. If these approaches are used, prudence would dictate utilizing both and making no conclusion unless the lower 95% confidence bound for the AUC exceeds 0.5 for both. Funding: National Science Foundation (CCF-0634794, partially).
Conflict of Interest: none declared.Page: 830 822830
B.Hanczar et al.van de Vijver,M.JA gene-expression signature as a predictor of survival in breast cancer. N. Engl. J.On the sampling distribution of resubstitution and leave-oneout error estimators for linear classifiers. Pattern Recogn., 42, 27052723.
APPENDIX AThe internal variance of a randomized error estimator, such as k-fold CV, is the variance of the estimator given the sample, namely, the variance due only to its internal random factors (Braga-Neto and). It is expressed as Var int = Var( |S), wherwher is a randomized error estimator and S is the sample. This variance is zero for non-randomized error estimators. The full variance, Var( ), of the error estimator is the one we are really concerned about, since it takes into account the uncertainty introduced by randomly sampling the data from the population. Using the well-known conditional-variance formula,we can break down Var( ) in the following way:The second term on the right-hand side is the one that includes the variability due to random sampling.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
