Motivation: Multidimensional scaling (MDS) is a well-known multivariate statistical analysis method used for dimensionality reduction and visualization of similarities and dissimilarities in multidimensional data. The advantage of MDS with respect to singular value decomposition (SVD) based methods such as principal component analysis is its superior fidelity in representing the distance between different instances specially for high-dimensional geometric objects. Here, we investigate the importance of the choice of initial conditions for MDS, and show that SVD is the best choice to initiate MDS. Furthermore, we demonstrate that the use of the first principal components of SVD to initiate the MDS algorithm is more efficient than an iteration through all the principal components. Adding stochasticity to the molecular dynamics simulations typically used for MDS of large datasets, contrary to previous suggestions, likewise does not increase accuracy. Finally, we introduce a k nearest neighbor method to analyze the local structure of the geometric objects and use it to control the quality of the dimensionality reduction. Results: We demonstrate here the, to our knowledge, most efficient and accurate initialization strategy for MDS algorithms, reducing considerably computational load. SVD-based initialization renders MDS methodology much more useful in the analysis of high-dimensional data such as functional genomics datasets.
INTRODUCTIONThe appropriate and faithful visualization of high-dimensional data is often a prerequisite for their analysis as the human visual cortex is still one of the most powerful tools to detect and conceptualize structure in data (). Furthermore, communication of numerical and statistical results is greatly aided by the intuition arising from appropriate representations of data. Different methods * To whom correspondence should be addressed.for the required dimensionality reduction have been developed (). An entire family of approaches, such as principal component analysis (PCA) finds the minimal orthonormal basis using a mathematical tool called singular value decomposition (SVD). These methods, using different similarity or dissimilarity measures such as covariance or correlation, order the ensemble of components by their statistical deviation, and for visualization only the first two or three components are retained. Thereby, the statistical information in the first components are entirely retained, whereas one of the subsequent components is entirely lost. Today's highdimensional biological datasets can easily contain thousands of instances (number of measures) with 10 5 10 9 variables (number of parameters measured). The repartition of information is usually homogeneous over the entire number of variables. In consequence, considering only the first components given by SVD-based techniques is not necessarily the best choice. Multidimensional scaling (MDS) is a methodology that reduces dimensionality using only the information of similarities or dissimilarities between instances, hereafter regrouped in the general term of 'distance'. The search for an optimal configuration, is reduced to finding the global minimum of a function evaluating the loss of distance information. To be sure to find an acceptable minima (i) an initial state for the optimization algorithm, and (ii) an optimization algorithm and the appropriate parameters have to be appropriately chosen. Recently,has shown that the best choice for the second is a molecular dynamics multidimensional scaling approach. We demonstrate here that the choice of the initial position is paramount to the quality of the representation and its computational efficiency. By using SVD for providing an initial configuration for MDS, we obtain a significantly increased computational efficacy. Interestingly, we also demonstrate that performing an iterative MDS or adding stochastic energy during the molecular dynamics, MDS execution do not increase performance or reproducibility of the algorithm. We also investigate the local structure of the geometric objects after dimensionality reduction with our different methodologies, and then evaluate the accuracy of the different approaches developed here on biological data. These investigations and the use of SVD to the initial state allow to better define and
C.Bcavin et al.control the dimensionality reduction process for high-dimensional data.
METHODS
SVDGiven a data matrix X with n rows and p columns and x ij its value in row i and column j, we denote  X i the p components vector corresponding to row i of the matrix, and  X j the n components vector corresponding to column j of the matrix. A set of vector  X i is then a set of instances, whereas a set of vector  X j is a set of variables. In all the following, we will use this notation for vectors extracted from X. It is known () that every rectangular matrix can be decomposed using its singular values:where U (left singular vectors) and V (right singular vectors) are both square orthogonal matrices, and S is a rectangular matrix containing the singular values (s i ) which are positive (S ii = s i and S ij = 0). U,S and V are reorganized in order to have s 1 > s 2 >  > s r , with r being the rank of S. Generally, before performing SVD X is centered, so the mean of each column is equal to zero. In this context, rank(X) = rank(S)  min(n1,p) if X is n.p. Singular value decomposition provides three major types of information:(i) A new data matrix X new , which represent the data points in a new orthonormal basis with a minimum number of components, and where distance between the instances is preserved.indicate the SD and relative contribution of the cloud of points on each principal component. (iii) The matrix V carrying the individual contributions to each principal component. These different types of information have already previously been used in the literature to infer biological knowledge in various settings);;. The simplest way to find SVD, is to search first for the eigenvalues and the eigenvectors of the inner and outer products. As finding the eigenvalues of a matrix X with n rows and p columns, is hard to perform for objects with a high number of variables, this step is only feasible if either n or p are small (typically inferior to 1000, which is usually the case in biological datasets). If both n and p are large, one is obliged to use iterative SVD techniques as shown in. One advantage of using SVD is its close link to classical techniques of dimensionality reduction such as PCA, classical scaling (cMDS), principal component correlation analysis (PCCA) and correspondence analysis. The different results of these techniques can be obtained using SVD and a proper normalization of the data, as shown below. SVD allows to demonstrate that the inner-product (XX t ) and outer-product (X t X) of a data matrix X have the same eigenvaluesNote also that missing values in data can be imputed using SVD [; Candes and Recht (2008);. If the number of missing values is relatively low, the Eckart Young theorem (), which is the most commonly used theorem for matrix approximation, assures that the result of the SVD will change only in the value of the last singular values. Hence, for a rapid imputation, the row average method () can be used which is generally sufficiently precise in most cases. Also, PCA is a very good choice for the initial state for K-means clustering (). In the new representation given by SVD, cluster structure of the data will then naturally appear, and thus provide a natural interpretation of clusters.
SVD and classical techniques of dimensionality reductionPCA relies on the search of the eigenvectors' covariance matrix. Hence, performing PCA reduces to finding the outer-product's eigenvectors. The singular values of X are the square root of the outer-product's eigenvalues. The link between PCA and SVD then becomes obvious (). Classical scaling (cMDS for classical Multidimensional Scaling) was invented to embed a set of instances in the simplest space possible, with the constraint of preserving the Euclidean distance between data points. Euclidean distance can be written as a sum of inner-products  X i.  X j , one can pass from an Euclidean distance matrix to an inner product matrix by a simple matrix manipulation called double centering (). Consequently, classical scaling consists in finding eigenvalue factorization of the inner-product matrix, so it can be performed using SVD. The link given by SVD between inner and outer product matrices implies that PCA and classical scaling give the same results, a fact reflected by classical scaling sometimes being referred to principal coordinate analysis. Principal component correlation analysis (PCCA) uses correlation between variables to find a minimal orthonormal basis. After a proper normalization of the data with their SD:, PCCA is performed by eigenvalue factorization of the outer-product matrix. Hence, after normalization of the data PCCA results are given by SVD. Correspondence analysis is used in the dimensionality reduction of contingency tables obtained after an operation of counting on categorical data (). This method can be used for microarray data analyses () as each value of gene expression is, in fact, a count of the number of RNAs produced. Generally speaking, this technique is used to compare two vectors in terms of their distribution profiles using the chi-square distance. When the distance is equal to zero, both vectors have the same statistical distribution. It can be shown () that  2 distance can be reduced to an Euclidean distance after normalization of the datThus, to find the minimal space which embeds the data and conserves the information of  2 distance one performs a cMDS or PCA on the rescaled data matrix using SVD results.
MDSMDS is a class of techniques to represent instances in an r dimensional space given an initial state and a similarity or dissimilarity matrix (;). Recently, molecular dynamics (MD) approaches have been used to perform MDS for high-dimensional objects drastically increasing quality of the dimensionality reduction (). We have also developed a similar approach based on a spring analogy. Data points are connected to all other instances with virtual springs. The springs will tend to return to their equilibrium length during molecular dynamics simulation. The equilibrium length for the spring between point i and point j will be defined as the Euclidean distance d(in the initial state. For each instance  X i , a force is defined F(  X i ), which is the sum of all spring interactions F sprwith the other instances  X j , minus a friction term to avoid oscillation of the spring network:being the distance between instances in the r dimensional space, k ij the strength of spring ij,  the friction parameter and m i the mass given to each point. We consider that every spring and all instances are equal in strength and weight so k ij and m i are the same for every i and j (k ij = k and m i = m). It is, however, possible to use different parametersfor instance, according to experimental precisionif different weights shall be considered for the different instances. A molecular simulation using the force vector is then executed. Following Newton's law it follows:and velocity of and instance at the next time step, a Verlet integration is used:with   X i (t) the temporal derivation of vector  X i (t). The algorithm is run with simulation time t increasing. To avoid divergence of the Verlet algorithm parameters of the simulation k, m,  t have to be well chosen. Here we used: k = 1, m = 5,  = 0.1 t = 0.02 (cf.). For the initial state, the data provided to the MDS algorithm were rescaled to fit in a hypercube with a diameter of 6 by multiplying the initial state matrix by a scalar . To control the minimization process at each time step, a cost function termed the Kruskal stress is calculated according to Cox and Cox (2000):this global parameter is a direct evaluation of the amount of energy in the system and hence the loss of distance information.
Datasets used in this studyTo test and illustrate the algorithm discussed here, we have used several publicly available datasets of different origin. We have used two different transcriptome datasets. Briefly, the cellular transcriptome is defined as the ensemble of RNA molecules resulting from gene expression in a cell. Using microarray technology, in the human case, some 30 000 different RNA species can be quantified simultaneously. The dataset here referred to 'd196Cell' includes 96 transcriptome measurements generated from 32 individual human tissues under non-pathological conditions. This dataset was initially published by, and is available for download from: http://mace.ihes.fr using accession number: 2914508814. The dataset here called 'd6CCYier'); mace access. no.: 2960354318] is composed of 12 human fibroblast transcriptome data points generated over 24 h during the cell cycle. Note that we eliminated 1 (Interleukin 8, IL8) of the 517 genes as an outlier from this dataset. The dataset 'd296Cell_T' (cf.) is a derivative of the initial dataset d196Cell', where only genes were retained that are specific to one and only one human tissue as provided in (), and removing again one outlier gene (Probe_ID: 162105). The dataset 'd896Cell_T' (cf.) is the transposed (Instances, Variables) dataset 'd296Cell_T'. All transcriptome datasets were median normalized in log2space and processed according to standard procedures (). Seven additional datasets with no relation to biology were used. Both originate from the Machine Learning Repository (): http://archive.ics.uci.edu/ml (i) 'Iris' here 'd3Iris',
RESULTS
Comparison of different initialization methods for MDSWe postulated that the inconveniences associated with the combined molecular dynamics MDS techniques (hereafter simply: MDS) related to the dependence on the choice of the initial condition for the simulation leading to insufficient control and being trapped in local minima on the one hand, as well as the large information loss when SVD techniques are used for dimensionality reduction on the other hand, can be overcome when both methods are combined. We therefore created an SVDMDS algorithm which uses SVD to compute the initial state of a molecular dynamics simulated MDS. This SVDMDS approach was then compared to SVD and MDS on 13 different datasets ().well illustrates the shortcomings of SVD and MDS alone and how SVDMDS overcomes those. The dataset 'd196Cell' containing 96 different instances was used to compute a 2D representation using SVD (), our combined SVDMDS approach () and two examples of MDS initialized by random positions defining a 12 unit hypercube (and D). According to the Kruskal stress e, MDS techniques (D) better preserve the distances between the instances and their relationship. The data cloud is better resolved (see also blow ups) and the global distance information loss is lower than for SVD. In order to demonstrate generality of our approach, we next analyzed the 12 remaining datasets () using four different approaches: (i) SVD only, (ii) SVDMDS, (iii) MDS initialized with all data points placed at zero with minimal random noise (zeroMDS), and (iv) MDS initialized with random positions (stochastMDS). The results are reported in. In all cases, we reduced the dimensions to two. It becomes again apparent from the Kruskal stress that the MDS-based techniques systematically outperform the SVD. While stochastMDS, zeroMDS and SVDMDS give similar results in terms of the final information loss, the number of timesteps needed to identify a minimum stress is greatly reduced using SVDMDS (and for four examples). Therefore, SVDMDS approaches the final state (here defined as a Kruskal stress value) faster than either of the MDS methods. We show an example of stress evolution inwhere stochastMDS and zeroMDS are slow due to the existence of local minima, and SVDMDS clearly outperform them.
Iterative dimensionality reduction using iSVDMDSWe next wondered whether the dimensionality reduction could be further improved by a step-wise reduction of one dimension after another. To this end, we compared again the performance of the threePage: 1416 14131421
C.Bcavin et al.(a)and Section 2.4) was represented in 2D space using: (a) SVD based on covariance, (b) SVD-initialized multidimensional scaling; (c) random initialized multidimensional scaling, and (d) as in (c) using the same algorithm and leading to a different random position matrix. The peripheral data points were color coded and labeled according to the human tissue analyzed. For (a) and (b) the central cloud of points has been zoomed into at the same scaling factor. The resulting Kruskal Stress e for each of the dimensionality reductions is indicated. Similar computations were used to generate, SVDMDS rapidly approaches a minimal Kruskal stress configuration over the simulation time. The previously described MDS procedure which uses stochastic initiation for the molecular dynamics simulation requires much more simulation time to find the same minimal stress configuration as the SVDMDS algorithm. Finally, the iterative iSVDMDS approach will also converge to the identical minimum obtained by the other methods; however, as for each component a separate simulation is performed the convergence time is greatly increased. Albeit many different simulations on the different datasets, we have never obtained a final configuration using iSVDMDS where the Kruskal stress would allow to conclude on an improved performance when compared to SVDMDS. Therefore, the iterative method does not allow for improved accuracy, but rather prolongs simulation time with no immediate gain (summarizes the results). We next compared iSVD and iSVDMDS methods to determine how the loss of information is distributed during iterative dimensionality reduction. As can be seen infor both procedures, the amount of stress or lost information increases both relatively and absolutely with the number of components removed. Note also, that the iSVDMDS method better preserves at every consecutive iteration the distance information of the object ().
Molecular dynamics dimensionality reduction with added stochasticityIn Andrecut (2009), an approach reminiscent of simulated annealing was used to avoid getting trapped in local minima during the molecular dynamics simulation. This combination of methods is equivalent to adding a stochastic force to all data points F stochastic (  X i ) =T * s(t) where s(t) is a random number given by a generalized Gaussian stochastic distribution, and T is the temperature of the system. By decreasing T exponentially during the simulation, one expects to reach the global minimum. Adding stochasticity to the molecular dynamics-driven MDS is, after, required to insure reproducibility of the algorithmic performance.. Results from the different MDS algorithms applied to the various datasets (c.f.To compare MDMDS with our SVDMDS algorithm, we have implemented different MDMDS algorithms with stochastic energy. We used two types of temperature decrease, the first linear, beginning with a temperature of 100 J and decreasing linearly to 0 J during 3000 steps of simulation; we call this method MDMDS linear. The second includes an exponential decrease from 100 J to below 0.1 J during 3000 steps of simulation; we call this method MD MDS exponential. The function s(t) uses random numbers generated uniformly between 0.5 and 0.5. As seen in, SVDMDS as well as the two MDMDS algorithms 'linear'and 'exponential'always identify final configurations with the same amount of residual energy. It can also be seen that SVDMDS converges faster for these four examples than the MDMDS methods. In conclusion, the two MDMDS Page: 1418 14131421algorithms do not improve MDS, on the contrary they converge slower. We next asked whether or not similarly adding stochasticity to the SVDMDS algorithm would improve its performance.and D illustrates that indeed adding different amounts of energy at different times of the simulation (arrows) does not lead to lower energy minima. The SVDMDS algorithm, similarly as the MDMDS algorithms (), always converges to the same energy state. This has also been confirmed using other datasets (data not shown). Taken together, the results using MDMDS-lin and MDMDS-exp and SVDMDS strongly suggest that only a single ground state is present. While we do not have any formal proof, we believe that the detailed analysis of the geometric structure of the data objects presented below also strongly argues in favor of a global energy minimum.
Page: 1417 14131421
Multidimensional scaling
C.Bcavin et al.
Geometric structureKruskal stress directly evaluates the distance information deformation.), that it rather evaluates global deformation of the cloud of instances. To gain information on local distances deformation, we define a new parameter, Entourage. For any one instance  X i in the reference distribution obtained through SVD (undistorted representation), we consider its k nearest neighbors: N ref i. In the new distribution obtained after dimensionality reduction, we also compute the k nearest neighbors for the same instance  X i , and obtain a list:), which will be the number of instances common to both. This operation is repeated for all instances i, and one obtains the Entourage parameter:with G = nk a normalization parameter (Ent  (0,1)).= 0.01, a difference of 1% between two values of Entourage corresponds to an average deformation of 1% in the local organization. This parameter has more signification for a small number of neighbors k compared to the total number of points n. The geometric properties of the data objects are analyzed using the Entourage parameter. We have plotted the relationship of Entourage and k for six different methodologies: zeroMDS, stochastMDS, SVDMDS, iSVDMDS, MDMDS-lin, MDMDS-exp infor eight different datasets. From the selected examples, it becomes clear that again the SVDMDS method outperforms the different types of MDS over a wide array of structures analyzed as the Entourage value is consistently higher no matter how many different k nearest neighbors are considered. The iterative iSVDMDS method, due to the accumulation of small residual errors during the molecular dynamics simulation, and the MDS method give similar results. At the cost of increasing computational load, the iSVD MDS better and better approximates the SVDMDS method. In conclusion, the SVDMDS method, under all conditions tested, better represents the geometric structure of the datasets in lowdimensional space when compared to the input object with rank(S) components (given by SVD). Note that this holds even for objects with equal stress.illustrates the problem of rotational variance when using stochastically initiated molecular dynamics simulations for MDS. When comparingand D as well as comparing them toand B that stochastMDS results produces near-optimal solutions (with respect to the Kruskal stress), the resulting orientation of the instances, however, is different (focus, for instance, on the relationship between 'skeletal muscle' and 'fetal liver'). SVDMDS on the contrary only produces a single result. This observation, taken together with the results on the relevance of stochasticity in the simulation obtained above, argues for the existence of different equivalent energy minima that only differ in the rotational orientation of the object and (at best) only minimally in the Kruskal-stress; a fact predicted by mathematical consideration. Hence, SVDMDS not only reduces significantly the computational load, but also insures uniqueness of the resulting representation. The quality of this final and unique representation can be demonstrated using the Entourage parameter. The increase in fidelity in the representation of data should not be underestimated (see also).This is reminiscent to techniques of principal manifold searches () where parameters describing topology, local organization or other geometric characteristics are used. A major advantage of using SVD to define the initial state is that it provides the inertia of each principal component. The comparison of the different internal structures of the studied datasets showed a vast variety of profiles. A good dimensionality reduction technique would ideally account for these differences. Taking into account the inertia, the stress and the Entourage during the MDS process will help to have an even more accurate representation of the data matrix in low-dimensional space.
Page: 1419 14131421
Multidimensional scaling
Data analysisIn order to demonstrate the applicability of the SVDMDS methodology and its superior performance, we reanalyzed a previously published biological dataset not yet used here (). The datasets consists of quantitative measures for 10 selected cytokines in a cohort of human malaria patients from central India displaying different severeness of disease as well as endemic and non-endemic control subjects. A total of 98 patients were included in the original study by. The main objective is to determine whether individual or combinations of cytokine measurements can be used to determine whether an individual is affected by cerebral malaria (CM), the most severe form of the disease, and how to distinguish CM from severe malaria (SM). Both forms of the disease require early detection and prognosis which are pressing matters for health caretakers. We have computed from the entire dataset [including the controls and patients with mild malaria (MM)] SVD-based and SVDMDSbased representations of the cytokine activity measurements in covariance space (and B). It becomes immediate evident that whereas the representation by SVDMDS identifies TNF as having a major contribution to one of the higher principal components, SVD alone does not reveal this prominent role for TNF leading to the conclusion that the main variability in the patient samples is due to IL2, IL6 and TGF [. The combination of IL2 and TNF measurements alone suffices, however, to separate SM (red) from CM (blue) patients in single linkage hierarchical clustering based on Euclidean distances (). The combination of IL2 and TNF would unlikely have been identified as effective by SVD alone (). The role of TNF in CM has been also clarified when investigating the auto-immune component of CM in
CONCLUSIONDimensionality reduction of complex, high-dimensional data is an important problem which becomes ever more complicated due to the increase of data concomitant with an increase in their dimensionality. This is particularly true for data from modern genomics analyses where more and more data with thousands of instances each over millions of variables are generated. We demonstrate here how a combined molecular dynamics simulation multidimensional scaling approach for dimensionality reduction of high-dimensional data can be improved by better defining the initial conditions. We have shown that singular value decomposition is most effective to create an initial condition for MDS. Using links between SVD and different standard data analysis methods, we demonstrate how our combined SVD MDS method can be used to improve geometric representation in low-dimensional space that are generally obtained with standard analysis methods (PCA, classical scaling, PCCA, correspondence analysis). We also show that the use of iterative reduction or Page: 1420 14131421stochastic energy does not increase performance of the algorithms in terms of finding a optimal solution. Finally, we have investigated the local structure deformation induced by dimensionality reduction, and confirmed the superior accuracy of the SVDMDS. Overall, the methodology developed here should further advance our capacity to analyze high-dimensional data such as the ones produced by functional genomics approaches.
C.Bcavin et al.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
