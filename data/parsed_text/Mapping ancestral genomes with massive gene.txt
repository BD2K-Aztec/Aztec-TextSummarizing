Motivation: Ancestral genomes provide a better way to understand the structural evolution of genomes than the simple comparison of extant genomes. Most ancestral genome reconstruction methods rely on universal markers, that is, homologous families of DNA segments present in exactly one exemplar in every considered species. Complex histories of genes or other markers, undergoing duplications and losses, are rarely taken into account. It follows that some ancestors are inaccessible by these methods, such as the protoâ€“monocotyledon whose evolution involved massive gene loss following a whole genome duplication. Results: We propose a mapping approach based on the combinatorial notion of 'sandwich consecutive ones matrix', which explicitly takes gene losses into account. We introduce combinatorial optimization problems related to this concept, and propose a heuristic solver and a lower bound on the optimal solution. We use these results to propose a configuration for the proto-chromosomes of the monocot ancestor, and study the accuracy of this configuration. We also use our method to reconstruct the ancestral boreoeutherian genomes, which illustrates that the framework we propose is not specific to plant paleogenomics but is adapted to reconstruct any ancestral genome from extant genomes with heterogeneous marker content. Availability: Upon request to the authors. Contact: haris.
INTRODUCTIONMapping ancestral genomes consists in ordering ancestral markers into chromosomes, according to the organization of the descendants of these markers in sequenced extant genomes. In the absence of a good model for genome structural evolution, mapping techniques for ancestral genomes, introduced by, have given the most reliable ancestral configurations on animals (), yeast (), or plant genomes (), and even on a wide eukaryote dataset (). These works also raised new methodological issues and stimulated a recent stream of algorithmic studies related to genome mapping (), * To whom correspondence should be addressed which had taken the back seat with the development of massive genome sequencing. The general principle of ancestral genome mapping is:(i) Define ancestral genome markers, constructed either from homologous gene families or from aligned chromosome segments;(ii) Infer, from the structure of extant genomes, a collection of relations between ancestral markers which are believed to be ancestral; and(iii) Assemble this collection into an ancestral genome.The relations between ancestral markers can take several forms, such as adjacency or distance between pairs of markers, or contiguity/synteny of a subset of markers for example. The combinatorial nature of these relations defines the abstract representation of the ancestral genome, from totally ordered protochromosomal segments () to contiguous ancestral regions () and ancestral linkage groups (). Up to now, most published methods require unique and universal ancestral markers, that is, each ancestral marker has exactly one descendant in every considered extant genome. This constraint, common to many genomemapping methods () and genome rearrangement studies (), results, in general, in more tractable algorithmic problems for the assembly phase. Recently, several works tried to account for the possibly complex history of markers, by integrating whole genome duplication and gene loss either at the level of ancestral markers definition (), or in the whole mapping process, allowing duplicated markers arising from a whole genome duplication (), or genes with duplications and losses (). In these works, either a backbone of universal makers is used, or only adjacencies between genes were considered. It means that gene loss is neglected, and is expected to produce a reasonable amount of noise in the assembly phase. The above assumptions are not appropriate anymore if there is a highly heterogeneous marker content within the extant descendants considered to reconstruct an ancestral genome. For example, in the early monocotyledon evolution, a whole genome duplication is believed to have occurred (), followed by numerous gene losses, representing
H.Gavranov c et al.a fundamental evolutionary mechanism for these genomes. Gene order in this ancestor is accessible only by the extant relations between paralogous genes that have been kept in two copies, either in the same species or in two different ones. But at this evolutionary distance, only 10% of the genes are of this kind. Universal markers are almost absent, preventing the use of all existing methods except the ones of Muffato (2010) and. However, adjacencies that can be inferred between ancestral markers are also very sparse, preventing the use of the method of. Finally, a method using distances and a reduction toward a Traveling Salesman Problem (TSP) is described in the PhD thesis of Muffato (2010), and is close to one of the heuristic principle we describe here. Among our contributions, here is the formalization of this problem and the explicit integration of gene losses in the process. We propose a solution in the form of a variant of the consecutive ones problem, which was also used in physical mapping of chromosomes and adapted to ancestral mapping (). We generalize the consecutive ones framework, which itself extends the methods based on adjacencies (). The consecutive ones problem is classically used in the following way (see Chauve and Tannier, 2008 for a detailed description). A binary matrix is built, whose columns are the ancestral markers and rows represent groups of markers that are believed to be contiguous in the ancestral genome. It is a binary matrix: there is a 1 in an entry if the marker belongs to the group, and 0 otherwise. Then the assembly phase infers an order of the columns (i.e. markers) such that on all rows the entry ones are consecutive. If such an order, which satisfies the consecutive ones property, does not exist, then a combinatorial optimization approach is used, such as extracting a maximum subset of rows or flipping a minimum number of entries. Groups of markers defining the rows of the matrix are obtained from the comparison of all pairs of extant genomes whose evolutionary path contains the ancestor of interest. The framework described above can be extended to handle the fact that a marker can be missing when comparing a pair of extant genomes, because of gene loss for example. It simply calls for a third type of entry: if a marker is missing, then it is not possible to say if it belongs or not to a given group of markers, so we may mark it with an entry X in the matrix row associated to this group of markers. In the assembly phase, it can play the role of a 1 or of a 0. The assembly problem is now to find an order of the columns such that on all rows, there is no entry 0 between two entries 1, or, if such an order does not exist, to approximate this property while optimizing a given combinatorial criterion. Note that in the absence of X entries, the problem is equivalent to the consecutive ones described above. This problem was introduced by Golumbic and Wassermann (1998) and Golumbic (1998), under the names 'sandwich interval hypergraph' or 'sandwich consecutive ones matrix'. It was proved that deciding if the columns of a matrix with entries 0, 1 or X (a ternary matrix) can be ordered such that no 0 is between two 1s is NP-complete and the possibility to use this concept in physical mapping to account for missing or uncertain data was already mentioned in these articles. In Section 2, we describe several computational results for the problem of ordering the columns of a ternary matrix. We define combinatorial criteria associated to matrices that do not have the sandwich consecutive ones property, a heuristic based on the technique of partition refinement, a reduction to the TSP, a local search principle and a lower bound that is used to assess the quality of the solutions computed with these methods. Next we apply these algorithmic results to the to reconstruct the gene order of the protochromosomes of the monocotyledon angiosperms before they underwent a whole genome duplication in Section 3. 1 We discuss the robustness of the results and the multiplicity of solutions in Section 3.3. The framework we present is not restricted to plant genomes. It generalizes several of the previous methods, accounting for gene loss as an additional feature. It can be used for any ancestral genome reconstruction. We demonstrate that former results, like the boreoeutherian ancestor, can be retrieved with this new framework with a good accuracy and an increased coverage (Section 4).
THE CONSECUTIVE ONE MATRIX SANDWICH PROBLEM
Problem definitionA ternary matrix M is a matrix with n columns and m rows, each entry being equal to 0, 1 or X. A binary matrix is a ternary matrix without X entries. In a ternary matrix M, an entry M ij = 0 is called a bad zero if there exist a < j and b > j such that M ia = 1 and M ib = 1. A row j is called a bad row if it has a bad zero. Two matrices are equivalent if one can be obtained from the other by a permutation of its columns. A ternary matrix M has the sandwich consecutive ones property (SC1P) if it is equivalent to a matrix with no bad zero. If M is binary and SC1P, then it has the consecutive ones property (C1P). Typically, we use a ternary matrix M to represent a set of features of an ancestral genome of interest: columns represent ancestral markers (such as ancestral genes for example), and each row represents a group of markers that are believed to be contiguous in this ancestral genome, with all columns with an entry 1 belonging to this group and possibly some columns with an entry X. The goal of the assembly phase in inferring an ancestral genome map is to order the columns of M to represent a possible order of the markers along the proto-chromosome of the ancestral genome. Ideally one would like to find a column order such that M has the SC1P. In practice, it happens, due to convergent evolution, errors in gene annotation or in homology assignment, that a matrix does not have the SC1P, that is, there is no permutation of the columns such that in each row, no zero entry is between two ones (). The case when a binary matrix does not have the consecutive ones property was the subject of several theoretical and experimental studies (e.g.). Different problems arise such as: and computing the largest C1P sub-matrix; and computing the permutation of columns (and rows) that produce the matrix closest to a C1P matrix; computing the minimal number of elements which can be flipped to obtain a C1P matrix. These define optimization problems, most of them NP-hard. Here we use their counterparts in the sandwich problem. A first natural function is defined as the number of bad rows inPage: i259 i257i265). This matrix has the SC1P, with columns order dabc.
Matrix(Right) Sub-matrix defined by rows (14, 12, 13, 16 and 43) and columns (1, 6, 7, 8, 12 and 13). This matrix does not have the SC1P (). The existence of this sub-matrix proves the optimality of the solution in the, because there is only one bad row in the solution. any equivalent matrix. The problem is then to delete the minimum number of rows so that the remaining matrix has the SC1P. We denote the value of this function for a permutation  on a matrixwhere C is a big constant. It means that we first aim at minimizing the number of bad rows, and secondarily to integrate bad rows with a minimum number of bad zeros. We are not aware of any software or even described algorithm attempting the resolution of a consecutive ones sandwich matrix problem. We present below several techniques, a lower bound and a software to solve it and assess the qualities of the solutions. Our solver is based on @BULLET A heuristic based on a partition refinement algorithm to decide if a binary matrix has the consecutive ones property () (Section 2.2). @BULLET A reduction to the TSP (Section 2.3). @BULLET A local search to find a local optimum close to the results found by the two above methods (Section 2.4). We also describe a lower bound on the number of bad rows in a matrix that does not have the sandwich consecutive ones property, based on the certificate described in McConnell (2004) (Section 2.5). It is used to assess the quality and, in some cases, to prove optimality of the solutions obtained.
A heuristic based on partition refinementThe heuristic we describe now is based on the C1P matrix recognition algorithm described in, and relies on the very general algorithmic tool of partition refinement. It is a generalization of this algorithm, in the sense that if the instance is a binary matrix with the C1P, it performs the partition refinement just as described in McConnell (2004). First, rows of the matrix are partitioned into connected components of the overlap graph as follows: vertices are rows and two rows define an edge if they overlap, i.e. if they have some common columns with an entry 1, but none is contained in the other. Then, for each component, rows are ordered according to a breadth first search, and processed according to this order. The goal is to partition the columns spanned by the component into a totally ordered set {X 1 , ... ,X k }. Every set X i (a set of columns) is unordered but the partition itself is. Components are processed independently. During the processing of a component, a given column can be assigned to several X i s, which is the main difference with the classical partition refinement algorithm used to decide the C1P. So the structure which is maintained is an ordered family {X 1 , ... ,X i } plus a set X 0 of unassigned columns, and a function from the columns to a sub-family of X 0 , ... ,X i. The image of a column is called its possible assignments (in the algorithm of McConnell (2004), there is only one possible assignment, and several assignments are used only if X entries are encountered in the matrix). Below, we still use the terminology partition for the intermediate X i s even if, formally, they do form a partition of the columns only at the end of the process. Initialization: at first, all columns spanned by the component are assigned to X 0. The first row r 1 is treated the ones in r 1 are assigned to X 1 , the zeros to X 0 and the X entries to both X 0 and X 1. Induction: then, for a row r j , processed after rows r 1 , ... ,r j1 , assume the current columns partition is {X 1 , ... ,X i }. Let {X a , .
.. ,X b }be the largest interval of the partition such that (1) for every column with a 1 in r j , one of its possible assignments is in either X 0 or X a , ... ,X b , and (2) for every column with a 0 in r j , some of its possible assignments are outside {X a+1 , ... ,X b1 }. If such interval does not exist, skip row r j and process the following row. Else add two sets: X a before X a and X b after X b. For each column c, (1) if it has a 0 in r j and a possible assignment in X a (respectively X b ), replace the assignment of c to X a (respectively X b ) by an assignment to X a (respectively X b ), (2) if it has a X in r j and a possible assignment in X a (respectively X b ), add X a (respectively X b ) to the possible assignments of c, and (3) if it has a 1 in r j and some possible assignments in {X a , ... ,X b }, then remove all its other possible assignments. Finally, remove empty sets. If there is a column with a 1 in r j and without a possible assignment within {X a , ... ,X b }, then this column is currently assigned to X 0. In this case, either a = 1 or b = i, as the rows are processed accorded to a breadth first search of the current component. If a = 1 (respectively b = i), then X a (respectively X b ) is empty (in the opposite case skip row r j and process to the following row). Let X a (respectively X b ) is the set containing all columns with a 1 in r j but without an assignment within the sets {X 1 ,...,X i }. Insert it before X a (respectively after X b ). If a column has a X in r j and a possible assignment to X 0 , then we add X a (respectively X b ) as a possible assignment for this column. The result of this algorithm is then a partition of the columns into a totally ordered set {X 1 , ... ,X k }. From a theoretical point of view, this heuristic has the important property that, if M does not contain any entry X, this partition is the one computed by the algorithm of McConnell (2004) to decide the C1P. If M does not satisfy the C1PX, then the columns order defined by this partition induces bad rows or bad zeros.
Reduction to the TSPIt is well known that some variations of C1P can naturally be reduced to the TSP. We describe here such an approach, bearing i259
H.Gavranov c et al.some similarity with the method of Muffato (2010) applied to teleost fishes: (i) construct a complete graph G = (V ,E) where the set of vertices corresponds to the columns of the matrix and (ii) assign to every edges an appropriate cost/weightTo solve the obtained TSP instances, we use the publicly available TSP solver Concorde, with Ilog CPLEX 11.0. Results of calculation are reported in. We tried both distances with several parameters, and chose the best solution. Except for huge matrices, the solution is not very sensitive to the parameter variation.
Local searchTo improve the solutions obtained by one of the heuristics described above, we devised a simple local search that modifies the order of the matrix columns. The five basic moves we considered here are: @BULLET move one column from a position to another one, @BULLET move a set of consecutive columns to another position, @BULLET swap the position of two columns, @BULLET reverse the order of a set of consecutive columns, and @BULLET move and Reverse, i.e. move a set of consecutive columns to another position and in reverse order.We implemented the local search with an objective function first tending to minimize the number of bad rows and secondarily trying to minimize the number of bad zeros in bad rows.
Lower boundTo measure the quality of the solutions given by the combination of a constructive heuristic followed by a local search, we define a lower bound for the objective function that counts the number of bad rows in a solution, i.e. a lower bound on the minimal number of rows to remove from the matrix so that, for the remaining rows, there is an order of the columns with no bad row. Our approach is based on the notions of forbidden substructure characterization and incompatibility graph developed by McConnell (2004).
The incompatibility graph and forbidden configurationsWe first recall the construction of the incompatibility graph. Let a and b be two columns of matrix M and denote by. The incompatibility graph associated to the sub-matrix in(Left). Red edges are associated with row 13, blue edges are associated with row 17 and green edges come from row 42. This graph is bipartite and the bipartition for SC1P is (A ={ab,ac,da,bc,db,dc},A) which also defines the order dabc.(a,b) the fact that a appears before b in the order of columns. Let G be an undirected graph whose vertices are the elements of {(a,b)|a,b are columns and a = b}. Edges of G indicate the incompatibility between column orders with respect to the SC1P. @BULLET In any given order, (a,b) and (b,a) cannot appear simultaneously. Thus, a first set of edges connects every pair (a,b),(b,a). @BULLET Suppose now that a,b,c are three columns and there exists a row r such that M ra = M rc = 1 and M rb = 0. In an order with the SC1P, (a,b) and (b,c) cannot hold both. We, therefore, say that (a,b) and (b,c) are incompatible and define an edge between these two vertices. We associate the row r to this edge; therefore, the graph G can have multi-edges between two pairs of vertices but these edges are distinguished by the associated rows. Note, however, that if any of these value is X, then we can not say anything about incompatibility.proved that the incompatibility graph of a binary matrix is bipartite if and only if the matrix itself has the C1P, which provides a key ingredient for a certificate for the C1P. A similar, although weaker property, also holds for the SC1P. Property: if the incompatibility graph of a ternary matrix M contains an odd-length cycle, then M does not satisfy the SC1P. Note that the vertices and edges of an odd-length cycle C o define a set of columns and rows and, therefore, define a sub-matrix of M, that we call a forbidden configuration and that we denote M C o , which does not have the SC1P by the property above. Examples of such sub-matrices are given in. They are sub-matrices of the matrix obtained from the monocot proto-chromosome A11 shown on. The incompatibility graphs for those two sub-matrices are drawn on Figures 2 and 3.
Disjoint forbidden configurationsTwo odd cycles are called strongly disjoint in the incompatibility graph if they are disjoint in the graph, and if they do not share any row in the matrix. If there exists a set of k odd cycles that are pairwise strongly disjoint odd cycles, this clearly implies that there is at least k bad rows, as each sub-matrix associated with a cycle contains at least one bad row. We then introduce the problem of computing the maximum number of odd cycles that are, pairwise, strongly disjoint, and we describe below an efficient heuristic for this problem. First, we compute a set of odd cycles as follows. One odd cycle is found by searching the graph, and, in the associated sub-matrix, we flip the zero entries to X. In this way, this odd cycle vanishes i260and we start again the search for another odd cycle, until the graph is bipartite. The set of odd cycles so obtained are still not strongly disjoint. So we find the maximum subset of strongly disjoint ones by solving an independent set problem on the graph which vertices are the odd cycles and two vertices are joint by an edge if they are not strongly disjoint. We solve this problem using an integer programming formulation and solver. For the matrix M with m columns, the number of vertices in the incompatibility graph is m(m1) but all previous calculations remain valid if we choose the suitable subset of columns, for example the set columns comprising one or several bad zeros and repeat the procedure to cover whole matrix. This is how, if needed, we can reduce the size of the graphs we deal with. The number of odd cycles can be very large, and this leads to rather big integer programs to solve. But all these programs were easily solved for small and medium instances (monocot ones) as it is reported here.
Matrix Sandwich
RECONSTRUCTING THE ANCESTRAL MONOCOTYLEDON GENOMEMonocotyledons are a branch of angiosperms whose genome has undergone a global duplication at an early stage of its evolution (). We describe here how to define ancestral markers for this genomes, then how to compute ternary matrices representing putative features of the ancestral monocotyledon genome, and finally the result of the computational methods described in Section 2 on these matrices.
Ancestral markers and ternary matricesTwo paralogous chromosomes or genes arising from a whole genome duplication are called ohnologous. Ohnologous chromosomes and chromosome segments were identified in, and we use the gene homologies computed in the same study. For example, Rice chromosomes 1 and 5 are ohnologous, as well as Sorghum chromosomes 3 and 9. As all four arise from the whole genome duplication (Rice 1 is orthologous to Sorghum 3 and Rice 5 to Sorghum 9, due to a speciation posterior to the whole genome duplication), Rice 1 and Sorghum 9 are ohnologous, as well as Rice 5 and Sorghum 3. This gives four ohnologous relations on these four chromosomes summarized in, where ohnology relationships between genes are drawn with gene positions in chromosomes. In the present work, we define a relation between genes as follows: an ohnologous pair of genes is a pair of paralogous genes that are located on two ohnologous segments. All ohnologous pairs on Rice chromosomes 1 and 5and Sorghum chromosomes 3 and 9 are drawn in. Genes are grouped into families defined by the transitive closure of the relation, and each family defines an ancestral marker. We then define a matrix M, whose columns are the ancestral markers and rows are defined in terms of adjacencies and common intervals. Then for each pair of ohnologous segments A and B, we computed common intervals () on the set of ancestral markers which have descendants in both segments. For each common interval I, a row of M is constructed as follows:@BULLET there is a 1 in column i if the ancestral marker i is in I, @BULLET there is a 0 in column i if the ancestral marker i has descendants in A and B but is not in I, and @BULLET there is an X in column i if the ancestral marker i has no descendant in either A, or B, or both. This defines the monocot ternary matrix. A toy example is given in, based on two segmental ohnologous relations. Common intervals are sets of ancestral genes which descendants are seen contiguous in two branches arising from the whole genome duplication, so they are believed to define a set of contiguous markers in the ancestor. In the example of, moving column A into the first position results in a matrix where no entry 0 is located between two entries 1, which means it satisfies the sandwich consecutive ones property. We applied the technique described above to define five ternary matrices defining 57 proto-chromosomes of the monocotyledon ancestral genome () (proto-chromosomes A4, A5, A7, A8 and A11). See.
Proto-chromosomes of the monocot ancestorFigures 6 and 7 give examples of the shape of the instances and of the solutions. The shown ternary matrices represent the input used to compute protochromosomes A11 and A5 (). Entries 1 are red, 0 are blue and X are green. All rows are represented, even the bad ones. Red segments represent common intervals. For A11, only 35 ancestral genes were considered, whereas 120 genes were considered in A5. A11 has only one bad row and as shown in the previous section the matrix does not have the SC1P. So this solution is optimal in terms i261of the number of bad rows, and in the bad row there is a solution with only one bad zero, then optimality is guaranteed also for the number of bad zeros. For A5, the lower bound gives at least 27 bad rows, whereas we find a solution with 34 ones (over a total of 548 rows). Statistics for the other chromosomes 2 are given in. To assess what we gained by modeling the result of gene loss instead of not taking it into account and consider it as a possible source of errors in the data, we also computed the solutions by replacing X entries by 0. We used the method of Chauve and Tannier (2008) to solve the problem of the minimum number of rows to discard, in order for the remaining matrix to have the C1P. In the A11 matrix, 11 rows have to be discarded (C1P) instead of only 1 (SC1P). For A5, 351 rows are bad instead of 59. So the noise due to some errors in the data is considerably reduced by paying attention to gene loss.Page: i263 i257i265
H.Gavranov c et al.
Matrix Sandwich
Validation and robustnessFigures 8 and 9 represent a sample of locally optimal solutions obtained by repeatedly applying the program with a randomization (the local search is easily randomized for example, as well as the order in which rows are processed in the partition refinement heuristic). In these figures, the order of the columns we represent is the one of a best found solution, and a gray square at coordinates i,j represents the proportion of solutions in which column i is in position j: the darker the square is, the larger the proportion is. The 'X' shape of the figures is explained by the symmetrical property of the problem: one order of the columns and its reverse have the same score according to all objective functions. Few columns have a compulsory position. But the distribution of the positions of most columns is far from uniform: there is a clear tendency to stay close to the position in the chosen best solution. In, we can see an inversion for which it is impossible to decide whether the ancestral state carries it or not (the small 'X' shapes surrounded by a black ellipse). This is not surprising from the examination of, where we can see that Rice chromosome 1 and Sorghum chromosome 3 carry one order on this part, while Rice 5 and Sorghum 9 exhibit the reverse order.So there has been an inversion along one evolutionary branch (the one leading to Rice1/Sorghum3, the other to Rice5/Sorghum9) after the duplication, but there is no way to decide on which one, so we cannot know the ancestral configuration.
RECONSTRUCTING THE MAMMALIAN ANCESTORThe SC1P of ternary matrices generalizes the C1P for binary matrices. So the framework we present is not only a way to reconstruct ancestral genomes with massive gene losses as the monocot one, but also all problems solved by C1P techniques can be a fortiori solved in this case, with possibly more accuracy in the presence of unequal gene/marker content in extant species. Indeed, as discussed in Pham and Pevzner (2010) for example, when large datasets of extant genomes are considered, the number of universal markers naturally decreases, due, for example, to lineagespecific rearrangements that split the occurrence of a marker into several smaller genome segment. As a consequence usual methods to infer synteny blocks, either from genes or DNA alignments, are not adapted anymore. Here we show that the SC1P can be used to handle such problems, and we illustrate this feature by reconstructing the boreoeutherian ancestor, which has been the subject of a large literature (including) and whose general architecture is mostly agreed upon by both computational and cytogenetics studies.
Ancestral markersWe used the Pecan alignments from Ensembl Compara (version 58,), which is a set of non-universal homologous markers within 15 amniote genomes, including 12 placental mammals (that were colinear in all genomes in which they are present were first grouped, and then alignments of length <100 kb in at least one species were discarded. Then an ancestral marker is defined by each alignment which has at least an occurence in two extant species whose evolutionary path goes through the boreoeutherian node. This gives 1724 ancestral markers covering 35% of the human genome, whereas there was 990 universal markers (i.e. present in all 15 species) covering 24% of the human genome. So allowing non-universal markers results in a significant improvement of the coverage of extant genomes by ancestral markers.
Reconstructing proto-chromosomesA ternary matrix for the boreoeutherian ancestral genome was constructed by performing pairwise comparisons of extant genomes. For each pair of species whose evolutionary path goes though the boreoeutherian ancestor, we computed conserved adjacencies and maximal common intervals on the set of ancestral markers which have a descendant in both genomes. The ternary matrix was then constructed in the same way than for the monocot ancestor, with entries X used to represent markers that were missing (lost) in at i263
H.Gavranov c et al.
A preliminary version of these results is integrated to a large paleogenomics study of cereals (Murat et al., 2010). It was obtained by the partition refinement heuristic, which was not described in Murat et al. (2010). All other methodological developments (lower bound, proof of optimality of the solutions, local search, reduction to TSP, robustness study), and the generalization to mammalian genomes, are new. i258 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
A4, A7, A8 as named in Murat et al. (2010), even if two of them are split into at two segments in our reconstruction, leading to 5, 6 or 7 proto-chromosomes according to the branch in which the two probable rearrangements occurred.
A8 99 161 93 55 514 250 133  A11 1 3 1 1 2 1 5 1  Column Mc describes the results of the heuristic of Section 2.2. It reaches the optimal value for instances A7 and A11 and consistently provides solutions of quality for other instances. TSP reduction is numerically less competitive but still gives meaningful results. The local search often reaches the reported values even if it starts from a random order of columns but its efficiency is enhanced with good starting solutions from the previous two procedures. Finally, in the LB column we report the obtained lower bounds for the number of bad rows. Bold values point at optimal solutions obtained by heuristics. i262 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
is clearly chimeric and joins two proto-chromosomes (one corresponding to human chromosome 11 and one corresponding to segments of human chromosome 19). It is interesting to see it is split into two segments when discarding the bad rows. Similar observation does not hold for segment 1, also probably chimeric. Understanding the signal that results in these CAR will be key to refine the SC1P approach. Finally, we can see that segment 8 seems to capture an association between human chromosomes 12, 22 and 10 (although lost when discarding bad rows) that has been described in cytogenetics studies (Froenicke et al., 2006) but never recovered in computational studies based on sequenced genomes until now. In conclusion, we recover an ancestral genome which fully meets the standards of usual reconstructions, with some additional interesting features and a higher coverage than if only universal markers were used. 5 CONCLUSION We introduce a general framework for ancestral genome reconstruction by genome mapping, which contains the principles of most former methods and adds the possibility to handle heterogenous genome content and, in particular, gene losses. We apply it to the reconstruction of the pre-duplication monocot ancestor, which is inaccessible to other methods requiring universal genes or conserved adjacencies. We also show that it can be applied to other taxonomic groups as illustrated with the boreoeutherian ancestor, by providing a solution with a higher coverage than if markers were restricted to universal markers. The solution we describe is based on a classical combinatorial problem, the sandwich consecutive ones matrix. We propose a software based on several algorithmic techniques to solve related optimization problems. We also assess the quality of our solutions by computing a lower bound of the optimal solution, and by representing a large set of locally optimum solutions. The results we obtain suggest that the framework is well adapted to handle a heterogeneous gene/marker content in paleogenomics studies. This should motivate further investigations on related methodological and algorithmic problems. In order to process large datasets, algorithms will have to be able to handle very large instances, with dozens of thousands of columns and possibly several hundreds of thousand of rows. Currently, we are limited to one order of magnitude below. Moreover, to decrease the number of entries X in the ternary matrices, and then the number of bad rows, better models of ancestral markers and common intervals with unequal gene/marker content should be investigated. It is also possible to generalise this framework by explicitly modeling gene duplications, adding the possibility of multiplicities as in Wittler and Stoye (2010). This addition will be the topic of a future work.
i265 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
