Motivation: Next-generation sequencing generates large amounts of data affected by errors in the form of substitutions, insertions or deletions of bases. Error correction based on the high-coverage information, typically improves de novo assembly. Most existing tools can correct substitution errors only; some support insertions and deletions, but accuracy in many cases is low. Results: We present Karect, a novel error correction technique based on multiple alignment. Our approach supports substitution, insertion and deletion errors. It can handle non-uniform coverage as well as moderately covered areas of the sequenced genome. Experiments with data from Illumina, 454 FLX and Ion Torrent sequencing machines demonstrate that Karect is more accurate than previous methods, both in terms of correcting individual-bases errors (up to 10% increase in accuracy gain) and post de novo assembly quality (up to 10% increase in NGA50). We also introduce an improved framework for evaluating the quality of error correction. Availability and implementation: Karect is available at: http://aminallam.github.io/karect.
IntroductionNext-generation sequencing (NGS) technologies generate at decreasing costs large amounts of data for tasks such as de novo genome assembly, resequencing, single-nucleotide polymorphism discovery, DNAprotein interaction discovery and identification of chromosomal rearrangements (). Reads produced by NGS technologies suffer from sequencing errors in the form of substitutions, insertions and deletions of bases, which complicate further processing.summarizes the types of errors in recent NGS technologies. Error correction methods utilize the high data coverage to correct the erroneous bases in reads. Existing methods are classified in five categories: (i) k-spectrum-based methods, such as Lighter (), Blue (), Trowel (), HECTOR (), BLESS (), Musket (), Reptile (), Quake (), Hammer () and the works ofthat decompose reads into the set of all k-mers. Error correction is based on the k-mer frequencies. Variants of these methods are used as preprocessing stages in several assemblers, such as SOAPdenovo (), ALLPATHS-LG (), SGA () and SPAdes (Suffix array/tree-based methods, such as SHREC (), HSHREC () and HiTEC (), are generalizations of k-spectrum methods that support multiple k values. They build a suffix array/tree of all read suffixes and correct errors using the k-mer frequency weights associated with the suffix tree nodes, whereas RACER (is based on the same concepts, without explicitly using a suffix array. Fiona () utilizes partial suffix trees along with statistical methods. (iii) Multiple sequence alignment (MSA)based methods consider each read r as reference and perform multiple alignment of reads that share at least one k-mer with r. Coral () creates a consensus sequence after each alignment with r. DAGCon () uses a directed acyclic graph instead of a consensus sequence. ECHO () computes consensus bases expectation maximization algorithm that by performing pairwise alignment among reads sharing at least one k-mer. MuffinKmeans () groups reads based on spectral clustering before applying MSA. (iv) Filtering methods, such as Diginorm (), exclude a substantial number of reads classified as erroneous based on k-mer frequencies. (v) Hybrid methods, such as LoRDEC (), Proovread (), LSC () and PBcR (), specifically target the correction of Pacific Biosciences reads, which are very long and chimeric, using Illumina reads. Read error correction has two major key challenges: (i) correcting reads associated with low-covered regions of the genome; reads having high error rate and reads that can be mapped to inexact repeat regions. (ii) Handling insertion and deletion errors (Most methods consider only substitution errors. At the time of writing this article, Blue, Fiona, HECTOR, MuffinKmeans, DAGCon, Coral and HSHREC support insertion and deletion errors. The current implementation of BLESS supports substitution errors only. Hybrid methods target specifically Pacific Biosciences reads.). This is important since sequencing machines that produce long reads, which are useful for obtaining high-quality assemblies, suffer from such errors (). We propose Karect (KAUST assembly read error correction tool). Karect belongs to the MSA category. It considers each read r as reference, performs multiple alignment for a set of reads similar to r and stores the accumulated results in a partial order graph (POG;). Compared with existing approaches, Karect introduces novel methods to select an optimized set of reads similar to r; represent reads in the graph; compute weights for the graph edges and construct corrected reads. Karect has the following advantages: (i) it supports substitution errors (called mismatches), insertion and deletion errors (called indels) and is compatible with most NGS technologies. (ii) It is fast, supports parallel execution on multi-core CPUs and can work with limited memory. (iii) It is effective against low-coverage regions, high error-rate regions and inexact repeat regions. (iv) Experiments on data from several genomes sequenced by various sequencing technologies show that Karect consistently outperforms existing techniques in terms of both individual-bases error correction (up to 10% increase in accuracy gain) and post de novo assembly quality (up to 10% increase in NGA50). This article also introduces an improved framework for evaluating the quality of error correction methods.
MethodsThe general framework of the introduced error correction mechanism is described in Algorithm CORRECTERRORS. The novel aspects of the algorithm are explained in the following subsections. Our framework uses POGs to accumulate partial alignment results. POGs are directed acyclic graphs that represent multiple alignment information of a set of sequences. POGs are used in various bioinformatics applications, including protein alignment () and the DAGCon module within the HGAP assembler ().
Selecting candidate readsLet R be the set of reads from the sequencing machine. We consider each r 2 R as reference read and select a set C r & R of candidate reads to align with r (line 3 in Algorithm CORRECTERRORS). Ideally, C r should contain all reads that have significant overlap with r. However, this would result in very high computational cost during the alignment phase. Therefore, existing error correction techniques rely on heuristics to select a small set of candidate reads. For example, Coral () selects reads that share with r at least an exact k-mer. Unfortunately, this heuristic is too restrictive and may incorrectly discard many useful reads. Karect employs an improved heuristic that becomes progressively less restrictive by allowing mismatches/indels. It generates all k-mers (Default k ranges from 27 to 42 based on number of reads.) of the reference r and each candidate read and splits each k-mer in three equal parts of length l  k=3. Let r i be a k-mer of r; an example for r i  AAACCCTTT is shown in. Let C ri be the set of candidate reads containing a k-mer that matches r i. To construct C ri , four types of matches are used: type (a): Karect first initializes(d). These k-mers match the underlined k-mer of the read r C ri to reads that share with r exactly k-mer r i (). Type (b): If less that m type (a) reads are found (m is a user-defined constraint [default m  max30; min150; 0:6estimated coverage.), Karect adds to C ri reads that may contain up to d mismatches/indels in the l-prefix or l-suffix of k-mer r i (), where d is a userdefined parameter (default d  2). To count the mismatches or indels, the Hamming or edit distance is used, respectively. Type (c): If jC ri j < m Karect generates two smaller k 0-mers of r i , where k 0  2l and searches for exact k 0-mer matches (). Type (d): If jC ri j < m, Karect searches for reads that contain up to d mismatches/indels in the l-prefix or l-suffix of the k 0-mer (). To reduce the effect of bias towards specific k-mers, C ri is allowed to include at most m reads sharing the same k-mer or k 0-mer. C ri reads are added to C r , and the process is repeated for other k-mers of r. For more details refer to the Supplementary Document.
Alignment and normalization of candidate readsOur goal is to correct reference read r. Karect aligns each read c in the candidate set C r , against r (line 5 in Algorithm CORRECTERRORS). The result includes the start and end of c or r (semi-global alignment) to allow the alignment of overlaps. We use a variant of the Needleman and Wunsch (1970) algorithm; refer to the Supplementary Document for details. To exclude candidate reads sequenced from different genome regions, an alignment is considered valid only if the overlap exceeds a threshold (Default s 1  maxmin0:7  avgReadLen; 35; 0:2refReadLen:) s 1 and the number of mismatches/indels within the overlap does not exceed a threshold (Default s 2  25% of the overlap.) s 2. This rudimentary filter may still accept some reads from irrelevant genome regions. To further minimize this problem, Karect assigns a weight w c to each read (refer to Section 2.5). Consider reference read r  CAA and candidate read c 1  GAAA. r can be transformed to c 1 by substituting C with G at position 1 and inserting A at position 4. Substitutions are modeled as deletions followed by insertions. Therefore, the alignment corresponds to del(C,1); ins(G,1); ins(A,4). Now consider another candidate read c 2  AAA. r can be transformed to c 2 by the following operations: del(C,1); ins(A,1). Observe that, inserting an A at position 1 generates the same string as inserting A at position 4. Therefore, an equivalent representation for the alignment is del(C,1); ins(A,4). We call this the normalized form of the alignment (line 6 in Algorithm CORRECTERRORS), where normalization means that operations are shifted as far as possible to the right. Normalization allows better grouping of operations of a set of candidate reads, which enables Karect to correct reference reads with high accuracy. In the previous example, after normalization it is revealed that, to correct r, we must insert an A at position 4, with high probability. The concept of normalization is also used in DAGCon (), but the resulting representation is suboptimal; the details are explained in the Supplementary Document. Note that normalization is not required if the sequencing technology generates only substitution errors.
Storing alignments in the POGEach normalized alignment is stored in a POG G r associated with the reference read r (line 7 in Algorithm CORRECTERRORS). Initially, G r represents only r. The candidate read alignments are then added incrementally in G r in a manner similar to DAGCon, with the difference that similar out-nodes (i.e. nodes connected by edges coming out from the same node) are merged instantly; this saves time and space. Also, in contrast to DAGCon, similar in-nodes (i.e. nodes connected by edges going to the same node) are not merged, since this is not required by our extraction algorithm; this also saves computational time.illustrates an example of aligning four candidate reads c 1 ;. .. ; c 4 to reference read r. The value on each edge corresponds to the number of alignments passing through that edge. We are going to modify these values in Section 2.5. For sequencing technologies that generate only substitution errors, instead of a POG we use an array of size jrj to accumulate alignment weights.
Extracting corrected read from the POGGiven POG G r for a reference read r, the corrected read r 0 corresponds to a path within G r (line 8 in Algorithm CORRECTERRORS). There are many ways to select such a path. For instance, it can be the path that maximizes the sum of edge scores, but the quality of error correction is expected to be low, because the heuristic favors longer paths. As another example, DAGCon assigns each node a score based on the weights of the out-edges and local coverage and selects the path that maximizes the sum of node scores. We propose a novel approach. First, we normalize all edge weights such that the sum of the out-edge weights of any node is 1 (). The rationale is that, after normalization, edge weights will reflect the transition probability between nodes. Then, the problem is mapped to the classic problem of finding the most reliable path in a network (), which is the path that maximizes the product of edge weights. Since POGs are directedIn the second row, c 1 introduces an insertion and a substitution. Next, c 2 includes a deletion, an insertion and a substitution and so on. At each row, the newly introduced changes are shown in bold. Normalized POG of. The extracted path is shown in bold Karectacyclic graphs, the score of the best path can be found by dynamic programming using the following recursive rule: pWs  1 pWu  max v2VGr fpWv  wv; ugThe score of the best path is pW(t) where s and t are unique start and end nodes, VG r  is the set of nodes in the POG, wv; u is the normalized edge weight between nodes v; u 2 VG r  or 0 if they are not directly connected. The corrected read r 0 corresponds to the nodes of the best path. For sequencing technologies that generate only substitution errors, r 0 is computed as the consensus (i.e. the sequence containing the most frequent base in each position) in a gap-free multiple alignment.
Handling high-error regionswhere overlapError is the edit distance between the overlapping regions of the two reads (or the Hamming distance if only substitution errors are considered), overlapSize is the size of the aligned reference read overlap and maxErrorRate is a constant (default is 0.25). Intuitively, the equation favors very similar reads that belong with high probability to the same genome region, to less similar ones that are still probable to be from the same region but highly affected by sequencing errors. This configuration enables our approach to handle high-error regions better than previous approaches. If quality values from the sequencing machine are available, we set edge weights to w c  1  10 0:1qci , where q ci is the quality value associated with the destination base at position i.
Handling coverage variabilityLet there be two regions in the genome that are very similar (e.g. they differ by only a few bases). Assume that coverage is not uniform, which is the typical case for NGS technologies and let one of these regions be more covered than the other. For example, assume genome region AAAAAAA is covered by 50 reads, whereas region AAACAAA is covered by only 20 reads. The POGs resulting from the methodology described above, will be biased towards converting all reads from the lesser covered region, to match those of the higher covered one; obviously this is wrong. To minimize this problem, if an original edge weight in the POG exceeds a threshold (Default s 3  min100; 0:42  estCoverage. The estimated coverage is multiplied by 1/2 for diploid genomes to get the coverage of a single chromosome copy; refer to the Supplementary Document for details.) s 3 , Karect eliminates all other out-edges from its source node before extracting the corrected read. By original edge, we mean an edge in the initial POG of the reference read r being corrected, such as the topmost POG in.
ResultsWe compare the error correction quality of Karect against the state-ofthe-art tools for substitution errors, namely Lighter (), Trowel (), BLESS (), Musket (), RACER (), SGA (), Quake (), Reptile () and Diginorm (). We also compare against the top performing tools that support insertion and deletion errors, namely Blue (), Fiona (), DAGCon (), Coral (), MuffinKmeans () and HSHREC (). We employ a Linux machine with 2  6-core Intel CPUs at 2.67GHz and 192GB RAM.
DatasetsWe use data from the 454, Ion Torrent and Illumina sequencing machines;lists the details. (i) The 454 datasets consist of fragment and paired-end libraries of Helicobacter pylori, Zymomonas mobilis and Escherichia coli. The same datasets were also used by, but in their study, they used only the pairedend library for Z.mobilis, whereas we use the fragment library, too, to increase coverage. Also, we discard a portion of the reads for H.pylori, to obtain moderate coverage.We used the GAGE reference of s.aureus.ill, which includes two plasmid sequences in addition to NC_010079.1. For the 454 and Ion Torrent datasets, we report the average read length, since reads do not have the same length.) to evaluate the resulting assemblies. Additional datasets for ultra-high coverage datasets and RNA-Seq data appear in the Supplementary Document.
Novel quality evaluation frameworkTwo main metrics have been used in previous work to evaluate the quality of error correction. The first one considers individual bases and differentiates correct base-operations from incorrect ones (). The second metric is applied on the granularity of entire reads and differentiates correct reads from incorrect ones (). Although each metric has its tradeoffs (), depending on the biological application, both are useful. For example, in the case of single-nucleotide polymorphism discovery, the base-level metric is more appropriate, whereas in the case of de novo assembly, the read-level metric is more suitable. For completeness, in this article, we report results for both metrics. However, the instantiations of the metrics in previous work have drawbacks. Below we propose improved versions for both metrics. In the following, we define true positive (TP) an instance that was wrong initially but became correct after the application of the error correction algorithm, denoted as: TP : wrong ! correct. In a similar way, we define true negative TN :correct ! correct; false positive FP : correct ! wrong and false negative FN : wrong ! wrong. We use the common definitions of Recall  TP=TP  FN; Precision  TP=TP  FP; FScore  2  Precision Recall=Precision  Recall and Gain  TP  FP=TP  FN.
Individual base-operations metricThe metric lists the edit operations to transform the original and corrected read to the reference region, respectively, and counts the differences (). It works when only substitution errors are considered, but it is not well defined in the presence of insertions and deletions (), because in this case there are multiple ways to list the edit operations. We propose an alternative definition based on the actual distance, instead of the list of edit operations. Let o and c be the original and corrected read, respectively. The distance between them is the number of bases that were wrong in o but got corrected in c (i.e. true positives TP), plus the number of bases that were correct in o but got wrongfully altered in c (i.e. false positives FP); formally Do; c  TP  FP. D is the Hamming distance for substitution-only errors or the edit distance, otherwise; in both cases, D can be computed unambiguously. Let P o be the set of several genome regions where o can be mapped with minimum distance, and p c be the member of P o that has the smallest distance to c; formally p c  argmin p2Po Dp; c. Using the same methodology as above, we define Do; p c   TP  FN and Dc; p c   FP  FN. Solving the system of equations results in FN  1 2 Do; p c  Do; c  Dc; p c ; TP  Do; p c   FN and FP  Do; c  TP.
Entire reads metricWe align the original read o with the reference genome and, similar to the previous section, we obtain the set P o of (possible many) genome regions where o can be mapped with minimum distance D. Let c be the corrected read after applying the correction algorithm on o. o and c are considered correct if they match exactly any region p 2 P o , else they are considered wrong. On the basis of this definition, we use the formulas from Section 3.2 to calculate TP, FP and TN.
Correction qualityWe evaluate Karect against existing tools in terms of recall, precision, Fscore and gain, using both the individual base operations and the entire reads metrics; Tables 3 and 4 summarize the results. For each dataset, the best result of each column is shown in bold. We
Karectuse the default parameters for all tools. If some parameter does not have default value, we select the value suggested by the examples associated with that tool. Since DAGCon is not a stand-alone error correction tool (it is a component inside HGAP assembler), we run it by selecting candidate reads as proposed in Section 2.1, then align them using edit distance and pass alignments to DAGCon. We disable trimming in all tools, except Quake. We run Quake such that it outputs corrected and uncorrected reads. Diginorm is not included in the tables, because it filters rather than correcting reads. More details about the setup are given in the Supplementary Document. Karect consistently outperforms existing methods for most datasets (up to 10% increase in accuracy gain). Summary of these results using sumof-ranks approach appear in the Supplementary Document. We also test Karect with ultra-high coverage datasets and RNA-Seq data. The results appear in the Supplementary Document.
De novo assemblyWe evaluate the effect of error correction on de novo assembly using the following top performing assemblers: Celera () and Newbler (from Roche company) for the 454 and Ion Torrent datasets; and Velvet (), SGA () and Celera for the Illumina datasets. Celera requires read quality values as input. Since Fiona, DAGCon and HSHREC do not output such information, we manually set their quality values to 'I'. Newbler is tested both with and without quality values as input; the later case is denoted by Newbler (NQ). We do not test Newbler with diginorm, since diginorm alters the read headers in a way that Newbler is unable to detect paired reads. SGA is run after filtering out its initial error correction stage. More details about the setup are included in the Supplementary Document. Assemblies are evaluated using QUAST (). All contigs/scaffolds are split such that each of them aligns to the reference genome; contigs/ scaffolds with less than 500 bp are excluded. For both contigs and scaffolds, we report the standard metrics: NGA50, LGA50, coverage, error rate, and number of global and local misassemblies and unaligned sequences; we report also N symbols rate for scaffolds; refer tofor details.All programs use 12 threads, except BLESS and Reptile use 1 thread. Reptile failed for c.elegans.ill. MuffinKmeans failed for human.c14.ill and c.elegans.ill. We omitted some Quake results since trimming reduces correction quality, by treating each trimmed base or read as false positive or false negative.GM, global misassemblies; LM, local misassemblies; UA, unaligned contigs/scaffolds; MM, rate of mismatches/indels per 100 kb; N, rate of N symbols per 100 kb; Cov, coverage (%). More methods appear in the Supplementary Document. Tables 5 and 6 summarize the results for the de novo assembly of the E.coli 454 Titanium dataset (insertion and deletion errors) and the S.aureus Illumina dataset (only substitution errors), respectively. The best result of each column is shown in bold. Results for the other datasets are included in the Supplementary Document. The results demonstrate that, compared with existing error correcting methods, Karect improves significantly the assembly quality (up to 10% increase in NGA50). Summary of these results using sumof-ranks approach appear in the Supplementary Document.
ConclusionWe presented Karect, a novel error correction technique for NGS data. Karect is based on multiple alignment, supports substitution, insertion and deletion errors and handles effectively non-uniform coverage as well as moderately covered areas. Extensive experimental evaluation demonstrates that Karect achieves superior error correction compared to existing state-of-the-art methods. Karect also enables substantially improved assemblies, when used as preprocessing step for modern assemblers. Currently, we do not support Pacific Biosciences data, because of chimeric reads; we are working on this issue.
FundingAA and PK are supported by the KAUST Base Research Fund of PK. VS is supported by the KAUST Base Research Fund. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
A.Allam et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
