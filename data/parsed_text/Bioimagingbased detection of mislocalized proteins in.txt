Motivation: There is a long-term interest in the challenging task of finding translocated and mislo-cated cancer biomarker proteins. Bioimages of subcellular protein distribution are new data sources which have attracted much attention in recent years because of their intuitive and detailed descriptions of protein distribution. However, automated methods in large-scale biomarker screening suffer significantly from the lack of subcellular location annotations for bioimages from cancer tissues. The transfer prediction idea of applying models trained on normal tissue proteins to predict the subcellular locations of cancerous ones is arbitrary because the protein distribution patterns may differ in normal and cancerous states. Results: We developed a new semi-supervised protocol that can use unlabeled cancer protein data in model construction by an iterative and incremental training strategy. Our approach enables us to selectively use the low-quality images in normal states to expand the training sample space and provides a general way for dealing with the small size of annotated images used together with large unannotated ones. Experiments demonstrate that the new semi-supervised protocol can result in improved accuracy and sensitivity of subcellular location difference detection.
IntroductionKnowing the subcellular locations of proteins in human cancer tissues can improve the understanding of protein functions and cancer pathogenesis (). It has been demonstrated that the translocation of protein might be a signal of cancer (). The cyclin D1 protein is an example: it shuttles between the nucleus and cytoplasm in a healthy cell and the reduction of exportation from the nucleus can lead to overexpression in the nucleus and the inactivation of the tumor-suppressing protein retinoblastoma (). Accurately detecting protein translocations in human cancer tissues can thus be of important help for clinical diagnosis and treatment. Because traditional wet lab experiments are expensive in time and costs (), automated methods are highly desired for handling the increasing amounts of biomedical data. Despite its importance, only a few studies have reported automated methods to detect translocation details in cancerous tissues until now. One reason is that sequence-based analysis by itself is not V C The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com sensitive enough for detection of protein translocation as translocation can be strongly effected by mutations outside the target sequence. For example, mutations in nucleoporin complexes can have dramatic effects on the nuclear localization of multiple other proteins (). Due to recent advances in microscopic imaging, image-based pattern analysis methods have gained popularity due to the intuitive and detailed information the images contain. For example, the Murphy group discussed the potential applications of their models based on automated analysis of fluorescence microscopy images to the analysis and classification of skin cancers ().compared the abilities of automated image analysis and pathologist visual examination in quantifying protein expression in ovarian cancer. Recently, our group developed a multilabel subcellular location predictor, iLocator, and identified several translocated proteins as potential cancer biomarkers (). To compare the localization difference of a protein in normal and cancerous tissues, we have to know its subcellular locations in both normal and cancerous states first. This can be achieved either through wet-lab experiments or computational predictions. Since image data with experimentally annotated subcellular locations in cancerous states are rare, prediction models have been used instead, especially in the large-scale screening. Due to the lack of location labels for proteins in cancerous states, however, most of the existing methods employed an approach named transfer learning, where models are first trained on proteins in normal tissues and then used to predict the localization of proteins in cancerous tissues (). The performance of these approaches is poor, where one reason is the subtle differences in subcellular location patterns between cancer and normal states, which are influenced by cell mutations and morphological changes. In fact, there are a large number of images of proteins with cancerous tissues. The Human Protein Atlas (HPA, version 11, http:// www.proteinatlas.org/) () database, for example, currently contains more than 1 million immunohistochemistry (IHC) microscopy images of proteins in cancerous tissues. But due to the lack of explicit subcellular annotations, no attempt has been made in using these images from cancerous tissues for constructing supervised models for cancer localization prediction. To address the issues, we present a heuristic semi-supervised learning framework for subcellular location prediction by taking advantage of the unannotated cancer samples in developing predictors. The key advantage of the proposed semi-supervised method, in comparison to the traditional supervised learning algorithms, is that it can train prediction models with only a few labeled image samples and a large pool of unlabeled samples (). An iterative and incremental strategy was designed to select unlabeled samples into the training set. To choose the most discriminative samples, we developed three different training modes: a single-training model consisting of only one classifier (), a co-training model consisting of two classifiers () and a tri-training model consisting of three classifiers (). Also, as the incorporation of prior knowledge can improve the performance of semi-supervised methods (), we took the location information from the corresponding normal tissues as prior knowledge to guide the selection process. Another advantage of the proposed semi-supervised framework is that the training samples become typically much more enriched compared with the traditional supervised learning. First, it selected useful lower-quality images from normal tissues for training. In general, researchers prefer using well-stained images in the training set (). But selecting only high-quality images may introduce bias into modeling because the number of high expression level images in the HPA is relatively small (). Therefore, instead of being discarded, some images of normal tissues with weak expression levels were selected for use in training by the semi-supervised strategy used in this study. Then, also the large cancer dataset was used for model construction by using the semi-supervised strategy of this study, which results in a much larger dataset useable for model construction. The final predictor by the semi-supervised training can be used for images from both normal and cancer tissues. We have tested the method on an independent cancer biomarker dataset composed of translocated or mislocated proteins, which have been confirmed by biological experiments. Comparing the prediction results from models trained with and without data from cancerous tissues shows that using the cancer data improves the sensitivity of detecting protein translocations or mislocations in human cancer tissues.shows the percentages of normal protein images with different levels of expression reliability in HPA version 11. Protein images with high and medium reliability corresponding to six subcellular locations in 11 tissues were collected (Supplementary). The overlapping part of two circles represents overlap on the protein level because some proteins have different reliability levels in different tissues. For example, ornithine carbamoyltransferase is one such protein because its reliability of expression in liver is high while in the colon it is medium. The IDN is randomly selected from the nonoverlapping proteins and avoids protein overlap with the training set. The ADN and BDN are composed of the remaining images with high and medium reliability levels, respectively. Note that IDN has intersection with neither ADN nor BDN at the protein level. (B) Some examples of protein images with different reliability levels and subcellular locations. (C) Summary of all the datasets used in this study. The CDC is built by images of 348 proteins in cancerous tissues, where the 348 proteins are proteins whose images in corresponding normal tissues are of high reliability of protein expression. The IBD contains 10 proteins that were reported being translocated in human cancers by the literatures (Supplementary). In the column of expression reliability, H means high and M means medium
Methods
DatasetsOur image data were extracted from the HPA database, where the reliability of the annotated protein expression data is scored as high, medium, low and very low quality, depending on the consistency of the expression profile with the available literature (). To compromise between image quality and model generality, we used the top two categories of IHC images, i.e. high and medium reliability levels (). Three normal datasets with high and medium reliability levels were used, where the datasets ADN and BDN are for training and the independent dataset (IDN) is for testing. In the experiments, we evaluated different supervised and semisupervised algorithms on the IDN, which is not contained in the training set for all the training stages. It should be noted that not all of the medium quality images in the BDN dataset were used. Only those that are capable of improving model performance were selected according to our semi-supervised strategy. The cancer dataset (CDC) contains 21 920 images, which were selectively added into the training set to improve prediction performance for proteins in cancerous tissues. One hundred and forty-seven images corresponding to 10 biomarker proteins in normal and cancerous tissues were retrieved from the HPA database and composed the independent biomarker dataset (IBD) dataset. This dataset was used to validate whether the sensitivity of detecting the subcellular location difference between normal and cancer statuses is improved by incorporating the cancer data into training. All these datasets are from 11 human tissues, i.e. breast, colon, liver, lung, lymph node, ovary, pancreas, prostate, kidney, thyroid gland and urinary bladder. They involve six major cellular organelles: cytoplasm, endoplasmic reticulum, Golgi apparatus, mitochondria, nucleus and vesicles. Among all the proteins in our datasets, 26% are multilabel proteins that belong to two or three organelles simultaneously. It should be noted that the label of each protein was obtained from the annotation of its immunofluorescence (IF) images with the same antibodies.
Image preprocessing and feature extractionBecause each original HPA image is the fusion of DNA and protein, the linear spectral separation method was used to separate DNA and protein channels (). Then we extracted the Haralick texture features, DNA distribution features and local binary patterns (LBP) features from these two channels (). Each of 10 Daubechies filters can generate 836 Haralick features. They are used to create separate feature sets referred to as db1 through db10. The dimensions of DNA distribution and LBP features are 4 and 256, respectively. A feature vector of 1096 components is used to represent the image in each Daubechies filter space. Many previous studies have demonstrated that feature selection from the high-dimensional vector is useful, so we used stepwise discriminant analysis as it has been demonstrated to work well in this field ().
Incremental semi-supervised learningWe prepared three datasets, i.e. ADN, BDN and CDC, to construct classifiers. Among them, ADN and BDN are normal datasets with different levels of reliability of protein expression, and CDC is a cancer dataset. All of the ADN dataset were used in our experiments because this dataset has the best quality. Then the samples in BDN and CDC datasets were selectively added to the training set by semi-supervised learning. A flow chart of the proposed method is shown in.
Incorporating new samplesThe requirement for a sample to be added to the training set is that its predicted label set is the same as the annotation in HPA and the other classifier(s). This is because such images have more obvious discriminative features for a certain class and they can therefore help to enhance the classification boundary of the current model. Since there is no subcellular location annotation of proteins in cancerous tissues in the HPA, we compare the prediction output to the annotation of corresponding proteins in normal tissues to judge whether a sample in the CDC dataset should be selected or not. This is reliable when considering more than 95% proteins are actually not cancer biomarkers (). Note that when adding the samples from the CDC set, the initial classifier(s) are the resulting classifier(s) after adding the BDN set. This ensures the generality of final predictor for both normal and cancer proteins. To test different strategies, we have implemented three training modes, i.e. singleclassifier mode, two-classifier mode and three-classifier mode. Details of their screening criteria to judge which samples need to be added are presented as follows. The single-classifier mode just constructs one classifier, which will be iteratively updated until the stop condition is reached. Before the iteration process, an initial classier is trained using the entire ADN dataset. In each iteration round, the classifier is used to predict(1) is used for determining the stop condition of iterations. The model is considered stable when eff j smaller than a threshold value (<0.01 in this study) the subcellular locations of the images in the candidate sample set and those images whose predicted subcellular locations are the same as the annotations in HPA are selected and put into the training set. The classifier is then updated based on the new training set, which is ready for the next iteration. According to the two-classifier mode, a predictor is composed of two classifiers, i.e. C 1 and C 2 , where their initial models are trained on A 1 and A 2 , which are generated from the ADN dataset via the bootstrap sampling method (). This sampling method randomly draws n independent samples with replacement from the original pooled set, where n is the number of samples in the pooled set. In this study, we sampled 4224 times with replacement from the ADN space and obtained approximately 63.2% of ADN images after discarding repeated images. This step can ensure A 1 & ADN; A 2 & ADN and A 1 6  A 2 , which guarantee the diversity of the initial models of C 1 and C 2. The candidate sample set was duplicated to two sets, B 1 and B 2 , which were used for updating C 1 and C 2 , respectively. In each iterative round of training, C 1 is firstly employed to predict the subcellular locations of the images in B 2 , then those images whose predicted subcellular locations are exactly the same as the annotations in HPA were removed from B 2 and added to A 2 for updating the C 2 model. Analogously, A 1 , the training set of
Stopping conditionAll the three modes are based on the iteration processes shown in. A critical question is when the iteration should terminate. The stopping condition of the iterations is determined by the effect of newly added samples to the classifier model. In this article, this effect is measured by the number of newly added samples and the change of the predicted scores for overlapping images in the current and previous rounds. To measure the change quantitatively, the t-test was used to compare the scores of two adjacent rounds and the average of the P-values was calculated. We thus define the effect aswhere n j is the number of samples newly added in the jth round, N is the total number of initial candidate samples, p j is the average P-values between round j and (j  1). The iteration stops when eff j < 0.01, which is determined according to our experimental results. Details by varying eff j are shown in Supplementary.
Dynamic threshold criterionHere, we used the support vector machine (SVM) as the classification model, and the LIBSVM-3.17 package is employed (http:// www.csie.ntu.edu.tw/$cjlin/libsvm/). The radial basis function was used as the kernel and its optimal width parameter was calculated by the data-driven calculator GFO (). To deal with multilabel proteins that can coexist in multiple subcellular locations, the binary relevance (BR) multilabel algorithm was used to deal with our datasets (). According to BR, one binary SVM model was trained for predicting the relevance of test images to one class, so each BR classifier contains six SVM models (). A six-dimensional (6D) score vector [s 1 , s 2 ,.. ., s 6 ] will be obtained per test image, where each score component represents the confidence of the input belonging to the corresponding class (six subcellular locations). Based on the outputted real-value confidence score vector, it is important to decide which class or classes should be assigned to a sample. In a previous work, we investigated the top criterion (T-criterion) and the threshold criterion (S-criterion) to decide the label sets in multilabel classifications (). The T-criterion considers that the label set consists of the labels with positive scores, and if all the scores are negative, the label with the maximum score is considered as the unique label. The assumption of the S-criterion is that the score values corresponding to the real labels are the largest, and, in the case of a multiplex sample, its multiple labels will have similar scores. So in the S-criterion, a threshold is determined to measure whether a score is close enough to the largest one. However, it is a static threshold that is applied to all the images to be classified. A static unified threshold may not fit for all images because the scales of score vectors for different images can be variable, especially for the images in different classes. To solve this problem, we proposed a dynamic threshold criterion (D-criterion) in this study, which can determine a specific threshold for each sample according to the scale and distribution of its score vector. For one image whose score vector is [s 1 , s 2 ,.. ., s 6 ], the D-criterion can be presented as: if all the six scores are negative, then the label with the maximum score is considered as the unique label; if the maximum score is positive, then y i  1; if s max  s i s max t or s i > h 1; otherwisewhere s max  maxfs 1 ; s 2 ;.. .; s 6 g; s max > 0;where y i is the prediction of the sample's relevance to the ith class, and t and h are two constant parameters that need to be determined. To derive these two parameters, the maximum a posteriori (MAP) principle is employed. First, the degree of closeness between s i and s max is defined asHere, we define H 1 to denote 'yes' and H 2 to denote 'no' when deciding whether the ith class should be assigned to the predicted label set according to the tdif value. Supposing H b is the final decision, the objective function with respect to tdif iswhere PtdifjH 1   PH 1  is the probability distribution of tdif for the positive samples in the ith class, and PtdifjH 2   PH 2  is for negative samples not belonging to the ith class. So to distinguish H 1 and H 2 with a minimum error, the tdif value in the intersection ofis taken as the parameter t according to the MAP principle (). As for the other parameter h, it is set to ensure all the high scores are not missed, and the confidence of the decision according to h isTherefore, given a confidence score a, h can then be calculated according to Equation (5). In this article, we set a  0.95, and its effects to h and classification performance are shown in Supplementary. The statistics ofare based on the score vectors obtained by using 5-fold cross validation on the training set. The calculation process is given in.
Evaluation metricsDue to the fact that we are facing multilabel proteins, five multilabel classification metrics, i.e. subset accuracy, accuracy, recall, precision and average label accuracy were employed to evaluate the performance of the predictors (see Supplementary text for details). Among them, we mainly use the subset accuracy, which is the most stringent one since it requires the predicted label set to be exactly the same as the true label set. In addition, we also measured the sensitivity and AUC of each binary classifier in the models (see Supplementary text for detail).
Results
Baseline supervised model resultsAs a baseline, the most straightforward supervised method was used to train classifiers for comparison. We took the entire ADN, entire BDN and a combination of them (ADN  BDN), respectively, as training sets to construct classifiers. Then these classifiers were tested on the independent IDN dataset, and generated the results of simple supervised learning for comparison using the T-criterion () and the D-criterion (), respectively. It can be seen fromand B that: (1) D-criterion outperforms T-criterion, demonstrating the effectiveness of the D-criterion;(2) Overall, the subset accuracies of classifiers trained on ADN are better than those on BDN, indicating that the image quality can affect the model performance; (3) Interestingly, in some cases, the results of ADN  BDN are not better than those only using ADN, indicating that not all of the medium quality images in BDN have a positive effect on performance. The first observation suggests that a dynamic threshold is better due to the specificity for testing samples, thus we will use the D-criterion in the following experiments. The second and third observations suggest that if we add all the BDN samples into ADN to train a supervised model, the performance does not improve sometimes. The reason could be that not all of the samples in theTwo constant parameters, t and h, are needed in this criterion (Equation 2). Suppose the ith score of a sample outputted from classifier is s i. When deciding whether the label i should be assigned to the predicted label set, we defined H 1 to denote yes and H 2 to denote no. t is set to distinguish H 1 and H 2 , while h is set to ensure that the labels with high scores are not missed. Both parameters are determined by maximizing posteriori principle, as well as score vectors of training set by 5-fold cross validation. (A) The histogram of tdif1. (B) The histogram of tdif2. tdif1 and tdif2 are tdif values corresponding to H 1 and H 2 , respectively (BDN are complementary to the ADN; furthermore, some lowquality samples in the BDN will degenerate the model. This motivated us to explore a better way to take advantage of the candidate image samples rather than simply employing all of them.
Improvements by selectively adding medium-reliability dataThe entire ADN was used as the initial training set, and then according to the semi-supervised iteration framework, not all of the BDN images, but only those which improve model performance were iteratively selected into the training set. The final results are three semi-supervised predictors, which are denoted as AsemiB 1 (one-classifier mode), AsemiB 2 (two-classifier mode) and AsemiB 3 (three-classifier mode), corresponding to the three training modes, respectively. The classifier of each round is tested on IDN, and the changes of subset accuracies are shown in. The changes of number of added images, and effects on each iterative round are illustrated inand D. It can be seen that as the round increases, the subset accuracy tends to increase in all modes. All the final subset accuracies when these iterations terminate, i.e. 51, 49 and 51%, are higher than the result of directly adding the entire BDN, which is 46% as shown in. Besides, both the number of added images and effect value in the iteration decrease sharply. This indicates that the influence of the added images on classification decreases as the round increases. At the end of iterations of the db7 model, 56.75, 61.37 and 52.86% images in BDN were chosen and added to the training sets of AsemiB 1 , AsemiB 2 and AsemiB 3 , respectively. Compareand B, we can see that all the subset accuracies of three semi-supervised modes are higher than those of supervised learning. Adding medium-reliability data into training set not only expands the training sample space, but also validates the effectiveness of the proposed semi-supervised idea. Considering that different semi-supervised learning methods have been widely used these years (), we also compared our methods with two stateof-the-art semi-supervised algorithms, i.e. low-density separation (LDS) and cost-sensitive semi-supervised SVM (CS4VM). LDS is a graph-based method, which represents each labeled and unlabeled sample as a node and tries to place decision boundaries in regions where there are few data nodes (). CS4VM incorporates the unlabeled data into the SVM by estimating their label means of misclassification costs ().shows the results of LDS and CS4VM when taking ADN as labeled data, BDN as unlabeled data and IDN as testing set. The performances of our proposed methods are better than LDS and CS4VM on the multilabel dataset of this article. One reason can be the multilabel sample classification is much more comprehensive than the single-label case used by the two algorithms. For instance, the LDS might be unable to accurately find the boundaries in a graph built by multilabel data, because some multilabel samples are near the low-density areas and confuse the decisions.
Performance of ensemble classifiersSince an ensemble of multiple classifiers generally achieves better performance, we constructed ensemble classifiers by combining the 10 classifiers with db1db10 features. The fusion method averages all the score vectors from the 10 single classifiers to get a final sixdimensional (6D) vector for each query image. These ensemble classifiers are tested on IDN to show their effectiveness on the normal dataset (and F). By comparing the results between the ensemble classifier and the single classifiers, we find that the ensemble classifier outperforms the single classifier on IDN dataset. For example, a 2% improvement of the subset accuracy was observed for the AsemiBC E 2 compared with the single classifier AsemiBC 2 on db7. The other merit of the ensemble strategy is that it can. Comparison of intranormal CC, intracancer CC and inter normal and cancer CC values. In the statistics, the high expression level dataset and CDC dataset are used as normal and cancer dataset, respectively. The db7 features are used and the feature dimension is 80 significantly reduce the negative bias by adding the cancer data to the training set. For instance, the subset accuracy of single AsemiBC 1 classifier on IDN with db7 feature is 47.5% (), which is 4.25% lower than the AsemiBC E 1. One final ensemble predictor without-adding CDC and one final ensemble predictor adding CDC were created. All the classifiers without-adding CDC were fused to create AsemiB E , and all the classifiers of adding CDC were fused to create AsemiBC E. Both of them could achieve good performance on IDN testing set (). It is worth pointing out that besides the most stringent metric in multilabel classification, subset accuracy (), we also used other indices to evaluate the AsemiB E and AsemiBC E and their results can be seen in Supplementary Tables S3 and S4. For example, the average label accuracy, which indicates the reliability of prediction for single locations, can achieve 87.04% for the final system (Supplementary), which implies the reliable detection of translocation from or to a specific location.
Detecting protein translocations of cancer biomarkersThe IBD set containing 10 reported biomarker proteins was used for validating whether the sensitivity of translocation detection can be enhanced by utilizing cancer data in the training phase. We compared the prediction results on the IBD set before and after adding the CDC dataset to see the effects of adding CDC data. The results from AsemiB E and AsemiBC E were compared, where the former did not incorporate the cancer data into training, whereas the latter did. To quantify the sensitivity of detecting the subcellular location changes, in addition to the predicted and reported location labels in the normal and cancer conditions, we also conducted independent sample t tests on the predicted score vectors to evaluate the significance of the location changes (Supplementary). The comparison results and P-values of the changes are shown in, from where we can see that: 1. The protein Bax and cyclin D1 prove that adding CDC dataset makes the classifiers more sensitive to detect the location changes occurring during cancer. In detail, protein Bax will partly translocate from the cytoplasm to the mitochondrion when lymphoma occurs (). This translocation cannot be found by the predictors trained only on normal data, but can be picked out by AsemiBC E , which was trained on both normal and cancer data. The protein cyclin D1 normally shuttles between cytoplasm and nucleus locations. However, in ovarian cancer cyclin D1 is found only in the nucleus (). AsemiB E predicts cyclin D1 its locations in cancer as both the nucleus and mitochondria, while AsemiBC E correctly predicts its cancer location as the nucleus only. 2. The loss of nuclear localization of PTEN in pancreatic cancer is correctly predicted by both AsemiB E and AsemiBC E (), demonstrating that the machine-learning systems are effective for the detection of protein mislocalization. 3. AsemiBC E is able to perform prediction better than AsemiB E for the IBD proteins in their normal states. For example, the protein BAG-1 is reported to reside in the nucleus in normal conditions and translocate to the mitochondria during colorectal cancer (). AsemiB E predicted BAG-1 would localize in both the cytoplasm and nucleus in the normal state, whereas AsemiBC E predicted only a nucleus location, which is experimentally correct. Other examples include NQO1 and GOLGA5. 4. The P-values also reveal the improved sensitivity for detecting protein translocations by the predictor of AsemiBC E. The lowerReported by literaturethe P-value, the more significant the change. There are a total of 16 experimentally known changed locations for the 10 proteins. Twelve of them have lower P-values in AsemiBC E with a P-value 0.00030.6167 compared with 0.0010.8560 in AsemiB E. These results suggest that the sensitivity of detecting protein subcellular location changes is enhanced by incorporating the cancer data into the model construction. 5. Although some improvements can be observed (with lower P-values) by incorporating the cancer images into the classification system construction, there are still considerable room for improvement. For instance, there are still some cases where none of the two predictors can get completely correct prediction. This suggests that tremendous future efforts are needed for further improvement.
Y.-Y.Xu et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
.3 Incorporating images from cancer tissues to the model To enhance the performance of predicting subcellular locations of proteins in cancerous tissues, we consider adding some images from cancerous tissues into the training set to eliminate the transfer prediction error caused by the difference between the normal and cancer data. Actually, we conducted an experiment to quantify the differences of patterns between the two states. Based on the proteins in CDC set, we used the correlation coefficient (CC) to measure difference between normal and cancer images, where we assumed that proteins in the CDC set did not change their locations in cancer states. This is reasonable when considering that more than 95% protein images in current HPA database are actually not cancer biomarkers (Glory et al., 2008). Each image was represented by its feature vector, and three CC matrixes were calculated: the first is the intra-CC in the normal images group, the second is the intra-CC in the cancer images group and the third is the inter-CC between normal and cancer sets. Figure 5 shows the averaged CC values based on six subcellular locations. It can be seen that the inter-CC values between normal and cancer images are lower than the intraCC values in all cases. In addition, we also calculated the P-values with the student t test between normal and cancer dataset, and P-values of all the subcellular locations are <0.05. These results demonstrate that even for the same organelle, there is a difference between the normal and cancer data. This suggests that the transfer method of using normal data as the training set to predict the cancer data may miss some specific features of proteins in the cancer state. After adding BDN in above section, we obtained three classifiers, i.e. AsemiB 1 , AsemiB 2 and AsemiB 3 , by semi-supervised learning. Following the incremental selective learning protocol, images from CDC were subsequently added to these classifiers, and we got AsemiBC 1 , AsemiBC 2 and AsemiBC 3 (Figs. 2EG and 4E). It can be seen that the subset accuracies of classifiers on the independent IDN set fluctuate and decline slightly, which is because the added cancer data affected the prediction performance of normal data. This also highlights the difference between normal and cancer data. Nevertheless, the decline in performance is not significant, and the subset accuracies still outperform the baseline results from supervised models.
Discussion and conclusions In this article, we present a new automated bioimage analysis system for sensitively detecting translocated or mislocated proteins in human cancers. The new system is featured with a semi-supervised learning engine, which can help to enlarge the training space by incorporating lower-quality or unlabeled data key to the performance of a statistic model. The other merit of the new system is the capability of predicting proteins that shuttle among multiple subcellular locations, and a new dynamic D-criterion is proposed to deal with the multilabel set determination problem by considering the specificity of each protein. The new developed system has opened a new avenue for bioimage-based automated biomarker detection work, which suits large-scale data analysis and complement research from biological experiments. We have shown that the strategy of selectively incorporating medium staining normal images with the developed semi-supervised framework is helpful for improving the classification accuracy on the normal images as demonstrated in the independent test dataset. On the other hand, some improvements were also observed when applying the semi-supervised algorithm for adding selected cancer images into training, but they have still considerable space for further improvement. For instance, some translocated or mislocated cancer biomarkers cannot be completely predicted, especially for those multi-label proteins. To further improve the performance of our system, some efforts will be made in future studies. First, we will aim to improve the multilabel classification algorithm by taking the label correlations into account. Multiplex proteins that may shuttle among more than one subcellular location indicate a complex subcellular protein organization in the cell. The benchmark dataset of this study contains 26% multilabel proteins. This ratio is even much higher to reach approximately 60% according to a recent study of applying IF and fluorescent-protein tagging techniques on mammalian cells (Stadler, 2013). In this article, we transformed the multilabel problem into six binary classification problems, ignoring the correlation among different subcellular locations. It is expected that incorporating correlations, such as proteins coexisting at different locations due to spatial proximity or functional reasons, will be useful for further improving the performance. Second, our imaging-based studies can be integrated with analysis of non-imaging data, such as proteomics and genomics analyses (Murphy, 2014). Amino acid sequence has been used for predicting protein subcellular locations for many years, and we have developed an efficient sequence-based subcellular location predictor called Cell-PLoc in previous studies (Chou and Shen, 2008; Shen and Chou, 2009). The Cell-PLoc can also deal with multilabel proteins and have wide coverage of subcellular components. Merging prediction results from different resources is a potential effective way for further enhancing the sensitivity for translocated proteins detection. The multiclassifier mode of this study also provides a feasible combination solution, which enables us to cotrain our image-based and sequence-based software to generate a better protein subcellular location prediction system.
