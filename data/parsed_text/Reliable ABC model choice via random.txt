Motivation: Approximate Bayesian computation (ABC) methods provide an elaborate approach to Bayesian inference on complex models, including model choice. Both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard ABC techniques. Results: We propose a novel approach based on a machine learning tool named random forests (RF) to conduct selection among the highly complex models covered by ABC algorithms. We thus modify the way Bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with RF and postponing the approximation of the posterior probability of the selected model for a second stage also relying on RF. Compared with earlier implementations of ABC model choice, the ABC RF approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least 50) and (iv) it includes an approximation of the posterior probability of the selected model. The call to RF will undoubtedly extend the range of size of datasets and complexity of models that ABC can handle. We illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets. Availability and implementation: The proposed methodology is implemented in the R package abcrf available on the CRAN.
IntroductionApproximate Bayesian computation (ABC) represents an elaborate statistical approach to model-based inference in a Bayesian setting in which model likelihoods are difficult to calculate (due to the complexity of the models considered). Since its introduction in population genetics (), the method has found an ever increasing range of applications covering diverse types of complex models in various scientific fields (see, e.g.). The principle of ABC is to conduct Bayesian inference on a dataset through comparisons with numerous simulated datasets. However, it suffers from two major difficulties. First, to ensure reliability of the method, the number of simulations is large; hence, it proves difficult to apply ABC for large datasets (e.g. in population genomics where tens tohundred thousand markers are commonly genotyped). Second, calibration has always been a critical step in ABC implementation (). More specifically, the major feature in this calibration process involves selecting a vector of summary statistics that quantifies the difference between the observed data and the simulated data. The construction of this vector is therefore paramount and examples abound about poor performances of ABC model choice algorithms related with specific choices of those statistics (), even though there also are instances of successful implementations. We advocate a drastic modification in the way ABC model selection is conducted: we propose both to step away from selecting the most probable model from estimated posterior probabilities and to reconsider the very problem of constructing efficient summary statistics. First, given an arbitrary pool of available statistics, we now completely bypass selecting among those. This new perspective directly proceeds from machine learning methodology. Second, we postpone the approximation of model posterior probabilities to a second stage, as we deem the standard numerical ABC approximations of such probabilities fundamentally untrustworthy. We instead advocate selecting the posterior most probable model by constructing a (machine learning) classifier from simulations from the prior predictive distribution (or other distributions in more advanced versions of ABC), known as the ABC reference table. The statistical technique of random forests (RF) () represents a trustworthy machine learning tool well adapted to complex settings as is typical for ABC treatments. Once the classifier is constructed and applied to the actual data, an approximation of the posterior probability of the resulting model can be produced through a secondary RF that regresses the selection error over the available summary statistics. We show here how RF improves upon existing classification methods in significantly reducing both the classification error and the computational expense. After presenting theoretical arguments, we illustrate the power of the ABC-RF methodology by analyzing controlled experiments as well as genuine population genetics datasets.
Materials and methodsBayesian model choice () compares the fit of M models to an observed dataset x 0. It relies on a hierarchical modelling, setting first prior probabilities pm on model indices m 2 f1;. .. ; Mg and then prior distributions phjm on the parameter h of each model, characterized by a likelihood function f xjm; h. Inferences and decisions are based on the posterior probabilities of each model pmjx 0 .
ABC algorithms for model choiceWhile we cannot cover in much detail the principles of ABC, let us recall here that ABC was introduced infor solving intractable likelihood issues in population genetics. The reader is referred to, e.g.),for thorough reviews on this approximation method. The fundamental principle at work in ABC is that the value of the intractable likelihood function f x 0 jh at the observed data x 0 and for a current parameter h can be evaluated by the proximity between x 0 and pseudo-data xh simulated from f xjh. In discrete settings, the indicator Ixh  x 0  is an unbiased estimator of f x 0 jh (). For realistic settings, the equality constraint is replaced with a tolerance region Idxh; x 0  , where dx 0 ; x is a measure of divergence between the two vectors and > 0 is a tolerance value. The implementation of this principle is straightforward: the ABC algorithm produces a large number of pairs h; x from the prior predictive, a collection called the reference table, and extracts from the table the pairs h; x for which dxh; x 0 . To approximate posterior probabilities of competing models, ABC methods () compare observed data with a massive collection of pseudo-data, generated from the prior predictive distribution in the most standard versions of ABC; the comparison proceeds via a normalized Euclidean distance on a vector of statistics Sx computed for both observed and simulated data. Standard ABC estimates posterior probabilities pmjx 0  at stage (B) of Algorithm 1 below as the frequencies of those models within the k nearest-to-x 0 simulations, proximity being defined by the distance between Sx 0  and the simulated Sx's. Selecting a model means choosing the model with the highest frequency in the sample of size k produced by ABC, such frequencies being approximations to posterior probabilities of models. We stress that this solution means resorting to a k-nearest neighbor (k-nn) estimate of those probabilities, for a set of simulations drawn at stage (A), whose records constitute the so-called reference(A) Generate a reference table including N ref simulations m; Sx from pmphjmf xjm; h (B) Learn from this set to infer about m at s 0  Sx 0  Selecting a set of summary statistics S(x) that are informative for model choice is an important issue. The ABC approximation to the posterior probabilities pmjx 0  will eventually produce a right ordering of the fit of competing models to the observed data and thus select the right model for a specific class of statistics on large datasets (). This most recent theoretical ABC model choice results indeed shows that some statistics produce nonsensical decisions and that there exist sufficient conditions for statistics to produce consistent model prediction, albeit at the cost of an information loss due to summaries that may be substantial. The toy example comparing MA(1) and MA(2) models in Supplementary Informations andclearly exhibits this potential loss in using only the first two autocorrelations as summary statistics.developed an interesting methodology to select the summary statistics but with the requirement to aggregate estimation and model pseudo-sufficient statistics for all models. This induces a deeply inefficient dimension inflation and can be very time consuming. It may seem tempting to collect the largest possible number of summary statistics to capture more information from the data. This brings pmjSx 0  closer to pmjx 0  but increases the dimension of Sx. ABC algorithms, like k-nn and other local methods suffer from the curse of dimensionality [see e.g. Section 2.5 in] so that the estimate of pmjSx 0  based on the simulations is poor when the dimension of Sx is too large. Selecting summary statistics correctly and sparsely is therefore paramount, as shown by the literature in the recent years.surveying ABC parameter estimation.] For ABC model choice, two main projection techniques have been considered so far.show that the Bayes factor itself is an acceptable summary (of dimension one) when comparing two models, but its practical evaluation via a pilot ABC simulation induces a poor approximation of model evidences (). The recourse to a regression layer like linear discriminant analysis (LDA,) is discussed below and in Supplementary Section S1. Other projection techniques have been proposed in the context of parameter estimation: see, e.g. Fearnhead and Prangle (2012);. Given the fundamental difficulty in producing reliable tools for model choice based on summary statistics (), we now propose to switch to a different approach based on an adapted classification method. We recall in the next section the most important features of the RF algorithm.
RF methodologyThe classification and regression trees (CART) algorithm at the core of the RF scheme produces a binary tree that sets allocation rules for entries as labels of the internal nodes and classification or predictions of Y as values of the tips (terminal nodes). At a given internal node, the binary rule compares a selected covariate X j with a bound t, with a left-hand branch rising from that vertex defined by X j < t. Predicting the value of Y given the covariate X implies following a path from the tree root that is driven by applying these binary rules. The outcome of the prediction is the value found at the final leaf reached at the end of the path: majority rule for classification and average for regression. To find the best split and the best variable at each node of the tree, we minimize a criterium: for classification, the Gini index and, for regression, the L 2-loss. In the randomized version of the CART algorithm (see Supplementary Algorithm S1), only a random subset of covariates of size n try is considered at each node of the tree. The RF algorithm () consists in bagging (which stands for bootstrap aggregating) randomized CART. It produces N tree randomized CART trained on samples or sub-samples of size N boot produced by bootstrapping the original training database. Each tree provides a classification or a regression rule that returns a class or a prediction. Then, for classification we use the majority vote across all trees in the forest, and, for regression, the response values are averaged. Three tuning parameters need be calibrated: the number N tree of trees in the forest, the number n try of covariates that are sampled at a given node of the randomized CART and the size N boot of the bootstrap sub-sample. This point will be discussed in Section 3.4. For classification, a very useful indicator is the out-of-bag error (). Without any recourse to a test set, it gives some idea on how good is your RF classifier. For each element of the training set, we can define the out-of-bag classifier: the aggregation of votes over the trees not constructed using this element. The out-of-bag error is the error rate of the out-of-bag classifier on the training set. The out-of-bag error estimate is as accurate as using a test set of the same size as the training set.
ABC model choice via RFThe above-mentioned difficulties in ABC model choice drives us to a paradigm shift in the practice of model choice, namely to rely on a classification algorithm for model selection, rather than a poorly estimated vector of pmjSx 0  probabilities. As shown in the example described in Section 3.1, the standard ABC approximations to posterior probabilities can significantly differ from the true pmjx 0 . Indeed, our version of stage (B) in Algorithm 1 relies on a RF classifier whose goal is to predict the suited model ^ ms at each possible value s of the summary statistics Sx. The RF is trained on the simulations produced by stage (A) of Algorithm 1, which constitute the reference table. Once the model is selected as m ? , we opt to approximate pm  jSx 0  by another RF, obtained from regressing the probability of error on the (same) covariates, as explained below. A practical way to evaluate the performance of an ABC model choice algorithm (test a given set of summary statistics and a given classifier) is to check whether it provides a better answer than others. The aim is to come near the so-called Bayesian classifier, which, for the observed x, selects the model having the largest posterior probability pmjx. It is well known that the Bayesian classifier minimizes the 01 integrated loss or error (). In the ABC framework, we call the integrated loss (or risk) the prior error rate, since it provides an indication of the global quality of a given classifier ^ m on the entire space weighted by the prior. This rate is the expected value of the misclassification error over the hierarchical priorIt can be evaluated from simulations h; m; Sy drawn as in stage (A) of Algorithm 1, independently of the reference table (), or with the out-of-bag error in RF that, as explained above, requires no further simulation. Both classifiers and sets of summary statistics can be compared via this error scale: the pair that minimizes the prior error rate achieves the best approximation of the ideal Bayesian classifier. In that sense, it stands closest to the decision we would take were we able to compute the true pmjx. We seek a classifier in stage (B) of Algorithm 1 that can handle an arbitrary number of statistics and extract the maximal information from the reference table obtained at stage (A). As introduced above, RF classifiers () are perfectly suited for that purpose. The way we build both a RF classifier given a collection of statistical models and an associated RF regression function for predicting the allocation error is to start from a simulated ABC reference table made of a set of simulation records made of model indices and summary statistics for the associated simulated data. This table then serves as training database for a RF that forecasts model index based on the summary statistics. The resulting algorithm, presented in Algorithm 2 and called ABC-RF, is implemented in the R package abcrf associated with this article.The justification for choosing RF to conduct an ABC model selection is that, both formally () and experimentally (), RF classification was shown to be mostly insensitive both to strong correlations Reliable ABC model choicebetween predictors (here the summary statistics) and to the presence of noisy variables, even in relatively large numbers, a characteristic that k-nn classifiers lack. This type of robustness justifies adopting a RF strategy to learn from an ABC reference table for Bayesian model selection. Within an arbitrary (and arbitrarily large) collection of summary statistics, some may exhibit strong correlations and others may be uninformative about the model index, with no terminal consequences on the RF performances. For model selection, RF thus competes with both local classifiers commonly implemented within ABC: It provides a more non-parametric modelling than local logistic regression (), which is implemented in the DIYABC software () but is extremely costlysee the method ofto reduce the dimension using linear discriminant projection before resorting to local logistic regression. This software also includes a standard k-nn selection procedure [i.e. the socalled direct approach in] which suffers from the curse of dimensionality and thus forces selection among statistics.
Approximating the posterior probability of the selected modelThe outcome of RF computation applied to a given target dataset is a classification vote for each model which represents the number of times a model is selected in a forest of n trees. The model with the highest classification vote corresponds to the model best suited to the target dataset. It is worth stressing here that there is no direct connection between the frequencies of the model allocations of the data among the tree classifiers (i.e. the classification vote) and the posterior probabilities of the competing models. Machine learning classifiers hence miss a distinct advantage of posterior probabilities, namely that the latter evaluate a confidence degree in the selected model. An alternative to those probabilities is the prior error rate. Aside from its use to select the best classifier and set of summary statistics, this indicator remains, however, poorly relevant since the only point of importance in the data space is the observed dataset Sx 0 . A first step addressing this issue is to obtain error rates conditional on the data as in. However, the statistical methodology considered therein suffers from the curse of dimensionality and we here consider a different approach to precisely estimate this error. We recall () that the posterior probability of a model is the natural Bayesian uncertainty quantification since it is the complement of the posterior error associated with the loss I ^ mSx 0  6  m. While the proposal offor estimating the conditional error rate induced a classifier given S  Sx 0  P ^ mSY 6  mjSY  Sx 0  ;involves non-parametric kernel regression, we suggest to rely instead on a RF regression to undertake this estimation. The curse of dimensionality is then felt much less acutely, given that RF can accommodate large dimensional summary statistics. Furthermore, the inclusion of many summary statistics does not induce a reduced efficiency in the RF predictors, while practically compensating for insufficiency. Before describing in more details the implementation of this concept, we stress that the perspective ofleads to effective estimates of the posterior probability that the selected model is the true model, thus providing us with a non-parametric estimation of this quantity. Indeed, the posterior expectation (1) satisfiesIt therefore provides the complement of the posterior probability that the true model is the selected model. To produce our estimate of the posterior probability Pm  ^ mSx 0 jSx 0 , we proceed as follows:1. We compute the value of I ^ ms 6  m for the trained RF ^ m and for all terms in the ABC reference table; to avoid overfitting, we use the out-of-bag classifiers; 2. We train a RF regression estimating the variate I ^ ms 6  m as a function of the same set of summary statistics, based on the same reference table. This second RF can be represented as a function. s that constitutes a machine learning estimate of Pm 6  ^ msjs; 3. We apply this RF function to the actual observations summarized as Sx 0  and return 1  .Sx 0  as our estimate ofThis corresponds to the representation of Algorithm 3 which is implemented in the R package abcrf associated with this paper.
Results: illustrations of the ABC-RF methodologyTo illustrate the power of the ABC-RF methodology, we now report several controlled experiments as well as two genuine population genetic examples.
Insights from controlled experimentsThe Supplementary Information details controlled experiments on a toy problem, comparing MA(1) and MA(2) time-series models, and two controlled synthetic examples from population genetics, based on single-nucleotide polymorphism (SNP) and microsatellite data. The toy example is particularly revealing with regard to the discrepancy between the posterior probability of a model and the version conditioning on the summary statistics Sx 0 .shows how far from the diagonal are realizations of the pairs pmjx 0 ; pmjSx 0 , even though the autocorrelation statistic is quite informative (). Note in particular the vertical accumulation of points near Pm  2jx 0   1. Supplementarydemonstrates the further gap in predictive power for the full Bayes solution with a true error rate of 12% versus the best solution (RF) based on the summaries barely achieving a 16% error rate.For both controlled genetics experiments in the Supplementary Information, the computation of the true posterior probabilities of the three models is impossible. The predictive performances of the competing classifiers can nonetheless be compared on a test sample. Results, summarized in Supplementary Tables S2 and S3 in the Supplementary Information, legitimize the use of RF, as this method achieves the most efficient classification in all genetic experiments. Note that that the prior error rate of any classifier is always bounded from below by the error rate associated with the (ideal) Bayesian classifier. Therefore, a mere gain of a few percents may well constitute an important improvement when the prior error rate is low. As an aside, we also stress that, since the prior error rate is an expectation over the entire sampling space, the reported gain may exhibit much better performances over some areas of this space. Supplementarydisplays differences between the true posterior probability of the model selected by Algorithm 2 and its approximation with Algorithm 3. Moreover, we found that the values of the votes provided by Algorithm 2 is only useful to assess the model that best fits the data but that any conclusion regarding level of confidence necessitates the computation of the posterior probability of the selected model provided by Algorithm 3.
Microsatellite dataset: retracing the invasion routes of the Harlequin ladybirdThe original challenge was to conduct inference about the introduction pathway of the invasive Harlequin ladybird (. We now compare our results from the ABC-RF algorithm with other classification methods for three sizes of the reference table and with the original solutions by. We included all summary statistics computed by the DIYABC software for microsatellite markers (), namely 130 statistics, complemented by the nine LDA axes as additional summary statistics (see Supplementary Section S4). In this example, discriminating among models based on the observation of summary statistics is difficult. The overlapping groups of Supplementaryreflect that difficulty, the source of which is the relatively low information carried by the 18 autosomal microsatellite loci considered here. Prior error rates of learning methods on the whole reference table are given in. As expected in such a high dimension settings (, Section 2.5), k-nn classifiers behind the standard ABC methods are all defeated by RF for the three sizes of the reference table, even when k-nn is trained on the much smaller set of covariates composed of the nine LDA axes. The classifier and set of summary statistics showing the lowest prior error rate is RF trained on the 130 summaries and the nine LDA axes. Supplementaryshows that RFs are able to automatically determine the (most) relevant statistics for model comparison, including in particular some crude estimates of admixture rate defined in, some of them not selected by the experts in. We stress here that the level of information of the summary statistics displayed in Supplementaryis relevant for model choice but not for parameter estimation issues. In other words, the set of best summaries found with ABCRF should not be considered as an optimal set for further parameter estimations under a given model with standard ABC techniques (). The evolutionary scenario selected by our RF strategy agrees with the earlier conclusion of, based on approximations of posterior probabilities with local logistic regression solely on the LDA axes, i.e. the same scenario displays the highest ABC posterior probability and the largest number of selection among the decisions taken by the aggregated trees of RF. Using Algorithm 3, we got an estimate of the posterior probability of the selected scenario equal to 0.4624. This estimate is significantly lower than the one of about 0.6 given inbased on a local logistic regression method. This new value is more credible because it is based on all the summary statistics and, on a method adapted to such an high dimensional context and less sensitive to calibration issues. Moreover, this small posterior probability corresponds better to the intuition of the experimenters and indicates that new experiments are necessary to give a more reliable answer (e.g. the genotyping of a larger number of loci).
SNP dataset: inference about human population historyBecause the ABC-RF algorithm performs well with a substantially lower number of simulations compared to standard ABC methods, it is expected to be of particular interest for the statistical processing of massive SNP datasets, whose production is on the increase in the field of population genetics. We analyze here a dataset including 50 000 SNP markers genotyped in four Human populations (). The four populations include Yoruba (Africa), Han (East Asia), British (Europe) and American individuals of African ancestry, respectively. Our intention is not to bring new insights into Human population history, which has been and is still studied in greater details in research using genetic data but to illustrate the potential of ABC-RF in this context. We compared six scenarios (i.e. models) of evolution of the four Human populations which differ from each other by one ancient and one recent historical events: (i) a single out-of-AfricaReliable ABC model choicecolonization event giving an ancestral out-of-Africa population which secondarily split into one European and one East Asian population lineages, versus two independent out-of-Africa colonization events, one giving the European lineage and the other one giving the East Asian lineage; (ii) the possibility of a recent genetic admixture of Americans of African origin with their African ancestors and individuals of European or East Asia origins. The SNP dataset and the compared scenarios are further detailed in the Supplementary Information. We used all the summary statistics provided by DIYABC for SNP markers (), namely 112 statistics in this setting complemented by the five LDA axes as additional statistics. To discriminate between the six scenarios of Supplementary, RF and other classifiers have been trained on three refer ence tables of different sizes. The estimated prior error rates are reported in. Unlike the previous example, the information carried here by the 50 000 SNP markers is much higher, because it induces better separated simulations on the LDA axes () and much lower prior error rates (). RF using both the initial summaries and the LDA axes provides the best results. The ABC-RF algorithm selects Scenario 2 as the predicted scenario on the Human dataset, an answer which is not visually obvious on the LDA projections ofin which Scenario 2 corresponds to the blue color. But considering previous population genetics studies in the field, it is not surprising that this scenario, which includes a single out-of-Africa colonization event giving an ancestral out-ofAfrica population with a secondarily split into one European and one East Asian population lineage and a recent genetic admixture of Americans of African origin with their African ancestors and European individuals, was selected. Using Algorithm 3, we got an estimate of the posterior probability of scenario 2 equal to 0.998, corresponding to a high level of confidence in choosing scenario 2. Computation time is a particularly important issue in the present example. Simulating the 10 000 SNP datasets used to train the classification methods requires 7 h on a computer with 32 processors (Intel Xeon(R) CPU 2 GHz). In that context, it is worth stressing that RF trained on the DIYABC summaries and the LDA axes of a 10 000 reference table has a smaller prior error rate than all other classifiers, even when they are trained on a 50 000 reference table. In practice, standard ABC treatments for model choice are based on reference tables of substantially larger sizes [i.e. 10 5 to 10 6 simulations per scenario (. For the above setting in which six scenarios are compared, standard ABC treatments would hence request a minimum computation time of 17 days (using the same computation resources). According to the comparative tests that we carried out on various example datasets, we found that RF globally allowed a minimum computation speed gain around a factor of 50 in comparison to standard ABC treatments: see also Supplementary Section S4 for other considerations regarding computation speed gain.Note. Performances of classifiers used in stage (B) of Algorithm 1. A set of 10 000 prior simulations was used to calibrate the number of neighbors k in both standard ABC and local logistic regression. Prior error rates are estimated as average misclassification errors on an independent set of 10 000 prior simulations, constant over methods and sizes of the reference tables. N ref corresponds to the number of simulations included in the reference table.
Practical recommendations regarding the implementation of the algorithmsWe develop here several points, formalized as questions, which should help users seeking to apply our methodology on their dataset for statistical model choice.Are my models and/or associated priors compatible with the observed dataset? This question is of prime interest and applies to any type of ABC treatment, including both standard ABC treatments and treatments based on ABC RF. Basically, if none of the proposed model-prior combinations produces some simulated datasets in a reasonable vicinity of the observed dataset, it is a signal of incompatibility and we consider it is then useless to attempt model choice inference. In such situations, we strongly advise reformulating the compared models and/or the associated prior distributions to achieve some compatibility in the above sense. We propose here a visual way to address this issue, namely through the simultaneous projection of the simulated reference table datasets and of the observed dataset on the first LDA axes. Such a graphical assessment can be achieved using the R package abcrf associated with this paper. In the LDA projection, the observed dataset need be located reasonably within the clouds of simulated datasets (seeDid I simulate enough datasets for my reference table? A rule of thumb is to simulate between 5000 and 10 000 datasets per model among those compared. For instance, in the example dealing with Human population history (Section 3.3), we have simulated a total of 50 000 datasets from six models (i.e. about 8300 datasets per model). To evaluate whether or not this number is sufficient for RF analysis, we recommend to compute global prior error rates from both the entire reference table and a subset of the reference table (for instance from a subset of 40 000 simulated datasets if the reference table includes a total of 50 000 simulated datasets). If the prior error rate value obtained from the subset of the reference table is similar, or only lightly higher, than the value obtained from the entire reference table, one can consider that the reference table contains enough simulated datasets. If a substantial difference is observed between both values, then we recommend an increase in the number of datasets in the reference table. For instance, in the Human population history example, we obtained prior error rate values of 4.22% and 4.18% when computed from a subset of 40 000 simulated datasets and the entire 50 000 datasets of the reference table, respectively. In this case, the benefit of producing more simulated dataset in the reference table seems negligible.Did my forest grow enough trees? According to our experience, a forest made of 500 trees usually constitutes an interesting trade-off between computation efficiency and statistical precision (). To evaluate whether or not this number is sufficient, we recommend to plot the estimated values of the prior error rate and/or the posterior probability of the best model as a function of the number of trees in the forest. The shapes of the curves provide a visual diagnostic of whether such key quantities stabilize when the number of trees tends to 500. We provide illustrations of such visual representations in the case of the example dealing with Human population history in. Such a graphical assessment can be achieved using the R package abcrf associated with this paperHow do I set N boot and n try for classification and regression? For a reference table with up to 100 000 datasets and 250 summary statistics, we recommend keeping the entire reference table, that is, N boot  N when building the trees. For larger reference tables, the value of N boot can be calibrated against the prior error rate, starting with a value of N boot  50 000 and doubling it until the estimated prior error rate is stabilized. For the number n try of summary statistics sampled at each of the nodes, we see no reason to modify the default number of covariates n try which is chosen as ffiffiffi d p for classification and d=3 for regression when d is the total number of predictors (). Finally, when the number of summary statistics is lower than 15, one might reduce N boot to N=10.
DiscussionThis article is purposely focused on selecting a statistical model, which can be rephrased as a classification problem trained on ABC simulations. We defend here the paradigm shift of assessing the best fitting model via a RF classification and in evaluating our confidence in the selected model by a secondary RF procedure, resulting in a different approach to precisely estimate the posterior probability of the selected model. We further provide a calibrating principle for this approach, in that the prior error rate provides a rational way to select the classifier and the set of summary statistics which leads to results closer to a true Bayesian analysis. Compared with past ABC implementations, ABC-RF offers improvements at least at four levels: (i) on all experiments we studied, it has a lower prior error rate; (ii) it is robust to the size and choice of summary statistics, as RF can handle many superfluous statistics with no impact on the performance rates (which mostly depend on the intrinsic dimension of the classification problem (), a characteristic confirmed by our results); (iii) the computing effort is considerably reduced as RF requires a much smaller reference table compared with alternatives (i.e. a few thousands versus hundred thousands to billions of simulations) and (iv) the method is associated with an embedded and error-freeReliable ABC model choiceevaluation which assesses the reliability of ABC-RF analysis. As a consequence, ABC-RF allows for a more robust handling of the degree of uncertainty in the choice between models, possibly in contrast with earlier and over-optimistic assessments. Because of a massive gain in computing and simulation efforts, ABC-RF will extend the range and complexity of datasets (e.g. number of markers in population genetics) and models handled by ABC. In particular, we believe that ABC-RF will be of considerable interest for the statistical processing of massive SNP datasets whose production rapidly increases within the field of population genetics for both model and non-model organisms. Once a given model has been chosen and confidence evaluated by ABC-RF, it becomes possible to estimate parameter distribution under this (single) model using standard ABC techniques () or alternative methods such as those proposed by
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
P.Pudlo et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
