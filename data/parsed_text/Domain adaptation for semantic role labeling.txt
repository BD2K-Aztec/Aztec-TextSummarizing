Motivation: Semantic role labeling (SRL) is a natural language processing (NLP) task that extracts a shallow meaning representation from free text sentences. Several efforts to create SRL systems for the biomedical domain have been made during the last few years. However, state-of-the-art SRL relies on manually annotated training instances, which are rare and expensive to prepare. In this article, we address SRL for the biomedical domain as a domain adaptation problem to leverage existing SRL resources from the newswire domain. Results: We evaluate the performance of three recently proposed domain adaptation algorithms for SRL. Our results show that by using domain adaptation, the cost of developing an SRL system for the biomedical domain can be reduced significantly. Using domain adaptation, our system can achieve 97% of the performance with as little as 60 annotated target domain abstracts. Availability: Our BioKIT system that performs SRL in the biomedical domain as described in this article is implemented in Python and C and operates under the Linux operating system. BioKIT can be downloaded at http://nlp.comp.nus.edu.sg/software. The domain adaptation software is available for download at
INTRODUCTIONAdvances in biology and life sciences have led to an exponential growth in the amount of biomedical literature. Thus, automatic information retrieval (IR) and information extraction (IE) methods become more and more important to help researchers to keep track of the latest developments in their field. Current IR is still mostly limited to keyword search and unable to infer the relationship between two entities in a text. A system that is able to understand how words in a sentence are related could greatly increase the quality of IE and would allow IR to handle more complex user queries. Semantic role labeling (SRL) is a shallow semantic processing task that has become increasingly popular in the natural language processing (NLP) community over the last few years. The task is to * To whom correspondence should be addressed. identify all parts of a sentence that represent arguments for a given predicate and subsequently label each argument with a semantic role. Roughly speaking, SRL can be thought of as the task of finding the words that answer simple questions of the form Who did what to whom when and where? The input to the SRL system is a single sentence and a predicate in that sentence. The output is the same sentence, but with labeled semantic roles. Consider the following example: Input: Transcription factor GATA-3In this example, the semantic role Arg0 is the cause of stimulate and the semantic role Arg1 is the thing stimulated (see Section 2 for a detailed description of semantic roles). This information is most valuable for IE () and other tasks such as question answering and summarization. Traditionally, most work in SRL has focused on documents from the newswire domain. While SRL works well on test sentences from the same domain, the models show a sharp performance drop when they are tested on a different domain (). Although there have been a number of efforts to apply SRL to the biomedical domain in recent years, the development of state-of-the-art SRL systems for the biomedical domain is hampered by the lack of large biomedical corpora that are labeled with semantic roles. The creation of such corpora is time consuming and expensive. In this article, we address SRL on biomedical text as a domain adaptation problem. The goal is to adapt an SRL system for the newswire domain (where a large annotated corpus is available) to the biomedical domain (where only a small amount of annotated text is available). This way, we can leverage existing corpora from the newswire domain and significantly reduce the cost of developing an SRL system for the biomedical domain. The main contributions of this article are: @BULLET it is the first work that performs a comparative evaluation of the performance of three recently proposed domain adaptation algorithms on the task of SRL for biomedical text; @BULLET it is the first work that investigates the extent of manual annotation needed to port an SRL system trained on newswire text to biomedical text, by explicitly determining theTo our knowledge, this is the first detailed study of domain adaptation for SRL in biomedical text, and our work demonstrates that domain adaptation can greatly reduce the cost of developing biomedical SRL systems.
SRLThe task of SRL is to find all arguments for a given predicate in a sentence and label them with semantic roles. The first step is to parse the sentence into a syntactic parse tree. The parse tree consists of the words in the sentence, their part-of-speech tags (e.g. NN, VBZ, etc.) and nodes with syntactic categories (e.g. S, NP, VP, etc.).shows the syntactic parse tree for the example sentence from Section 1 (the labels Arg0 and Arg1 are not part of the syntactic parse tree). The next step is the argument identification step, where the SRL system has to find the boundaries for all the arguments in the sentence. The annotation standard for semantic roles demands that the boundaries align with nodes in the syntactic parse tree. Thus, argument identification is equivalent to deciding which nodes in the parse tree, including the part-of-speech tags, span arguments. As shown by the example in, the system should find that the NP node that dominates Transcription factor GATA-3 and the NP node that dominates HIV-1 expression span arguments and all other nodes do not. Finally, the system has to determine the semantic role for all identified nodes. This step is called argument classification. In our example, the first identified NP node should be labeled Arg0 and the second should be labeled Arg1, as shown in. For the predicate stimulate, Arg0 and Arg1 represent the cause of stimulate and the thing stimulated, respectively. In general, Arg0 refers to the agent and Arg1 refers to the theme of the predicate. Each of the semantic roles Arg2-5 does not have a general meaning that stays consistent across different predicates. The semantic role Arg2, for example, is the instrument for the predicate stimulate, but for the predicate increase, Arg2 is the amount increased. The semantic roles Arg0-5 are called core arguments, because they represent the essential arguments of a predicate. A predicate and the semantic roles that can appear with it are called a predicate argument structure (PAS) or proposition. Additional to its core arguments, a predicate can appear with any number of adjunctive arguments. Adjunctive arguments express general properties such as time, location, manner, etc. They are labeled with ArgM plus a functional tag, e.g. ArgM-LOC, ArgM-TMP or ArgM-MNR. The combined
FEATURES AND MACHINE LEARNING METHODSThis section describes how SRL can be solved by supervised machine learning algorithms. First, the input sentence has to be parsed into a syntactic parse tree. In this article, we assume that this step has already been solved and that the correct syntactic parse tree is available to us. The next step is to learn classifiers for the argument identification and argument classification step. The classifier for argument identification performs a binary classification for every node in the parse tree to decide whether the node spans an argument or not. The classifier for argument classification performs a multiclass classification to predict the semantic role for a node in the parse tree, given that the node spans an argument. By casting SRL as a machine learning problem, there are two key decisions that have to be made: the choice of features and the choice of the machine learning algorithm. In this article, we adopt the features used in other stateof-the-art SRL systems, which include the seven baseline features from the original work oflists the features that we use for easy reference. The machine learning algorithm in our experiments is a maximum entropy (maxent) classifier. 1 Since their introduction by, maxent classifiers have successfully been applied to many NLP problems, including SRL (). Maximum entropy classifiers do not require any independence assumptions, which allow great flexibility in encoding linguistic knowledge via features. The model takes
D.Dahlmeier and H.T.Ngwhere y is a semantic role, x is an input vector, f i are feature functions,  i are the weights that are learned during training and Z is a normalization term. A detailed description of maximum entropy classifiers can be found in Ratnaparkhi (1998).
DOMAIN ADAPTATION ALGORITHMSThe task of domain adaptation is to adapt a classifier that is trained on some source domain to a new target domain. Domain adaptation algorithms can be divided into two categories: unsupervised and supervised domain adaptation algorithms. Unsupervised algorithms only use unlabeled instances from the target domain, while supervised algorithms assume that there is a small amount of labeled target instances available during training. The algorithms that we evaluate in this article are all supervised. The algorithms are presented below:@BULLET Instance weighting (InstWeight): The essential problem when applying a classifier to data from another domain is that the joint distribution P(X,Y ) of features and class labels in the target domain will be different from the source domain. Instance weighting () is a general framework to tune the estimate for P(X,Y ). The probability P(X,Y ) can be factored in the following way:The first component is the likelihood of the class given the features and the second is the prior probability of observing the features. The difference in P(X,Y ) can arise from either P(X) or P(Y |X). InstWeight tries to tackle the difference in the conditional probability P(Y |X). By weighting the instances in the training set, the domain adaptation algorithm can try to adjust the probability estimate for the target domain. Intuitively, if the estimated probability density for an instance does not match the probability density in the target domain very well, then the learning algorithm should give less weight to this instance. To do this, the algorithm weights an instance by the ratio Pt (Y |X) Ps(Y |X) between the probability densities in the target and source domain. @BULLET Augment method (Augment): Daum III (2007) proposed a domain adaptation strategy that is based on feature space augmentation. The algorithm takes each feature vector and maps it to a feature space of a higher dimension. The mapping depends on whether the instance is from the source or from the target domain. Assume that x  X is a feature vector in the original feature space. We define mappings s and t for the source and target domain, respectively:where 0 is a zero vector of length |x|. The transformation can be interpreted in the way that it takes each feature vector and makes three versions out of it: one 'general' version, one 'source-specific' version and one 'target-specific' version. The algorithm is surprisingly easy to implement and is independent of the machine learning algorithm that is used. @BULLET Instance pruning (InstPrune): instance pruning () trains a classifier on the target domain instances and uses this classifier to predict class labels for all instances from the source domain. The top N instances that are predicted wrongly, ranked by prediction confidence, are removed from the source domain. The intuition here is that these instances are very different from the target domain and will confuse the classifier during training. The remaining instances from the source domain are then used to train the classifier. Instance pruning is actually another form of instance weighting where the weight for a wrongly predicted source instance is set to zero. InstPrune depends on the parameter N. Setting N too low will hurt the performance, because it leaves too many confusing source instances in the training set. Setting N too high will also result in poor performance, because all information from the source domain is pruned away.In addition to the above domain adaptation algorithms, we implement the following three baseline algorithms: @BULLET Source only (SrcOnly): this baseline simply ignores the target domain data and trains a classifier on only the source domain data. @BULLET Target only (TrgtOnly): at the other extreme, the TrgtOnly baseline trains a classifier on the target domain data only, ignoring any source domain data that are available. @BULLET Source and Target (All): the simplest way to combine source and target domain data is to train a classifier on the combined dataset from both domains. This we call the All baseline. The potential problem with this algorithm is that when the source domain dataset is much larger than the target domain dataset, the learning algorithm might regard the target domain instances as 'noise' and essentially ignore them.We evaluate all six algorithms for the SRL task on biomedical text. The details of our experiments are given in the next section.
EXPERIMENTS
DatasetsThis section presents the details of the datasets that we used in our experiments. The source domain data comes from the PropBank corpus (), which is the most commonly used corpus for SRL. The corpus is built from financial news articles from the Wall Street Journal and is available through the Linguistic Data Consortium (http://www.ldc.upenn.edu). We use sections 221, which form the standard training set used in SRL evaluations, as our source domain dataset. The source domain dataset has a total of 36 090 annotated sentences with their syntactic parse trees and over 90 000 annotated PAS. The target domain dataset consists of the BioProp corpus (). The corpus is created from 500 MEDLINE article abstracts. The articles were selected based on the keywords human, blood cells and transcription factor. To our knowledge, BioProp is the only resource for biomedical SRL that uses full syntactic parse trees. The parse trees are taken from the Genia Treebank (GTB;). The GTB is available for download from the Genia project web site ( http://www-tsujii.is.s.utokyo.ac.jp/GENIA/). During preprocessing of the data, we found that nine abstracts from the BioProp were missing in GTB and that another 45 abstracts did not contain any annotated PAS. The remaining 446 abstracts contain 1635 sentences with a total of 1982 PAS. The statistics of the datasets are given in. It is obvious that BioProp is much smaller than PropBank, not only in terms of the number of sentences, but also in the number of PAS and verbs that are covered. The reason is that the creators of BioProp concentrated on 30 important or frequent verbs from the biomedical domain, while PropBank annotates PAS for all verbs. We also observe that the semantic roles Arg3 and Arg4 are very rare in BioProp and that Arg5 is not used at all.
Experimental setupWe investigate the performance of the SRL system on argument identification, argument classification and the combined SRL task. All experiments are conducted using 5-fold cross-validation on the target domain dataset. The 446 abstracts in the target domain dataset are split into five equal portions. Thus, there are four portions with 89 abstracts and one portion with 90 abstracts. The split is done randomly to guard against any selection bias. The SRL system is trained on four of the portions plus the complete source domain data and tested on the remaining portion of the target domain data. This is done for each of the five portions in turn and the results areaveraged over all classified instances. We further ensure that all sentences from one abstract end up in the same portion, to avoid a situation where the classifier is trained and tested on sentences from the same abstract. The evaluation metrics are described in the next section. During our experiments, we gradually increase the number of target domain abstracts that are available during training from 8 to 356 abstracts (357 in the case where the fold with 90 abstracts is used for training). This allows us to assess the impact of the target domain data. The order in which abstracts are added is random, but a particular randomly chosen order of abstracts is used in each experiment where abstracts are added incrementally. In all experiments, we use gold standard syntactic parse trees, including part-of-speech tags, which we take from the PropBank and BioProp corpus. The InstPrune algorithm has a parameter N that needs to be tuned. For each of the five portions of target domain data, we tune N through 4-fold cross-validation on the four portions of target domain data that are used as training data. The classifier is trained on three portions of the target domain data plus the complete source domain data for different values of N and tested on the remaining one portion of target domain that is part of the training data. This is done for each portion of the training data and the results are averaged. The best value of N for each of the five portions is kept. Note that no data from the portion that is used during testing is used to tune the parameter.
Domain adaptation for SRL
Evaluation metricsArgument identification and the combined SRL task are evaluated in terms of precision (p), recall (r) and F 1 measure (
RESULTSThis section presents the results of our experiments. Before we started experiments on the target domain data, we performed a test on the PropBank corpus to ensure that our SRL system represents a strong baseline. We trained the classifier on sections 221 and tested on section 23, which is the standard evaluation setting.The results are 95.11% F 1 measure for argument identification, 90.58% accuracy for argument classification, and 86.79% F 1 measure for the combined SRL task. This confirms that our model performs comparably with other state-of-the-art SRL systems ().
Argument identificationThe first experiment examines the system's performance for the argument identification step. The learning curves for the domain adaptation algorithms and the baselines are shown in. The first observation that can be made is that the SrcOnly baseline achieves a high F 1 measure of 90.49%, only 5% lower than the F 1 measure on PropBank. The TrgtOnly baseline performs poorly in the beginning but improves as more target domain abstracts are added. The All baseline shows no significant improvement over SrcOnly. For the domain adaptation algorithms, InstPrune and Augment perform better than InstWeight and also better than the three baselines.
Argument classificationThe second experiment examines the system's performance for the argument classification step. The learning curves for the domain adaptation algorithms and the baselines are shown in. The SrcOnly baseline achieves 81.50% accuracy, a drop of over 9% from 90.58% accuracy on PropBank. The TrgtOnly baseline improves quickly with more target domain data. The three domain adaptation algorithms perform similar to or slightly above the TrgtOnly baseline. None of the domain adaptation algorithms can clearly outperform the others.
Combined SRL taskThe third experiment examines the system's performance for the combined SRL task. The results are shown in.abstracts from the target domain are added. The All baseline performs decently. Our initial concern that the larger source domain data would dominate the effect of the target domain data appears to be unjustified. The best performing algorithm is InstPrune, followed by Augment. InstPrune shows a consistent improvement over all three baselines for 32 or more target domain abstracts. We recall that InstPrune is actually a version of InstWeight where misclassified source domain instances are weighted with zero weights. Our experiments show that the more aggressive strategy of InstPrune shows better results than InstWeight. We performed the Wald test for statistical significance to determine whether the improvement for InstPrune and Augment could have occurred by chance. The test was always performed against the best performing baseline algorithm. The domain adaptation algorithm performed worse than or equal to the baseline for two data points for InstPrune and four data points for Augment. For InstPrune, the improvement is statistically significant (P < 0.05) for all remaining data points. For Augment, the improvement is statistically significant (P < 0.05) for all remaining data points, except for 16 abstracts. The detailed results of all six algorithms are given in. Overall, we observe that by using PropBank data and domain adaptation algorithms, the SRL system can achieve accurate results with only a fraction of the target domain abstracts. For example, with 60 abstracts (17% of the data) and InstPrune, we can get 97% (= 81.92% 84.44% ) of the performance that we get when using 356 abstracts without domain adaptation.
ANALYSISIn this section, we analyze the results to better understand why SRL on BioProp is difficult and why domain adaptation helps. One reason why SRL on Bioprop is difficult is that the vocabularies in the newswire domain and in the biomedical domain are very different, so there are many words in the target domain that the model has not seen during training. Another reason is that a word can have a different dominant meaning in the source and target domain. Consider the following two examples for the predicate increase:Source domain:In the first example, increase has an intransitive usage where the subject is Arg1 (thing increasing). This usage can typically be found in the source domain. That is why the SrcOnly baseline wrongly predicts Arg1 for LTB4. In the target domain, we often observe increase with a transitive usage where the subject is Arg0 (causer of increase) and the object is Arg1. With domain adaptation, the system correctly classifies LTB4 as Arg0, for all three domain adaptation algorithms. This example suggests that predicates with different usage in the source and target domain are more difficult to predict than predicates with similar usage in both domains. To quantify this difference, we split the target domain data into two sets: one set contained only instances of predicates that have a similar usage and the other set only contained instances for predicates which have a different usage. To decide which predicates have similar or different usage, we referred to the data provided in. We tested the InstPrune algorithm, which performed best in our previous experiments, on the combined SRL task for the two datasets, using the same experimental setup as before. The results are shown inand 6. We observe a significant gap between the SrcOnly baseline results inand 6. This empirically confirms our conjecture that predicates with different usage are more difficult to predict without domain adaptation. When domain adaptation is used, predicates with different usage can be predicted as accurately as predicates with the same usage. Finally, we wanted to find out if domain adaptation only improves the performance for predicates that appear frequently in the target domain, or if infrequent predicates see an improvement as well. We again split the target domain data into two sets, depending on whether the predicate belongs to the most frequent verbs in the target domain or not. The information on which predicates are frequent can be found in. Again, we tested theThe best baseline and domain adaptation algorithm for each column are printed in bold face. Statistically significant improvements for Augment and InstPrune over the best baseline algorithm are marked with an 'asterisk'. All results are obtained using 5-fold cross-validation.InstPrune algorithm on the combined SRL task on the two datasets, using the same experimental setup as before. Our results show that SRL performance for infrequent predicates is about 12% lower in F 1 measure than the performance for frequent predicates. Thus, even for infrequent predicates, domain adaptation improves SRL performance, although performance is slightly lower.
Domain adaptation for SRL
70
RELATED WORKIn the last few years, there have been a number of efforts to bring SRL to the biomedical domain.developed PASBio that contains and analyzes PAS for over 30 verbs and has become a standard for annotating PAS for molecular events. Shah andapplied SRL in the LSAT system to identify sentences that talk about gene transcripts.analyzed PAS in medical case reports, but they did not present a functioning SRL system.applied SRL to abstracts from randomized controlled trial reports, but they limited the scope to five verbs only.presented an SRL system that extracted information about protein movement. They created a corpus for 34 verbs and 4 semantic roles. Their work was more problem-specific than general SRL, as they only focused on a very problem specific set of semantic roles. Most recently,presented a neural network-based SRL system for relation extraction. Their emphasis was not on accurate SRL but on fast processing speed. All of the above SRL systems use a word-by-word or chunkby-chunk approach instead of a full constituent-by-constituent, syntax-based approach, although the latter approach is the state-ofthe-art in SRL. To our knowledge, the only system for biomedical SRL on full syntactic parse trees is the BIOSMILE system by. They observed that their SRL system did not perform well on BioProp if it was only trained on PropBank. However, their results are not directly comparable with ours, because they only used a smaller portion of PropBank to train their model and they did not use any domain adaptation algorithms. Thus, their work did not investigate how well a state-of-the-art SRL system performs on biomedical text if it is trained on the whole PropBank corpus and uses domain adaptation algorithms.
CONCLUSIONIn this article, we study the effect of domain adaptation for SRL in the biomedical domain. We evaluate three different domain adaptation algorithms on the BioProp corpus using a competitive,
D.Dahlmeier and H.T.Ngstate-of-the-art SRL classifier. We conduct a systematic, detailed comparison of different domain adaptation algorithms for different number of target domain training examples. Our results show that by using just the existing SRL resources and domain adaptation, significant improvements can be achieved with only a small number of annotated target domain data. We believe that our findings will be helpful for applying SRL to new domains in the biomedical field.
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
We use the implementation in the DALR package (Jiang and Zhai, 2007)
