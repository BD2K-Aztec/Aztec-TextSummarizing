Motivation: Alzheimer's disease (AD) is a dementia that gets worse with time resulting in loss of memory and cognitive functions. The life expectancy of AD patients following diagnosis is $7 years. In 2006, researchers estimated that 0.40% of the world population (range 0.17â€“0.89%) was afflicted by AD, and that the prevalence rate would be tripled by 2050. Usually, examination of brain tissues is required for definite diagnosis of AD. So, it is crucial to diagnose AD at an early stage via some alternative methods. As the brain controls many functions via releasing signalling proteins through blood, we analyse blood plasma proteins for diagnosis of AD. Results: Here, we use a radial basis function (RBF) network for feature selection called feature selection RBF network for selection of plasma proteins that can help diagnosis of AD. We have identified a set of plasma proteins, smaller in size than previous study, with comparable prediction accuracy. We have also analysed mild cognitive impairment (MCI) samples with our selected proteins. We have used neural networks and support vector machines as classifiers. The principle component analysis, Sammmon projection and heat-map of the selected proteins have been used to demonstrate the proteins' discriminating power for diagnosis of AD. We have also found a set of plasma signalling proteins that can distinguish incipient AD from MCI at an early stage. Literature survey strongly supports the AD diagnosis capability of the selected plasma proteins.
IntroductionAlzheimer's disease (AD) is the most common form of dementia. According to a cohort longitudinal study, $1015 persons per 1000 persons per year get dementia of which 510 get AD (). In AD, an unknown process divides Amyloid Precursor Protein into smaller fragments. One of these fragments gives rise to fibrils of beta-amyloid, which gets deposited outside neurons in dense formation known as senile plaques (). Tau protein becomes hyper-phosphorylated and creates neurofibrillary tangles (). In recent years researchers have devoted great efforts for the development of AD diagnosis tools (). Most of the articles have investigated on a small set (two to four) of Cerebro Spinal Fluid (CSF) proteins (). Few attempts have been made using serum proteins () and using both CSF and serum proteins (). Some have tried tofind biomarker proteins that can discriminate AD and non-AD patients () and some have tried to predict AD from mild cognitive impairment (MCI) patients (). In the study by, a semi-supervised distance metric learning using random forests with label propagation is proposed for prediction of AD. In, concentration of T  tau, P  tau 181 and Ab42 in CSF are reported to be associated with future development of AD in patients with MCI.have proposed a microarray-based method that has selected 18 blood plasma signalling proteins to classify and predict clinical AD diagnosis. We have used the same dataset and found nine plasma signalling proteins for the same problem with comparable prediction accuracy. We have also found a set of useful plasma signalling proteins that can predict AD from MCI with a better prediction accuracy. The selection of biomarker plasma proteins from a large set of proteins is a feature selection problem. Feature selection methods can be broadly classified as filter methods and wrapper methods. In filter methods, the features are given importance solely depending on the properties of the features themselves. These methods ignore the tool finally used to recognize the patterns. But the utility of a feature depends on the pattern recognition tool being used and the problem being solved. A set of features good for a particular pattern recognition problem and a tool may not be as good for a different pattern recognition tool. The wrapper-based feature selection methods utilize the classifier itself to find the relevance of the feature. Thus, wrapper methods are generally considered better as compared with filter methods (). One of the early filter methods is Relief (). In Relief, given a feature vector, two more instances of feature vectors are considered; one from the same class and the other from the other class. The weight of a feature is decreased if the value of that feature differs more from the value in the instance of the same class than the value in the instance of the different class and vice versa. Relief was modified into ReliefF by. KernelPLS () is a kernel-based multivariate feature selection method that selects features taking into account possible non-linear relation between features as well as that between features and target. In the study by, a partial least square (PLS) algorithm finds a low-dimensional approximation of the input matrix that can explain as close as possible the target vectors. The support vector machine recursive feature elimination (SVMRFE) method () eliminates poor features using an iterative process. It starts with all features and removes one feature at a time based on a feature ranking score that is computed using the coefficients of the weight vector of a linear SVM. At each iteration, the feature with the smallest ranking score is eliminated. In this study, neural networks (NNs) are used for feature selection as well as for classification. We have used both multilayer perceptron (MLP) and radial basis function (RBF) NNs for classification. RBF NNs are used for feature selection in the studies by Basak and Mitra (1999) and Chakraborty and Pal (2008). The Group Feature Selection RBF (GFSRBF) is proposed byfor selecting useful groups of features where each feature group corresponds to a sensor. In, this network has been adapted for feature selection with controlled redundancy. Here, we have adapted GFSRBF for feature selection without any explicit control on redundancy for selection of plasma proteins that can predict clinical AD. Our approach is described in the following section.
ApproachFor diagnosis of AD from plasma samples, we need to identify the plasma proteins that carry AD specific signature. As the initial set of plasma proteins, we consider the set of 120 proteins reported in. To select an adequate set of proteins we use a modified RBF NN for feature selection. This is an integrated method where feature selection and system identification are done simultaneously. In this method, a feature attenuator is associated with each feature. For a useful feature, its attenuator allows the feature to get into the network. For an unimportant feature, its attenuator does not allow the feature to affect the network. To verify the AD-specific signature of these selected features (proteins), we subject them to different classifiers. We have also tested if the selected proteins form natural AD and non-AD-specific clusters. We have compared the results with that of Ray et al. (2007) as well as those by two filter methods (ReliefF and kernelPLS) and a wrapper method (SVMRFE).
Methods
Feature selection RBFLet p be the number of features, c be the number of classes and X be the dataset, X  fx 1 ; x 2 ; :::; x n g & < p with associated output in < c. In general, an RBF network has three layers. Layer 1, which is called input layer, has p nodes. Layer 3, which is called output layer, has c nodes. The architecture of RBF depends on number (let us denote it by h) of nodes present in layer 2, which is called hidden layer or basis function layer. The required value of h depends on the dataset. In an RBF network, each node in the input layer is connected to each node in the hidden layer, and each node in the hidden layer is connected through some weights to each node in the output layer. There is no connection between the nodes in the same layer. Each node in the hidden layer uses a Gaussian basis function, / j ; j  1; :::; h: / j x  expf k x  u j k 2 =r 2 j g;where u j is the centre and r j is the spread of the jth basis function. Note that u and x both are in < p. Let O k be the output of the k th node of the output layer, thenwhere w jk is the weight between jth node in hidden layer and kth node in output layer. In (2), O k is unbounded as w jk can take any value. For classification problems, desired output lies in. So, we add a standard sigmoidal function to each output node.For classification problem, (3) is used and for regression, (2) is used. We can rewrite (1) aswhere C i j  exp fx i  u ij  2 =r 2 j g:Value of C i j depends only on the ith feature of the input. To eliminate the features which have derogatory effect on the classification/regression problem, as done in, we design C i j as:The term, (1  e b 2 i ), is called the feature attenuator. When b i approaches 0, C i j approaches 1 and therefore C i j has almost no effect on / j. When b i is high, the feature attenuator approaches 1 thus there is almost no change to the value of C i j. The value of b i is learnt along with w jk during training through back propagation algorithm. The architecture of the FSRBF is shown in.
Learning rulesLet the desired output label associated with a data point be d  d 1 ; d 2 :::; d c  T. Thus, the instantaneous error for a data point x isWe use gradient descent technique to learn w jk and b i .w jk t  1  w jk  g w dE=dw jk tb i t  1  b i  g b dE=db i t ;where g w and g b are learning rates. For initial assignment of centres to the hidden nodes, we use the k-means clustering of the training dataset. After the k-means clustering, the spread (r) value for each RBF is chosen as the minimum distance between its cluster centre and all the other cluster centres. The connection weights between the hidden layer and the output layer are initialized with random values in [0.5, 0.5]. When total error on all training samples goes below a predefined tolerance, we terminate the learning.
DatasetWe have used the same dataset reported inMCI' dataset we mean these three test datasets, respectively. Out of 47 subjects diagnosed with MCI at blood draw, 22 converted to AD within 25 years (MCI ! AD), eight converted to OD (MCI ! OD), whereas 17 were still diagnosed as MCI, 46 years later (MCI ! MCI) (). The datasets are available at http://www.nature.com/nm/journal/v13/n11/suppinfo/ nm1653_S1.html.
Experiment designWe do a 5-fold cross-validation (CV) on the training data for architecture selection for FSRBF. For each fold we run FSRBF 10 times and in each run we initialize the w jk and initial centres of k-means clustering randomly. We take the result averaged over these 10 runs as the final result of that fold, thus decreasing the effect of these random initializations. The number of hidden nodes is varied between 2 and 30. FSRBF is run 10 times on the best architecture selected from the 5-fold CV to get 10 sets of b values. The b values are then averaged over different runs. FSRBF assigns an importance (weight) to each feature during the training in terms of b values. The features are sorted in descending order of the values of b. We need to put a threshold either on the number of features to be selected or on the value of b to select the features. Features for which b value is <0.1135 produce component basis function value > 0.95 even at 2r distance, therefore, do not have much effect on / j (). Here also we choose m features (proteins) which have average b value > 0.1135. To further condense the feature set, we take the feature with the highest average b value and do a 5-fold CV for RBF with different architectures. We increase the number of features by one, i.e. take the two features with highest average b value and again do a 5-fold CV for RBF with different architectures. This process is repeated until 5-fold CV is done on all m features. We select the number of features f 0 for which validation result is the best. This approach is briefly depicted in. Note that this may not be the only or the best strategy. This is one of the strategies to select a small set of proteins for early detection of AD. Before applying FSRBF on AD data, we test FSRBF on some synthetic datasets with known characteristics.
Results
Performance on synthetic dataWe generate five types of synthetic data. We name them 'A', 'B', 'C', 'D' and 'E'. To test whether the feature selection algorithm can recognize the situation when none of the features carry any class information, we create the dataset A. All the four features of dataset A are assigned random values. The class labels are also assigned randomly to the samples. Therefore, in dataset A, none of the features is important. To test the feature selection algorithm on cases where the classes are linearly separable, we create datasets B and C. We also test the algorithm's efficiency on identifying correlation between features. The first and the second features of dataset C are highly correlated, correlation coefficient being 0.97. We also need to test the cases where the classes are not linearly separable. Therefore, datasets D and E are created. Whereas datasets B and C are linearly separable, dataset D cannot be separated by a single straight line. Dataset D resembles XOR data in appearance. In dataset E, samples belonging to one class surround the other class. Therefore, the classes are not linearly separable. The scatter plots of the datasets are displayed in. We also test the behaviour of FSRBF with increasing number of noisy features. We compare our results with two state-of-the-art filter methods: kernelPLS () and ReliefF () and a wrapper method SVMRFE () in terms of type I (false positive) and type II (false negative) errors. For dataset A where none of the four features carry class information, FSRBF selects two features incurring type I error, whereas KernelPLS and SVMRFE select all the four features as useful incurring a high type I error. Only ReliefF assigns low weights to all the features resulting in zero type I and type II error. For linearly separable dataset B all feature selection methods select the right feature. For dataset C that contains correlated features, the three methods FSRBF, KernelPLS and ReliefF select all important (including correlated) features. None of the unimportant features get selected even in the presence of 48 noisy features (zero type I and type II errors). Though SVMRFE is able to recognize correlated features, as the number of noisy features increases, SVMRFE tends to select some unimportant features and discards some important features. For dataset D, FSRBF and for dataset E, FSRBF and ReliefF select only the important features (zero error). KernelPLS and ReliefF for dataset D and KernelPLS for dataset E select only some unimportant features discarding both the important features. Therefore, type I and type II errors both become high. As the number of noisy features increases, SVMRFE, for datasets D and E, usually discards one of the important features and selects one of the unimportant features. All the four feature selection methods under consideration assign a numerical importance (b for FSRBF) to each feature. For an ideal feature selection method, it is expected that the important features be assigned the highest weights and therefore the highest ranks even in presence of noisy features. We analyse in, the change of rank of the two important features of datasets D and E (with nonlinearly separable classes) with increasing number of noisy features for the four competing feature selection methods. We are more interested in datasets D and E as all the four competing methods produce zero error only for linearly separable datasets. The three methods KernelPLS, ReliefF and SVMRFE produce high or moderate type I and type II errors for non-linearly separable datasets D and E whereas FSRBF produces zero type I and type II errors in non-linear cases also. Fromit can be seen that ReliefF maintains the highest ranks for both the important features, feature 1 and feature 2, for dataset E, but fails to maintain the same for dataset D. For both the datasets D and E, only FSRBF consistently maintains thehighest ranks (1 and 2) for the two important features, even in presence of noisy features. Note that these results are average over 10 runs of FSRBF, where the network architecture is selected using 5-Fold CV, but in some individual runs FSRBF may not maintain the highest ranks for features 1 and 2. Except for dataset A, FSRBF produces zero error for all the cases. Given the application of feature selection in identifying biomarkers of critical biological phenomena (e.g. existence of diseases like Alzheimer, cancer), it is important that none of the features that carry important class information gets discarded. Our limited experiments with simple synthetic datasets reveal the success of FSRBF in this regard, but this does not mean that this will always be the case with real life datasets. Further results on synthetic datasets can be found in Section 1 of the Supplementary Material. Next we apply FSRBF on AD dataset.
Performance on AD dataOur architecture selection scheme suggests an FSRBF with 13 hidden nodes (validation accuracy 90.25%). Therefore, we run FSRBF NN with 13 hidden nodes on AD training data. We have executed FSRBF on an 80 core computing system involving @2.13 GHz Xeon(R) E7-L8867 processors with 512 GB RAM. Note that though the above-mentioned system is a multi-core multiprocessor system we did not explicitly exploit the parallel processing capability of the system. The CPU time (using only single core of a single processor) for one run of FSRBF with 500 iterations for the parameter updates (Equations 8 and 9) is 20.14 s. It is worth noting that there is a natural parallelism in an RBF network. The computations done at the hidden nodes can be done in parallel which can drastically reduce the computation time. Using the average b values obtained after applying the first four steps in, we find 34 signalling proteins with b value > 0.1135. To condense the feature set further, 5-fold CV on n signalling proteins (sorted according to b values) where n  1; 2; :::; 34 is performed.shows the change of training and validation error with increasing number of signalling proteins. It is seen inthat validation error is least for nine signalling proteins. We select these nine signalling proteins as our final set of predictors. The selected nine proteins along with their importance in terms of b values averaged over 10 runs of FSRBF are shown in. Seven predictors out of these nine are common with 18 predictors reported by. The remaining two (marked in bold font in) are new findings with FSRBF. Correlation analysis reveals that the maximum correlation between any two of the nine proteins is 0.39 and five of the nine proteins have high correlation (>0.61) with one or more proteins from the remaining set of 111 proteins. That is FSRBF has selected only one of the several highly correlated and important proteins. The details are given in Section 2 of the Supplementary Material.
Prediction result with the selected proteinsTo test the usefulness of the features selected by FSRBF, we subject the AD dataset with selected features to different classifiers, e.g. RBF, SVM with linear kernel, SVM with RBF kernel and MLP. For selection of an appropriate architecture/hyperparameters for these classifiers, we do a 5-fold CV on the training dataset. Each classifier is trained with the 'train' dataset using both sets of features (nine features found by FSRBF and 18 features found by) and tested on the four datasets, 'train', 'first test', 'OD' and 'MCI'.These results are depicted in.indicates that for most of the classifiers, the set of nine proteins improves the classification accuracies of the MCI dataset (47 samples) into AD and nonAD classes by $4%. On the other hand, for most classifiers, the set of 18 proteins better classifies the OD dataset (11 samples) by $18%. Therefore, when the set of 18 proteins performs better, the improvement in performance appears higher than the cases when the set of nine proteins perform better. But, the OD dataset has only 11 samples and hence $18% better classification amounts to just two extra correct classifications. Moreover, we have identified a much smaller set of uncorrelated proteins which leads to low computational complexity and may result in decreased diagnostic expenses. We draw a heat-map using our nine proteins in. Observe that most AD samples, shown in blue, form a cluster in the middle and the non-AD samples form two clusters on the two sides. This pattern shows that our nine proteins form AD-specific signature. A heat-map for the 18 proteins identified inis shown in Supplementaryand is compared in Supplementary Section 2 with the heat-map presented in. To compare the efficiency of FSRBF with that of kernelPLS, ReliefF and SVMRFE, we run them on the same training set (Section 3.2) for AD-specific feature selection. Each feature selection method assigns a numerical value to each feature according to the importance of the feature. For each method we have considered the top 20 features and then identified the features that are common to all four sets. We have found eight such common biomarkers. Of these eight proteins, seven are present in the set of 18 reported in. These seven are shown inin non-bold font. The biomarker FAS which is selected by FSRBF as the ninth important feature, but not reported in, is also selected by ReliefF, kernelPLS and SVMRFE. For the next set of experiments, we train the classifiers with the train set of 83 samples and test the performance on the first test set of 81 samples (Section 3.2).compares the performances of the features selected by different methods. Different metrics: recall/sensitivity, specificity, precision and F-score have been used for performance comparison. These metrics are plotted against increasing number of features as selected by different feature selection methods. The classifier used is SVM with linear kernel. We choose LIBSVM implementation of SVM with default value (0.5) of theshows the experimental results in terms of F-score. Note that high F-score indicates both high sensitivity and high precision. For 14 proteins identified by, addition of TGF-b and FAS increases the F-score. It is observable that each of the proteins MCP-3, PARC, ICAM, IGFBP-6 and IL11 (feature indices 5, 8, 14, 15 and 16, respectively, in) identifies all the samples as non-AD, resulting in zero sensitivity and thus no F-score. TGF-b and FAS, when added to each of these five proteins, results in a significant F-score.
PCA and Sammon mapping of selected proteinsTo further analyse the characteristics of the selected nine proteins, we perform principal component analysis (PCA). Experiments show that for our nine proteins, 92.69% variance is concentrated in seven principal components (PCs) whereas, for 18 proteins, 91.12% variance is concentrated in nine PCs.shows the scatter plot of the first test dataset in 2D principal domain of our nine proteins and Supplementaryshows similar scatter plots for the train set and the rest two test sets: OD and MCI. For a fair comparison, Supplementaryalso shows similar scatter plots in 2D principal space for the 18 proteins reported by. The two figures reveal that clustering capability of both the protein sets (nine and 18) is comparable. Since we have used non-linear architecture for selection of plasma signalling proteins, linear PCA may not reflect the appropriate discriminating power of the selected predictors. So, we have also explored non-linear Sammon mapping. The 2D Sammon projection of the first test set using nine signalling proteins and that of the 18 signalling proteins (reported by) are shown inand Supplementary, respectively. For the sake of completeness, the 2D Sammon projections of the training set and the remaining two test sets, OD and MCI, using our nine signalling proteins as well as using the set of 18 signalling proteins (reported in) are also shown in Supplementary. In the two scatter plots of, especially in, two distinct clusters corresponding to AD and non-AD samples can be observed.
Identifying incipient AD from MCI samplesThe last columns of the bar-graphs in, and Supplementaryand h, show that both the set of nine and the set of 18 signalling proteins are not good enough to identify those pre-symptomatic individuals with MCI who will eventually suffer from AD. To identify the bio-markers that can single out the MCI patients who will gradually convert to AD, we do the feature selection experiments afresh on the MCI data. Since limited numbers (47) of samples are available, we do the experiments in 10 folds. First, the set of 47 MCI data samples is divided into 10 parts say, D 1 ; D 2 ; :::; D 10. We use each D i as the test data and the remaining data T i  [ j6 i D j as the training data. We perform 10-fold CV on T i to choose the architecture with the highest validation accuracy. Once the architecture for T i is selected, FSRBF is run on T i 10 times with different initializations and tested on D i as test data. So we get 10 sets of b values for each D i. So for 10 iterations, we get a total of 100 sets of b values. The features (signalling proteins) for which b value is >0.1135, are chosen from each set. Then we select 29 signalling proteins for which frequency count is >50. The frequency of the selected signalling proteins in these 100 sets of b values is shown in. Note that this set of 29 proteins is not necessarily the minimal set of plasma proteins that carry AD specific signature in MCI patients. The frequency of occurrence of these proteins being more that 50 in 100 runs of FSRBF indicates that these proteins may carry important AD specific signature in MCI patients. A heat-map along with a dendrogram constructed using 'correlation' distance and 'average' linkage for the 29 proteins is shown in Supplementary
Discussion
Biological relevance of the selected proteinsIn Section 4, we notice that seven of our nine proteins selected from AD 'train' set are the same as those reported inshows that different subsets of this set of 29 proteins are found to play significant roles in different biological processes that are related to AD and/or other neurological brain disorders. This is discussed in detail in Section 3 of the Supplementary Material. Some regulatory protein interaction networks for different biological processes created using gene mania online (http://www. genemania.org/) and defined by the 29 plasma proteins are shown in Supplementary. The result of DAVID query (http://david. abcc.ncifcrf.gov/conversion.jsp) for specific biological processes controlled by the indicated 29 proteins is shown in Supplementary. Literature further reveals that many of the 29 proteins as listed inare mentioned to play important roles either in distinguishing AD and non-AD disorders or brain injury or related diseases. For example, in connection with dementia/AD we find mention of IFGBP-2 in;
Pros and cons of FSRBFAlthough we have used FSRBF for discovering genetic markers for AD, the FSRBF algorithm described in Section 3.1 is a general algorithm for feature selection, which judiciously combines the feature selection process and the universal function approximation capability of RBF NN. Therefore, FSRBF can be used for any feature selection task including biological data as well as data where classes are not linearly separable. FSRBF can also be used for function approximation type problems. While selecting features, FSRBF exploits the non-linear interactions among features and also that between the features and the target outputs. This is a unique advantage of FSRBF and that is why it can yield better results compared with several filter-based methods. However, the results of FSRBF depend on the initialization and this dependence may become more prominent when we have many correlated features because FSRBF cannot control the level of redundancy among the set of selected features. Moreover, since the present version of FSRBF uses Euclidean distance in the activation function of the hidden layer nodes, if the data are in really very high dimension, FSRBF may not yield the most desired results. For such datasets a hierarchical version of FSRBF may be developed, which we plan to investigate in future.
ConclusionsIn this study, we have used FSRBF, an adapted version of the GFSRBF neural network for selection of plasma signalling proteins that can predict clinical AD. Compared with a state-of-the-art method, FSRBF finds a smaller set of plasma proteins exhibiting comparable power in discriminating Alzheimer's patients from NDC. FSRBF is also found to be very effective in finding a set of plasma signalling proteins that can distinguish incipient AD from MCI. The biological relevance of the selected proteins in AD and MCI is discussed. The utility of the selected proteins is further established using PCA, heat-map and Sammon projections. FSRBF is a general purpose tool and can be used for finding markers for other diseases as well as for non-biological numeric data. Conflict of Interest: none declared.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
S.Agarwal et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
