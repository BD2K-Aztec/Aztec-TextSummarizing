Motivation: Mired by its connection to a well-known N P-complete combinatorial optimization problem—namely, the Shortest Common Superstring Problem (SCSP)—historically, the whole-genome sequence assembly (WGSA) problem has been assumed to be amenable only to greedy and heuristic methods. By placing efficiency as their first priority, these methods opted to rely only on local searches, and are thus inherently approximate, ambiguous or error prone, especially, for genomes with complex structures. Furthermore, since choice of the best heuristics depended critically on the properties of (e.g. errors in) the input data and the available long range information, these approaches hindered designing an error free WGSA pipeline. Results: We dispense with the idea of limiting the solutions to just the approximated ones, and instead favor an approach that could potentially lead to an exhaustive (exponential-time) search of all possible layouts. Its computational complexity thus must be tamed through a constrained search (Branch-and-Bound) and quick identification and pruning of implausible overlays. For his purpose, such a method necessarily relies on a set of score functions (oracles) that can combine different structural properties (e.g. transitivity, coverage, physical maps, etc.). We give a detailed description of this novel assembly framework, referred to as Scoring-and-Unfolding Trimmed Tree Assembler (SUTTA), and present experimental results on several bacterial genomes using next-generation sequencing technology data. We also report experimental evidence that the assembly quality strongly depends on the choice of the minimum overlap parameter k.
INTRODUCTIONSince the pioneering work of Frederick Sanger in 1975, when he developed the basic DNA sequencing technology, Sanger chemistry has continued to be employed widely () in * To whom correspondence should be addressed. practically all large-scale genome projects. However, whole-genome sequence assembly (WGSA) pipelines in these projects have usually resorted to the shotgun sequencing strategy in order to reconstruct a genome sequence, despite the limitation that Sanger chemistry could only generate moderate-sized reads (around 1000 bp) with no location information. While many recent advances in sequencing technology has yielded higher throughput and lower cost, the limitations imposed by the read lengths (ranging between 35 and 500 bp) still plague the genomics science, forcing it to work with draft-quality, unfinished, genotypic and misassembled genomic data. The problem is, however, complicated by the presence of haplotypic ambiguities, base-calling errors and repetitive genomic sections. Recall that to obtain the input read data, the DNA polymer is first sheared into a large number of small fragments; and then either the entire fragment or just its ends are sequenced. The resulting sequences are then combined into a consensus sequence using a computer program: DNA sequence assembler. It is desired that the consensus has as small a base-level discrepancy with respect to the original DNA polymer as possible. Researchers first approximated the shotgun sequence assembly problem as one of finding the shortest common superstring of a set of sequences (). Although this was an elegant theoretical abstraction, it was oblivious to what biology needs to make correct interpretation of genomic data. In fact, it misses the correct model for the assembly problem for at least three different reasons: (i) it does not model possible errors arising from sequencing the fragments; (ii) it does not model fragment orientation (the sequence source can be one of the two DNA strands); (iii) most importantly it fails in the presence of repeats in the genome. Faced with this theoretical computational intractability (N P-complete), most of the practical approaches for genome sequence assembly were devised to use greedy and heuristic methods that, by definition, restrict themselves to find suboptimal solutions (see). Note that if the DNA was totally random then the overlap information would be sufficient to reassemble the target sequence and greedy algorithms would perform always well (). However, this argument is mostly irrelevant, since the problem is complicated by the presence of various non-random structures, in particular in eukaryotic genomes (e.g. repeated regions, rearrangements, segmental duplications). In the case of human genome, initially two unfinished draft sequences were produced by different methods, one by the International Human Genome Sequencing Consortium (IHGSC) and another by CELERA genomics (CG), with the published IHGSC assembly constructed by the program GigAssembler devised at
G.Narzisi and B.Mishrathe university of California at Santa Cruz (UCSC). As noted, in a recent article by Semple (2003): 'Of particular interest are the relative rates of misassembly (sequence assembled in the wrong order and/or orientation) and the relative coverage achieved by the three protocols. Unfortunately, the UCSC groups were alone in having published assessments of the rate of misassembly in the contigs they produced. Using artificial datasets, they found that, on average 10% of assembled fragments were assigned the wrong orientation and 15% of fragments were placed in wrong order by their protocol (). Two independent (more recent) assessments of UCSC assemblies have come to the similar conclusions'. Resolving these ambiguities requires the development of novel tools that can combine different technologies into one unified assembly framework, specifically combining shortrange information (e.g. provided by sequence reads) together with single-molecule long-range information (e.g. provided by optical maps). However, there is no unanimous agreement, at least within the computer science community, that this problem has exhausted all reasonable methods of attack. For instance, Karp (2003) observed, 'The shortest superstring probleman elegant but flawed abstraction: [since it defines assembly problem as finding] a shortest string containing a set of given strings as substrings. The SCSP problem is N P-hard, and theoretical results focus on constant-factor approximation algorithms  Should this approach be studied within theoretical computer science?' In contrast to the work in computational biology, there have now emerged examples within computer science, where impressive progress has been made to solve important N P-hard problems exactly, despite their worst-case exponential time complexity: e.g. Traveling Salesman Problem (TSP), Satisfiability (SAT), Quadratic Assignment Problem (QAP), etc. For instance, recent work ofdemonstrated the feasibility of solving instances of TSP (as large as 85 900 cities) using branch-and-cut, whereas symbolic techniques in propositional satisfiability (e.g. DPLL SAT solver), employing systematic backtracking search procedure (in combination with efficient conflict analysis, clause learning, nonchronological backtracking, 'two-watched-literals'unit propagation, adaptive branching and random restarts), have exhibited the capability to handle more than a million variables. Inspired by these lessons from theoretical computer science, a novel approach, embodied in SUTTA algorithm, was developed. In the process, several related issues were addressed: namely, developing better ways to dynamically evaluate and validate layouts, formulating the assembly problem more faithfully, devising superior and accurate algorithms, taming the complexity of the algorithms and finally, a theoretical framework for further studies along with practical tools for future sequencing technologies. Because of the generality and flexibility of the scheme (it only depends on the underlying sequencing technologies through the choice of score and penalty functions), SUTTA is capable, at least in principle, of agnostically adapting to various rapidly evolving technologies. It also allows concurrent assembly and validation of multiple layouts, thus providing a flexible framework that combines short-and longrange information from different technologies. The main aim of this article is to elaborate upon the mathematical, algorithmic and technical details of this method. The article also demonstrates SUTTA's feasibility through several in silico experiments using real paired and unpaired data from next-generation sequencing. Two possible overlaps: left overlap is normal (both reads pointing to the same forward direction) right overlap is innie (the second read B is reverse complemented and is pointing in the backward direction). technology. Finally contig size and assembly quality are shown to be critically dependent on the minimum overlap parameter k.
SYSTEM AND METHODS
The sequence assembly problemIn order to naturally derive SUTTA's method, it is best to start with a formulation of the assembly problem as a constrained optimization problem. For this purpose, SUTTA relies on the same notational framework (but a different approach of attack) as that first introduced by Myers (1995). For convenience, an essential set of definitions is summarized below. The output of a sequencing project consists of a set of reads (or fragments) F ={r 1 ,r 2 ,...,r N }, where each read r i is a string over the alphabet = {A,C,G,T }. To each read is associated a pair of integers (s i ,e i ),i where s i and e i are, respectively, the starting and ending points of the read r i in the reconstructed string R (to be generated by the assembler), such that 1  s i ,e i |R|. The order of s i and e i encodes the orientation of the read (whether r i was sampled from Watson or Crick strand). The overlaps (best local alignment) between each pair of reads may be computed using the SmithWaterman algorithm () with match, mismatch and gap penalty scores dependent on the errors introduced by the sequencing technology. Exact string matching is instead used for short read from next-generation sequencing, since they usually provide high coverage, thus allowing tolerance to an increased false negatives. Note also that by restricting to exact matches only, the time complexity of the overlap detection procedure is reduced from a quadratic to a linear function of the input size. 1 The complete description of an overlap  is given by specifying: (i) the substrings .Aof the two reads that are involved in the overlap; (ii) the offsets from the left-most and right-most positions of the reads .A hang and .B hang ; (iii) the relative directions of the two reads: normal (N), innie (I); (iv) a predicate suffix  (r) on a read r such that: suffix  (r) = true iff suffix of r participates in the overlap  false iff prefix of r participates in the overlap Figure 1 illustrates two possible overlaps (normal and innie). Note that a right arrow represents a read in forward orientation, conversely a left arrow represents a read that is reverse complemented. Definition 1 (Layout). A layout L induced by a set of reads F ={r 1 ,r 2 ,...,r N } is defined as:Informally a layout is simply an ordered sequence of reads connected by overlap relations. Note that the order of the reads in L is a permutation of the reads in F. The previous definition assumes that there are no containments 2 ; without loss of generality, contained reads can be initially removed (in a preprocessing step) and then reintroduced later after the layout has been created. Among all the possible layouts (possibly, super-exponential in the Page: 155 153160
SUTTAnumber of reads), it is imperative to efficiently identify the ones that are consistent according to the following definition: Definition 2 (Consistency property). A layout L is consistent if the following property holds for i = 2,...,N 1:The consistency property imposes a directionality to the sequence of reads in the layout. The estimated start positions for each read are given by:Consistent layouts must also satisfy the following property:Note that in practice the maximum error rate is used during the overlap computation to filter only detected overlaps between two reads r 1 and r 2 whose number of errors is no more than (|r 1 |+|r 2 |). Equipped with this set of definitions, the sequence assembly problem is formulated as follows:Each of these properties plays an important role in resolving problems that arise when real genomic data is used (e.g. data containing repeat-regions, rearrangements, segmental duplications, etc.). Note that, in the absence of additional information, among all possible layouts the minimum length layout is typically preferred (shortest superstring), although this choice is difficult to justify. As the genomic sequence deviates further and further from a random sequence, normally minimum length layout starts introducing various misassembly errors (e.g. compression, insertions, rearrangements, etc.). Note that, traditionally, assemblers have only optimized/approximated one of the properties [i.e., listed above, while checking for the others in a post-processing step. SUTTA, in contrast, views this problem as a constrained optimization problem with the feasible region, determined by the consistent layouts. It converts the group of constraints into appropriate score functions and uses them in combination to search for the optimal layout. This article illustrates these ideas with all but (OM) constraints, which will be described in a sequel. Finally note that this list of constraints is not exhaustive and it will likely change from year to year as new sequencing technologies become available and new types of long-range information become possible to produce. It is thus important to have an assembly framework that could dynamically and effortlessly adapt to the new technologies.
SUTTA algorithmTraditional graph-based assembly algorithms use either the overlap-layoutconsensus (OLC) or the sequencing-by-hybridization (SBH) paradigm, in which first the overlap/DeBruijn graph is built and the contigs are extracted later. SUTTA instead assembles each contig independently and dynamically one after another using the Branch-and-Bound (B&B) strategy. Originally developed for linear programming problems (), B&B algorithms are well-known searching techniques applied to intractable (N P-hard), combinatorial optimization problems. The basic idea is to search the complete space of solutions. However, the caveat is that explicit enumeration is practically impossible (i.e. has exponential time complexity). The tactics honed by B&B is to limit itself to a smaller subspace that contains the optimumthis subspace is determined dynamically through the use of certain well chosen score functions. B&B has been successfully employed to solve a large collection of complex problems, whose most prominent members are TSP (traveling salesman problem), MAX-SAT (maximal satisfiability) and QAP (quadratic assignment problem). The high level SUTTA pseudocode is shown in Algorithm 1. Here, two important data structures are maintained: a forest of double-trees (D-tree) B and a set of contigs C. At each step, a new D-tree is initiated from one of the remaining reads in F. Once the construction of the D-tree is completed, the associated contig is created and stored in the set of contigs C. Next the layout for this contig is computed and all its reads are removed from the set of all available reads F. This process continues as long as there are reads left in the set F. Note that for the sake of a clear exposition, both the forest of D-trees B and the set of contigs C are kept and updated in the pseudocode; however, after the layout is computed, there is no particular reason to keep the full D-tree in memory, especially, where memory requirements are of concern.Finally, note that the proposed Algorithm 1 is input order dependent. SUTTA adopts the policy to always select the next unassembled read with highest occurrence as seed for the D-tree [also used by Taipan;. This strategy has the property to minimize the extension of reads containing sequencing errors. However, empirical observations indicate that changing the order of the reads rarely affects structure of the solutions, as the relatively longer contigs are not affected. An explanation for this can be obtained through a probabilistic analysis of the data and a 01 law resulting from such an analysis.
Overlap score (weighted transitivity)Like any B&B approach, a major component of SUTTA algorithm is the score function used to evaluate the quality of the candidate solutions that are dynamically constructed using the B&B strategy. SUTTA employs an overlap score based on the following argument. Large de novo sequencing projects typically have coverage higher than three, and this implies that frequently two overlapping regions of three consecutive reads in a region of correct layout share intersections. Events of this type are 'witness' to a transitivity relation between the three reads and they play an important role in identifying true positive 3 overlaps with high probability.shows an example of transitivity relation between three reads A,B and C. During contig layout construction, the overlap score uses the following basic principle to dynamically compute the score value of a candidate solution: if Page: 156 153160read A overlaps read B, and read B overlaps read C, SUTTA will score those overlaps strongly if in addition A and C also overlap: if((A,B)(B,C))then{S((A,B,C))
G.Narzisi and B.MishraNote that the score of a single overlap corresponds to the score computed by the SmithWaterman alignment algorithm (for long reads) or exact matching (for short reads). Clearly the total score of a candidate solution is given by the sum of the scores along the overlaps that join the set of reads in the layout L plus the score of the transitivity edges (if any):where O and T are respectively the set of overlaps and transitivity edges (with respect to the set of reads, defined by the layout L) and S() is the score (SmithWaterman, exact match, etc.) for the overlap. This step in SUTTA resembles superficially to the Unitig construction step in overlap-layout-consensus assembler, carried out by removing 'transitive' edges. However, unlike SUTTA, in the overlap-layout-consensus approach the weights of the overlaps are ignored in meaningfully scoring the paths. Since Unitig construction can be computationally expensive, largescale assemblers like CELERA have adopted the best-buddy algorithm, where Unitigs are computed as chains of mutually unique best buddies (adjacent reads with best overlap between each other). Finally, it must be noted that the overlap scores are insufficient to resolve long repeats or haplotypic variations. The score functions must be augmented with constraints (formulated as reward/penalty terms) arising from mate-pair distance information or optical map alignments.
Node expansionThe core component of SUTTA is the B&B procedure used to build and expand the D-tree (create_double_tree() procedure in Algorithm 1). The high-level description of this procedure is as follows:(1) Start with a random read (it will be the root of a tree; use only the read that has not been used in a contig yet, or that is not contained).Create RIGHT Tree: start with an unexplored leaf node (a read) with the best score value; choose all its non-contained right-overlapping reads (Extensions() procedure in Algorithm 2); filter out the set of overlapping reads by pruning unpromising directions (procedures in Algorithm 2); expand the remaining nodes by making them its children; compute their scores. (Add the 'contained' nodes along the way, while including them in the computed scores; check that no read occurs repeatedly along any path of the tree.) STOP when the tree cannot be expanded any further.available: if at the end of the pruning process there are still multiple directions to follow the branching is either terminated (conservative) or not (aggressive). In the aggressive case, the algorithm chooses the direction to follow with the highest local overlap. Algorithm 2 is applied twice to generate LEFT and RIGHT trees from the start read. Next, to create a globally optimal contig, the best LEFT path, the root and the best RIGHT path are concatenated together.illustrates the steps involved in the construction of a contig. The amount of exploration and resource consumption is controlled by the two parameters K and T : K is the max number of candidate solution allowed in the queue at each time step, while T is the percentage of top ranking solutions compared with the current optimum score. At each iteration, the queue is pruned such that its size is always  max(K,T |Q|), where |Q| is the current size of the queue. Note that while K remains fixed at each iteration of Algorithm 2, the percentage of top ranking solutions dynamically changes over time. As a consequence, more exploration is performed when many solutions evaluate to nearly identical scores.
Algorithm 2: Node expansionInput: Start read r 0 , max queue size K, percentage T of top ranking solutions, dead-end depth W de , bubble depth W bb , mate-pair depth W mp Output: Best scoring leaf V := ; /* Set of leaves *//* Get the best scoring node */
Search strategyA critical component of any B&B approach is the choice of the search strategy used to explore the next subproblem in the tree. There are several Page: 157 153160
SUTTAvariations among strategies (with no single one being universally accepted as ideal), since these strategies' computational performance varies with the problem type. The typical trade-off is between keeping the number of explored nodes in the search tree low and staying within the memory capacity. The two most common strategies are Best First Search (BeFS) and Depth First Search (DFS). BeFS always selects among the live (i.e. yet to be explored) subproblems, the one with the best score. It has the advantage to be theoretically superior since whenever a node is chosen for expansion, a best-score path to that node has been found. However it suffers from memory usage problems, since it behaves essentially like a Breadth First Search (BFS). Also checking repeated nodes in a branch of the tree is computationally expensive (linear time). DFS instead always selects among the live subproblems the one with largest level (deepest) in the tree. It does not have the same theoretical guarantees of BeFS but the memory requirements are now bounded by the product of the maximum depth of the tree and the branching factor. The other advantage is that checking if a read occurs repeatedly along a path can be done in constant time by using the DFS interval schemes. For SUTTA, we use a combined strategy: using DFS as overall search strategy, but switching to BeFS, when choice needs to be made between nodes at the same level. This strategy can be easily implemented by ordering the set of live nodes L of Algorithm 2 using the following precedence relation between two nodes x and y:depth(x) > depth(y) or depth(x) == depth(y)score(x) > score(y) ,where depth is the depth of the node in the tree and score is the current score of the node (defined in section 2.3). Because BeFS is applied locally at each level, the score is optimized concurrently.
Pruning the TreeTransitivity pruning: the potentially exponential size of the D-tree is controlled by exploiting certain specific structures of the assembly problem that permit a quick pruning of many redundant and uninformative branches of the treesurprisingly, substantial pruning can be done only using local structures of the overlap relations among the reads. The core observation is that it is not prudent to spend time on expanding nodes that can create a suffix-path of a previously created path, as no information is lost by delaying the expansion of the last node/read involved in such a 'transitivity' relation. This scenario can happen every time there is a transitivity edge between three consecutive reads (see), and it is further illustrated inwith an example. Suppose that A,B 1 ,B 2 ,...,B n are n+1 reads with a layout shown in. The local structure of the D-tree will have node A with n children B 1 ,B 2 ,...,B n. However , since B 1 also overlaps B 2 ,B 3 ,...,B n , these nodes will appear as children of B 1 at the next level in the tree. So the expansion of nodes B 2 ,B 3 ,...,B n can be delayed because their overlap with read A is enforced by read B 1. Similar argument holds for nodes B 2 ,B 3 ,...,B n. In the best scenario, this kind of pruning can reduce a full tree structure into a linear chain of nodes. Additional optimization can be performed by evaluating the children according to the following order (h 1  h 2    h n ), where h i is the size of the hang 4 for read B i. This ordering gives higher priority to reads with higher overlap score. This explains how the Transitivity() procedure from Algorithm 2 is performed. Zig-zag overlaps mapping: although based on a simple principle, the time complexity of the transitivity pruning is a function of how quickly it is possible to check the existence of an overlap between two reads (corresponding to the dashed arrows of). The general problem is the following: given the set of overlaps O (computed in a preprocessing step) for a set of reads F, check the existence of an overlap (or set of overlaps) for a pair of reads (r 1 ,r 2 ). The naive strategy that checks all the possible pairs takes time O(n 2 ) where n =|O|. If a graph-theoretic approach is used, 4 Size of the read portion that is not involved in the overlap.The idea is to build a hash table, in which a pair of reads is uniquely encoded to a single location of the table by using the following hash function:where a and b are the unique identification numbers of the two reads. This is the well-known zig-zag function, which is the bijection often used in countability proofs. The number of possible overlaps |H(a,b)| between two reads is always bounded by some constant c, which is a function of the read length, genome structure (e.g. number of simple repeats) and the strategy adopted for the overlap computation (SmithWaterman, exact match, etc.). In practice, the costant c is never too large because, even when multiple overlaps between two reads are available (typically 4), only a small subset with a reasonably good score (i.e. above a threshold) is examined by the algorithm.
LookaheadMate-pairs: had the overlapping phase produced only true positive overlaps, every overlapping pair of reads would have been correctly inferred to have originated in the same genomic neighborhood, thus turning the assembly process to an almost trivial task. However, this is not the casethe overlap detection is not error free and produces false positive or ambiguous overlaps abundantly, specially when repeat regions are encountered. A potential repeat boundary between reads A, B and C is shown in. Read A overlaps both reads B and C, but B and C do not overlap each other. Thus, the missing overlap between B and C is the sign of a possible repeat-boundary location, making the pruning decisions impossible. However, SUTTA's framework makes it possible to resolve this scenario by looking ahead into the possible layouts generated by the two reads, and keeping the node that generates the layout with the least number of unsatisfied constraints (i.e. consistent with mate-pair distances or restriction fragment lengths from optical maps). SUTTA's implementation generates two subtrees: one for node B and the other for C (see). The size of each subtree is controlled by the parameter W mp , the maximum height allowed for each node in the tree. The choice of W mp is both a function of the size of the mate-pair library, local genome coverage and the genome structure. For genomes with short repeats, a small value for W mp is sufficient to resolve most of the repeat boundaries, and can be estimated from a kmer analysis of the reads. However, some genomes have much higher complexity (family of LINEs, SINEs and segmental duplications with varying homologies), in that case a higher value of W mp is necessary, but can be estimated adaptively. Once the two (or occasionally more) subtrees are constructed, the best path is selected based on the overlap score and the quality of each path is evaluated by a reward/penalty function corresponding to mate-pair constraints. For each node in the path, its pairing mate (if any) is searched to collect only those mate-pairs, crossing the connection point between the subtree and the full tree, which are then Page: 158 153160. Lookahead: the repeat boundary between reads B and C is resolved looking ahead in the subtree of B and C, and checking how many and how well the mate-pair constrains are satisfied. scored by the following rule:
G.Narzisi and B.MishraHere l is the distance between the two reads in the layout,  and  are the mean and SD of the mate-pair library,  is a parameter that controls the relaxation of the mate-pair constrains (in the results, fixed at  = 6), and r 1  r 2 denotes that the two reads are oriented toward each other (with the 5 ends farthest apart). Such a score can be easily shown to give higher value to layouts with as few unsatisfied constraints as possible. Note that the mate-pair score is also dependent on local coverage of the reads, so its value should be adjusted/normalized to compensate for the variation in coverage. By penalizing the score negatively and positively according to the constraints, the current formulation assumes uniform coverage. However more sophisticated score functions could be employed, if it is necessary to precisely quantify the extent to which the score varies with coverage. The mate-pair score f of the full path P is given by the sum of the scores of each pair of reads with feasible constraints in P:Note that the current formulation of S MP models only mate-pairs libraries whose reads face against each other. However, most current assemblies use a mixture of paired-end and mate-pair datasets that differ in insert size and read pair orientation. SUTTA's mate-pair score can be easily adapted to support any read pair orientation and insert size. Memory management is very important during lookahead: the subtrees are dynamically constructed and their memory deallocated as soon as the repeat boundary is resolved. Also note that the lookahead procedure is performed every time a repeat boundary is identified, so the extra work associated with the construction and scoring of the subtrees is performed only when repeated regions of the genome are assembled. Finally note that, the construction of each subtree follows the same strategy (from Algorithm 2) and uses the same overlap score (defined in section 2.3); however, recursive lookahead is not permitted. The mate-pair score introduced in (9) is used only to prune one of the two original nodes under consideration (or both, in the rare but possible scenarios, where neither of the subtrees satisfies the mate-pair constraints). This explains how the MatePairs() procedure from Algorithm 2 is performed.
Dead-ends and bubbles: base pair errors in short reads form next-generation sequencing produce an intuitively non-obvious set of erroneous paths in the graph and tree structures. Because perfect matching is used to compute the overlaps, according to where the base error is located two possible ambiguities need to be resolved: dead-ends and bubbles. Dead-ends consist of short branches of overlaps that extend only for very few steps and they are typically associated with base errors located close to the read ends. Bubbles instead manifest themselves as false branches that reconnect to a single path after a small number of steps. They are typically caused by single nucleotide difference carried by a small subset of reads. The lookahead procedure is easily adapted to handle these kind of structures. Specifically for dead-ends, each branch is explored up to depth W de and all the branches that have shorter depth are pruned. In the case of bubbles, both branches are expanded up to depth W bb and, if they converge, only the branch with higher coverage is kept and the other one pruned.
RESULTSWe have compared SUTTA to several well-known short read assemblers on three real datasets from Illumina next-generation sequencing data using both mated and unmated reads. The following assemblers are used in the comparison: Edena 2.1.1 (), Velvet 1.0.13 (), Taipan 1.0 (), ABySS 1.2.3 (), SSAKE 3.6 () and EULER-SR 1.1.2 (). Although these datasets do not represent the state of the art in sequencing technology (for example, Illumina can currently generate longer reads up to 100 bp), they have been extensively analyzed by previously published short read assemblers.). The experimental results show that SUTTA has comparable performance to the best state-of-the-art assemblers based on contig size comparison. This comparison is to be interpreted in the context of our experimental evidence that the choice of the minimum overlap parameter k affects both contig size and assembly quality (presented below, see Figs 6 and 7).and 2 present the comparison based on contig size analysis for all three genomes. Only contigs of minimal length 100 bp are considered in the statistics. A contig is classified as correct if it fully aligns to the genome with a minimum base similarity of 98% (for S.aures and H.acininychis) and 95% (for E.coli). Inspecting the results in, it is evident that SUTTA performs comparatively well relative to these assemblers. In particular, SUTTA a , thanks to its aggressive strategy, assembles longer contigs but it pays in assembly quality by generating more misassembly errors. SUTTA c instead behaves more conservatively and generated less errors but without excessively sacrificing contig length. The choice between the aggressive strategy and the conservative one is clearly based on the overall quality of the input set of reads and the genome structure. For example, in the case of an error-free dataset and a genome with few and short repeats, we may opt for an aggressive strategy. In the Page: 159 153160 SUTTAcase of mate-pair data, SUTTA produces shorter contigs compared with ABySS, Velvet and EULER-SR; however, SUTTA's overall assembly quality is superior with fewer and shorter misassembled contigs.
Contig size analysis
Overlap size kThe min overlap length k is a determinant parameter and its optimal setting strongly depends on the data (coverage). This is particularly pronounced for short reads since the required overlapping length represents a significant part of the read length. Therefore, the effective coverage E cov = N(lk) G is significantly sensitive to choice of the minimum overlap parameter k, where N is the number of reads, l is the read length and G is the genome size.shows the relation of the N50 size versus the minimum overlap parameter k for two of the genomic datasets analyzed in this article. Clearly, there is a trade-off between the number of spurious overlaps and lack of overlaps as the values for k move from small to larger numbers. Increasing the overlap allows to resolve more ambiguities, but in turn requires a higher coverage depth to achieve the same N50 value. It is important to emphasize that the optimal value for k depends on the genome structure and coverage (S.aureus and E.coli have different optimal value) and so it needs to be tuned accordingly. Finally, the availability of mate-pairs definitely improves the results and enables assembly of longer contigs for the E.coli genome.
Featureresponse curve dynamicsThe choice of the minimum overlap parameter k not only affects the estimated length of the assembled contigs but also changes the overall quality of the assembled sequences. In order to show this phenomenon, we use a new metric to examine the assembly quality that we call 'Feature-Response' (FR) curve. Similarly to the receiver operating characteristic (ROC) curve, the FR curve characterizes the sensitivity (e.g. coverage) of the sequence assembler as a function of its discrimination threshold (number of features/errors). Features correspond to doubtful regions of the assembled sequence and are computed using the amosvalidate pipeline developed by. Faster the FR grows better is the assembly quality, because higher genome coverage is achieved with less errors (see the Supplementary Material for more details on the FR curve).shows the dynamics of the FR curve for E.coli as a function of the minimum overlap parameter k. Similarly to the plots in, both small and large values of k produce more assembly errors, while the best value lays in the middle range of 2529. There seems to be a phase transition for k = 33 and k = 34, this is due to the fact the probability to detect a perfect match overlap of higher size (k > 32) becomes more unlikely without increasing the coverage. Both average contig length and N50 value decrease such that more contigs of size smaller than the insert size are created. All these contigs then violate the mate-pair constraints and result in a high number of features/errors.
Computational performanceBecause of the theoretical intractability of the sequence assembly problem and because, in principle, SUTTA's exploration scheme could make it generate an exponentially larger number of layouts, SUTTA could be expected to suffer from long running time and high memory requirements. However, our empirical analysis shows that SUTTA has a competitively good performancethanks to the B&B strategy, well-defined scoring and pruning schemes, and a careful implementation. SUTTA's computational performance was compared with Velvet, ABySS, EULER-SR, Edena and SSAKE on the S.aureus genome using a four quad core processor machine, Opteron CPU 2.5 GHz (see the Supplementary Material for the comparative table). SUTTA has an assembly time complexity similar to Edena, SSAKE and EULER-SR. Velvet and ABySS have the best computational performance. Velvet, ABySS and Edena consume less memory than SUTTA; however, note that SUTTA relies on AMOS to maintain various genomic objects (reads, inserts, maps, overlaps, contigs, etc.), which are not optimized for short reads. At the current stage of development, SUTTA is limited to relatively small gnomes and its time complexity increases with mate-pairs constrain computation, but is expected to improve with reengineering planned for the next versions. Finally, note that typically two-thirds of total SUTTA's running time is dedicated to the computation of overlaps, leaving only a one-third of the total time to assemble the contigs.
CONCLUSIONSequence assembly accuracy has now become particularly important in: (i) genome-wide association studies, (ii) detecting new polymorphisms and (iii) finding rare and de novo mutations. New-sequencing technologies have reduced cost and increased the throughput; however, they have sacrificed read length and accuracy by allowing more single nucleotide (base-calling) and indel (e.g. due to homo-polymer) errors. Overcoming these difficulties without paying for high computational cost requires (i) better algorithmic framework (not greedy), (ii) being able to adapt to new and combined hybrid technologies (allowing significantly large coverage and auxiliary long-rage information) and (iii) prudent experiment design. We have presented a novel assembly algorithm, SUTTA, that has been designed to satisfy these goals as it exploits many new algorithmic ideas. Challenging the popular intuition, SUTTA enables 'fast' global optimization of the WGSA problem by taming the complexity using the B&B method. Because of the generality of the proposed approach, SUTTA has the potential to adapt to future sequencing technologies without major changes to its infrastructure: technology-dependent features can be encapsulated into the lookahead procedure and well-chosen score functions.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
See the Supplementary Material for more information about the overlapper. 2 These are reads that are proper subsequences of another read.
The two reads correctly originate from the same place in the genome.
