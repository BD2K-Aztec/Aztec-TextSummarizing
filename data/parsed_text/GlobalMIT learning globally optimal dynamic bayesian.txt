Motivation: Dynamic Bayesian networks (DBN) are widely applied in modeling various biological networks including the gene regulatory network (GRN). Due to the NP-hard nature of learning static Bayesian network structure, most methods for learning DBN also employ either local search such as hill climbing, or a meta stochastic global optimization framework such as genetic algorithm or simulated annealing. Results: This article presents GlobalMIT, a toolbox for learning the globally optimal DBN structure from gene expression data. We propose using a recently introduced information theoretic-based scoring metric named mutual information test (MIT). With MIT, the task of learning the globally optimal DBN is efficiently achieved in polynomial time. Availability: The toolbox, implemented in Matlab and C++, is available at
INTRODUCTIONBayesian network (BN) has found applications in modeling various biological networks including the gene regulatory network (GRN). The two important limitations when applying static BN to these domain problems are: (i) BN does not have a mechanism for exploiting the temporal aspect of time-series data, such as time-series microarray data; and (ii) BN does not allow the modeling of cyclic phenomena, such as feedback loops, which are prevalent in biological systems (). These drawbacks have motivated the development of the so-called dynamic Bayesian network (DBN). Its simplest model, the first-order Markov stationary DBN, assumes that both the structure of the network and the parameters characterizing it remain unchanged over time. The value of a variable at time (t) is assumed to depend only on the value of its parents at time (t 1). DBN not only accounts for the temporal aspect of time-series data (i.e. an inter time-slice edge must always be directed forward in time), but it also allows the modeling of feedback loops. Since its inception, DBN has received particular interest from the bioinformatics community (; Kim * To whom correspondence should be addressed).
METHODMost algorithms to date for learning DBN structure employ a local search strategy, such as hill climbing with random restart, or a meta stochastic global optimization framework such as genetic algorithm or simulated annealing, as exemplified by several softwares such as BANJO () or bnlearn (). This is due to several NP-hardness results in learning static BN structure (see, e.g.). Recently, Dojer (2006) has shown otherwise that learning DBN structure, as opposed to static BN, does not necessarily have to be NP-hard. In particular, it was shown that, under some mild assumptions, there are algorithms using the minimum description length (MDL) and BDe scores, which find the globally optimal network with a polynomial worst-case time complexity. These algorithms have been realized within the BNFinder software (). In our experiments, we observed that BNFinder+MDL is very fast, whereas BNFinder+BDe is very time demanding: a single run on a dataset of 20 genes and 300 observations can take up to a day (). This is in concordance with the theoretical worst-case complexity analysis, where the algorithm would have to exhaustively evaluate all possible parent sets of cardinality from 0 to p * 1. Let k be the number of discrete states of each variable, N be the number of experiments, then for MDL, p * MDL is given by log k N, while for BDe, p * BDe ==N log  1 k, where 0 <<1 is the network complexity penalty parameter (default value log 1 = 1 for BNFinder). In general, p * BDe scales linearly with the number of data items N, making its value of less practical interest, even for very small datasets. Although being more expensive, BNFinder+BDe is still recommended over BNFinder+MDL, 'due to its exactness in the statistical interpretation' (). Further, de Campos (2006) also showed that BDe seems to learn more accurate networks than MDL [which is also equivalent to the Bayesian Information Criterion (BIC)]. Mutual information test (MIT) is a recently introduced scoring metric for learning BN (). To understand MIT, let X ={X 1 ,...,X n } denote the set of n variables with corresponding {r 1 ,...,r n } discrete states, D denote our dataset of N observations, G be a DBN, and Pa i ={X i1 ,...,X is i } be the set of parents of X i in G with corresponding {r i1 ,...,r is i } discrete states, s i =|Pa i |. The MIT score is then defined as:is the mutual information between X i and its parents as estimated from D.  ,l ij is the value such that p( 2 (l ij )   ,l ij ) =  (the chi-square distribution at significance level 1), and the term l i i (j) is defined as:
N.X.Vinh et al.where  i ={ i (1),..., i (s i )} is any permutation of the index set {1...s i } of Pa i , with the first variable having the greatest number of states, the second variable having the second largest number of states, and so on. MIT falls under the same category of information theory-based scores as the MDL, BIC and Akaike Information Criterion (AIC). Briefly speaking, under MIT, the goodness-of-fit of a network is measured by the total mutual information shared between each node and its parents, penalized by a term which quantifies the degree of statistical significance of this shared information. Through extensive experimental validation, de Campos (2006) suggested that, for the task of learning static BN, MIT can compete favorably with Bayesian scores (BDe), outperforms BIC/MDL and should be the score of reference within those based on information theory. However, as opposed to the other popular scoring metrics, to our knowledge MIT has not been considered for DBN learning. In our recent work (), we have shown that under the same set of assumptions made in Dojer (2006), there exists a polynomial worst-case time complexity algorithm for learning the globally optimal DBN structure with MIT. We call this algorithm GlobalMIT. The polynomial worst-case time complexity of GlobalMIT is characterized by:It can be seen that p * MIT depends only on ,k and N. In the worst case, our algorithm will have to examine all the possible parent sets of cardinality from 1 to p * MIT 1. Since there are O(n p * MIT ) subsets with at most p * MIT 1 parents, and each set of parents can be scored in polynomial time, globalMIT admits an overall polynomial worst-case time complexity in the number of variables (see also GlobalMIT user guide within the online supplementary material for further details). Our experimental evaluation inshowed that GlobalMIT is very competitive in terms of network quality. In other words, GlobalMIT seems to combine the strength of both MDL (speed) and BDe (solution quality).
ImplementationThe algorithm involves investigating, for each variable, every potential parent set of increasing cardinality (not exceeding p * MIT 1) until the globally optimal solution has been found. One important observation for the efficient implementation of GlobalMIT is the following decomposition property of the mutual information:This implies that the mutual information can be computed incrementally, and suggests that, for efficiency, the computed mutual information values should be cached to avoid redundant computations. We provide an implementation of the GlobalMIT algorithm as a Matlab toolbox. The toolbox also supports simple data pre-processing functionalities, such as data discretization and mapping, and simple post-processing such as visualization and quality assessment. For improved performance, we also provide an implementation of GlobalMIT in C++. Our experiments showed that the C++ version of GlobalMIT is up to 40 times on average faster than the Matlab version. For seamless and easy use of GlobalMIT, interface modules provide connection between Matlab and the C++ version, allowing the users to perform all operations in Matlab. We experimentally compared the runtime of GlobalMIT to BNFinder () with both the MDL and BDe metrics. The test was carried out on a synthetic dataset of 20 genes  2000 observations, generated from a gene regulatory network (Network No. 1) as described in. On a Core 2 Duo PC with 4 GB of main memory, GlobalMIT C++ and BNFinder + MDL took slightly >1 h to analyze this dataset, while BNFinder+BDe took >3 days. It is noted that currently GlobalMIT only learns DBN with intertime slice edges, i.e. edges from Xj. Learning DBN which allows both inter-and intratime slide edges falls back to an NPhard problem. However, intratime slice edges representing (almost) instantaneous genetic interactions, if of interest, can be learned separately using some BN learning algorithm, then combined with the intertime slice edges, followed by some post-processing for the final result. Our future work includes expanding GlobalMIT in this direction. Also, we are extending our framework to handle edges spanning either two or several time slices, which represent variable, longer time-delayed genetic interactions that are also abundant in genetic networks ().
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
