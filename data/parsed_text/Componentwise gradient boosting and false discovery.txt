Motivation: Technological advances that allow routine identification of high-dimensional risk factors have led to high demand for statistical techniques that enable full utilization of these rich sources of information for genetics studies. Variable selection for censored outcome data as well as control of false discoveries (i.e. inclusion of irrelevant variables) in the presence of high-dimensional predictors present serious challenges. This article develops a computationally feasible method based on boosting and stability selection. Specifically, we modified the component-wise gradient boosting to improve the computational feasibility and introduced random permutation in stability selection for controlling false discoveries. Results: We have proposed a high-dimensional variable selection method by incorporating stability selection to control false discovery. Comparisons between the proposed method and the commonly used univariate and Lasso approaches for variable selection reveal that the proposed method yields fewer false discoveries. The proposed method is applied to study the associations of 2339 common single-nucleotide polymorphisms (SNPs) with overall survival among cutaneous melanoma (CM) patients. The results have confirmed that BRCA2 pathway SNPs are likely to be associated with overall survival, as reported by previous literature. Moreover, we have identified several new Fanconi anemia (FA) pathway SNPs that are likely to modulate survival of CM patients. Availability and implementation: The related source code and documents are freely available at
IntroductionRapid advances in technology that have generated vast amounts of data from genetic or genome studies have led to a high demand for developing powerful statistical learning methods for extracting information effectively. For instance, understanding clinical and pathophysiologic heterogeneities among subjects at risk and designing effective treatment for appropriate subgroups is one of the most active areas in genetic studies. Wide heterogeneities present in patients' response to treatments or therapies. Understanding such heterogeneities is crucial in personalized medicine, and discovery of genetic variants offers a feasible approach. However, serious statistical challenges arise when identifying real predictors among hundreds of thousands of candidates, and an urgent need has emerged for the development of effective algorithms for model building and variable selection. The last three decades have given rise to many new statistical learning methods, including CART (), random forest (), neural networks (), SVMs () and high dimensional regression (). Boosting has emerged as a powerful framework for statistical learning. It was originally introduced in the field of machine learning for classifying binary outcomes (), and later its connection with statistical estimation was established byproposed a gradient boosting framework for regression settings. B hlmann andproposed a component-wise boosting procedure based on cubic smoothing splines for L2 loss functions. B hlmann (2006) demonstrated that the boosting procedure works well in high-dimensional settings. For censored outcome data, Ridgeway (1999) applied boosting to fit proportional hazards models, and Li and Luan (2005) developed a boosting procedure for modeling potentially non-linear functional forms in proportional hazards models. Despite the popularity of aforementioned methods, issues such as false discovery (e.g. seletion of irrelevant SNPs) and difficulty in identifying weak signals present further barriers. Simultaneous inference procedure, including the Bonferroni correction, has been widely used in large-scale testing literature. However, in many highdimensional settings, such as in genetic studies, variable selection is serving as a screening tool to identify a set of genetic variants for further investigation. Hence, a small number of false discoveries would be tolerable and simultaneous inference would be too conservative. In contrast, the false discovery rate (FDR), defined as the expected proportion of false positives among significant tests (), is a more relevant metric for false discovery control under the framework of variable selection. However, few existing variable selection algorithms control false discoveries. This has brought an urgent need of developing computationally feasible methods that tackle both variable selection and false discovery control. We propose a novel high-dimensional variable selection method for survival analysis by improving the existing variable selection methods in several aspects. First, we have developed a computationally feasible variable selection approach for high-dimensional survival analysis. Second, we have designed a random sampling scheme to improve the control of the false discovery rate. Finally, the proposed framework is flexible to accommodate complex data structures. The rest of the article is organized as follows. In Section 2 we introduce notation and briefly review the L 1 penalized estimation and gradient boosting method that are of direct relevance to our proposal. In Section 3 we develop the proposed approach, and in Section 4 we evaluate the practical utility of the proposal via intensive simulation studies. In Section 5 we apply the proposal to analyze a genome-wide association study of cutaneous melanoma. We conclude the article with a brief discussion in Section 6.
Model
NotationLet D i denote the time from onset of cutaneous melanoma to death and C i be the potential censoring time for patient i, i  1;. .. ; n. The observed survival time is T i  minfD i ; C i g, and the death indicator is given by d i  ID i C i . Let X i  X i1 ;    ; X ip  T be a p-dimensional covariate vector (contains all the SNP information) for the ith patient. We assume that, conditional on X i , D i is independently censored by C i. To model the death hazard, considerwhere k 0 t is the baseline hazard function and b  b 1 ;    ; b p  is a vector of parameters. The corresponding log-partial likelihood is given bywhere R i  f' : T ' ! T i g is the at-risk set. The goal of variable selection is to identify S 0  fj : b j 6  0g, which contains all the variables that are associated with the risk of death.
L 1 penalized estimationTibshirani (1997) proposed a Lasso procedure in the Cox model, e.g. estimate b via the penalized partial likelihood optimization ^ b  argmax b fl n b  kjjbjj 1 g;where kk 1 is the L 1 norm. To solve (1), Tibshirani (1997) considered a penalized reweighted least squares approach. Let X  X 1 ;. .. ; X n  be the p  n covariate matrix and define g  X T b. Let l 0 n g and l 00 n g be the gradient and Hessian of the log-partial likelihood with respect to g respectively. Given the current estimator ^ g  X T ^ b, a two-term Taylor expansion of the log-partial likelihood leads to l n b % 1 2 z^ g  X T b T l 00 n ^ gz^ g  X T b;where z^ g  ^ g  l 00 n ^ g 1 l 0 n ^ g. Similar to the problem of conditional likelihood (), the matrix l 00 n ^ g is non-diagonal, and solving (1) may require On 3  computations. To avoid this difficulty, Tibshirani (1997) used some heuristic arguments to approximate the Hessian matrix with a diagonal one, e.g. treated off-diagonal elements as zero. An iteratively procedure is then conducted based on the penalized reweighed least squares 1 n X n i1 w^ g i z^ g i  X T i b 2  kjjbjj 1 ;where the weight w^ g i for subject i is the ith diagonal entry of l 00 n ^ g.
Component-wise gradient boosting and false discovery controlTo obtain a more accurate estimation,where z  ^ g  Az^ g and X   AX. Alternatively, Geoman (2010) combined gradient descent with Newton's method and implemented his algorithm in an R package penalized.
Gradient boostingGradient boosting has emerged as a powerful tool for building predictive models; its application in the Cox proportional hazards models can be found in Ridgeway (1999) and Li and Luan (2005). The idea is to pursue iterative steepest ascent of the log likelihood function. At each step, given the current estimate of b, say ^ b, let ^ g  X T ^ b. The algorithm computes the gradient of the log-partial likelihood with respect to g i , the ith component of g,for i  1;    ; n, and then fits this gradient (also called working response or pseudo response) to X by a so-called base procedure (e.g. least squares estimation). Specifically, to facilitate variable selection, a component-wise algorithm can be implemented by restricting the search direction to be component-wise (B hlmann and Yu, 2003;). For instance, fit component-wise modelfor j  1;. .. ; p. Computeand update ^ b j ?  ^ b j ?  v ~ b j ? , where v is a positive small constant (say 0.01) controlling the learning rate (). For least squares estimation, the gradient boosting is is exactly the usual forward stagewise procedure (termed as linear-regression version of the forward-stagewise boosting in Algorithm 16.1 of). B hlmann and Hothorn (2007) refer to the same procedure as " L2boost ". This approach is to detect a component-wise direction along which the partial likelihood would ascend most rapidly. At each boosting iteration only one component of b is selected and updated. The variable selection can be achieved if boosting stops at an optimal number of iterations. This optimal number works as the regularization parameter and it can be determined by cross-validation (). However, as we will show in simulation, the cross-validated choice still includes certain amount of false positive selections. A computationally feasible method is needed to control false discoveries.
Control of the false discovery rate (FDR)Benjamini and Hochberg's FDR-controlling procedure (), or BH's procedure for short, is a recent innovation for controlling the FDR. Consider a setting where we test a large number of tests simultaneously. Let R be the number of total discoveries (selection of SNPs) and let V be the number of false discoveries (selection of irrelevant SNPs). If we denote the False Discovery Proportion by FDP  V=R; then FDR is simply the expectation of false discovery proportion (FDP). In the simplest setting (i.e. P-values associated all component tests are independent), BH's procedure is able to control the FDR at any preselect level 0 < q < 1 (called the FDR-control parameter). In the past 20 years, BH's procedure has inspired a great deal of research: many variants of the procedure have been proposed, and many insights and connections have been discovered. For instance,) and Storey (2003) have pointed out an interesting connection between the BH's procedure and the popular Empirical Bayes method. In particular, they proposed a Bayesian version of the FDR which they call the Local FDR (Lfdr) and showed that two versions of FDR are intimately connected to each other. Another useful variant of BH's procedure is the Significance Analysis of Microarrays (SAM;), a method that was originally designed to identify genes in microarray experiments. While the success of the BH's procedure hinges on an accurate approximation of the P-values associated with individual tests, SAM is comparably more flexible for it is able to handle more general experimental layouts and summary statistics, where the P-values may be hard to obtain or to approximate. See Efron (2012) for a nice review on FDR-controlling methods, Lfdr and SAM.
Proposed methods
Component-wise gradient boosting procedureTo introduce the proposed method, we first consider a variant of component-wise gradient boosting method that is computationally efficient in high-dimensional settings.Algorithm 1 (Componentwise Gradient Boosting) Initialize ^ b 0  0. For m  1;    ; M stop , iterate the following steps:(a) For j  1;. .. ; p, compute the componentwise gradient(b) Compute j ?  argmax 1 j p jG j j:where ~ b j ? can be estimated by one-step Newton's update((d) Iterate until m  M stop for some stopping iteration M stop .Algorithm 1 is closely connected to the traditional boosting procedure we described in Section 2.3, which first computes the working response, U i , and then fits the working response to each covariate by least squares. For instance, under the chain rule of differentiation,where G j was defined in (3). In contrast, Algorithm 1 is based on gradient with respect to b and it avoids the calculation of working response. Such a component-wise update is connected with a minimization-maximization (MM) algorithm (). For instance, in a minorization step, given the mth step estimate ^ b m1 , an application of Jensen's inequality leads to the following minority surrogate functionwhere gb j j ^ b m1  is defined implicitly, all a j ! 0; P j a j  1 and a j > 0 whenever X ij 6  0. In the maximization step, we maximize (or monotonically increase) the selected component of the surrogate function to produce the next iteration estimators, e.g. consider gb j ? j ^ b m1  and update b j ?. Then the boosting algorithm monotonically increase the original log-partial likelihood by increasing the surrogate functions. Note that as long as the ascent property is achieved, the choice of a j is not crucial, e.g. it can be considered as part of a control for step size. Moreover, as one only needs to increase the surrogate function instead of maximizing it, one-step Newton iterations (with step-size control) shall provide sufficient and rapid updates at each boosting step. The parameter v can be regarded as controlling the step size of the one-step Newton procedure. This may explain the reason that in practice the best strategy for learning rate of a boosting procedure is to set v to be very small (v < 0.1). Instead of using ~ b j ? , an alternative approach is to use the normalized updates with norm normalized to be 1, e.g. v  signG j . Its main disadvantage is that its performance is sensitive to the choice of learning rate. Although signG j  provides an ascent direction, a sufficiently small step length may be needed. Empirically we found that the procedure with fitted ~ b j ? provides better performance.
Boosting with stability selection for false discovery controlStability Selection was recently introduced by Meinshausen and B hlmann (2010) as a general technique designed to improve the performance of a variable selection algorithm. The idea is to identify variables that are included in the model with high probabilities when a variable selection procedure is performed on randomly sampled of the observations. For completeness of exposure, we summarize the procedure of stability selection as follows. Let I be a random subsample of f1;    ; ng of size bn=2c, draw without replacement. Here bn=2c is defined as the largest integer not greater than n=2. For variable j 2 f1;    ; pg, the random sampling probability that the jth variable is selected by the stability selection iswhere ^ SI  fj : ^ b I j 6  0g denotes the variable selected by the variable selection procedure based on the subsample I, and the empirical probability Pr  is with respect to the random sampling.For a threshold P thres 2 0; 1, the set of variables selected by stability selection is then defined asA particularly attractive feature of stability selection is that its relatively insensitive to the tuning parameter (e.g. M stop for boosting) and hence cross-validation can be avoided. However, a new regularization parameter needs to be determined is the threshold P thres. To address this question, an error control was provided by an upper bound on the expected number of falsely selected variables (Meinshausen and B hlmann, 2010; Theorem 1). More formally, let Ej ^ SIj be the expected number of selected variables and define V to be the number of falsely selected variables. Assume an exchangeable condition, then the expected number V of falsely selected variables is bounded for P thres 2 0:5; 1 byBased on such a bound, the tuning parameter P thres can be chosen such that EV is controlled at the desired level, e.g. for EV < 1, ifThe property of the above procedure relies on restricted assumptions such as exchangeability condition (e.g. the joint distribution of outcomes and covariates is invariant under permutations of non-informative variables), which, as noted by Meinshausen and van de Geer (2011), are not likely to hold for real data. In genetic studies with extensive correlation structure among SNP markers, the exchangeability condition fails and using threshold in (4) has been shown to suffer a loss of power (). Moreover, in computing the threshold in (4), we face a tradeoff. Commonly used variable selection procedures will select certain amount of false positives. On one hand, we want Ej ^ SIj to be large to select the true informative predictors, but on the other hand, a large Ej ^ SIj also can render P thres large (which leads to too conservative threshold). If Ej ^ SIj > p 1 2 , we cannot control the error EV with the formula in (4). To improve the performance of stability selection and determine a data-driven threshold for the selection frequency, we adopt the idea of SAM () and propose a random permutation based stability selection boosting procedure.
Algorithm 2 (Boosting with Stability Selection and Permutation)(a) For s  1;    ; 100, we draw random subsample of the data of size bn=2c. On the sth subsample, implement the proposed boosting approach (e.g. Algorithm 1). Record the set of selected predictors at the sth subsampling, ^ S s  fj : ^ b s j 6  0g, and compute ^where I(A) is an indicator function taking the value 1 when condition A holds and 0 otherwise.(b) For b  1;    ; B, randomly permute the outcomes so that the relation between covariates and outcomes is decoupled. Repeat the stability-based boosting described in step (a) on the permuted sample and record the set of selected predictors ~ S b , and compute ~ PThen this ^ P thres q can be used to determine the selected variables. If q  0.2 and 5 variables are selected with selection frequency greater than ^ P thres 0:2, then 1 of these 5 variables would be expected to be false positive.
SimulationsFinite-sample properties of the proposed method were evaluated through a series of simulation studies. Death times were generated from the exponential model, ktjX i   0:5expX T i b for i  1;. .. ; n, where n  1000 and X i  X i1 ;    ; X i2000  T came from multivariate normal distributions. These 2000 predictors were in 10 blocks with equal numbers of predictors within each block. We considered three simulation schemes with within-block correlation coefficients varying between 0.2, 0.5 and 0.8. For all three schemes, the between-block correlation coefficients were 0 (i.e. independent between blocks). We chose 10 true signals; one from each block, with true b in 60:5; 61; 61:5; 62; 62:5. All other covariate effects are zero. Censoring times were generated from uniform distributions, with the percentage of censored subjects then being approximately 2030%. Each data configuration was replicated 100 times. We first assess the speed of our algorithm.compares the computation time for the proposed approach with Lasso for proportional hazard models (implemented with R package penalized). These timings were taken on an Dell laptop (model XPS 15) with quad-core 2.1-GHz Intel Core i7-3612QM processor and 8 GB RAM. Numerically, we find the proposed approach is faster than R package penalized. As a gradient based method, at each iteration the computational speed of the proposed approach is faster than those approaches that require inverting the Hessian matrix. It is known that finding the proper regularization parameter is difficult for the Lasso procedure, especially for survival settings for which piece-wise linear solution path (LARS;) is not available and a grid search () or bisection method () is required (e.g. multiple Lasso procedures are needed for a series of tuning parameters). In contrast, in boosting procedure, the number of iteration works as tuning parameter and the selection of optimal tuning parameter can be implemented in a single boosting procedure. Moreover, the optimal choice is less critical as boosting is more robust to overfitting (). We compared the proposed methods, Lasso for proportional hazard models, univariate approaches with either Bonferroni correction (termed Univariate Bonferroni in) or Benjamini and Hochberg's (1995) procedure for FDR control (below a threshold 0.2; termed Univariate FDR in). For Lasso and the boosting approach without stability control (Algorithm 1), 10-fold crossvalidation was implemented to determine the optimal tuning parameters (e.g.). For the boosting approach with stability selection, we repeatedly drew 100 random subsamples of the data of size bn=2c. Both the thresholds defined in formula (4) and Algorithm 2 with q  0.2 (termed S-Boosting-1 and S-Boosting2 respectively) were used for variable selection.shows that the boosting without stability selection (termed Boosting in) outperform the univariate approaches in the average number of false positives (FP), average FDP, average number of false negative (FN) and the empirical probabilities to identify the true signal (Power). Though the Lasso has comparable performances in terms of FN and Power, the FPs of the boosting methods are substantially fewer than the Lasso. Finally, the proposed boosting method with stability selection and permutation (S-Boosting-2) further reduces the FPs and it outperforms S-Boosting-1.
Application of cutaneous melanoma dataCutaneous melanoma (CM) is one of the most aggressive skin cancers, causing the greatest number of skin cancer related deaths worldwide. Among the CM patients, wide heterogeneities are present. The commonly used clinicopathological variables, such as tumor stage and Breslow thickness (), may have insufficient discriminative ability (). Discovery of genetic variants would offer a feasible approach to understanding mechanisms that may affect clinical outcomes andThe boosting procedure is described in Sections 3.1; The Lasso is implemented using R package penalized; 10-fold cross-validation was implemented to determine the optimal tuning parameters.the sensitivity of individual cancer to therapy (). We applied our proposed procedures to a genome-wide association study reported byto analyze the association of 2339 common single-nucleotide polymorphisms (SNPs) with overall survival in CM patients. Our goal was to identify SNPS that are relevant to overall survival among the patients. The dataset contains a total of 858 CM patients, with 133 deaths observed during the follow-up, where the median follow-up time was 81.1 months. The overall survival time was calculated from the date of diagnosis to the date of death or the date of the last followup. Genotyped or imputed common SNPs (minor allele frequency ! 0:05, genotyping rate ! 95%, Hardy-Weinberg equilibrium P-value ! 0:00001 and imputation r 2 ! 0:8) within 14 autosomal FA genes or their 620-kb flanking regions were selected for association analysis (). As a result, 321 genotyped SNPs and 2018 imputed SNPs in the FA pathway were selected for further analysis. Other covariates to adjust for included age at diagnosis, Clark level, tumor stage, Breslow thickness, sentinel lymph node biopsy and the mitotic rate. The proposed boosting procedure with stability selection was implemented to select informative SNPs (coded as 0, 1; without or with minor alleles). The importance of predictors is evaluated by the proportion of times that the predictor is selected in the model among the 100 subsamples. We also compared the proposed methods with the Lasso, the boosting procedure without stability selection and univariate approaches. The results are summarized in. The Lasso procedure selected 25 SNPs. Among them, 12 SNPs with absolute coefficients larger than 0.01 are listed in. None of these predictors pass the univariate approaches with Bonferroni correction orprocedure for FDR control (with a threshold 0.2). As we found in Section 4, these results argue that the univariate approaches may have more false negatives than other methods. In contrast, the boosting procedure selected 7 predictors, which were a subset of top 12 SNPs selected by the Lasso. To further control the false selections, the estimated false discovery rate, Fdr, were also calculated to determine a datadriven threshold for the selection frequency such that Fdr 0:2. Three of the SNPs selected by both Lasso and boosting pass the threshold ^ P thres 0:2  72%. The remaining variables find insignificant support from stability selection.summarizes the numbers of selected variables from the Lasso and the boosting without or with stability selection. These results are consistent with those from simulation section. The Lasso tends to select too many variables. The boosting selects substantially fewer variables than the Lasso. The boosting procedure with stability selection provides a control for false positives.shows the stability path (selection frequencies across boosting iterations). The variables with selection frequencies larger than the threshold (estimated empirical Bayes false discovery rate Fdr 0:2; based 500 permuted samples) are plotted as solid lines, while the path of the remaining variables are shown as broken lines. The top 3 variables stand out clearly and the number of boosting iteration is less critical. A Manhattan plot was given inwith the dashed horizontal line corresponding to the estimated threshold ^ P thres 0:2  72%. Three variables have selection frequencies larger than this dashed horizontal line. The vertical blue lines highlight the selection frequencies of the four previously-detected SNPs that are associated with overall survival of CM patients by. The red vertical lines highlight the SNPs whose selection frequencies pass the estimated threshold. The lower panel ofillustrates pairwise correlations across the 2339 SNPs with the strength of the correlation, from positive to negative, indicated by the color spectrum from red to dark blue. One of the top SNPs in our finding, rs74189161 (with selection frequency  72% and Fdr  0:16) is strongly correlated with rs3752447 identified by, with correlation coefficients r 2  1 (calculated with plink v1.07;). Besides confirming the previously reported SNP, we also found some novel signals. For example, we identified a cluster of signals around SNP rs356665 in gene FANCC and a SNP rs3087374 in gene FANC1. Both two genes have previously been reported having regulation effects with the FA pathway (). Mutations in the FA pathway are identified in diverse cancer types () and therefore are likely to modulate the survival of CM patients.Reducing the number of false discoveries is often very desirable in biological applications since follow-up experiments can be costly and laborious. We have proposed a boosting method with stability selection to analyze high-dimensional data. We demonstrated and compared performances of the proposed method and the commonly used univariate approaches or Lasso for variable selection. The proposed method outperformed other methods in terms of substantially reduced false positives and low false negatives. Finally, it is worth mentioning that the traditional gradient boosting approach described in Section 2.3 cannot accommodate some important models, including survival models with timevarying effects wherein the generic function eta not only depends on X, but also on time. In contrast, the proposed modification of gradient boosting works in flexible parameter spaces, even including infinite-dimensional functional spaces. In the latter case, as the search space is typically a functional space, one needs to calculate the G teaux derivative of the functional in order to determine the optimal descent direction. We will report the work elsewhere.
FundingDrs Li and Lin's research is partly supported by the Chinese Natural Science Foundation (11528102). Dr Wei's research is partly supported by NIH grants R01CA100264 and R01CA133996. Dr Hyslop's research is partly supported by a NIH grant P30CA014236. Dr Lee's research is partly supported by NCI SPORE P50 CA093459, and philanthropic contributions to The University of Texas M.D. Anderson Cancer Center Moon Shots Program, the Miriam and Jim Mulva Research Fund, the Patrick M. McCarthy Foundation and the Marit Peterson Fund for Melanoma Research. Conflict of Interest: none declared.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
K.He et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
