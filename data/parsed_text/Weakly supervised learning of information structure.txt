Motivation: Many practical tasks in biomedicine require accessing specific types of information in scientific literature; e.g. information about the methods, results or conclusions of the study in question. Several approaches have been developed to identify such information in scientific journal articles. The best of these have yielded promising results and proved useful for biomedical text mining tasks. However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different tasks. A potential solution to this problem is to employ weakly supervised learning instead. In this article, we investigate a weakly supervised approach to identifying information structure according to a scheme called Argumentative Zoning (AZ). We apply four weakly supervised classifiers to biomedical abstracts and evaluate their performance both directly and in a real-life scenario in the context of cancer risk assessment. Results: Our best weakly supervised classifier (based on the combination of active learning and self-training) performs well on the task, outperforming our best supervised classifier: it yields a high accuracy of 81% when just 10% of the labeled data is used for training. When cancer risk assessors are presented with the resulting annotated abstracts, they find relevant information in them significantly faster than when presented with unannotated abstracts. These results suggest that weakly supervised learning could be used to improve the practical usefulness of information structure for real-life tasks in biomedicine. Availability: The annotated dataset, classifiers and the user test for cancer risk assessment are available online at
INTRODUCTIONMany practical tasks in biomedicine require accessing specific types of information in scientific literature. For example, a biomedical scientist may be looking for information about the objective of the study in question, the methods used, the results obtained or the * To whom correspondence should be addressed. conclusions drawn by the authors. Similarly, many biomedical text mining tasks (e.g. information extraction, summarization) focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for the classification of sentences in scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of the approaches classify sentences according to typical section names seen in scientific documents (), while others are based e.g. on argumentative zones (), qualitative dimensions () or conceptual structure () of documents. The best current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (). However, relying on fully supervised machine learning (ml) and a large body of annotated data, existing approaches are expensive to develop and port to different domains and tasks, and thus intractable for use in real-life applications. A potential solution to this bottleneck is to develop techniques based on weakly supervised ml instead. Making use of a small amount of labeled data and a large pool of unlabeled data, weakly supervised learning (e.g. semi-supervision, active learning, co/tri-training, self-training) aims to keep the advantages of fully supervised approaches. It has been applied to a wide range of natural language processing (nlp) and text mining tasks, including namedentity recognition, question answering, information extraction, text classification and many others (), yielding performance levels similar or equivalent to those of fully supervised techniques. In this article, we investigate the potential of weakly supervised learning for Argumentative Zoning (az) of biomedical abstracts. az provides an analysis of the argumentative structure (i.e. the rhetorical progression of the argument) of a scientific document (). It has been used to analyze scientific texts in various disciplinesincluding computational linguistics (), law, (), biology () and chemistry ()and has proved useful for nlp tasks such as summarization (). However, the application of az to different domains has resulted in laborious annotation exercises that suggests that a weakly supervised approach would be more practical for the real-world application of az.Taking two supervised classifiers as a comparison pointSupport Vector Machines (svm) and Conditional Random Fields (crf)we investigate the performance of four weakly supervised classifiers for az: two based on semi-supervised learning (transductive svm and semi-supervised crf) and two on active learning (Active svm alone and in combination with self-training). We apply these classifiers to az-annotated biomedical abstracts in the recent dataset of. The results are promising. Our best weakly supervised classifier (Active svm with self-training) outperforms the best supervised classifier (svm), yielding high accuracy of 81% when using just 10% of the labeled data for training. When using just onethird of the labeled data, it performs as well as a fully supervised svm, which uses 100% of the labeled data. The abstracts in the dataset ofwere selected on the basis of their suitability for cancer risk assessment (cra). This enables us to conduct user-based evaluation of the practical usefulness of our approach for the real-world task of cra. We investigate whether cancer risk assessors find relevant information in abstracts faster when the abstracts are annotated for az using our best weakly supervised approach. The results are promising: although manual annotations yield the biggest time savings: 1013% (compared with the time it takes to examine unannotated abstracts), considerable savings are also obtained with weakly supervised ml annotations: 78% (using active svm with self-training). In sum, our investigation shows that weakly supervised az can be employed to improve the practical applicability and portability of az to different information access tasks and that its accuracy is high enough to benefit a real-life task in biomedicine.
METHODS
DataWe used in our experiments the recent dataset of, consisting of 1000 cra abstracts (7985 sentences and 225 785 words) annotated according to az. Originally introduced by Teufel and Moens (2002), az is a scheme that provides an analysis of the argumentative structure of a document, following the knowledge claims made by the authors. The dataset ofhas been annotated according to the version of az developed for biology papers () (with only minor modifications concerning zone names). Seven categories of this scheme (out of the 10 possible) actually appear in abstracts and in the resulting dataset. These are shown and explained inreported the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. The agreement ( = 0.85) is relatively high according to Cohen (1960).
Weakly supervised learning
Automatic classification
Features and feature extractionThe first step in automatic classification is to select a set of features that may indicate az categories in abstracts. Following, we implemented a set of features that have proved successful in related works, e.g. (@BULLET Voice. The voice of verbs (active or passive) in the corpus.These features were extracted from the corpus using a number of tools. A tokenizer was used to detect sentence boundaries and to perform basic tokenization (in extreme cases, processing complex biomedical terms e.g. 2amino-3,8-diethylimidazoquinoxaline). The C&C tools () were used for pos tagging, lemmatization and parsing. The lemma output was used for Word, Bi-gram and Verb features, and the gr output for gr, Subj, Obj and Voice features. The 'obj' marker in a subject relation indicates passive voice [e.g. (ncsubj observed_14 difference_5 obj)]. Verb classes were obtained automatically using unsupervised spectral clustering (). To reduce data sparsity, we lemmatized the lexical items for all the features, and removed words and grs with <2 occurrences and bi-grams with <5 occurrences.
Machine learning methodsThe next step is to assign sentences in abstracts to zone categories using machine learning. Support vector machines (svm) and conditional random fields (crf) have proved the best performing fully supervised methods in recent related works e.g. (). We therefore implemented these methods as well as weakly supervised variations of them: active svm with and without self-training, transductive svm and semi-supervised crf. Supervised methods: svm aims to find the maximum-margin hyperplane, which has the largest distance to the nearest data points of any class. The problem is defined as:where x is data, y is its label, w is a normal vector to the hyperplane and 2 |w| is the margin. We used Weka software () employing the smo algorithm (Platt, 1999b) with linear kernel for svm experiments. crf is an undirected graphical model that defines a probability distribution over the hidden states (e.g. label sequences) given the observations. The probability of a label sequence y given an observation sequence x can be written as:where F j (y,x) is a real-valued feature function of the states and the observations;  j is the weight of F j , and Z(x) is a normalization factor. We used Mallet software (http://mallet.cs.umass.edu) employing the l-bfgs algorithm () for crf experiments. Weakly supervised methods: Active svm (asvm) starts with a small amount of labeled data, and iteratively chooses a certain amount of unlabeled data, about which the classifier is least certain, to be manually labeled (the labels can be restored from the fully annotated corpus) for the next round of learnig. We used an uncertainty sampling query strategy (). In particular, we compared the posterior probabilities of the best estimate given each unlabeled instance, and chose the instances with the lowest probabilities to be labeled for later use. The probabilities can be calculated by fitting a Sigmoid after the standard svm () and, in the multi-class case, combined using a pairwise coupling algorithm (). We used the-M flag in Weka for computing the posterior probabilities. Active svm with self-training (assvm) is an extension of asvm where each round of learning has two steps:(i) Active learning (a) Train a new classifier on all the labeled examples.(b) Apply the current classifier to each unlabeled example.(c) Find n examples about which the classifier is least certain to be manually labeled.(ii) Self-training(a) Train a new classifier on both labeled and unlabeled/machinelabeled data using the estimates from step (i)(b).(b) Test the current classifier on test data. Transductive svm (tsvm) is an extension of svm that aims to:is unlabeled data and y (u) the estimate of its label. The idea is to find a prediction on unlabeled data such that the decision boundary has the maximum margin on both the labeled and the unlabeled (now labeled) data. The latter guides the decision boundary away from dense regions. We used UniverSVM software (http://3t.kyb.tuebingen.mpg.de/bs/people/fabee/ universvm.html) employing the cccp algorithm () for tsvm experiments. Semi-supervised crf (sscrf) can be implemented by entropy regularization (), which extends the objective function on Labeled data,) to minimize the conditional entropy of the model's predictions on Unlabeled data. We used Mallet software for sscrf experiments.
Evaluation methods
Y.Guo et al.as test data and the remaining nine folds as training data (with x% being manually labeled). The results were then averaged. As randomly selected labeled data were used for svm, crf, tsvm and sscrf, the results for these methods were averaged from five runs. Following Dietterich (1998), we used McNemar's test () to measure the statistical significance of the differences between the results of supervised and weakly supervised learning. The chosen significance level was 0.05.
User test in the context of craA major time-consuming component of chemical cancer risk assessment (cra) is the review and analysis of existing scientific literature on the chemical in question. MEDLINE (http://www.nlm.nih.gov/databases/ databases_medline.html) abstracts are typically used as a starting point in this work. Risk assessors (e.g. toxicologists, biologists) read the abstracts of interest, looking for various information in them (e.g. about the methods, results and conclusions of the study in question) (). One way to speed up this work is to annotate the abstracts with categories of information structure so that the information of interest can be found faster.investigated this idea first and showed that time savings can be obtained in literature review when abstracts are annotated either manually or automatically (using fully supervised ml) according to different information structure schemes. We evaluated our weakly supervised approach to az in a similar way but re-designed the evaluation ofso that it is better controlled and covers a wider range of information. Cancer risk assessors working in Karolinska Institutet (Stockholm, Sweden) provided us with a list of 10 questions considered when studying abstracts for cra purposes.). We then designed an online questionnaire where each questionanswer pair is displayed to an expert on a separate page and the zone(s) most relevant for answering the question are highlighted with colors as to attract expert's attention (). Two experts participated in the test: one professor level expert (A) with a long experience in cra (over 25 years) and one more junior expert (B) with a PhD in toxicology and over 5 years of experience in cra. Each expert was presented with the same set of 200 abstracts focusing on four chemicals (butadiene, diethylnitrosamine, diethylstilbestrol and phenobarbital): (i) 50 unannotated, (ii) 50 manually annotated, (iii) 50 assvm-annotated and (iv) 50 randomly annotated abstracts (i.e. annotated so that sentences were assigned to zones on the basis of their observed distribution in the training data). We compared the time it took for experts to answer the questions when presented with abstracts in (i)(iv), and examined whether the differences are statistically significant (significance level of 0.05, MannWhitney U Test (). In addition, we evaluated the impact of (i)(iv) on the quality of experts'answers by examining inter-expert agreement.shows the results for the four weakly supervised and two supervised methods when using 10% of the labeled data (i.e. 700 sentences). assvm is the best performing method, with an accuracy of 81% and macro F-score of 0.76. asvm performs nearly aswell, with an accuracy of 80% and F-score of 0.75. Both methods outperform supervised svm with a statistically significant difference (P < 0.001). tsvm is the lowest performing svm-based method: its performance is lower than that of the supervised svm. Yet, it outperforms both crf-based methods. sscrf performs slightly better than crf with 1% higher accuracy and 0.01 higher F-score. Only two methods (asvm, tsvm) find six out of the seven possible zone categories. Other methods find five categories. The 12 missing categories are low frequency categories, accounting for 1% of the corpus data each (). The results for other categories also seem to reflect the amount of corpus data available per category (), with res being the highest and obj the lowest performing category with most methods.shows the learning curve of different methods (in terms of accuracy) when using 0100% of the labeled data. assvm outperforms other methods, reaching its best performance of 88% accuracy when using 40% of the labeled data. It outperforms asvm (the second best method) in particular when 2040% of the labeled data is used. When using 33% of the labeled data, it performs already as well as fully supervised svm (i.e. using 100% of the labeled data). svm and tsvm tend to perform quite similarly with each other when >10% of the labeled data are used, but when less data are available, tsvm performs better. Looking at the crf-based methods, sscrf outperforms crf in particular when 1025% of the labeled data are used. However, neither of them reaches the performance level of svm-based methods, which is in line with the results of fully supervised crf and svm in. To investigate which features are the most useful for weakly supervised learning, we took our best performing method assvm and conducted leave-one-out analysis of the features with 10% of the labeled data. The results inshow that Location is by far the most useful feature, in particular for bkg, meth and con. The overall performance drops 8% in accuracy and 0.09 in F-score when removing this feature. Removing POS has almost equally strong effect, in particular on bkg and meth. Also Voice, Verb class and GR contribute to general performance. Among the least helpful features are those which suffer from sparse data problems, e.g. Word, Bi-gram and Verb). They perform particularly badly when applied to low frequency zones.shows the time (measured in seconds) it took for experts A and B to answer questions (individual and total) when presented with (i) unannotated, (ii) manually annotated, (iii) assvm annotated and (iv) randomly annotated abstracts (see Section 2 for details of(i)). time stands for the sample mean, and save for the percentage of time savings.shows the statistical significance (P-values, MannWhitney U Test) of the differences between the results for different abstract groups [e.g.
RESULTS
Automatic classification
User test. Looking at the overall figures (i.e. Total), both manual (ii) and assvm (iii) annotations help users find relevant information significantly faster than plain text abstracts (i): the percentage of time savings ranges between 7% and 13%, and the corresponding P-values ranges between < 0.001 and 0.027. Although manual annotations save more time than assvm annotations (13% versus 7% for A, and 10% versus 8% for B), assvm annotations are surprisingly useful. Random annotations (iv) have a negative effect: both experts spend more time examining (iv) than (i) abstracts: 6% for A and 19% for B. Looking at the results for individual questions, (ii) and (iii) are more helpful for answering broader questions (e.g. Q9 Is the outcome of the study expected/unexpected/neutral?) than more specific questions (e.g. Q4 Is exposure length mentioned?). Although (ii) is more helpful than (iii) for most questions, the majority of differences are not statistically significant, showing that assvm annotations are almost equally useful as manual annotations. assvm annotations have a negative effect on Q4 and Q5. Q4 and Q5 focus on meth which is a higher frequency (accounting for 18% of the corpus) but less predictable (0.76 F-score for assvm) category.time save time save time save time save time save time save time save time save time save time save time save
Page: 3184 31793185
Y.Guo et al.AAs we mentioned in Section 2.3: 'we compared the time it took for experts to answer the questions when presented with abstracts in (i)(iv), and examined whether the differences are statistically significant [significance level of 0.05, Mann-Whitney U Test (. Values in bold are less than 0.05, indicating that the differences are statically significant. * After rounding, this value is 0.00Since Q3 is a multiple-choice question, we report the inter-expert agreement for each option: Q3a,b,c.shows the joint probability of users' agreement on the answers. Annotations (ii), (iii) and (iv) do not affect the users' agreement a lot: 0.820.86 and 0.81 with and without annotations. Interestingly, experts tend to agree the most on the answers when using assvm annotated abstracts. This demonstrates that automatic annotation does not affect the quality of the answers.
DISCUSSION AND CONCLUSIONSOur results show that weakly supervised ml can be used for the identification of information structure in biomedical abstracts. In our experiments, the majority of weakly supervised methods: assvm, asvm and sscrf outperformed their corresponding supervised methods: svm and crf. assvm/asvm selects the most difficult instances (or the instances distinct from the existing labeled data) to be manually labeled and then used for the next round of learning, offering a wider coverage of the possible inputs than svm. sscrf extends crf by taking into account the conditional entropy of the model's predictions on unlabeled data (favoring peaked, confident predictions) so that the decision boundary is moved into the sparse regions of input space. The best performing weakly supervised methods were those based on active learning. When using 10% of the labeled data, active learning combined with self-training (assvm) outperformed the best supervised method svm with a statistically significant difference. assvm reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as fully supervised svm when using just one-third of the labeled data. This result is in line with the results of other text classification works where active learning has proved similarly useful, e.g. Esuli and;. In addition, we have demonstrated that the accuracy of our best weakly supervised method (assvm) is high enough to benefit a real-life task in biomedicine: cancer risk assessors find relevant information in abstracts significantly faster (78%) when the abstracts are annotated using assvm (as opposed to being unannotated). In sum, our research shows that application of az-style approaches to real-world biomedical tasks can be realistic as only a limited amount of labeled data is needed for it. To the best of our knowledge, no previous work has been done on weakly supervised learning of textual information structure according to the family of schemes we have focused on;;. Previous works on these schemes have been fully supervised in nature. In addition, although some works have been evaluated in the context of text mining tasks (e.g. information extraction, summarization), the only previous work which has reported user-centered evaluation in the context of a real-life biomedical task is that of. In the future, we plan to improve and extend this work in several directions. Semi-supervised learning (tsvm and sscrf) did not perform equally well as active learning in our experiments, although it has proved successful in related works e.g. (). We suspect that this is due to the high dimensionality and sparseness of our labeled dataset. Given the high cost of obtaining labeled data, methods not needing it are preferable. We plan to thus experiment
Page: 3185 31793185
Weakly supervised learningwith more sophisticated active learning algorithms, e.g. margin sampling (), query-by-committee (QBC) () and svm simple margin (). Combinations of other weakly supervised methods, e.g. EM+active learning () and co-training+EM+active learning () would also be worth investigating. In addition, we plan to replace the svm-based model with other models e.g. Logistic Regression, which outperforms svm in active learning as reported in (). crf-based active learning might be a good option too. The work presented in this article has focused on the az scheme. In the future, we plan to investigate the usefulness of weakly supervised learning for identifying information structure according to other popular schemes, e.g. () and not only in scientific abstracts but also in full journal papers, which typically exemplify a larger set of scheme categories. Focusing on full journal papers will also enable further user-based evaluation. For example, although abstracts are used as a typical starting point in cra, subsequent steps of cra focus on information in full articles. These more challenging steps may benefit from az (and other type of) annotations to a greater degree.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
