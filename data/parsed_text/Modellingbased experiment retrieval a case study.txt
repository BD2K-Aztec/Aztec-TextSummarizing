Motivation: Public and private repositories of experimental data are growing to sizes that require dedicated methods for finding relevant data. To improve on the state of the art of keyword searches from annotations, methods for content-based retrieval have been proposed. In the context of gene expression experiments, most methods retrieve gene expression profiles, requiring each experiment to be expressed as a single profile, typically of case versus control. A more general, recently suggested alternative is to retrieve experiments whose models are good for modelling the query dataset. However, for very noisy and high-dimensional query data, this retrieval criterion turns out to be very noisy as well. Results: We propose doing retrieval using a denoised model of the query dataset, instead of the original noisy dataset itself. To this end, we introduce a general probabilistic framework, where each experiment is modelled separately and the retrieval is done by finding related models. For retrieval of gene expression experiments, we use a probabilistic model called product partition model, which induces a clustering of genes that show similar expression patterns across a number of samples. The suggested metric for retrieval using clusterings is the normalized information distance. Empirical results finally suggest that inference for the full probabilistic model can be approximated with good performance using computationally faster heuristic clustering approaches (e.g. k-means). The method is highly scalable and straightforward to apply to construct a general-purpose gene expression experiment retrieval method. Availability and implementation: The method can be implemented using standard clustering algorithms and normalized information distance, available in many statistical software packages.
IntroductionAs the use of high-throughput molecular measurement technologies continues to spread, an ever increasing amount of data from biological experiments is being stored in publicly available repositories. It is then often of interest for researchers to retrieve experimental datasets with relevance to a given experiment, in order to increase the power of statistical analyses and to be able to make novel findings not obtainable from one experiment alone. The current standard practice relies on). However, despite efforts to maintain compliance with standard formats of documenting experiments, e.g. the MIAME standard (), information about experiments may often be missing, insufficient or suffer from variations in terminology (e.g.). In view of the challenges associated with keyword-based retrieval, the complementary task of querying a database of experiments using measurement data, instead of keywords, has recently received increased attention in the literature. Most earlier content-driven methods used for retrieval of gene expression data represent each experiment in terms of a profile over genes, or alternatively, over known gene sets or gene modules predicted from other data sources, see(2012) and references therein. A representative example is to compute differential expression profiles of case versus control, use the correlation between activity profiles as the measure of relevance, and retrieve the experiments with the highest correlations (e.g.). This requires auxiliary information about the experiments, namely case and control labels of experiment samples, and possibly additional a priori defined sets of important genes. In the context of gene expression time series, representative examples of retrieving gene expression profiles include. Recently, two feasibility studies have gone beyond reducing experiments into single profiles by using probabilistic modelling of the experiments in the database being queried., assumed that the query dataset can be explained as a mixture of the learnt models, each model learnt from one dataset, such that the measure of relevance is given by the inferred mixture weights. In a slightly different approach (), experiments were retrieved by evaluating the posterior marginal likelihoods, given the query data, of individual models stored for the experiments in the database. In this paper, we introduce a method for retrieving full datasets, i.e. experiments consisting of multiple samples, which is also based on probabilistic modelling. However, instead of using the query dataset itself as a query, we use a model learnt from it. The measure of relevance is therefore not a likelihood, but instead a suitably defined metric between the models. The argument is that for noisy and complex datasets, it is beneficial to extract relevant characteristics of the query dataset in the same way as was done with the datasets that are being queried. We also make explicit the importance of marginalizing out nuisance parameters which are not directly relevant for the retrieval task. For example, in a gene expression study, one is often more interested in how sets of genes are co-regulated, rather than their exact expression values which are additionally affected by numerous other influences. We tackle the specific problem of retrieving gene expression experiments by using a product partition model () to cluster together genes that show similar expression patterns across a number of samples. By integrating out expression levels of the gene sets (i.e. cluster-specific information), only the co-expression patterns revealed by the clustering structure are retained. The clustering induced by the query dataset is then finally compared with the clusterings associated with the experiments in the database using the normalized information distance (). Notice that this approach does not involve any 'training stage', compared to that of, and the retrieval step does not involve solving an optimization problem, compared toWhile gene clustering has a long history in characterizing gene expression datasets (), it appears not to have been used in the context of experiment retrieval before. The use of gene clustering provides a straightforward way of characterizing each experiment with minimal preprocessing of the data while capturing central co-expression patterns. Essentially all previous approaches for retrieving gene expression data have converted the data to differential expression (or gene set enrichments) requiring fixed and known casecontrol distinctions. In contrast, we have only applied standard quality control and RMA normalization steps carried out in-house at the European Bioinformatics Institute (EBI) for datasets in the Expression Atlas database (see). Our experimental evaluation further suggests that, for the current application, inference of the full probabilistic model can be approximated by some computationally faster heuristic clustering algorithm, such as k-means (see Supplement Section A of the online material). The computational simplicity makes the method highly scalable and easy to apply in a black-box manner, as a general-purpose retrieval scheme.
ApproachLet D q denote a data matrix from some experiment of interest, and let fD m g M m1 be a database of M datasets from previously conducted experiments. The aim is to retrieve datasets from among the fD m g M m1 with similar characteristics as the query dataset D q. Due to the complex nature of the data, there is no single sensible or obvious way of comparing datasets (matrices of possibly different sizes). We propose using a model to characterize each dataset, with the aim of reducing noise and making relevant aspects of the data more tangible, while making the experiments comparable. The retrieval task then consists in ranking the models fM m g M m1 , inferred from fD m g M m1 , with respect to their similarity with the query model M q inferred from D q. Note that in a broad sense, the commonly used differential expression can be considered as one model type, and clustering as another. To elaborate on the above idea further, we will now assume that the data generating mechanism of each dataset can be represented in terms of a probabilistic model with density f in some family ff jhjh 2 Hg. Often, the parameter h can be decomposed as h  k; w, where w is the parameter or characteristic of interest (e.g. gene clusters) and k is a nuisance parameter (e.g. average expression level of the gene cluster). Marginalizing out (integrating the density over) k then yields a model family completely determined by w 2 W. Making this operation explicit, the key quantity used in inferring a representative model for a dataset D is the marginal likelihood, pDjw   K f Djk; wp kj w kjwdk;where p kjw jw is a prior density on K. Ideally, we would then proceed with a fully Bayesian approach to infer a posterior density (or distribution) p w jD over W, and use M : p w jD to characterize D. However, for computational reasons we will here choose only a single element of W to represent D. Under zero-one loss, the optimal choice is then the maximum a posteriori (MAP) solutionwhere p w is a prior over W. Accordingly, we now define the representative model for D as M : ~ w .If a suitable function d : M  M ! R can be defined for the pairwise relations between the elements of the model space M, a natural ranking among M 1 ;. .. ; M M 2 M will be induced by evaluating dM q ; M m  for all m. For coherence of the ranking scheme, we will make a further assumption that d is a metric.With the above conditions satisfied, the function d conforms to the intuition of a distance, and furthermore, provides a solid foundation for the design of data structures and algorithms, as the model space M forms a metric space. We finally note that metrics are also available for probability distributions, making the described framework applicable in cases where computational resources allow for representing the elements of M as full posterior distributions.
Methods
Probabilistic model for gene clusteringThe first task in constructing a retrieval scheme is to choose an appropriate model for the experiments. While several different approaches, with varying aims and assumptions, exist for modelling gene expression data, a particularly simple and frequently used approach is that of gene clustering (e.g. D'haeseleer), which seeks to cluster together genes that show similar expression patterns across a number of samples. Here, we use a probabilistic clustering approach which simultaneously infers both the number of clusters as well as the optimal clustering structure. Consider first a gene expression data matrix D of dimension n  p, where n is the number of genes and p is the number of samples. A clustering S  fs 1 ;. .. ; s k g is a partition of the set N  f1; .
. .; ng into k 2 f1;. .. ; ng non-empty and non-overlapping subsets, or clusters, such that [ k c1 s c  N and s c \ s c 0  ;, for c 6  c 0. We focus here on a probabilistic formulation of clustering, which makes explicit use of partition structures, namely the product partition model (PPM). Technically, PPM assumes that items in the same cluster are exchangeable and items in different clusters are independent (see). Using the terminology of Section 2, the parameter of interest for this model is the partition structure S, while the nuisance parameter is a vector of cluster-specific model parameters, k  k 1 ;. .. ; k k . This leads to a marginal likelihood of the form (see Equation(1))where D sc denotes the subset of D which is indexed by s c. Note that the assumption of independence between clusters entails constructing the marginal likelihood as a product of cluster-specific components. The prior distribution for S will likewise be constructed as a product,; for all k 2 f1;. .. ; ng;where K ensures normalization to 1 over the model space S and hs c  ! 0 for all subsets s c. Note that (4) actually specifies the joint distribution for S and k, but since the latter is implied by the former, we omit k from the notation. It can be shown that a PPM with K and hs c  chosen such thatwhere js c j is the number of observations in cluster s c and g 0 > 0 controls the tendency to form new clusters, can be obtained by integrating out the model parameters in a Dirichlet process mixture model (). The cluster-specific marginal likelihoods pD sc js c  in Equation(3) can in principle take any suitable form. Here, we assume that for D sc  x ij ; i 2 s c ; j  1;. .. ; p, the observations in each sample j are independently generated from Nl cj ; s 1 cj  with a conjugate NormalGammal 0 ; q 0 ; a 0 ; b 0  prior on the unknown model parameters. Furthermore, we make the simplistic assumption that the samples themselves are independent, conditional on a cluster assignment (see, for a discussion about the implications of this assumption in a classification context). The resulting clusterspecific marginal likelihoods may then be written asintroduced a PPM for clustering mixed discrete and continuous data, where the continuous component was of form (6). Following their implementation, we normalize each column of the data matrix D  [ k c1 D sc to have zero mean and unit variance, and set the hyperparameter values to l 0  0 and q 0  a 0  b 0  1. Furthermore, the model is equipped with a prior of the form (5), with g 0  1. Finally, combining Equations (3)(6), an optimal clustering ~ S w.r.t. a dataset D is given by the MAP solution (see Equation (2))3.1.1 Inference To find the optimal clustering ~ S 2 S as defined in Equation (7), we use a stochastic greedy search algorithm, which moves in the model space by successive application of move, split and merge operators; for further details, see. While being more efficient for the optimization task than standard Markov chain Monte Carlo methods, for large amounts of data the algorithm still requires a considerable amount of computation time. To that end, some computational simplifications based on heuristic clustering procedures are discussed in Supplement Section A of the online material. Note, however, that all results presented in Section 4 of the main text are based on the stochastic greedy search algorithm described above, instead of these simplifications.
Distance metric for clusteringsAssuming now that each of the experiments in a database has been represented with a clustering S 2 S, the remaining task is to find a function d which can be defined on S and satisfies conditions (M1) (M4) above. In recent years, a new generation of information-theoretic distance measures has emerged (see e.g.), which possess many desirable properties, such as the metric property, and which have been employed because of their strong mathematical foundation and ability to detect non-linear similarities.conducted a systematic comparison of information-theoretic distance measures, concluding that the preferred 'general-purpose' measure for comparing clusterings is the normalized information distance, later denoted d NID. To give a definition of this measure, we first introduce some notation. Briefly, for two clusterings S and S 0 , the number of items co-occurring in clusters s c 2 S and s c 0 2 S 0 is given by n cc 0  js c \ s 0 c 0 j, withn cc 0  n. The marginal sums are denoted by n c key realization in the derivation of information-theoretic distance measures is that each clustering induces an empirical probability distribution over the set f1;. .. ; kg, such that the probability of a randomly chosen item i 2 N being in cluster s c is given by Pi 2 s c   n c =n. Similarly, the joint probability of the pair i; j 2 N  N co-occurring in clusters s c and s 0 c 0 is given by Pi; j 2 s c  s 0 c 0   n cc 0 =n. The entropy of a clustering S, describing the uncertainty associated with assigning items into the clusters of S, is then formulated asThe mutual information of clusterings S and S 0 , which measures how much having knowledge of S 0 reduces H(S) (or vice versa), is further defined asPi; j 2 s c  s 0 c 0 logPi; j 2 s c  s 0 c 0  Pi 2 s c Pj 2 s 0 c 0  :It can also be interpreted as a measure of dependence in the sense that if S and S 0 are independent, then IS; S 0   0. Finally, from the above quantities we obtain d NID as d NID S; S 0   1  IS; S 0  maxfHS; HS 0 g :
Results
Data and experimental setupTo evaluate the modelling-based retrieval scheme developed in Sections 2 and 3, we used as a starting point all differential expression experiments conducted on the A-AFFY-44 affymetrix genechip available in Expression Atlas (EA; http://www.ebi.ac.uk/gxa, see) as of June 4, 2014. Only experiments with both measurement data and analytics data available were considered. Furthermore, experiments with a very small number of genes were discarded. Since most experiments had expression measurements for more than 54 670 genes, this number was set as the lower limit. Based on the above selection process we obtained an initial set of 447 experiments. In a second stage, we selected a subset of these experiments based on the availability of experimental factor ontologies (EFO; http://www. ebi.ac.uk/efo/, see), which were used as ground truth in the evaluation. More specifically, we retained those experiments which had at least one of the EFO types 'cell type', 'disease' or 'organism part' present. Moreover, experiments having multiple values for a given EFO type were excluded, and finally only experiments with the same EFO value present in at least two experiments were included in this study, resulting in a final set of 251 experiments (for a list of accession numbers, see Supplement Section D). The number of samples per experiment varied between 6 and 353, the median number of samples being 22. Out of the final set of 251 experiments, three partly overlapping subsets corresponding to each of the EFO types were formed. These consisted of 103 experiments with values recorded for 'cell type', 76 with values for 'disease' and 174 with values for 'organism part'. The number of different EFO values in these sets of experiments were 23, 19 and 32, respectively. In evaluating retrieval performance with respect to a given EFO type, experiments having the same value were considered mutually relevant, and other experiments irrelevant. Note that the above EFO types were not the main conditions of interest on which differential gene expression had been studied in the experiments, but were chosen to give a more general description of the experiments. A more complete ground truth was not readily available as most other EFO types were only present in small subsets of the experiments. Retrival performance was measured using precision and recall, taken as an average of successively using each of the experiments as a query to retrieve among the remaining experiments. In order to reduce the number of genes for clustering, we initially selected for each of the 251 experiments the top 5 genes resulting from a 'non-specific' search in EA, in which genes with the highest absolute values of t-statistics in any available contrast come first, irrespective of whether they are reported with high t-statistics in the remaining contrasts (for further details about listing genes in EA, see). Finally, by taking the union of these genes over all experiments, we arrived at 1125 genes per experiment. The selection process per se is not an essential part of our approach but done for computational convenience only. In a preliminary stage of our analyses, we experimented with different numbers of genes but found that this only had a minor impact on the results, see Supplement Section B for further details. The datasets used in the analyses, along with code for downloading, selecting experiments and genes, processing and analyzing the data are also available in the online supplementary material of this paper.
Comparison of retrieval schemesWe will now proceed to evaluating the performance of the retrieval approach proposed in Section 2. For gene expression data, we learn for each experiment a Gaussian product partition model (PPM) which implies a clustering over genes, see Section 3. The clustering S q learned from the query data is then related to the clusterings S 1 ; .. . ; S M by evaluating the distances d NID S q ; S m ; m  1;. .. ; M, see Equation (8). This approach will be contrasted with two alternative approaches for content-based retrieval previously suggested in the literature. The first one of these is closely related to the proposed approach in that it learns a PPM for each experiment in the database. However, instead of evaluating distances, it evaluates the marginal likelihoods pD q jS m  of the learnt models, given the query dataset. A higher likelihood is then an indication of a higher relevance to the query dataset. A similar approach, albeit for a different model family, was recently suggested in. The term 'modelling-based retrieval' has previously been used byto describe an approach based on probabilistic modelling but using a likelihood as the measure of relevance. To make a distinction between the approach proposed here and approaches based on evaluating likelihoods, we will in this comparison refer to the former as model-distance-based retrieval and the latter as likelihood-based retrieval. See Section 5 for a further discussion about the differences between the two approaches. The second alternative approach, differential expression based retrieval, assumes that a statistical test to detect differentially expressed genes has been conducted beforehand. The method is then based on correlating the gene-specific differential expression p-values of the query experiment with those of the database experiments. An approach similar to this was suggested by. If targeted at differential expression profiles obtained under specific conditions known to be important, this scheme has much potential to achieve good retrieval performance. On the other hand, it assumes more background knowledge and preprocessing of the data than the suggested retrieval schemes based on gene clustering. Here, we do not assume a specific condition of interest but choose in each experiment for the selected 1125 genes the smallest p-values under any of the conditions tested and reported in Expression Atlas. We also experimented with a much larger set of 40569 genes, constituting the maximal common set of genes tested in all experiments, but this resulted in slightly inferior performance. The correlation measure used was Pearson's correlation. We finally note that differential expression based retrieval schemes can also be formulated under the general framework of Section 2 using some appropriate probabilistic model for differential expression, as formulated in e.g.. The results of the comparison between the retrieval schemes are shown in. Here, the model-distance-based retrieval scheme clearly outperforms the two other schemes. A notable feature of the results is the surprisingly poor performance of the likelihood-based approach. This may be due to the well-known fact that gene expression measurements tend to be extremely noisy. In essence, the marginal likelihood pD q jS m  measures how well the query dataset D q is predicted by a model S m , learnt from dataset D m. Even if experiments q and m are in some way related, the idealized model S m may still not provide a good prediction for data D q. Therefore, instead of using the complex and possibly very noisy dataset D q as query input, retaining only the characteristics relevant for retrieval in both D q and D m may help to improve performance, as illustrated in the results.
Biological information in gene clusteringAny single EFO type will necessarily capture only one aspect of an experiment, whereas a meaningful retrieval task usually involves an evaluation of relevance between experiments in terms of a combination of aspects. It is therefore of interest to study the effect of composing the ground truth as a combination of multiple EFO types. In the current experimental setup, the ground truth for each of the EFO types 'cell type', 'disease' and 'organism part', can be represented as a symmetric binary matrix G of dimension M  M, such that entry g i;j  1 iff experiments i and j are mutually relevant. A ground truth which requires a match in t EFO types can then be formed by summing the three matrices and requiring g i;j  t. In, the model-distance-based retrieval scheme is evaluated against ground truth relevances requiring (a) any EFO type to match (t ! 1) (b) two or more matches (t ! 2) and (c) all EFO types to match (t  3). The number of experiments satisfying these conditions are 251, 54 and 6, respectively. Intuitively, the ground truth can be considered increasingly informative as the number of matching EFO types required to declare relevance increases. A retrieval scheme capturing biologically relevant information should then be in better agreement with a more informative ground truth. Although the curves ofand b are not directly comparable due to the differing number of experiments used, the shape of the latter gives an indication of a better agreement. In, owing to the small number of available experiments, the ground truth is compared with the single most relevant experiment (out of five possible ones) retrieved for each query. Here, the retrieval result matches the ground truth in four of the six queries.
Annotations and gene clustering combinedAs noted previously in Section 1, information about experiments may often be missing, insufficient or suffer from variations in terminology () despite a formal declaration of compliance with MIAME criteria (). Hence, even in cases where keyword-based retrieval is of primary interest, it may be advantageous to complement a query with information provided by gene clustering. A straightforward way of combining these two types of information is the following. Assume that a database of M experiments is being queried and that L M experiments are found to match the keyword query. More formally, the result can be encoded as a binary vector of length M with L elements having value 1. A model-distance-based retrieval scheme, on the other hand will return a vector of length M with each element representing the distance of the corresponding experiment-specific model to the query model. Element-wise multiplication of these vectors then effectively induces a ranking of the experiments retrieved in the keyword-based query. The underlying idea is that this ranking will reflect some information which is not present in the queried keyword(s) alone. To test the combined method, we considered all experiments matching in both 'cell type' and 'organism part', resulting in a totalof 43 experiments (all other combinations of two EFO types resulted in significantly less experiments). A match in both of these EFO types was used as ground truth. The idea was then to retrieve experiments assuming only one of the EFO types to be known, complementing keyword-based retrieval with rankings from modeldistance-based retrieval. Retrieving experiments assuming only 'cell type' to be known resulted in an average precision of 0.55 for keyword-based retrieval and 0.61 (mean average precision) for combined retrieval, the corresponding numbers being 0.81 and 0.84, respectively, when only 'organism part' was assumed to be known. In both cases we were able to see a slight improvement in performance for the combined approach, suggesting that keyword-based retrieval may benefit from being complemented with auxiliary information, such as gene clustering.
DiscussionIn this paper, we have introduced a general probabilistic framework for content-driven retrieval of experimental datasets. Compared to earlier works which also employ probabilistic modelling (e.g.), we do not use the likelihood of the query data as a measure of relevance, but instead learn a model of the query data and compare models. We argue that this reduces noise in the query input. With nuisance parameters further marginalized out, only characteristics relevant for the retrieval task are retained. A special instance of the general framework introduced in this paper has been previously used as a comparative method in a simulation study () with performance slightly inferior to a likelihood-based approach. The simulation setting in that earlier study was, however, very simplistic compared to datasets encountered in many real-life scenarios, such as that of Section 4, where the model-distance-based approach was now seen to clearly outperform its likelihood-based counterpart. Contrary to likelihood-based approaches, the model-distancebased approach requires all models under consideration to belong to the same family. Although this may seem somewhat restrictive, in particular for the potential future scenario in which individual researchers independently store models in a repository along with their datasets (e.g.), there are also scenarios where the assumption is feasible. Datasets which arise as a result of some specific type of experiment are often in practice modelled using a fairly standardized set of approaches. In particular, if the models are constructed automatically, or by a curator of a data repository, the assumption of the models belonging to the same family is feasible. As a specific application of the general framework, in Sections 3 and 4 we proposed a retrieval scheme for gene expression experiments based on gene clustering. It turned out, with our current data, that clustering was even a surprisingly good model for this purpose; with minimal preprocessing and prior knowledge about the experiments, it was able to yield reasonable retrieval performance (Section 4.2) and to capture biologically relevant characteristics about the experiments (Section 4.3). Finally, we showed that it is straightforward to combine model-distance-based (or any modelling-based) retrieval with retrieval using available keywords (Section 4.4).. Evaluation of model-distance-based retrieval scheme with respect to a ground truth requiring (a) at least one, (b) at least two, (c) exactly three matching EFO types. The rightmost subfigure compares the ground truth matrix (rectangles with black borders) with the single most relevant retrieved experiment per query (grey rectangles) for the six experiments having a simultaneous match in all three EFO types. Accession numbers for the experiments are provided as a reference
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
P.Blomstedt et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
