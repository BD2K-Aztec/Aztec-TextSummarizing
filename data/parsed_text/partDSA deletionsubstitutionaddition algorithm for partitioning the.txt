Motivation: Until now, much of the focus in cancer has been on biomarker discovery and generating lists of univariately significant genes, as well as epidemiological and clinical measures. These approaches, although significant on their own, are not effective for elucidating the synergistic qualities of the numerous components in complex diseases. These components do not act one at a time, but rather in concert with numerous others. A compelling need exists to develop analytically sound and computationally advanced methods that elucidate a more biologically meaningful understanding of the mechanisms of cancer initiation and progression by taking these interactions into account. Results: We propose a novel algorithm, partDSA, for prediction when several variables jointly affect the outcome. In such settings, piecewise constant estimation provides an intuitive approach by elucidating interactions and correlation patterns in addition to main effects. As well as generating 'and' statements similar to previously described methods, partDSA explores and chooses the best among all possible 'or' statements. The immediate benefit of partDSA is the ability to build a parsimonious model with 'and' and 'or' conjunctions that account for the observed biological phenomena. Importantly, partDSA is capable of handling categorical and continuous explanatory variables and outcomes. We evaluate the effectiveness of partDSA in comparison to several adaptive algorithms in simulations; additionally, we perform several data analyses with publicly available data and introduce the implementation of partDSA as an R package.
INTRODUCTIONBy pinpointing and targeting specific early events in disease development, clinicians aim toward a more preventative model of attacking cancer. These early events can be measured as genomic, epidemiologic and/or clinical variables. Genomic variables can be measured on expression or comparative genomic hybridization (CGH) microarrays, epidemiologic variables with questionnaires, * To whom correspondence should be addressed. and clinical variables with pathology and histology reports. These measurements are then used to predict clinical outcomes such as number of nodes involved or response to treatment as measured by decrease in tumor size. In theory, given the interactions of biological components inherent in carcinogenesis, model building should require the examination of all possible combinations of variables. However, in practice, it is frequently impossible to search over this entire set. Tree-based methods, which rely on the construction of piecewise constant estimators avoid this problem by attempting to approximate the search using recursive binary partitioning. Perhaps the most popular methodology in use today is classification and regression trees (CART ;). There are three main aspects to this tree-structured estimation scheme: (i) the splitting rule for generating the candidate predictors; (ii) the selection of a 'right-sized' tree, referred to as pruning; (iii) estimation of the parameter of interest within each terminal node. Solutions to each of these problems typically involve optimization of a lossbased criterion. Subsequently, the final CART model is represented primarily by a list ofAlthough the CART algorithm does not intentionally build 'or' statements, it is sometimes possible post-analysis to form such conjunctions with two or more terminal nodes. Unlike CART, which fits a piecewise constant estimate within every terminal node, multivariate adaptive regression splines (MARS;) fit piecewise linear functions. The result is a function that is continuous with respect to a continuous covariate. MARS was originally developed for a continuous outcome although adaptations are possible for those that are categorical. After CART and MARS, the most analogous method in the statistical literature is Logic Regression, a novel algorithm that constructs predictors as Boolean combinations of binary covariates (). One restriction of Logic Regression is that it does not allow for continuous covariates, only binary. Therefore, to accomplish the task of aggressively searching a highly complex variable space with a variety of variable types, we propose partDSA: a deletion/substitution/addition algorithm for partitioning the covariate space in prediction. In addition to generating 'and' statements, partDSA explores and chooses the best among all possible 'or' statements to build the most parsimonious model. There are numerous motivating examples of this in carcinogenesis. For instance, during the cell cycle there may be several regions of DNA that have been altered leading to a gain on one chromosome and a loss on another. Although thesePage: 1358 13571363
A.M.Molinaro et al.two mutations may be mutually exclusive, the end result could be similar. As such, we would want to account for 'or' orderings, e.g. 'loss at locus 1 'or' loss at locus 3 'and' gain at locus 2 predicts outcome of 'y', in addition to a list of 'and' statements. In addition, partDSA allows for variables measured as continuous, categorical or ordinal that include covariates from various types of arrays as well as histology, epidemiology and pathology measures. We suggest that partDSA is a flexible and aggressive data-adaptive tool that, depending on the unknown underlying data-generating distribution, will perform as well as, if not better than, previously suggested approaches. For example, CART utilizes a limited set of moves amounting to forward selection (node splitting) followed by backward elimination (tree pruning). In contrast, our proposed algorithm will not only split partitions (nodes in tree estimation) it will also combine and substitute partitions. These additional moves will allow us to unearth intricate correlation patterns and further elucidate interactions in addition to main effects. In the following, we introduce the key elements of partDSA and relevant notation. We report results from both simulations and publicly available data from a cancer study. Lastly, we summarize the method and discuss the advantages and potential limitations of our method as well as the direction of future research.
METHODSpartDSA is based on a unified loss-based methodology for estimator construction, selection and performance assessment employing crossvalidation. In this approach, the parameter of interest is defined as the risk minimizer for a suitable loss function and candidate estimators are generated using this (or possibly another) loss function. Cross-validation is applied to select an optimal estimator among the candidates. Our general statistical framework of the unified loss-based methodology and its theoretical foundations are described in Van der Laan and Dudoit (2003) and, more specific to recursive partitioning, in. To motivate the partDSA algorithm, we begin with the data structure and loss functions specific to univariate outcome prediction. Subsequently, partDSA is formally introduced, including the main components of moves, ordering, and risk functions.Observed data structure: in the univariate outcome prediction problem, we are interested in building and evaluating the performance of a rule or procedure fitted to n independent observations, corresponding to the n independent subjects in a study. Accordingly, we observe a random sample) contains a univariate outcome Y and a collection of p measured explanatory variables, or features,. For example, in microarray experiments W includes RNA or protein expression, chromosomal amplification and deletions or epigenetic changes; while in proteomic data, it includes the intensities at the mass over charge (m/z) values. The collection of features may also contain explanatory variables measured in the clinic and/or by histopathology such as a patient's age or tumor stage. We denote the distribution of the data structure X by F X. The variables that constitute W can be measured on a continuous, ordinal or categorical scale. Although this covariate process may contain both time-dependent and time-independent covariates, we will focus on the time-independent W. The univariate outcome Y may be a continuous measure such as tumor size, a categorical or ordinal measure such as stage of disease, or a binary measure such as disease status. The goal of univariate outcome prediction is to predict Y from the collection of features, W , via the use of statistical learning algorithms. Loss functions for univariate outcome prediction: partDSA employs loss functions for two key stages of model building: separating the observed sample into different groups (or partitions) and selecting the final prediction model. More generally, in the context of piecewise constant estimation (described in Section 2.1.1), the candidate estimators are generated by partitioning a suitably defined covariate space into disjoint and exhaustive partitions. As illustrated in, univariate outcome prediction, multivariate outcome prediction and density estimation can each be handled by specifying a suitable loss function. Here, we will focus solely on univariate outcome prediction and refer the reader to the aforementioned for the loss-functions and implementation of the two other scenarios. Where we observe n observations of X 1 ,...,X n , the parameter of interest,  0 , is a mapping  : S  IR, from a covariate space S into the real line IR. Denote the parameter space by. The parameter  0 can be defined in terms of a loss function, L(X,), as the minimizer of the expected loss, or risk. That is,  0 is such thatThe purpose of the loss function L is to quantify performance. Thus, depending on the parameter of interest, there could be numerous loss functions from which to choose. If the outcome Y is continuous, frequently the parameter of interest is the conditional mean  0 (W ) = E[Y | W ] which has the corresponding squared error loss function, L(X,) = (Y (W )) 2. Another common parameter of interest is the conditional median  0 (W ) = Med, which has the corresponding absolute error loss function L(X,) =| Y (W ) |. If the outcome Y is categorical, the parameter of interest involves the class conditional probabilities, Pr 0 (y|W ). For the indicator loss function, L(X,) = I(Y = (W )), the optimal parameter is  0 (W ) = argmax y Pr 0 (y | W ), the class with maximum probability given covariates W. One could also use a loss function that incorporates differential misclassification costs. Note that in the standard CART methodology,favor replacing the indicator loss function in the splitting rule by measures of node impurity, such as the entropy, Gini, or twoing indices. The indicator loss function is still used for pruning and performance assessment. It turns out that the entropy criterion corresponds to the negative log-likelihood loss function, L(X,) =log(X), and parameter of interest  0 (X) = Pr 0 (Y |W ). Likewise, the Gini criterion corresponds to the loss function L(X,) = 1(X), with parameter of interest  0 (X) = 1 if Y = argmax y Pr 0 (y | W ) and 0 otherwise. These modifications amount to using different loss functions for the same parameter at different stages of the model-building process.
partDSAAs an alternative to previous methods for generating piecewise constant estimates, we describe a completely data adaptive, aggressive and flexible algorithm to search the entire covariate space. Here, we detail how to approximate the parameter space by a sequence of subspaces of increasing dimension and generate candidate estimators for each subspace as well as define the moves and specific ordering of partDSA.
Generating candidate piecewise constant estimatorsAs defined,), where Y is the random outcome and W is a p-vector of baseline covariates. Define a countable set of basis functions, { j : j  IN}, indexed by the non-negative integers IN. These basis functions are simply set indicators {R j : j  I}, which form a partition of the covariate space S, where I is an index set, I  I, and I is a collection of subsets of IN. Here, R j denotes partitions of S that are disjoint (R j R j =, j = j ) and exhaustive (S =  jI R j ). Now, every parameter   can be written (and approximated) as a finite linear combination of the basis functionswhere for a given index set I  I, the coefficients  = ( 1 ,.
..,
partDSAThe complete parameter space can be written as the collection of basis functions { j : j  IN} and represented byIn general, it is not possible to consider all candidate estimators   . Define a sieve, { k }, of subspaces k  , of increasing dimension approximating the complete parameter space aswhere k denotes the index set size (i.e. number of basis functions). Now, for every k we want to find the estimator that minimizes the empirical risk over the subspace k. That can be done by initially optimizing over the regression coefficients   B I for a given index set I and then optimizing over the index sets I. Estimation of regression coefficients  for a given subset of basis functions:For each subspace I , the regression coefficients  are estimated by minimizing the empirical risk, i.e.where P n denotes the empirical measure. It is possible to write the I-specific estimators asasAn example of this is with the squared error loss function;   I is then the least squares linear regression estimator corresponding with the variables identified by the index set I.Optimization over the index sets I: partDSA utilizes three specific moves, or step functions (described below), to generate index sets (i.e. different partitionings of the covariate space) with the goal of minimizing a risk function over all the generated index sets. For a particular partitioning, or index set, I  I, the empirical risk of the I-specific estimator is) is an estimator based on the empirical distribution P n .With the empirical risk function, the algorithm searches to minimize it over all index sets I of size less than or equal to k, where k = 1,...,K. For each k there is a best partitioning that can be denotedThe algorithm searches for an approximation of, which is designated as I k (P n ) and the resulting estimator asas as k = k (P n ). This results in a sieve of increasingly complex estimators indexed by k. Cross-validation is employed for two purposes: to select an optimal estimator among the candidates generated in the sieve and to assess the performance of the resulting estimator (). Both are based on the chosen loss function. For selecting the optimal estimator, an alternative to cross-validation would be to minimize the empirical risk as a measure of error across the entire parameter space. However, this estimate would be highly variable and too data-dependent. Van der Laan and Dudoit (2003) derive finite sample and asymptotic optimality results for the cross-validation selector for general data-generating distributions, loss functions and estimators. The implication of these results is that selection via cross-validation is adequate in thorough searches of large parameter spaces.
Algorithm movesIn addition to generating 'and' statements, partDSA explores and chooses the best among all possible 'or' statements by employing three different step functions: Deletion, Substitution, and Addition. These functions generate index sets I, i.e. partitionings, and map the index set I  I of size k into sets of index sets of size k 1, k, and k +1. The goal is to use these moves to minimize a risk function based on the chosen loss over all the generated index sets. They are defined as: @BULLET Deletion: A deletion move forms a union of two partitions of the covariate space regardless of their spatial location, i.e. the two partitions need not be contiguous. Formally, given a particular partitioning, i.e. an index set I  I, which consists of k basis functions representing indicator functions of partitions, such that |I|=k, we define the set DEL(I)  I as that which contains all possible unions of two disparate partitions. This new set, DEL(I), is of size C k 2. @BULLET Substitution: A substitution move divides two disparate partitions into two (disjoint and mutually exhaustive) subsets each and then forms combinations of the four subsets resulting in two new partitions. Thus, this step forms unions of partitions (or subsets within the partitions) as well as divides partitions. Formally, given an index set I  I, where |I|=k, we define the set SUB(I)  I by splitting all partitions into two subsets each and subsequently forming all unique combinations of the 2k subsets. This new set, SUB(I), is of size 6 * C k 2 , due to the six unique combinations for every two partitions (i.e. four subsets). @BULLET Addition: An addition move splits one partition into two distinct partitions. Formally, given an index set I  I, where |I|=k, we define the setSimilar to CART, 'best split' is the split that most decreases the residual sum of squares for the entire space. However, as all splits are individually examined it is simply the split that minimizes the within-node (i.e. partition) sum-of-squares. By implementing these moves, a sieve of increasingly complex predictors can be generated. Each of these predictors represents the 'best' predictor of size k, where k = 1,...,K. As previously mentioned, to select the best partitioning, i.e. index set, cross-validation is used. A unique and highly important contribution of partDSA is through the Deletion and Substitution steps as this is when unions of partitions are formed. These unions of potentially disparate partitions result in 'or' statements. Thus, we can define subsets of the partitions R j of the covariate space S as S l , where each partition can be a union of several subsets or smaller partitions of the covariate space. The subsets S l themselves are disjoint (S l S l =, l = l ) and exhaustive (S = lI S l ). The predicted value for each of the partitions is constant resulting in a histogram estimate of the regression surface. This allows basis functions, which may be comprised of numerous 'and' and 'or' statements.
Algorithm ordering Havingoutlined the three moves, Deletion, Substitution and Addition and the risk functions, the ordering is the final step of the algorithm. Minimizing the empirical risk function results in a sieve of estimators indexed by k. The vector BEST (k) will be used to store the estimated empirical risk corresponding with the best partitioning of size k. Given the goal of minimizing I  f 1 (I), there are three steps to this process:(1) Initiate Algorithm. The algorithm is initiated by setting the running partitioning, I 0 , to the null set. For piecewise constant regression, the null set is that set which includes the entire covariate space as one partition.@BULLET Else, find an optimal updated I = of size k among all allowed substitution moves, where@BULLET Else, find an optimal updated I + of size k +1 among all allowed addition moves, where(3) Stop Algorithm. If |I|=COG stop the algorithm.When the algorithm is stopped there is a list of best estimators I k (P n ), where k = 1,...,COG. As detailed in Section 2.1.1, cross-validation is used to select the best k. An example in two dimensions (i.e. with Z 1 and Z 2 as covariates) is shown in. partDSA is initiated with all observations in one partition. After performing Addition and Deletion steps the partitioning reads:@BULLET Given a low value for Z 1 AND a low value for Z 2 , the patient has the highest value of the outcome Y .@BULLET Given a high value of Z 1 AND a high value of Z 2 , the patient has the lowest value of the outcome Y .@BULLET Given a low value of Z 1 AND a high value of Z 2 OR a low value of Z 2 AND a high value of Z 1 , the patient has an intermediate value of the outcome Y .There are several ways to limit the number of basis functions for the risk function. The first is to restrict the minimum number of observations in a partition R j or subset S l such that no more splits can occur if that minimum is reached. In CART, this is referred to as 'minbucket.' The second is to require a prespecified improvement in the empirical risk before the BEST () can be replaced (we refer to this as minimum percent difference, or MPD). For example, for a new estimator of size k to replace the current best estimator of size k, with MPD = 0.3, the new empirical risk must be 30% less than that of the best estimator's.
RESULTS
Synthetic dataTo understand the flexibility and aggressive nature of partDSA, we compared it to the best adaptive algorithms in the statistical literature: CART (), MARS () and Logic Regression (). All four algorithms can be used for modeling categorical or continuous outcomes with binary predictors; while all but Logic Regression can also be used with continuous predictors. In the simulations reported below, forpartDSA and CART, the minimum number of observations per node was set to 20, the maximum number of partitions to 10 and the minimum percent difference to 0.01. For both, the best model was chosen by the 1+SE rule suggested in. We used the R packages partDSA and rpart, to run partDSA and CART, respectively. For MARS, we used the R package earth, the penalty was set to two, the number of interactions to four, and the maximum number of model terms to 15. The best model was chosen to minimize the GCV score. For Logic Regression, we used the R package LogicReg, the penalty was set to two and the best model was chosen by randomization. In the results, model size refers to the number of final partitions for partDSA, terminal nodes for CART, terms for MARS and leaves for Logic Regression. In addition to model selection measures, the error rate of the fitted model relative to the true model is reported for each simulation. The first simulation, similar in nature to that of, was generated with 50 training sets each containing 250 observations and nine binary covariates. Fifty independent test sets with 1000 observations each were also generated to evaluate the performance of the predictors built on the training sets. For both sets, variables X1 and X3X9 had an independent Bernoulli(0.5) distribution. Variable X2 had a Bernoulli(0.2) distribution if X1 = 0 and Bernoulli(0.8) otherwise. A continuous outcome was generated from the model Y = 5 * (X1X2)+N(0,,), where = 3 if ( X1X2) and equals 1 otherwise. A CART tree representing this model is shown in. This simulation generates most observations from either terminal node A or C with few in B. Additionally, nodes B and C have the same outcome. The smaller number of observations and higher variance (i.e. = 3) associated with terminal node B is a realistic scenario intended to illustrate the difference between CART and partDSA. Specifically, partDSA will combine terminal nodes B and C providing a more stable estimate of the outcome for those observations. The results are shown in. The second simulation was identical in setting to the first except the outcome was binary as opposed to continuous. The outcome was generated such that Y had a Bernoulli(0.1) distribution if ( X1 X2); a Bernoulli(0.9) distribution if X1; or a Bernoulli(0.6) distribution if ( X1X2). The results are shown in.Page: 1361 13571363 partDSAThe third simulation includes continuous covariates. As a result, partDSA could only be compared to CART and MARS. Similar to the previous simulations, 50 training sets of 250 observations and 50 independent test sets of 1000 observations were generated. However, now X1X9 have an independent Uniform(0,1) distribution. The outcome Y was distributed as follows: Y = 5 * (X1  0.5X2  0.15)+N(0,,), where = 4 if (X1 > 0.5 X2  0.15) and equals 1 otherwise. For partDSA and CART the minimum number of observations in each node was decreased to 15; while the penalty for MARS was increased to 5 and the degree was decreased to 2. The results are shown in. In all three simulations, partDSA chooses the correct predictors and builds a model of the appropriate size. Additionally, in comparison to the other algorithms, partDSA's selected model has the lowest error. CART also chooses the correct predictors but tends to build too small of a model (especially with a continuous outcome in the second simulation), which results in associated errors ranging from slightly larger to double that of partDSA's. MARS selects the two correct predictors but also an incorrect predictor leading to a larger model than necessary in all three settings. With a binary outcome, MARS's error is double that of partDSA's, while it is only marginally larger with a continuous outcome. Logic Regression selects the two correct predictors but has a tendency to also choose an incorrect predictor. This happens more frequently with a binary outcome resulting in too large of a final model. Results from additional scenarios are shown in the Supplementary Material including simulations with continuous covariates and a binary outcome (Simulation 4), a combination of continuous and binary covariates with a continuous outcome (Simulation 5), Simulations 15 with 500 observations in the training set, as well as 500 variables in Simulation 4 (see, Supplementary Tables 18).
Data analysisIn addition to synthetic data, we evaluated the performance of partDSA with publicly available data. One such analysis is presented here, while the others are included in the Supplementary Material. The chosen data analysis for presentation focuses on a diffuse large B-cell lymphoma (DLBCL) study as described in. The purpose of this study was to determine whether the prognosis of DLBCL patients is associated with molecular features of the tumors. As only 3540% of DLBCL patientsassigned subsets of the genes significantly associated with survival into four different gene signatures: germinal center B-cell (GCBcell; 151 genes), MHC class II (MHCII; 37 genes), lymph-node (LN; 357 genes) and Proliferation (Prolif; 1333 genes). For the purposes of their study,used a linear combination of the four gene signatures along with the BMP6 gene score in a Cox's proportional hazard model to predict a survival outcome. Here, we use the same gene signatures and scores as covariates in order to predict lymphoma subtype. For the purposes of our analysis, the total 240 patients were repeatedly divided into training and test sets each of size 120. The patients were randomly assigned to the training or test sets such that each subtype was proportionally represented. This stratification allows for equal representation of all three subtypes such that classification relying on majority consensus is not biased toward any of the three (). In this analysis, the prediction errors among CART, partDSA, and MARS were compared. For CART and partDSA, the analysis conducted employed 10-fold cross-validation in the training set to select the best number of partitions, corresponding to the first minimum over the 10-folds of estimated error. The entire training set was then used to build a model of this selected size and the risk, i.e. prediction error, was subsequently assessed on the test set. The minimum number of observations in each node was set to 20, the maximum number of possible partitions to 10, and for partDSA, the minimum percent difference to 0.2. For MARS, the penalty was set equal to two, maximum number of interactions to four and maximum model size to 15. The analysis was repeated 50 times each time with a different split of the data into a training and test set.reports the average selected model size, the average number of unique variables and the average prediction error for the models chosen by the three algorithms. The models from partDSA and MARS result in similar error rates; both are slightly less than that of CART 's. Importantly, the model size for partDSA is almost half that of MARS meaning that partDSA is describing the same covariate space with fewer coefficients, i.e. more efficiently. Note, a naive estimator with no coefficients, i.e. the predicted value for the test set is set equal to the most frequent outcome value, would result in a test set error of 52%. For illustration purposes, the description of a partDSA model built with one split of the training/test sets follows. In this example, the best number of partitions as chosen by the first minimum of the 10-fold cross-validated error was four. The two gene signatures selected to build the partitions are GCBcell and proliferation. The final partitions and predicted subtypes can be written as:, the numbers of patients within each of the lymphoma subtypes are listed in the columns separately for the training and test sets. The true subtypes are separated in rows by the partDSA predicted subtype. Also, included is the prediction error. From this table, partitions two and four (both predict subtype GCB) have the lowest error for the training and test sets, misclassifying 13 patients in the training set and 17 in the test set. Partition three (predicts subtype ABC) has the next lowest prediction error for both sets, misclassifying 6 out of 27 resulting in a training set error of 22% and 8 out of 29 resulting in a test set error of 28%. The numbers in the first partition reflect the increased difficulty with classifying the Type III subtype. The overall prediction error for the training set is 25% and for the test set is 27.5%. The final model can also be shown graphically as seen in, while a tree depicting the CART analysis for the same data is shown inin the Supplementary Material. Here, for partDSA, the results are shown as a tree where the four terminal partitions are represented by the colors red, green, orange and purple. The data adaptivity of partDSA is visually represented by the same green color oval representing one final partition (similar to CART 's terminal nodes) with predicted GCB subtype. In comparison, the CART tree for the same data only splits on one variable (GCBcell signature) resulting in two terminal nodes with predicted subtypes of GCB and ABC (Supplementary). Thus, the CART analysis does not allot any patients for the third subtype, Type III. Importantly, note that the partDSA partitioning shown inis not equivalent to a CART tree. The reason is that a CART tree with the identical structure would estimate outcomes forof the corresponding subtypes that are also at risk of being pruned via cross-validation. Thus, by choosing the best of the 'or' statements in the form of a single partition, partDSA builds a more parsimonious model that maintains a greater number of observations in the final partitions and, consequently, is better able to estimate the parameter of interest.
ImplementationpartDSA is implemented as an R package (). Currently, the package accommodates continuous and categorical outcomes and covariates. The user has the choice of the v for v-fold cross-validation; size of the minimum partition, minbucket; largest number of partitions to explore, COG; minimum percent difference for splitting, MPD; and, loss function. The choices of loss functions are Gini and negative log-likelihood for categorical outcomes and the squared error for continuous outcomes. Additionally, partDSA is available in serial and parallel versions. As partDSA performs an exhaustive search of the covariate space, the running time of this algorithm is an important consideration. By implementing a parallel version the running time is quite reasonable. To demonstrate, we tested six datasets (available in R), five of them ran in under 30 s and the sixth ran in just over a minute. All simulations were run on the Yale University Life Sciences High Performance Computing Center's Bulldogi, a cluster of 170 Dell PowerEdge 1955 nodes, each containing 2 dual core 3.0 Ghz Xeon 64 bit EM64T Intel cpus, for a total of 680 cores. Each node has 16 GB RAM. The parameter values were set to: minbucket = 6, COG = 10, and MPD = 0.1, the defaults for the R package.shows the improvement in running time between a sequential and parallel version of partDSA.
CONCLUSIONpartDSA is a novel tool for generating a piecewise constant estimation sieve of candidate estimators based on an intensive and comprehensive search over the entire covariate space. The strength of this new algorithm is that it builds precise estimates of the parameter of interest based on 'and' and 'or' statements. These conjunctions allow combinations and substitutions of partitions for the purpose of discovering intricate correlation patterns and interactions in addition to main effects. As such, partDSA provides users an additional tool for their statistical toolbox. Depending on the application, partDSA will supersede other methods by being not only more aggressive but also more flexible. As seen in the synthetic and real data sections, as well as the Supplementary Material, partDSA consistently had a lower error than CART, MARS and, when applicable, Logic Regression. In addition, partDSA selected the correct variables and built more Page: 1363 13571363
partDSAparsimonious models than the other algorithms. Importantly, due to partDSA's formation of an 'or' statement as a single partition, a greater number of observations are assigned to the final partitions in comparison to CART resulting in more precise and stable estimates of the parameter of interest (as described at the end of Section 3.2). Due to a focus on minimizing the empirical risk of an estimator, partDSA can accommodate numerous settings with either continuous or categorical outcomes. As a result, the user can decide which loss function is most appropriate for their application. In the partDSA R package, the squared error loss function is implemented for continuous outcomes as well as the Gini and negative log-likelihood loss functions for categorical outcomes. There are several directions for future work. First, with a continuous outcome, partDSA employs a squared error loss function for two key stages of model building: separating patients into different partitions and selecting the final prediction model. However, an immediate difficulty arises with censored survival data, for the set of observed event times cannot be treated as an uncensored sample; that is, we must modify the loss function so that it can accommodate censored outcome data in a meaningful way. Therefore, the first direction for future work is to expand partDSA for censored outcomes with several appropriate loss functions. Next, to assess the value of an individual variable in the current implementation, partDSA returns a simple frequency of the number of times a variable is selected for each partitioning. The second direction will include exploring more informative variable importance measures and including the relevant ones in the R package.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org 1357 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
