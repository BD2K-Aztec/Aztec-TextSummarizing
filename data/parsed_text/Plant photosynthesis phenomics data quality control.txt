Motivation: Plant phenomics, the collection of large-scale plant phenotype data, is growing exponentially. The resources have become essential component of modern plant science. Such complex datasets are critical for understanding the mechanisms governing energy intake and storage in plants, and this is essential for improving crop productivity. However, a major issue facing these efforts is the determination of the quality of phenotypic data. Automated methods are needed to identify and characterize alterations caused by system errors, all of which are difficult to remove in the data collection step and distinguish them from more interesting cases of altered biological responses. Results: As a step towards solving this problem, we have developed a coarse-to-refined model called dynamic filter to identify abnormalities in plant photosynthesis phenotype data by comparing light responses of photosynthesis using a simplified kinetic model of photosynthesis. Dynamic filter employs an expectation-maximization process to adjust the kinetic model in coarse and refined regions to identify both abnormalities and biological outliers. The experimental results show that our algorithm can effectively identify most of the abnormalities in both real and synthetic datasets.
IntroductionPlants capture sunlight to fix CO 2 into energy rich molecules, thus supplying our ecosystem with O 2 and essentially all of its biological energy, including 100% of our food. Recent work has focused on improving the efficiency of photosynthesis to meet our growing needs for food and fuel (). To develop efficiencyboosting mechanisms that reduce energy losses or enhance CO 2 delivery to cells during photosynthesis, advanced technologies in high-throughput plant photosynthetic phenotyping and phenoinformatics have been developed (). These technologies have allowed plant photosynthesis phenotypic variability to be characterized and to be related to putative biological functions, leading to a better understanding of the underlying mechanisms that control photosynthetic properties under various environmental conditions. Plant phenomics is a first-class asset for understanding the mechanisms regulating energy intake in plants (). Plant phenotyping systems monitor photosynthetic performance for many plants both continuously and simultaneously. Phenomics datasets are large and continue to grow as we increase duration of sampling and resolution. Yet despite the size and richness of the data, small clusters of erroneous values, which give the appearanceof real differences in biological responses, can skew the analysis towards an invalid interpretation (). There are several ways in which a measurement can be in error: errors originating from instrumentation malfunctions, biased values from miscalibrated sensors and inevitable errors of precision. All these issues compromise the downstream data analysis tasks. Given the value of clean data for any operation, the ability to improve data quality is a key requirement for effective knowledge mining from large-scale phenotype data. In this article, we focus on data abnormalities detection, which is a type of measurement errorto demonstrate how clean phenotype data can be obtained. Similar to sensor data, abnormalities in plant phenotype data deviate significantly from expected patterns and are visible outliers in the whole dataset (). The majority of abnormalities in plant phenotyping originate from instrumentation malfunctions (e.g. loss of sensor synchronization during measurement) or non-biological statistical outliers caused by data collection limitations (e.g. deterioration of signal-to-noise ratio for a sample as it progresses through the experiment). Data abnormalities are often viewed as outliers in the whole dataset. Recent work has shown the effectiveness of applying data mining techniques, especially outlier detection, for the purpose of data cleaning (), making it possible to automate the cleansing process for a variety of domains (). In these methods, by detecting the minorities of values that do not conform to the general characteristics of a given data collection, outliers are identified and are considered violations of association rules or other patterns in the data. However, the existing models are not suitable for phenotype data cleaning. These methods, while applied to phenotype data, may remove outliers including both measurement errors and true biological discoveries, since true biological discoveries, to some extent, are outliers as well. Furthermore, detecting abnormalities from long time-series phenotype data requires handling a high temporal dimension, which increases the model complexity. To identify and remove abnormalities in phenotype data and to minimize the deletion of biological discoveries, we have developed a coarse-to-refined residual analysis algorithm, called dynamic filter. Dynamic filter has three key steps: (i) identify abnormal candidates at the coarse level, (ii) refine abnormality identification in a projected feature space and (iii) iteratively identify abnormalities at the refined level. Dynamic filter can speed up the data preparation process and make it more effective. Such improvements will minimize time-consuming and labour-intensive data preparation and increase the significance and confidence in biological discoveries. In summary, our model has the following advantages: @BULLET To our knowledge, dynamic filter is the first work to integrate biological constraints with time-series phenotype data for data cleaning. @BULLET Our model can identify both abnormalities and biological discoveries. @BULLET Dynamic filter outperforms the existing solutions by optimizing the fitness between phenotype data and biological constraints.
BackgroundData cleaning is the process of identifying incorrect or corrupted records in a dataset. The goal of data cleaning is to ensure an accurate representation of the real-world constructs to which the data refer. Removing impurities from data is traditionally an engineering problem, where ad hoc tools made up of low-level rules (such as detecting syntax errors) and manually tuned algorithms are designed for specific tasks (such as the elimination of integrity constraints violations) (). Detection and elimination of complex errors representing invalid values, however, go beyond the checking and enforcement of integrity constraints. They often involve relationships between two or more attributes that are very difficult to uncover and describe by integrity constraints. Recent work has shown the effectiveness of applying techniques from statistical learning for the purpose of data cleaning. In particular, outlier detection methods have made it possible to automate the cleansing process for a variety of domains (). However, none of the existing outlier-detection based methods are suitable for phenotype data cleaning. First, both biological discoveries and errors of detection are difficult to separate from distribution. Second, the cohesiveness rule used in temporal data cleaning is not applicable for the phenotype data, because (i) a non-cohesive time-serial could represent an interesting phenotype pattern rather than an error; (ii) all the observations at the same time point may be similarly affected by a systematic abnormal event (). Alternatively, rather than checking the raw values, residue analysis can be employed to model the differences between the real values and the theoretical curve, which is usually derived from biological constraints such as generalized light reactions (). This is often called the goodness-of-fit model. The goodness-of-fit based data cleaning models can be classified into two categories. First, statistical distribution characters such as mean, standard deviation, confidence interval or range have been used to find unexpected values indicating possible invalid values (). Such simple methods can be efficiently applied to big data. However, these parameters (such as mean) are inclined to be biased by abnormalities with large deviations. Since it does not take into account local characteristics of data, there is a risk of mislabelling a range of normal data as abnormalities and vice versa. Second, combined data-mining techniques are used to identify patterns that apply to most residual records. A pattern is defined by a group of residuals that have similar characteristics (behaviour for certain percentage of the fields in the dataset). Outliers are then identified as values that do not conform to the patterns in the data. Among them, the Hampel filter uses the median of neighbouring observations as a reference value and looks for local outliers in a streaming data sequence (). While the Hampel filter is suitable for temporal data cleaning, it assumes that the data are independent and identically distributed, which is not valid under dynamic environmental conditions. It should be noted that while the goodness-of-fit based data cleaning models focus on the modelling of deviation, they are not aware that the theoretical curve, which is used as the reference, may not always be precise. Typically, theoretical curves derived from biological knowledge are simple compared with the real-world situation. It is therefore inappropriate to directly use the imperfect theoretical curve to infer abnormalities. In this article, we develop a coarse-to-refined residual analysis model called dynamic filter to effectively identify abnormalities in plant photosynthesis phenotype data. Our model derives a theoretical curve from the photosynthetic biological constraints; adjusts the theoretical curve to fit the phenotype data via optimization and studies the deviations of individual phenotype values from Plant photosynthesis phenomics data quality controltheoretical curve. The resulting patterns in residuals indicate abnormalities, which are types of errors of detection, and the optimized theoretical curves reveal true biological outliers.
MethodsIn this section, we first introduce the theoretical curve of time-series steady-state quantum yield data and then introduce a framework for abnormality detection. In this article, the time-series steady-state quantum yield of photosystems II (denoted as U II ) is chosen for abnormality detection for three reasons. First, U II can be readily measured using fluorescence video imaging making it useful for high-throughput phenotyping. Second, because it reflects light-driven electron transfer, it can be used as an indicator of photosynthetic rates and efficiency, albeit with the caveat that it reflects the sum of CO 2 fixation, photorespiration and other processes (O  gren and). Finally, U II is a good demonstration of the approach because it tends to follow, to a reasonable degree, relatively simple saturation behaviours. Given an adequate model, the cleaning procedure described in the manuscript may also be applied to other photosynthetic parameters like non-photochemical quenching (NPQ), which can display complex behaviours.
Theoretical photosynthetic curveAn abnormality in residual analysis is an observation exhibiting a large difference between the theoretical value and the observed value and may indicate a data entry error from the phenotyping sensors. To derive the theoretical curve, we model U II with the photosynthesis-irradiance (PI) curve (see) (). As a derivation of MichaelisMenten kinetics, one of the bestknown models of enzyme kinetics in biochemistry (), PI is modelled as a hyperbolic curve (see) in Equation (1), revealing the empirical relationship between solar irradiance and photosynthesis ().where P is photosynthetic rate at a given light intensity, P max is the maximum potential photosynthetic rate per individual, I is a given light intensity and i 1=2 is half-saturation constant.shows the generally positive correlation between light intensity and photosynthetic rate. The PI curve has already been applied successfully to explain ocean-dwelling phytoplankton photosynthetic response to changes in light intensity () as well as terrestrial and marine reactions.We describe the photosynthetic rate P in terms of linear electron flow () and associate both temporal steady-state quantum yield of photosystems II U II and temporal light intensity i with time t, as shown in Equation (2):where t is a time point in a user-defined temporal region T (t 2 T); U II t and i(t) represent the steady-state quantum yield of photosystems II and light intensity at t; max U II  is the maximal U II in T; and the half-saturation constant i 1=2 is the light intensity at which the photosynthetic rate proceeds at half P max. See proof in Supplementary Section S1. One may reasonably ask if the NPQ or photoinhibition would affect the theoretical model for light saturation. In fact, NPQ has (surprisingly) little effect on the relationship between U II and light intensity, as can be readily seen in the fact that the U II light saturation curves for wild type and the npq4 mutant of Arabidopsis are essentially identical despite large differences in qE (i.e. rapidly reversible photoprotection of NPQ) (). The reason for this apparent disconnect is that, at high light, the slowest step in the light reactions of photosynthesis occurs subsequent to light absorption at the cytochrome b6f complex and is finely regulated by the pH of the lumen (). Light absorption become rate limiting only at NPQ levels much higher than those observed here. The biological role of NPQ under most conditions appears to be in regulating electron transfer but in preventing the build up of reactive intermediates within the photosystem II reaction centre (). Thus, the effects of moderate levels of NPQ and photoinhibition should have little effect on the behaviour of the wild-type system. However, under extreme conditions of in mutant lines with altered behaviour producing high levels of NPQ or photoinhibition, we expect to see behaviour that deviates from that produced by the model. These instances will be detected as outliers and flagged for further investigation of possible biological discoveries. Consequently, the half-saturation constant i 1=2 can be learned using all U II t and i(t) in T with a non-linear regression method (). Note that the half-saturation constant can be dramatically different between plants and between leaves in plants. Thus, the general shape of the curve is typically maintained but not its maximal or half-saturation light intensity. Finally, given i 1=2 , the residual value at each time point t is defined aswhere rsd(t) is the residual value at time t; and U II t is the observed value and U 0 II t is the theoretical value of steady-state quantum yield at t calculated using Equation (2). We note that there are multiple models for PI curves, which give similar responses to light (. In this article, we chose the MichaelisMenten kinetics model because it is convenient to use and fits plant photosynthesis rate data well (see). It should be noted that an important feature of our approach is that these alternative models can be easily added or substituted for comparison.
Framework of dynamic filterDynamic filter is a coarse-to-refined residual analysis approach, which has three major steps as shown in. We defineNote that confidence interval a  99% is commonly used in literature (), but is adjustable by users. In this article, by adopting the concept of confidence interval, we assume that (i) the majority of the phenotype values are correct, and (ii) they form the major distribution in the residual data, which is also distinctly different from the distribution(s) of the residual data of the abnormalities.Step 1. Coarse process to identify abnormal candidates Given a set of phenotype data U II , we adopt Equation (2) to generate the theoretical values of steady-state quantum yields for each plant, denoted as fU 0 II g, by using the whole time-serial as temporal region T, aka the coarse level. For the dataset used in Section 4, the smallest value of time interval is 10 min, and the scale of T in the whole dataset is 3 days. Consequently, we generate the residual data of all plants {rsd} using Equation (3), and model them using a Gaussian mixture model (GMM) (see details in Section 3.3 and example in). Finally, we generate the abnormality candidate set fU abn g with Definition 1. In, solid points are abnormality candidates in the coarse process. Clearly, because of the simplified PI curve model, not all the abnormality candidates are correctly identified.Step 2. K-Nearest Neighbors (KNN) process to refine abnormality identification Abnormality candidates may have certain intrinsic patterns of distribution highly related to certain ranges of feature space. For example, accidental dysfunction of data-capturing devices may cause abnormalities concentrated around some regions, which form statistical patterns on the distribution plot of the feature space. From a statistical viewpoint, abnormalities should be away from normal values in the feature space, and values with similar features tend to have the same labels. This leads to a refinement process to exploit the patterns of abnormalities candidates on selected feature space, and to make use of these patterns to refine abnormality identification, as described in Algorithm 1. Specifically, we first select the optimal features from U II , rsd, i, t, etc., in which abnormalities and normal values are maximally separated. To solve this feature reduction problem, linear discriminant analysis (LDA) is adopted to get the principal components of the optimal feature space (Algorithm 1 line 920, see details in Section 3.3). Second, we apply K-nearest-neighbour approach on the selected feature space, such that each abnormality candidate will be relabelled as its majority label of k-nearest-neighbours (Algorithm 1 line 47) ().
Step 3. Refined process to identify abnormalities in local regionsBecause the theoretical values fU 0 II g are learned with the simplified PI curve model at the coarse level, not all the assignments of theDEFINITION 2: Temporal Checking Region. A checking region r consists of at most m normal values flanking the selected abnormal candidates, depending on data availability, denoted as fU nor g, and at most n abnormal candidates such that the last abnormal candidate is constrained to be at most l-timepoints away from the first one, denoted as fU abn g. In Definition 2, m, n and l are user-defined parameters that determine the size of a temporal checking region. A check region has at most m  l  n normal values and at most n abnormities. Note that abnormal candidates can be continuous or discontinuous, and two checking regions may share common normal values. In the refined process, an expectation-maximization (EM) process is employed to repeatedly optimize the results in each temporal region r. Pseudo-code of the EM process is shown in Algorithm 2. In the E step, using the local normal values fU nor g in checking region r as inputs, we regenerate the theoretical values fU 0 g with Equation (2). Then the residuals {rsd} for both the abnormal candidates fU abn g and the normal values fU nor g are regenerated using Equation (3) (Algorithm 2 line 68). In the M step, we redefine the abnormal candidate set fU abn g with the statistical distribution of the new residual data {rsd} according to Definition 1. Specifically, a value falls off the confidence interval threshold of the major distribution of the normal residual values will be moved to fU abn g; and if an abnormal candidate is within the confidence interval threshold of the major distribution of the normal residual values, it will be labelled as normal and be moved to fU nor g (Algorithm 2 line 10 and 11). The EM process will stop when the label assignment is stable.(cf) shows the iterative process in a checking region. Since checking regions may share common values, the results from different regions may be conflicted. For example, a phenotype value is identified as an abnormality in one region but is considered a normal value in another region. To solve conflicts and consequently improve performance, we employ an information sharing process in the end of the EM process to broadcast all the local results to all the checking regions. If conflict exists, voting results will be used to redefine abnormal candidates in the selected feature space (Step 2), and the EM process will rerun on the new checking regions. The process will repeat till the results converge.and h demonstrate that all the abnormalities are identified.
Related WorksWe introduce the GMM and the LDA used in Section 3.2 as follows.
Gaussian mixture modelA GMM is a parametric probability density function represented as a weighted sum of Gaussian component densities (). GMMs are commonly used as a parametric model of the probability distribution of continuous features (). The probability density function is given by the equation:where x is a D-dimensional continuous-valued vector, x i , i  1;. .. ; M, are the mixture weights, and gxjl i ; R i ; i  1;. .. ; M are the component Gaussian densities. Each component density is a D-variate Gaussian function of the form:with l i be the mean vector and R i be the covariance matrix (i  1;. .. ; M). The mixture weights satisfy the constraint that P M i1 x i  1. GMM parameters are estimated from training data using the maximum likelihood parameter estimation or maximum a posteriori estimation (). In this article, residuals are 1D scalar data, we use l i and r i to represent the mean and variance of residuals. 3.3.2 LDA for feature selection LDA is a method used in statistics, pattern recognition and machine learning to find a linear combination of features, whichAlgorithm 1 KNN process to refine results 1: procedure Refine(W; C ; k) 2: . W is original feature space, C is the set of labels (abnormality or normal) 3: W proj FeatureSelection W; C 4: for w i in W proj do 5: C i majority label of k-nearest-neighbours 6: end for 7: return C 8: end procedure 9: procedure FeatureSelection(W; C) 10: l 1 jCj P w 11: for i from 1 to 2 do . process both kinds of labels in C 12: W i Features of i th label 13:is projected space 20: end procedure Algorithm 2 EM optimization on each local region r procedure EM_Optimization(U; i; a) 2: . U is phenotype values in a local region, i is light, a is confidence interval Let U nor and U abn be normal values and abnormalities in U 4: repeat E-step:rsd min ; rsd max ) 12: until U nor and U abn are stable end procedure characterizes or separates two or more classes of objects or events, such that the inter-class variance is maximized and the intra-class variance is minimized (). The resulting combination may be used as a linear classifier, or more commonly, for dimensionality reduction before later classification. In this article, we seek combination of features, with which normal values (one class) are centred around one area, while abnormalities (another class) are centred around a distinctively separated area. Suppose there are C classes, and each class has n i points, mean l i and intra-class variance R i. Then the inter-class variance may be defined by the sample covariance of the class means:and the intra-class variance of whole dataset is SW  P C i1 SW i (). The class separation in a direction x ! in this case will be given by:The objective function is to maximize S and it can be shown that when x ! is the eigenvector of SW 1 SB, S will have maximized value corresponding to eigenvalue ().
ExperimentWe compared dynamic filter on both real and synthetic datasets with two widely used data cleaning algorithms: (i) a statistical approach that classifies abnormalities based on standard variance () and (ii) Hampel filter that identifies abnormalities based on digress from median of trends (). Note that all the three methods were applied on the same phenotype residual data for a fair comparison. For performance evaluation, we used both the precision-recall curve and the Matthews correlation coefficient (MCC) (). The MCC that can appropriately represent a confusion matrix is computed with:
Real phenotype datasetWe first tested the performance of dynamic filter using the plant photosynthetic phenotype data consisting of 106 Arabidopsis thaliana plants (confirmed T-DNA insertion mutants and wild types) sampled at 64 time points under dynamic light conditions (). The photosynthetic phenotype values vary dramatically across plants, reflecting potential differences in development, stress responses or regulation of processes such as stomatal conductance, photodamage and storage of photosynthate (). Experts went through the data and manually marked the ground truth of abnormalities, and found the error rate is 6.5%. The experimental results shown inindicated that dynamic filter is significantly better than the other two approaches in the precision-recall curve. Specifically, dynamic filter yields Area Under Curve (AUC) as high as 0.964, higher than the AUC of simple statistics and Hampel filter (0.147 and 0.543, respectively).shows our model is also significantly better according to MCC. Furthermore, it shows that dynamic filter is insensitive to the selection of the confidence interval threshold, which is distinctly different from the other algorithms that rely on well-picked parameters. Note that the AUC of dynamic filter without KNN is 0.862 (), implying that KNN refinement (Step 2) is a key component of dynamic filter. Specifically,shows how KNN refinement improved the performance of data cleansing. On the U II versus residual plot shown in(detailed visualization on), some isolated normal values are misclassified as abnormalities, and certain abnormalities misclassified as normal values. Clearly, these values do not conform with the most nearby values. By applying KNN refinement, this misclassification is effectively corrected (and d). We systematically tested the performance of the different components of dynamic filter.shows the performance improvement by comparing dynamic filter with a model without KNN refinement (v5), iteration of EM (v4), consensus on all regions (v3), reassignment of normal values and abnormalities in EM (v2) or even without the whole refined process (v1). It implies that the refined process, especially the KNN and EM refinement, is the key of performance improvement. Figures 7 and 8 show case studies on the real data. In, the experiment was run on a wild-type reference plant, Arabidopsis Col-0.
Plant photosynthesis phenomics data quality controlIn the coarse process, the residual analysis was applied to identify the abnormal candidates (and solid points in). Clearly, six solid points on the bottom were incorrectly labelled as abnormalities, which were gradually corrected in the refined process (and d).shows a true biological discovery on the real data. Our screen revealed accession ELY exhibiting photosynthetic characteristics markedly different from the reference (Col-0). It would however be labelled as abnormal and subsequently deleted by the existing outlier-detection based data cleaning methods, resulting in over-clean problem. Dynamic filter identifies ELY correctly and suggests that the differences in its quantum yield are caused by the monotone decrease of i 1=2 regardless the change of sunlight (see). The non-negligible deviation between the observed values and the theoretical curve learned from the coarse phase of dynamic filter (see) implies the theoretical model is simple compared with the realworld situation. Instead of directly use the PI curve to infer abnormalities, we optimize the fitting results in the refined phase of dynamic filter, resulting in almost perfect match between the observed values and the theoretical curve (see). Furthermore, we varied the size of the temporal checking region and compared the performance in. The results inreveal that dynamic filter achieves the best performance when m is between 10 and 15. This number allows enough training data for the refinement process, meanwhile avoiding NPQ variation over long time interval.shows that performance of dynamic filter is relatively stable against max number of abnormalities n, implying that robustness of dynamic filter is high.
Synthetic datasetSince the true biological discoveries in the real data are unknown, we further tested dynamic filter on serials of synthetic datasets. The synthetic datasets were generated by varying four parameters systematically: lights and i 1=2 being smoothly or abruptly changed, abnormalities being continuously or discontinuously distributed, and error ratio being low or high. Furthermore, we added variations representing abnormalities and biological discoveries (different i 1=2 values) in the synthetic datasets. In total, 63 kinds of synthetic datasets in nine groups were generated, and for each kind of synthetic data, we repeatedly generated 100 datasets.shows the robustness of dynamic filter on different synthetic datasets generated under nine different settings. The performance is evaluated using MCC on both abnormalities and on biological outliers. Each figure represents synthetic data generated under different settings (see details in supplementary section S2). Each point inrepresents a MCC score of biologicaldiscovery identification at x-axis and a MCC score of abnormality identification at y-axis. The highest possible value is (1.0,1.0). The experimental results show that dynamic filter (red circle) is better than the other two methods in almost all the synthetic datasets. This is because dynamic filter can identify and remove abnormalities while reserving biological discoveries (see supplementary Tables S1 and S2 for performance comparison on MCC and true positive rate, respectively).
ConclusionWith an aim towards identifying targets for improving energy yield, advanced technologies in high-throughput plant photosynthetic phenotyping have been developed (). These systems can be used to quantify photosynthetic behaviour in genetically diverse populations and to draw relationships among genotype, phenotype and biological function, leading to a better understanding of the underlying mechanisms that control the photosynthetic properties under various environmental conditions (). As a consequence of the long-time high-throughput plant phenotyping, the scale of plant phenomics data grows exponentially. However, the quality of phenotype data may be skewed by sources of noise that are difficult to remove in the data collection step. The purpose of plant phenotyping is to discover phenotype values that are significantly different from a reference. But phenotype values leading to biological discoveries may be obscured by abnormal values caused by errors during detection. To ensure high data quality, effective data cleaning should be considered a primary task. However, since advanced data cleaning algorithms are primarily based on indiscriminate outlier detection, they may remove both abnormalities and biological discoveries not separable in the data distribution. We have developed a new coarse-to-refined model called dynamic filter to effectively identify both abnormalities and biological discoveries by adopting a widely used photosynthetic model. Specifically, dynamic filter is a residual analysis approach by dynamically tracing statistical distributions of all samples rather than individuals, and incorporating EM for performance optimization in refined checking regions. We note that certain events, such as transient changes in growth environment, could introduce signals similar to growth lighting malfunction, which could be wrongly labelled as abnormalities by dynamic filter. Therefore, instead of automatically deleting all the predicted abnormalities, we send all of them to domain experts for confirmation. Meanwhile, all raw data are kept for any rollback operation. Experimental results show that our model is significantly better than the existing data cleaning tools on both real-phenomics data and synthetic data. Dynamic filter may have a wide impact because of the rapid increase of large-scale phenotyping technologies. It should be noted that although we used a photosynthesis-specificPlant photosynthesis phenomics data quality controlcurve, the model itself is independent of actual biological constraints. In principle, our approach can be used to clean data for any number of phenotypes as long as suitable theoretical curves can be derived for their behaviour. Implementation for new use cases would involve substituting the appropriate theoretical curve into the program, calculating the residuals of fits to the datasets and optimizing the fitting procedure as described in.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
L.Xu et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
