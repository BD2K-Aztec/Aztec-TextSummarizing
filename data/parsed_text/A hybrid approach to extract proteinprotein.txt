Motivation: Proteinâ€“protein interactions (PPIs) play an important role in understanding biological processes. Although recent research in text mining has achieved a significant progress in automatic PPI extraction from literature, performance of existing systems still needs to be improved. Results: In this study, we propose a novel algorithm for extracting PPIs from literature which consists of two phases. First, we automatically categorize the data into subsets based on its semantic properties and extract candidate PPI pairs from these subsets. Second, we apply support vector machines (SVMs) to classify candidate PPI pairs using features specific for each subset. We obtain promising results on five benchmark datasets: AIMed, BioInfer, HPRD50, IEPA and LLL with F-scores ranging from 60% to 84%, which are comparable with the state-of-the-art PPI extraction systems. Furthermore, our system achieves the best performance on cross-corpora evaluation and comparative performance in terms of computational efficiency. Availability: The source code and scripts used in this article are available for academic use at
INTRODUCTIONExtraction of proteinprotein interactions (PPIs) from literature is an important research topic in the field of biomedical natural language processing (NLP;). Numerous PPIs have been manually curated and stored into databases, such as BIND, HDPR and IntAct. However, this task has been proven time and resource consuming. As a consequence, most data on PPIs can only be found in literature (). Several approaches for extracting PPIs from biomedical text have been reported. These methods range from co-occurrence to more sophisticated machine learning (ML) systems augmented by NLP techniques. Co-occurrence is the simplest approach, which results in high recall but low precision. Rule-or pattern-based approaches can increase precision but significantly lower recall. In addition, these rule sets are derived from training data and are therefore not always applicable to other data they are not developed for (). * To whom correspondence should be addressed.Recently, many ML-based methods have employed NLP techniques such as shallow parsing or full parsing (). Since full parsing produces more elaborate syntactic information than shallow parsing, PPI extraction systems based on full parsing can potentially yield better results (). The output of the parser can be represented either as constituent trees or dependency trees. In this case, the PPI extraction task is treated as a binary classification problem which requires a formal protein pair representation and a suitable ML method. A protein pair (an instance) can be represented by a set of features, which are derived from the sentence or its syntactic structure. A ML method is then used to distinguish between positive and negative instances (). Many linguistic features and ML methods have been proposed for the PPI extraction task. Based on feature types, these approaches can be divided into three groups. The first group focuses on lexical and word context features.designed a subsequence kernel which uses the following information in a sentence: before the first protein, between two proteins and after the second protein, and combined these features to obtain patterns.extended this approach by using a bag-ofwords (BOWs) and adding a local context kernel. The second group exploits syntactic features of a sentence.used various syntactic path features and context features related to words before, between and after two interacting entities.proposed a method based on information found in the predefined levels of the dependency trees, such as local dependency contexts of the protein names and tree's roots.enhanced previous work by proposing a walk kernel which explores the shortest dependency path between two proteins and a modified dependency tree with the parts-of-speech (POS) features. As an alternative to previous approaches,introduced an all-paths graph kernel. They represented a sentence with a dependency graph and considered dependencies connecting two entities outside the shortest path as well as on the shortest path. Along with the proposed methods,also studied individual impact of a variety of feature types on the PPI extraction task. In addition, a study ofhas shown that the accuracy of syntactic parsers also contributes to the overall performance of the PPI systems. To compensate for the limitations of each individual feature set and parser errors,proposed a method that combines all the lexical and parsing features using multiple kernels and parsers. Their system achieved the state-ofthe-art performance on a number of benchmark datasets. However,
Q.-C.Bui et al.the studies byalso demonstrated that if two feature types have overlapping rather than complimentary effects, dropping one of them can result in a computationally more efficient method and potentially make a mining algorithm more robust. This argument was also confirmed in the work ofwho showed that excluding the BOWs feature leads to better performance on the AIMed corpus. Although many approaches have been proposed in the past, the problem of finding the most suitable features for extracting PPIs remain. Adding more features might sometimes improve performance, but they can introduce noise in other cases, or require more computational resources (). In this study, we propose a novel method that consists of two phases. First, we apply semantic rules to partition the dataset into subsets according to its semantic properties and extract candidate PPI pairs from these subsets. Second, we introduce enhanced feature sets for use with a ML classifier to classify these extracted PPI pairs. To the best of our knowledge, this is the first method that categorizes data into subsets and selects the most appropriate features for each subset. As a result, we increase the robustness of the learning method and make it computationally effective. In general, a PPI extraction system consists of two subtasks: recognizing protein names (NER) and extracting PPI pairs. However, this study only focuses on the PPI extraction task with an assumption that relevant named entities were given.
METHODSThe workflow of the proposed system is as follows:(1) Text preprocessing.(2) Extracting candidate PPI pairs.(3) Classifying extracted PPI pairs.
Text preprocessingText preprocessing includes converting protein names using a predefined rule set, filtering out input sentences with only one protein, splitting input sentences contain multiple clauses, and parsing sentences using the Stanford lexical parser ().
Processing protein namesIn order to improve accuracy of the parser, we replace all mentioned protein names with a place holder, i.e. PRO1, PRO2 (we refer to them as PRO*). We define a rule set to resolve the problem with embedded protein names (e.g. AIMed corpus), protein names that share prefix or suffix (e.g. AIMed and BioInfer corpora) and protein names including multiple positions. After this process, the number of proteins in the sentence is not changed. The list of original protein names for each sentence is maintained for reference purposes.
Replacing parenthetical remarks and splitting a sentenceIf no word inside parentheses is a protein name, then all words and the parentheses are removed. In case the sentence consists of multiple clauses, the system splits it into clauses. The resulting sentence is referred to as a simplified sentence. In the last step of text preprocessing, a simplified sentence that contains at least two protein names is analyzed with the Stanford lexical parser to produce a syntactic tree. All parse trees are stored in a local database for later use. An example of a sentence and its output after text-preprocessing step is given below:
Extracting candidate PPI pairsPrevious studies () have shown that, in biomedical text, relation between two entities (protein protein, proteingene, drugmutation and others) can be expressed in the following abstract forms:Here, REL is a cue word (interaction, inhibit, etc.) and can be a noun or a verb, word * are tokens between PRO* and REL. PRO i and PRO j can be any protein pair with j  i+1 (e.g. <PRO1, PRO2>). In addition, a closer look at the annotated corpora (e.g. AIMed and BioInfer) reveals that we can define two more forms: Form 4 (compound form): PRO i /PRO i+1 or PRO i PRO i+1 or PRO i-PRO i+1 , if two entities appear in the sentence with the patterns above, they seem to have interaction.Form 4 is expressed as a pattern and requires an exact match. For other forms, there might be one or more tokens between <PRO i , REL>, <REL, PRO j > or <PRO i , PRO j >. Based on these basic forms, we now map the semantic relations of these forms into parse trees. For convenience, we define the following patterns:
Example: PRO1/PRO2 binding; PRO1-PRO2 compound.(a)Here S (clause), NP (noun phrase), VP (verb phrase), PP (prepositional phrase) and SBAR (subclause) are constituents of the parse tree, PRO* is a place holder for a protein name, and REL is a cue word of the input sentence. All patterns above are written using the Tregex syntax (http://nlp.stanford.edu/software/tregex.shtml), which is developed within the Stanford parser package.illustrates some parse trees with the patterns mentioned above, e.g. the parse tree inshows a full clause,shows a NP pattern andshows a subclause.In the following section, we describe the procedures and algorithm to extract candidate PPI pairs from a parse tree or a simplified sentence: Procedure for Form 1: this procedure requires a parse tree which contains full clause, partial clause or subclause patterns. The procedure is as follows:
Relation list(a) Check a head word of VP that corresponds to satisfied pattern. If this verb belongs to the relation list, use it as REL.((c) parse tree satisfies subclause pattern. Some features (D1, D2, H1, H2) and paths from the join node connecting a given protein pair (PRO1, PRO2) are also shown in (a).
A hybrid approach to extract PPICreate a left_keys list: use all keys in NP if a satisfied pattern is full clause or subclause. If a pattern is partial clause, then find a sister NP immediately preceding VP.(d) Form candidate PPI pairs: enumerate the left_keys and right_keys to compose a triple <PRO i-REL-PRO j >.Procedure for Form 2: form candidate PPI pairs from a simplified sentence if they satisfy the following form: PRO i word * REL word * PRO j. Procedure for Form 3: this procedure requires a parse tree which contains NP pattern. The procedure is as follows:(a) Check a head word of NP that corresponds to satisfied pattern. If this noun belongs to the relation list, use it as REL.(b) Find a proposition as a splitter in pattern's PP phrase in order to create left_keys and right_keys lists.(Procedure for Form 5: form candidate PPI pairs from a simplified sentence if they satisfy the following form: PRO i word * PRO j word * REL,and if the distance from the PRO i to the REL is shorter than five tokens. For these procedures, the extracted candidate PPI pairs obtained from the same procedure are grouped together, i.e. outputs from Form 1 are grouped into group 1. Among these groups, group 2 overlaps group 1 when REL is a verb, with the purpose to recover PPI pairs that the group 1 failed to extract due to the parser errors (). As a consequence, in some cases, a PPI pair may belong to more than one group. Therefore, the order in which patterns are applied is important (as described in the Algorithm 1 below).
FeaturesIn this section, we propose feature sets for ML classification. Our feature sets are combinations of some features that were previously proposed by) and. For each candidate PPI pair extracted from an input sentence, the system outputs a triple, e.g. <PRO i , REL, PRO j > then the following features are generated: Keyword: is a relation word (REL) from the extracted triple. In addition, we also count the number of protein names (C1) and relation words (C2) in each simplified sentence. Distance: we use two features: D1, D2 () to measure the distance (number of words/tokens) between PRO i-REL and REL-PRO j (or between REL-PRO i and PRO i-PRO j for group 3). Height: we use two features: H1, H2 () to measure the distance from the joint node connecting PRO i and PRO j in the parse tree. These features are similar to D1 and D2 except that they measure the number of nodes on the paths from a local root to PRO i and PRO j. POS: we use two lists: Pos1, Pos2 () to store POS and syntactic features from the joint node connecting PRO i and PRO j in the parse tree, respectively. Lexical: this feature is a modification of a BOWs. Instead of using all tokens, we only consider tokens that belong to the relation list and a list of prepositions: and, or, by, through, in, of, to and between. If a token is a protein (PRO*), then its value is replaced with 'KEY'. We use four lists of tokens: L1: a list of tokens between PRO i and REL, or between REL and PRO i for group 3. L2: a list of tokens between REL and PRO j , or between PRO i and PRO j for group 3.Selection of features for each individual group: the important benefit of partitioning data into subsets is that we can select the most appropriate features for each subset. Let us consider the following two cases: a PPI pair in form 4 (compound form) and a PPI pair in form 1 (full clause). For a PPI pair in a compound form, e.g. PRO1-PRO2, the shortest path features proposed in the previous work become useless because no feature can be extracted from this path. In this case, the BOW features seem to be the most appropriate ones. In contrast, for the PPI pairs in form 1, e.g. PRO1 interacts with PRO2, the shortest path, and the tokens between PRO1 and PRO2, play an important role. Based on the properties of each group of extracted PPI pairs, we manually select features that are potentially suitable for that group.shows the list of features corresponding to each group. Furthermore, we also use the Ranker (attribute selection) method from the WEKA ML package () to determine the length of the POS and lexical lists, which are limited to maximum six attributes. ML method: Support vector machines (SVMs) have been widely used in the PPI extraction task, and has shown competitive performance over other learning methods (). In this work, we use SVM classifier with a default RBF kernel and tune the complexity parameter C by using CVParameterSelection function from the WEKA. All individual features mentioned above are combined into a single feature vector for the classification task. Depending on each group of PPI candidates, the number of features ranges from 14 to 26 as shown in. For example, for a candidate PPI pair <PRO1, PRO2> extracted from group 1 (as shown in), the following features are generated: REL: mediate; D1: 1, D2: 6; C1: 3, C2: 2; H1: 5, H2: 4; L1: null, null, null; L2: association, with, null; Pos1: NP, PP, NP, NN, null, null; Pos2: VB, PP, NP, NN, null, null, true. For more details on text preprocessing, PPI extraction and feature generation steps, see Supplementary source code provided in this article.
Page: 262 259265
Q.-C.Bui et al.
RESULTS AND DISCUSSION
DatasetsWe use five corpora (http://mars.cs.utu.fi/PPICorpora/GraphKernel .html) which have been converted into a unified format and are provided by: AIMed, BioInfer, HPDR50, IEPA and LLL.shows the quantitative information of all five corpora.
Evaluation methodsWe perform two types of evaluation, a single corpus test and a cross-corpora test. In the single corpus test, we evaluate the performance of the proposed method by 10-fold abstract-wise crossvalidation (CV), and use the one-answer-per-occurrence criterion (). We split the corpora as recommended by Airolain order to directly compare our results against the performance of other systems. In the cross-corpora test, we conduct two experiments. First, we use one corpus for training and test the system on the four remaining corpora. Second, we use four corpora (ALL) for training and test on the remaining corpus. Precision, recall and F-score are used as evaluation metrics. Let TP denote numbers of true positives, FP denote the number of false positives and FN denote the numbers of false negatives. The measures are defined as follows:shows the results of the PPI extraction algorithm on five corpora. In order to calculate recall for each corpus, we use the number of original positive pairs from(e.g. for AIMed, TP + FN = 1000, for BioInfer, TP + FN = 2534). It is worth noting that, in some systems (), selfinteraction pairs are removed prior to the evaluation; therefore, the F-score can be higher if we adopt these settings.also presents the properties of each extracted group in each corpus and the differences between corpora. Clearly, group 1 is the most common among all corpora and accounts for >50% of TP pairs of all corpora except for BioInfer. The first three groups are also common for all five corpora and account for >80% of the extracted pairs. However, groups 4 and 5 are more corpus-specific and can be found mostly in AIMed and BioInfer corpora. This information provides more insight for understanding the properties of five corpora studied previously by).The results indemonstrate that the proposed algorithm is capable of extracting of >80% of positive pairs (recall > 80%) on AIMed, HPRD50, IEPA and LLL corpora. This means that these four corpora share some common patterns or the same annotation policy. However, our method can only extract 67% positive pairs from BioInfer. To find out why our algorithm has lower recall on the BioInfer corpus, we examine sample sentences from BioInfer which the system failed to extract. By analyzing these sentences, we discovered that in some cases, BioInfer has a special annotation policy. Consider, for example, the following sentence:
Performance of PPI extraction algorithm
A hybrid approach to extract PPI
BioInfer.d436.s0: Moreover, ectopic expression of PRO0 and PRO1 can stimulate the PRO2 promoter in an PRO3-dependent manner, and this is inhibited by coexpression of the PRO4 (PRO5) PRO6.For this input sentence, our system extracts only four TP pairs, but the number of positive PPI pairs provided by the BioInfer corpus is 11 (see sentence BioInfer.d436.s0 in BioInfer corpus for more detail). Therefore, to increase recall on BioInfer, additional study of this corpus is needed.also shows that the performance of algorithm 1 is comparable with other ML-based systems. Moreover, it outperforms the best rule-based system on all five corporafor more details].
Single corpus evaluationWith the extracted PPI pairs obtained in the previous step, we use a standard SVM classifier to classify them. Comparing with the original data from, the number of positive and negative instances of the AIMed and BioInfer corpora are quite balanced (). To calculate the F-score, we accumulated all TP and FP pairs from the 10-fold CV test (reported by WEKA via the confusion matrix). For TP + FN values, similarly to Section 3.3, we use the original positive pairs of each corpus in. For the IEPA, HPRD50 and LLL corpora, however, we only use the first three groups of extracted pairs because the number of candidate pairs in the groups 4 and 5 is too small for the 10-fold CV.shows the performance of the classifier on five corpora. The results indicate that by using our feature vectors, the classifier can further boost the performance of the overall system. When comparing precision on each corpus before and after applying classification, we can see a significant increase in precision (P values). Among all corpora, the AIMed corpus gains the most increase in precision with P of 14%. Even on a small corpus like LLL, with already very high precision, the final precision still gainsshows that our system achieves the best performance on three corpora: IEPA, HPRD50 and LLL using 10-fold CV when compared with other results reported so far. To study the benefit of filtering data (obtained after applying rules on the full dataset) and partitioning data, we conduct the experiment in which the full dataset and the filtered dataset use all features in. However, in this experiment the REL feature is not available, D1 and D1 features are replaced by D1+D2 and L1 and L2 features are merged.summarizes the performance of the system on the full dataset (all PPI pairs), filtered dataset and partitioned dataset. The results fromshow that when evaluating on filtering dataset, the system has better performance on all five corpora compared with full dataset. In addition, the performance further improve on three largest corpora (AIMed,BioInfer and IEPA) when we partition data and select feature specific for each sub-dataset. This clearly shows the benefit of our hybrid approach that combines rule with partition data.illustrates a comparison of our system (BKS) against recent work on the AIMed corpus, which is considered the de facto benchmark for the PPI extraction task. However, the comparison may not be straightforward because these PPI systems can use different text preprocessing and learning methods. The results show that our system with its F-score of 61.2% is comparable with the state-of-the-art systems proposed by. In addition, our approach significantly outperforms other methods based on one parser's output.
Cross-corpora evaluationIn addition to the standard 10-fold CV, recent studies also suggested to test existing approaches to PPI extraction using cross-corpora. This type of evaluation can show whether a system trained on one corpus can perform equally well on other corpora, with an Page: 264 259265Columns correspond to training and rows correspond to testing corpora. ALL refers to a situation where four corpora are used for training and the remaining corpus for testing. Precision (P) and F-score (F) are shown in percentage (%). assumption that these corpora addressed the same PPI task. In this setting, one corpus is used for training and the remaining corpora are used for testing. For the evaluation, we only use the two largest corpora, i.e. AIMed and BioInfer, for training since the other corpora are too small after partitioning onto five groups. Furthermore, we conduct the experiment proposed by, in which four corpora are used for training and the remaining corpus for testing.shows the results of the cross-corpora evaluation on five corpora. Here, the columns correspond to the training set and the rows correspond to test sets.illustrates that our system achieves the best performance when BioInfer is used as the training set. Note that we do not use data from group 4 to group 5 due to theirs corpus-specific properties; therefore, the performance might be higher if these groups are used. This finding is also consistent with the evaluation results by. In their work, better performance is also obtained with BioInfer being used for training. Interestingly,shows that even though the F-score on each corpus in the cross-corpora test is lower than F-score in the single corpus test, the precision is significantly higher when compared against the initial values in. This means that performance on all corpora can be boosted when precision is given a priority. This also implies that the classifier is able to learn from crosscorpora training data. In other words, the proposed feature vectors are compatible across the corpora.shows the performance of our system (BKS) in the crosscorpora setting compared with other approaches. The results indemonstrate that BKS outperforms others when AIMed and BioInfer corpora are used as training sets. In addition, for all evaluation tests, we use the same settings for training corpora except for the complexity term (C parameter of the RBF kernel). This is one step closer to the real world situation. However,have obtained higher performance compared with ours by applying corpus weighting.
Q.-C.Bui et al.
Computational performanceAnother important aspect in PPI extraction is the computational time taken to train and test the system. A previous study byhas shown that in order to reduce the number of computational resources used by ML, one has to consider a trade-off between performance (in this case measured by the F-score) and the computational time required by ML. Despite thisfact, only few systems report on how many computational resources their systems use for training and testing.demonstrate that by using suitable features as well as a learning method, they can improve the performance of their previously proposed system (). In addition, Fayruzov et al.(2009) have pointed out that the more kernels the system uses the more computational resources are needed.shows the running time of our system on the full dataset compared against partitioned dataset of the AIMed corpus. The system runs much faster on the partitioned dataset with both RBF kernels from WEKA and LibSVM (http://www.csie.ntu.edu. tw/cjlin/libsvm/). In addition, since our method partitions data into five groups, we also test it in a node with eight CPUs (Xeon 3.0 GHz). In this test, we use the RBF kernel from LibSVM and the classifiers are run in parallel. The maximum time used by the system is 12 s. This means that our system not only runs fast on a single PC, but can also be used in parallel, which is particularly suited for large-scale experiments.
CONCLUSIONSIn this article, we propose a novel method for extracting PPI pairs from literature. Our approach combines the strength of both semantic rules and ML classification. The evaluation on five benchmark corpora has shown that our system achieves results comparable with the best PPI extraction methods on a single corpus. It outperforms other systems on cross-corpora test and has fast running time. The proposed method consists of two phases: partitioning data into subsets then extract candidate PPI pairs from these subsets, and classifying extracted PPI pairs. The advantages of this method are
Page: 265 259265
A hybrid approach to extract PPI4-fold. First, it filters out a significant amount of negative (noninteraction) PPI pairs, thus balancing the number of positive and negative pairs in training data. Second, by partitioning data into subsets, we can select the most appropriate features for each subset, which potentially improves the final performance. Third, our system only uses a small set of features and therefore performs the best in terms of computational resources. Finally, the classifier can be run in parallel on each subset, which is desirable for the large-scale experiments. In addition, our method uses five semantic rules; therefore, it is generic and can be easily applied to new datasets. Furthermore, it is easy to set-up because it only uses publicly available NLP tools and a standard ML package. In addition, the proposed method can also be extended to extract new relation types in biomedical text, e.g. complex event extraction (). For future work, we plan to integrate a NER tagger into our system in order to study the effect of its performance when a gold-standard NER is not given. In addition, the performance of the overall PPI extraction task largely depends on the output of the extraction phase. Increasing recall in this phase would further boost the performance, especially in case of the BioInfer corpus.
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
