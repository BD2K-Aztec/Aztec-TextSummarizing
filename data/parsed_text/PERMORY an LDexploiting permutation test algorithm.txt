Motivation: In genome-wide association studies (GWAS) examining hundreds of thousands of genetic markers, the potentially high number of false positive findings requires statistical correction for multiple testing. Permutation tests are considered the gold standard for multiple testing correction in GWAS, because they simultaneously provide unbiased type I error control and high power. At the same time, they demand heavy computational effort, especially with large-scale datasets of modern GWAS. In recent years, the computational problem has been circumvented by using approximations to permutation tests, which, however, may be biased. Results: We have tackled the original computational problem of permutation testing in GWAS and herein present a permutation test algorithm one or more orders of magnitude faster than existing implementations, which enables efficient permutation testing on a genome-wide scale. Our algorithm does not rely on any kind of approximation and hence produces unbiased results identical to a standard permutation test. A noteworthy feature of our algorithm is a particularly effective performance when analyzing high-density marker sets. Availability: Freely available on the web at
INTRODUCTIONThe analysis of genome-wide association studies (GWAS) using hundreds of thousands of single nucleotide polymorphism (SNP) markers requires strict control of the type I error (). Many simple approaches to multiple testing correction such as the Bonferroni method fail to account for linkage disequilibrium (LD) among SNPs, which leads to an overly conservative P-value correction. The resulting loss of power matters increasingly because the number of genetic markers and the marker density both grow constantly (). Permutation-based corrections fully account for the correlation among SNPs caused by LD and therefore are considered the gold standard of multiple testing correction in GWAS. They provide the highest statistical power among the procedures controlling * To whom correspondence should be addressed. family wise type I error risk. On the other hand, they require a lot more computational effort than the simple Bonferroni adjustment. For example, running a large number of permutations (~100K) for large-scale marker sets using standard software such as PLINK () can take up to several years of computing time (). Progress has been made by the introduction of accelerated permutation procedures (). The software PRESTO () allows to perform moderate numbers of permutations (1000 to 10 000) for large datasets within a day or more and thus already the calculation of adjusted P-values in the region of 10 3 to 10 4. Nevertheless, there has been an ongoing demand for faster methods to compute genome-wide adjusted P-values, which has motivated the development of various approximation algorithms over the last years. A first alternative approach is based on the Bonferroni correction adjusting the testing threshold for M markers being tested to  = /M.suggested to replace the 'Bonferroni M' by an effective number of independent tests (M eff ), which is derived from eigenvalues of the marker's correlation matrix. In this way, information about the correlation between SNPs is used and therefore results in a less conservative P-value adjustment than Bonferroni; that is, M eff < M. Based on the initial idea, several authors proposed different ways of estimating M eff (). However, in general it still yields conservative estimates in comparison with the permutation test (). Another alternative framework is based on the multivariate normal distribution (MVN), which is used as an approximation of the unknown distribution of the marker set. Lin (2005) and Seaman and Mllerwere the first to propose MVN-based methods for multiple testing adjustment in association studies, followed by Conneely and Boehnke (2007) who increased its efficiency by numerically computing the asymptotic MVNs (), instead of deriving them by simulation. However, due to the numerical limitations of integrating high-dimensional MVNs, these approaches require a block-wise strategy in large marker sets, which does not consider correlations between disjoint marker blocks. To answer this problem,proposed a resamplingbased method called SLIDE, which uses a sliding window locally accounting for the inter-marker correlation. However, both accuracy and computational efficiency depend on the size of the window; that is, extending the window increases accuracy but at the same time results in a considerable loss of computational efficiency.
R.Pahl and H.Schfer
Fig. 1. Flow chartpresenting the basic algorithm structure. The key optimization modules are placed in the center lane (blue). For each marker, one of the three methods is chosen at runtime to be used for permutation. Altogether, numerous approximation methods have been proposed over the last few years steadily improving accuracy, but some concerns still remain. First of all, there is no agreement about a standard alternative method. Second, it is usually left to the user to set one or more method-specific parameters, such as setting the window size in SLIDE, which can complicate application for nonexpert users, especially since an optimal parameter choice frequently depends on the structure of the data at hand. Finally, alternative methods usually cannot cope with missing values and hence require additional imputation methods. In contrast permutation tests are not only the gold standard but also well known and easily applied independently from the underlying data. Thus, it is desirable to make permutation tests feasible for genome-wide application, which is done with the present article. We propose an optimized permutation test algorithm called PERMORY, which in terms of runtime performance is comparable to the fastest available approximation methods. Since our algorithm does not involve any kind of approximation, it provides exact results identical to those of a standard permutation test. Based on a standard permutation test, our algorithm includes several modules that exploit certain properties of genetic data such as high correlations between markers. The basic structure of our algorithm is depicted in. In the following Section 2, we introduce some general notation followed by a description of the algorithms behind the most important modules () and how they affect computational speed. In Section 3, we compare our method with existing permutation-based implementations as well as state-of-the-art alternative methods. We particularly consider different types of SNP chips and demonstrate that PERMORY performs especially well for high-density marker sets. We also provide examples of application to real data.
METHODS
General notation
Familywise error rateThe approach to multiple testing correction used throughout this article relies on controlling the familywise error rate (FWER)also known as the overall type I error ratethe probability of observing one or more false positives in a family of tests (). In particular, we consider the common frequentists approach of controlling the FWER in the strong sense such that FWER   for some significance level .
Statistical modelFor a marker with two alleles, a genotype is defined as the number of minor alleles, yielding a set of three possible genotypes ={0,1,2}. Here, we do not treat missing genotype values for sake of simplicity, but the methods presented in this article can be readily extended to incorporate them as well. 1 In a standard case control association study involving N individualsat each markereach individual is genotyped with one of the three genotypes. For each marker, we can construct a 23 contingency table (). To detect a disease marker, we compare genotype frequencies between affected (cases) and nonaffected (controls) individuals. A commonly applied test statistic is based on Armitages trend test (), which can be written as ()where x i = i, i = 0,1,2, denote the scores of the three genotypes, and r i and n i denote the genotype counts in the cases and the pooled sample, respectively (). Basically any test statistic suitable for analyzing casecontrol SNP data can be calculated from these counts. Let g  N , be a vector (or array) of length N that contains the genotype data of a marker for all individuals coded as 0,1,2 for the number of minor alleles. If g is stored in terms of an array gin the computer memory, the entire genotype information is extracted by accessing each of the N cells of g. Thus, calculating the test statistic for a marker basically consists of two steps:(1) determine genotype frequencies by accessing all N cells of the corresponding array g; and(2) conduct arithmetic operations to compute the test statisticWhen using a permutation test, each permutation of the casecontrol status modifies the genotype counts in the contingency table so that both steps have to be done over and over again. Due to the fact that GWAS involve large populations of up to several thousand individuals, Step 1 is much more time consuming than Step 2 so that accelerating the permutation test means to improve the way of determining the genotype frequencies.
Page: 2095 20932100
A fast LD-exploiting permutation test algorithm
Permutation matrixAll algorithms in the present article require the permutations to be stored in advance. Formally, let P  {0,1} KN be the K N permutation matrix that encodes K permutations of the diseases status and i  I = {1,2, ..., N} the index for an individual: P= 1, i-th individual of k-th permutation is affected 0, not affectedIn our software implementation, large numbers of permutations are processed in blocks of 10K permutations, which costs a little extra time due to repeated file reading but has the added benefit that PERMORY usually does not need more than 1 GB of RAM (tested successfully for 10 9 permutations using 3K cases and 3K controls.).
Dummy code and bit arithmeticDifferent software packages internally use different data formats for keeping the genotype data. Often the data are stored in integer or Boolean typed arrays, which is not ideal with respect to memory consumption. Here, we propose the usage of binary dummy codes, one for each genotype except the common genotype. Formally, for some genotype array g, let U g  {0,1} (||1)N be the dummy coded genotype matrix:We omit the index for the common genotype because its frequency can be derived via the marginals of the contingency table (). Thus, the row index  starts at 1. Since here ={0, 1, 2}, U consists of just two rows. Internally each row of U is stored as a bitset, reducing the required memory significantly as compared with integer arrays. While some other programs already use economic ways of storing the genotype data, for example, by 'packing' several genotypes into each internal word of data, our approach additionally enables efficient bit arithmetics on the bitsets. The pooled genotype frequencies n  () for some array g now can be derived as n  = N i=0 U gby simply counting bits, which is done a lot faster than summing up integer arrays. A little extra work is required, because we are not interested in the pooled frequencies n  but rather in the case frequencies r  ; that is, we must only count the 'case-bits' but not the 'control-bits'. An efficient approach is to set the 'control-bits' to '0' before the bit counting takes place. For this purpose each permutation array 2 P[] is applied as a bit mask using the logical AND conjunction, in this way blanking out the 'control bits' in the dummy bitset. The corresponding pseudo-code can be outlined as follows:Listing 1 Permutation using bit arithmeticsAn example of the basic bit arithmetic algorithm is presented in.
Genotype indexingLet d  {0,1} N be a vector encoding the disease status for R cases and S = N R controls, i-th individual is affected 0, else hence N i=1 d= R. As a matter of fact, any permutation d of d does not change the number of cases (i.e. N i=1 d= R) so that n 0 ,n 1 ,n 2 ,R,S and N () are all invariant with respect to permutations of d. Furthermore, the s i for i = 0,1,2 can be derived as s i = n i r i and r 0 as r 0 = Rr 1 r 2 (). Thus, for any permutation of the genotype data, determining just the genotype counts r 1 and r 2 is entirely sufficient for the construction of the corresponding 23 contingency table. Browning (2008) was the first to use this property; instead of checking the disease status of every single individual, he basically considered only the heterozygous and least common homozygous genotypes and determined how many of them referred to affected individuals, thereby obtaining r 1 and r 2. Formally, he treated the index setsThat is, X g  for  = 1,2 contains all indices i of the data array g that belong to a specific genotype  while D contains all i that are marked affected. The genotype frequencies in cases are then determined as cardinalities of the pairwise conjunctions of these sets:presents both the standard and the corresponding genotype indexing approach for determining the genotype counts of a genotype array. Note that here as well as in the following listings we just consider a single genotype array to simplify matters.A graphical example of the genotype indexing approach is given in.
Transposed permutationIn theory, the genotype indexing algorithm should compute in (|X 1 |+ |X 2 |)/N = (n 1 +n 2 )/N the time than the standard approach, but in practice this is not the case due to compiler and low-level optimization. The standard approach allows for a much better optimization in this regard because accessing all N cells in the genotype array gis implemented as a loop with a strictly monotonically increasing index. In contrast, the marker indices in the genotype indexing approach are not monotonic and only known at runtime. To answer this problem, we modify the inner permutation loop such that it no longer depends on the way the genotype frequencies are derived. First, consider the usual way of conducting all permutations in sequential order:(1) Outer loop: consider permutation k (k = 1,...,K).(2) Inner loop: derive the genotype frequencies for the permuted affection status.(3) Next Permutation. Using our notation, the permutation matrix P is processed row by row. Second, consider processing P column-wise instead.(1) Outer loop: consider individual i (i = 1,...,N).
R.Pahl and H.Schfer(2) Inner loop: count in how many of all permutations the individual is affected, that is, compute K k=1 P.Basically, the entire set of permutations now is processed individual by individual or, using our notation, P is transposed and then processed the usual way, which we therefore call 'transposed permutation'. As a result, the index of the sum in the inner loop has become monotonically increasing (i.e. k = 1,...,K) and genotype indexing in combination with transposed permutation (GIT) indeed requires just about (n 1 +n 2 )/N the time than the standard approach and therefore is very effective for permutation of markers with low minor allele frequencies.
Listing 3 Transposed permutation and genotype indexing combinedNote that we need to keep track of the resulting genotype frequencies separately for each permutation. For this purpose, we define R  N K2 0 as the K 2 matrix of genotype frequencies resulting from K permutations where Rand Rcorrespond to r 1 and r 2 of the k-th permutation, respectively. At the end of the procedure (Listing 3), row Rcan be used to construct the 23 contingency table of the k-th permutation.
Reconstruction memoizationIn computing, memoization 3 is a (machine-independent) strategy to speed up computer programs by avoiding the repeated calculation of results for previously processed inputs. A memoized function remembers the results, and subsequent calls with remembered inputs return the remembered result rather than recalculating it. That is, memoization is a means of lowering a function's time cost in exchange for space cost. As a matter of principle, a function can only be memoized if calling the function has the exact same effect as replacing that function call with its return value. Let (g,f ) be the distance between two genotype arrays g and f defined aswhich is the total number of different positions between both arrays. First, consider two markers with identical genotype arrays g = f , or (g,f ) = 0. Since a permutation of the disease status does not modify the genotype data itself, the genotype frequencies r 1 and r 2 of both markers will be pairwise identical for any permutation. With all r 1 and r 2 resulting from K permutations of g being stored in R g, we can therefore omit all permutations for f and instead set R f= R g, thus memoizing the case frequencies under all permutations for f. Second, assume g is equal to f except for one single genotype in some individual x (i.e. g= fand (g,f ) = 1). For each permutation in which the x-th individual is affected, for some genotype  {0,1,2}, there are six possible distinct pairs of genotypes and for each pair R fcan be constructed from R gas shown in. Thus, in the second scenario, the case frequencies r 1 and r 2 for f can be almost completely memoized, requiring only one or two additional operations () per permutation. For each permutation in which the x-th individual is not affected, hence x /  D [Equation (5)], the r 1 and r 2 do not change at all so that simply
a Provided the x-th individual is affectedIt follows by induction that for any f = g the corresponding R fcan be constructed from R gby sequentially applying the scheme fromfor the set of diverse individuals that can be expressed in terms of the dummy codes [Equation(3)] as follows:For each permutation, the number of operations that are needed to construct R ffrom R gis equal to Y1,g,f + Y2,g,f . All these operations are entirely independent from the particular texture of each permutation so that once the required operations are determined for two pairs of genotype data arrays, they can be repeatedly applied for arbitrary permutations. Since the overall procedure is not only solely based on memoization but also requires reconstruction, we call it the reconstruction memoization (REM) method. In the implementation of the REM method, we again make use of transposed permutation.
Listing 4 Permutation using the REM approachNote that in order to keep the implementation simple and efficient, we do not distinguish between affected and non-affected individuals. Instead, we just add zeros in the latter case.provides a schematic representation with an example of a single permutation, for which the case frequency r 1 is derived. The two presented genotype arrays differ at three positions {2,4,8}; that is, (g,f ) = 3, which implies three add/subtract operations per permutation. Basically (g,f ) and 2(g,f ) are the upper and lower bound, respectively, of the required number of operations per permutation. For a set of candidate genotype arrays {g 1 ,...,g m }, we achieve the maximal amount of memoization by using the genotype array that is most similar to f. That is, for  = 1,2 we search for an index   such thatThe determination of   provokes some additional computational effort, but is implemented efficiently using the dummy codes because this way computing (g,f ) is reduced to computing the Hamming distance Ham(U g,U f) between binaries, which can be done via the XORfunction. In the final permutation test algorithm, we process the genotype data marker by marker and store the permutation results for each processed marker, thereby building a set of candidate genotype arrays.
A fast LD-exploiting permutation test algorithm
Sliding tailOne last problem remains to be solved in order to apply REM to large-scale datasets. The REM method exchanges time for space, which in this case means to keep the permutation results R g  N K2 0 of processed markers in the limited working memory of the computer. In addition, the determination of   [Equation (6)] gets increasingly expensive with the growing set of candidate genotype arrays. However, considering the fact that genotype data of closely related markers is often highly correlated, it follows that the desired  , or at least one that provides a close to minimal distance, is presumably found in the neighborhood of the marker in question. It is therefore sufficient to only keep the permutation results for markers that are located within a limited range relative to the considered marker. Formally, given a fixed range size c and assuming the markers to be treated in sequential order, for some genotype array g i , we consider just the set {g ic ,...,g i2 ,g i1 } as the candidate range of genotypes to be used with the REM algorithm. Once the first c markers in the set have been processed, the range can be thought of as a sliding tail, keeping the data information of the last c markers. It is important to note that in order to benefit from the REM method using the sliding tail, the markers must be sorted in accordance with their genomic locations. In reality, c is chosen to be small in comparison with the number of permutations K, which makes the determination of   relatively cheap. In our algorithm we set the default value to c = 100, which is basically adequate for any data situation. A smaller value (e.g. c = 25) might improve the performance when using a small number of permutations (like 1000) or with marker sets that show low inter-marker correlation. Likewise, a higher value (e.g. c = 200) favors a lot of permutations (>100 000) and a very high marker density. The amount of improvement in either direction, however, is marginal at best, mainly because the most correlated markers are expected to be located very close to each other. In other words, a tail of size c = 100 covers the most correlated markers most of the time so that in practice tuning this value is probably of no use. On the other hand, it might make sense to tune that option in huge simulation studies where the computation might take several weeks.
Method choiceBoth GIT and REM are most applicable for specific kinds of marker data. While the GIT method performs well with low minor allele frequencies, the REM method benefits from highly correlated markers. The computational performance of both methods hence varies with the particular marker at hand. In contrast, the performance of the bit arithmetic method solely depends on the number of the individuals and hence is constant over all markers. During computation, for each marker, we estimate the performance of the methods based on the characteristics of the marker at hand (i.e. minor allele frequency, similarity to neighbored markers) and accordingly choose the fastest method to perform all permutations for that marker ().
RESULTSWe compare our method with existing permutation-based software, namely PRESTO 1.0.1 () and PLINK 1.06 () as well as alternative approaches, for which we select simpleM () representing the methods using M eff and SLIDE 1.0.4 () representing the MVN framework, respectively. To the best of our knowledge, these two algorithms both represent the fastest and most accurate methods of their class. We do not treat the RAT software by, which is based on importance sampling, because it was designed as a special application to adjust a single, preferably highly significant P-value, whereas here we are interested in simultaneously adjusting a wide range of P-values.
AccuracyWe initially present an accuracy evaluation, which serves both as a recap of accuracy results for the alternative methods and as a proof of concept for the permutation-based algorithms. We follow () using a similar type of presentation and the same basic dataset, which is the chromosome 22 data (5563 SNPs) of the Type 2 diabetes (T2D) study (1928 cases + 2934 controls) as part of the Welcome Trust Case Control Consortium Phase I study (WTCCC, 2007). 4 We randomly shuffle casecontrol status and compute P-values 5 until we obtain a dataset with uncorrected Pvalues in the range of 10 7 to 10 5 (x-axis in). This forms) after casecontrol status has been (re-)allocated randomly to the individuals. For each sampling-based method, we use 1 M permutations to derive corrected P-values. Each point constitutes the relative adjustment error as the ratio of the adjusted P-value and the corresponding reference P-value derived by a permutation test using 100 M permutations. The shaded area represents the 95% confidence region that covers the relative sampling error regarding 1 M permutations; that is, each point within that area is considered an accurate adjustment. All permutation-based methods are colored gray because they are expected to stay within the confidence region of the permutation reference P-values. (*For simpleM the default PCA cutoff of 0.995 is used.) our basic dataset. Next we calculate adjusted P-values for this dataset using a permutation test with 100 million permutations, which results in adjusted P-values ranging from 0.00057 to 0.066. These P-values constitute the reference adjusted P-values for this dataset, which are compared with the P-values produced by PERMORY, PRESTO and PLINK, each with 1 M permutations, SLIDE with a windows size of w = 100 and 1 M samplings, and simpleM using its default settings, respectively. We assume a method is accurate if its adjusted P-values are close to the reference P-values. In, we depict the relative error of the methods as ratios between adjusted and reference P-values. We also construct a confidence region in order to cover the relative sampling error, caused by the Monte Carlo approach (in contrast to exhaustive enumeration of all possible permutations) of the presented methods. 6 As expected, all permutation-based methods stay within the confidence region of the reference. The SLIDE curve is consistent with the results presented inas (for this dataset) it is nearly as accurate as the permutation test. Instead of simpleM,in their work used a similar program called Keffective, for which simpleM might be considered an improved version (). In our simulation, simpleM performs slightly better than Keffective did in the work of. However, it still exhibits a trend of increasingly conservative adjustment for less significant P-values,The corresponding confidence intervals for 10 6 permutations as indicated inare (0.92, 1.08) and (0.99, 1.01), respectively. which is a consequence of the plain Bonferroni-like correction using just a single threshold (M eff ) for all markers. With the exception of simpleM, which is not based on sampling, the precision of the other methods incan be controlled by the number of the applied permutations.illustrates how the confidence region would change inif we had applied a different number of permutations. Particularly, the curve of the simpleM method would be covered by the confidence region if the simulation was based on <10K permutations, at least for the range of the depicted P-values. This also shows that for increasingly smaller adjusted (or true) P-values, a decent number of permutations is required in order to achieve a high precision.
R.Pahl and H.Schfer
Runtime performanceAll computations were done on a 64 bit AMD 2.4 GHz CPU running Debian GNU/Linux v5.0. First, the runtime performance is presented for real data from the Welcome Trust Case Control Consortium Phase II (WTCCC2) study. The data consists of 5667 individuals genotyped on the 1.2 M Illumina chip. Particularly, we use the data from all 1 115 428 SNPs of the 22 autosomal chromosomes. We create a casecontrol dataset by randomly dividing the 5667 individuals into 2833 cases and 2834 controls and then compute corrected P-values using 1K, 10K, 100K and 1M permutations, respectively (). Note that the actual P-values have no impact on the performance results and are thus omitted. Since simpleM is not permutation based, its result is placed in the footnote of the table. The PLINK 7 software was not primarily constructed for permutation testing and therefore yields an impractical runtime result for this data set. Using 10K permutations, PRESTO 8 still would take about 31 h to analyze all 1.1M SNPs, while PERMORY finishes in just 3 h, which is comparable to both approximative methods SLIDE and simpleM (). Overall the runtimes of the permutation-based methods increases linearly with the number of permutations, and so does the precision. Basically, a precision of requires ~1// permutations (). To determine the effect of different marker densities on the relative performance of our algorithm, we secondly create simulated datasets for three standard SNP chip sizes: 500K, 1M and 2.4M, the latter mimicking marker sets today already being routinely used in genetic meta-analysis studies. The 500K marker set consists of SNPs from the Illumina Human660W-Quad, the 1M of SNPs from the Illumina Human1M-Duo and the 2.4M of SNPsThe runtime of simpleM using default principal component analysis (PCA) cutoff of 0.995 is 3 h.from HapMap phase 2 (). The data are simulated with the software HAPGEN proposed by, which mimics LD patterns in human populations based on existing phased haplotype data. As suggested on the HAPGEN web site, we use phased data releases of the CEU population from HapMap (http://hapmap.ncbi.nlm.nih .gov/downloads/phasing/2007-08_rel22/phased/), along with the recommended recombination rate files (http://mathgen.stats.ox.ac .uk/wtccc-software/recombination_rates/). We create casecontrol data comprising 3K cases and 3K controls for each chip size and measure the runtimes of computing adjusted P-values using the different algorithms (). Since the limited genetic diversity in the HapMap samples could have potentially lead to overestimating the efficiency of the proposed method, we have used the recombination option (-r) of the HapGen software. 9 This approach seems to be valid since the runtime results of PERMORY
Page: 2099 20932100
A fast LD-exploiting permutation test algorithm
Software implementationPERMORY is written in the C++ language and makes extensive use of the Boost C++ Library (www.boost.org). For future releases, multithreading support is planned to increase the efficiency of the software on multi-core CPUs. The name PERMORY was coined by the words permutation and memory, emphasizing the use of memoization in the algorithm.
DISCUSSIONMultiple testing adjustment is important for genetic data analysis but it has been computationally challenging to use the gold standard method, permutation tests. One can think of two general approaches to this problem: either accelerate the permutation procedure or take an efficient approach to compute approximation and improve its accuracy. In recent years, research primarily has focused on the latter approach. We employed the former and have developed a permutation algorithm optimized for use with genetic data. Our algorithm not only presents a notable improvement over existing permutation test implementations but even can compete with the fastest alternative methods. We showed that our algorithm is also well equipped for the analysis of increasingly denser and larger marker sets including growing sample sizes. PERMORY hence relieves the computational burden of permutation testing on a Page: 2100 20932100
R.Pahl and H.Schfergenome-wide scale, for faster or more accurate determination of genome-wide P-values, respectively. It also extends application in research, for example, enabling more extensive simulation studies using permutation. In the present article, we have covered genotypic trend tests for bi-allelic markers and binary traits. The PERMORY software at this point (version 0.4.0) also supports allelic tests and we plan to integrate support for multi-allelic markers in a future release. Extending the algorithm to the analysis of data with quantitative phenotypes and multiple measured phenotypes should be possible but adds a level of complexity to both the algorithm and the software implementation and therefore is subject of further research.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org 2093 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from [15:11 30/7/2010 Bioinformatics-btq399.tex]
We interpret missing values as kind of a special genotype leading to an additional column in the contingency table. The PERMORY software handles missing values.
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
By using a '*' in the first (or second) position inside the brackets (i.e. [*, ] or [ ,*]) we refer to the entire column (or row) of that matrix.
While memoization might be confused with memorization (because of the shared cognate), memoization has a specialized meaning in computing.
We downloaded the corresponding dataset (example1.slide.gz) from their web site http://slide.cs.ucla.edu. 5 The analysis includes all non-polymorphic SNPs.
The sampling error of the reference permutation using 100 M permutations is negligibly small and therefore ignored.
Command line options: plink-noweb-model-trend-mperm ... 8 Command line options: java-Xmx2G-jar presto.jar missing = ? test = t ...
Command line: hapgen-h haplotype-data.haps-l legend-file.leg-r mapfile.map-Ne 11418-snptest-n 3000 3000. in Figure 4 (1M SNPs) are similar to the real data runtime results in Table 4 (10K permutations). While the simpleM algorithm displays the fastest performance for the smaller chips, PERMORY shows the best performance for the 2.4M SNP chip, which is mainly attributable to the REM module (Fig. 2C). Overall the 2.4M chip exhibits higher inter-marker correlation and lower allele frequencies due to an increased number of rare variants, both of which is explicitly exploited by the REM and the GIT algorithms, respectively. Note that for SLIDE we use a window size of w = 250 with the 2.4M chip, considering the fact that if the marker density increases, SLIDE must adjust its window size 10 in order to maintain a nearly full accuracy in the correction of the P-values. Apart from the number of genetic markers, the number of individuals that are included in todays meta-analysis or upcoming GWAS is likely to increase as well. Since a permutation test shuffles the phenotype status, an increasing number of individuals might adversely affect the performance of permutation test procedures. For this reason, we also perform runtime tests for a large sample comprising 10K cases + 10K controls but overall the relative performances between the methods is not much different as compared with Figure 4 (results not shown). It is worth noting in this context that the running time for permutation testing basically can be considered independent of sample size if the null distribution for large samples is estimated by only a subset of the samples as proposed by Dudbridge (2006). Another basic approach to enhancing permutation tests consists of parallelization using more than one CPU. If p permutations are desired and n CPUs are available, we only need to perform p/n permutations on each CPU and combine the resulting P-values. In this case the total runtime is cut down linearly in the number of CPUs.
This choice is in accordance with the authors of SLIDE who suggest a window size of w = 100 and w = 1000 in scenarios of collecting a set of 1 million and 10 million SNPs, respectively.
