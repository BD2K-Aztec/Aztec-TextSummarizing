Motivation: Assembling peptides identified from tandem mass spectra into a list of proteins, referred to as protein inference, is an important issue in shotgun proteomics. The objective of protein inference is to find a subset of proteins that are truly present in the sample. Although many methods have been proposed for protein inference, several issues such as peptide degeneracy still remain unsolved. Results: In this article, we present a linear programming model for protein inference. In this model, we use a transformation of the joint probability that each peptide/protein pair is present in the sample as the variable. Then, both the peptide probability and protein probability can be expressed as a formula in terms of the linear combination of these variables. Based on this simple fact, the protein inference problem is formulated as an optimization problem: minimize the number of proteins with non-zero probabilities under the constraint that the difference between the calculated peptide probability and the peptide probability generated from peptide identification algorithms should be less than some threshold. This model addresses the peptide degeneracy issue by forcing some joint probability variables involving degenerate pep-tides to be zero in a rigorous manner. The corresponding inference algorithm is named as ProteinLP. We test the performance of ProteinLP on six datasets. Experimental results show that our method is competitive with the state-of-the-art protein inference algorithms. Availability: The source code of our algorithm is available at: https:// sourceforge.net/projects/prolp/.
INTRODUCTIONProtein identification using tandem mass spectrometry (MS/MS) is the most widely used tool for detecting proteins from biological samples. In a typical shotgun proteomics experiment (), proteins in a sample are first digested into peptides, and the resulting mixture of peptides is subjected to mass spectrometry to generate tandem mass spectra. After spectra acquisition, the peptide that generates each spectrum is identified with peptide identification algorithms. From these putative peptide identifications, the proteins that are present in the sample are detected with protein inference algorithms. Computationally, the input for the protein inference problem is a bipartite graph: the left is a set of identified peptides and the right is the set of candidate proteins that have at least one constituent peptide. The inference problem considered here is to find a subset of proteins that are truly present in the sample. However, such protein inference problem is only partially solved since several technical challenges still remain unconquered. One of the most challenging problems is the peptide degeneracy issue, which arises when a single peptide can be mapped to multiple proteins. The performance of protein inference algorithms mainly depends on our capability of assigning these degenerate/shared peptides to proteins that really generate them. To date, there are already many protein inference algorithms available in the literature (). The reader can refer to a recent survey () for details. Here, we shall discuss briefly how these methods tackle the peptide degeneracy issue and present our research motivation. Existing protein inference algorithms solve the peptide degeneracy problem in quite different ways. Generally, they fall into two categories, as listed in the subsequent sections. Inference algorithms in the first category solve the peptide degeneracy problem with some simple rules or assumptions in an implicit manner. One typical example is the widely used two-peptide rule, which regards all candidate proteins that have at least two matching peptides as true positives (TPs). The underlying assumption is that degenerate peptides should belong to all proteins that they can match. In contrast, IDPicker () formulates the protein inference problem as a set covering problem and solves it with a greedy algorithm. In the greedy selection procedure, proteins that can match the maximal number of uncovered peptides are selected in an iterative manner. The underlying assumption is that each degenerate peptide should be assigned to one protein only. Inference algorithms in the second category treat the peptide degeneracy issue explicitly in terms of conditional probability. Briefly, they either model the conditional probability of one protein being present given a peptide or model the conditional * To whom correspondence should be addressed. probability of one peptide being present given a protein. ProteinProphet (), one of the most widely used protein inference methods, learns 'degenerate peptide weight' using an EM-like algorithm. In fact, such degenerate peptide weight corresponds to the probability of one protein being present conditional on the presence of a given peptide. Alternatively, MSBayesPro () utilizes the concept of peptide detectability, which is defined as the probability of detecting a peptide in a standard sample by a standard proteomics routine if its parent protein is expressed. Fido () models the probability with which a sample peptide is generated from a protein containing it with a constant probability. HSM () considers five types of mechanisms that a peptide can be generated by a protein, i.e. the conditional probability that one peptide is present has five possible values. The attempts of treating the peptide degeneracy problem rigorously in the second category have obtained promising results; however, they still have some limitations. First, ProteinProphet employs an EM-like iterative procedure to estimate protein probabilities. This method is described procedurally rather than derived from a well-defined optimization model. In contrast, MSBayesPro, HSM and Fido derive their models from clear, explicitly stated statistical assumptions. However, they formulate the protein inference problem as a combinatorial optimization problem. This means that they may generate different inference results from the same dataset when obtaining the optimal solution is too time-consuming. Second, Fido and HSM use a very small set of parameters to approximate all possible values that the conditional probability can take. Such a simplification makes it possible to create efficient accompanying algorithms, but it may also limit the capability of achieving better inference performance. In contrast, there are no such limitations in ProteinProphet and MSBayesPro. Unfortunately, the conditional probabilities in ProteinProphet are calculated using a formula that has not been rigorously justified. The peptide detectability values in MSBayesPro are predicted using a complex model trained on other datasets. Finally, existing methods involve many parameters that are not easy to specify. For example, Fido needs to have a grid search in order to find good values for its three parameters. The aforementioned observations motivate our research. In this article, we take a step further toward completely solving the protein inference problem with particular emphasis on peptide degeneracy. To that end, we present a linear programming (LP) model for protein inference, which is built on two simple probability equations. We first introduce the joint probability that both a protein and its constituent peptide are present in the sample. To obtain a linear model, we use a mathematical transformation of this joint probability as the variable. The marginal probability of a peptide being present can be expressed as a formula in terms of the linear combination of these variables. If we assume that the marginal probability of each identified peptide being present is known, the protein inference problem could be formulated as the following optimization problem: 'minimize the number of proteins of non-zero probabilities while the calculated peptide probability should be as close to its known value as possible'. We show that this optimization problem actually can be written as a LP problem, which has only one parameter that is easy to specify and has a clear interpretation. This new protein inference algorithm is named as ProteinLP. Experimental results on six datasets show that our ProteinLP algorithm is a competitive and complementary approach for protein inference. The main contributions of the work described in this article can be summarized as follows:To our knowledge, our work is the first LP formulation for the protein inference problem. Our method guarantees to find the optimal solution. Instead of using conditional probability, our model is the first attempt of addressing the peptide degeneracy problem with the joint probability. It greatly simplifies the model without sacrificing the discrimination power.The rest of this article is organized as follows. In Section 2, we describe our method in detail. Section 3 presents the experimental results and Section 4 concludes the article.
METHODSGiven m candidate proteins and n identified peptides, the protein inference problem can be formulated as an optimization problem: select a possibly small subset of candidate proteins that best 'explains' these peptides. Such an optimization problem can be formulated in quite different ways. In this section, we present a LP model for protein inference, which can be solved very quickly with standard LP solver. We use a vector of indicator variables x 1 ,. .. , x j ,. .. , x m  to denote the set of m candidate proteins and another indicator vector y 1 ,. .. , y i ,. .. , y n  to denote the set of n identified peptides. In addition, we assume that we know the probability that each peptide is present in the sample, which is provided by peptide identification algorithms such as Mascot () or post-processing tools such as PeptideProphet (). The peptide probability vector is. Protein identification using mass spectrometry in shotgun proteomics. In the experimental process (from left to right), proteins are digested into peptides, which are then subjected to mass spectrometry to produce MS/MS spectra. In the data analysis process (from right to left), there are two major computational problems: peptide identification and protein inference. This article focuses on developing effective algorithms for protein inference denoted by z 1 ,. .. , z i ,. .. , z n . Notation and definitions used in this article are summarized in.
ModelLet Pry i  1 denote the probability that peptide i is present and Prx j  1 denote the probability that protein j is present in the sample. A peptide is present if at least one of its parent proteins is present:where Pry i  1, x j  1 denotes the joint probability that peptide i and protein j are both present in the sample. Similarly, for each protein j, we haveThrough the logarithmic transformation, we convert the product relation in Equations (1) and(2) into the sum relation so as to build a LP model:Since the peptide probability and protein probability are not linear with respect to the joint probability, we use p ij  ln 1  Pr y i  1, x j  1     instead of Pry i  1, x j  1 as the variable of our model. Then, both the peptide probability and protein probability can be expressed as a function of the linear combination of these variables. In other words, we can use the sum of p ij to calculate both peptide probability and protein probability. From aforementioned analysis, we can see that the joint probability of peptide and protein can serve as the bridge between peptide probability and protein probability. On the one hand, we can use the joint probability to explain the known peptide probability. On the other hand, we can calculate the unknown protein probability and tackle peptide degeneracy issue through joint probability. Therefore, the protein inference problem is equivalent to finding an optimal joint probability matrix, calculated from the matrix P  p ij . Based on aforementioned observations, we present a LP formulation for the protein inference problem:8i, j : p ij $ 0 if j 2 Nei  0 else , & 10 where Ne(i) is the set of all proteins that can generate peptide i. In, we provide a vivid illustration on the main idea of this LP model. Some further remarks on the model and constrains are listed below. The constraints (8) and (9) control the difference between the observed and calculated peptide probabilities. Here, we regard z i as the observed peptide probability and Pry i  1 as the calculated value where P m j1 p ij  ln 1  Pry i  1  . In constraints (8) and (9), 2 0, 1 is the only parameter of our model, which is the difference between the observed and calculated peptide probability. This parameter reflects our confidence on peptide identifications. For instance,  0 means that we believe the input peptide probability is perfectly accurate so that we have to adjust the variable p ij to make the equation hold. Hence, this parameter has a clear interpretation and it can be specified with ease. In our implementation, we use  0 as the default setting. The constraint (7) is to find the minimum value in p j (the jth column of matrix P). Since only a subset of candidate proteins are truly present in the sample, some protein probability values should be zero. In order to achieve this goal, we control the maximum joint probability assigned to each protein. Since ln1  x is a monotonic decreasing function, the maximum joint probability Pry i  1, x j  1 corresponds to the minimum valueThen we maximize it in the objective function (5) so as to shrink some protein probabilities to 0.The observed peptide probability z i can be equal to one. This will cause a problem in our implementation since ln1  x is minus infinity when x  1. To address this problem, we reset the observed peptide probability to 0.99999 when z i  1. p ij 0 and t j is the minimum value in p j so that t j should be not more than zero, as specified in constraint (6). For notation convenience, we use n  m variables in the LP formulation described earlier in the text. In fact, the actual number of variables is less than n  m since the peptideprotein bipartite graph is very sparse. As shown in constraint (10), we set all p ij  0 if peptide i is not contained in protein j and consider only the remaining joint probabilities as variables. This greatly improves the running efficiency of our method. Constraint (10) also ensures that Pry i  1, x j  1 falls intosince it is a probability value.,. .. , i,. .. , n All n peptides identified by peptide identification algorithms 1,. .. , j,. .. , m All m proteins that might have generated these n peptides y 1 ,. .. , y i ,. .. , y n  Peptide vector: indicator variables of peptides' presences if peptide i is present, y i  1; otherwise y i  0 x 1 ,. .. , x j ,. .. , x m  Protein vector: indicator variables of proteins' presences z 1 ,. .. , z i ,. .. , z n  The probabilities of peptides' presences estimated by peptide identification
algorithms or PeptideProphetIn the model, we group the proteins with the same set of identified peptides together and regard each group as a single entity.Our LP model is quite flexible and can be extended easily. For instance, we currently assign a global deviation threshold to all peptides. In fact, we can also use an individual deviation threshold i for each peptide i. This will provide us the possibility of assigning larger deviation thresholds to certain peptides that are suspected to be error-prone.After obtaining the solution matrix P, the protein probability is calculated as:
EXPERIMENTS
DataWe use six datasets in our experiments. All the datasets are publicly available. Among these six datasets, 18 mixtures (), Sigma49 and yeast () have a corresponding protein reference set as the set of ground-truth proteins. An identified protein is labeled as a true identification if it is present in the protein reference set. Another three datasets are DME (), HumanMD () and HumanEKC (), which have no reference sets and we use the target-decoy strategy for performance evaluation. In this target-decoy strategy, the MS/MS spectra are searched against a mixed protein database containing all target protein sequences and an equal number of decoy sequences. Then, we consider an identified protein as a true identification if it comes from the target protein database. The detailed information about the six datasets can be found in the supplementary Tables S1 and S2.
Database searchThe search engine used in our experiment is X!Tandem (v2010.10.01.1) (). For 18 mixtures, Sigma49 and yeast datasets, all MS/MS data are searched against their own protein sequence databases. For DME, HumanMD and HumanEKC, the spectra need to search against both target and decoy protein databases. During the database search, we use default search parameters wherever possible, assuming that parameters have already been optimized. Some important parameter values are listed in the following: fragment monoisotopic mass error  0.4 Da; parent monoisotopic mass error  100 ppm; minimum peaks  15 and minimum fragment m/z  150. Peptide probabilities are computed using PeptideProphet included in Trans-Proteomic Pipeline (TPP) v4.5. Any peptide identifications with probability 50.05 are excluded in the input. For any peptide sequence that is matched by multiple spectra with different scores, we choose the highest identification score.
Protein inferenceWe compare our method with ProteinProphet (), MSBayesPro () and Fido (). All these three algorithms treat the peptide degeneracy issue explicitly in terms of conditional probability, and their software packages are publicly available. For the proteins that cannot be distinguished with respect to identified peptides, ProteinProphet, Fido and ProteinLP put all of them into the same group. Whenever we refer to the number of TPs or false positives (FPs) identified at a threshold or use these values in a calculation, all proteins in the group are reported and the group probability is used as their protein probabilities. Alternatively, we can select one representative from each protein group in the performance comparison for these three algorithms (please check the supplementary Section 1 for details).
ProteinProphet We run ProteinProphet includedin the TPP (v4.5) software with the default parameter values.
MSBayesProWe first obtain the predicted peptide detectabilities from http://darwin.informatics.indiana.edu/applications/PeptideDetectabilityPredictor/. This website currently only predicts scores of tryptic peptides. For those non-tryptic peptide identifications, we assign detectability scores to them by ourselves. The principle is peptide detectability  median (predicted detectability scores from the same parent protein)/3. Then, we run MSBayesPro for the first time with the peptide probability file and peptide detectability file as input to estimate the protein priors. Finally, we run MSBayesPro for the second time to obtain the protein probabilities using priors from the first run as additional input. The probability of each protein is reported according to the value of Positive_Probability_by_memorizing no matter what MAP_state_by_Memorizing value is.
FidoWe run Fido with its default parameter setting.
ProteinLPWe use Glpk for Java (v4.47) as the LP solver and set  0 in the experiment for ProteinLP.
ResultsWe evaluate the performance of different methods by creating a curve, which plots the number of TPs as a function of q-value. An identified protein is labeled as a TP if it is present in the corresponding protein reference set or target protein sequence database1 is the joint probability that peptide i and protein j are both present in the sample. In the model, the linear program has two kinds of constraints: column constraints and row constraints. The row constraints require that for each peptide i, the difference between the observed peptide probability and the calculated peptide probability should be no greater than a threshold. The column constraints can shrink some protein probabilities to 0 and as a FP otherwise. Given a certain probability threshold t, suppose there are T t TPs and F t FPs, the false discovery rate (FDR) is estimated as FDR t  F t =F t  T t . The corresponding q-value is defined as the minimal FDR that a protein is reported: q t  min t 0 t FDR t 0. The curve is produced by varying the probability threshold t. The probabilities of top-scoring proteins in the several methods are all equal to one, and the order of these proteins in the output file is random. Thus, we skip these proteins with same probabilities and start from the one with a different score when calculating the q-value.plots the number of TPs identified by four methods at different q-values. Some important observations are summarized as follows. First, none of these four methods can always achieve the best performance over all datasets. Throughout six datasets, our method is stable and never performs the worst. Globally,, Fido, ProteinProphet (PP) and MSBayesPro (MSB). Because people are particularly interested in the performance of different algorithms when the q-value or FDR is very small, we only plot the curve up to 0.05 along the x-axis for yeast, DME and HumanMD datasets. Fido has a minimum non-zero q-value of 0.08 on yeast dataset, which is40.05. To plot the curve of Fido within the slot of, we use the maximal number of TPs achieved at q-value  0.08 as the value of y-axis at q-value  0.05. Note that such an operation overestimates the actual performance of Fido on the yeast data. Since the maximum q-value for HumanEKC is 50.04, we choose 0.03 as the maximal value of x-axis. We cannot set the q-value range very small for 18 mixtures and Sigma49 datasets since the probabilities of top-scoring proteins in the several algorithms are all equal to one, hence we have to ship these proteins with same probabilities and then calculate the q-value of the first appearing protein with a different probability ProteinLP is approximately (or tied with other algorithms) the best inference algorithm on four datasets (yeast, DME, HumanMD and HumanEKC) and the second best on 18 mixture data. Locally, it beats ProteinProphet five times, outperforms both MSBayesPro and Fido four times. Second, ProteinLP has the largest number of TPs among the highest ranking proteins when q-value  0 (i.e. 0 FPs) on three datasets: DME, HumanMD and HumanEKC. Other three algorithms can also achieve such a property on some datasets. The number of these datasets is 1, 0 and 2 for ProteinProphet, MSBayesPro and Fido, respectively. Finally, the experimental results from simple 18 mixture to complex human data show the trend that ProteinLP is more powerful on processing the MS data generated from real samples. To compare the capability of different methods in tackling the peptide degeneracy issue, we present the identification results of four methods when inferring proteins containing a high-scoring degenerate peptide in. For each dataset, we count the number of TPs and FPs identified by ProteinProphet, MSBayesPro, Fido and ProteinLP with the same number of reported proteins. Then, we divide the identified proteins into two classes: 'degenerate proteins', which contain a high-scoring degenerate peptide and 'simple proteins' which do not contain any such degenerate peptide. From, we have the following observations. First, ProteinProphet and ProteinLP can identify more degenerate proteins than MSBayesPro and Fido in most cases. This is because both ProteinProphet and ProteinLP tend to assign a degenerate peptide to the parent protein with more identified peptides. As a result, some degenerate proteins will have much higher probability than other proteins. In contrast, MSBayesPro and Fido do not have such a tendency. When we let different methods report the same number of proteins, ProteinProphet and ProteinLP will return more degenerate proteins since these proteins are ranked more front by these two methods. Second, MSBayesPro can always report the least number of FP degenerate proteins on 18 mixtures, Sigma49 and yeast at the cost of identifying less TP degenerate proteins. All four methods report zero FP degenerate proteins on HumanMD and HumanEKC datasets. Third, ProteinLP is able to identify more TP degenerate proteins than the other three methods on DME, HumanMD and HumanEKC datasets. Our method never reports the most FP degenerate proteins. Moreover, ProteinLP identifies the least number of FP degenerate proteins on DME dataset. Overall, MSBayesPro is more powerful in controlling the false discovery rate with respect to degenerate proteins, whereas our method offers a reasonable trade-off between TP and FP rates. Using the same set of identified proteins in, we also plot two groups of Venn diagrams to further check the overlap and difference among (degenerate) proteins identified by different inference algorithms in supplementary Figures S3 and S4. These figures show that the set of proteins identified from the same dataset by different methods can vary significantly. Moreover, ProteinLP can always report some additional (degenerate) proteins that have never been identified by the other three methods on all datasets except Sigma49. This fact further confirms that ProteinLP can serve as a strong and complementary approach for protein inference. ProteinLP requires only one parameter:. We choose  0 as the default setting. To test the effect of this parameter, we run ProteinLP over a rough grid of that ranges from 0 to 0.9. We omit parameter value of 1.0 since all the protein probabilities are zero under this parameter setting. We use the number of TPs at certain q-value threshold as the performance metric to assess the effect of different parameters. We choose 0.3 as the q-value threshold for 18 mixtures and Sigma49 and 0.01 for all the other four datasets, respectively. As shown in, the performance of our method is sensitive to different parameter specifications, and  0 is not the best choice. To address the parameter selection problem, we develop an entropy-based approach for setting a proper value automatically (see supplementary Section 2 for details).For the six datasets, we count the number of true positives and false positives identified by ProteinProphet (PP), MSBayesPro (MSB) and Fido and ProteinLP (PLP) among their top-k ranked proteins, where k is 31, 43, 538, 175, 124 and 196 for 18 mixtures, Sigma49, yeast, DME, HumanMD and HumanEKC datasets, respectively. The value of k is determined according to the number of proteins with probability of 1.0 reported by ProteinProphet. We divide the identified proteins into two classes: 'degenerate proteins' are proteins that share a high-scoring (! 0:90) peptide with another protein and 'simple proteins' do not share such a peptide with any other protein.
CONCLUSIONTo solve the peptide degeneracy problem, we need to know which protein really generates the degenerate/shared peptide. The most natural idea is to model or infer the conditional probability of one peptide (protein) being present given a protein (peptide). However, we may still need the protein probability as the variable in the mathematical formulation besides the conditional probability. This will lead to a hard optimization problem that cannot be optimally solved. In fact, the joint probability that both a protein and its constituent peptide are present in the sample can also provide similar discrimination information for assigning a degenerate peptide to the right protein. Therefore, the main advantage of ProteinLP over other methods is the use of joint probability as the variable, which avoids modeling the protein probability and the conditional probability simultaneously so that the optimization formulation is greatly simplified. In the future work, we plan to incorporate some supplementary information such as proteinprotein interactions into the LP model to help solving the degeneracy issue. For example, if we know that there is an interaction between two proteins, then it can be expected that the existence of one protein may lead to the presence of another protein. To utilize such interaction information, we can introduce a linear constraint on the probability difference between two interacting proteins to enforce their coexistence.
The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
T.Huang and Z.He at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
A LP model for protein inference at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
