Motivation: Proteins are responsible for a multitude of vital tasks in all living organisms. Given that a protein's function and role are strongly related to its subcellular location, protein location prediction is an important research area. While proteins move from one location to another and can localize to multiple locations, most existing location prediction systems assign only a single location per protein. A few recent systems attempt to predict multiple locations for proteins, however, their performance leaves much room for improvement. Moreover, such systems do not capture dependencies among locations and usually consider locations as independent. We hypothesize that a multi-location predictor that captures location inter-dependencies can improve location predictions for proteins. Results: We introduce a probabilistic generative model for protein localization, and develop a system based on it—which we call MDLoc—that utilizes inter-dependencies among locations to predict multiple locations for proteins. The model captures location inter-dependencies using Bayesian networks and represents dependency between features and locations using a mixture model. We use iterative processes for learning model parameters and for estimating protein locations. We evaluate our classifier MDLoc, on a dataset of single-and multi-localized proteins derived from the DBMLoc dataset, which is the most comprehensive protein multi-localization dataset currently available. Our results, obtained by using MDLoc, significantly improve upon results obtained by an initial simpler classifier, as well as on results reported by other top systems.
IntroductionProteins are responsible for a multitude of diverse vital tasks in all living organisms (). Given that a protein's function and role are strongly related to its subcellular location, protein location prediction is an important research area (). Furthermore, the location of a protein helps understand the protein's prospective utility as a drug target (). Methods for determining protein locations include experimental as well as high-throughput computational ones. The experimental methods accurately determine protein locations, but are typically time consuming and are typically not cost effective for finding locations for a large number of proteins. Such methods include mass spectrometry () and green fluorescence detection (). On the other hand, the computational methods are fast, and can potentially predict locations for proteins whose actual locations have not yet been experimentally determined. Most of the prediction systems represent proteins using sequence-derived features and utilize machine learning methods (e.g.). Proteins move from one location to another and localize to multiple subcellular compartments (). For instance, the enzyme TREX1, which assists in DNA repair, is V C The Author 2015. Published by Oxford University Press.
i365This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.comi374 doi: 10.1093/bioinformatics/btv264 ISMB/ECCB 2015 primarily present in the cytoplasm but is also transported to the nucleus in response to DNA damage (). Thus, predicting multiple locations for proteins is important, as protein movement across locations enables the protein to serve multiple distinct functions. Nevertheless, all prediction systems mentioned earlier and most current systems assign only a single location per protein. Since proteins localize systematically, and translocation occurs only among specific locations for the purpose of a particular subcellular function, our hypothesis is that modeling inter-depedencies among locations can assist in predicting locations of proteins more accurately. Posing the problem using computational, machine-learning terms, assigning multiple locations to proteins is a multi-label classification task. Traditional single-label classification assigns a single label (location) to each instance (protein), and is addressed by methods such as Support Vector Machines (), navenave Bayes or neural networks (). Multi-label classification, on the other hand, aims to associate each instance with possibly multiple classes. Some of the simplest and commonly used approaches transform the multi-label classification task into one or more single-label classification task(s) (); such approaches do not capture label inter-dependencies. More sophisticated multi-label classification approaches attempt to capture label inter-dependencies and incorporate them into the classification process. Such multi-label classification methods have not yet been employed in the context of protein location prediction. In this article, we present a new, dependency-based probabilistic generative model for eukaryotic protein localization, and develop a multi-location prediction systemwhich we call MDLoc, to predict locations of multiply localized proteins. As was done before (), we use sequence-derived features and Gene Ontology (GO) terms to represent proteins. Here we introduce a new model using Bayesian networks to directly address and capture inter-dependencies among locations. Furthermore, we present the concept of location dependency sets and use a mixture model to represent feature dependency on location-combinations. The new system uses a generative model and an iterative procedure for estimating its parameters, and effectively improves the estimation process of multi-locations. Our method is based on iteratively learning parameters of the locationBayesian-network and the mixture model, while re-inferring the location estimates of the proteins in each iteration. This improves on our preliminary system, which comprised a collection of Bayesian network classifiers, where location inter-dependencies were not learnt as part of the model but rather captured based on simple estimates of location values (). We evaluate MDLoc on a dataset derived from the DBMLoc dataset (), which is the most comprehensive protein multi-localization dataset currently available, using multiple runs of 5-fold cross-validation. We show that the performance of MDLoc on multi-localized proteins improves over earlier results for a top performing system, YLoc  (). The improved results obtained by MDLoc demonstrate the advantage of utilizing location inter-dependencies and feature dependencies on locations in the prediction process. The rest of the article proceeds as follows: Section 2 surveys methods for protein multi-location prediction. In Section 3, we introduce the concept of location dependency sets and provide relevant notations; we also present our new probabilistic generative model for protein localization, which captures dependencies between protein-features and locations. In Section 4, we discuss the model parameters, the learning procedure used for finding them, and the inference technique used for predicting multiple protein locations. Experiments and results are presented in Section 5, followed by conclusions and future directions.
Related workA number of recent location prediction systems attempt to predict multiple locations for proteins, however their performance leaves much room for improvement. While most use sequencederived features (e.g. amino acid composition) and GO terms to represent proteins and to predict protein locations, a few are based solely on sequence-based similarity. The former class of methods incorporate one or more of the following classifiers: knearest neighbors (k-NN,), Support Vector Machines (), navenave Bayes () and neural networks (). KnowPred site () is an example of the latter, similarity based, class of methods. Systems that use k-NN adaptations to predict multiple locations for proteins include WoLF PSORT (), EukmPLoc (), iLoc-Euk () and an ensemble system (). WoLF PSORT outputs for a query protein the location-combination that is most frequent among the protein's k-NN in the training set; the predictions are thus restricted to location-combinations already present in the set. Both iLoc-Euk and Euk-mPLoc compute a score for each candidate location, based on the query protein; iLoc-Euk outputs locations having the highest scores; the number of locations is the same as that associated with the query protein's nearest neighbor in the dataset; Euk-mPLoc assigns the protein to locations whose score lies within a certain deviation from the highest score. All the methods described thus far treat locations as independent from one another and do not utilize possible inter-dependencies among locations in the prediction process. A few systems, however, have tried to make use of location inter-dependencies to predict multiple locations for proteins. For example, the classifier byattempts to use pairwise location-correlation in the prediction process, but does not use more complex inter-dependencies. YLoc  () introduces a new class for each location-combination represented in the training dataset and uses a navenave Bayes classifier to predict a probability distribution over these new classes. Thus, each classifier prediction is restricted to locationcombinations in the training set. YLoc  's performance was evaluated using the most comprehensive protein multi-localization dataset and is the highest among current multi-location prediction systems. In our earlier preliminary work (), we used a collection of Bayesian network classifiers to predict multiple locations of proteins. The simplified model used did not incorporate location-interdependencies into the iterative learning process, but rather utilized one-time estimates of location values to establish interdependencies. The performance of that classifier was comparable to that of YLoc  when using the same dataset, but did not improve on it. In the next section, we present a new probabilistic generative model for protein localization that directly incorporates the learning of location inter-dependencies into the iterative learning process. Additionally, we introduce the concept of location dependency sets, which enables us to capture feature dependencies on locationcombinations in a mixture model setting. The resulting system MDLoc, shows significant improvement, according to all evaluation metrics, compared with previously reported performance for protein multi-location prediction. i366 R.Simha et al.
A probabilistic generative model for protein localizationAs we and others have done before (), we represent each protein P as a weighted feature vector, f ! P  hf P 1 ;. .. ; f P d i where d is the number of features. Let S  fs 1 ;. .. ; s q g be the set of q subcellular components in the cell. Each protein P localizes to at least oneand possibly more than onelocation. The locations of each protein P are represented by a location indicator vector, l ! P  hl P 1 ;. .. ; l P q i of 0/1 values, where l P i  1 if P localizes to s i , and l P i  0 otherwise. We view each location indicator l P i as a value taken by a random variable L i and each feature f P j as a value taken by a random variable F j. Given a protein P, represented as a vector f ! P , the multi-localization task amounts to assigning a (correct) 0/1 value to each of the entries l P i .
Modeling location inter-dependencyWe use Bayesian networks to model inter-dependencies among subcellular locations. A Bayesian network consists of a directed acyclic graph G  L; E whose set of nodes L corresponds to random variables and set of edges E indicates dependencies among the variables. In our case, nodes represent location variables denoted L  fL 1 ;. .. ; L q g. Each variable L i corresponds to a location s i within the cell and takes on a 0/1 value.shows an example Bayesian network we learn over location variables. A directed edge, for instance, from membrane to cytoplasm represents the assertion that knowing that a protein localizes to the membrane influences the level of belief about the protein localizing to the cytoplasm. According to the conditional independence relationship encoded in the Bayesian network, each variable L i is conditionally independent of its non-descendants given its parents PaL i  (for additional details see). The joint distribution of the location variables can thus be calculated as:PrL i jPaL i :3.
Capturing location-feature dependencyThe value of each feature represents a certain characteristic of a protein, such as the relative abundance of each amino acid in the protein's amino-acid composition (). In our experiments, we use the exact same features used by Briesemeister et al. (2010a), as explained in Section 5.1. For the purpose of predicting locations for a protein, we view a protein as though it was generated through a stochastic process, in which each of its feature values was determined. The value of each feature variable F j (1 j d) is assigned based on the values taken by one or more location random variables; that is, each feature value may depend on multiple locations and not just on one. For instance, consider a feature capturing the abundance of tryptophan (Trp) residues in the amino acid composition of a protein; we denote the random variable associated with this feature by F Trp. The value of this feature varies greatly between proteins known to localize to the membrane vs. those that are known to localize to both the membrane and the cytoplasm. Specifically, the probability of a membrane protein to have more than three Trp residues (formally denoted as the conditional probability: PrF Trp > 3jL Mem  1), is 0.36 [this high probability agrees with the well-established importance of Trp's role in membrane proteins (. In contrast, the probability of proteins known to be multi-localized to both the membrane and the cytoplasm to have more than three Trp residues (PrF Trp > 3jL Mem  1; L Cyt  1), is only 0.15 (the probability values are calculated based on the dataset described in Section 5.1). Thus, the feature value depends on more than a single location value. To accurately capture the dependency between protein features and location-combinations, we view each feature value as depending on a set of location indicator values. Recall that we view a protein as represented by (i.e. comprised of) a set of features. As such, we view each possible location of a protein P as depending on a set of locations to which proteins with similar feature values (including P itself) are likely to be localized. We thus introduce the concept of location dependency sets. For a location s i , its dependency set comprises the minimal set of locations fs i1 ;. .. ; s im g such that the likelihood of a protein to localize to s i depends on (i.e is correlated or anti-correlated with) its likelihood of to localize to each of fs i1 ;. .. ; s im g. Using the Bayesian network framework, we note that a dependency as described above between the locations s ij and s i can be represented as a directed edge from the graph node L ij to L i. Given a Bayesian network that represents the dependencies among locations in this way, we can thus denote the location dependency set for each location variable L i as the parents of L i in the Bayesian network. As such, we define q location dependency sets, one set per location,where PaL i  (1 i q) denotes the parents of location variable L i in a Bayesian network. Given a Bayesian network G, the steps involved in protein generation are discussed in the rest of this section. We use a coin-toss model to set location indicator values, and two die-roll processes to set feature values. For each feature, one die roll is used to select a location dependency set, and another to assign the actual feature value. We next describe each of the steps in detail.
Setting location valuesAs part of the generative process for a protein P, we view the value of a location indicator l P i (1 i q) as set by tossing a coin C i ; if the coin comes up Heads, the location indicator l P i is set to 1; otherwise l P i  0. The probability of C i to come up Heads is: PrL i  1jPaL i . Values comprising the location indicator vector l ! P are thus set by tossing the location-specific coins in a sequence one after the other. We assume that there is a specific order in which the coins are tossed. To establish the order, we use a topological ordering of location variables in the Bayesian network G denoted as L t1 ;. .. ; L tq , where each parent in the network appears before its descendant; an example of such an ordering of nodes based on the network inis L 2 ; L 1 ; L 3 ; L 4 ; L q ; L 5. Consequently, coin C t1 is tossed first, and based on its outcome, the location indicator value l P t1 is set, then C t2 is tossed and l P t2 is set, and so on, until C tq is tossed and l P tq is set.
Setting feature valuesWe further view each feature value f P j 1 j d as selected from among n j possible distinct values by adhering to the following steps: 1. A dependency set is selected: A location dependency set is chosen based on a probability distribution over q such sets2. A feature-value is assigned: Based on the values taken by variables in a selected location set, the feature value is chosen. Given that the set LS k was selected, we assume that a die D Fj k with n j faces is rolled to pick a value for feature F j. If the die D Fj k lands with the v j th face up, the feature value is set to v j. The probability of D Fj k to come up as v j is: PrF j  v j jl k ; PaL k , where F j is the random variable associated with the jth feature.Based on this model, each feature value f P j is set independently of other features to construct the complete feature vector of the proteinThe generative process for a protein P is summarized as shown inWhile this assumption may oversimplify the underlying biological mechanisms, it works well in practice and has proven useful before (). Moreover, our model carefully accounts for inter-dependencies among locations, as well as among locations and features, thus indirectly capturing interdependencies among features. 2. Given the values taken by a location variable L k and its parents PaL k  in a selected location dependency set LS k , the feature value for a protein, f P j , is conditionally independent of all other location values, formally:shows the protein generation process using the standard notation of a probabilistic graphical model. Nodes represent random variables and directed edges represent dependencies among variables. The values of location and feature random variables are governed by a probability distribution and as such are denoted using circles. In contrast, the value of each location dependency set variable LS k is assigned deterministically based on the values of the location variable L k and its parents PaL k , and is denoted as a square. The variables representing locations, features, and location dependency sets are observed and hence are shown as shaded; the rest of the variables are latent and are shown unshaded. The latent variable k Fj takes on a value k, indicating the selection of the location set LS k , with a probability h k F j . As was shown in, edges among location variables capture inter-dependencies among locations. The rectangular plate notation is used to represent replication of feature and location set variables with the same dependencies. The lack of featurefeature edges captures the conditional independencies among features given location sets.Under the independence assumptions and the structure of our model described earlier, the joint probability of the location indicator vector l ! P and the feature vector f ! P is expressed as:where each term corresponds to a parameter of the generative model as described below: a. Q q i1 PrL i  l P i jPaL i  is the factorization of the joint probabil, over the individual q location indicator values; b. PrF j  f P j jL k ; PaL k  denotes the conditional probability of a feature value f P j (1 j d, where d is the total number of features), given the values taken by a location variable L k and its parents PaL i  comprising the location dependency set LS k (under the current model G);the probability that the location dependency set LS k was selected for a given feature F j and a location indicator vector l !
P .
Model learning and protein multi-location predictionIn this section, we introduce the procedure used for learning the structure and the parameters of our generative model and for predicting multiple locations for proteins. We present an expectation maximization (EM) algorithm to estimate the hidden parameters and explain the inference technique used for multi-location prediction. As our goal is to predict multiple locations for proteins, we use the probabilistic generative model presented in Section 3 to predict a 0/1 value for each location variable L i. To obtain the model, we use an iterative process (see) in which the structure of a Bayesian network and the parameters of the generative model [shown in Equation (5)] are learned. Each iteration consists of first learning a network structure and estimating its parameters, and following the learning by performance assessment of the resulting model by using it to infer the locations of proteins in the training dataset. This process is continued until a stopping criterion is met, namely, until the prediction performance of the learned model on the proteins from the training-set does not improve between two successive iterations. Typically the process does not require more than ten iterations to complete. To measure prediction performance in each iteration, we use the F 1-score metric, which is formally defined later in Section 5.2. We next discuss the procedures used for learning the structure and the parameters of our model.
Model learningIn each iteration of the learning process, we obtain a Bayesian network structure of locations using the software package BANJO () and estimate the model parameters shown in the previous section in Equation (5). The initial Bayesian network structure is learned from protein locations in the training set, and iteratively updated to reflect the most-recently estimated locations. To estimate the model parameters described in components (a) and (b) of Equation (5), we calculate the maximum likelihood estimates from frequency counts in the training dataset. As for component (c) there, the location set probability h l ! k F j  for a given location indicator vector l ! and a feature F j cannot be directly computed from the dataset. We thus use an EM algorithm () to estimate the hidden parameter h ! l ! F j , as described next. In the E-step, for each protein P and each of its feature values f P j in the training set, we compute the probability of a location set LS k to be used to determine the protein's feature value as:In Equation (6), for each location indicator vector l !
P and featureF j , the distribution h ! l ! P F j  over q location sets is initialized as uniform; thus initially h l ! P k F j   1=q for all k, 1 k q). The conditional probability, Prf P j jl P k ; PaL k , of a feature value f P j given the location set LS k (where LS k  L k [ PaL k ) is initialized to the maximum likelihood estimate computed using the training dataset. In the M-step, we re-estimate all the model parameters. For each location indicator vector l ! and feature F j , the probability of a location dependency set LS k is re-estimated as:where v j is a feature value of F j and k denotes the selection of the dependency set LS k. That is, in the numerator, for each feature, F j , we go over all feature values v j that F j takes, and all proteins in the set that have this feature value; we sum the probability of having used the dependency set LS k to generate feature value v j weighted by the probability of observing that feature value. The denominator is a normalization factor ensuring that probabilities sum to 1. The probability of a set LS k to be selected for determining f P j ; Prk Fj  kjf P j ; l ! P , is calculated in the E-stepThe conditional probability is then calculated as:. A summary of our model-learning process. The rectangular boxes represent steps in the learning process, the diamond indicates checking for a stopping criterion, and the oval represents the output, which in our case is the learned model. Directed edges indicate the order among stepsThis re-estimation formula is similar to the one shown in Equation (7), but taking into account only those proteins in the training set that are localized to the locations included in the dependency set LS k. The process of alternating between the E-step and the M-step is carried out until convergence is reached, i.e. until changes to the hidden parameter values between iterations are no greater than 0.05. Throughout the estimation process, we use Laplace smoothing to avoid overfitting, by adding fractional pseudocounts to observed counts of events (). The smoothing parameter (a) is set to 0.5, which is close to the count of rare events and almost insignificant compared with counts of frequent ones. We next present the inference procedure that we use for predicting protein locations.
Multiple location predictionGiven a protein P, represented as a feature vector fspectively, we calculate the conditional probability,The value assignment to L i that produces the highest probability is the one used as the current estimate for L i. As noted earlier, the process typically requires about ten iterations to reach convergence. We next describe our experiments and the results obtained using the protein generation model.
Experiments and resultsWe implemented our algorithms for learning parameters of the generative model and for inferring locations using Python. We have applied our system MDLoc to the largest available dataset of multi-localized proteins, previously used for training YLoc  (). Next, we describe the dataset and the evaluation methods we use, followed by experiments and results obtained using MDLoc. We also provide several specific examples demonstrating the utility of incorporating location inter-dependencies into the prediction process.
DataIn our experiments, we use a dataset first constructed for an extensive comparison of multi-location prediction systems as part of the evaluation of YLoc  (). It contains 5447 single-localized proteins, originally published by, and 3056 multi-localized proteins, originally published as part of the DBMLoc dataset (). As in a true prediction scenario it is not known a priori whether a protein may localize to a single or to multiple locations, we train our system on the combined set of proteins, thus enabling it to handle the actual prediction task. The dataset is already homology-reduced, i.e. proteins sharing >80% sequence identity with another protein in the dataset were removed. We compare the performance of our system to that of others using only multi-localized proteins (3056 proteins) because the only results publicly available for the other systems were obtained on this dataset (). The singlelocalized proteins are from the following locations (abbreviations and number of proteins per location are given in parentheses): cytoplasm (cyt, 1411 proteins); endoplasmic reticulum (ER, 198); extra cellular space (ex, 843); golgi apparatus (gol, 150); lysosome (lys, 103); mitochondrion (mi, 510); nucleus (nuc, 837); membrane (mem, 1238); peroxisome (per, 157). The multi-localized proteins are from the following pairs of locations: cyt_nuc: 1882 proteins; ex_mem: 334; cyt_mem: 252; cyt_mi: 240; nuc_mi: 120; ER_ex: 115; ex_nuc: 113. Note that all the multi-location subsets used have over 100 representative proteins. We use the exact same representation of a 30-dimensional feature vector as used for evaluating YLoc  (for further details see Briesemeister et al., 2010b): (i) thirteen features derived directly from the protein sequence data; (ii) nine features constructed using pseudo-amino acid composition (); (iii) two annotation-based features constructed using two distinct groups of PROSITE patterns; (iv) six annotation-based features based on GO-annotations.
Experimental setting and performance measuresWe compare the performance of MDLoc to that of our preliminary system () and to other systems, specifically, YLoc  (), Euk-mPLoc (), WoLF PSORT () and KnowPred site (), whose results on the multilocalized proteins are described in a previously published comprehensive study by. The comparison uses the exact same dataset from that study, and employs multiple runs of stratified 5-fold cross-validation. That is, we ran 5-fold-crossvalidation five complete times (25 runs in total), using a different five-way split each time. The use of multiple runs with multiple splits helps validate the stability and the significance of the results. The total training time for our system for the 25 training experiments is about 8 hours (wall-clock), when running on a standard Dell Poweredge machine with 32 AMD Opteron 6276 processors. To formally define the evaluation measures we use, let D be a dataset containing proteins. For a given protein P, let M P  fs i j l P i  1, where 1 i qg be the set of locations to which protein P localizes according to the dataset, and let, where 1 i qg be the set of locations that a classifier predicts for P, where ^ l P i is the 0/1 prediction obtained for location s i. We use adapted measures of multi-label precision and recall denoted Pre si and Rec si and defined as follows ():We also use the adapted measure of accuracy proposed byfor evaluating multi-label classification. Some of these measures have also been previously used for multilocation evaluation ().The multi-label accuracy and the F 1-label score used for the evaluation of YLoc  () are computed as:Finally, to evaluate the correctness of predictions made for each location s i , we use the standard precision and recall measures, denoted by Pre-Std si and Rec-Std si and defined as: Pre-Std si  TP=TP  FP and Rec-Std si  TP=TP  FN, where TP (true positives) denotes the number of proteins that localize to s i and are predicted to localize to s i , FP (false positives) denotes the number of proteins that do not localize to s i but are predicted to localize to s i , and FN (false negatives) denotes the number of proteins that localize to s i but are not predicted to localize to s i. The F 1-score for location s i is defined as:
Classification resultsIn this section, we compare the performance of our system with that of existing location prediction systems over the commonly used set of multi-localized proteins. We also report experiments using the combined set of single and multi-localized proteins as mentioned in Section 5.1. Our analysis includes an examination of the perlocation break-up of the results. Additionally, we focus on several specific examples demonstrating the benefit of incorporating location interdependency into our prediction system.shows the F 1-label score and the accuracy obtained by our current system MDLoc compared with those obtained by other multi-location predictors [YLoc  , Euk-mPLoc, WoLF PSORT and KnowPred site as reported byand by our preliminary system (Bayesian network classifiers, denoted BNCs,), using the same set of multi-localized proteins and evaluation measures. The table shows that MDLoc performs better than the existing top-systems, including YLoc  which has the best performance reported so far and whose predictions are based only on location-combinations in the training set. In contrast, MDLoc is not limited to the location-combinations in the training set, as it represents dependency of features on location-combinations in a generalizable manner, and directly captures inter-dependencies among locations. The only other system that attempts to capture such dependencies is our preliminary system BNCs. To illustrate the use of interdependency, consider the protein Securin which is included in our dataset and localizes to both the cytoplasm (cyt) and the nucleus (nuc). Securin, initially present in the cytoplasm, translocates to the nucleus in response to DNA damage (). While MDLoc assigns it to both the cyt and the nuc, YLoc  assigns it to the nuc only. Our system utilizes the dependency between nuc and cyt (represented by a directed edge between the two locations, see) to make an accurate multi-location prediction. Location dependencies reflect intrinsic relationships that locations share with each other, and in this case, it is well-known that proteins shuttle continuously between the nucleus and the cytoplasm to control a variety of functions such as cell cycle progression (Gama-Carvalho and Carmo-Fonseca, 2001). MDLoc's benefit from capturing the interdependency between cyt and nuc is also reflected in its significantly higher Multilabel-Precision and Multilabel-Recall (Pre si and Rec si , respectively) for the cyt and the nuc as shown in. As another example, consider Protransforming growth factor alpha (TGF-alpha), a protein that assists in cell growth (See NCBI's Gene database, http://www.ncbi.nlm.nih.gov/gene/7039), localizes to both the extracellular space (ex) and the plasma membrane (mem), and is correctly assigned by MDLoc to both. Here MDLoc employs the well-known dependency between the extracellular space and the plasma membrane, as reflected for instance in the exocytic trafficking pathway (), and in the transition of proteins such as hsp 90-alpha (initiated by TGFalpha) from the extracellular space to the plasma membrane in response to stress (). Again, the value of utilizing interdependencies is demonstrated in MDLoc's significantly improved precision in terms of Multilabel-Precision (Pre si ) on the ex and mem proteins (while still retaining a similar level of recall, Rec si , to that of YLoc  ).Standard deviations are shown in parentheses (if available). The highest values are shown in boldface. (A) Overall F 1-label scores and overall accuracy (Acc) obtained using our current system MDLoc, our preliminary system (denoted BNCs,), YLoc  (), Euk-mPLoc (), WoLF PSORT () and KnowPred site (). The four rightmost columns are taken directly from
Protein (multi-)location prediction i371As an example for MDLoc's ability to handle proteins whose location-combination is not included in the training set, consider Transmembrane emp24 domain-containing protein 7 (emp24). It localizes to the ER and transports secretory proteins to the golgi complex (gol) (). (The tables shown do not include ER and gol proteins, as the number of proteins from either of these locations in the dataset is very small.) MDLoc assigns emp24 to both the ER and the gol, whereas YLoc  assigns it to the ER only. As indicated before, MDLoc makes use of the dependency which captures the relationship between the ER and the gol, both of which act as components in the exocytic trafficking pathway (). We thus see that MDLoc is not restricted to predicting only pre-defined location-combinations.shows the per-location prediction results for multilocalized proteins obtained by MDLoc compared with those obtained by YLoc  (). Per-location predictions for the other systems are not shown here as they are not publicly available. Results are shown for the five locations with the largest number of associated proteins. For each location s i , we show Multilabel-Precision (Pre si ) and Multilabel-Recall (Rec si ) as well as standard precision (Pre-Std si ) and recall (Rec-Std si ). For the cytoplasm and the nucleus, which have a large number of proteins, the precision and recall values obtained using MDLoc are significantly higher in most cases than those obtained using YLoc . For locations with much fewer proteins, while the recall values when using MDLoc are marginally lower than when using YLoc  , MDLoc's precision values are typically significantly higher than those of YLoc . We note that YLoc  assigns each protein to all the locations whose probability exceeds a pre-defined threshold; as such, the number of locations it assigns exceeds that to which the protein actually localizes resulting in a lower precision. In contrast, MDLoc does not simply assign a protein to each location whose probability is higher, but rather, it simultaneously considers a set of locations and assigns each protein to the set whose overall probability is high, leading to a higher precision.shows the per-location prediction results on the combined dataset of both single-and multi-localized proteins obtained by MDLoc, in comparision to those obtained by BNCs (). While MDLoc's precision values are somewhat lower than those of BNCs, MDLoc's recall is typically higher. MDLoc simultaneously infers the probability of a set of locations; in contrast, BNCs uses an independent Bayesian network structure to infer the probability of each location separately. As such, the likelihood of BNCs to correctly assign the combination of several locations to a protein is much lower than its probability to correctly assign a single location, which directly translates into a relatively low recall measure. When using MDLoc, the increase in recall values for almost all cases is higher than the decrease in the precision values, except in the case of the extracellular space (ex). Notably, proteins in the extracellular space all originate from or are bound toward another location within the cell and as such predicting them as extracellular is challenging for most prediction systems. Moreover, MDLoc assigns some proteins hitherto known to localize only to a single location into multiple locations. It is likely that at least some of these additional predicted locations are indeed correct and can be the subject of an experimental validation. For instance, Calreticulin (Cal) is currently annotated by SwissProt as localized to the ER only. However, MDLoc assigns it to both the ER and the ex, and work bysuggests that it indeed relocates from the ER to the ex. We also examine the statistically significant differences in the Multilabel-Recall for the location with the highest number ofmulti-localized proteins (cytoplasm, 2374 proteins) and the location with the lowest number (endoplasmic reticulum, 115 proteins). The Multilabel-Recall for cytoplasm (Rec cyt ) increases from 0.80 when classifying using BNCs, to 0.83 when using MDLoc. Similarly, the Multilabel-Recall for endoplasmic reticulum (Rec ER , not shown in) increases from 0.64 to 0.69. This analysis demonstrates the advantage of using MDLoc for predicting protein locations, not just for locations that have a large number of associated proteins but also for locations that are associated with relatively few proteins.shows the prediction results obtained using MDLoc in contrast to those obtained using BNCs () for all location-combinations, using multi-localized proteins only. For each location combination in the dataset, we show the number of proteins with correct predictions for both locations, as well as for the first of the two locations, and for the second, separately. For almost all combinations, the number of proteins whose location is correctly predicted by MDLoc is significantly higher than the corresponding number when using BNCs. We examine the predictions for the location-combination with the highest number of proteins (cytoplasm and nucleus1882 proteins) and its constituent locations (cytoplasm1411 and nucleus837 proteins). As can be seen from the table, the number of multi-localized proteins whose combined-location is correctly predicted increases significantly from 976 when classifying using BNCs, to 1253 when using MDLoc. The increase shows that location inter-dependencies learnt using MDLoc help to improve predictions for multi-localized proteins.
Conclusion and future workWe presented a new probabilistic generative model for protein localization based on Bayesian networks and a mixture model, and developed a system MDLoc, to predict multiple locations for proteins. MDLoc takes advantage of the location inter-dependencies and location-feature dependency to provide a generalizable method for predicting multiple locations for proteins. Our results demonstrate the utility of using location inter-dependencies in the prediction process, and show that the performance of MDLoc improves over current state-of-the-art reported results. MDLoc significantly improves over our own preliminary method which used a relatively simple collection of Bayesian network classifiers () whose performance was on par with that of YLoc  (). In our previous method, location inter-dependencies were not learnt as part of the model but rather captured based on simple estimates of location values. In contrast, MDLoc uses a generative model comprising Bayesian networks to directly address and capture interdependencies among locations, and a mixture model to represent feature dependency on location-combinations. We iteratively learn a Bayesian network over location variables while estimating the locations using expectation maximization. Our future work includes exploring alternative ways to learn the mixture model parameters, to evaluate the model learned in each iteration of our current process, and to perform multi-location inference. We will also conduct experiments testing our system's performance on more complex location-combinations. Having a larger set of multi-localized proteins from plant-, fungi-and animalspecific organelles will also enable us to explore the possibility of building a model for each taxonomic group. As another direction, we will also experiment with features other than the ones previously used by YLoc  , utilizing multiple datasources, which is likely to be more appropriate for representing proteins in the context of multi-location prediction.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Protein (multi-)location prediction i369 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
R.Simha et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Protein (multi-)location prediction i373 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
