Motivation: New application areas of survival analysis as for example based on micro-array expression data call for novel tools able to handle high-dimensional data. While classical (semi-) parametric techniques as based on likelihood or partial likelihood functions are omnipresent in clinical studies, they are often inadequate for modelling in case when there are less observations than features in the data. Support vector machines (SVMs) and extensions are in general found particularly useful for such cases, both conceptually (non-parametric approach), computationally (boiling down to a convex program which can be solved efficiently), theoretically (for its intrinsic relation with learning theory) as well as empirically. This article discusses such an extension of SVMs which is tuned towards survival data. A particularly useful feature is that this method can incorporate such additional structure as additive models, positivity constraints of the parameters or regression constraints. Results: Besides discussion of the proposed methods, an empirical case study is conducted on both clinical as well as micro-array gene expression data in the context of cancer studies. Results are expressed based on the logrank statistic, concordance index and the hazard ratio. The reported performances indicate that the present method yields better models for high-dimensional data, while it gives results which are comparable to what classical techniques based on a proportional hazard model give for clinical data.
INTRODUCTIONSurvival analysis concerns the study of the time to occurrence of a certain event. It is best known in cancer studies where one is interested in characterizing which patients will relapse after surgery or pass away. Other applications can be found in electronic or mechanic components where the characterization of the lifetime is useful for optimizing maintenance strategies. Typically, survival data contain censored observations. Censoring indicates a lack of information on the outcome. For example, in a clinical study * To whom correspondence should be addressed. examining relapse of breast cancer patients, where patients are included between 1990 and 2000 and followed until 2008, not all patients will have experienced relapse. For these patients, the failure time is right censored. The statistical literature describes different models for the analysis of failure time data, an overview of which can be found in. The largest breakthrough in modelling survival data came in 1972 when Cox proposed his proportional hazard model (ph) (). The ph model is a semi-parametric model assuming that the hazard of an observation (the instantaneous risk of occurrence of the event given that the event did not occur before) is proportional to a 'baseline' hazard common to all observations. Proportionality is modelled as the exponential of a linear function of the covariates. The semi-parametric character of this model comes from the fact that the baseline hazard is left unspecified. Success of this model is to a certain extend due to the description and analysis of a corresponding partial likelihood function whose properties are proven to be quite similar to ordinary likelihood functions. Although the ph model is perhaps the most common survival model, some drawbacks remain. At first, the model is based on the assumption that hazards for different subjects are proportional to each other, an assumption which is not always realistic. A second drawback is the restrictive parametric form in which the variables affect the outcome. During the last decade, different methods dealing with one or both of those drawbacks have been proposed. The linear parametric form was generalized by means of anova models, splines and artificial neural networks;. Models dealing with non-linear covariate effects and not imposing the proportional hazards assumption were proposed in the field of artificial neural networks (). Due to the good performances obtained with machine learning methods in regression and classification (), ideas underlying methods of machine learning started to pervade also traditional modelling areas. Support vector machine (SVM) regression for censored data was proposed byPage: 88 8794
V.Van Belle et al.as proposed in Vanwith the linear and non-linear ph model () and with the partial logistic artificial neural network model with automatic relevance detection (plannard) (). Due to the growing interest in micro-array data within survival studies, the need for survival methods which perform well on high-dimensional data is increasing. Therefore, ssvm is compared with other survival methods, specifically adapted for high-dimensional data. Merely a few papers till date approach this problem in the context of svms, see for example Evers and Messow (2008). Many earlier published studies propose to reduce the problem of estimating a good prognostic index to a classification problem. Essentially, they discriminate between observations with (i) a poor prognosis (non-survivors) and (ii) a good prognosis (survivors). Although this approach is plausible in case all observations are followed for an equally long follow-up period, it remains open how this approach can be applied when censoring can occur at arbitrary moments. For more details, we refer to;. This article is organized as follows. Section 2 describes the setup of survival analysis and gives some more details about the ph, plann and ssvm methods. After a short introduction of the ssvm method, a feature selection technique is proposed. This method results in a sparse coefficient vector and will turn out to be useful for highdimensional datasets. In Section 3, a comparison is made between ssvm, the ph and the plannard approaches. In a first experiment, the methods are compared on clinical cancer datasets. A second experiment involves high-dimensional micro-array data. The article ends with a discussion and some concluding remarks.
METHODSThis section describes three different approaches for estimating prognostic indices for survival problems. In the remainder of this text, the p-th covariate of observation i will be denoted by x p i ; the vector containing all covariates of the i-th observation is represented as x i. The p-th element of a vector w will be denoted by w p. Consider a dataset D ={(x i ,y i , i )} n i=1 , where x i ,y i and  i represent the covariate vector, a positive survival time and an event indicator for observation i, respectively. The event indicator is equal to 1 if an event occurred, and zero if the subject is (right) censored at time y i. The survival function S(t) = P(y > t) is defined as the probability of not having experienced the event until time t. The hazard (t) is defined as the risk of the event to occur at time t, given that the event did not occur before that time:
The proportional hazards modelThe proportional hazard model () is built on two assumptions:(i) proportional hazards and (ii) linear parametric form in the covariates. Let (x, t) be the hazard, x a specific covariate vector and t the time at which one wants to estimate the hazard. The model becomes (x,t) =  0 (t)exp(w T x).The risk associated with an observation with covariates x i is related to w T x i , also called a prognostic index. The parameters w are inferred by maximizing the partial likelihood function defined aswhere R j represents all patients at risk at the j-th failure time and k is the number of distinct failure times. In practice, the log partial likelihood (w) = log(L(w)) is maximized.
Penalized proportional hazard regressionIn order to reduce the problem of overfitting, penalized forms of the likelihood function were introduced. A penalization term that is often used is called ridge regression or weight decaywith   0 a regularization constant. We will refer to this model as ph l2 in the remainder of this work. A second penalized model (ph l1 ) maximizes the log partial likelihood subject to d p=1 |w p |s, where s needs to be optimized
Including non-linearities: ph with penalized smoothing splinesTo relax the ph model towards non-parametric effects of the covariates, the use of penalized splines in ph models was proposed by Eilers and Marx (1996), among others. The ph model is rewritten aswhere f (x) represents a function of x. When using B-splines, this function is a linear combination of m basis functions B a :The penalized partial likelihood function then becomeswith  a positive regularization constant. This approach will be used for comparison and will be denoted by ph pspline .
Practical approaches for handling high-dimensional dataWhen dealing with high-dimensional data, the ph model needs to be adapted to avoid overfitting as before. Three different practical adaptations of the standard ph model are studied and implemented in, and matlab code was provided by the authors. A first method (pcr) uses (unsupervised) principal component analysis (PCA) to select a number of principal components of the expression data accounting for the largest variation in gene expression profiles. The selected principal components are then used as covariates () in a standard ph model. The spcr method selects a subset of genes which correlate best with the observed survival using a univariate ph model and applies pcr on the resulting genes (). The pls method creates new features as a linear combination of the covariates and uses these as input variables for the ph model ().
Multi-layer perceptron models
The partial logistic artificial neural network model Biganzoli et al.(1998) proposed a partial logistic artificial neural network (plann) for the analysis of survival data. This multi-layer perceptron contains three layers:(i) an input layer containing a neuron for each input p = 1,...,d, and one neuron for the time variable; (ii) a hidden layer containing h = 1,...,H hidden neurons; and (iii) an output layer with one output neuron. The model is trained as follows. First the time in which observations were followed is divided into time intervals [t k1 ,t k ],k = 1,...,K. The goal is then to estimate the chance that an observation will experience the event at study within each of these intervals, given that they did not relapse before. Therefore, each data point is replicated for each time interval in which the outcome is known. The input of the model consists of two parts: the covariate vector and a time variable indicating the time interval. The output is one if the patient experienced the event under study within the considered time interval, zero otherwise. For discrete time studies, the output of the model will represent the predicted hazard within each time interval.proposed to incorporate Bayesian automatic relevance determination (ard) in plann. A penalization term  is linked with each parameter of plann and Bayes' theorem is used as a regularization framework. As the ph model, plann estimates parameters by optimizing the likelihood function of the parameters w, given the data D, the penalization terms  and the model hypothesis space H. According to Bayes' theorem, this can be expressed as:
Page: 89 8794
Improved performance with Survival-SVM
Feature selection: PLANN model with automatic relevance detectionThe plannard procedure has three levels. On the first level, the regularization parameters  are assumed to be fixed and the prior distribution P(w|,H) of the weights w is assumed to be normal, centered at zero, with variance 1/. P(w|D,,H) can then be optimized in function of w. On a second level, the regularization parameters are estimated. On a third level, the evidence in support of a particular model hypothesis H is estimated. The prior distribution P(|H) is assumed to be log-normal. A flat prior is assumed for the model space. The optimal parameters are found by iterating over the three levels. The values of  are inversely proportional to the relevance of the variables. Seefor more details.proposed the use of an svm approach for survival analysis with ranking and regression constraints. Comparing these methods with those using only ranking constraints, the former performed significantly better. In this article, we refer to model 2, as discussed in the latter article as to survival svm (ssvm). The approach taken in ssvm is quite different from the other methods discussed in the previous subsections. Instead of inferring the hazard directly, a utility function of the covariates is searched such that the resulting utility values are as concordant as possible with the corresponding observed failures. For a thorough investigation of this reasoning and its relation with the technique of maximizing the margin in standard svms, see Van. A good concordance is found by implementing ranking constraints, penalizing misranking between pairs of observations. In addition, regression constraints are included. Those direct the estimate towards prediction of the events. In Van, it is empirically observed that such mechanism yields improved estimates over a pure ranking-based approach. Technically, consider a transformation with feature map (x) of the covariates x such that the utility estimate for observation i equals u(
Support vector machines
Survival support vector machineswith  and  positive regularization constants and i,i1 , i and  * i slack variables. In the above formulation, the data points are sorted according to their failure (or censoring) time such that y i  y i1. The first set of constraints leads to a solution for which u(x i )  u(x i1 ) if y i  y i1 is mostly satisfied: those are referred to as to the ranking constraints. The second and third set of constraints are referred to as the regression constraints, where the equality between y i and u(x i ) is desired only for non-censored observations. Since  * = 0 for right censored cases ( i = 0), the outcome is targeted higher than the survival time for these cases. Non-linearities are modelled by the choice of (x). However, thanks to the kernel trick or K(with x p the p-th covariate of observation x, c = max p min p , with min p and max p the minimal and maximal value of the p-th covariate in the given training dataset D. For categorical and binary data, K p (,) is defined asThe estimated outcome u(x ) for any new observation x is then given aswith { i },{ i } and { * i } the sets of Lagrange multipliers corresponding to the first, second and third set of constraints in (9). For more information on this subject, we refer to). The resulting optimization problem is a convex Quadratic Programming (QP) problem, which can be solved efficiently using contemporary solvers.
Feature selection in linear models: positivity constraintsIn clinical applications, one is not only interested in trying to find a 'good' prognostic index. Searching which variables/genes are relevant (and should be measured in the future) and which are not needed, is equally important. Feature selection is included in ssvm by constraining the weights w to positive weights and will be denoted as ssvmp. If the true parameters were positive, this constraint would not introduce extra bias unlike an L 1 approach. This estimate is obtained by solving the problem min,...,n.This set of constraints is only relevant after preprocessing the data, in order to ensure that negative relations are not ignored. Therefore, the concordance () between each variable and the failure time is calculated. Each variable x p with a concordance less than 0.5 is switched as x p x p. Constraining the weights to be positive will then lead to weights close to zero for irrelevant variables, and higher weights for relevant variables. This technique is closely related with the non-negative least squares estimator, and with the non-negative garotte estimator ().Page: 90 8794
V.Van Belle et al.
Feature selection in non-linear models: a two-step approachSince the above formulation cannot be extended directly to the case non-linear kernels are used, we propose a two-step approach in order to include feature selection [see also Van. In a first step, the method as described in (9) is solved. Using a non-linear but additive kernel K(, the estimated non-linear transformation of each covariate p can be estimated asxThe second step performs feature selection by consideringxconsidering consideringx p as the covariates and applying method (13) with a linear kernel. We will refer to this approach using a clinical kernel in the first step as ssvmp clinical .
RESULTSIn subsection 3.1, the performance of ssvm, ph and plannard approaches are compared based on clinical datasets. In subsection 3.2, ssvm is compared with other methods, able to deal with highdimensional data. The design parameters , ,  and s are tuned using a 10-fold cross-validation criterion. The different methods will be compared using three performance measures. Clinicians are typically interested in groups of patients with higher or lower risk profiles. Therefore, a first performance measure denotes the concordance between the predicted and observed order of relapse: the concordance index (c-index) (). In the same perspective, patients are divided into two risk groups according to the prognostic index obtained with each model. The median prognostic index is used as threshold for defining the two groups. The logrank test  2 statistic, measuring the difference in survival between both groups will be reported. As a third measure the hazard ratio as calculated by a univariate ph model using the estimated prognostic indices, normalized to the unit interval, is reported. For all three measures, a higher value corresponds to a better performance. In all experiments, datasets were 50 times randomly divided into training (2/3 of the data) and test (1/3 of the data) sets. Models were estimated based on the training data. Results are calculated based on the test data.gives an overview of the different methods and their properties. Complementary to the reported figures relating performance and clinical usefulness of the presented methods, a comparison regarding the effective CPU time required to train each method is reported in the Supplementary Material. Those results confirm the claim of fast and effective implementations compared with state-of-the-art approaches if a suitable contemporary optimization solver is used.
Clinical cancer dataThis subsection compares performances obtained by ssvm, ssvmp, ph and plannard methods applied on six different clinical datasets 1 : @BULLET The leukemia dataset () contains observations of 129 patients with leukemia. The available variables are as follows: treatment (daunorubicin or idarubicin), sex, age, Karnofsky score (indicating how well a patient having cancer is functioning, expressed on a scale from 0% to 100% ), baseline white blood cells, baseline platelets, baseline haemoglobin, kidney transplant (binary), complete remission and time until complete remission. In a first experiment, the endpoint is time until death (LD). In a second experiment, the endpoint is time until complete remission (LCR) and time until death is not taken into account. @BULLET The Veteran's Administration Lung Cancer Trial (VLC;) incorporated 137 men with advanced inoperable lung cancer. Patients were randomized to a standard or test chemotherapy. Only nine patients were still alive at the end of the study. The available variables are as follows: the Karnofsky performance score, age, prior therapy (binary), histological type of the tumour, treatment and months between diagnosis and randomization. @BULLET The data on prostatic cancer (PC;) contains 506 patients with four types of treatment (placebo, 0.2, 1.0 or 5.0 mg diethylstilbestrol daily). Although this dataset contains information on competing risks, we use it to estimate survival where death due to prostatic cancer is the event under study. The variables are as follows: treatment, age, weight index, performance index, history of cardiovascular disease, size of the tumour, a combined index of stage and histologic grade and serum haemoglobin. Of total, 483 patients had complete information on the variables mentioned above. In all, 125 (26%) patients died due to prostatic cancer during the study. All other patients were right censored at their date of last follow-up.@BULLET The Mayo Clinic Lung Cancer Data (MLC;) is a subset of data concerning advanced lung cancer patients, conducted at the North Central Cancer Treatment Group, Rochester Minnesota (). The subset used here contains 167 patients with full information on the following variables: age, sex, the physician's estimate of the ECOG performance and Karnofsky score, patient's estimate of the Karnofsky score, calories consumed at meals and weight loss in the last 6 months. In all, 120 (72%) patients died during the study period.@BULLET The German Breast Cancer Study (BC;) is a dataset containing observations of 720 breast cancer patients, who were recruited in 41 centres between July 1984 and December 1989. Available variables are as follows: hormonal therapy (binary), menopausal status (binary), patient's age at diagnosis, tumour grade, tumour size, the number of positive lymph nodes, expression of the progesterone and oestrogen receptors (in fmol). The study is performed on the 686 cases with complete data. In all, 299 (44%) of these patients had a breast cancerrelated event (remission) during the study period.The ssvm and ssvmp methods are tested using two different kernels: theresults. None of the methods performs overall better or worse than the other methods.
Improved performance with Survival-SVM
High-dimensional micro-array dataThisThe ssvm and ssvmp methods will be compared with ph models adapted for the high-dimensionality in these datasets. Since variable selection is a major issue in high-dimensional datasets, the number of coefficients w p with an absolute value larger than 10 8 is reported as # weights.indicates that the clinical kernel performs better than the linear one. However, when including a feature selection algorithm, the results of the linear kernel become significantly better. The ssvmp method significantly outperforms all other tested methods. In addition to a better performance, the ssvmp method with a linear kernel results in a sparser model than the other methods. Due to the fact that most ph models approach dimensionality reduction by composing new features as a function of the measured covariates, nearly all gene expressions need to be obtained for any test case [see e.g. Nguyen and Rocke (2002);. The ssvmp method on the other hand is able to obtain a high performance based on less gene expressions.
DISCUSSION AND CONCLUSIONSThe ph model is most often used in clinical applications, thanks to its easy applicability and interpretability. The disadvantages are that it assumes in its standard form proportional hazards, as well as a linear parametric form of the covariates. Both assumptions can be relaxed, the first one e.g. by including time-dependent variables, the second one e.g. by using regression splines. The plann model is a multi-layer perceptron model incorporating non-linearities in the covariates as well as interactions. The main disadvantage of such models is the difficulty to interpret them. Clinicians are generally interested in the contribution of each covariate to the estimated risk. Due to the complex architecture of multi-layer perceptrons, this contribution is less straightforward to recover. Including automatic relevance determination in the plann framework is a step in the right direction, although two problems remain. The first disadvantage of plann and plannard is that they both reduce the survival problem to time-dependent classification problems. Therefore, all patients need to be replicated at each time at which they are at risk. This replication leads to an exponential increase of the complexity of the estimation problem. The second problem occurs when dealing with high-dimensional data. For these datasets, the parameter space for multi-layer perceptrons becomes very large,
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Data available on http://lib.stat.cmu.edu/datasets and http://cran.rproject.org/web/packages/survival/index.html.
