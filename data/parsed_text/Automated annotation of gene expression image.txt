Motivation: Computational approaches for the annotation of phenotypes from image data have shown promising results across many applications, and provide rich and valuable information for studying gene function and interactions. While data are often available both at high spatial resolution and across multiple time points, phenotypes are frequently annotated independently, for individual time points only. In particular, for the analysis of developmental gene expression patterns, it is biologically sensible when images across multiple time points are jointly accounted for, such that spatial and temporal dependencies are captured simultaneously. Methods: We describe a discriminative undirected graphical model to label gene-expression time-series image data, with an efficient training and decoding method based on the junction tree algorithm. The approach is based on an effective feature selection technique, consisting of a non-parametric sparse Bayesian factor analysis model. The result is a flexible framework, which can handle large-scale data with noisy incomplete samples, i.e. it can tolerate data missing from individual time points. Results: Using the annotation of gene expression patterns across stages of Drosophila embryonic development as an example, we demonstrate that our method achieves superior accuracy, gained by jointly annotating phenotype sequences, when compared with previous models that annotate each stage in isolation. The experimental results on missing data indicate that our joint learning method successfully annotates genes for which no expression data are available for one or more stages.
INTRODUCTIONThe use of high-throughput image acquisition, such as in phenotypic screens, has been quickly increasing and thus provides a new source of data for computational biologists. Microscopy of colored or fluorescent probes, followed by imaging, is able to deliver spatial and temporal quantitative phenotype information such as gene expression at high resolution (). In addition, expression patterns can be documented and distributed over the internet as a valuable resource to the research community. Recent advances in throughput, or long-term investment in specific projects, have by now generated large collections of images. Such image databases are traditionally analyzed through direct inspection by human curators; an example is the Berkeley Drosophila Genome Project (BDGP) gene expression pattern database (). In this dataset, images are assigned to stage ranges within the 17 embryonic stages defined by developmental features, and annotated collectively in small groups using a controlled vocabulary (CV), i.e. annotation terms. This allows researchers to search image databases and compare spatial and temporal embryonic development. Given the very diverse nature of imaging technology, samples and biological questions, computational approaches have often been tailored to a specific dataset. For example, the image-based profiling of gene expression patterns via in situ hybridization (ISH) requires the development of accurate and automatic image analysis systems for using such data, to understand regulatory networks and development of multicellular organisms. Images are affected by multiple sources of noise due to experiments or microscopy (incomplete or multiple embryos, variance of probes across genes, illumination artifacts), making the extraction and registration of embryos non-trivial (). Peng and Myers (2004) andintroduced an automatic image annotation framework using various high-dimensional feature representations and classifying frameworks: Principal Component Analysis (PCA), wavelets, Gaussian mixture models, Support Vector Machines (SVM), Quadratic Discriminant Analysis. Each image may show the embryo under different views: lateral, dorsal or ventral; this is a challenge for gene annotation, as embryonic structures may be visible in only certain views. Yet, recent studies have shown that incorporating images from multiple views could consistently improve the annotation accuracy (). It is desirable to represent images in a way that takes advantage of image features and offers robustness to image distortions. In contrast to such large feature sets prone to high redundancy and high computational costs,identified a set of basic expression patterns in Drosophila. A set of 39 welldefined clusters describing specific regions of embryo expression were determined from 2693 lateral views of early development. As with the majority of described approaches, this study involved a high level of human intervention in selecting 'good' images for training/testing purposesa potential drawback, considering the rapid increase in the size of ISH image collections. In contrast, Pruteanuproposed a new approach for automatic annotation of spatial expression patterns using a 'vocabulary' of basic patterns that involved little to no human intervention. This work provided a flexible unsupervised framework in competitively predicting gene annotation terms, while using only a small set of features. *To whom correspondence should be addressed.  The Author 2013. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com A particular aspect that has been largely neglected by computational approaches so far is that data acquired from such experiments often span multiple time points or conditions. Phenotypes are typically annotated stage-by-stage, without jointly learning the salient temporal dependencies across multiple time points, which should allow for an overall higher accuracy; e.g. the annotation terms predicted for earlier stages should inform the prediction at later stages. Furthermore, many genes are annotated with more than one term from the vocabulary, creating an additional dependency structure between annotations within the same stage range. In this article, we address the automatic annotation of Drosophila embryo gene expression sequences, building on state-of-the-art models from computer vision and machine learning. There are several challenges that need to be addressed when approaching this problem through computational methods. As we mentioned previously, the image acquisition process results in embryonic structures with multiple perspectives, shapes and locations. Moreover, the shape/position of the same embryonic structure may vary from image to image: 'variation in morphology and incomplete knowledge of the shape and position of various embryonic structures' have made the gene annotation task more prohibitive (). We first show that a non-parametric Bayesian factor analysis (BFA) approach, the infinite factor model, allows for an efficient and sparse feature representation of the Drosophila gene expression dataset. Then, we propose a conditional random field (CRF) to tackle the time-evolving annotation task. Experiments show that the exploitation of dependencies across adjacent developmental stages leads to annotation accuracy superior to existing Drosophila gene expression annotation approaches. The proposed framework also tackles the missing data scenario: for many genes, one or more stage ranges are absent from the image collection; in such cases, human annotators would take into account the entire set of expression data from adjacent stages to successfully annotate the available images. The challenge to automatize this process is novel and represents a step closer toward a fully automatic gene annotation pipeline. These predictions can be later analyzed by biologists to assess the correctness of the image acquisition and the level of interest for that particular gene. Finally, for a given gene, the described framework predicts the entire set of annotation terms simultaneously, taking full advantage of the term dependencies that exist at the stage-range level. The rest of this article is organized as follows: in Section 2, we focus on data description and introduce the sparse BFA-CRF framework. Experimental results are reported in Section 3, followed by conclusions and future work in Section 4.Expression is visualized by RNA ISH, which provides an effective way of locating specific mRNA sequences by hybridizing complementary mRNA-binding oligonucleotides and a suitable dye (). The mRNA expression apparent in the captured in situ images was verified by independently derived microarray time-course analysis using Affymetrix GeneChip technology (more details can be found at http:// insitu.fruitfly.org, and in). Gene expression patterns were documented by taking low (2) and high (20) magnification images, at multiple developmental stages. The low-magnification digital images were taken to capture groups of embryos, to provide a permanent record of the hybridization in each well. Each slide was then further examined under higher magnification using a Zeiss Axiophot optical microscope. Images were assigned to developmental stage ranges following the sequence of events taking place at specific times after fertilization, using the 17 stages defined in (). In this analysis, we focused on the first 15 hr of Drosophila development, spanning embryonic stages 46, 78, 910, 1112 and 1316. Developmental stages 13 were skipped owing to predominant ubiquitous expression patterns not of interest to our analysis. Any gene is represented, on average, by approximately 12 images; however, the number of images per gene varies from 1 to 80. This variability reflects the BDGP strategy to document highly dynamic, complex and novel patterns, while lowering the number of images documenting common expression patterns. Among those, there are images with noninformative patterns due to poor-quality staining/washing or non-tissue specific expression (maternal or ubiquitous). Images within the same window can show different patterns due to embryo orientation or the relatively long developmental time spanned by a stage range. Images are annotated with ontology terms from a CV describing developmental expression patterns (). This vocabulary has been developed and refined by FlyBase () over the past few years, allowing human curators to compare their findings with expression data assembled from the literature, expansion of annotations to greater detail and thorough searches of datasets based on Gene Ontology schema. The annotations used throughout this project consisted of a subset of about 300 of the 5800 annotation terms in the FlyBase CV, many of which only apply to later stages of development. As mentioned previously, we use all available images in our approach, i.e. including those taken with any embryo orientation: lateral, dorsal and ventral. Before extracting features, we segmented and registered images using a previously described probabilistic segmentation approach based on statistical shape models (). This provides us with 240  120 pixel images, mostly containing a single embryo in a standard dorsal(up)/anterior(down) orientation and no background. In, we show a particular gene expression pattern across five developmental stage ranges of interest. The complexity and variability of the image data led to competitive but not perfect results, in terms of precise embryo extraction as well as embryo orientation, which increased the challenge of automatic gene annotation. We here use the average intensities in a down-sampled fixed grid size of 80  40 pixels as input features for the entire collection of images within the BDGP dataset.
MATERIALS AND METHODS
Data description
Feature extractionsparse BFAaffected only by a few underlying estimated factors, and as a result, many of the mixing weights in the model will be (near) zero. The sparse Bayesian factor model was derived using the following matrix representation:is a p  n dimensional data matrix, with n the number of features, quantifying the associated gene-expression values for p images (genes) under consideration. Each row of X is called a gene pattern with dimension 1  n. Here, we assume that each gene pattern is already normalized to zero mean. A is the factor loading matrix with dimension p  k, which contains the linear weights. S is the factor matrix with dimension k  n, with each element modeled by a standard normal distribution. Each column of S is the factor score for feature i (i  1, 2,. .. , n) and each row is called a factor. E is the additive Gaussian noise with dimension p  n. Both A and S are inferred by the model simultaneously. From the model we can see that each row of X is modeled by a linear combination of the factors (rows of S), indicating that the variability of the original p feature patterns can be explained by only k latent factors. The model can also be written in vector form as follows:where x j and a j denote the j th row of X and A, respectively, and the basis matrix S is shared across all samples. Indeed, factor analysis is an unsupervised dimensionality reduction method used widely in data analysis and signal processing (). To impose the sparsity required by the underlying biological assumption where spatial gene expression patterns are modeled only by a few domains (factors), we used the Student-t distribution, which consists of a Gaussian distribution and a Gamma prior on the precision parameter. The sparseness is directly controlled by the precision parameter j,m ; the objective of imposing sparseness is to automatically shrink most elements in A near zero. The updating equations, along with a full description of the sparse factor model used in this manuscript can be found in. For an extension of our previous work, which largely focused on two developmental stage ranges only, the number of factors (k) for every developmental stage range needed to be determined in an ideally unbiased fashion. For this, we used an adaptive Gibbs sampler, which automatically truncated the loading and factor matrices through an adaptive selection of the number of important factors. This sparse Bayesian infinite factor model, first introduced by Bhattacharya and Dunson (2011), obviates the need for pre-specifying the number of factors; the effective number of factors (here denoted by k*) is chosen such that the contribution from adding additional factors is negligible. This approach has been shown to produce accurate estimates of the true effective number of factors k*; the adaptation of the Gibbs sampler occurs every 10 iterations at the beginning of the Markov chain but decreases in frequency exponentially fast, so as to satisfy the diminishing adaptation condition in Theorem 5 of Roberts and Rosenthal (2007). More specifically, the decreasing frequency is modeled as an exponential pt  exp 0  1 t  3 at the t th Gibbs iteration with 0 and 1 chosen so that adaptation occurs every 10 iterations initially but then decreases in frequency exponentially fast. The loadings matrix is adaptively modified by monitoring theAutomated annotation of gene expression image sequences columns with all elements within some pre-specified small neighborhood of zero. For some iterations, columns may be discarded or a new column could be simply added to the matrix; the remaining parameters of the model are modified accordingly. The parameters of the factors (in the case of adding some) are estimated from their prior distribution to fill in the necessary values.
Conditional random fieldsProbabilistic graphical models such as Bayesian networks and random fields are increasingly popular choices for statistical modeling of complex biological relationships. Although Bayesian networks provide a viable solution for directed acyclic relationships where the direction of causality can be easily identified, undirected graphical models offer a clear advantage for highly connected relational structures that are not simple chains or trees. Among undirected models, CRFs () have proven to be among the most powerful predictors owing to their inherently discriminative (rather than generative) nature. In a CRF, the observable variables (X  {X i }) and unobservable variables (Y  {Y i }) are treated separately, with the unobservables globally conditioned on the observables. The relationships among the unobservables are modeled via an undirected graph G  (Y,E), in which the Y i s are the nodes (vertices), and edges E Y  Y are pairs (Y i , Y j ); the edges serve to denote direct non-independence relations between pairs of Y i s. In particular, a node Y i is taken to be conditionally independent of all other nodes Y j , given the immediate neighbors N G of Y i in the graph:The well-known HammersleyClifford theorem () provides a means of computing conditional densities via decomposition of the graph into cliques. In particular,where Y I denotes the nodes in clique I, and Z(X) is a normalizing constant; it is assumed that P(Y) 40 for all possible joint assignments to Y.  I is called the potential function for clique I; in practice, these are often pooled among like-sized cliques. Because cliques larger than some reasonable size N are typically ignored, modeling is accomplished by choosing a suitable set { 1 ,  2 ,. ..  N } of potential functions for different clique sizes; the i s and any additional parameters of the 's can be trained discriminatively via cross validation. Exact inference with a CRF is tractable if the graph can be converted into a chain or a tree. To this end, a junction tree can be obtained by collapsing tight clusters of nodes into meta-nodes and extracting a maximal spanning tree from the resulting structure (). The sumproduct algorithm () can then be applied to propagate local densities across the tree, permitting exact computation of posterior probabilities for joint or individual value assignments to nodes in the graph, or identification of the maximum a posteriori assignment; for linear-chain CRFs, these are analogous to the well-known ForwardBackward and Viterbi algorithms for hidden Markov models (). To infer the presence or absence of specific annotation terms for individual embryo images, we constructed a CRF structured as shown in. Each node Y i denotes the status of an annotation term: Y i  1 means present (the annotation term applies to the image), Y i  0 means absent (the annotation term does not apply). Columns correspond to developmental stages. All of the nodes in a column are directly connected via an edge to all nodes in adjacent columns (blue lines). Within a column, the nodes are connected in a linear chain (i.e. each node has exactly 1 or 2 neighbors within the column), with the order of the chain chosen so as to maximize the total mutual information between all adjacent pairs in the chain; this maximization was carried out via a standard traveling-salesman heuristic in Matlab. Each column was constrained to include only the most popular annotation terms in the training partition (for more details, see Results). The sparse image factors (previous section) were included as observables X i ; the X i s were specific to each column, and numbered from k  57 to k  160, depending on developmental stage, with later stages having more factors. We defined potential functions for cliques of size up to 2:is a multinomial and P(XjY i ) is a multivariate Gaussian with diagonal covariance, both trained by simple counts (maximum likelihood) from the training partition during cross validation (see Results). Coefficients 1 and 2 were estimated by maximizing the training-partition classification accuracy via simple hill climbing.  1 we refer to as the node potential, as it is associated with single-node cliques, and  2 we refer to as the edge potential, as it is associated with two-node cliques (individual edges in the graph).
RESULTSIn this section, we describe the application of a sparse BFA-CRF framework for automatic time-course gene expression pattern annotation. Our procedure starts by extracting sparse meaningful features (sBFA) from the entire collection of Drosophila embryos, suitable for downstream temporal analysis based on a conditional-random-fields approach. We then use the estimated factor loadings as observed variables in the CRF framework, so as to infer most likely annotation terms for previously unseen images.
Factor inference/decomposition of expression patternsThe BDGP collection divides early embryogenesis of Drosophila into six developmental stage ranges, 13, 46, 78, 910, 1112, 1316, and most of the CV terms are stage-range specific. As mentioned previously, we skipped stage range 13 owing to lack of informative images, as well as a low number of annotation terms associated to it. We applied the sBFA model to the entire set of images from the five stage ranges of interest. These spanned thousands of images (), with each stage being annotated by a set of 40150 annotation terms. To illustrate the potential of the sBFA for decomposing expression patterns into meaningful features, we show selective estimated factors from developmental stages 910. The model began with the set of 5929 embryo images and estimated the loadings and factor matrices while having full control of the degree of sparsity (throughout our analysis, the sparsity on the factor loading matrix A was controlled by a scale parameter of the Gamma prior distribution on the precision parameter , h 0  10 6 ).illustrates a selection of the estimated factors that, per ensemble, correspond to lateral, dorsal and ventral views, demonstrating the ability of the model to automatically extract distinct patterns for different embryo orientations. As mentioned in 'Materials and Methods' section, the number of factors was determined in an unsupervised fashion, for every developmental stage range, using the sparse Bayesian infinite factor model. The estimated number of factors can be found in; in addition, we compared these findings with an empirically determined estimation akin to. As illustrated in, the range of factors is similar for both scenarios: fully unsupervised (infinite factor models) or estimated by underlying i30 biological assumptions (generate and test). A convergence check on the estimated number of factors for two randomly chosen stage ranges is illustrated in. Similar to the sBFA analysis, the Bayesian infinite factor model was run for 6000 Gibbs iterations, discarding the first 1000 and estimating the model parameters on the remaining 5000 iterations. The feature extraction/selection process was followed by filtering non-informative (such as ubiquitous) gene expression patterns. Using Euclidean distances between estimated sparse factor analysis weights and a null vector as reference, we separated informative images from the non-informative ones (for a full description see). We successfully removed 2.69% non-informative images ().
Large-scale annotation of time-course expression patternsIn evaluating the performance of the sBFA-CRF framework, we used the estimated sparse loadings/features only on the set of genes in common between all five stage ranges of interest and a repertoire of annotation terms from a CV. The most popular annotation terms were independently selected for each stage range, to cover $85% of the entire set of genes. This resulted in a set of 1807 images and 48 annotation terms distributed as follows: 14 terms for stage range 46, 8 terms for stage range 78, 9 terms for stage range 910, 9 terms for stage range 1112 and 8 terms for the last stage range 1316 (). To assess the relative utility of various parts of our model, we determined the prediction accuracy of the full model compared with versions of the model handicapped in various ways. In particular, we considered including (in separate experiments) the following sets of edges in the CRF:Relationships across adjacent stage ranges and within stage ranges (between annotation terms): blue and green lines in(full model, scenario A). Relationships across adjacent stage ranges only: blue lines in(scenario B). Relationships within stage ranges only: green lines in(baseline, scenario C).Note: The annotation terms represent a fraction of the total CV; for any given stage range, they cover $85% of the total number of genes of interest, being frequent enough to show statistical significance.
i31
Automated annotation of gene expression image sequencesFor example, when images are annotated for individual stage ranges in isolation, the relationships indicated by the edges between adjacent stages are ignored. Two examples of Drosophila expression pattern images across time are shown in: we are interested in modeling the edge potentials between developmental stage ranges, as well as those between annotation terms within each stage range. Annotation accuracy was computed as a global measure, across all annotation terms and stage ranges, by comparing the ground truth (human curated labels) with the sBFA-CRF predictions. As previous methods had largely focused on the annotation of individual stage ranges, one term at a time, and are largely trained on heavily curated benchmark data rather than the whole BDGP dataset, a fair comparison to these approaches was not feasible. To put our approach into context, we therefore compared the results generated by the sBFA-CRF framework with our own recent binary Support Vector Machines (SVM)based classification system described in Pruteanu. We had previously shown that this approach provides competitive and often superior classification results compared with the best competing approaches, even when working with the full BDGP image data instead of 'cleaned' benchmarks. Our previous method used independent SVM classifiers for each annotation term and stage range, disregarding relationships within and between adjacent stage ranges. This resulted in lower annotation accuracy, as shown in. The SVM results are comparable with the new CRF baseline scenario, which only considers edge potentials between annotation terms within the same stage range (both models simply ignore any temporal/transition information that might improve the overall accuracy). The advantage of using the edge potentials between adjacent stage ranges translates into an absolute increase of 34% in accuracy (i.e. a relative reduction of the error rate of 420%). All models were applied to the same set of 1807 images, using 10-fold cross validation; mean values across five runs are shown.
Missing-data annotation analysisIn addition to improved gene annotation accuracy, the sBFACRF framework provides an elegant means of annotating images in missing-data scenarios. During CRF decoding, the most likely configuration of the model (i.e. values of the unobservables, Y) is computed using relationships between adjacent stage ranges, as well as within each stage range. In the case of missing data, the most likely state for a given node Y i with no directly related observables X is estimated entirely through relationships in the random field. This allows us to infer annotation terms for missing images, which is of utmost importance in scenarios where, for a given gene, data have been collected for only a subset of the stage ranges. In evaluating the performance of the sBFA-CRF model in annotating missing data, we manually removed 525% of images from the 1807 gene set, by randomly selecting genes and removing their corresponding images; for missing images, the node potentials were set to 1. Results are shown in, where the model included edge potentials between adjacent stage ranges, as well as within stage ranges (CRF scenario A); we were able to annotate with 80% or better accuracy even for scenarios with 25% missing data. Remarkably, our model outperforms the SVM classification framework (which had access to full data) even when 15% of the data are withheld from the sBFA-CRF.illustrates particular cases with genes that are correctly annotated, for stages where their images were missing. As previously mentioned, this is of particular interest to biologists who require predictions for stages where images have not yet been collected. Our results confirm that the proposed time-course pipeline leads to highly successful expression pattern classification, despite the presence of uninformative images, registration errors and missing data in considerable amounts. Lastly, we compared the sBFA-CRF predicted labels to the human curated ones (the ground truth), so as to identify genes and annotation terms for which the annotations were different but the outcome from our model appeared consistent. We
i32recognize that for the same annotation term, the corresponding regions in different images may have significant variations in visual appearances, which would lead to a difficult manual annotation task and could sometime generate ambiguous outcomes. We show three examples for which the sBFA-CRF annotations are opposite from the human curated ones and are likely correct given the full context (). For all three scenarios, we confirmed our findings with the BDGP human curators. Arguably, the model was correct in predicting different annotation terms in the following examples, including gene FBgn0003502, where human curators initially decided that expression in 'procephalic ectoderm AISN' is not detected for stage range 46; however, the sBFA-CRF predicted label, as well as a second careful visual inspection, would suggest the contrary. In addition, we identified another instance where 'ventral ectoderm anlage' should have been annotated for gene FBgn0022073 in stage range 78. The last scenario (gene FBgn0033988) corresponds to a case where all images are extremely difficult to annotate owing to out-of-focus staining issues or overall noise. On a second inspection, the model was arguably correct in labeling the annotations for both stage ranges 46 and 910.
CONCLUSIONSWe have described a novel sBFA-CRF model to automatically annotate Drosophila embryo gene-expression time-course data.Note: sBFA-CRF and SVM models: mean annotation accuracy, over 5 N-fold cross validation runs (N  10). Scenario (A) includes relationships between adjacent stage ranges and within stage ranges; scenario (B) considers only relationships between adjacent stage ranges; scenario (C) models only relationships within stage ranges (baseline). For the SVM model, we used independent classifiers for each annotation term and stage range.
i33
Automated annotation of gene expression image sequencesThe sparse BFA framework represents an efficient feature selection technique, which automatically determines the feature-space dimension, using a non-parametric implementation. The learned features are then used as observed variables in the CRF framework, so as to infer most likely annotation terms for previously unseen images. The CRF encodes temporal relationships between adjacent stage ranges throughout Drosophila development. By capturing the temporal sequence, the model is able to predict the entire collection of annotation terms in a single run and achieves superior performance when compared with highly competitive models that annotate stages in isolation. In addition to improved annotation accuracy, the experimental results demonstrate the success of the method in handling missing-data scenarios. This is extremely useful in real-life scenarios when estimates are needed over the ensemble of annotation terms, with only partial data being collected. One promising extension to our approach would be to include 'latent' annotation terms in the CRF structure, to account for additional rare annotation terms for which we would not attempt to obtain a prediction. These latent terms could have limited connectivity in the graph, so as to allow large numbers of latencies to be included without compromising decoding efficiency. Such an extension may well improve prediction accuracy for the primary terms, even if the latent terms are themselves difficult to accurately predict (due to paucity of training data for those terms). It would also increase the flexibility of the resulting model: while we currently select the primary annotation terms manually based on their popularity among genes in a given stage range, a simple threshold on the number of genes being annotated, together with an appropriate means of ranking terms, would allow to automatically partition the primary versus latent sets. Based on our experience with the BFA-CRF model described here, additional work along these lines seems promising. Finally, we are continuing to develop this approach in close collaboration with biologists so as to suggest outliers or interesting patterns during the anticipated expansion of the BDGP collection to the whole Drosophila genome. We plan to incorporate the sBFA-CRF framework into a Fiji plug in (Schindelin et al.,. Missing-data gene annotation analysis. Shaded boxes indicate the stage range for which data were missing (we manually removed those images). In two cases, the entire set of annotation terms is correctly annotated within the stage range despite the fact that no images were available (third and sixth rows). In these two scenarios, the CRF used the relationships across adjacent stages, to estimate the most likely configuration. A selection of the correctly and incorrectly annotated terms for the stage range with missing data are shown in the last two columns. Annotation accuracy results for missing data scenarios. The accuracy values were computed as global measures, across the entire set of 1807 genes. For each case, we randomly selected 525% of the complete gene set and removed their corresponding images, so as to simulate missing data scenarios. The green bar indicates the annotation accuracy for the full dataset scenario (previous analysis); the red bar corresponds to the SVM analysis
i342012)a gene annotation tool that would accurately assign annotation terms to new/unseen images, in a timely manner.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Sparse Bayesian factor analysis (sBFA) is a statistical method for modeling many dependent random variables through linear combinations of a few hidden variables (Gorsuch, 1983). This model is designed to address the high-dimensional setting where hundreds or thousands of genes are simultaneously examined. The sparsity assumption is the key feature in our model that allows us to scale stable and accurate inference to a large number of images/genes represented by many image input features. For high-dimensional models, sparsity helps prevent sampling errors from swamping out the true signal in data, leading to stable parameter estimates. In our framework, sparsity implies that each image/gene is i28 I.Pruteanu-Malinici et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
