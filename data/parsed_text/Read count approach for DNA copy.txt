Motivation: The advent of high-throughput sequencing technologies is revolutionizing our ability in discovering and genotyping DNA copy number variants (CNVs). Read count-based approaches are able to detect CNV regions with an unprecedented resolution. Although this computational strategy has been recently introduced in literature, much work has been already done for the preparation, normalization and analysis of this kind of data. Results: Here we face the many aspects that cover the detection of CNVs by using read count approach. We first study the characteristics and systematic biases of read count distributions, focusing on the normalization methods designed for removing these biases. Subsequently, we compare the algorithms designed to detect the boundaries of CNVs and we investigate the ability of read count data to predict the exact number of DNA copy. Finally, we review the tools publicly available for analysing read count data. To better understand the state of the art of read count approaches, we compare the performance of the three most widely used sequencing technologies (Illumina Genome Analyzer, Roche 454 and Life Technologies SOLiD) in all the analyses that we perform. Contact:
INTRODUCTIONHuman genomes are characterized by genetic variants that range from the single base pair to large chromosomal events. Recent studies have clearly shown that human genomes differ more as a consequence of structural variants than of single-base pair differences (). Structural variants (SVs) are operationally defined as genomic events >50 bp () that include copy number variants (CNVs) and balanced rearrangements such as inversions and translocations. With the sequencing of human genomes now becoming routine, the challenge is to discover the full extent of structural variations and understand its effect on human diseases, complex traits and * To whom correspondence should be addressed. evolution. The last few years have seen the emergence of several high-throughput sequencing (HTS) platforms that are based on various implementations of cyclic-array sequencing (). The commercial products that are based on this sequencing technology include the Roche's 454, the Illumina's Genome Analyzer (GA), and the Life Technologies's (LT) SOLiD. Although these platforms are quite diverse in sequencing biochemistry as well as in how the array is generated, all of them allow to sequence millions of short sequences (reads) simultaneously and are able to sequence a full human genome per week at a cost 200-fold less than previous methods. The advent of HTS platforms has opened many opportunities for the study of genomic variants. The first HTS-based approach to detect SVs was based on paired-end read mapping (PEM), which identifies insertions and deletions by comparing the distance between mapped read pairs to the average insert size of the genomic library. Although this method is able to identify deletions <1 kb with high sensitivity, it does not allow for the discovery of insertions larger than the average insert size of the library and of the exact borders of SVs in complex genomic regions rich in segmental duplication (). An altenative HTS-based approach is based on split-read (SR) methods that allows to detect deletions and small insertions on the basis of a split sequence-read signature: the alignment to the genome is broken and a continuous stretch of gaps in the read indicates a deletion or in the reference indicates an insertion. Although SRs approach can be devised to detect a wide range of SV classes with exact breakpoint resolution, it is currently reliable only in the unique regions of the genome. In this scenario, a very promising approach for the identification of SVs using HTS technologies consists in measuring the number of reads aligned to the human reference genome (). Assuming the sequencing process is uniform, the number of reads mapping to a region is expected to be proportional to the number of times the region appears in the DNA sample. Following this assumption, the copy number of any genomic region can be estimated by counting the number of reads [read counts (] aligned to that particular region.were the firsts to use this approach to detect copy number alterations between tumour and healthy samples of the same individual, whileproposed to use RC data to look for genomic regions that differ in copy number between normal individuals of the 1000 Genomes Project (). At present, few algorithms for RC analysis have been packagedPage: 471 470478
Read counts and CNVsinto pipelines and are publicly available, including RDxplorer (), ReadDepth (), CNAseg (), CNV-seq (), JointSLM () and CNVnator () (seeand Supplementary Material). The analysis pipeline implemented in these packages for discovering CNVs is conceptually derived from array-CGH (aCGH) data analysis and can be divided into four fundamental steps:Data preparation consists in filtering and counting the number of mapped reads in non-overlapping genomic windows of length W. Once the RCs have been estimated, the first transformation applied to the data, referred to as normalization, adjusts the individual RC to appropriately mitigate systematic biases so that meaningful biological comparisons can be made. Normalized RCs are then sorted according to genomic position and statistical methods are applied to detect the boundaries of the regions with changed copy number. The last step of the analysis pipeline consists in estimating the DNA copy number of each region within breakpoints. In the present article, we face the many aspects that cover the detection of CNVs by using RCs approach. We study all the steps necessary to infer the copy number of a genomic segment: RC estimation, RC normalization, CNV regions detection and copy number estimation. To better understand the state of the art of RC approach, we compare the performance of the three most used sequencing technologies (Illumina GA, Roche 454 and Life Technologies SOLiD) by using both high (2040) and low coverage (4 6) sequencing data generated by the 1000 genomes project consortium (seeand Supplementary Material for more details).
METHODS
Data preparationRC method belongs to the category of resequencing approaches, and the first fundamental step of this analysis consists in mapping the set of short reads against a reference genome by means of short read aligners (see Supplementary Material). Once short reads have been aligned to the reference genome, we need to perform a series of preparation steps before RC estimation. These steps include: @BULLET Removal of duplicated sequences. @BULLET Removal or flagging of sequences with low mapping quality (MQ). @BULLET Choice of the best window/bin size.The main purpose of removing duplicates is to mitigate the effects of PCR amplification bias introduced during library construction. All the analyses performed in this article have been made on aligned data with duplicated reads removed by means of rmdup command of samtools () (Supplementary Material). After duplicated reads removal, low MQ sequences need to be taken into consideration: sequences with low MQ score usually fall in repetitive regions of the reference genome or have low base quality. For these reasons,removed all the reads with MQ < 30. Conversely,used the reads with MQ score equal to zero to classify CNVs called in duplicated or retrotransposon regions of the reference genome. Finally, the best window/bin size needs to be estimated.chose a window size of 100 bp for the high coverage data of the 1000 genomes project (2040 coverage) because a larger window size would provide less precision in defining the breakpoints of CNVs and because at 30 coverage, the distribution of RCs of 100 bp windows are well approximated by a normal distribution, while RCs in smaller window sizes are not.used a bin size of 50 bp for the analysis of the COLO-829 malignant melanoma cell line sequenced at 40 coverage.found that the optimal bin size, and thus breakpoint resolution accuracy, scales inversely with the coverage, resulting in  100 bp bins for 2030 coverage,  500 bp bins for 46 coverage and  30 bp bins for 100 coverage. At present, only two papers have introduced a method to automatically estimate the best window size.propose to estimate the best window size by modelling RCs by means of a negative binomial distribution. They generate a negative binomial distribution with mean  =  (with  = N * W /G, where N is the total number of read, W is the bin size and G is the genome size), and size parameter  = /(i1), where i is the index of dispersion. Then they generate distributions using the expected number of reads with copy number of one, two and three, and choose a threshold value for gains and losses that minimizes the number of bins that are misclassified. The FDR can then be calculated as the number of misclassified bins divided by the total number of bins. Xie andcalculate the best possible resolution (i.e. the best possible window size) according to a preset value for significance level p and CNV detection threshold r (Supplementary Material).
RC biases and normalizationRCs data are affected by two main sources of bias: the local GC content and the genomic mappability. The correlation between read coverage and DNA) is due to the fact that the genome contains many repetitive elements and aligning reads to these positions leads to ambiguous mapping. In order to minimize the effect of these sources of variation and make data comparable within and between samples, RCs need to be normalized. At present, there are two approaches for correcting RC data for sequencing biases due to local GC content.proposed to mitigate the dependence between local GC content and RC by using the ratio of the number of reads in tumour DNA and its paired normal DNA, processed at the same time.proposed to adjust the RCs by using the observed deviation in coverage for a given GC percentage. In practice, for all the GC percentages (0, 1, 2, ... , 100%) they determined the deviation of coverage from the genome average and then corrected each RC according to the following formula:where RC i are read counts of the i-th window, m GC is the median RC of all the windows that have the same GC percentage as the i-th window, and m is the overall median of all the windows. Mappability normalization has been faced in two different papers.proposed to correct for mappability by multiplying the number of reads in a given bin by the inverse of the percent mappability in that region, whreaseproposed to use an undecimated discrete wavelet transform (DWT) to smooth RCs in the regions of low alignability. Here we propose to correct RC for mappability bias with a novel normalization scheme inspired by the GC content normalization of: RCs are corrected by using the observed deviation in coverage for a given mappability score. Each RC is corrected by the following formula:where RC i are RCs of the i-th window, m MAP is the median RC of all the windows that have the same mappability score as the i-th window, and m is the overall median of all the windows.
CNVs detection algorithmsOnce the RCs have been corrected for local GC content and mappability, the data that we obtain are mathematically very similar to the signal obtained from aCGH experiments. Deletions or duplications are identified as decrease or increase of RC across multiple consecutive windows. Moreover, like aCGH data, RC signals are affected by noise caused by mapping errors and random fluctuations in genome coverage. For these reasons, the events in RC data can be detected using the same algorithmic approaches that have been used for aCGH data. At present, few statistical methods have been developed and tested for the detection of CNVs from RC data. Some of these algorithms come from microarray literature while others have been tailored for this kind of data.used the circular binary segmentation algorithm (CBS) (), originally developed for genomic hybridization microarray data, and both applied it to sequencing data generated by the Illumina GA platform.extended the shifting level model (SLM) (introduced CNASeg, an HMM-based algorithm to segment the RC data, followed by a segment merging step. The CNASeg method was originally applied to cancer sequencing data generated by the Illumina GA platform. While the first four statistical methods require only one sample at once, the approaches of Xie and Tammi (2009) andneed the sequencing data of two samples. The EWT and CNV-seq methods are sliding window approaches that converts RC data into a statistic (t-statistic for CNVseq and Z-score for EWT) and infer altered regions by using the distribution of that statistic. Conversely, the CBS, SLM, MSB and CNASeg algorithms are segmentation methods that allow to split RC data into segments, each containing the same number of DNA copies. Segments with an altered DNA copy number are detected by means of a simple threshold method ().
Copy number estimationThe statistical methods introduced in the previous section allow for the identification of genomic regions with altered DNA copy counts by detecting the border of consecutive windows with increased or reduced RCs. Once the limits of the altered region has been detected, the estimation of DNA copy number (genotyping) for those regions must be performed.estimated DNA copy number by rounding the median of the RCs (normalized to copy number 2) of each detected region to the nearest integer, whileassigned copy number to each genomic region by calculating its RC signal normalized to the genomic average for the region of the same length. These simple estimation strategies follow the assumption that the sequencing process is uniform and consequently the number of read that maps to a genomic region is expected to be proportional to the number of times the regions appears in the DNA sample: a genomic region that has been deleted (duplicated) will have less (more) reads mapping to it than a region not deleted (duplicated).
Read counts and CNVs
RESULTS
Data filtering and bin size estimationTo understand the effect of removing sequences with low MQ on RC distributions, we calculated the signal to noise ratio (SNR) for different MQ threshold and different windows size W (see Supplementary Material for more details) and the results are reported in Supplementary. The SNR has been calculated by means of the following formula:where m is the median value of the normalized RC of genomic regions predicted as two copies while  2 is the variance of the normalized RC of regions predicted to be one copy by. The results of these analyses show that filtering out reads with low MQ slightly affects the SNR of RC data. These results suggest not to remove low MQ reads, since they can be used for subsequent analysis as in. In order to investigate the performance of the two bin size estimation methods proposed by Xie and Tammi (2009) and, we simulated sequencing data for different coverage for the three HTS platforms (see Supplementary Material for more details). The results of these analyses are reported in Supplementary. As expected, the larger is the coverage of the sequencing data and the smaller is the bin size predicted by the two methods. The method proposed by Xie and Tammi (2009) estimates bins of similar size for the Illumina and SOLiD platforms. This is due to the fact that the Xie and Tammi estimation procedure does not take into account the overdispersion of RC distributions. Conversely, the approach proposed byallows for a better estimation of the bin size for the three HTS technologies, taking advantage of the use of the index of dispersion. However, the bin sizes predicted by the two methods are not optimal: for Illumina platform, at 30 coverage, the Miller method estimate a bin size of 1000 bp, while at 5 coverage it estimates a bin size of 6600 bp. These estimates are at least 10 times higher than those reported by: for instance, by using a bin size of 100 bp for 2030 coverage data, the JointSLM and EWT algorithms were able to detect CNV regions as small as 500 bp with a true positive rate >0.8 and with a minor fraction of false positive events of this size (). The use of the bin sizes estimated by the method of(i.e. 1000 bp for high coverage) does not allow for the detection of CNVs as small as 500 bp leading to a loss of resolution and accuracy in the discovery of genomic variants.
RC distribution and biasesThe detection of CNVs using RC analysis is based on the assumption that the reads are randomly and independently sampled from any location of the test genome with equal probability. Under this assumption, the distribution of the count of reads that map into a window of the reference genome should be Poissonian. However,have previously reported that RCs by Illumina GA follow a Poisson distribution with a slight overdispersion. In order to study the properties of RC distribution, we analysed high and low coverage sequencing data generated by the 1000 genomes project consortium and we used different values of W for the three HTS platforms (see Supplementary Material for moredetails). The results of these analyses are summarized inand Supplementaryand clearly show that RC data can be modelled by means of a negative binomial distribution. According with the results of, we found that RC distribution for Illumina and SOLiD platforms exhibit an index of dispersion (ratio between variance and mean) largely greater than one. Conversely, 454 platform produces the RC data distribution with the lower ratio between variance and mean (Supplementary Tables S1 and S2). The overdispersion of RC data distributions can be accounted for to three main sources: @BULLET The existence of genomic regions of duplications and deletions (CNVs).) Correlation between RC data and region mappability for the three HTS platforms (GA, 454 and SOLiD). Upper border of the dashed lines represent the 90th percentile of the normalized RCs, while the lower border represent the 10th percentile. Upper border and lower border of the solid lines represent the 90th percentile and the 10th percentile, respectively, of the poisson distribution with mean = mean value of the RCs. To obtain the histograms without CNV (grey bar) of, d and g, we removed from RC data all the CNV regions that belong to the Database of Genomic Variants and the CNV regions previously identified byfraction of the genome subject to variation of the copy number is 3.7% (). This means that a considerable amount of genomic regions contains an average number of reads smaller or greater than the global average number of reads. The removal of genomic regions with known CNVs reduces the index of dispersion of RC data distribution for all the three HTS platforms for high and low coverage data. We investigated the relationship between RC and GC content (Supplementary Materials) for all the three HTS platforms and according with the results ofwe observed that RC is maximum for values of GC content between 35% and 60% while it decreases at both extremes. In particular, we observed that GC content bias is larger for GA and SOLiD platforms, while it is smaller for the ROCHE data (see Figures 2b, e and h and Supplementary). This is confirmed by the statistics reported in Supplementary Tables S3 and S4, where the percent variation between the raw index of dispersion and the GC index of dispersion is much larger for GA and SOLiD data than for the Roche data. The analysis of regional mappability (, f and i and Supplementary) show a strong correlation between RC data and genome mappability: the RC distribution for high mappability score is closer to Poissonian than genomic regions with low mappability (low mappability regions show large RC overdispersion). Moreover, for GA and 454, the mappability has little effect on the mean number of aligned reads at each bin of mappability score, while, for SOLiD platform, the mappability strongly affect the RC mean value. Also these results are confirmed by the statistics reported in Supplementary Tables S3 and S4: the percent variation between the raw index of dispersion and the mappability index of dispersion is very large for the SOLiD platform (65% and 83% for high coverage and about 60% for low coverage) while it is comparable to other source of variation for GA and 454 platforms. This can also explain why SOLiD platform shows the highest value of the index of dispersion.
RC data normalizationIn order to evaluate the performance of the five normalization approaches described in Section 2, we applied them to the high and low coverage data generated by the 1000 genomes project consortium and the results of these comparisons are reported inand Supplementaryfor GC content and inand Supplementaryfor mappability. The ratio approach proposed by(see, f and i and Supplementary) is not able to remove the GC content effect for all the three HTS platforms, while it performs very well in mitigating the mappability bias also in the case of the SOLiD platform where the mappability effect is very strong (see, j and o and Supplementary). The GC content normalization approach proposed by(see, e and h and Supplementary) is able to properly remove the GC content effect for all the three HTS platforms. The comparison between the other three mappability normalization methods clearly show that the approaches proposed by(see, h and m and Supplementary) and(see, i and n and Supplementary) are not able to completely correct the non-linear bias produced by genomic regions with low mappability. These analyses also demonstrate that the approach introduced bygenerates additional biases and has the disadvantage that much data are discarded since RCs with extremely low mappability (< 25%) are filtered out to prevent overcorrection. The additional bias generated by themethod is due to the assumption that RC is proportional to the percent of mappability: multiplying RC data by the inverse of percent mappability leads to overcorrection. Conversely, the mappability normalization scheme proposed in this article (, g and l and Supplementary) allows to correct this bias without filtering out RC data. Moreover, also in the case of the highly biased SOLiD data the median method permits correction of RC mean value. The results of all these analyses indicate that the normalization method byand the median method are the best strategies to remove GC content and mappability biases, respectively. Moreover,
Page: 475 470478
Read counts and CNVs
CNV regions identificationTo test the ability of different algorithms in detecting CNVs of different size, we made an intensive simulation based on synthetic chromosomes generated from RCs of the individuals NA12878 and NA19240 for high coverage sequencing data and the individuals NA11840, NA11830 and NA12043 for low coverage sequencing data. The principal aim of these simulations is to evaluate and compare the capability of each algorithm in detecting sudden shifts in the mean value of the signal as a function of the width of the shift. For this purpose, we have built a benchmark synthetic dataset generated by using GC-and mappability-adjusted RC data with the same normalization scheme: GC correction was performed by means of thenormalization scheme, while the mappability bias was corrected by means of the median normalization scheme. Each synthetic chromosome was generated by sampling RC data windows from genomic regions previously predicted as two-copy and one-copy byto simulate both normal copy count and altered regions (see Supplementary Material for more details). We compared the performance of the six algorithms described in Section 2: two sliding window methods (CNVseq, EWT) and four segmentation algorithms (CBS, SLM, MSB and the HMM of CNASeg). To evaluate the performance of the six algorithms, we used two different strategies. To test the capability of each algorithm in discovering CNVs, we used the approach previously used by: a detected segment is considered a true positive (TP) if there is any overlap between the detected segment and the synthetic altered region, and is considered a false positive (FP) if there is no overlap with a synthetic altered region. To understand the accuracy of the six methods in detecting CNVs at the boundaries (breakpoints detection), we computed the receiver operating characteristic (ROC) curve as inand we calculated the area under the ROC curve (AUC). The results of all the simulations are summarized inand SupplementaryS38. Globally, the algorithms that ensure the best results in terms of both sensitivity and specificity are the EWT and the SLM methods. The CBS and MSB algorithms obtain good results in detecting alterations made of a large number of windows (N = 50 and N = 100), while their performance reduces for alterations made of a small number of windows (N = 5 and N = 10). The HMM algorithm of the CNASeg package performs well on RCs generated from high coverage sequencing data while leads to modest results for low coverage data. The CNVseq method gives poor results for both high and low coverage data with all the bin size we simulated. The CNVs detection analyses (Supplementary Figures S31S38) show that for high coverage data from GA platform the EWT and SLM algorithms are able to detect genomic alterations as small as 500 bp and 1 kb, respectively, with a TPR >0.8 and with a minor number FP events. On the same data, the CBS, MSB and CNASeg methods enable the detection of CNVs as small as 25 kb with a TPR >0.8 and small number of FP events. For low coverage data (Illumina GA platform), the EWT and SLM methods are able to discover genomic alterations as small as 5 kb, while CBS, MSB and CNASeg detect CNVs as small as 25 kb. The CNVseq algorithm identifies a very large number of FP events for both high and low coverage sequencing data making its use difficult for the detection of genomic regions involved in CNVs. The ROC curves reported in Supplementary Figures S11 S30 show that segmentation algorithms (SLM, CBS, MSB and the HMM of CNASeg) have low FPR at the expense of low TPR, while smoothing algorithms (EWT and CNVseq) have high TPR at the expense of high FPR. Moreover, as reported in, ROC curves are informative in understanding how an algorithm performs in estimating the boundary of the altered region. When the algorithm over-estimates the boundary, FPR increases while TPR remains fixed. Conversely, when an algorithm under-estimates the boundary, TPR decreases while FPR remains fixed. Bearing this in mind, the ROC curves reported in Supplementary Figures S11S30 suggest that segmentation methods (SLM, CBS, MSB and the HMM of CNASeg) tend to under-estimate the boundaries of CNV, while the EWT algorithm tend to over-estimate the boundaries of the CNV Page: 476 470478, CNAseg, CBS, SLM and MSB) are compared in the analysis of synthetic chromosomes. Synthetic chromosomes are obtained from high and low coverage sequencing data of the three HTS platforms for bin size of different length (W = 100 bp, W = 500 bp, W = 1000 bp and W = 2000 bp). For each algorithm, the area under the curve is averaged across 1000 simulations. Each bar of the plot is obtained averaging the performance of each algorithm for all the alteration widths simulated (N = 5, N = 10, N = 20, N = 50, N = 100).regions. According to the AUC results, the CNVseq algorithm obtain poor results in terms of both sensitivity and specificity. All these results reflect the algorithmic nature of each method: as reported in aCGH literature, smoothing approaches allow for the detection of small genomic events but with low breakpoint resolution, while segmentation strategies are more suited for the identification of larger events with high breakpoint resolution. The barplots ofand Supplementary
A.Magi et al.
Read counts and CNVsbin sizes we studied. Moreover, we found that the detection of DNA alterations in samples with low sequencing coverage is identical to analysing samples with high sequencing coverage if a bin size is larger. In particular, we found that the Illumina platform obtains high accuracy for bin size W = 100 bp at high coverage and W = 500 bp at low coverage, while the 454 and SOLiD platforms give good results for bin size W = 500 bp for high coverage and W = 1000 bp for low coverage.
Copy number estimationTo study the relationship between DNA copy number and RCs data, we examined several broad genomic regions that were previously reported to have copy numbers equal to 0, 1, 2, 3 and 4 by(Supplementary Material). We analysed RC data of these regions for different window sizes for the high and low coverage sequencing data generated with the three HTS platforms. The results of all these analyses are reported in(high coverage data) and Supplementary(low coverage data) and clearly reflect the overdispersion of the RC distributions generated by the three sequencing technologies. For GA and 454 platforms, we observed an excellent agreement between mean RCs and DNA copy number for high coverage data for all the bin sizes we analysed, while for SOLiD platform we found that RC are not well correlated with validated DNA copy number. For low coverage data, we found a smaller correlation coefficient for all the HTS platforms. However, also in this case GA and 454 technologies better predict the absolute value of DNA copy number.
DISCUSSION AND CONCLUSIONThe use of RC of sequences aligned to a reference genome is, at present, the most powerful method to accurately predict absolute copy numbers of genomic regions. Although this computational strategy has been recently introduced in literature, much work has been already done for the preparation, normalization and analysis of this kind of data. Normalization methods allow to remove systematic biases due to local GC content and region mappability: the results reported here clearly show that the best way to remove local GC content bias is the Yoon et al. approach, while the best scheme to correct mappability bias is the median method proposed in the present article. CNVs detection algorithms can be exploited to detect CNVs with an unprecedented resolution that in the best case reaches the order of hundreds of base pair. In all the simulations, we performed we found that the EWT and SLM algorithms give the best results in terms of both sensitivity and specificity. The resolution of CNV detection can be improved by increasing the SNR of RC signals: reducing the sequencing error rate or increasing the coverage of the sequencing experiments will improve the performance of statistical methods in detecting small shifts in the signals. Automatic strategies for bin size calculation fail in estimating the optimal bin size whatever the coverage. Although the method proposed bymodels RC data by means of a negative binomial distribution, it overestimates the bin size leading to a loss of resolution accuracy. After an intensive simulation on synthetic data, we found that the best way to choose the optimal bin size is following the suggestion of100 bp window for 2030 coverage, 500 bp window for 46 coverage and 50 bp window for 100 coverage. The comparison between the three HTS platforms clearly showsthat the Illumina platform allow to obtain the best level of accuracy and resolution in the discovery of genomic regions involved in CNV and the prediction of absolute DNA copy number. Although RC approach is the only sequencing-based method to accurately predict absolute DNA copy numbers, it has distinct advantages and disadvantages over other approaches in detecting certain classes of SVs and the breakpoint resolution is often poor with respect to other sequencing-based methods. The analyses employed with different approaches [PEM, SR () and RC] on the same data in the framework of the 1000 Genomes Project show that RC-based approach can better ascertain CNVs in segmental duplication than PEM-based methods. Conversely, RC methods mostly miss CNVs consisting entirely of a single retrotransposon (LINE, SVA or HERV-K) that are easily detected by PEM and SR approaches. Additionally, RC analysis is not able to detect balanced rearrangements that can be instead discovered by PEM and split read methods. The comparison with the CNVs identified by microarray technologies shows that the great majority of the calls overlapped between the two methods (). Despite the great overlap Page: 478 470478
A.Magi et al.between RC and microarray calls, RC is better than microarray at detecting smaller events and does not suffer from oversaturation at high copy counts, allowing a more accurate estimation of very high copy counts. While the sections discussed above describe the great progress achieved over the last 3 years in using RC data to discover CNVs, much work remains. When RC data are used to analyse tumour samples, statistical approach to infer copy number should take into account cellularity and tumoural heterogeneity and for this reason we would need a more sophisticated approach similar to CGHcall (van de) or FastCall () instead of using the simple rounding to the closest integer. Finally, the breakpoint resolution of RC methods is often poor with respect to other sequencing-based approaches. However, given the approximate CNV breakpoint detected by an RC approach, the detection precision can be brought to 1 base resolution by refining the breakpoint by means of SRs techniques. The ultimate way to accurately detect all forms of genomic structural variants is de novo assembly (). Nevertheless, assembly approaches are still in their early stages and are capable to type structural variants only if the sequence reads are long and accurate enough to allow de novo assembly. Moreover, assembly algorithms have been shown to collapse in highly repeated and highly duplicated genomic regions (). Albeit assembly approaches show a lot of potential (facilitating the pairwise genome comparison) their application as routine methods still need further efforts, in both computational and technological developments.
ACKNOWLEDGEMENT
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
