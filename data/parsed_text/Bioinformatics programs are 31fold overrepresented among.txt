Motivation: To analyze the relative proportion of bioinformatics papers and their non-bioinformatics counterparts in the top 20 most cited papers annually for the past two decades. Results: When defining bioinformatics papers as encompassing both those that provide software for data analysis or methods underlying data analysis software, we find that over the past two decades , more than a third (34%) of the most cited papers in science were bioinformatics papers, which is approximately a 31-fold enrichment relative to the total number of bioinformatics papers published. More than half of the most cited papers during this span were bioinformatics papers. Yet, the average 5-year JIF of top 20 bioinformatics papers was 7.7, whereas the average JIF for top 20 non-bioinformatics papers was 25.8, significantly higher (P < 4.5 Â 10 À29). The 20-year trend in the average JIF between the two groups suggests the gap does not appear to be significantly narrowing. For a sampling of the journals producing top papers, bioinformatics journals tended to have higher Gini coefficients, suggesting that development of novel bioinformatics resources may be somewhat 'hit or miss'. That is, relative to other fields, bioinformatics produces some programs that are extremely widely adopted and cited, yet there are fewer of intermediate success.
BackgroundComputational methods have long been important for data analysis and interpretation in biomedicine, rising steadily in their use over time and prevalence in most disciplines (). The birth of bioinformatics, as a field, is difficult to trace to a specific date, but rather it began to emerge as soon as early computers did in the 1950s () and grew proportionally as access to computational technology increased. In the early days, it did not make much sense to think of bioinformatics as its own field, as computers were just tools for the analysis, organization and sharing of data. Often, whomever was most comfortable in a lab with using computers became an early bioinformatician by default. But as macromolecular data (e.g. DNA, protein) grew in both amount and diversity, and patterns began to emerge in terms of evolution by recombination and mutation rates (), as well as the effects that mutations might have on the 3D structures of proteins, it became increasingly apparent that a specialized combination of skills would be required to advance understanding. Such data was rapidly becoming impossible to analyze without computational methods. Although formal training was not necessary, one would need to understand both biology and computer scienceneither one alone would be sufficient, which is still true today. For example, although sequence alignments could be (and were) done manually, algorithmic methods () would rapidly become indispensable, particularly as more data became publicly available. As strange as it may sound to the modern bioinformaticist, new sequence data was published in print form as late as 1984 (). In the absence of specialists to turn to, geneticists and biologists would learn what computational methods they needed to continue to advance their understanding of molecular biology. As the types of algorithmic approaches used became more applicable to different fields, such as finding common substrings () or phylogenetic distance calculation (), which could be used for analysis of macromolecules or text, for example, there began an increased interest in understanding how computational methods could advance our understanding of biology and medicine in ways that had not yet been anticipated. For example, could protein-coding regions be predicted solely from genomic sequence data? Because research into such methods would often be somewhat of tangential interest to biomedical and computer science journals alike, specialized journals would be needed. In 1985, Computer Applications in the Biosciences (CABIOS), the predecessor to the journal Bioinformatics, was one of the first to focus on this new area (). The advantage to publishing in a cross-disciplinary journal is that it enables both recognition and reporting of novel research that might otherwise be rejected from either parent field alone. The disadvantage is that such papers often receive less interest from either of their parent fields alone. The impact of some of bioinformatics 'superstar' papers (i.e. those that receive the highest number of total citations among all papers published within a period of at least a year) has been anecdotally discussed before, such as BLAST () and SHELX (), but the goal of this report was to examine how often papers reporting the development of novel bioinformatics methods or software achieve a disproportionately high impact ('superstar' status) among all papers published in science during the same year, and whether or not the potential impact was evident at the time. The assumption made for the latter estimate is that authors generally try to publish in higher impact journals whenever possible, although other considerations such as journal suitability will also factor into their decisions. If superstar papers in a specific field tend to be disproportionately published in journals with lower journal impact factors (JIF), then it is reasonable to hypothesize that the editors, reviewers and/or possibly the authors did not foresee the impact the paper would have on science. Another motivation for this study is that recent studies have highlighted problems bioinformaticians face in being formally recognized. For example, bioinformaticians often are hired or funded to play more of a collaborative role within the research of other scientists, which creates challenges in terms of career advancement because traditional metrics tend to reward the primary drivers of projects (). Bioinformaticists are often middle authors on papers, and prior studies have shown that, as the number of authors on the byline grows, the perceived contributions of the middle authors drops rapidly (). Also, another study found that while two-thirds of the biology papers they analyzed mentioned the use of software, less than half actually cited it (). Similarly, another study found that very few of the users of ArrayExpress properly cited it () suggesting that the impact of bioinformatics software may be underestimated by citation metrics.Given that key authorship positions on papers are valued disproportionately and that JIF is commonly used as a metric to evaluate the relative importance or impact of one's work, whether the actual impact of primary bioinformatics research correlates with its perceived impact at the time of publication is important to know. Or in other words, are important bioinformatics solutions generally recognized to the same degree that important biological findings are when they are first sent for publication? To answer this question, citations are used as a proxy for importance, largely because they are quantitative, non-controversial and readily obtainable. Since the degree of importance can only be objectively estimated in retrospect, historical data is used. And, because the concept of citation-based 'importance' is relative (i.e. the number of citations to a paper is only interpretable within the context of a reference group), all papers published during a given year were used as the reference point because they have had approximately the same amount of time to accrue citations. Finally, for a time, the number of popular references to 'bioinformatics' was rapidly on the rise, as the potential of the field was being recognized, but such references have recently been on the decline (). One might suggest perhaps bioinformatics did not live up to the initial hype, but there is usually 'term fatigue' associated with new words and also as bioinformatics approaches become ubiquitous, it may be merging back into the parent fields from which it was born. Nevertheless, this trend change suggests it is an appropriate time to examine the question: How much of an impact has bioinformatics had upon science?
Methods
Journal impact factors and citationsStatistics on citations were downloaded from the Institute for Scientific Information (ISI)'s Web Of Science (http://apps.webof knowledge.com/) using their web interface on March 21, 2016. Review papers were excluded and results were sorted by the number of citations found in the Web of Science Core Collection, The default number of citations provided by ISI includes citations from datasets (from their Data Citation Index), which sometimes is highly disproportionate to their influence as measured by literature-based citations. For example, PMID 23470992 only has 24 citations from papers, but 8697 from deposited datasets, making it the most cited paper of 2013 by that metric. To calculate journal impact factor (JIF), 2013 5-year impact factors were used (the average number of times articles from a journal published in the preceding five years were cited in 2013). The 5-year JIF was used because it should fluctuate less than the regular 2-year JIF and be more reflective of long-term trends. Although JIFs change over time, the general trend is towards a gradual increase in all JIFs (), so to control for this, the JIF was pegged to a fixed reference point (2013), which is similar to the approach used in economics to control for the changing value of fiat currencies. All but 5 journals were mapped to 2013 5-year impact factors. Journals that underwent name changes (e.g. CABIOS became Bioinformatics) were mapped to the current name. For journals that had less than 5 years of impact factor data as of 2013 (not common), the average of existing years was used. Null values were used for the unmapped journals in the statistical calculations. To estimate the prevalence of bioinformatics papers within MEDLINE using the same criteria as used for the analysis of the top 20 most cited papers, queries were constructed using Medical SubHeadings (MeSH). First, all queries were restricted to the years in question 19942013, reviews were excluded, and only papers with abstracts were included (to screen out news stories and editorials). MeSH terms are assigned to each abstract by curators, who also annotate the 'major topics' of a paper. The major topics chosen to search for bioinformatics papers were 'algorithms', 'software', 'bioinformatics' (synonymous with 'Computational Biology' in MeSH) and 'databases, factual'. On the most stringent setting, there would be no expansion of the major topics into their subtopics (e.g. 'Software' subtopics include video games, web browsers and word processors), Less stringent would be to allow expansion to subtopics, and least stringent would be to simply look for any of these annotations and their subcategories within any of the MeSH headings assigned to a paper. To calculate the Gini coefficients, citation counts were obtained from MEDLINE for all non-review articles for each journal for the entire period of analysis (19942013) and analyzed together to enable a calculation of their general, long-term trend.
Classification of publicationsClassification of a paper as a bioinformatics paper was not always straightforward. For example, some report primarily on the software with minimal discussion on the underlying algorithms (e.g.), while others describe statistical or mathematical methods/formulas without providing software (e.g.). Papers such as the initial Gene Ontology report () were classified as a bioinformatics paper not because they provide analysis software, but because one goal of the GO was to create an accessible resource that enabled the computational analysis of scientific data. It could reasonably be argued that papers primarily about mathematical or statistical methods without software implementation, particularly those that are general and could be used in many fields, are not really bioinformatics papers. For example, papers that reported the development of general analysis methods such as support vector machines () or a method to estimate the false discovery rate (). These are methods that are widely implemented in bioinformatics software but were not designed specifically for biomedical applications. Thus, these types of papers were classified as 'methods' papers and considered both separately and as bioinformatics-related papers to see how the results were affected. Additionally, distinctions are sometimes drawn between bioinformatics and computational biology, the latter being more oriented towards computational analysis of molecular objects in three or four dimensions. In this report, the term 'bioinformatics' will take the broader definition of the term and encompass all computational methods of biological data analysis.
Analysis of the highest impact papers in scienceThe start date of 1994 was chosen because it is approximately when the Internet Age began, during a time when personal computers became increasingly affordable and web browsers enabled nonprogrammers to more easily navigate the World Wide Web. The year 2013 was chosen as the end point because, even though statistics are available for 20142015, membership in the Top 20 for more recent papers is likely to be less stable and, therefore less informative. Analysis was limited to the top 20 papers partly to keep the problem tractable, but also because the number of papers follows an exponential curve and the 20th paper, very roughly, is the approximate point where many curves begin to inflect () whereby papers on the left (most cited) end will be substantially more stable as members of the top 20 as time goes by than papers that are separated in their rank by fewer citations. The left side of this curve also represents papers that could be said to have had a disproportionate impact on science. For example, in, the top 20 papers garnered 14.2% of the citations to the top 500 papers, despite being only 4% of the total.
ResultsThe 400 top papers, 20 per year for 20 years contained a total of 113 bioinformatics papers (28.25%). When counting methods papers as bioinformatics-related papers, there were 136 (34%). In more than half the years evaluated (55%), a bioinformatics program was the most cited paper (). This is striking because the relative fraction of bioinformatics papers within the literature as a whole is much smaller. To get an approximate estimate of how many bioinformatics papers are in MEDLINE, under the same definitions as used here to evaluate the Top 20 most cited, three MeSH-based queries were constructed varying in their stringency (see methods). As a baseline, 47/50 (94%) of papers from journals with 'bioinformatics' in their name were evaluated and determined to be reporting the development of novel software or methods (the others were analysis papers). The MeSH-based queries were launched and, as a positive control, the number of papers in journals containing 'bioinformatics' in their name using the same query was evaluated. A sampling of 50 articles from each query was manually evaluated to estimate the number of false-positives (FP) returned (Supplementary). As shown in, the coverage of the least restrictive query (90%) is close to the baseline estimate of the fraction of bioinformatics papers that contain software/methods development (94%). Using this estimate, approximately 1.1% of the papers in MEDLINE report the development of bioinformatics methods or software. Thus, bioinformatics papers are approximately 26 to 31-fold over-represented among science's most cited papers, depending on whether methods papers are counted as bioinformatics. A comparison of the journal impact factors (JIF) between the annually most highly cited bioinformatics papers and nonbioinformatics paper shows that despite their disproportionate impact on science, top 20 bioinformatics papers tend to be published in journals with a significantly lower average 5-year JIF (8.0) relative to the top non-bioinformatics paper (25.8, P < 1.5  10 24 ).Similarly, bioinformatics and methods papers combined had an average JIF of 7.7 versus 25.8 (P < 4.5  10 29 ).
Examining the robustness of the resultsThe top 20 selection criteria (no reviews, no dataset-based citations) were chosen to enable a comparison of literature-based citations to primary research, but to see how the results changed, the filters were removed. Supplementaryshows the resultsthe number of bioinformatics papers with the most citations dropped from 11 to 9, and the number of bioinformatics papers within the 400 total dropped to 80 (103 if including methods papers). The JIF disparity was still significant, and bioinformatics papers were still disproportionately enriched, from 18 to 23-fold. The same data was also examined using the 2013 (2-year) JIF, but the results were not significantly changed (supplementary Tables S1 and S2).
Is the journal impact factor gap narrowing?It is reasonable to consider that, because bioinformatics was a relatively nascent field during much of the period analyzed, that perhaps it would simply take time for journals to recognize that bioinformatics approaches are consistently among the higher-impact papers each year and thus be incentivized to consider more such papers. Plotting the average top 20 JIF for bioinformatics and nonbioinformatics papers over the two decades analyzed showed the top 20 non-bioinformatics paper JIFs were increasing at a similar rate (slope  0.36) than bioinformatics papers (slope  0.40) (). Although it might be argued the gap is slowly closing, thereShown are the fraction of all known bioinformatics papers during the query period returned by each query, the number of total results, the estimated false-positive rate judged by a sampling of 50 papers returned by the query (Supplementary), the percent of all papers published during this time the query returned, and the estimated number of bioinformatics papers by subtracting out the estimated false-positives.. Average journal impact factor (JIF) over the past 20 years for bioinformatics papers (green), bioinformatics and methods papers (blue) and for non-bioinformatics papers (red). Linear trend line slopes also shown is quite a bit of year-to-year variability, making it difficult to draw strong conclusions.
What might account for the JIF-citation gap?If bioinformatics journals publish a disproportionate number of the most highly cited papers, then it's reasonable to ask why their JIFs aren't higher. Although the number of citations per paper in journals normally follows a power law distribution, one possibility might be that there is an even greater skew among bioinformatics papers. To examine this, the Gini coefficient was calculated for 12 of the journals that most frequently published a top 20 most-cited paper between 1994 and 2013. The Gini coefficient is more commonly used as a metric of income inequality, but can also be used to represent citation inequality (). If every paper published in a journal is cited equally, then the Gini coefficient would be 0. If all the citations went to one paper, it would be 1.0. Citations (see methods) were compiled for all non-review journal articles for the period 19942013 and analyzed as a whole (i.e. not by year).shows that journals focusing on bioinformatics methods tend to have higher Gini coefficients.
DiscussionThe higher Gini coefficients for bioinformatics journals suggest that development of novel bioinformatics resources may be somewhat 'hit or miss'. That is, some approaches become widely adopted and produce a disproportionate number of extremely highly cited papers, while most are not widely adopted and, at least relative to some of the other journals analyzed here, there are not as many papers that fall between the two extremes. This may also provide a potential explanation for the JIF gap: Methods developed to solve biological problems, whether novel ones or significant improvements on prior methods, may be technically sound but if it is difficult to know a priori which ones will be widely adopted or sorely needed, then it would be understandable for that to diminish enthusiasm. There is a degree of subjectivity when classifying papers strictly into 'bioinformatics' versus 'non-bioinformatics', as some cases are not clear (e.g. should a minor permutation of an existing method be counted as a new method?). But the magnitude of the differences found suggests that the main conclusions are not likely to be altered by a few differences of opinion regarding paper classification.
Alternative metrics to estimate bioinformatics impactIf JIF is not a good estimate of the relative scientific impact of a bioinformatics program, then it is worth considering alternative metrics of impact, or 'altmetrics' as they have come to be known (). There are a large number of possibilities in general (), but the primary issues would be which ones would best help normalize recognition in the identified problem areas for bioinformatics. At least three challenges have been identified in the recognition of bioinformatics-related contributions to science. The first is career advancement for bioinformaticists that make most of their contributions as collaborators (). It seems reasonable that, as long as the H-index metric is not weighted to penalize author position on the byline as has been proposed (), that this would reward collaborative ('team') efforts. However, the main point that not all author positions are equal is still valid, particularly in light of the fact that the average number of authors per paper in MEDLINE has been steadily rising (), and this would not address that. Second, for bioinformatics research that does not result in a publication, the general solution would be to quantify how useful the program was to others. Recently, for example, Depsy.org was launched to quantify the impact of the 'software that powers science' (). It combines full-text mining for mention of software packages with software archive statistics on the reuse of code and number of downloads. Third, for researchers publishing their bioinformatics solutions to solve biomedical problems, this study has highlighted the gap between the retrospective impact of highly influential papers and their initial perceived importance. One alternative metric might be to use the number of citations instead of JIF, as citations are both quantitative and already well accepted. The downside is that citations take time, whereas JIF is known immediately. Plus, the average scientist can more easily estimate the relative significance of an impact factor number than a raw citation count.shows that three of the major updates to MEGA (Molecular Evolutionary Genetics Analysis), a phylogenetics program, were the most cited paper in the year they were published. Bioinformatics, as a field, is somewhat unique in that the product of bioinformatics research can continually evolve. Yet, not unlike experimental methods papers, such as methods to analyze quantitative PCR (qPCR) data, which also appeared three times in the top 20. Authors of 'superstar' bioinformatics programs (defined loosely here as those that appear in the top 20 most cited papers of any year) may be understandably tempted to publish in journals with the highest JIF. But if they consider that they also have many other bioinformatics projects that have not reached superstar status, then they would also recognize at least two things: First, the number of citations to their program alone and the fact it was among the most cited papers for that year is a reportable accomplishment on any curriculum vitae, regardless of the journal it appeared in, and likely sets it apart from the majority of papers in any journal anyway. Second, by publishing it in a bioinformatics-related journal historically associated with other superstar papers (e.g.), they are essentiallyBMC Bioinformatics did not have any top 20 papers, but was added for additional general perspective on bioinformatics journals (bolded). Similarly, J Virol and Blood were also added as representative of long-standing 'mainstream' biomedical journals.
Investing superstar updates in bioinformatics journals
The impact of bioinformatics programsboosting the impact of all future papers they and their colleagues publish there by association. So the motives for preferring traditional bioinformatics-oriented journals as a venue to publish superstar update reports need not be purely altruistic.
ConclusionThis study highlights the disproportionate impact bioinformatics has had on science in the past two decades. The Gini coefficient analysis of citations by journal suggests a greater gap between citationrich and citation-poor bioinformatics papers, which suggests that it may be worth examining in greater depth why some of the less cited methods fared so poorlywere they simply not needed or was the right audience simply not aware of their existence? Are researchers reluctant to adopt new methods, even if better, if a solution already exists? It suggests bioinformatics, as a field is somewhat 'hit or miss' or high-risk/high-reward, relative to other fields. As researchers, bioinformaticians face challenges in being recognized for their work that are somewhat unique to their field. It's tempting to suggest a change to the system is in order, but outside of bioinformatics there seems little incentive to do so. The use of altmetrics may be one immediate solution, but that presumes that others will officially recognize and accept their use. Another solution is for authors of superstar papers to invest in bioinformatics as a field by publishing substantial updates and improvements to their programs in bioinformatics-related journals. Awareness of the underestimated impact of bioinformatics is only part of the problem, the other part is finding effective solutions.
FundingWe would like to thank the National Science Foundation (grant # ACI1345426) for support. Conflict of Interest: none declared.Only journals with at least 3 papers, 75% or more of which were bioinformatics-related are shown.
J.D.Wren at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
