Motivation: In high-throughput screens (HTS) of small molecules for activity in an in vitro assay, it is common to search for active scaffolds, with at least one example successfully confirmed as an active. The number of active scaffolds better reflects the success of the screen than the number of active molecules. Many existing algorithms for deciding which hits should be sent for confirmatory testing neglect this concern. Results: We derived a new extension of a recently proposed economic framework, diversity-oriented prioritization (DOP), that aims—by changing which hits are sent for confirmatory testing— to maximize the number of scaffolds with at least one confirmed active. In both retrospective and prospective experiments, DOP accurately predicted the number of scaffold discoveries in a batch of confirmatory experiments, improved the rate of scaffold discovery by 8–17%, and was surprisingly robust to the size of the confirmatory test batches. As an extension of our previously reported economic framework, DOP can be used to decide the optimal number of hits to send for confirmatory testing by iteratively computing the cost of discovering an additional scaffold, the marginal cost of discovery.
INTRODUCTIONAll screeners must decide which initial positives ('hits') from a high-throughput screen (HTS) to submit for confirmatory experiments (). In HTS of small molecules for biological activity, several hits are often experimentally confirmed by ensuring they exhibit the characteristic doseresponse behavior common to true actives. Sending the wrong molecules for confirmatory testing wastes resources, reducing the amount of useful information in screening results; this is especially true in the context of small molecule screens which are often very noisy. * To whom correspondence should be addressed.Most commonly used hit selection methods focus on maximizing the number of successfully confirmed molecules in subsequent confirmatory experiments. These methods include correcting time-and well position-dependent systematic error in measurements (), exploiting chemical information (), better normalizing screening data (), choosing better experimental controls () or bypassing hit selection entirely by testing all molecules in doseresponse experiments (). These methods are effective, increasing the number of successful confirmations, but largely ignore the fact that screeners are often looking to maximize the total number of clusters groups of molecules with similar structurecontaining examples with confirmed activity (). Some methods use molecular clustering to improve the design of HTS experiments. For example, molecular clusters have been used to reduce the total number of molecules in the primary screen by about two-thirds (). Similarly, other studies pick molecules for follow-up using statistical tests on the data from a single-dose screen to find clusters of active molecules (). These methods favor clusters that contain several active molecules, and attempt to send all these active molecules for confirmatory testing. As intended, these methods successfully increase the number of active molecules identified in confirmatory testing. In contrast, diversity-oriented prioritization (DOP) aims to maximize the diversity of confirmed actives by maximizing the number of clusters with at least one successfully confirmed active. Rather than picking groups of similar molecules, DOP picks molecules from as many different groups as possible given the cost constraint. This aim is motivated by the fact that screeners often cluster confirmed active molecules to pick series of molecules to optimize, and that the number of series with at least one confirmed active is a reasonable way of measuring the information obtained from a screen (). The DOP method extends a recently described economic framework for interpreting HTS data, initially introduced to decide how many hits to send for confirmatory testing (). This framework is used to iteratively choose each batch of hits to be sent for confirmatory testing so as to maximize the expected surplus of the batch. The expected surplus is computed using three mathematical models: utility, cost and predictive. The utility model specifies the preferences of the screener, the cost model tracks
S.J.Swamidass et al.the cost of running a confirmatory experiment and the predictive model guesses the outcome of future confirmatory experiments. DOP extends this framework by introducing a new utility model, from which we derive a new method of prioritizing hits. We validated the DOP methodology and the algorithm that implements it with both retrospective and prospective experiments. These experiments demonstrate that DOP can substantially increase the number of active scaffolds discovered in a HTS experiment.
DATAThe technical details of the assay and subsequent analysis can be found in PubChem (PubChem AIDs 1832 and 1833). For 300000 small molecules screened in duplicate, we defined activity as the mean of final, corrected percent inhibition. After molecules with autofluorescence and those without additional material in stock were filtered out, 1322 with activity greater than 25% were labeled 'hits' and tested for doseresponse behavior in the first batch. Of these tested molecules, 839 yielded data consistent with inhibitory activity. Each hit was considered an 'active' if the effective concentration at half maximal activity (EC50) was less than or equal to 20 M. Using this criterion, we determined 410 molecules to be active.
METHODS
Scaffold clustersThere are two common strategies used to cluster small molecules: similaritybased and scaffold-based clusterings (). Similarity-based clustering groups structurally similar moleculesas measured by fingerprint similaritytogether. Within each cluster, molecules' structures are very close, but it may not be possible to align molecules because there may not be substructures common to all the molecules in the cluster. In contrast, scaffold-based clustering groups molecules into clusters with a well-defined common substructure. Therefore, within each scaffold cluster, molecules are easily aligned (). Often, HTS campaigns aim to identify as many new scaffolds as possible. Sometimes intellectual property concerns dictate both avoiding particular scaffolds and defining discoveries using the scaffold concept. Scaffolds are often the starting points from which lead-refinement proceeds (). Therefore, we focused on scaffold-based clustering. Nonetheless, our methods can be easily adapted to similarity-based clustering. We computed scaffolds from the structure of each molecule using the molecular framework algorithm described by Bemis and Murcko (1996): contiguous ring systems and the chains that link two or more rings together. Molecular frameworks are only an approximation of a medicinal chemists subjective concept of a scaffold (). Nonetheless, frameworks are commonly used in chemical informatics because they are clearly defined and easy to compute. Although we define scaffolds as molecular frameworks, DOP is compatible with more sophisticated scaffold detection algorithms; all it requires is that molecules are grouped appropriately. In order to ensure our findings were not overly dependent on the choice of scaffold definition, all experiments in this study were replicated using a modification of each scaffold that replaces every atom in the scaffold with a carbon. The results of this variation are not presented because they are not notably different. This observation suggests that similar results would also be observed with other scaffold-detection algorithms, though we have not directly verified this.
Utility modelThe preferences of screeners are difficult to assess and often inconsistent between different experts (). Nonetheless, some parts of their preferences can be modeled. In this study, a scaffold was considered to be 'active' if at least one example of the scaffold was confirmed as active. Consistent with prior work (), one unit of discovery was defined as a single active scaffold. Of course, more robust definitions of an active scaffold are possiblefor example, defining scaffolds active if they have two or three confirmed active exampleshowever, these definitions require more complicated algorithms to implement and will be elaborated in future work. The utility model U(D) was defined as a function of the total discovery so far, D: the number of scaffolds with at least one example confirmed active, corresponding with maximizing the number of scaffolds identified, giving chemists maximally diverse candidate starting points for follow-up chemistry.
Cost modelThe cost model used is relevant to the implementation of DOP in two ways. First, in some scenarios, the cost of acquiring different molecules varies. In the context of HTS, however, molecules under consideration are usually equally accessible. Therefore, we assumed that it costs the same amount to send each molecule for confirmatory testing. Second, there are both large fixed and smaller variable costs associated with sending molecules for confirmatory tests. Under these circumstances, confirmatory tests are most efficiently performed in large batches, just as is done in practice.
Predictive modelWe considered two predictive models: a logistic regressor (LR) () and a neural network with a single hidden node (NN1) (). Both are structured to use the screen activity as the single independent variable and the result of the associated confirmatory experiment as the single dependent variable. Networks with more hidden layers work as well, but do not yield substantially better results. Both the LR and NN1 models were trained using gradient descent on the crossentropy error using the monotonic prior defined in the Appendix A (with k = 2 and  = 0.5) along with a Gaussian prior on weights not addressed by the monotonic prior (). In general, such a protocol yields models whose outputs are interpretable as a probabilities,if molecule x is confirmed active 0 if molecule x is confirmed inactive, z x if molecule x has not been testedwhere z x is the output of the predictive model on the molecule x. There is little distinction between LR and NN1 in practice and, in fact, virtually any probabilistic method can be used by the DOP method.
Prioritization algorithmThe economic framework prescribes choosing the next batch to maximize the expected surplus (ES) after the next batch is screened,where D is the number of scaffolds in the next batch, D is the number of scaffolds discovered so far, C is the cost expended confirming molecules so far and C is the cost of screening the next batch. Removing the cost terms, which are constant across all molecules, shows that maximizing the ES is equivalent to maximizing expected utility (EU),Furthermore, because U() is likely to be approximately linear over the narrow distribution of D , this equation is well approximated by,a well-studied approximation from the economics literature (). Because U(
) is monotonicallyPage: 2273 22712278
Diversity-oriented prioritizationincreasing, maximizing the EUand also maximizing ESis approximately equivalent to maximizing the expected marginal discovery (EMD)and therefore we propose prioritizing molecules by choosing the next batch of molecules so as to maximize the EMD of the batch. A different protocol is required to select the molecules in the first batch because it is impossible to compute the EMD without knowing the results of at least some confirmatory experiments. For the first batch, we chose molecules with the top activity in the screen while at the same time assuring that no more than one example of each scaffold was selected.
Computing EMDThe algorithm for computing the EMD requires the output from the predictive model and assumes that each probability of successful confirmation is independent of the others, and that within a scaffold group each molecule is screened in the order of decreasing probability of activity. With these assumptions, the EMD of the x-th molecule in the scaffold is EMD(x) = P(x-th is first active),where P(x-th is first active) is the probability molecule x is the first confirmed active within its scaffold group, and the total EMD of the next batch Eis computed as the sum of the EMDs associated with each molecule in the batch. To do this, we used an algorithm to compute P(x-th is first active). For each group of molecules with a common scaffold, we defined the probability that each molecule is active using Equation (1). The molecules in the group were sorted in decreasing order of probability so that {P(1)  P(2)  P(3)  ...}. Assuming the untested molecules must be prioritized in this order, the probability that the j-th molecule in this list is the first active is EMD(j) = P(j is first activewhere the probability that molecule j is active, P(j), is multiplied by the probability that all prior molecules in the scaffold group are inactive, j1 k=1 [1P(k)]. The EMD of the next batch was thus maximized by choosing the untested molecules with highest EMD. These EMD's remain ordered in decreasing order so that {EMD(1)  EMD(2)  EMD(3)  ...}. Within a scaffold group, therefore, untested molecules were prioritized in the same order as their initial HTS activity. Depending on how many total molecules are to be tested, the algorithm will usually pick one molecule from each scaffold but will occasionally choose more than one. With respect to molecules from other scaffold groups, however, their order may be shuffled: exactly the behavior we seek.
RESULTS
Scaffold distributionThe skewed distribution of scaffolds in HTS data motivated our approach. Ignoring this distribution, resources would be wasted trying to confirm examples of scaffolds that have already been discovered; in other words, there is significant redundancy both in HTS libraries and the hits from screens of these libraries. We considered three sets of molecules: the full set of molecules from the screen (the Library), the first batch sent for doseresponse confirmation (the Dosed) and the molecules subsequently confirmed as active (the Active). There were 301 617, 1322 and 410 molecules, and 84 440, 1043 and 331 scaffolds, respectively, in each of these sets. Each scaffold is represented by, on average, 3.57, 1.27 and 1.24 examples. The molecules are not distributed evenly amongThe 'Library' is the approximately 300 000 molecules screened in the initial HTS assay, 'Dosed' are the molecules that were sent for dose-response confirmation in the first batch, and 'Active' are those molecules subsequently confirmed as active. For each set of molecules, the top table displays the frequency of the top five most common scaffolds and the percentage of the total dataset that each scaffold group represents. The bottom table displays the number of scaffolds with exactly 1, 2, 3, 4 or 5 examples in the data (frequency) and the percentage of data that these scaffold groups represent. scaffolds; a few scaffolds are disproportionately frequent, but > 50% of the Dosed set is composed of scaffolds with only one example (). These results reflect the prior observation that molecules follow a power-law distribution when clustered (). Similar distributions would be expected if the structures had been clustered by almost any other clustering algorithm. The scaffold distribution sets an upper bound on DOP's efficiency. Of the 1322 molecules sent for doseresponse experiments, only 79% could be, in a best-case scenario, the first example of their scaffold group. Therefore, at most we could expect a 21% reduction in the number of confirmatory experiments required to discover a fixed number of scaffolds. This estimation is based on one dataset and would need to be revised upwards or downwards with other datasets. Furthermore, this estimate only considers the proportion of singletons; more accurate predictions could be constructed. Nonetheless, it provides a useful theoretical baseline against which to gauge DOP's empirical performance.
Predicting yieldOne test of the DOP algorithm is to assess whether it can accurately predict yieldthe number of scaffold discoveries in a batch of experimentsusing Equations (6) and (7) in conjunction with either of the two predictive models, LR and NN1. In this experiment, 1322 molecules with known doseresponse outcomes were ordered by their initial HTS activity. They were then divided into plates of 30 molecules each. The yield of each plate was predicted by training a predictive model (LR or NN1) on the outcome of all prior confirmatory tests, as described in the Section 3. The predicted probabilities of activity were then used in conjunction with Equations (6) and (7) to yield a final prediction. The predicted yield is close to the empirical number of discoveries ().This experiment demonstrates that the DOP algorithm can predict the number of scaffold discoveries in a plate. There is some systematic inaccuracy in these predictions; both LR or NN1 seem to overestimate the number of discoveries by a small amount. It is possible that this systematic error is due to dependencies in the data ignored by our model: for example, successfully confirmed actives are likely to share a common scaffold. Nonetheless, the predictions are close to the observed yield.
Reordering hitsAnother test of DOP is to verify that it modifies the order in which molecules are sent for confirmatory testing. Furthermore, because confirmatory experiments are usually batched in large groups, it is important to study how the DOP rankings change as the batch sizes are varied. In this experiment, we compared the order of molecules ranked by initial HTS activity with the ordering generated by the DOP algorithm. For comparison, the DOP algorithm was run six times using two different predictive models and three different batch sizes: 1 (unbatched), 30 and 300. This experiment yields several important observations. First, both NN1 and LR yielded almost identical rankings (R 2 = 0.999). This is an important result that suggests that DOP is robust to the predictive model: subtle differences in the predictive model may not dramatically affect how molecules are ordered. Second, DOP may be surprisingly robust to batch size. Using a batch size of 30 was virtually identical to the unbatched DOP (). There were more noticeable differences with unbatched DOP and DOP using a batch size of 300. Nonetheless, using DOP with a batch size of 300 yielded rankings much closer to the unbatched DOP rankings than to the original HTS rankings. Finally, in all cases, 200 molecules were not ranked because their EMDs were equal to zero before they were prioritized. This is exactly the desired behavior; testing these 200 molecules would be redundant because they have scaffolds that had already been confirmed active.
Increasing scaffold discovery rateThe most important in silico test of DOP is to verify that it increases the rate of scaffold discovery. While it is clear that DOP reorders molecules relative to the HTS activity, the rate at which scaffolds are discovered must increase in order to conclude that DOP is preferable to ordering molecules by HTS activity. In this experiment, molecules were prioritized by DOP in batches of 30. Compared with prioritizing by HTS activity, the total number of scaffolds discovered was plotted against the total number of confirmatory experiments (). DOP shifts the curve upward, indicating that for any specific number of confirmatory experiments, more scaffolds were discovered using DOP. On average, DOP discovered 1.18 more scaffolds per batch than HTS activity prioritization. DOP increased the scaffold discovery rate.
Effect of batch sizeFor DOP to be useful in practice, it must be robust to large batch sizes. We might expect a trade-off between batch size and efficiency; larger batch sizes might decrease DOP efficiency due to inaccuracies in the predictive model and uncertain outcomes in confirmatory experiments. However, the ranking data suggest that DOP may be robust to batch size. We performed a more comprehensive test to resolve these conflicting expectations. In this experiment, DOP was run with several different batch sizes using both predictive models. Compared with ordering molecules by HTS activity, the number of confirmatory experiments required to discover 50, 75 and 100% of the scaffolds was reduced by 817% (). Surprisingly, these numbers were largely consistent (and often identical) across all batch sizes and predictive models. Batch size did not appreciably affect the rate at which scaffolds are discovered; DOP is robust to batch size.
Prospective validationAlthough these retrospective experiments are promising, the most important test of DOP is a prospective experiment. In this experiment, we used a batch size of 500 molecules. DOP, using both LR and NN1, was used to pick the next 500 molecules to test. These two lists of candidates were compared with the next batch of 500 molecules selected by HTS activity. The yield of all three lists was predicted and as many molecules as were available were obtained and sent for confirmatory testing. In this case, of the 500 compounds suggested, 479 from the LR and NN1 batches and 477 from the HTS batch were sent for testing. Even without the results of the confirmatory tests, this experiment reinforced several results from the retrospective experiments. First, the batches suggested by LR and NN1 were almost identical, with only 10 molecules different, corresponding with the observation that LR and NN1 yielded almost identical rankings in the retrospective experiments. Second, both DOP batches were substantially different than the HTS batch, with 105 molecules different in the LR list and 95 different in the NN1 list, again corresponding with similar observations from the retrospective experiments. More importantly, LR predicted a 12.5% increase in scaffold discovery (126 compared with 112). Likewise, NN1 predicted a similar 8.7% increase in the rate of scaffold discovery (113 compared with 104). The results of the confirmatory testing also correspond with the results from the retrospective experiments. First, and most importantly, more scaffolds were discovered in the DOP batches; 170 and 166 scaffolds were discovered in the LR and NN1 batches, compared with 153 scaffolds in the HTS batch. Finally, the predicted yield was close to, but still underestimated, the actual Page: 2275 22712278The order of selection for confirmatory experiment given by NN1 (same as order of selection by LR). 'HTS' is the scaffolds discovered when tested in the order of their HTS results. number of scaffolds discovered: LR predicted DOP to discover 126 (actual 170) and HTS to discover 112 (actual 153). NN1 predicted DOP to discover 113 (actual 166) and HTS to discover 104. The predictions are underestimations of the actual yields. Predicted increase in scaffold discovery were more accurate. LR predicted 12.5% more scaffolds discovered in DOP than HTS (actual 11.1%). NN1 predicted 8.7% more (actual 8.5%).
Diversity-oriented prioritization
DISCUSSIONIn both retrospective and prospective experiments, DOP increased the rate of scaffold discovery from an HTS experiment. The key result of this study is that screeners preferences, when more accurately modeled, can be applied to change the order in which molecules are tested so as to increase the rate at which active scaffolds are discovered.Confirmatory tests run on the same set of molecules, sorted by DOP with different batch sizes. The table indicates the number of confirmatory experiments required before some percentage (75 or 100%) of the total active scaffolds in the set were discovered. Percentages in parenthesis indicate improvement with respect to confirmatory tests run in order of HTS results.Several additional issues arise when using DOP in practice. First, there will be several molecules that are very likely active but will not be sent for confirmatory testing. These molecules are de-prioritized because they belong to scaffold groups with at least one example already confirmed active. This is not the behavior some might expect from a prioritization algorithm. One extension to address this concern would be to report the probability of activitythe output of LR or NN1for all the molecules from scaffold groups with at least one confirmed active. A succinct way to summarize this information is to report the total number of actives and inactives expected if all the molecules in the group had been tested (). DOP could be used as the first stage of a two-stage prioritization protocol, where the first few batches are selected to maximize scaffold discovery. In the second stage, with the screener's input, a number of scaffolds could then be selected for follow-up experiments based on their structures, the initial results from the confirmatory experiments and the predicted number of actives and inactives associated with them. Additional examples of the selected scaffold would then be sent for confirmatory testing to build enough
APPENDIX A
A.1 Monotonic modelsModels like LR and NN1 estimate the correct weights by using gradient descent to maximize the log likelihood of the model with respect to the training data:where i ranges over the training data, t i {1,0} is the outcome of the confirmation experiment on the i-th training example, z i is the output of the model on the i-th training example and w is the weight vector whose elements are chosen to maximize the log likelihood L. Sometimes this process yields weights that are inaccurately estimated. Usually, the inaccuracy only subtly affects the model's yield predictions and the order in which molecules are prioritized. Occasionally, when trained on small amounts of data, this inaccuracy can be more dramatic. The worst failure occurs when a model inaccurately learns that the molecules with the worst activity in the initial screen are the most likely to be active in the confirmatory experiment. This, in turn, prioritizes the most clearly inactive molecules from the initial screen above all others. This failure can be prevented by ensuring that the model is positive monotonic: that its output always increases as its inputthe activity in the initial screenincreases. We accomplish this by adding a gamma prior to components of the weight vector. In the case of LR, the prior is placed on the weight multiplied by the input data, but not on the weight added to the product of this multiplication. In the case of NN1, the prior is placed on the weights that multiply either input or hidden nodes, but not on any of the threshold weights. Gamma priors on these weights ensure that trained models will always be positively monotonic with respect to their inputs no matter what the training data. The gamma prior is implemented by defining a probability distribution over select weights in the model,where j ranges over the components of w to which the gamma priors are applied. k and  are the position and shape parameters of the distribution, and () is the gamma function. This amounts to changing the objective of the training algorithm to maximize likelihood,Notably, the likelihood is undefined when any of the weights with a gamma prior are <0, ensuring that the weights which maximize the likelihood will always yield a monotonic model. Care must be taken to use an optimization algorithm that appropriately handles undefined output from the objective function. Alternatively, in conjunction with the gamma prior, each w j can be reparameterized to use a new set of variables v such that w j (v j ) = e v j for the weights with gamma priors and w j (v j ) = v j for the rest. Now, instead of maximizing the likelihood by adjusting each w j , the optimization algorithm adjusts each v j. The mathematical details of this strategy yield the objective,
S.J.Swamidass et al.where v is optimized by the gradient descent algorithm. Assuming that the gradient of w is computable, the gradient of v is computed using the chain rule,for the the weights with a gamma prior and Lfor the rest of the weights. Finally, the optimal w can be computed from the optimal v after training.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
evidence to establish the scaffold as a true active. Such a twostep protocol, first maximizing scaffold discovery then confirming more examples of interesting scaffolds, provides an example of how our methods could be modified to better model screeners' preferences. Better utility functions might seek to maximize the number of scaffolds with a clique of at least two or three confirmed actives. Clique-oriented prioritization (COP), however, requires a more complicated algorithm that will be presented in future work. Importantly, DOP can and should be used simultaneously with other HTS analysis algorithms. For example, other prioritization methods designed to reduce false positivesby using better controls, chemical information, or other strategiesare all compatible with DOP; the priorities generated by these methods can either be substituted for the HTS activity or be presented as an additional independent variable to the predictive model. Furthermore, DOP is a direct extension of a previously defined economic framework and can, therefore, be used to compute the marginal cost of discovery (MCD) (Swamidass et al., 2010). The MCD is the cost required to discover one more scaffold, and yields an optimal strategy for deciding how many and which molecules should be sent for confirmatory testing. The MCD is effectively the price of the next active, and the screener should keep screening molecules until the MCD rises too high and the utility of the next scaffold is not worth the cost of finding it. DOP is not without limitations. Most importantly, DOP relies on a definition of an active scaffold that may not be optimal. Furthermore, DOP requires that the results from enough confirmatory experiments are known for the predictive model to be trained. In contrast, most other HTS hit selection methods are applied to primary HTS data without knowing anything about the outcome of any confirmatory experiments (Karnachi and Brown, 2004; Varin et al., 2010; Yan et al., 2005). This limitation essentially requires HTS to proceed iteratively, with at least two batches. Future work will include methods of circumventing this limitation by using predictive models that do not require confirmatory experiments to be parameterized. Also, as presented, DOP assumes that there are no errors in the confirmatory experiment, that each molecule's potency is measured accurately in the doseresponse experiment. However, just like the primary screen, there can be substantial noise in the confirmatory experiment (Eastwood et al., 2006). One method of accounting for errors in the confirmatory experiment is by labeling molecules by their probability of having a satisfactory potency when taking the noise of the assay into account. This strategy would, likely, improve the quality of the predictive models and more comprehensively address the problem of noise in HTS projects. A detailed description and evaluation of this approach will be considered in future work. 6 CONCLUSION When screeners' preferences are modeled, they can be applied to change which hits are selected from a high-throughput screen and sent for confirmatory testing. For example, DOP relies on the observation that screeners are looking for active scaffolds more than they are looking for active molecules. In this study, both retrospective and prospective experiments demonstrate that DOP can increase the rate of scaffold discovery from 8% to 17%. DOP is robust to batch size, and can be applied even when using very large batch sizes common in HTS experiments. More accurate models of screeners' preferences may prove even more useful.
