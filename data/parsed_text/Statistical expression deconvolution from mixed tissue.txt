Motivation: Global expression patterns within cells are used for purposes ranging from the identification of disease biomarkers to basic understanding of cellular processes. Unfortunately, tissue samples used in cancer studies are usually composed of multiple cell types and the non-cancerous portions can significantly affect expression profiles. This severely limits the conclusions that can be made about the specificity of gene expression in the cell-type of interest. However, statistical analysis can be used to identify differentially expressed genes that are related to the biological question being studied. Results: We propose a statistical approach to expression deconvolution from mixed tissue samples in which the proportion of each component cell type is unknown. Our method estimates the proportion of each component in a mixed tissue sample; this estimate can be used to provide estimates of gene expression from each component. We demonstrate our technique on xenograft samples from breast cancer research and publicly available experimental datasets found in the National Center for Biotechnology Information Gene Expression Omnibus repository. Availability: R code (http://www.r-project.org/) for estimating sample proportions is freely available to non-commercial users and available at
INTRODUCTIONIn the past decade, gene expression profiling has demonstrated an amazing potential for identifying disease biomarkers and improving our understanding of cellular processes (). An issue not often discussed is that many biological samples contain mixtures of cell or tissue types (); for example, cancer cells may only constitute part of a biopsy sample. The amount of each mRNA detected in a microarray experiment is influenced by the composition of the sample; observed changes in gene expression may simply reflect a change in the distribution of the cell types in the sample population (). In breast cancernoticed that the proportion of benign tissue of biopsy samples can significantly affect expression profiles, and taking into consideration this proportion can improve response prediction. * To whom correspondence should be addressed.Sample heterogeneity severely limits the conclusions that can be made about specificity of gene expression and may explain in part why the results of numerous gene expression experiments have failed rigorous validation (). Given a heterogeneous sample there exist laboratory approaches to separate cells of distinct types. Laser capture microdissection (LCM;) is a popular technique for isolating regions of a biological sample that are separated by distances of a few cell widths. However, the cell types of interest need to be morphologically distinct. LCM, is very time-consuming and specialized equipment, is required to obtain a sufficient quantity of biological material for profiling. If the sample of interest is in suspension, cell-sorting methods can be used to isolate cells of interest. This requires a suitable biomarker for the cell type of interest. The main drawback of cell sorting with respect to profiling is that the act of separation itself can alter gene expression (). We present a method for deconvoluting expression from a heterogeneous sample into components that reflect the contributions to the observed expression attributable to each component cell or tissue type. The key component of this method is the estimation, from a mixed tissue sample, of the proportion of mRNA from a single tissue type. Estimation is based on specific logarithmic data transformations and theory from differential geometry regarding the radius of curvature (). We demonstrate our method on several datasets from breast cancer xenograft studies, from both proprietary sources and the National Center for Biotechnology Information (NCBI) Gene Expression Omnibus (GEO) repository (http://www.ncbi.nlm.nih.gov/geo/;).
APPROACHSeveral approaches have been taken to the problem of expression deconvolution and each approach depends on access to different types of information, different statistical assumptions and different objectives. If there are genes known to be expressed exclusively in one tissue type, then these genes can be used to estimate the proportion of expression coming from that tissue. For example, the program DECONVOLUTE () uses simulated annealing and genes expressed only during specific cell cycles to identify the proportion of cells in each cycle from an asynchronous cellular sample. These methods depend on known tissue-or cell-specific genes, and technology that can detect their expression with little or no cross-hybridization. If these conditions is not met, widely varying estimates of p A can be obtained by selecting different subsets  The Author(s) 2010. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited. of tissue-specific genes. Note that low specificity of microarray hybridizations has been suggested to be one of the prime measures affecting discrepancies in gene-expression profiles between different probes targeting the same region of a given transcript or between different microarray platforms (). We do not assume knowledge of cell-or tissue-specific genes in our method, although such knowledge may be available, particularly for samples from xenograft studies (where the tissues of interest are from different species). Similarly, several researchers have used expression data from purified reference tissue types to determine the expression of each tissue type in heterologous samples (). For example,use a method similar to that of, mentioned above, to determine the proportions of each cell type in a mixed sample. This method generates estimates by obtaining solutions to linear equations via simulated annealing. These approaches depend on having expression data from a purified reference sample for each cell or tissue type, which may not be available. Another approach uses proportions of each sample or cell type, assessed by pathologists, to establish either tissue-specific expression or differential expression between mixed and control samples. Inlinear regression models, regressing expression on fractional content of tumor (or stroma), were used to estimate the expected cell-type expression as the regression coefficient. A more sophisticated statistical approach was used byto determine differential expression in the presence of mixed cell populations. In his approach, a pathologist's assessments of the proportions of each cell type were used in a hierarchical mixture model to model the data. A combination of methods of moments procedures and the expectationminimization (EM) algorithm provided estimates of the model parameters. Although not shown in the publication, this method could be adapted to provide expression estimates specific to each cell type, as opposed to estimates of differential expression. Unfortunately, the assessment of a pathologist only provides the proportion of each cell or tissue type in the sample, and not an assessment of the amount of mRNA or protein attributable to each. It is well known that the total amount of mRNA generated by tumor cells, for example, is much higher than the amount generated by normal cells. As a result estimates of expression based on pathological assessments of tissue proportions may not be accurate. Finally, an approach exists to use expression data from a single cell type to determine the proportion of each cell type in a heterogeneous sample (). This method depends on the estimation of the minimum of a proportion, a minimum that provides a good estimate in noiseless or simulated data. However, this minimum is much more difficult to estimate in noisy data, and microarray data is inherently noisy. Our research builds upon this work by providing a method for estimating this minimum that has reasonable accuracy and can be applied in situations where one or multiple heterologous samples are available.
METHODSFirst, we will discuss the idea of estimating the proportion of a single cell or tissue type in a two-type mixed sample. We will then describe the role of data transformation in this estimation and the interest in finding the pointof minimum radius of curvature. Finally, we will describe the use of the bootstrap () for obtaining a standard error for our estimate.
Proportion of tumor as a minimum ratioThe idea of estimating the proportion of one type in a two-type mixed sample comes from. As they describe, let A be a purified sample of one type and AB be a mixed sample, composed of tissue or cell types A and B. Let E i (AB)(E i (A)) be expression of gene i in Sample AB(A) for i = 1,...,m. Let E(AB) ={E 1 (AB),...,E m (AB)}. We want to estimate p A , the proportion of expression in the mixed sample (Sample AB) due to tissue type A. For a given gene i we can express E i (AB) asIn the noiseless case,Note that for a fixed p A this ratio is at its minimum when E i (B) = 0, since expression is assumed to be non-negative. Hence, ifThus, under the assumption that E i (B)  0 for some sequence of i's,This can be seen inthat the values of R i are sorted from lowest to highest. Unfortunately, the minimum ratio is an underestimate of the true proportion value p A for simulated noisy data and for observed data. For example,shows observed data from a titration series (p A = 0.25,0.5 and 0.75) of breast cancer cell mRNA (MDA231) and normal mouse lung mRNA. By a titration series, we mean a set of mixed samples (breast cancer and normal lung) in which each samplehas a fixed proportion of each tissue/cell type. What we observe is expression data from each mixed sample in this series, so a total of three samples with proportions of breast cancer mRNA to normal mouse lung mRNA of {(0.25,0.75),(0.5,0.5) and (0.75,0.25)}. Hence for p A = 0.25 the observed data is expression from a mixed sample (AB) composed as 0.25 * A+0.75 * B. The 'electronic data' is the same data as shown in. The values of min i R i are very accurate estimates of p A for the 'electronic' data but are poor estimates of p A for the observed data. Clearly, the ability of min i R i to estimate p A is greatly affected by the noise in the data; understanding and incorporating the noise and its effect on min i R i in the estimation process is the key to finding an accurate estimate of p A .
Data transformationThefor some >0 and for all i. The untransformed values of R i have a skewed distribution with a long tail of large values (data not shown). As such the mean of the R i s is larger than the median. The above transformation, by decreasing large values and increasing small values, brings the mean and the median closer together. We discovered that across several datasets a value for  does exist for which min i tR i = min i tE i (AB)/tE i (A) is an accurate estimate of p A. Unfortunately, this value for  varies with each dataset and with the value of p A , i.e. within each dataset and across datasets the value of  that provides an accurate estimate of p A is different for each value of p A. For any given dataset and value of p A we could successfully model  as a function of p A , using a function of the form log( * p  A +1)/(p A 1) for some , > 0. However, this function depends on p A , the value we are trying to estimate.We acknowledge that the minimum value of tR i is sensitive to the noise in the data, particularly in relation to the mean or the median. Hence we decided to explore the possibility of using information from a summary statistic of tR i (e.g. mean or median) as a function of  to determine the correct value of , and hence the value of our estimate mintR i. The mean of tR i (tR()) as a function of  is defined aswhere m is the number of. The value of  that provides the most accurate estimate of p A is marked by a vertical black line. The value of  that provides the most accurate estimate of p A , in these plots and many others, is located at what one may refer to as the 'knee' or 'elbow' of the curve. This point may be familiar from principal components analysis as the point on a scree plot that indicates the number of significant principal components (). To calculate this point, we need a mathematical definition for the 'elbow' of a curve.
Minimum radius of curvatureWe want to find the value of  at the 'elbow' of the curve defined by tR as a function of . The 'elbow' of a curve is the point at which the tradeoff between pulling low values up and pulling high values down (values of R i ()) is optimal. Here, we formalize this by choosing that point at which the radius of curvature is at its minimum. The radius of curvature (s) is defined as the inverse of the vector norm of the second derivative of the curve, expressed as a function of arc length s, i.e.where C is the curve of interest originally parameterized in terms of x (). Thus, to find the value of  of interest several steps are required. First, we need to represent the function tR() as a curve in the plane. Second, we must reparameterize this curve in terms of the arc length s. Third, we use the reparameterized curve to determine the value of arc length s * that minimizes the radius of curvature (s). Finally, we determine the value of  that corresponds to s * .
Radius of curvature in terms of arc lengthRecall that a parameterized curve in the plane is of the formwhere x 1 ,x 2 are the coordinate functions, e 1 = (1,0) and e 2 = (0,1) the natural basis and  the parameter of the curve. To define the radius of curvature of x() at a point x, we first reparameterize in terms of arc length s
. The arcPage: 1046 10431049
J.Clarke et al.length parameterization is defined to be the parameterization with unit speed along the curve. This eliminates the possibility of an unnaturally high or low radius of curvature simply due to the local speed of transversal of the curve. The arc length s of a curve is defined aswhere |||| is the Euclidean norm. Now consider a function f () and observe that its graph (,f ()) is a geometric curve in the plane. Thus, as in Equation(2), we can writeSo the arc length parameter [] is given in terms of  bySince the parameterization is in terms of unit speed, it is invertible, so we can write  = (s) as well. Thus, s = s( ) and s = s( ). The radius of curvature of a geometric curve C as stated in Equation (1) can now be defined asfor the curve C = (,tR()). We will argue that choosing  to minimize (s) leads to a good estimate of p A over.
ImplementationGiven the definitions in the last subsection, it remains to obtain the arc length parameterization for the curve (,tR()) and find the value of  that corresponds to s * . Replacing f () with tR() we have the following: Partition the intervalwhere we approximate tR( i ) asThis will yield a one-to-one relationship between  and s, hence a oneto-one relationship between s and tR(). Once we have this we can find the value of s that minimizes the radius of curvature ((s)), i.e. maximizes f (x((s)))
Determining s * and To find the maximum of f (x((s))) = (tR((s))) 2 =|tR((s)) | we find the value of s, s * , which maximizes the absolute value of the second derivative with respect to s using centered difference approximations ()Using this approximation, we calculate tR((s)) over a range of valuesand determine the value s * that minimizes tR((s)) .Note that this method for finding s * (and subsequently ) only works if the two axes of the plot for tR i , are similarly scaled. If the two scales are not equal, they must be equalized prior to calculating s * by rescaling one axis to be the same length as the other. For example, to rescale the axis for tR we would use values of the following in place of tR (max()min()) * (tRmin(tR)) max(tR)min(tR) . That is, the range of the function tR is the same as the range of the parameter . This 'scaling', like the arc length parameterization, seems necessary to prevent arbitrary choices from dominating the solution. One key task is choosing k large enough so that the approximation of the second derivative with respect to second differences is accurate over the range [ 0 , j ]. We found that k of several thousand worked well in the examples in Section 4.
Bootstrap estimates of standard errorWe used a simple bootstrap resampling procedure () to generate standard errors for our estimate of p A. For a given dataset of n observations and m genes, we draw T bootstrap samples; each sample contains expression values of m genes drawn at random with replacement where m  0.6 * m (so a total of nm values). From each sample j, j = 1,...,T , we calculate the mean of tR i (tR()) across values of  in a given range. We then determine the value of  that corresponds to the minimum radius of curvature (s * ) of tR(), plotted as a function of  (as described in Section 3.3). This value of  is used to generate tR i for the genes in sample j and determine its minimum, i.e. our estimate of p A. The result of our bootstrap procedure is T estimates of, one for each sample. The SD of these estimates is taken as the standard error of our estimate of p A , and a (1) confidence interval for our estimate is calculated asare the (/2)th and (1(/2))th percentiles of our 100 estimates of p A. Stated as psuedo-code for clarity, our procedure is as follows:(1) Generate T bootstrap samples where each sample contains nm expression values, i.e. expression values for m genes from each sample. The m genes, m  0.6 * m, are selected at random and with replacement.(2) For each sample, calculate the values of tR i , i = 1,...,m , for a range of values of .(3) For each sample, calculate the values on the curve (,tR) for a range of values of , using the result of step 2.(4) For each sample, use the curve calculated in Step 3 to determine the value of  that corresponds to the minimum radius of curvature s * (as described in Section 3.3). Label this value as  j for each sample j,j = 1,...,T .(5) For each sample j, use the values of tR i that correspond to  j (as calculated inStep 2) and determine its minimum, i.e. our estimate of(6) Calculate the standard error of our estimate (as the SDare the (/2)th and (1(/2))th percentiles of our 100 estimates of p A ). As a sort of stability analysis, we chose a range of values for T in our computations below to see whether there was any obvious relationship between the size of T and the likelihood that a bootstrapped confidence interval contained the true value. The results insuggest that the size of T and the accuracy of the bootstrap intervals is slight at most.
RESULTSWe implemented our procedure for estimating p A in several gene expression datasets, both proprietary and public, in which expression data was generated from samples composed of two tissue/cell types. Some of the samples consist of different cell types from the same Page: 1047 10431049organism, while other samples are a mix of cell types from different organisms. The proportion of each component type is known, as the data come from titration series; we use these values to assess the accuracy of our estimates.
Statistical expression deconvolution
DataOur data consists of six datasets obtained either from the University of Miami School of Medicine (UMiami) or the NCBI GEO (). The UMiami datasets were created as a titration series of RNA from breast cancer cells (either MDA231 or MCF7) and normal mouse lung cells. The expression platform is Illumina Human WG-6 version 2 (MCF7) or version 3 (MDA231) (); chips were processed at two different laboratories. The data from GEO includes titration series of Universal Human Reference RNA and Human Brain RNA from the MAQC study (). We selected data processed at two different laboratories and on two different platforms, either Human6 BeadChip 48K version 1 () or HG-U133 Plus 2.0 GeneChip (). Two other datasets from GEO were also included in our studies; these data include two titration series of mouse T and B cells (). These sets were processed on the Mouse 430A version 2 GeneChip platform (). The details of each dataset are presented in. The method of normalization of gene expression data can impact substantially which probes are identified as detected and which probes are identified as differentially expressed between conditions (). For this reason, we implemented several normalization methods on our proprietary datasets, while using the available normalized data for the publicly available datasets. The normalization methods for the Illumina data include quantile normalization and qspline normalization as implemented in the R package beadarray () and cubic normalization as implemented in the Illumina BeadStudio software (). After normalization, only those genes with a detection P-value <0.01 in all samples () or considered present in all samples according to the Affymetrix MAS5 algorithm () were included in further
Accuracy of estimationSelect results of our bootstrap estimation procedure for each dataset are shown in. For the UMiami datasets, we chose to display results for only one normalization method for brevity. In 90% of cases, our point estimate is within 5% of the true proportion; in 80% of cases, the 90% bootstrap confidence interval for our estimate contains the true value of p A. We note that our method found the BIIB 100 dataset to be the most challenging. This is no surprise as this titration series was designed with very low levels of mRNA, as a challenge to the procedure used for RNA amplification prior to running the expression assay (). In other words, this data was generated from a very small amount of biological material so the estimation of the proportion of the biological components is very challenging. There is evidence inof an interaction between the normalization procedure and the accuracy of our estimation method. For example, we tend to overestimate p A when the data is qspline normalized, as with the UMiami MDA231 data, but we tend to underestimate p A when the data is quantile normalized. We note that this relationship could also be a consequence of other experimental variables, such as the expression platform or the specific laboratory in which the data were generated. Further, datasets and analysis are required to determine which factors (e.g. normalization, platform and laboratory) have significant effects on the accuracy of our procedure. In addition, the stated confidence level of the confidence intervals (90%) is predicated on the validity of the underlying model (). Because our underlying model has some level of uncertainty, the stated level of confidence is an overestimate of the actual level of confidence. In other words, model uncertainty Page: 1048 10431049tells us that a true 90% confidence interval is larger than the stated 90% confidence interval. In light of this the accuracy of our method is most likely better than the results inwould suggest. An accurate estimate of p A can be used to generate estimates of expression specific to each tissue/cell type. Given expression from a mixed sample AB and an estimate of p A we can estimate E(A) and
J.Clarke et al.As we observe E(A), we can compare E(A) with the observed E(A) to assess the quality of our estimate. Whether the error in using E(A) as an estimate of E(A) can be used to improve our estimate of p A is a topic for future research.
DISCUSSION AND CONCLUSIONSWe have demonstrated a statistical method for estimating the proportions of each sample (Samples A and B) in a two-sample mixture (AB). This method requires expression data generated from the mixed sample AB and expression data generated from a purified sample of one type A. Given this information, the method approximates the proportion p A as the minimum of the ratios of expression in the mixed and purified samples, where the minimum is taken over genes. For this estimate to be accurate, it is required that the data be transformed; the value of the parameter of the transformation is determined by a geometric argument involving the minimum radius of curvature of a function, parameterized as a curve in the plane. Our results show that our method provides a reasonably accurate estimate of p A on both proprietary and publicly available datasets. As demonstrated ina large value of p A (say, over 0.5) can have a substantial effect on the results of tests for differential expression and confound tumor classification. However, whether a large p A should be cause for concern depends on the specific study. We would argue that p A should be assessed in all samples, but the action of the investigator in response to a large value of p A may vary from no action to discarding the sample from further consideration. In the case where p A is very large, our method will still give a reasonable estimate of E(B)( E(B)) but the variability in this estimate could be large. Whether a large p A necessitates a renormalization of the data is unknown; we conjecture that if E(A) and E(A) are comparable then renormalization is unnecessary. The results presented are preliminary and as such further research is required to optimize and validate our method. Our bootstrap point estimates and confidence intervals could be substantially improved by increasing the number of bootstrap samples T and running diagnostics to ensure that the number of samples and size of samples are adequate for generating valid bootstrap quantities of interest (). In addition, we would like to explore the relationship between the method of normalization and our estimation technique. By altering the noise distribution, normalization alters the relationship between the noise and the values of R i , thereby influencing the accuracy of minR i as an estimate of p A. The extent of this influence is unknown, but further research may help determine which normalization method yields the most accurate estimate of p A. Finally, the calculation of the radius of curvature depends on the estimation of the second derivative of the curve; we approximate the second derivative by the second difference equation [. This approximation is accurate if the curve is smooth and is well sampled, i.e. the distance between s k and s k+1 is small. Using a well-sampled curve in our method can be computationally expensive if the range of value of  (i.e. values of s) is large. We would like to design a variation of our method which starts with a sparsely sampled curve over a large range of values of  and iteratively narrows the range of interest and increases the sampling density as information about the probable location of s * is obtained. This should yield a better estimate of p A at lower computational expense. We hope to implement this variation and provide our approach to the statistics community as an R package (R Development Core Team, 2009). Our definition of the 'elbow' of a curve as the point of minimum radius of curvature is applicable to other problems in statistics, such as the choice of the number of principal components in a principal components analysis (). One existing way to make this choice is to identify the 'elbow' of the curve from a scree plot and choose the number of components closest to the 'elbow'. Our procedure for finding the minimum radius of curvature, coupled with a curve-fitting method, may be directly applicable to this problem. This would provide a formalization, in the spirit of Zhu and Ghodsi (2006), of what is currently an ad hoc approach.
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
