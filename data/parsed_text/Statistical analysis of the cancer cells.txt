Motivation: As cancer progresses, DNA copy number aberrations accumulate and the genomic entropy (chromosomal disorganization) increases. For this surge to have any oncogenetic effect, it should (to some extent) be reflected at other molecular levels of the cancer cell, in particular that of the transcriptome. Such a coincidence of cancer progression and the propagation of an entropy increase through the molecular levels of the cancer cell would enhance the understanding of cancer evolution. Results: A statistical argument reveals that (under some assumptions) an entropy increase in one random variable (DNA copy number) leads to an entropy increase in another (gene expression). Statistical methodology is provided to investigate the relation between the genomic and transcriptomic entropy using high-throughput data. Analyses of multiple high-throughput datasets using this methodology show a close, concordant relation among the genomic and transcriptomic entropy. Hence, as cancer evolves, and the genomic entropy increases, the transcriptomic entropy is also expected to surge.
INTRODUCTIONDNA copy number aberrations are one of the key characteristics of cancer (). In fact, the accumulation of DNA copy number aberrations is the most consistent feature of cancer progression (). The entropy at the genomic level (chromosomal disorganization) of cancer cells thus exceeds that of healthy cells and tends to surge as cancer progresses. DNA copy number aberrations affect mRNA expression levels as the central dogma of molecular biology suggests and numerous high-throughput studies have shown (e.g.). Aberrations need not only affect the expression of its driver gene, but may also alter expression levels of the other genes that map to the aberrated segment (). In fact, genomic aberrations also affect expression levels of other transcripts like microRNAs (e.g.). The close relation between the genome * To whom correspondence should be addressed. and transcriptome suggests that the entropy increase spreads to other molecular levels of the cell's regulatory system, and is expected to manifest itself most prominently in the transcriptome. Indeed, for this surge in genomic entropy to have any phenotypic (oncogenetic) effect, it needs (to some extent) to propagate to the transcriptomic level and beyond. Cancer may be considered an evolutionary process, driven by random variation and natural selection (). During its life a cell may undergo heritable genetic alterations (e.g. DNA copy number aberrations). Such alterations may be neutral but may also affect the cell's phenotype. Irrespective of the type of alteration, any cell is subject to natural selection: it has to survive in the environment of the organism's tissue. Within this framework, a cancer cell can be thought of as having acquired alterations that resulted in beneficial traits to survive, proliferate and metastasize (). Evolution explores different paths via random variation, and the path that leads to a faster entropy increase is naturally selected (). As cancer evolves/progresses, the entropy at the genomic level increases (). Here we investigate, using high-throughput studies, whether this is reflected at the transcriptomic level (as suggested by). If so, this may shed light on the path of evolution of the cancer cell. Before we facilitate the study of the propagation of increased entropy of genome to transcriptome in the cancer cell, we first provide a statistical argument that suggests this indeed seems plausible. We then present statistical methodology to analyze highthroughput genomic experiments in order to answer the following related questions: @BULLET Is the entropy of a cancer sample's transcriptome higher than that of a normal sample?@BULLET Is a cancer sample's genomic entropy associated to that of its transcriptome?These questions are portrayed schematically in. We illustrate how the discussed methodology may be utilized by application to multiple datasets.
METHODS
MotivationWe provide statistical motivations for the hypothesis that an increase of the entropy at the genomic level leads to an increase of the entropy at the):For our first motivation, we assume that DNA copy number and gene expression are both measured without error and their relation at any loci may be described by a strict monotone increasing function g():In addition, we note that, according to Theorem 1 of Ramsay (1998), any smooth monotone function g(x) may be represented as:where c 0 and c 1 are constants and w is a coefficient function that isTo show that H(g(), we first note that the entropies of X 1 and g(X 1 ) are linked via:It thus suffices to show that:In case g() is a linear map, g(x) = ax, this is immediate. To prove the desired inequality for other choices of g(), we note that, appealing to Representation (1), the original inequality follows if we prove:Integration by parts yields:the expression levels of two genes that map to these loci. Furthermore, assume that the relation between DNA copy number changes and gene expression at both loci may be described by Y = X + with X and  independent, E(Y |X) = X, and   0 to reflect the empirical observation that a(n) increase/decrease in DNA copy number leads to a(n) increase/decrease in gene expression. Theorem 1 then tells us that if locus 2 is more prone to be aberrated at the genomic level than locus 1 (statistically operationalized as X 1  disp X 2 ), this is reflected in the transcriptome (under the assumption of a simple linear model) and locus 2 will exhibit more abnormal expression levels than locus 1. Corollary 1 is formulated in terms of the dispersive ordering, whereas our interest is in the entropy ordering. The relevance of Theorem 1 stems from the fact that dispersive ordering implies entropy ordering, i.e. X 1  disp X 2  H(X 1 )  H(X 2 ) (). Corollary 1 is illustrated for the entropy ordering by two examples. In the first, let),  1  N (0, 2  ) and 2  N (0, 2  ), all are independent. Then:, this difference is non-negative. In the second example, letCauchy(0,  ) and  2  Cauchy(0,  ), all are independent. Then, using a result from Blyth (1986):, this difference is non-negative.
ExperimentsTwo types of experiments are considered. The first (referred to as experiment of Type I) involves n C samples from cancerous tissue of a particular same type. For all samples, both a DNA copy number and a gene expression profile are assumed to be available. The random variables X ij and Y ij represent the DNA copy number and the expression level of gene j, j = 1,...,p, of sample i, i = 1,...,n C , respectively. Together the DNA copy number profiles of all samples make up X = (X ij ) i=1,...,n C ;j=1,...,p , the np genomic aberration matrix, which we assume to contain no missing values. The gene expression matrix Y is defined similarly, also without missing values. The DNA copy number and gene expressions profiles of sample i are thus the i-th rows of matrices X and Y, respectively, and will be denoted X i and Y i. The second type of experiment (Type II) involves n = n N +n C samples, of which n N originate from normal, healthy tissue and n C from cancerous tissue of the same type. For all samples, only a gene expression profile is available. The gene expression data of experiments of Type II is denoted as above by Y = (Y ij ) i=1,...,n;j=1,...,p .
EntropyIn the present context, entropy measures the diversity (at the molecular level) of the samples under study. Here diversity (entropy) at the transcriptomic Page: 558 556563
W.N.van Wieringen and A.W.van der Vaartlevel is compared between normal and cancer samples. At the genomic level, the entropy of normal samples reaches a minimum (the DNA copy number of its autosomale genome equals two), whereas that of cancer samples can be anything as DNA copy number aberrations may be abound. From Section 2.1 we thus expect that this propagates to the expression levels, resulting in a higher entropy of the cancer transcriptome. To test this, we use the entropy difference between the normal and cancer samples as test statistic, and generate its null distribution by resampling. In order to calculate the test statistic, we first point out how the entropy of a set of samples may be calculated from high-dimensional data (p > n), then discuss the test. The entropy of a multivariate normally distributed random variable, Y T i  N (,), is given by 1 2 log+ 1 2 p. For low-dimensional situations (p < n) the determinant of , and thus the entropy of Y i , is straightforwardly obtained by estimating by S = 1 n (YY Y) T (YY Y),, and computing the product of its eigenvalues. However, in case p > n the estimate S is singular and consequently det(S) = 0. This problem can be overcome when using a shrunken estimate of (). The shrunken estimate of the covariance matrix is given by:  = (1)S+T, where S as before, T a diagonal matrix with diag(T) = diag(S) and the optimal (in some sense)  equals [are the eigenvalues ofSof ofS = T 1/2 ST 1/2. The non-zero s coincide with the square of the singular values of (YY Y)T 1/2 /  n. Hence, we are able to estimate the entropy when p > n. To contrast the multivariate normality-based entropy, we also use a nonparametric motivated entropy estimate (). Hereto we need the k-th nearest neighbor probability density estimate of f () given by:the volume of the unit ball in R p , and d k (Y i ) the Euclidean distance between Y i and its k nearest neighbor. The entropy is then estimated by:the average entropy of the observations (as determined within the sample). To test for a difference in entropy, we use the following test statistics:are the estimated entropy in cancer and normal sample, respectively. To obtain the null distribution of this test statistic, we permute 1 the sample labels and recalculate the test statistic. This process is repeated L times. The significance level of the tests is now calculated by {#|T obs  T for = 1,...,L}/L, the proportion of the null distribution that exceeds the observed test statistic.
Mutual informationTo investigate the genomictranscriptomic entropy association, we use the concept of mutual information, a general measure of dependence between two random variables. Here, it measures the amount of information shared by genome and transcriptome. Or, loosely, how much knowledge of the genomic entropy tells us about the transcriptomic entropy. Formally, mutual information is defined as ():dydx.Mutual information and entropy are related via I(is the conditional entropy of Y given X and H(Y ,X) the joint entropy of Y and X. Hence, by studying the mutual information between Y and X, we compare the unconditional entropy of the gene expression to its conditional counterpart, conditional on DNA copy number. In case, gene expression is independent of DNA copy number: H(Y|X) = H(Y) and I(Y;X) = 0. Mutual information can thus be used to test whether the transcriptomic entropy is associated with the genomic entropy, using resampling to assess whether mutual information deviates significantly from zero. We describe how to estimate I(Y;X) for the distributions considered in Section 2.3. In case of multivariate normality,the mutual information in the high-dimensional setting of p > n is estimated by:where  Z the shrinkage parameter of shrunken estimate of ZZ ,  Z j the eigenvalues of T 1/2diagonal matrix with diag(T ZZ ) = diag(S ZZ ), and  Y j and  X j defined similarly. Again to contrast the multivariate normality assumption, we also estimate the mutual information under the assumption of the k-th nearest neighbor distribution. We follow the approach described in. The joint entropy, the entropy of Z = (X,Y) T , can be estimated using (3). To obtain the mutual information we need to subtract if from the entropies of X and Y, using the same k. However, this results in a biased mutual information estimate (). To resolve thispropose, in the estimation of the marginal entropies, to fix the nearest neighbor distance instead of k. Let d k (Z i ) be the distance (with respect to the uniform norm) of Z i to its k-th nearest neighbor, and k x (i) and k y (i) the number of samples that fall marginally in the balls B(). Now, following, the marginal entropies are estimated, by, e.g.:where  is the digamma function and k Z refers to the k chosen for Z. The mutual information is then estimated byHby byH kzwhere last term of entropy estimates (5) cancel out as the same nearest neighbor distances are used in the calculation of joint and marginal entropies. Estimator (6) is less biased than a mutual information estimate using the same k for the estimation of H(X),H(Y) and H(Z) (). Note that the estimator). Under the null hypothesis, there is no association between copy number and expression, consequently the permutation only yields the random behavior of the test statistic. For each permutation, the test statistic is recalculated. After L permutations, the P-value is calculated as in Section 2.3.
SIMULATIONHere we report a simulation study that serves three ends: (i) to make an informed choice on the nearest neighbor parameter k for entropy and mutual information estimation, (ii) to investigate the behavior of the entropy and mutual information estimators under increased high dimensionality and (iii) to study the relation between the two entropy estimators (similarly for the two mutual information estimators). The simulation study involves artificial data, which facilitates insight on all three ends, as they are sampled from a known distribution with known entropy and mutual information. The setup is described next. Artificial datasets involve n = 20 samples from a p-variate, p = n with  varying from 0.5 to 50, normal distribution N (0,). Prior to the generation of each dataset, the covariance matrix is itself randomly drawn from an inverse Wishart distributionthe sampled covariance structure mimics that found in real datasets. For each , multiple datasets are drawn, for which the entropy is estimated (assuming normality and k-NN) and the true entropy using the corresponding drawn is calculated. For the investigation of mutual information datasets with twice the number of genes are drawn, which is randomly split in two equal sized parts. In the estimation of the k-nearest neighbor entropy and mutual information k = n , where  ranges from 0.05 to 0.95. The simulation results are first used to find an optimal  for entropy and mutual information estimation (when assuming k-NN). Hereto the correlation betweenHbetween betweenH knn and H true and betweenIbetween betweenI knn and I true are calculated for all combinations of  and . Ideally, these correlations are high, and an optimal choice of  results in the highest correlations over the whole range of . The correlations are displayed as a contour plot in. From, it is clear that both entropy and mutual information estimation are served best by choosing a small  (irrespective of the high-dimensionality parameter ). To investigate the effect of increased high dimensionality on entropy and mutual information, we again calculate the correlations betweenHbetween betweenH and H true and betweenIbetween betweenI and I true (assuming both normality and k-NN), which are denoted  normH. These correlations are plotted against  (plots not shown, but can be deduced fromfor the k-NN assumption). This reveals that the correlations at first decay rapidly as  is increased, but thatitself is not surprising, but the leveling off is encouraging, as the proposed estimators, although noisy, do provide information on the quantities of interest. Finally, we investigate whether the estimates of the different operationalizations of entropy and mutual information induced by the different distributional assumptions yield concordant results. We now correlat H norm withHwith withH knn andIand andI norm withIwith withI knn. For the simulated data, Spearman's correlation between the entropy estimates is high (varying from 0.80 to 0.95, with highest values assumed for small k) and between the mutual information estimates is substantial (varying from 0.40 to 0.75, with highest values assumed for small k). The concordance of the results from both entropy and mutual information operationalizations indicates that they indeed measure the same quantity. By lack of information on the true entropy and mutual information, which is the case when analyzing real data, corroboration between the two operationalizations enhances the confidence in the results.
APPLICATION TO CANCER DATAThe aforementioned methodology is applied to publicly available datasets representing both experiment types described in Section 2.2. The first five columns ofgive an overview of the datasets analyzed. Details on array platforms, preprocessing, etc. are provided in the Supplementary Material. Here we only point out that we have used normalized instead of segmented or called copy number data (refer to, for details on the differences between these data) in the analysis of Type I experiments, as beforehand the multivariate normality assumption is unlikely to hold for the latter two. See Section 4.4 for a discussion on normalization.
Analyses of Type II experimentsWe analyzed the Type II experiments listed inthis figure, it is clear that the cancer samples are much more spread out, indeed indicating a higher entropy. The Kim and Mougeot datasets are of particular interest as they comprise, next to the analyzed normal and cancer samples, samples from other cancer stages. The former also yields samples from an intermediate and a more advanced stage: 'pin' (13 samples) and 'metastatic' (20 samples), whereas the latter contains two intermediate categories: 'benign' (18 samples) and 'borderline malignant' (3 samples). This allows further investigation of the transcriptomic entropy increase with advanced cancer stage. All possible stage pairs are compared, testing for larger entropy in the more advanced cancer stage. Tables 2 and 3 contain the results. In the Mougeot dataset, not all entropy comparisons are significant, which is (to a large extent) due to the low sample size of the 'borderline malignant' group (consisting of only three samples). Nonetheless, the general picture that emerges from the pairwise analyses in both datasets is that the entropy of a cancer stage exceeds that of preceding stages.The entropy surge may be interpreted as an increased diversity among the cancer samples. This may suggest that cancers develop along many different routes, though all leading to unproliferated growth. Consequently, it may prove difficult to develop one therapy that will benefit all individuals with the same cancer. A large (genomic and transcriptomic) diversity increases chances of individuals being and becoming resistant against the therapy, as has been observed in many cases.
W.N.van Wieringen and A.W.van der Vaart
Page: 561 556563
Analysis of the cancer cell's molecular entropy
Analyses of Type I experimentsFor all Type I experiments listed in, the mutual information I(Y;X) between gene expression and DNA copy number is calculated. To evaluate whether I(Y;X) deviates significantly from zero, a permutation test as described in Section 2.4 with L = 1000 permutations is conducted. The results are presented in. Due to very different (and in these high-dimensional situations hard to verify) distributional assumptions, the P-values may differ between the two presented tests. Nonetheless, in almost all Type I datasets the mutual information deviates significantly from zero, under both distributional assumptions. Hence, the unconditional transcriptomic entropy H(Y) significantly exceeds the conditional transcriptomic entropy H(Y|X), conditional on the DNA copy number. The Kim dataset, however, shows deviating behavior with clear non-significant P-values. This seems due to the fact that these cancer samples contain relatively few (compared to, e.g. the Bergamaschi dataset) genomic aberrations (as determined by the calling method CGHcall, Van). When analyzing the metastatic samples of the Kim dataset, that contain more genomic aberrations, the permutation test for H 0 : I(Y;X) = 0 yields P-values 0.076 (normality) and 0.064 (k-NN). This suggests that also in prostate cancer the genomic entropy surge is propagated to the transcriptome, although perhaps at a later stage in the progression of the disease. To accompany the above test results, we propose a visualization. The k-NN entropy statistic (3) is composed of the entropies at each observation. Each sample's contribution to the k-th nearest neighbor genomic entropy estimate may then be plotted against its contribution to the k-th nearest neighbor transcriptomic entropy estimate. If indeed the entropies of the two molecular levels are closely related, we expect the 'marginal' entropies at eachobservation, log, to be positively associated. This is visualized for two datasets in the upper panels of. Indeed, as the test results reported insuggest, the 'marginal' entropies of the two molecular levels reveal a positive association. The concordant surge in entropy of the cancer cell's molecular levels may be interpreted as follows. Hereto we exploit the reciprocal link between entropy and information (). As cancer progresses, the information content of a cancer cell's genome declines until some minimum necessary for the cell to function and proliferate is reached. The above shows that this information decline is reflected at the transcriptomic level, which is a prerequisite for it to have any phenotypic/oncogenic effect. Together these observations suggest that in the evolution (which naturally selects the path of steepest entropy ascent, Kaila and Annila 2008) of the cancer cell, natural selection acts on those parts of the genome and transcriptome that contribute to the cancer cell's minimum information content that enable it to realize its small but focused agenda, making more copies of itself.
PotentialBesides providing insight into cancer progression, entropy may be used in clinical cancer research. We illustrate this by two examples, which can be the basis for further research. We shall refer to the observation that molecular entropy increases with cancer progression as the Entropy Assumption. In the first example, we aim to reconstruct (unsupervisedly) the order of the samples' cancer advancement using the Entropy Assumption. A simple unsupervised procedure to achieve this would be to start with all samples together, and remove them one by one in accordance with the largest entropy decrease of the expression levels caused by a sample's removal. If the Entropy Assumption has some value, the order of removal is, negatively related to the samples' cancer advancement: Page: 562 556563
W.N.van Wieringen and A.W.van der Vaart
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
The non-parametric bootstrap seems inappropriate here (and in Section 2.4) as it draws datasets of the same dimensions with identical replicate samples, which inflates the compactness (decreases entropy) of the dataset.
