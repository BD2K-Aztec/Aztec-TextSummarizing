Motivation: Identifying subclonal mutations and their implications requires accurate estimation of mutant allele fractions from possibly duplicated sequencing reads. Removing duplicate reads assumes that polymerase chain reaction amplification from library constructions is the primary source. The alternative—sampling coincidence from DNA fragmentation—has not been systematically investigated. Results: With sufficiently high-sequencing depth, sampling-induced read duplication is non-negligible, and removing duplicate reads can overcorrect read counts, causing systemic biases in variant allele fraction and copy number variation estimations. Minimal overcorrection occurs when duplicate reads are identified accounting for their mate reads, inserts are of a variety of lengths and samples are sequenced in separate batches. We investigate sampling-induced read duplication in deep sequencing data with 500Â to 2000Â duplicates-removed sequence coverage. We provide a quantitative solution to overcorrec-tion and guidance for effective designs of deep sequencing platforms that facilitate accurate estimation of variant allele fraction and copy number variation. Availability and implementation: A Python implementation is freely available at https://bitbucket.org/wanding/duprecover/overview. Contact:
INTRODUCTIONMany somatic mutations, including known driver mutations, are found in only a subset of tumor cells (). Detecting the presence of these subclonal mutations and estimating their population size can critically affect the clinical diagnosis and therapeutic intervention of individual cancer patients (). This realization has led to the rapid development of deep sequencing as a molecular diagnostic platform in cancer clinics (). Estimating the variant allele fraction (VAF) from somatic samples sheds light on the intrinsic sample heterogeneity that originates from somatic mutations, and hence the etiology of many diseases, particularly cancer (). In addition, genomic regions (such as genes or exons) may exist in different numbers of copies due to mutational events such as duplication and deletion. This is referred to as copy number variation. In oncology, comparisons between copy numbers of different genes or between copy numbers of the same gene from different samples (normal versus tumor tissue, for instance) disclose signs of any selective pressure driving tumorigenesis (). Both tasks can be approached by counting reads from next-generation sequencing (NGS) experiments (). In practice, read counting is complicated by amplification bias, namely, the bias as a result of the preference of the polymerase chain reaction (PCR) in reproducing reads of different lengths and compositions (). Removing duplicate readsreads of the same length and sequence identityis a widely used practice to correct this bias when analyzing NGS data () (). The underlying assumption of this approach is that PCR amplification is responsible for most of the read duplication. Extending from this assumption, a long-standing recognition has been held in the community that removing duplicate reads at least does not harm the data. An alternative source of read duplication is sampling coincidence, whereby inserts are fragmented at identical genomic positions during library construction. The practice of removing duplicate reads is well justified only when the sequencing depth is low and sampling coincidence is unlikely. This was true when most NGS applications were of low sequencing depths and were oriented toward uncovering germline mutations from monoclonal samples. However, as recent studies that aim to detect rare somatic mutations from heterogeneous samples have pushed sequencing depth to a high magnitude (), the validity of this assumption requires serious re-evaluation. This article provides a quantitative understanding of the source of read duplication by quantifying the read duplication that is induced by sampling coincidence. By providing a statistical formulation for the bias of the allele fraction estimator based on de-duplicated reads, we are led to conclude that at a high sequencing depth, the practice of duplicate read removal can overcorrect amplification bias. From simulations, we show that the extent of overcorrection is jointly determined by the sequencing depth, the variance of the insert size, the strategy *To whom correspondence should be addressed.used for marking duplicate reads and intrinsic sequence properties, such as the existence of segregating sites in the neighboring region and the linkage disequilibrium (LD) pattern among sites. To quantify the amount of sampling-induced read duplication, we applied our model and overcorrection amendment method to data from a clinical cancer sequencing platform that produces 500 to 2000 sequence coverage to exons in 202 targeted cancer genes. Consistent with the currently applied assumption behind duplicate read removal, we found that PCR amplification, rather than sampling coincidence, is responsible for most read duplication. When duplicate reads are removed, the read depth is not as high as originally designed from the experiment, reflecting an insufficient sample complexity in the experiment. However, for reads that are treated as single-end reads because the corresponding mates cannot be identified ($one-tenth of reads), sampling-induced read duplication is not rare. Further, when we artificially mixed different deep sequencing samples to a much higher read depth, we observed more sampling-induced read duplication, as expected. Hence, we predict that further increases in sequencing depth or reduction in insert size variation may lead to non-negligible biases that require a method of correction such as what we provide in this article. In the field of RNA-seq, where read count is used to estimate transcription level, two recent studies have taken into account 'natural duplication' (). This concept is analogous to what we study in this article, albeit studied without systematic investigation of segregating sites or VAF bias. With that in mind, the contribution of this article is 3-fold. First, we call attention to the potential bias in estimating VAF and copy number variation due to overcorrecting read counts in deep DNA sequencing (particularly whole exome sequencing for clinical applications). Although duplicate read removal does not lead to substantial overcorrection on the datasets we studied, our simulations demonstrate that overcorrection from duplicate read removal could be substantial at smaller insert size variances and higher read depths. Second, we provide insights into the design of ultra-deep sequencing experiments such that duplicate read removal is most effective and overcorrection is minimal. Third, we propose a practical computational method for estimating the amount of sampling-induced read duplication for evaluating whether a dataset is amenable to de-duplication and for amending the overcorrection. Through simulations, we show that our methods can recover the true VAF or copy number variation (up to the extent permitted by the data).
METHODS
Modeling sampling-induced read duplicationWe start by considering a single nucleotide variation (SNV) site with no segregating sites in the neighboring region. Let n be the number of inserts that cover site v (read depth), the VAF of allele i at site v be denoted by p i , P N i p i  1, p be the vector composed of p i and N jAEj, where AE is the set of alleles. For a single site, AE fA, T, G, Cg. Because v is the only segregating site in this region, we classify unique insertsinserts with distinct start and end locations when alignedby the insert size and the allele identity at v. Each element in the matrix m  fm l, h g stands for the number of unique inserts of length l and covering allele h 2 AE at v. Likewise, c is a matrix where c l, h denotes the number of reads (not necessarily unique) of length l and covering allele h at v. The probability of observing unique read configuration m given the underlying allele fraction p can be modeled by marginalizing out all possible values of insert size configuration n 0 and read count configuration c.where n 0 is the vector of the number of inserts of each insert size, i.e. n 0 l is the number of inserts of insert size l. Pn 0  can either be learnt from data or modeled using a multinomial distribution with normally distributed means (Supplementary Section S1). Pcjn 0 , p models the sampling of inserts that covers each of the alleles in a multinomial distribution,assuming the probability of an insert covering allele h is independent of the probability of it having a certain insert size, l. The term Pmjc models the sampling coincidence. Assuming that the position samplings of reads of different lengths and allele identities are independent, the joint probability can be expressed in the product Pmjc  Q l, h Pm l, h jc l, h . Given an insert size l, there are  minl, 2r ways of positioning the read, where r is the read length (). In other words, there are only possible unique inserts that can cover site v for each allele. We assume for one sampling that seeing each of the candidate inserts is equally probable. The probability of obtaining m l, h unique inserts from sampling a total of c l, h reads can be described by the following distribution:). See Supplementary Section S1.1 for a detailed derivation of this expression.
Generalizing to multiple sitesThe aforementioned formulation can be extended to multiple sites that are close to each other and can potentially be covered by the same insert. Because neighboring SNV sites can alter the probability of samplinginduced duplication at the target site (Section 3), it is best to consider SNV sites that are covered by at least one insert as one multiple nucleotide variation site, when estimating sampling-induced duplication. Let be a maximum phasable window (MPW), which is defined as a set of contiguous SNV sites in the genome such that no insert simultaneously covers an SNV site inside and outside the window. Let w be the number of SNV sites in , H be the set of all possible haplotypes and N jHj. For example, if all sites are biallelic, N  2 w. The distances between these SNV sites (the number of nucleotides in between plus one) are denoted by d 1 , d 2 , :::, d w1. An insert cover is the set of SNV sites that are visible from the insert.  is the set of all insert covers (seeand d for an illustration). For inserts composed of one single-end read, there can be  w  1 2   ww  1=2 different insert covers. For inserts of two paired-end reads, the number of all possible insert covers is P i2, 3, 4  w  1 i . The number of SNV sites included in the cover 2  is denoted by . For each cover , H denotes the set of haplotypes defined by sites in. Two haplotypes defined on different covers are compatible if they agree on the shared sites. Haplotypes defined on nonoverlapping inserts are by definition always compatible. We use the notation gfflh, g 2 H 1 , h 2 H 2 to indicate compatibility. Because a certain insert size allows for only a subset of insert covers of , we use notation l  to denote the set of insert covers that is allowed by inserts of insert size l. We use m l, to denote the number of unique inserts observed from the data that have insert size l and that cover 2 , and l, to denote the number of all possible unique inserts of length l and that cover (see Supplementary Section S2 for a computation of l, in both cases of single-end and paired-end reads). The fraction of unique reads reporting each haplotype on the entire MPW is p i , i 2 H. In contrast to the single site case in which each read that covers the target site v can unambiguously resolve the allele identity, only the cover  that includes all sites (the 'widest' cover) can phase the full haplotype. The other covers only restrict the possibilities to a smaller subset of haplotypes. Reads of different covers contribute differently to the VAF for the full haplotype. Therefore, we need to separately consider read counts for reads with different lengths and covers. Following the notations used in the single site case, m is the number of unique reads that at least partially overlap with , except that m is now a multilinear integer field: N    H ! N. The domain of m is jointly and incrementally delineated by (i) the insert size (l 2 0, L), (ii) the insert cover ( 2 l) and (iii) the haplotype revealed by the cover (h 2 H  ). The same applies to c, which denotes actual insert counts with a specific l, and h. Extending from Equation (1), the probability of obtaining m l, , h unique inserts can be modeled by marginalizing out all possible insert size configurations (n 0 ), cover configurations (n 00 ) and read count configurations (c):Pn 00 jn 0 Pn 0 : 4Here, n 0 l (an element of n 0 ) is the number of reads (not necessarily unique) that are of insert size l, n 00 l, (an element of n 00 ) is the number of reads that are of insert size l and cover (see Supplementary Section S3 for details), n 0 is a vector and n 00 is a matrix. When there is only one site (w  1),  contains only one cover. Hence, n 0 and n 00 have the same dimension and a one-to-one correspondence. The aforementioned derivation reduces to the single nucleotide case, h , assuming independence of sampling from the insert size, cover and haplotypes. Pm l, , h jc l, , h  has the same form as the single site case] after replacing m l, h with m l, , h , c l, h with c l, , h and with l,. The probability of the insert's haplotype is modeled as a multinomial distribution. And the assignment of inserts to different haplotypes is assumed to be independent from the insert size and the cover. The haplotype sampling has the following form:which is the same as Equation (2), except that q h  P g2H , gfflh p g. That is, the expectation of haplotype h is the sum of the expectations of all the haplotypes g 2 H on the full MPW that are compatible with h. The term Pn 00 jn 0  models the sampling of covers given inserts with particular sizes in a multinomial distribution or n 00 $ Mn 0 , . The expectation for the count of inserts having a particular cover is proportional to the number of unique inserts with that cover:
Variant allele fraction estimatorFor cases with single SNV sites, the allele fraction is estimated by computing the proportion of all unique inserts of that allele over all possible insert sizes, i.e.For cases with multiple sites, due to the ambiguity encountered in phasing the haplotype on the full MPW from inserts with & , the haplotypes of the entire MPW can be estimated by maximizing the likelihood function defined in Equation (5) and replacing c by m (which is considered the corrected read count), i.e. ^ p 0  arg max p Pmjn 0 , p:
Amending read count overcorrectionIn practice, the number of unique inserts (m) is used where read count (c) is meant because the observed read count is believed to have been distorted by amplification biases. However, due to the presence of samplinginduced read duplication, such treatment can be a substantial overcorrection of the read count (Section 3). Hence, we seek a maximum likelihood estimation ^ c of c from m byHere, the likelihood function is given by Equation(3) with irrelevant multiplicative terms omitted. The Stirling numbers are precomputed and retrieved in constant time. Equation (6) assumes m l, h 5 and 6  0. When  0, no such insert is possible. When m l, h  , ^ c l, h is unbounded. This is the case of saturation. Intuitively, the more inserts one provides, the more likely saturation will be attained. Hence, inserts of lengths that result in saturation do not contribute to our understanding of the VAF, and we exclude inserts of such lengths in our read count correction. In the case where read counts from multiple alleles are compared, inserts of a length are excluded for all alleles even if they saturate the read count for only one allele. The allele fraction is then estimated from the corrected read count byh , where L is the set of all insert sizes such that inserts of that insert size saturate the read count for none of the alleles, i.e. L  fljm l, h 6  , 8h 2 AEg., the correction is negligible. The corrected read count surges drastically as the unique read count approaches the read length. Generalizing to multiple sites, the de-duplicated read counts for a given insert size, cover and haplotype can be amended by the same formula as Equation (6), but replacing m l, h with m l, , h and with l,. The fractions of haplotypes of the entire MPW are estimated from the corrected c by maximizing the likelihood function defined in Equation (5). Here, the count of inserts of each insert size and cover (n 00 ) can be obtained from the data. After omitting the irrelevant multiplicative terms, the log likelihood function breaks down towhere c h P l, 2C c l, , h. As in the case of single site, we exclude in our estimation all the combinations of insert size and cover such that any haplotype of the combination is saturated: C  fl, jm l, , h 6  , 8h 2 H g. Owing to the constraint P h p h  1, we regard the frequency of an arbitrarily chosen haplotype as being dependent on the other haplotype fractions: p  1  P h6  p h. The gradient of the likelihood function has the following expression:Then we apply a conjugated gradient method to optimize the likelihood function. Because the estimation of c l, , h is subject to greater randomness when m l, , h ! l, , we loosen the criterion for saturation to m l, , h ! l,  , where the parameter  1 (i.e. we also exclude combinations of insert size and cover with m l, , h  l,  1).
RESULTS
Overcorrection from duplicate read removalThe number of unique reads of a particular length that can cover a site is equal to the read length. Intuitively, when the read depth tends to infinity, the number of reads that support each allele at the target site is equal to the read length regardless of the underlying allele fraction. In Supplementary Section S4, we give a brief proof of this intuitive conjecture that ^ p 0 is biased.
Highercoverage and lower insert size variance result in greater bias We use simulations to investigate various factors that affect bias in the following. We restrict our analysis to single-end reads for this section. The conclusions can be readily extended to paired-end reads by properly choosing the number of reads/inserts that cover a site by replacing  r with  minl, 2r. Hence, in the following text, we use the terms 'read'/'insert' and 'read length'/'insert size' interchangeably unless specified. Consider a scenario where three alleles exist for a site v in the sample. These three alleles have the following VAFs: 1/8, 3/8 and 4/8. We first consider the simplified scenario where the insert size is fixed at 200. We compare the biases against the numbers of reads that cover a specific site v (or the coverage at v). As expected, the estimated allele fractions deviate from their true value as coverage increases (see Supplementary). Thus, we corroborate our theoretic prediction that the VAF estimation may be biased when sequencing depth is high and sampling-induced read duplication is non-negligible. When the insert size is allowed to vary, we investigate the joint effect of the mean as well as the variance of the insert size distribution under the aforementioned simulation setting. We observe that the longer and the more variant the insert size, the greater the number of unique reads that can cover site v (see Supplementary). This can be understood by knowing that higher variance in insert size effectively helps differentiate reads and reduce sampling-induced read duplication. The extension of PCR amplification also depends on the insert size. The use of precision in selecting the insert size helps reduce the PCR amplification bias; but on the other hand, it introduces bias if duplicate reads are removed. Thus, the choice of different insert size selection directly affects the subsequent data analysis strategy, namely, whether to perform duplicate read removal (Section 4). For samples with clonal heterogeneity, sites are usually biallelic and the fraction of the minor variant has the most clinical relevance. In, we provide an estimation of the magnitude of the bias at different settings of the two most important factors: the coverage and the insert size variation. By mapping to this heat map, one may get a rough estimate of the magnitude of the bias in his/her estimation of the VAF. For example, when one has an insert size standard deviation $20 and coverage $5000, the bias is $0.01, which is $20% compared with the true value at 0.05. See Supplementary Section S7 of bias plots at different VAF values (which are qualitatively similar). 3.1.2 Low entropy allele fractions are more biased, and germline samples remain unbiased In investigating the bias in specific allele fraction configurations, we also observe that the VAF bias is inversely correlated with the information entropy of the VAF configuration, which is defined asThis trend can be understood by considering the fact that the convergent b p i  1=N 8i  1, ::, Nall alleles are equiprobablehas maximum entropy given the number of alleles. To illustrate this by simulation, we randomly sample VAFs of the four alleles and for each VAF, we estimate the magnitude of the bias (Supplementary). The higher the VAF entropy, the lower is the VAF bias (Spearman's  0:72, P  7:3  10 84 ). Another corollary from this observation is that VAF estimation
Segregating sites in the neighboring region and LDIn deriving the overcorrection bias on single sites, we assume that the neighboring region has no segregating sites. In cases where the neighboring region does contain segregating sites, we can estimate the asymptotic value of the estimator ^ p 0 as coverage tends to infinity by limwhere i is the number of unique haplotypes that cover allele i within the reach of each insert size. In Supplementary, we simulate the previous example of three alleles but introduce segregating sites in the neighboring region and investigate the bias. Intuitively, neighboring segregating sites help increase the number of unique reads that cover the target site. When all the segregating sites are in linkage equilibrium, the magnitude of the bias is reversely associated with both the number of segregating sites and the number of alleles per site (Supplementary). Because of linkage equilibrium, i = P j j  1=jAEj. In other words, each allele at the target sites is paired with an equal number of haplotypes defined by the alleles in the neighboring sites ( i  j , 8i, j 2 AE). Therefore, the limiting VAF estimates remain the same. In computing i , the knowledge of the positions of these segregating sites or how far they are from the target site is a prerequisite. The farther they are from the target site, the less likely it is that a read can be positioned to cover both the target site and the segregating sites, resulting in a weaker effect of these segregating sites. We simulate a simplified scenario with only one extra segregating site (Supplementary). The magnitude of the bias is compared against the distance between the neighboring segregating site and the target site. As expected, we observe that the longer the distance, the greater is the bias. Combining the previous conclusion that the presence of a segregating site reduces the bias helps us understand this result by considering a diminishing reductive effect from the sites farther from the target. When the neighboring sites are in LD with the target site, the limiting VAF estimates deviate from 1=jAEj. For simplicity in computing the LD, we consider a scenario where both the target site and the segregating site (there is only one) contain two alleles. We consider two scenarios: (i) the target site has an allele fraction of p 1  0:2, p 2  0:8 and (ii) the target site has an allele fraction of p 1  p 2  0:5. The first scenario corresponds to the case where there is sample heterogeneity or somatic copy number variation. The second scenario corresponds to pure germline samples. In the first case, the segregating site has allele fraction q 1  0:5, q 2  0:5. In the second case, we assess both q 1  q 2  0:5 and q 1  0:3, q 2  0:7 for the allele fraction on the segregating site. In both scenarios, we find that both the mean and the variation of the VAF determined by the level of LD and allele fraction estimated from germline samples remain unbiased (see Supplementary Section S10 for details). In summary, the bias caused by overcorrecting samplinginduced read duplication when removing duplicate reads is affected not only by the number of segregating sites in the neighboring region but also their positions and the linkage pattern between the target site and the segregating sites. For a correct estimation of the allele fraction at a site where the neighboring region contains other SNV sites, it is more appropriate to treat these SNV sites together with the target site and estimate haplotype frequency on the entire DNA segment (the so-called MPW), the neighborhood of which we can assume has no segregating sites (Section 2). The subsequent allele fraction at a single site is a direct summation of the fractions of all the haplotypes with the allele defined on this segment.
Read count amendment in simulationsTo assess our amendment to the overcorrection of duplicate read removal, we conduct two simulations. In one, we target detecting copy number variation by estimating the ratio of the copy number at a single site in two different samples. In the other simulation, we estimate the VAF at a multiple nucleotide variation site composed of two neighboring SNV sites. Suppose we have two sets of reads that cover a site v. They correspond to tumor and normal samples from the same cancer patient. We want to compute the copy number variation between the two samples. Suppose e 1 and e 2 are the respective copy numbers of a site from the two samples, respectively, and c 1 and c 2 are the observed counts of reads from the two samples. To model the amplification bias, we introduce such thatTo illustrate the joint effect of bias that arises from amplification and bias from removing sampling-induced duplication, we consider the situation where e 1 =e 2  1=3. We suppose that amplification has preferentially enriched alleles in the second sample by  1=2, i.e. each read in the second sample is amplified twice as much as the reads in the first sample. At low coverage, if one removes all read duplicates (as depicted in red in Supplementary), one obtains a copy number variation with an expectation closer to the true ratio of 1/3. But the expectation starts to deviate from the true copy number variation and approaches the saturation ratio at one as coverage increases. The blue dots and boxes correspond to a process of counting reads without removing read duplicates. Their expectations remain at the biased ratio of 1/6. In both cases, the variance drops with the rise of coverage. This illustrates that although the removal of duplicate readscorrects for amplification bias at a low coverage, as coveragethe coverage limit is only several thousands. Further, because not all sites are equally represented in the original DNA sample and some samples (e.g. those from tumors) have an abnormally higher ploidy and hence a higher L, the coverage limit may be even lower. Therefore, if one starts from a small amount of DNA (e.g. 10 ng) and sequences to a high coverage (e.g. 10 000), most reads will necessarily be duplicates from PCR amplification. That being said, for samples with high coverage and truly high complexity, sampling-induced read duplication will become nonnegligible. Many factors besides the issue of read duplication can impact the VAF estimation. For example, bias in ligating fragments to the adaptor during library construction in the experiment can cause uneven sampling of inserts at different genomic positions. Our model assumes a uniform fragility of the genome; however, real sampling may be biased by base content and produce certain inserts more often than others, which will further aggravate sampling-induced read duplication. This variation in the capturing efficiency also applies to different exons in whole exome sequencing. In addition, the quality of alignment impacts the read count for a mutation site. For example, most alignment algorithms tend not to map/report mutations, SNVs or small indels, at the ends of the reads. This results in an underestimation of reads that have mutations at their ends. Further, reads that originate from paralogous regions may be mismapped, which will confound the VAF estimation when alleles from all paralogous sites are agglomerated and there exists a coverage bias between the paralogous region and the target region. This issue may also result from erroneous fetching of DNA sequences from undesired sources. Our method can be extended to the scenario where one has knowledge of the mutations and is further interested in estimating the copy number variation (as in some cases of RNA-seq). For RNA-seq data, sampling-induced read duplication can be more extensive due to the uneven distribution of gene expression (highly expressed genes are more susceptible; see Supplementary Section S15 for an application to RNA-seq data). Compared with RASTA () and iReckon (), our method does not make decisions as to the amount of sampling-induced read duplication for each unique insert/read. Rather, our method relies on the number of observed unique reads covering the target mutation site to infer the true count of reads that are not necessarily unique. Besides computational remedies to amplification duplications, some recently emerging experimental techniques are showing promise as ways to resolve the current dilemma of read duplication. For example, in digital-PCR (), each fragment is given a unique identifiable barcode before amplification, which allows for the estimation of fragment abundance by merely counting the barcodes. With further reduction in cost and removal of restrictions in sequencing depth (), such a technique may replace the currently used paradigm of estimating allele frequencies.
CONCLUSIONRemoving read duplicates, while correcting for PCR amplification bias, could introduce another bias owing to overcorrection of read counts as a result of sampling-induced read duplication. This bias is of particular concern when the sequencing is deep (e.g. 45000) and the insert size is short and non-variant. A maximum likelihood amendment can be applied to the number of de-duplicated reads to account for sampling-induced read duplication. Sampling-induced read duplication in most current ultra-deep sequencing experiments is not prevalent due to the presence of a substantial amount of PCR amplificationoriginated duplicate reads. Nevertheless, attention must be paid to duplicate read removal in ultra-deep sequencing experiments that perform fewer rounds of PCR amplification and use tightly selected insert sizes.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
W.Zhou et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Bias from duplicate read removal at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
.3 Sampling-induced read duplication in clinical sequencing data We apply our methods to a cancer dataset, T200 (SRA accession: SRP033243), in which the exons of 200 cancer-related genes are sequenced to a read depth of 2000 $ 3000 (500 $ 2000 after removing read duplications). DNA samples obtained from lung cancer cell lines with known mutations are sequenced on a HiSeq 2000 (Illumina Inc., San Diego, CA, USA) on a version 3 TruSeq paired end flowcell according to the manufacturer's instructions, at a cluster density between 700 and 1000 K clusters/mm 2. In the T200 dataset, the insert sizes have mean  173:2 and a relatively large variance (  51:1). Compared with a normal distribution, the distribution of the insert size is skewed toward the long end (Supplementary Fig. S13). Because most reads are of a fixed read length (76 in some of the samples and 100 in others, depending on the sequencing platform), we regard reads of other read length as of poor quality and exclude them from the analysis. From 675 SNV sites called from one sample, 371 contain no other segregating sites within 700 bp, and the other 304 SNV sites are organized into 77 MPWs. Each contains 232 SNV sites. Figure 4 shows plots, for the single site cases, of the read counts before and after duplicate read removal (y-and x-axes of the 'x' markers) together with the maximum likelihood amendment of the read count based on de-duplicated reads (dots). The left panel shows sampling-induced duplication based on marking the duplicate reads and accounting for the mate reads. In contrast to the large deviation of the 'x' markers from the x  y line, the dots are close to this line, meaning that sampling-induced read duplication is rare in this case. Most read duplication is probably due to PCR amplification rather than sampling coincidence. The right panel shows the same result but for marking the duplicate reads as if they are single-end reads. This is done because in some instances, the two mate reads are not well sequenced or mapped. In this dataset, we found 204 895 such single-end reads aside from 790 142 complete inserts (the mate reads of which can be identified). In contrast to paired-end reads, single-end reads are more susceptible to sampling-induced read duplication because they lack the mate read information that differentiates themselves from other reads. Similar results are observed from multiple sites (Supplementary Fig. S14). To illustrate sampling-induced read duplication under this insert size variation, a higher read depth is necessary. We mix reads from 82 samples from the T200 dataset. In this way, we obtain one dataset of up to 10 000 coverage. After de-duplication, only about one-tenth of the reads is left (Table 1); therefore, the read depth is not as high as it seems. Only complete inserts are used in computing the VAF. In the rightmost column of Table 1, p 00 v indicates the VAF when accounting for samplinginduced duplicate reads, but not those originating in the amplification process. In the four sites we show in Table 1, the value of p 00 v lies between that of the allele fraction calculated from raw inserts (p v ) and the allele fraction calculated from de-duplicated inserts (p 0 v ). In all four sites, p 00 v deviates from p 0 v , which is an indication of the presence of sampling-induced duplication. The magnitude of deviation is higher for sites with higher coverage, which is consistent with the simulation results. Fig. 4. Read count in T200 data. Left panel: paired-end reads duplication removal for single SNV site. That is, insert size is accounted for while marking duplicates. Right panel: Treat paired-end reads as single-end reads, i.e. the mate reads are ignored. Sites are single SNV sites. The 'x' markers correspond to counting reads without removing duplicates. Dots correspond to the read count correction using the model presented in this article. The dashed gray line is the x  y line. Each dot corresponds to a combination of the site, allele at the site and insert size of the reads that cover the site. The same unique read counts can be corrected to different values as a result of different insert sizes
Note: All sites are biallelic. n is the number of inserts; n d is the number of inserts after de-duplication; p v is the VAF computed using all inserts; p 0 v is based on deduplicated inserts; and p 00 v is computed by applying the maximum likelihood amendment described in this article.
