Motivation: Next-generation sequencing (NGS) allows for analyzing a large number of viral sequences from infected patients, providing an opportunity to implement large-scale molecular surveillance of viral diseases. However, despite improvements in technology, traditional protocols for NGS of large numbers of samples are still highly cost and labor intensive. One of the possible cost-effective alternatives is combinatorial pooling. Although a number of pooling strategies for consensus sequencing of DNA samples and detection of SNPs have been proposed, these strategies cannot be applied to sequencing of highly heterogeneous viral populations. Results: We developed a cost-effective and reliable protocol for sequencing of viral samples, that combines NGS using barcoding and combinatorial pooling and a computational framework including algorithms for optimal virus-specific pools design and deconvolution of individual samples from sequenced pools. Evaluation of the framework on experimental and simulated data for hepatitis C virus showed that it substantially reduces the sequencing costs and allows deconvolution of viral populations with a high accuracy. Availability and implementation: The source code and experimental data sets are available at
IntroductionNext-generation sequencing (NGS) generates a large number of viral sequences carried in samples of infected individuals, offering novel prospects for studying microbial populations and understanding pathogen evolution and epidemiology. NGS provides an opportunity to implement a large-scale molecular surveillance of infectious diseases for monitoring of disease dynamics and providing for informed guidance for planning public health interventions. Although NGS offers a significant increase in throughput, sequencing of viral populations from a large number of specimens is prohibitively expensive and time consuming. Therefore development of a strategy for rapid and cost-effective massive viral sequencing is a key to effective molecular surveillance. Owing to a high mutation rate, RNA viruses exist in infected hosts as highly heterogeneous populations of genetic variants, which are commonly referred to as quasispecies. Genetic viral variants can be detected using highly variable subgenomic regions that can be easily amplified and sequenced. Although genetic information presented in short subgenomic regions does not allow for identification of all variants, it is sufficient for transmission networks inference (), study of drug resistantce () and intrahost viral evolution (). To reduce the cost of sequencing of multiple viral samples, multiplexing through barcoding is usually used. Although this is a straightforward approach to a simultaneous sequencing of many viral strains, it requires individual handling of each sample starting from nucleic acid extraction to PCR and library preparation, which increases the costs of sequencing per specimen (). Decoding of samples sequenced using large libraries of barcodes may be highly affected by NGS errors (). Additionally, besides introduction of bias in amplification of different viral variants using PCR primers with different barcodes (that may affect distribution of reads) (), maintaining a large library of barcodes is daunting (). An alternative strategy is combinatorial pooling. Commonly, it was used for tests producing binary results (). Recently, several pooling strategies were proposed for more complex assays based on DNA sequencing, SNP calling and a rare alleles detection (). The pooling problem for viral quasipecies sequencing is fundamentally different from all existing pooling protocols. Previously developed methods assume that a single (consensus) sequence must be reconstructed for each sample. In contrast, for viral quasispecies sequencing it is imperative to reconstruct the whole population structure of each sample that includes multiple sequence variants and their frequencies. This problem formulation and the nature of heterogeneous viral populations require a completely novel approach for pool design and deconvolution. We propose a protocol for a cost-effective NGS of complex viral populations, which combines barcoding and pooling and includes the following steps (): (i) mixing samples in a specially designed set of pools so that the identity of each sample is encoded in the composition of pools; (ii) sequencing pools using barcoding; (iii) pools deconvolution (PD); i.e. assignment of viral variants from the pools to individual samples. This approach significantly minimizes the number of PCR and NGS runs, reducing the cost of testing and hands-on time. Additionally, pooling provides opportunity for PCR amplification of viral variants from each sample in different mixtures of samples generated in each pool, thus introducing variation in amplification biases and contributing to sequencing of a more representative set of viral variants from each sample. Pooling-based sequencing of highly mutable viruses such as human immunodeficiency virus and hepatitis C virus (HCV) is particularly difficult because of the complexity of intrahost viral populations, the assessment of which can be distorted by PCR or sampling biases. It is essential to detect both major and minor viral subpopulations, since the latter often have important clinical implications (). Mixing of a large number of specimens or specimens with significant differences in viral titers may contribute to underrepresentation of viral variants from some samples in pools, suggesting that size and composition of pools should be carefully designed. Stochastic sampling from genetically diverse intra-host viral populations usually produces variability in compositions of sets of variants in different pools obtained from the same sample. Additionally, mixing specimens may differentially bias PCR amplification, contributing to mismatching between viral variants sampled from the same host in two pools with different specimen compositions. Thus, straightforward approaches cannot be used for samples deconvolution, indicating that a more complex approach based on clustering techniques is needed. To increase the effectiveness of cluster-based deconvolution and minimize possible clustering errors, it is important to minimize mixing of genetically close samples as can be expected in epidemiologically related samples and samples collected from a small geographic region. In this article, we present the first computational framework for combinatorial pool-based sequencing of highly heterogeneous viral samples. The framework includes pools design and PD stages. We formulate the pool design problem as an optimization problem and propose a heuristic algorithm to solve it. We propose a method for inference of samples from sequenced pools based on a novel maximum likelihood clustering algorithm for heterogeneous viral samples. We report the results of the framework evaluation using simulated and experimental HCV data.
Methods
Pools designThe basic idea of the overlapping pools strategy for sequencing n samples is to generate m pools (mixtures of samples) with m ( n in such a way that every sample is uniquely identified by the pools to which it belongs (). Then, after sequencing of pools the obtained amplicon reads can be assigned to samples by a sequence of set-theoretic intersections and differences of pools. The unique assignment is only possible if for any two samples there is a pool separating them, i.e. containing exactly one of those samples. Example. Consider 3 samples S 1 ; S 2 ; S 3 and 2 pools P 1  S 1 [ S 2 ; P 2  S 2 [ S 3 (). These pools satisfy the separation requirement, and, therefore, each sample can be recovered, e.g.Without constraints, n samples can be inferred using log n d e 1 pools (Theorem S1, also indirectly follows from). However, heterogeneous viral samples impose restrictions on pools composition: (i) the number of samples per pool should not be too high; (ii) samples with drastically different viral titers or samples, which may be epidemiologically related, should not be mixed together. These constraints make the pool design problem computationally hard. We formalize the constraints and formulate the pool design problem as an optimization problem on graphs. For a set of samples S  fS 1 ;. . .; S n g consider a samples compatibility graph G  GS with the vertex set V  VG and the edge set E  EG, such that V(G)  S and S i S j 2 EG if and only if the samples S i and S j could be mixed together. As aforementioned, information on viral load, epidemiological linkage, geographic location, age/social groups, time of infection, risk factors, etc. may be used to determine compatibility of specimens. Every feasible pool is a clique of the graph G. Let T be an upper bound for pools size. The problem of optimal pool design for viral samples sequencing can be formulated as follows: Viral Sample Pool Design (VSPD) Problem: given a graph G  V; E and T > 0, find a minimum set of cliques P  fP 1 ;. . .; P m g such that 1) [ m i1 P i  V; 2) for every u; v 2 V there is a clique P i 2 P separating u and v; 3) jP i j T for every i  1;. . .; m. Due to the condition 2), at most one vertex v 2 VG is not covered by a clique from P. Thus, any family of cliques satisfying 2)and 3) can be forced to satisfy 1) by adding the single clique {v}. Therefore the condition 1) is not essential and can be dropped. VSPD is NP-hard (Theorem S2), and we propose a heuristic (Algorithm 1) for it. We consider a graph H with V(H)  V and ij 2 EH if and only if the pair of vertices (i, j) is not separated yet. Initially, H is a complete graph. For A V a cut in the graph H is the pair A; V n A, the size of the cut cA; V n A is the number of edges with one end in A and the other end in V n A. At each iteration, Algorithm 1 finds and adds to the solution a locally optimal pool, i.e. a clique of G that separates the maximal number of nonseparated samples (see example in Supplementary).The crucial step of Algorithm 1 is locally optimal pool finding (step 3), which represents a previously unstudied discrete optimization problem further referred as Locally Optimal Pool (LOP) Problem. LOP is not approximable within the factor n 1e for any e > 0 (Theorem S3), and it can be reformulated as follows: find a partition X  A x ; B x  of the set V minimizing the function f X  cA x ; B x   MjEGA x jsubject to the constraint jA x j T. Here M  jEHj  1; G is a complement of a graph G, and GA x  is the subgraph of G induced by a set A x. So, f X ! 0 if and only if A x is a clique. Therefore, for any optimal solution of the problem (1), the set A x is a clique. We propose a heuristic to solve the problem (1). Initially, we relax the constraint jA x j T. For a vertex v 2 A x consider the solution X 0  A x0 ; B x0   A x n fvg; B x [ fvg. Thenwhere deg H U v denotes the number of vertices from the set U V adjacent to v in a graph H. In particular, if v is non-adjacent to some vertex u 2 A x , then D 1 > 0. For v 2 B x and the solution X 0  A x 0 ; B x 0   A x [ fvg; B x n fvg we haveAccording to (2)and(3), any initial solution can be iteratively improved by moving vertices from one part of the partition to the other until a locally optimal solution (A l , B l ) cannot be further improved. According to (2), A l is a clique. However, the objective function value in a local optimum may be significantly lower than the value of the global optimum. It is also possible that cA l ; B l   0, when EH 6  ;, which will cause Algorithm 1 to go into an infinite loop. To overcome these problems we use a tabu search strategy. The basic idea is that if after the moving of a vertex v the algorithm arrives at a local optimum, its value is compared with the current best solution A  ; B  , v is moved back and the moving of v isAlgorithm 1 VSPD Algorithm 1: P ;; H complete graph on n vertices 2: while EH 6  ; do 3: find a subset A x V such that jA x j T , A x is a clique of the graph G and cA x ; V n A x  in the graph H is maximal. 4: P P [fA x g 5: remove from H all edges uv with u 2 A x and v 2 V n A x. 6: end while 7: return P prohibited for the next k t iterations. The process stops, when it reaches a local optimum, which has been visited before with the same configuration of the algorithm states. Finally, the set A  is reduced to the allowed size. This idea is implemented in Algorithm 2 (see also Supplementary). The default value of k t is 1. If cA  ; B    0, we increase k t by one and repeat Algorithm 2.
Deconvolution of viral samples from pools2.2.1 Deconvolution using generalized intersections and differences Let P be the set of pools designed using Algorithm 1 and sequenced by NGS. As aforementioned, the obtained reads theoretically can be assigned to samples using set-theoretic intersections and differences of pools. However, owing to the heterogeneity of viral populations and sampling bias, individual viral variants and even viral subpopulations sequenced from a certain sample may be different in each pool containing that sample (see Supplementary). It hampers the usage of straightforward set-theoretic operations. For a pool P i , let SP i  be a set of IDs of samples mixed in it. In particular, we consider each individual sample R j as a pool with jSR j j  1. A generalized intersection (GI) of pools P 1 and P 2 is a pool P 1 \P 2 withIndividual samples are inferred from pools by a sequence of GIs and GDs (Algorithm 3). GDs reduce to GIs, which are calculated using a clustering-based approach (Algorithm 4). In some cases, when samples with substantial difference in heterogeneity are mixed together, highly heterogeneous samples can be partitioned into multiple clusters, while samples with lower heterogeneity are joined into one cluster. Such clustering may lead to incorrect detection of GIs and consecutive fail of samples separation. To avoid this effect, the parameter W of Algorithm 4 with the default value W  2 is introduced. If certain samples are not found by Algorithm 3 (i.e. some sets from R are empty), we increase W and repeat Algorithm 3.
Algorithm 2 Locally Optimal Pool Problem Algorithm1: Find the solution (X, Y) of Max-Cut problem using 0.5-approximation algorithm () applied to the graph H.D 1 ; if u 2 A i and tabu i u  0  see 2; D 2 ; if u 2 B i and tabu i u  0  see 3; 0; if tabu i u > 0:Computational framework for NGS
Maximum Likelihood k-Clustering of viral samplesIn this section, we consider Viral Sample Clustering (VSC) Problem: given a set R of NGS reads drawn from a mix of k0 viral samples, partition R into k  Wk 0 subsets consisting of reads from a single sample. The presence of numerous variants, extreme heterogeneity of viral populations and sequencing errors make VSC challenging. Although a commonly used clustering objective is to minimize intracluster distances or distance to cluster centers (e.g. the k-means algorithm), we propose to use a statistically sound objective of maximizing likelihood. An input of our algorithm is a multiple sequence alignment of R represented as a matrix with columns corresponding to the consensus positions and rows corresponding to aligned reads. Our model assumes that each read is emitted by a particular genotype. The proposed clustering (a) finds k genotypes G 1 ;. .. ; G k that most likely emitted R; (b) assigns each read to a cluster corresponding to a genotype that most likely emitted it. Formally, given a set of reads C i , a genotype G i  g(C i ) is a matrix with columns corresponding to an alignment positions and 5 rows corresponding to the alleles fa; c; t; g; dg, where each entry G i e;m is a frequency of allele e in m-th position among all variants in C i. Given a set of reads R, an optimal k-genotype is a set G   f G 1 ;. .. ; G k g of k distinct genotypes that most likely emitted R:is the probability to observe read r  r 1 ;. .. ; r m , o r is its observed multiplicity, f i is the frequency of the genotype G i andThe log-likelihood of the set of genotypes G equals to 'G  X r2R o r logPrrjG. We iteratively estimate the missing data f i and p ir (the frequency of genotype G i and the portions of reads originated from G i , that mathches r) using Expectation Maximization algorithm and solve the easier optimization problem of maximizing the log-likelihood of the hidden modelOur clustering method is described in Algorithm 5. The parameters e there is the mutation rate.
NGS errors and sequencing failures processingBefore applying deconvolution algorithms, the data are preprocessed to remove sequencing errors and PCR chimeras. Since errors may be sample-specific, the following pipeline is used: (i) each pool is partitioned into clusters using Algorithm 5; (ii) NGS error correction algorithm is applied to each cluster. This algorithm is specific to sequencing platform and sequenced species. (iii) Corrected reads from each cluster are used to reinstate pools. Failure to recover sequences from some samples within certain pools may result in algorithm's inability to separate these samples. Given the aforementioned pools design constraints, use of appropriate PCR conditions and high NGS throughput, we expect that the probability of a complete loss of a sample within a pool is very low. Nevertheless, if sequencing of some samples within certain pools fails, the workflow summarized in Algorithm 6 allows to detect and eliminate the negative effects of the failures.Calculate the updated frequency of each genotype G i 2 G t as the portion of all reads emitted by G i :Require: Datasets R  fR 1 ;. .. ; R n g produced by Algorithm 3. 1: Identify failed samples S f fj : R j  ;g (the sample can be also considered failed, if it does not have a required number of reads, i.e. jR j j D) and
Experimental pools and sequencingSerum specimens collected from HCV-positive cases () were used to sequence HCV HVR1 region. Seven samples S 1 ;. . .; S 7 were mixed to form 4 pools P 1 ;. . .; P 4 as follows: P 1 was created by mixing samplesSpecimens and pools were sequenced using 454 GS Junior System (454 Life Sciences, Branford, CT). Total nucleic acids extraction was performed using MagNA Pure LC Total Nucleic Acid Isolation Kit (Roche Diagnostics, Mannheim, Germany) and reverse-transcribed using SuperScript Vilo cDNA synthesis kit (Invitrogen, Carlsbad, CA). HVR1 amplification was accomplished using two rounds of PCR. For the 1st round, regular region-specific primers were used. Forward and reverse tag sequences consisting of primer adaptors and multiple identifiers (MID) were added to the HVR1-specific nested primers. Pools were processed as a single specimen, tagged with a single MID. PCR products were pooled and amplified by emulsion PCR using the GS FLX Titanium Series Amplicon kit, and bi-directionally sequenced. The NGS reads were identified and separated using sample-specific MID tag identifiers. Low quality reads were removed using GS Run Processor v2.3 and the obtained datasets were processed using error correction pipeline with algorithms KEC and ET ().
Results
Pools designPool design algorithm was evaluated using 3 sets of simulated data. 1) Complete graphs with n  4;. . .; 1024 vertices and with T  1. For every test instance, exactly log n d e1 pools were constructed, coinciding with the theoretically justified estimation. Hence, in this case Algorithm 1 produces optimal solutions. 2) Random graphs, where each vertex v receives a random titer w v 2 f1; Lg, and two vertices u and v are adjacent, when jw u  w v j R. This family of instances represents titer compatibility model, i.e. it simulates the case when two samples could be mixed together only if their viral titers are not sufficiently different. 25 000 test instances were generated with n  10;. . .; 1000; L  20 ; R  4 and with the pools sizes thresholds T  n, 55, 35, 25, 15. For each n the mean reduction coefficient (the ratio of a number of pools and a number of samples) was calculated (). For n  1000 the reduction of the number of sequencing runs varies from more than 21 for T  n to 6 for T  15. 3) Random graphs, where each edge is chosen with probability p  0:25; 0:5; 0:75; 1 and pools sizes are bounded by T  35. 20 000 test instances with n  10;. . .; 1000 were generated (). A reduction of the sequencing runs number is also high, although it is generally lower than in 2) (from $13-fold reduction for P  1 to more than 3-fold reduction for P  0.25, n  1000). The reduction coefficient in all these cases is a decreasing function of n, which suggests a higher reduction for the larger n.
Pools deconvolution3.2.1 Simulated pools of experimental data 450 test instances with n  10;. . .; 150 samples and with pool sizes thresholds T  15, 25, 35 were generated using 155 HCV HVR1 samples from the collection of Centers for Disease Control (). Samples were cleaned using KEC and ET (). Test instances were generated as follows: (i) n samples were chosen randomly; (ii) a random samples compatibility graph on n vertices was generated based on the titer compatibility model and pools were designed using Algorithm 1; (iii) pools were created by taking D  10 000 randomly selected reads from the samples composing each pool (in order to simulate a sampling bias). The number of reads per pool corresponds to the sequencing settings, under which the data used for simulation were obtained [454 GS Junior System (454 Life Sciences, Branford, CT) with 810 MIDs per sequencing run]. For all test instances all samples were inferred, i.e. all n data sets produced by Algorithm 3 were non-empty. The number of reads, which were not classified into samples was extremely low (): in average 99.996% of reads (T  15), 99.993% (T  25) and 99.984% (T  35) were assigned. An overwhelming majority of reads was classified correctly (): in average, 99.998% for T  15, 99.982% for T  25 and 99.959% for T  35. There is no clear correlation between percentages of classified and correctly classified reads and the samples number. We call an incorrect assignment of reads to samples in silico contamination. The average percentage of samples without in silico contamination ranges from 100 to 98.13% (T  15), from 100 to 96.13% (T  25) and from 100 to 93.8% (T  35) (). In silico contaminants within contaminated samples in average constitute 0.163% (T  15), 0.545% (T  25) and 0.892% (T  35) of all reads (). Root Mean Square Error (RMSE) of deconvoluted haplotypes frequencies estimation is in average 0.0310.107% (T  15), 0.0250.139% (T  25) and 0.0280.174% (T  35) (). Both the percentages of in silico contaminated samples and RMSE increase with n. The accuracy of samples deconvolution is affected by the number of allowed samples per pool. The algorithm is more accurate for smaller pools, although the accuracy for larger pools remains high. Working times of pools deconvolution are shown in Supplemetary. To test failure detection and processing workflow, 150 test instances with T  15 were generated, where in addition to stepsre-sequence them (either individually or using the same pooling framework). For every j 2 S f replace R j by the obtained data set. 2: for every i 2 f1;. .. ; ng n S f and j 2 S f do 3: Q i;j  R i \R j ; R i R i n Q i;j ; R j R j [ Q i;j 4: end for 5: return R
Computational framework for NGS(1)(3) a random subset of up to 50% of pools was selected, and in each of these pools all sequences originated from a random subset of up to 25% of samples were removed. The instances were processed by Algorithms 3 and 6 (resequencing was simulated by taking sequences from the corresponding individual samples). For all test instances all samples were inferred, and the quality of deconvolution was comparable with the quality without failures (Supplementary).
Experimental poolsExperimental pools (Section 2.4) were deconvoluted using Algorithm 3, and the obtained samples (further refered as p-samples) were verified by comparison with the individually sequenced samples (i-samples). 10 references were taken from each i-sample, and the correctness of deconvolution was assessed by finding the closest reference to each p-sample sequence. Sequences were aligned using Muscle (). In average, 259 unique haplotypes per p-sample were obtained, which exceeds the numbers obtained in other studies after the standard sequencing using 454 Junior System and error correction (). In total 99.96% (5463 of 5465) of reads were correctly assigned to samples. Two reads assigned to sample S 7 showed a higher similarity to a reference from S 6. The subsequent analysis showed that these reads are distant from S 6 and S 7 as well as from each other: minimum distance from these reads to haplotypes from S 6 and S 7 is 25 and 26, respectively, and the distance between them is 20, while the mean distance among haplotypes of i-samples S 6 and S 7 is 3.64 bp (std 1.21 bp) and 6.12 bp (std 5.25 bp), respectively. Therefore, these two reads are likely to be sequencing artifacts. In general, the percentage of haplotypes from i-samples found in p-samples was not high (), with an average of 14.66%. However, when the frequencies of these haplotypes were considered, the level of agreement was much higher, with an average total frequency of 56.94% (). In particular, all individually sequenced haplotypes with frequencies !10% and 72.73% of haplotypes with frequencies !5% were found in p-samples. In general haplotypes from i-and p-samples cover the same areas of the sequence space, although some branches are formed by variants sequenced in only one of experiments (). The differences between haplotype frequencies distributions for i-and p-samples were measured using Jensen-Shannon Divergence (JSD) and correlation coefficient (Corr) (). JSD varies from 0.15 (S 1 ) to 0.65% (S 7 ). A correlation between frequencies distributions is positive and statistically significant for all samples except S 7 , in which a large cluster of variants was not detected by the individual sequencing, but was found in the pooling experiment ().In this study, we present a novel computational framework for massive NGS of highly mutable viruses. To the best of our knowledge, this is the first pooling framework applicable for sequencing of viral quasispecies, which takes into account extensive heterogeneity of viral populations, the large number of distinct viral variants sequenced from each sample and the effects of PCR and sampling biases. The proposed strategy drastically reduces the cost of sequencing per specimen (Supplementary Section S5), while still providing sufficient amount of information in support of molecular surveillance and other applications of viral sequences in clinical and epidemiological settings. The framework is applicable to viral agents infecting humans and animals and, with further development of experimental protocols, it should serve as a cost-effective foundation for molecular surveillance of infectious diseases. Ultra-deep sequencing of viral samples produces a wide range of intrahost viral variants and allows for detecting even minor viral subpopulations. Pooling of several specimens reduces the depth of sequencing for each specimen, but this reduction is not detrimental, since each specimen is usually used in more than one pool. As specimen is tested more than once, the number of sequenced variants is increased, so representative sampling of viral subpopulations infecting each patient can be improved. The experiments conducted here showed that comparable subpopulations were recovered from individual specimens and from pools, at least at the pooling scale used in this study. Both individual sequencing and pooling produce sequences covering approximately the same areas of the sequence space, thus providing a consistent structure of a viral population. Repeat sampling from the same complex viral population often results in poorly matched sets of sequences, thus presenting a significant challenge to pools deconvolution. Such stochastic sampling has a potential to diminish the effectiveness of pool-sequencing and usefulness of the obtained sequences by impeding the correct allocation of sequences to samples, leaving some samples without sequences assigned or allocating only a fraction of the sequences. The clustering approach developed in this study significantly improves assignment of sequences to samples and, thus, not only substantially overcomes the aforementioned potential pitfalls, but converts stochastic sampling into an advantage. Clustering also eliminates the detrimental effect of NGS errors, since erroneous reads tend to concentrate around correct haplotypes. The sequencing cost and accuracy of deconvolution are two major measures of quality of our framework. These two measures are in conflict with each other: while increase in pool size improves cost-effectiveness of sequencing by reducing the number of sequencing runs, it reduces accuracy of deconvolution. Considering that deconvolution accuracy significantly depends on the genetic complexity of intrahost viral populations, an optimal pool size should be carefully selected for each virus and genomic region. In conclusion, success of the pool-based sequencing of viral populations depends to a significant degree on the efficacy of sequence assignments and the risk of under-representation of viral variants from some samples, owing to PCR and sample biases. The pool design and clustering algorithms presented here substantially minimize the detrimental effect of these biases on the sequencing quality. Further reduction of the biases using generalizations of error-correcting codes and optimization of experimental conditions may further improve the strategy, facilitating its application to molecular surveillance and study of infectious diseases.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
P.Skums et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
