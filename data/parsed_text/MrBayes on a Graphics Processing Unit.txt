Motivation: Bayesian phylogenetic inference can be used to propose a 'tree of life' for a collection of species whose DNA sequences are known. While there are many packages available that implement Bayesian phylogenetic inference, such as the popular MrBayes, running these programs poses significant computational challenges. Parallelized versions of the Metropolis coupled Markov chain Monte Carlo (MC 3) algorithm in MrBayes have been presented that can run on various platforms, such as a graphics processing unit (GPU). The GPU has been used as a cost-effective means for computational research in many fields. However, until now, some limitations have prevented the GPU from being used to run MrBayes MC 3 effectively. Results: We give an appraisal of the possibility of realistically implementing MrBayes MC 3 in parallel on an ordinary four-core desktop computer with a GPU. An earlier proposed algorithm for running MrBayes MC 3 in parallel on a GPU has some significant drawbacks (e.g. too much CPUâ€“GPU communication) which we resolve. We implement these improvements on the NVIDIA GeForce GTX 480 as most other GPUs are unsuitable for running MrBayes MC 3 due to a range of reasons, such as having insufficient support for double precision floating-point arithmetic. Experiments indicate that run-time can be decreased by a factor of up to 5.4 by adding a single GPU (versus state-of-the-art multicore parallel algorithms). We can also achieve a speedup (versus serial MrBayes MC 3) of more than 40 on a sufficiently large dataset using two GPUs.
INTRODUCTIONGiven the DNA sequences of several organisms, Bayesian inference can be used to infer their phylogenetic 'tree of life'. MrBayes () is a popular software package that implements the Metropolis coupled Markov chain Monte Carlo (MC 3 ) sampling method for Bayesian inference of phylogeny. * To whom correspondence should be addressed.Metropolis coupled MCMC is ideally suited to implementation on a parallel processing machine or even on a network of workstations (each processor being assigned one chain), since each chain will in general require about the same amount of computation per iteration, and interactions between chains are simple. (In fact, several modified versions of MrBayes MC 3 have been developed that allow the user to run multiple chains in parallel on multi-CPU-based hardware (cf. Section 2.1). The purpose of this article is to instead analyze the possibility of implementing MrBayes MC 3 in parallel on a graphics processing unit (GPU). Note that, in the rest of this article, whenever we discuss MrBayes, we implicitly refer to version 3.1.2 (the current version).
MrBayes MC 3 overviewMrBayes MC 3 is capable of Bayesian inference of phylogeny in a range of situations. In this article, we exclusively focus on the 4-by4 nucleotide substitution GTR+ and GTR+I+ models, which has the settings nucmodel = 4by4 nst = 6 and either rates = gamma or rates = invgamma (these are set with the lset command), and the format setting: datatype = dna We will now give a quick overview of this computation, a detailed description of which can be found in Huelsenbeck and Ronquist (2005). MrBayes MC 3 runs H Markov chains, each containing a proposed phylogenetic tree, which we denote  1 , 2 ,..., H. In each iteration, we randomly perturb each tree to give another list of trees  1 , 2 ,..., H and for each Markov chain i decide whether or not to replace  i with  i. Afterwards, we decide whether or not to swap the states of two chains j and k, where j and k are chosen at random. By default, MrBayes runs two independent analyses of the same data, which helps the user determine when an accurate tree is found. Each analysis runs H = 4 Markov chains, three heated chains and one 'cold' chain, which allows for more rapid mixing (). For each chain, a series of computations are performed in the order depicted in. The gray box indicates what is computed at a given stage, and the arrow indicates what information is required at a given stage. We let Q denote the (instantaneous) rate matrix and X denote the DNA sequence data of the taxa. Further, we letX u denote the DNA sequence data at site u and  m denote the base frequencies of nucleotide m {A,C,G,T }. We compute the conditional likelihoods using the recursive algorithm presented in Felsenstein (1981). The whole procedure is performed repeatedly, until after many generations (i.e. iterations) the cycle is broken. More generations imply a more reliable result.
J.Zhou et al.
GPU overviewModern GPUs have a large number of cores, and a single GPU can run tens of thousands of threads concurrently. Despite GPUs being originally designed for high-performance graphics processing, due to being powerful and relatively inexpensive, GPUs have started to be used in a vast range of scientific applications (). Moreover, a GPU can be conveniently added to an existing desktop computer, just as a typical graphics card. The NVIDIA Corporation introduced Compute Unified Device Architecture (CUDA) () which enables developers to write programs for the NVIDIA GPU using a minimally extended version of C language. It is arguably the most important reason for the prosperity of general purpose computing on GPUs. The GPU-side function of a CUDA program is called a kernel, which is executed by numerous GPU threads concurrently. The GPU threads executing the same kernel form one or more GPU thread blocks. The NVIDIA GPU has several different memory types with different behaviors. One is called global memory, which is the largest memory available but is also the slowest. All GPU threads have access to the same global memory. To transfer data between CPU and GPU, we must use global memory. Another memory type important for this article is shared memory, which is smaller but faster than global memory. Each GPU thread block has its own shared memory accessible to all threads of the block. The memory access procedure is, therefore, an important factor in the efficiency of a GPU-based algorithm. In particular, frequent global memory access will incur a performance drag. For this article, we exclusively use the NVIDIA GeForce GTX 480, which is priced at US$449.99 (amazon.com, December 2010). We have been quite selective in choosing this particular GPU for several reasons. For example (i) many GPUs are incompatible with CUDA, (ii) many GPUs are unsuitable for running MrBayes MC 3 due to inadequate ability to work with double precision floating-point arithmetic and (iii) some GPUs are simply too expensive.
PARALLEL MRBAYES MC 3A range of algorithms for implementing MrBayes MC 3 in parallel have been proposed which we will discuss in this section.), proposed a parallel version of the MC 3 method which allows any number of Markov chains to be run in parallel on any number of cores. These modifications were subsequently made for MrBayes MC 3 in, which we call hMC 3. Van der, presented a version of MrBayes MC 3 for networked computers, such as over the internet.
Multi-CPU-based architecture
GPU architectureA parallel version of MrBayes MC 3 was proposed in () and implemented on a range of platforms including the GPU; we call this program gMC 3 , which we have obtained via private communication with the authors. However, the largest overhead for the GPU systems is the transfer of the data to and from the graphics card. This results in a large penalty in the execution time to such an extent that for the 8800GT its execution time is at the end larger than the baseline. (The most time-consuming component of MrBayes MC 3 is the evaluation of the conditional likelihoods cl (see also the Supplementary Material). The single goal of gMC 3 is to distribute the computation of cl among GPU threads, ignoring all other influencing factors. When the CPUGPU communication overhead is taken into account, the algorithm's overall performance is poor. The aim of this article is, therefore, to resolve the problems with gMC 3 and consider the prospect of running MrBayes MC 3 on a GPU in a realistic setting.
Other GPU-based phylogeneticsIt is, however, already possible to run other packages for Bayesian phylogenetic inference on a GPU., implemented Randomized Axelerated Maximum Likelihood (RAxML) () on a GPU, although recent RAxML development seems to be focused on SSE3 technology., implemented a library, called beaglelib, for evaluating phylogenetic likelihoods on GPUs, which provides an open API for a range of phylogenetic softwares. It has recently been reported that future versions of MrBayes will encompass the beagle-lib, thus enabling GPU computing. However, it is still a work in progress, so we do not present a comparison with MrBayes+beagle-lib in this article.
Page: 1257 12551261
MrBayes on a GPU
AN IMPROVED ALGORITHM FOR THE GPU
OverviewWe will now offer some simple, but significant, improvements on the earlier proposed gMC 3 algorithm on the GPU. We call this improved algorithm nMC 3 and we depict the concept infor the i-th chain. Algorithm 1 is a pseudo-code description of one CPU process in nMC 3. In Algorithm 1, we also label where Stages 15 (as in
for all non-root nodes k do4.
for all discrete rates r do
5.Compute transition probability matrix
end for
7.
end for
for all non-leaf nodes k do12.
Call kernel to compute conditional likelihoods clfor all sites u, discrete rates r, and nucleotides n using Felsenstein's algorithm 13.We also note that, while the results of nMC 3 and MrBayes MC 3 will be similar provided enough generations are performed, they will not necessarily be identical.
end for
The nitty-grittyThe aim of this section is to give additional details of nMC 3 , expanding on the overview given in Section 3.1.
Parallelism overviewThe nMC 3 algorithm parallelizes MrBayes MC 3 at two levels. First, we distribute multiple Markov chains among multiple processes on the CPU side, using a message passing programming model. Secondly, on the GPU side, the task of computing a single cl is assigned to a single GPU thread. Within a thread block, we include the computation of cl for all nucleotides n, discrete rates r and as many sites u as possible. Moreover, we also compute L u on the GPU side using a similar task assignment as for cl, the purpose of which is to reduce the CPUGPU communication overhead (cf. Section 3.2.3). The management of the GPU threads for computing cl and L u is performed automatically by the GPU.
PipeliningIn nMC 3 , we employ pipelining, where each Markov chain is treated as a workflow as depicted in. A workflow consists of one CPU process and one GPU stream. Since we can only efficiently run one CPU process per available CPU core, CPU processes are formed from a collection of assignedPage: 1258 12551261
J.Zhou et al.Markov chains. From a workflow's perspective, it shares a single CPU process with other workflows (if necessary). For example, one such arrangement is depicted below, where 8 Markov chains 1,2,...,8 are used and 4 CPU cores a,b,c,d are available.The advantages are that (i) multiple workflows can perform their respective CPU-side tasks in parallel, (ii) among workflows assigned to the same CPU process, when a workflow is required to perform its GPU-side tasks, another workflow may continue to use the available CPU core and (iii) within a workflow, the CPU process and GPU stream can be run at the same time. Combined with NVIDIA's new generation Fermi architecture (employed by the GeForce GTX 480), it is therefore possible to simultaneously perform(1) parallel execution of CPU processes,(2) parallel execution of GPU streams (i.e. concurrent kernel execution) and(3) data transfers between CPU and GPU, thereby improving concurrency and overlapping CPUGPU communication with computation.
CPUGPU communicationIn each iteration of each Markov chain i, on the newly proposed phylogenetic tree  i , for non-leaf node k, site u, discrete rate r and nucleotide n  {A,C,G,T }, we are required to compute a conditional likelihood cl n = cl( i ,k,u,r,n) using Felsenstein's recursive algorithm, which requires knowledge of the corresponding transition probability matrices P L and P R and conditional likelihoods cl L b and cl R b , where b {A,C,G,T }, from the two child nodes of k. In nMC 3 , in each iteration of each chain, on the new tree: @BULLET we first compute the transition probability matrices P for each non-root node, then use only one data transfer to upload all of the P to the GPU global memory before commencing Stage 2.@BULLET The cl of the leaf nodes are determined by the DNA sequence data, which we transfer to GPU global memory during initialization. @BULLET Subsequently, we call a kernel to compute the site likelihoods L u on the GPU side. The base frequencies  m are uploaded to the GPU as kernel parameters. Then in each workflow, we transfer the results to the CPU side as a single batch.For gMC 3 , to compute cl = cl( i ,k,u,r,n) over all u, r and n at a non-leaf node k of tree  i , (i) the required P L and P R are computed on the CPU side and transferred together to the GPU side (utilizing linear memory), (ii) two data transfers retrieve all of the required cl L and cl R , respectively, from the CPU side and (iii) the newly computed cl are returned back to the CPU side. We approximate the number and size of transfers for a single generation in. However, we wish to caution the reader that this table is intended as a rough guide aimed to highlight the
Global memory versus shared memoryA standard technique in GPU programming is to transfer repeatedly used data to shared memory, thus reducing the total number of global memory accesses.@BULLET For fixed i, k and r, we can reuse P L and P R , for the computation of cl = cl( i ,k,u,r,n) over various n and u. @BULLET For fixed i, k, u and r, we can reuse cl L b and cl R b for b  {A,C,G,T }, for the computation of cl n over various n.With this in mind, inside a GPU kernel, nMC 3 first transfers the required P L , P R , cl L b and cl R b from global memory to shared memory, then accesses them from shared memory whenever required for some computation. This technique was also employed in gMC 3. Furthermore, in nMC 3 , a similar technique is also used during the computation of site likelihoods L u , when the base frequencies  m are repeatedly used.
SynchronizationBy synchronization, we refer to when some component of a program is paused and waits for some dependent computations or transfers before continuing. In nMC 3 :@BULLET Within a workflow. The CPU process is instructed not to begin Stage 4 until its corresponding GPU stream has returned the required L u. @BULLET Between two workflows. After Stage 5, two chains are chosen at random to swap their states (in practice, only the heat values of both chains need to be swapped). For this to occur, we only need to synchronize the two CPU processes where the two chains reside, respectively, while the remaining CPU processes canPage: 1259 12551261In gMC 3 , however, the CPU and GPU synchronize whenever there is a data transfer between them.
MrBayes on a GPU
EXPERIMENTS AND DISCUSSIONIn this section, we will give experimental results that indicate the performance and scalability of nMC
Architecture
DatasetsFor testing, we use several datasets that were used by the fourth author in phylogenetic research. Dataset 1 is a group of 18S rDNA from 26 representative species belonging to 13 different families of Trichophora (Insecta: Hemiptera: Heteroptera), which was used to study the evolutionary relationships among the main lineages of Eutrichophora (). Dataset 2 is a group of 18S rDNA from representatives of all groups at order level in Euhemiptera, which includes 33 family-level taxa, which was used to investigate the effect of rDNA length variation on alignment and phylogenetic reconstruction among the suborders of Hemiptera (). Dataset 3 is a group of metazoan 18S rDNA which includes 111 taxa at class or order level and was used to reconstruct the phylogenetic relationships between the phyla of metazoans. Dataset 4 is a group of eukaryotic 18S rDNA which includes 234 taxa at class or order level and was used to reconstruct the phylogenetic relationships between the kingdoms of eukaryotes. Dataset 5 is a group of 23S 28S rDNA from cellular organisms which includes 288 taxa at class or order level and was used to study the positional homology of nuleotides according to length variation in the secondary structure of corresponding rRNA. The results of Datasets 3, 4 and 5 are yet to be published. All five datasets are available for download with the implementation of nMC 3 .is the time taken to analyze the datasets on the platform described in Section 4.1 with (i) serial MrBayes MC 3 using 1 CPU process, (ii) either pMC 3 or hMC 3 (whichever is faster) using 4 CPU processes, (iii) gMC 3 using one GPU and one CPU process and (iv) nMC 3 using either one or two GPUs and 4 CPU processes. We run each experiment only once, since the difference in run-times between repeated experiments is negligible (at most 1%).Predictably, when using two GPUs the algorithm runs almost twice as fast as when using a single GPU.lists the various speedups, computed from the data in. Note that, we are unable to run dataset 5 using nMC 3 with 4 CPU processes and a single GPU due to insufficient GPU global memory.and 4, respectively, plot the speedup on the dataset consisting of the last a taxa of either Dataset 4 or Dataset 5, as a varies.plots the speedup on a group of simulated datasets of 60 taxa, which were generated with Seq-Gen version 1.3.2 (), consisting of b sites, as b varies. In each case, we run nMC 3 , gMC 3 , pMC 3 , or hMC 3 for 10000 generations. Furthermore, all the simulated datasets are also available for download with the implementation of nMC 3 .
Results
Run-time Included in
Speedup
Scalability
DiscussionFiguresTwo important factors that affect the performance of a parallel algorithm are communication overhead, where processes require data from other processes, and load imbalance, where one process takes significantly longer than another. As the problem size increases, we attribute these near-constant speedups to load imbalance for pMC 3 , and both load imbalance and communication overhead in hMC 3. For gMC 3 , communication overhead increases with the problem size, as does the time required for the component that is run in serial.
CONCLUSIONWe have analyzed the prospect of realistically implementing a GPU-based version of MrBayes MC 3 on an ordinary computer. For this research, we exclusively focused on the 4-by-4 nucleotide substitution GTR+ and GTR+I+ models, and used the NVIDIA GeForce GTX 480. We tested the algorithm on datasets of interest to biologists, along with a group of simulated datasets. The experiments suggest that a single GPU can improve the performance of MrBayes MC 3 by up to a factor of roughly 19 (versus serial MrBayes MC 3 ). Moreover, we can increase this speedup further by using additional GPU(s). @BULLET It is now possible to use a GPU to substantially reduce the run-time of MrBayes MC 3 .@BULLET For MrBayes MC 3 , achieving a speedup on a GPU will, for an appropriately sized dataset, be less expensive than achieving the equivalent speedup using multi-CPU-based hardware. Moreover, a GPU can be conveniently added to an existing desktop computer.As time progresses, we anticipate that the GPU will offer increasingly greater potential for Bayesian phylogenetic inference. However, we encountered several obstacles in this research that the reader should be aware of. @BULLET Currently, very few GPUs are suitable for running the proposed algorithm due to inadequate support for double precision floating-point arithmetic. NVIDIA's Fermi architecture is a recent technology that provides sufficient support for double precision floating-point arithmetic. @BULLET There is a restriction on the largest dataset that can be processed by a single GPU due to memory capacity. It is possible toPage: 1261 12551261
MrBayes on a GPUlessen this restriction by running the proposed algorithm with additional GPU(s) or on fewer CPU processes. @BULLET Only NVIDIA GPUs support CUDA, which enables general purpose applications to run on GPUs.Aside from the above, we implement the proposed algorithm only on Linux, although the source code is available for download and modification.
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from [13:22 1/4/2011 Bioinformatics-btr140.tex]
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
