Motivation: The post-genome era sees urgent need for more novel approaches to extracting useful information from the huge amount of genetic data. The identification of recurrent copy number variations (CNVs) from array-based comparative genomic hybridization (aCGH) data can help understand complex diseases, such as cancer. Most of the previous computational methods focused on single-sample analysis or statistical testing based on the results of single-sample analysis. Finding recurrent CNVs from multi-sample data remains a challenging topic worth further study. Results: We present a general and robust method to identify recurrent CNVs from multi-sample aCGH profiles. We express the raw dataset as a matrix and demonstrate that recurrent CNVs will form a low-rank matrix. Hence, we formulate the problem as a matrix recovering problem, where we aim to find a piecewise-constant and low-rank approximation (PLA) to the input matrix. We propose a convex formulation for matrix recovery and an efficient algorithm to globally solve the problem. We demonstrate the advantages of PLA compared with alternative methods using synthesized datasets and two breast cancer datasets. The experimental results show that PLA can successfully reconstruct the recurrent CNV patterns from raw data and achieve better performance compared with alternative methods under a wide range of scenarios. Availability and implementation: The MATLAB code is available at
INTRODUCTIONCopy number variations (CNVs) are genomic alterations characterized as abnormal number of copies in one or more segments of DNA. The size of segments varies from 1 kb to 3 Mb (). Studies have indicated that CNVs are associated with human diseases. For instance,found that individuals who carried fewer copies of gene CCL3L1 than average were significantly more susceptible to HIV. Each missing copy increases the susceptibility by 4.5 10.5%. Several other types of human diseases, such as cancers and autoimmune diseases, have also been linked with CNVs (). Therefore, it is important to investigate the contribution of CNV to complex diseases. Array-based comparative genomic hybridization (aCGH) is a high-throughput and high-resolution approach for measuring changes of copy numbers in thousands of DNA regions (). In a typical aCGH experiment, genomic DNAs are extracted from test samples and reference samples and differentially labeled with two dyes. The labeled DNAs are mixed together and hybridized to a microarray spotted with DNA probes. The ratio between the fluorescence intensity of the test DNA and that of the reference DNA at a given probe measures the ratio between the copy number in the test genome and that in the reference genome. Data extracted from an aCGH experiment are generally in the form of log 2 ratios. A value greater than zero indicates a possible gain in the copy number, while a value less than zero indicates a possible loss. Detection of CNVs from aCGH data is to locate changepoints in log 2-ratio profiles that partition each chromosome into discrete segments. Because of the high noise level in the intensity values of aCGH data, it is difficult to identify the boundaries of CNVs using the typical thresholding approach (). Many methods have been developed to analyze single-sample aCGH data, including breakpoint detection (), signal smoothing (Ben) and Hidden Markov Models (). Reviews and comparisons can be found inand Willenbrock and Fridlyand (2005). However, finding CNVs from single samples is only the initial step in the search of disease-associated genes. It has been pointed out that the recurrent CNVs (CNVs at the same genomic locations appearing frequently over multiple individuals) are more likely to encompass disease-critical genes (). Recently, an increasing number of methods have become available for finding recurrent CNV regions from aCGH data of multiple samples. A recurrent CNV region is often defined as a set of consecutive probes that are altered in a group of samples (). Some methods identify recurrent CNV regions via hypothesis testing by comparing the frequency of alternations at each probe with a null distribution. Examples include. These methods rely on the preprocessing *To whom correspondence should be addressed. of single-sample profiles, including smoothing, segmentation and CNV calling, which may miss some weak but common variations (). Some other methods attempt to detect recurrent CNV regions directly from raw profiles through multiple-chain Hidden Markov Models (), joint segmentation () and matrix factorization methods (). Most of these methods assume a generative model that describes the characteristics of recurrent variations and fit the model under the assumption of Gaussian noise. However, these methods rarely consider the influence of individual-specific variations, which typically have large intensity values and cannot be modeled as Gaussian noise. As a result, individual-specific variations will be mixed with recurrent variations and eventually corrupt the model fitting. In this article, we develop a robust and efficient method to identify recurrent CNVs from raw aCGH profiles. The main contributions are summarized as follows:We formulate the problem as a matrix decomposition problem, where the raw data matrix is decomposed into a low-rank component, a sparse component and a noise component. These three components correspond to recurrent CNVs, individual-specific variations and random noises, respectively. The recurrent CNVs can be easily identified from the low-rank component by thresholding or more sophisticated statistical analysis. The proposed formulation is convex. An efficient and scalable algorithm is developed to find the globally optimal solution based on the state-of-art convex optimization techniques.The relationship between our model and other related models is explained via simulation of six popular scenarios. The rest of this article is organized as follows. In Section 2, we introduce our model, formulation and algorithm. In Section 3, we report experimental results. Finally, we conclude our article with some discussions in Section 4.
METHODS
FormulationIn Rueda and Diaz-Uriarte (2010), six popular recurrent CNV scenarios are defined in detail. Our method for detecting recurrent CNVs is motivated by the low-rank property of these scenarios as illustrated in. Basically, the number of recurrent CNV regions determines the rank of the matrix composed of multi-sample profiles. In Scenarios (1), (3) and (5), for example, the rank of the matrix equals to 1 if we assume the matrix values are 1, 1 and 0 for the probes marked as gain, loss and no alteration, respectively. In Scenarios (2) and (6), two recurrent regions exist and the rank is 2. In Scenario (4), the pattern is more complex and the rank is 3. Therefore, the problem of identifying recurrent CNVs can be treated as a problem of recovering a low-rank matrix from input data. Mathematically, we express each dataset by a matrix D 2 R np , where each row d i is a log 2-ratio profile of one sample, and n and p are the numbers of samples and probes, respectively. Our task is to recover a lowrank component X from D, such that rank X   is small. The typical approach for low-rank approximation is to compute the singular value decomposition of the input matrix and form a new low-rank matrix using the first few singular vectors (). This procedure is also named principal component analysis in statistics. The drawback of traditional principal component analysis is that it is only optimal under the assumption of Gaussian noise. However, aCGH data have large intensity values, which cannot be modeled as Gaussian noise. These individual-specific intensity values will severely corrupt the low-rank approximation and make the fitted model to deviate far away from the true model. Inspired byCand s et al.(2011), we propose to use the following decomposition model to detect the recurrent CNVs from noisy input:where X is a low-rank component, E is a sparse component and " is a noise component. In aCGH data analysis, the low-rank component corresponds to the recurrent CNVs. The sparse component corresponds to individual-specific CNVs or gross measurement errors that sparsely appear at different locations for different samples. The noise component corresponds to the small perturbation of the intensity value at each probe, which is often modeled by i.i.d. Gaussian distribution with a zero mean. To achieve the decomposition, the following minimization problem is considered:is the Frobenious norm and jEjj 0 is the ' 0-norm that counts the number of non-zero values in E. The solution to (2) will give a penalized maximum likelihood estimate with respect to the variables X, E,. The proposed model in (2) is intractable because both the rank operator and the ' 0-norm are combinatorial operators, which make (2) a NPhard problem. The traditional work-around for such problems is to use the convex relaxation. Therefore, we replace the rank by the nuclear norm, which is defined as jXjj   P r i1 i , where 1 ,    , r are the. The vertical axis indicates the sample index and the horizontal axis indicates the probe index. If the intensity ratio is piecewise constant along the probe axis for each sample, the intensity ratios of the entire dataset can be approximated by a low-rank matrix singular values of X. It is the tightest convex surrogate to the rank operator () and has been widely used for low-rank matrix recovery Cand s et al., 2011). We also replace the ' 0-norm by the ' 1-norm. The ' 1-norm is defined as jXjj 1  P i, j X ij. The ' 1 relaxation has proven to be a powerful technique for sparse signal recovery (). In addition, we like to introduce a smoothness penalty on each row of X to reflect the prior that the recovered profile should be piecewise constant. The total-variation norm, which is defined as jjxjj TV  P p i2 jx i  x i1 j, is adopted in the proposed model. Finally, the problem to be solved reads as follows:where x i is the i-th row of X. The equality constraint in (2) is eliminated by replacing " with D  X  E. The optimization in (3) is convex. Thus, the global optimal solution can be found. As the recovered X is the piecewise-constant and low-rank approximation of D, we name our model PLA. We will introduce the algorithm in the next subsection.
AlgorithmAlthough the problem in (3) is convex, it cannot be solved directly using generic convex optimization software such as CVX () because of the large size of our problem. In this article, we propose an efficient and scalable algorithm based on the alternating direction method of multipliers (). The first step is to separate the two non-smooth functions of X by introducing an auxiliary variable Z and rewrite (3) as follows:s:t: XZ 4 Next, the augmented lagrangian is introduced to eliminate the equality constraint in (4), which readswhere h, i denotes the inner product, Y is the dual variable and is an adaptively tuned parameter that controls the convergence of the algorithm. The final result of optimization will not be affected by if it is chosen properly. Please refer to () for more details. To solve (4), the following updating steps are alternated until convergenceThe iteration of above steps will finally converge to the solution of problem (4). The theoretical proof for the convergence can be found in (). The remaining issue is how to solve the steps in (6) to (8). The problem in (6) can be reduced to minthat becomes a nuclear-norm regularized least-squares problem and has the following closed-form solution ()where D refers to the singular value thresholding (SVT)Here, x   maxx, 0: fu i g, fv i g and f i g are the left singular vectors, the right singular vectors and the singular values of M, respectively. The problem in (7) can be rewritten as follows:Apparently, each row of Z can be updated separatelyProblem (14) is the fused lasso signal approximation problem that can be solved efficiently (). The problem in (8) can be rewritten as follows:It admits a closed-form solutionwhere S M ij  signM ij M ij    refers to the elementwise softthresholding operator (). Overall, the algorithm to optimize the proposed model in (3) is summarized in Algorithm 1. The convex program will give a global optimal solution independent of initialization.(1) Input: D
Parameter selection(2) Initialize all variables to be zero.(4) Update X by solving (10) via singular value thresholding.(5) Update Z by solving (13) via fused lasso solvers.(6) Update E by solving (15) via soft thresholding.(7) Update dual variable Y according to (9).(8) until convergence(9) Output: X and E adjusted slightly to obtain the best results in specific applications. m is the larger dimension of the input matrix. In our problem, m  p, i.e. the number of probes. In the synthesized experiments, we simply set  1 = ffiffi ffi p p. On real datasets, the recurrent CNVs rarely form a perfectly low-rank matrix, and we use  2 1 = ffiffi ffi p p to keep sufficient variations in X. The parameter 1 serves as a threshold in the SVT step in (11). It should be large enough to threshold out the noise but not too large to over-shrink the signal (). A proper value is 1   ffiffi ffi n p  ffiffi ffi p p , which is the expected ' 2-norm of a n  p random matrix with entries sampled from N 0, 2 . As CNVs are sparse in the data, we can estimate from the data by the median-absolute-deviation estimator (^  1:48 median jD  medianDj   17As 2 only controls the smoothness of the recovered profiles, we empirically set 2  0:01 1 .
RESULTS
Synthetic datasets3.1.1 Accuracy comparison In Rueda and Diaz-Uriarte (2010), six scenarios of recurrent CNVs are discussed (illustrated in). We adopt these six scenarios to generate synthetic data. For each scenario, 50 samples of aCGH profiles with a length of 200 probes are generated. The default signal value for no variation is set as 0. The recurrent variations are located in the interval from Probe 76 to Probe 125 with the patterns identical to the six scenarios given in. The log 2 ratio is 1 for a gain and 1 for a loss. For each sample, an individualspecific variation with a length of 20 probes is added at a random location that does not overlap with the recurrent region. The log 2 ratio of each individual-specific variation is randomly sampled from f2,  1, 1, 2g. Finally, we add i.d.d Gaussian noises to all probes.The synthetic dataset of Scenario 2 is illustrated in the first panel of. Each row of the matrix is a profile of a sample. There are two recurrent variation regions. The first one is located at Probes 76 $ 95, where 30 samples have gains. The second one is located at Probes 96 $ 125, where 20 samples have losses. The task is to recover the recurrent patterns from this matrix. Our results are given in the second and third images in. The recurrent CNVs are clearly presented in the lowrank component, while the individual-specific CNVs are mostly included in the sparse component. The rank of the recovered low-rank component is 6, which is slightly larger than the truth. We used the default parameter setting without any prior or tuning. The results of three closely related methods are given in the bottom row. TVSp () aims to detect all CNVs. Therefore, it cannot separate recurrent variations from individual-specific variations. MSSCAN () attempts to find the common change points across samples and uses the mean intensity in each segment to reconstruct the profiles. Consequently, the segmentation process is largely influenced by the individual-specific variations resulting in unfaithful reconstruction of recurrent patterns. FLLat () tries to recover the common features across profiles. However, the recovery is corrupted because FLLat adopts a least-squares loss to fit the model without considering the individual-specific signals. To quantitatively evaluate these methods, we calculate the true-positive rate (TPR) and false-positive rate (FPR) of recurrent CNV identification and plot the receiver operating characteristic curves (TPR versus FPR) under six scenarios with a noise level of  1. The receiver operating characteristics (ROC) curves are shown in. A curve closer to the top and left borders indicates a better performance. To illustrate the importance of smoothness penalty in PLA, we also give the result ofPLA without smoothness by setting  0 in (3). The ROC curves are produced by thresholding the recovered matrix with different thresholds. For Scenarios 1, 3 and 5, both PLA and FLLat achieve nearly perfect results. For Scenarios 2, 4 and 6, the performance of them drops while PLA apparently outperforms FLLat. The difference between these two groups of scenarios is that there is only one recurrent region in Scenario 1, 3 and 5, while the other three scenarios involve more complicated recurrent patterns. When raw aCGH data contains two or more recurrent regions, FLLat is more prone to be influenced by the individual-specific variations. The expected number of recurrent regions is given as an input in FLLat, while it is not required in our algorithm. In practice, it is unrealistic to have such information. MSSCAN cannot differentiate between the recurrent and individual-specific CNVs during the segmentation step. Therefore, the output segmented matrix will be influenced by the individual-specific CNVs in a way similar to what is shown in. Consequently, the corresponding ROC of MSSCAN is lower compared with PLA. It can also be observed that, in all six scenarios, removing the smoothness constraint will degrade the performance of PLA, which tells us that it is important to consider the smoothness constraint in the recurrent CNV identification.
Detection limit To illustratethe limit of population frequency for a variation being detected as a recurrent variation by our model, we simulate a dataset with 50 samples and a single variation region. The number of samples carrying the variation and the length of the variation are varying. To see whether the variation signal is detected, we calculate the relative difference between two matrices by jjX  X 0 jj F =jjX 0 jj F , where X is the recovered low-rank matrix and X 0 is the simulated variation signal. A smaller value indicates a better approximation to the variation signal by the recovered low-rank component. The results are summarized in. When the signal is strong (covering sufficient number of probes), it can be detected even if the recurrent frequency is small. For instance, the variation with a length of 10 probes is well detected if more than five samples carry it. When the signal is abrupt (covering few probes), it is unlikely to be detected even if the frequency is large because the signal is more likely to be included in the sparse component and the smoothness constraint will also prevent such signal from being detected. In practice, a spike-like signal is unlikely to be a true CNV. The results also depend on the relative weight between the low-rank term and the sparse term in our formulation. We used the default parameter setting in this experiment.
Real applicationsTo illustrate the applicability of our method in real cases, we have applied PLA to analyze two independent breast tumor datasets. We focused on the analysis of Chromosome 17, which has many frequently altered regions (Bekhouche et al.,The heat maps of input data are shown in the top panels in, followed by the low-rank and sparse components recovered by PLA. As we can see, the recurrent patterns are clearly presented in the low-rank components, while the individual-specific variations are mostly included in the sparse components. The rank of the low-rank component is 11 inand 55 in. To summarize the results, we calculated the frequency of gains G i and losses L i at each probe i bywhere T is a threshold that is set be 0.25 and 1 denotes the indicator function. The results are given in the fourth row in. The recurrent regions can be clearly identified from the frequency plots, which are mainly located in the chromosome regions 17q11.2, 17q12, 17q21.3-q22 and 17q25. These identified regions match the results from both references (). Many breast cancer-related genes are located in these regions. For example, genes ERBB2 and C17orf37 are located around Probe 3460 in, where a high peak appears in the frequency plot. Gene ERBB2 status is often used to indicate grades or stages of breast tumors. Gene C17orf37 is abundantly expressed in breast cancer and is claimed to be a tumor biomarker (). For simple. Only Chromosome 17 is shown here. From top to bottom are the heat map of the input matrix, the recovered low-rank component, the sparse component, the recurrent frequency of alternations by thresholding the output of PLA and the recurrent frequency of alternations by thresholding the output of MSSCAN, respectively. For the bar plots, positive and negative y-values correspond to gains and losses, respectively. A plot illustrating the conditions under which a variation is detected. A smaller value indicates a better detection comparison, the results using MSSCAN are shown in the bottom panels. The frequency plot of MSSCAN is obtained by thresholding the segmentation result of MSSCAN. Compared with MSSCAN, our result is more sparse but accurately captures the recurrent regions that have been verified to have high correlation with breast cancer using gene expression ().
Computational costWe test the algorithm on a desktop PC with a 3.4 GHz Intel i7 CPU and 8 GB RAM. For the dataset from Pollack et al.
DISCUSSIONIn this article, we propose a new method to identify recurrent CNVs via recovering a low-rank matrix from raw aCGH profiles. The proposed method models different scenarios in a single framework. With a convex formulation, our algorithm guarantees a global optimal solution. We demonstrate the ability of our method to separate recurrent CNVs from other variations in both simulated data and real data. The low-rank matrix output of our method can be used as an input to other recurrent regionfinding algorithms based on permutation test or other statistical analyses. After the decomposition, the individual-specific CNVs are mostly included in the sparse component, as shown in. However, the sparse component may also contain noise and measurement error. To precisely detect individualspecific CNVs, we may have to apply a postprocessing step to the sparse component. Another possible approach is to first use existing algorithms (e.g. fused lasso and TVSp) to remove noise in aCGH profiles and then apply our model to decompose the processed profiles into recurrent CNVs (low rank) and individual-specific CNVs (sparse).
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
X.Zhou et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
PLA at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
