Motivation: Although genome-wide association studies (GWAS) have identified many disease-susceptibility single-nucleotide polymorphisms (SNPs), these findings can only explain a small portion of genetic contributions to complex diseases, which is known as the missing heritability. A possible explanation is that genetic variants with small effects have not been detected. The chance is <8% that a causal SNP will be directly genotyped. The effects of its neighboring SNPs may be too weak to be detected due to the effect decay caused by imperfect linkage disequilibrium. Moreover, it is still challenging to detect a causal SNP with a small effect even if it has been directly genotyped. Results: In order to increase the statistical power when detecting disease-associated SNPs with relatively small effects, we propose a method using neighborhood information. Since the disease-associated SNPs account for only a small fraction of the entire SNP set, we formulate this problem as Contiguous Outlier DEtection (CODE), which is a discrete optimization problem. In our formulation, we cast the disease-associated SNPs as outliers and further impose a spatial continuity constraint for outlier detection. We show that this optimization can be solved exactly using graph cuts. We also employ the stability selection strategy to control the false positive results caused by imperfect parameter tuning. We demonstrate its advantage in simulations and real experiments. In particular, the newly identified SNP clusters are replicable in two independent datasets.
INTRODUCTIONFor the purpose of understanding complex diseases, genome-wide association studies (GWAS) use high-throughput technologies to assay hundreds of thousands of single-nucleotide polymorphisms (SNPs). By October 2010, about 700 GWAS covering 150 diseases * To whom correspondence should be addressed. and traits have been published and nearly 3000 SNPs have been reported as significantly associated with diseases or traits (). Despite of the success of GWAS, most of the findings only explain a small portion of genetic contributions to complex diseases. For example, 18 SNPs identified from type 2 diabetes (T2D) only account for 6% of the inherited genetic risk. This phenomenon is referred to as the missing heritability (). To find the missing heritability, some possible methods have been suggested from biological views (), such as detecting genetic variants with small or moderate effects, identifying rare genetic variants and structural variants that contribute to the disease risk and detecting genegene interactions underlying the human diseases (). To speed up the search of genetic variants that underly human diseases, more powerful computational and statistical tools are needed. It is typical to test about 500K SNPs simultaneously in GWAS. For large-scale inference, controlling the false discovery rate (FDR) turns out to be a powerful strategy (). The most commonly used FDR procedure is called BH procedure (), which is based on thresholding the ranked P-values. It has been observed that BH procedure is able to control the FDR at nominal level for casecontrol studies, but its power decreases as the dependency among SNPs increases (). A better FDR control can be achieved in SNP analysis by exploring the dependency structure (e.g. linkage disequilibrium). A direct simulation approach (DSA) based on multivariate normal distribution (MVN) was proposed in;. Although these studies primarily focused on datasets used in candidate gene studies, their work suggested that the block-wise strategy might be a possible approach for genome-wide studies. By extending the framework of MVN, a sliding-window approach was further proposed to account for the dependency among locally intercorrelated markers (). To be more adaptive to the dependency structure of the human genome, a hidden Markov chain Model (HMM), which is a more flexible structure compared with the block-wise and slidingwindow structures, was proposed to account for the dependence (). However, applying these methods to analyze real SNP datasets in GWAS is very time consuming. Moreover, some methods report many false positive results in real applications. Thus, finding the missing heritability is still a challenging issue.Page: 2579 25782585
CODEIn this article, we propose a computational method to detect disease-associated SNPs with relatively small effects:(1) The chance is less than 8% that a causal SNP is directly genotyped. The effect sizes of their neighborhood SNPs may become weak due to their imperfect linkage disequilibrium with the causal one ().(2) Except for a few risk SNPs with relatively large effects, most of the disease-associated SNPs show small effects (odds ratio< 1.5) (). Due to limited sample size, it is still statistically challenging to detect SNPs with small effect sizes even though they are directly observed.We assume that the disease-associated SNPs account for only a small portion of the entire SNP set (i.e. the sparsity assumption). Therefore, they can be considered as outliers. On one hand, the risk SNPs with small effects can only be detected as a group due to the limited sample size. On the other hand, adjacent SNPs tend to form a local SNP cluster due to the linkage disequilibrium (LD) structure of human genome. If there exists a causal SNP which is not directly observed, the test statistics of its neighboring SNPs would tend to be different from zero. These facts motivate us to impose a spatial continuity constraint for outlier detection. Hence, we formulate the problem as Contiguous Outlier DEtection (CODE) and exactly solve it by graph cuts. As the solution depends on two parameters for controlling sparsity and spatial continuity, we present our heuristics for these parameter tuning. We further use stability selection to reduce the effects of imperfect parameter tuning. The rest of this article is organized as follows. In Section 2, we present our formulation, algorithm and other issues such as parameter tuning and stability selection. Section 3 discusses some closely related methods. Section 4 reports experiment results. Finally, we conclude our article in Section 5.
METHOD
Problem statementSuppose we have a dataset with L SNPs in a casecontrol study. The z value of each SNP can be easily obtained using the Cochran-Armitage trend test or univariate logistic regression (see the Supplementary Material). Letbe a set of z values. For disease-unassociated SNPs, their z-values asymptomatically follow the standard normal distribution given a finite sample size. For disease-associated SNPs, their z-values will be more significantly different from zero as the sample size increases (). Based on the set of z values, the SNPs can be partitioned into two groups ():@BULLET The null group G 0 : SNPs are unassociated with the disease. @BULLET The non-null group G 1 : SNPs are associated with the disease. Our goal is to classify L SNPs into these two groups. Throughout this article, we will use the following norms of a vector x: x 0 denotes the 0-norm, which counts the number of non-zero entries.the 2-norm and the squared 2-norm, respectively.
FormulationWe make the following assumption: most SNPs come from the null group and the associated SNPs are considered as outliers of the entire SNP set. Thus, we formulate the above problem as an outlier detection problem using the mean-shift model ():where  ={ 1 , 2 ,.Since most SNPs come from the null group,  will be a sparse vector. This makes the decomposition in Equation (1) a well-posed problem. Correspondingly, we propose to solve the following minimization problem:
.., L } and ={Since we are interested in the associated SNPs, i.e. the non-zero entries of  , we introduce s = (s 1 ,...,s L ),s i {0,1},i = 1,...,L as the support of  :Suppose SNP A is a causal SNP of the disease, its z value z A will be significantly different from zero. As adjacent SNPs tend to form a local SNP cluster due to the block-wise structure of the human genome, the z-values of its neighboring SNPs also tend to be different from zero, but the magnitudes of these z values will be smaller than z A due to their imperfect LD with SNP A. In order to make use of the neighborhood information to increase the statistical power, it is necessary to introduce a spatial continuity constraint on s. To do so, we firstly rewrite Equation (2) as a minimization problem over s. It is easy to see that, as long as  i = 0, we must have  i = z i to minimize Equation (2). Thus, Equation (2) has the same minimizer as the following energy function:Let P s (z) be the orthogonal projection of z onto the support s,and P s  (z) be its complementary projection, i.e.Noticing that  0 ==s 0 , we can rewrite Equation (4) as:s.t. s i {0,1},i = 1,...,L.Notice that s i = 1 indicates the i-th SNP is detected as an associated one. Now we introduce a spatial continuity regularizer on s to model the LD effect of neighboring SNPs. We propose to solve the following optimization problem:s.t. s i {0,1},i = 1,...,L.The fused term  2 L1 i=1 w i |s i s i+1 | encourages the associated SNPs to be detected in a block-wise manner by penalizing the first-order difference. Our assumption is that the first-order model is a reasonably good approximation of an LD block. This is similar to the ideas of HMM and the fused Lasso (). We allow different weights between adjacent SNPs to accommodate different local LD effects. Specifically, let r i,i+1 be the correlation of SNP i and SNP i+1. Here we use r 2 i,i+1 as w i. In fact, r 2 i,i+1 is the composite LD value of SNP i and SNP i+1, which is a robust surrogate of the standard LD measure r 2 when the linkage phase is unknown ().
AlgorithmNow we investigate how to minimize the energy in Equation (7) over s. Noticing that s i {0,1}, the energy can be rewritten as follows:The above energy is in the standard form of the first-order Markov random fields (MRFs) with binary labels, which can be solved exactly in polynomial time using graph cuts (). Next, we will describe the energy minimization via graph cuts briefly. The idea is to represent the energy by a graph and minimize the energy by finding the minimum cut of the graph. An example of an undirected graph G is as shown in. Graph nodes correspond to variables s i  s, and two terminal nodes are added indicating binary labels: a 0-terminal and a 1-terminal. There are two types of undirected edges in G: the neighboring link that connects each pair of nodes corresponding to neighboring variables in s and the terminal link that connects each variable node to two terminal nodes. In order to construct a graph representing Equation (8), we assign a weight to each link as follows:where e {s i ,s i+1 } is the weight of the link between s i and s i+1 , e {s i ,0} is the weight of the link between s i and the 0-terminal and e {s i ,1} is the weight of the link between s i and the 1-terminal. A cut of G is to partition G into two subgraphs by 'cutting' some edges such that two terminals are separated completely. The cost of a cut is defined as the sum of the weights of the cut edges.shows an example of a graph cut, in which s 1 ,...,s i are connected to the 0-terminal and s i+1 ,...,s L are connected to the 1-terminal after the cut. When the weight is assigned as Equation (9), the cost of this cut is j=i, which exactly equals to the energy value in Equation (8) for s j = 0,j = 1,...,i and s j = 1,j = i+1,...,L. Generally, two facts can be proved (): (i) there is a one-to-one mapping between a cut and a configuration of s, since any s i will be connected to either the 0-terminal or the 1-terminal after the cut. (2) For the corresponding cut and configuration of s, the cut cost equals to the energy value in Equation (8). Note that the cut cost comes from three sources: if s i is connected to the 0-terminal, e {s i ,1} will be cut and 1 2 z 2 i is added to the cost. If s i is connected to the 1-terminal, e {s i ,0} will be cut and  1 is added to the cost. If neighboring s i and s i+1 are connected to different terminals, e {s i ,s i+1 } will be cut and  2 w i is added to the cost. The sum of these costs equals the energy in Equation (8). Thus, the minimizer of Equation (8) s * can be obtained by finding the cut with the minimal cost, which can be solved efficiently using the standard max-flow algorithm ().
Parameter tuningWe need to specify two parameters  1 and  2 in our method, where  1 controls the sparsity of s and  2 controls its spatial continuity. Correctly choosing  1 and  2 can reduce false positive result and also increase statistical power. When z 1 ,...,z L are independent and identically Gaussian distributed variables, then z i  N (0, 2 ),i = 1,...,L, a simple choice for  1 is  1 =  2log(L) which is derived in Donoho and Johnstone (1994). This choice is due to the fact that the expected maximum of |z i |,i = 1,...,L is approximately  2log(L) when z i  N (0, 2 ),i = 1,...,L. The above formula assumes that there are L independent SNPs. In reality, the effective number of SNPs is smaller than L due to the LD effect. Recently,used a permutation-based method to estimate the effective number of SNPs. According to their estimation, the effective number of the 2.7 million HapMap SNPs is about one million. Thus, We set  1 =  2log(L/E), where L/E is the effective number of SNPs withNext, we empirically choose  2 in the following way:  2 begins with a large value  max 2. Then we solve problem (7) using graph cuts and obtain the optimal solution of s. After that, we get the residual r = P s  (z) and compute the variance of r, denoted as var(r). We gradually decrease  2 such that var(r) gets close to  2. Specifically, when var(r) > 2 , we decrease  2 by a factor :  2   2. We repeat this process until var(r) 2 <. The idea behind this procedure is that the variance of z i ,i  G 0 (z values of the null group) should be close to  2. In our experiment, we set  max = 600 ( max 2 can be easily chosen as long as most of s are zero.  max 2 = 600 is satisfactory in general),  = 0.9 and  = 0.01. The remaining issue is about the parameter . Here we would like to estimate  using empirical Bayesian inference as proposed in Efron (2010). We assume z i  N (, 2 ),i  G 0. The choice of  = 0 and  = 1 implies that the theoretical null distribution is used in statistical inference. However, the value of  can be quite different in large-scale inference, as pointed out by. Instead of directly using the theoretical null distribution,proposed to estimate the empirical null distribution. Regarding to SNP data analysis, we employ the locfdr algorithm () to estimate  and , and remove the mean of z: z i  z i ,i. In practice, we find that the estimated  and  are very close to 0 and 1, respectively. This means that the empirical null distribution is very close to the theoretical null distribution. Our observation is consistent with the result inwhen the locfdr algorithm was applied to SNP data analysis.
Stability selectionAlthough the heuristic for parameter tuning sounds reasonable, it may produce too many false positive results. To reduce false positives, we employ the stability selection strategy () to reduce the effect of parameter tuning. Using stability selection, detection of a SNP as an associated SNP does not rely on a single run of a particular parameter setting, but depends on the probability that it is detected under model perturbation via subsampling. Specifically, for each subsamling round, we randomly sample half of the cases and half of the controls from the entire dataset. Half subsampling is very close to the Bootstrap method, which has been theoretically analyzed in; Friedman and Hall (2007). Let z * b denote the set of z values for the b-th subsampling. We can efficiently obtain z * b using the CochranArmitage trend test that can be implemented using Boolean operations (). After that, we run our model on z * b and obtain the support s * b. Note that s * i,b = 1 indicates that the i-th SNP is detected in the b-th subsampling. Therefore, the probability of the i-th SNP being detectedPage: 2581 25782585
CODEcan be easily obtained bywhere B is the number of subsampling. Typically, we set B = 100 as in Meinshausen and Buhlmann (2010). We can obtain a set of interesting SNPs as A  ={i :  i  }, where  is a threshold. The number of SNPs in A  is denoted as |A  |.
Estimate of FDRIt has been shown that the false positive error can be controlled using this strategy () for the Lasso model (). However, this theoretical result cannot be directly applied here due to our different formulation (7). Thus, we need to estimate the FDR of A  for a given threshold . Although we do not have independent hypothesis for each SNP, we can still useas a rough estimator for FDR (; Tibshirani and), where N  is the number of SNPs picked at threshold  under the null distribution. We can use permutation to obtain the number of SNPs picked under the null distribution. Specifically, for a given threshold , we do T permutations. During the t-th permutation, we permute the casecontrol label to generate a null dataset, denoted as D (t). Then, we run B times subsampling on D (t). For each subsampling, we use the heuristic for parameter tuning and solve problem (6). After B times subsampling, we obtain  (t) according to Equation (10). For the given threshold , we have Ai  }. Then the final estimation of FDR  is given bywhere | A | denotes the number of SNPs in A .
Analysis of multiple chromosomesChromosomes are inherited independently based on Mendel's Law for most diseases. In this article, we treat different chromosomes separately. Suppose there are K chromosomes. Let L k be the number of SNPs of the k-th chromosome and z (k) ={z} be their z values. We have K k=1 L k = L. There will be no fused terms between different chromosomes. Accordingly, we solve the following K optimization problems separately:For parameter tuning, we set  1 =  k log(L/2.7), where  k is estimated from the k-th chromosome. We use L rather than L k for setting  1 because we are analyzing multiple chromosomes. The strategy for tuning  2 is the same as what we described in Section 2.4.
RELATIONSHIP BETWEEN OUR METHOD AND OTHER METHODSFused Lasso () is a closely related method and it has been used in hot spot detection for CGH data (). To analyze the SNP data using the idea of the fused Lasso, the observed z values can be decomposed as:The fused Lasso () could be modified slightly as follows:where r i,i+1 is the correlation of SNP i and SNP i+1, sign(r i,i+1 ) is used to adjust the sign difference between  i and  i+1 , and w i is the weight for the ith fused term. Similarly, w i can be used to accommodate the local correlation structure of adjacent SNPs, e.g. w i = r 2 i,i+1. Notice that each element of  is a real number while each element of s in Equation (7) can only be 0 or 1. Interestingly, both the final solutions of Equation (15)   and of Equation (7)  s are piecewise constant. As pointed out by, the final solution of Equation (15)   is a biased estimation due to the shrinkage effect of 1 regularization (). Since s is discrete, our formulation (7) can be rewritten as 0 regularization:s.t. s i {0,1},i = 1,...,L.whereIt is known that 0 regularization and 1 regularization have different mathematical properties (): compared with 1 regularization, 0 regularization gives unbiased estimation but larger variance. The performance of these two formulations depends on the problem at hand. The locfdr method () extends the original FDR () to the local FDR using the two-group model (the relationship between local FDR and FDR is given in the Supplementary Material). It assumes that z i with i  G 0 comes from the null distribution f 0 (z| 0 ) with probability p 0 and others come from the alternative distribution f 1 (z| 1 ) with probability p 1 = 1p 0 , where  0 , 1 are parameters of the distributions f 0 and f 1. Under mild assumptions, p 0 , 0 ,p1, 1 can be accurately estimated from data. It turns out that this simple two-group model works well in practice. The local index of significance (LIS) method () can be considered as an extension of the locfdr method. Specifically, let I i be the group indicator of the i-th SNP, I i = 0 if i  G 0 and I i = 1 otherwise. HMM is used to model the dependence of the adjacent indicators I i and I i+1. It further assumes that z i with i  G 0 comes from the null distribution f 0 (z| 0 ) and others are from the alternative distribution f 1 (z| 1 ). All the parameters are estimated using expectation maximization (EM) algorithm.has shown that this model is particularly powerful for identifying non-null SNP clusters. We will compare CODE with these methods in simulation studies.
RESULTS
Simulation studiesIn this section, we compare our method with the locfdr method (), LIS () and the modified fused Lassoour simulation. This two regions are chosen by the following criteria: first, within the region, the LD structure is well maintained. Second, the two regions are distantly separated such that their causal signals do not interfere with each other. After analyzing the LD pattern of the WTCCC data, we choose the 2000-th SNP rs1575568 and the 5000th SNP rs2985526 to be the causal SNPs in our simulation. Then we generate casecontrol data using the following logistic regression model:where Y is the casecontrol label, X 1 ,X 2 are the genotype corresponding to 'rs1575568' and 'rs2985526', respectively. Here we have assumed the additive model of the disease risk on the log scale. In order to simulate small or moderate effects of the causal SNPs, we set  1 =  2 = 0.25,0.3,0.4 corresponding to odds ratio (OR) 1.28, 1.35, 1.49, respectively. For fixed  1 , 2 , we adjust  0 such that the simulated casecontrol data is balanced. In order to mimic the situation that the causal SNPs are not directly genotyped, we remove these two causal SNPs. To avoid the case that the causal SNPs have been well tagged, we further remove the SNPs which are almost perfectly correlated with them (r 2  0.95). When the causal variant is not included, the definition of true and false positives may be problematic. Here all the identified SNPs, which are close to and highly correlated with the causal one (0.5  r 2  0.95), are recognized as true positives, otherwise as false positives. The performance of these methods are show in. When the signal is moderate (OR = 1.49), all the methods shows similar performance. As the signal gets weaker and weaker, CODE is clearly the winner followed by the locfdr method and the modified fused Lasso. It is not surprising that CODE outperforms the locfdr method. On one hand, the spatial continuity constraint enables a SNP with the weak effect to be detected by borrowing the strength from its neighbors. On the other hand, this constraint prevents a strong noise signal from being detected if its neighborhood signals are all weak. However, it seems surprising that the modified fused Lasso does not perform as well as CODE since it also uses neighborhood information. Our explanation is based on the following facts: @BULLET The estimation obtained from 1 regularization is biased but with smaller variance. @BULLET The estimation obtained from 0 regularization is unbiased but with larger variance. For CODE, which uses 0 regularization, the large variance has been greatly reduced by the stability selection strategy, where subsampling and aggregation are used like 'Bagging' (). For the fused Lasso, the improvement of reducing variance is limited while bias remains.There is a possibility that we may underestimate the power of the modified fused Lasso due to parameter tuning. The heuristics of parameter tuning is not suitable for the modified fused Lasso. Thus, we choose Bayesian Information Criterion (BIC) to select a good model for the modified fused Lasso. Here arises the issue of the degree of freedom (DF). Recall that the number of non-zero blocks in the solutionsolution solution is defined as DF of the original fused Lasso (). Since a different weight w i is used for each fused term in the modified fused Lasso, the number of non-zero blocks can only be an approximation of the true DF. This inexact DF may lead to imperfect model selection with BIC. As we have used the stability selection strategy, which makes the final result insensitive to parameter tuning, the underestimation of the power of the modified fused lasso should be small. We do not show the result of LIS in the main article, because we find that LIS produces too many false positives (Supplementary Material). Probably, this is because the EM algorithm gets stuck in a local optimum when estimating the parameters in HMM.
Calibration of the threshold of the selection probability based on empirical studiesWe have proposed a procedure to estimate FDR () of our method in Section 2.6. For a given threshold , T permutations are involved (e.g. T = 100) to estimate FDR  , Page: 2583 25782585 CODEwhich is computationally intensive. A lower threshold  may produce more false positives and a higher threshold  corresponds to less false positives but may miss some true positives. Here we would like to conduct some empirical studies to connect the threshold  with the local FDR of the locfdr method. In practice, this empirical calibration may provide a guideline for users to choose a suitable threshold .Here we still use the WTCCC control dataset with 3000 individuals. Based on this real data, we run T = 1000 permutations. Specifically, at the t-th permutation, we randomly assign the case control label for each individual to generate the t-th null dataset D t , and then we obtain a set of z values of D t , denoted as z t. We run the locfdr method on z t and record the local FDR of z t , denoted as locfdr t. We also run our method on the null dataset D t by B = 100 subsampling, and then we obtain  t ().After all T permutations, we obtainAfter that, we can apply different local FDRs to locfdr and check how many false positives will be reported. Also, we can apply different threshold  to  and check the number of false positives.provides the calibration result using the SNP data from chromosome 10. Under the local FDR 0.2, which is the default setting of the locfdr method, we have observed 442 false positives. For our method, 441 false positives have been observed when  = 0.30. The correspondence also holds for some other thresholds, for example, local false positive rate 0.15 (256 false positives reported by the locfdr method) roughly corresponds to  = 0.36 (246 false positives). We have observed that this relationship is rather stable when other SNP datasets are used. Thus, we recommend  = 0.30 as the default setting for our method.
Computational timeComputational efficiency is very important in real applications. We test the running time of our method on a desktop computer. The results are shown in. The locfdr method has a computational advantage compared with CODE. It can finish the analysis of 30000 SNPs within 1 s after the z values are obtained. For the modified fused Lasso, we solve 100 combinations ofusing the split Bregman method () for each subsampling. It takes 2148 s for a dataset with L = 30,000,n = 5,000. LIS takes about 563 s to finish the analysis.L = 30 000,n = 5000 45 L = 10 000,n = 5000 13The reported CPU time is for B = 100 subsamplings. In each subsampling, it involves calculation of z values and solving multiple times of Equation (7) using graph cuts.Thus, CODE has computational advantages over the fused Lasso and LIS.
Experiments on two independent datasets of Crohn's DiseaseWe apply our method to two independent dataset of Crohn's Disease (CD). One CD dataset comes from WTCCC (), in which about 500K SNPs are genotyped in about 5K subjects. We apply the standard quality control procedure to preprocess this dataset. The number of remaining SNPs is around 360 000. The other CD dataset comes from, in which 308 332 autosomal SNPs were assayed on the Illumina HumanHap300 chip. After the same quality control strategy, the number of remaining SNPs is 291 964. We call this dataset as 'Duerr's CD dataset'. CODE and the locfdr method are applied to these two datasets. The results are listed in Tables 2 and 3. As we can see, these SNPs are identified by both methods and their corresponding genes are replicable in these two independent datasets. According to the data base search result from FunctSNP (), functional SNPs are indicated by '*' in the table. Genes IL23R, NOD2, ATG16L1 and ZNF365 have been confirmed to be Crohn's disease susceptibility genes (). Although the analysis result does not provide new discoveries, it demonstrates that CODE and the locfdr method are very powerful because these replicable signals have not been detected in the original analysis (). Regarding to Gene CYLD which is identified in these two datasets by both methods, our analysis result suggests that it might be a CD susceptibility gene although it has not been discussed in the literature. Moveover, CODE has identified some new signals which are replicable in these two independent datasets. These results are given in Tables 4 and 5. For Duerr's CD dataset, the locfdr method suggests that rs1000141 is associated with CD with local FDR 0.1723. Our method identifies this SNP with selection probability 0.59 and SNP rs3792091 with selection probability 0.47. These two SNPs locate in gene SAG. For the WTCCC CD dataset, our method also identifies SNPs (rs2304773-rs2241873) which locate in gene SAG with selection probability 0.56 while the locfdr method consider them as insignificant ones. It is interesting that SNP rs2304773 is a functional SNP according to the data base search result of FunctSNP (). Based on this finding, we conjecture that Gene SAG is associated with CD. In addition, our method also identifies some other SNPs which locate genes NKD1 and PLD5 in both datasets. The biological interpretation of these findings remains unclear. It would be of great interest if their biological functions could be investigated.The genes located by these SNPs are replicable in these two data sets. The columns are the SNP name of an identified SNP, its z value given by the Cochran-Armitage trend test, the selection probability of our method, the local false discovery rate given by the locfdr method, the SNP position, the gene in which the SNP located, the chromosome position of the gene. We run B = 100 subsampling in CODE to estimate the selection probability. The functional SNPs are indicated with * in the column 'SNP name'. 'DS' in the column 'Gene' indicates that the gene is at the down stream of the given SNP.'US' in the column 'Gene' indicates that the gene is at the upstream of the given SNP.
Page: 2584 25782585
C.Yang et al.
CONCLUSIONIn order to detect disease-associated SNP clusters, our formulation makes use of information among adjacent SNPs. It turns out that our formulation is an 0 regularization problem and it can be exactly solved by graph cuts. We have shown that this method is particularly powerful when the effective size is small or moderate, which may be of great help in finding the missing heritability. Using two independent CD datasets, we demonstrate that CODE is able to reliably detect weak signals. CODE begins with z values assumed to be normally distributed. When applying CODE to statistics with other distributions, they need to be transformed to be normally distributed as what has been done for the locfdr method ().
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Notice that the change of E only makes  1 slightly different, e.g. when L = 30 000 and  = 1, we have  1 4.32,4.29 and 4.22 for E = 2.7,3 and 4, respectively. In this sense, the increasing number of reported SNPs and the correspondingly change of E do not make  1 too much difference.
