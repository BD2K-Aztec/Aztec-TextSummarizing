Motivation: It is commonly assumed in pattern recognition that cross-validation error estimation is 'almost unbiased' as long as the number of folds is not too small. While this is true for random sampling, it is not true with separate sampling, where the populations are independently sampled, which is a common situation in bioinformatics. Results: We demonstrate, via analytical and numerical methods, that classical cross-validation can have strong bias under separate sampling , depending on the difference between the sampling ratios and the true population probabilities. We propose a new separate-sampling cross-validation error estimator, and prove that it satisfies an 'almost unbiased' theorem similar to that of random-sampling cross-validation. We present two case studies with previously published data, which show that the results can change drastically if the correct form of cross-validation is used. Availability and implementation: The source code in C+ +, along
INTRODUCTIONThe most important property of a classifier is its error rate (probability of misclassification) because the error rate quantifies the predictive capacity of the classifier. If the feature-label distribution is known, then the true error can be found exactly; however, in practice, the feature-label distribution is unknown and the error must be estimated. If the sample is small, then the estimation must be computed using the same data as that used for training the classifier. Perhaps the most commonly used training data-based classification error estimator is cross-validation. It has a long history going back to 1968 (). In its most basic form, the k-fold cross-validation error estimate, ^ " cvk n , for a sample of size n (it is assumed that k divides n) is computed by selecting randomly a partition of the sample into k data 'folds' (subsets), for each fold applying the classification rule on the data not in the fold, computing the error rate of the designed classifier on the left-out fold, and then averaging the resulting k error rates. When k = n, one gets the leave-one-out estimator, ^ " l n. Cross-validation's salient good property is that, under random sampling, it can be proved (see) that it is 'almost unbiased', in the sense thatwhere " n is the true error (probability of misclassification) of a classifier designed on a sample of size n. Hence, the bias is not too great as long as n/k is small. For leave-one-out, E^ " l n =E" n1 , and the estimator is essentially unbiased. The salient point motivating the present article is that (1) depends on the sampling being random, and that when sampling is not random, there can be severe bias. The importance of bias for an arbitrary error estimator ^ " n can also be gleaned from its role in the estimator root-mean-square error:where Bias^ " n =E^ " n  " n  and Var dev ^ " n =Var^ " n  " n  (Braga). As mentioned previously, for classical cross-validation under random sampling, it follows from(1) that, if n/k is small, then Bias^ " n  % 0, in which case RMS^ " n  % Var 1=2 dev ^ " n . While the variance of CV is known to be large in small-sample cases (Braga), it will typically reduce to zero as n ! 1 (). However, the bias introduced by application of the classical CV estimator under nonrandom sampling will generally not approach zero as n ! 1. The result is an inconsistent estimator, which is imprecise under arbitrarily large sample sizes. Under random sampling, an independent and identically distributed (i.i.d.) sample S is drawn from the mixture of the populations  0 and  1. This means that if a sample of size n is drawn for binary classification, then the numbers of sample points n 0 and n 1 drawn from the populations  0 and  1 , respectively, are random variables n 0 $Binomialn; c and n 1 $ Binomialn; 1  c, where c=PY=0 is the a priori probability that the label Y is zero, i.e. the sample point comes from population  0. This random-sampling assumption is so pervasive that it is usually assumed without mention and in books is often stated at the outset and then forgotten. For instance,state, 'In typical supervised pattern classification problems, the estimation of the prior probabilities presents no serious difficulties'. They are referring to the fact that the prior probability c=Pr Y=0 can be consistently estimated by the sampling ratio, ^Numbers: n0 n ! c in probability. However, suppose the sampling is not random, in the sense that the ratios r= n0 n and 1  r= n1 n are chosen before the sampling procedure. In this separate-sampling case, S=S 0 [ S 1 , where the sample points in S 0 and S 1 are selected randomly from  0 and  1 , but given n, the individual class counts n 0 and n 1 are not determined by the sampling procedure. With separate sampling, we have no sensible estimate of c. Recognition of this particular problem of estimating the prior probability when sampling is separate and its effect on linear discriminant analysis (LDA) goes back to 1951 (). Often, one says that for separate sampling the ratios r= n0 n and 1  r= n1 n are chosen 'prior to' the sampling procedure. But there is in fact no temporal meaning to this. For instance, one could simply separately randomly sample  0 and  1 with n 0 and n 1 being randomly selected by a process independent of the sampling procedure, and the sampling would still be separate. The point is that r cannot be reasonably used as an estimate of c.(taken from Esfahani and Dougherty, 2014) illustrates the effects of separate sampling on the expected true classifier error for two classification rules and multivariate Gaussian distributions of equal and unequal covariance structures and dimensionality d = 3. For a given sample size n, sampling ratio r, and classification rule, the expected true error rate E" n jr is plotted for different class prior probabilities c, for LDA and a non-linear radial basis function support vector machine (RBF-SVM). For each r and n, n 0 is determined as n 0 =dnre. We observe that the expected error is close to minimal when r = c and that it can greatly increase when r 6  c. This kind of poor performance for separate sampling ratios not close to c is commonplace (). In this article, we investigate the effect of separate sampling on cross-validation error estimation. We will see that for a separatesampling ratio r not close to c there can be large bias, optimistic or pessimistic. A serious consequence of this behavior can be ascertained by looking at. Whereas the expected true error of the designed classifier grows large when r greatly deviates from c, a large optimistic cross-validation bias when r is far from c can obscure the large error and leave one with the illusion of good performanceand this illusion is not mitigated by large samples! To overcome the bias problem for classical crossvalidation with separate sampling, we introduce a new crossvalidation estimator designed for separate sampling and prove that it satisfies a bias property analogous to (1).
SYSTEMS AND METHODS
Discriminant analysisWe treat classification via discriminants to facilitate demarcation of the individual contributions of the class-conditional distributions to the error analysis. A sample-based discriminant is defined as a (measurable) function W n : S  <, where from the definition, we see that we actually have a family of discriminants indexed by n. A discriminant W n defines a classification rule via  n SX= 1; W n S; X 0 0; otherwise ;where X comes from either  0 or  1. Because any classification rule  n can be expressed as a discriminant via W n S; X=I nSX=0  I nSX=1 , where I A is the indicator function, discriminant analysis is completely general. We assume a common sense property of discriminants, that the order of the sample points within a sample does not matter. With separate sampling, there are two separate samples S 0 =fX 1 ;. .. ; X n0 g and S 1 =fX n0+1 ;. .. ; X n0+n1 g from populations  0 and  1 , respectively. To demarcate the separatesampling case from the random-sampling case, we will write the discriminant and corresponding classifier by W n0;n1 S 0 ; S 1 ; X and  n0;n1 S 0 ; S 1 ; X, respectively, with the latter defined in the same manner as (2) with W n0;n1 S 0 ; S 1 ; X replacing W n S; X: The true classification error with random sampling is given bywhere
Classical cross-validation error estimationFor U & f1;. .. ; ng, let S U denote the sample S with the points indexed by U deleted, and define W U n S; X=W nm S U ; X ; 7 where jUj=m is the size of U. Now let k divide n and consider a (random) partition fU i ; i=1;. .. ; kg of f1;. .. ; ng. Then the classical k-fold cross-validation estimator is given byIf k = n, this reduces to the leave-one-out estimatorwhere we have omitted the braces around the singleton index set {i}. Using the classical definition of cross-validation, (1) does not hold with separate sampling, in general. To demonstrate this, let N 0 = X n i=1 I Yi=0 be the (random) number of points from population  0 in the sample S; the expected cross-validation error rate under separate sampling is E^ " cvk n jN 0 =n 0 . For simplicity, we consider leave-one-out cross-validation. From (9),On the other hand, it follows from(3) that E" n1 jN 0 =n 0 =c E" 0 n1 jN 0 =n 0  +1  cE" 1 n1 jN 0 =n 0  =c E" 0 n0;n11 +1  cE" 1 n0;n11  : 11
Cross-validation for separate samplingTo adapt cross-validation to separate sampling, let U & f1;. .. ; n 0 g, let V & fn 0 +1;. .. ; n 0 +n 1 g, let S U 0 and S V 1 denote the samples S 0 and S 1 , with the points indexed by U and V deleted, respectively, and defineIf c is known (or known to a high degree of accuracy), then one can use it in (14). If c is unknown, then there is no proper cross-validation estimator of the overall error rate. If k 0 =n 0 and k 1 =n 1 , then the k 0 ; k 1 -fold cross-validation estimators defined previously reduce to separate-sampling leaveone-out estimators:Therefore, from(13),In the case of the separate-sampling leave-one-out estimator defined in (16), the preceding theorem reduces to E^ " l n0;n1 =E" n01;n11  : 19
RESULTS AND DISCUSSION
Simulation study with synthetic and real dataWe have performed a set of experiments using both synthetic models and real data to examine the behavior of classical and separate-sampling cross-validation under separate sampling. Throughout we use 5-fold cross-validation. We consider four well-known classification rules: LDA, Quadratic Discriminant Analysis (QDA), Linear Support Vector Machine (L-SVM) and RBF-SVM (see the Supplementary Material for definitions of these classification rules). To generate synthetic data, we use a model with class-conditional 3-dimensional Gaussian distributions, Nl y ; S y , y = 0, 1, where l 0 =0; 0;. .. ; 0; 0; l 1 =0; 0;. .. ; 0;  and S y has 2 on the diagonal and y off the diagonal. The pair  0 ; 1  can take on the values (0.8, 0.8) or (0.8, 0.4). We set so that the Mahalanobis distance between the classes for equal covariance matrices and the Bhattacharyya distance between the classes for unequal covariance matrices is 3. We consider n = 80 and n = 1000, so that we can compare small-sample and largesample results. We consider four public microarray real datasets: pediatric acute lymphoblastic leukemia (ALL;), acute myeloid leukemia (AML;), multiple myeloma () and breast cancer ().provides a summary of these real datasets, including the total number of features and sample size. For a detailed description of the data preparation, the readers are referred to the Supplementary Materials. The experiments on real data are essentially similar to those on synthetic data except that in real data experiments we use t-test feature selection to reduce the dimensionality to d = 3. In real data experiments, we consider only n = 80, which allows sufficient data for holdout error estimation. All experiments are performed for a range of r= n0 n 2 0:15; 0:85. We fix n and determine n 0 according to n 0 =dnre. At each iteration, S 0 and S 1 are randomly picked from either a synthetic model or real data to train the classifier and compute the two cross-validation estimates. Finding the bias requires knowing the true error, which is estimated on 5000 independent sample points from the synthetic distributions, or held out points in the case of real data; however, owing to separate sampling the ordinary holdout method cannot be applied, and we use separate-sampling holdout as explained by Esfahani and Dougherty (2014). We consider c=0:001; 0:1; 0:3; 0:4; 0:5; 0:6; 0:7; 0:9; 0:999. For each classification rule, we repeat the process of obtaining the true error and its estimates 4000 times for each value of r and c to obtain a distribution of estimates and true errors from which to compute the bias. In, we provide the results for the synthetic data with unequal covariance matrices [ 0 ; 1 =0:8; 0:4] for n = 80, 1000 and for two of the real datasets (). The complete set of results is given in the Supplementary Material. In the figure, from left to right, the columns correspond to LDA, QDA, L-SVM and RBF-SVM, respectively. The top two rows of the figure correspond to the real data from, and the third and fourth rows correspond to the synthetic data with n = 80 and n = 1000. The x-axis corresponds to the sampling ratio r, the y-axis gives the bias, the solid lines are for the proposed separate-sampling cross-validation, the dashed lines are for classical cross-validation, and the colors code the value of c. The trends are consistent across all experiments (including those in the Supplementary Material): (i) for classical crossvalidation with c near 0.5, there is significant optimistic bias for large jr  cj; (ii) for classical cross-validation with small or large c, there is optimistic bias for large jr  cj and pessimistic bias for small jr  cj as long as jr  cj is not very close to 0; (iii) for separatesampling cross-validation, estimation is slightly optimistic and almost unbiased across the range of jr  cj. Combined with the results of Esfahani and Dougherty (2014), the bias behavior of classical cross-validation is especially harmful for large jr  cj because it masks the increase in classifier error that occurs for large jr  cj, as shown in. Furthermore, although the deviation variance of classical cross-validation can be mitigated by large samples, the bias issue generally remains just as bad for large samples.
Two case studiesTo further illustrate the effects of separate sampling on classical cross-validation bias, we consider two published studies. The first () uses a colon microarray dataset containing gene-expression measurements taken from 2000 genes for 62 tissue specimens, 40 tumorous tissues (class 0) and 22 normal tissues (class 1). Using the SVM-RFE classification rule (), the authors split the data into a training and a test set, each including 31 specimens, by sampling without replacement, such that the training data contain 20 tumorous and 11 normal specimens. They compare the 10-fold cross-validation error using (8) to the standard holdout estimate obtained by counting the errors on the test set. But the standard holdout estimate is unbiased under random sampling, not separate sampling. For the latter, holdout estimation must take into account the value of c to be unbiased (). Assuming the classifier is applied to the US population, based on the incidence rate of colorectal cancer among the US population, which is 40/100 000 (), c=40=100 000. The black solid and dotted curves inerror, while the proposed cross-validation scheme is almost unbiased. In the second case study, we use the Parkinson's dataset used by. This dataset contains 22 biomedical voice features and 195 measurements in which 48 belong to individuals with Parkinson (class 0) and 147 measurements are taken from healthy individuals (class 1), so that r=0:246. The authors use this dataset to construct classifiers for diagnosis of Parkinson's disease based on distorted voice features. Four classifiers are constructed: naive Bayes (NB;), C4.5 (), kNN (k = 5) () and RBF-SVM. Althoughhave reported the estimated classical cross-validation error on a single sample of the data, we repeat the sampling procedure 200 times to get an estimate of the expected cross-validation error using both the classical (8) and the corrected cross-validation scheme (14). We assume the prior probability c of Parkinson's disease is determined by the incidence rate of Parkinson's disease across the USA, which is 13.4/100 000 (Van Den). In, the white bars are the expected classical cross-validation error rates; the shaded bars are the estimated error rates using the separate-sampling cross-validation scheme. The bars show the averaged estimated error obtained on 200 samplings of the data. The behavior observed inmakes it plausible that the error estimates for classical cross-validation will exceed those of separate-sampling cross-validation, which is nearly unbiased. This is true in all cases except for NB. However, if we look carefully at, we see that the point at which the bias becomes optimistic (for increasing r) can be well left of 0.5. This point is affected by the covariance structure and the classification rule. In this case, for NB, it is to the left of 0.246.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
U.M.Braga-Neto et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Cross-validation under separate sampling at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
CONCLUDING REMARKS We show in this article that classical cross-validation may display substantial bias when it is applied in the separate sampling scenario, which is common in biomedical studies. If one wishes to use cross-validation with separate sampling, then one should use the separate-sampling version of cross-validation, which is proposed here, or else, significant bias may result. This means that one must know the prior probability c (at least a good approximation of it). A similar requirement was made by Esfahani and Dougherty (2014) to ensure proper performance of the classification rule. Using a sampling ratio significantly different from c will result in poor classifier design and, often, optimistic bias to obscure the poor design. As concluded by Esfahani and Dougherty (2014), given the ubiquity of separate sampling in biomedicine, although it would incur some cost, it would behoove the medical community to gather population statistics so that accurate estimates of prior class probabilities would be available. In the absence of such statistics, separate sampling should not be used. Funding: This work was supported by NSF award CCF-0845407 (Braga-Neto) and NIH grant 2R25CA090301 (Nutrition, Biostatistics and Bioinformatics) from the National Cancer Institute. Conflict of interest: none declared.
