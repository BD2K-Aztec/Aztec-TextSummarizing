Motivation: Error correction is critical to the success of next-generation sequencing applications, such as resequencing and de novo genome sequencing. It is especially important for high-throughput short-read sequencing, where reads are much shorter and more abundant, and errors more frequent than in traditional Sanger sequencing. Processing massive numbers of short reads with existing error correction methods is both compute and memory intensive, yet the results are far from satisfactory when applied to real datasets. Results: We present a novel approach, termed Reptile, for error correction in short-read data from next-generation sequencing. Reptile works with the spectrum of k-mers from the input reads, and corrects errors by simultaneously examining: (i) Hamming distance-based correction possibilities for potentially erroneous k-mers; and (ii) neighboring k-mers from the same read for correct contextual information. By not needing to store input data, Reptile has the favorable property that it can handle data that does not fit in main memory. In addition to sequence data, Reptile can make use of available quality score information. Our experiments show that Reptile outperforms previous methods in the percentage of errors removed from the data and the accuracy in true base assignment. In addition, a significant reduction in run time and memory usage have been achieved compared with previous methods, making it more practical for short-read error correction when sampling larger genomes. Availability: Reptile is implemented in C++ and is available through the link: http://aluru-sun.
INTRODUCTIONHigh-throughput sequencing is profoundly changing the way genetics data are collected, stored and processed (). The advantages of the new technology have led to revitalization of old techniques and discovery of novel uses, with growing applications in resequencing, de novo genome assembly, metagenomics and beyond (). New technology inevitably comes with challenges. For many next-generation sequencers, the advantage of deeper and cheaper * To whom correspondence should be addressed. coverage comes at the cost of shorter reads with higher error rates compared with the Sanger sequencing they replace. Genome assembly, the de novo inference of a genome without the aid of a reference genome, is challenging. Sanger reads, typically 7001000 bp in length, are long enough for overlaps to be reliable indicators of genomic co-location, which are used in the overlap-layout-consensus approach for genome assembly. However, this approach does poorly with the much shorter reads of many next-generation sequencing platforms (e.g. 35100 bp for Illumina Genome Analyzer II). In this context, de Bruijn graph () and string graph () based formulations that reconstruct the genome as a path in a graph perform better due to their more global analysis and ability to naturally accommodate paired read information. As a result, they have become de facto models for building short-read genome assemblers, e.g. ALLPATHS (), Velvet (), ABySS () and Yaga (). Error correction has long been recognized as a critical and difficult part of these graph-based assemblers. It also has significant impact in other next-generation sequencing applications such as resequencing. We give a brief review of several well-known error correction methods. Alignment-based error correction methods, such as MisEd () for Sanger reads, require refined multiple read alignments and assume unusually isolated bases to be read errors. Like the Sanger-motivated assembly algorithms, these approaches do not adapt well to short reads. Hence,proposed the spectral alignment problem (SAP): in a given dataset, a kmer is considered solid if its multiplicity exceeds a threshold, and insolid otherwise. Reads containing insolid kmers are corrected using a minimum number of edit operations so that they contain only solid kmers post-correction. Similar approaches have been adapted and used by others (). To overcome the typically long run times of SAP-based approaches,proposed SHREC, a method based on a generalized suffix tree constructed from short-read data using both forward and reverse complementary strands. SHREC compares the multiplicity of a substring, represented by a node in the suffix tree, with its expected frequency of occurrence calculated analytically, assuming uniform sampling of the genome and uniformly distributed sequencing errors. The nodes with observed counts that deviate beyond a tolerable threshold from their expected values are considered erroneous. An erroneous node is corrected to a sibling when applicable, and all its descendants are transferred to the selected sibling. Well-engineered code is necessary to cope with the largePage: 2527 25262533
Reptilememory requirement of the suffix tree data structure. Unlike these general purpose error correction methods, FreClu () targets transcriptome data. The error rates for each position of a read are estimated in the same experiment via a set of control reads of a known bacterial artificial chromosome (BAC) sequence. Reads are clustered using the estimated error rates, and after error correction, FreClu could map 5% more reads back to the reference genome. Error correction of short-read data is particularly challenging because of the massive datasets, non-uniformly distributed read errors introduced at relatively high rates, and non-uniform coverage of the target genome. Next-generation short-read sequencers produce hundreds of millions of reads in a single run, and this trend of fast, massive data generation is continuing to accelerate. To process these data, even an efficient linear space algorithm could easily exceed available memory on a standard desktop computer. The high rate of sequencing errors also significantly increases memory usage due to the introduction of numerous erroneous kmers that are not present in the genome. Errors are typically identified as the unusual reads or kmers occurring less frequently than a threshold calculated under the assumptions of uniform coverage and error distribution. Neither assumption holds true in real data, leading to an excess of false error predictions. In this article, we present Reptile, a scalable short-read error correction method that effectively addresses the above challenges. We draw upon the k-spectrum approach pioneered in earlier results, but explore multiple alternative kmer decompositions of an erroneous read and use contextual information specified by neighboring kmers to infer appropriate corrections. Reptile also incorporates quality score information when available. We present algorithmic strategies to store kmer Hamming distance graphs and efficiently retrieve all graph neighbors of a kmer as candidates for correction. We compare our results with SHREC (), a high-quality short-read error correction method, and one of the more recent in a line of continuously improving error correction protocols. In all experiments with Illumina datasets, Reptile outperforms SHREC in percentage of errors corrected and accuracy of true base assignment. Futhermore, a significant reduction in memory usage and run time makes Reptile more applicable to larger datasets. As with most current approaches including SHREC, Reptile is targeted to short reads with substitution errors, assuming insertion and deletion errors are rarely produced by short-read sequencing technology ().
NOTATIONSLet R ={r 1 ,r 2 ,...,r n } be a collection of short reads sequenced from genome G. For simplicity (but without loss of generality), we assume each read r i has a fixed length L. The coverage of the genome by the reads is given by Cov = nL |G| , where |G| denotes the genome length. Define the k-spectrum of a read r to be the set r k ={r[i : i+k 1]|0  i < L k +1}, where rdenotes the substring from position i to j in r. We index the positions of a string starting from 0. The kspectrum of R is given bywhether or not they occur in R k .
METHODSThe success of any error correction method relies on an adequate coverage of the target genome. If we know the genomic location of every read, we could layout all reads that contain a specific genomic position into a multiple alignment () and correct all erroneous bases to the consensus base under the reasonable assumption that errors are infrequent and independent. For instance, base T in r 3 would be considered a sequencing error to be corrected to the consensus base A. The main idea underlying Reptile is to create approximate multiple alignments, with the possibility of substitutions, in the absence of location information. Multiple alignments with substitutions could be created by considering all reads with pairwise Hamming distance less than some threshold, but such alignments are already hard () and even in high coverage situations, the occurrence of many exactly coincident reads, e.g. r 0 and r 1 in, are rare. We therefore resort to alignments on subreads, the substrings of a read. Storing R, let alone all its subreads, could be memory intensive, not to mention the memory required to store information required for error correction. Inspired by the idea for bounding memory usage with de Bruijn graphs in short-read assembly, we work with kmer subreads of input data, where the memory of storing the k-spectrum R k is bounded by O(min(4 k ,n(L k +1))). Typically, k is chosen so that the expected number of occurrences of any kmer in the genome should be no more than one, i.e. 4 k > |G|. Therefore, choosing 10  k  16 is sufficient for microbial or human genomes, in which case the k-spectrum would fit within 4 GB RAM regardless of input size. Focusing on reasonably short kmers has several advantages. First, we expect an adequate number of kmers to align to the same position along the genome even with relatively low coverage (e.g. 40). High local coverage is needed to identify erroneous bases. For instance, in, there exist five subreads, four copies of  2 and one copy of  2 , aligning to the same starting position in the genome, but this number reduces to three for the longer subread  2 || 0  3. Second, it is less compute intensive to identify N d i when k is small, since there are fewer ways to select d out of k positions. Last, sequencing errors in kmers are much less frequent compared with full-length reads, so d need not be large.
Fig. 1. Gis the target genome, shown as a bold line; r i 's (0  i  8) represent reads, shown as thin lines;  j (0  j  8),  2 and  2 are kmer instances in the reads, shown as rectangles. Every read is drawn aligned to its origin of sequencing position on the target genome. The bases at two positions in the
X.Yang et al.Nevertheless, relying solely on short kmers can easily lead to ambiguities when resolving erroneous bases. For instance, in, without knowing the alignment, it is unclear if  2 should be corrected to Note that if t i is specified as r, then the overlaps between consecutive tiles can be inferred; i.e. l i 's can be derived from t i 's. Multiple tilings exist for any read. For example, both ((If read r contains errors and T r is a tiling of r, then we expect to find a tiling T s of the true read s as one of the d-mutant tilings of T r , where constituent tiles of T s have higher coverage than those of T r. However, in some cases, T s will not be found among the d-mutant tilings.
The algorithmFor ease of presentation, we assume R can fit in main memory (this requirement will be relaxed later). The algorithm consists of two major phases. We first provide a brief overview, and subsequently describe the algorithm in more detail and analyze its time and space complexity.(1) Information extraction.(b) Derive Hamming graph(c) Compute tile occurrences.(2) Individual read error correction.(a) Place an initial tile t at the beginning of the read.(b) Identify d-mutant tiles of t.(c) Correct errors in t as applicable.(d) Adjust tile t placement and go to step 2b, until tile placement choices are exhausted.Given a read r  R, any of its constituent kmers  is a vertex v in the Hamming graph. The d-neighborhood of  is accessible via the edges incident to v. Hence, if  contains at most d substitution sequencing errors, the kmer it should be corrected to exists in its d-neighborhood. By building local, approximate alignments of tilings constructed from d-neighborhoods, our strategy identifies a tiling of the true read as a high frequency tiling.
Phase 1: information extractionConstructing R k involves one linear scan of each read in R. This takes O(nL) time. We maintain R k in sorted order using O(|R k |log|R k |) time. The space requirement for R k is given by |R k |=O(min(4 k ,n(L k +1)). Any non-ACGT characters (due to difficulty in base calling) are initially converted to A, which will be validated or corrected later by the algorithm. During error correction, it is important to have fast access to the dneighborhood of any kmer, ideally in constant time per neighboring kmer. One could do so by storing the entire Hamming graph G H , but it would require large amount of memory. If we assume G as a random string, and errors accumulate independently with probability p e , then the probability that a node is linked to another is, including the chance that another random kmer in the genome is within d Hamming distance of the current kmer and the chance that the current kmer contains up to d errors. Thus, the expected memory usage is OAlternatively, we could recover all edges associated with a given kmer  i by checking whether each kmer in its complete neighborhood,  j  N d ci , exists in R k. If  j  R k , then there is an edge between v i and v j in theTo reduce the average time for inferring N d i of  i , we replicate R k in memory and sort each replicate on a different subset of positions in the kmer string, using the following strategy:(a) Store indices 0 to k 1 in a vector A.(b) Divide A evenly into c (d < c  k) chunks, each of size k/c or k/c.(c) For every choice of c d chunks, sort R k by masking the indices from selected chunks and store the sorted results separately.To identify N d i of  i , we query  i against each sorted k-spectrum R k j (0 ) by binary search considering only indices used for sorting R k j. All kmers that belong to the d-neighborhood of  i are adjacent to  i in at least one R k j. If sequencing errors accumulate independently, then the expected number of elements of N d i found in every R k j is h =|R k |/4 kdk/c. Hence, we need approximately c d h|R k |log|R k | expected time to recover all edges of the Hamming graph, i.e. O(|R k |log|R k |) time assuming both c d and h are constants. Typically, |R k | << 4 k , therefore, choosing a larger c value will use more memory, but less expected run time. As an example, in a real Escherichia coli dataset with 160 coverage, storing 13 copies of R k required only 560 MB memory, but the average number of hits per 13mer in each 13-spectrum was less than one. Therefore, identifying each element of N 1 for a 13mer took constant time on average. The above method provides an exact solution for identifying all edges in the Hamming graph. Alternatively, a simpler recursive approximation derives N d by inferring N 1 for every element in N d1. This stategy might be more biologically meaningful (), but is only an approximation since an edge between two vertices v i and v j could be recovered only if there exists a path connecting them such that adjacent vertices represent kmers that differ by exactly one position. In this case, choosing a smaller k, using a larger dataset, or having a higher sequencing error rate all improve the chance to identify all edges. Tiles are l-concatenations of consecutive or overlapping kmers found in reads. Here, we use one fixed value of l but several different values of l can be used to consider tiles with different lengths. We compute the multiplicities of tiles by a linear scan of every read to record all tiles, followed by a sort of the collected tiles and one linear scan of the sorted list. This process takes O(|R 2kl |log|R 2kl |) time, where |R 2kl |=O(min(4 2kl ,n(L 2k +l +1))). Meanwhile, we record the number of occurrences of each tile, where every position has a quality score exceeding some threshold Q c. Typically, a quality score is associated with every base of a short read. The score indicates the probability p e that the corresponding base is sequenced incorrectly.
ReptileFor instance, Illumina GenomeAnalyzer encodes the quality score as Q = 10log 10 (p e /1p e ). A higher score indicates a more reliable base call. To deal with the double strandedness of the target genome, we consider both the forward and reverse complementary strands of every read. Edge identification in the Hamming graph takes twice the time, but no additional memory is needed since R k is already generated using both strands.
Phase 2: error correctionWe use the contextual information in read r to identify sequencing errors through the process of choosing a tiling T r and comparing it with its d-mutant tilings. In particular, if r contains x errors, and we choose any tiling T r , then an error-free tiling T s belongs to the collection of d-mutant tilings of T r if d  x. Under the standard assumption of uniform coverage, the tiles of T s should be substantially more abundant than at least some of the tiles of T r with errors. After T s is identified, the true read s can be readily inferred from T s. In practice, x could be large, and sequencing errors tend to cluster toward the 3 end of a read. Since we prefer d to be small to limit memory usage, run time and false error detection, it is entirely possible that T s is not one of the d-mutant tilings of T r. On the other hand, an alternate tiling r of r may lower the maximum number of mutations per kmer to below d such that s with high frequency tilings is one of the d-mutant tilings of r. In the case that there is no such tiling r , we examine a subset of constituent tiles in r. If a high coverage path of these selected tiles is present, the tiles are corrected. With some errors removed, a tiling may now exist that contains the true read among its d-mutant tilings. These observations are sufficient to motivate the following procedure for identifying and correcting read errors. Place a tile t on r and attempt to correct t via comparisons with its d-mutant tiles (tile correction). If t is validated or corrected, move to the next tile in the standard tiling and repeat. If t cannot be corrected or validated, look for an alternative tiling, presumably one that avoids clusters of more than d errors that are thwarting attempts to find error-free tiles within the d neighborhoods. We first describe tile correction in Algorithm 1, then the overall procedure for read correction in Algorithm 2. Tile correction. For each tile in R, we have recorded its multiplicity O c in R and the number O g of those instances where the quality score of every base exceeds Q c. If a short read dataset comes with unreliable or missing quality score information, we set O g = O c. Otherwise, O g is a better estimate of the number of error-free occurrences of each tile. The tile correction procedure is given in Algorithm 1. A decision to correct tile t is based on a comparison of the high-quality occurrence counts O g of t compared with its d-mutant tiles. As a rule of thumb, there must be compelling evidence before a correction is made. Any tile is automatically validated if its occurrence count exceeds an upper threshold C g (lines 12). A low occurrence tile with no d-mutant tiles is validated only if it occurs more than a low threshold C m times (lines 46). When there exist d-mutant tiles of t with much higher frequencies (at least C r times, C r > 1) than t and which differ at low-quality bases (< Q m ) in t, it is likely that t contains error(s) and the true tile is one of its d-mutant tiles. In such cases, a correction is possible only if there is a unique d-mutant tile with the closest Hamming distance, including a mutation at a low-quality base in t (line 14). We will replace t with this tile. Otherwise, we choose not to modify t to avoid false corrections. Similar reasoning applies to t with very low multiplicity (lines 17 and 18). Since there exist a constant number of d-mutant tiles of t, and correcting t requires comparison of every base between t and t  T , Algorithm 1 takes constant time. Read correction. The overall procedure for correcting a read is given in Algorithm 2. In line 1, an initial tile is chosen and d 1 and d 2 , two parameters specifying the maximum Hamming distance allowed in the two constituent kmers while identifying mutant tiles, are initialized to d. The algorithm terminates when no more plausible tiles can be identified. Within the while loop, d-mutant tiles are identified for the current tile (line 3), which is validated or corrected using Algorithm 1 (line 4). The new placement of tile t next is chosen according to which of the three decisions is made by Algorithm 1 (line 4). Repeated placement of t next according to decisions,Algorithm
tis valid; return.
7.
end if
8.return due to insufficient evidence.If T =, t is valid; return.
13.For every t  T , record those positions differed from t and corresponding quality scores.
14.Correct t to t and return) for all t  T. 2) at least one of the corrected bases has quality score less than Q m in t.
15.If t is not unique, return due to insufficient evidence (ambiguities).Correct t to t ; return.
19.
else
end if 22. end ifthrough(lines 68), gradually forms a validated or corrected tiling of read r, although some reads may never be fully validated. To better understand how the rules in lines 68 choose a tiling, we illustrate with an example infor d = 1. An initial tile t 0 is chosen as shown. Since there exists one error in each of the constituent kmers, t 0 can be corrected. Tile t 1 is chosen as the next tile according to, but two sequencing errors within the second kmer of t 1 lead to an inconclusive decision. Hence, t 1 is not selected in the tiling and an alternative tile  1 is chosen according to. The algorithm iterates and if tiles can be validated or corrected at every stage, we are able to complete a tiling moving from 5 to 3 along the read. Unfortunately, non-uniform coverage and the existence of more than one reasonable d-mutant tile can lead to an inconclusive decision in Algorithm 1 regardless of tiling choice. In, the 5 to 3 tiling encounters an inconclusive dead-end at arrow a 3. To move past dead-end tiles, a non-overlapping tile  2 is chosen by]. A small unvalidated gap is left in the middle of the 5 to 3 tiling of this read. The example shows only the tiling from 5 to 3. The same strategy is applied in the 3 to 5 direction. We briefly analyze the run time of Algorithm 2. Tiles are sorted, so tile information is accessed in O(log(nL)) time. Therefore, line 3 requires O(log(nL)) time. Once some tiles have been corrected, the search space for new d-mutant tiles shrinks when d 1 or d 2 is set to 0 in line 5. The maximum number of non-overlapping tiles in a tiling is the constant L/|t|. Hence, the time spent in correcting each read is O(log(nL)), and the overall run time of Algorithm 2 is O(nLlog(nL)).Based on the decisions made on t in the former step, tile t next of r will be chosen according to. An illustration of choosing a tiling of a read for d = 1. A read is represented on top by a concatenation of rectangles, where each rectangle denotes a kmer. Each tile is represented by a concatenation of two adjacent arrows, which denote its kmer composition. For simplicity of illustration, we choose the read length to be divisible by k and each tile is a 0-concatenation of two adjacent kmers. X's denote sequencing errors. Each bold arrow, a i (1  i  4), denotes tile with insufficient evidence for correction. The placement of an alternative tile is indicated by a dotted arrow. be adjusted to consider the error rates of the particular next-generation sequencing equipment in use. Given Q c and counts of high-quality tile ocurrences, we choose C g so that only a small percentage (e.g. 13%) of tiles have high-quality multiplicity greater than C g. C m is chosen so that a larger percentage (e.g. 46%) of tiles occur more than C m times in R. As C m value decreases, more errors are corrected at the cost of an increased risk of false error correction. The specific values chosen depend on the histogram of tile occurrences. By default, we set C r = 2 such that a low frequency tile could only be corrected to a tile with at least twice the frequency. Increasing C r improves the confidence in error correction. Finally, we choose k ==log 4 |G|| when an estimate of the length of the genome is available, otherwise, a number between 10 and 16 should work. Tile size is 2k, so kmer overlap is 0 to a few bases. The maximum Hamming distance d is set to one by default. But when k is chosen to be relatively large (e.g. 14 to 16), increasing d allows us to identify more sequencing errors but incurs a longer run time and increases the risk of false error prediction.
Choosing parameters
Overall complexityCombining the analysis for each step, the overall run time is O(nLlog(nL)) and the space usage is O(|R k |+|R |t| |). When the collection of input short reads R does not fit in main memory, we propose a divide and merge strategy where R is partitioned into chunks small enough to occupy just a portion of main memory. For each chunk, we stream through each read and record the k-spectrum and tile information, merging it with the data from previous chunks. Reads need not be stored in memory after they have been processed. A similar strategy is applied for error correction: R is reloaded into memory in chunks, tilings and d-mutant tilings are inferred for each read, and errors are corrected as warranted.
RESULTSWe evaluated Reptile on several Illumina/Solexa datasets and compared the results with SHREC () version 2.0, a recent high-quality short-read error correction method that is itself shown to give superior results over prior k-spectrum approaches. We omitted evaluation on simulated data because simulations with random errors or synthetic genomes do not accurately reflect actual short-read sequencing errors (), and could even be misleading. Our test datasets are Illuminagenerated short reads of well-characterized, Sanger assembled bacterial genomes. Knowledge of the genomes is needed for determining the accuracy of the error correction methods. The six experimental datasets, downloaded from the sequence read archive at NCBI, are listed in. Datasets D1 (Accession Number: SRX000429), D2 (SRR001665_1), D5 (SRR022918_1) and D6 (SRR034509_1) are Illumina reads from the E.coli str. K-12 substr (NC_000913) genome (4.64 Mbp); datasets D3 (SRR006332) and D4 are Illumina reads from the Acinetobacter sp. ADP1 (NC_005966) genome (3.6 Mb). The first four datasets are generated by Solexa 1G Genome Analyzer, where each read has the same length 36 bp. The latter two datasets are generated using the more recent Illumina Genome Analyzer II, with read lengths of 47 bp in D5 and 101 bp in D6. D1 has high coverage and low error rate. D2 has typical coverage and low error rate. D3 has high coverage and high error rate. D4 is derived from D3 by randomly selecting short reads amounting to 40 coverage. This is done for evaluating performance on a low coverage, high error rate dataset. Both D5 and D6 have higher error rates. In addition, >13.9% of the reads in D6 contain ambiguous nucleotides, denoted by character N. Since SHREC cannot process non-ACGT characters, we eliminated all reads with ambiguous bases, even though Reptile has no such limitation. The number of discarded reads is indicated in column 5,. Similar to, we evaluated error correction results with the aid of RMAP (v2.05) (), which maps short reads to a known genome by minimizing mismatches. We allowed up to five mismatches per read in the first four datasets Page: 2531 25262533Error rate is estimated by mapping the reads to the corresponding genome using RMAP, and finding mismatches based on uniquely mapped reads.and allowed up to 10 mismatches (default value of RMAP) in D5 and fifteen mismatches in D6 since the reads are longer in the latter two datasets. Reads that could not be mapped to the genome, or that map to multiple locations, are discarded. The mismatches between uniquely mapped reads and the genome are considered read errors. Quality of the datasets varied as shown in, with the percentage of reads that are uniquely mapped ranging from 62.5 to 96.7%. The large percentage of unmappable reads, the higher error rates as well as the large percentage of reads with ambiguous bases indicate that D5 and D6 have lower quality than D1 to D4. Since the goal of error correction is to identify and correct each erroneous nucleotide, we assess the quality of error correction at the base level. A true positive (TP) is any erroneous base that is changed to the true base, a false positive (FP) is any true base changed wrongly, a true negative (TN) is any true base left unchanged, and a false negative (FN) is any erroneous base left unchanged. Then Sensitivity = TP/(TP + FN) and Specificity = TN/(TN + FP). Note that these definitions are different from those used by, which target read-level error detection (whether a read is flagged as containing an error or not). This is a less stringent measure because any read containing errors was classified as TP provided at least one of its errors was detected and irrespective of whether they were accurately corrected or not. We propose two additional measures for assessing the quality of error correction: @BULLET Erroneous base assignment (EBA): let n e denote the number of erroneous bases that are correctly identified but changed to a wrong base. Then, EBA = n e /(TP+n e ) reflects how well we are able to correct an erroneous base to the true base after a sequencing error has been identified. A lower value of EBA indicates a more accurate base assignment. @BULLET Gain: (TP  FP)/(TP + FN). This measures the percentage of errors effectively removed from the dataset, which is equivalent to the number of errors before correction minus the number of errors after correction divided by the number of errors before correction. Clearly, Gain should approach one for the best methods, but may be negative for methods that actually introduce more errors than they correct.
ReptileWe regard these measures as important because they penalize failing to detect an erroneous base, correctly detecting an erroneous base but wrongly correcting it, and characterizing a correct base to be an erroneous base. In particular, we strongly advocate the Gain measure as it captures data quality post-error correction compared with the quality prior to the correction. The results of running Reptile and SHREC on the six datasets are summarized in. Due to the larger memory usage of the SHREC program, we were not able to obtain results for D3, D5 and D6. In all other cases, Reptile had higher Gain and lower EBA than SHREC. With other parameters fixed in Reptile, we varied maximum d value used for inferring Hamming graph in D1 and D2. As expected, the run time significantly increased as d increased, since the size of d-neighborhood for each kmer increased. Also, we see an increase in both TP and FP and four to five times higher EBA, indicating that when we increase the search space, we run the risk of false error detection and correction but increase the chance to identifying more errors. An inherent difficulty in using any method is the challenge of choosing optimal parameters. The results reported inare obtained when using the parameter choices suggested in Section 3. To show that even better performance is possible, we applied a series of parameter choices to dataset D3 (). Gain improved from 63% with the default parameters to as high as 72%. We chose to report on Reptile using the default parameters for all cases inbecause it is unfair to choose optimal parameters for each individual case based on our knowledge of the genome, which would generally not be known. Similarly, we used the default parameter settings for SHREC. Using a different combination of parameters may vary the results of both SHREC and Reptile. In this article, we have presented a method to select parameters for Reptile based on known quantities such as kmer frequency and quality score histograms. A similar guidance is needed for the SHREC program and is beyond the scope of the article. Note that we do not take into account improved results that can only be obtained by the knowledge of the genome (). One can observe that our method of parameter estimation based on statistics from the dataset is performing better than analytical calculations based on the assumptions of uniform error distribution and uniform coverage of genome by reads. In addition, we compared the run time and memory usage of SHREC and Reptile. SHREC is a multi-threaded program while the current release of Reptile can only use a single core. Hence, we report run times in total CPU hours. Both methods were run on a SUN Fire X2200 workstation with dual quad-core 2.3 GHz AMD Barcelona three processors with 8 GB RAM and 4 GB swap memory, running Debian GNU/Linux x86_64. Results in
X.Yang et al., 45), respectively. The last sample point uses parameters(compare D1 and D2), while the memory usage of SHREC increased with the number of reads, genome length and sequencing errors. In addition, although D1 contains many more reads (20.8M) than D3 (17.7M), the higher error rate significantly increased memory usage in both methods. To enable fair comparison with SHREC, the above experiments were carried out by excluding all reads containing ambiguous bases. However, Reptile does have functionality to deal with ambiguous bases, which is useful in the following cases: (i) if a read contains few ambiguous bases, the surrounding high-quality regions provide sufficient information to infer correct bases by referencing the k-spectrum; (ii) in some datasets, neglecting reads containing ambiguous bases leads to excessive loss of data, which further distorts uniformity of the sampling. For instance, as much as 13.9% reads in dataset D6 contain N's. Reptile attempts to correct ambiguous bases in regions where their density is low. If a read contains too many ambiguous bases, it is low quality and untrustworthy. Some reads may have ambiguous bases clustered in some region, e.g. the 3 region, while otherparts may still be of good quality. It is more meaningful to try correcting ambiguous bases in the latter parts alone, since a cluster of ambiguous nucleotides in a read makes it unlikely to pinpoint other reads that have the same genomic co-location. Formally, Reptile attempts to correct an ambiguous base b of read r, if in any substring rthat contains b, there are no more than d ambiguous bases. The ratio of d to w constrains the maximum density of ambiguous bases allowed in attempting error correction. By default, w is set to k (to equal kmer length), while d is set to the maximum Hamming distance allowed (Section 3). To implement the above idea, all ambiguous bases satisfying the density constraint are changed to one of the bases from the set {A, C, G, T} initially (default 'A'), and will be validated or corrected later by the algorithm. To test the accuracy of this procedure as well as study the effect of the choice of the default base, we conducted Reptile runs on the full datasets of D2 (36 bp) and D6 (101 bp) by setting the ambiguous bases to the chosen default. The results are presented in. The default base used is shown under Column 'N'. Accuracy is defined to be the percentage of ambiguous bases that have been successfully corrected (again, only reads uniquely mapped by RMAP are considered as truth is unknown otherwise). The last four, (i) the accuracy of ambiguous base correction is high and consistent with the overall EBA rate, (ii) changing the default base slightly influenced the results due to the resulting differences in k-spectrum composition and (iii) the sensitivity and Gain values are slightly lower than reported in, mainly because the ambiguous bases that were left uncorrected by Reptile could sometimes be uniquely mapped to the reference genome using RMAP, hence increasing the FN value.
DISCUSSIONThe proposed error correction algorithm is conservative because it avoids changing bases unless there is a compelling underrepresentation of a tile compared with its d-mutant tiles. Actual errors in read r cannot be corrected if r occurs in a very low coverage region of the genome or there exist multiple candidate d-mutant tiles, probably because of genome repetition. On the other hand, a tile may be miscorrected if it contains a minor variant of a highly repetitive element in the genome or it traverses a low coverage region that is similar to other regions with normal coverage. Our method is not unique in being challenged by non-uniform coverage on repetitive genomes. Error correction for highly repetitive genomes is essential for successfully assembling larger eukaryotic genomes but none of the existing methods successfully addresses this problem, including Reptile. Short-read mapping provides a reasonable method to evaluate error correction methods in well-assembled, low repetition genomes. Nevertheless, it is not possible to unambiguously determine all errors. There are natural polymorphisms among bacterial lines, and some presumed polymorphisms may be unrecognized assembly errors. Furthermore, the mapping software chooses among alternative mappings by invoking parsimony, but there is some chance that the true number of errors is less than the minimum. Lastly, mapping software cannot map reads that contain more than a constant number of substitutions, typically just two, with full sensitivity, although we considered 5 here and tested as many as 15 with similar results. Despite these limitations, we believe that most errors are correctly identified, and this approach can provide a fair comparison of error correction methods. We and others () have found that sequence quality scores provide valuable information. Our use of quality scores probably helped us account for the error patterns in nextgeneration sequencing data () without explicitly modeling them. However, it has been observed () that high quality scores may be too optimistic and low quality scores too pessimistic in estimating sequencing errors in Solexa data. Since quality scores may not be precise measures of misread probabilities, the current version of Reptile uses quality score information in a very simple manner, but can be modified to make more sophisticated use of quality scores if warranted. Finally, although quality scores are needed to run Reptile, it can be run effectively without scores by setting all quality scores and the threshold Q c to the same value. There remain several additional challenges in next-generation sequencing error correction. One challenge is to distinguish errors from polymorphisms, for example, single nucleotide polymorphisms (SNPs). Reptile could accommodate SNP prediction with modification in the tile correction stage (Algorithm 1), where ambiguities may indicate polymorphisms. Another challenge is the growing read length of upcoming high-throughput sequencers. Currently, we define tiles as concatenations of two kmers. it might prove useful to extend the tile definition to more than two kmers in order to address error correction in much longer reads.
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
