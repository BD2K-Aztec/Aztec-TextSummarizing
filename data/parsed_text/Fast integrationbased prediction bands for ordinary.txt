Motivation: To gain a deeper understanding of biological processes and their relevance in disease, mathematical models are built upon experimental data. Uncertainty in the data leads to uncertainties of the model's parameters and in turn to uncertainties of predictions. Mechanistic dynamic models of biochemical networks are frequently based on nonlinear differential equation systems and feature a large number of parameters, sparse observations of the model components and lack of information in the available data. Due to the curse of dimensionality, classical and sampling approaches propagating parameter uncertainties to predictions are hardly feasible and insufficient. However, for experimental design and to discriminate between competing models, prediction and confidence bands are essential. To circumvent the hurdles of the former methods, an approach to calculate a profile likelihood on arbitrary observations for a specific time point has been introduced, which provides accurate confidence and prediction intervals for nonlinear models and is computationally feasible for high-dimensional models. Results: In this article, reliable and smooth point-wise prediction and confidence bands to assess the model's uncertainty on the whole time-course are achieved via explicit integration with elaborate correction mechanisms. The corresponding system of ordinary differential equations is derived and tested on three established models for cellular signalling. An efficiency analysis is performed to illustrate the computational benefit compared with repeated profile likelihood calculations at multiple time points. Availability and implementation: The integration framework and the examples used in this article are provided with the software package Data2Dynamics, which is based on MATLAB and freely available at http://www.data2dynamics.org.
IntroductionOne major task in Systems Biology is to infer knowledge about biological processes via mathematical representations of the underlying biochemical reactions. At the beginning, information given in experimental data is exploited to build a suitable model, either biologydriven or by reverse-engineering algorithms (). An important step of this process is the model selection. For this purpose, reliable confidence intervals on the models of choice are crucial. Subsequently, predictions about the models' unobserved components or extrapolation to different experimental conditions are desired. In Systems Biology, mechanistic models based on ordinary differential equations (ODEs), i.e. where each model component has a biological process as counterpart, are frequently used to describe the dynamics of biological interactions. Thus, calculation of confidence intervals typically has to deal with nonlinear, stiff ODE systems with a large number of parameters and V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com sparse observations of the model components. In the case of linear models, propagation of uncertainties is well studied. Yet, in ODEs, parameter transformations to link the model responses linearly to its parameters are in general not achievable. As a consequence, the classical approach based on Fisher-information () is insufficient. Alternatively, sampling approaches like Markov chain Monte Carlo () can be applied to infer knowledge about the model behaviour through dense sampling of the parameter subspace which is in agreement with the measured data. Nevertheless, a dense sampling is not feasible for high-dimensional spaces, known as curse of dimensionality (). In addition, prior information which is essential for some sampling methods is normally not given in the context of biochemical models, which often results in a weakly confined parameter space from which samples have to be taken. To remedy these deficiencies, the validation and prediction profile likelihood for arbitrary model observables have been introduced in, based on theoretical foundations made in Hinkley (1979) and Bjornstad (1990). While the prediction profile likelihood determines confidence intervals on the model response, the validation profile likelihood includes the noise of a potential validation measurement. Both are generated without sampling of a high-dimensional space. Instead, the one-dimensional model prediction space is evaluated and parameters are computed via penalized maximum likelihood estimation. Thereby, an efficient and reliable propagation of uncertainties from the experimental data to the model response is accomplished and predictions as well as confidence intervals are attained. In this study, we exploit that prediction intervals (PIs) for different time points are connected through an implicit function. This enables us to calculate a fast approximation of point-wise prediction bands (PBs) for a certain confidence level. The term point-wise will be omitted from here on. To obtain PBs, a system of ODEs for the time propagation of a specified point on the validation profile will be derived and a dedicated explicit integration scheme is applied together with elaborate correction mechanisms. In contrast to PBs, which includes the uncertainty of an additional measurement (), point-wise confidence bands (CBs) represent the uncertainty of the current model, only. To deduce the latter, PBs can be calculated for the limit case of a decreasing uncertainty of the additional measurement, r ! 0 (). The presented method is tested and verified on three established models for cellular signalling, published inand Bachmann et al. (2011).
Methods
Profile likelihood and validation profilesBiochemical components in a reaction network can be described by ODEs _ xt; ut; h  f xt; ut; h:Thereby, the time evolution of the concentrations of the molecular compounds is given through integration of Equation (1). The internal model state xt; ut; h can depend on an external, time-dependent input function u(t). In addition, it is dependent on dynamic and initial value parameters, which are comprised in h together with observational parameters. Further, the model response gxt; ut; h; h links the internal state to observed data y(t) via yt  gxt; ut; h; h  t;where additive Gaussian errors $ N0; r 2  are assumed.To assess the discrepancy between model and experimental data, the scaled log-likelihood can be calculated throughFurther information on the integrator and optimization routines used in this article can be found in Supplementary Section S1. An accurate confidence interval for a parameter of interest can be deduced with the profile likelihood approach of Venzon and Moolgavkar (1988) throughPLh j   min hi6 j v 2 h:The confidence interval to a given confidence level a spans all values of h j with PLh j  below a threshold given by the a-quantile of a v 2-distribution with one degree of freedom, denoted by icdf v 2 1;a :CI hj;a  h j j PLh j  v 2  ^ h  icdf v 2 1;a  n o :For weak assumptions (), e.g. for a high amount of informative data, 1  a specifies the probability that, for repeated experiments, the true value of h j lies within the boundaries of the confidence interval. If a flat profile is observed for a parameter, it indicates a structural non-identifiability, meaning that either no information about the respective parameter is given in the measurements, or that other parameters can fully compensate for changes in the model response arising by fixation of the parameter of interest (). Besides, a profile likelihood which exceeds the threshold icdf v 2 1;a  in maximal one parameter direction is called practically non-identifiable (), indicating that the information within the data is not sufficient to restrict the parameter to a finite confidence interval. With additional information provided through measurements or through model reduction, practical non-identifiabilities can be resolved. In, profile likelihood-based confidence intervals and their computation have been generalized to confidence and prediction intervals for arbitrary observables. For its calculation, an auxiliary data point z s with standard deviation ~ r is added to the loglikelihood at time-point s and observable g z by the algorithm. Since no experimental data are necessary for the observable of interest, g z , it allows for computation of PIs on new experimental conditions or previously non-observed model components. The extended v 2 then readsand the estimation for the validation profile likelihood is given bywith optimized parameter set ~ h, which is now optimized for both the measurements y and the fixed auxiliary data point z s. Thereby, z s can be interpreted as a parameter for which a PI can be computed.reflects the expectation that a validation experiment lies within a given confidence level a. In accordance with the parameter profile likelihood described above, flat validation profiles or ones that do not exceed the threshold in both directions are called structural or practical non-observable, respectively. As z s can be interpreted as parameter of VPLz s , structural or practical non-observability arises if an adapted h can alter the model response for observable g z while preserving the v 2 value or keeping it below the threshold set by a in at least one direction. On the contrary, the confidence interval represents a prognosis of the current model structure and parameter calibration, without noise of an additional measurement. Thus, the auxiliary data point z s can be substituted by a model constraint g z xs; us; h; h z s. Alternatively and to avoid the challenging task of constrained optimization (), the limiting case ~ r ! 0 can be used to calculate the prediction profile likelihood (), as outlined in Kreutz et al. (2012).
Integration method for PBsBased on the validation profile likelihood, PBs can be deduced via calculation of PIs for multiple time points, linked through e.g. a smoothing spline (). To compute them more accurately and efficiently, an integration method is derived in this study. For the integration, the auxiliary data point z s at the threshold of a specified confidence level a, determined by Equation (9), serves as starting point. It is treated explicitly time dependent. For given confidence level a and time s, a unique upper and lower time-course of z s with corresponding ~ hs constitutes the PBs. To obtain PBs for a specified confidence level a, the corresponding v 2-level sets a constraint for the integration. Further, the parameters are supposed to satisfy the optimality condition of Equation (8). This leads toEquation (10) cannot be resolved for the desired quantity zs analytically. However, its derivative and the corresponding change in the optimized model parameters ~ h can be computed via the implicit function theorem. As shown in Supplementary Section S2, after differentiation of Equation (10) with respect to s, the following set of differential equations is obtained: d ds z  f xs; us; ~ hs;The system of differential equations can be solved in forward direction as well as in backward direction after the transformation s ! s, which helps to reveal partial non-observabilities as discussed in more detail in Section 3.1. The Hessian matrix in Equation (11b) is given bywhereby second derivatives are omitted due to their expensive numerical computation. In addition, the influence of the term with second derivative can destabilize the optimization process, since the approximation is strictly positive definite ().
Explicit integration and correction mechanismsSystems of ODEs describing biochemical reactions are in general stiff, thus they require an implicit integration scheme. Yet, these need the Jacobian of the right-hand side of the ODE system, which is in this case composed of first derivatives of the underlying mechanistic model. For that reason, exact second derivatives would be necessary for an implicit integration of Equations (11) (Terms including third derivatives would emerge if the correct Hessian is utilised, but are dropped in the approximation of Equation (12)), but are computationally too expensive. Thus, explicit integration via the RungeKutta scheme of fourth-order (Butcher, 1963) with elaborate correction mechanisms is executed. The stiffness of the ODE system of Equations (11) rules out integrators with adaptive stepsize as well, because the chosen step size quickly approaches 0. To compensate for mis-specifications made in Equation (12) and to overcome integration artefacts due to the explicit integration, the appendage). For c  1, Equation (13) represents a gradient descent approach. The correction strength c is adjusted throughout the integration by comparison of the vector norm between the suggested correction and the uncorrected step:Still, the explicit integration scheme can lead to a v 2 value outside of the chosen confidence level and the parameter set after an integration step can be off its minimum. An additional correction to the explicit integration steps can take advantage of the fact that the desired v 2 value is known beforehand and is to be satisfied within close bounds throughout the integration. In every step of a correction, the present v 2 value and its optimized parameters are obtained via constrained optimization of the likelihood given the auxiliary data point. If the current v 2 value leaves the interval icdf v 2 1;a ; icdf v 2 1;a0:01 , the auxiliary data point is interpolated by a quadratic fit between the two points defined by the best fit v 2  ^ h and the current auxiliary data point with its corresponding v 2 value. If the re-optimized value remains outside of the icdf v 2 1;a ; icdf v 2 1;a0:01  band, a polynomial, succeeded by a cubic interpolation spline is fit to the old plus updated auxiliary data points with their respective v 2 values. As last resort, the validation profile likelihood is computed for the current time point and serves as new starting point for further integration. Non-identifiabilities of the model are dealt with a cut-off tolerance on the singular values that come up in the inversion of the Hessian matrix. Nearly flat parameter profiles would lead to large parameter changes hampering the integration of the models' ODE system and bursting any validity of the linear sensitivities of Equation (11b). The pseudocode illustrating the integration procedure described in this section is outlined in Algorithm (1).
Results
Illustrative modelA small model (ABC) of two consecutive reactionswith estimated initial concentration of A is utilized here to demonstrate the performance of the integration method and illustrate the scenario of non-observability. For k 1  0:05; k 2  0:1 and A 0  1, 11 data points with Gaussian noise $ N0; r 2  with r  ~ r  0:1 were simulated for states B and C, spaced equidistantly between t  0. .. 100. In this setting, all three parameters are identifiable. To evaluate the accuracy of the integrated PBs, PIs for distinct time points were calculated utilizing the analytic solution of the ABC model. The analytic solution and derivatives together with detailed information about the toy model can be found in Supplementary Section S4. These are shown together with the integrated PBs for a  95% in. The PIs comprise negative measurements, because Gaussian errors are assumed for the auxiliary data point. Since a high time resolution is achieved by integration of PBs and because no time points have to be specified beforehand, the time point for complete transition of state A to state B and C with confidence level a can be specified, in this example at roughly 7 min. Also, the peak concentration of state B in consistency with the data can be determined to lie between 7 and 12 min. In a second experimental setup with measurements for state B at the beginning and end of the shown time interval only, the height and time point of the peak of state B are not determined.illustrates the lack of information in the available data. Since the rise in concentration of state B is not covered, backward integration of Equation (11) from t  100 min is performed. An agreement between integrated PBs and. PBs for component B of the ABC model with only two hypothetical data points for state B at time points t  0 and 90 min, shown as dots. The best fit is depicted as solid line and stars indicate the 95% thresholds of multiple validation profiles. Between t  0 and t  18 min, the concentration of B is non-observable, indicated by the shaded area excessing the y-axis validation profile-based PIs can be seen for the chosen time points 60, 80 and 90 min. However, the PBs widen to very large concentrations of state B while lowering s, with the last time point of the integration at s  18 min and an integrated upper threshold of 280. Hence, a partial non-observability arises between t  0 and 18 min.
Application to an established modelTo demonstrate the efficiency and robustness of the integration method on biochemical models, a model of the Epo-induced signalling via the Janus family of kinases (JAK) and the signal transducer and activator of transcription (STAT), published byis utilized here. The model revealed that rapid nucleocytoplasmic cycling of STAT5 is a key component and acts as remote sensor between nucleus and receptor within the JAKSTAT pathway. The model scheme is depicted in. To visualize the capabilities of the integration approach, the model was extended by a hypothetical observation of the STAT5 concentration in the nucleus. In addition, a model without relocation of STAT5 from the nucleus to the cytoplasm is constructed, which is used for model selection between these two opposing hypotheses. The PBs on all model observables, namely the phosphorylation of STAT5 (pSTAT) and the total STAT5 (tSTAT) concentration in the cytoplasm as well as for the introduced concentration of STAT5 in the nucleus (nSTAT) were computed and are shown intogether with the 95% threshold points of the validation profile-based PIs. Apart from a good agreement of both, the distinction of the conditions with and without export of STAT5 from the nucleus to the cytoplasm can be observed. The model output of the latter could be verified by e.g. an experiment with inhibited relocation of STAT5 to the cytoplasm. Also, the transient versus sustained STAT5 concentration in the nucleus could be taken as hypothesis for a validation experiment. To distinguish between both proposed models and as model selection step, the measurements of the total STAT5 concentration for late time points could be consulted. Since the PBs of the concurring models are clearly apart there, the available data evidently favours the model with relocation, as it is identified in Swameye et al. (2003).
Efficiency of the integration methodThe efficiency of the integration method compared with validation profile-based PIs was assessed with the illustrative model and three established models, published inand. More information about their ODE systems and the utilized datasets can be found in the Supplementary Section S5. It was measured on a MacBookPro from Mid-2014 with a 2.8-GHz Intel Core i5. Due to conceptional differences of the validation profile likelihood approach and an integration of PBs, a direct comparison is not feasible without specification of some assumptions. To obtain fast and sufficiently accurate upper and lower bounds of the PIs via the validation profile likelihood method, roughly 10 steps of. Model structure of the JAKSTAT signalling pathway, as published in. After binding of Epo to the Epo-receptor, JAK2 is activated and the intracellular domain of the receptor gets tyrosin-phosphorylated in turn. Subsequently, STAT5 is recruited to the receptor, gets phosphorylated and dimerises, then migrates to the nucleus where gene transcription is initiated. Thereafter, de-phosphorylated STAT5 is exported to the cytoplasm. Phosphorylated STAT5 (pSTAT) and total STAT5 (tSTAT) in the cytoplasm with data points as dots, as well as the STAT5 concentration in the nucleus (nSTAT). The best fit is shown as solid black line for the model with allowed export of STAT5 from the nucleus back to the cytoplasm (Control) and as dashed grey line without this relocation (Inhibition). Integrated PBs for confidence level a  95 % are indicated as grey area, the corresponding 95% thresholds of the PIs via validation profiles as stars VPLz s  with varying z s were calculated. In addition, the validation profiles were evaluated for 10 distinct time points equidistantly spread over the model's time horizon. Considering the integrated bands, two integration steps per time unit (mostly minutes) were taken. For both methods, the total computation times were averaged afterwards to attain the runtime per time point. The results shown inillustrate that the integration is performed roughly 615 times faster. Even with precise integration, denoting an integration procedure with a correction after each step, re-assuring that the a threshold is maintained, the time span for integrated PBs is around five times lower compared with multiple computation of validation profiles. The difference in the relation of both integration times originates from the number of correction cycles needed during the integration procedure. Also, the optimization time at each point on the validation profile can vary by a factor of 10, together with a variation of the actual required steps. As further advantage of the integration approach, its accuracy does not depend on the amount of points calculated on the PI, since the confidence level a is maintained through a constraint. In addition, the true PBs can differ from those interpolated through few distinct time points, especially in areas of high variability and in vicinity of sharp peaks.
DiscussionOne of the main purposes of mathematical modelling is to acquire knowledge about the outcome of perturbed experimental conditions or not yet measured model components, which includes arbitrary functions like the integral of time courses, total concentrations or the recurrence time after stimulation. This constitutes a challenging task for large non-linear models with sparse observations, as they typically arise in biological applications. Sampling approaches like MCMC have serious drawbacks in this setup due to the curse of dimensionality. They are feasible within low parameter dimensions or if appropriate prior knowledge is present. In contrast, models in Systems Biology often possess non-identifiable parameters where the search space is weakly confined. The nonlinearity also hampers translation of one-dimensional parameter profiles into PBs (). Thereby, the trajectories associated to the optimized parameter sets for each point on the parameter profiles (see Equation 6) are utilized. Subsequently, the envelopes of these trajectories constitute an approximation of the PBs. Yet the uncertainty is in general underestimated, because comprehensive sampling of the model prediction space is not guaranteed by evaluation along onedimensional parameter manifolds (see Section 2.1). To resolve the mentioned impediments, a new approach based on re-optimization of the likelihood with a varying constraint or an auxiliary data point was introduced in. Therein, the one-dimensional prediction space is evaluated to directly infer the prediction and confidence intervals from the measured data. Apart from a constraint on the model response, the confidence interval can be obtained through decreasing the standard deviation of the auxiliary data point (), too. As a result, PBs can be obtained by interpolation between PIs for multiple time points. In this article, a fast integration method based on the validation profile likelihood approach is developed in order to attain accurate PBs. The integration starts from an arbitrary point in time on the corresponding PI, e.g. the 95% thresholds for t  0, leading to the 95% PBs for the observable of interest. In the context of statistical testing, the PBs enclosing the confidence region control the probability of errors of type 1, hence they specify the probability that a single validation measurement at a specific time point is within the interval. If PBs with high precision are desired, the integration procedure can be performed coupled with a correction at every step. Thereby, robust and precise PBs, still faster than distinct validation profile likelihood computations, are achieved. The superiority originates from the relationship between two adjacent time points, expressed in Equation (11) and because only the auxiliary data point on the specified confidence level a is required. In contrast, the validation profile likelihood approach provides a number of possible validation measurements with their corresponding v 2 value, which can be used to compute PIs to arbitrary confidence levels a. The efficiency advantage compared with distinct validation profiles is eliminated when only a rough estimate of the PBs is desired, or when the linear Taylor approximation of the required sensitivities breaks down or requires a very small step size. In addition, the methods mentioned above can be faster and still reliable if the model is in the so-called asymptotic setting and behaves approximately linear (). An explicit integration scheme was chosen for the integration of PBs, since the computation of second derivatives arising within the Jacobian matrix of the right-hand side of Equation (11b) is not efficient. To circumvent the latter, automatic differentiation () was carried out for comparison, yet it was significantly slower compared with the implemented explicit integration followed by elaborate correction mechanisms. The scope of PIs is to provide a powerful tool for experimental design and model selection as well as to determine observability of different model components. Through integration, PBs are available for the whole time span of an observable. The latter comprises experimentally measured conditions as well as extrapolation to new experimental conditions and new arbitrary functions of both, e.g. ratios or sums. In addition, availability of smooth PBs provides new possibilities for those functions, like the duration for the recurrence to the initial (steady-state) concentration or the area under the curve. The information provided by PBs can also be included in reverse engineering algorithms for the inference of ODE systems, e.g. for the description of gene regulatory networks via genetic programming (). For this purpose, the observability and width of PBs for specified model components can be included in the applied fitness functions (). For the special case of fixed model parameters, the width of the PBs will be exclusively determined by the uncertainty ~ r of the auxiliary data point, i.e. the penalty term in Equation (7). Therefore, 2~ r provides a lower bound to the width of PBs for a  95%. For experimental design and the improvement of model observability, regions with PI ) 2~ r indicate that the current model exhibits vast uncertainty there. In such a case, a measurement would provide additional information for reducing parameter and prediction uncertainties. On the contrary, a PI approaching its minimum characterizes an informative observable to discriminate the current from alternative model structures.Precise integration labels an integration with initiated correction at every step. The respective models are published in
Fast integration-based prediction bandsConcerning model observability, an inflating size of PBs can be interpreted as partial non-observability, as there might be lack of information for the long-term behaviour or the primary activation, respectively. In addition, non-observability of a model component is detected in the integration procedure, since the requirement on the v 2 level cannot be satisfied and the integration fails.
ConclusionThe propagation of parameter uncertainties to model predictions is non-trivial for large, non-linear models. Yet besides Systems Biology, various areas in science apply computer-aided simulations based on large non-linear differential equations to study a system's behaviour. All attempts to attain model prediction uncertainties are hampered by the curse of dimensionality and a weakly confined parameter space. Also, the non-linear dependence of parameters results in an arbitrarily complex shape of confidence regions for parameter estimates, which cannot be translated into PIs through error propagation. To elude sampling of a high-dimensional parameter space, the validation profile likelihood approach was introduced in Kreutz et al. (2012) to compute data-based PIs. Thereby, statistically accurate PIs are obtained by continuous variation of an auxiliary data point, whereby parameters are re-optimized. In this article, reliable and smooth PBs over time are achieved via integration techniques with elaborate correction mechanisms, which detect non-observabilities of the model during the integration. The idea is related to the validation profile likelihood and takes the auxiliary data point at the threshold of a specified confidence level a as starting point. By decreasing the error of the data point, the integrated PBs converge into CBs. The efficiency and applicability of the integration method was successfully demonstrated on three established ODE models for cellular signalling. The results of the validation profile likelihood approach for distinct time points were reproduced and an accurate integration could be executed roughly five times faster. The introduced approach helps to resolve a main impediment in current applications of System Biology, namely an accurate and efficient computation of PBs and CBs, which help to perform experimental design, model selection and network inference through e.g. reverse engineering. The presented framework and the examples used in this article are available within the software package Data2Dynamics (), which is MATLAB based, open source and freely available at http://www.data2dynamics.org. Further instructions to execute the presented framework therein are given in Supplementary Section S3.
Funding
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
H.Hass et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
