Motivation: Genomic copy number (CN) information is useful to study genetic traits of many diseases. Using array comparative genomic hybridization (aCGH), researchers are able to measure the copy number of thousands of DNA loci at the same time. Therefore, a current challenge in bioinformatics is the development of efficient algorithms to detect the map of aberrant chromosomal regions. Methods: We describe an approach for the segmentation of copy number aCGH data. Variational estimator for genomic aberrations (VEGA) adopt a variational model used in image segmentation. The optimal segmentation is modeled as the minimum of an energy functional encompassing both the quality of interpolation of the data and the complexity of the solution measured by the length of the boundaries between segmented regions. This solution is obtained by a region growing process where the stop condition is completely data driven. Results: VEGA is compared with three algorithms that represent the state of the art in CN segmentation. Performance assessment is made both on synthetic and real data. Synthetic data simulate different noise conditions. Results on these data show the robustness with respect to noise of variational models and the accuracy of VEGA in terms of recall and precision. Eight mantle cell lymphoma cell lines and two samples of glioblastoma multiforme are used to evaluate the behavior of VEGA on real biological data. Comparison between results and current biological knowledge shows the ability of the proposed method in detecting known chromosomal aberrations.
INTRODUCTIONRecent biological studies show the close relationship between chromosomal regions aberrant in copy number (CN) and diseases like tumor () and mental retardation (). High resolution CN estimation makes use of comparative genomic hybridization arrays. DNA from a test sample and normal reference sample are labeled differentially, using different fluorophores, and hybridized to several thousand probes. The ratio of the fluorescence intensity of the test to that of the reference DNA is then calculated, to measure the CN changes for a particular location in the genome. In particular, the logR ratio (LLR) gives an indirect measure of CN of each probe by plotting the ratio of observed to expected hybridization intensity. After a microarray has been constructed and hybridized, the corresponding image can be acquired and additional analysis steps can be used both to reduce noise and to increase the statistical confidence of the observations (generally each probe is spotted in several copies) (). The output of this process is a list of LRR values with the respective genomic positions. The so called segmentation algorithms use this list of measurements to compute the features (breakpoint positions and kind of mutation) of the aberrant regions along the genome. The availability of efficient segmentation algorithms plays an important role in the mapping of aberrant chromosomal regions. Accurate aberration mapping can be used to extract new insight on the mechanisms leading to genetic diseases. For this reason in literature, several segmentation approaches have been proposed. In likelihood function-based approaches (), breakpoint positions are estimated by using a maximum likelihood criterion in which penalty terms are used to limit the complexity of the solution. Often penalty terms are controlled by weights which can be chosen adaptively to the data (). An interesting likelihood function-based approach is DNACopy () which is based on a modification of the original binary segmentation proposed by. DNACopy segments the chromosome into contiguous regions of equal CN using a non-parametric permutation reference distribution which takes in account the effect of noise. In particular, the authors model CN data as a sequence of random variables and the maximum likelihood is used recursively to look for change points where adjacent random variables have a different distribution function. In addition, DNACopy uses a pruning algorithm to control the number of regions. Willenbrock and Fridlyand (2005) andshowed that DNACopy algorithm performs well in terms of sensitivity and false discovery rate both on synthetic and real data. Other statistical models frequently used for CN segmentation are Bayesian approaches and Hidden Markov models. In the Bayesian framework, the prior distributions are combined with some posterior distribution functions to construct the most plausible hypothesis concerning the data segmentation (). In Hidden Markov model-based approaches, hidden states represent the underlying CN of probes. In Fridlyand et al.Page: 3021 30203027
VEGA(2004), the model is characterized in terms of three parameters: the initial state probability, the transition probability and the collection of Gaussian emission probability functions defined within each state. SMAP () is a recent approach based on the discrete-index hidden Markov model where a maximum a posteriori approach is used to split the chromosome into regions. The authors adapt the maximum a posteriori approach so that user-defined priori informations can be integrated within their model for limiting noise influence (modeled by a Gaussian distribution). In recent literature, variational-based approaches are emerging to deal with the CN segmentation problem (). The works of Mumford and Shah (1989) andare the pioneers of discontinuity-adaptive variational models which have been successfully applied in a wide variety of problems. The original model, based on the Total Variation norm (), was proposed to recover image corrupted by noise preserving important image features such as object edges (). Afterwards, discontinuity-adaptive variational models have been applied in many different research areas, such as texture segmentation (), medical image analysis () and shape identification in 'synthetic-aperture radar' imagery (). Variational models are based on the minimization of a functional controlling the similarity between the computed segmentation and the observed image, penalizing at the same time complex solutions. The complexity of the solution is controlled by the scale parameter (also called regularization parameter). As the regularization parameter increases less regions are computed, therefore the choice of a good regularization parameter is a common open question in many variational data analysis algorithms. Discontinuity-adaptive variational models are very skillful in segmentation of piecewise constant (PWC) images. In PWC images, the pixels belonging to the same object have the same intensity, but noise changes image features and the segmentation task becomes more difficult. This situation is very similar to aCGH data, for which segmentation errors are due to noise that shifts LRR values. A recent work () presents a CN segmentation algorithm based on the total variation minimization process proposed by. Here, we propose a new segmentation algorithm (VEGA) based on the Mumford and Shah variational model (). The PWC assumption is used to define a functional considering both accuracy and parsimony of the boundaries. The segmentation problem is put as a minimization problem of an energy functional encompassing both the quality of interpolation of the data by a piecewise constant function and the 'complexity' of the solution measured by the length of the boundaries between segmented regions. It is well known that the resulting energy functional is non-convex and can have many local minima. In order to efficiently compute a solution here we adopt a greedy steepestdescent algorithm based on a pyramidal multiscale approach. The resulting algorithm belongs to the class of region growing segmentation algorithms similar to that proposed in (). In addition, we propose a data-driven heuristics for the computation of a suitable regularization parameter. The use of a variational segmentation approach for CN variation estimation has also been proposed in (), but there are some significant differences with the method proposed here. First, VEGA is based on the Mumford and Shah model while Ultrasome () adopts the Rudin's model (). Moreover, VEGA performs the minimization of the energy functional with a bottom-up approach, by a sequence of successive merging of smaller regions into larger ones. This leads to a greedy multiscale algorithm driven by an increasing series of values of the regularization parameter similar to the image segmentation approach proposed by. Whereas, in () a dynamic programming approach is used, by choosing a fixed value of the regularization parameter. In order to validate our approach, we choose as comparison methods DNACopy (a likelihood function-based approach whose good performance have been demonstrated), SMAP (a recent statistical model-based approach) and Ultrasome (that uses a variational model as in VEGA). Results are compared both on synthetic and on real biological data. Both SMAP (version 1.12.0) and DNACopy (version 1.16.0) are available as Bioconductor R packages, while Ultrasome (version 2.0) is available in a command line version (for Windows and Linux) and a graphical user interface version (for Windows).
METHODS
Mumford and Shah modelThe basic idea of the Mumford and Shah model () is the so-called piecewise smooth model. Given an observed signal u o defined on the domain , we can model u o by a partition of into a set of disjoint connected components i , with
... nin such a way that the signal u 0 varies smoothly within each i and it varies discontinuously across the boundaries between different i. The set of points on the boundary between the i is denoted as. This means that the segmentation problem is put as a problem of optimal piecewise smooth approximation, i.e. we look for an approximation u of u 0 whose restrictions to the regions i are smooth. The search of the optimal piecewise smooth approximation of u 0 can be cast into the minimization of the following functional:where  and  are two non-negative parameters weighting the different terms in the energy: the first term requires that u approximates u o , the second term takes in account the variability of u within each connected component i and the third term penalizes complex solutions in terms of the length of the boundaries ||. Smaller values of E are associated with better solutions (u,,) for the observed signal u o. A special case of Equation (1) is obtained when the approximation u of the signal u o is considered to be a piecewise constant function (u constant within each connected components i ). For this case, Mumford and Shah (1989) proposed the so-called piecewise constant Mumford-Shah model:It is easy to show that, given a fixed set of boundaries , in order to have a minimum the variables u i should be set as the mean of u o within of each connected component i. In this work, we adopt the Mumford and Shah model defined in (2) for segmenting CN data. The role of the two terms of (2) is important, the first term can be considered as the error in the approximation of u by a constant function within each region and the second term as a penalty to complex segmentations consisting of many small regions with irregular boundaries. Therefore, this kind of functional is a compromise between the accuracy of the approximation within each region and parsimony of the boundaries.
S.Morganella et al.The resulting segmentation depends on the scale parameter , indeed it determines the amount of regions of the computed segmentation: when  is small many boundaries are allowed so the resulting segmentation will be fine, while as  increases the segmentation will be coarser and coarser.
The proposed approach
The model Let D R n the data vector containing n LRR probes of a chromosome where the observations are ordered by the respective genomic position. We define a segmentation S of D as a set of ordered positions (breakpoints) b 1 ,...,b M+1 partitioning D into M connected regions R ={R 1 ,...,R M }. The region R i is identified by the indexes in [b i ,b i+1 ) with i = 1,...,M and where the breakpoints b 1 and b M+1 are fixed to the values 1 and n+1, respectively. We use the one-dimensional version of the piecewise constant Mumford and Shah functional, in this case the length of the boundaries between regions has no influence on the segmentation, and the second term of (2) reduces to the number of regions, denoted here as M.Given the n-dimensional data D, it is easy to show that the optimal segmentation must be chosen among the 2 n possible solutions. In genomic data, we have a resolution that provides tens of thousands of observations, so brute force algorithms cannot be applied and suitable solutions must be found by using heuristic strategies. Here, we use a greedy procedure as explained below.
Minimization processThe minimization of (3) is carried out by a region growing process with small regions progressively merged to create larger ones leading to a pyramidal algorithm going from finer segmentations to coarser segmentations. Given two adjacent regions R i and R j , it can be shown that the reduction of the energy (3) after the merging of these two regions is given by:where R i ,R j  R with i = j, |R i | and u i are the length and LRR mean value of the i-th region, respectively. |||| represents the L 2 norm. Following a greedy procedure, we start with a segmentation having n regions each for each LRR measure, then at each step we choose as the next pair of regions to be merged that producing the maximum decrease of the energy functional. For a fixed value of , the algorithm iteratively computes the pair of adjacent regions for which (4) is as negative as possible, if such pair of regions exists. If no such pair of regions exists, then the value of  is increased. The resulting method is therefore considered a multiscale algorithmsince the value of  represents the scale, as  grows the segmentation gets coarser. The algorithm stops when the maximum value of the scale parameter is reached. The sequence of values of this parameter is called -schedule by analogy with temperature schedule of simulated annealing.
-schedule selectionThe  values determine the quality of the final segmentation so the choice of the -schedule is very important for final segmentation. Here, we propose a dynamic -schedule selection where the  stop value is computed considering both the sequences of  values and the data variability. We modify the Full -Schedule Segmentation approach proposed byso that it can work on one-dimensional spaces. In particular from Equation (4) we associate to each breakpoint b i with i = 2,...,M (b 1 and b M+1 breakpoints are fixed) the value:Note thatthat that i represents the cost required for merging the regions R i1 and R i. The i-th breakpoint can be deleted (and the adjacent region are merged) if if if i <. If more regions remain and n  i agrees the previous inequality, then there is no merging that decreases the functional and a new scale parameter, , must be chosen. Here, we select the next scale parameter value as the smallestsmallest smallest i and adding to this value a positive constant close to zero. By using this update rule, new region merges are allowed, so the region growing process will be composed by fine merging operations.
Optimal  selectionA critical point for many variational approaches is the selection of the stop condition, i.e. the selection of the last value of  for which the output solution is obtained. Different values of the regularization parameter can be associated to suitable segmentations. In order to provide a stopping criterion, we use a modified version of the minimum variance method which was first studied by Otsu (1979) in image segmentation.proposed a stopping criterion based on the maximization of the ratio  2 (B)/ 2 (T ) where  2 (B) and  2 (T ) are the between class-variance and the total variance, respectively.  2 (B) measures the variability between different segmented regions and it can be calculated considering only the adjacent regions. Our idea starts from the consideration that th  i measures the degree of compatibility between two adjacent regions, so we can us  i to estimate the variability between adjacent regions (similarly to the between classvariance). Therefore, measuring the distance between two consecutive -schedule values ( =  l+1  l ), we can obtain information on the consistency of the segmentation. In particular, small  values indicate a merging of compatible regions, in contrast the more  grows the more we deviate from the compatibility between adjacent regions. In order to take also in account the total variability of the data we use the standard deviation (SD), , computed chromosome by chromosome, and the proposed stop criterion is:where  is a positive constant. We tested different values for  and we obtained the best performance by using  = 0.5. In absence of further prior information, this value allows to obtain segmentation results consistent to the data taking also in account the complexity of the solution. All results reported in this article were obtained by using  = 0.5. The detailed VEGA algorithm is summarized in pseudocode in the Supplementary Material.
Computational complexityIf we have n probes then the starting segmentation will have n regions corresponding with n+1 breakpoints that have to be maintained in order to efficiently extract the minimum at step 8 of the Algorithm 1 (Supplementary Material). Here, we use a priority queue based on a heap data structure (). Therefore, step 4 requires O(n), step 8 requires O(1), whereas steps 11 and 12 require O(logn) each. Considering that the cycle in steps 718 is repeated at maximum n times, the complexity of the algorithm VEGA is O(nlogn).
Region labelingThe region growing process described above produces a segmentation S composed by M regions R ={R1,...,R M } each represented by the value u i , the mean of the observations contained in the i-th region (with i = 1,...,M). In order to assign a label to each region indicating if this region correspond to a normal, loss or gain aberration we use the rule proposed by Willenbrock and Fridlyand (2005): a region R i is labeled as loss if  i < 0.2, while it is labeled as a gain if  i > 0.2 and for values 0.2   i  0.2 the region is considered normal.
RESULTSIn order to evaluate the performance of our approach, we use both synthetic and real data. We compare our results with the ones obtained by DNACopy (), Ultrasome () and SMAP ()
VEGA
Fig. 1. ROC curves at different aberration widths and SNR. The x-axis is the FPR and the y-axis isthe TPR. The curves were generated by measuring the TPR and FPR on the simulated data published in. Blue is VEGA, red is DNACopy (), green is Ultrasome () and black is SMAP (). label (loss, normal or gain), we apply to Ultrasome and DNACopy the same label assignment rules used in our approach (see Section 2.2.6), while when the fitted LRR in each region is required we use for SMAP the mean value of all LRRs contained in the region. Algorithms ran with their default parameters, except for SMAP for which parameters were chosen by the user.
Results on synthetic data
Evaluation metricFor a quantitative evaluation of the performance we used the Precision, the Recall and their harmonic mean (F-measure) which are three widely used statistical classifications. The Precision can be seen as a measure of exactness or fidelity, whereas Recall is a measure of completeness. Both measures are defined by using the concepts of true positive (TP), false positive (FP) and false negative (FN). A TP represents a perfect correspondence between the computed label and the true ground, a FP occurs when the algorithm detects a mutation that is not present in the true ground and we have a FN evaluation when a mutation in the true ground is not detected by the algorithm. Algorithm accuracy has also been reported by calculating the receiver operating characteristic (ROC) curve as described inwhere the true positive rate (TPR) was defined as the number of probes inside the aberration whose fitted values are above the threshold level divided by the number of probes in the aberration and the false positive rate (FPR) was defined as the number of probes outside the aberration whose fitted values are above the threshold level divided by the total number of probes outside the aberration.
Validation on Lai et al. synthetic dataGiven that several different segmentation methods have been published in literature, this makes it difficult to have a fair comparison between all of them. Here, as in, we address this problem by using already available synthetic data previously published in. About a dozen methods have been benchmarked using this dataset, and we use it to evaluate the performance of VEGA. Moreover,found DNACopy as the method performing consistently well on both synthetic and real data, so we include the performance of DNACopy in our comparison. The synthetic dataset proposed bysimulates four aberration widths (5, 10, 20 and 40) in different noise conditions (signal-to-noise ratio SNR of 1, 2, 3 and 4). For each aberration width and SNR, 100 artificial chromosomes of 100 probes were generated. As shown in, in high SNR conditions (SNR values of 3 and 4) VEGA results are very similar to the ones of DNACopy, while for low SNRs and small aberration widths (lower right panels) VEGA appears to perform better than DNACopy. By analyzing SMAP's performance we can notice how it is strongly influenced by the aberration width and for SNR of 1 its performance is not simple to be interpreted. Eventually this may be due to the used parameter setting. For Ultrasome, we can notice that although it has acceptable performance for high SNR levels (4 and 3), it has some problems in low SNR scenarios.
Page: 3024 30203027
S.Morganella et al.
Validation on the proposed synthetic dataAlthough the dataset proposed byallows an extensive comparison of segmentation algorithms it is far from real aCGH data where, for each chromosome, thousands of probes are observed and multiple gain and loss mutations are expected. In order to overcome these limitations we created an artificial dataset simulating the profiles of three chromosomes having different size of 500, 750 and 1000 probes (50 profiles for each chromosome were generated). In each sample, a set of mutations were randomly inserted with size ranging from 11 to 25 where both position and class (loss or gain) of each aberration was randomly chosen. In particular, 10 mutations (with size ranging from 11 to 20) were inserted within the data with 500 observations and 15 aberrations (with size ranging from 11 to 25) were inserted within the samples having 750 and 1000 probes. The model used for the generation of the synthetic profiles follows:where d m is the observed LRR for the probe m which is the sum of two components, the first one is the real LRR x m for this probe and the second one,  m is the noise component corrupting the data. The true component x m can assume values.58 for loss, normal and gain, respectively. The noise component is modeled as a white Gaussian process  m  N (0,). Performance assessment is made by varying for each sample the value of  from 0 to 1 with step 0.1. In, Recall, Precision and F-measure of each considered method are reported with respect to the noise SD . Observing the results obtained for the different chromosome sizes (SupplementaryS4), we note that the number of probes does not affect the performance of the compared algorithms. Analysing the trends of the considered approaches with respect to the noise level  (and), we note that SMAP works well until the value of  is not greater than 0.5, after this value its performance tends to drop down. Also DNACopy's performance is negatively affected by the noise, but the trend is less steep than SMAP. In contrast, both VEGA and Ultrasome results are not drastically affected by the values of  but Ultrasome does not compute the correct segmentation in no-noise condition ( = 0), this is caused by the number FN and FP that produce a Precision of 0.937 and a Recall of 0.926. This first analysis demonstrates that variational approaches tend to be more robust to noise than the other considered ones. These considerations are also supported from the ROC curves and the corresponding area under the curves (AUCs) reported in Supplementary Figures S4S11 where in addition we can see that these algorithms are more skillful at detecting losses than gains.shows the results of the compared algorithms on a simulated chromosome profile of 1000 probes for  noise values of 0 (A) , 0.5 (B) and 1 (C). In these Figures, black lines represent the true profiles and the red lines show the profiles computed by the algorithms. Comparing computed and true profiles we can note that in absence of noise ( = 0), all algorithms provide the correct segmentation, while for  = 0.5 VEGA and SMAP compute more consistent profiles. Finally in high noise condition ( = 1), only VEGA and Ultrasome seem to provide an acceptable segmentation.
Results on real data
Glioblastoma Multiforme (GBM) dataGBM is a particular malignant and aggressive type of brain tumor. Several chromosomal mutations have been observed in glioma as gain on chromosome 7 and losses of chromosomes 10, 13 and 22. We used two samples (GBM31 and GBM29) representing primary GBMs in the glioma data fromwhich were used in. In sample GBM31, chromosome 13 is characterized by a large region of loss (). Results show that VEGA, DNACopy and SMAP identified the expected deletion while Ultrasome did not detect this mutation. For the chromosome 7 of the sample GBM29, three small amplifications with high amplitude around EGFR locus are expected (). Among the compared algorithms, SMAP is the only one that did not find the gains, DNACopy combined the first two amplifications together while both VEGA and Ultrasome detected all three mutations.
Mantle Cell Lymphoma (MCL) dataMCL is an aggressive lymphoma with median patient survival times of  3 years. We used eight MCL cell lines (Granta-519, HBL-2, JVM-2, NCEB-1, REC-1, SP49, UPN-1 and Z138C) previously analyzed by. This analysis was performed by SMRT aCGH containing 97299 elements, representing 32433 overlapping genomic segments spanning the entire human genome (provided a comprehensive list of cytobands with the respective biological references for these cell lines. In Page: 3025 30203027 VEGAaddition, they confirmed the deletion of 13q14.3 cytoband in Granta519 by using fluorescence in situ hybridization (FISH) analysis. This list was used as our 'ground truth' to assess the performance of VEGA, Ultrasome, DNACopy and SMAP (). By using this 'ground truth' we can see that Ultrasome did not identify some interesting mutations such as the gain of 8q24.13-8q24.21 [this cytoband contains the well-characterized MYC oncogene that is reported to be overexpressed in MCL (and the loss of 13q14.2-13q14.3 which has been validated for Granta-519 by using FISH analysis. Moreover, for this last chromosomal region SMAP found a gain which is in contradiction with respect to FISH analysis. Loss of 9p21.3 was confirmed by all approaches and only Ultrasome considered this region as normal in Z138. In Granta-519, the loss of 1p36.11 was detected only by VEGA, while SMAP did not find a loss for this cytoband for Granta-519, SP49 and Z138. By analyzing Granta519, we can see that VEGA and DNACopy agreed with DeLeeuw's analysis for all reported cytobands except for the mutation in 12q13.13-12q13.2 for which no method provided a loss. In contrast, both Ultrasome and SMAP had some problems in region detection for this cell line. As a further a qualitative analysis of VEGA, Ultrasome, DNACopy and SMAP approaches, we analyzed the chromosome 8 of NCEB-1 for which a gain covering the whole long branch is expected (). In Supplementary, the segmented regions for this chromosome are shown, and this figure shows that Ultrasome detected very few and small aberrant regions on the long branch of this chromosome, SMAP provided a very 'jagged' segmentation, while VEGA and DNACopy computed the expected amplification, but DNACopy detected an unexpected normal region. Supplementaryshows the number of aberrant regions obtained by the different approaches. From this table, we note that SMAP computed the maximum amount of regions (97.06), on the contrary Ultrasome provided a mean of 25 aberrant regions for cell line, in the middle we find VEGA and DNACopy algorithms with a similar mean of aberrant regions (85.94 and 86.75, respectively).
DISCUSSION AND CONCLUSIONSIn this article, we present VEGA: a new approach for DNA copy number segmentation. VEGA uses a variational-based method for image segmentation, known as Mumford and Shah model. This model is modified so that it can be efficiently applied in CN segmentation. A greedy multiscale region growing process is used to find the solution. The region growing process follows a -schedule selection that provides an efficient scheme to control the quality ofwith the respective mutations: grey and red boxes indicate loss and gain, respectively. For each cytoband the results of VEGA (V), Ultrasome (U), DNACopy (D) and SMAP (S) are reported. L indicates loss, N indicates normal and G indicates gain. the segmentation. The optimal scale parameter value is computed in an automatic way considering both the -schedule and the data variability. VEGA algorithm works separately on each chromosome and it has computational complexity of O(nlogn) where n is the number of observed probes for the considered chromosome. We compare VEGA with three approaches that can be considered the state of the art in CN segmentation: DNACopy (a likelihood function-based approach), SMAP (an algorithm based on discrete hidden Markov model) and Ultrasome (an algorithm that as VEGA uses a variational-based model). Results on synthetic data show the expected robustness to noise of variational-based approaches (VEGA and Ultrasome), and the performance of VEGA in all noise conditions. DNACopy and SMAP have similar performance for low noise levels but their performance may be significantly affected by noise. In order to assess the performance of the compared approaches on real data, we use eight MCL cell lines and two samples of glioblastoma multiforme. The 'true ground' for this data is composed by a list of cytobands which are extracted from current biological knowledge. Results on real data show the ability of VEGA in segmentation of well-characterized mutations and they also confirm the properties of the DNACopy algorithm. In contrast, real data analysis reveals some limitations of SMAP and Ultrasome. In addition, both synthetic and real results show important differences between VEGA and Ultrasome performance. These differences are mainly due to: the underlying model (VEGA uses Mumford and Shah model while Ultrasome uses Rudin model), the minimization process (VEGA uses a region growing process while Ultrasome uses a top-down approach) and the selection of the scale parameters used in our segmentation process. We think that SMAP and Ultrasome performance can be improved by a tuning of their input parameters. The problem is that these algorithms require a significant number of arguments and the tuning process can be hard for non-expert users.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from [11:27 23/11/2010 Bioinformatics-btq586.tex]
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
