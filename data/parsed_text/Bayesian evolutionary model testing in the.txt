Motivation: The advent of new sequencing technologies has led to increasing amounts of data being available to perform phylogenetic analyses, with genomic data giving rise to the field of phylogenomics. High-performance computing is becoming an indispensable research tool to fit complex evolutionary models, which take into account specific genomic properties, to large datasets. Here, we perform an extensive Bayesian phylogenetic model selection study, comparing codon and nucleotide substitution models, including codon position partitioning for nucleotide data as well gene-specific substitution models for both data types. For the best fitting partitioned models, we also compare independent partitioning with standard diffuse prior specification to conditional partitioning via hierarchical prior specification. To compare the different models, we use state-of-the-art marginal likelihood estimation techniques, including path sampling and stepping-stone sampling. Results: We show that a full codon model best describes the features of a whole mitochondrial genome dataset, consisting of 12 protein-coding genes, but only when each gene is allowed to evolve under a separate codon model. However, when using hierarchical prior specification for the partition-specific parameters instead of independent diffuse priors, codon position partitioned nucleotide models can still outperform standard codon models. We demonstrate the feasibility of fitting such a combination of complex models using the BEAGLE library for BEAST in combination with recent graphics cards. We argue that development and use of such models needs to be accompanied by state-of-the-art marginal likelihood estimators because the more traditional and computationally less demanding estimators do not offer adequate accuracy. Contact:
INTRODUCTIONThe startling advances in sequencing technology have led to a dramatic increase in the scale and ambition of phylogenetic analyses. As more and more complete genomes are sequenced, phylogenetics is entering a new erathat of phylogenomics, which uses phylogenetic principles to extract information from genomic data (). Until recently, molecular phylogenies based on a single or few orthologous genes often yielded contradictory results (). One branch of the expanding field of phylogenomics aims to reconstruct the evolutionary history of organisms on the basis of their genomes, as opposed to performing single-gene studies, which have long dominated the field (). By expanding the number of characters that can be used in phylogenetic reconstruction from a few thousand to tens of thousands, access to genomic data could potentially alleviate sampling problems that hampered previous phylogenetic analyses. Phylogenomic analyses involve estimating the underlying evolutionary history of sequences either as an intermediate goal or as an end point. Statistical phylogenetics provides a framework for estimating historical patterns, inferring intrinsic parameters of evolutionary processes, and testing hypotheses under the auspices of the neutral theory of molecular evolution (). In contrast to the few genes that were previously available, the complete genomes of many species that are now accessible for inferring evolutionary relationships constitute large quantities of data that lead to reduced estimation errors associated with site sampling, to very high power in the rejection of simple evolutionary hypotheses and to high confidence in estimated phylogenetic patterns. Traditionally, phylogenetic analyses based on many genes combined data into a contiguous block, a practice that is still commonly used today. Under this concatenated model, all genes are not only assumed to evolve under the same tree but also to evolve at the same rate. Although multigene datasets have the advantage of providing greater resolutionwith more information it is likely to find trees that more accurately reflect evolutionary historyit may prove challenging to account for the heterogeneous nature of the data. Different genes undergo different selective pressures, and the degree of site rate heterogeneity may vary from gene to gene (). Likelihood calculations on trees have been shown to clearly benefit from accommodating different evolutionary pressures (as observed in different codon positions, or different genes) in heterogeneous data (). Selection is a key evolutionary process in shaping genetic diversity and a major focus of phylogenomics investigations (). Using statistical methods in evolutionary genetics, researchers frequently evaluate the strength of selection operating on genes or even individual codons in the entire phylogeny or in a subset of branches. Codon substitution models have been particularly useful for this purpose because they allow estimating the ratio of non-synonymous and synonymous substitution rates (dN/dS) in a phylogenetic framework. Two different versions of codon substitution models were simultaneously introduced that both allowed estimating a single dN/dS ratio across all sites and branches (). Various extensions have since been proposed, such as codon models that model variation in dN/dS among sites complemented with empirical Bayes approaches to identify the sites under specific selection regimes (). Codon substitution models can to some extent be approximated by partitioning nucleotide models according to codon positions, which accommodates differences in evolutionary dynamics at the three codon positions. For example, Yang (1996a, b) takes into account the nucleotide frequency bias, the substitution rate bias and the difference in the extent of rate variation among the three codon positions and shows that incorporating these features can yield drastically different divergence time estimates compared with models not incorporating this complexity. Although full codon models approximate biological reality more closely, codon position partitioned nucleotide models have the advantage to be far more computationally efficient. While Yang (1996a, b) only used the HKY evolutionary model () in his analyses,also included the GTR model (Tavare,Tavare, 1986) in a comprehensive model evaluation study (see also).showed that codon position partitioned nucleotide models are biologically motivated, computationally practical alternatives to codon models for the analysis of protein-coding sequences. It is therefore not surprising that such approaches are also being exploited for detecting positively selected sites (). The large state space of full codon substitution models renders these models computationally expensive compared with standard nucleotide substitution models, which explains why they are frequently fit to trees to scrutinize selection processes but generally not used to reconstruct phylogenies. However, as computational power has increased, phylogenetic inference using codon-based models is becoming more and more realistic. Here, we examine whether the use of full codon models is feasible in the phylogenomics era by fitting several codon models to a full genome mitochondrial dataset in a Bayesian framework. We use state-of-the-art model-selection approaches to assess whether increased biological realism, as obtained by modelling gene-specific properties, goes hand in hand with increased model performance. Finally, we provide estimates of computation time required on both multi-core CPU systems and a system equipped with one of the latest graphics cards available.
METHODS
DataTo examine the differences in model fit between nucleotide and codon models and the performance gains that graphical processing units (GPUs) afford in statistical phylogenetics, we fit a range of evolutionary models (see next section) to the mitochondrial genomes from 62 extant carnivores and a pangolin outgroup (). This genomic sequence alignment contains 10 869 nt columns that code for 12 mitochondrial proteins and when translated into a 60-state vertebrate mitochondrial codon model, yields a total of 3623 alignment columns, of which 3601 site patterns are unique. Conducting a Bayesian phylogenetic analysis on such an extensive dataset using a codon substitution model would take up an unpractically large amount of time, but the computational hindrance has to a large extent been removed by recent developments that exploit a special codebase in CUDA to perform the calculations on graphics cards (). The analyses reported there could at that time not be run in double precision (i.e. increased precision for floating point numbers to minimize rounding errors) on a single graphics card due to memory restrictions. With the advent of new graphics cards and the release of the BEAGLE library (), we here revisit codon substitution model estimates for the carnivores dataset.
Evolutionary modelsGoldman and Yang (1994) and Muse and Gaut (1994) developed the first codon-based evolutionary models (GY and MG, respectively), i.e. models that have codons as their states, incorporating biologically meaningful parameters such as transition/transversion bias, variability of a gene and amino acid differences. While nucleotide models have 4 states and amino acid models have 20 states, a full vertebrate mitochondrial codon model has 60 states (ignoring the four nonsense or stop codons). Both the GY and MG codon models assume that mutations occur independently at the three codon positions and therefore only consider substitutions that involve a single-nucleotide substitution. As in nucleotide models, codons are also assumed to evolve independently from one another. Although more realistic codon models have been proposed, e.g. for mammalian genes (), we restrict ourselves to the standard GY codon substitution model implementation in BEAST () and allow for substitution rate heterogeneity among codons using a discrete gamma distribution (i.e. each codon is allowed to evolve at a different substitution rate) (Yang, 1996a, b).
Model selectionWe use state-of-the-art marginal likelihood estimation techniques to compare different models using Bayes factors. In particular, we focus on recent implementations of path sampling (PS) and stepping-stone sampling (SS) in BEAST (), which have been previously introduced in Bayesian phylogenetics by Lartillot and Philippe (2006) and, respectively. Although these approaches are computationally demanding, they considerably outperform the still widely used harmonic mean estimator (HME). The HME has been shown to systematically overestimate the marginal likelihood, even for simple (Gaussian) cases, while PS and SS estimate the marginal likelihood with much lower error (). A recent study comparing the performance of HME, PS and SS as marginal likelihood estimators for coalescent models and molecular clock models in BEAST () has also demonstrated that PS and SS yield reliable model selection whereas the HME performs poorly. Further evaluations of these estimators were carried out in the context of a comparison between model selection and model averaging (), providing additional evidence against the use of the HME and sHME. Here, we include the HME and sHME for the sake of comparison and it serves to illustrate that they may lead to flawed conclusions. The HME only requires samples from the posterior and can therefore be calculated from an MCMC sample that is obtained by a standard Bayesian phylogenetic analyses under a particular model. If one collects n samples from the posterior, the HME is estimated as followswith pYjM the marginal likelihood and pYj i , M the likelihood (with M the model under evaluation). The sHME is based on a mixture of the prior and the posterior, but in practice it only uses samples from the posterior (). If one collects n samples from the posterior, the sHME is estimated as followsBEAST uses a value of 0.01 and a simple, readily converging iterative scheme to calculate the sHME. PS and SS rely on drawing MCMC samples from a series of distributions, each of which is a power posterior differing only in its power, along the path going from the prior to the unnormalized posterior defined by the model M.where pYj, M is again the likelihood function and pjM the prior. This formulation ensures that the power posterior is equivalent to the posterior distribution when  1:0 and equivalent to the prior distribution when  0:0. Our implementation of PS in BEAST () involves adapting the original PS assumptions, i.e. spreading the different values of evenly between 0.0 to 1.0 and only collecting one sample from each power posterior (before is updated).found that the efficiency of PS can be considerably improved by choosing values according to evenly spaced quantiles of a Beta, 1:0 distribution rather than spacing values evenly from 0.0 to 1.0. As suggested by, we use a value of  0:3, which results in half of the values evaluated being50.1. We have also chosen to use multiple samples per , leading to the following expression for the (log) marginal likelihood, assuming K  1 path steps, which yield a collection of samplesFor SS, we followin calculating the marginal likelihood using n samples from a series of K  1 power posteriors as followsTo perform the marginal likelihood estimations, we use the BEAST software package () to draw inference under the standard nucleotide and the codon partition models as well as estimating the marginal likelihoods for those models using the different Monte Carlo estimators presented earlier. We use BEAST in combination with BEAGLE (), an application programming interface (API) and library that enables one to exploit the massive parallelism that modern-day graphics cards (GPUs) have to offer, for the computationally demanding codon models. We compare the performance of two (re)scaling schemes that BEAGLE offers when calculating (the partial) likelihoods to help avoid roundoff (): 'delayed' (which postpones scaling until the first underflow or overflow) and 'always' (which yields a better precision in each iteration at an increased computational cost). The posterior-based estimators were run for 10 million iterations, discarding the first million as burn-in (2 million for the codon-based models). PS and SS were run for an initial burn-in of NVIDIA GTX 680 (1 GPU) and GTX 690 (2 GPUs) graphics cards, which have taken a performance hit in their scientific computing capabilities at the expense of a newly released range of NVIDIA workstation cards, which have more graphics memory and higher double precision performance and should hence be more suited for scientific computing.
RESULTSWe first compared the standard HKY and GTR nucleotide models, assuming varying rates across sites [modelled using a discrete gamma distribution with four rate categories (Yang, 1996a, b)], with the HKY-based and GTR-based codon position partitioned nucleotide models analysed in the work ofand the codon model of Goldman and Yang (1994), also accommodating varying substitution rates across codons. For each of the models, the marginal likelihoods were calculated assuming both a strict clock and a uncorrelated relaxed clock with an underlying lognormal distribution (UCLD) (). The results of this model comparison are listed in. We consider the common codon partition (CP) models where all three codon positions are considered separately (denoted CP 123 ) and where the first and second codon position are grouped together (denoted CP 112 ), as the third codon positions generally evolve much faster than the first and the second codon positions. None of the marginal likelihood estimators inselects the GY94 codon model as the best-fitting mode, indicating that increased biological realism offered by explicitly model substitution in codon space is not reflected in an increased model fit. The model that seems to most adequately capture the substitution complexity in the data is a full fledged codon position partitioned nucleotide model, which assumes a separate GTR model for each codon position, as well as different rate heterogeneity patterns and nucleotide frequency compositions across codon positions (the partitioning also specifies different relative rates for the codon positions). The various estimators consistently select that model as the best model, with apparently only small differences in the ranking of the models. We note, however, that the HME and sHME yield drastically different estimates for the log marginal likelihood compared with PS and SS, which supports earlier claims that such posterior-based estimators overestimate the marginal likelihood (). The GY94 codon model ranks among the various codon position partitioned nucleotide models, albeit in different parts of the overall ranking, depending on the estimator used. Standard nucleotide models, with or without varying rates across lineages, appear to be too simplistic for this dataset, as established by all the model-selection approaches. For each of the models tested, a relaxed molecular clock (with underlying lognormal distribution; UCLD) is consistently shown to outperform a strict clock. Next, we introduce gene-specific partitioning for the models tested in. To this purpose, we assume one model instance for each of the 12 genes present in the dataset, resulting for example in 12 GY94 codon models being estimated and 36 GTR models being estimated for the most parameter-rich codon position partitioned models, along with 36 gamma distributions being estimated and 36 sets of empirical frequencies being used. For each of these models, we have calculated marginal likelihoods assuming both a strict and a relaxed clock ().Note: Posterior-based model selection approaches (i.e. HME and sHME) were run for 10 million iterations (of which 1 million serve as burn-in; 2 million for the codon models), while PS and SS were run for 64 path steps/ratios with 250 000 iterations (25 000 iterations burn-in) per path step/ratio after an initial 1 million iterations that serve as burn-in (2 million for the codon models). Rescaling in BEAGLE was set to default (delayed).The gene-specific partitioning tested inyields a different model ranking compared with. Whereas a gene-independent codon model was outperformed by its codon position partitioned nucleotide approximation, this is no longer the case when gene-specific evolutionary patterns are taken into account. The gene-specific codon model that allows for different (relative) rates between the different genes, with a different gamma distribution to model varying rates across sites within each gene, achieves the best performance as assessed by both PS and SS. However, the posterior-based model-selection approaches (HME and sHME) fail to detect this, and also other subtle differences can be observed for these estimators when comparing various versions of the gene-specific codon position partitioned nucleotide models in. The strong tendency to overestimate marginal likelihoods by the HME is also reflected inreveals that each of the models analysed benefits from taking into account gene-specific properties of the dataset studied, i.e. that it is composed of 12 protein-coding genes (seefor gene-specific estimates of the key parameters of the codon model). One gene that stands out from all the others is the ATP8 gene, exhibiting a dN/dS ratio that is between 3 to 15 times higher than that of the other genes and the lowest transition/transversion ratio of the genes considered. Further, the ATP8 gene is the only gene with a rate heterogeneity parameter lower than 1, implying that most sites have very low substitution rates (Yang, 1996a, b), but few sites also have high rates, whereas all the other genes have on average more intermediate rates across sites. The COX1, COX2, COX3, CYTB and ND1 genes have clearly lower dN/dS ratios, and the CYTB, ND1 and ND3 genes have a much higher transition/ transversion ratio. Finally, allowing for different relative rates between the different genes shows that certain genes (such as ATP6 and CYTB) may evolve twice as fast as some of the other genes (e.g. ATP8, COX1 and COX2). Gene partitioning allows capturing variation in the substitution process but considerably increases the number of parameters that need to be estimated, in particular for the GTR-based codon position partitioned nucleotide substitution models. Because standard diffuse priors offer little protection againstNote: Posterior-based model-selection approaches (HME and sHME) were run for 10 million iterations (of which 1 million serve as burn-in; 2 million for the codon models), while PS and SS were run for 64 path steps/ratios with 250 000 iterations (25 000 iterations burn-in) per path step/ratio after an initial 1 million iterations of burn-in (2 million for codon models). Rescaling in BEAGLE was set to default (delayed). over-parameterization, and prior specification impacts marginal likelihood estimates, we also explore HPM approaches allowing to share information between different genes through hierarchical prior specification (). We put hierarchical priors on the gene-specific and ! parameters of the GY94 codon model, on the parameters and the five free evolutionary parameters of the GTR model. Log marginal likelihoods, shown in, were calculated using the same settings as in Tables 1 and 2. The HPM approach offers marked improvements of the marginal likelihoods, and as expected, this is more pronounced for the parameter-rich codon position partitioned nucleotide models (about 300 log units higher) compared with the codon model (about 100 log units higher). As a consequence, a GTR-based nucleotide model partitioned according to gene and codon position now yields the highest marginal likelihood. Similar to the partitioning with independent prior specification, we can notice discrepancies in model selection outcome between the HME and sHME on the one hand, and PS and SS on the other (). In the past decades, codon models have largely been avoided for phylogenetic reconstruction because they are computationally prohibitive. Recent development of dedicated APIs and high-performance computing libraries have, however, made it possible to harness the large numbers of computing cores available in graphics cards, which is particularly useful in the case of datasets with many unique site patterns (), such as the one analysed here. At the time of the introduction of these techniques in statistical phylogenetics, the dataset analysed here could not be analysed in double precision on what were state-of-the-art graphics cards at that time because they were limited in the amount of memory. Hence, to analyse this dataset using codon models, three graphics cards were combined in the analysis of this dataset (), amounting to a considerable cost, we compare the performance of a GTX 590 graphics card and a 40-core Xeon(R) E7-4870 2.40 GHz system using a auto-sizing thread pool, two different BEAGLE rescaling schemes and different numbers of BEAGLE instances, both for a single codon model and a codon model for each gene. We demonstrate that, for a single codon model and using a delayed rescaling scheme (the default in BEAGLE) with a single-core instance, a state-of-the-art graphics card offers a speed increase with a factor over 30 compared with a singlecore instance on a modern CPU running BEAST (i.e. without BEAGLE). Because the number of BEAGLE instances divides the likelihood calculations into two or more parts, thereby allowing each core to calculate part of the likelihoods corresponding to unique site patterns, multi-core or multi-GPU systems may benefit from using two or more BEAGLE instances. The speedup further increases to a factor 460 when two such BEAGLE instances are used, which allows for both GPUs on the graphics card to be used and half of the site patterns being calculated on each GPU. The speed increase provided by a multi-core CPU system levels off at a factor of 12, with the maximum performance being reached when using approximately 32 cores, illustrating that a costly multi-core CPU architecture cannot achieve the same degree of speed ups as a graphics card. No further increases in performance by using additional cores could be obtained. Although we cannot exclude communication latency as a possible cause for this observation, the most likely explanation is the considerable difference in memory bandwidth between both systems, with the GTX 590 sporting a theoretical total memory bandwidth of 327.7 Gb/s, much higher than the performance of a multi-core CPU system such as ours, yielding a typical memory bandwidth of 67 Gb/s [benchmarked using a multithreaded version of stream;. The situation is different when attempting to fit independent codon models to each of the 12 genes and each with its own gamma distribution to model site heterogeneity. The partitioning strategy implies a change in the meaning of BEAGLE instances; the specification of 12 partitions translates into 12 likelihoods that need to be evaluated, which are naturally distributed over multiple cores as 12 instances. The specification of more BEAGLE instances partitions the likelihood calculations even more by splitting further each partition (e.g. two instances would yield 24 sets of likelihood). This implies that there is little benefit in using instances on the GTX 590 graphics card, as each of the GPUs already handles six likelihood sets. However, this approach may still profit from a 40-core CPU system, where initial increases in the number of BEAGLE instances yield the highest speedups, but there is a diminishing
DISCUSSIONThe Bayesian phylogenetic model comparison we present here consistently shows that partitioning by gene yields an increased model fit. Using standard diffuse priors, a separate codon model for each gene accompanied with gene-specific among-codon rate variation and gene-specific relative substitution rates offers the best performance, followed by codon partition models and trailed by standard nucleotide models. However, when substituting the independent diffuse priors by a hierarchical prior specification over the gene-specific parameters, a more parameter-rich GTR-based nucleotide substitution model partitioned according to gene and codon position emerges again as the best fitting model. These results can only be uncovered using recent model-selection approaches, such as path sampling and stepping-stone sampling. By demonstrating an increased model fit for gene partitioning, we corroborate the results of earlier studies, e.g. by, who combined morphology and nucleotide data from four genes in a study on model heterogeneity across data partitions. Through Bayes factor comparisons the authors showed a dramatic increase in model fit when extending twopartition models (one partition for the morphology data and one joint partition for the four genes) to five-partition equivalents (one partition for each of the four genes), emphasizing the importance of accommodating across-partition heterogeneity. The authors also showed that within-partition rate variation was by far the most important model component (i.e. much more than across-partition heterogeneity), but that the difference in fit between substitution models was only pronounced when comparing JC () and GTR (Tavare,Tavare, 1986). It is important to note that the Bayes factor comparisons reported in () are calculated using the HME (). Because convincing evidence has been presented for the poor performance of the HME in recent years (,b), we advocate for caution when interpreting such results and encourage the use of PS and SS (over HME and sHME). Consistent with the increased model fit for the gene-partitioned models, we observed considerable variation in evolutionary parameters across the 12 mitochondrial genes. We examined whether this parameter variation observations could be associated with the asymmetrical mutation bias gradients in vertebrate mitochondrial genomes. Faith and Pollock (2003) andobtained the same relative gene order with respect to the duration of the single-strand state of the parental H strand: COX15COX25ATP85ATP65COX35ND35 ND4L5ND45ND15ND55ND25CYTB. When comparing the relative orders of , , ! and the relative rate inagainst this relative gene order, only ranking according to the relative rate results in a similar order, with the exception of ATP6 and the ND genes: COX15COX25ATP85COX35ND4L 5ND55ND15ND25ND35ND45ATP65CYTB. Our comparison of independent diffuse priors and hierarchical priors for the gene partitioned models illustrates the impact of prior specification on marginal likelihood estimation and model selection. We have previously highlighted that the outcome of Bayesian model selection is dependent on prior choices (). Through hierarchical prior specification, HPMs offer a middle ground between the extreme scenarios of independently fitting different models across genes and fitting a single model to all genes (). While accommodating parameter variation among genes will be appropriate in most cases, there is less information available in each gene to inform the gene-specific parameters. HPMs allow borrowing of strength of information from one partition by another, providing more precise gene-specific parameter estimates, and resulting in further model improvements in our comparisons. BEAST supports a generic implementation of hierarchical prior specification and HPM approaches can therefore be applied to different problems, such as HIV within-host evolution for different (groups of) patients () and phylogeographic problems (). By adopting hierarchical prior specification across gene-specific parameters in GTR nucleotide substitution models with both gene and codon position partitions, we notice a better model fit compared with gene partitioning with a standard codon substitution models. However, we are essentially comparing the most complex parametrization among conventional nucleotide substitution models with the simplest codon substitution model, and many assumptions can still be relaxed to make codon models more realistic. To illustrate this point, we observe a marginal likelihood improvement of about 300 log units or more between the HKY-based and GTR-based codon position partitioned nucleotide models. So, we also expect model fit improvements for a codon model that would consider different substitution rates for the different types of nucleotide substitutions within a codon, instead of merely distinguishing between transitions and transversions as is done in the GY94 model. Such a GTR-type of model applied at the nucleotide level, but with the constraint that the nucleotide sequence must encode some fulllength amino acid sequence, is the rationale of the codon substitution models in the style of the codon model of Muse and Gaut (1994). The codon model of Muse and Gaut (1994) may offer a more realistic parameterization than the GY94 model, which has no natural mechanistic interpretation at the nucleotide level (), resulting in a possible increase in model fit over the GY94 model. More importantly, codon models can model varying selective pressure, but a gene-specific dN/dS is a very coarse approximation of this variation and many realistic codon codons now accommodate among-site variation in dN/dS (), in dN and dS separately (Kosakovsky), and/or among lineage variation in dN/dS (). Further research is needed to implement such models in BEAST () and to assess their model fit. For each of the models tested in this manuscript, be it with or without gene partitioning, a relaxed molecular clock (with underlying lognormal distribution; UCLD) is shown to outperform a strict clock, using all of the estimators. Given that mammalian datasets of mitochondrial DNA exhibit a wide variation in substitution rate across lineages, additional clock models, such as autocorrelated [see e.g.] or random local clocks () should ideally be included in our model comparison. For example,have shown that the distribution of estimated mitochondrial substitution rates across species shows a very large variance, with the rates spanning two orders of magnitude. The authors also show that the family taxonomic level explains 75% of this variance, while the order taxonomic level explains 21%, indicating that entire orders could all have (for example) low substitution rates, which may be appropriately modeled using autocorrelated relaxed clock models (). Finally, we have reported massive increases in computation speed using the BEAGLE library for BEAST in combination with the latest graphics cards. However, the GTX 590 we used here is essentially designed to offer tremendous single precision performance, as required for visualization purposes in the gaming community. Hence, its double precision performance is not keeping the same development pace, which will also be the case for future cards from the same series. Double precision performance has recently increased with the advent of a new line of nVidia Tesla K20 graphics cards, designed for scientific computing. Further research will be needed to determine to what extent these new cards can improve efficiency in the field of phylogenetics.
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Bayesian model testing at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
.5 million iterations, after which 64 power posteriors were run for 275 thousand iterations each, discarding the first 25 thousand iterations as the burn-in. 2.4 Prior specifications In recent work, we have shown the importance of using proper priors (probability distributions that integrate to 1) when performing Bayesian model selection and by extension, when performing Markov chain Monte Carlo analyses (Baele et al., 2013a). The frequently used constant function, often inaccurately called a uniform distribution, over an infinite interval is an example of an improper prior; the use of such priors may lead to a posterior distribution that does not exist. Further, recent modelselection approaches such as PS and SS explicitly sample from the prior distribution when calculating the marginal likelihood. It is important to stress that an improper prior distribution frequently leads to an infinite marginal likelihood (even if the estimation method returns a non-infinite value), which in turn implies that the Bayes factor is not well-defined (Friel and Petitt, 2008), making inference based on improper priors highly suspect. Despite this importance, attributing little attention to proper prior specification has unfortunately been common practice when calculating Bayes factors in phylogenetics, which can inadvertently affect model comparison conclusions. We use the following priors in our analyses: a birthdeath process (Gernhard, 2008) was used as a tree-prior with a diffuse normally distributed prior on the log growth rate and a uniform prior (between 0.0 and 1.0) on the relative death rate; a diffuse normally distributed prior on the log transition/transversion parameter of the HKY model and of the GY94 codon model; a diffuse normally distributed prior on the log coefficient bearing on the non-synonymous/synonymous rate ratio; diffuse gamma distributed priors on the relative rate parameters of the general time-reversible (GTR) model (Tavare,Tavare, 1986); an exponential prior on (each of) the rate heterogeneity parameter(s) (Yang, 1996a, b); and an exponential prior on the standard deviation of the lognormal distribution specifying the rates of an uncorrelated relaxed clock model (UCLD). As an alternative to the diffuse priors for substitution model parameters in gene-partitioned models, we also explore hierarchical phylogenetic modelling (HPM) approaches (Suchard et al., 2003). HPMs use hierarchical prior distributions on the gene-specific parameters that are in turn characterized by unknown estimable hyperparameters, which ensures that within-partition parameters vary around an unknown common mean for each gene. 2.5 Hardware/Software Nucleotide substitution models and their codon position partitioned versions were fitted on a 40-core Intel Xeon(R) E7-4870 2.40 GHz system, using a single BEAGLE instance (Ayres et al., 2012). Given the relatively low dimensions of these models and the lower number of unique site patterns (compared with those of higher-dimension models, such as codon models), the proposed system provides reasonable computing performance, which will not be further discussed here. The use of codon models, considered to be more realistic models of sequence evolution in coding genes, however, requires considerable computational effort to evaluate the likelihood of phylogenetic histories, the number of which increases drastically with the number of sequences in the dataset. Parallel calculation of the finite-time transition probabilities is therefore key to speeding up such analyses, preferably on adequate hardware. We perform the codon-based analyses on a state-of-the-art desktop PC sporting a 3.4 GHz Intel Core i7 CPU and 16 GB of 1.6 GHz DDR3 RAM, equipped with an NVIDIA GTX 590 consisting of two GPUs and hence with a total 1024 CUDA cores and 3 Gb of GDDR5 RAM. At the time this study was initiated, this graphics card was the fastest consumer-oriented card available with satisfactory double precision performance for scientific computing. More recent releases include the
G.Baele and P.Lemey at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
