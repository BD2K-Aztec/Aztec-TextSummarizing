Motivation: With the development of high-throughput genomic and proteomic technologies, coupled with the inherent difficulties in obtaining large samples, biomedicine faces difficult small-sample classification issues, in particular, error estimation. Most popular error estimation methods are motivated by intuition rather than mathematical inference. A recently proposed error estimator based on Bayesian minimum mean square error estimation places error estimation in an optimal filtering framework. In this work, we examine the application of this error estimator to gene expression microarray data, including the suitability of the Gaussian model with normal– inverse-Wishart priors and how to find prior probabilities. Results: We provide an implementation for non-linear classification, where closed form solutions are not available. We propose a methodology for calibrating normal-inverse-Wishart priors based on discarded microarray data and examine the performance on synthetic high-dimensional data and a real dataset from a breast cancer study. The calibrated Bayesian error estimator has superior root mean square performance, especially with moderate to high expected true errors and small feature sizes. Availability: We have implemented in C code the Bayesian error estimator for Gaussian distributions and normal–inverse-Wishart priors for both linear classifiers, with exact closed-form representations, and arbitrary classifiers, where we use a Monte Carlo approximation. Our code for the Bayesian error estimator and a toolbox of related utilities are available at
INTRODUCTIONClassification is a major constituent of bioinformatics, in particular, phenotypic discrimination, which can be accomplished via many different data types, such as gene expression, protein expression or sequence data. The misclassification error of a classifier quantifies its predictive capacity, the key aspect of any scientific model. * To whom correspondence should be addressed.Thus, accuracy of the error estimation represents the salient epistemological issue in classification, model validity (). The main measure of error estimation accuracy is the root mean square (RMS) error of the estimator,where  tru and  est are the true and estimated errors of the classifier and E is expectation with respect to the random sampling procedure. Given a large data sample, the data can be split between training and test data, the classifier designed on the training data and classifier error estimated on the test data. In this scenario, there is a satisfactory distribution-free bound, RMS  1/2  m, where m is the size of the test sample (). However, when the sample is small, splitting the data is unacceptable because the classifier will be trained on too small a set, thereby resulting in poor classifier design. Thus, in small sample settings (the concern of this article), a classifier is trained and its error estimated on the same data. A number of training data-based error estimators have been proposed in the past and we will consider several in this article. Perhaps the one most commonly employed in bioinformatics is cross-validation. In this method, the data are partitioned into k folds (subsets); at each state of the procedure, one fold is held out, a surrogate classifier trained on the remaining folds and its error estimated on the held-out fold. The error of the classifier (originally trained on the full sample) is estimated by the average surrogate errors on the left-out folds. In the special case k = n, the sample size, each held-out fold consists of one point and the error method is termed 'leave-one-out'. For leave-one-out, there is only one partition of folds; however, when k < n evaluating all combinations of partitions is computationally prohibitive. Hence, in this case partitions are randomly chosen to make the estimation. Although cross-validation is close to being unbiased if k is not too small, it tends to have a large variance for small samples (Braga) and also to be poorly correlated with the true error (), the two combining to create a large RMS for small samples [for a review of error estimation performance, see. A natural question arises: can cross-validation be used for small samples or, equivalently, are there small-sample cases in which the RMS of cross-validation is sufficiently small so that it can be considered a valid error estimator? To answer this question, one might first ask if it is possible to use distribution-free bounds. NotPage: 1823 18221831only are there very few cases in which such bounds are known, but also when they are known they are so loose as to be useless in practice. For instance, consider the following leave-one-out RMS bound for the k-nearest neighbor classification rule with random tie breaking ():
Application of the Bayesian MMSE error estimator to microarray dataIf k = 3 and the sample size is n = 100, then the bound is approximately 0.353, which is useless. Let us now consider bounds when there are distributional assumptions. We consider a feature-label distribution having two equally probable Gaussian class-conditional densities sharing a known covariance matrix and the linear discriminant analysis (LDA) classification rule. For this model, we possess analytic representation of the joint distribution of the true error with the leave-one-out estimator ().shows the RMS to be a one-to-one increasing function of the Bayes error for dimensions p = 5,10,25, and sample sizes n = 20,40,60, the RMS and Bayes errors being on the y and x axes, respectively. In this model, where the Bayes error is a function of the distance between the classconditional means, the maximum RMS is bounded and does not exceed 0.15, even with only 20 sample points. Moreover, if one wishes to bound the RMS below some tolerance, , one need to only make an assumption on the minimum distance between the means, which corresponds to a maximum Bayes error. This kind of behavior, where the RMS of leave-one-out is tolerable when the Bayes error is small, is often observedindeed, we see this inof this articlebut it has only been quantified in a small number of cases (Braga). The upshot of these considerations is that if cross-validation is going to be used when the sample size is small, there must be modeling assumptions to make the RMS acceptable. Hence, why not take a Bayesian minimum mean square error (MMSE) approach and thereby guarantee that the average RMS across the model family for the resulting error estimator is minimal among all possible error estimators? That is what is done in Dalton and Dougherty (2011a, b), where a parameterized family of classconditional feature distributions is assumed, a prior distribution is applied to the parameters of the model, and this prior along with observed data are used to compute an unbiased, MMSE estimate of classification error. An advantage of this approach, besides achieving average minimum RMS across the model family, is that it depends only on the form of the designed classifier, not the classification rule used to design the classifier. In particular, it is independent of the feature selection method, which is part of the classification rule. Two problems naturally arise. First, how does one arrive at a prior distribution governing the model? This issue arises in any Bayesian method and, as previously explained, would arise in the context of small-sample error estimation even if one were to use a classical error estimator. The current paper proposes a method to determine a prior distribution when using microarray data. The second issue is the difficulty of deriving an analytic expression for the Bayesian MMSE estimator. This is done for discrete classification under a family of generalized beta prior distributions in Dalton and Dougherty (2011a) and for linear classifiers applied to Gaussian distributions under normalinverseWishart prior distributions in Dalton and Dougherty (2011b). While we are not advocating the abandonment of analytic methods, it is practically useful to have software that can arrive at the Bayesian MMSE estimator via Monte Carlo methods. Currently, approximation is necessary when using a non-linear classifier, where a closed form solution for the model is not known. This article develops and provides publicly available software.
SYSTEMS AND METHODS
Modeling microarray dataWe assume two classes and require the training sample to consist of normalized log ratios. Thus, use of normalization schemes such as total intensity normalization or the LOESS method, which are popular transformations before high-level analysis is applied, are required. Logtransformed gene expression values have nearly Gaussian class-conditional distributions (with unknown parameters) (). To further validate a Gaussian modeling assumption, during feature selection we will permit only features that pass a ShapiroWilk Gaussianity test. Note that Bayesian error estimators designed under the Gaussian model are robust in the sense that performance is still good when the true distributions are Johnson distributions (), which are a class of non-Gaussian distributions with four free parameters to control mean, variance, skewness and kurtosis. Normalinverse-Wishart priors compose a flexible class of distributions with many degrees of freedom to facilitate calibration of the priors to gene expression microarrays. Further, this family of priors possesses a fast closedform solution when used with linear classification. In problems where the Gaussian model applies and one wishes to use a linear classifier, the benefit one might gain by having more control over the prior is not worth the much greater amount of time required to run an integral approximation code and the effort of designing a specialized model, especially for small samples where one cannot afford a very complex model anyway. Hence, we focus on calibrating normalinverse-Wishart priors. Assuming the parameters between classes are fairly independent, we have justified the assumptions posed by Dalton and Dougherty (2011a), the others being that the class-conditional distributions are relatively Gaussian and that normalinverse-Wishart priors are adequate for representing prior knowledge. We are left to devise a method of generating priors for the mean and covariance of each class.That is, the mean conditioned on the covariance is Gaussian with mean m and covariance /, and the marginal distribution of the covariance is an inverse-Wishart distribution. The hyperparameters of () are a real number   0, a length D real vector m, a real number  and a non-negative definite DD matrix S. For linear classification, we also restrict  to be an integer to guarantee a closed form solution. The hyperparameters m and S can be viewed as targets for the mean and the shape of the covariance, respectively. The larger  is the more localized the prior is about m, and the larger  is the less the shape of is allowed to wiggle. Given n y observed sample points, we update the prior for class y to a posterior,  * . This posterior has the same form as the prior, with updated hyperparameters given by
Thewhere  and are the sample mean and sample covariance of points from class y, respectively. To ensure a proper prior, we require >D1, S positive definite, and >0. These restrictions are not mandatory as long as the posterior is proper with  * > D1, S * positive definite and  * > 0. Assuming the a priori probability of class 0 is uniform between 0 and 1 and assuming prior (and posterior) independence between this and the distribution parameters in each class, the Bayesian MMSE error estimator can be expressed aswhere n = n 0 +n 1 is the total number of sample points.may be viewed as the posterior expectation of the error contributed by class y. With a fixed classifier and given , the true error,where y is the parameter space of class y. For non-linear classifiers, this integral must be approximated with Monte Carlo methods. For a linear classifier, i.e. a classifier of the formwhere g(x) = a T x+b with some constant vector a and constant scalar b,whereand I x;, is the regularized incomplete beta function. For positive integer N, I x; 1 2 , N 2 has a closed form solution, in particular,(N2 )/2 k=0(2k 1)!! (2k)!!Page: 1825 18221831
Application of the Bayesian MMSE error estimator to microarray data95% confidence in both classes are used, unless there are not enough features passing the test, in which case we select a fixed number of features with the highest sum of the ShapiroWilk test statistics in each class. In the final stage of feature selection, we reduce the feature set to D features. This is done either by applying a t-test if it has not already been applied in the first stage or by using the same t-test statistics from the first stage to pick the D most differentially expressed Gaussian features. This implementation employs classifier-independent feature selection schemes, such as the t-test and ShapiroWilk test. However, even for classifier-dependent schemes, once the feature selection and classification schemes have been implemented, the Bayesian error estimator (BEE) may be calculated as a deterministic function of the fixed classifier. This is in contrast to cross-validation, which uses surrogate classifiers to estimate the error of the designed classifier.
Estimating prior hyperparametersWhen calibrating priors for microarrays, what data should be used and how? With the explosion of microarray experimentation over the last decade, the genomics community has amassed an enormous database of gene expression data, and trends in the entire history of microarray experimentation could be used to find a prior, perhaps conditioned on a particular organism, tissue, gene and/or type of abnormality, depending on the nature of the experiment at hand. However, different microarray experiments are currently very difficult to compare, although there have been some recent efforts to normalize and integrate different datasets (). The method employed here uses discarded gene expression data, consisting of a subset of the features from the microarray data that are not used for classification, to calibrate the priors of the Bayesian error estimator. Though these features are not used in the actual classifier, they may implicitly contain useful calibration information such as the varying concentrations of DNA material used in each microarray, background intensities and other characteristics of the digitized images of a microarray slide. And although calibration requires a large amount of data and in microarray gene expression analysis we typically expect a very small sample setting, the huge number of discarded features ensures that there is enough data for a successful calibration of the hyperparameters. It is possible to define a prior on the entire feature set and to compute the Bayesian error estimator over the reduced feature set based on the marginal distribution of this prior on only the selected features. However, the following approach directly defines a prior on only the selected features. We essentially use a method of moments approach to calibrate the hyperparameters; however, estimating a vector m and matrix S may be problematic for a small number of sample points, so to simplify the analysis we assume the following structure on these hyperparameters:where m is a real number,  2  0, and 1    1. This structure is justified because prior to observing the data, there is no reason to think that any feature, or pair of features, should have unique properties. With this simplification, our problem is now reduced to estimating five scalers for each class: , m,In the first stage of a method of moments approach, we find the theoretical first and second moments of the random variables  and (random because of the prior distribution applied to them) in terms of the hyperparameters we wish to estimate. Throughout the remainder of this section, a subscript i represents the i-th element of a vector, and a subscript jk represents the j-th row, k-th column element of a matrix. First consider the parameter , with a marginal prior having an inverseWishart distribution with hyperparameters  and S. The mean of this distribution is well known (),and given the previously defined structure on S, we obtainDue to our imposed structure, only Eand Eare needed. The variance of the j-th diagonal element in inverse-Wishart distributed may be expressed aswhere we have applied Equation (4) in the second equality. Solving for ,We next consider the mean, , which is parameterized by the hyperparameters  and m. The marginal distribution of the mean is a multivariate Student's t-distribution given by Rowe (2003):The mean and covariance of this distribution are well known:With the assumed structure on m, we obtainFinally, our objective is to approximate the expectations in Equations (4) through (8) using calibration features left out of the classification scheme. Suppose the calibration data for the current class consists of n sample points with E D features. Let  E be the sample mean and E be the sample covariance matrix of the complete set of E features in the calibration data. From these we wish to find several sample moments of  and in our original D feature problem, that is, to find E, Varand Var 11 , where the hats indicate the sample moment of the corresponding quantity. All these are scaler quantities. To compress the set of E features in the calibration data to solve an estimation problem on just D features, and ultimately to find these scaler sample moments in a balanced way, we emulate the feature selection process by assuming that the selected features are drawn uniformly. Since any of the E features is equally likely to be selected as the i-th feature, the sample mean of the mean of the i-th feature, E, is computed as the average of the sample means of all E features in the calibration data. This result is the same for all i, and we use Eto represent all features. In particular,Thanks to uniform feature selection, all other moments are balanced over all features or any pair of distinct features. The remaining sample moments are obtained in a similar manner: VarPage: 1826 18221831
L.A.Dalton and E.R.DoughertyHere, Var  1 represents the variance of each feature in the mean. We also have Eand Erepresenting the sample mean of diagonal elements and off-diagonal elements in , respectively. Finally, Var 11 is the sample variance of the diagonal elements in. Plugging our sample moments into Equations (4) through (8), we obtainNote Equation (6) for  was plugged into Equation (4) to obtain the final  2. In sum, calibration for the prior hyperparameters is defined byEquations (14) through (18), the sample moments being given in Equations (9) through (13). The estimates of  and  can be unstable, since they rely on second moments, Var 11 and Var  1 , in a denominator. These parameters can be made more stable by discarding outliers when computing the sample moments. Herein, we discard the 10% of the  E i with largest magnitude and the 10% of the E ii with largest value. This method is one of many possible approaches; for simplicity and to avoid an over-defined system of equations, we do not incorporate the covariance between distinct features in  [that is,, the variance of off-diagonal elements in, or the correlation between distinct elements in , though it may be possible to use these to improve the estimates of the hyperparameters. It may also be feasible to use other estimation methods, such as maximum likelihood. Furthermore, the method proposed here to calibrate the priors is a purely data-driven technique for easy and general application to microarray experiments. Ideally, the best way to calibrate priors would be to incorporate data and biological knowledge specific to the particular features selected for classification.
RESULTS AND DISCUSSIONWe present two sets of results demonstrating good performance of Bayesian error estimators, one on synthetic high-dimensional data with three-stage feature selection and a second based on breast cancer data with two stages of feature selection.
High-dimensional synthetic dataIn this section, we apply our Bayesian prior estimation method to synthetic high-dimensional microarray data. We use the same synthetic data model provided in, which models many observations made in microarray expression-based studies, including blocked covariance matrices to model groups of interacting variables with negligible interactions between groups. Our model emulates a full feature-label distribution with 20 000 total features. Features are categorized as either 'markers' or 'non-markers'. Markers represent features that have different classconditional distributions in the two classes and are further divided). into two subtypes: global markers and heterogeneous markers. Nonmarkers have the same distributions for both classes and thus have no discriminatory power, and are also divided into two subtypes: highvariance non-markers and low-variance non-markers. A summary of the feature types is shown in. Twenty features are global markers, which are homogeneous in each class. In particular, the set of all global markers in class i has a Gaussian distribution with mean  gm i and covariance matrix gm i. Within class 1, we assume each sample point belongs to one of two equally likely subclasses named 0 and 1, representing different stages or subtypes of cancer. Each subclass is associated with 50 heterogeneous markers, which are jointly Gaussian with mean  hm 1 and covariance hm 1. Sample points associated with the other subclass have the same distribution as class 0, which is Gaussian with mean  hm 0 and covariance hm 0. Each heterogeneous marker may only be associated with one subclass, thus there are 100 total heterogeneous markers in the model. We simplify the model by assuming that  gm i and  hm i have the form m i  1,1,...,1 for fixed scalers m i. We assume gm i and hm i have the form  2 i , where  2 i are constants and , not to be confused with the definition in Section 2.2, has a block covariance structure, i.e.with  being a 55 matrix with 1 on the diagonal and  = 0.8 off the diagonal. That is, we group markers into blocks of five features, where the blocks are independent from each other, and the markers within each block are correlated with a relatively high correlation coefficient to emulate a pathway. We generate 2000 high-variance non-marker features, which have independent mixed Gaussian distributions given by pN(m 0 , 2 0 )+ (1p)N(m 1 , 2 1 ), where m i and  2 i are the same scalers defined for markers. The random variable p is selected independently for each feature with a uniform distribution overand is applied to all sample points of both classes. These features can be viewed as genes regulated by mechanisms unrelated to those that regulate the class 0 and class 1 phenotypes. The remaining features are low-variance non-marker features, each having independent univariate Gaussian distributions with mean m 0 and variance  2 0 .In this model, heterogeneous markers are Gaussian within each subclass, but the class-conditional distribution for class 1 is a mixed Gaussian distribution (mixing the distributions of the subclasses), and is thus not Gaussian. Further, the high-variance features are also mixed Gaussian distributions, so this model incorporates both Gaussian and non-Gaussian features to challenge the ShapiroWilk Gaussianity test in the feature selection scheme. To simplify our simulations, we set the a priori probability of both classes to 0.5 and fix the parameters m 0 = 0 and m 1 = 1. We also define a single parameter
Page: 1827 18221831
Application of the Bayesian MMSE error estimator to microarray data, which specifies the difficulty of the classification problem. A summary of our synthetic high-dimensional data model parameters is given in. In all simulations, the values for  2 are chosen so that a single global feature (note that all global features are identical) has a specific Bayes error. We call this the 'Bayes error' in the remainder of this section, and it is given by  * = 1/(2) , where is the unit normal Gaussian cumulative distribution function, so for instance, we use  = 0.9537 for a Bayes error of 0.3. Under this high-dimensional model, we run several Monte Carlo simulations. In each experiment, we fix the training sample size, n, the number of selected features, D, and the difficulty of the classification problem via . The synthetically generated samples are non-stratified, meaning that in each iteration the sample size of each class is not fixed but determined by a binomial (0.5,n) experiment, and the corresponding sample points are randomly generated according to the distributions defined for each class. Once the sample has been generated, we apply the three-stage feature selection scheme outlined in Section 2.4. In the first stage, we apply a t-test to obtain 1000 highly differentially expressed features by removing most non-informative features. In the second stage, we apply a ShapiroWilk Gaussianity test and eliminate features that do not pass the test with 95% confidence. The number of features output in this stage is variable. If there are not at least 30 features that pass the test, then we return the 30 features with the highest sum of the ShapiroWilk test statistics for both classes. In the final stage, we use the same t-test values computed before to obtain the final set of D highly differentially expressed Gaussian features, which will be used to design our classifier. The 1000D features that pass the first stage of feature selection but are not used for classification are saved as calibration data. The feature selected training data are then used to train an LDA classifier. With the classifier fixed, 5000 testing points aredrawn from exactly the same distribution as the training data and used expressly to approximate the true error. Subsequently, several training data error estimators are computed, including leave-oneout (loo), 5-fold cross-validation (cv), 0.632 bootstrap (boot) and bolstered resubstitution (bol) (see the Supplementary Material for details). Two Bayesian error estimators are also applied, one with flat non-informative priors defined by   = 1 (flat BEE), and the other with priors calibrated as described in Section 2.5 (calibrated BEE). Since the classifier is linear, these BEEs are computed exactly. This entire process is repeated 120 000 times to approximate the RMS deviations from the true error for each error estimator. We first analyze the quality of features selected by the threestage feature selection algorithm.shows the percentage of selected features that are global features with respect to the expected true error of the designed classifier. We would like to graph performance with respect to Bayes error, which is a more pure measure of the difficulty of a classification problem, but evaluating Bayes error on our high-dimensional model is difficult and it may not be close to the true error of the designed classifier. Hence, in our graphs we focus on performance with respect to expected true error. Similarly,graphs against feature size with a fixed Bayes error of 0.3. Recall that this model uses 20 000 features, of which only 20 are global features that most effectively discriminate the classes. As long as the feature size is reasonable given the difficulty of the problem (expected true error and sample size), this percentage is quite large. However, infor sample size 60 we see that a feature size larger than 7 will result in <80% of the selected features being global features. This illustrates the necessity of restricting feature size in a small sample setting, and is consistent with earlier studies showing the difficulty of finding good feature sets when the number of features is large and the sample is small (). The graphs inshow the percentage of selected feature sets that are not rejected by a multivariate ShapiroWilk test on either class at a 95% significance level. There are several multivariate Gaussianity tests based on the ShapiroWilk statistic. We used Villasenor, which generalizes the classical univariate ShapiroWilk test to the multivariate case by transforming the data into a set of approximately independent standard normal random variables, and essentially summing up the standard ShapiroWilk statistic on each dimension. The results show that even though the three-stage feature selection algorithm only uses a univariate Gaussianity test, and univariate normality does not Page: 1828 18221831
L.A.Dalton and E.R.Doughertyimply multivariate normality, the resulting feature set still tends to have a high probability of passing the multivariate Gaussianity test. We next turn our attention to the RMS performance of error estimators under our synthetic high-dimensional model, where a summary of all simulation settings are available in. Our first battery of simulations inshows RMS deviation from true error for all error estimators with respect to expected true error for LDA classification with 1, 3, 5 or 7 selected features and either 60 or 120 sample points. Given the sample sizes, it is prudent to keep the number of selected features small to have satisfactory feature selection () and to avoid the peaking phenomena (). Lines marked with 'o' represent the Bayesian error estimator with flat priors, and lines marked with 'x' represent the Bayesian error estimator with the calibrated priors. The key point in these graphs is that the calibrated BEE has best performance in the mid and high range. For an expected true error of about 0.25 and n = 60, the RMS for the calibrated BEE outperforms 5-fold cross-validation for D = 1,3,5 and 7 by 0.0507, 0.0300, 0.0335 and 0.0379, respectively, representing 64, 32, 30 and 29% decrease in RMS, respectively. For n = 120, the decrease in RMS for D = 1,3,5 and 7 is 0.0366, 0.0175, 0.0192 and 0.0198, respectively, for 67, 34, 35, and 33 percent decrease in RMS, respectively. All other error estimators typically have best performance for low expected true errors, with the flat BEE having even better performance than the classical error estimation schemes. Indeed, all graphs exceptdemonstrate that either the flator calibrated Bayesian error estimator is the best scheme over the whole range of expected true error. Our next set of graphs inshow simulation results with respect to feature size. For reference, graphs of the expected true error for these simulations are shown in. Calibrated priors provide the best performance, except when combining large feature and small sample sizes, in which case a flat prior performs best. In fact, performance of the calibrated BEE intends to be best precisely in the rage of feature sizes with the highest percentage of global features and the lowest true errors. For example, the calibrated BEE infor sample of size 60 has the best performance up to 7 features, where inthe percentage of selected features being global is greater than about 80% and inthe true error has started to level off. Note, also, the consistently superior performance of the calibrated BEE over the non-Bayesian estimators for n = 60; indeed, throughout the range of feature sizes, the calibrated BEE has an RMS of at least 0.0263 smaller than the best performing non-Bayesian error estimator, which represents an improvement of at least 14%. Note the upward RMS trend inand the downward trend infor the non-Bayesian error estimators. Although it can be dangerous to generalize about the behavior of error estimators, let us at least conjecture. We see inthat the true error is large for n = 60, with little improvement as we increase the number of features and, in fact, increasing true error as the number of features passes 7, which is a clear sign of the peaking phenomenon. Thus, for n = 60, adding features creates a more difficult estimation problem that is not offset by easing error estimation on account of small true errors. On the other hand, inwe see a fast reduction of true error for n = 120 as more features are added, thereby greatly easing the error estimation problem and resulting in the declining RMS trend in. While these comments apply directly to the non-Bayesian error estimators, they apply to the Bayesian estimators relative to their change of slope. The flat BEE is relatively constant inbut falls along with the non-Bayesian error estimators in, whereas the calibrated BEE consistently rises inbut remains relatively flat in.
Empirical breast cancer dataWe applied the Bayesian error estimator to normalized gene expression measurements from a breast cancer study
Application of the Bayesian MMSE error estimator to microarray data(a)(van de). This study used 295 sample points, with 180 assigned to class 0 (good prognosis) and 115 in class 1 (bad prognosis), and provides a 70 feature prognosis profile. From the original 295 points, we randomly draw a non-stratified training sample of size n. Since the number of features in the dataset is relatively small, we apply only the last two stages of our feature selection scheme in Section 2.4. The first stage selects features passing a ShapiroWilk Gaussianity test with 95% confidence and must report at least D features, while the second stage selects D features with the highest t-test statistic. The 70D features not used for classification are retained as calibration data for Bayesian error estimation. After feature selection, we train an LDA, QDA or 3NN classifier. The remaining sample points are used as holdout data to approximate the true error of the designed classifier. The previously considered error estimators are also evaluated from the training samples [except in the case of 3NN where semibolstering is used instead of bolstering owing to its superior performance for 3NN (Braga, along with exact Bayesian error estimators (for LDA) or approximate Bayesian error estimators (for QDA and 3NN). Both flat and calibrated priors are applied. This process is repeated either 100 000 times (for LDA) or 10 000 times (for QDA and 3NN) to estimate the average RMS deviation of each error estimator from the true error. The Bayesian error estimator priors are calibrated as discussed in Section 2.5. A typical prior with 2 features and 40 sample points is  = 16.80, m =0.004,  = 12,  2 /( D1) = 0.042 and  = 0.020 for class 0, and  = 2.78, m =0.068,  = 10,  2 /( D 1) = 0.024 and  = 0.073 for class 1. These indicate that the good prognosis class (0) has a distribution with a more concentrated mean (since  is much larger) and the mean is close to 0, which is expected since the data have been normalized. On the other hand,  is fairly large for both classes, suggesting that the variance of each feature in either class is probably close to the prior expected variance,  2 /(  D1). Interestingly, the variance is a bit larger for class 0 and  is usually small but positive. Figures 8, 9 and 10 provide simulation results for LDA, QDA and 3NN, respectively. Each figure contains subplots representing fixed feature sizes between one and five, and one figure showing the expected true error for all simulations with the corresponding classifier. A summary of the simulation settings is shown in. The uniform prior performs well over a wide range of sample and feature sizes, and generally shows significant improvement over the classical error estimators. Prior calibration can have even more pronounced improvement, especially for small feature sets. Also, although the uniform prior often performs better than the calibratedprior for high feature sizes, see for examplefor 5 features, we observe inthat true error does not improve much, and may actually get worse, for as little as 5 features. This may indicate that when there is not enough calibration data for good prior design, there is also probably insufficient data for good classifier design.
L.A.Dalton and E.R.Dougherty
Concluding remarksOur synthetic data simulations demonstrate the power of prior knowledge in two ways: we may assume a low Bayes error by using a flat prior and outperform the classical error estimators where they perform best, or we may calibrate a prior, even using purely datadriven methods, and obtain superior performance in the midrange of Bayes errors. Also note that for moderately difficult classification problems which are typical in a small sample biological setting, the midrange is precisely where training data error estimation is needed. One might argue that there is a risk with postulating a low-Bayes-error prior since, although it will show excellent performance if the Bayes error is truly low, it will suffer for large Bayes errors. In, not only does performance deteriorate with increasing Bayes error for the Bayesian MMSE estimator, but also the performance of cross-validation. This should not be surprising because the use of cross-validation presupposes that the Bayes error is small because its performance seriously degrades for increasing Bayes error. This behavior, noted more than 30 years ago in a simple 1D Gaussian model (), has been(b)demonstrated via large simulations for both discrete and Gaussian models (Dalton and Dougherty, 2011a, b), and has been analytically proven in the Gaussian model (). In other words, unless one is not interested in error estimator performance, use of cross-validation carries with it implicitly assumed prior knowledge. If one knows that the Bayes error is low, then why not define a prior model based on this assumption to design a Bayesian error estimator with even better performance?
Application of the Bayesian MMSE error estimator to microarray data
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
x k if N > 1 is even, (3) where !! is the double factorial. 2.3 Implementation of exact and approximate Bayesian error estimators Assuming a Gaussian model with normalinverse-Wishart priors for the Gaussian distribution parameters, with fixed hyperparameters for the priors of each class, we use the observed sample to update the hyperparameters of the posteriors. We also check that these posteriors are valid density functions, and if they are not, by default the code reports the error contributed by that class to be 0.5. Note that the Bayesian error estimator is most useful in a small sample setting, but the sample size must not be so small that the posterior is not a valid density function. This may happen, for instance, if we use a flat prior with  +D+2 = 0 and the sample size for class y is n y  2D+1, so that  * =  +n y  D1. In such cases, the Bayesian error estimator is meaningless because the available information is not sufficient for estimation, but generally there are also too few sample points for any error estimator to provide meaningful results. Given valid normalinverse-Wishart posteriors, the closed form Bayesian error estimator in Equation (2) for linear classification is easily evaluated. For arbitrary classifiers, we approximate the Bayesian error estimator in Equation (1) with a Monte Carlo approach. For each class, we generate a random mean and covariance pair according to the specified posterior normalinverse-Wishart distribution. Several algorithms for generating normalinverse-Wishart distributed multivariate sample points are available, see Johnson (1987). For each mean and covariance pair, the true error contributed by the class for the designed classifier is approximated by generating 10 000 sample points from the Gaussian distribution having the specified mean and covariance, and finding the error of these sample points on the classifier. The Bayesian error estimator is computed by averaging these true errors over 2500 random sets of mean and covariance pairs. A toolbox of C code for Bayesian error estimation is publicly available. This includes the exact Bayesian error estimator for linear classifiers, the approximation code described above for arbitrary classifiers, a threestage feature selection algorithm discussed in Section 2.4, as well as code implementing the method of generating priors described in Section 2.5. Simulations demonstrating the accuracy of this approximation with synthetic data and LDA classification are available in the Supplementary Material. 2.4 Feature selection We use a three-stage feature selection method based on the t-test and a Gaussianity test to reduce the original feature set to D features. Since this article is not focused on optimizing a classification scheme, but rather on investigating the performance of error estimators, this feature selection scheme is intended to be a simple possible scheme to produce highly differentially expressed Gaussian features. In the first stage, only highly differentially expressed features or features with a high likelihood of biological significance are selected. These may be selected by a t-test or based on biological knowledge. This stage reduces the number of features from tens of thousands to a few hundred. The second stage applies a ShapiroWilk hypothesis test (Shapiro and Wilk, 1965) on each feature of each class. Only features passing the ShapiroWilk test with
