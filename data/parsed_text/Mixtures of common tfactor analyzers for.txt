Motivation: Mixtures of factor analyzers enable model-based clustering to be undertaken for high-dimensional microarray data, where the number of observations n is small relative to the number of genes p. Moreover, when the number of clusters is not small, for example, where there are several different types of cancer, there may be the need to reduce further the number of parameters in the specification of the component-covariance matrices. A further reduction can be achieved by using mixtures of factor analyzers with common component-factor loadings (MCFA), which is a more parsimonious model. However, this approach is sensitive to both non-normality and outliers, which are commonly observed in microarray experiments. This sensitivity of the MCFA approach is due to its being based on a mixture model in which the multivariate normal family of distributions is assumed for the component-error and factor distributions. Results: An extension to mixtures of t-factor analyzers with common component-factor loadings is considered, whereby the multivariate t-family is adopted for the component-error and factor distributions. An EM algorithm is developed for the fitting of mixtures of common t-factor analyzers. The model can handle data with tails longer than that of the normal distribution, is robust against outliers and allows the data to be displayed in low-dimensional plots. It is applied here to both synthetic data and some microarray gene expression data for clustering and shows its better performance over several existing methods. Availability: The algorithms were implemented in Matlab. The Matlab code is available at http://blog.naver.com/aggie100.
INTRODUCTIONModel-based methods have been widely used for both clustering and classifying high-dimensional microarray data ().compared various clustering techniques and showed that modelbased method performed well for microarray gene clustering. * To whom correspondence should be addressed.The finite normal mixture model with unrestricted componentcovariance matrices is a highly parameterized model (). Banfield andintroduced a parameterization of the componentcovariance matrix based on a variant of the standard spectral decomposition, and its program MCLUST () has been often used. But if the number of genes p is large relative to the sample size n, it may not be possible to use this decomposition to infer an appropriate model for the componentcovariance matrices. Even if it were possible, the results may not be reliable due to potential problems with near-singular estimates of the component-covariance matrices when p is large relative to n. In this case, mixtures of factor analyzers (MFA) is a useful model to reduce the number of parameters by allowing factor-analytic representation of the component-covariance matrices.proposed the MFA adopting a finite mixture of factor analysis models, which was considered for the purposes of clustering by) and).) applied MFA to tissue samples with microarray gene expression data for clustering.used MFA to classify microarray data successfully. Recently,proposed a penalized MFA to allow both selection of effective genes and clustering of high-dimensional data simultaneously.has proposed another penalized model-based clustering method with unconstrained covariance matrices. In practice, for example, where there are several different types of cancer, there is often the need to reduce further the number of parameters in the specification of the componentcovariance matrices by factor-analytic representations.introduced some parsimonious MFA models, which include various MFA models with fewer parameters.proposed another parsimonious factor mixture model to allow both dimension reduction and variable selection. Baek and McLachlan (2008) andproposed the use of mixtures of factor analyzers with common component-factor loadings (MCFA) and applied it to a microarray dataset for clustering. The method considerably reduces further the number of parameters, and allows the data to be displayed in lowdimensional plots in a straightforward manner in contrast to MFA. Several analyses of many real datasets, however, have suggested that the empirical distribution of gene expression levels is approximately log-normal or sometimes with a slightly heavier tailed t-distribution depending on the biological samples under investigation (). In particular,applied the ShapiroWilks test to Affymetrix microarray expression data and concluded thatPage: 1270 12691276
J.Baek and G.J.McLachlannon-normal distributions are common (up to 46% of probe sets). Lnnstedt and Speed (2002) also noted that outliers occur frequently in microarray experiments. Therefore, the above approaches are sensitive to both non-normality of the data and extreme expression levels as they are based on a mixture model in which the multivariate normal family of distributions is assumed for the component-error and factor distributions.extended MFA to incorporate the multivariate t-distribution for the component-error and factor distributions. In this article, we propose an extension of MCFA to incorporate the multivariate t-distribution to handle the data with tails longer than that of the normal distribution. In the next section, we review briefly the MCFA approach as proposed by Baek and McLachlan (2008) and considered further in. We then describe the mixtures of tfactor analyzers model with common factor loadings (MCtFA) and develop its implementation via the expectation-maximization (EM) algorithm. In Section 3, its application is demonstrated in the clustering of two microarray gene expression datasets. The results so obtained illustrate the improved performance of MCtFA over MCLUST, MFA and MCFA for these two datasets. A short discussion is given in Section 4.
METHODS
Mixtures of common t-factor analyzers and its EM algorithmFinite mixture models are being increasingly used to model the distributions of a wide variety of random phenomena and to cluster datasets; see, for example, McLachlan and Peel (2000a). Let Y = (Y 1 , ..., Y p ) T be a p-dimensional vector of feature variables. For continuous features Y j , the density of Y can be modelled by a mixture of a sufficiently large enough number g of multivariate normal component distributions,where (ydenotes the p-variate normal density function with mean  and covariance matrix. Here, the vector of unknown parameters consists of the mixing proportions  i , the elements of the component means  i and the distinct elements of the component-covariance matrices i (i = 1, ..., g). We focus on the use of mixtures of factor analyzers to reduce the number of parameters in the specification of the component-covariance matrices, as discussed in, McLachlan and Peel (2000a) and. With the factor-analytic representation of the component-covariance matrices, we have thatwhere A i is a pq matrix and D i is a diagonal matrix. To see this, we first note that the MFA approach with the factor-analytic representation (2) on i is equivalent to assuming that the distribution of the difference Y j  i can be modelled asfor j = 1, ..., n, where the (unobservable) factors U i1 , ..., U in are distributed independently N(0, I q ), independent of the e ij , which are distributed independently N(0, D i ), where D i is a diagonal matrix (i = 1, ..., g). However, this model may not lead to a sufficiently large enough reduction in the number of parameters, particularly if g is not small. Hence for this case, Baek and McLachlan (2008) andproposed the MCFA approach whereby the distribution of Y j is modelled as Y j = AU ij +e ij with prob.  i (i = 1, ..., g)for j = 1, ..., n, where the (unobservable) factors U i1 , .
.., Uwhere A is a pq matrix,  i is a q-dimensional vector, i is a qq positive definite symmetric matrix, and D is a diagonal pp matrix. With the restrictions (4) and (5) on the component mean  i and covariance matrix i , respectively, the MCFA approach provides a greater reduction in the number of parameters compared with MFA (seewherewhere (y,;) = (y) T 1 (y), and the vector of unknown parameters consists of the degrees of freedom  i in addition to the mixing proportions  i and the elements of the  i , i , A and D (i = 1,...,g). As in the mixture of common factor analyzers model, A is a pq matrix and D is a diagonal matrix. In order to fit the model (6) with the restriction (7), it is computationally convenient to exploit its link with factor analysis. Therefore, we assume that the distribution of Y j of MCtFA is modelled as (3), where the joint distribution of the factor U ij and the error e ij needs to be specified so that it is consistent with the t-mixture formulation (6) for the marginal distribution of Y j. In the EM framework, the component label z j associated with the observation y j is introduced as missing data, where z ij = (z j ) i is one or zero according as y j belongs or does not belong to the i-th component of the mixture (i = 1, ..., g; j = 1, ..., n). The unobservable factors u ij are also introduced as missing data in the EM framework. Now we postulate that conditional on membership of the i-th component of the mixture the joint distribution of Y j and its associated factor (vector) U ij is multivariate t-distribution. That is,where This specification of the joint distribution ofand hence thatThus, with this formulation, the error terms e ij and the factors U ij are distributed according to the t-distribution with the same degrees of freedom. However, the factors and error terms are no longer independently distributed as in the normal-based model for common factor analysis, but they are uncorrelated. To see this, we have from(9) that conditional on w j , U ij and e ij are uncorrelated, and hence, unconditionally uncorrelated. By adopting a common factor loading matrix and the t-distribution for the factors and error terms, the MCtF model has fewer parameters and is more robust against extreme observations, thus providing a better fit to data with skewed heavy tails. We can obtain the maximum likelihood estimator of the vector of unknown parameters in the mixture of common t-factor analyzers model specified by (6) and (7) as follows. We use a modified version of the AECM algorithm outlined infor mixtures of t-factor analyzers. We assume that the component-indicators z ij , the factors U ij in (3) and the weights w j in the characterization (9) of the t-distribution for the i-th component distribution of Y j and U ij are all missing. We have from(9) thatThus in the EM framework for this problem, the complete data consist, in addition to the observed data y j , of the component-indicators z ij , the unobservable weights w j , and the latent factors u ij. The complete-data log likelihood for formed on the basis of the complete data is given by
E-stepIn order to carry out the E-step, we need to be able to calculate the conditional expectation of the terms, we have that conditional on y j and w j , the i-th component-conditional distribution of U ij  i is multivariate normal with mean  T i (y j A i ) and covariance matrix (The conditional expectation of W j given y j and z ij = 1 is given by). The conditional expectation of Z ij given y j is given by the posterior probability  i (y j ;) that y j belongs to the i-th component of the mixture;(i = 1,...,g; j = 1,...,n).
CM-step We use two CM stepsin the AECM algorithm, which correspond to the partition of into the two subvectors 1 and 2 , where 1 consists of the mixing proportions, the elements of  i and the degrees of freedom  i (i = 1,...,g). The subvector 2 consists of the elements of the common factor loadings matrix A, the i and the diagonal matrix D. On the first cycle, we specify the missing data to be the componentindicator variables Z ij and the weights w j in the characterization (9) of the t-distribution for the component distribution of y j. On the (k +1)-th iteration of the algorithm, we update the estimators of the mixing proportions usingwhere the posterior probabilities are calculated using (11). The updated estimate of the i-th component factor mean is given bywhere the current weight w) is formed using the current value (k) for in (10). In the case where the degrees of freedom  i in the component t-distributions are not specified but are to be estimated from the data, we have to update the estimate of  i on the first cycle. The updated estimate  (k+1) i of  i does not exist in closed form, but is given as a solution of the equation(i = 1,...,g), and () is the digamma function. The estimate of is updated so that its current value after the first cycle is given by (k+1/2) = (On the second cycle of this iteration, the complete data are expanded to include the unobservable factors U ij associated with the y j. An E-step is performed to calculate Q(; (k+1/2) ), which is the conditional expectation of the complete-data log likelihood given the observed data, using = (k+1/2). Then the new posterior probability,The CM-step on this second cycle is implemented by the maximization of), Page: 1272 12691276
J.Baek and G.J.McLachlanand the updated estimate D (k+1) is given by. Then the updated estimate A (k+1) is obtained byWe have to specify an initial value for the vector of unknown parameters in the application of the EM algorithm. A random start is obtained by first randomly assigning the data into g groups. Using the sample mean and sample covariance matrix of each randomly partitioned data, the initial parameter estimates are obtained as described in the Appendix of. We can portray the observed data y j in q-dimensional space by plotting the corresponding value of th u ij , which are the estimated conditional expectations of the factors U ij , corresponding to the observed data points y j .We letletlet ij denote the value of the right-hand side of (12) evaluated at the maximum likelihood estimates of  i ,  i and A. We can define the estimated valu u j of the j-th factor corresponding to y j as,...,n).An alternative estimate of the posterior expectation of the factor corresponding to the j-th observation y j is defined by replacing  i (y j ;  ) byzby byz ij in (13).compared different clustering methods for the analysis of 35 cancer gene expression datasets. For our experiment, we considered 2 of these 35 datasets. We applied MCtFA to cluster each of these two datasets, and compared its performance with other methods. We compare our method with MCLUST, MFA and MCFA. The performance is measured by the Adjusted Rand Index (ARI;) and the error rate since the true membership of each observation is known. MCLUST is a software package that implements Gaussian mixture models via EM algorithm and the Bayesian Information Criterion (BIC,) for model-based clustering (). In MCLUST, the component-covariance matrix i is parameterized by eigenvalue decomposition in the form
RESULTSi , where  i is a constant, E i is the matrix of eigenvectors, i is a diagonal matrix with elements proportional to the eigenvalues of i. Different conditions on  i , i and E i characterize the volume, shape and orientation of each component distribution in MCLUST. We deal with the 10 submodels of MCLUST: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV, VVV(,). For MFA, we assumed the covariance matrix of errors is equal for each component. We took advantage of the mclust software for R () and the EMMIX program () for MFA, and developed programs for the MCFA and the MCtFA approaches, using the MATLAB language. The first set concerns both breast and colon cancer data (), which consists of 104 gene expressions for 52 matched tissue pairs of two different cancer types (32 pairs of breast tumour and 20 pairs of colon tumour). There are 22 283 genes in the original data, butselected 182 genes by filtering uninformative genes. It has been reported in many analyses of real datasets that the empirical distribution of gene expression levels is approximately log-normal or sometimes (on the log scale) with a slightly heavier tailed t-distribution depending on the biological samples under investigation (). Thus, these data may also have many extreme expressions for each gene.shows boxplots of the expression levels for the first 10 genes. The distribution of each gene is skewed and has a very long tail. The rest of the genes also have similar shaped distributions. In particular, gene 6 has very high expression levels for six particular tissues.We implemented the MCLUST, MFA, MCFA and MCtFA procedures with g = 2 components with the number of factors q ranging from 1 to 9, using 50 starting values for the parameters. For each value of q, we computed the ARI and the associated error rate. The results are presented in. We have also listed in this table the values of the BIC and the Approximate Weight of Evidence (AWE:) for each model with different q. AWE is a model selection criterion based on an approximation to the classification log-likelihood. AWE is defined as
Page: 1273 12691276
Mixtures of common t-factor analyzersis the entropy of the classification matrix with (i,j)-th element equal to  i (y j ;) and m is the number of (free) parameters. It penalizes complex models more severely than BIC, and thus selects more parsimonious models than BIC. It would appear that BIC works well at a practical level; see, for example, Fraley and Raftery (1998). Further,proved that BIC provides a consistent estimator of g (). But BIC can lead to too few or too many clusters in practice, depending on the problem at hand. For the present problem of choosing the number of factors q, it would appear fromthat it leads to too many factors being fitted in the mixtures of factor analyzers model. An apparent explanation for this is that for the present dataset (and the other one to follow), the data are not well represented by the true models of clusters and/or the true clusters are not well separated. As a consequence, BIC leads to too many factors in the mixture model being fitted to the data. The AWE criterion is preferable to BIC here as it leads to fewer factors since it places a higher penalty on more complex model due to the presence of twice of the entropy and the extra constant term (2EN( )+3m+mlog(n)) in (14). We also considered the ICL criterion which chooses q to minimize 2logL()+2EN( )+mlog(n), which is similar to the AWE criterion. They are the same apart from the additional penalty of 3m+mlog(n) imposed by AWE, which for our present problem leads to a better choice of q. It can be seen that the lowest error rate (0.0288) and highest value (0.8867) of the ARI is obtained by using q = 6 factors with the MCtFA model, which coincides with the choice on the basis of AWE. The lowest error rate (0.1154) of the MFA model is obtained for q = 3 factors. The best result of MCFA model is obtained with its lowest error rate 0.0865 for q = 1 factor (AWE suggests using q = 7). MCLUST chose VVI model as its best and its error rate is 0.3462. It can be seen that the error rate and ARI for MCtFA are better than those for MCLUST, MFA and MCFA. We have also calculated BIC for all models. It can be seen that it failed to select the best modelfor each method. BIC reached its minimum for largest q(q = 9) of each method, so it selected a more complex model than the one with highest ARI and lowest error rate. In the case where the distribution from which the data arose is not in the collection of considered mixture models, BIC criterion tends to overestimate the correct size regardless of the separation of the clusters (). To illustrate the usefulness of the MCtFA approach for portraying the results of a clustering in low-dimensional space, we have plotted inthe estimated factor scoresscoresscores j as defined by(13) with the implied cluster labels shown. In this plot, we used the third and sixth factors in the fit of the MCtFA model with q = 6 factors. It can be seen that the clusters are represented in this plot with very little overlap. The estimated factor scores were plotted according to the implied clustering labels. The degrees of freedom of the factor t-distributions for both groups were estimated as 1.0 and 1.0, which means their tails of the distributions are very thick and long. We can easily detect 6 distinct extreme tissues as shown in, which are known to be the 9th14th colon tumor tissue.shows the expression levels of all genes for the 6th17th colon tumor tissues. In, we observe that all of these 6 outliers are very different from others since they have extremely large expression levels not only of the 6th gene shown in, but also of other genes. The second dataset to which we applied our method is a lung cancer data (), for which the number of classes is not small (g = 5). It consists of 203 gene expressions partitioned into five subpopulations: four lung cancer types and normal tissues.selected 1543 informative genes from the original 12 600 genes. There are big differences among the class sizes of the data. The number of tissues for each class is 139, 17, 6, 21 and 20, respectively.shows the Page: 1274 12691276boxplots of the expressions of a gene plotted for each subpopulation. It can be seen that there exist skewed distributions mixed with symmetric distribution with or without extreme observations for five components in the plot. Since the selected (1543) genes are still too many for the mixture model, we grouped the genes into 50 clusters and selected the centroid from each cluster of genes. That is, we applied the k-means algorithm to the 1543 gene expressions and clustered them into 50 groups of similar characteristics. Then we extracted the centroid from each group to make 50 new features for the mixture models. We implemented the MCLUST, MFA, MCFA, and MCtFA approaches with g = 5 components for the number of factors q ranging from 1 to 10, using 50 starting values for the parameters. For each value of q, we computed the ARI and the error rate. The results are presented in. We have also listed in this table the values of BIC and AWE for each model with different levels of q. MCtFA attains its largest ARI (0.7322) and lowest error rate (0.1133) for q = 6, although AWE suggested the model with q = 7. We notice that there is little difference between the AWE values for q = 6 and for q = 7. The lowest error rate for MCFA is 0.2611 for q = 2 and the largest ARI is 0.4570 for q = 9. The error rate (NA) of MCFA for q = 1 was not able to be calculated since the estimated number of clusters was less than the true value 5. Neither BIC nor AWE indicated the best model for MCFA. MFA reached its best ARI (0.3487) and error rate (0.3498) for q = 7. The minimum BIC and AWE were obtained at q = 6, and at q = 1, respectively. The best model for MCLUST showed similar performance (ARI: 0.3021, error rate: 0.3350) to MFA. MCtFA again performed better than the other methods for this dataset. We display the data using the estimated factor scores of our model in 3D space (). In the latter, we used the second, the fourth and the fifth factors in the fit of the MCtFA model with q = 6 factors. The estimated factor scores were plotted according to the implied clustering labels. It can be seen that the five clusters are represented in this plot with very little overlap. The degrees of freedom of the factor t-distributions for the components were estimated as 1.1, 1.3, 7.8, 4.0 and 4.1. There are two distributions with long tails [ 1 = 1.1(triangle),  2 = 1.3(circle)] in the plot. We have also given inthe plot corresponding to that inwith the true cluster labels shown. There are 23 misallocated tissues which can be seen in other's clusters, but as a whole there is a good agreement between the two plots.
J.Baek and G.J.McLachlan
DISCUSSIONFor clustering high-dimensional data such as microarray gene expressions, MFA is a useful technique since it can reduce the number of parameters through its factor-analytic representation of the component-covariance matrices. However, this approach isnot provide a sufficient reduction in the number of parameters, particularly when the number of clusters (subpopulations) is not small. In this article, we proposed a new mixture model which can reduce the number of parameters further in such instances and cluster the data containing outliers simultaneously by introducing a mixture of t-distributions with both component-mean and component covariance represented by common factor loadings. We call this approach mixtures of common t-factor analyzers (MCtFA). We describe the implementation of an EM algorithm for fitting the MCtFA. This approach also has the ability to portray the results of a clustering in low-dimensional space. We can plot the estimated posterior means of the factorsfactorsfactors j as defined by(13) with the implied cluster labels. On the other hand, the approaches MCLUST, MFA and MtFA cannot project high-dimensional objects in lowdimensional space. The applications of MCtFA to two cancer microarray datasets have demonstrated the usefulness and its relative superiority in clustering performance over MCLUST, MFA and MCFA. It has shown that our method works well for clustering data containing outliers. Moreover, it provides information on the distribution structure of each subpopulation by displaying the estimated factor scores in low-dimensional space. We observed also that the proposed approach fitted the experimental datasets better than the other approaches, and the performance difference between MCtFA and the others becomes even greater when the number of clusters is not small, such as in the case of second dataset (Section 1.1 of Supplementary Material). Often BIC is used to provide a guide to the choice of the number of factors q and the number of components g to be used. However, it did not always lead to the correct choice of the best model. That is, BIC can lead to too simple or too complex model in practice, depending on the problem at hand. Simulation studies reported inand McLachlan and Peel (2000a) show that BIC will overrate the number of clusters under misspecification of the component density, whereas several alternative criteria such as the AWE and ICL criterion are able to identify the correct number of clusters even when the component densities are misspecified (Frhwirth). In both of our real data applications, we observed that BIC did not choose the best q. An apparent explanation for this is that BIC tries to choose more complex model since some of the subpopulations of the datasets have skewed distributions and have several extreme outliers. On the other hand, AWE leads to the best or almost the best model with smallest error rate since it is more robust against misspecification of the component densities for the experimental datasets. Recently, Frhwirth-Schnatter and Pyne (2010) reported that AWE picked the correct model for both skew-t and skew-normal mixture distributions. Also a small simulation study confirms the better performance of the AWE over the BIC when the distribution of the data has skewed heavy tails due to some extreme observations (Section 1.2 of Supplementary Material). In future work, we wish to investigate the use of various model selection criteria on choosing the number of factors q and the number of components g in mixtures of t or skewed distributions.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
|w j ,z ij = 1  N p+q ( * i ,K i /w j ), (9) where w j is a value of the weight variable W j taken to have the gamma distribution f G (w j ; i /2, i /2), where
