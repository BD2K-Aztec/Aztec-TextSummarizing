Motivation: Word Sense Disambiguation (WSD), automatically identifying the meaning of ambiguous words in context, is an important stage of text processing. This article presents a graph-based approach to WSD in the biomedical domain. The method is unsupervised and does not require any labeled training data. It makes use of knowledge from the Unified Medical Language System (UMLS) Metathesaurus which is represented as a graph. A state-of-the-art algorithm, Personalized PageRank, is used to perform WSD. Results: When evaluated on the NLM-WSD dataset, the algorithm outperforms other methods that rely on the UMLS Metathesaurus alone. Availability: The WSD system is open source licensed and available from http://ixa2.si.ehu.es/ukb/. The UMLS, MetaMap program and NLM-WSD corpus are available from the National Library of Medicine http://www.nlm.nih.gov/research/umls/, http://mmtx.nlm.nih.gov and http://wsd.nlm.nih.gov. Software to convert the NLM-WSD corpus into a format that can be used by our WSD system is available from http://www.dcs.shef.ac.uk/âˆ¼marks/biomedical_wsd/ under open source license. Contact
INTRODUCTIONThe biomedical scientific literature is now so large that automated tools are necessary to access it effectively (). However, this process is made difficult by the fact that terms in natural language can be ambiguous, i.e. may refer to more than one possible concept. For example, in the biomedical domain the word 'cold' is ambiguous and can mean (at least) 'common cold', 'cold temperature' or 'cold sensation'. Word Sense Disambiguation (WSD) systems aim to solve this problem by identifying the meanings of ambiguous words in context (). For example, WSD would aim to identify that the meaning of cold in the sentence The role of zinc in treating cold symptoms is 'common cold'. This information could be used to improve literature searches, by ensuring that the documents returned only contain ambiguous terms when they are used in a meaning that is relevant to the search, and is also beneficial for other applications that are useful for biomedical researchers, such as automated indexing, information extraction and knowledge * To whom correspondence should be addressed. discovery (). This article describes an approach to WSD in the biomedical domain that is based on graph-based algorithms. Since the approach is unsupervised, it does not require any labeled training data and relies on information from the Unified Medical Language System (UMLS) Metathesaurus () instead. The UMLS Metathesaurus is converted into a graph to which the Personalized Page Rank algorithm () is applied to carry out WSD. Section 2 describes previous work on WSD in the biomedical domain and the use of graph-based algorithm for WSD. Section 3 describes our approach to WSD in the biomedical domain using the UMLS Metathesaurus and Personalized Page Rank algorithm. The approach is evaluated against a standard dataset and the results were analysed in Section 4. These results are discussed in Section 5. The conclusions are found in Section 6.
RELATED WORKThe problem of WSD has been explored since the 1950s and is regarded as an important stage in text processing (). The majority of approaches have explored the problem in a domain-independent setting, although several researchers have developed systems specifically intended to resolve the ambiguities that are found in the biomedical domain (). The most popular approaches for WSD in biomedicine are based on supervised learning, for example. Although studies on domain-independent WSD have shown that supervised approaches outperform alternative ones, they require labeled training examples which may not be available and are expensive to create. This limitation means that most supervised approaches (including those mentioned above) can only disambiguate a small sample of words for which training data can be found, and this limits their usefulness in practise. In the context of biomedicine,avoided this problem by using Medline as training data and exploiting information it contains about the source of each abstract. This approach assigns Semantic Types from the UMLS Metathesaurus but is unable to distinguish between meanings with the same Semantic Type. Unsupervised approaches do not require labeled training examples and often make use of knowledge bases, such as the UMLS Metathesaurus.describes such an approach that uses the UMLS to generate textual definitions for the possible
E.Agirre et al.meanings of ambiguous terms. WSD is carried out by comparing the context of the ambiguous term with the definitions of each possible sense and choosing the one with the most words in common. The approach is evaluated against 13 terms from the NLM-WSD corpus (see Section 4) and performance of 48.11% reported. Graph-based methods have recently become widely used for domain-independent knowledge-based WSD (). These methods represent the knowledge base as a graph which is then analysed to identify the meanings of ambiguous words. An advantage of this approach is that the entire knowledge base can be used during the disambiguation process by propagating information through the graph. This article presents an unsupervised knowledge-based WSD algorithm which is capable of disambiguating all words that are ambiguous in the UMLS Metathesaurus. Relations in the UMLS Metathesaurus are used to create a graph which is analysed using the Personalized PageRank algorithm to rank possible meanings of ambiguous words based on their structural importance in the graph and their relation to the words in context. This algorithm has previously been applied in a domain-independent setting, using WordNet as the knowledge base (), and shown to outperform other, more ellaborate, graph-based algorithms ().
GRAPH-BASED WSD
PageRank and Personalized PageRankThe PageRank algorithm () is a method for ranking the vertices on a graph according to their relative structural importance. It was originally developed to rank World Wide Web pages based on the number of pages that link to them. Here, we describe it as an algorithm for generic graphs. The next sections describe how to use it for WSD using the UMLS. PageRank uses a random walk model, where a random surfer starts a walk from an arbitrary node in the graph and, at each step, chooses an outgoing edge of the node at random to follows. The surfer may also decides to stop following edges and teleport to any node in the graph. The PageRank score of a vertex yields the probability that the random surfer is found in that vertex, assuming that the random walk continues indefinitely. Specifically, let G be a graph with N vertices (v 1 ,...,v N ). For a given vertex v i , let In(v i ) be the set of vertices pointing to it, and let d j the out-degree of vertex v j. The PageRank of vertex v i is defined as:where c is the so-called damping factor, a scalar value between 0 and 1. The PageRank for a vertex v i is the addition of two terms. The first term models the probability of the random surfer arriving to v i following the edges going from any vertex v j to v i , given by the sum of the probabilities of each vertex v j having an edge to v i times the weight of the edge, as given by the inverse of the degree of v j. The second term represents the probability of the surfer randomly jumping to any node with equal probability. The damping factor c models the relative importance of each of the two terms.The second term can also be seen as a smoothing factor that makes any graph fulfil the property of being aperiodic and irreducible, and thus guarantees that PageRank calculation converges to a unique stationary distribution. PageRank is calculated by applying an iterative algorithm that computes Equation (1) repeatedly until convergence below a given threshold is achieved or until a pre-specified number iterations have been executed. The damping factor is usually set in the range. Previous experiments () lead us to choose a damping factor of 0.85.shows a sample graph (a) and the PageRank values for this graph (b). Initially, P for all four nodes are initialized with a uniform distribution, i.e. 0.25. 1 Given a damping factor of 0.85, in the first iteration the PageRank values are updated as follows:where the superscripts correspond to the current iteration, i.e. P(A 0 ) corresponds to the initial value and P(A 1 ) to the first iteration. The second iteration would calculate P(A 2 ) based on P(D 1 ), and so on. After a few iterations, convergence is attained and the PageRank values shown in graph () are obtained. In certain situations, including graph-based WSD, we would like to include information about the relative importance of vertices in the graph. That is, given a set of vertices of interest, we would like to know which other vertices are closely related to them in the graph. For instance, we may be interested to know which nodes in graph () are closely related to node D (as shown in). Personalized PageRank () computes the structural importance of the vertices in a graph when some vertices are more relevant than others for the task at hand. In order to introduce Personalized PageRank, we first rewrite Equation (1) in compact form by using matrices as follows. Let M be a N N transition probability matrix, where M ji = 1 d i if a link from v i to v j exists, and zero otherwise. Let v be a stochastic normalized N 1 vector whose elements are all 1 N. Then, the calculation of the PageRank Vector P over the graph G is equivalent to resolving the following Equation:Page: 2891 28892896
Graph-based WSD
Using Personalized PageRank for WSDTo use Personalized PageRank for WSD, the UMLS is represented as a graph in which the concepts are vertices and relations between them edges. Given the context of an ambiguous word (e.g. 'cold' in the context mentioned in), WSD is carried out by initializing v with equal values for all concepts that appear in the context (and zero for the rest), applying Personalized PageRank and then selecting the concept corresponding to 'cold' that has the highest PageRank. Two sources of information are required for the Personalized PageRank algorithm to be used for WSD: a Knowledge Base and a dictionary. The Knowledge Base (KB) consists of a set of concepts and relations between them. It can be naturally represented as an undirected graph G = (V ,E) where nodes represent KB concepts (v i ) and the relation between concepts v i and v j is represented by an undirected edge e i,j. The dictionary maps words and phrases found in documents to their possible concepts in the KB.) to identify co-occurrences. The MRCOC table includes details about the strength of the co-occurrence relation between concepts based on the number of co-occurrences identified. It is straightforward to convert the information contained in the MRREL and MRCOC tables into a graph. The concepts form the vertices with the relations listed in the tables being used to define the edges between them. No weights are used for the relations that are extracted from the MRREL table. In the case of the MRCOC table, we did use the strength of co-occurrence to produce some subsets of the graph (see Section 4.2).
Knowledge Base
DictionaryThe dictionary contains mappings from words and phrases in text to UMLS CUIs. It is created using the MetaMap program () that splits the input text into phrases and maps each onto the set of possible CUIs that they could refer to, known as candidates. The set of candidates for each word or phrase in the context of the ambiguous terms are extracted from Page: 2892 28892896
E.Agirre et al.the MetaMap output and used to create the dictionary to define the possible CUIs for each word in its context.
Static PageRank baselineApplying the traditional PageRank algorithm over the graph created from the UMLS leads to all CUIs being ranked according to their PageRank value, i.e. a context-independent ranking of CUIs. This can be used to create a WSD system by examining the relative rankings of the CUIs for a target word and returning the highest ranking one. We call this application of PageRank to WSD Static PageRank, since it does not change with the context, and use it as a baseline. The static baseline favours concepts with high degree, and thus disambiguates each word to the concept having most connections in the graph, regardless of context.
Personalized PageRank Static PageRankis independent of context, but this is not what we want in a WSD system. Given an input piece of text we want to disambiguate all content words (i.e. nouns, verbs, adjectives and adverbs) in the input based on the words in the context. This can be achieved using Personalized PageRank as follows. Given an input text, we extract the list W ={W 1 ,...,W m } of content words which have an entry in the dictionary and can therefore be related to UMLS concepts. Note that monosemous words will be attached to just one concept, whereas polysemous words may be attached to several. The context words are first inserted into G as nodes, and linked with directed edges to their respective concepts. The Personalized PageRank of the graph G is then computed by concentrating the initial probability mass uniformly over the newly introduced word nodes. As the words are linked to the concepts by directed edges, they act as source nodes injecting mass into the concepts they are associated with, which thus become relevant nodes and spread their mass over the UMLS graph. The resulting Personalized PageRank vector can be seen as a measure of the relevance of UMLS concepts given the context. As a result of the disambiguation process, every UMLS concept receives a score. Each target word can then be disambiguated by examining each of its possible concepts in the graph, G, and selecting the one with the highest score.shows an example. A problem occurs if the possible CUIs of the target word being disambiguated are themselves related. In this situation, those CUIs reinforce each other and reduce the influence of the other senses in the context. With this observation in mind, we introduce a change in the algorithm: for each target word W i , we concentrate the initial probability mass in the senses of the words surrounding W i , but not in the senses of the target word itself, so that context words increase its relative importance in the graph. The main idea of this approach is to avoid biasing the initial score of concepts associated to target word W i , and let the surrounding words decide which concept associated with W i has more relevance.
Interoperability and performance of the systemUKB is open source, programmed in C++ and easily integrated in thirdparty software as a library. For instance, the open source multilingual text-processing package Freeling 2 incorporates UKB. The steps needed to run our system are as follows. Before performing WSD, the MRREL and MRCOC tables from the UMLS need to be converted to a binary graph format. Given a target document, we first run MetaMap to construct the dictionary for the WSD system. The Personalized PageRank algorithm can then be run. This uses MetaMap's output for the target document, the graph and the dictionary. It outputs the disambiguated concepts in the form of CUI numbers with weights. The performance of our system on a PC with 2 QuadCore Xeon processors at 3160 MHz and 32 G of memory was the following: building the binary graph from the UMLS tables takes 21.8 s, loading the binary graph takes 5.6 s and 1.6 G of memory. WSD is performed at a rate of 37 instances per minute.
EVALUATIONThe WSD system was evaluated using the NLM-WSD corpus. This contains 50 ambiguous terms with 100 instances of each. The instances are abstracts containing the ambiguous term randomly extracted from those added to Medline in 1998. The 5000 instances were manually disambiguated by 11 annotators, who tagged each occurrence of the target term with the corresponding meaning. Some instances were tagged 'None' to indicate that the annotators did not consider any of the possible meanings in UMLS applied. Following standard practice (), these instances were not used in the evaluation, yielding a total of 3983 examples and 49 terms. (One term, 'association', was excluded since all 100 instances were labeled as 'None'.) In addition to the full NLM-WSD dataset, a subset of 13 of these terms was also used for evaluation. This subset was used by McInnes (2008) and consists of terms that have a majority sense that accounts for less than 65% of the instances and whose possible senses do not share the same semantic type. A window of 20 terms around the target word (i.e. the 10 preceding and 10 following terms) are used as the context. This is created by using MetaMap to identify the terms around the target word (see Section 3.2.2). Any phrases that are not mapped onto a CUI are discarded and the terms that form each of the remaining phrases are used to create the context. The damping factor (Section 3.1) was set to 0.85. The values of these parameters were selected based on previous work (). Section 4.4 reports a post hoc analysis exploring the effects of varying them. The 2007AB version of the UMLS was used for the experiments. This version was chosen since we had access to a mapping between the NLM-WSD sense labels and UMLS CUIs. Such a mapping is only required to evaluate our approach and it would be possible to use the approach described in this article to carry out WSD relative to any version of the UMLS. The mapping from the 2007AB version of the UMLS was created with the assistance of publicly available software and manually verified.shows results of the system evaluation. Performance is measured by accuracy, the percentage of instances correctly disambiguated. Note that our algorithm returns a sense for all instances. The confidence interval, computed using bootstrap resampling with 95% confidence (), is also shown. The top part of the table shows the results on the full NLMWSD dataset. The first two rows show the result using Personalized Page: 2893 28892896The best performance is obtained using the graph created from the MRREL table. Results decrease when the extra relations from the MRCOC table are added. This drop in performance was unexpected since co-occurrence information is generally considered to be very useful for WSD (). However, the MRCOC table only contains relations for some CUIs, unlike the MRREL table which contains relations for all CUIs. This negatively affects Personalized PageRank since it is more likely to select CUIs that are more highly connected and the fact that some CUIs do not appear in the MRCOC table creates a bias towards those which do. Analysis indicated that there are only four terms ('ganglion', 'man', 'secretion' and 'surgery') for which all of the possible CUIs appear in the MRCOC table. For 25 terms some of the CUIs appear in the MRCOC table while others do not. In addition, relations in the MRCOC table are generated automatically and may also be noisy. The lower part ofreports results on the 13 terms used by. The best approach, Personalized Pagerank using MRREL, achieves better results than those reported by, the state-of-the-art in knowledge-based WSD. Possible reasons for this improved performance are that the MRREL table contains information that is more useful for WSD than the CUI definitions used byand that the graph-based algorithm used in our approach benefits from being able to make use of information from the entire UMLS.
Results
Graph-based WSD
UMLS subsetsIn this section, we explore the effect of using subsets of the UMLS: relations from various vocabularies in MRREL and ranked relations from MRCOC. A greedy algorithm () was used to identify a set of vocabularies that included the CUIs used as possible senses of the terms in the NLM-WSD dataset. One vocabulary, MTH(UMLS Metathesaurus), was excluded from the set of vocabularies considered since it consists of concepts that were created specifically to create the Metathesaurus, rather than being an independent vocabulary in its own right. The greedy algorithm generated a set of four vocabularies: AOD (Alcohol and Other Drug Thesaurus), MSH (Medical Subject Headings), CSP (Crisp Thesaurus) and SNOMEDCT (SNOMED Clinical Terms). One of the possible senses for 'resistance' is only found in the MTH vocabulary and this term cannot be represented using these vocabularies. Note that the union of all four subsets covers all senses of the target words, but does not contain all relations in MRREL.shows the results when the Personalized PageRank algorithm is applied to graphs created using single vocabularies and the combination of all four. No single vocabulary includes all possible concepts for every sense and the column marked 'Terms' indicates the number of terms for which all possible concepts are included in a vocabulary or set of vocabularies. Results in the 'Acc.' column list the WSD performance over those terms using the graphs created using the single vocabularies (or their combination) while the 'MRREL' column lists the results using the graph created from the MRREL table over the same terms. Performance using individual vocabularies, or the combination of four vocabularies, is always lower than when the full MRREL graph is used. This indicates that our algorithm is able to exploit information from the multiple vocabularies that are combined to form the MRREL table in the UMLS and that including additional vocabularies, even ones that are not necessary to represent all of the possible meanings for ambiguous terms, improves WSD performance.reports results when adding several subsets of MRCOC to the MRREL relations. Instead of using all relations in MRCOC, we aim to identify the most useful ones using the Mutual Information (MI) statistic () that ranks pairs of concepts based on the probability that they occur more frequently than would Page: 2894 28892896
E.Agirre et al.be expected by chance. Several subsets of MRCOC relations were generated by ranking them by MI and successively adding those with the highest score to the graph created from the MRREL table. MRREL + MRCOC 1 adds approximately 750 000 new co-occurrences and MRREL + MRCOC 2 adds around 2 million. The table shows that performance drops systematically as cooccurrence relations from the MRCOC table are added to the graph. This situation is somewhat different from the positive effect of adding information from the MRREL table. These results enforce our hypothesis that co-occurrence relations negatively change the topology of the graph, degrading the performance of our algorithm. The smallest drop in performance is obtained when MRCOC 1 is added, suggesting that MI is a useful technique for selecting the most informative co-occurrence relations.shows the results when a graph is created using the combination of the four vocabularies that were considered. (There is no value for 'resistance' since one of its senses is not included in the subset of four vocabularies, see Section 4.2.) Finally, the column 'Full' shows results when the graph created using the entire MRREL table from the UMLS is used.shows a significant variation in performance for individual terms with results ranging between 99% ('secretion') and 11.1% ('fit'). The PPR algorithm has a bias towards senses that are highly connected within the graph. For some terms there are significant differences between the connectivity for the possible senses. For example, one possible meaning of 'fit', C0036572 'Seizures', is linked to 1561 other CUIs in the MRREL table while the alternative meaning, C0424576 'Fit and well', is only linked to 18. The majority of errors for this term were caused by PPR assigning C0036572 to examples for which the correct CUI was C0424576. The table also shows that the graph producing the best performance varies for each word. Overall, the graph created from the full MRREL table produces the best score for slightly more of the ambiguous terms (25) than either the individual vocabularies (20) or their combination (24). However, note that the overall average performance using the graph created from the full MRREL table is significantly better than when the subset is used (see).
Word by word analysis
Exploring context length and damping factorOur algorithm has two free parameters: context length and damping factor of the PageRank formula. Default values for these parametersshows the results obtained using different values for these two parameters.shows that the best results are obtained using a context of 30 words. However, the difference in performance compared to using our default context (20 words) is relatively small. Performance deteriorates when the context is limited (shorter than 10 terms). The lowest performance is obtained when no context is used (i.e. context size of 0), which corresponds to the static algorithm. Our algorithm is robust to the actual context length used, given a minimum amount of context. The best results when the damping factor was varied were obtained using a damping factor of 0.70, although performance was very close to the default value used in our experiment (0.85). This shows that the method is robust to changes in damping factor, provided it does not vary too far from the values suggested in the literature ().
DISCUSSIONThere is some debate on how accurate WSD performance has to be to assisted in applications.carried out experiments suggesting that a WSD system would need to correctly disambiguate 90% of words in order to be useful for Information Retrieval. However, more recent experiments () have shown that WSD with lower performance can improve Information Retrieval results, showing that the way in which the output of the WSD system is used is as important as the WSD performance. It has also been shown that WSD can improve several applications including Cross-lingual Information Retrieval (), Machine Translation () and Information Extraction (). Performance of the WSD components of these systems is generally not reported but it is likely that it will be lower than the result obtained by the best system in a community evaluation exercise of all-words WSD systems, 65% (). This suggests that the WSD accuracy obtained by our system could be used to improve performance of applications.The results reported here are based on the only available dataset, NLM-WSD, which contains terms that are frequent and ambiguous (). The inter-annotator agreement for this data set is relatively low [kappa score 0.47 (which indicates that disambiguating these examples is not easy for humans. The performance of our approach for all ambiguous terms in a document cannot be reliably extrapolated but may be higher than the results reported here given the challenging nature of the NLM-WSD dataset. It is possible that the performance of our algorithm could be further improved by adding other parts of the UMLS, such as the Semantic Types, to the graph, or by making use of domain information from the MeSH codes, which has been shown to be useful for supervised WSD in the biomedical domain ().
CONCLUSIONSThis article presents a WSD system for biomedical documents. The system is unsupervised and is able to disambiguate all words that are ambiguous in the UMLS Metathesaurus. Disambiguation is carried out by converting tables from the UMLS Metathesaurus into a graph and using the Personalized PageRank algorithm to select the best sense for each ambiguous word. Experiments show that the best results were obtained using the combination of all vocabularies in the MRREL table of the Metathesaurus. Performance of the approach reported here surpasses results reported for other systems that used the UMLS Metathesaurus as a knowledge source.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org 2889 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Note that the initial distribution values do no affect the final PageRank, provided that the algorithm converges.
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
In PageRank, the vector v is uniformly distributed, thereby assigning equal probabilities to all vertices in the graph when random jumps are made. However, in Personalized PageRank the vector v can be non-uniform and assign stronger probabilities to certain vertices, effectively biasing the resulting PageRank vector to prefer these vertices. For example, if we concentrate all the probability mass on a unique vertex v x , all random jumps on the walk will return to v x and consequently its rank will be high; moreover, the high rank of v x will cause all the vertices in its vicinity to also receive high rank. The importance of vertex v x in the initial distribution of v then spreads through the graph during successive iterations of the algorithm. In this case, the personalized P vector represents the importance of every vertex in the graph relative to vertex v x. Personalized PageRank can be solved using the same kind of algorithms as standard PageRank. Figure 1 shows the result of standard PageRank in (b), where v is uniform, and the result of Personalized PageRank in (c) for the case where v is set to 0 for all vertices except D, which is set to 1. For standard PageRank, vertex C receives a PageRank value of 0.29 [as in graph (Fig. 1b)], meaning that a random surfer on this graph would spend 29% of the time on that node. C has the highest rank among the graph nodes, and therefore node C is the most important node in the graph. On the contrary, if we use Personalized PageRank and the random surfer makes all random jumps to D [as in graph (Fig. 1c)], then the rank of D is now the highest, followed by node Awhich is linked directly to D and nodes C and B.
http://nlp.lsi.upc.edu/freeling/
For example, the term 'adjustment' contains CUIs that are included in MSH and SNOMEDCT. However, neither vocabulary includes all three possible CUIs for this term. 4 For example, the two CUIs for 'immunosuppression' are included in three vocabularies (CSP, MSH and SNOMEDCT) with SNOMEDCT producing the highest performance.
