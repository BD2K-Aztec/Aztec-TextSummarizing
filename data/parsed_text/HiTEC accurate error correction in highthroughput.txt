Motivation: High-throughput sequencing technologies produce very large amounts of data and sequencing errors constitute one of the major problems in analyzing such data. Current algorithms for correcting these errors are not very accurate and do not automatically adapt to the given data. Results: We present HiTEC, an algorithm that provides a highly accurate, robust and fully automated method to correct reads produced by high-throughput sequencing methods. Our approach provides significantly higher accuracy than previous methods. It is time and space efficient and works very well for all read lengths, genome sizes and coverage levels. Availability: The source code of HiTEC is freely available at
INTRODUCTIONDNA sequencing technologies have produced a revolution in biological research. Since the introduction of the Sanger method (), hundreds of bacterial and eucaryotic genomes have been sequenced, including several human genomes. This led to a significant number of biological discoveries. High-throughput sequencing technologies, such as Illumina's Genome Analyzer, ABI's SOLiD and Roche's 454, see e.g. (), produce gigabytes of data in a single run, thus taking sequencing to a whole new level. They provide the ability to answer biological questions with revolutionary speed. Some of their many applications include whole-genome sequencing and resequencing, single nucleotide polymorphism (SNP) discovery, identification of copy number variations, chromosomal rearrangements, etc. The impact of these technologies for everyday life, yet to be fully understood, will be far reaching. Many algorithms and software tools have been created to deal with the large amount of data produced by these technologies. Two of the fundamental and most investigated problems are read mapping and genome assembly. The former assumes the existence of a reference genome and attempts to find the location of newly sequenced reads from a different genome of the same species (; * To whom correspondence should be addressed.). The latter attempts to reconstruct the genome that originated the reads (). In spite of the many different approaches, these tools employ to solve their problems, they all share several common issues, such as very large data size, repeats in genomes and sequencing errors. The first two cannot be changed and we shall concentrate here on sequencing errors. Attempts have been made to either correct such errors or discard the erroneous reads. Some assembly tools include a spectral alignment-based read correction preprocessing step (), whereas others pre-filter the reads (). The very recent approaches of Salmela (2010);;are exclusively dedicated to read correction. The general idea for correcting reads is to use the high coverage of the current sequencing technologies in order to identify the erroneous bases in the reads. Each base is usually sampled many times and the correct value will prevail. The way such information is used can be spectral alignment () or subtree weight in suffix trees (). Whereasprovides an efficient implementation of the Euler-SR read correction algorithm ofby using CUDA-enabled graphics hardware (their program will subsequently be referred to as CUDA), the SHREC program byuses a novel idea, by employing weighted suffix trees. The algorithm of Salmela (2010) is a generalization of SHREC to mixed sets of reads. Error correction is quickly identified as a key problem in highthroughput sequencing data. Another software, Reptile, has been developed bysimultaneously with ours. It is also based on the k-spectrum approach of Euler-SR and CUDA. Our High Throughput Error Correction (HiTEC) algorithm uses a thorough statistical analysis of the suffix array built on the string of all reads and their reverse complements. It is intuitively explained in Section 2.2 and fully analyzed in the remaining of Section 2. We have tested in Section 3 our algorithm on many datasets, simulated or real, from, as well as on several new ones. The accuracy of HiTEC is significantly higher than the accuracy of all the other programs. (The accuracy is the ratio between the number of corrected reads and the number of initially erroneous reads.) Further, our own testing reveals a significant difference between the accuracy obtained by running the SHREC program and that reported by(We provide the values for both.) This is due to the fact that SHREC requires trying several parameter sets in order to find those providing the highest accuracy, which is likely not possible in real situations where the accuracy cannot be measured. HiTEC is not only more accurate but also more robust. Our algorithm works for a wide range of read lengths and coverage levels and it is the only one to do so with automatic adjustment, depending on the data set, based on our statistical analysis. In addition to high accuracy, the time and space complexities are very good. Our current serial implementation of HiTEC is comparable with Reptile and is about six times faster than the parallel implementation of SHREC on the four-processor machine we used for testing. The space consumption is comparable with Reptile and lower than that of SHREC for all tests. Nevertheless, we plan to improve the time and space of our algorithm by providing a parallel implementation. This and other further research directions are presented in Section 4 together with a summary of the achievements.
METHODS
Strings and suffix arraysThis section contains the basic definitions for strings and recalls the suffix array data structure. Consider the alphabet of four bases ={A,C,G,T}. A string is any finite sequence over. The set of all strings over is denoted by * . The length of a string s is denoted by |s| and, for 1  i |s|, sis the i-th letter of s. A substring of s is any consecutive sequence of letters from s, i.e. s=ss...s; in particular, for |s|=m, s = s. For i = 1 we obtain a prefix and for j = m a suffix of s. The reverse complement  s of s is obtained by first reversing s and then applying the transformation A  T,C  G. For example, if s = CAT, then  s = ATG. Clearly,   s = s. Let suf i denote the suffix sof s. Assuming a total order on the alphabet , the suffix array of s, denoted SA, gives the increasing lexicographical order of the suffixes of s, i.e. suf SA< suf SA
< ... <suf SA. For example, the suffix array of the string ACTAACACTG is shown in the second column of. The suffix array is often used in combination with the longest common prefix (LCP) array that gives the length of the longest common prefix between consecutive suffixes of SA, i.e. LCPis the length of the longest common prefix of suf SAand suf SA; see the fourth column of. By definition, LCP=0. The suffix array data structure has been introduced by; the SA array can be computed in O(m) time and space by any of the algorithms of;; the LCP array can be computed also in O(m) time and space by the algorithm of. However, suboptimal algorithms exist that behave much better in practice. We have used the libdivsufsortthe reads came from is shown at the bottom. The letter (inside the frame) following the witness u = CTGTTGTCTC (underlined) should be T and not A. The support values are supp(u,T) = 5 and supp(u,A) = 1. If we omit the gray part, then the remaining suffixes are lexicographically sorted, as in SA. library of Yuta Mori 1 in our program. Also, since we need only bounded LCP values, we preferred a direct computation of the LCP, thus avoiding () altogether.
Basic idea for correcting errorsThe basic idea for correcting errors in reads is intuitively explained below. Consider a genome G of length L that will be modeled as a random string over , where each letter appears with equal probability 0.25. Assume also that n reads, r 1 ,r 2 ,...,r n , each of length , have been produced from G with the per-base error p. That is, each read has been obtained by randomly choosing a substring of length of G and then changing each letter into any of the other three with equal probability p 3. (Reads containing any letter not in are discarded.) We call the position erroneous if its letter is different from the corresponding one in the genome and correct otherwise. We construct the string of all reads and their reverse complementswhere $ is a letter not in. Denote the suffix array and longest common prefix array of R by SA and LCP, respectively. The basic idea for correcting an erroneous position in a read is as follows.Assume that the read r i , sampled starting on position j of the genome, contains an error in position k and that the previous w positions, r i [k w..k 1], are correct. That is r i = xuay, where x,u,y  * ,|x|=k w1, |u|=w and a . The letter a is different from the letter b = G [j +k 1] that actually appears in the genome. However, many other reads will have sampled that region correctly, i.e. they contain the correct substring ub. The fact that u is followed more often by b than by a will let us suspect that a should actually be b and, if the evidence is strong enough, we shall replace a by b. The string u is witnessing that a is an error and also that it should be changed into b. Formally, a witness is any substring u of R, of an a priori fixed length w, such that u contains no occurrence of $. For a  , the support of u for a, supp(u,a), is the number of occurrences of the string ua in R. Seefor an example. Our algorithm consists of computing the support values and using them, based on thorough statistical analysis, to correct erroneous bases. Note that our support values are similar with the weights of the suffix tree vertices in SHREC. The similarity with SHREC stops here as we use more efficient data structures for computation and completely different procedure for correcting.
Statistical analysisWe now formalize the idea in the previous section. For a given witness u, we define the cluster of u as the set of all positions in R where an occurrence of u followed by a letter different from $ starts. The size of this cluster is clust(u) = a supp(u,a) .
Page: 297 295302
HiTECAll these positions are consecutive in SA and so are all occurrences of u supporting the same letter. This makes it easy to compute the support values and cluster size, given the suffix array. The clusters, corresponding to witnesses of a given length w, are easily found using the LCP values: a cluster consists of all consecutive positions with LCP values w or higher so that the (w+1)st letter is not $. In, the occurrences of a witness are shown in the order in which they appear in the suffix array. Assume for now that any witness u of length w does not appear elsewhere in the genome since additional occurrences would make the identification of the errors more difficult. Due to repeats in the genome, this may not be possible but what we can do is reduce the probability of random occurrences in our Bernoulli model. We have two competing goals here. A long u will be less likely to appear again in G but will be covered by fewer reads, thus reducing its useful support. We shall return to this issue. We first estimate the support given by a witness u to a letter a following it. We need to distinguish between the case when the witness contains errors and when it does not. As a witness may appear with errors in some reads and without errors in others, precise definitions are needed. A witness is correct if it occurs as a substring of G and erroneous otherwise. Consider the case of a correct witness u = G [i..i+w1] supporting a correct letter a = G. A read covering both has to start within the intervaland thus the expected number of pairs (u,a), both u and a correct, given that supp(u,a) = k, isAssume next that w is correct but a is an error. We now need those reads covering ua that have no error inside u but the original letter in a's position, say b, has been replaced by a. Again, there are w positions where they can start but the probability that a given read starting in that interval has the errors exactly as specified is q e = wThe expected number of such (u,a) pairs, u correct, a erroneous, given that supp(u,a) = k, isIn the case when u is erroneous and a is correct, we may assume that u contains only one error, since otherwise the support is much lower. Then, we are interested in all the reads covering ua and containing the same error as u. Therefore, the reasoning is very similar with the one for the case when u is correct and a is erroneous. In the remaining case, when both u and a are erroneous, the support is much lower. The above analysis helps us compute a threshold, T , that will distinguish the support by a correct witness for a correct letter from the support when either the witness or the position, or both, are erroneous. Often, there is an interval of values k where both W e (k) and W c (k) are very small and any T in this interval is good. Also, this interval grows when the error rate decreases and hence a good value of T remains good when some of the errors have been corrected. An example is shown inwhere the values of W e (k) and W c (k) are plotted for error 0.01 in the left plot; the right one shows the region where both W e (k) and W c (k) are very small. To cover also the case when such a region, with very low values of both W e (k) and W c (k), does not exist (as it happens for low coverage), we increase the value of T by an experimentally computed constant of two:A problem is that some reads will have their errors distributed in such a way that there are no w consecutive correct positions. That makes it(k) and W e (k) for L = n = 4.2 mil., = 70, w = 21, p = 0.01 are shown in the left plot, whereas the right one shows the region of the left one where the values of both W c (k) and W e (k) are very low. The value of the threshold T in this example equals 9.Then, the expected number of such reads isThese values for error rates 0.01, 0.02 and 0.03 are shown for a 4 MB genome in the left plot in. The number of reads with k errors decreases with,) increases and so the maximum is reached somewhere around 45 errors. We are interested in the total number of reads uncorrectable with a witness of length w, i.e.The percentage this number represents, in our example in, for a witness of length w = 21, out of the total number of expected erroneous reads, E e = (1(1p) )n, is 0.15 for p = 0.01, 0.87 for p = 0.02, and 2.56 for p = 0.03. We need to lower therefore the length of the witness in order to correct some of these reads. When the witness length is reduced to 18, the percentages drop to 0.02, 0.17 and 0.68, respectively (see the right plot in). However, another problem appears. The number of uncorrectable reads drops with the decrease of the witness length but the probability of the witness occurring more than once in the genome is no longer negligible causing correct positions to be wrongly changed as follows. Assume that ua appears in G and that ua is sampled by a read as va, that is, a is correct but u contains errors that change it into v. The length of v is also w and the Page: 298 295302, for L = n = 4.2 mil., = 70 and p = 0.03. probability of v appearing in G is non-negligible. Assume v occurs in G at some position followed by b = a. Then the support given by v to b will be very large and to a very small, causing a to be changed, incorrectly, into b. We now approximate the number of such errors. We need that u has errors, a is correct, v appears in G and b = a. The probability for this to happen is
L.Ilie et al.The probability that at least one correct position in one read is changed this way is 1(1q w ) w and the expected number of destructible reads, that is, correct reads the are turned erroneous this way isIt can be seen fromthat the optimal values for p = 0.01,0.02,0.03 are w m = 19,17,16, respectively. Choosing w = w m provides, theoretically, the highest accuracy for the current iteration and a greedy strategy becomes apparent. However, it turns in out that a combination of values that are close to the optimal w m and the smallest w that causes essentially no correct reads to be changed, that is,works best in practice. While witnesses of length w M effectively correct all but the uncorrectable U(w M ) reads, those of length w m will create large enough stretches of consecutive correct positions inside an additional U(w M )U(w m ) reads so that they become correctable by witnesses of length w M. Also, w M satisfies the conditions under which we computed the parameter T and hence it will be also used for this purpose. The sequence of witness lengths used in the HiTEC algorithm, denoted w seq = w seq, is:Finally, some reads will contain several errors that often require several iterations of our correcting procedure. Instead of estimating at the beginning the number of iterations needed, it is more reliable to stop the process when the number of corrected positions in a single iterations drops below a certain threshold. Such an approach, with an experimentally computed value for the threshold will be used in our algorithm.
The algorithmThe pseudo code of the HiTEC algorithm that results from the above reasoning in shown in. Note that only L and p are required as input parameters. An approximate value for L is probably known before the experiment. If not, then L can be estimated by a biological experiment or an expectationmaximization procedure [such as in (. An approximate value of p should be known from the machine that does the sequencing. HiTEC (r 1 ,.
..,r n )given: n reads r 1 ,...,r n (of length each); L and p output: n corrected reads(4) 7. construct R and compute SA and LCP 8. compute the clusters in SA for all witnesses of length w 9. for each witness u with clust(u)  T +1 do 10. Corr {a | supp(u,a)  T } 11. Err {a | supp(u,a) 23. return all r j 's from RAs previously mentioned, in Step 7, we used for the construction of the SA array the libdivsufsort library, the state-of-the-art algorithm for suffix array construction, which is significantly faster and more space efficient than the theoretically optimal algorithms. For the LCP array, we only need to check in Step 8 whether the LCP values are smaller or larger than w. We eliminate the need to store the LCP array by employing a direct computation of these values. Cache effects ensured that the time remains essentially the same. For each witness u with a large enough cluster, the sets of correct and erroneous letters supported by u are built in Steps 1011. If there is no ambiguity, then we correct in Step 14. If there is ambiguity, then we check also the next two letters (Step 18) in order to decide how to correct. In either case, the position of a in the string R corresponds to a position inside a read r (which can be some r j or  r j ) and we correct both r and its reverseThe procedure stops when the number of bases changed during one iteration drops below 0.01% of the total number of bases (Step 22). In any case, no more than nine iterations are performed. This last condition is necessary since the ratio between the number of bases changed in one iteration and the total number of bases becomes less reliable as an indicator of the actual number of corrected reads when the number of iterations increases. One practical improvement not mentioned in the algorithm is that, due to the excellent performance of HiTEC for modest coverage, datasets of high coverage can be split into several sets of lower coverage that are independently corrected, thus saving space.
RESULTS
AccuracyThe accuracy is defined as the ratio between the number of corrected reads and the number of initially erroneous reads. The erroneous/correct status of each read (before and after correcting) has been determined by a straightforward search (using suffix arrays) of all reads in the genome sequence. To be precise,. If we denote by TP,TN,FP,FN (true/false positive/negative) the number of erroneous reads that are corrected, correct reads that are left unchanged, correct reads that are wrongly changed and erroneous reads that are left unchanged, respectively, then we have err bef = TP +FN, err aft = FP +FN and thereforeWe have compared the accuracy of our algorithm on quite a number of datasets, 2 including those of. All programs have been run with default parameters. Several bacterial genomes, see, were downloaded from GenBank under the accession numbers given. Later on we shall refer to these genomes by the IDs given in parentheses in the first column. To be precise, we constructed a number of datasets by uniformly sampling reads with given length, coverage and per-base error rate from the above genomes. The datasets can be considered identical with those ofbecause they have been generated from the same genomes with the same parameters. Experiments show that the differences between the accuracy of a program on different datasets generated with the same parameters as above are insignificant. The datasets used byare shown in, those ofin, andcontains a mixture of read lengths and coverage levels taken from the longest genome considered.contains several real sets of Illumina reads. The first real dataset was used also by; it is available from www.genomic.ch/edena.php and it was previously used by. Both the first and the second real datasets were used by. The second one is available from sharcgs.molgen.mpg.de/download.shtml and it was used initially by. The third one is new and is available from clcbio.com/index.php?id=1290, the CLCbio website, as an example of NGS data. The reads in each of the first three dataset were selected using RMAP () as previously done, according to Schmidt (Personal communication):The read length is 70 bp and coverage is 70 for all datasets.The read length is 35 bp and coverage is 70 for all datasets.The per-base error rate was computed by counting the total number of mismatches from the output file of RMAP. The fourth real dataset has been suggested by one of the reviewers as a most recent example of Illumina reads. It has accession number ERA000206. Due to its very large size, the SHREC program could not produce any results. The performance of HiTEC is very high in spite of the fact that no selection of the reads using RMAP has been done such as for the previous three real datasets.contains the relevant datasets from. A common feature of the sets of reads in the Sequence Read Archive (SRA, http://www.ncbi.nlm.nih.gov/sra) is that the sequence of the genome from which the reads were sequenced is not known and therefore, considering another genome instead can produce misleading results. Out of the datasets of, the percentage of the reads that can be mapped on the indicated reference genome is around 97% for the first two datasets and around 6070% for the other ones. This is a clear indication thatthe genomes for the last four datasets are not the actual genomes that produced the reads. Therefore, we could meaningfully use only the first two. In Tables 25, the 'SHREC' column gives the accuracy values we obtained by running the SHREC program, whereas the values in the 'SHRECpaper' column are taken from. The values in the 'CUDA' column are taken from, that is, we did not test the program. [The values are missing for tests not performed by. Several comments are in order. First, the accuracy of HiTEC is significantly higher than that of all the other programs for all experiments. The difference is significantly higher for the real datasets. While the accuracy of HiTEC is similar to the case of simulated data and is not affected by the error rate or coverage, CUDA's accuracy is much lower compared with the simulated data and SHREC's accuracy decreases dramatically with the increase of the error rate. Reptile's accuracy is comparable to that of SHREC when quality scores are unavailable () and still significantly below the accuracy of HiTEC even when using quality scores (). HiTEC performs particularly better for lower coverage. Second, there is a significant difference, which increases with genome length and error rate, between the accuracy of SHREC as resulting from our testing of the software and that provided by. This is due to the fact that Schroder et al.. Anyway, the accuracy of HiTEC is significantly higher than all the other accuracy values. Third, the SHREC program was run with the same number of iterations as ours, as resulted from the stopping criterion in Step 22. In most cases, the accuracy of HiTEC after one or two iterations is already higher than that of SHREC after nine iterations. Similarly with SHREC, the parameters of Reptile are fixed. That means they do not adapt to the data. As a result, Reptile could not correct any errors of the fourth dataset from. One last remark, our measure, accuracy, is different from the gain of. For the datasets considered in, the gain for HiTEC is 83.33 and 82.22, respectively, whereas Reptile produces 82.81 and 72.53. So, gain indicates again that HiTEC has better performance.
L.Ilie et al.
Time and spaceIn addition to obtaining very high accuracy, our algorithm has also very good time and space complexities, due to the use of good data structures. The tests shown inwere performed for the datasets inon a Sun Fire V440 Server, with four UltraSPARC IIIi processors at 1593 MHz, 4 GB RAM each, running SunOS 5.10. Our serial implementation of HiTEC is about six times faster than the multithreaded SHREC. The space required by our algorithm is comparable to that of Reptile and both are lower than SHREC's. Reptile is slightly faster; however, the running time of HiTEC includes many iterations. In fact, HiTEC achieves higher accuracy sooner.
Very large genomesWe have also performed measurements to predict the ability of our algorithm to correct errors in the case of very large genomes. We plotted in, the percentage of U(w)+D(w) (uncorrectable plus destructible reads) for genome size 1 GB. We used two common read lengths from Illumina: 75 and 100. In practice, the error rate increases with read length but so does our algorithm's performance, only faster. While for reads of size 35 the ratio of those that can be corrected decreases below 50% for very large genomes, the situation is much better already for read size 50. In a 1 GB genome, for an error rate of 0.01 and a read length 50, our algorithm can correct up to 97.72% of the erroneous reads but when we increase the read length to 100, we can correct up to 97.70% even for a very high error rate of 0.03.
CONCLUSION AND FUTURE RESEARCHThe main goal of this article is to provide an algorithm that is highly efficient at correcting the errors of high-throughput sequencing technologies. According to the extensive testing we have performed, our algorithm exceeds significantly the accuracy of previous algorithms. Also, a thorough statistical analysis makes our program the only one that is able to automatically adjust to the input data. The testing has been done on Illumina reads but the approach is suitable for any type of reads for which the errors consist mainly of substitutions. The ability of our program to correct increases with the length of the reads and, according to, the read length is going to grow with the third generation of sequencing technologies, such as single molecule sequencing or nanopore sequencing. We therefore hope that our program will be very competitive even with the rapid change of sequencing technologies. We plan to work on a parallel implementation which will be able to handle the even more massive outputs to come. The new implementation will be capable of dealing with all types of reads as well as with mixed sets of reads. Quality scores as well as additional knowledge of the bias of the sequencing devices concerning the actual distribution of the positions of the reads in the genome could help us improve further the accuracy of our algorithm.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
libdivsufsort: a lightweight suffix sorting library, http://code.google .com/p/libdivsufsort/.
Most of these tests were performed on the SHARCNET high performance computers: www.sharcnet.ca.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from [09:23 19/1/2011 Bioinformatics-btq653.tex]
