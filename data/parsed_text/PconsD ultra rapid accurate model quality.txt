Clustering methods are often needed for accurately assessing the quality of modeled protein structures. Recent blind evaluation of quality assessment methods in CASP10 showed that there is little difference between many different methods as far as ranking models and selecting best model are concerned. When comparing many models, the computational cost of the model comparison can become significant. Here, we present PconsD, a fast, stream-computing method for distance-driven model quality assessment that runs on consumer hardware. PconsD is at least one order of magnitude faster than other methods of comparable accuracy. Availability: The source code for PconsD is freely available at
INTRODUCTIONPredicting the 3D structure of protein from its sequence remains one of the yet unsolved problems of molecular biology. Recent improvements in the field allow for constructing more accurate models of protein structures, be it through relying on homologous structures, folding proteins ab initio or by combining multiple approaches. Regardless of the approach chosen, there is a need for a method of discriminating good (native-like) models from the mispredicted ones. This is the role of Model Quality Assessment Programs (MQAPs). The most successful approaches to MQAP problem is the use of clustering methods, as introduced by the Pcons method in CASP5 (). They are based on the premise that among different models of the same protein, the one that is most similar to the others is most likely to be correct. It is widely assumed that if a sufficiently accurate model generation method is used, most of the predictions will tend to cluster near the correct fold (). Consequently, the largest cluster of protein models is most likely to contain models of native-like fold, and cluster centroid is the most likely candidate for the most accurate structure in the ensemble. Most of clustering-based MQAPs rely on repeated pairwise structural superposition to find a largest subset of residues superimposable between models in question within a certain threshold. This approach is a computationally relatively expensive process, which renders clustering approaches unfeasible for model ensembles larger than some thousand proteins. An alternative to rigid superposition is comparing the inter-residue distance matrices of the models. This approach has been successfully used for quality assessment, both in terms of similarity of predictions to the native structure () and predicting the quality of models (). Authors postulate that this approach is capable of capturing the interactions relevant for protein structure better than rigid C superposition, as it accounts both for the local proximity of relevant residues (distances close to each other in the sequence space) and the general fold of the protein (long-range distances). Although avoiding the computational expense of repeated superposition, distance matrix comparison still requires 0:5  m  n 2 operations for computing the distances and 0:25  m 2  n 2 operations for their comparison, when given m models of n residues each. In this work, we introduce PconsD, a new model quality assessment program, which uses a massively parallel, OpenCLbased streaming approach, which attempts to alleviate the scaling issues and provide an efficient platform for future development in distance-driven quality assessment.
APPROACHPconsD is implemented in Python and OpenCL, a framework for massively parallel data-driven computation, that is capable of execution across heterogeneous platforms, such as CPUs or graphical processing units (GPUs). This work uses commodity GPUs as a platform of choice, but it can be easily ported to other platforms. GPU have been designed originally to handle 3D computer graphics computations, which require a vast amount of relatively simple, mutually independent calculations, based on large input matrices. The problem of computing distance matrices and comparing them is well suited to this computing paradigm. PconsD operates on sets of models in PDB format, requiring the residue numbering to be consistent among models to maintain short running time. Except of the initial population of an array containing atom coordinates, all the computation happens on GPU, allowing for massively parallel data processing. Program computes similarity score between all the C-C (in case of glycinesC) atoms in the protein, averages them and outputs the computed similarity score. Here, PconsD relies on linear scoring function with 5 A  cut-off, 0), but other functions can also be used, see Supplementary Material. To assess the time performance, four we ran four quality assessment methodsPconsD, ModFoldClustQ (), Pcons () and a navenave predictor based on *To whom correspondence should be addressed.  The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com pairwise model comparison by TM-scores (), on subsets of randomly selected models or varying size (from 10 models to 250 000 models). All the models have been preloaded into memory, and all the softwares were stored on a Linux tmpfs random access memory disk, to avoid the effect of i/o on the benchmarking.
RESULTSPconsD has been blindly benchmarked in model quality assessment category of CASP10 experiment. According to CASP assessors, PconsD is at least as good a model classifier as other clustering MQAP methods () and comparable with compound methods [methods combining multiple approaches to quality assessment, such as Pcomb (), ModFoldclust2 (), MUFOLD-QA (, both as far as distinguishing good models from bad ones, and as far as selecting best model in the ensemble are concerned, see. PconsD selected more good models (with less than two GDT-TS () points loss to the best model in the ensemble) than Pcons (one of state-of-art clustering methods) or Pcomb (the best MQAP in CASP10 in terms of picking best models). It also picked fewer poor models than Pcons (410 GDTTS points loss to the best model in ensemble). The slightly worse performance in terms of average GDT-TS is due to prediction targets on which clustering did not work sufficiently well (i.e. 'difficult', free-modeling targets). PconsD seemed to be slightly less suitable for ranking models, when considering the Pearson correlation coefficient with superposition-based metrics, such as GDT-TS, but it still performs better in this category than MUFOLD-QA. The main strength of PconsD lies in its outstandingly short prediction time. The prediction performance and scaling have been assessed on a set of up to 250 000 models (55 GB of PDB files) of PTS EIIA type-2 domain from Escherichia coli (pdbid: 1a3a:A), produced by ab initio folding protocol implemented by ROSETTA (). The benchmark results show that PconsD for non-trivial task sizes is at least an order of magnitude faster than Pcons, which is already almost two orders of magnitude faster than the navenave clustering using TM-score, thanks to aggressive optimization (),. PconsD scales linearly with the amount of models, until it needs to perform problem domain partitioning, on which moment it starts scaling quadratically. On the Nvidia GTX 560 used for testing PconsD, the running time with 10 8 contacts (5000 models of 143 residues each, 1000 of 300 residues and so forth) is 51 min.
ACKNOWLEDGEMENTThe benchmarking results were kindly provided to us by Andriy Kryshtafovych.Note: Official results from CASP10. Pearson r: correlation between method score and GDT-TS. delta GDT-TS: average GDT-TS loss that is difference in GDT-TS between top-ranked model and best model in model ensemble. Next row specify the fraction of targets for which the selected model had a negligible (52) GDT-TS loss. Classification shows Matthew's correlation coefficient for discriminating between correct and incorrect models. Comparison is done between two pure clustering methods (PconsD and Pcons.net) and three compound methods (Pcomb, MFC2: ModFoldclust2, MUF: MUFOLD-QA).
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
McGuffin,L.J. and Roche,D.B. (2010) Rapid model quality assessment for protein structure predictions using the comparison of multiple models without structural alignments. Bioinformatics, 26, 182188. Roche,D.B. et al. (2011) The IntFOLD server: an integrated web resource for protein fold recognition, 3D model quality assessment, intrinsic disorder prediction, domain prediction and ligand binding site prediction. Nucleic Acids Res., 39 (Suppl. 2), W171W176. Shortle,D. et al. (1998) Clustering of low-energy conformations near the native structures of small proteins. Proc. Natl Acad. Sci. USA., 95, 1115811162. Wallner,B. and Elofsson,A. (2007) Prediction of global and local model quality in CASP7 using Pcons and ProQ. Proteins, 69 (Suppl. 8), 184193. Wang,Q. et al. (2011) MUFOLD-WQA: A new selective consensus method for quality assessment in protein structure prediction. Proteins, 79 (Suppl. 10), 185195. Zemla,A. et al. (1999) Processing and analysis of CASP3 protein structure predictions. Proteins, (Suppl. 3), 2229. Zhang,Y. and Skolnick,J. (2004) Scoring function for automated assessment of protein structure template quality. Proteins, 57, 702710.
M.J.Skwark and A.Elofsson at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
