Motivation: Time-to-event regression models are a critical tool for associating survival time outcomes with molecular data. Despite mounting evidence that genetic subgroups of the same clinical disease exist, little attention has been given to exploring how this hetero-geneity affects time-to-event model building and how to accommodate it. Methods able to diagnose and model heterogeneity should be valuable additions to the biomarker discovery toolset. Results: We propose a mixture of survival functions that classifies subjects with similar relationships to a time-to-event response. This model incorporates multivariate regression and model selection and can be fit with an expectation maximization algorithm, we call Cox-assisted clustering. We illustrate a likely manifestation of genetic het-erogeneity and demonstrate how it may affect survival models with little warning. An application to gene expression in ovarian cancer DNA repair pathways illustrates how the model may be used to learn new genetic subsets for risk stratification. We explore the implications of this model for censored observations and the effect on genomic pre-dictors and diagnostic analysis. Availability and implementation: R implementation of CAC using standard packages is available at https://gist.github.com/programeng/ 8620b85146b14b6edf8f Data used in the analysis are publicly available.
INTRODUCTIONIn cancer genomic studies, unobserved heterogeneity obfuscates the effort to build accurate descriptive models of risk stratification. In ovarian cancer, for example,argue that the set of patients with the same clinical disease have distinct molecular diseases. With respect to inference, this implies that the same regression models may not be valid for every patient and further that it is unclear which patients should be considered together (). Therefore, a major statistical task is to organize patients into previously unknown classes while simultaneously fitting their time-to-event models.The examples throughout the article are taken from our ongoing analysis of data from The Cancer Genome Atlas (TCGA), which has the goal of cataloging all of the genomic alterations in cancer. For each patient, there is a tremendous amount and variety of data: 12 000 genes in expression arrays, 1 million singlenucleotide polymorphism genotypes, exome and whole-genome sequences, methylation of thousands of CpG islands and the expression of microRNA. From this plurality of data, we anticipate that exploratory methods will serve to extract and characterize genetic subgroups relevant to survival time clinical outcomes. For summarizing the impact of a genetic signature, one often stratifies patients to demonstrate separation between classdefined survival curve estimates. Unfortunately, asreview, current methods either awkwardly dichotomize a continuous score at a post hoc threshold or rely on hierarchical clustering to define subgroups with no necessary relation to survival. In that sense, it is desirable to have a method that identifies genetic subgroups supervised by their survival times. The standard methods for dealing with non-homogenous time-to-event data do not apply when our goal is to discover unknown subgroups. Continuous frailty models () treat all individuals separately and therefore do not produce subgroups. O'emphasize the use of random effects and stratified regression models when subgroups are known. Classification and regression tree methods have been adapted for survival responses (). However, these methods partition the predictor space to form a single piecewise functional estimate, and our interest is in the subgroups that exist in similar regions. Some treatment of heterogeneity relevant to survival time where a subgroup of patients does not expire appears in the cure rate model literature (). Our situation is distinct in three ways: a subgroup may have variable time-to-event outcomes (cured patients have infinite survival times), the variables of interest to each mixture component may be distinct and the set of patients in each subgroup is not known. In this article, we propose a discrete mixture regression model that synergizes with potential heterogeneity in time-to-event data. Concretely, we assume that observations belong to unlabeled classes with class-specific proportional hazards (PH) regression models relating their genetic covariates to survival time outcomes (Section 3). This conditional semi-parametric model leads to a surprising variety of model effects, which we illustrate *To whom correspondence should be addressed.  The Author 2014. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com in Section 2. In Section 4, we describe an algorithm and the considerations for fitting the model. Simulations highlight the use of censored data (Section 5) and a data analysis demonstrates a single-pathway hypothesis-driven model (Section 6). Our discussion (Section 7) again emphasizes the exploratory role that this analysis may address.
ILLUSTRATIONS
Genetic heterogeneityThere is evidence that distinct molecular subgroups lead to the same clinical presentation of ovarian cancer (). This form of genetic heterogeneity may arise because the commonalities leading to cancer may aggregate within pathways and not on the level of genes (). In the following case study, we highlight the ability of the mixture to produce unusual associations between covariates and survival and how it may augment our understanding of subgroup discovery. Suppose that X $ N 0, 1 represents a single typical normalized gene expression measurement and that patients do have survival times arising from two distinct hazard models, h 1 tjx  h 0 t exp2x and h 2 tjx  h 0 t exp2x. These hazards represent an extreme version of heterogeneity in expression; in one class, the gene has a protective effect and in the other it is equally deleterious. Assuming the baseline hazard is exponential h 0 t  1, we generate 1000 complete survival times, Y, under each of these hazards and plot them on the log scale with their randomly generated expression in. Without knowledge of the true classes, fitting a standard Cox regression to these data finds no significant relationship between Y and X ( ^  0:0314, P  0:1). This effect is strong enough that the relationship is easily identified if the true classes are known ( ^ 1  1:93, P50:001 and ^ 2  2:12, P50:001). A standard diagnostic technique to estimate non-linear relationships between Y and X is to use a smoothing estimate on the added variable plot (). In this case, it fails to identify any important effects. Estimating a time-varying effect is another diagnostic for assessing PH (). Again it does not discern any non-proportional effect (P  0:116) or time-varying effect (). So, by the standard analyses, this important gene would not be identified for further study. With respect to gene expression analyses, this is a case where differential expression (DE) models that look for mean difference will fail: there is no true underlying survival difference between classes that may be attributed to X (). This means that models that try to estimate a rule 1 fX4cg that can classify patients are not applicable. So, the mixture reflects a different way to
Cytoreduction and epiregulinWe draw known class labels from a measured surgical covariate, which is presently used as a biomarker. Cytoreduction, the amount of tumor remaining after surgery, is a measure of success of surgical debulking, which is a component of primary therapy in ovarian cancer. Patients who are suboptimally cytoreduced have a clinically significant amount of residual tumor that will seed future recurrent disease and progression (). Using the TCGA study to be introduced in Section 6, we separate the patients into optimal and suboptimal categories and provide a kernel smoothing estimate of their hazards (). The estimated hazards are clearly non-proportional [P  0.007,], reflecting the early protective effect of optimal cytoreduction and its transient nature. Fitting separate models in each subgroup, we searched for genes whose relationship with survival inverts over classes finding epiregulin (interaction P  0.004), which has been recently highlighted as a progression marker (). In optimally cytoreduced patients, increased expression is detrimental to survival ( ^  0:156, P  0:014); in suboptimal patients, increased expression is protective ( ^  0:452, P  0:012). In, we have plotted the estimated survival for high and low epiregulin expression for optimal and suboptimal patients. Noting that epiregulin has been shown to inhibit epithelial tumor cells and stimulate cancer-associated fibroblasts (), the surgical outcomes may indicate a more epithelial or more fibrous tumor; a fair biological explanation is that epiregulin expression leads to the inhibition of tumor burden (a better prognostic outcome), or the stimulation of fibroblasts that leads to cancer progression. Thus, these effects do exist and imply surprisingly deep biological connections. Following a genomic survey, this type of effect is an ideal target for functional studies. Given that we want to identify genetic subgroups with different prognoses, we should favor a model that admits unknown and possibly dramatically different survival experiences. The mixture model should let us estimate labels and should be able to resolve non-PH.
METHODSLet Y i , i , x i , i  1,. .. , n be an independent right-censored sample with regression covariates x i  x i1 , x i2 ,. .. , x ip  0 , where i  1 indicates that the complete time has been observed. We will denote the collections of survival times, censoring indicators and covariate vectors as Y  Y 1 ,. .. , Y n ,    1 ,. .. , n  and x  x 1 ,. .. , x n , respectively. To account for heterogeneity, we propose that each patient arises from one of K latent classes with probability k , k  1,. .. , K, P k k  1. We assume Cox's PH model () within each class k, so that the covariate vector x enters the model log-linearly via a class-specific hazard: log h k tjx  log h 0k t  x 0 k. For a general introduction to the Cox regression, see Hosmer et al. (2011). In particular, recall that a rightcensored observation following a PH model has the density f k y, jx  h 0k y expx 0 k    exp H 0k y expx 0 k    , 1 where h 0k t and H 0k t are the baseline hazard and baseline cumulative hazard for the kth class. The mixture density may be written asIf we also observe the latent class U  U 1 , U 2 ,. .. , U n , where U i $ Multinomial, U i 2 f1, 2,. .. , Kg and u ik  1 fUikg , we may write the density of the complete data asTo estimate the regression coefficients and baseline hazard parameters, we propose maximizing this likelihood via the expectation maximization (EM) procedure () described in Section 4. The discrete mixture leads to the model's interpretation as organizing observations into clusters that are not known a priori. This type of clustering should not be confused with clustered survival data, which typically refers to the case where class labels identifying multiple observations from the same source (e.g. treatment centers or year of diagnosis) are known. Instead, observations are gathered according to their best-fitting regression model. Additionally, our mixture relaxes the PH assumption; we only need to assume that hazards are proportional within their given clusters. The practical interpretation of this property is highlighted in the example given in Section 2.2. Continuous frailty () and random effects) are other common methods for accommodating heterogeneity, but they still rely on the PH framework. In addition to presenting a novel approach for handling heterogeneity and subgroup discovery for time-to-event data, our approach offers a new contribution to the finite mixture model literature (). The problem of finite mixtures has been explored in mixed effects models (), generalized linear models () and discrete-time survival models (MuthenMuthen and); our approach extends the idea to PH regression.
ALGORITHM: COX-ASSISTED CLUSTERINGWe refer to the following algorithm for maximizing the mixture likelihood as Cox-assisted clustering (CAC). For convenience, we write the parameters to be estimated as   1 , 2 ,. .. , K , the mixing proportions, h  h 01 t,. .. , h 0K t   , the set of baseline hazard functions and   1 ,. .. , K , the coefficient vectors. We further abbreviate the hazards at their evaluation points: h 0ki  h 0k Y i  and H 0ki  H 0k Y i . The complete data likelihood with mixing parameters and class-specific parameters h and may be separated into a mixing distribution part and a component distribution part: log L, h, ; Y, , Ujx  log L 1 ; U  log L 2 h, ; Y, jU, x:The first is simplyand the likelihood associated with the component distributions isTo compute the maximum likelihood estimate, we follow an EM approach that estimates and optimizes the observed data log likelihood by plugging ^ u ik  Eu ik jY i , i , x into the complete data likelihood. Supposing that the current values of the parameters at the mth iteration are m k , h m 0ki , H m 0ki and m k , the algorithm proceeds as follows. In the E-step, conditional mean isafter the application of Bayes rule. We note that, unlike the standard Cox regression setting, computing the baseline hazard is necessary to compute the conditional means. It can be shown that if we assume a common baseline hazard across clusters, the E-step update depends only on the current estimates of and. In the M-step, the update for mixing proportions k is straightforward:To update h, we make a profile likelihood argument that leads to a partial likelihood (). Suppose we hold k constant. Maximizing over h 0k , we obtain profile estimates of the hazards as a function of the m1 k that are similar to Breslow (1974):The profiled M-step objective is a partial likelihood weighted by the ^ u ik :Each component indexed by k may be maximized separately to obtain the m1 k update using standard statistical software. The M-step is operationally equivalent to fitting K-weighted Cox models. Finally, one iterates between the E and M step until the increment in log-likelihood is small.
Starting conditions and number of classesAs an iterative procedure, the EM algorithm requires initial values 0 , 0. For analyses that begin with strong biological hypotheses, the corresponding parameters may be set directly. An alternative is to choose starting parameters by assigning observations to specific classes and estimating the initial 0 and 0. This is equivalent to setting an initial value for every ^ u ik and running the algorithm forward. This assignment may be random; one may set a randomly selected ^ u ik to 0.8, say, and divide the remaining weight among the other classes. In practice, we use multiple random starts and pick the best by the fitted loglikelihood. As in other clustering problems (), we select the number of classes using the Bayesian information criterion (BIC). Let LK  Lh K , K , K ; Y, , Ujx, where we have added the K subscript to emphasize the dependence. The BIC criterion is expressed as BICK  2 log LK  pK logn, 11 where p is the dimension of X and n is the number of observed patients. The value of K minimizing BIC(K) is a penalized compromise between fit and complexity. Also, while Volinsky and Raftery (2000) propose weighting by the number of observed events, log P i i , because logn is always larger, the standard BIC is a more conservative criterion. As a measure of model sensitivity to additional clusters, we consider an adaption of the DFBETA statistic (). Given a model fit for K classes, each patient i can be assigned a 1693
Cox-assisted clusteringparameter vector  ~ u K i  given their assigned class, ~ u K i. For each component j, we simply consider the average change over K:This statistic will be large when the coefficients change dramatically between cluster numbers. Conversely, if the (K  1)th cluster simply subdivides an existing cluster, the statistic will be small.
SIMULATION STUDIESAlthough there are several properties of the model and algorithm to highlight, we focus on its treatment of censored data and a demonstration of its estimation ability. Let true class indicator U i 2 f1, 2g be evenly split among 2 n observations with a single covariate X 1 ,. .. , X 2n  $ N 1 2n , I 2n  independent normal with mean ! 0 and variance 1. As is common in gene expression studies, we will work with scaled and centered X, so reflects the sensitivity of the analysis to this standardization. The relationship between survival and X is controlled by ! 0, where the first class has 1  and the second class has 2  . The survival time for the ith patient is then T i  i expXi U i   where i $ Exponential1. The censoring time is generated from C i $ Uniform0, , where depends on the choice of and and a target censoring rate. Finally, the observed survival time is Y i  minT i , C i . We set n  200 patients in each class and set  3 so that 1  3 and 2  3. We target 40% censoring by setting  exp0:99 for  0 and  exp12:83 for  5. For this simulation, we run our algorithm at the true number of clusters K  2. We study the same scenarios over 1000 simulations. In, we report the estimated k (choosing ^ 1 ^ 2 for identifiability) alongside the oracle estimator that knows the true classes. Intuitively, if the data are perfectly classified, the oracle estimate will have properties consistent with the well-studied Cox model estimate. Thus, the accuracy, the proportion of patients assigned to their true class, is an ideal measure of loss of performance due to uncertainty. Considering the standard Cox model in this heterogeneity setting, the median parameter estimate for the  0 case is 1.8e-05 (range 0.260.28) implying that heterogeneity has masked all detectable association with the covariate of interest. The results imply that the clustering algorithm works well despite heavy censoring and mean mis-specification. We note a bias toward larger absolute parameter estimates ( CAC, 1 4 oracle, 1 ) that we believe comes from the algorithm greedily reinforcing what it has already learned. A censoring bias in the  5 scenario appears as CAC, 2 is smaller than it should be; this group is more likely to be censored so it has a lower effective sample size. Variation in per cluster censoring rates is a novel data consideration, and we recommend tracking the number of events in each cluster (see for example Section 6). When the cluster sizes are generated by P i U i $ Binomial 2n, 1=2, the estimates are similar and the standard errors increase reflecting the variation in U i (Supplementary Material). To consider the ability of CAC to identify heterogeneity, we tested the above  0 scenario with the DFBETA statistic noting that DFBETA(K  2) is greater than DFBETA(K  3) in all cases; the median DFBETA(K  2) was 60 (IQR: 92), implying the model was much more sensitive to two clusters than one, whereas the median DFBETA(K  3) was 0.91 (IQR: 0.21), implying that two clusters are sufficient. Conversely, if we generate data where all patients come from the same class, DFBETA(K  2) is larger than DFBETA(K  3) in 61.2% of cases with medians 0.83 and 0.81, respectively. This implies that, along with appropriate context and judgment, the DFBETA statistic is a useful tool for diagnosing heterogeneity.
DNA REPAIR EXPRESSION SUBGROUPS IN OVARIAN CANCERBecause of its frequency among gynecological cancers, its high lethality and poor options for treatment (), serous ovarian cancer was a pilot target for molecular characterization in TCGA (The Cancer Genome Atlas Research Network, 2011). The study collected banked surgical samples from n  503 patients with highly annotated clinical follow-up whose cancers had been surgically debulked and who had been treated with platinum-based chemotherapy (). Platinum resistance is an important concept in the treatment of ovarian cancer because these cancers respond poorly to any type of chemotherapy (). Although resistance is not an ideal predictive marker because it is defined through treatment, the development of an independently queried molecular model is precisely the promise of a large repository study like TCGA. One unaddressed complication is the expectation of genetic heterogeneity: patients with similar survival outcomes may have dissimilar molecular profiles. If this heterogeneity appears to take the form of subgroups and mixtures (as in the illustrations), we anticipate that our model and algorithm will be able to address it.Therefore, we demonstrate the use of our model to explore possibly heterogenous data by modeling a potential mechanism of platinum resistance in TCGA patients. Because recent reviews of resistance highlight the homologous repair pathway for repairing DNA damage (), we focus on modeling the function of this set of genes. The homologous repair pathway is defined by Kyoto Encyclopedia of Genes and Genomes annotation (hsa:03440) () and corresponds to 27 unique gene symbols. We fit our model for K  2,. .. , 10 using 100 random starts for each K and selecting the best fit by log likelihood. By BIC, we select K  5 clusters (Supplementary Material). Survival times are truncated at 60 months of observation to reduce the influence of 76 patients who are observed beyond the time of interest. In total, 186 of 503 (37%) patients are censored before 60 months.describes the quality of the cluster fits by the relative weight of each cluster ( P i ^ u ik ), the number of patients assigned (n) and the mean posterior probability for patients in their assigned clusters. The number of events in each cluster and the restricted mean (up to 60 months) are listed. We observe that, although they have the largest number of patients assigned, clusters 4 and 5 have the smallest mean posterior probabilities, implying that their members are less similar internally. The clustering appears to be driven by the poor prognosis patients in clusters 1, 2 and 3. Noting the presence of crossing survival functions, the five class log-rank test is significant (P  1.26e-09). With respect to low posterior probabilities, we observe that the algorithm makes intuitively reasonable use of the censored observations. We plot the maximum posterior probability for each patient by their survival time in. Because censored patients do not have definitive events, the algorithm is less certain about which cluster to assign them. Patients with the least follow-up time have maximum posterior probability close to 0.2 (i.e. 1 of 5), and as they are observed, longer the certainty of their maximum assigned cluster rises. To wit, the hardest to classify patients are the least observed. Within each cluster, we check model diagnostics and look for influential points. Noting that there was moderate evidence (P  0:059) for non-PH () when considering all the patients as a single group, after fitting our model, the within-cluster tests are all strongly insignificant. All influence statistics for all genes in each cluster are smaller than 1 standard deviation, implying no leverage points. Because the mixture allows different clusters to have different baseline hazard functions, in, we used a kernel smoothing algorithm to visualize their estimates (). We emphasize the non-proportionality of the hazards for clusters 3 and 5: patients in cluster 3 have a sudden acceleration in their hazard after 30 months, which may be consistent with the loss of effect in platinum chemotherapy. In, we have plotted the estimated coefficients for clusters 3 and 5. Keeping in mind that the linear predictor in the Cox model scales hazard relative to the cluster-specific baseline hazard, we highlight three genes. RPA4's coefficients  3 , 5   7:13,  5:90 imply that it has a strong deleterious effect in cluster 3 (exacerbating the jump in hazard), while it has a strong protective effect in cluster 5. Contrast this change with RPA3 0:96,  6:24, which only increases risk in cluster 5, and TOP3A 7:54,  0:54, which is protective in cluster 3 only. At this point in the analysis, these genes are good candidates for follow-up studies: we have identified their effect specific to a subgroup of patients. Further, the clustering model may still recover a sense of DE for survival data. Because we have learned risk classes, we may consider DE across clusters. We focus on DE across clusters 3 and 5, SHFM1 (Bonferroni adjusted t-test, P  0.011), RPA3 (P  0.011) and RAD51L3 (P  0.020), all have significant shifts in expression. Notably, RPA4 and TOP3A do not show significant DE, implying that they fit into the class of variables that only differ in regression model effects. Representative of prognostic signature development,conducted a study of selected DNA repair pathways and produced a risk signature in this dataset. To compare with their model (using 151 genes across eight pathways) and a typical Cox model approach (using the 27 gene homologous repair pathway), we stratified patients based on survival to 3.7 years] and produced receiver operating characteristic plots for all of the signatures (). While the standard Cox model underperforms at area under the curve (AUC) 0.61 (comparable with a clinically derived model reported by Kang and colleagues), the K  5 CAC model has AUC 0.73 comparable with Kang's model (AUC 0.70). The key advantage of the CAC approach is the ability to describe risk sets. By itself, the Cox model describes continuous risk and must be dichotomized post hoc for survival curve plots of high-and low-risk subsets. In, we have illustrated the typical continuous risk score plot that describes sensitivity of high-and low-risk sets to the cutpoint. The CAC model naturally separates patients into risk classes (clusters 3 and 5 are highlighted,) and these can be further described by processing their continuous risk scores. Based on our analysis, we conjecture that we are able to identify a subgroup of patients (cluster 3) who experience a significant increase in hazard around month 30. We are able to identify genes whose expression leads to increased risk specific to a subgroup or whose relationship inverts across clusters. There is a tremendous amount of untapped information remaining in the fitted model. For example, every pairwise comparison between clusters should be informative as well as their holistic
Cox-assisted clusteringinterpretation, foreshadowing the utility of this methodology for exploratory data analysis.
DISCUSSIONIn this article, we have presented a model for heterogeneity in time-to-event data. Although its actual formulation is straightforward, the treatment of unknown classification, a consideration of the implications for censoring, the effect on genomic predictors and diagnostic analysis have not been previously considered. Finally, we have presented a novel and informative analysis in Section 6, which begins to identify the set of survivalassociated and subgroup-dependent alterations in expression. Admirably, this model relaxes the whole model PH assumption to conditional PH given cluster membership. In an exploratory situation, the utility of this flexibility cannot be overstated. Both our simulated and applied analysis highlight that our understanding of censoring has been augmented and the use of information in the model is intuitively simple. In a data analytic view, informaticists are familiar with unsupervised clustering analysis and class-label supervised clustering analysis. Our algorithm may be seen as a way to use a survival time (possibly censored) to supervise the clustering of gene expression data. This clustering property is distinct from ensemble-type methods [e.g., where covariate information may be used to reweight components. As we saw with its treatment of posterior class probabilities, the mixture believes each observation comes from a single component, while averaging over weak learners or hidden layers may be relevant to a more admixed sample. With respect to developing ovarian cancer biomarkers, our data analysis has shown an example where class identification leads to risk stratification. We might further identify high-and low-risk classes within the assigned clusters as is standard practice, but this is no longer a required post-processing part of the expression analysis. The CAC algorithm has also given us its posterior weights allowing a concrete measure of uncertainty for downstream analyses. In the TCGA project, as in many other cancer genomic studies, there are issues of both high-dimensional data and variable selection. As presented, the CAC regression framework does not incorporate these, but it can be extended with additional study.
ACKNOWLEDGEMENTS
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
K.H.Eng and B.M.Hanlon at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
