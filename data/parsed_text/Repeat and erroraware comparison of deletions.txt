Motivation: The number of reported genetic variants is rapidly growing, empowered by ever faster accumulation of next-generation sequencing data. A major issue is comparability. Standards that address the combined problem of inaccurately predicted breakpoints and repeat-induced ambiguities are missing. This decisively lowers the quality of 'consensus' callsets and hampers the removal of duplicate entries in variant databases, which can have deleterious effects in downstream analyses. Results: We introduce a sound framework for comparison of deletions that captures both tool-induced inaccuracies and repeat-induced ambiguities. We present a maximum matching algorithm that outputs virtual duplicates among two sets of predictions/annotations. We demonstrate that our approach is clearly superior over ad hoc criteria, like overlap, and that it can reduce the redundancy among callsets substantially. We also identify large amounts of duplicate entries in the Database of Genomic Variants, which points out the immediate relevance of our approach. Availability and implementation: Implementation is open source and available from https://
IntroductionNext-generation sequencing (NGS) technology has led to generation of 'Big Data' also in biology. The rapid accumulation of NGS data has triggered the development of an equally overwhelming amount of tools for their exploration, including many tools for the prediction of structural variants, see the reviews by. An immediately arising, pressing concern are cross-genome and cross-project comparability of variant call sets (e.g. from The 1000 Genomes Project Consortium, 2010; The Genome of the). While resolution of related issues is instrumental for successful genetics research, there are only little statistically and algorithmically rigorous approaches addressing this. Here we suggest a formal framework for identifying duplicate deletion calls. A major source of problems about NGS data formats and tool evaluation is the repetitiveness inherent to the majority of genomes (). The resulting read mapping ambiguity can decisively hamper the exact allocation of structural variations (SVs) as well as insertions and deletions (indels) within a genome. On top of that, another major source of prediction inaccuracies are the technical and theoretical, tool-specific limitations. These are often well-known. For instance, sequencing errors, ambiguities during the alignment of reads to a reference, or point mutations close to the breakpoints might cause split-read approaches to be inaccurate by a few bases. Paired-end mapping approaches, on the other hand, infer the deletion size according to the difference of the expected fragment length and the resulting distance of the mapped reads. Since the original fragment length is only known approximately, the deletion size predictions are less accurate than for split-read aligners. In addition, the location of the deletion needs to be confined to the region between the paired ends, which introduces further inaccuracies. See for examplefor details on the different kinds of approaches; popular split-read aligners are Pindel () and Platypus (), while Breakdancer () and Clever () are paired-end mapping approaches. As we will outline in the following, the simultaneous presence of repeats and (even slightly) inaccurate breakpoint predictions makes it difficult to compare two deletion calls, as done when merging call sets created by different tools (as e.g. in the recent consensus caller by) or when searching variant databases (e.g.). Since indels and SVs play important roles in cancer and many other diseases (), and in general populations (), alleviating such issues can make a highly beneficial contribution to the field.
ExampleAs a simple example for the issues to be discussed, consider the toy reference genome ACTGCTGCA, which has CTG repeated two times. First, consider two tools one of which predicts CTG at positions 2 4 to be deleted, while the second one predicts TGC at positions 68 to be deleted. The two predictions are equivalent insofar as the resulting donor genome sequence is, in both cases, ACTGCA. Let us assume that this is the correct donor genome sequence to be predicted. The VCF format () addresses this by 'left-aligning' all predictions, that is, by reporting the leftmost deletion which is equivalent to the prediction made. Note that, after left-aligning, the calls of the two tools in our example are identical. Now consider that the second tool errs, and mistakenly predicts that only nucleotides 78 are deleted. In this case, leftaligning the call of the second tool has no effect, because the predicted deletion differs from the repeat unit. As a result, the two predictions, although virtually identical, do not only deviate by 1 base pair in length, but also by 5 base pairs in terms of their left breakpoint. In particular, the two predictions do not overlap. Overall, induced by the combination of repeat structure and a (minor) misprediction, spotting their similarity has become more difficult.
Our contributionWe suggest a formal model for the identification of similar deletions, which takes both repeat structure and prediction inaccuracies into account. Furthermore, we present an efficient algorithm based on this model that allows for (i) pairwise comparison of all elements of a set of deletions and (ii) comparison of all elements of one set of deletions to all elements of a second set. The first mode of operation facilitates the identification of (clusters of) duplicate entries in databases while the second mode allows comparing deletion calls made by different tools. In the latter case of comparing two call sets, one hopes to obtain a one-to-one mapping, that is, each deletion from one set matches exactly one deletion from the other set. In practice, however, several deletions in one set may match one or more deletions in the other, or do not match any of the other deletions at all. Also, deletions within one set may already be redundant, because they are similar or even equivalent to one another. To address this, we introduce a maximal matching based framework to distinguish all relevant cases and compute appropriate matching statistics. As experiments, we first screen the Database of Genomic Variants (DGV,) for duplicates, and reveal hundreds of clusters of similar deletions. We also run the four state-ofthe-art deletion prediction methods Breakdancer (), Clever (), Pindel () and Platypus () on an approved benchmarking dataset which reflects real human genome sequence context (). Furthermore, we demonstrate that using our comparison procedure is superior to using ad hoc criteria like 50% reciprocal overlap, which has been popular both in evaluating calls and merging independent callsets into a 'consensus' callset. The developed algorithms are implemented in the easy-to-use open source software package Repeat-and Error-Aware Detection of Duplicate Indels (READDI) to compare sets of deletions given in VCF or BED format, facilitating its use in NGS data analysis pipelines and for curating databases.
Related workOur approach on taking repeat structure into account is related to the problem of tandem repeat finding, see e.g.. We use the same machinery of Longest Common Extension (LCE) queries (), now for identifying leftmost and rightmost shifts for each deletion. The fact that deletions and insertions can be shifted without altering the resulting sequence has already been observed by, but the connection to LCEs has been missed there.conducted a study of indel equivalence classes based on the same notion of equivalence as we study here for the case of deletions, i.e. taking repeat-induced equivalence into account, but not the notion of similarity we propose for inaccurate predictions. They call an indel ambiguous if it belongs to an equivalence class consisting of more than one element. Then they analyse ambiguous indels in the Ensembl database () and reveal that the exact position of such variants can affect the functional annotation; they show examples of ambiguous indels crossing transcript boundaries, affecting start codons and splice sites, and being involved with triple-repeat diseases. The study ofemphasizes the importance of robust methods to identify potentially identical variations in databases. We complement their study by extending the model of similarity to involve inaccurate predictions. From the algorithmic perspective, we also give a faster routine to detect equivalent deletions with the connection to longest common extension queries. We also complement the experimental analysis by screening a different variant database, and by studying the hybrid effect of repeats and inaccurate predictions with several existing deletion prediction tools on realistic ground-truth datasets. Rather than comparing sets of deletions directly, there exists also a quite different approach based on alignments: Apply each set of deletions to the reference sequence and compare the created sequences using optimal pairwise alignments. The alignment score gives a ranking for deletion prediction in the case that one of the sets corresponds to the ground truth. This approach was recently extended to the diploid ground-truth setting (M kinen and Rahkola, 2013) with an O(dn) algorithm, where d is the unit cost edit distance between predicted sequence and ground-truth, and n is the maximum sequence length. In practice, this approach is not scalable to whole-genome comparisons (M kinen and Rahkola, 2013); computation took 6 to 11.5 hours, depending on prediction accuracy, only for the 63 Mbp long human chromosome 20. Even if the alignment-based approach could be sped up with more algorithm engineering, one should notice that extracting variants from the alignment result in a canonical manner is a non-trivial problem. Placing gaps within alignments has remained one of the most prevalent computational challenges in bioinformatics: See for examplefor an illustration of effects such as 'gap wander' or 'gap annihilation', all of which, of course, equally apply when aligning NGS reads. One can try to attach more biological semantics to indels by modeling them as traces of recombination events (), but the optimal solution of such recurrence takes cubic time.
MethodsWe first formalize the phenomenon that deleting two different intervals from a given reference results in the same string. In the second section, we extend this concept to an error-tolerant model of similarity. After that, we introduce an algorithm to find all pairs of similar deletions from two given sets.
Model: exact caseIn the following, let jSj be the length of a string S. We denote deletions by intervals i; j, where S n i; j denote the string that results from removing all characters from (and including) position i to (and including) j from S. As discussed in Section 1.1, deleting different segments of a given string can yield the same result. We call two such deletions equivalent.
DEFINITION 1:Deletions i; j and i 0 ; j 0  are equivalent w.r.t. a reference sequence S, written i; j , i 0 ; j 0 , if and only if S n i; j  S n i 0 ; j 0 . We further define their shift being si; j; i 0 ; j 0  : ji 0  ij. Out of a set of equivalent deletions, we are especially interested in the leftmost and rightmost representative, formally defined as follows.DEFINITION 2: (rightmost and leftmost shift): Let S be a reference string. Then the leftmost and rightmost shift of a deletion i; j are defined as: Li; j : argminf i 0 j i 0 ; j 0  , i; jg and Ri; j : argmaxf i 0 j i 0 ; j 0  , i; jg, respectively.Given two deletions d 1 and d 2 of the same length, the following statements are equivalent:The left-or rightmost shift of a deletion i; j can be computed quickly in practice by shifting the deletion one by one as long as Si  1  Sj or Si  Sj  1, respectively. They can even be computed in constant time after preprocessing the reference:The left-or rightmost shift of a deletion can be computed efficiently by preprocessing the reference sequence to allow for constant time longest common extensions. PROOF:The longest common extension LCE S i; j  l is such that Si; i  l  1  Sj; j  l  1 and Si  l 6  Sj  l. After linear time preprocessing of S, every LCE S i; j can be computed in constant time (). The rightmost shift of a deletion i; j is i  l; j  l where l  LCE S i; j  1. The leftmost shift can be computed analogously. h Due to the repetitive structure of genome sequences, large shifts are possibleeven larger than the deletions, that is, there are deletions which are equivalent but not even overlapping. When comparing two sets of deletions, e.g. in benchmarking different deletion prediction tools, or searching for deletions in a database, the phenomenon of equivalent deletions has to be kept in mind. Equivalency in the exact sense is not a matter of sequencing depth, read quality or read length, but of the repeated nature of genome sequences. Additionally, since in practice, both predictions and database entries do usually not come at base pair resolution, some errortolerance is necessary to account for equivalence.
Model: similarityEven a deviation of only a single base in deletion breakpoint predictions can implicate or prevent that deletions are shiftable, which can lead to very large differences in the resulting repeat-corrected breakpoints. To account for such breakpoint inaccuracies, we define the distance Ki; j; i 0 ; j 0  : ji  i 0 j  jj  j 0 j;which, like all following terms, takes both left and right breakpoint equally into account. Based on this, we define neighborhoods around deletions as follows.DEFINITION 3 (k-neighborhood):The k-neighborhood of a deletion d  i; j comprises all deletions with a distance of at most k:The value k is called neighborhood size. For comparing two predictions made by two different methods or called on different data, we use separate neighborhood sizes k 1 and k 2 for each of the two data sets. That means, when comparing predictions from a highly accurate method (or called from very accurate data) with less accurate ones, we can set k 1 to a smaller value than k 2. Note that the output of some deletion callers already explicitly includes tool-specific neighborhoods (a.k.a. confidence intervals), for example, in form of windows for both position and length, or both breakpoints. We will, in the following, base all our considerations on the k-neighborhood model from above. We do this for the sake of simplicity; our approach to similarity of deletions is generic in the choice of neighborhood definition, such that also other definitions can be used. We say that two deletions are similar if there are equivalent deletions within their neighborhoods. Repeat-and error-aware comparison of deletionsDeletions d 1 and d 2 are (k 1 ,k 2 )-similar, if and only if there are deletions dThis quantity is instrumental for distinguishing between two factors of similarity: The neighborhood size that is required to overcome a possible inaccuracy of deletion predictions on the one hand, and the actual shift due to a repetitive structure of the sequence on the other hand. Motivated by maximum parsimony, we minimize the inaccuracy first and the shift second, describing a scenario with as few errors as possible as well as a minimum shift. DEFINITION 5 (minimum shift):Given two (k 1 ,k 2 )-similar deletions d 1 and d 2 , their minimum shift is defined as
Matching sets of deletionsBy determining similar deletions within one dataset, we can identify duplicate calls and merge them, e.g. by selecting one representative. To compare two datasets D 1 and D 2 , we compute the similarity relations between all deletions in D 1 to all others in D 2. This can be modeled as a bipartite graph with node sets D 1 and D 2. Two nodes/ deletions d 1 2 D 1 and d 2 2 D 2 are connected by an edge (d 1 , d 2 ) if they are (k 1 , k 2 )-similar. Then, a maximum cardinality matching allows to compare or merge the two datasets: Each deletion is related to at most one in the other set. Matched deletions are in common, unmatched are unique. However, we have to keep in mind that both D 1 and D 2 might contain duplicates, i.e. we have to take (k 1 , k 1 )-similar deletions within D 1 and (k 2 , k 2 )-similar deletions within D 2 into account. These can be taken care of either prior to building the graph or in a post-processing step. To evaluate a callset P against a given 'truth' T, e.g. calls predicted by a different tool or a collection of deletions in a database, the obtained bipartite matching can easily be interpreted as follows. Matched (k p , k t )-similar deletions correspond to true positives (TPs), and unmatched deletions in P and T correspond to false positives (FPs) and false negatives (FNs), respectively. To account for duplicates within the two sets, after performing the matching, we also determine all (k p , k p )-similar deletions in P and (k t , k t )-similar deletions in T. If a deletion in P is unmatched but similar to a TP, it might be considered as being a duplicate of a TP and should thus not be counted as a FP. We report those deletions as similar positives (SPs) and do not count them as FPs. Vice versa, we report unmatched deletions in D as similar negatives (SNs) and do not count them as FNs. Further, we do not count several unmatched deletions as individual false positives (or negatives) if they are similar. Instead, we report components of (k p , k p )-similar deletions in P as false positive components (FPCs) and components of (k t , k t )-similar deletions in T as false negative components (FNCs), see. This allows us to define a duplicate-aware recall by TP TPFNC and precision by TP TPFPC .
AlgorithmIn the following, we describe an algorithm that, for two given lists of deletions, as specified by reference coordinates, determines all pairs of similar deletions. We assume the lists being sorted by deletion start coordinates. The algorithm outputs all pairs of deletions for which the deletion in the first list is left of or at the same position as the one in the second list. To obtain all pairs of similar deletions of two distinct lists, the algorithms is executed twice: to get the pairs where the deletion from the first list is left and from the second list is right and vice versa. To screen a single list for duplicates, the algorithm is called once by passing this list twice as parameter. Pseudo code is given in Algorithm 1. The for-loop in line 2 iterates over all deletions in the first list. For each considered deletion d 1 , we aim at finding similar deletions d 2 in the second list with a starting position right of or at the same position as d 1. How far a particular deletion can be shifted depends on whether its length matches a repeat unit. Therefore, we enumerate all possible lengths of deletions in the neighborhood of d 1 (line 3); for a given length ', we compute where the rightmost such deletion starts (line 4) and use an LCE query to determine how far it can be shifted to the right (line 5). In line 6, we then iterate over all potentially similar deletions d 2 in the second list, skipping those whose neighborhood does not contain deletions of length ' (lines 7 and 8). In case d 2 is so far to the right that a shiftable deletion of length ' cannot 'mediate' a similarity relationship, we can stop and do not need to consider any deletions further to the right in the second list (lines 9 and 10). For the remaining candidates d 2 , we check in lines 11 and 12 whether there is a length-' deletion in its neighborhood that can establish (k 1 , k 2 )-similarity (cf. Definition 4): Line 11 computes the leftmost length-' deletion in the neighborhood of d 2. Recall that the length-' deletion starting at start_max 1 is still in the neighborhood of d 1 and that all length-' deletions starting between start_max 1 and shifted 1 are equivalent. Therefore, testing for (k 1 , k 2 )-similarity (mediated by a length-' deletion) of d 1 and d 2 boils down to a simple comparison in line 12. The three nested loops in Algorithm 1 give rise to a worst-case runtime of OjL 1 j  jL 2 j  k 1 , where jL 1 j and jL 2 j are the lengths of the two deletion lists. In practice, however, the runtime behaves linearly in jL 1 j  jL 2 j (rather than being linear in jL 1 j  jL 2 j) since for each deletion d 1 2 L 1 , only a local neighborhood is considered for the search of candidates in L 2 : the loop in line 6 only considers deletions d 2 2 L 2 that are right of d 1 and the search is terminated in line 10 as soon as d 2 is too far right. This 'locality' of the algorithm is achieved by considering each candidate length ' separately (line 3) and was a major design goal. Since no recursion stack or complex auxiliary data structures are required, the memory requirement ismainly given by the input and outputin practice dominated by the size of the reference sequence.
Results and discussion
DatasetsVenter predictions and venter truth. We use a benchmark dataset based on Craig Venter's genome () that was described in. It consists of three human genomes that represent a mother-father-child trio and are realistic in particular in terms of true sequence context and amounts of variants across the whole variant spectrum (including also mixed events and inversions, for example). Here, we use the father genome. By Venter Truth, we refer to the set of known true differences of this genome to the reference genome. From this genome, we sample 2  100 bp reads with HiSeq error profiles to 30  coverage using SimSeq () and run Breakdancer (, version 0.7.9.1) to produce several call sets referred to as Venter Predictions. While Breakdancer and Clever are internal-segmentsize based tools that yield rather inaccurate deletion calls, Pindel and Platypus, as split-read aligners, yield calls that are often highly accurate. In our experiments, in particular the results for the Platypus calls show a very similar behaviour to those for the Pindel set. These call sets serve as examples to demonstrate the behavior of our similarity criterion on realistic datasets produced by state-of-the-art tools. Here, we do not focus on comparing the tools' performance rates. Database of Genomic Variants. The DGV () contains curated structural variations in human, detected in healthy control samples. It comprises different types of structural variations, predictions from different studies found by different techniques. We downloaded DGV variants mapped to GRCh 37 (release date 201410-16) and extracted deletion predictions (varianttype 'CNV' and variantsubtype 'deletion' or 'loss'). Throughout our experiments, we focus on deletions between 20 and 10 000 bp. Note that our tool is generic in deletion size. However, shorter deletions are still within reach of standard read mappers, and therefore less prone to prediction inaccuracies. Removal of duplicates among very long deletions is easy due to their size and because they are very few in number.
Removing duplicates in single datasetsWe determined all (k, k)-similar deletions in Venter Truth and in the DGV for k  25, and for each (k, k)-similar pair of deletions d 1 , d 2 (excluding cases where d 1  d 2 ), we computed their minimum shift s min k;k d 1 ; d 2 . In addition to about 60 s for reading the reference sequence, this took about 8 s (Venter) and 95 s (DGV) on a standard laptop without computing the minimum shift, and about 45 s (Venter) and 600 s (DGV) including the computation of the minimum shift. The peak memory usage was reached after reading the input files, where the reference sequence of about 3 GB clearly dominated all other data structures since the given 1020 000 deletions are represented by four integers each, i.e. only some kilobytes were used in total. A general observation is that a small minimum shift often implies that the k-neighborhoods of d 1 , d 2 overlap. For increasing minimum shift, however, the amount of overlapping neighborhoods necessarily decreasesfor pairs with a minimum shift larger than k, by definition, a shift is required, which translates into the fact that the k-neighborhoods are disjoint. Note that a shift might not be required even if s min k;k d 1 ; d 2  is larger than zero, because the definition of s min k;k prioritizes minimizing k over minimizing the shift. Venter truth. This dataset is based on a de novo sequence assembly using Sanger sequencing (). Therefore it is of very high quality and presumably contains nearly no duplicates due to technical errors. Duplicates reported by our algorithm are thus likely to be false positives, that is, cases where two deletions are indeed biologically different, but are falsely assumed to be the same by our method. In total, we found 25 such pairs of similar deletions involving 50 out of 10 001 deletions in the dataset (0.5%), 32 of which (0.3%) required shifting. The fraction of 0.5% can be interpreted as an estimate of the false positive rate of our method. When manually inspecting the reported pairs, however, one can hypothesize that some of them are not wrongly called by our method but indeed represent artifacts in the dataset, meaning that the false positive rate could be even lower. Some such cases are listed in Section A of the Supplement. When choosing k larger than 25, the resulting rates naturally increase. For k  60 and k  120, we obtain 190 and 464 deletions involved in similar pairs, respectively. Depending on the application, the corresponding false positive rates of 1.9 and 4.6% may still be acceptable. Database of genomic variants. Unlike the Venter dataset, the DGV contains massive amounts of virtual duplicates (15.8%): Seefor a histogram of the number of similar deletions versus the minimum shift observed. For about one third of the non-unique deletions, K min  0; for larger values of K min , about 70500 deletions have been found each. (See Supplement B for a plot of the distribution.) Since the DGV is a collection of deletions resulting from many, often large-scale studies, and entries are merged per study Repeat-and error-aware comparison of deletionsonly and not across studies, this high rate is expected and reflects concordance of the contained studies. But it is noteworthy that there is a large amount of deletion duplicates with large minimum shifts, which could not be detected without shifting the calls, as indicated by the gray part of the bars in. As shown by the striped bars, about 7.3% of those deletions that we detected being nonunique cannot be detected by the criterion used in DGV, i.e. 70% overlap. The experiment described in the previous paragraph suggests that this discrepancy is unlikely caused by false positives from our approach. To collect further evidence of a low false positive rate, we analyzed pairs of matching deletions in DGV for which one deletion originated from Venter's genome and one deletion originated from the 1000 Genomes Project and observed that the 1000 Genomes deletions that are part of such a pair are strongly enriched for events called in Caucasian individuals, which is consistent with Craig Venter's ancestry (see Section C in the Supplement for details). We are thus confident that our false positive rate is low. Based on the evidence presented in this section, we conclude that ad-hoc criteria alone are insufficient when it comes to creating consensus call sets from multiple, individual call sets.
Pairwise comparison of call setsWe evaluated our matching approach by identifying duplicates among the call sets from the four tools mentioned above (Venter Predictions) and also by identifying duplicates among the individual call sets and the true annotations.Tools versus 'Truth'. As outlined above, for each pair of sets of deletions, we consider a graph where nodes are calls and edges are drawn if two deletions are k; k T -similar, where k refers to the tool in use and k T refers to the 'Truth'. Thereby, we fix k T to 10since the 'Truth' has been determined experimentally, it may be prone to inaccuracies itself, which justifies a choice of k T larger than zero. However, we vary k, so as to explore the corresponding effects for the different tools. As shown in, the running times for determining similarity and performing the matching increase approximately linearly from about 6 s for k  0 for each dataset to, e.g. $26 s for the Clever set of size 7473 with k  100, not including $50 s for initial imports of the reference sequence. Results are shown inand Supplement D. We observe that, first, both precision and recall, and thus F-measure, approach a stable value for increasing k and, second, the speed and the limit of this trend differ between the tools. The two insert size based approaches, Clever and Breakdancer, reach their limits slower (in k) than Pindel and Platypus, which are split-read aligners, hence are more accurate. The limits, in turn, give evidence of the quality of the call sets. For further analyses of the different datasets, we determined the minimum value of k (in steps of 10) for which the F-measure exceeds 95 % of its limit. This method is also included in our software package READDI. For the Clever data, we obtained k  60 and for Breakdancer k  120. For Pindel and Platypus, the above procedure yielded k  0. To nevertheless allow for some minimum flexibility, we chose k  10 for these data sets in all experiments reported below. Choice of criteria: (k 1 , k 2 )-similarity versus overlap. We ran our matching algorithm both using similarity as edge criterion and defining similarity by 50% reciprocal overlap, which reflects a naive, but popular criterion for comparing results and merging duplicate calls ().shows Venn diagrams for Clever and Pindel predictions. The amount of duplicates identified varies drastically between the. Non-unique deletions in the DGV dataset. Minimum deletion size was 48 bases. Identical predictions and deletions larger than 10 K bases have been filtered out (118 349 deletions of 159 136 were left). Pairs of similar deletions with k  25 have been computed and these pairs involved 18 753 (15.8%) deletions, the minimum shift to any similar deletion has been recorded. Gray bars indicate those deletions that are similar with a shift required, i.e. the 25-neighborhoods of the two deletions are disjoint. The striped bars indicate those deletions which are also non-unique according to 70% overlap. (An overlap of gray and striped areas does not imply an overlap of the underlying sets of deletions.) Note the logarithmic scale of the y-axis. F-measure for callsets of different tools compared with truth (Venter dataset). Predictions have been compared with varying neighborhood sizes to the truth with a fixed neighborhood size of k T  10. Precision and recall have been determined by the matching approach described in Section 2.2 and are shown in Section D in the Supplement two different criteria, where the similarity criterion detects substantially more duplicates than the overlap criterion, 3225 common calls compared with 2193 (47.1 % more). When comparing Pindel to Breakdancer and Clever to Breakdancer, we observe an increase in the number of common calls of 49.1 % and 60.0 %, respectively (see Venn diagrams in Section E of the Supplement). As we demonstrated in Section 3.2, the false discovery rate of our similarity criterion is rather negligible, which points out that overlap leaves substantial amounts of common calls unidentified.shows precision/recall plots for evaluating the tools relative to the two criteria. The results demonstrate that the choice of criterion can distort qualitative statements about the accuracy of call sets substantially. For example, Breakdancer and Clever perform worse than Pindel under the overlap criterion, whereas they perform better than Pindel in the length range 4110 K under the similarity model. These comparisons demonstrate that a sound definition of two predictions 'being the same' is crucial for the purposes of evaluating SV discovery tools.
Model accuracyTo gather additional evidence that our model is superior to the overlap criterion, we performed the following experiment. We computed all pairs of similar deletions within and between the call sets of the three tools Breakdancer, Clever and Pindel. For each connected component in the resulting graph of deletions, we replaced all similar deletions by one representative, favoring predictions by Pindel over those by Clever over those by Breakdancer and favoring left deletions over right. In the end, 11 095 of 16 813 deletions were left in the consensus set. We repeated the above steps defining edges by 50% reciprocal overlap instead of by our model of similarity and obtained a consensus of 12 947 deletions. To assess the two consensus sets by a method independent of the two criteria, we compare them to the ground truth on single-nucleotide level. That is, for each nucleotide that is part of a deletion in each consensus set, we determine whether it is also part of a deletion in Venter Truth. On that basis, we compute recall and precision for the two consensus sets, stratified by deletion length ranges. This stratification is important as not to skew statistics towards longer deletions. Results are visualized in. Although the consensus set based on similarity was by 14.3 % smaller than the one based on overlap (11 095 versus 12 947 deletions), recall was virtually the same across all length ranges. The precision of our model was higher across all size ranges and the difference is especially large for medium size deletions (100200 bases), where similarity outperforms overlap by about 20 percentage points. We hypothesize that flexibility plays an important role especially in this range, because split read approaches lose predictive power while deletions of a few hundred bases are usually detected with some inaccuracy by paired-end mapping approaches. Further, shifts can be large in this length range in comparison to the deletion sizecovered by (k, k)-similarity but not by the overlap criterion.
ConclusionsThe above analyses show that a mistaken ansatz to duplicate identification can have negative effects. We have pointed out that repeats lead to ambiguities that have the potential to introduce errors. We have also pointed out that the quality of breakpoint predictions can vary substantially among variant discovery tools, which can equally hamper the identification of duplicates among deletion calls. In the light of increasing amounts of projects that aim at large-scale discovery of genetic variants in populations or in diseased cells, sound standardization and consistency among calls reported is an urgent and vital issue.. Effect of using our similarity criterion instead of 50% reciprocal overlap for comparing precision and recall of different tools with respect to Venter Truth (separated by deletion size). Lines connect corresponding statistics for the two criteria. Recall has been determined by comparing all predictions to only small or large true deletions, resp. Precision has been determined by comparing only small or large predictions, resp., to all true deletions. The used definition of neighborhood for the tools is given in the legend. For 'truth', k  10 has been chosenTo overcome this, we have presented a framework that accounts for repeat-aware and error-tolerant duplicate identification, hence soundly captures both error sources. We have devised a maximum matching algorithm that can make use of this framework, and which one can employ to spot duplicates among deletions. As we demonstrated, the algorithm operates at negligible false discovery rates. Thereby, it outperforms ad-hoc criteria, such as 50% reciprocal overlap substantially. As future work, we will invest in adjusting our framework, which is generic in its core ideas, to other variant types. Including insertions, although being the reversal process of deletions, is not straightforward. The complexity arises from the difference that a deletion is uniquely described by its coordinates, but in contrast, an insertion prediction also includes the inserted sequence. In particular, allowing for small errors in the predictions is not trivial. Another interesting, and potentially promising line of research is to integrate allele frequency related statistics, likely as a postprocessing step. Moreover, it may be interesting to study selecting k not only conditioned on the method in use, but also on coverage and/or on locus, and combinations thereof, in a sound machine learning framework.
5280
R.Wittler et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Repeat-and error-aware comparison of deletions
