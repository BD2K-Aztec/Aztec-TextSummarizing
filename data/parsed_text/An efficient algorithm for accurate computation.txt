The Dirichlet-multinomial (DMN) distribution is a fundamental model for multicategory count data with overdispersion. This distribution has many uses in bioinformatics including applications to metagenomics data, transctriptomics and alternative splicing. The DMN distribution reduces to the multinomial distribution when the overdispersion parameter is 0. Unfortunately, numerical computation of the DMN log-likelihood function by conventional methods results in instability in the neighborhood of Â¼ 0. An alternative formulation circumvents this instability, but it leads to long runtimes that make it impractical for large count data common in bioinformatics. We have developed a new method for computation of the DMN log-likelihood to solve the instability problem without incurring long runtimes. The new approach is composed of a novel formula and an algorithm to extend its applicability. Our numerical experiments show that this new method both improves the accuracy of log-likelihood evaluation and the run-time by several orders of magnitude, especially in high-count data situations that are common in deep sequencing data. Using real meta-genomic data, our method achieves manyfold runtime improvement. Our method increases the feasibility of using the DMN distribution to model many high-throughput problems in bioinformatics. We have included in our work an R package giving access to this method and a vingette applying this approach to metagenomic data. Availability and implementation: An implementation of the algorithm together with a vignette describing its use is available in Supplementary
INTRODUCTIONThe analysis of count data () or categorical data () is an important topic in statistics and has a wide variety of applications in bioinformatics. The advent of high-throughput sequencing technologies () provides unprecedented opportunities for investigating new and more powerful analysis methods on count data (). The Poisson distribution is a basic distribution for modeling count data. An important property of the Poisson distribution is that the mean and variance are the same, which is called equidispersion. However, the mean and the variance of real count data are often not the same; in fact, the variance is often greater than the mean. This makes the Poisson distribution not ideal for analyzing such data because the equidispersion assumption is violated. The phenomenon where a dataset exhibits greater variance than what would be expected in a statistical model is called overdispersion. A commonly used overdispersed model for the Poisson distribution is the negative-binomial distribution. This distribution has been extensively studied in Hilbe (2011) and is an indispensable model for high-throughput sequencing data (). Another fundamental model in count data analysis is the multinomial (MN) distribution, which is useful for analysis of count proportions among multiple categories. One important use case of the MN distribution is Fisher's exact test of contingency tables (), which has been used in the analysis of alternative 3 0 UTR utilization () and splicing (), as well as metagenomics (). In the regression context, MN logistic regression is also commonly used (). However, real data often exhibit heterogeneity that is usually thought to be caused by dependencies or the similarity of responses of members of the same cluster in cluster sampling (). This leads to extra-multinomial variation (), i.e. overdispersion with respect to the MN distribution. The modeling of overdispersion of the MN distribution has been addressed by extending the MN distribution to the Dirichlet-multinomial (DMN) distribution (). The beta-binomial (BB) distributiona special case of the DMN distribution with only two categorieshas been studied by many (). Because of its flexibility and its mathematical convenience, the DMN distribution is widely applied to diverse fields, such as topic modeling (), magazine exposure modeling (), word burstiness modeling (), language modeling and () multiple sequence alignment (). Bouguila (2008) also considered a generalization of the DMN distribution and applied it to count data *To whom correspondence should be addressed. clustering. Another related distribution for handling overdispersion is the Dirichlet negative MN distribution () allowing the modeling of correlated count data without an upper bound, which has many possible uses in biostatistics and bioinformatics (). Likelihood functions play a key role in statistical inference (). For example, likelihood functions can be used for parameter estimation, hypothesis testing and interval estimation. In the context of the DMN distribution, there has been recent research to investigate the Fisher information matrix () and maximum likelihood estimation (MLE) (). Not all statistical inference methods are based on likelihood functions. For instance Kim and Margolin (1992) developed a method for testing the goodness of fit of the MN distribution against the DMN distribution based on the C test statistic (), a flexible framework built on the likelihood approach that enables the analysis of complex experimental designs () that frequently appear in genomic and bioinformatics studies. In this article, we study the fundamental problem of the evaluation of the DMN log-likelihood function. In Section 2, we demonstrate the instability and runtime problems of two existing methods for computing the DMN log-likelihood function and propose a novel parameterization of the log-likelihood function to allow smooth transition from the overdispersed case (the DMN distribution) to the non-overdispersed case (the MN distribution). For this new parameterized form, in Section 3 we introduce a new formula based on a truncated series consisting of Bernoulli polynomials. In Section 4, a mesh algorithm is devised to increase the applicability of this new formula. In Section 5, we show numerical results of the mesh algorithm, confirm its stability and runtime improvements. Finally, we applied our method to human microbiome data and demonstrated its large performance improvement over the most accurate existing method.
DMN DISTRIBUTIONThe DMN distribution, a.k.a., the compound MN distribution (), is an extension of the MN distribution. The probability mass function (PMF) of the K categories MN distribution of N-independent trials is given bywhere n! denotes the factorial of a non-negative integer n; the observations x  x 1 , :::, x K , satisfying P K k1 x k  N, are non-negative integers; and p  p 1 , :::, p K , satisfying P K k1 p k  1, are the probabilities that these K categories occur. The DMN distribution can be generated if the probabilities p follow a prior distribution (of the positive parameters   1 ,. .. , K ) conjugate to the PMF f MN x; N, p ()This distribution is called the Dirichlet distribution whose normalized form iswhere x is the gamma function and A  P K i1 i. The PMF of the DMN distribution is derived by taking the integral of the product of the Dirichlet prior (2) and the MN likelihood (1) with respect to the probabilities p (),where, same as the MN distribution, x  x 1 ,. .. , x K  are non-negative integers, satisfying N  P K k1 x k. The DMN distribution reduces to the BB distribution when there are only two categories (K  2). The first term on the right side of (3) does not depend on the parameter . For common uses of the likelihood function in statistics, e.g. in the maximum-likelihood estimation, we are not interested in the first term but in the product of the remaining two terms, i.e. we are interested in the last two terms of the DMN likelihood function in (3)By taking the logarithm of both sizes of (4), we get the log-likelihood functionWhen A ! 1, it can be shown that the DMN distribution is reduced to the MN distribution. As  1=A becomes 0 under this limit, it is convenient to use the parameter instead of A. The parameter characterizes how different a DMN distribution is from the corresponding MN distribution with the same category probabilities. The greater the parameter , the greater the difference. This additional parameter gives the DMN distribution the ability to capture variation that cannot be accommodated by the MN distribution. We call the overdispersion parameter in this article, with the understanding that the greater the , the greater the variance. As an example,shows that increasing the dispersion parameter of the BB dispersion increases the variance of the count of the first category x 1. Using  1=A, (5) becomes ln Lp, ; x  ln 1=  N  ln 1= . The PMFs of a family of the BB distributions (N  10) with different dispersion parameters. The spread of the distributions increases with the dispersion parameter, whereas the mean remains constant where p . One shortcoming of (6) is that it is undefined for  0. Hence, R (R Core) functions implementing (6), such as dirmult () from dirmult () and betabin () from aod (), return NaN when  0. Another shortcoming of (6) is that as ! 0 the function implementing (6) is unstable as shown in. Alternatively, the likelihood representation used in the method in the R package VGAM (), (4) can be written aswhere  1=1  A is the overdispersion parameter defined therein, which is different from our definition of the overdispersion parameter in (6). The log-likelihood function can be written as ln Lp, ;When there is 0 overdispersion (  0), (7) reduces to the MN log-likelihood and it is numerically stable when ! 0. But the number of terms on the right side of (7) is proportional to N. When N is large, the runtime is long.
APPROXIMATION OF PAIRED LOG-GAMMA DIFFERENCEwhere B n denotes the nth Bernoulli number (B n  B n 0). Let z  1=x and a  y, the difference between the above two equations isis the old type Bernoulli polynomial (). The infinite seriesconverges absolutely when y is an integer and jxj minjy  1j, jyj51 (). Note that x  0 is a removable singularity of ln 1=x  y ln 1=x  y ln x. Using the properties of analytic functions, we have () ln 1=x  y  ln 1=x  y ln x  D 1 x, y 12Therefore, for any integer y, we can use the following approximation ln 1=x  y  ln 1=x % y ln x  D m x, y 13 when y is an integer and jxj minjy  1j, jyj51 andThe error is bounded byif jxj minjy  1j, jyj5, where is a constant51. For the application of computing the DMN log-likelihood function, we have x ! 0 and y 2 N . So we instead require xy 16 for simplicity. The error bound (15) can be arbitrarily small for arbitrarily large m without considering the numerical errors in computing the Bernoulli polynomials n y. In practice, high order polynomials are difficult to compute using floating-point arithmetic (). Because the subscript n equals the order of the Bernoulli polynomial n y, if m is too large, the error of each terms of D m x, y may actually be large, which makes D m x, y inaccurate. Hence, we do not want too many terms in (14). So we choose m  20 such that n y (n m) are still numerically accurate. We also do not need the error bound (15) to be smaller than the machine epsilon of the double precision data type (% 2:22  10 16 ). Therefore, we choose  0:2, which leads to an error bound of $1:30  10 16 , which is a little less than the machine epsilon.
THE MESH ALGORITHM FOR COMPUTING THE DMN LOG-LIKELIHOODWe apply (13) to compute the DMN log-likelihood function (6). To cope with the requirement (16), by using the idea of analytic continuation (), we introduce a mesh algorithm and allow the computation of the DMN log-likelihood using the approximation (13) in the whole parameter domain of the DMN log-likelihood function. First, we study ln Lp, ; x in (6) in detail. Let x  be the vector of the non-zero elements in x, p  be a vector of the corresponding elements in p and K  be the length of x  , then (6) becomes ln Lp, ; x   ln 1=  N  ln 1=  |fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl ffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl ffl}after taking the sum over k on both sides, we haveTherefore, theWhen some of the K  (**) terms in (17) do not meet the condition (18), we can rewrite the vector x into the sum of L terms choosing the terms to meet this conditionWe describe the choice of x l below. For convenience, we defineNote that we have the following relation between the adjacentBy taking the sum of all the elements in each vector in (22), we haveStrictly speaking, (25) is undefined for  0, but when  0, all l should be 0 s. To make (25) numerically valid for all 2 0,  1, we writeSimilarly, for p, we haveThen, using (24) and (26), (6) can be broken into a sum of L log-likelihoods, ln Lp, ; xThe sum in (29) is used in our algorithm to evaluate the loglikelihood function. We can always increase L and set x l intelligently, so that the condition (16) is satisfied for all the terms in the last formula in (29). In this case, each of the L terms in (29) can be computed using (20). This means that the log-likelihood function ln Lp, ; x can be evaluated incrementally on a mesh (). Hence, we name this method the mesh algorithm. Note that there can be many ways to generate the mesh. We describe below how the mesh is generated in our implementation. We first create an initial mesh with the following schemewhere  b c denotes the floor function. The level of mesh L is chosen so that it is the smallest integer satisfyingThis initial mesh needs to be adjusted because the end of the mesh should total to match x i exactly. To do so, let L 0 i be the smallest number satisfyingFor each i, all the remaining x l i (l4L 0 i ) are set to 0. With this adjusted mesh, we can use the approximation (20) to compute the DMN log-likelihood (29).shows an example with L  3. The 3 segments of x 2 is 0; hence, there are only two non-zero segments as shown. x 1 , x 3 and x 4 are broken into three non-zero segments. The last nonzero segments of all the four lines are adjusted so the segment sums equal x i (i  1, 2, 3, 4), respectively. Note that the time complexity of the mesh algorithm is prothe time complexity of (7) (VGAM). The difference becomes especially prominent for high count data (Figs 6 and 8).
THE NUMERICAL RESULTSWe implemented the mesh algorithm for computing the DMN log-likelihood in C. In this section, we demonstrate the accuracy and runtime of the mesh algorithm. All experiments were run on a Linux machine with a 4-core Intel Xeon CPUs E5630@ 3.53 GHz. Each log-likelihood function call is single-threaded. In contrast to,shows that the mesh algorithm is numerically stable when approaches 0.For the mesh algorithm, the evaluation is accurate and stable when the dispersion parameter approaches 0. The aod(dirmult) algorithm is unstable. The parameters are x  2, 3, 1 and p  :2, :3, :5graphical depiction of the mesh algorithm for evaluation of the log-likelihood in (29). The count in the ith category is represented by a line segment and can be partitioned into a sum of L  3 sub-counts represented by sub-segments. At points connected by the dashed lines ( l ), the DMN log-likelihood can be evaluated using (20), and there are three such evaluations in this example We compute the error of the mesh algorithm by comparing its results with the results of an implementation of (4) in Sage (), which can achieve arbitrarily high accuracy.compares the error of the mesh algorithm and the error of the method in VGAM. We can see the mesh algorithm is more accurate.shows that the runtime of the mesh algorithm increases more slowly as the counts n increase than the method in VGAM. Note that only R code is used to implement the method in VGAM, whose speed can be improved by using C. However, its runtime scalability with respect to the parameter n is intrinsic to the representation of the log-likelihood function (7) and independent of the implementation. The slower increase of runtime is especially important for the high count that is typical in contemporary high-throughput sequencing datasets.
BIOINFORMATICS APPLICATIONWe demonstrate below the application of our new method in analyzing human microbiome data from the Human Microbiome Project clinical production pilot study (The NCBI BioProject website, 2010). This dataset consists of the pyrosequencing of 16S rRNA genes in samples from four body sites, namely, saliva, throat, tongue and palatine tonsil of 24 human subjects (). The sequences obtained from the V1V3 and V3V5 variable regions of the 16 S ribosomal RNA gene are classified into the 20 most abundant taxa at the genus level and the remaining sequences are classified as the 21st taxa ().shows the taxa distribution with each sample of the saliva dataset. On real datasets, the mesh algorithm is much faster than the algorithm in VGAM. For example, the mesh algorithm improves the speed by over 50 on the saliva dataset (). Because the C-based test () rejects the hypothesis that the data from any of the four body sites are distributed according to the MN distribution (the P-values are 0 for all the four body sites), we use the DMN distribution to model each of the four datasets.shows the maximum likelihood estimates of the dispersion parameters for the data from all four body sites.
DISCUSSIONOverdispersion is important and needs to be accommodated in modeling count data. To handle overdispersion in MN data, the DMN distribution is commonly used. The numerical computation of the log-likelihood function is important for performing statistical inference using this distribution. Previous work has provided useful methods for this calculation, but the requirements of bioinformatics are difficult to satisfy. Our method solves the accuracy and runtime challenges.. The mesh algorithm is much faster than the algorithm in VGAM for the DMN log-likelihood computation. The parameters are x  n1, 2, 3, p  1=6, 1=3, 1=2 and  1=60. The computation using VGAM is only up to n  1  10 5 , as it takes too much runtime when n is beyond this point. Each boxplot represents 100 DMN log-likelihood evaluationsOverdispersion is commonly found in high-throughput sequencing data. The overdispersed Poisson model (the negative-binomial distribution) has been used to detect differential gene expression. However, the DMN distribution has seen limited use in analyzing high-throughput sequencing data, possibly because the existing methods based on the DMN distribution did not anticipate the high counts and the vast amount of such count tables extracted from the high-throughput sequencing technologies. To overcome the instability problem and the runtime problem of the existing methods for computing the log-likelihood, we derived a new approximation of the DMN log-likelihood function based on Bernoulli polynomials. Using a novel mesh algorithm, we are able to compute the log-likelihood for any parameters in the domains of the log-likelihood function. Comparing with the existing methods, the mesh algorithm is more accurate and is much faster. We demonstrate the application of the new method in analyzing human microbiome data with a large runtime improvement. This method is generally applicable to other scenarios involving proportions, such as alternative exon utilization () and alternative poly-A utilization (). For example, suppose we have 10 000 alternative splicing events that need to be tested and each test requires 1000 log-likelihood function evaluations. Our method can reduce the runtime to hours instead of potentially days. This work paves the way for application of the DMN distribution to model overdispersion in large-scale count data available in the high-throughput sequencing era.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
P.Yu and C.A.Shaw at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
High-performance DMN log-likelihood function computation at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
