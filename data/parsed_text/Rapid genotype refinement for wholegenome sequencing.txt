Motivation: Whole-genome low-coverage sequencing has been combined with linkage-disequilibrium (LD)-based genotype refinement to accurately and cost-effectively infer genotypes in large cohorts of individuals. Most genotype refinement methods are based on hidden Markov models, which are accurate but computationally expensive. We introduce an algorithm that models LD using a simple multivari-ate Gaussian distribution. The key feature of our algorithm is its speed. Results: Our method is hundreds of times faster than other methods on the same data set and its scaling behaviour is linear in the number of samples. We demonstrate the performance of the method on both low-and high-coverage samples. Availability and implementation: The source code is available at https://
IntroductionThe 1000 Genomes Project (1000GP) has pioneered the approach of combining low-coverage whole-genome sequencing (LCWGS) with linkage disequilibrium (LD)-based genotype refinement to successfully build large panels of accurately genotyped individuals (). This has provided a cost-effective alternative to sequencing many individuals at high-coverage. However, genotype refinement has a large computational burden. For example,quote around 32 compute years to perform haplotype estimation on 1092 LCWGS individuals using the 1000GP haplotype estimation pipeline. This figure measures the cost of haplotype phasing (which our method does not address) as well as genotype refinement. Given increasing sample sizes, decreasing sequencing costs and the typically super-linear scaling of refinement algorithms, we are fast approaching a point where computation will account for a substantial proportion of the cost of such analyses. Low-coverage genotyping typically proceeds by calculating genotype likelihoods (GLs) at a fixed set of variants (SNPs and small indels) from read alignments, the variant list being created at an earlier variant discovery step. These GLs reflect the likelihood of the read data conditional on each of the three possible genotypes (assuming a bi-allelic site). These uncertain GLs are then refined into genotypes by exploiting LD, the correlation between physically close variants across individuals. This final step is often referred to as genotype refinement and involves one (or more) phasing and imputation algorithms. The most accurate phasing and imputation techniques typically employ hidden Markov models (HMMs) which are computationally demanding, examples include Beagle (), Thunder () and SHAPEIT (). The final genotypes of 1000GP were created using a combination of SHAPEIT and Beagle; starting haplotypes were generated with the faster Beagle method and then were further refined using the slower, and more accurate, SHAPEIT (). A closely related problem is the imputation of variants into study samples assayed on DNA microarrays from reference panels of sequenced individuals (). Several very fast methods have recently emerged for this scenario (). These rely on theavailability of phased haplotypes for both study and reference data and it is not clear such algorithms will generalize to the LCWGS use case. An alternative to HMM-based imputation is simply to predict genotypes as linear combinations of other genotypes at physically close flanking markers, modelling the correlation between variants as a multivariate normal (MVN) distribution. This idea was first introduced by Wen and Stephens (2010), where it was used in the more traditional setting of imputing genotypes into DNA microarray samples from a reference panel. Menelaou andintroduced a related approach, MVNcall, that performs imputation on LCWGS data for which the individual has also been assayed on a DNA microarray, exploiting the 'backbone' of confident microarray genotypes to improve genotypes at non-microarray sites. We introduce a new technique based on MVN representations of LD that extends these ideas to the LCWGS-only imputation scenario. The method exploits various efficient linear algebra operations, making it hundreds of times faster than the fastest HMM method. This speed comes with a decrease in accuracy compared with HMMs, but is still substantially more accurate than genotype calls made using no LD information. In the 'Methods' section, we outline the model and its implementation. In our 'Results' section, we contrast the speed and accuracy of our technique with Beagle on 2535 samples from 1000GP Phase 3 (LCWGS) and 3781 samples taken from the UK10K project (UK10K). Finally, we demonstrate the applicability of LD-based genotype refinement in the high-coverage WGS setting, something that has not been investigated to date. The method is implemented in a software package called MarViN (MultiVariate Normal imputation) and is freely available under the GPLv3 license.
Materials and methodsWe assume that N diploid individuals have been sequenced and used to detect M bi-allelic polymorphisms. We record the number of copies of the non-reference (alternate) allele in a matrix G ij 2 f0; 1; 2g;where the indexes i and j label polymorphic sites and individuals respectively. We assume that we have been given GLs PR ij jG ij  k;where k 2 f0; 1; 2g and R ij denotes the reads aligning to site i in individual j.
Single-site modelWe now describe a simple Expectation-Maximization (EM) algorithm that we use to initialize our model. We apply Bayes' theorem to obtain posterior probabilities of genotypes:PG ij  kjR ij  / PG ij  kPR ij jG ij  k:where P(G ij  k) is the prior probability of seeing genotype k and is initialized asThis constitutes the E-step of our routine. The M-step involves reestimating our prior, P(G ij  k). First, we estimate site allele frequencies asAssuming Hardy-Weinberg equilibrium, our updated prior is thenThe E-step and M-step are iterated and generally converge rapidly.
Multi-site modelThis EM algorithm gives an estimate of G ij that takes into account the population allele frequency at site i but ignores any correlation with flanking sites (i.e. LD). We now describe how to improve the estimate of G using LD. A simple way to encode LD is with the M  M covariance matrix R, whereFollowing Wen and Stephens (2010), we make the assumption that the probability density for the vector of dosages g (j) for individual j, the jth column of the genotype matrix g j i  G ij , is MVN: Pg j  g / exp 1 2 g  l T R 1 g  lWe can then ask 'what is the distribution for the dosage at site i of individual j conditional on the dosages at all other sites?' For the MVN, a closed form expression for this conditional probability exists:where g j i' refers to all genotypes excluding site i andand the matrix ~ X i is the inverse of the matrix formed by deleting the ith row and column from R. In words, what we are doing is using the genotype matrix to estimate allele frequencies and LD. Fixing these, we re-estimate the genotype matrix using the MVN assumption. The approach is similar to our single site EM algorithm, but with the simple population frequency prior in Equation (6) replaced with the more sophisticated population LD prior in Equation (9). Examining the terms closely, we see that r i is independent of the individual, as is the quantityThus we need only calculate it once. Rewriting Equation (11), we see that updating the mean of individual j is achieved by evaluatingat a cost of one dot product per site per individual.Rapid genotype refinement for whole-genome sequencing data
Algorithm descriptionWe initialize G using the single-site model described in Section 2.1. We then repeat the following steps for a default of five iterations: 1. Calculate l and R from G. 2. Calculate t im and r i for all sites i and m. 3. For all sites i and individuals j, calculate ij. 4. Update PG ij  kjR ij  using Equations (3) and (9). 5. Recalculate G.We take the final estimate of G as our imputed genotypes. We could iterate steps 2 and 3, reusing the covariance matrix obtained at the beginning of the iteration but we found this to be unhelpful in practice.
Calculating XComputationally, step 1 is dominated by the calculation of R, which takes O(NM 2 ) operations. Step 2 requires a matrix vector product for every individual and so is also O(NM 2 ). However, a straightforward implementation of step 3 would be O(M 4 ), since a matrix must be inverted at each site at a cost of O(M 3 ) per inversion. To see how step 3 can be sped up, consider the case where we want to update the marker 1 while fixing the M  1 markers to the right. We write the covariance matrix in the following form:where R 11 is 1  1 and R 22 is M  1  M  1. To calculate r 1 , we require[compare Equation (10)]. The big overhead here is calculating R 1 22. We definewhere the blocks are sized to match the corresponding submatrices of R. By making an LDU decomposition, we can show thatwhich is known as the Schur complement of X 11 in X. This gives us ~ X 1  R 1 22 which we can use in Equation (12). Consider the variant at site i. The matrix we need to invert in order to evaluate the conditional expectation is the inverse of a submatrix of R formed by deleting the ith row and column of R. Swapping rows i and 1 and columns i and 1 of R puts the matrix we need the inverse of in the position of R 22 in Equation (14). A row and column can be swapped by pre-and post-multiplying with a permutation matrix P. R ! PRP T :Because permutation matrices are orthogonal we have thatThe required inverse for variant i can be obtained by applying Equation (17) again on the permuted matrix. In practice we just swap rows and columns of the matrix the usual way, which is equivalent to the multiplication. This trades M matrix inverses for a single matrix inverse plus M matrix operations of complexity O(M 2 ) each (matrix-vector products), giving an O(M 3 ) overall cost.
Using a reference panelIf we have a small number of individuals to impute and a reference panel formed from a large number of individuals with hard genotypes assigned, we can impute individuals using the panel by following the procedure below: 1. Calculate allele frequencies l and the covariance matrix R from the panel. 2. Use the panel allele frequencies to obtain an initial estimate of G from the GLs. 3. Calculate t im and r i for all sites i. 4. For each individual with genotype g (j) to be imputed, the following steps are performed K times: 5. For all sites i, calculate v ij 6. Update PgCalculating t im is O(M 3 ) and R is O(NM 2 ), both of which must be done once per panel. To impute each new individual then requires performing O(M 2 ) operations for each of K iterations, where K was be around 5 in practice, we found that performing more than five iterations did not improve the quality of the imputation in almost every case.
Regularizing the covariance matrixTo guard against degeneracy due to perfect correlation and force the variance to be non-zero, we performed Tikhonov regularization on the covariance matrix, i.e. applied the transformation R ! R  kI:By scanning a range of possible values of k we found k  0:06 to be an effective value for the regularization parameter, the same value as found in Menelaou and Marchini (2013). Alternative regularization methods (such as adding a matrix proportional to the diagonal of the covariance matrix, as done in the Levenberg-Marquardt algorithm) were evaluated but were not found to confer a significant improvement. After Wen and Stephens (2010), we also modify the mean as follows:This correction is relevant in the case of small cohorts where the empirical mean may be a bad estimate of the true mean, the specific form above is derived inusing the model of. In our case, with cohorts of 2500 or more, the difference between this and the sample mean is very small.
ImplementationWe implemented our method in C  using the the Eigen matrix library (Ga el Guennebaud, Beno^ t Jacob and others, Eigen v3, http://eigen.tuxfamily.org) for matrix manipulations and HTSlib () for streaming the input VCF/BCF files.
Data
Low-coverage dataWe make use of two different publicly available large cohorts to evaluate our method in the low-coverage scenario. First, the 1000GP Phase 3 samples which consist of 2535 samples from a heterogeneous mix of 26 populations, each sample sequenced to an average of 7.4. Second, data from the UK10K control group, a more homogeneous cohort than the 1000GP samples comprising 3781 samples, each sequenced to around 7. We only evaluated SNPs with minor allele count >1 in these comparisons. As validation data, we used freely available high-coverage (>80) data from Complete Genomics (CG). A subset of the 1000GP samples (287) were also sequenced by CG. To create validation data for the UK10K samples, we took 63 of the European CG samples and calculated GLs at the UK10K sites for these samples from their respective low-coverage BAM files using bcftools (). MarViN imputation was performed in 200 kbp windows with an overlap of 100kbp between windows. We performed a number of small timing experiments on a 2 Mbp region of chr20, and a more rigorous accuracy experiment using the entire chr20 for both cohorts. A summary of the samples and number of variants is in. On both these cohorts, we compared MarViN with two alternative genotype refinement schemes: Beagle 4.0 (r1399) () and the 'no-LD' method we described in Section 2.1, which does not use LD information. We chose Beagle as a comparison due its popularity, ease-of-use and relative speed compared with other HMM routines. Notably the SHAPEIT pipeline (used to produce 1000GP Phase 3 haplotypes) requires running Beagle as a first step, and hence is more accurate but slower than Beagle. Given we expect MarViN to be substantially faster, but also less accurate, than Beagle, it is reasonable to conclude that MarViN will be faster (and less accurate) than other more computationally demanding HMM based routines.
High-coverage dataWe took 50 coverage of 100 bp-paired reads sequenced from the widely studied NA12878 sample (ENA AC:ERR194147). These were aligned with BWA-MEM 0.7.12 () and small variants were called according to GATK3.3-0 best practices (), the associated GLs were supplied to MarViN. If an alternate allele for a variant in the 1000GP reference panel was not detected in a given sample then we used the GL taken from the homozygous-to-reference interval in the gvcf file that overlapped the variant site. MarViN can only improve genotyping at variants seen in the reference panel (variants with LD and frequency information). Any variant called in an individual that has been seen in a curated panel such as 1000GP is likely to be real given sufficient coverage (some amount of false discovery in 1000GP notwithstanding), since these variants have already been carefully filtered. Variants called in an individual that are not present in 1000GP require more scrutiny, although we still expect tens to hundreds of thousands of novel (mostly rare) variants in a given sample. Hence we apply the hard filters described in Li (2015) to non1000GP variants using hapdip (http://bit.ly/HapDip). For variants called by GATK that intersect with 1000GP, we are less stringent, only filtering on the genotype quality (GQ) field, the phred-scaled probability that a genotype is incorrect. The GQ field is produced both by GATK and MarViN. When setting up the reference panel, we excluded NA12878 and all other CEPH1463 pedigree members from the 1000GP Phase 3 panel so as not to bias results. We only considered bi-allelic SNPs with an alternate allele count of at least five, reasoning that very rare variants were unlikely to benefit greatly from LD-based refinement. We ran MarViN for five iterations with a window size of 210 kbp with overlap of 5 kbp at each end (so each window overlaps by 10 kbp). As truth data, we used the highly accurate NA12878 call set from Platinum Genomes v7.0.0 (http://www.illumina.com/plati numgenomes). This consists of variants and confident homozygousreference intervals generated from multiple aligners/callers on the 17-member CEPH1463 pedigree. The reliability of the variant calls is enhanced by retaining only those calls whose inheritance pattern across the pedigree is consistent with Mendelian inheritance. GATK/ MarViN callsets were compared to this truth data using hap.py (https://github.com/Illumina/hap.py), a tool which compares variants via alignment and exact matching.
Results
Low-coverage genotype refinementWe first evaluated each method's speed and accuracy as a function of sample size by sampling subsets of the UK10K cohort of sizes N  {100,, 3844} and performing genotyping on a 2 Mbp window of chromosome 20 (3537 Mbp) containing 14 416 SNPs. We measured the non-reference discordance (NRD) of each method, which is defined as (FP  FN)/(FP  TP  FN), where TP, FP and FN count the number of true positive, false positive and false negative genotypes involving an alternate allele call. The advantage of NRD over discordance is that genotypes that are homozygous-reference (in both the imputed and truth set) are ignored, these counts are typically large and represent easy genotypes to call, causing a simple discordance metric to be overly optimistic. Timings were performed on a an Intel Xeon E5-2670v2 CPU with no other compute intensive processes running. We do not report compute times for no-LD as this process is dominated by I/O operations.plots NRD (left) and compute time in hours (right) against sample size. When N  100; no-LD, MarViN and Beagle had NRD of 5.74, 5.20 and 1.26% meaning MarViN was substantially less accurate than Beagle. However, MarViNr, accuracy dramatically increases with sample size. MarViN had 0.71% NRD at N  1000 and 0.63% at N  3844 versus 0.59 and 0.38% for Beagle. Although still less accurate than Beagle, MarViN,e speed advantage widens with increasing N, it being 104 and 1445 faster than Beagle for N  1000 and 3844, respectively. Notably MarViNhad around 9-fold fewer errors than no-LD at N  3844 and required minimal compute resources (%14CPU 14 CPU minutes for N  3844). We then evaluated each method for both the 1000GP Phase 3 cohort (N  2535) and the UK10K (N  3844) for the entire chromosome 20 (1.63 and 0.49 million SNPs, respectively). To achieve a fair comparison of compute requirements, we gave each method exclusive use of a 20-core compute node (2  10-core E5-2670v2 CPUs), running Beagle with 20 threads and running 20 simultaneous MarViN processes (concatenation time is included in the results).summarizes the accuracy and compute times. MarViN was 360 faster than Beagle on UK10K and 46 faster on 1000GP. MarViN.y speed advantage on 1000GP is decreased relative to UK10K due to a much larger number of SNPs. MarViN had higher NRD than Beagle with 1.66 versus 0.90% on 1000GP and 0.64 versus 0.41% on UK10K. Although these accuracy differences may seem small, the error rates are concentrated at low-frequency genotypes. For example, NRD for UK10K on MAF <5% SNPs was 6.52, 2.66 and 1.20% for no-LD, MarViN and Beagle, a larger difference than when common variants are also considered. Nevertheless, MarViN has 4.64 and 9.82 fewer errors than the naive no-LD routine. We then investigated accuracy at different allele frequencies by binning genotypes by allele frequency and calculating Pearsonti correlation coefficient (r 2 ) between the imputed genotypes and the high-coverage validation genotypes within each bin.plots r 2 against allele frequency (log 10 scale) for 1000GP (left) and UK10K (right). We see for common variants (AF ! 2%) Beagle and MarViN are roughly equal (and substantially better than no-LD). Beagle outperforms MarViN at lower allele frequencies.andalso suggest that MarViN performs less well on heterogeneous cohorts such as 1000GP, compared with relatively homogeneous cohorts like UK10K.
High-coverage genotype refinementFigure 3 plots recall (proportion of PG SNPs detected and correctly genotyped) against precision (the proportion of called SNPs that are concordant with PG) for GATK before and after refinement with MarViN for increasingly liberal filters on the GQ field. MarViN refinement yields a modest, but consistent, improvement in SNP recall for a given level of precision (and vice versa).further breaks down these results. First, there were 243 381 SNPs called by GATK that were not in 1000GP (with minor allele count >4), these were filtered using the hard filters in hapdip. Such SNPs cannot be further refined but we report them forTime is the compute time in hours on a 20-core (2E5-2670v2 CPUs) server with 132GB RAM when using 20 threads. DIS is the percentage of discordant genotypes between the imputed genotypes and high-coverage genotypes on the CG validation samples. NRD is the discordance when not counting genotypes that were homozygous reference in both the imputed and high-coverage genotypes.Precision % Recall % GATK GATK+MarViN. Pearson.t correlation coefficient between imputed and true genotypes for different cohorts as a function of allele frequency for different data sets. Left: 1000GP Right: UK10Kcompleteness. Of these, 143 247 SNPs were validated in the Platinum Genomes dataset and a total of 2386 were classified as false positives due to having either incorrect genotypes, incorrect alleles or being called in a known homozygous-reference region. This yields a precision of 98.36%, which as one might expect, is lower than calls that intersect with 1000GP variants. For SNP calls that intersect 1000GP, we only applied a GQ
96.
DiscussionThe algorithm presented in this article is at least two orders of magnitude faster than Beagle on the UK10K cohort. Although this speed does come with a decrease in accuracy (particularly for rare variants), our method still makes nearly 10-fold fewer errors than a genotyping routine that does not take LD into account. The rapidly growing size of reference panels may soon preclude the use of super-linear complexity techniques such as Beagle, since computation will become too expensive. For example, the Haplotype Reference Consortium () has collected 32 488 LCWGS samples to create a reference panel for imputation. Extrapolating from, it seems unlikely it would be tractable to run Beagle on a cohort of this size. One possible use of MarViN would be to quickly generate an initial estimate of genotypes, which could then be supplied as starting values to a more sophisticated routine, reducing the number of iterations the latter needs to perform. MarViN might also be an ideal routine for intermediate coverage (%15) projects. The reduced accuracy of MarViN compared to Beagle at lower frequency variation is likely due to the limitations of modelling the population using one vector of allele frequencies and one covariance matrix. This simplistic model may not capture more subtle population substructure. Notably MarViN performs better on the more homogeneous UK10K cohort than on the 1000GP cohort which has far more population structure (although also has a smaller sample size). One possible way to improve this situation would be to add more flexibility to the MarViN model by using an MVN mixture distribution, but we leave this for future work. We have also demonstrated the efficacy of genotype refinement in the high-coverage scenario, the first such investigation to our knowledge. A modest gain in recall for SNPs was achieved at a cost of a negligible decrease in precision. We also attempted refining indels with this approach, gains in recall were indeed observed but were accompanied by unacceptable increases in the false-discovery rate (FDR). This may be due to a higher FDR in the 1000GP indels and could perhaps be solved via aggressive filtering. Although the improvements seen on high-coverage data are modest, we nevertheless believe it noteworthy that results achieved from high-coverage data can be improved at all by this method. Moreover the efficiency of our method means it adds little additional overhead to processing pipelines for WGS data, whereas genotype refinement using existing HMM-based methods would be a considerable computational undertaking.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
R.Arthur et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
