Motivation: Feature selection is a key concept in machine learning for microarray datasets, where features represented by probesets are typically several orders of magnitude larger than the available sample size. Computational tractability is a key challenge for feature selection algorithms in handling very high-dimensional datasets beyond a hundred thousand features, such as in datasets produced on single nucleotide polymorphism microarrays. In this article, we present a novel feature set reduction approach that enables scalable feature selection on datasets with hundreds of thousands of features and beyond. Our approach enables more efficient handling of higher resolution datasets to achieve better disease subtype classification of samples for potentially more accurate diagnosis and prognosis, which allows clinicians to make more informed decisions in regards to patient treatment options. Results: We applied our feature set reduction approach to several publicly available cancer single nucleotide polymorphism (SNP) array datasets and evaluated its performance in terms of its multiclass predictive classification accuracy over different cancer subtypes, its speedup in execution as well as its scalability with respect to sample size and array resolution. Feature Set Reduction (FSR) was able to reduce the dimensions of an SNP array dataset by more than two orders of magnitude while achieving at least equal, and in most cases superior predictive classification performance over that achieved on features selected by existing feature selection methods alone. An examination of the biological relevance of frequently selected features from FSR-reduced feature sets revealed strong enrichment in association with cancer. Availability: FSR was implemented in MATLAB R2010b and is available at
INTRODUCTIONAn open problem in the field of microarray analysis is the classification of samples in terms of their cancer subtype using * To whom correspondence should be addressed. copy number data derived from high-density single nucleotide polymorphism (SNP) arrays. Accurate classification of clinically important subtypes can help define subgroups of patients who would benefit from more individualized treatment regimes. While classification of microarray data has received considerable interest in the gene expression domain, challenges of a different nature arise when dealing with copy number in high-density SNP array datasets. SNP microarray datasets have more features than gene expression microarrays. Typically, gene expression datasets can comprise tens of thousands of features (genes), while SNP array datasets can comprise hundreds of thousands of features (SNPs). This implies that in SNP array datasets, the number of features are typically several orders of magnitude larger than the available sample size. Consequently, feature selection approaches that work well on gene expression datasets may not necessarily scale well to the increased number of features in SNP array datasets. Moreover, SNP microarray datasets possess a lower signal to noise ratio in comparison to gene expression microarrays. The raw copy number measurements can thus be extremely noisy as compared with gene expression measurements on microarray datasets. While there have been previous attempts at classifying datasets generated from SNP microarrays, these studies were largely based on the earliest generation of SNP microarrays () with 1000 features, which makes these datasets similar in dimensions to gene expression datasets. Therefore, the challenges in scalability are not as apparent in these datasets. Among these studieswas based solely on genotype data from SNP microarray datasets and not copy number data, whereas Wang (2006) was based only on loss of heterozygosity (LOH) data that is derived from genotype calls. Other earlier attempts at classifying samples based on copy number datasets were largely based on the lower resolution array Comparative Genomic Hybridization (aCGH) technology (). Again the issue of scalability was not explored since the number of features in aCGH is much smaller than those of the Affymetrix SNP array. The scalability of feature selection in SNP array datasets with large numbers of features for the purpose of sample classification is an open problem that needs to be addressed. Our aim in this article is to propose a feature reduction approach that is computationally more efficient than existing feature selection approaches () while achieving at least comparable or superior classification accuracy, when applied to copy numberPage: 152 151159
G.Wong et al.data generated from SNP microarrays to obtain more accurate classification of samples by cancer subtype. SNP microarrays have a significant amount of in-built redundancy in their probe design, and probe readings are constantly affected by experimental noise. This implies that the raw SNP microarray measurement values (log2 ratios) will contain many redundant as well as irrelevant features, which makes them suited to feature reduction. Our proposal is that by eliminating redundant and irrelevant features in a computationally efficient manner, we can effectively reduce the original feature set down to a compact set of interesting features where we can continue to apply existing feature selection approaches in a more efficient manner without sacrificing classification accuracy. The feature selection approaches that are suitable for use with our proposed Feature Set Reduction (FSR) method are largely filter-based feature selection approaches. Filterbased feature selection approaches tend to be more computationally efficient than other feature selections approaches, such as wrapperbased and embedded approaches, and also have the added advantage of having performance that is independent of the classifier used. The challenges that we have identified and addressed in designing our proposed method FSR are (i) how to reduce feature set size to enable more efficient feature selection; (ii) how to achieve at least equivalent if not superior predictive classification performance on classifiers trained on selected features, as opposed to features selected by the original feature selection algorithm on the entire feature set; and (iii) how to reduce the overall feature selection runtimes and scalability with respect to sample size and sample resolution. Accordingly, we demonstrate the effectiveness of FSR in terms of: (i) reduction in empirical execution time for selecting the top-ranked features; (ii) improvement in predictive classification accuracy when applied with existing feature selection approaches; and (iii) scalability with respect to sample size and sample resolution over stand-alone feature selection methods.
AIMIn this section, we present the aim of our approach, along with the required inputs and expected outputs, as well as the requirements of our proposed FSR algorithm. The aim of our approach, FSR, is to improve the efficiency and accuracy of feature selection for the purpose of scalable classification of high-dimensional SNP microarray samples. In particular, we focus on the application of our proposed approach to the task of classifying the cancer subtype of samples based on SNP microarray datasets. The role of FSR is to efficiently filter uninformative features prior to using a more computationally expensive feature selection algorithm. The inputs to FSR are a set of samples, each described by a vector of real-valued features giving a dataset rR MN where M is the number of features and N is the number of samples. In addition, a class label l n {1,...,C} is available for each sample n where C is the number of classes in the dataset. The corresponding output is a reduced set of discrete-valued features,  rZ M N where M << M. We require that FSR be computationally efficient, so that it can be used as a pre-filter preceding a filter-based feature selection approach. Specifically, it must be scalable to high-resolution microarray datasets with >10 5 features, typical of the current generation of SNP microarrays by Affymetrix and Illumina. It must be able to deal with datasets whose number of features, M, is several orders of magnitude larger than the number of available samples N, i.e. M >> N. In such datasets, not all combination of values are present in the available samples, and our approach needs to be able to estimate the utility of features given such a limited number of examples.
FSR ALGORITHMIn this section, we describe our FSR algorithm. We introduce two measures that we propose to gauge the relevance and discrimination strength of features, which we refer to as the distinct segment value score and modal entropy. Using these measures to assess the utility of a feature, we subsequently select a compact set of features that strongly characterize samples that belong to the same class, and help to discriminate samples belonging to different classes. In the Supplementary Material, we provide a worked example to illustrate the main steps of our algorithm. The underlying principle of the FSR approach is that a useful feature is one whose values are highly consistent among samples from the same class, while being sufficiently differentiated from the values that appear among samples from the other classes. Note that raw copy number measurements, also known as log 2 ratios, are continuous values that indicate the copy number status of a fragment of DNA. For the purposes of feature selection and classification, discretization of log 2 ratios into labels of copy number gain, loss or no change often conveys sufficient information about the copy number state of the DNA fragment in question. Consequently, the use of precise continuous measurement values is not required for classification. Since single copy changes are most common (e.g. the duplication of a single copy of a DNA segment) and changes in copy number of two or more are far less common, we choose to employ a three-level discretization to simplify the representation of copy number in SNP array datasets. The thresholds we choose have been commonly employed in previous studies (). Specifically, the selected thresholds for copy number gain and loss are  gain = 0.3 and  loss =0.3, i.e, log 2 ratios beyond 0.3 are discretized to 1, values below 0.3 are discretized to 1 and all other values to 0. We denote the discretized matrix asr{1as asr{1,0,+1} MN .
Distinct segment value scoreFor each feature, there is a need to measure the consistency of feature values in each class among the samples of that class, as well as the variability of values in samples outside of this class, to help gauge the usefulness of the feature in discriminating classes. For this purpose, we have designed a measure referred to as the distinct segment value score. We define a segment to be a set of samples that belong to the same class. We also define a distinct feature value in our context as a discretized feature value that is frequent or consistent in one class but is infrequent or inconsistent in other classes. We begin by sorting the samples in the dataset in numeric class order so that samples of the same class are adjacent to each other. Each feature can assume one of three discretized values {1, 0, +1} corresponding to loss, no change and gain, respectively. In deriving the distinct segment value score, we must first define the notions of class modes and normalized counts.
Normalized countWe define a normalized count to be the proportion of samples in a given class having a given feature value in that class. This provides an indication of how frequently a value occurs within samples of a specific class. The normalized count of feature m having value  in class segment c is defined asPage: 153 151159
FSRwhere the discretized features value {1,0,+1}, and the count is defined asMore specifically, the count of samples with value  for feature m in class segment c is defined as f (m,c,) = n1,.
..,N where ln=c
Class mode A class modeis the most frequently occurring discretized feature value in samples belonging to a specific class. The class mode for feature m in class segment c is defined as
Normalized count for class modeThe normalized count for the class mode is defined intuitively as
Computation of scoreThis score can be used to measure the consistency of the mode of a feature in the samples of a class segment. We propose the distinct segment value score as a method for rating the ability of two different values  and  for a feature m to discriminate between samples that belong to a class c and samples that belong to the other classes {1,...,C}\c. For example, consider a feature m that corresponds to the first row in. In this idealized example, all samples in the same class have the same discretized feature values. In this example, the feature values  =1 (loss) and  = 0 (no change) have the ability to discriminate samples that belong to class c 1 from samples that belong to the other classes c 2 ,c 3 and c 4. Our aim is to find a measure that can be used to identify features with these types of discriminative values. We quantify this intuition in our distinct segment value score as follows:where  m,c, corresponds to the normalized count of the value  for feature m in class segment c, while the second term is the average normalized count of value  =  for feature m in all other class segments c = c. Note that if all samples in class c have the same value  for feature f , then  m,c, = 1. Likewise, if all samples in all the other class segments c = c have the value, where a high score corresponds to a desirable feature with values  and  that differentiate samples of one class from samples of the other classes. For example,  m,1,0,1 refers to the distinct segment value score for feature m, when feature m assumes a value of 0 in class segment 1 and a value of 1 in the other class segments.
Feature pruning (Stage 1)In total, M C D(D1) combinations of the distinct segment value scores,  m,c,, need to be computed. D is the number of discretization levels and for the FSR algorithm is fixed at a constant of 3. A high  score indicates consistency and differentiability and thus is used as a measure to prune features. We define the empirical mean of all distinct segment value scores to be   , and the empirical SD of all distinct segment value scores to be  . For each feature, m, we compute C D(D1) distinct segment value scores. Thus the scores can be stored in an M by (C D(D1)) matrix, where each column represents the scores for a particular (c,, ), which we refer to as a class-versus-rest value combination, i.e. the column of distinct segment value scores when the value  occurs in a class segment c and the value  occurs in all other class segments. For an illustrative example, see Supplementary. The efficiency of the FSR algorithm can be further enhanced by parallelizing the computation of  m,c,, , since the distinctsegment value score for each class-versus-rest value combination can be computed independently of each other. Parallelization here would speed up this stage of the algorithm by a factor of C D(D1) and is an area we have identified for future development. For each class-versus-rest value combination, we consider only the indices of the top F = C D scores as explained below. For pruning purposes, we retain the original index of the feature (in the top F) if its score exceeds the threshold of   + . This is employed to ensure that overfitting or over-representation of features for any particular class-versus-rest value combination does not occur. For three-level discretization, the value of D is constant, fixed at 3, so the limit F essentially depends on the number of classes present in the dataset. The more fragmented the dataset is in terms of classes, the greater the threshold F to ensure sufficient feature representation. After considering all class-versus-rest value combinations, duplicate feature indices that were retained are merged. In the next subsection, we discuss the second feature-value pattern that is of interest to us. It occurs when a feature has unique values in exactly two classes such as the feature corresponding to the first row in. We term this type of feature-value pattern as 'two-unique', which suggests that there are exactly two class segments where the feature has a distinct unique value. Such a feature would be able to discriminate simultaneously between two classes that bear these unique values. To handle this scenario, we introduce next the concept of modal entropy.
Modal entropyFor certain features, two of the classes may each have their own unique class modes. Such features are useful in helping to discriminate between two classes simultaneously. To enable efficient identification of features that possess this property, we use the concept of modal entropy. This approach helps us to avoid exhaustive feature value comparisons, which could otherwise result in a combinatorial explosion. Modal entropy is the entropy of the class modes for each class segment of samples. We define 'two-unique' as a feature that has distinct modes in exactly two class segments as shown in. Since it is possible to compute the modal entropy for a 'two-unique' feature pattern for datasets with any number of classes, such feature value patterns can be identified efficiently through the computation of modal entropy. The modal entropy for feature m is defined as:[Page: 154 151159
G.Wong et al.Consider a four-class example with three-level feature value discretization. A 'two-unique' pattern for any feature occurs when we have either one of the following combinations of modes in each class segment: {1,0,1,1}, {1,0,1,1}, {0,1,1,1}, {1,1,0,0}, {1,1,0,0}, {0,1,1,1}. Note the combinations of modes listed are not exhaustive. The modal entropy of any combination shown or otherwise in this example is equal to 0.25log 2 0.250.25log 2 0.250.5log 2 0.5 = 1.5. This value is referred to as  two in Algorithm 1, i.e. it is the modal entropy value which corresponds to a feature that possesses the 'two unique' pattern and can be pre-computed for any number of classes. After computing the modal entropy for all features, we can filter the feature indices based on the pre-computed threshold  two .
Feature pruning (Stage 2)After the set of features displaying 'twounique' patterns has been identified, this set is further pruned to ensure that the modes in each class segment have a clear majority within that class segment. For this purpose, we introduce a measure known as the sum of normalized counts for class modes. As defined previously, the normalized counts indicate the proportion of samples in each class segment that have a particular discretized feature value. When applied to class modes, the normalized counts for class modes simply refer to the proportion of samples in each class segment that possess the most frequently occurring discretized feature value. For example, if there is a normalized count of 1,1 = 0.66 for feature 1 in class segment 1 compared to a normalized count of 2,1 = 0.5 for feature 2 in class segment 1, then feature 1 has a mode that is more consistent in the class segment and is more likely to be better at discriminating for class segment 1 than feature 2. The sum of normalized counts for class modes  m for a feature m is simply the aggregation of all normalized counts for class modes across all class segments for the 'two-unique' feature m.A higher sum indicates a more consistent 'two-unique'pattern for that feature. The maximum score attainable is equal to the number of classes in the dataset, C. A score equal to C implies that the feature has an ideal 'two-unique' pattern where all the samples in each class segment have the same value (i.e. the mode) and exactly two class segments have distinct modes. This ideal scenario is illustrated in. In reality, it is unlikely that this ideal scenario will occur. Therefore, we select a more realistic threshold of  m > 2C 3 , where C is the number of classes in the dataset, to retain the set of features that exhibit a sufficiently strong 'two-unique' pattern. This threshold ensures that the mode is sufficiently consistent within each class segment without being too stringent in pruning the features. Refer to the Supplementary Material for examples of the empirical distribution of  m in three different evaluation datasets.
Output feature setThe output of the FSR algorithm is the union set of features selected by the two filtering stages, i.e. distinct value and 'two-unique'. This feature set can then be used as the input to a subsequent feature selection algorithm for ranking. The ordered features can then be used to train a classifier for the task of classifying samples into their cancer subtype. This FSR algorithm is summarized in Algorithm 1. Note that this algorithm can also be applied to binary class datasets (C = 2) as discussed in Supplementary Section 2. The FSR algorithm reduces the feature set by identifying features that consistently exhibit either of the two patterns of discrimination discussed. The FSR algorithm does not collectively rank the two reduced sets of features. We do not feel a compelling need to further implement a collective ranking / selection stage for FSR as after the dimensions of the feature sets have been reduced substantially by at least two orders of magnitude, existing feature selection algorithms can be employed rather efficiently for the purpose of feature selection or ranking.
Algorithm 1 FSRInput: r m,n : raw copy number of feature m in sample n l 1,...,N {1,.
..,C}
EVALUATIONIn this section, we begin by describing the aims of our evaluation. Next, we discuss the choice of datasets and classifier used to evaluate the performance of FSR in our experiments, and the procedure for preprocessing the raw data files (CEL files) acquired to produce raw copy number data (log 2 ratios). We then discuss the choice of feature selection approaches selected for use with FSR, and the choice of classifier used to evaluate the predictive classification accuracy of selected features from an FSR-reduced feature set. We subsequently present our results on predictive classification accuracy over the selected datasets with and without the application of FSR, as well as the speedup in execution times for feature selection with the use of FSR, and the scalability of FSR with respect to sample size, sample resolution as well as the number features selected. We also examined the biological relevance of the top features selected by FSR-paired approaches.
Aim of evaluationThe aims of our evaluation are to assess the performance of FSR in terms of predictive classification accuracy and change in execution times when paired with existing filter-based feature Page: 155 151159 FSR selection approaches, as well as its scalability with regards to sample size, sample resolution and the number of features selected.
Datasets
Choice of datasetsThe datasets (sarcoma, lymphoma and leukemia) and their respective CEL files used in our experiments were acquired from the Gene Expression Omnibus (GEO) high throughput public data repository, which is built and maintained by the National Center for Biotechnology Information (NCBI) (). The datasets were chosen because they were publicly available, had relatively large sample sizes and provide some diversity in terms of the range of subtypes among the samples. The classification task applied to the selected datasets is that of cancer subtype discrimination, where subtype classes were previously determined in the original references as shown in Supplementary Table S1. We also obtained three independent validation datasets based on mucosa-associated lymphatic tissue (), mantle cell lymphoma () and chronic lymphocytic leukemia () to evaluate the robustness of our trained classification models to independent datasets.
PreprocessingThe log 2 ratios were processed from the SNP array CEL files using the publicly available CNAG software package (). Tumor samples were normalized from a pool of 20 unmatched references randomly selected from the HapMap project () for the specific array type. For gender-based diseases, the references selected were of the same sex. No subsequent segmentation or smoothing was applied to the generated raw copy number data (log 2 ratios). Non-autosomal probesets are excluded from the analysis for non-gender specific diseases to avoid confounding copy number differences in the sex chromosomes. A summary of the size of the complete feature set (probesets) for all datasets used can be found in Supplementary Table S1.
Feature selection approaches for couplingFilter-based approaches and wrapper-based approaches are the two main categories of feature selection approaches. Filter-based approaches evaluate the usefulness of a feature subset by examining the intrinsic characteristics of the data, based on the relation of each single feature with the class label according to a pre-defined metric. Features are ranked in terms of the metric based on some probabilistic or distance measure, with the top scoring features chosen to build the classifier. In essence, filter-based approaches perform feature selection independent of the classification method. Classifier independence suggests that it is possible that the selected features might not be optimal for the eventual classifier. However this also implies that overfitting is also less likely to occur with filterbased feature selection. Filter-based approaches are commonly used for gene selection in microarray sample classification tasks (). With wrapper-based feature selection approaches, the feature selection algorithm is coupled with the classification algorithm. The wrapper-based approach is not extensively used in microarray sample classification, primarily due to the large combination of feature subsets that needs to be explored. For this reason, we choose to focus on filter-based feature selection approaches. We have selected three approaches to be coupled with FSR for the purpose of assessing the predictive classification accuracy of a classifier built on the selected features. The feature selection approaches (mRMR, reliefF and F-test) were selected because of their contrasting optimization functions, their relative computational efficiency and on the basis that they are commonly used in the domain of gene expression microarray analysis, which is closest to our application domain of SNP microarrays since no specific approach has been developed for feature selection on copy number data produced on SNP microarrays. The minimum Redundancy Maximum Relevance (mRMR) () feature selection approach frames feature selection as a dual optimization problem, maximizing relevance with class labels and minimizing redundancy among features. mRMR handles both continuous and discretevalued features. The authors have proven that it is an optimal first-order incremental selection. ReliefF () is an approach designed to estimate the quality of attributes in problems with strong dependencies between attributes. The key idea of reliefF is to estimate the quality of attributes according to how well their values distinguish between instances that are similar to each other. The F-test is a statistical approach that performs ranking according to the F-statistic between features and class labels. The F-statistic test is a generalization of the t-statistic for two classes. Our MATLAB implementation of mRMR, reliefF and F-test was based on the libraries of LIBGS ().
Classifier selectionWe used a multiclass SVM as our classifier, as it is stable in performance with highly competitive classification accuracy over other classification and clustering approaches. The multi-class SVM used in our experiments is based on the one-versus-rest strategy () and was implemented in MATLAB as part of the LIBLINEAR package version 1.8 (). LIBLINEAR is a linear classifier package that supports data with millions of instances and features, making it highly suited to the dimensions of SNP array-based datasets.
Results
Predictive classification accuracyWe evaluate the predictive classification accuracy of our feature prefiltering approach by employing 3-fold cross-validation on an SVM trained on the prefiltered feature set. The 3-fold cross-validation performed was stratified, i.e. we ensured that the class distribution of samples in each fold was approximately equal. FSR was only applied to the folds that were selected for training the classifier, and the samples in the test fold were completely concealed from the feature reduction stage. In, we report the accuracy through 3-fold cross-validation achieved on each of the test datasets. We also report inthe corresponding execution times for the FSR-paired approaches in comparison to the stand-alone feature selection approaches. In summary, the three multiclass datasets to which we applied our FSR approach achieved predictive accuracies that were significantly higher than the majority class percentages of each respective dataset, which is our performance baseline. This supports the effectiveness in the use of SNP microarray datasets in the task of disease subtype determination in the context of the various cancers that we have considered. Generally, for all three datasets Page: 156 151159represent the 95% confidence intervals for mean accuracy. Non-overlapping confidence intervals are deemed to be statistically significant. We observed that FSRcoupled approaches are generally at least of equal performance to the corresponding stand-alone feature selection approaches and in most cases exhibit a statistically significant improvement over the stand-alone feature selection approach. The classification accuracy achieved with FSR coupling was clearly superior to the majority class percentages, which is our baseline for each dataset. the classification accuracy of the FSR-paired approaches exceeds the accuracy achieved by the respective stand-alone approaches. There were, however, three cases (Sarcoma: mRMR and reliefF, Lymphoma: reliefF) where for small numbers of features (< 100), the stand-alone approaches achieved marginally better classification accuracy than the FSR-paired approach, but eventually leveled off to a significantly lower accuracy as the number of features increase. It is also worth noting that across the three FSR-paired scenarios, there was much less variation in the maximum accuracy achieved using the different hybrid schemes (FSR-mRMR, FSR-reliefF and FSR-FTest). For example, for the sarcoma dataset, mRMR achieved a mean maximum accuracy of 0.68 (@ 300 features) while FTest achieved an accuracy of only 0.45 (@ 300 features), whereas for the paired approaches of FSR-mRMR and FSR-FTest, the respective accuracies were 0.695 and 0.68. This suggests that our FSR-paired approach achieves not only superior classification accuracy, but also more stable classification performance. We also assessed our trained classification models on three independent cancer datasets. Based on the known cancer type (sarcoma, lymphoma and leukemia) of each given sample, the appropriate classifier is used to determine the cancer subtype of the sample. Each of the three classifiers were trained with the top  most frequently selected features from 3-fold cross-validation, where  is determined as the number of features at which point classification accuracy starts to level-off (this was identified from the plots shown in). The number of selected features used for training the lymphoma cancer subtype classifier was 190 while for the leukemia subtype classifier this was 150. After training, we classified all the samples from the three independent validation datasets with the relevant classifier (as determined by its cancer type). The accuracies obtained for each of the three independent test datasets is summarized in Supplementary Table S4. For each dataset, the accuracy achieved with the FSRcoupled approach was superior to the classification achieved with the corresponding stand-alone approach. The classification accuracies obtained for each subtype dataset was comparable to the accuracy obtained previously via 3-fold cross-validation. In Section 6 in Supplementary Material, we compare the classification accuracy and execution time of using an SVM classifier based on all features (i.e. without feature selection) compared with an SVM classifier on the FSR-reduced datasets. We show that by using FSR + SVM, we can achieve a statistically significant improvement in accuracy compared with SVM alone, with an order of magnitude reduction in execution time. The impact of different discretization levels and segmentation on predictive classification accuracy was also assessed and documented in detail in Sections 7 and 8 of the Supplementary Material. In summary, our selected discretization threshold of 0.3 was robust and performed better than other thresholds, and the application of segmentation on copy number did not improve predictive classification accuracy, presumably since narrow informative regions of copy number change are lost as a result. Furthermore, in Section 10 of the Supplementary Material we show that feature reduction using other traditional approaches such as principle component analysis are less effective than FSR at achieving good classification performance on SNP datasets.
G.Wong et al.
Execution time for feature selectionWe implemented FSR on MATLAB R2010b. In, we compare the empirical runtimes for selecting the top ranked features for each dataset. All experiments were executed on a PC equipped with an Intel Core-i7 920 processor with 12 GB DDR3 RAM. Our results show that the use of FSR reduces the mean empirical runtimes for feature selection by at least an order of magnitude and extends to beyond two orders of magnitude for reliefF., we compare the size of the original feature set to the size of the FSR-reduced feature set. Essentially, a dataset of hundreds of thousands of features is typically reduced to hundreds of features. As demonstrated by the classification accuracy achieved on the pruned datasets in, we can conclude that the pruned feature set provides a compact and informative representation of the entire feature set as the features selected from the pruned feature set have yielded at least equal and, more frequently, a statistically significant Page: 157 151159 FSROverall, feature selection with our hybrid approach was at least an order of magnitude faster than stand-alone feature selection using mRMR, reliefF or F-test alone. The empirical runtimes recorded here for the FSR-paired approaches consist of the time required to reduce the original feature set with FSR as well as the time to rank the reduced feature set with each of the three selected feature selection approaches. The maximum memory used by FSR for each dataset is 190 MB for the sarcoma dataset, 122 MB for the lymphoma dataset and 332 MB for the leukemia dataset.The size of the reduced feature set depends on the level of redundancy of the features in the dataset, the variance of the values in the dataset as well as the number of classes. improvement in classification accuracy compared with their standalone counterparts.
Feature set reduction In
ScalabilityScalability is an important consideration for feature selection algorithms as the cost of higher resolution arrays falls and the use of higher resolution arrays becomes commonplace. It is also important that feature selection algorithms scale to large sample sizes as studies with several thousand samples are gradually becoming more common.
Number of features selected:for certain feature selection algorithms, selecting a different number of top ranked features does not impact runtimes since the entire set of features are already ranked. For approaches such as mRMR, this is not the case. Other approaches such as DDP () scale even more poorly with respect to the number of top ranked features selected. However, with FSR-mRMR, the complexity with respect to the number of features selected is reduced from quadratic to linear as shown in.(a) mRMR has quadratic complexity to the number of features selected. FSR-mRMR improves on this with a linear complexity to the number of features selected with a very gentle gradient. This is largely due to the greatly reduced search space with FSR pairing. Our results show that FSR reduces the feature space by more than two orders of magnitude. (b) reliefF and mRMR appear to scale linearly to sample resolution while the F-test appears to scale quadratically. All three FSR-paired combinations execute in close to constant time with respect to sample resolution (i.e. number of features). (c) reliefF has clear linear complexity to sample size (albeit a rather steep one) while mRMR and F-test are less evidently linear as well. The FSR-pairings show near constant complexity to sample size.
Sample size:for the purpose of classification performance, learning from more samples will help improve classification accuracy if the samples possess a good-quality signal. Certain feature selection algorithms are not suited to large sample sizes and hence are unable to exploit the availability of a larger training set. In, we see that the time complexity of the FSR-paired approaches are almost constant with respect to the sample size.
Page: 158 151159
G.Wong et al.
DISCUSSION
Biological relevance of features selectedA further aim was to determine if the associated genes of the most frequently selected SNPs using the FSR-coupled feature selection approaches were biologically relevant. Detailed results of our evaluation of biological relevance are included in the Supplementary Material. In summary, for the lymphoma dataset on which we based our analysis, we found strong enrichment of genes associated with cancer and more specifically with many subtypes of lymphoma. In contrast, for each set of genes associated with the most frequently selected SNPs in the stand-alone approaches, the association with cancer and lymphoma was less frequent. This provides evidence that the most frequently selected SNPs by FSR-coupled approaches are not spurious but highly involved in cancer progression. Biological relevance in the most frequently selected features could partly explain better generalization achieved with FSR-coupled approaches leading to improved classification accuracy.
Comment on utilizing FSR for the discovery of biologically relevant regionsFSR selects SNPs that are representative of copy number changes either in focal regions or longer contiguous regions without a bias for the selection of either. Selected SNPs are essentially markers for the regions. In some cases, more than one SNP will be selected from a particular region depending on the FSR computed scores. However, it is unlikely that all SNPs will be selected for any particular region. A potential advantage is that the selection of features (SNPs) distributed over multiple genomic loci is likely to be more robust in classification performance compared with an approach that utilizes all SNPs within a few regions. Many of the SNPs within each region are highly correlated and therefore will result in a highly redundant feature set with many features providing equivalent information. For a fixed number of features, this could adversely impact the generalization ability and ultimately the classification accuracy resulting from an over-representation of SNPs in particular regions and insufficient representation in other regions. Having multiple markers over multiple regions is likely to be more robust for classification purposes. FSR is intended for usage as a tool for 'compressing' the representation of a SNP dataset with minimal loss of informative SNPs, and therefore minimizing redundancy is an important consideration. From the identified markers, the user can be directed to regions of the genome where aberrations are consistent across genomes. The approach, however, does not provide the actual genomic boundaries of the aberration. For this purpose, we would suggest the use of a segmentation tool such as HaarSeg (Ben). However with segmentation, there is a bias for long contiguous regions of copy number change and narrow regions are often 'lost' through the application of a segmentation algorithm on copy number. These 'smoothed' narrow regions of copy number change could potentially be informative for subtype discrimination and therefore their loss could result in poorer classification performance.
Comment on use of prior biological knowledgeCertain feature selection approaches (), most commonly in the gene expression domain, incorporate the use of prior biological knowledge in terms of known genes associated with disease to prune the dimensions of the dataset and thus enhance feature selection efficiency and performance. Reliance on prior biological knowledge, however, would limit the discovery of new biomarkers that can potentially discriminate well between disease subtypes. Thus for this reason, we have chosen not to incorporate the use of prior biological knowledge in our FSR algorithm to ensure the discovery of all relevant markers in the dataset for cancer subtype discrimination.
Comment on copy number versus gene expression for cancer subtype profilingIn recent years, gene expression analysis has been conducted in tandem with genotyping experiments on SNP arrays to provide an integrative understanding of the underlying mechanisms of disease progression and to rationalize changes observed in gene expression. The objective of our proposed approach FSR parallels this. We are proposing the use of copy number profiles for classification of cancer subtypes to enhance the robustness of subtype determination in various cancers over the exclusive use of gene expression data. Copy number, an absolute measure of physical change in the genome, has been shown to be a stable and reliable measure for the discrimination of several cancer subtypes. In ovarian cancer (), gene expression analysis was applied to discover relevant pathways and subgroups of clinical importance. However, variability in percentage cancer epithelium and the use of diverse control tissues were cited for the lack of consensus between related ovarian cancer studies based on gene expression alone. More importantly, the authors also state that genomic alterations may provide a more robust and reliable basis for predicting the location of driver genes over the use of gene expression.demonstrated that genomic aberrations that characterize most ovarian cancers were highly comparable at a global level i.e. very similar regions of frequent copy number aberrations were identified in related studies. In the context of breast cancer (), distinct spectra of copy number aberrations were identified underlying the different subtypes of breast cancer, suggesting that breast cancer subtypes progress along distinct genetic pathways. The distinct copy number profiles observed in these subtypes provide further impetus to pursue the potential utility of DNA copy number for accurate cancer subtype profiling and classification. In view of these examples, we believe that copy number can serve as an accurate alternative basis for the classification and profiling of various cancer subtypes.
Application of FSR to next-generation high-throughput sequencing dataData from sequencing can be used to determine copy number variation over very narrow regions. Data of this nature is wellsuited for FSR reduction. The sequencing data first needs to be aligned with a reference genome. Then copy number measurements are taken as the ratio of counts between each sample and a reference.
CONCLUSIONIn this article, we have presented a novel feature set reduction method, FSR. We have demonstrated its effectiveness in pruning complete feature sets involving a variety of cancers down to compact sets of potentially interesting features, which ultimately yielded better predictive classification accuracy for an SVM trained on these features. We have shown that FSR is computationally efficient. The speedup in empirical runtimes achieved with an FSR-hybrid feature selection approach was at least an order of magnitude faster as compared with feature selection using the respective stand-alone approach. FSR-paired approaches also demonstrated significantly better classification accuracy over their stand-alone counterparts in many of the test scenarios. The performance of FSR has implications and applications in the field of cancer genomics. Cancer is often regarded as a single disease with many variations. Each subtype of cancer has a different set of risk factors, different rates of progression, different treatment options and a different prognosis. Being able to classify cancer subtypes more accurately from selected biomarkers can be useful in achieving a more accurate prognosis of the disease as well as in making more informed choices about treatment options. From a computational perspective, the ability to prune feature sets with minimal loss of information facilitates the subsequent application of a wider selection of feature selection approaches, some of which may have been intractable on the original feature set due to its size. FSR is suited for application on high-density microarray datasets to analyze diseases such as cancer that typically have a small to moderate number of classes reflecting subtypes, grades or stages of the disease. As microarray resolution grows to improve coverage of the human genome, FSR can be a useful tool in enhancing the computational efficiency and predictive accuracy of disease subtype classification tasks performed on the datasets produced on these microarrays for potentially more accurate disease prognosis and personalized treatment.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
