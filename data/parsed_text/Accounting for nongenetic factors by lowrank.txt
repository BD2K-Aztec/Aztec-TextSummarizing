Motivation: Expression quantitative trait loci (eQTL) studies investigate how gene expression levels are affected by DNA variants. A major challenge in inferring eQTL is that a number of factors, such as unobserved covariates, experimental artifacts and unknown environmental perturbations , may confound the observed expression levels. This may both mask real associations and lead to spurious association findings. Results: In this article, we introduce a LOw-Rank representation to account for confounding factors and make use of Sparse regression for eQTL mapping (LORS). We integrate the low-rank representation and sparse regression into a unified framework, in which single-nucleotide polymorphisms and gene probes can be jointly analyzed. Given the two model parameters, our formulation is a convex optimization problem. We have developed an efficient algorithm to solve this problem and its convergence is guaranteed. We demonstrate its ability to account for non-genetic effects using simulation, and then apply it to two independent real datasets. Our results indicate that LORS is an effective tool to account for non-genetic effects. First, our detected associations show higher consistency between studies than recently proposed methods. Second, we have identified some new hotspots that can not be identified without accounting for non-genetic effects.
INTRODUCTIONNowadays both gene expression levels and hundreds of thousands of single-nucleotide polymorphisms (SNPs) can be measured by high-throughput technologies. This allows us to systematically explore the relationship between gene expression levels and genotypes: whether a gene is differentially expressed with different genotypes (or alleles) at a specific locus. The loci that are associated with gene expression levels are known as 'expression quantitative trait loci' (eQTL) (). Recently, a large number of eQTLs have been found in eQTL studies (). These findings provide insights on how gene expression levels are affected by specific genetic variants (). They may further help to prioritize disease-associated loci and contribute to disease understanding (). An important issue in eQTL mapping is that a fairly large proportion of the measured gene expression variations may not be caused by genetic variants, but by some other factors, including cellular state (), environmental factors () and experimental conditions (). A typical example is the batch effect, which may arise when sub-groups of samples were processed by different laboratories, different technicians or on different days. Because these factors are unrelated to genetic variants, we call them non-genetic factors in the rest of the article. Some of the non-genetic effects can be directly measured. For example, when the batch information is available, the batch effects may be adjusted, e.g an empirical Bayes method named 'Combat' (). However, in practice, non-genetic factors may not be directly and completely observable and thus remain hidden. For example,showed that cell culture conditions have an unnegligible influence on a large number of genes. Gagnonreported that a substantial within-batch effect exists in the Microarray Quality Control study (). 'Expression heterogeneity' (EH) arises when these hidden factors are not taken into account in statistical analysis.showed that EH not only leads to the reduction of statistical power but also spurious association signals in eQTL mapping. Recently, capturing EH in gene expression studies has drawn the attention of researchers. Many methods have been proposed to infer the hidden factors by some forms of factor analysis, and adjust the inferred factors as if they were observed (). One well-known method that attempts to address these issues is the Surrogate Variable Analysis (SVA;). It performs principal component analysis while taking genotypes into consideration and uses permutation to choose the number of principal components.proposed the intersample correlation emended (ICE) eQTL mapping method, in which a linear mixed model was introduced to model the hidden factors. When modeling EH,used the covariance matrix of the gene expression data as the EH covariance matrix in their ICE model. However, this estimate is inconsistent and thus reduces the power of eQTL mapping.introduced another linear mixed model, named *To whom correspondence should be addressed. 'LMM-EH', which corrected the inconsistency of the estimated EH covariance matrix. Once the latent covariance matrix has been estimated, LMM-EH can scan every gene-SNP pair. Alternatively,jointly modeled SNPs, gene probes and hidden confounders into a Bayesian framework. Despite its greatly increased power in eQTL mapping, its heavy computational burden might limit its usage.proposed another model named 'PANAMA' and borrowed some computational techniques from Gaussian process () and further improved the performance of eQTL mapping. However, during the model optimization, PANAMA may be trapped in a local optimum because the optimization problem is not convex. In this article, we introduce an alternative formulation to address this issue. We propose a LOw-Rank representation to account for non-genetic factors and make use of Sparse regression for eQTL mapping (LORS). We integrate the low-rank representation and sparse regression into a unified framework, in which SNPs and gene probes can be jointly analyzed. Given the two regularization parameters, the optimization of the model structure is a convex problem. We have developed an efficient algorithm to solve this convex problem and its convergence is guaranteed. We demonstrate its usefulness through its applications to both synthetic data and real data.
MODELBefore introducing our formulation, we summarize the notations used in this article. We consider the following norms of a vector v 2 R n : the ',. .. , r are the singular values of W and r is the rank of W and the 'elementwise' ' 1 norm kWk 1  P ij jW ij j. Let Y be an n  q matrix corresponding to a gene expression dataset, where n is the number of samples and q is the number of genes. Let X be an n  p matrix corresponding to a SNP dataset, where p is the number of SNPs. To model the relationship between Y and X, we propose to decompose Y as:where B 2 R pq is the coefficient matrix, 1 2 R n1 is a vector whose entries are all 1, is a 1  q matrix with j , j  1,. .. , q being the j-th intercept and e 2 R nq is a Gaussian random noise term with zero mean and variance 2 , i.e. e ij $ N 0, 2 . Here we introduce L 2 R nq in our model to account for the variations caused by a few hidden factors. This model implies that gene expression levels are influenced by genetic factors, non-genetic factors and random noises. To make the decomposition (1) possible, we make the following assumptions:There are only a few hidden factors that may influence gene expression levels. Thus, L is a low-rank matrix. Here, we also implicitly assume that the hidden factors have global effects rather than local effects. The gene expression level may only be affected by a small fraction of SNPs. This implies that the coefficient matrix B should be sparse.Based on these assumptions, we propose to solve the following optimization problem: minwhere kBk 1 is the elementwise ' 1 norm defined before, r 0 and t 0 are some fixed constants. To make the minimization problem tractable, we relax the rank operator on L with the nuclear norm, which has been proven to be an effective convex surrogate of the rank operator (). Now we rewrite (2) in a Lagrange form minwhere kLk  is the nuclear norm of L, and are regularization parameters that control the sparsity of B and the rank of L, respectively. Now it is a convex optimization problem and can be solved efficiently. Missing data are commonly encountered when analyzing gene expression data. Here we extend our basic model (3) in the following to handle missing data naturally. Suppose we only observed a subset of entries in Y, indexed by :. The unobserved entries are indexed by : ?. Mathematically, we can define an orthogonal projection operator P that projects a matrix W onto the linear space of matrices supported by :: P : Wi, j  0, if i, j 2 : W ij , if i, j = 2 : & 4 and P : ? W is its complementary projection, i.e. P : W  P : ? (W)  W. Because we want to find a sparse coefficient matrix B and a low-rank matrix L based on the observed data, we propose to solve the following optimization problem:where the first term kP : Y  XB  1  Lk 2 F is the sum of squared errors on the observed entries indexed by :.
ALGORITHMTo solve the optimization problem (3) efficiently, we need the following lemma [the proof can be found in (S W  UD V T with D  diagd 1    ,. .. , d r     7 UDV T is the Singular Value Decomposition (SVD) of W, D  diagd 1 ,. .. , d r , and t   maxt, 0.We adopt an alternating strategy to solve problem (3). For fixed B and , the optimization problem becomes minBy Lemma 1, we have a closed-form solution for L: L  S Y  XB  1  9 For fixed L, the optimization problem becomes min
B,1 2 kY  XB  1  Lk 2 F  kBk 1 10It can be further decomposed into q independent Lasso problems ():where Y j , L j and B j are the j-th column of Y, L and B, respectively. The Lasso problem can be solved efficiently by the coordinate descent algorithm (). Now we have Algorithm 1:Algorithm 1 A fast algorithm to solve problem (3)Input: Y 2 R nq , X 2 R np , ,. Initialize B 0, 0.Iterate until convergence: L-step: L  S Y  XB  1. B, -step: Solve q independent Lasso problems (11) by the coordinate descent algorithm. Output: B, L, .So far we have developed the algorithm for solving problem (3). To derive an algorithm to solve optimization problem (5), we need the following lemma [its proof was given by (: LEMMA 2. Soft-impute algorithmBy Lemma 1, the optimal value Z  of the optimization problem (12) can be obtained via updating Z using Z S P W  P ? Z 13 with an arbitrary initialization.We also adopt the alternating strategy to solve (5). For fixed B and , optimization problem (5) becomes minBy Lemma 2 we haveAgain, this problem can be decomposed into q independent Lasso problems as follows:, .
.. , q 17Now we have Algorithm 2:Algorithm 2 A fast algorithm to solve problem (5)Input: Y 2 R nq , X 2 R np , ,. Initialize B 0, 0.Iterate until convergence: L-step: iteratively update L using (15). B, -step: Solve q independent Lasso problems (17) using the coordinate descent algorithm. Output: B, L, .The convergence analysis of our algorithms and the CPU timings are provided in the Supplementary Document.
PARAMETER TUNINGWe have two parameters that need to be tuned in our models. Here we propose a cross-validation-like strategy to select these two parameters. The idea is as follows: Let : be the index of the observed entries of Y. We randomly divide : into training entries : 1 and testing entries : 2 : : 1 S : 2  : and : 1 T : 2  ;. The sizes of : 1 and : 2 are roughly the same. We may solve problem (5) on a grid of (, ) values on the training data:Then we evaluate the prediction error (19) on the testing datawhere we write B, and L as B, , ,  and L,  to emphasize that B, and L depend on the parameters and. We can then choose the parameter setting   ,  , which minimizes the prediction error (19). However, searching for two parameters on a grid of values may be too computationally expensive when dealing with large datasets. Instead, we search a good value with fixing  1 and then perform a one dimensional search on a sequence of values. In our implementation, we first set the maximum rank of L, denoted as rank max L, equal to minn, q=2. Then, we start from a large max , which equals to the second largest singular value of matrix P : Y. After solving minif rankL5rank max L, we reduce by a factor  0:9 and repeatedly solve (20) until rankL ! rank max L. Using warm-start, this sequential optimization is efficient (). Then we choose a value, which minimizes the prediction error Err  1 2 kP :2 Y  Lk 2 F
21Let ^ be the value corresponding to the minimal prediction error (21). Now we can perform a one dimensional search for a good value for. We generate a sequence of values with length n equally decreasing from max to max on the log scale, where max is the smallest value such that all entries of B, ^  are zero. Typically, we set n  20 and  0:05. For each value, we solve min B, L,and evaluate the prediction error:Then we choose the value corresponding to the minimal prediction error (23). Now we can solve model (5) using ( ^ , ^ ) as regularization parameters, and obtain a sparse matrix B ^ , ^  and a low rank matrix L ^ , ^ .
DISCUSSION
Relationship between our method and other methodsTo our knowledge, LMM-EH () proposed the first framework, where multiple gene expression levels and confounder effects can be jointly analyzed in eQTL studies. For the j-th gene expression level in the LMM-EH model, it assumes the following structure:where Y 2 R nq , X 2 R np are the expression and SNP data matrices, respectively. Here e j denotes Gaussian noise, i.e. e j $ N 0, 2 e I, and u j denotes a random effect, i.e. u j $ N 0, AE, where is a scalar and AE 2 R nn. Assuming the independence among Y j , j  1,. .. , q, and integrate out u j and e j , we arrive at the following form:LMM-EH adopts the following strategy to estimate the covariance matrix AE and other model parameters   fB, , e g: First, it estimates AE from the null model, which does not include any SNPs, denoted as b AE (). Second, using b AE in model (24) as a known covariance and estimate   fB, , e g for all gene-SNP pairs (one gene versus one SNPs at a time). PANAMA extends LMM-EH and allows joint analysis of all SNPs (). Specifically, PANAMA models the relationship between gene expression levels and SNPs as follows:here is the intercept, B and W are the corresponding coefficients representing the effects of SNPs and hidden factors H. PANAMA assigns independent Gaussian priors for B and W:where K is the number of hidden factors. Assuming Y j , j  1, .. . , q are independent and integrating out B and W, the model becomeswhere the intercept term is dropped for notation convenience and   ff 2 i g, f 2 k g, 2 e g. In principle, parameter estimation in (28) can be done by borrowing some computational tricks from Gaussian process model optimization (). Computation becomes prohibitive when all genome-wide SNPs are included. In this case, PANAMA adopts a heuristic strategy: PANAMA begins with the null model (i.e. the model does not include SNPs). It first uses principal components to initialize H and gradually adds significantly associated SNPs into model (28), and re-estimate model parameter  and H. This process iterates until no significantly associated SNPs are added into the model. In summary, H and  are jointly optimized during the iterations. Our model (1) can be regarded as an equivalent form of (26) because a low rank matrix can always be written as L  HW with H 2 R nr , W 2 R rq , where r is the rank of L. Unlike PANAMA, both our formulations (3) and (5) are joint convex w.r.t (B, , L). When the tuning parameters ( and ) are given, our algorithms are guaranteed to converge to the optimal solution without any heuristic. Furthermore, we do not assume that Y i , i  1,. .. , q are independent. This can be seen from Lemma 1 and Lemma 2: information among multiple gene expression is used jointly by singular value decomposition. Compared with PANAMA, the proposed method LORS has its disadvantages. Using PANAMA's formulation, statistical significance of the associations can be evaluated. Currently, we can not provide a rigorous statistical significance test of the estimated coefficient matrix B. The difficulty comes from the unknown statistical property of the nuclear norm. How to do statistical tests with the nuclear norm regularization needs to be investigated in the future. In this article, we use permutation to obtain a rough estimate of false discovery rate (FDR) for our method.
A screening method based on LORSAlthough optimization of our LORS model (3) is a convex problem, it is still too computationally intensive to directly use it for analyzing human-size datasets (e.g. the number of genes q % 20 000, the number of SNPs p % 500 000). One can see the computational bottleneck in the (B, ) step of Algorithm 1 and 2. In this step, q Lasso problems need to be solved, each of which involves p variables. To overcome this computational difficulty, we propose to solve the following optimization problem:where Y 2 R nq is the entire data matrix of gene expression, X j 2 R n1 is the j-th SNP, j 2 R 1q is the coefficient of the j-th SNP corresponding to its effect size on q genes. Here we consider one SNP at a time, and thus, we do not add L 1 regularization. Clearly, it can be considered as the single-variable version of LORS. Thus, we call this screening method as 'LORS-Screening'. The algorithm to solve (29) is given in the Supplementary Documents. The computational time of LORS-Screening is given in the Supplementary Material. For large datasets (e.g. human datasets), we recommend to use LORS-Screening to reduce the number of SNPs. After the screening process, we may select top d SNPs for each gene (based on the absolute value of the coefficients). Then we can fit the LORS model using the selected SNPs. This strategy is similar to the single-variable screening step followed by joint analysis in linear regression (). According to the property of L 1 regularization, LORS can identify at most n non-zero coefficients for each gene. Here we may set d  n.
RESULTS
Synthetic dataTo avoid the simulation setup favoring our own model, we use LMM-EH model (24). Specifically, we generate genetic effects, non-genetic effects and noises as follows:Genetic effects: each SNP is generated independently and the minor allele frequencies of these SNPs are uniformly distributed in the interval (0.1, 0.4). The coefficient matrix B is a sparse matrix with 1% non-zero entries. These nonzero coefficients are generated using standard Gaussian distribution. Let G denote the genetic effect G  XB. Non-genetic effects: The covariance matrix AE is generated by HH T , where H 2 R nK and H i, j $ N 0, 1. Here K is the number of hidden factors. The random effect u j is drawn from N 0, AE. Let u  u 1 ,. .. , u q .Now we haveIn the following simulation studies (Sections 6.2 and 6.3), we set n  100, p  100 and q  200. To evaluate the performance under different signal-to-noise ratios, we define SNR 1 and SNR 2 as:Parameters and e can be used to control SNR 1 and SNR 2. An example of synthesized datasets is given in the Supplementary Document.
Influence of parameter tuningBefore we compare LORS with some other methods, we would like to empirically evaluate our parameter tuning procedure. Given a dataset, we need to randomly partition the observed entries into two parts: : 1 and : 2. Basically, we train our model based on : 1 for different parameters and choose a good parameter configuration such that the trained model has an accurate prediction on : 2. There may be two concerns: (i) Because the random partition may introduce randomness in our modeling process, does this strategy provide a stable parameter selection? (ii) Can this strategy adapt to different noise level? To answer these questions, we do 100 random partitions of a synthetic dataset, and run our method based on each partition separately. The distribution of the selected parameters is shown in. First, one can see that the selected parameters ,  do not change a lot during 100 random partitions. The stability of our method should be attributed to the continuity property of the ' 1 norm (), that is, a small change of dataset will not cause a big change of the optimization solution. Second, when the signal becomes weaker, i.e. SNR 1  1 (the left panel of) reduces to SNR 1  1=2 (the right panel of), a larger will be selected to prevent the noise from entering the model. This shows that our parameter tuning strategy can adapt to different noise levels. In Section 3.2 of the Supplementary Document, we provide more evidence to show that the random partition in our parameter tuning has little effects on eQTL mapping (i.e. the estimation of matrix B).
Performance evaluationWe will mainly compare our method LORS with PANAMA. The reasons are: (i) PANAMA can be regarded as an extension of LMM-EH as we discussed above. (ii)showed that PANAMA significantly outperforms other related methods, including SVA, PEER and ICE. Here we include the results from standard linear regression as reference, and compare LORS, LORS-Screening and PANAMA with the standard linear regression. To compare our method with PANAMA under different settings, we vary SNR 1 , SNR 2 and K. For each setting, we report the averaged result from 50 realizations.shows the comparison results for different combinations of SNR 1 ,. The distribution of selected parameters (100 random partitions of training and testing data) for synthetic datasets. Left panel: the synthetic dataset is generated with n  100, p  100, q  200, SNR 1  1 and SNR 2  1=3. For the parameter , 27 and 28 are selected in the sequence of values in most cases; for parameter , 13 is selected in most cases. Right panel: the synthetic dataset is generated with n  100, p  100, q  200, SNR 1  1=2 and SNR 2  1=3. For , 20 and 21 are often selected; for , 12 is selected in most cases SNR 2 and K (more simulation results can be found in the Supplementary Document). From these simulation results, we can see the following:When the number of hidden factors increases, the performance of both LORS and PANAMA degrades slightly. When the genetic effects and non-genetic effects are small (compared with noises), e.g. SNR 1  1=2, SNR 2  1), LORS is only comparable with standard linear regression and PANAMA is even worse. This is because the noise plays a dominant role here, it is difficult to account for non-genetic effects under this situation. As the genetic effects and non-genetic effects become more apparent, both LORS and PANAMA perform better than standard linear regression. As we mentioned in Section 5.1, LORS and PANAMA share the same model structure, and they differ in how the model structure is inferred. We suspect that PANAMA may be trapped at a local optimum during its model optimization. As a result, LORS may have better performance than PANAMA. Regarding to LORS-Screening, it turns out that LORSScreening is slightly worse than LORS but comparable with PANAMA. Because the computational cost is largely reduced, it is preferred in large data analysis.
Estimate of false discovery rateOwing to lack of statistical tests, it is necessary to provide a way to estimate the FDR of our method. Because correlation exists among the rows and columns of Y, exactly estimating FDR becomes difficult. Here we follow the strategy of () to obtain a rough estimator of the true FDR, which may serve as a guideline when applying our method. We use d FDR  N A 32 as a rough estimator for FDR, where N is the number of associations identified at threshold under the null distribution, A is the number of associations identified at threshold in the original dataset. We use permutation to obtain the number of associations identified under the null distribution. Specifically, for a given threshold , we do T permutations. At the t-th permutation, we permute the rows of the expression dataset Y to generate a null dataset, denoted as e Y t. Then, we run LORS on the permuted dataset ( e Y t , X) and obtain the number of associations by applying the threshold to the estimated matrix b B, denoted as e A t . After T permutations, the final estimation of d FDR is given byIn each panel, we vary SNR 1  f1=2, 1, 2g, SNR 2  f1, 1=2, 1=3g to compare the performance of LORS, PANAMA and standard linear regressionWe report estimated FDR based on the simulated data as described in Section 6.1 inof the Supplementary Document. The permutation strategy provides a reasonably good estimate for the true FDR. The estimated FDR often overestimates the true FDR (due to the correlation), and thus, it may serve as a conservative guideline.
Application to eQTL data from yeastWe applied our method to two yeast datasets for eQTL mapping. The first dataset is from(GEO accession number GSE 1990), which consists of 7084 probes and 2956 genotyped loci in 112 segregates. The second one comes from, which includes 5493 probes measured in 109 segregates. Analysis of these two datasets provides us an opportunity to demonstrate the benefit of our method because the two expression data share the same genetic effect but different confounding effects. The significant linkage peaks given by standard linear regression are shown in(A). We can clearly see the confounding effects that lead to spurious associations. We also show the result given by LORS in(B). In total, LORS has detected about 10 000 associations according to non-zero B values. Because LORS does not perform statistical significance tests, we are not able to report our result based on statistical significance. In practice, people may be more interested in the top signals that will be followed up for replications. Thus, we only show the top 1000 associations based on the absolute value of B. The plots of all associations are also given in the Supplementary Document for completeness. It can be seen that the confounding effects are successfully accounted by LORS, and thus spurious associations are greatly reduced. To quantitatively evaluate the ability of accounting for confounding effects, we compare the reproducibility of the results given by LORS and PANAMA. We examine the reproducibility based on the following two criteria:The consistency of detected SNP-gene associations. Let S 1 and S 2 be the sets of SNP-gene associations detected in the two yeast datasets, respectively. The most T significant associations from the two datasets are denoted as S T 1 and S T 2 .The consistency is defined asT , where jS T 1 \ S T 2 j denotes the size of S T 1 \ S T 2. For LORS, the ranking is based on the absolute value of B. For PANAMA, the ranking is based on the q-value.The consistency of detected hotspots. For a SNP, we can count the number of associated genes from the detected SNP-gene associations (for LORs, all SNPGene pairs with a non-zero B are defined as associations. For PANAMA, SNPGene pairs with a q-value 50.001 are defined as associations, we tried different cutoffs from 0.01 to 0.001, the results are similar), i.e. the regulatory degree of the SNP. SNPs with large degrees are often referred to as hotspots. According to SNPs' regulatory degrees, we sort them in a descending order and denote the sorted SNPs lists as L 1 and L 2 for the two yeast datasets. Let L T 1 and L T 2 be the top T SNPs in the sorted SNP lists, respectively. The consistency of detected hotspots is defined asFor Brem's dataset (), the estimated sparse matrix B given by LORS has about 6000 non-zero entries in total. Among them, there are 4500 entries with abs B40:01 and 2500 entries with abs B40:05, respectively. For Simth's dataset (), the estimated B has about 10 000 non-zero entries in total. There are about 4500 entries with abs B40:05. (To provide a meaningful guideline of the thresholds, we estimate FDR using 50 permutations. The estimated FDR corresponding to different thresholds are provided in the Supplementary Document. It tells us that FDR 0.01 when threshold ! 0.01). In, we show the consistency of the top 4500 associations. The consistencies of hotspots are shown in. It seems to be counter-intuitive that the fraction of consistency of PANAMA increases as the number of detected association increases. In fact, the consistency of PANAMA increases to 0.12 and then drops. We provide the detailed information in the Supplementary Document. From Figures 4 and 5, it can be seen that LORS achieves better consistency than PANAMA. So far we have shown that spurious associations can be reduced by successfully accounting for non-genetic effects. Now we are going to show whether it could help to detect more biologically relevant associations. We take a closer inspection of the top 15 hotspots, as listed in. In most cases (12/15), associated genes are enriched with at least one GO category, which implies that they are biologically relevant findings. In particular, we detect two novel hotspots (NO. 9 and NO. 13), which can not be detected by standard linear regression (adjusted P 40:1). For these two hotspots, the associated genes areenriched in GO categories. In detail, for hotspot NO. 9, five of the 18 associated genes are functional in response to toxin, they are AAD4, YDL218W, YLL056C, AAD6 and SPS100. The hotspot eQTL is cis-linked to one of them, AAD4, which apparently explains the detected association. Hotspot NO. 13 locates at transcription factor (TF) CAT8. Based on the transcriptional regulation information in yeast from both direct (Chip-chip) or indirect (Microarrayswild type versus TF mutant) evidence (), 9 (they are ADY2, PUT4, GAP1, ATO3, ALP1, YDR222w, CWP1, ADH2 and LPX1) of the 15 associated genes can be potentially regulated by CAT8 (adjusted P 510 10 , the details of the p-value calculation is given in the Supplementary Document). Interestingly, five genes (i.e. ADY2, PUT4, GAP1, ATO3, ALP1) are functional in organic acid transport and CAT8 is known to regulate acid transport pathway (). Identification of this hotspot provides a positive example and indicates that, when non-genetic effect has been successfully accounted for, we may be able to detect more biologically relevant trans eQTL.
CONCLUSIONSIn this article, we have introduced a method named 'LORS' to account for non-genetic effects in eQTL mapping. LORS provided a unified framework in which all SNPs and all gene probes can be jointly analyzed. The formulation of LORS is a convex optimization problem and thus its global optimum can be achieved. We also developed an efficient algorithm to solve this problem and guaranteed its convergence. We demonstrated its performance using synthetic datasets and real datasets. A limitation of LORS is that we do not provide a rigorous statistical significance test of the estimated coefficient matrix B. Here we simply rank associations based on the estimated sparse matrix abs(B) and estimate FDR.
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
C.Yang et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
