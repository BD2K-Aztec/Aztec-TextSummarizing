Motivation: BigWig, a format to represent read density data, is one of the most popular data types. They can represent the peak intensity in ChIP-seq, the transcript expression in RNA-seq, the copy number variation in whole genome sequencing, etc. UCSC Encode project uses the bigWig format heavily for storage and visualization. Of 5.2 TB Encode hg19 database, 1.6 TB (31% of the total space) is used to store bigWig files. BigWig format not only saves a lot of space but also supports fast queries that are crucial for interactive analysis and browsing. In our benchmark, bigWig often has similar size to the gzipped raw data, while is still able to support $5000 random queries per second. Results: Although bigWig is good enough at the moment, both storage space and query time are expected to become limited when sequencing gets cheaper. This article describes a new method to store density data named CWig. The format uses on average one-third of the size of existing bigWig files and improves random query speed up to 100 times. Availability and implementation: http://
INTRODUCTIONAs the next-generation sequencing (NGS) cost reduces, huge amount of reads can be generated nowadays. After aligning the reads on a reference genome, we can generate the read density, i.e. the number of NGS reads covering each base in the genome. Density data are useful because it can be used to represent the transcript expression in RNA-seq (), the peak intensity in ChIP-seq (), the copy number variation in whole genome sequencing (), etc. For example,shows plots of density signals of a ChIP-seq region and a RNA-seq region, respectively. Currently, read density is often represented using the wiggle (wig) format, the bedGraph format or the bigWig format. They all store the densities of NGS reads along the whole reference genome. Wig and bedGraph are uncompressed text formats, thus, are usually huge. BigWig () is the compressed form of wig and bedGraph. Its compression approach is to sort and partition the density data into blocks and compress them by gzip. BigWig also supports a few types of queries over any selected region: coverage, max, min, average and standard deviation. These queries facilitate efficient downstream analysis and enable fast visualization of the data. With bigWig format, UCSC genome browser () can support interactive browsing of density data. In fact, bigWig is one of the most popular track types. In the hg19 browser, $4400 tracks (10% of all hg19 tracks) are bigWig tracks, and they use 1.6TB (it is equivalent to 31% of the total space for all UCSC hg19 tracks). To reduce space and improve query speed, the resolution of the density signals of some UCSC tracks has been reduced, which affects the accuracy. In the future, it is important to reduce the storage space of density data and improve their query speed while maintaining the accuracy of the data. Our project aims to develop an alternative storage format for density signal. Our design is based on careful observations of the data and knowledge of succinct and compressed data structures. For example, we observed that mapping locations of NGS are usually overlapped. Regions with non-zero intensity are often clustered. This fact enables us to reduce the space. Another observation is that the density values of adjacent regions are not independent. Storing the differences between adjacent density values can reduce the size of 80% of the datasets in UCSC hg19. To enable fast queries, we use data structures like SDArray () that can compress data while still allowing random access. We also adopt a modified Cartesian tree () that uses linear number of bits and provides constant time min/max query. Similar to UCSC bigWig tool, cWig tool also implements the remote file access feature. In this feature, the program and the data file can be placed in different computers. The program can answer queries by accessing the data file through the HTTP/ HTTPS network protocol. In our experiment using all UCSC hg19 database, the cWig format uses on average one-third of the size of existing bigWig files, and uses much lower space in high resolution data files. In addition, it also improves query speed by 10100 times depending on the query types.
BACKGROUNDSUCSC database stores and displays many types of genomerelated data. They can generally be divided into three groups of formats. Sequence formats: store raw DNA sequences and quality scores. Examples include SAM/BAM (), FASTQ () formats. Annotation formats: store information about some biological features (e.g. genes, variants) *To whom correspondence should be addressed. located in a genome. Some popular formats in UCSC are Bed, BigBed () and VCF (). Some annotation formats are designed to keep different types of features, for example, () and (). Signal formats: store continuous numerical signal values for each genome bases. Examples include Wiggle, BedGraph and bigWig formats. The sequence and annotation files can be big, but they only require simple queries, i.e. list or count all sequences/annotations in a given region. This query can be solved by adding some index pointers on top of the existing formats. The signal files are structurally simple; however, it requires fast summary operations over some long regions. This article focuses on improving the existing signal formats. The raw density dataset is usually big (measured in Giga bytes per file) and contains a lot of duplicated information. To reduce size, bigWig applies the following compression scheme. It keeps a set of non-overlapping intervals such that the bases in each interval share the same signal value. Intervals with zero intensity or missing values are usually omitted. All intervals are sorted by their starting positions and they are partitioned into blocks of 512 by default. Each block of intervals and their corresponding signal values are compressed using the gzip algorithm in zlib library. To allow partial random access, bigWig stores the starting locations of all blocks using an R-tree-based index (), which is commonly used for geographical data. In addition to the original data, bigWig also stores extra tables to provide fast computation of four summary operations over any query interval. These operations are mean, min/max, coverage and standard deviation. They are crucial for UCSC genome browser visualization function.Before we formally define the four operations, we need some additional notations. Let r k be the value at position k of the genome. If there is no value at position k, we denote r k as NaN. Operations that involve NaN are NaN+x=x, NaN  x=x, 1=0=NaN, min NaN; x=x, max NaN; x=x, where x is any value (including NaN). For any query range p::q, let N be the number of positions k in p::q, where r k 6  NaN. The four operations are defined as follows.coveragep; q: Proportion of positions k where r k 6  NaN, that is, N=q  p+1. meanp; q: The arithmetic mean of the non-NaN values in p::q, that is, 1min valp; q and max valp; q: the minimum/maximum value in p::q, that is, min k=p::q fr k g and max k=p::q fr k g. stdevp; q: The standard deviation of the non-NaN values in p::q, that is,The extra tables in bigWig file stores precomputed answers of the operations in different zoom levels. For example, zoom level 1 stores answers for regions of length 50 000 bases and zoom level 2 stores answers for regions of length 5000 bases. The precomputed tables are also indexed using R-trees.
OBSERVATIONSThis section describes our observations on the bigWig data in UCSC hg19 database. bigWig groups bases that have the same values into intervals instead of storing signal values for each individual base. The problem becomes storing a set of tuples, i.e. s i ; e i ; v i  where, s i and e i are the start and the end positions of the intervals in a genome; and v i is the signal value of the bases in the interval s i ::e i. As the positions and the values are highly independent across the database, we study them separately in the next two subsections.
Observations on interval positionsThis section discusses our observations on the characteristics of the interval data s i ::e i stored in bigWig format. For high-density regions, NGS reads are often overlapped. Once the reads are piled up to generate the coverage data, each high-density region is expected to form a set of consecutive intervals. To illustrate,shows the density plots of a ChIP-seq region and a RNA-seq region. In both data types, we observed that the position intervals are usually consecutive (i.e. the start of the next interval equals the end of the previous one). To precisely measure this characteristic, we define a measurement called consecutiveness, which is the percentage of intervals in a signal data file that have their start positions equal the end positions of their adjacent intervals. The consecutiveness is zero when no interval stays next to another. It approaches one when all intervals are chained together.plots the proportion of bigWig files in UCSC hg19 database based on consecutiveness. We found that 81% of the files have the consecutiveness 40.5. To have a clear picture,further shows the relationship between the consecutiveness and the coverage. (Recall that the coverage is the) Intuitively, we expect high coverage files have high consecutiveness. This is actually true as shown in the figure. Most of the Chip-seq data files (highlighted in red oval) are high in both coverage and consecutiveness. However, many RNA-seq files only have high consecutiveness. That means high consecutiveness may be a characteristic of RNA-seq data. Section 4.2 will use this property to reduce the space consumption for storing the positions of the intervals.
Observations on signal valuesThis section discusses our observations on signal values in bigWig files. Let v i be the signal value of an interval s i ::e i .shows that signal values of adjacent intervals are similar for most cases. We suspect that storing the differences (i.e. v i+1  v i ) may be better than storing the raw signal values (i.e. v i ). To validate this observation, we compare the entropy of raw signals and the entropy of signal differences of adjacent intervals. [Under certain conditions, entropy () is the minimum number of bits required to store each element in a sequence of values.]shows that, among all UCSC bigWig files, the average entropy of raw signals is $4.9 bits, whereas the entropy of differences is around 3.2 bits. This means that, with a suitable compression scheme, storing differences uses less space than storing raw signal values on average. To be more precise, we try to find the list of bigWig tracks, where storing differences is better by computing the discrepancy between the two entropies for each bigWig track.shows the histogram plot of the results. We found that 81% of the bigWig tracks (represented by the area under the curve on the right side of the zero line) give smaller entropy when the differences of the adjacent signals values are stored. In other words, we can classify the files into two classes. The first class is smaller by storing differences of the signals. The second class is smaller by storing raw signal values. Our second observation is that certain signal (or difference) values occur more frequently in the bigWig file. To be precise, we define the number of frequent signal (or difference) values in a bigWig file as the minimum number of distinct values whose sum of occurrences makes up 75% of the total number of values in that file.shows the number of bigWig files that have x frequent signalvalues are usually close to zero. For integer signal files,and b show the typical distributions of signal differences. They usually contain one or two peaks in the center. For floating point signal files,shows the typical distribution of the signal differences. They often have dense values near zero. We ran a simple classifier on the database and found that of 2627 integer signal files, 1851 files have two peak shape that look like, whereas 813 files have shape that are similar to. In summary, we have three observations for the signal values.More than half of the data is better stored by differences. Most data files have a small set of frequent values. The frequent values are usually small and close to zero. We will use these observations to design schemes for storing signal values.
METHODSUsing the knowledge from the observations of all bigWig files in UCSC hg19 database, this section presents our storage scheme.
SDArrayOne of the frequently used components in our design is SDArray proposed by (). It can be seen as a compressed array of increasing integers. We use this data structure for storing both data and index pointers. The advantages of this data structure over the traditional search tree is that, it uses nearly optimal number of bits while still provides O(1) time to access and less than O log 2 m time to search (where m is the number of elements). There are a few alternative compressed structures, which have similar properties as described by. SDArray is used because of its speed and simplicity. In addition, it has good compression ratio when the values are not dense, which is commonly observed in our data. The details of SDArray are as follows. Consider an array of nondecreasing nonnegative integers P1::m. Storing P1::m explicitly costs mdlog 2 ne bits (where n is the biggest number). SDArray is a compressed data structure storing the array P1::m and enables constant time access of any element Pi. It also provides an operation called rankP; x to find the first element Pi that is greater than or equal to x, i.e. rankP; x=min fi jPi ! x; i 2 1::mg. Let n=Pm. The SDArray for the array P uses 1:56m+mlog 2 n=m+om bits and computes rank operation in Olog 2 min n=m; m time. This data structure is better than explicit storage when n ) m and m44.
Compression schemes for interval positionsConsider a set of m position intervals fs i ::e i ji=1::mg. Without loss of generality, assume the intervals are sorted in increasing order of s i. This section describes two alternative schemes (basic scheme and space saving scheme) to store the position intervals. Our two schemes also support random access of the values s i and e i. To implement compatible bigWig operations, our schemes require an operation called find intervalp that finds the maximal index i, such that s i p, and an operation called cover lenk that reports the total length of the first k intervals (i.e.The basic scheme has better access time for the queries, whereas the space saving scheme is more compact when there are many consecutive intervals. CWig uses the space saving scheme, if the consecutiveness (defined in the observation section) is 40.5; otherwise, it uses the basic scheme. Basic scheme: The basic scheme stores the starting positions and interval lengths in two SDArrays: S1::m and L1::m+1, respectively, such that Si=s i , L0=0 and Li=and L, s i and e i equal Si and Si+Li+1  Li, respectively. Operation find_interval(p) equals rankS; p. Operation cover_len(k) equals the value of the k-th entry of L plus k. Hence, all operations take Olog 2 n=m time. The space complexity for this scheme is m3:12+log 2 n=m+ log 2 l=m+om bits, where n=s m , and l is the total length of all intervals (i.e. Lm). This scheme enables efficient query. It also has good space usage when the intervals are sparse (e.g. in RNA-seq datasets). Space saving scheme: By the observations in the previous section, the space saving scheme groups the consecutive intervals into segments to save space. Precisely, we group consecutive intervals s i ; e i ;. .. ; s j ; e j  into one segment, if e k =s k+1 for k=i; ::; j  1. The space saving scheme stores the starting positions of segments, the numbers of intervals in each segment and the lengths of all intervals. Assume that there are g segments, we store: G1::g is a length-g array, where Gj is the start position of the j-th segment. I c 1::g+1 is an array such that (I c i+1  I c i) equals the number of intervals in the i-th segment. L1::m+1 contains the prefix sum of the lengths (same as the one in basic scheme).To find the start of the interval i (i.e. the value of s i ), we first compute the segment j that contains the interval i by calculating j=rankI c ; i, then s i =Gj+Li  LI c j. The end of the interval, e i =s i +Li+1  Li. function find_interval p j=rankG; p i=rankL; p  Gj+LI c j if (i5I c j+1) then return i else return LI c j+1The operation find_interval(p) can be computed using a two-step algorithm. The first step finds the segment nearest to p. Because the intervals inside each segment are consecutive, the second step finds the index of the interval that contains p, using the distance between p and the start of the segment. The operation cover_len(k) equals the value of the k-th entry of L plus k. The space complexity for this scheme is 1:56m+mlog 2 l=m+g3:12+ log 2 n=g+log 2 m=g+og+m where l is the total length of the intervals, g is the number of groups and m is the number of intervals. The estimated space requirement is better than the basic scheme when 2g5m. That is when each group on average has more than two intervals (i.e. the consecutiveness is 40.5).
Compression schemes for signal valuesBy the observations in Section 3.2, we design our compression scheme for storing values and the auxiliary data structure to support the required query. The compression has two main stages. The first stage converts the signals into integers and decides whether we need to store the raw signal values or the differences based on the entropy. It also applies some common transformations to make numbers easier for compression. The second stage uses a mixture of methods to compress the integers. Transformations: Let V=fv 1 ; v 2 ;. .. ; v m g denote the signal values. For floating point datasets, we convert all signal values into integers by multiplying with a scale factor. Precisely, we scan all values in V and identify the maximum number of digits after the decimal point; then, every value is multiplied by the same scaling factor f=10. For practical purpose, we keep at most seven fractional decimal digits of precision, which is compatible to the precision level in bigWig format. It is similar to use IEEE's 32-bit floating point numbers for storing signal values. The next step is to decide whether we store the signal values or differences. To make the decision, we compute the entropy of the values and the differences. If the entropy of the values is smaller, we will store the set B=fb i g such that b i =v i f for i=1::n where, f is the scaling factor. Otherwise, we store the set B=fb i g such that b i =v i+1  v i f for i=1::n  1. To avoid the gaps between the numbers introduced by the scaling, we convert B into C such that c i equals the rank of the values of b i in sorted order. Compression: The previous section showed that only a few signal differences have high frequency. Furthermore, many signal differences with high frequency are scattered around zero. To capture this type of distributions, we use two compression methods: Huffman code and Elias delta code. Each method has its own strength and weakness. Elias delta code () is a variable length encoding scheme for positive integers. It represents an integer x in blog xc+2blog 2 blog 2 x+1c c+1 bits. This compression scheme is asymptotically optimal when the numbers are uniformly random in a large range. Huffman code () is a variable length encoding scheme for a set of symbols (i.e. characters). It encodes each symbol by a new sequence of bits. This compression wastes at most 1 bit per symbol when the probability distribution is known. However, because it needs to store a symbol mapping table, the method is not practical when the number of symbols is large. To encode the set of numbers C from the transformation stage, we use Huffman code to capture the small set of frequent numbers and use Elias delta code for the rest. The details are as follows. We construct a Huffman code with 128 symbols. The most frequent 127 values in C are encoded by 127 Huffman symbols. The remaining values share the 128th Huffman symbol as their prefix and use the delta code values as suffixes. The weights used to build the Huffman symbols are the frequencies of the values. Note that we choose 128 symbols becauseshowed that most of the files have 5100 frequent values. The signal values V is, therefore, represented by storing the value C, and necessary information to reverse transform from values C to values V (e.g. the factor f, the scheme is raw values or differences, the ranks, the Huffman code table). Auxiliary data structures for queries: We also require a few additional auxiliary data structures and intermediate operations to implement the summary operations defined in Section 2 (i.e. min/max, average and SD). To support the min and max operations, we use Cartesian tree from(). This structure uses 2m+om bits. It supports computation of the minimum/maximum values in any range using O(1) time. Formally, the data structure provides two operations min idxi; j =arg min k2i::j fv k g and max idxi; j=arg max k2i::j fv k g. For the average and SD operations, we need auxiliary data structures to compute two intermediate operations: sum and square sum of the values. The intermediate operations are defined as follows: cover valTo implement operations cover val and cover val sqr, we keep one sampled value in every 64 values of the functions. The sampled values are stored in SDArray for fast access. To compute the values that are not sampled, we jump to the nearest sampled value and sequentially extract s j ; e j ; v j  to compute the exact sum.
QueryPrevious subsections have outlined our storing scheme for the positions and values of the intervals. This section shows how to use these components to support the four summary query operations defined in Section 2. In general, given a query region p::q, the query asks for some summary values (e.g. average, min/max, SD, coverage) of the signal values of the genome positions from p to q. The details are as follows. Coverage query: Given the input region p::q, the coverage query coverage(p,q) computes the proportion of non-NaN bases. Note that the number of non-NaN bases, which equals 1 qp+1 q  coverage0; q  p  1coverage0; p  1. Let j be the largest index such that s j is less than or equal to q (i.e. j=find intervalq). We have q  coverage0; q=cover lenj  min fe j  s j ; q  s j g+1. Similarly, we can compute p  1  coverage0; p  1 using find intervalp  1 and the interval values. Min/max query: The minimum/maximum of signal values in a query region p::q can be computed in three steps. First, we find the set of intervals fs i ; e i ;. .. ; s j ; e j g that overlap with the query region p::q. This can be done by computing find intervalp and find intervalq. The second step uses operations min idxi; j or max idxi; j to find the index of the minimal/maximal value in constant time. The last step extracts the actual signal values. Mean query:is the value of the i-th base, and n is the number of non-NaN bases, i.e. n=q  p+1coveragep; q.Similarly, we can computeStandard deviation query: stdevp; q can be computed using the formula ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
Remote file accessOur solution for remote access feature is to use a simple network layer that handles HTTP 1.1 byte ranges and keep-alive protocols. Once a data file is placed under a web server that supports the HTTP protocol (e.g. Apache, Microsoft IIS and nginx), it can be queried from different computer to get any block of data. The implementation also supports HTTPS protocol if OpenSSL library is available. To avoid duplicated data transfer and network protocol overhead, a simple file caching scheme is implemented. Any data requested over the network is read in blocks of 16 KB and stored in a cache file. An additional bit-map file is kept to mark down blocks that have been saved locally. Multiple queries to some close locations are likely to access the same data block, hence, do not incur new network request. In addition, the overhead to start transferring data over the network is high (e.g. in milliseconds). It is more beneficial to transfer data in blocks. To enhance the performance of block transferring and file caching, cWig reorganizes the component data structures to make data access localized. It groups small, fixed size and frequently accessed fields of different data structures into a consecutive segment called 'control segment'. (The segment usually stores the length, counter and metadata of the data structures.) The large and variable length data are stored in another segment of the file. When the data structure is loaded remotely, the data in the control segment is more likely to be transferred in one request and cached; therefore, it helps to reduce the delay between queries.
EXPERIMENT RESULTSIn this section, we present three sets of experiments. The first set of experiments compares the sizes of bigWig and cWig files. It also compares different alternatives of our design to support our final choice. The second set of experiments compares the speed between bigWig and cWig in one machine. The last set of experiments compares the remote query speed of cWig's and bigWig's tools. We use three datasets for the experiments: full dataset for size measurement, sampled dataset for the speed measurement on one computer and a few selected files for the remote access experiments. The full dataset consists of all bigWig files in UCSC hg19 database ($4400 files). The UCSC bigWig files use a total of 1.6 Terabytes. To have a clear picture, we categorize the files in UCSC into groups by value types (i.e. integer signal versus floating point signal) and by data types (i.e. ChIP-seq, RNA-seq, DNAse, FAIRE and Other). This dataset is used in the section on file size comparison. The sampled dataset is a subset of the full dataset. The files are grouped similarly as the full dataset. However, each group only contains 510 sampled files. (The detailed list of files can be found in the Supplementary C.) The sampled datasets are used for running time comparison. Furthermore, three files from UCSC hg19 of different sizes are selected for the remote query speed experiments. Note that the name bigWig, cWig or gzip is used to refer to both the file format and tool/program to access the format. For bigWig, there are a few tools that can create, extract and randomly access the format. We use the latest version of the tool provided by the original authors (in).
File sizes comparisonCompare different methods:shows results that compare different storage formats for different data types. The methods used in this experiment are (i) bedGraph is the raw text format of the input file, (ii) gzip_bg is the gzip compressed bedGraph format, (iii) bigWig is the method from UCSC, (iv) val_delta is our method that stores the raw signal values using delta code, (v) diff_delta is our method that stores signals by their differences using delta code only and (vi) huff128 and (vii) huff1024 are our methods that store signals by their differences using a mix of Huffman code and delta code. huff128 encodes the most frequent 127 values by unique Huffman symbols, whereas the rest of the values are encoded by delta code. huff1024 is similar to huff128; but the number of Huffman symbols are 1023. For clarity,shows only four types of data: ChIP-seq, RNA-seq FAIRE and Other. (For full result, please refer to Supplementary B.) The bars in the background show the relative ratios between the compression schemes. Among our methods, huff128 and huff1024 are consistently better than val-delta and diff-delta. huff128 and huff1024 give similar size. This supports the observations in Section 3.2 that, higher number of Huffman symbols does not improve compression. Based on this experiment, we choose huff128 as our default compression method for cWig format. Compared with bigWig and gzip, our methods use at most half of their sizes. In most of the files, the file sizes of bigWig and gzip are similar because the bigWig uses gzip to compress their main data. However, for high-resolution files, e.g. FAIRE data type, bigWig uses considerably more space than gzip. We found that this space is usually accounted for its indexing structures to support random access and queries. Compare ours and bigWig:compares the file sizes between cWig and bigWig formats.plots the original. This table indicates the mean file sizes for storing ChIP-seq, RNA-seq and Other data types using the raw text format (bedGraph) and six different compression schemes. The bars in the background show the relative ratios between the compression schemes bigWig size versus the reduction that we can achieve.is a table that summarizes the ratios based on the data types. It shows that our format is (in average) 3.6 times smaller than bigWig. In particular, cWig is more compressible for high resolution datasets, e.g. FAIRE and DNase. We noticed some users truncate the significant digits of the values to reduce the file sizes of bigWig. We conducted an experiment to investigate its effect on both formats. The detail is included in the Supplementary D.
Running time comparisonLinear compression and extraction:shows the average compression/decompression speed for different methods. Because the compression/decompression speed is consistent with the input file size, we only show the average processing time in terms of megabytes per second. The figure shows that bigWig and gzip have similar compression speed. Our program is about two times faster. For decompression speed, our program is $150% faster than bigWig, but slower than gzip. Random queries: This set of experiments measure the query speed of operations coverage, minimum and average for both our tool and bigWig tool. We tested three sets of queries: (i) each query is a random interval. The order of queries is also random.This set is intended to simulate the actual list of queries made by the bioinformaticians). Because the speed of both programs for query types (i) and (ii)are not significantly different, we only summarize the speed for query types (i) and (iii). In addition, because the query speed for the three operations in bigWig is similar, we only report the average query speed of bigWig.shows that the query speed of our program is $10 100 times faster than that of bigWig, depending on query type. In our program, coverage queries are much faster than the minimum and the average queries because coverage queries only use the interval position component. The minimum queries are faster than the average queries in sparse files where there are a lot of regions without values. We noticed that there is a big difference in bigWig speed between random queries and real queries. After some investigations, we found that bigWig query speed may be affected by the query interval length. It is slower for shorter intervals. We create a query file that has the same starting positions as the real query file, but increased in the interval lengths. bigWig is much faster when the interval lengths are larger than 1 million bases. Note that the average interval length of the random query inis around half the chromosome length, whereas the average interval length of the genes is only 54 783.. Average compression and decompression speed in Megabytes per second (higher is better) with SDs for each method. Average query time in nanoseconds (lower is better) for randomly generated queries and gene region queries
Remote file access speedIn this experiment, we measure the query speed in different network conditions. We select three input files of different sizes from UCSC hg19 database. They are called 'small', 'medium' and 'big'. The sizes of the corresponding bigWig files are 824 KB, 98 MB and 5.5 GB, respectively. The sizes of the corresponding cWig files are 414 KB, 35 MB and 1.4 GB, respectively. The query speed is measured in two different network conditions: 'SG' and 'US'. 'SG': the files and the programs are both hosted in Singapore and connected through the Internet. The average round trip time is $100 ms; the bandwidth is $510 MB/s. 'US': the programs are in Singapore, and the files are hosted in California, USA. The round trip time is $210 ms, the bandwidth is $300850 KB/s. Similar to the previous experiment, we use the human genes regions as the query set.compares the running times of bigWig and cWig under different network conditions and using different input files. (Note that, there is no measurement for big file under 'US' network condition owing to our resource limitation.) In these experiments, the CPU times of both programs are accounted for 510% of the total running times for medium and big input files. The programs spend most of their time waiting for network responses. Our file size significantly helps in the experiments on the medium file. Because cWig file is smaller, the queries on this file get cached in fewer iterations. For small file, the time difference is not significant. Both programs can cache the small file after a few queries. For big file, both programs fail to cache the file, and hence, both methods spend similar amount of time to wait for the network to respond.
CONCLUSIONThis article proposed the file format for cWig to store signal data. Comparing with bigWig, cWig not only uses lesser space but also provides faster queries. This format should be useful for visualization applications like UCSC genome browser () and Broad Institute Integrative Genome Viewer () and for Biologists to analyze and discover features in their data. In the future, we would like to extend our idea to represent other types of data [like bigBed () and BAM (. We also want to consider lossy compression methods to gain better compression over noisy data.
Conflictsof Interest: none declared.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
D.Huy Hoang and W.-K.Sung at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
CWig at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
n X q i=p r 2 i  1 n 2 meanp; q 2 q , where, r i and n are defined same as above. Using similar approach as the mean query, the sum of squared
