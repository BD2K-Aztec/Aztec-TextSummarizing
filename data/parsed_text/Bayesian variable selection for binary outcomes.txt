Motivation: The advent of new genomic technologies has resulted in the production of massive data sets. Analyses of these data require new statistical and computational methods. In this article, we propose one such method that is useful in selecting explanatory variables for prediction of a binary response. Although this problem has recently been addressed using penalized likelihood methods, we adopt a Bayesian approach that utilizes a mixture of non-local prior densities and point masses on the binary regression coefficient vectors. Results: The resulting method, which we call iMOMLogit, provides improved performance in identifying true models and reducing estimation and prediction error in a number of simulation studies. More importantly, its application to several genomic datasets produces predictions that have high accuracy using far fewer explanatory variables than competing methods. We also describe a novel approach for setting prior hyperparameters by examining the total variation distance between the prior distributions on the regression parameters and the distribution of the maximum likelihood es-timator under the null distribution. Finally, we describe a computational algorithm that can be used to implement iMOMLogit in ultrahigh-dimensional settings (p >> n) and provide diagnostics to assess the probability that this algorithm has identified the highest posterior probability model. Availability and implementation: Software to implement this method can be downloaded at:
IntroductionRecent developments in bioinformatics and cancer genomics have made it possible to measure thousands of genomic variables that might be associated with the manifestation of cancer. The availability of such data has resulted in a pressing need for the development of statistical methods to use these data to identify variables that are associated with binary outcomes (e.g. cancer or control, survival or death). The topic of this article is a statistical model for identifying, from a large number p of potential feature vectors, a sparse subset that are useful in predicting a binary outcome vector. Throughout this article, we assume that the binary vector of interest is denoted by y, and that the matrix of potential explanatory variables is denoted by X. Letting X k denote the submatrix of X containing the 'true' predictors, we assume thatwhere F denotes a known binary link function (assumed to be the logistic distribution in what follows), and p is the n vector of success probabilities for y. The regression coefficient b k represents the nonzero regression effect for each column of X k in predicting p. The primary statistical challenge addressed in this article is the selection of the submatrix X k to be used for the prediction of p. A number of related methods have been proposed to address this problem. These include the LASSO (), which is a penalized likelihood method that maximizes a product of the binary likelihood function implied by (1) and a constraint on the sum of the absolute value of components of the regression coefficient b k. A closely related method called Smoothly Clipped Absolute Deviation (SCAD) () uses a non-convex penalty function and has been demonstrated to have certain oracle properties in idealized asymptotic settings. Other penalized likelihood functions include the adaptive LASSO () and the Dantzig selector (); these methods share asymptotic properties similar to SCAD. In ultrahigh-dimensions (p >> n), an effective computational technique for implementing the techniques described above is the Iterative Sure Independence Screening (ISIS) procedure (), which iteratively performs a correlation screening step to reduce the number of explanatory variables so that penalized likelihood methods can be applied. ISIS has been used in conjunction with several penalized likelihood methodsincluding adaptive LASSO (), the Dantzig Selector (), and SCAD ()to perform model selection. A number of Bayesian methods have also been proposed for variable selection. Notable among these are the approaches proposed by, which used a mixture-of-normals approximation to spike-and-slab priors on the regression coefficients.proposed a hierarchical probit model along with MCMC based stochastic search to perform gene selection in high-dimensional settings using a latent response variable and Gaussian priors on model coefficients.provided a Bayesian approach to this problem employing singular value regression and classes of informative prior distributions to estimate coefficients in high-dimensional settings.studied mixtures of g priors for Bayesian variable selection as an alternative to default g priors to overcome several consistency issues associated with the default g prior densities. Along more similar lines,studied the utilization of non-local priors in Bayesian classifiers where they also address the problem of identifying variables with high predictive power. Except for, each of the Bayesian methods described above impose local prior densities on regression coefficients in the true model. That is, the prior density on the regression coefficients has a positive prior density function at 0 (and in most cases has its mode at 0), which from a Bayesian perspective makes it more difficult to distinguish between models that include regression coefficients that are close to 0 and those that do not. Johnson and Rossell (2012) proposed two new classes of non-local prior densities to ameliorate this problem. In the model selection context, non-local prior densities are 0 when a regression coefficient in the model is 0. This makes it easier to distinguish between coefficients that do not have an impact on the prediction of y from those that do.used a Markov Chain Monte Carlo (MCMC) algorithm to sample from the posterior distribution on the model space; the convergence properties of this algorithm were studied in Johnson (2013). The primary goal of this article is to extend the methodology proposed infor application to binary outcomes and to compare the performance of this algorithm to leading penalized likelihood methods. In addition, we describe a default procedure for setting the hyperparameters (i.e. tuning parameters) in the non-local priors, and we examine a numerical strategy for identifying the highest posterior probability model (HPPM).
MethodsLet y n  y 1 ;. .. ; y n  T denote a vector of independent binary observations, X n an n  p matrix of real numbers, b a p  1 regression vector, and x i the i th row of X n. We denote a model by k  fk 1 ;. .. ; k j g where 1 k 1 <    < k j p and it is assumed that b k1 6  0;. .. ; b kj 6  0 and all other elements of b are 0. The design matrix corresponding to model k is denoted by X k and is defined to have cardinality k. We assume that the columns of X have been standardized. The i th row of X k is denoted by x ik. Assuming the logistic link function for F in (1), the goal of the model selection procedure proposed in this article is to identify sparse regression models that have high predictive probability. We propose to do this by identifying the highest posterior probability model k for data y, distributed according tounder prior constraints on the model space and the assumption of non-local prior density constraints on the regression parameter b k. Our primary focus is on the case p >> n. Bayesian model selection is based on the calculation of posterior model probabilities. From Bayes theorem, the posterior probability of model j 2 J is specified aswhere m k y n    py n jb k p k b k db k :The art in implementing a Bayesian model selection procedure thus focuses on specifying the prior densities p k b k  for b k under each model, as well as the prior model probabilities pk for the models themselves. Except for the intercept, we assume non-local priors on the components of the regression vector in each model. These non-local priors are described in the next section. Discussion of the prior on the model space is described after that.
Non-local priorsThe form of the non-local prior densities imposed on the (non-zero) regression coefficients b k in this article take the form of a product of independent iMOM priors, or piMOM densities, expressible as pb k js; r  s rk=2Here b k is a vector of coefficients of length k, and r; s > 0. The hyperparameter s represents a scale parameter that determines the dispersion of the prior around 0, while r is similar to the shape parameter in the Inverse Gamma distribution and determines the tail behavior of the density. An example of an iMOM density is illustrated infor the particular case of r  1 and s  3. An important feature of this non-local prior, as highlighted in, is that these priors do not necessarily impose significant penalties on non-sparse models, provided that the estimated coefficients in the non-sparse models are not too small. That is, large values of regression coefficients are not penalized since the value of the exponential kernel in (5) tends to 1 as b i becomes large. This fact lies in stark contrast to most penalized likelihood methods.
Prior on model spaceTo define the prior on the model space, we adopt a subjective version of the prior proposed by. In the fully Bayesian version of the beta-binomial prior, this formulation specifies that the prior probability for model k is pk  Ba  k; b  p  k Ba; b ;where B(a, b) denotes the beta function and a and b are prior parameters that describe an underlying beta distribution on the marginal probability that a selected feature is associated with a non-zero regression coefficient in (2). This type of prior on the model size is also recommended in, where it is suggested that an exponential decrease in prior probabilities with model size provides optimal results when the prior density on regression parameters has the form of a double exponential. To incorporate our belief that the optimal predictive models are sparse, we arbitrarily set a  mink  ; blogpc, and b  p  a. For large n, this implies that we expect, on average, a feature vectors to be included in the model. Here, k   argmax k p k < 2 n. This choice of k  for the prior hyperparameter reflects the belief that the number of models that can be constructed from available covariates should be smaller than the number of possible binary responses. Similarly, by restricting a to be less than logp, comparatively small prior probabilities are assigned to models that contain more than logp covariates. Finally, we impose a deterministic constraint on model size and define Pk  0 if k > n=2. A sensitivity analysis for a and b in (6) is provided in Section 4.1.1.
Choosing hyperparametersA critical aspect of implementing our model is the choice of the hyperparameters r and s. The value of r determines the tail behavior of the piMOM prior, while s plays a role similar to the tuning parameter in penalized likelihood methods, with its value largely determining the minimum value of a component of b k that will be selected into a high posterior probability model. To pick an appropriate, application-specific value for s, we adopt a strategy in which we compare the null distribution of the maximum likelihood estimator for b k (i.e., when all components ofTo find an appropriate value of r for the piMOM prior (5), we impose a constraint that the prior mass assigned to the interval (10,10) equals 0.95. This constraint is imposed because coefficients larger than 10 in magnitude are not expected when the columns of the design matrix have been standardized. Together, these constraints identify a unique combination of r and s for the piMOM prior. A numerical strategy for finding this hyperparameter vector is outlined in Algorithm 1.Notice that this procedure for choosing the hyperparameters depends on the prior on the model space. This implies that s will tend to be larger in larger models, because it is more likely that the sampled columns X will exhibit high collinearity in large models. Ideally, we would adjust s for each individual model, but as mentioned earlier it was not computationally feasible to do so for the applications and simulations reported in this article.
Numerical aspects of implementationThe model described in Section 2 leads to a joint density for the data, model k and its parameters. As a result, the posterior distribution of model k and its coefficients can be expressed asBecause of the high dimension of the parameter space and the complexity of the posterior density function in (7), it is not feasible to maximize this function analytically to obtain the HPPM. To search for the HPPM, we therefore utilized a Markov chain Monte Carlo algorithm. To reduce the dimension of the parameter space, we used a Laplace approximation to marginalize over the regression coefficient b k associated with each model. The resulting approximation to the marginal posterior density of the data y under model k can be expressed asHere ~ b k is the MAP estimate of b k and jRj is the determinant of the Hessian of the function f y n ; b k   logpy n jb k  logp k b k , computed at ~ b k. The elements of the Hessian matrix can be expressed asA simple birth-death scheme was used to sample from the posterior distribution. At each iteration of MCMC algorithm, each of the p covariates was visited in random order. The update at position i was performed by proposing a candidate model by flipping the inclusion state of that variable in the model. The candidate model was accepted using a Metropolis algorithm where the probability of accepting the candidate model, k cand , wasThe MAP estimate for b k was obtained using the nlminb() function in R. We assumed that an intercept was present in all models.
Convergence diagnosticsConvergence diagnostics of MCMC can be used to assess whether an adequate number of iterations have been performed. Because of the high dimension of the parameter space for even moderately large p, we implemented a modified coupling diagnostic () to assess the probability that our MCMC algorithm had identified the true model. In the standard implementation of this method, one randomly initializes two MCMC chains by independently including each variable in the model according to a fixed probability. The components of the model in each chain are then updated synchronously, using the same uniform random deviate to perform acceptance/ rejection of the candidate models. The chains are said to couple when the models from each chain are identical. Note that once the chains become coupled, they never uncouple. In theory, the distribution of the number of updates of the chains required to obtain coupling can be used to establish a bound on the total variation distance (TVD) between iterates in the chain and the target distribution. In our implementation of the coupling diagnostic, we started 100 pairs of model chains. Each pair was updated until either they had coupled or all p components in each of the chains had been updated N times where N  250. The (local) HPPM identified by each chain was recorded, and then the HPPM's for the 100 chains were compared. We then identified the global HPPM among the 100 models in the paired chains, and also examined the proportion of chains that had both coupled and identified the 'global' HPPM.
ResultsTo investigate the performance of the proposed model selection procedure, we applied our procedure to both simulated data sets and real data. We compared the performance of our algorithm to ISISSCAD () in both real and simulated data because ISIS-SCAD has proven to be among the most successful model selection procedures used in practice. For the real data analyses, we also compared our method to another Bayesian procedure based on the product moment prior ().Bayesian variable selection for binary outcomes
Simulation studiesIn all simulation studies, we assumed that the response vector represents a sequence of Bernoulli samples whose component probabilities of success are given byfor a true model k. Elements of the design matrix X were sampled from a multivariate normal distribution with mean 0 and covariance matrix R, where the diagonal elements of R were 1 and off diagonal elements were 0.5. That is, R  1 0:5    0:5 0:5 1    0:5 .. . .. . .. . .. . 0:5 0:5    1Different combinations of n and p were investigated. Moreover, different ranges of regression coefficients were tested. In our simulations, the true model contained three variables. The following combinations of n, p and b were used to perform the simulation studies.The hyperparameters s and r for the piMOM prior were selected by the procedure explained in Section 2.2.1 for each of the 10 combinations of n and p. Values of s and r selected by this procedure are summarized in Tables 1 and 2, respectively. To run ISIS-SCAD, we used the R package 'SIS' () available from CRAN. The variable selection procedure in both algorithms was run 50 times for each of the 30 combinations of n, p and b. In each trial, true and false positive values for iMOMLogit and ISIS-SCAD were counted by comparing the selected model with the true one. TP and FP rates were defined as the average true and false positive values over 50 trials. A true positive, TP, was defined to be the number of variables that were correctly selected, while false positives, FP, were the number of variables that were mistakenly selected. Figures 2 and 3 show average TP and FP counts of both methods for all combinations of n and p and b  b 1. The figures for b 2 and b 3 are provided in the supplementary materials and demonstrate similar trends. In all cases, the average FP count for iMOMLogit was less than ISIS-SCAD, while its average TP count was higher. The only case where both iMOMLogit and ISIS-SCAD had the same average TP count was when they both found the true model in all 50 simulation trials. We next compared the performance of both methods in estimating the regression coefficients. For each simulation setting, we compared the mean squared error in estimating the probability of success for each binary observation by performing 10-fold cross validation. The point estimate ^ b was estimated as the posterior mode under the HPPM. The predicted value of ^ p was then computed according to (1). Note that the prediction of the response vector involves both coefficient estimation and variable selection. The mean squared error of prediction (MSE) was defined as follows:The comparison between cross validated MSEs of both methods is shown inand 5. As in the comparisons of TP and FPrates, these figures suggest that iMOMLogit is preferred to ISISSCAD in estimating the success probabilities of binary observations.
Sensitivity analysis for prior parameters on model spaceTo assess the sensitivity of our results to the prior hyperparameters on the model space (6), we conducted a brief sensitivity analysis under the simulation settings for which n  200, p  1000 and b  4; 5; 6 T. We also fixed b  p  a as before. This insured that the prior mean of the number of variables selected would be a. Based on the default procedure for defining a described in Section 2.2, the default value for a in this setting was 6. We examined sensitivity to this choice of a by varying a around this default value within the interval (3, 9). To quantitatively assess the sensitivity of the selection procedure to values of a in this range, we examined the consequent changes to MSE^ p described in (13). This measure incorporates errors in both variable selection and coefficient estimation. The figure provided in the supplementary material depicts MSE^ p for different values of a in the described simulation setting. As shown in that figure, model output does not change dramatically with changes in a, varying by at most 4:8  10 5 from the default choice of a.
Real data analysisWe applied iMOMLogit to two data sets, one with a small sample size and one with a large sample size. These two data sets are publicly available and have good clinical annotations. The first data set was the Golub leukemia data (). The goal of our analysis for these data was to discriminate between two types of acute leukemia, myeloid (AML) and lymphoblastic (ALL). The design matrix consisted of gene expression levels produced by cDNA microarrays from bone marrow samples, and was pre-processed by RMA (). There are 72 samples and 7,129 genes in the data set. The second data set was the clear cellTo run pmomPM method, we used the R package 'mombf' () available from CRAN. In contrast to iMOMLogit and ISIS-SCAD, the mombf package focuses on prediction using Bayesian model averaging, rather than on the identification of biologically important genes using the HPPM. Because of the behavior of the pMOM prior near the origin, the pMOM model selects many more genes in the models over which it averages. Though model averaging can improve prediction accuracy (), the current version of the mombf package does not provide estimates of the HPPM, which complicates comparisons with the other methods considered here. These attributes of the pmomPM method are illustrated in the examples that follow.
Leukemia dataFollowing, we split the data into training and test sets. The training set contained 38 samples, with 27 ALL and 11 AML. The testing set contained 34 samples, with 20 ALL and 14 AML.summarizes the results of applying iMOMLogit, ISISSCAD and pmomPM to these data. The error rate for predicting the test data observations was 5.88% for iMOMLogit, which misclassified 2 out of 34 observations, samples 17 and 31. Both ISIS-SCAD and the method described inresulted in an errorBayesian variable selection for binary outcomesrate of 14.7%. ISIS-SCAD achieved this error rate by finding two significant genes, 'Zyxin' and 'FAH', whereasselected 50 genes. The pmomPM method achieved an error rate of 23.53% with an average model size of 11.08. None of the genes were assigned marginal posterior probability of 0.5 by the pmomPM method; the highest marginal posterior probability of any gene was 0.052, acheived by CD33. iMOMLogit selected a model containing only one gene named 'Zyxin', which perfectly predicted the classifications in the training data. This gene was also listed in the top 50 genes reported by, and was found to be advantageous for classifying the two types of leukemia in four published data sets (). The gene 'FAH' found only by ISIS-SCAD is involved in certain metabolic pathways that are not known to be associated with leukemia (Kegg.org). Following the methodology discussed in Section 3.1, 74% of pairs of chains that were updated using the coupling algorithm found the same highest posterior probability model (HPPM). Among all pairs, 95% coupled.
Renal cell carcinoma dataThe second data set was generated by the Cancer Genome Atlas Research Network (2013) and contained Illumina HiSeq data on mRNA expression for 467 patient samples. The survival outcomes of these patients were available. A hierarchical clustering of the gene expression data [preprocessed using DeMix () to remove stromal contamination] were performed on the data. That led to the identification of four clusters of patients based on survival times. To apply iMOMLogit, we considered two of those clusters, presenting the best and worst survival outcomes and labeled them as 0 (worst) and 1 (best). The resulting number of samples included in our analysis was 193, with 14150 features in the design matrix. The results using iMOMLogit, ISIS-SCAD and pmomPM are summarized in. To compare methods, we performed a 10fold cross-validation. The error rate of iMOMLogit was 9.79%, ISIS-SCAD's error rate was 12.97%, and pmomPM was 9.84%. In the model selected by iMOMLogit, there were 3 significant genes named 'C7orf43', 'NUMBL' and 'SAV1', with the latter two being uniquely identified by our model. 'NUMBL' participates in the Notch signaling pathway and is believed to contribute to nervous system tumors (glioma) () as well as lung cancer (). The Notch signaling pathway is highly conserved, manages communication between adjacent cells and maintenance of adult stem cells, and is linked to the development of various cancers (). Not surprisingly, we identified NUMBL as differentiating two groups of kidney patients. 'SAV1' has been reported to play a role in kidney cancer (), and is located in a Hippo signaling pathway (Kegg.org). The Hippo signaling pathway is highly conserved and controls epithelial tissue growth. Recently, its relation to other signaling pathways has been studied to identify new therapeutic interventions for cancer (). Among all pairs of chains with different random starts, 32% of them reported the same global HPPM and 6% of paired chains were coupled. This suggests that convergence in this data set was more problematic, and that our multiple coupled chain approach (or other modifications of the standard, single chain MCMC algorithm) is required to identify the HPPM model. The genes uniquely selected by ISIS-SCAD were 'C19orf66', 'ATXN7L2' and 'MIICAL1'. 'ATXN7L2' was previously reported to be associated with non-small cell lung cancer (), whereas 'MICAL1' was previously reported to control survival in melanoma cell lines. As for the leukemia data, the pmomPM selected substantially more genes in each of its sampled models, and the genes selected in each model were highly variable. The average model size of the pmomPM method for this data set was 13.84. As before, none of the genes were assigned marginal probability of 0.5; the highest marginal posterior probability assigned to any gene was 0.33, for API5. The genes identified by iMOMLogit seem to be more biologically meaningful and better annotated in the literature for ccRCC than those selected by ISIS-SCAD.
DiscussionIn this article, we introduced a Bayesian method, iMOMLogit, for variable selection in binary response regression problems in high and ultrahigh-dimensional settings. There are many applications associated with these type of data. Such data are of great interest to bioinformaticians and biologists, who routinely collect gene expression data to find prognostic features to classify cancer types. For two real datasets, iMOMLogit identified sparse models with low prediction error rates. In both cases, biological considerations suggest that the genes reported by iMOMLogit appear to be valid predictors of biological outcomes. The primary disadvantage of the iMOMLogit procedure is that it is computationally much more intensive than ISIS-SCAD and related penalized likelihood methods. We are currently investigating methods for reducing the computational burden of our algorithm by implementing various screening procedures that are similar to those used in ISIS-SCAD.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
b k are 0), obtained from a randomly selected design matrix X k , to the prior density on b k under the alternative assumption that the components are non-zero. By choosing s to be just large enough so that the intersection of these two densities falls below a specified threshold, we are able to approximately bound the probability of false positives in the model, while at the same time maintaining sensitivity to regression coefficients that fall outside of the distribution of MLEs that estimate 0. In principle, we could employ this strategy to obtain a distinct value of s for each model k, but were unable to do so in this article because of the computational expense this procedure would impose. Instead, we mixed over models to obtain a single value of s. Numerically, our strategy is implemented as follows. We begin by sampling a model from the prior on the model space. That is, we randomly sample k columns of X where k is determined by a draw from the prior on the model space. A Bernoulli vector of length n with success probability ^ p is generated, where ^ p is the proportion of successes in the observed data. Then the MLE for the model is estimated using standard logistic regression software with an intercept included in the model. This process is repeated N times to obtain a normal density approximation to the marginal density of maximum likelihood estimates under the condition that all true regression coefficients (except for the intercept) are 0. Typically, N  O10 4 . Next, piMOM priors corresponding to different values of s are compared to the null distribution of the MLE. Based on these comparisons, we numerically determine the value of s so that the overlap of these densities falls below a threshold of p 1=2. This overlap value is chosen heuristically in a way that suggests the number of false positives will decrease to 0 as p and n become large. Other thresholds of the form p a might also be considered, but we have found that a  1=2 provides good performance in a wide range of simulation studies and in real data examples. Further justification for this threshold is provided in the supplementary data. Notice that for a fixed p, the dispersion of the null distribution of the MLE around 0 decreases as the sample size n increases, although the rate of decrease is also affected by the structure of the design matrix X. This effect is illustrated in Table 1. We also note that a similar procedure for setting the scale parameter for local priors on the regression coefficients could potentially be implemented. Unfortunately, the application of this procedure to local priors can require extremely large values of tuning parameters in order to 'squash' the prior near 0 and achieve small overlap with the null distribution. As a consequence of this fact, the tuning parameters selected by this procedure will not reflect any reasonable prior belief on the values of the regression parameters in a logistic model with a standardized design matrix.
A.Nikooienejad et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
