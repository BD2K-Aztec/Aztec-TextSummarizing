Motivation: Discriminant analysis is an effective tool for the classification of experimental units into groups. Here, we consider the typical problem of classifying subjects according to phenotypes via gene expression data and propose a method that incorporates variable selection into the inferential procedure, for the identification of the important biomarkers. To achieve this goal, we build upon a conjugate normal discriminant model, both linear and quadratic, and include a stochastic search variable selection procedure via an MCMC algorithm. Furthermore, we incorporate into the model prior information on the relationships among the genes as described by a geneâ€“gene network. We use a Markov random field (MRF) prior to map the network connections among genes. Our prior model assumes that neighboring genes in the network are more likely to have a joint effect on the relevant biological processes. Results: We use simulated data to assess performances of our method. In particular, we compare the MRF prior to a situation where independent Bernoulli priors are chosen for the individual predictors. We also illustrate the method on benchmark datasets for gene expression. Our simulation studies show that employing the MRF prior improves on selection accuracy. In real data applications, in addition to identifying markers and improving prediction accuracy, we show how the integration of existing biological knowledge into the prior model results in an increased ability to identify genes with strong discriminatory power and also aids the interpretation of the results. Contact:
INTRODUCTIONDiscriminant analysis, sometimes called supervised pattern recognition, is a statistical technique used to classify observations into groups. For each case in a given training set a p1 vector of observations, x i , and a known assignment to one of G groups are available. On the basis of these data, we wish to derive a classification rule that assigns future cases to their correct groups. If the distribution of the np matrix X of the data, conditional on the group membership, is assumed to be a multivariate normal, then this statistical methodology is known as discriminant analysis. We consider the typical problem of classifying subjects according to phenotypes via gene expressions and propose a method to * To whom correspondence should be addressed. include a variable selection procedure into the inferential process, for the identification of the important biomarkers. We build upon a conjugate normal discriminant model, linear or quadratic, and include a stochastic search variable selection procedure via an MCMC algorithm. Furthermore, we use dependent priors that reflect known relationships among the genes. Recently, there has been a rapid accumulation of biological knowledge in the form of various genegene networks. The importance of incorporating such biological knowledge into the analysis of genomic data has been increasingly recognized. Here, we view a genegene network as an undirected graph with nodes representing genes and edges representing interactions between genes. We capture this information via a Markov random field (MRF) prior that maps the connections among genes. Our prior model assumes that neighboring genes in the network are more likely to have a joint effect on the relevant biological processes. Similar priors have been used in linear regression models by, Wei and Pan (2010) andand in gamma-gamma models by). We extend their use to the discriminant analysis setting. We illustrate our method for the case of quadratic discriminant analysis, where different groups are allowed to have different covariance matrices. We show good performances on simulation studies and illustrate the method on benchmark datasets for gene expression. In particular, we compare the MRF prior to a situation where independent Bernoulli priors are chosen for the individual predictors and show that employing the MRF prior leads to more accurate selection. Other authors have reported similar results., in particular, comment on the effect of the MRF prior on the selection power in their linear regression setting. They also notice that adding the MRF prior implies a relatively small increase in computational cost.) andreport that their methods are quite effective in identifying genes and modified subnetworks, with higher sensitivity than commonly used procedures that do not use the network structure, and similar or, in some cases, lower false discovery rates. In real data applications, in addition to improving prediction accuracy, we show how the integration of biological knowledge into the prior model results in an increased ability to identify genes with strong discriminatory power and also aids the interpretation of the results. The rest of the article is organized as follows: in Section 2, we introduce discriminant analysis under the Bayesian paradigm and describe how to perform variable selection. We also propose a way to incorporate information about genegene networks into the prior model. In Section 3, we present the MCMC algorithm for posterior inference. In Section 4, we investigate performances of the
F.C.Stingo and M.Vannucciproposed method on simulated data and conclude, in Section 5, with applications to benchmark datasets for gene expression where we incorporate the genegene network prior.
BAYESIAN DISCRIMINANT ANALYSISLet X indicate the observed data and let y be the n1 vector of group indicators. We assume that each observation comes from one of G possible groups, each with distribution N( g , g ). We represent the data from each group by the n g p matrixwith g = 1,...,G and where the vector  g and the matrix g are the mean and the covariance matrix of the g-th group, respectively. Here the notation VM  N (A,B) indicates a matrix normal variate V with matrix mean M and with variance matrices b ii A for its generic i-th column and a jj B for its generic j-th row, see Dawid (1981). Taking a conjugate Bayesian approach, we impose a multivariate normal distribution on  g and an inverse-Wishart prior on the covariance matrix g , that is,This parametrization, besides being the standard setting in Bayesian inference, allows us to create a computationally efficient variable selection algorithm by integrating out means and covariances and designing Metropolis steps that depend only on the selected and proposed variables, see Section 3. In discriminant analysis, the predictive distribution of a new observation x f is used to classify the new sample into one of the possible G groups. This distribution, see Brown (1993) among others, is a multivariate T-student of the typewhereThe probability that the future observation, given the observed data, belongs to group g is then given bywhere y f is the group indicator of the new observation. By estimating the prior probability that one observation comes from group g with with with g = n g /n, the previous distribution can be written in closed form aswhere p g (X f ) indicates the predictive distribution defined in (2). The new observations is then assigned to the group with the highest posterior probability.
Likelihood and prior setting for variable selectionOur aim is to construct a classifier while simultaneously selecting the discriminating variables (i.e. biomarkers). Here, we extend an approach to variable selection proposed byfor model-based clustering to the discriminant analysis framework. As done by these authors, we introduce a (p1) latent binary vector  , whose elements equal to 1 indicate the selected variables, i.e.  j = 1 if variable j contributes to the classification of the n units into the corresponding groups. We use the latent vector  to index the contribution of the different variables to the likelihood. Unlike, we avoid any independent assumption among the variables by defining a likelihood that allows to separate the discriminating variables from the noisy ones aswhere w g is the prior probability that unit i belongs to group g,) is the | c |1 vector of the non-selected variables and X i( ) is the | |1 vector of the selected ones, for the i-th subject. Under the normality assumption on the data, the likelihood becomesWe complete the prior model by defining an improper noninformative prior on the vector w = (w 1 ,...,w G ) as a Dirichlet distribution, w  Dirichlet(0,...,0). We discuss priors for the latent indicator  in the next Section. Note that, with the inclusion of the variable selection mechanism, the predictive distribution (3) does not change as it depends only on the selected variables. Without loss of generality, at least for the inferential procedure described in Section 3, we can assume that the set of nonselected variables is formed by only one variable so that the prior parametrization can be simplified using the scalar  2 instead of 0( c ) and the (| |1) vector  instead of the | c || | matrix B, withTo obtain this parametrization, the commonly used assumption 0( c ) = k 0 I | c | is needed, see for examplefor a model-based clustering context, andfor a graphical model context.
Prior distribution for the integration of gene network informationAlthough the model allows for dependencies among the variables, it is not straightforward to specify dependence structures known a priori on the covariance matrix prior. When prior information is available, a better strategy is to incorporate it into the priorPage: 497 495501
Bayesian selection for discriminant analysisdistribution on . The prior model for this parameter is indeed quite flexible and allows the incorporation of biological information in a very natural way. Here, we use in particular biological information that derive from existing databases on genegene networks. We encode such genegene network information in our model via a MRF prior on . A MRF is a graphical model in which the distribution of a set of random variables follows Markov properties that can be described by an undirected graph. In our context, the MRF structure represents the genegene network, i.e. genes are represented by nodes and relations between them by edges (direct links). With the parametrization we adopt the global MRF distribution for  is given by p( |d,F)  exp(d T  + T F ),with d = d1 p and 1 p the unit vector of dimension p, and F a matrix with elements {f ij } usually set to some constants f for the connected nodes and to 0 for the non-connected ones. Here d controls the sparsity of the model, while f affects the probability of selection of a variable according to its neighbor values. This is more evident by noting that the conditional probabilitywith N j the set of direct neighbors of variable j in the MRF, increases as a function of the number of selected neighbor genes. With this parametrization, some care is needed in deciding whether to put a prior distribution on f. Allowing f to vary can in fact lead to a phase transition problem, that is, the expected number of variables equal to 1 can increase massively for small increments of f. This problem can happen because Equation (6) can only increase as a function of the number of x j 's equal to 1. If a variable does not have any neighbor, its prior distribution reduces to an independent Bernoulli with parameter p = exp(d)/[1+ exp(d)], which is a logistic transformation of d.
MCMC FOR POSTERIOR INFERENCEWith the main purpose being variable selection, we perform posterior inference by concentrating on the posterior distribution on . This distribution cannot be obtained in closed form and an MCMC is required. The inferential procedure can be simplified by integrating out the parameters w g ,, 2 , 0 , g and g , obtaining the following marginal likelihood:We implement a Stochastic Search Variable Selection (SSVS) algorithm that has been successfully and extensively used in the variable selection literature, see Madigan and York (1995) for graphical models,for linear regression models,for classification settings with probit models andfor clustering, among others. This is a Metropolis type of algorithm that uses two types of move, the addition/deletion of one selected variable or the swapping of one selected variable with a non selected one, as follows: @BULLET with probability , add or delete one variable by choosing at random one component in the current  and changing its value; @BULLET with probability 1, swap two elements by choosing independently at random one 0 and one 1 in the current  and changing their values.The proposed  new is accepted with probability given by the ratio of the relative posterior probabilities of new versus current modelBecause these moves are symmetric, the proposal distribution does not appear in the ratio above. In addition, the calculation of (7) can be simplified using a factorization of the marginal likelihood that allows to treat the part that involves the non-significant variables as one-dimensional, seefor the full details. The MCMC procedure results in a list of visited models,  (0) ,..., (T ) and their corresponding posterior probabilities. Variable selection can then be achieved either by looking at the  vectors with largest joint posterior probabilities among the visited models or, marginally, by calculating frequencies of inclusion for each  j and then choosing those  j 's with frequencies exceeding a given cut-off value. Finally, using the selected variables new observations are assigned to one of the G groups according to (3).
SIMULATED DATAWe first validate our approach through simulations. We consider simulated scenarios that mimic the characteristics of gene expression data, in particular the relatively small sample size with respect to the number of variables and the fact that variables exhibit correlation structure. We focus on situations where most of the variables are noisy ones, to test the ability of our method to discover relevant covariates in the presence of a good amount of noise. More in details, we generated a sample of 50 observations from a mixture of three multivariate normal densities, induced by six variables,,...,50, and where Iis the indicator function. The first 20 samples arose from the first distribution, the next 15 came from the second group and the last 15 from the third group. We then divided the observations into two sets, obtaining a training set of size 33 and a validation set of size 17. The training set was formed by 13 units from group 1, 10 from group 2 and 10 from group 3 while the validation set by 7 units from group 1, 5 from group 2 and 5 from group 3. We set the means of the normal distributions equal towhere 1 p is a unit vector of dimension p = 6. We constructed the covariance matrices of the six variables in the following way: the Page: 498 495501
F.C.Stingo and M.Vannuccielements on the diagonals were set to  2 1 = 3, 2 2 = 2 and  2 3 = 2.5. The correlation structures of the six variables were then represented by 32 grids with elements equal to 0.2 if two variables were connected and 0 otherwise. We arbitrarily connected each variable in the 32 lattice systems to either 2 or 3 other variables. This generating mechanism creates correlation also between variables not directly connected in the lattice systems. We report here results obtained by considering four different settings: in settings (i) and (ii) an additional set of s = 100 noisy variables was generated. Settings (iii) and (iv) used 1024 noisy variables. The noisy variables were generated using a linear regression model where each of the six discriminatory variables affected three noisy variables and where the covariance structure of the error terms corresponded to a 1010 (or 33 32) lattice system with correlations equal to 0.1 and variances set to 1 for settings (i) and (iii) and 2 for settings (ii) and (iv). This generating mechanism produced the following empirical correlation: in setting (i) the correlations between the noisy variables were in the range (0.42, 0.54), those between the discriminatory variables were in the range (0.55,0.80) and those between the noisy variables and the discriminatory ones in (0.39,0.59). In setting (ii), the correlations between the noisy variables were in the range (0.42,0.54), those between the discriminatory variables in (0.55,0.80) and those between the noisy variables and the discriminatory ones in (0.39,0.50). In setting (iii), the correlations between the noisy variables were in the range (0.60,0.59), those between the discriminatory variables in (0.55,0.80) and between the noisy variables and the discriminatory ones in (0.52,0.70). Finally, in setting (iv) the correlations between the noisy variables were in the range 0.60,0.59, those between the discriminatory variables in (0.55,0.80) and those between the noisy variables and the discriminatory ones in (0.52,0.64). In every setting, we permuted the columns of the data matrix X, to disperse the predictors. We set  = 3, the minimum value such that the expectation of exists, and, as suggested by, specified10, h 0 = 100 to obtain priors fairly flat over the region where the data are defined. Some care is needed in the choice of g and k 0. As suggested by, these hyperparameters should be specified in the range of variability of the data. We found that a value around the mean of the first l eigenvalues of the covariance matrix of the data, with l the expected number of significant variables, led to good results. We set g = 0.05 1 I || and k 0 = 10 4 , a value close to the mean of the remaining pl eigenvalues, and assumed unequal covariances across the groups. In an effort to show the advantages of using the MRF prior described in Section 2.2, we repeated the analysis of the four scenarios twice, the first time using the MRF prior with f = 1 and the second time using a simple Bernoulli prior on . We set the expected number of included variables to 10. For each setting, we ran one MCMC chain for 100 000 iterations, with 10 000 sweeps as burn-in. Each chain started from a model with 10 randomly selected variables. In our Matlab implementation, the MCMC algorithm runs in only 911 minutes, depending on the scenario, on an Intel Core 2 Quad station (2.4 GHz) with 4 GB of RAM. Our results suggest that the MRF prior helps in the selection of the correct variables: in all four scenarios, the posterior probabilitiesof the discriminatory variables are higher when the MRF prior is used. In addition, some of the discriminatory variables are not selected when the MRF prior is not used., in particular, shows plots of the marginal posterior probabilities of inclusion of single variables, p( j = 1|X,y), for two of the simulated scenarios. In setting (i) with the MRF prior, a threshold of 0.5 on the posterior probability results in the selection of five of the six significant variables, while a perfect selection is achieved with a threshold of 0.4. Without the MRF prior, a threshold of 0.5 on the posterior probability results in the selection of only four significant variables, while five discriminatory variables are selected with a threshold of 0.4. When calculating the posterior probabilities of class memberships for the 17 observations of the validation set, based on the selected variables, our method perfectly assigns units to the correct groups when the MRF prior is used, while unit 6 is missclassified if this prior is not used. A similar behavior was observed in scenario (ii). The method performed well also when increasing the number of noisy variables to 1024, with the only difference that the posterior probabilities were generally lower. In setting (iii), using the MRF prior we obtained a perfect selection of all six significant variables with a threshold of 0.19 on the marginal of p( j = 1|X,y), without any false positives. Without MRF prior, the best selection was obtained with a threshold of 0.14 and led to the selection of five of the six significant variables. A threshold of 0.5 led to the selection of four of the six discriminatory variables, both with and without the MRF prior, while a threshold of 0.4 led to the selection of five of the six discriminatory variables when the MRF prior is used and to the selection of four of the six discriminatory variables without the MRF prior. In the most difficult simulation scenario, setting (iv), with a threshold of 0.5 the algorithm with the MRF prior selected three significant variables without any false positive, while when the MRF is not used a threshold of 0.5 led to correctly select two significant variables, without any false positives. A third discriminatory variable was included with a threshold of 0.28.
Page: 499 495501
Bayesian selection for discriminant analysis
BENCHMARK DATASETSIn this section, we use benchmark examples for gene expression analysis to highlight the characteristics of our proposed method. We focus in particular on performances of the MRF prior (5). We first analyze the widely used leukemia data ofthat comprises a training set of 38 patients and a validation set of 34 patients. The training set consists of bone marrow samples obtained from acute leukemia patients while the validation set consists of 24 bone marrow samples and 10 peripheral blood samples. The aim of the analysis is to identify genes whose expression discriminate acute lymphoblastic leukemia (ALL) patients form acute myeloid leukemia (AML) patients. Following, we truncated expression measures beyond the threshold of reliable detection at 100 and 16 000, and removed probe sets with intensities such that maxmin 5 and maxmin 500. This left us with 3571 genes for the analysis. Expression readings were log-transformed and each variable was rescaled by its range. Because of the distributional assumptions behind discriminant analysis, in real data applications it is a good practice to check for normality of the data and apply appropriate transformations, see for example Jafari and Azuaje (2006), among others. The results we report here were obtained by specifying an MRF prior model of type (5) on  that uses the gene network structure downloaded from the public available data base KEGG. The network structure was obtained using the R package KEGGgraph of Zhang and Wiemann (2009). All the 3571 probes were included in the analysis. Note that some of the genes do not have neighbors. In our analysis, we assumed that the non-significant variables are marginally independent of the significant ones. We also set the hyperparameters toThis setting is similar to whatused in, who analyzed the same dataset using a mixture model for cluster analysis. As for the hyperparameters of the MRF prior, parameterized according to Equation (6), we set d =2.5 and f = 0.5. The choice of d, in particular, reflects our prior expectation about the number of significant variables, in this case set equal to 7.5% of the total genes analyzed, while a moderate value was chosen for f to avoid the phase transition problem. Two samplers were started with randomly selected starting models that had 10 and 2 included variables, respectively. We ran 150 000 iterations with the first 50 000 used as burn-in. We assessed concordance of the two chains by looking at a scatter plot of the marginal posterior probabilities p( j = 1|X,y) across the two MCMC chains (shown) and at the correlation coefficients between these probabilities (r = 0.95). Results we report here were obtained by pooling the outputs from the two chains together.shows the marginal posterior probabilities of inclusion of single genes according to the pooled MCMC output. A threshold of 0.85 on the marginal probability of inclusion resulted in 29 selected genes. A heatmap of the 29 selected genes is given in. This figure shows that the selected genes are able to separate the ALL patients, indexed from 1 to 20, from the AML patients, indexed from 21 to 34, with the only exception of unit 31. Indeed, the unsupervised clustering analysis represented by the dendrogram on top ofcreates a group formed by the entire set of ALL patients, plus unit 31, and other two groups formed by only AML patients, confirming that the 29 selected genes have a very good discriminatory power. In addition,shows the posterior probabilities of class memberships for the 34 units of the validation set, calculated based on the 29 selected genes. According to these probabilities, 33 of the 34 samples were corrected classified.An innovative feature of our method relies in the employment of the MRF field prior. When applying our model without the MRF prior, we noticed a slight decrease in the classification power. In particular, a threshold of 0.95 resulted in 63 selected genes, as shown in, and in the correct classification of 30 of the 34 patients of the validation set. Of the 29 genes selected with the MRF field, 26 were included in the set of 63 selected without MRF prior. This result indicates that using information on gene networks, as captured by our MRF prior, leads to an increased ability to identify genes with strong discriminatory power. Additional insights on the selected genes can be found by looking at the prior network. For example,shows the subnetwork of the KEGG network we used that includes the selected genes C5AR1 and GYPA. We notice that the two selected genes appear to be both connected to a same set of genes, including ACTA1, SLC5A2 and LAD1. Such information can be valuable for the biological interpretation of the selection results. Some of the genes selected by our method are known to be implicated with the differentiation or progression of leukemia cells. For example,have found that cyclooxygenase-2 (with corresponding gene symbol PTGS2), selected by our method with posterior probability of 0.93, increases tumorigenic potential by promoting resistance to apoptosis. Also,have highlighted the pathogenic role of the vascular endothelial growth factor (VEGF)-C, a recognized tumor lymphangiogenic factor, in leukemia via regulation of angiogenesis through upregulation of cyclooxygenase-2.have found that CD44 gene, selected with posterior probability of 0.98, is involved in the growth and maintenance of the AML blast/stem cells., who studied the mechanisms underlying the elimination of leukemic stem cells (LSCs), also identified CD44 as a key regulator of AML LSCs. Moreover, gene CyP3 (corresponding symbol PPIF) and gene Adipsin (corresponding symbol CFD), selected with posterior probability of 0.97 and 0.99, respectively, were also selected in the original analysis of. Next we analyzed the data fromon colon cancer, another benchmark for gene expression analysis. We split the 40 tumor and 22 normal colon tissues into a training set of 47 units and a validation set of 15 units. All gene expression profiles were log-10 transformed and standardized to zero mean and unit variance. We again downloaded the gene network structure from the public available data base KEGG using the R package KEGGgraph. We
F.C.Stingo and M.VannucciFor the MRF prior, we set f = 1 and the expected number of included variables equal to 10. We assumed unequal covariances across the groups. Using a threshold of 0.45, we selected 10 genes that were able to correctly classify all the units of the validation set. Most of the 10 selected genes are known to be implicated with the development of colorectal cancer. For example,found that BCL2-like 1 (BCL2L1), selected by our method with posterior probability 0.79, is of prognostic significance in colorectal cancer.reported that transforming growth factor beta receptor II gene (TGFBR2), selected with posterior probability 0.46, is expressed in tumor cell membranes of colorectal cancers andfound that this gene is mutated in most microsatellite instability-high (MSI-H) colorectal cancers. When we repeated the analysis without the MRF prior, the algorithm selected a set of 10 genes that correctly classified 14 out 15 samples. The two sets of 10 genes, selected with and without MRF, respectively, shared only a single gene. The two benchmark datasets we have analyzed have been extensively studied in the literature and similar prediction results have been obtained by other classification methods. For example, in their paperused the 50 most correlated genes to build a class predictor that incorrectly classified 5 out of the 34 samples of the test set. Dettling (2004) reports comparative results on a number of datasets, including the two we have analyzed, using several classification methods, including Boosting, random forest, support vector machine, nearest neighbor clustering and diagonal linear discriminant analysis. For his analyses, he reports the average misclassification rate calculated over 50 random splits. For the colon cancer dataset, for example, the misclassification rate achieved is around 15%, with DLDA achieving a best rate of 12.9%. To make a more direct comparison with the results obtained by, we ran our Bayesian algorithm, with the MRF prior, 50 times, using different splits of the data. Following, we assigned two-thirds of the samples to the training set and one-third to the validation set. Multiple holdout runs are not commonly adopted in Bayesian modeling, due to the impossibility of specifying the hyperparameters on a case-by-case and, in our case, to the difficulty of setting a selection criterion. With the same specification setting across all 50 splits, we obtained misclassification rates that were remarkably similar to the best techniques used in Dettling (2004). For the Leukemia dataset, we achieved an average misclassification rate of 3.9%. With the exception of only one case, where 3 units of the validation set were misclassified, the method correctly classified at least 22 out of 24 samples, with 17 of the 50 splits achieving perfect classification. For the colon cancer dataset, the average misclassification rate was 16.4%.
CONCLUSIONWe have illustrated how to perform variable selection in discriminant analysis following the Bayesian paradigm. In particular, we have considered the typical problem of classifying subjects according to phenotypes via gene expression data and have proposed prior models that incorporate information on the network structure of genes. Our method allows the classification of future samples and the simultaneous identification of the important biomarkers. Our simulation studies have shown that employing the MRF prior improves on selection accuracy. In applications to benchmark gene expression datasets, we have found that the integration of existing biological knowledge into the prior model results in an increased ability to identify genes with strong discriminatory power and aids the interpretation of the results, in addition to improving prediction accuracy.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
