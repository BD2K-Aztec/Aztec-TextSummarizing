Motivation: We address a common problem in large-scale data analysis, and especially the field of genetics, the huge-scale testing problem, where millions to billions of hypotheses are tested together creating a computational challenge to control the inflation of the false discovery rate. As a solution we propose an alternative algorithm for the famous Linear Step Up procedure of Benjamini and Hochberg. Results: Our algorithm requires linear time and does not require any P-value ordering. It permits separating huge-scale testing problems arbitrarily into computationally feasible sets or chunks. Results from the chunks are combined by our algorithm to produce the same results as the controlling procedure on the entire set of tests, thus controlling the global false discovery rate even when P-values are arbitrarily divided. The practical memory usage may also be determined arbitrarily by the size of available memory. Availability and implementation: R code is provided in the supplementary material.
IntroductionIn many fields the substantially increased scale of data available has resulted in a significant increase in the size of multiple hypotheses testing problems. In genetics, in particular, typical GWAS studies consist of 10 5  10 6 SNPs () while eQTL studies (), newly advanced methylation studies (), and imaging studies () usually start with 10 9 tests. These testing problems are hugescale as opposed to large-scale used byto describe studies consisting of hundreds to thousands of hypotheses. It is preferable to control the false discovery proportion rather than the number of false positives for a huge-scale testing problem. Therefore the FDR or the pFDR approaches are favored and both tend to offer larger, more powerful sets of results than those yielded by the conservative FWER control. These huge-scale multiple hypotheses testing problems create numerous computational challenges when many tests, say of the order 10 6 , are performed with all of the P-values of more or less equal importance. As a result some simpler testing procedures such as rigid P-value thresholds may be used that sacrifice power and correctness. Alternatively tests may be separated or chunked into smaller sets or chunks that are more computationally feasible.notes that the problem of separating hypotheses tests has not received great attention and warns of some pitfalls in chunking P-values, but focuses on grouping tests that share a biological property rather than arbitrary, computationally feasible chunks. Cai and Sun (2009) and later () propose alternative solutions to Efron's grouping problem but do not address the problem of arbitrary, computationally feasible chunking. We confront the computationally feasible chunking problem for theBenjaminiHochberg false discovery rate (). We show on data from Stranger's HapMap study () that if results from separate tests are not combined correctly, there is considerable inflation of type I error, offer an explication for this occurrence, and propose our algorithm as a solution. Consider a huge-scale testing problem of size m where our goal is to select exactly R ! 0 significant tests. Of the R significant discoveries, exactly V ! 0 tests will be false discoveries (i.e. truly nonsignificant tests that are declared significant). A common approach in multiple hypotheses testing problems is to control the family-wise error rate, FWER  PrV ! 1, the probability of selecting at least one false discovery. For huge-scale testing a more favorable alternative is to control the false discovery proportion, FDP  V=maxR; 1, the proportion of truly false tests among the significant R. Some will prefer to control the positive FDR, pFDR  EFDPjR > 0, the expectation of the FDP when significant tests are selected, while others will opt to control the false discovery rate, FDR, the expectation of the FDP, E(FDP). The FDR is always of a potentially smaller magnitude than the FWER and of the pFDR (FDR  pFDR  PrR > 0). Yet, in reality for huge-scale testing, FWER > > FDR and sometimes, pFDR % FDR. Therefore both FDR and pFDR control approaches tend to offer larger, more powerful sets of results than those that might be offered by the conservative FWER control. For a further discussion about FWER, FDR and pFDR refer to Farcomeni (2004). In huge-scale testing when the m P-values are partitioned into chunks, it is challenging to control the FWER, pFDR or FDR over the entire collection of m P-values. Controlling these error rates on a per chunk basis, if not done correctly, may interfere with the overall results by introducing more false discoveries. Although the same difficulty arises for the FDR and pFDR control (), it is easier to illustrate this for the FWER. Consider, for instance, an example of FWER control using the Bonferroni approach by collecting all P-values less than a=m. Assuming that the number m of P-values happen to be very large so that m should be divided into k chunks, each of of size m i so that m  P k i1 m i. Applying Bonferroni in chunks of size m i will tend to select more significant results than applying it over the entire set of m  P m i P-values since a=m is less than a=m i. In the case of FWER control, using a fixed bound of a=m for all the chunks is theoretically preferred but often yields no significant results. A stricter constant cut-off on all sets of tests as suggested by Dudbridge and Gusnanto (2008) for GWAS was developed based on results from simulated GWAS. However such an ad-hoc approach eliminates from the entire multiple hypotheses testing problem any knowledge of the actual significance level a used.
The BenjaminiHochberg linear step up procedure for controlling the false discovery rateThe BenjaminiHochberg Linear Step Up (LSU) procedure is designed to control the FDR at desired signficance level a (). As a result for huge-scale multiple hypotheses tests of equal importance, controlling a proportion of false discoveries, especially on the average, has increased power over procedures that control the FWER such as Bonferroni or a rigid cut-off bound such as 5  10 8 suggested for GWAS (). The larger the multiple hypotheses testing problem is the more powerful the LSU is over procedures that control the FWER. While the LSU procedure is still one of the most cited procedures, its application had required sorting all P-values in decreasing order to look for the largest P-value that satisfies a simple condition. In face of a huge-scale testing problem rather than apply LSU, some researchers had preferred to use harsh P-value cut-offs as mentioned above or to divide their huge-scale set of tests into computationally feasible smaller chunks and apply a multiple hypotheses testing procedure on each chunk selecting as the final significant results the union of the results in each of the chunks.warns of the danger in such aggregation from the perspective of pFDR, pointing out that some chunks might have a larger proportion of significant results than others, and aggregating the significant results can yield misleading estimates. Moreover different chunking of tests might yield different sets of significant results. Analysis done by different groups of populations or chromosomes may not give the same number of significant tests as analysis that is applied on equal sized subsets; for example, it is well known that chromosome 6 () has a higher proportion of significant HLA SNPs than other chromosomes. We shall show in Section 2 that sorting the P-values, arbitrary thresholds, and arbitrary aggregation of results are not necessary and do not improve computational time efficiency compared to our algorithm.
A faster algorithm for LSUOur alternative algorithm to the BenjaminiHochberg LSU, FastLSU, performs linear scans instead of sorting P-values, but takes into account the overall size of the testing problem. FastLSU tiles the LSU procedure to give one global set of results that does not differ from applying LSU to the entire set of tests. Our algorithm addresses the same objective function as the original LSU. Our approach is provably faster than the conventional approach that relies on sorting P-values. It may also be used on arbitrary chunks of arbitrary size with an arbitrary space constraint in order to return the same set of significant results as those from applying LSU to the entire set of tests. In the following section we address the difference between grouping and chunking tests and the difficulty in arbitrarily chunking tests by giving examples of inflation of type I error. In Section 3 we present FastLSU on a single set of tests and prove its equivalence to LSU, time efficiency and space efficiency. We present FastLSU on arbitrary chunks and also show its correctness and efficiency in Section 4. We offer suggestions for finalizing the report of significant tests in Section 5 and conclude with discussion in Section 6. R Code for an implementation of the algorithm is given in the supplemen tary materials.
2An exercise of three HapMap groups and their chunking for stranger's cis-eQTL study FastLSU controls the global FDR, not a family-based bound or a group-based bound. Our final results are not affected by the procedure of separating tests. This is an important distinction because chunking is arbitrary and based primarily on computational efficiency without any consideration for relationships between the hypotheses being tested. This is in contrast to group-testing procedures where for example, hypotheses may be grouped based on experimental knowledge such as all the tests from the same chromosome, the back and front parts of the brain (), or different population groups in HapMap (). In Section 3.2 we will show how the FastLSU algorithm can even improve the efficiency of a group-based controlling procedure. FastLSU is particularly suited to the current applications in genetics because we typically seek the significant set of SNPs or genes, and the family structure is usually of less importance than managing the FastLSUcomputational burden. Genetic family structures typically do not require any correction for family selection because each genetic family tends to contribute significant results of its own. (This is the case for the following example of 3 families of HapMap). For this reason in the remainder of this section, we will give motivating examples of applying FastLSU on the three groups of approximately 14  10 6 cis-eQTLs of Stranger's HapMap study () in order to demonstrate the problems with arbitrary chunking without combining the results as FastLSU does.presents an eQTL study over 4 HapMap population samples: 30 Central Europeans(CEU), 45 Chinese (CHB), 45 Japanese(JPT) and 30 trios from Nigerians(YRI). To increase power each group is analyzed separately. We follow the recommendations of the SeeQTL website () and consider the CHB and JPT together. We define cis-eQTLs as within 1 Mb upstream or downstream of a gene. Each population has a set of approximately 14 million P-values that were split into chunks of the following sizes: 1M, 900K, 800K, 700K, 600K, 500K, 250K, 100K, 50K, 25K, 10K and 5K. We contrast the differences in the number of significant results when the results are combined by taking the union of the results of LSU on each chunk versus those selected by FastLSU Algorithm 2 in. Applying the FastLSU yields 9228 significant results for CEU, 6497 for YRI and 33 507 for the CHB & JPT group. Most notable is that the results for FastLSU Algorithm 2 do not change across the chunk sizes whereas the alternative of taking the union of the results on each chunk increases not only the number of significant tests selected, but also the maximal P-value reported as the chunk size decreases. For example, applying FastLSU at level 10% for the complete chunk of approximately 14M P-values yields 9228 discoveries for the CEU. However, applying LSU on 1370 chunks of size 10K yields 16 925 significances which is equivalent to an overall FDR control of 23.75%. When 10K sized chunks are used for the CHB-JPT group, 41 587 P-values are selected as significant and this would require overall FDR control of a  15:58% For the YRI group when 10K sized chunks, there are 10 330 significant P-values and an a  21:1% would be required for this overall FDR control. This implies that performing the analysis using 13001400 chunks of size 10K rather than a single chunk inflates the type I error by 50%. While this is compelling experimental evidence, a more compelling explanation is that the union of partial orderings on subsets of a set is in general not equal to the partial ordering on the entire set. Moreover when the size of the chunks decreases, the significance interval is being divided into larger sub-intervals with each more likely containing more P-values spanning a greater range of values, so that the set of selected significant P-values will more likely contain a larger cut-off of P-values, and therefore a considerably larger value for the maximal significant P-value. The maximal value of significant P-values we obtain for the case of a single chunk is 6:7 10 5 for CEU, 4:4  10 5 for YRI and 2:5  10 4 for CHB-JPT. For chunks of the size 100K (about 13 to 14 chunks), the max P-values are slightly higher especially for the CHB-JPT group: 1:7 10 4 for CEU, 3:5  10 4 for YRI and 1:2  10 3 for CHB-JPT. For chunks of 10K size (about 130140 chunks) the maximal P-values are considerably higher still: 0.0063 for CEU, 0.0034 for YRI and 0.011 for CHB-JPT. As we mentioned, there is no need to apply any correction for group or family selection as required bysince all three groups in the HapMap example give significant results.
The FastLSU algorithmThe usual way to apply the LSU () at level 0 a 1 is to sort the P-values in descending order, p m ! p m1 !    ! p 1. Starting from the largest P-value to the smallest, we need to look for the kth largest P-value that satisfies p k < ka=m. One may view the LSU algorithm () as a search for optimal index r r  argmaxfi : p i < ia=mg:This observation motivates the following Fast Linear Step Up (FastLSU) algorithm that controls the FDR at level a:At the first step, we find 9 P-values < 0.05. That is, r 1  9. At the second step, r 2  7, 7 P-values are < 9  0:05=15  0:03. At the third step, r 3  5 P-values are < 7  0:05=15  0:2333. We stop with r 4  4 P-values that satisfy < 4  0:05=15  0:01333. The selected P-values are 0:0001; 0:0019; 0:0004; 0:0095, and the reader can check that these 4 P-values are exactly the ones selected using the original LSU P-value sorting algorithm.
The equivalence to LSU and computational efficiencyTHEOREM 1. For a significance level a, the Algorithm 1 maximizes the same objective function (1) that is used by the LSU. Therefore,the FastLSU controls the FDR at level a and gives the same selected set of significant results as would be obtained by applying the BenjaminiHochberg LSU FDR controlling procedure (). PROOF. Consider a batch of P-values which we will denote as C. Let 0 t 1 and define St : C  fp < t : p is Pvalue 2 Cg to be the number of P-values from C smaller than t. The set S ia m   consists of all P-values from C that are smaller than ia m. Hence, for the case of a single batch, it is possible to verify that the search for r  argmaxfi : i  #S ia=m  gis equivalent to looking for the largest rth P-value satisfying p r ra m , as requested by (1). THEOREM 2. The FastLSU Algorithm 1 requires O(m) time and O(m) space where m is the number of P-values to be considered. The proof to Theorem 2 is given in the Appendix A with accompanying pseudocode for procedural language implementations. The main observation behind the linear time algorithm is that it is a constant time check where, in terms of what proportion of units of size a=m, a P-value is relative to a. It is then only a single scan to count the number of P-values that fall within each range relative to a and to find the range of P-values that satisfy the LSU condition. This is not commonly how statisticians consider LSU because we are not comparing the P-values directly to each other; we are comparing them by examining the range in which they fall relative to a and only implicitly to each other. We demonstrate the linear scans used in the linear algorithm in the following example. Example 2. To demonstrate this equivalence we apply FastLSU on the example appearing in the original 1995 FDR paper () with a  0:05. Instead of sorting all 15 P-values we apply Algorithm 1 using 3 linear scans and find 4 significant P-values. Consider the 15 P-values from the example given in the original LSU paper () from Example 1. In the first linear scan, we label each P-value with what interval k, k  mp a AE  , such that the P-value is at most ka=m respectively as follows:; For P-values greater than a, a label  is used. During the same linear scan, we can also maintain counts for the number of P-values labeled with k from 1 to m  15 as follows:
Applying FastLSU over families or groups of P-values of equal relevanceEfron (2008) distinguishes between the groups of imaging P-values that arise from the front brain and the back brain, and develops an empirical Bayes set-up to combine the significant P-values from the two families.extended Efron's idea of groups into the voxel families of MRI where each set of P-values from a specific voxel of a certain location are treated as a separate, homogeneous group of relevance. For the first step each voxel group is analyzed by applying the LSU at level a. For the second step the number of significant groups is collected and LSU is performed again with another significance level a  to correct for the selection of groups. If, for example, S groups out of G are shown to have at least one significant P-value for LSU of level a, a  is set to Sa=G for each of the groups (). Given the equivalence between LSU and FastLSU, FastLSU may be used for each step in the approach of Benjamini and Bogomolov. The second step needs only be done for groups that have at least one significant result. Since only significant P-values for each group need to be considered, FastLSU may be applied to each group by using a  ). To be more precise, the complete grid of k values should be collected in each step and then the q-values should be corrected accordingly.An example of Algorithm 2 applied to the example appearing in the original 1995 FDR paper () is given in the Appendix A. There are many ways one can alter Algorithm 2. For example, P-values that do not satisfy the condition in the iterative step that preserves the LSU may be either flagged or dropped. Alternatively Algorithm 1 may be applied to each chunk independently starting with the initial value for r 0  m  m i  r 1 i and still tiling by the total number of tests, m. The advantage of this is that this may be done in parallel if desired. The results of the chunks may then be combined into a single set, if space permits, and Algorithm 1 applied to this set to return the final set of significant results. If space does not permit the results of the chunks to be combined, they may be combined up to the space limit. Then the iterative step of Algorithm 2 above may be applied to the resultant chunks starting by checking for P-values less than m 0 a=m where m 0 is the size of the union of the results.
Srga
Applying LSU on arbitrary chunks of P-valuesbe the count for the chunk at this step.Mark these P-values as significant under LSU.
FastLSUTHEOREM 3. For a significance level a, and a collection of c chunks the Algorithm 2 gives the same selected set of significant results as would be obtained by applying the BenjaminiHochberg () LSU FDR controlling procedure over the set of all P-values from the c chunks. PROOF.Suppose we have exactly c disjoint chunks of P-values C 1 ; C 2 ;. .. ; C c , with sizes m i (i  1; 2;. .. ; c) such that C  [ c i1 C i and jCj  m  P c i1 m i P-values. Let r C be the number of selected significant P-values that satisfy the LSU objective (1):Algorithm 2 can be applied to each c chunks and the last step of Algorithm 2 is to finalize the selection. Accordingly for chunk C i we search for the largest r  i that satisfy the objective:We claim thatThis implies that each of the P-values selected significant from the search for (3) within chunk C i must be selected while searching for argmaxIt is possible to verify that p k:Ci the kth P-value in chunk C i cannot be larger than, p kmmi , the k  m  m i  th P-value in the union C:In particular, by the condition (4), let p r  i  be the largest P-valueholds trivially since at most the entire set of tests may be selected as significant). To prove (5) by contradiction, let us compare between p rC  to p r  i kmmi. If we assume that, then we can define a positive integerfor which following the inequality in (7) holds,However, this is in contradiction to the condition (3) that provides p rC  < r C  a=m. In conclusion we show that applying FastLSU over the global union of P-values give the exact same selection of significant P-values. We search for r  C over the union of the results on each chunk of sizeSince the inequality (5) ensures that P-values that were not selected under the condition (4) would have not been selected under the condition (3), we conclude that r  C  r C.  THEOREM 4. The FastLSU Algorithm 2 requires O(m) time and O(m) space where m is the number of P-values to be considered. The practical memory usage may be restricted to an arbitrary limit for the largest chunk size, m . Proof for the running time and space limitations are shown in the Appendix A.
Useful tips for finalizing the report of significant P-valuesIf we assume that the R tests were declared significant by applying either the LSU or FastLSU, we offer tips for finalizing the set of significant P-values. In the remainder of this section we explain how to protect the FDR control of FastLSU against dependency structures and when such correction is actually needed. We will also explain how to compute q-values (adjusted P-values) for the final results without keeping the entire set of P-values and show how to add a set of simultaneous confidence intervals for the significant test statistics while accounting for the selection effect.
Correcting against general case of dependenceThe LSU procedure is conservative under the general type of positive regression dependence on subsets (PRDS) (), so applying the LSU at significance level of a always ensures FDR a for PRDS P-values. The PRDS class contains a larger set of structures, among them, the independent case and any positively associated P-values such as P-values obtained from a two-sided t test. Since the LSU and FastLSU are equivalent, applying the FastLSU at level a on PRDS P-values will control the FDR at level-a, as well. For other types of non PRDS dependency, such as in the case of pairwise comparisons, it is recommended to use the Benjamini Yekutieli procedure () that applies LSU using a   a=c instead of a where c  P m k1 1=k % ln m  0:5772. By the same argument the FastLSU under non PRDS will have FDR a when applied under a   a ln m0:5772. As a matter of practice, we suggest to first perform FastLSU using the significance level a. Then, if the P-values are non PRDS, correct the R selected P-values by applying FastLSU again over the single batch consisting of R P-values using a   Ra  m. This approach provides FDR a since a  < a.
How to compute q-values to a selected subset of significant testsWhen it is preferable to report q-values or adjusted P-values, we suggest how this may be done more efficiently. If we assume that R tests were selected as significant, let the P-value p R be the largest P-value satisfying p R m=R < a. All P-values larger than p R were not selected as significant since p i m=i > a for i > R and have q-values > a. Therefore it is sufficient to consider only the set of selected R P-values. The LSU q-value, q i , for the P-value, p i has the form (q i  min jR;R1...;i1 fp i m=i; q j g; for i < R q R  p R   m=R; for i  R:From this we can see that the algorithms presented by () and () are ORlogR. One needs only sort the R selected P-values in descending order and then beginning from largest P-value assign the corresponding q-value in a final linear scan recording the minimum q-value assigned thus far. The q-values will also be descending assigned in this way and there is no need to compare previous values except the minimum q-value thus far. The value for the q-value for q i will only change when it is less than the minimum seen thus far and the minimum q-values will span a range of P-values until either a sufficient number of P-values have been covered or a sufficient range in P-values have been covered. Alternatively, one can use existing procedures (such as the function p.adjust in R software or PROC MULTTEST in SAS) to compute adjusted P-values for the set of R selected P-values and multiply the results by R/m to correct them.
Confidence intervals for selected subset of significant resultsA less common approach in genetic studies is to report the significant test results by constructing a set of confidence intervals for the test statistics. While P-value is merely a measure of the magnitude of the test statistic, a confidence interval may offer the additional information about the dispersion of that magnitude. The selection adjusted confidence intervals () offer an appropriate construction that corrects against the false coverage effect of selection. For a useful example see () for how this method is used for the significant log-fold changes of RNA Microarrays.
A1. PROOFS AND CODESwhere a is the significance level and p  is the given P-value being labeled. If p  is labeled with bin k less than or equal to m, increment the count for bin k and increment current P-value count, m . If a p  has label k greater than m, it may be filtered, so no counts need to be incremented for such P-values (although they can be labeled with arbitrarily large values for k). Running time and memory: Labeling each P-value and incrementing the labeled bin count requires only constant time and a single pass through the P-values. In addition to storing the P-values, the P-value labels and bin counts also need to be stored, also requiring O(m) space each, and a variable for the current P-value count,Accumulate. In this step we find the r  the significant bin to return all P-values in bins less than or equal to this bin as significant. To do so, starting from the highest labeled bin's count, i.e. for m, keep a partial sum of the total number of P-values in the bins thus far. If the current bin's has a non-zero bin count and its value is equal to m  minus then the current partial sum, then return the current bin as r  the significant bin. Running time and memory: This step can be done in a single scan of the bin counts and only requires additional variables for the significant bin, r  and the partial sum.Return. Return as significant all the P-values that were labeled with a k less than or equal to r . Running time and memory: This requires only a single scan of the P-value labels and no additional space.
A1.2 Pseudocode for proof of theorem 2Let p_vals). Further assume that for some reason the 15 P-values are divided into two chunks. The first chunk, say C 1 , consists of the first 8 P-values:Next, we follow the last step of Algorithm 2 which is a combination step and applies on the collection of 8 P-values selected at the former steps: f0:0298; 0:0278; 0:0001; 0:0019; 0:0004; 0:0201; 0:0095; 0:0344g: All 8 selected P-values are clearly < 0.05. Second, 5 P-values < 8  0:05=15  0:0267, and third scan finds out 4 P-values < 5 0:05=15  0:0167 that also satisfy < 4  0:05=15. The selected P-values are, again, 0:0001; 0:0019; 0:0004; 0:0095.
A1.4 Proof of theorem 4Proof. [Proof of Theorem 4] For m P-values arbitrarily divided into n chunks of size m c for c  1; :::; n such that the maximum chunk size is m  , we show that the algorithm is still linear in m and never uses more than m  space.Bin. This step is as for Algorithm 1. It is important to note that the binning is done relative to m and not the size of the chunk. The only important difference is that bin sums are not maintained because of the m  space limitation. Labels need not be stored either. This step also counts the sum, m 0 , of all P-values across all groups that are less than a. Running time and memory: This requires a linear scan. A count variable can be kept for each group in order to get m 0. Accumulate. For each group, find the bin sums for the largest m  partition of bins not covered yet, i.e. find bin sums for bins m 0  j m   1 to m  j  1m  for j  1; :::; n and we do both of the following before incrementing j. Accumulate bin sums across the chunks. This can be done in a linear scan of the chunks and a single array accumulating bin sums across chunks. Finding r is as in Algorithm 1 Step 2. However, now only m  bins may be checked at a pass before needing to increment j. The subtotal of P-values counted thus far is maintained after j is incremented. Running time and memory: This step must be repeated at most n times and requires a linear scan of the data. At any point at most m  space plus several count variables are used.Return. This step is as in Algorithm 1.
FastLSU
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
V.Madar and S.Batista at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Discussion We presented an efficient algorithm to apply correctly the Benjamini Hochberg Linear Step Up FDR controlling procedure in a huge-scale testing problem. Since we have shown that our algorithm requires only linear time, our algorithm is provably not any more computationally burdensome than even using a rigid Bonferroni cut-off for control. However, unlike the rigid Bonferroni cut-off, our approach ensures the FDR control at level a and this is a more powerful alternative to controlling the FWER, especially when the multiple testing problem is of huge-scale. Our approach is also scalable to any subsetting or chunking of the overall subset of P-values. In addition we offered tips for performing the LSU over a hugescale multiple hypotheses testing problem, such as showing how to correct for dependency or how to compute q-values directly from the subset of LSU significant results. We hope that these algorithms and tips offer better insight into the huge-scale testing problem rather than just black-box solutions. We encourage altering the steps in performing Algorithm 2, for instance, either by applying it sequentially or in parallel or a mixture of both. The amount of inflated type I error we observed during the exercise on different chunk sizes of the HapMap populations strongly suggests the need for greater diligence in correctly separating hypotheses tests, whether the objective is to control the FWER, the FDR, or pFDR. This observation was also reported by Efron (2008), but a full investigation on real data with decreasing chunk sizes was not performed. Our suggested approach solves this for the case of the BenjaminiHochberg FDR. As we have alluded, a similar approach can be adopted to control correctly the EfronStorey's pFDR (Storey, 2002) and this remains open for further research. In addition the severe phenomena of inflated type I error we observed suggests more care may be required in reporting and interpreting results in genetics literature especially in the case of GWAS and eQTL studies. Namely to properly understand the significance results there is a need for consistent consideration of the algorithms or software used for controlling and separating the hypotheses tests and for recording the chunk sizes used for a study.
