Motivation: Protein residue–residue contacts continue to play a larger and larger role in protein tertiary structure modeling and evaluation. Yet, while the importance of contact information increases, the performance of sequence-based contact predictors has improved slowly. New approaches and methods are needed to spur further development and progress in the field. Results: Here we present DNCON, a new sequence-based residue– residue contact predictor using deep networks and boosting techniques. Making use of graphical processing units and CUDA parallel computing technology, we are able to train large boosted ensembles of residue–residue contact predictors achieving state-of-the-art performance. Availability: The web server of the prediction method (DNCON) is
INTRODUCTIONThe prediction of protein residueresidue contacts is seen by many as an important intermediate step for gaining traction on the challenging task of tertiary structure prediction. This idea has been spurred further recently by encouraging results that demonstrate that predicted contact information can indeed be used to improve tertiary structure prediction and effectively transform some unfolded structures into their folded counterpart (). The addition of a contact guided structure modeling category in the Critical Assessment of Techniques for Protein Structure Prediction (CASP) on a rolling basis has also aided in sparking interesting in residueresidue contact prediction. Beyond the scope of tertiary structure prediction, protein residueresidue contacts have been used in drug design (), model evaluation () and model ranking and selection (). Existing methods for residueresidue contact prediction can be broadly categorized as sequence based or template/structure based. Sequence-based methods attempt to predict contacts from the primary sequence or information that can be derived directly from the sequence. A number of these sequence-based methods use various machine learning methods such as support vector machines (), neural networks (), hidden Markov models (), Markov logic networks (), random forests () and deep architectures () to make residueresidue contact predictions. Other sequence-based approaches have used evolutionary information contained in multiple sequence alignments (MSAs) to identify possible contacts (). Methods using MSAs were among the first sequence-based approaches tried but suffered from low accuracies caused by indirect or transitive correlations. More recent developments byusing large MSAs and sparse covariance matrices have been better able to identify contacting residues from the alignments and resulted in significant improvements in accuracy. Template-/structurebased methods operate by extracting contact information from structural data. For template-based methods, this structural data comes in the form of templates (i.e. homologous proteins with known structure). Once templates have been found and aligned, residueresidue contacts are predicted using the contacts found in the structures (). Given the relatively high quality of the tertiary structure models generated by template-based techniques, residueresidue contact data is most useful when dealing with hard targets (i.e. those for which a structural template does not exists or hard to identify by sequence alone). For hard targets, the conformational search is much larger and overall model quality is usually much lower. Thus, there is great interest in high quality sequence-based residueresidue contact predictors that do not rely on template data. Such contact predictors would be able to provide additional information when generating models for hard targets. Unfortunately, recent assessments of state-ofthe-art sequence-based contact predictors routinely report average accuracies in the 2030% range, indicating a need for further development and new methods and ideas (). Here we present a new sequence-based residueresidue contact predictor using deep networks (DNs) and boosting. Our method differs from other implementations of deep architectures owing to its boosted nature, overall network architecture and training procedure. More specifically, for training, we initially use an unsupervised approach to learn patterns in the data and initialize *To whom correspondence should be addressed. parameters and then fine tune them with back propagation. Furthermore, by using the computational power of graphical processing units (GPUs) and CUDA, we were able to train large boosted ensembles of DN classifiers achieving state-ofthe-art performance.
METHODS
Datasets and evaluation metricsSeveral datasets were used to train and evaluate our residueresidue contact predictor. The primary dataset, DNCON, was formed by an advanced search of the Protein Data Bank filtering the results by 30% sequence similarity and a resolution of 02 A  (). The results from this initial search were then filtered by sequence length and disorder content, retaining sequences that were 30300 residues in length and contained fewer than 20% disordered residues (i.e. coordinates were missing for fewer than 20% of the residues in the experimentally determined structure). The resulting set of proteins was then merged with the training set from SVMcon and then filtered by three existing datasets D329, SVMCON_TEST and CASP9, which were used as evaluation sets. The filtering process ensured that the pairwise sequence identity between the merged dataset and any sequence in the evaluation sets was 25%. The end result of the search and filter process was our primary dataset, DNCON, consisting of 1426 proteins. This dataset was then randomly split into two sets: DNCON_TRAIN consisting of 1230 proteins and DNCON_TEST consisting of 196 proteins. Supplementaryillustrates the entire dataset generation and filtering process. The evaluation datasets used included D329, a set of 329 proteins used to evaluate ProC_S3 (); SVMCON_TEST, a set of 48 short to medium length proteins used to evaluate SVMcon (); CASP9, a set of 111 targets used during the ninth Critical Assessment of Techniques for Protein Structure Prediction (); CASP9_HARD, a subset of 16 targets taken from the CASP9 set that are solely composed of free modeling (FM) or free modeling/template-based modeling (FM/TBM) domains; and DNCON_ TEST. Owing to the filtering process used in the creation of our dataset, all evaluation datasets are independent (i.e. 525% sequence identity) to the training set. In this study, two amino acid residues are said to be in contact if the distance between their C atoms (C for glycine) in the experimental structure is 58 A . Short-range contacts are defined as residues in contact whose separation in the sequence is !6 and512. Likewise, medium-range contacts are residues in contact whose separation in sequence is !12 but 524 and long-range contacts are defined as having separation in the sequence !24 residues. These definitions are in agreement with recent studies and CASP residueresidue contact assessments (). A common evaluation metric for residueresidue contact predictions is the accuracy of the top L/5 or L/10 predictions where L is the length of the protein in residues. In this context, accuracy (Acc) is defined as the number of correctly predicted residueresidue contacts divided by the total number of contact predictions evaluated. We also considered the coverage (Cov) of residueresidue contact predictions, which is defined as the number of correctly predicted contacts divided by the number of true contacts. As predicting short-, medium-and long-range contacts have varying degrees of difficulty, it is common to separate predicted contacts by sequence separation and then calculate the accuracy and coverage of the top L and L/5 predictions for each range. Note that this evaluation is done on a per target basis and irrespective of the domain architecture. Estimates for the standard error for accuracy and coverage were calculated using the sample mean and sample variance of the per-target accuracies and coverages.
Restricted Boltzmann machine and deep belief networkThe general framework used for classifying residueresidue contacts was a combination of restricted Bolzmann machines (RBMs) trained to form DNs. A RBM is a two-layer network that can be used to model a distribution of binary vectors. In this model, a layer of stochastic binary nodes representing feature detectors are connected via symmetric weights to stochastic binary nodes that take on the values of the vectors to be modeled (). Conceptually, the layer of stochastic nodes corresponding to the feature detectors can be viewed as the 'hidden' or 'latent' data and the other layer of nodes as the 'visible' data. The energy of a particular configuration of this network can be defined byEv, hwhere v i and h j are the states of the i th and j th nodes, b i is the bias for the i th visible node, c j is the bias for the j th hidden node and w ji is the weight of the connection between the i th visible and j th hidden nodes. A probability can then be assigned to a configuration of visible data bywhere Z is a normalizing constant and the sum is over all possible configurations of h. Training consists of adjusting the weights of the model such that real data (e.g. training data) has a higher probability than arbitrarily chosen configurations of visible nodes. This can be done using a process called contrastive divergence, which adjusts the weights in a manner that seeks to minimize an approximation to a difference of KullbackLeibler divergences (). The use of contrastive divergence learning as opposed to maximum likelihood learning has to do with a problematic term in the gradient of average log likelihood function, which is exponential in nature and difficult to approximate. With contrastive divergence, the problematic term cancels out. Full details are provided in Hinton's presentation on training products of experts (2002). In this work, the weights in the n-th epoch of training were updated as follows:In these equations, the angle brackets represent averages over the batch. The subscripts 'data' and 'recon' are descriptors that illustrate that the first average is taken over the data and the second average over reconstructions of the data after one round of Gibbs sampling. For 54 data , p 0 j is the probability that the j-th hidden unit will be activated when driven by the data and calculated asis the sigmoid function. For 54 recon , p 1 i is the probability that the i th visible unit will be activated and is calculated aswhere h j is a binary value set to 1 with probability p 0 j. The final value to be computed is p 1 j , which is the probability that the j-th hidden unit will be activated when driven by the probabilities of the reconstructed visible nodes and calculated asIn Equations 3, 4 and 5, " is the learning rate, is the weight cost and the momentum. These parameters and update equations were used in accordance with recent findings on how to train RBMs in practice (). In our study, the learning rate " was set to 0.01 for w and 0.1 for the biases, and the weight cost was set to 0.0002. The momentum was initially set to 0.5 and after 5 epochs of training increased to 0.9. Training a RBM was done using batches of 100 training examples over 20 epochs. During training, the average free energy of the training data was compared with that of a small holdout set taken from DNCON_TEST to confirm that the RBM was not over-fitting the training data (). RBMs are particularly useful to initialize weights in DNs. This can be done by learning RBMs in a stepwise unsupervised fashion. After training the first RBM, it is applied to the training data and for each training example, the probabilities for activating each hidden node can be calculated and used to train another RBM. This process of training a RBM and then using the hidden activation probabilities as inputs to the next level can be repeated several times to create a multilayer network. For the last level, a one-layer neural network can be added. All the nodes can then be treated as real-value deterministic probabilities and the entire network can be fine tuned using a standard back propagation algorithm to adjust the parameters (). To facilitate working with large RBMs and DNs, we implemented the training and classification procedures in terms of matrix operations and used CUDAMat (), a python library that provides fast matrix calculations on CUDA-enabled GPUs. CUDA is a parallel computing platform that provides high-level access to the computing cores of certain graphics processing units (http://www.nvidia.com/object/cuda_home. html). Using CUDAMat and GPUs allowed us to train DN classifiers with on the order of 1 million parameters in under an hour.
Prediction of medium-/long-range contactsFor the prediction of medium-and long-range contacts, we trained multiple ensembles of DNs. The inputs for each DN included sequence-specific values for the residues in two windows centered around the residueresidue contact pair in question, several pairwise potentials, global features and values characterizing the sequence between the contact pair (see Section 2.7 for details). The target was a single binary value that represented the pair being in contact or not. For the size of the windows, we tried lengths of 7, 9, 11, 13, 15, 17 and 19. The overall size of the input feature vector varies from 595 (for windows 7 residues long) to 1339 (for windows 19 residues in length). The variability in the size of the input vector stems from the fact that several features used are residue specific, and consequently, as the number of residues included in the input window grows so does the size of the input vector. The overall architecture of the DN was (5951339)-500-500-350-1 (see Supplementary). Each layer was trained in a stepwise fashion as a RBM using the previously described process with the exception of the last layer, which was trained as a one-layer neural network. The entire network was fine tuned using back propagation to minimize the cross-entropy error and done over 25 epochs with mini batches of 1000 training examples (see Supplementary). To create boosted ensembles of classifiers, we trained several DNs in series using a sample of 90 000 medium-/long-range residueresidue pairs from a larger pool. The pool of training examples used came from the training dataset, DNCON_TRAIN, and consisted of all medium-and long-range contacts up to 120 residues in sequence separation and a random sample of approximately twice as many non-contacting pairs. Initially, the residueresidue pairs in the training pool were uniformly distributed and had an equal chance of being included in the training sample. After each round, the training pool was evaluated using the new classifier and the pool was reweighted based on the performance of the classifier. The probability of training data that was misclassified was increased, whereas correctly classified data had its probability of selection decreased. This was done using a variant of AdaBoost (). More specifically, let x i represent the i-th example in the training pool and y i " {0, 1} be the class of the i-th example. Also, let W t (i) be the probability of selecting the i-th example from the training pool in the t-th round of boosting and call the DN classifier trained in round t to be m t (), which outputs a value between 0 and 1. Now, after each round of boosting, W t (i) is updated via " t , t and h t () in the following manner.After 35 rounds of boosting, the final prediction for an input x i is given by H(x i ) and is a value between 0 and 1.Supplementaryshows a boosted ensemble. Additionally, we found that after several rounds of boosting, the weights of a number of particularly difficult training examples became disproportionately large and dominated the selected training data. Models trained on these sets did not generalize well and prohibited boosting beyond 10 rounds. This phenomenon is not new and has been studied elsewhere (). One solution was to reinitialize the weights on all the training examples after 7 rounds, which in effect joins bagging with several rounds of boosting. Another solution was to combine reinitializing the weights with a trimming procedure, which removed up to 30% of the hard training cases. This was achieved by removing (i.e. trimming) those training examples that were 5 times more likely to be selected than by chance.
Consensus predictions for medium-/long-range contactsIn addition to training ensembles for various fixed-length windows, we also averaged the prediction scores for contacts across ensembles. In all, there were 490 classifiers with 35 coming from each possible combination of window size (7, 9, 11, 13, 15, 17 or 19) and sampling scheme (reweighted or reweighted with trimming).
Prediction of short-range contactsFor the prediction of short-range contacts, we trained one ensemble of DNs. The input for each DN was a window 12 residues in length and the target was all short-range contacts whose residue pairs were contained in the window. In all, 400 features were used for each window and the target contained 21 predictions. The overall architecture of the DN was 400-500-500-250-21 (Supplementary). Each layer was trained as a RBM using the previous described process, and the entire network was fine tuned using back propagation to minimize the cross-entropy error and done over 20 epochs with mini-batches of 1000 training examples. As the 12 residue window slides across the sequence, most short-range residue pairings appear and are predicted multiple times and the final prediction for a residueresidue pair is calculated by averaging the predicted values across all windows. To create an ensemble of short-range predictors, we trained 30 DNs. When selecting the training set for each model, we randomly sampled 80 000 short-range windows from a pool. For the short-range predictor, the training pool consisted of all possible 12 residue windows contained in the training dataset, DNCON_TRAIN. This resulted in a pool of 198 333 training examples. The initial probability of choosing a training example was uniform and the probability of being selected was updated after each round using a procedure similar to that outlined for the medium-/longrange predictors. The only difference was in how the probabilities were updated. As the output for short-range predictions had multiple values, the probability of an example was increased in a way that was proportional to the number of incorrectly classified targets for the example. Equation 13 indicates how the weights were updated. is the percentage of the 21 short-range targets that were misclassified for a training example.The predicted value for a short-range residueresidue pair was the average of the predictions across the ensemble.
DNCONThe sum of the aforementioned components is DNCON. It is a sequencebased residueresidue contact predictor capable of predicting short-, medium-and long-range contacts. For medium-and long-range prediction, DNCON uses a consensus from the medium-/long-range boosted ensembles. For short-range predictions, DNCON uses the short-range ensemble trained on fixed windows of 12 residues. The entire boosted network is used when making residueresidue contact predictions.
Features used and generationThe features we used for training our residueresidue contact predictors are consistent with those used by many other predictors (). These included predicted secondary structure and solvent accessibility, values from the position-specific scoring matrix (PSSM) and several pre-computed statistical potentials. To obtain the PSSM, PSI-BLAST () was run for three iterations against a non-redundant version of the nr sequence database filtered at 90% sequence similarity. The secondary structure and solvent accessibility were predicted using SSpro and ACCpro from the SCRATCH suite (). The Acthely factors are scaled representations of five numerical values that characterize a residue by electrostatic charge, codon diversity, volume, polarity and secondary structure (). Finally, we mention that all features which took values outside the range from 0 to 1 were rescaled to be from 0 to 1 so as to be compatible with the input layer of the RBM.alpha helix and beta sheet residues, and 1 input for the relative position of the center of the window with respect to the sequence length (i.e. midpoint/protein length). Thus for short-range predictions, there were a total of 400 features (12  31 local features  28 global features). For medium-and long-range contacts, we used features coming from two windows centered on the residue pair in question as well as pairwise and global features. For each residue in a window, we used the same features as in the short-range residue window (i.e. predicted secondary structure and solvent accessibility, information and likelihoods from the PSSM and Acthley factors). We also encoded these features for a small window of five residues centered at the midpoint between the residue pair to be classified. For global features, we used the same global feature set as described for the short-range contact predictor (i.e. protein length, relative position of contacting pair, percentage of predicted exposed, alpha helix and beta sheet residues) and an additional set of 11 binary features to encode the separation of the residue pair in sequence (112, 1318, 19 26, 2738, 3950, 5162, 6374, 7586, 8798, 99110, and 111120). Finally, we used a number of pairwise features that depended on the residue pair, and these included Levitt's contact potential (), Jernigan's pairwise potential (), Braun's pairwise potential (), the joint entropy of the contact pair calculated from the residue frequency counts in the PSSM, the Person correlation coefficient and cosine calculated on the residue frequency counts for the pair in the PSSM and the four-order of weighted means for secondary structure and solvent accessibility for the sequence segment between the residue pair ().
RESULTS AND DISCUSSION
Performance of DNCONTo evaluate our residueresidue contact predictor, we evaluated its performance on a number of datasets and compared it with two state-of-the-art contact predictors, ProC_S3 and SVMcon. These methods were ranked as the best sequence-based residue residue contact predictors in CASP9 ().shows accuracy and coverage of the top L/5 and top L predictions for ProC_S3, SVMcon and DNCON. The predictions for SVMcon and ProC_S3 were downloaded from the official CASP website (http://predictioncenter.org/download_area/ CASP9/predictions/). The evaluation dataset for this comparison was CASP9_HARD, a set of 16 proteins that were comprised solely of domains classified as FM or FM/TBM by CASP9 assessors. The FM and FM/TBM classification indicates that template-based information was scant or difficult to obtain for these targets. As seen in, DNCON performed well on these targets, achieving state-of-the-art performance for accuracy and converge of long-range contacts when considering the top L or L/5 contact predictions. Given the comparable performances of the methods, we also examined if the methods were identifying the same contacts or if they were in some sense complementary. We discovered that while there was some overlap between prediction sets ($1830%), each method was identifying a number of unique true contacts among those selected (see Supplementary Tables S1S3 for full details). While evaluating residueresidue contacts on hard targets is arguably the best means of evaluation (i.e. it is on these types of targets that contact information may have the largest impact), the drawback is that these datasets are usually composed of a small number of targets. To increase the robustness of our evaluation, we also compared DNCON with SVMcon and ProC_S3 on larger datasets, namely SVMCON_TEST and D329.presents the accuracy and coverage of the top L/5 and top L predictions for SVMcon and DNCON on the SVMCON_TEST, the dataset used to evaluate SVMcon. Similarly,presents the results for ProC_S3 and DNCON on the D329, a dataset used to evaluate ProC_S3. The predictions for SVMcon on the SVMCON_TEST dataset were obtained by downloading SVMcon and made locally and evaluated with our pipeline. Note that the values for ProC_S3 on the D329 dataset are those reported byin their assessment of their method. Both of these evaluations show that the performance of DNCON is on par with the two state-of-the-art contact predictors (). As an additional comparison between DNCON, ProC_S3 and SVMcon, we evaluated each method on all valid CASP9 targets. Again, the predictions for SVMcon and ProC_S3 were downloaded from the official CASP website. Note that this evaluation was done using the entire protein and meant to complement the assessment technique used by CASP assessors, which evaluates predictions on a per domain basis. Tables 4 and 5 show the results of the three methods when evaluated on long-and medium-range contacts. Once again, DNCON performs competitively against SVMcon and ProC_S3. The final evaluation set used was DNCON_TEST, an evaluation set of 196 proteins from the dataset that we curated.shows the performance DNCON on our evaluation set. We also calculated the sensitivity of DNCON on DNCON_TEST at the 95% specificity rate. For long-range contacts, 38% of the true long-range contacts can be recovered at the 95% specificity rate (i.e. 95% of non contacts are recovered). For medium-range contacts, the sensitivity is 44% at the 95% specificity level. In addition to the evaluation on the entire DNCON_TEST dataset, we also evaluated our method on three subsets of DNCON_TEST. Using the CATH structure classification database (), we indentified and grouped 140 of the proteins in DNCON_TEST as mainly alpha, (29), mainly beta,(30) and alpha beta, (81). These results are summarized in. Interestingly, our method appears to have more difficulty with mainly alpha proteins than with the mainly beta or alpha beta mix. Difficulty in predicting mainly alpha proteins has been noted elsewhere () and is a starting point for future study. Finally, we assessed our contact predictor by evaluating its performance when considering neighborhoods. In this setting, a predicted contact is considered correct if there is a true residueresidue contact with AE residues for small values of (e.g. for  1, a predicted contactwould be counted correct if there were a true contact at, [iAE1,j], [i,jAE1] or).states the accuracy of DNCON on the DNCON_TEST dataset for  1 and  2. These results demonstrate that while the predictions contain some noise, which prohibits residue-level precision, in general, the contact predictions are accurate and contain a strong signal. This strong signal, particularly at the short and medium range, could provide valuable local information, which could be propagated and incorporated into the prediction of longer range contacts. Recent work by Di Lena et al., has demonstrated that propagating neighboring contact predictions can indeed increase performance (2012). Given the flexibility of the DN architecture and its ability to handle a large number of input features, this type of local contact information could easily be included for longer range predictions. This is a line of investigation we are currently pursuing for a future work. As for long-range contact predictions, these too exhibit a strong relatively accurate contact signal, which may prove more useful than the residue-specific accuracies indicate, particularly for the purposes of guiding a search through the protein conformation space. An evaluation of SVMcon and ProC_S3 on DNCON_TEST using the neighborhood criteria found that ProC_S3 and DNCON perform comparably and both outperform SVMcon (data not shown).
Value of boosting and ensemblesTo determine the value of the boosted ensembles and the consensus approach, we studied the performance of the predictions for various configurations of ensembles and across rounds of boosting.characterizes the affect of boosting on the accuracy of the top L and L/5 long-range predictions (see Supplementaryfor effect on medium-range predictions). These particular figures are of the ensemble with windows of 13 residues in length and with the reweighted sampling scheme. They show the benefit of boosting and are typical of the affect seen in other ensembles. To determine the effect of the window size on the method's performance, we evaluated the performance using ensembles comprising DNs with only one window size and reweighting scheme. It is interesting to note that while all of the ensembles perform roughly the same, there is a marked difference in the performance of the individual ensembles and the consensus prediction for top L/5 predictions. Accuracies for the individual ensembles are in the range of 0.240.28 for the top L/5 longrange predictions, whereas the accuracy of their consensus is 0.34 (Supplementary). Similarly, for the top L/5 mediumrange predictions, the accuracy jumps from the 0.32 to 0.34 range to 0.38.
CONCLUSIONIn this work, we have presented DNCON, a new method for protein residueresidue prediction. The approach is based on two concepts, boosted ensembles and DNs, which are novel in. Accuracy of the top L and L/5 long-range contact predictions for a boosted ensemble (13-win). The graph plots accuracy as a function of the number of rounds of boostingShort (  1) 0.789 0.657 0.532 Medium (  1) 0.648 0.552 0.463 Medium (  2) 0.749 0.678 0.607 Long (  1) 0.550 0.476 0.415 Long (  2) 0.638 0.578 0.552 the context of residueresidue contact prediction. When compared with the current state-of-the-art, DNCON performs favorably, achieving state-of-the-art performance in the critical area of accuracy on top medium and long range contact predictions. When allowing for less than residue level precision, the performance of DNCON is even more impressive. Given the strong contact signal present for short-and medium-range contacts and the fast flexible architecture of DNs, in the future, we plan on modifying the DNs such that they can incorporate and propagate predicted short-to medium-range contacts when making longer range predictions. We also plan on refining the parameters and network architecture used to increase performance. The method is available as a web service at http://iris.rnet.missouri.edu/ dncon/.
The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Predicting protein residueresidue contacts at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
J.Eickholt and J.Cheng at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
