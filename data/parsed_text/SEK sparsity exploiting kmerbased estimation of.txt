Motivation: Estimation of bacterial community composition from a high-throughput sequenced sample is an important task in metage-nomics applications. As the sample sequence data typically harbors reads of variable lengths and different levels of biological and technical noise, accurate statistical analysis of such data is challenging. Currently popular estimation methods are typically time-consuming in a desktop computing environment. Results: Using sparsity enforcing methods from the general sparse signal processing field (such as compressed sensing), we derive a solution to the community composition estimation problem by a simultaneous assignment of all sample reads to a pre-processed reference database. A general statistical model based on kernel density estimation techniques is introduced for the assignment task, and the model solution is obtained using convex optimization tools. Further, we design a greedy algorithm solution for a fast solution. Our approach offers a reasonably fast community composition estimation method, which is shown to be more robust to input data variation than a recently introduced related method. Availability and implementation: A platform-independent Matlab implementation of the method is freely available at http://www.ee.kth.se/ ctsoftware; source code that does not require access to Matlab is currently being tested and will be made available later through the above Web site.
INTRODUCTIONHigh-throughput sequencing technologies have recently enabled detection of bacterial community composition at an unprecedented level of detail. The high-throughput approach focuses on producing for each sample a large number of reads covering certain variable part of the 16S rRNA gene, which enables an identification and comparison of the relative frequencies of different taxonomic units present across samples. Depending on the characteristics of the samples, the bacteria involved and the quality of the acquired sequences, the taxonomic units may correspond to species, genera or even higher levels of hierarchical classification of the variation existing in the bacterial kingdom. However, at the same time, the rapidly increasing sizes of read sets produced per sample in a typical project call for fast inference methods to assign meaningful labels to the sequence data, a problem that has attracted considerable attention (). Many approaches to the bacterial community composition estimation problem use 16S rRNA amplicon sequencing where thousands to hundreds of thousands of moderate length (around 250500 bp) reads are produced from each sample and then either clustered or classified to obtain estimates of the prevalence of any particular taxonomic unit. In the clustering approach, the reads are grouped into taxonomic units by either distance-based or probabilistic methods (), such that the actual taxonomic labels are assigned to the clusters afterward by matching their consensus sequences to a reference database. Recently, the Bayesian BeBAC method () was shown to provide high biological fidelity in clustering. However, this accuracy comes with a substantial computational cost such that a running time of several days in a computing-cluster environment may be required for large read sets. In contrast to the clustering methods, the classification approach is based on using a reference database directly to assign reads to meaningful units representing biological variations. Methods for the classification of reads have been based either on homology using sequence similarity or on genomic signatures in terms of oligonucleotide composition. Examples of homology-based methods include MEGAN () and phylogenetic analysis (von). A popular approach is the Ribosomal Database Project's (RDP) classifier, which is based on a na ve Bayesian classifier (NBC) that assigns a label explicitly to each read produced for a particular sample (). Despite the computational simplicity of NBC, the RDP classifier *To whom correspondence should be addressed.  The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com may still require several days to process a dataset in a desktop environment. Given this challenge, considerably faster methods based on different convex optimization strategies have been recently proposed (). In particular, sparsity-based techniques, mainly compressive sensing-based algorithms (), are used for estimation of bacterial community composition in (). However, () used sparsity-promoting algorithms to analyze mixtures of dye-terminator reads resulting from Sanger sequencing, with the sparsity assumption that each bacterial community comprises a small subset of known bacterial species, the scope of the work thus being different from methods intended for high-throughput sequence data. The Quikr method of () uses a k-mer-based approach on 16S rRNA sequence reads and has a considerable similarity to the method (SEK: Sparsity Exploiting K-mers-based algorithm) introduced here. Explained briefly, the Quikr setup is based on the following core theoretical formulation: given a reference database D=fd 1 ;. .. ; d M g of sequences and a set S=fs 1 ;. .. ; s t g of sample sequences (the reads to be classified), it is assumed that there exists a unique d j for each s l , such that s l = d j. In general, all reference databases and sample sets consist of sequences with highly variable lengths. In particular, the lengths of reference sequences and samples reads are often different. Violation of the assumption leads to sensitivity in Quikr performance according to our experiments. Another example of fast estimation is called Taxy (), which addresses the effect of varying sequence lengths (). Taxy uses a mixture model for the system setting and convex optimization for a solution. The method referred to as COMPASS () is another convex optimization approach, similar to the Quikr method, that uses large k-mers and a divide-andconquer technique to handle large resulting training matrices. The currently available version of the Matlab-based COMPASS software does not allow for training with custom databases, so a direct comparison with SEK is not yet possible. To enable fast estimation, we adopt an approach where the estimation of the bacterial community composition is performed jointly, in contrast to the read-by-read analysis used in the RDP classifier. Our model is based on kernel density estimators and mixture density models (), and it leads to solving an under-determined system of linear equations under a particular sparsity assumption. In summary, the SEK approach is implemented in three separate steps: off-line computation of k-mers using a reference database of 16S rRNA genes with known taxonomic classification, online computation of k-mers for a given sample and then final online estimation of the relative frequencies of taxonomic units in the sample by solving an under-determined system of linear equations.
METHODS
General notation and computational resources usedWe denote the non-negative real line by R +. The ' p norm is denoted by jj:jj p , and E: denotes the expectation operator. Transpose of a vector/ matrix is denoted by (.) t. We denote cardinality and complement of a set S by jSj and S, respectively. In the computations reported in the remainder of the article, we used standard Matlab software with some instances of C code. For experiments on mock community data, we used a Dell Latitude E6400 laptop computer with a 3 GHz processor and 8 GB memory. We also used the cvx () convex optimization toolbox and the Matlab function lsqnonneg() for a leastsquares solution with non-negativity constraint. For experiments on simulated data, we used standard computers with an Intel Xeon x5650 processor and an Intel i7-4930K processor.
k-mer training matrix from reference dataThe training step of SEK consists of converting an input labeled database of 16S rRNA sequences into a k-mer training matrix. For a fixed k, we calculate k-mers feature vectors for a window of fixed length, such that the window is shifted (or slid) by a fixed number of positions over a database sequence. This procedure captures variability of localized k-mer statistics along 16S rRNA sequences. Using bp as the length unit and denoting the length of a reference database sequence d by L d , and further a fixed window length by L w L d and the fixed position shift by L p , the total number of subsequences processed to k-mers is close to b LdLw Lp c. The choice of L w may be decided by the shortest sample sequence length that is used in the estimation, assuming the reads in a sample set are always shorter than the reference training sequences. In practice, for example, we used L w = 450 bp in experiments using mock communities data. The choice of L p is decided by the trade-off between computational complexity and estimation performance. Given a database of reference training sequences D = {d 1 ,. .. , d M } where d m is the sequence of the mth taxonomic unit, each sequence d m is treated independently. For d m , the k-mer feature vectors are stored column-wise in a matrix X m 2 R
SEK modelFor the mth taxonomic unit, we have the training setwhere we used an alternative indexing to denote the lth k-mer feature vector by x ml. Letting x and C m denote random k-mer feature vectors and mth taxonomic unit, respectively, and using X m , we first model the conditional density pxjC m  corresponding to mth unit by a mixture density as follows:, x ml is assumed to be the mean of distribution p ml , and  ml denotes the other parameters/properties apart from the mean. In general, p ml could be chosen according to any convenient parametric or non-parametric family of distributions. In biological terms, ml reflects the amplification of a variable sequence region and how probable that is in a given dataset with a sufficient level of coverage. The approach of using training data x ml as the mean of p ml stems from a standard approach of using kernel density estimators [see Section 2.5.1 of (. Given a test set of k-mers (computed from reads), the distribution of the test set is modeled as follows:where we denote probability for taxonomic unit m (or class weight) by pC m , satisfying X M m=1 pC m =1. fpC m g M m=1 is the composition of taxonomic units. The inference task is to estimate pC m  as accurately and fast as possible, for which a first order moment matching approach is developed. We first evaluate the mean of x under px as follows:Introducing a new indexing nXnm; l= X m1 j=1N j +l, we can writeIn our approach, we use the sample mean of the test set. The test set consists of k-mers feature vectors computed from reads. Each read is processed individually to generate k-mers in the same manner used for the reference data. We compute sample mean of the k-mer feature vectors for test dataset reads. Let us denote the sample mean of the test dataset by 2 R 4 k 1 + , and assume that the number of reads is reasonably high such that % Ex. Then we can writeConsidering that model irregularities are absorbed in an additive noise term n, we use the following system modelUsing the sample mean and knowing X, we estimate from(3) asIn (4), ^ ! 0 means 8n; ^ n ! 0. We note that the linear setup (3) is underdetermined as 4 k 5N (in practice 4 k ( N) and hence, in general, solving(3) without any constraint will lead to infinitely many solutions. The constraints (4) result in a feasible set of solutions that is convex and can be used for finding a unique and meaningful solution. We recall that the main interest is to estimate pC m , which is achieved in our approach by first estimating and then pC m . Hence, represents an auxiliary variable in our system.
Optimization problem and sparsity aspectThe solution of (3), denoted by ^ , must satisfy the constraints in (4).Hence, for SEK, we pose the optimization problem to solve as follows:The new algorithm introduced here is referred to as OMP +;1 sek , and its pseudo-code is shown in Algorithm 1. In the stopping condition (step 7), the parameter is a positive real number that is used as a threshold, and the parameter I is a positive integer that is used to limit the number of iterations. The choice of and I is ad hoc, depending mainly on user experience.(Residual) 7: until j jj ~ jj 1  1j  or i ! I Output:Compared with the standard OMP, the new aspects in OMP +;1 sek are as follows:In Step 3 of Iterations, we only search within positive inner product coefficients. In Step 5 of Iterations, a least-squares solution ~ i with non-negativity constraint is found for ith iteration via the use of intermediate variable i 2 R i1 +. In this step, X Si is the sub-matrix formed by columns of X indexed in S i. The concerned optimization problem is convex. We used the Matlab function lsqnonneg() for this purpose. In Step 6 of Iterations, we find the least squares residual r i. In Step 7 of Iterations, the stopping condition provides for a solution that has an ' 1 norm close to one, with an error decided by the threshold. An unconditional stopping condition is provided by the maximum number of iterations I. In Step 2 of Output, the ' 1 norm of the solution is set to one by a rescaling.The computational complexity of the OMP +;1 sek algorithm is as follows.The main cost is incurred at Step 5 where we need to solve a linearly constrained QP using convex optimization tools; here we assume that the costs of the other steps are negligible. In the ith iteration X Si 2 R 4 k i + and i ( 4 k , and the complexity required to solve Step 5 is O4 k i 2  (). As we have a stopping condition i I, the total complexity of the OMP +;1 sek algorithm is within OI  4 k I 2 =O4 k I 3 . We know that optimal solution of P +;1 sek using convex optimization tools requires a complexity of ON 3 . For a setup with I54 k ( N, we can have O4 k I 3  ( ON 3 , and hence the OMP +;1 sek algorithm is typically much more efficient than using convex optimization tools directly in a highdimensional setting. It is clear that the OMP +;1 sek algorithm is not allowed to iterate beyond the limit of I; in practice, this works as a forced convergence. For both OMP +;1 sek and P +;1 sek , we do not have a theoretical proof on robust reconstruction of solutions. Further, a natural question remains on how to set the input parameters and I. The choice of parameters is discussed later in Section 3.4.
Overall system flowchartFinally, we depict the full SEK system by using a flowchart shown in. The flowchart shows main parts of the overall system and associated off-line and online computations.
Mock communities dataFor our experiments on real biological data, we used the mock microbial communities database developed in (). The database is called even composition Mock Communities (eMC) for chimeric sequence detection where the involved bacterial species are known in advance. Three regions (V1V3, V3V5, V6V9) of the 16S rRNA gene of the composition eMC were sequenced using 454 sequencing technology in four different sequencing centers. In our experiments, we focused on the V3V5 region datasets, because these have been earlier used for evaluation of the BeBAC method). The test dataset consists of 91 240 short length reads from 21 different species. The length of reads has a range between 450 and 550 bp, and the bacterial community composition is known at the species level, by the following computation performed in (). Each individual sequence of the 91 240 read sequences was aligned (local alignment) to all the reference sequences of reference database D mock known described in the Section 2.7.2 and then each read sequence is labelled by the species of the highest scoring reference sequence, followed by computation of the community composition referred to as ground truth.
2.7.2 Training datasets (Reference) We used two different databases (known and mixed) generated from the mock microbial community database (). The first database is denoted by D mock known and it consists of the same M = 21 species present among the reads described in Section 2.7.1. The details of the D mock known database can be found in Experiment 2 of (). The database consists of 113 reference sequences for a total of 21 bacterial species, such that each reference sequence represents a distinct 16S rRNA gene. Thus, there is a varying number of reference sequences for each of the considered species. Each reference sequence has a length of $1500 bp, and for each species, the corresponding reference sequences are concatenated to a single sequence. The final reference database D mock known then consists of 21 sequences where each sequence has a length of $5000 bp. To evaluate influence of new species in reference data on the performance of SEK, we created new databases denoted by D mock mixed E. Here E. A flowchart of full SEK system represents the number of additional species included to a partial database created from D mock known , by downloading additional reference data from the RDP database. Each partial database includes only one randomly chosen reference sequence for each species in D mock known and hence consists of 21 reference sequences of a length of $1500 bp. For example, with E = 10, 10 additional species were included in the reference database and consequently D mock mixed 10 contains 16S rRNA sequences of M = 21 + 10 = 31 species. Several instances of D mock mixed E were made for each fixed value of E by choosing a varying set of additional species and we also increased E from 0 to 100 in steps of 10. Note that, in D mock mixed E, the inclusion of only single reference sequence results in reduction of biological variability for each of the original 21 species compared with D mock known .
Simulated dataTo evaluate how SEK performs for much larger data than the mock communities data, we performed experiments for simulated data described below.
Test datasets (Reads)Two sets of simulated data were used to test the performance of the SEK method. First, the 216 different simulated datasets produced in () were used for a direct comparison with the Quikr method and the RDP's NBC. See [(), Section 2.5] for the design of these simulations. The second set of simulated data consists of 486 different pyrosequencing datasets constituting 4179M reads generated using the shotgun/ amplicon read simulator Grinder (). Read-length distributions were set to be one of the following: fixed at 100 bp, normally distributed at 450 AE 50 bp, or normally distributed at 800 AE 100 bp. Read depth was fixed to be one of 10K, 100K or 1M total reads. Primers were chosen to target either only the V1V3 regions, only the V6V9 regions or else the multiple variable regions V1V9. Three different diversity values were chosen (10, 100 and 500) at the species level, and abundance was modeled by one of the following three distributions: uniform, linear or power-law with parameter 0.705. Homopolymer errors were modeled using Balzer's model (), and chimera percentages were set to either 5 or 35%. As only amplicon sequencing is considered, copy bias was used, but not length bias.
2.8.2 Training datasets (Reference) To analyze the simulated data, two different training matrices were used corresponding to the databases D small and D large from (). The database D small is identical to RDP's NBC training set 7 and consists of 10 046 sequences covering 1813 genera. Database D large consists of a 275 727 sequence subset of RDP's 16S rRNA database covering 2226 genera. Taxonomic information was obtained from NCBI.
RESULTS
Performance measure and competing methodsAs a quantitative performance measure, we use variational distance (VD) to compare between known proportions of taxonomic units p=pC 1 ; pC 2 ;
Results for mock communities dataUsing mock communities data, we carried out experiments where the community composition problem is addressed at the species level. Here we investigated how the SEK performs for real biological data, also vis-a-vis relevant competing methods.
k-mers from test datasetIn the test dataset, described in Section 2.7.1, the shortest read is of length 450 bp. We used a window length L w = 450 bp and refrained from the sliding-thewindow approach in the generation of k-mers feature vectors. For k = 4 and k = 6, the k-mers generation took 21 and 48 min, respectively.
Results using small training datasetIn this experiment, we used SEK for estimation of the proportions of species in the test set described in Section 2.7.1. Here we used the smaller training reference set D mock known described in Section 2.2. The experimental setup is the same as shown in Experiment 2 of BeBAC (). Therefore, we can directly compare it with the BeBAC results reported in (). SEK estimates were based on 4-mers computed with the setup L w = 450 bp and L p = 1 bp. The choice of L p = 1 bp corresponds to the best case of generating training matrix X, with the highest amount of variability in reference k-mers. Using D mock known , the k-mers training matrix X has the dimension 4 4  121 412. For the use of SEK in such a high dimension, the QP P +;1 sek using cvx suffered of numerical instability, but OMP +;1 sek provided results in 3.17 s, leading to a VD = 0.0305. For OMP +;1 sek ; and I in algorithm 1 were set to 10 5 and 100, respectively; the values of these two parameters remained unchanged for other experiments on mock communities data presented later. The performance of SEK using OMP +;1 sek is shown in, and compared against the estimates from BeBAC, Quikr and Taxy. The Quikr method used 6-mers and provided a VD = 0.4044, whereas the Taxy method used 7-mers and provided a VD = 0.2817. The use of k = 6 and k = 7 for Quikr and Taxy, respectively, is chosen according to the experiments described in. Here Quikr is found to provide the least satisfactory performance in terms of VD. BeBAC results are highly accurate with VD = 0.0038, but come with the requirement of a computation time in the order of 430 h. On the other hand OMP +;1 sek had a total online computation time around 21 min that is mainly dominated by k-mers computation from sample reads for evaluating ; given pre-computed X and , the central inferenece (or estimation) task of OMP +;1 sek took only 3.17 s. Considering that Quikr and Taxy also have similar online complexity requirement to compute k-mers from sample reads, OMP +;1 sek can be concluded to provide a good trade-off between performance and computational demands.
Results for dimension reduction by higher shiftsThe L p = 1 bp leads to a relatively high dimension of X, which is directly related to an increase in computational complexity. Clearly, the L p = 1 bp shift produces highly correlated columns in X, and consequently it might be sufficient to use k-mers feature vectors with a higher shift without a considerable loss in variability information. To investigate this, we performed an experiment with a gradual increase in L p. We found that selecting, 0.1197, respectively. Therefore, shifts around L p = 15 bp appear to be sufficient to reduce the dimension of X, while maintaining sufficient biological variability. Hence the next experiment (in Section 3.2.4) was conducted using L p = 15 bp.
Results for mixed training datasetIn this experiment, we investigated how the performance of SEK varies with an increase in the number of additional species in the reference training database, which are not present in the sample test data. We used reference training datasets D mock mixed E described in Section 2.2, where E = 0,10,20,. .. , 100. For each non-zero E, we created 10 reference datasets to evaluate variability of the performance. The performance with one-sigma error bars is shown in. The trend in the performance curves confirms that the SEK is subjected to gradual decrease in performance with the increase in the number of additional species; the trend holds for both P +;1 sek and OMP +;1 sek. Also, being optimal the performance of QP P +;1 sek is found to be more consistent than the greedy OMP +;1 sek .
Results for simulated dataThe simulated data experiments deal with community composition problem at different taxonomic ranks and also with large size of X in (3). Owing to the massive size of X, a direct application of QP P +;1 sek is not feasible, and hence we used only OMP +;1 sek. For all results described, and I in algorithm 1 were set to 10 5 and 409, respectively.
Training matrix constructionIn forming the training matrix for D small , the K-mer size was fixed at K = 6, and the window length and position shifts were set to L w = 400 and L p = 100, respectively. This resulted in a matrix X with dimensions 4 6  109 773. For the database D large, a training matrix X with dimensions 4 6  500 734 was formed by fixing k = 6, L w = 400 and L p = 400. Calculating the matrices took $2.5 and $11 min, respectively, using an Intel i7-4930K processor and a custom C program. Slightly varying L p and L w did not significantly change the results contained incompares the mean VD error at various taxonomic ranks as well as the algorithm execution time between SEK (OMP +;1 sek ), Quikr and RDP's NBC. As shown in, using the database D large, SEK outperforms both Quikr and RDP's NBC in terms of reconstruction error and has comparable execution time as Quikr. Both Quikr and SEK have significantly lower execution time than RDP's NBC. Using the database D small (not shown here), SEK continues to outperform both Quikr and RDP's NBC in terms of reconstruction error, but only RDP's NBC in terms of execution time, as SEK had a median execution time of 15.2 min versus Quikr's 25 s. All three methods have increasing error for lower taxonomic ranks, but the improvement of SEK over Quikr is emphasized for lower taxonomic ranks.summarizes the mean VD and algorithm execution time over the second set of simulated data described in Section 2.8 for Quikr and SEK both trained on D small. Part (a) ofdemonstrates that SEK shows much lower VD error in comparison with Quikr at every taxonomic rank. However, part (b) ofshows that this improvement comes at the expense of moderately increased mean execution time. When focusing on the simulated datasets of length 100 bp, 450 AE 50 bp and 800 AE 100 bp, SEK had a mean VD of 0.803, 0.410 and 0.436, respectively. As L w was set to 400, this indicates the importance of choosing L w to roughly match the sequence length of a given sample when forming the k-mer training matrix if sequence length is reasonably short ($400 bp).Number of extra new species in reference (E) VD. For mock communities data: VD performance of SEK against increasing reference database D mock mixed E, where E = 0,10,20,. .. , 100. The left figure is for P +;1 sek and the right figure is for OMP +;1 sek. The results show that both SEK implementations are subjected to a gradual decrease in performance with the increase in the number of additional speciesSEK somewhat experienced decreasing performance as a function of diversity: at the genus level, SEK gave a mean VD of 0.467, 0.579 and 0.603 for the simulated datasets with diversity 10, 100 and 500, respectively.
Results for second set of simulated data
Remarks on parameter choice and errorsIn SEK, we need to choose several parameters: k, L w , L p , and I. Typically an increase in k leads to better performance with the fact that a higher k always subsumes a lower k in the process of generating k-mers feature vectors. The trend of improvement in performance with increase of k was shown for Quikrand we believe that the same trend will also hold for SEK. For SEK, the increase in k results in exponential increase in row dimension of X matrix and hence the complexity and memory requirement also increase exponentially. There is no standard approach to fix k, except a brute force search. Let us now consider choice of L w and L p. Our experimental results bring the following heuristic: choose L w to match the read length of sample data. On the other hand, choose L p as small as possible to accommodate a high variability of k-mers information in X matrix. A reduction in L p results to a linear increase in column dimension of X. Overall users should choose k, L w and L p such that the dimension of X remains reasonable without considerable loss in estimation performance. Finally, we consider and I parameters in Algorithm 1 that enforce sparsity, with the aspect that computational complexity is O4 k I 3 . In general, there is no standard automatic approach to choose these two parameters, even for any standard algorithm. For example, the unconstrained Lagrangian form of LASSO mentioned in section 2.4 also needs to set the parameter by user. For Algorithm 1, 0551 should be chosen as a small positive number and I can be chosen as a fraction of row dimension of X that is 4 k , of course with the requirement that I is a positive integer. Let us choose I=b  4 k c where 05 1. In case of a lower k, the system is more under-determined and naturally the enforcement of sparsity needs to be slackened to achieve a reasonable estimation performance. Hence for a lower k, we need to choose a higher that can provide a good trade-off between complexity and estimation performance. But, for a higher k, the system is less under-determined, and to keep the complexity reasonable, we should choose a lower. For mock communities date, we used k = 4 and I = 100, and hence = 100 4 4 % 0:4, and for simulated data, we used k = 6 and I = 409, and hence = 409 4 6 % 0:1. Further, it is interesting to ask what are the types of errors most common in SEK reconstruction. In general, SEK reconstructs the most abundant taxa with remarkable fidelity. The less abundant taxa are typically more difficult to reconstruct and at times each behavior can be observed: low frequency taxa missing, miss-assigned or their abundances miss-estimated.
DISCUSSION AND CONCLUSIONIn this article, we have shown that bacterial compositions of metagenomic samples can be determined quickly and accurately from what initially appears to be incomplete data. Our method SEK uses only k-mer statistics of fixed length (here k $ 4,6) of reads from high-throughput sequencing data from the bacterial 16S rRNA genes to find which set of tens of bacteria are present out of a library of hundreds of species. For a reasonable size of reference training data, the computational cost is dominated by the pre-computing of the k-mer statistics in the data and in the library; the computational cost of the central inference module is negligible, and can be performed in seconds/minutes on a standard laptop computer. Our approach belongs to the general family of sparse signal processing where data sparsity is exploited to solve underdetermined systems. In metagenomics, sparsity is present on several levels. We have used the fact that k-mer statistics computed in windows of intermediate size vary substantially along the 16S rRNA sequences. The number of variables representing the amount of reads assumed to be present in the data from each genome and from each window is thus far greater than the number of observations, which are the k-mer statistics of all the reads in the data taken together. More generally, although many bacterial communities are rich and diverse, the number of species present in, for example, the gut of one patient, will almost always be only a small fraction of the number of species present at the same position across a population, which in turn will only be a small fraction of all known bacteria for which the genomic sequences are available. We therefore believe that sparsity is a. For simulated data: comparison of SEK (OMP +;1 sek ) with Quikr on the second set of simulated data. (a) VD error averaged over all 486 simulated datasets versus taxonomic rank for SEK and Quikr trained using D small. (b) Algorithm execution time for SEK and Quikr trained using D small. Whiskers denote range of the data, vertical black bars designate the median and the boxes demarcate quantiles rather common feature of metagenomic data analysis that could have many applications beyond the ones pursued here. The major technical problem solved in the present article stems from the fact that the columns of the system matrix X linking feature vectors are highly correlated. This effect arises both from the construction of the feature vectors, i.e. that the windows are overlapping, and from biological similarity of DNA sequences along the 16S rRNA genes across a set of species. An additional technical complication is that the variables (species abundances) are non-negative numbers and naturally normalized to unity, although in most methods of sparse signal processing there are no such constraints. We were able to overcome these problems by constructing a new greedy algorithm based on OMP modified to handle the positivity constraint. The new algorithm, dubbed OMP +;1 sek , integrates ideas borrowed from kernel density estimators, mixture density models and sparsity-exploiting algebraic solutions. During the article preparation, we became aware that a similar methodology (Quikr) has been developed by. Although there is a considerable similarity between Quikr and SEK, we note that Quikr is based only on sparsityexploiting algebraic solutions, while SEK further exploits the additional sparsity assumption of non-uniform amplifications of variable regions in 16S rRNA sequences. We hypothesize that the improvement of SEK over Quikr is mainly because of the superior training method of SEK. The comparison between the two methods reported above in Figures 2, 4 and 5 shows that SEK performs generally better than Quikr. The development of two new methodologies independently and roughly simultaneously reflects the timeliness and general interest of sparse processing techniques for bioinformatics applications.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
S.Chatterjee et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
P +;1 sek : ^ = arg min  k  X
k 2 ; ! 0; jj
jj 1 =1 5 where '+' and '1' notations in P +;1 sek refer to the constraints ^ 2 R N + and jj ^ jj 1 =1, respectively. The problem P +;1 sek is a constrained least squares problem and a quadratic program (QP) solvable by convex optimization tools, such as cvx (CVX). In our assumption 4 k 5N, and hence the required computation complexity is ON 3  (Boyd and Vandenberghe, 2004). The form of P +;1 sek bears resembance to the widely used LASSO method from general sparse signal processing, mainly used for solving underdetermined problems in compressive sensing (Candes and Wakin, 2008; Chatterjee et al., 2012). LASSO deals with the following optimization problem [see (1.5) of (Effron et al., 2004)]: LASSO : ^ lasso = arg min  k  X
k 2 ; jj
jj 1 where 2 R + is a user choice that decides the level of sparsity in ^ lasso ; for example, = 1 will lead to a certain level of sparsity. A decreasing leads to an increasing level of sparsity in LASSO solution. LASSO is often presented in an unconstrained Lagrangian form that minimizes fk  X
k 2 2 +jj
jj 1 g, where decides the level of sparsity. P +;1 sek is not theoretically bound to provide a sparse solution with a similar level of sparsity achieved by LASSO when a small 51 is used. For the community composition estimation problem, the auxiliary variable defined in (2) is inherently sparse. Two particularly natural motivations concerning the sparsity can be brought forward. Firstly, consider the conditional densities for taxonomic units as shown in (1). Regarding the conditional density model for a single unit, a natural hypothesis for the generating model is that the conditional densities for several other units will induce only few feature vectors, and hence ml will be negligible or effectively zero for certain patterns in the feature space, leading to sparsity in the auxiliary variable (unstructured sparsity in ). Secondly, in most samples only a small fraction of the possible taxonomic units is expected to be present, and consequently, many pC m  will turn out to be zero, which again corresponds to sparsity in (structured block-wise sparsity in ) (Stojnic, 2010). In practice, for a highly under-determined system (3) in the community composition estimation problem with the fact that is inherently sparse, the solution of P +;1 sek turns out to be effectively sparse because of the constraint j
jj 1 =1. 2.5 A greedy estimation algorithm For SEK we solve P +;1 sek using convex optimization tools requiring computational complexity ON 3 . To reduce the complexity without a significant loss in estimation performance we also develop a new greedy algorithm based on orthogonal matching pursuit (OMP) (Tropp and Gilbert, 2007); for a short discussion of OMP with pseudo-code, see also (Chatterjee et al., 2012). In the recent literature, several algorithms have been designed by extending OMP, such as, for example, the backtracking-based OMP (Huang and Makur, 2011), and, by a subset of the current authors, the look-ahead OMP (Chatterjee et al., 2011). Because the standard OMP uses a least-squares approach and does not provide solutions satisfying constraints in (4), it is necessary to design a new greedy algorithm for the problem addressed here.
SEK at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
