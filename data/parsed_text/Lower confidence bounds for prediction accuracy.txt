Motivation: Implementation and development of statistical methods for high-dimensional data often require high-dimensional Monte Carlo simulations. Simulations are used to assess performance, evaluate robustness, and in some cases for implementation of algorithms. But simulation in high dimensions is often very complex, cumbersome and slow. As a result, performance evaluations are often limited, robustness minimally investigated and dissemination impeded by implementation challenges. This article presents a method for converting complex, slow high-dimensional Monte Carlo simulations into simpler, faster lower dimensional simulations. Results: We implement the method by converting a previous Monte Carlo algorithm into this novel Monte Carlo, which we call AROHIL Monte Carlo. AROHIL Monte Carlo is shown to exactly or closely match pure Monte Carlo results in a number of examples. It is shown that computing time can be reduced by several orders of magnitude. The confidence bound method implemented using AROHIL outperforms the pure Monte Carlo method. Finally, the utility of the method is shown by application to a number of real microarray datasets. Availability: The R computer program for forming confidence bounds is freely available for download at the URL http://dobbinke .myweb.uga.edu/RprogramAROHILloweraccuracybound.txt.
INTRODUCTIONThis article presents a novel approach to Monte Carlo simulations in high dimensions. The approach permits routines to be written in simpler and faster code using mathematical modeling. The savings comes from reducing the computational dimension of the Monte Carlo simulation from very high dimension to a much lower dimensional space. As we will show, this computational savings makes it possible to provide R functions to perform statistical analyses that previously required a compiled language such as C++. Moreover, the R programs are significantly faster and more transparent (enhancing reproducibility) than the compiled programs, because the underlying models have been streamlined. We make * To whom correspondence should be addressed. available such a program with this publication. More generally, this approach, by providing a faster method for performing simulations, can enable method developers to consider the robustness of novel procedures across a wider range of simulation scenarios than would otherwise be feasible. Monte Carlo simulations are commonly encountered in papers on high-dimensional methodologies. Perhaps the most common use of Monte Carlo simulations is to evaluate the performance characteristics of novel statistical procedures, such as the performance of classifiers based on partial least squares (), regularization methods for variable selection () or evaluation of multiple hypothesis testing error control methods (). The advantages of Monte Carlo investigations are that the truth can be known exactly, and model assumptions can be violated in systematic ways to explore the limits of robustness. Some other methodologies also use Monte Carlo simulations as part of their algorithms. This new simulation procedure is called Adequate Representation Of High dimensions In Low dimensions (AROHIL) Monte Carlo. There are two types of AROHIL Monte Carlo. The first type does not involve any resampling. The basic idea behind this type of Monte Carlo is to split the Monte Carlo simulation into two subsimulations. One simulation represents the dimension-reducing feature selection step. The second simulation represents the conditional distribution of the features given that they were selected, and can typically be carried out in a space with dimension similar to the number of features selected. The second type of AROHIL Monte Carlo does involve resampling, such as bootstrap or cross-validation. In this case, resampling creates complex inter-relationships among the resampled datasets. To capture these inter-relationships, we propose a relatively simple hierarchical model that requires generation of a single highdimensional vector, and then a series of low-dimensional vectors conditionally generated given the high-dimensional vector. To our knowledge, there has not been work to develop a general methodology along the lines presented here. Work with a similar spirit can be seen in the high-dimensional literature. For example, Venkatraman and Olshen (2007) developed a faster version of their earlier method () for performing circular binary segmentation. Monte Carlo methods for estimating the distribution of functionals in complex statistical models have a longer history, and include rejection sampling, importance sampling (e.g.), Markov chain Monte Carlo (e.g.) and related algorithms such as the Gibbs sampler (). But these methods do not achieve the reduction in the
METHODSAROHIL requires that the high-dimensional model be converted into two submodels: (i) a feature selection model and (ii) a conditional distribution in the reduced-dimensional space model. The feature selection model (i) reflects the transition from a high dimensional space to a low dimensional space. The conditional distribution model (ii) reflects the distribution of lowdimensional statistics conditional on their inclusion in the feature selection modelthat is, the distribution induced by the feature selection step. Below, we discuss examples of each for settings in which there is no resampling, and in which resampling is involved. The complexity of modeling either step will vary with the probability model and the prediction algorithm.
ApplicabilityThe computer program and related working examples presented in this article are complicated implementations of results in Dobbin (2009). The complexity may obscure the range of applicability of the method.presents some simpler contexts for AROHIL Monte Carlo. The first row of the table are the feature selection models. The second row of the table are the conditional distribution models. The columns of the table represent different multivariate normal data models. The compound covariate predictor () is the prediction algorithm. Modifications can be made to adjust for different multivariate data distributions and prediction algorithms, although these may be non-trivial in some cases. The first row ofpresents the selection models. For example, consider the cell with 'Indep. Bernoulli R.V.'s'. It indicates that independent Bernoulli random variables can be used when data are multivariate normal, with diagonal covariance matrix, and genes are selected one-at-a-time based on t-test P-values being below a threshold (such as 0.001). The success probability is the power if the gene is differentially expressed, or the significance level if the gene is not differentially expressed. On the other hand, as indicated in the cells with 'Fixed number k', if the top k features are selected for the model, where k is fixed, then the selection 'model' is deterministic and just returns the number k always. Finally, in the setting of a more complex covariance structure, the selection model needs further refinement. Nevertheless, even in the most general setting of a positive definite covariance matrix of arbitrary form (cell with 'MVN and WishartModel'), a single multivariate normal variate and a single Wishart variate are all that are required. Details are provided in the Supplementary Material. The second row ofpresents the conditional distribution models. For example, consider the cell with 'Indep truncated distn's'. Here, the covariance is diagonal and a P-value cutoff is used to select features. Weights for the CCP are t-test statistics. Thus, the distribution of the predictor in the reduced space is the same as a set of independent, truncated, non-central Student's t distributions. Non-centrality parameters can be calculated from the model parameters. Truncation points are determined by the feature selection stringency used. The distribution of the CCP cutpoint for classification can be generated in a fairly simple way. Details are described in the Supplementary Material.
Working exampleThe computer program provided with this article uses AROHIL Monte Carlo for several objectives. When no resampling is required, models are similar to the example in Column 1 of, and details are given in Section 5 of the Supplementary Material. The discussion below will focus on the more challenging context of high-dimensional resampling. The objective will be to obtain the quantiles of the distribution of the predictive accuracy estimate from leave-one-out cross-validation. This working example of resamplingbased AROHIL Monte Carlo is motivated by previous work (). In our working example, the Monte Carlo simulation data are from the homoscedastic multivariate normal modelwhere D is a diagonal covariance matrix. Let 2 j be the j-th element of  x  y , and  2 j the j-th diagonal element of D. Features are selected with pooled variance t-test P-values below a threshold . Selected features are given weights equal to the pooled variance t-test statistic, and unselected feature are given weight zero. LetLLet LetL be the p1 vector of weights. Let w be a p1 future observation. The classification rule iswhere 1 A is the indicator function of event A, and C x is the class of x's and C y the class of y's.Page: 3131 31293134
Accuracy bounds by AROHIL
AROHIL Monte Carlo models for resampling-based feature selectionThe feature selection step in the context of traditional Monte Carlo resampling requires generating data from the probability model X  F where X is np (Supplementary Figs S3 and S4). The next step is to resample from X to produceXproduce produceX which ispisisp. For example, in LOOCVLOOCVLOOCV = n1; in bootstrap, usuallyusuallyusually = n. The T MC = g(  X) is a vector of statistics used for feature selection, andSand andS MC = h(  T MC ) a binary vector of feature selection indicators. Then, the same dataset is resampled from multiple times, denoted r times, producingSproducing producingS 1 MC ,...,  S r MC ; for example, in LOOCV r = n; in bootstrap, r = B where B is the number of bootstrap samples. Ideally, the AROHIL approach will produce a set of indicator vectorsS vectors vectorsS 1 AROHIL ,...,One way to do this is to generate first Let B full be a vector of statistics for feature selection based on the full dataset. Consider the following hierarchical model,if the distribution functions F B and F T (|B) can be derived from the model, then this can ensure Equation (2) is satisfied. For the working example, the backbone vector B full is the P-dimensional vector of pooled T statistics from the full dataset. The distribution of the each element of B full is independent: Student's t with n2 degrees of freedom and non-centrality parameter calculated from the model parameters. Therefore, generating B full is straightforward. Let  X (i) and D (i) be the mean and pooled covariance estimates when sample i is left out from the X's. It is shown in the Supplementary Material that for large n we have the approximation,Then the powers can be calculated for feature g bywhere B full,g is the g-th element of B full. Finally, each S r AROHIL |B full is a vector of independent (conditional on B full ) Bernoulli random variables, where the g-th entry has success probability q g .
AROHIL Monte Carlo models for resampling-based conditional distributionsAfter gene selection, traditional Monte Carlo works in the reduced dimensional space. Let k(r) be the dimension of the reduced space on resampled dataset r, and T MC,k(r) the vector of statistics in the reduced dimensional space. The objective function of interest is). AROHIL Monte Carlo generates T AROHIL,k(r) directly from a probability model. Then Q AROHIL,k(r) = h(T AROHIL,k(r) ) and ideally oneLet B full be a vector of statistics for feature selection based on the full dataset. Then AROHIL modeling can use the hierarchical model,In the working example, B full is the vector of pooled variance t-statistics. The gene selection model reduces the dimension of the space using B full. The elements of Q i AROHIL are thenLthen thenL k elements, plus a classification cutpoint. ThTh L k elements are generated from truncated Student's t distributions with n3 degrees of freedom and non-centrality parameter calculated from the model. (Another approach would be to generate th L k elements by adding noise to the corresponding elements of B full. But this led to computational problems due to the fact that the sum must be truncated so as to ensure coherence with the selection model.) This leads to the approach that Q i AROHIL is generated by:where truncMVT (a,b,c) is a vector of independent Student T random variables, truncated away from zero at a, with non-centrality vector b, and with degrees of freedom c. See Section 5 in Supplementary Material for further details.
RESULTSAROHIL Monte Carlo was applied in multiple places to the algorithm of Dobbin (2009). Briefly, the method of Dobbin (2009) constructs a lower confidence bound for the true prediction accuracy of a classifier developed on high-dimensional data. This bound provides an estimate of the variability in the leave-one-out estimate of prediction accuracy which is otherwise problematic to assess. The motivation was to convert the method of Dobbin (2009) from a set of C++ programs, to a single R program, while at the same time reducing the computation time. We first compare pure Monte Carlo to AROHIL Monte Carlo computation times in some simple settings. Then we turn to the implementation of the method of Dobbin (2009) and analyze some of the results to validate the AROHIL method, and to show how intermediate steps of AROHIL Monte Carlo algorithms can be checked. We first performed a set of simulations to benchmark the computational savings of AROHIL Monte Carlo compared with traditional Monte Carlo on some simple examples. Results are presented in. As can be seen from the table, computational costs in high dimensions can be reduced several orders of magnitude by using AROHIL Monte Carlo instead of traditional Monte Carlo. On the fourth row of the table, representing a 5000 dimensional space, the computation time is reduced from over a day to under a minute. Also note that the estimates from the two methods are practically identical. Now we turn to the AROHIL program implementation. Three key intermediate steps in the application of the AROHIL method to the algorithm of Dobbin (2009) are as follows:(1) Generate the distribution of cutpoints used for classification of samples.(2) Generate mean accuracies corresponding to a particular highdimensional Mahalanobis distance between the class means.of the parameters, with  = 0.001,  = 1, P dimensions, n = 60, using CCP predictor over 500 simulations. When Sim = Resampling, simulations are estimating the 90th percentile of the leave-one-out cross-validated prediction accuracy distribution for fixed values of the parameters, with  = 0.001,  = 1, P dimensions, n = 60, using CCP prediction over 1000 simulations. No Resamplings done in R on a 32-bit operating system, Resamplings done in R on a 64 bit operating system. No Resampling estimates include standard deviations in parentheses.
K.K.Dobbin and S.Cooke(3) Generate the lengths of predictors,, coverage probabilities are quite close to nominal over a wide range of settings, and appear to improve over Dobbin (2009) by being less conservative. The AROHIL method was used to evaluate coverage probabilities in the presence of violations of model assumptions. Results are shown in. As can be seen in the table, the coverage probabilities do break down in extreme cases, particularly for very heavy tailed distributions, such as the Student's t distribution with 1 degree of freedom (where the variance is infinite). But overall the method is quite robust. In, the method was applied to five datasets evaluated into construct lower confidence bounds for prediction accuracy. Datasets were downloaded from the BRB Array Tools data archive (). Note that for three of the five datasets, a 90% lower confidence bound does not contain 50%, indicating better than chance classification. The more conservative 97.5% lower bound is above 50% for two of the datasets. This is in contrast to, in which all their 95% two-sided intervals (equivalent to our one-sided 97.5% interval) contained 50%. Importantly, these two datasets () have found supporting evidence in subsequent publications (), which seems to suggest that the method ofis overly conservative compared with our AROHIL Monte Carlo.
DISCUSSIONWe have presented a mathematical modeling approach to speed up high-dimensional Monte Carlo simulations by reducing the effective dimension of the space in which the simulations are performed. We have described in a general way how this approach can be usedin the case of simple Monte Carlo simulations, and also Monte Carlo simulations that require resampling, such as bootstrap or crossvalidation. The modification for the resampling setting is achieved by constructing a hierarchical model for which the distributions of the functionals of interest match (or approximately match) the pure Monte Carlo distributions. This new method is called AROHIL, and can enable complex and slow high-dimensional simulations to be converted into simpler and much faster low-dimensional simulations. We have discussed how this method can be used to improve robustness evaluations and to disseminate software. As an example, we are disseminating an AROHIL program with this article, and have presented a robustness evaluation of this previously published method. In the discussion below, we discuss AROHIL Monte Carlo generally first, and then the implementation program provided in this article. We have discussed one detailed example of how high-dimensional leave-one-out cross-validation Monte Carlo can be converted into an AROHIL Monte Carlo. Generalizing this to other cross-validations, such as 10-fold cross-validation, is straightforward. Bootstrapping by AROHIL would require a further modification. We showed in this article that AROHIL for cross-validation is performed by calculating the distribution of a backbone vector of statisticsis the effect size for individual differentially expressed features.  is the correlation parameter for CS and AR(1). ' a" is the mean true accuracy over all simulations. '90% LB Coverage' is the coverage probability of 90% lower confidence bound, using either AROHIL or an exact binomial confidence interval constructed (naively/incorrectly) from the LOOCV accuracy estimate.Applications to real datasets used in.  a loocv is the leave-one-out cross-validation accuracy. Dim is the number of features, n 1 and n 2 are the number from each class. '90% LB' is the 90% lower confidence bound computed by AROHIL Monte Carlo; and similarly '97.5% LB' is a 97.5% LB, comparable to the 95% twosided intervals used in. For thedataset, the outcome is survival status at 3 years. For the van'tdataset, outcome is 5-year metastases-free survival. For thedataset, outcome is survival status. For thedataset, outcome is survival status. For thedataset, outcome is survival status. For all datasets, the significance level for gene selection was  = 0.001. that represents the full dataset, then calculating the conditional distribution of key cross-validation statistics when a sample is left out. For bootstrapping, an extra level would need to be added to the hierarchical model that would represent the overlap pattern between the bootstrap samples. This pattern could be represented by a simple multinomial model with probability 1/n on each of the n samples for each of the bootstrap draws (sampling with replacement). Then the conditional distribution given the backbone vector and the pattern can be derived in a straightforward way and used to generate the bootstrap sample. We have discussed that sometimes AROHIL models will require approximations to the pure Monte Carlo distribution. Importantly, such approximations must be checked carefully to ensure that they are true to the original model. On the other hand, it does not seem reasonable to 'throw the baby out with the bathwater' and abandon AROHIL Monte Carlo when any approximations are required. In many cases, these approximations are straightforward to check over the range of simulation settings that are of interest. We have termed the dimension reduction step of AROHIL as adequate, and not attempted here to define this idea exactly. Dimension reduction could be based on more general notions such as sufficiency. A potential area of future research is to find a more formal approach to the dimension reduction step which would establish that the statistics used by the AROHIL Monte Carlo are capturing all the key aspects of the pure Monte Carlo. A potential critique of the AROHIL approach is that it requires some work to build the mathematical models used to reduce dimension. While it is true that this method requires some extra work, which is not generally worth the trouble in lower dimensional settings, the computational savings in high dimensions is so large that it can not only be worthwhile but also critical. Furthermore, very complex high-dimensional procedures can be challenging to implement, and thoroughly checking for coding bugs, information leak or inadvertent neglect of specification of all parameters and assumptions, can be fraught with difficulties. An important aspect of AROHIL is that implementation is simplified, i.e. the added complexity of the mathematics is often more than compensated for by the greater simplicity and transparency of the computer code. We argue that this results in a cleaner and overall simpler procedure than traditional brute force Monte Carlo, where any errors are often buried in long computer code scripts. We have found that the AROHIL Monte Carlo approach results in very short and simple code compared with pure Monte Carlo. For example, the R script we are providing with this publication is much shorter and simpler than the original code from Dobbin (2009), consisting of multiple C++ programs and steps to integrate the outputs together. The resulting simplification of the code is likely to greatly enhance reproducibility of high-dimensional studies, which has been a continuing challenge to this area. The accompanying AROHIL program is implemented with one informative feature, which was used in all the coverage probability simulations in this article. The program is also available with a userselected number of features. The number of informative features has relatively small effect on the confidence interval bound, and the number of informative features is unknown. Hence, it is preferable to have a program in which the user does not have to come up with this unknown quantity. See Section 6 in Supplementary Material for the table of simulation results showing the stability of bounds across different numbers of informative features. An alternative approach would be to search over different possible numbers of informative features to find a worst-case scenario setting, resulting in more conservative confidence bounds. The accompanying AROHIL program is implemented with a diagonal covariance matrix. This is done not because the true covariance for high-dimensional data is likely to be diagonal, but because it is generally not possible to estimate the covariance matrix
Accuracy bounds by AROHIL
K.K.Dobbin and S.Cookewell in high dimensions. Covariance matrices may be estimated with a shrinkage estimate of the formshrinkform formshrink = wDiag(  )+(1w)  (), as was done for. An area of potential future work is to use covariance estimates to better tune this method to a given set of data. Finally, there are a few additional comments on the program implementation provided in this article. This program provides a lower confidence bound on prediction accuracy, taking as input the observed leave-one-out cross-validated accuracy, the dimension of the feature space, the number of differentially expressed features and the stringency used for feature selection. This program implements the method of Dobbin (2009), which is a Monte Carlo-based method that assumes a multivariate normal distribution. While the simulations presented in this article and Supplementary Material suggest that the method is quite robust to model violations, it should be noted that the gold standard in high dimensions is generally considered to be non-parametric resampling-based methods. For example,have developed a bootstrap method for constructing confidence intervals for prediction accuracy. This bootstrap method could serve as a more robust check on the interval constructed with the AROHIL program provided here. One can also note that in evaluating whether a classifier is statistically significantly better than chance, permutation tests () are probably more appropriate than confidence-boundbased approaches. One should also not use this method in a vacuum, but note that previous work has been done. For example,have presented sample size guidelines for studies that may indicate whether one should expect a confidence bound to be reasonably tight. Also, if the class prevalences are highly imbalanced (e.g. 90% from one class and 10% from the other), then overall classification accuracy is probably not as important as other quantities, such as positive or negative predictive values. See Dobbin and Simon (2011) for discussion.
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
