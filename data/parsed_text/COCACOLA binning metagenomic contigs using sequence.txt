Motivation: The advent of next-generation sequencing technologies enables researchers to sequence complex microbial communities directly from the environment. Because assembly typically produces only genome fragments, also known as contigs, instead of an entire genome, it is crucial to group them into operational taxonomic units (OTUs) for further taxonomic profiling and down-streaming functional analysis. OTU clustering is also referred to as binning. We present COCACOLA, a general framework automatically bin contigs into OTUs based on sequence composition and coverage across multiple samples. Results: The effectiveness of COCACOLA is demonstrated in both simulated and real datasets in comparison with state-of-art binning approaches such as CONCOCT, GroopM, MaxBin and MetaBAT. The superior performance of COCACOLA relies on two aspects. One is using L 1 distance instead of Euclidean distance for better taxonomic identification during initialization. More importantly, COCACOLA takes advantage of both hard clustering and soft clustering by sparsity regularization. In addition, the COCACOLA framework seamlessly embraces customized knowledge to facilitate binning accuracy. In our study, we have investigated two types of additional knowledge, the co-alignment to reference genomes and linkage of contigs provided by paired-end reads, as well as the ensemble of both. We find that both co-alignment and linkage information further improve binning in the majority of cases. COCACOLA is scalable and faster than CONCOCT, GroopM, MaxBin and MetaBAT.
IntroductionMetagenomic studies aim to understand microbial communities directly from environmental samples without cultivating member species (). The next-generation sequencing technologies allow biologists to extract genomic data with unprecedented high resolution and sufficient sequence depth, offering insights into complex microbial communities even including species with low abundance (). To further investigate the taxonomic structure of microbial samples, assembled sequence fragments, also known as contigs, need be grouped into operational taxonomic units (OTUs) that ultimately represent genomes or significant parts of genomes. OTU clustering is also called binning (or genomic binning), serving as the key step toward taxonomic profiling and downstream functional analysis. Therefore, accurate binning of the contigs is an essential problem in metagenomic studies. Despite extensive studies, accurate binning of contigs remains challenging for several major reasons, including chimeric assemblies owing to repetitive sequence regions within or across genomes, sequencing errors or artifacts, strain-level variation within the same species, etc. () The currently available binning methods can be broadly categorized into classification and clustering approaches. Classification approaches are 'taxonomy dependent', that is, reference databases are needed for the assignment from contigs or reads to meaningful taxons. The classification is either based on homology owing to sequence identity, or genomic signatures such as oligonucleotide composition patterns and taxonomic clades. Homology-based methods include MEGAN (), which assigns reads to the lowest common taxonomic ancestor. Examples of genomic signature-based methods include PhyloPythia () and Kraken (), which are composition-based classifiers and naive Bayesian classifier (), a clade-specific approach. In addition, hybrid methods are available to take both alignment and composition-based strategy into consideration, such as PhymmBL () and SPHINX (). In comparison, clustering approaches are 'taxonomy independent', that is, no additional reference databases or taxonomic information is needed. These approaches require similarity measurements from GC content, tetra-mer composition () or Interpolated Markov Models (), to contig coverage profile (). Recently, several methods have been developed to bin contigs using the coverage profiles of the contigs across multiple metagenomic samples (). Here the coverage of a contig is defined as the fraction of reads mapped to the contig in a sample. The idea is that if two contigs are from the same genome, their coverage profiles across multiple samples should be highly correlated. These methods can be further improved by integrating coverage profiles with the sequence tetra-mer composition of the contigs (). Among these methods, GroopM () is advantageous in its visualized and interactive pipeline. On one hand, it is flexible, allowing users to merge and split bins under expert intervention. On the other hand, in the absence of expert intervention, the automatic binning results of GroopM is not as satisfactory as CONCOCT (). CONCOCT () makes use of the Gaussian mixture model (GMM) to cluster contigs into bins. Also, CONCOCT provides a mechanism to automatically determine the optimal OTU number by variational Bayesian model selection (). MetaBAT () calculates integrated distance for pairwise contigs and then clusters contigs iteratively by modified K-medoids algorithm. And MaxBin () compares the distributions of distances between and within the same genomes. In this article we present COCACOLA, a general framework for contig binning incorporating sequence COmposition, CoverAge, CO-alignment and paired-end reads LinkAge across multiple samples. By default, COCACOLA uses sequence composition and coverage across multiple samples for binning. Compared with recent approaches such as CONCOCT, GroopM, MaxBin and MetaBAT, COCACOLA performs better in three aspects. First, COCACOLA reveals superiority with respect to precision, recall and Adjusted Rand Index (ARI). Second, COCACOLA shows better robustness in the case of varying number of samples. COCACOLA is scalable and faster than CONCOCT, GroopM, MaxBin and MetaBAT. In addition, the COCACOLA framework seamlessly embraces customized knowledge to facilitate binning accuracy. In our study, we have investigated two types of knowledge, in particular, the coalignment to reference genomes and linkage between contigs provided by paired-end reads. We find that both co-alignment and linkage information facilitate better binning performance in the majority of the cases.
Materials and methods
Problem formulationA microbial community is composed of a set of OTUs at different abundance levels, and our objective is to put contigs into the genomic OTU bins from which they were originally derived. OTUs are expected to be disentangled based on contigs comprising either the discriminative abundance or dissimilarity among sequences in terms of l-mer composition. The rationale of binning contigs into OTUs relies on the underlying assumption that contigs originating from the same OTU share similar relative abundance as well as sequence composition. Formally, we encode the abundance and composition of the k-th OTU by a M  V dimensional feature vector, W k ; k  1; 2;    ; K, where M is the number of samples, V is the number of distinct l-mers and K is the total OTU number. Specifically, W mk represents the abundance of the k-th OTU in the m-th sample, m  1; 2;    ; M, respectively. And W Mv;k stands for the l-mer relative frequency composition of the k-th OTU, v  1; 2;    ; V. Similarly, the feature vector of the n-th contig is denoted as X n. Let H kn be the indicator function describing whether the n-th contig belongs to the k-th OTU, i.e. H kn  1 means the n-th contig originating from the k-th OTU and H kn  0 otherwise. Therefore, X n can be represented as:where N is the number of contigs. Equation (1) can be further written into the matrix form: X % WH s:t: W ! 0; H 2 0; 1 f g KN ; kH n k 0  1where W  W 1 ; W 2 ;    ; W K   is a M  V  K non-negative matrix with each column encoding the feature vector of the corresponding OTU. And H  H 1 ; H 2 ;    ; H N   is a K  N binary matrix with each column encoding the indicator function of the corresponding contig. kH n k 0  P K k1 H kn  1 ensures the n-th contig belongs exclusively to only one particular OTU. The matrices W and H are obtained by minimizing a certain objective function. In this article we use Frobenius norm, commonly known as the sum of squared error: arg minwhere H serves as a coefficient matrix instead of an indicator matrix. In the scenario of Equation (4), W k , the feature vector of the k-th OTU represents the centroid of the k-th cluster. Meanwhile, each contig X n is approximated by a weighted mixture of clusters, where the weights are encoded in H n. In other words, relaxation of binary constraint makes the interpretation from hard clustering to soft clustering, where hard clustering means that a contig can be assigned to one OTU only, while soft clustering allows a contig to be assigned to multiple OTUs. It has been observed that by imposing sparsity on each column of H, the hard clustering behavior can be facilitated (). Therefore, Equation (4) is further modified through the Sparse Non-negative Matrix Factorization form ():where k  k 1 indicates L 1-norm. Owing to non-negativity of H, kH n k 1 stands for the column sum of the n-th column vector of H. The parameter a > 0 controls the trade-off between approximation accuracy and the sparseness of H. Namely, larger a implies stronger sparsity while smaller value ensures better approximation accuracy.
Feature matrix representation of contigsSimilar to CONCOCT (), each contig longer than 1000 bp is represented by a M  V dimensional column feature vector including M dimensional coverage and V dimensional tetra-mer composition. The coverage denotes the average number of mapped reads per base pair from each of M different samples. While the tetra-mer composition denotes the tetra-mer frequency for the contig itself plus its reverse complement. Owing to palindromic tetra-mers, V  136. Adopting the notation of CONCOCT (), the coverage of all the N contigs is represented by an N  M matrix Y, where N is the number of contigs of interest and Y nm indicates the coverage of the n-th contig from the m-th sample. Whereas the tetra-mer composition of the N contigs are represented by an N  V matrix Z where Z nv indicates the count of v-th tetra-mer found in the n-th contig. Before normalization, a pseudo-count is added to each entry of the coverage matrix Y and composition matrix Z, respectively. As for the coverage, a small value is added, i.e. Y 0 nm  Y nm  100=L n , analogous to a single read aligned to each contig as prior, where L n is the length of the n-th contig. As for the composition, a single count is simply added, i.e. Z 0 nv  Z nv  1. The coverage matrix Y is first column-wise normalized (i.e. normalization within each individual sample), followed by row-wise normalization (i.e. normalization across M samples) to obtain coverage profile p. The row-wise normalization aims to mitigate sequencing efficiency heterogeneity among contigs.The composition matrix Z is row-wise normalized for each contig (i.e. normalization across M tetra-mer count) to obtain composition profile q:The feature matrix of contigs is denoted as X  p q   T , as the combination of coverage profile p and composition profile q. To be specific, X is a M  V  N non-negative matrix of which each column represents the feature vector of a particular contig.
Incorporating additional knowledge into binningWe consider two types of additional knowledge that may enhance the binning accuracy (). One option is paired-end reads linkage. Specifically, a high number of links connecting two contigs imply high possibility that they belong to the same OTU. Because the linkage may be erroneous owing to the existence of chimeric sequences, we keep linkages that are reported through multiple samples. The other option is co-alignment to reference genomes. That is, two contigs mapped to the same reference genome support the evidence that they belong to the same OTU. We encode additional knowledge by an undirected network in the form of a non-negative weight matrix A, where A nn 0 quantifies the confidence level we believe the n-th contig and the n 0-th contig to be clustered together. Based on the aforementioned matrix A, a network regularization item is introduced to measure the coherence of binning ():where Tr indicates the matrix trace, the sum of items along the diagonal. D denotes the diagonal matrix whose entries are column sums (or row sums owing to symmetry) of A, i.e. D nn  P N n01 A nn 0. The Laplacian matrix () is defined as L  D  A. With convention we use normalized Laplacian matrix instead, that is, L  D 1=2 LD 1=2  I  D 1=2 AD 1=2 I  A. By incorporating the network regularization in, the objective function in Equation (5) changes to the following form:where the parameter b > 0 controls the trade-off of belief between unsupervised binning and additional knowledge. Namely, large b indicates strong confidence on the additional knowledge. Conversely, small b puts more weight on the data. To use multiple additional knowledge sources together, a combined Laplacian matrix is constructed as a weighted average of individual Laplacian matriceswhere each positive weight a d reflects the contribution of the corresponding information. For simplicity, weights are treated equally in the article.
Optimization by alternating non-negative least squaresAmong comprehensive algorithms to solve Equation (9), the multiplicative updating approach () is most widely used. Despite its simplicity in implementation, slow convergence is of high concern. This article adopts a more efficient algorithm with provable convergence called alternating non-negative least squares (ANLS) (). ANLS iteratively handles two nonnegative least square subproblems in Equation (10) until convergence. The ANLS algorithm is summarized in Algorithm 1.We solve Equation (10a) by block coordinate descent, that is, we divide Equation (10a) into N subproblems and minimize the objective function with respect to each subproblem at a time while keeping 5 the rest fixed:where the matrix H old denotes the value of H obtained from the previous iteration. Following Jacobi updating rule, we combine N subproblems in Equation (11) into the matrix form:where 0 1N is a N dimensional row vector of all 0, e 1K is a K dimensional row vector of all 1.
Initialization of W and HNote that we need to initialize W and H as the input to Algorithm 1. A good initialization not only enhances the accuracy of the solution, but facilitates fast convergence to a better local minima as well (). We initialize W and H by K-means clustering, namely, W is set to be the K-means centroid of X with each column W k corresponding to the feature vector of the k-th centroid. Meanwhile, H is set to be the indicator matrix encoding the cluster assignment. The distance measurement contributes crucially to the success of binning. Ideally, a proper distance measurement exhibits more distinguishable taxonomic difference. The traditional K-means approach takes Euclidean distance as default measurement to quantify closeness. However, as for the coverage profile,shows L 1 distance produces more reasonable binning results than Euclidean and correlation-based distances. As for the composition profile, L 1 distance also reveals superiority over Euclidean and cosine distances (). Therefore, our method adopts Kmeans clustering with L 1 distance. Once preliminary K-means clustering is achieved, we eliminate suspicious clusters with few contigs using the bottom-up L Method (). Performance comparisons with respect to L 1 and Euclidean distance are given in the supplementary material.
Parameter tuningWe have two parameters a; b to be tuned in our algorithm. Traditional cross-validation-like strategy demands searching through a two dimensional grid of candidate values, which is computationally unaffordable in the case of large datasets. Instead, we first search a good marginal a value by fixing b  0. After that, a one-dimensional search is performed on a range of candidate b values while keeping a fixed. In our implementation, when b  0, a is approximated by the regression of the corresponding Lagrange Multipliers from N constrained problems argmin Hn!0 kX  WHThe resulting a is denoted by a . Then we run the algorithm with respect to each candidate b and fixed a  a  , resulting in corresponding binning results with various cluster number. Notice that traditional internal cluster validity indices are only applicable on the basis of fixed cluster number scenario (), such as Sum of Square Error and Davies-Bouldin index (). To be specific, the indices have the tendency toward monotonically increase or decrease as the cluster number increases (). We tackle the impact of monotonicity by adopting TSS (Tang-Sun-Sun) minimization index (), that is, we choose the candidate b with minimum TSS value, recorded as b . Then we can solve Equation (9) by using a  ; b   as selected regularization parameters.
Post-processingThe resulting binning obtained from Algorithm 1 may contain clusters that are closely mixed to each other. Therefore, we define separable conductance as an effective measurement to diagnose the coupling closeness of pairwise clusters, so as to determine whether to merge them. Namely, we consider each cluster as having a spherical scope centered at its centroid. To be robust against outliers, the radius is chosen as the third quartile among the intra-cluster distances. The separable conductance between the c 1-th cluster and the c 2-th cluster, sepc 1 ; c 2 , is defined as the number of contigs from the c 1-th cluster also included in the spherical scope of the c 2-th cluster, divided by the smaller cluster size of two. Intuitively, the separable conductance exploits the overlap between two clusters. The procedure of post-processing works as follows: we keep picking the pair of clusters with maximum separable conductance and merge them until it fails to exceed a certain threshold. The threshold is set to be 1 in this study.the ability of CONCOCT to cluster contigs in complex populations (). The species were approximated by the OTUs from HMP with >3% sequence differences. Each species was guaranteed to appear in at least 20 samples. A total of 37 628 contigs remain for binning after co-assembly and filtering. The simulated 'strain' dataset aimed to test the ability of CONCOCT to cluster contigs at different levels of taxonomic
Datasets
Algorithm 1. Optimization by ANLSInput: feature matrix X 2 R MVN , initial basis matrix W 2 R MVK and coefficient matrix H 2 R KN , tolerance threshold e, maximum iteration threshold T 1: repeat 2: Obtain optimal H of Equation (10a) by fixing W 3: Obtain optimal W of Equation (10b) by fixing H 4: until A particular stopping criterion involving e is satisfied or iteration number exceeds T Output: W,H resolution (). To be more specific, the simulated). A total of 9417 contigs remain for binning after co-assembly and filtering. In addition to two simulated datasets, we use a time-series study of 11 fecal microbiome samples from a premature infant (), denoted as the 'Sharon' dataset. Because the true species that contigs belong to are not known, we assign the class labels by annotating contigs using the TAXAassign script (). As a result, 2614 of 5579 contigs are unambiguously labeled on the species level for evaluation. Another real dataset embody 264 samples from the MetaHIT consortium () (SRA:ERP000108), the same dataset used in MetaBAT (), denoted as the 'MetaHIT' dataset. In all, 17 136 of 192 673 co-assembled contigs are unambiguously labeled on the species level for evaluation.
Evaluation criteriaWe use the standard measures including precision, recall and ARI to evaluate the clustering results. Their definitions are given in the supplementary material.
ResultsGiven the same input, i.e. sequence composition and coverage across multiple samples, we show the effectiveness of COCACOLA on simulated 'species' and 'strain' datasets, in comparison with three state-of-art, methodologically distinct methods for contigs binning: CONCOCT (), GroopM (), MaxBin () and MetaBAT (). The comparison excludes Canopy () that is based on binning co-abundant gene groups instead of binning contigs. Furthermore, we investigate the performance improvement of COCACOLA after incorporating two additional knowledge, co-alignment to reference genomes and linkage between contigs provided by paired-end reads, as well as the ensemble of both. Results reveal both information facilitating better performance in the majority of cases. Finally, we report the performance of COCACOLA on two real datasets.
Performance on the simulated datasetsEven though both COCACOLA and CONCOCT are able to determine the OTU number automatically, an initial estimation of OTU number K is needed to start from. Because the OTU number is usually unknown, we study the binning performance with respect to the value of K chosen empirically. Comprehensive studies on binning performance with respect to varying K are given in the supplemen tary material. We observed that K-means clustering tends to generate empty clusters given large K. Our strategy is to increase K until there are more than K=2 empty clusters, and we choose the corresponding K as the input. At this stage, we emphasize more on the redundancy of OTU number rather than the accuracy. Thus, we obtain K  192 and K  48 as input to the simulated 'species' and 'strain' dataset, respectively. For the simulated 'species' dataset,(a) compares COCACOLA against CONCOCT, GroopM, MaxBin and MetaBAT in terms of precision, recall and ARI. The precision of COCACOLA is 0.9978, suggesting that almost all contigs within each cluster originate from the same species. In comparison, the precision of CONCOCT, GroopM, MaxBin and MetaBAT is 0.9343, 0.9324, 0.9973 and 0.9958, respectively. The recall obtained by COCACOLA is 0.9993, implying that nearly all contigs derived from the same species are grouped into the same clusters. In contrast, the(b). The precision, recall and ARI of COCACOLA reach 0.9766, 0.9747 and 0.9512, respectively. In comparison, CONCOCT, GroopM, MaxBin and MetaBAT achieve 0.8733, 0.9525, 0.8151 and 0.8730 in terms of precision, 0.9552, 0.7805, 0.9167 and 0.8009 in terms of recall, 0.8809, 0.7529, 0.757 and 0.5858 in terms of ARI, respectively. We conclude that COCACOLA performs well in constructing species from highly complicated environmental samples. Besides, COCACOLA performs well in handling strain-level variations, which cannot be fully resolved owing to assembly limitation ().
The effect of incorporating additional knowledge on binningWe investigate the performance improvement of COCACOLA after incorporating two additional knowledge as proposed in the 'Methods' section, in particular, co-alignment to reference genomes and linkage between contigs provided by paired-end reads. Moreover, we study the ensemble of both. The comparison is between the binning result by COCACOLA incorporating additional knowledge against the result without. The comparison is based on sub-samples of the simulated 'species' dataset. We choose subsamples of size ranging from 10 to 90, with 10 as increment. To avoid duplicate contribution from a particular sample, we choose sub-samples without overlapping. Therefore, the numbers of subsamples are 9, 4, 3, 2, 1, 1, 1, 1, 1, respectively. Because the contributions from additional knowledge nearly diminish when the sample size exceeds K  30, therefore we focus on the 16 cases from K  10 to K  30. In terms of co-alignment, we design the symmetric weight matrix A nn 0  1 if contig n and contig n 0 are aligned to the same species using the TAXAassign script (). As shown in(ac), the precision is improved noticeably in 7 cases and decreased in 3 cases, the recall is improved noticeably in 11 cases and decreased slightly in 1 case, the ARI is improved noticeably in 10 cases and decreased slightly in 2 cases. In terms of linkage, we design the symmetric weighted matrix A nn 0 as the number of samples supporting linkage connecting contig n and contig n 0. As depicted in(df), the precision is improved noticeably in seven cases and decreased in two cases, the recall is improved noticeably in seven cases and decreased slightly inThe first option is co-alignment information to reference genomes, depicted by (ac). The second option is paired-end reads linkage, depicted by (df). The ensemble of both is depicted by (gi) four cases, the ARI is improved noticeably in five cases and decreased in three cases. In terms of the ensemble of co-alignment and linkage, as depicted in(gi), the precision is improved noticeably in 10 cases and decreased in 3 cases, the recall is improved noticeably in 13 cases and no case suffers decreasing, the ARI is improved noticeably in 11 cases and decreased in 1 cases. We have the following conclusions: (i) When there are sufficient number of samples, the contributions from additional knowledge diminish. (ii) Additional knowledge such as co-alignment and linkage information facilitate better overall performance in the majority of cases. (iii) Ensemble of both information performs more stable than individual information.
Performance on real datasetsApplying COCACOLA to the 'Sharon' dataset ((c)), given initial choice of K  30, the precision, recall and ARI reach 0.9889, 0.9759 and 0.9670, respectively. In comparison, CONCOCT, GroopM, MaxBin and MetaBAT achieve 0.9801, 0.9820, 0.7077 and 0.9705 in terms of precision, 0.9606, 0.9147, 0.9767 and 0.8344 in terms of recall, 0.9600, 0.9126, 0.5639 and 0.8634 in terms of ARI, respectively. COCACOLA identifies six OTUs corresponding to six reported genomes. In comparison, CONCOCT, GroopM, MaxBin and MetaBAT identify 14, 24, 5 and 11 OTUs, respectively. Next, we investigate the performance improvement of COCACOLA after incorporating additional knowledge. We use linkage information only because it is circular to use TAXAassign script () on both alignment and labeling. COCACOLA still identifies six OTUs, with the precision, recall and ARI reaching 0.9923, 0.9797 and 0.9743, slightly outperforms the case without additional knowledge. Applying COCACOLA to the 'MetaHIT' dataset ((d)), given initial choice of K  100, the precision, recall and ARI reach 0.9082, 0.8272 and 0.7717, respectively. In comparison, CONCOCT, GroopM, MaxBin and MetaBAT achieve 0.8933, 0.5247, 0.6655 and 0.5738 in terms of precision, 0.7901, 0.6843, 0.8228 and 0.7397 in terms of recall, 0.7518, 0.3757, 0.5866 and 0.1088 in terms of ARI, respectively. Next we investigate the performance improvement of COCACOLA after incorporating linkage information. The performance is further slightly improved from 0.9082 to 0.9084 in terms of precision, from 0.8272 to 0.8350 in terms of recall and from 0.7717 to 0.7844 in terms of ARI, respectively.
Runningtime of COCACOLA, CONCOCT, GroopM, MaxBin and MetaBAT COCACOLA shares the same data parsing pipeline as CONCOCT and differs only in the binning step, whereas GroopM uses its own workflow. It is reasonable to compare running time of binning directly between COCACOLA and CONCOCT. To bring GroopM into context, we take into account the stages related to binning and therefore exclude the data parse stage. As for MaxBin and MetaBAT we simply pre-calculate the abundance and depth information. MaxBin involves multi-threaded parameter, which is set as the number of cores. All of five methods run on the 12-cores and 60GB-RAM computing platform provided by the USC High Performance Computing Cluster. The comparison is conducted on both the simulated datasets and real datasets (). We conclude that COCACOLA runs faster than CONCOCT, GroopM, MaxBin and MetaBAT.
DiscussionIn this article, we develop a general framework to bin metagenomic contigs using sequence composition and coverage across multiple samples. Our approach, COCACOLA, outperforms state-of-art binning approaches CONCOCT (), GroopM (), MaxBin () and MetaBAT () on both simulated and real datasets. The superior performance of COCACOLA relies on several aspects. First, initialization plays an important role in binning accuracy. Second, COCACOLA uses L 1 distance instead of Euclidean distance for better taxonomic identification. Third, COCACOLA takes advantage of both hard clustering and soft clustering. Specifically, soft clustering (such as the GMM used by CONCOCT) allows a contig to be assigned probabilistically to multiple OTUs, hence gains more robust results in general in comparison with hard clustering (such as the Hough partitioning used by GroopM). However, in complex environmental samples with strain-level variations, the corresponding OTUs are closely intertwined. Whereas soft clustering in turn further mixes the OTUs up and thus deteriorates clustering performance. COCACOLA obtains better trade-off between hard clustering and soft clustering by exploiting sparsity. However, we notice that binning metagenomic contigs remains challenging when the number of samples is small, regardless of using COCACOLA, CONCOCT, GroopM, MaxBin or MetaBAT. With small number of metagenomic samples, the relationship between the contigs cannot be accurately inferred based on the relationship between the abundance profiles. Therefore, future research needs to study how to re-weight the contributions of abundance profiles and composition profiles in unsupervised () or semisupervised () scenario. Moreover, recent studies suggest that Euclidean or L 1 distance between l-mer frequencies do not perform as well as alternative dissimilarity measurements such as d  2 and d shepp 2 () in comparing genome sequence. However, the use of such measurements is computationally challenging, which needs further exploration. The COCACOLA framework seamlessly embraces customized knowledge to facilitate binning accuracy. In our study, we have investigated two types of knowledge, in particular, the co-alignment to reference genomes and linkage of contigs provided by paired-endthough the contributions from additional knowledge diminish when there are sufficient number of samples, they play an important role in binning results when the number of samples is small. In future studies, we intend to explore better customized prior knowledge. one option is exploiting phylogenetic information in taxonomic annotation (). Another option relies on identifying functional annotation of contigs, including open reading frames that are likely to encode proteins (), or co-abundance gene groups (), etc. We have also investigated the ensemble of both co-alignment and linkage knowledge, and it shows more stable performance than individual information. In future studies, we aim to find optimal weights () instead of equal weights.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Y.Y.Lu et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
