Motivation: Identification of every single genome present in a microbial sample is an important and challenging task with crucial applications. It is challenging because there are typically millions of cells in a microbial sample, the vast majority of which elude cultivation. The most accurate method to date is exhaustive single-cell sequencing using multiple displacement amplification, which is simply intractable for a large number of cells. However, there is hope for breaking this barrier, as the number of different cell types with distinct genome sequences is usually much smaller than the number of cells. Results: Here, we present a novel divide and conquer method to sequence and de novo assemble all distinct genomes present in a microbial sample with a sequencing cost and computational complexity proportional to the number of genome types, rather than the number of cells. The method is implemented in a tool called Squeezambler. We evaluated Squeezambler on simulated data. The proposed divide and conquer method successfully reduces the cost of sequencing in comparison with the naı¨venaı¨ve exhaustive approach. Availability: Squeezambler and datasets are available at
INTRODUCTIONCritical applications, including the Human Microbiome Project (), biothreat detection and combating antibiotic resistant pathogens, necessitate identification of all distinct genome sequences in a bacterial sample. When prior knowledge is available about which organisms may be present in the sample, flow cytometry and 16S rRNA gene sequencing may be used. However, metagenomics is usually used for analyzing the genomics of relatively abundant microbes when no prior knowledge is given. Metagenomics consists the study of the variation of species in a complex microbial sample. Because the vast majority of environmental bacteria elude cultivation, metagenomics investigates microbial communities by sequencing sampled genome fragments without the need for culturing. Such a heterogeneous pool of sequencing reads can also be assembled to yield a superposition of highly abundant genomes in the sample (). There are two problems with metagenomics: (i) the resulting assembly contains multiple genomes superimposed, and (ii) only highly abundant genomes survive the sampling process. Advances in DNA amplification technology have enabled whole genome sequencing directly from individual cells without requiring growth in culture. Single-cell sequencing methods have enabled investigation of novel uncultured microbes (). These culture-independent single-cell studies are a powerful alternative to metagenomics studies. Genomic sequencing from single bacterial genomes was first demonstrated with cells isolated by flow cytometry (), using multiple displacement amplification (MDA) (). MDA is now the preferred method for whole genome amplification from single cells (). The first attempt to assemble a complete bacterial genome from one cell () further explored the challenges of assembly from amplified DNA, including amplification bias and chimeric DNA rearrangements. Amplification bias results in orders of magnitude difference in coverage () and absence of coverage in some regions. Chimera formation occurs during the DNA branching process by which the 29 DNA polymerase generates DNA amplification in MDA (). Subsequent studies continued to improve single-cell assemblies (). A nearly full potential of single-cell genome assembly has recently been realized by the work offollowed by those of. Owing to recent progress in single-cell DNA amplification techniques and de novo assembly algorithms, the genomes of all bacterial species in a sample can be captured one cell at a time. However, there are often millions of cells per sample, in which case the navenave deep sequencing of every cell becomes prohibitively costly. Moreover, it is expected that deep sequencing of every cell is often not necessary, as the majority of biologically important samples are sparse in the sense that many cells are biological replicates. Compressed Cand s and) and distilled (adaptive sampling and refinement) sensing methods () have been proposed in the past decade to exploit sparsity for reducing the cost of sensing and reconstructing signals in various spaces, ranging from Banach spaces to Boolean algebras (). Inspired by those advances, we give an *To whom correspondence should be addressed. algorithm in this article that exploits sample sparsity to reduce the cost of sequencing without compromising the accuracy of identification of all distinct genomes, even the ones that are minimally represented in the sample.
APPROACHA navenave approach to solve the problem, which we call single-cell co-assembly strategy, is to amplify the genome of each cell, barcode them individually, pool them, sequence in one sequencing run and demultiplex based on the barcode. In this approach, each cell should be isolated and its DNA extracted and amplified using MDA. Although there is currently no high-throughput device to perform these processes, one could envision automated microfluidic devices that will be capable of high-throughput separation, DNA extraction, amplification and barcoding of single cells in the near future. The output sequencing reads could then be co-assembled using our tool HyDA (). In HyDA, the read dataset of each cell is assigned a unique color. All the colors are co-assembled in one colored de Bruijn graph. This approach requires enough unique barcodes to tag the fragments of each cell. Also, barcodes attached to each fragment are sequenced, which imposes additional sequencing cost. Fabrication of so many unique barcodes becomes prohibitively expensive for a large number of cells, and therefore this navenave approach will not work. The number of distinct genomes in a microbial community is often much less than the number of cells. For example,estimated the number of detected distinct species in the human gut to be in the order of 1000, whereas the number of microbial cells in a human body, most of which reside in the gut, is in the order of 100 trillion. We call this effect the sparsity of distinct genomes in a sizable microbial population. The navenave approach does not exploit the sparsity to reduce the cost of sequencing. Here, we proposed to exploit the spareness by adopting a divide and conquer strategy to reduce the amount of required barcodes and sequencing. After isolation of each cell and extraction of the DNA, every DNA is amplified and stored separately, e.g. in a microfluidic droplet. The main idea is to sample the amplified DNAs adaptively, which is essentially allocating sequencing and barcoding resources dynamically over the course of multiple sensing iterations. Initially, the algorithm has one group of cells, which is the entire sample. In each iteration, each group is divided into two equally sized subgroups. A small amount of DNA from each cell in a subgroup is sampled, pooled and sequenced. In practice, one barcode per subgroup is used to multiplex and demultiplex the sequencing in one run. The amount of sampling from each cell is computed based on the results of previous iterations. This is called resource allocation and is similar to the one proposed byand Wei and Hero (2012). The resulting read datasets, one per subgroup, are then co-assembled and compared using HyDA to decide pairwise subsumption of subgroups. The cells in those subgroups that are subsumed by other subgroups, even in previous iterations, are eliminated from further analysis. This procedure is continued until each remaining group contains only one cell. The resulting non-redundant single element groups capture all of the distinct genome sequences.
METHODSAlthough distinct genomes are often identified as different species, there are numerous cases where distinct genomes are categorized as varied instances of the same species or even the same strain. Instead of identification of strains and species that are currently phenotypic notions, the goal of our approach is to find all distinct genomes in a sample. We define two genome sequences to be distinct if the ratio of their differences over the whole genome size is above a threshold. That threshold is input by the user and controls a trade-off between sensitivity and specificity. Co-assembly and comparison of multiple input read datasets lie at the core of both approaches we take in this article. Although there are assembly tools for single-cell genomic data, such as SPAdes () and IDBA-UD (), and also co-assembly tools for normal multicell genomic data such as Cortex (), we use HyDA, which is the only tool to date that has both functionalities (). However, the novel ideas proposed here can also be implemented using other assemblers. For the sake of completeness, HyDA algorithm is summarized in the following.Bruijn graph, is a combinatorial structure that can be used to assemble a number of input read datasets, each represented by a color, superimposed on a single de Bruin graph (). The output of such assembly methods is a number of assembled sequences (contigs) and the corresponding average multiplicity in each color. Our de Bruijn graph of the input reads is stored in a hashed collection of splay trees whose vertices are k-mers with an array of multiplicity counts (one entry per color), in-and out-edges and internal flags. A 1-in 1-out chain of k-mers is condensed into an equivalent long sequence, which is called a unitig. A maximal unitig, which cannot be extended further because of a branch in the graph, is a contig. Note that in HyDA, our condensation is solely based on the topology of the graph without any attention to the colored multiplicities. Ignoring multiplicities for condensation is purposefully done, and constitutes the feature that allows the assembler to deal with black out regions in single-cell MDA (). Contigs with low coverage are often caused by sequencing error. The low-coverage contig removal process is iterated with an increased cutoff in each round. In each iteration, those contigs whose maximum coverage over all colors is less than the cutoff are eliminated, and the remaining graph is recondensed. This causes some contigs to merge into larger ones with recomputed average coverages. This process is similar to VelvetSC's low-coverage contig removal, but instead of considering one average coverage per contig, HyDA considers the maximum of average coverages for all the colors of each contig (). In this case, only those contigs that have low coverage in all colors are considered erroneous and removed. Another possible approach is to eliminate those contigs for which the mean of average coverages for all colors is less than the cutoff. However, if we were to follow this approach, a contig that is well covered in one color but is poorly covered or absent in hundreds of colors would be lost, as the mean would dilute the effect of coverage in one color among hundreds of colors. This approach would not work for us here because our goal is to be able to preserve rare contigs.
Co-assembly algorithm
Redundancy removalTo remove redundant genomes, we define a relation that is reminiscent of subset relation on the set of contigs for each color. Note that our goal here is to remove redundant genomes, which are collections of contigs, rather than to remove individually redundant contigs. Let A  fa 1 , a 2 ,. .. , a r g be the set of remaining contigs after iterative error removal. Let M j a i  denote the average coverage of contig a i in color j, for 1 i r and 1 j s, where s is the number of colors. Pick ! 0, and let A j  fa i 2 A j M j a i 4g & A be the set of contigs for each color j. The parameter determines the tradeoff between specificity and sensitivity. We chose  0 in this study, but a non-zero might be needed if there are erroneous or contaminant k-mers in one color that also occur in the true genomic sequence of another color. We define D A i , A j  2 R on the set F  fA 1 , A 2 ,. .. , A s g as: D A i , A j    jjA i nA j jj jjA i jj , 1 in which A i nA j  fa 2 A i ja = 2 A j g, and jj  jj denotes the total assembly size. We define:A i " A j iff 0 D A i , A j , 2 in which ! 0. Particularly, " 0 becomes the subset relation and can be used to detect and remove redundant collections of contigs, i.e. those that are subsumed by a larger collection. However, in reality, the mathematical subset relation is not adequate as there are various types of noise, including sequencing errors, intrastrain variations such as single nucleotide polymorphisms and indels, contaminations added in the amplification and sequencing process and lack of coverage in some regions caused by the MDA. Hence, the definition of subset should be loosened by choosing a small but non-zero value for. Beside , the value of gives a trade-off between specificity and sensitivity of recognizing distinct genomes. If is small, the algorithm detects two equivalent genomes as distinct, and if is large, distinct genomes are declared equivalent. To see how is chosen, refer to Sections 3.2 and 4.2. The results are shown in. Finally, we compute a non-redundant set of assemblies E  fA i1 , A i2 ,. .. , A it g F, such that for every distinct pair 1 a, b t, A ia 6" A ib and A ib 6 " A ia .
Divide and conquer strategy exploiting sparsityLet n be the number of cells in the sample, and denote the cells by S i , i  1,. .. , n. Our algorithm aims to assemble all of the distinct genomes and identify at least one cell per distinct genome. To reach this goal, our algorithm iteratively pools samples of amplicons from different cells, tags each pool with a unique barcode, mixes the barcoded pools and has the result sequenced. The objective is to minimize the total number of bases required to be sequenced as well as the number of different barcodes needed. In the first iteration, we divide the n cells S 1 ,. .. , S n into two setsOur algorithm samples equal amount of amplicons from each cell in I 1, 1 and I 1, 2. The amplicons in each set are pooled and tagged by two distinct barcodes. The barcoded amplicons are sequenced to reach a desired number of base pairs. This number is an input parameter of our algorithm. We define the total number of base pairs sequenced from I 1, i to be b 1, i , for i  1, 2. The two read datasets are co-assembled by HyDA using two colors. The result is two sets of contigs for each color, A 1, 1 and A 1, 2. We calculate D 1 A 1, 1 , A 1, 2  and D 1 A 1, 2 , A 1, 1  as defined in (1), in which 1  = max j jI 1, j j, is an input parameter and j  j is the set cardinality. We choose to be the maximum allowable difference between the assembly of two single cells from the same strain. Based on these values, we decide whether the relations, 1 is a subset of A 1, 2 , then all of the distinct genomes in I 1, 1 are present in I 1, 2 ; therefore, the cells in I 1, 1 do not need further sampling. This applies to I 1, 2 too, if A 1, 2 is a subset of A 1, 1. If both relations hold, one of I 1, 1 or I 1, 2 is eliminated arbitrarily from further analysis. Each remaining set I 1,  is divided into two subsets for analysis in the second iteration.The same splitting process occurs in the subsequent iterations. Assume I i, 1 ,. .. , I i, mi are the remaining sets in iteration i. Each set I i, j is sampled to produce b i, j base pairs, barcoded uniquely, pooled and sequenced. All of the new sequence datasets and those obtained in all previous iterations are co-assembled. In the co-assembly, the previous datasets help to improve the assembly of the new ones. The resulting contig set of I i, j is denoted by A i, j. For all j, k  1. .. m i , the relations A i, j " i A i, k are evaluated, where i is a threshold whose calculation will be explained below. The cells in those I i, j whose assemblies are subsumed will be removed from further analysis. All the remaining ones are partitioned into two disjoint subsets. Denote the new subsets by I i1, 1 ,. .. , I i1, mi1. Note that in iteration i, m i unique barcodes are needed. Therefore,is the maximum number of barcodes required for the entire algorithm. Parameters b i, j play a key role in the algorithm. We propose an adaptive calculation of b i, j to minimize, without losing accuracy, the total base pairs sequenced:Assume I i, j is a set that is created by dividing the set I i1, k in iteration i 1 into two. We are motivated to choose the total number of sequenced base pairs from cells in I i, j to be proportional to the total assembly size jjA i1, k jj, i.e. b i, j  c  jjA i1, k jj, 5 where c is an input parameter indicating the estimated average coverage of each iteration. We say A i, j are accurate enough if the partial order relation " i can be assessed accurately. If c is large and the assembly of iteration i  1 is accurate enough, then in iteration i, adequate base pairs are sequenced to allow an accurate enough assembly of set I i, j. Another factor that affects accuracy of the assessment of these relations is the choice of i. The threshold i in the i th iteration is used to detect cells with similar genomes despite small differences in their assemblies. We propose to use the following threshold: i  max 1 j mi jI i, j j : 6 Recall that is the maximum allowable difference between the assembly of two single cells with similar genome sequences. To account for the. The divide and conquer algorithm for an example with 10 cells and 3 distinct genomes shown in different colors. Each row corresponds to one sequencing round. The number of barcodes in each round is the number of blue boxes in the corresponding row worst possible case, it is assumed that there are jI i, j j distinct genomes in each group I i, j. Therefore, max 1 j mi jI i, j j captures the maximum number of distinct genomes in I i, j from any I i, k. With these assumptions, i is a conservative threshold. This threshold will guarantee that two distinct genomes are detected, but it has the possibility of detecting similar genomes as distinct. In the last iteration of the algorithm, when every group consists of one cell, the threshold is. Note that the number of iterations, which is the number of sequencing rounds, is always log 2 n AE  .
ImplementationWe implemented our algorithm in a tool called Squeezambler 1.0 in C. Our tool and datasets are available at http://compbio.cs.wayne. edu/software/squeezambler/.
RESULTSBecause we did not have access to real data, we tested our algorithm using simulated data. We used our tool MDAsim 1.0 () to simulate 100 MDA processes (one process per cell) from 9 distinct genomes. The output of MDAsim 1.0 was fed into an Illumina read generator, ART (), to generate Illumina reads, with realistic errors, from the simulated amplicons. The set of generated reads for each cell was treated like a microfluidic droplet from which samples without replacement are extracted in each iteration of Squeezambler 1.0. We assume that MDA products are contamination-free, which require a contaminant-free automated microfluidic cell sorting, amplification and sampling device.
DatasetsA total of 115 MDAs were simulated from nine distinct genomes chosen from the list of species found in a gut metagenomics study () that have a complete or draft reference genome. Recall that we are simulating MDA from a reference genome; therefore, we needed a reference genome for the chosen species.summarizes the identification number in the National Center for Biotechnology Information database (NCBI ID), name, size, reference status (complete or draft) and the number of cells we have simulated. The number of cells is approximately proportional to the abundance mean of the corresponding species in. We ran MDAsim 1.0 with a diverse set of parameters, one for each cell, to capture the diverse nature of MDA coverage bias. ART, an Illumina read generator, was then deployed to generate 100 bp Illumina reads from the simulated amplicons. The amplification gain of MDAsim 1.0 was 300 and that of ART was 8 from which one-eighth were selected randomly to obtain a total gain of 300. We assembled the dataset of each cell individually with HyDA 1.0, and the resulting assemblies have between 0.1 and 4.2% missing reference bases measured by Gage (), which is similar to the real world situation with a successful MDA reaction (). We made an error profile that matches the error statistics of Illumina HiSeq 2000 for ART. Using our profile, ART injects on average 1% error into the reads, because of which we need to eliminate erroneous contigs in the assembler. HyDA 1.0, and also its predecessor Velvet-SC, has an iterative algorithm to remove low-coverage contigs, which is explained in Section 3.1.1. In each iteration, Squeezambler 1.0 provides HyDA 1.0 with a coverage cutoff as a percentage of the mean coverage of each color. That percentage is constant in all iterations. We designed three collections of cells, the statistics of which are summarized in. In the first collection, there are 62 cells with 6 distinct genomes. In this collection, we put 22 different MDA results from NC_004663.1 and 22 from FP929051.1 toplay the role of highly abundant genomes in a sample as well as 1 from NC_016776.1 and 2 from NC_008532.1 to represent low-abundance genomes in the same sample. In the first collection, the number of distinct genomes is approximately one-tenth of the number of cells, but with a wide range of abundances. The second collection is the sparsest collection among the three, where the number of distinct genomes is approximately one-twentieth of the number of cells. The third collection is the most diverse of the three, where the number of distinct genomes is approximately one-sixteenth of the number of cells.
Simulation resultsWe ran Squeezambler 1.0 for the three collections described above, the results of which are summarized in. Most of the Squeezambler 1.0 parameters were the same for all three collections. The assembly inclusion threshold constant per cell was chosen  0:2, which means at most 20% of the assembly can vary among multiple instances of the same genome. Taking into account the genomic sequence loss caused by the MDA, sampling of the amplicons and sequencing errors, 20% is a reasonable choice. This is not an optimized value and is chosen based on the authors' intuition. We chose conservatively in this work so that distinct genomes are not lost but some equivalent genomes are detected as distinct. Finding the optimal value for needs a thorough study, which is beyond the scope of this article. The k-mer size for HyDA 1.0 was chosen to be k  55, which is a common choice for the chosen Illumina error profile (). The coverage cutoff, as a percentage of the coverage mean, was chosen to be 100%, and the minimum contig length was 100 bp for HyDA 1.0. The coverage mean is estimated based on the assembly size in the first iteration, which is often larger than the actual genome size because of a myriad of erroneous low coverage k-mers. This causes the initially estimated coverage mean to be a small fraction of the final coverage mean after error removal. Squeezambler 1.0 has an option to choose the number of initial groups in the first iteration, g. If g is chosen to be equal to the number of cells, then Squeezambler 1.0 simulates the single-cell co-assembly of all the cells. If g  2, then Squeezambler 1.0 simulates the divide and conquer algorithm described in Section 3.2. Although experimenting with different g values may improve the results, we do not have data for it. Before any sequencing is done, the algorithm has no idea about the genome sizes, various distinct genomes and abundance of each genome. Therefore, an unbiased algorithm has to sequence from each cell exactly the same amount right in the first iteration. That amount in our algorithm, denoted by b 1, j =jI 1, j j, is an input parameter to Squeezambler 1.0, which was chosen to be between 1 Mbp and 63 Mbp as reported in the third column of. In our setup, the size of the nine distinct genomes varies between 1.8 and 6.3 Mbp; see. Therefore, 1 Mbp sequencing provides between 1/6 and 1/2 coverage, and 63 Mbp sequencing provides between 10 and 35 coverage. The input parameter c, which controls the amount of sequencing in subsequent rounds, was chosen to be c  10, which means 10 expected coverage from each genome in each collection. We observed that in practice 10 coverage of each distinct genome provides sufficient information for reliable evaluation of ". This is consistent with the Lander andanalysis, in which the statistics of gaps and contigs in terms of coverage is characterized. Based on that analysis, 10 coverage yields the entire genome without gap with high probability. Our divide and conquer algorithm exhibits significant improvement in maximum barcodes, and in most cases the total number of base pairs sequenced, over the single-cell co-assembly method. For the 97 cells, 5 distinct genomes collection, our divide and conquer algorithm requires only 2.9 Gbp sequencing and 10 barcodes in comparison with 3.0 Gbp sequencing and 97 barcodes consumed by the single-cell co-assembly method. Similarly for the 62 cells, 6 distinct genomes collection, our divide and conquer algorithm requires only 3.0 Gbp sequencing and 10 barcodes in comparison with 3.9 Gbp sequencing and 62 barcodes required by the single-cell co-assembly method.Even though for the 112 cells, 7 distinct genomes collection, our divide and conquer algorithm outperforms single-cell co-assembly in terms of the number of barcodes, by 33 versus 112, it requires 7.1 Gbp sequencing, which is more than that used by single-cell co-assembly (3.5 Gbp). In all of our experiments, all distinct genomes were correctly detected. Therefore, our results exhibit ultimate sensitivity. However, in some experiments, multiple cells with similar genomes were identified as distinct, which is not an issue for our problem, because, based on the results of Squeezambler 1.0, those cells that are identified as with distinct genomes can be deeply sequenced and assembled afterward. For the 62 cells, 6 distinct genomes collection, the number of detected distinct genomes was between 6 and 8. For the 97 cells, 5 distinct genomes collection, that number was between 5 and 11, and for the 112 cells, 7 distinct genomes collection, that was between 7 and 14. This specificity is reported in the sixth column of. Note that the number of sequencing rounds (iterations) for single-cell co-assembly is always 1, and for divide and conquer with g  2, it is log 2 n AE  . Owing to the computational intensity of MDAsim 1.0, HyDA 1.0 and Squeezambler 1.0, we report our results for only small examples to provide a proof of concept. We also chose our parameters conservatively, and without optimization, so that we do not compromise accuracy. Moreover, our examples are in the order of 100 cells and 6 distinct genomes, whereas real world samples are much sparser, as the number of cells may be in the order of billions and the number of distinct genomes at most in the order of thousands. Therefore, we expect the reduction in the total sequencing and maximum barcodes to be higher for real world applications than what we report in this article.
CONCLUSIONWe presented an adaptive divide and conquer algorithm for distilled sequencing and de novo assembly of distinct genomes in a bacterial community, e.g. human gut microbiome. Samples derived from such communities are often sparse in the sense that the number of distinct genomes is much less than the number of cells. Our algorithm exploits sparsity to decrease the amount of sequencing and the number of multiplexing barcodes needed for single-cell sequencing and de novo assembly. We implemented our algorithm in a tool which we call Squeezambler and performed simulation experiments to demonstrate its power. Our results show that (i) the number of required barcodes with our divide and conquer algorithm is less than that required by the navenave approach, and that (ii) the amount of sequencing needed remains the same or decreases. Owing to the computational intensity of the problem, only small examples with low sparsity were studied in this work. Real-world samples are much sparser ($1000 species in $10 14 cells) than the examples here ($5 species in $100 cells). Also, the parameters used to run our tool were chosen conservatively and without optimization. Therefore, we expect the improvement of our algorithm to be higher than what we reported in this article in real-world situations. Squeezambler 1.0 identifies all distinct genomes in the sample, which are candidates for different strains/species. Those cells that are identified as having distinct genomes need to be subsequently deeply sequenced and assembled to obtain a more detailed assembly. Funding: NIH RO1 DK089167, STTR R42GM087013 and NSF DBI-0965741 (to S.D.). Conflict of Interest: none declared.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Z.Taghavi et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Distilled genome sequencing and assembly at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
