Motivation: Ontologies are used in the annotation and analysis of biological data. As knowledge accumulates, ontologies and annotation undergo constant modifications to reflect this new knowledge. These modifications may influence the results of statistical applications such as functional enrichment analyses that describe experimental data in terms of ontological groupings. Here, we investigate to what degree modifications of the Gene Ontology (GO) impact these statistical analyses for both experimental and simulated data. The analysis is based on new measures for the stability of result sets and considers different ontology and annotation changes. Results: Our results show that past changes in the GO are non-uniformly distributed over different branches of the ontology. Considering the semantic relatedness of significant categories in analysis results allows a more realistic stability assessment for functional enrichment studies. We observe that the results of term-enrichment analyses tend to be surprisingly stable despite changes in ontology and annotation. Contact:
INTRODUCTIONOntologies are increasingly used in the life sciences (). They provide a uniform vocabulary to describe and structure a domain of interest. As a prime example, the Gene Ontology (GO)contains knowledge about biological processes (BPs), molecular functions (MFs) and cellular components (CCs). The categories of GO are presented as a directed acyclic graph (DAG), with the edges representing relationships between the categories. Higher-level categories represent more abstract descriptions and encompass all-child categories. These categories are used to semantically describe genes and gene products (). Genes are therefore described by the particular category to which they are annotated and, by extension, by all-parent categories that give a more abstract description. Applications that perform functional enrichment analysis of gene sets take advantage of this property to identify more general categories that contain significantly more signal than expected at random (). Ongoing scientific research provides new domain knowledge that needs to be incorporated into ontologies and annotations. In the case of GO, these changes are incorporated on a regular basis with regular public releases (). This ontology evolution has been previously analyzed () and yielded insights into the differences between ontology versions (). Typical ontology changes include the addition of new categories and relationships as well as the revision of the existing structure (). These ontological modifications can trigger changes in the annotation (), e.g. when a category is removed, the annotations need to be moved or deleted. Further, annotations may be edited to reflect new knowledge or to eliminate inconsistencies (). The GO has evolved substantially since its inception in 2000. Its three sub-ontologies evolved at different rates and in different ways. Between 2007 and 2010, BP increased by about 70%, compared with CC (%40%) and MF (%20%) (see Supplementary). Applying the method described in, we can identify how much different parts of GO have evolved by aggregating the change intensity in subtrees.illustrates that different subsections of the GO-MF evolve differently. The non-uniform distribution of changes may have an effect on functional enrichment analyses. The aforementioned studies only considered changes in the ontology while neglecting the potential effects on downstream analyses; for example, how these changes may lead to different results in functional enrichment analyses. While it is rather obvious that the high degree of occurred changes in GO and its annotations will impact analysis results, it is still unknown whether earlier findings are significantly affected or even invalidated. The impact of ontology changes on functional enrichment analyses may depend on where the changes are located in the ontology and what kind of changes dominate. For example, additions of categories at the leaf level might be less critical than structural revisions within the ontology. We provide a method to test to what degree changes of GO and GO annotations (GOAs) may affect functional enrichment analyses. We demonstrate the applicability and usefulness of our approach by analyzing two real-world experimental datasets as well as 50 random datasets. For the experimental datasets, we provide an in-depth study of the underlying changes and their impact on the analysis results. The presented analysis is informative for both ontology curators and users of functional enrichment methods. *To whom correspondence should be addressed.
METHODS
Ontology and annotation modelFor our study, we use the GO, which is represented as a DAG, in which categories are linked by directed edges representing the relationships. Each category c has a unique identifier (e.g. 'blood coagulation' has the identifier GO:0007596). Various annotation sets A are associated with the GO. We use the GOA (). Annotations to one category (A(c)) include annotations to c and to all its descendant categories (subgraph of c). The evidence supporting the annotation of a gene to a category is represented as an evidence code, which can be used to assess the origin and likely the quality of an annotation, e.g. experimentally verified or automatically generated. Our evaluation requires consideration of multiple versions of ontologies and annotation sets. New versions always supersede the information of older versions. Furthermore, an annotation set is always associated with a particular ontology version. We denote an ontology version as O v and an annotation version as A v , where v stands for the date of release.
Ontology change detectionTo understand the evolution of ontologies, we use a previously published diff algorithm (available in the CODEX web tool ()) to identify the changes that occurred between two versions of an ontology. The algorithm () uses a set of rules to first identify basic changes (insert/update/delete) that are then aggregated into a smaller set of more complex (semantic) changes, such as merge, split or changes of entire subgraphs. The following ontology changes are relevant for our study: addC  addition of a category toObsolete  mark a category as obsolete merge  merge of two or more categories into one category split  split one category into two or more categories substitute  substitute one category by another addR/delR  addition/deletion of a relationship between two categories move  move a category from one parent to another parent category.
Term enrichment using FUNCWe use the program FUNC () to carry out the functional enrichment analysis. FUNC tests each category of the input ontology for significance and then carries out randomizations to correct for multiple testing. For our further analysis, we consider the family-wise error rate corrected P-values associated with each category.
Stability measuresWe propose two kinds of stability measures to assess the impact of ontology and annotation changes on the experimental results set of a functional enrichment analysis. For this purpose, we consider a fixed set of genes, and we compute experimental result set (ER) for different points in time with freely chosen ontology and annotation versions. We will use the example result sets displayed into illustrate our stability measures.
Basic stability measureFor our measures, we use the following cardinalities for result sets ER i and ER j produced with different ontology or annotation versions: ER i j j, ER j  number of categories in ER i and ER j ER i \ ER j  number of overlapping categories between ER i and ER j ER i n ER j  number of categories only in ER i but not in ER j ER j n ER i  number of categories only in ER j but not in ER i :Note that categories are different if their unique identifier (accession) differs. For determining the overlap, we count categories with identical accessions in both ER i and ER j : The key idea for assessing the basic stability of a result set is the following. A result set is considered stable in comparison with an older set if both sets share all categories and no set has unique categories. With fewer categories shared, the measure decreases to indicate instability. Based on the common set similarity measure dice, we can compute the basic result set stability as follows:RSstab basic ER i , ER j   2  ER i \ ER j ER i j j ER j :The stability returns a value between 0 and 1, whereby 0 denotes complete instability between the two result sets (no overlapping categories). A stability of 1 means that two considered result sets completely overlap. For instance, the example shown inresults in a stability of 0.57 since ER i (ER j ) contains eight (six) categories of which four overlap. We use an analogous measure to determine the stability between annotation versions (Astab). For this, we consider the whole annotation set A or annotations to a specific category A(c). We calculate the set of annotations that are identical between the two versions. This value is then normalized by the total number of annotations. The calculation of the stability measure for A (and analogous for A(c)) is thus identical to the stability between result sets:While there are alternative set similarity measures, such as Cosine or Jaccard, we consider dice as a suitable approach. It corresponds to the harmonic mean, and we compare sets of similar size. According to, the Jaccard and Cosine measures produce slightly different values than dice, especially in case of low set overlap.proposes a more general measure based on information theory, taking probabilities instead of set overlap into account. This approach is particularly suited for the evaluation of term-enrichment tools, and we consider this a topic for future work.
Region stability measure The basic stability measureevaluates the overlap of categories in the result sets without considering the semantic relatedness of the categories, i.e. it treats them as independent from each other. This may result in an apparent instability even when differing categories are semantically related. We therefore generalize our model to include structural similarity. We first enhance the result sets by grouping together semantically related categories within so-called category regions (CRs). We then define the stability with respect to the CRs of the result sets. The basic model without grouping remains valid as a special case of the region-based approach. Semantic grouping of ER categories: We base the semantic grouping of categories in the experimental result sets on the distance within the ontology. This takes advantage of the fact that related categories tend to be in close proximity, i.e. they are either directly connected by an ontology relationship or only a few relationship 'edges' apart. We control the grouping by a distance parameter d; the base case without grouping corresponds to d  0. For d40, we recursively group together all categories that are connected by d edges (see Supplementary Algorithm 1). For the example shown in, we obtain for d  1 three regions in ER i and four regions in ER j , e.g. in ER j , we group c 6 and c 13 into one region. For d  2, the number of regions is reduced to two in ER i and three in ER j , e.g. c 14 and c 17 are grouped together with c 6 and c 13 in ER j :Alternative methods exist to group categories in the result sets by applying different semantic similarity measures (e.g.) or classification algorithms (e.g.). Our approach makes use of semantic similarity based on ontology structure and is simple to apply. The approach is also in agreement with term-enrichment approaches such as FUNC that determine the significance of categories based on the ontological structure that try to restrict significant categories to few areas within an ontology. Determining region stability: The semantic grouping of categories allows us to determine the stability of results sets based on their CRs and to assume stability as long as the same or at least overlapping regions are retained in the result sets. We therefore determine the number of regions in ER i having an overlap with regions in ER j and vice versa (see Supplementary Algorithm 3). We consider two regions as overlapping if they share at least one category. We denote the overlapping regions in ER i with CR o i and in ER j with CR o j , respectively. We can now use the information about overlapping regions to compute the region stability as follows:The stability values are, as before, distributed between 0 and 1; for d  0 (no grouping), the region stability equals the basic stability. In, the region stability is 0.57 for d  0, 0.71 for d  1 and even 0.8 for d  2. Increasing the distance leads to fewer but larger regions that more likely overlap with the regions of updated result sets. Non-overlapping regions indicate the addition or removal of larger areas in the ontology and, thus, more significant changes than individual category changes quantified with the basic stability measure.
DatasetsWe re-analyzed two datasets that were first tested in 2007 (). The authors performed functional enrichment analyses of genes that show signals of positive selection in primates and rodents. We repeat the analyses using newer versions of ontology and annotation. We consider yearly versions between 2003 and 2010, i.e. eight GOA versions) and the corresponding GO versions (). All newer and older versions are compared with the version used in the original publication (GOA 47 and GO 012007 ). For testing, we used the Wilcoxon rank test with 10 000 random sets and a cutoff of at least 20 genes per category. A significant category has to have a P-value that does not exceed a value of 0.05. To test whether the observations from the real datasets apply generally, we generated 50 datasets, randomly seeding significant categories. For this, a set of categories is chosen using a fixed first ontology version and marked as significant by choosing a higher ratio of genes that are. Stability measures. Colored nodes denote significant result set categories. We consider two versions i and j of the result set ER (for clarity with stable ontology structure). Yellow categories are significant in both result sets ER i and ER j , red (green) categories are only significant in ER i (ER j ). Stability is computed for CRs grouped using distance d  0.. .2. For d  0, each concept is considered as a region (yellow concepts overlap). For d  1 (d  2), overlapping CRs are connected by a dashed black (continuous grey) line marked as significant. Then, this simulated dataset can be tested using a different GO version. We generate datasets with the GO versions from 2007 and 2010. We then test the 2007 dataset with the 2010 version (Task A) and vice versa (Task B).
RESULTS AND DISCUSSIONBetween 2003 and 2010, GO grew by a factor of 2.4. Similarly, the number of input annotations increased by factor 2.7 for mouse and human (see Supplementary). However, some annotations were also removed. A comparison of whole annotation sets of 2007 and 2010 shows substantial instabilities, A 2010   0:7], i.e. every third annotation was affected by a change. To understand the impact of ontology and annotation evolution on term-enrichment results, we measure the stability for two real-world datasets (primate and rodent dataset) at different time points. We quantify changes in result sets using our basic stability measure and analyze causes for changes using Astab and diff. To identify crucial changes in the results, we applied our region-stability measure. We analyzed 50 random datasets to test whether our observations are generalizable.
Primate and rodent datasets3.1.1 Basic stability We computed result sets for yearly versions between 2003 and 2010 for the primate () and rodent datasets (). We compared the result sets of each version (ER comp ) against the reference version ER ref (from 2007) by computing the overlap and difference of significant categories. The primate result set () contains 19 significant categories in 2007. In general, ontology versions that are closer in time tend to share a higher fraction of significant categories. Since 2008, the results are more stable. A substantial fraction of significant categories are detected in 2007 and 2010, as evidenced by a stability measure (RSstab basic ) of 0.81. The rodent result set () contains 23 significant categories in 2007. The preceding (2006) and successive (2008) version overlap by only 8 and 15 categories, respectively. Using the 2010 version, only eight categories of the reference result set remained, and no new categories were detected as significant. Overall, the results set stability (0.52) is lower compared with the primate dataset.shows a comparison of the result set stability RSstab basic between 2007 and 2010 for both the datasets. To identify the main cause for the changes observed between significant categories found in different years, we tested the results changing ontology and annotation independently. Changing only the ontology affected both datasets (rodents 0.84, primates 0.7). Changing the annotation version (A w ) but fixing the ontology only marginally affected the primate result set (0.97) while it substantially reduced the stability of the rodent result set (0.5). This shows that both ontology and annotation evolution have an impact on the results of term-enrichment analyses. We explore the causes for the differences in significant categories between 2007 and 2010 (). We used diff to identify ontology changes that have caused changes in ER. Moreover, we analyzed the annotation stability (Astab) of all significant result categories c to see whether changes in annotations predominate in some cases. First, there were three new significant categories for primates in 2010. Two categories ('molecular transducer activity' GO:0060089, 'system process' GO:0003008) were added between 2007 and 2010 such that they were additionally detected in the functional enrichment analysis. Another category ('cognition' GO:0050890) has been included in the result set since it received additional annotation. On the other hand, no new categories were detected as significant in rodents. Several categories were no longer significant in both rodent and primate datasets. Three of the 15 non-significant categories in the rodent dataset are directly affected by an ontology evolution operation (merge) while most other categories that became non-significant show a strongly reduced annotation stability of less than 0.7. For instance, for 'regulation of immune system process' ('GO:0002682') only 22 out of 143 annotations in 2010 overlap to the annotation set of 2007 (Astab 0:3). This is in contrast to the primate dataset, where three of the categories were affected by ontology evolution operations (merge, substitute, toObsolete) while only one category shows a large change in annotation. We further observed strong structural changes in the direct semantic context of significant categories: addR j j3148, delR j j1011, move j j10257 for primates (rodents) (see Supplementary). Such structuralmodifications may influence annotation propagation. Adding or deleting relationships to an upper category c leads to a changed annotation set A(c) and, thus, to a reduced annotation stability Astab and possibly to a changed significance of c. Overall, most unaffected significant categories showed a higher annotation stability than categories that gained or lost significance. Electronic annotation that is based on automated methods to infer the function of genes may produce less reliable annotations than manually curated entries. To test this hypothesis, we repeated our analysis of the rodent and primate datasets using only manually curated annotation. However, because as much as 60% of the annotation is derived from automated approaches, the dataset is too small to draw reliable conclusions (primates ER 2007 j j 4, rodents ER 2007 j j 9; see Supplementary Figs S6 and S7 for details).
Region stabilityApplying the CR stability measure (distance d  1) summarizes significant categories into semantically related regions (). For primates, we identify four CRs. All four regions overlap, and the contents of the regions changed only slightly from 2007 to 2010. Considering regions instead of single categories, we obtain perfect stability (RSstab region  1 instead of RSstab basic  0:81). For rodents, there were four significant CRs in 2007 (CR 1 , CR 2 , CR 3 , CR 4 ) and 2010 (CR 2 , CR 3 , CR 4a , CR 4b ) but only three of them overlapped. One region (CR 1 'catabolism') lost significance, due to strong annotation evolution (see). Moreover, there was one very large region (CR 4 'response to stimulus'), where 13 categories were no longer significant in 2010. Six remaining categories were split into two regions (CR 4 a 'response to stress', CR 4 b 'immune system process'). The rodent dataset has a CR stability of RSstab region  3  4=4  4  0:875 (instead of RSstab basic  0:52). Comparing CRs incorporates semantic information, which leads to higher stability values since larger regions are considered. If the CR stability is reduced, we argue that there is a meaningful difference in results. With the datasets analyzed here, we see that ontology evolution operations and annotation changes can have effects on term-enrichment analyses. There is a substantial variability between the primate and rodent datasets. While the primate results proved to be relatively stable, we see meaningful changes for the rodent data. The primate dataset was more influenced by ontology changes while annotation changes had a higher impact on the rodent dataset. Term-enrichment analysis will often yield semantically related categories reducing the influence of changes in semantically related categories. The majority of changes have no effect on the semantic interpretation of functional enrichment analysis, although we found some instances in which the interpretation may change. We conclude that term-enrichment results are relatively robust to ontology and annotation evolution.
Simulated datasetsWe observe that several categories in the real datasets are affected by evolution of ontology and annotation, and that these changes lead to differences in the enrichment analysis results. To test whether the patterns we observe are generalizable, we generated 50 datasets, randomly seeding significant categories. Repeating this generation of datasets for two versions of the) enables us to test to what extent differences between these two ontology versions influence enrichment results. To distinguish semantically related categories that are likely equally affected by changes, from semantically distinct categories, we apply the CR stability measure with distance d  1.shows average values over all 50 random experiments (details in Supplementary Tables S4 and S5). Note that the. All other categories are present in both version sets. The lower boxes highlight evolution operations that happened to categories (dotted boxinformation reducing/revising, striped box-added information). Result categories (including GO accessions) for both species and years are shown in Supplementaryrandom datasets are larger and thus cover larger ontology parts, possibly leading to a generally lower stability. When testing changes from 2007 to 2010 (Task A), we observe on an average nineBecause we define categories as significant based on a fixed P-value cutoff, some of the categories that lose or gain significance may do so due to small fluctuations in P-value. To test for the relative contribution of this effect to the observed changes, we computed the differences of P-values between 2007 and 2010 for each significant category.shows the distribution of P-value differences for lost and gained significant categories. Most of the categories reveal only a relatively small change in P-value (50:1), showing that most category gains and losses are driven by small fluctuations in significance. However, some categories show substantial P-value differences, suggesting that real structural changes in the ontology or annotation are the basis for the change in P-value.
CONCLUSIONSEnrichment analyses use ontology and annotation to detect significantly enriched categories of genes. We studied the impact of ontology and annotation evolution on term-enrichment analyses by comparing term-enrichment results over different ontology and annotation versions. We proposed different measures to assess the stability of result sets and applied them to analyze the impact of evolution for two real-world and 50 random datasets. The GO undergoes continuous changes in its structure and the annotation due to the ongoing incorporation of new knowledge. These changes are unequally distributed and can cluster in regions representing specific topics. At the level of individual categories, the results of term-enrichment analyses can be significantly affected by ontology and annotation evolution. However, these changes do not necessarily change the interpretation of the result since these terms are often semantically related. This effect is captured by our CR stability measure. The experimental evaluation showed that term-enrichment analyses are generally robust to ontology and annotation evolution. Using our measures, users can identify categories that tend to change their level of significance due to structural ontology changes or heavily changed annotation sets. The two following audiences can benefit from our methods and results:Average number of significant result categories ( ER j j) and more compact CRs ( CR j j) only in 2007 (missing), only in 2010 (new), avgRSstab basic   average of basic stability, avgRSstab region   average of CR stability.(i) Ontology curators: For these users, it is important to determine whether planned changes in the ontology or the annotations result in a semantic change. We show here that structural ontology changes do not necessarily imply a semantic change of the results, and we provide stability measures that allow testing of proposed changes in the GO and the implications for functional enrichment analyses.(ii) Biologists using the GO for functional enrichment analyses: For these users, it is interesting to know that enrichment results may change over time due to changes in the ontology and annotations and that interpretation of their own results should be made with this in mind.We already provide a tool for ontology evolution evaluation (). We plan to extend this tool to detect annotation evolution and to incorporate the here-presented stability measures. The tool may be of particular interest for curators of ontologies to judge the potential impact of changes for users.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
A.Gro et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Impact of evolution at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
