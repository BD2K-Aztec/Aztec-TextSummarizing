Motivation: Direct sequencing of microbes in human ecosystems (the human microbiome) has complemented single genome cultivation and sequencing to understand and explore the impact of commensal microbes on human health. As sequencing technologies improve and costs decline, the sophistication of data has outgrown available computational methods. While several existing machine learning methods have been adapted for analyzing microbiome data recently, there is not yet an efficient and dedicated algorithm available for multiclass classification of human microbiota. Results: By combining instance-based and model-based learning, we propose a novel sparse distance-based learning method for simultaneous class prediction and feature (variable or taxa, which is used interchangeably) selection from multiple treatment populations on the basis of 16S rRNA sequence count data. Our proposed method simultaneously minimizes the intraclass distance and maximizes the interclass distance with many fewer estimated parameters than other methods. It is very efficient for problems with small sample sizes and unbalanced classes, which are common in metagenomic studies. We implemented this method in a MATLAB toolbox called MetaDistance. We also propose several approaches for data normalization and variance stabilization transformation in MetaDistance. We validate this method on several real and simulated 16S rRNA datasets to show that it outperforms existing methods for classifying metagenomic data. This article is the first to address simultaneous multifeature selection and class prediction with metagenomic count data. Availability: The MATLAB toolbox is freely available online at
INTRODUCTIONThe human body is inhabited by on the order of 10 14 bacteria, collectively known as the human microbiota, which contains 100 times more genes (the microbiome) than in the human genome. * To whom correspondence should be addressed. Since these microbes interact with our bodies and provide functions lacking in our genome, changes in the microbial community structure are thought to impact our health. Given the vast number of genes in the microbiome and our inability to sequence all of them, a marker gene is often used for comparison among samples. The 16S rRNA gene is the most common marker gene used since it is universally present and well conserved among prokaryotes. Sequencing of 16S rRNA in an environment containing a mixed population allows the surveying of community structure without biases from culture-based methods at a relatively low cost. Whole genome shotgun (WGS) sequencing of the community (a metagenome), on the other hand, can provide estimates of functional capabilities of microbiome (), but the cost is substantially higher. A main promise of metagenomics is that it will accelerate the discovery of novel genes and new drug target and provide new insights into diseases with unknown etiologies (). The first step of 16S rRNA metagenomic analysis usually involves the classification of sequences by organism to reduce the dimensionality of the dataset (from millions of sequences to thousands of organisms). In the process, the number of sequences classified to each organism is kept to provide an estimate on organism abundance. Classification of sequences are done by comparing sequences from a sample to 16S rRNA from known taxa or by clustering of similar sequences into operational taxonomic unit (OTU), which represent an unnamed taxon. The end result is a series of sequence read counts associated with taxa in the sample. A popular software package for assigning 16S rRNA to known taxa based on k-mer frequencies is called ribosomal database project (RDP) Classifier (). In addition to sequence classification, software such as Mothur () and QIIME () provides diversity metrics and sample comparison statistics allowing comparison of microbiome profiles using alpha (within-community) and beta (across-communities) diversities. For graphical comparison, MEGAN () can compare the OTU composition of frequency-normalized samples (). From these analyses, we can figure out what organisms are in each of the samples or classes of samples. While these tools can be used to compare and cluster samples based on microbiome profiles, they do not allow for the identification of differentially abundant microbes in samples. Therefore, the important question of which organisms (by virtue of their presence/absence or relative abundance) distinguish one
Sparse distance-based learning for metagenomic datasample class from another cannot be answered by these software packages. This can be done using MetaStats (), but this software can neither identify multiple differentially abundant microbes simultaneously nor classify samples into multiple classes. MetaStats also suffers the multiplicity problem with multiple tests.applied some existing machine learning approaches to the classification of microbiome data, but novel and efficient software is not yet available for multiclass classification of microbiome count data using supervised learning methods. Machine learning for multiclass (and more general multilabel) classification has been applied to microarray analysis, text mining and image identification (). The main objective of supervised learning is to predict the class of a future sample given the class and metagenomic count data. Most supervised learning methods fall into two general categories: instance-based and model-based learning. Instancebased learning (IBL), such as k-nearest neighbor (KNN) (), predicts the class of a sample with unknown class by considering the classes of k-nearest neighbors. It is more robust for data with unbalanced classes and is efficient for multiclass classification with a small number of features. However, accuracy diminishes with increasing irrelevant features because of the curse of dimensionality. On the other hand, model-based learning methods, such as support vector machine (SVM) and logistic regression, are mainly designed for binary classification. They are designed to separate two different classes as far as possible without considering the intraclass distances. Multiclass problems are often handled by combining binary classifier outputs, such as one class against the other (one versus one) or one class against the rest (one versus rest). However, when sample sizes are small, accuracy is reduced potentially due to noise and overfitting can occur since a high number of parameters needed to be estimated from a small number of samples [either c(c1)n/2 or (c1)n parameters needed to be estimated with c classes and n features]. Furthermore, these methods also create unbalanced classification problems with the one versus rest rule even if the original dataset is balanced. Instance-based learning only takes into account the minimal distance, while modelbased learning incorporates maximizing the interclass distances (e.g. maximizing the margin in SVM). The integration of instance-based and model-based methods can maximize the interclass distances while minimizing the intraclass distances. Current integration () only considers the labels of neighborhood instances as additional features for logistic regression, without utilizing the robustness of instance-based learning for unbalanced classes. Moreover, this method estimates many parameters and creates unbalanced classes in multi-class classifications, even if the original dataset is balanced. Because of the common issues associated with clinical samples: (i) small sample size and (ii) unbalanced classes, we propose a novel approach for multiclass classification through integrating instance-based and model-based learning to overcome these challenges in metagenomic data. Our proposed approach combines the KNN and SVM to simultaneously maximize the interclass distance and minimize the intraclass distance. This approach is robust for unbalanced classification, can classify multiple classes simultaneously without creating unbalanced classes and perform simultaneous feature (variable) selection and multiclass prediction with a simple parameter regulation, while estimating fewer parameters than previous approaches (only the same as the number of features). We apply our approach to 16S rRNA count data from metagenomic samples to select microbial taxa (features) that can distinguish one class of samples from others. The number of microbial taxa (features) is determined through cross-validation with smallest prediction error. We then use the selected features to build a weighted KNN classifier to predict a class for each sample. Because the dependence of the variance of the metagenomic count data for each taxon on the abundance of the taxa violates the homogeneity of variance assumption required for the application of many statistical methods, we developed variance stabilization methods to make non-homoskedastic count data easily tractable by standard machine learning methods. The current widely used data normalization method with proportion (relative abundance) only accounts for different levels of sampling across multiple individuals without adjusting for differences in variance. In this article, we describe several data normalization methods for variance stabilization before applying our proposed classification method. We evaluate the performance of our tool (MetaDistance) using simulated datasets and two publicly available real metagenomic datasets. The proposed methods are robust for all datasets and efficient for microbial feature identification and sample phenotype prediction.
METHODSTo understand the association between microbiota profiles and clinical phenotypes such as obesity, it is crucial to develop new supervised learning tools. We assume there are two or more populations with different clinical phenotypes (e.g. obese and lean, or different treatments and controls), each having multiple samples. We assume a set of non-overlapping taxa has been chosen, e.g. all genus-level groups appearing in the data. For each sample, we have one metagenomic count feature for each taxon, indicating the number of 16S rRNA sequence reads from the given sample assigned to that taxon, as shown in the following:where X is the metagenomic count matrix with n samples and m features, x ij denotes the total number of reads assigned to feature j in sample i, and y is the clinical phenotypes with g categories. y i  C ={c 1 ,...,c g }. Our goals are to identify features whose abundance in different populations is different, and to estimate the power of those identified features in predicting clinical phenotypes.
Data normalization and transformationThere are two sources of bias in the metagenomic count data: (i) different levels of reads (sampling) across multiple samples and (ii) dependence of the variance of x ij on its particular value. The large the count value, the larger the variance. Validity of many statistical procedures relies upon the assumptions of normal distribution and homogeneity of variances. However, the metagenomic count and related percentage data have variances that are functions of the mean and are not normally distributed but instead are described by Poisson, binomial, negative binomial or other discrete distributions. The variance heterogeneity and non-normality of the metagenomic count data can seriously increase either(1) Converting the raw abundance measure to a proportion (percentage) representing the relative contribution of each feature to each sample. This is to adjust for the sampling depth (read count) differences across samples. Mathematically, we normalize the metagenomic count matrix X into a proportion matrix P with(2) We then employ either the square root transformation or the arcsine transformation to the metagenomic proportion matrix P (or original count matrix X): @BULLET Square root transformation: This can be used either with the proportion matrix P or the original count matrix X, the transformed feature matrix@BULLET Arcsine transformation: This is well suited for metagenomic proportion data P withThis is very similar to the arcsine transformation with original count data X (), which defined aswhere L = max(X)+4 is the largest count value in count matrix X plus a constant number 4.Before we do any transformations, we will compute the mean and variance for each sample with matrix P or X, and then test the assumption of homogeneity of variances with Bartlett's test (). Either the square root or arcsine transformation will be used. Practically, if the percentage data have homogeneous variances, no transformation is needed. For data with variance heterogeneity, if the data lie in the range of 00.3 or 0.71 but not both, the square root transformation should be used. Otherwise, the arcsine transformation should be used. In most cases, we find both transformations increase predictive power and have similar performance. In this article, we therefore utilize the arcsine transformation with proportion data for all of our experiments.
Sparse-weighted distance learning with integrated KNN and SVMA general multiclass classification problem may be simply described as follows. Given n samples, with normalized features,}, where z i is a multidimensional feature vector with dimension m and g classes with class label y i  C ={c 1 ,...,c g }, find a classifier f (z) such that for any normalized feature vector z with class label y, f (z) predict class y correctly. Given two samples z i and z j , we introduce a general weighted distance functions for KNN as follows:where |.| denotes the absolute value, w k  0 for k = 1,...,m are the nonnegative weights and p is a positive free parameter. In particular, whenas the distance of z l to class c s. Finally, we assign z l to a class c j by means of a minimal distance vote.
Efficient quadratic SVM method for weight estimationNow, the problem left is how to find optimal w for high-dimensional metagenomic data. As we discuss earlier, we want to choose w with small intraclass distance and large interclass distances simultaneously and automatically identify the features relevant to the phenotypes. We, therefore, propose an efficient quadratic SVM for weight estimations as follows:is an element-wise operation, and , k and p will be determined through cross-validation. In Equation (2), the first constraint represents the KNN intraclass distances, and we restrict them to a soft upper bound 1. The second constraint indicates the interclass distances with a soft lower bound 2. Hence, we can enforce a soft margin 1 between the intraclass and interclass distances. Therefore, the solution of Equation (2) will guarantee a small KNN intraclass distance and large interclass distance simultaneously. Finally, the reason we used KNN instead of all the samples in the same class for the first constraint is that samples in one class may have multimodal distributions. It is too stringent and unrealistic to require that all samples in one class have small distances. Equation(2) is equivalent to the following problem:(3) is a much simpler truncated quadratic programming with non-negative constraints. It can be solved very efficiently, even if the problem has both large sample size and high dimension. The first-order derivative for Equation (3) is as follows:Based on Equation (4) and w k  0, we implement a standard conjugate gradient method () with non-negative constraints in MetaDistance. Because E is a convex optimization with a convex constraint,
Sparse distance-based learning for metagenomic dataa global optimal solution is guaranteed theoretically. The global minimum of E is reached when each element of w k satisfies one of two conditions: either (i) w k > 0 and (E/w k )|  w = 0 or (ii), w k = 0 and (E/w k )|  w  0. In the first case, the feature is identified as important by receiving a positive weight while the corresponding term in the gradient reaches zero. In the second case, the feature is eliminated as the corresponding term in the gradient remains positive even when w k reaches zero, at the edge of the feasible region. Letting, we have the following iterative gradient algorithm for E maximization and optimal weight estimation: Algorithm for optimal weight estimation: given p, k,  and = 10 6 , initializing, for k = 1,...,m.t , where t: the number of iterations and  t : the step size. @BULLET d t is updated with the conjugate gradient method:
Evaluation criteria and choices of parametersThe performance of Metadistance for multiclass classification is mainly evaluated with the prediction (test) error. The small the prediction error, the better the prediction accuracy. The average area under the ROC curve (AUC) is also used as a performance measure. AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. The large the AUC, the better the model performance. AUC for each class versus the rest is calculated with the KNN distance between the test data and each class. Intuitively, if a test sample is from that training class, their distance will be small. The average prediction AUC is then used as a measure of overall model performance. The free parameters , k and p are also determined by cross-validation with the smallest prediction error. The regular parameter  controls the sparsity of the model. The larger the value of , the fewer microbial features will be selected. If  is too small, there will be overfitting and little sparsity. If  is too large, the produced classifier will be very sparse (very small number of features with non-zero weight) but have poor predictiveness. The optimal  and the number of predictive features (variables) with non-zero weights are determined with smallest prediction error through 10-fold cross-validation. The parameters p and the number of nearest neighbors k are also decided by cross-validation. We limit k = 1,2,...,min n i , where min n i is the smallest sample size for one class. Our computational experiments with simulation and real data show k is in the range 525 for the best performance. For simplicity, we choose p = 1 or 2 only in all the computational experiments, but other choices of p do improve the predictive power of our method. Users should feel free to choose different P values in their computations. Quadratic SVMs are implemented in the MetaDistance software.
RESULTS
Simulation Data: in silicometagenomic datasets were generated to contain five classes (groups) in four samples sizes (10, 20, 50 and unbalanced sample size with 10, 20, 30, 40 and 50 for each class, respectively). Datasets (x ij ) were generated from negative binomial (NB) distributions with different means and dispersion parameters. The means for NB are simulated from the Gamma distribution with a mean () of 100 and variance ( 2 ) of 1000. The variance of NB is  2 NB = + 2 /scale, where scale = 1. We simulated 1000 features for each sample from NB distributions, which contained the first 10 relevant features having different distributions with distinguished s. We used 2-fold cross-validation to evaluate the method. First, we normalized the data with proportion and arcsin transformation, and then divided the data into training and test equal subsets. The training subset was used for model construction, while the test subset was used to evaluate performance. The model parameters k, p and  are determined from only the training data with leave-one-out cross-validation. Each simulation was performed 100 times for each sample size (). MetaDistance can identify differentiated features with high accuracy even if the sample size is small or unbalanced (). As the sample size increases so does frequency of correctly identified features. At 10 samples per class, MetaDistance identifies 80% of relevant features with over 72% accuracy and 30% of features with over 92% accuracy compared with 50% features with over 93% accuracy when there are 20 samples per class. Increasing the sample size to 50 leads to identification of 100% of relevant features over 93% accuracy. Our method performs well even if the dataset has highly unbalanced sample size. The method is able to identify 40% of relevant features accurately in all experiments with both sample size of 50 and unbalanced. The average number of features identified increases with the sample size for each class (). For example, a sample size of 50 for each class identifies 9.87 (of 10) relevant features on average. For comparison purpose, we
Z.Liu et al.also apply F-test (ANOVA) to 100 simulation data with the sample size of 50 for each class. In all, 7691 features are identified and the average number of features selected is 82.6 with P < 0.05. With P = 0.00005, 1627 features are selected and the average number of selected features is 20.2, which indicate that the false positive rate is high even if we adjust the multiplicity problem for multiple comparisons with a very conservative Bonferroni rule. Metadistance identifies multiple features simultaneously without encountering the multiplicity problem and it is more accurate in identifying true predictive features than the statistical test. The prediction error was calculated and compared with KNN and multinomial logistic regression (mlogit) in R (http://www.rproject.org/) (). MetaDistance outperforms KNN and mlogit in terms of prediction error rate. KNN had the highest error rate, likely due to the curse of dimensionality, making feature selection important when applied to high-dimensional data. Mlogit also performs more poorly than MetaDistance, due to unbalanced classification problem and small sample size. Since we observe alower error rate in MetaDistance with larger sample sizes, these results suggest an increases of sensitivity and specificity as sample size increases. In addition, the average prediction error (0.22) with the unbalanced dataset is slightly better than the prediction error (0.23) with the dataset of sample size 50, indicating our method is robust with unbalanced data.Benchmark metagenomic data: we applied our method to 815 16S rRNA metagenomic samples from six human body habitats (): external auditory canal (EAC), gut, hair, nostril, oral cavity (OC) and skin. Since the sample sizes range from 14 to 612 per habitat, this highly unbalanced dataset is dominated by one class (skin), which could create challenges for classification. Sequencing reads were classified into taxonomic groups using the RDP classifier using confidence threshold  0.5 1. About 5% of the sequences are not assigned to a genus by RDP. (56017/1070702 sequences are not assigned ) (). The aim of the analysis was to identify taxonomic makers per habitat, whereas the original study used a binary classifier to determine whether samples originated from the gut, OC or other sites (). Using 100 permutations, we split the data into training (2/3 of samples) and test (1/3 of samples) and estimated parameters  , p and k with 10fold cross-validation with the training data only. Relevance count was calculated by the number of permutations a taxon is selected in a model. This analysis was performed at the bacterial family and genus levels of taxonomic assignment, with optimal parametersof (410, 11, 1) for family and (450, 8, 1) for genus. The performances for this dataset are not sensitive to the parameter selections and the prediction errors are quite similar with a wide range of values for the parameters (). MetaDistance identified 11 taxonomic markers at the family and genus level (), most of which have relevance counts of 100. The mean relative abundance (reads) of selected taxa across samples varies from 7 to 339 reads (column 3, 6). The abundance of these taxa could be used as markers to distinguish samples from different classes. Calculated prediction error rate of 0.075 (family) and 0.064 (genus) are smaller than reported for OTU analysis (). Using the abundances of these taxa, we are able to correctly classify 94.1% of samples to their correct body habitat ().
Sparse distance-based learning for metagenomic dataThis accuracy ranges by body habitat between 60% (Hair) to 100% (Gut). The average AUC is 0.99 across six classes. These results suggest that MetaDistance can accurately predict classes even with highly unbalanced sample sizes. For comparison purpose, we also analyzed the metagenomic OTU count data with similar procedure using only 552 non-transplanted samples (details in Supplementary Material). MetaDistance achieves the best predict error (0.08). We also compute the average prediction AUC based on the KNN distance between the test data and each class. The average prediction AUC is 0.99 with only 13 OTUs, compared with 27 OTUs reported by. In addition, Gut and OC are perfectly separated from other classes, which is consistent with the result of.
Metagenomic data of skin sites:using this same dataset, we repeated our analysis on 612 skin samples (), with the aim to classify each sample into a subhabitat (for these subhabitats. Compared with the one-versus-one strategy, which needs to estimate 66 models, only one model is needed with MetaDistance to identify features which are differentially abundant and capable of predicting classes. We divided the data into two parts: one with 2 3 of the samples from each class as the training data and the remaining 1 3 samples as the test data. The parameters , p and k were estimated using 10-fold cross-validation with the training data only. The parameter p has the choice of value 1 or 2 only, k is chosen from 1 to 20, and  is selected from 1 to 40 with steps of 1. To prevent bias arising from a specific partition, we split the data 100 times and reported the relevance counts of the the identified taxa. The optimal parametersand (210, 5, 1) (genus). As shown in, we selected 12 taxonomic marker at the family and genus level assignment based on relative abundances, with test errors at 0.30 (family) and 0.31 (genus), comparable to previously reported best results (). The mean relative abundance (reads) of selected taxa across samples varies from 11 to 390 reads (columns 3, 6). Many of these markers are similar to those found in the habitat comparison, where skin samples were compared with other body habitats. Some of these taxa have been linked to disease, such as Prevotellaceae/Prevotella, which has been shown to be less prevalent in lean subjects (). Additionally, species from the family Acinetobacter have been linked to disease and are target for health studies (). We also plot a receiver operating characteristic (ROC) curve for each class versus the rest based on the KNN distance between the test data and each class from one run. Intuitively, if a test sample is from that training class, the KNN distance will be small. Otherwise, the distance will be large.shows the ROC curves and predictive AUC values at the bottom-right corner of each subplot. It is shown that Class 9: plantar foot is the easiest skin site to be separated from other classes with the prediction AUC of 0.99, and Class 2: external nose is the hardest skin site to be classified correctly with the test AUC of 0.74. The average test AUC for all classes is 0.88. The results indicate that the 12 identified taxa at family level have the predictive power for skin site discrimination. Obviously, it is more crucial to select important taxa that are highly discriminative forthis type of task, since the sites of sampling would most likely to be known. However, MetaDistance can certainly be applied for both taxa selection and class prediction with other types of metagenomic data where the category labels (such as disease status) are more expensive to obtain.
CONCLUSIONSWe have proposed a sparse distance learning method (MetaDistance) for multiclass classification through combining instance-based (KNN) and model-based (SVM) learning methods. The proposed method can identify phenotype-associated taxa and perform class prediction simultaneously. It is robust for unbalanced classification and can classify multiple classes simultaneously without creating unbalanced classes. In addition, this method estimates a small number of parameters (only the same as the number of features) and is very efficient for problems with small sample sizes, high dimensions and unbalanced classifications with many classes, which is common in genomic data. Experiments with limited simulation and real datasets demonstrated its effectiveness. While this method was tested on 16S rRNA, it can easily be applied to identify marker genes from WGS metagenomic and digital gene expression survey (SAGE) analysis without modification.
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
