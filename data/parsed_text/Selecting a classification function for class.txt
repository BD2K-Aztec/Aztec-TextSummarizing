Motivation: Class predicting with gene expression is widely used to generate diagnostic and/or prognostic models. The literature reveals that classification functions perform differently across gene expression datasets. The question, which classification function should be used for a given dataset remains to be answered. In this study, a predictive model for choosing an optimal function for class prediction on a given dataset was devised. Results: To achieve this, gene expression data were simulated for different values of gene-pairs correlations, sample size, genes' variances, deferentially expressed genes and fold changes. For each simulated dataset, ten classifiers were built and evaluated using ten classification functions. The resulting accuracies from 1152 different simulation scenarios by ten classification functions were then modeled using a linear mixed effects regression on the studied data characteristics, yielding a model that predicts the accuracy of the functions on a given data. An application of our model on eight real-life datasets showed positive correlations (0.33â€“0.82) between the predicted and expected accuracies. Conclusion: The here presented predictive model might serve as a guide to choose an optimal classification function among the 10 studied functions, for any given gene expression data. Availability and implementation: The R source code for the analysis and an R-package 'SPreFuGED' are available at Bioinformatics online.
IntroductionMicroarray gene expression profiling has become a widely used tool to identify particular disease subpopulations and to perform diagnostic and prognostic predictions (). In clinical practice, they are used in diagnostic and prognostic analyses while in preclinical studies (toxicogenomics), they involve predicting the toxicity of compounds in animal models with the goal of speeding up the evaluation of toxicity for new drug candidates (). Though class prediction analysis is a common practice, the question that remains to be addressed is, given the wide availability of classification functions nowadays, which classification function do we use for a particular dataset? Classification functions have been shown to perform differently across gene expression datasets (). Moreover, the MAQC-II initiative has pointed out that classification function is one of the variables that explains the variabilitybetween gene expression class prediction performance (). While substantial amount of information is known about the characteristics of classification functions and class prediction building procedures, little is known about which data characteristics have impact on the performance of a class prediction model. For instance, diagonal linear discriminant analysis (DLDA) assumes no covariances and hence no correlations between variables and might fail if the data is highly correlated. On the other hand, linear discriminant analysis (LDA) assumes a common covariance matrix for the classes and thus to some extent, accounts for correlations (). In addition, pernalized regressions like ridge, lasso, elastic net are capable to handle correlated variables. Support Vector Machine (SVM), though commonly understood as a method of finding the maximum-margin hyperplane, may also be seen as a regularization function estimation problem, corresponding to a hinge loss function with a quadratic penalty as that of ridge regression (). And it has been shown by () that if a group of non-distinct variables are selected as input variable set, its training time lengthened and the errors become bigger. On the other hand, tree-based methods are by nature designed to capture interactions between variables while neural networks might capture other complex structures within a given dataset. Given the above observations, it is obvious that the performance of these functions depends on the characteristics of the data in question. Despite this, the literature on how to choose a classification function for a given dataset is sparse. A common practice is comparing several classification functions and selecting the one with the minimum error rate but this has been pointed by, Tibshirani and Tibshirani (2009) and Varma and Simon (2006) to lead to selection bias. As such, some experimenters adhere to one or a few classification functions irrespective of the dataset, disease or medical question being addressed. While others choose a classification function for their datasets by affinity or familiarity without taking into account the characteristics of such data. A simulation study byshows that correlation is one of the data characteristics that affect the performance of most probabilistic classification functions. In addition,showed that correlation structures differ across gene expression data of different etiological diseases. The study byshows that microarray gene expression data characteristics like log 2 fold change of expression values, number of deferentially expressed genes and pairwise correlations between genes are associated to the accuracy of classification functions. However, this study was conducted in real-life gene expression datasets, where the magnitude and/or direction of association might have been confounded by unobserved data characteristics. In this study, we aim to provide a guideline for making a choice of a classification function for a binary class prediction problem based on observed magnitudes and directions of the data characteristics, using accuracy as a measure of evaluation. We investigate the effect of sample size, proportion of deferentially expressed (DE) genes, genes' variances, log fold changes, pairwise correlations between DE and noisy genes on the accuracy of classification functions using extensive simulations. The remainder of this article is organized as follows: methodology to simulate data, classification functions considered and the building and evaluation of class prediction models are presented in Section 2; Section 3 contains a predictive summary of the results of class prediction models for different simulated scenarios; Section 4 provides an application of our predictive model from the simulated results on real-life microarray gene expression datasets and Section 5 presents a discussion.
Methods
Simulated data (scenarios)To simulated gene expression data, we hypothesized that sample size, proportion of DE genes, genes' variances, log fold changes, pairwise correlations between DE and noisy genes might be associated to the performance of classification functions. These six variables were to be systematically varied in our simulations. From observed correlation structures in real-life gene expression datasets (), we generalized the structure as shown in, containing three clusters referred to as up-regulated (UR), down-regulated (DR) and noisy genes. The absolute values of pairwise correlation for DE genes (q) were varied as 0.00, 0.25, 0.50 and 0.75 with UR cluster taking oppositely-signed correlation values for DR cluster. The pairwise correlations both within the noisy cluster and between the noisy and the DE clusters were per gene-pair randomly drawn from a normal distribution centered at zero with a standard deviation h i.e. N0; h where h  0.00, 0.25, 0.50, 0.75. The scenario q  h  0.00 corresponds to complete independence. Resulting correlation values lying outside the intervalwere uniformly converted to the intervals. With the correlation values and the variances, the within covariance matrices R 0 and R 1 were constructed for the two classes. In addition, the proportion of DE genes (p) was also allowed to take up 1, 3 and 5% of the total number of genes, as values. This resulted to 192 different complex covariance matrices that were used to simulate the data for different values of other variables. Finally, two different values of absolute log 2 fold change (D) and three different sample sizes (n) were considered (). For a fixed number of genes (p  1000) and n samples, the samples' labels (0,1) were generated from a Bernoulli distribution with a probability 0.5 and the gene expression data of p  n dimension was generated from a multivariate normal distribution with mean vectors from a uniform distribution, U(6,10) of length p and the covariance matrices corresponding to the above description, using Cholesky decomposition Golub and van Loan (1996) as a method to determine the root of the covariance matrix. The mean log 2 expression values of DE genes were incremented or decremented with the corresponding log 2 fold change value for samples in class 1. The choice of multivariate normal(UR), down-regulated (DR) and noisy (Non-DE) genes distribution and mean vector corresponds to the practical assumption that gene expression data are normally distributed in log 2 scale and based on observation that the log 2 expression values often fall in the interval (0, 16). For each combination of the values of the data characteristics, the dataset was simulated as shown in(Algorithm 1), yielding 1152 different simulation scenarios, each of which was randomly replicated 1000 times.
Classification functionsTen elective choices of classification functions were chosen to represent the broad list in the literature that falls within the categories: discriminant analyses or Bayes classifiers, tree-based, regularization and shrinkage, nearest neighbors and neural networks methods. For discriminant analyses, linear discriminant analysis (LDA), quadratic discriminant analysis () and shrunken centroid discriminant analysis (SCDA) or prediction analysis of microarrays (PAM) () were selected. Random forest (RF) () was chosen as tree-based method while support vector machines (SVM) (Sch lkopf and), ' 1 penalized logistic regression or Lasso (PLR1) (), ' 2 penalized logistic regression or Ridge (PLR2) () as well as ' 1 and ' 2 penalized logistic regression or Elastic net (PLR12) () were considered for regularized and shrinkage methods. Finally, k-nearest neighbors (KNN) and feed-forward neural network (NNET) () were the lone choices for nearest neighbors and neural networks respectively. In machine learning, opinions are that super learners might provide good class predictions but model complexities of these learners are usually high. As such, super learners might not be useful in clinical practice where physicians often want simple class prediction models, that might yield a subset of genes (and possibly coefficients) for easy interpretation. This is because given a subset of genes, focus can be geared toward these genes rather than the entire genome for which experiments are often costly and time consuming. Thus, our choices of classification functions were driven by the choices often made and considered useful in clinical practice.Pairwise correlations of DE genes (q) 0, 0.25, 0.5, 0.75 Gene' variances (r 2  1/k) $Exp(k) k  0.25, 0.50, 1, 1.5 Pairwise correlations of noisy genes (c) $N(0,h) h  0, 0.25, 0.5, 0.75 50% of p were each up-and down-regulated.For each value of the six variables, the covariance matrix was constructed in step 1, the learning and test data were simulated at steps 24 and class prediction models were built and validated in step 5. Steps 25 were then repeated 1000 times
Building and evaluating classifiersTo assess the dependency of the chosen classification functions on characteristics of the simulated gene expression data, we built on each simulated dataset, class prediction models with all the classification functions listed above. The simulated dataset was considered as a learning set and for classifiers that require pre-selection of genes because of their limitation to accommodate a number of parameters greater than the number of samples (i.e. LDA, QDA and NNET), the genes were ranked by their moderated t statistics () using the learning set. The learning set was split into a 1 3 inner-test set and 2 3 learning set using 5-fold Monte-Carlo-cross-validation (MCCV) with stratification. The parameter(s) of the classification functions were subsequently tuned using the inner-learning set and evaluated with the inner-test set. These tuning parameters were: number of genes (top k) for LDA and QDA; shrinkage intensity of class centroids for SCDA; with a fixed forest size of 500 trees, the number of variables randomly sampled as candidates at each split and minimum size of terminal nodes for RF; with a linear kernel, the cost of regularization for SVM; ' 1 penalty for Lasso; ' 2 penalty for Ridge; ' 1 & ' 2 penalties for Elastic net; number of nearest neighbors for KNN and finally, the number of genes (top k), number of units in a hidden layer and decay weights for NNET. With the optimal parameter(s) for each classification function, the class prediction models were built using the learning set. The resulting models were evaluated on a test set consisting of 5000 samples generated from the same model as the learning set (see). The error rates of the classification functions on this test set were recorded. The process was repeated 1000 times (sampling both learning and test sets) for each simulation scenario and the resulting error rates over the 1000 replications were used for further analyses.
Random effects linear regressionAn average of the error rates of each and every classification function over 1000 replications for each simulated scenario was computed yielding 11 520 data points resulting from the 1152 different simulation scenarios by 10 classification functions. The error rates were then transformed to accuracies (1  error rate  0:001 and these accuracies were modeled using a linear random effects regression model with the classification function as the random effects clustering variable, by transforming the accuracies to an unbounded range using the logit function. For the ' th standardized study factor, the random effects model is written as:where 0 < px ij  < 1 is the average accuracy in scenario i for classification function j, # j  # 0j ; # 1j  0 $ N 0; D  are respectively the random intercepts and slopes of the classification functions while ij $ N 0; r 2   are the independent and identically distributed residuals, also independent from the random effects # j. D is a 2  2 covariance matrix of the random effects. All the aforementioned study factors were evaluated by univariate and multivariate linear random effects regression models. Multivariate regression evaluation was done by a backward selection approach. In each step, two nested models, with and without a particular study factor, were compared by log-likelihood ratio test at 5% significance. Each factor ' was also evaluated by its explained-variation defined as:where MSE null and MSE l are the mean square errors of the null (random intercept only) and the l th standardized study factor models respectively. The explained variation of the selected multivariate model was also evaluated.
SoftwareAll statistical analyses were performed in R software version 3.2.0, and Bioconductor () using the following packages: mvtnorm () for simulating data, limma () for ranking genes via linear models, CMA () for predictive classification modeling, lattice () for visualization and lme4 () for linear random effects modeling. Additionally, we have developed an R package called 'SPreFuGED': Selecting a Predictive Function for Gene Expression Data, that allows researchers to determine an optimal function for a given dataset.shows the average error rates over the 1000 random replicates (y-axis) of the functions (x-axis) for different combinations of variances, pairwise correlations of noisy (non-DE) genes and DE genes for a fixed sample size (n  100), proportion of DE genes (p  5%) and log 2 fold change (D  1). From this figure, one sees that, the error rates for all functions increase with increasing variances (from top-to bottom-row), pairwise correlation values of non-DE genes (from left-to right-column) and pairwise correlation values of DE genes (different colored lines). On the other hand, other scenarios for different values of sample size, proportion of DE genes and log 2 fold change (SupplementaryC) indicate a negative association of sample size, proportion of DE gene and log 2 fold change to the error rates. The non-constant variability of the error rates between classification functions across scenarios indicates a scenario-specific optimality for each and every classification function. The average accuracies (1  average error rates  0:001) of the simulations were summarized to a data matrix as shown on. For each of the predictive variables, a linear random effects regression model was fitted as described in the method section. The individually explained variances of the study factors are depicted on. This figure shows that sample size, pairwise correlations of non-DE genes and the proportion of DE genes are the leading factors respectively accounting for approximately 17, 14 and 13% of the null variance. While genes' variances and fold change respectively account for 8 and 7% of the null variance, pairwise correlations between DE genes accounts for simply 1%. As observed graphically, the univariate models (results not shown) confirmed a positive association of sample size, proportion of DE genes and fold change, and a negative association of pairwise correlations of non-DE, DE and the genes' variances to the accuracies. For the multivariate linear random effects regression model, we started with a complex model of random intercepts and slopes and three ways interactions of the predictive factors. Starting with pairwise correlation between DE genes because of its low individually
Results
Classification in gene expressionexplained variance, we eliminated variables using the log-likelihood ratio test. We ended up with the model presented onconsisting of the fixed effects two ways interactions of all the six predictive factors, random intercepts and slopes. This model explains approximately 70% of the null variance as illustrated on. The left panel ofpresents the estimates of fixed effects, the standard errors and the t statistics while the top-right panel presents the net effect of a standard deviation (SD) unit increase of a given factor conditional on common values of other factors. Finally, the bottom-right panel presents the performances of the classification functions at different values of the predictive factors. From the top-right panel of this table, one notices that a 1 SD unit increase in sample size, corresponding to n  89.67 will lead to an increase in the Log odds (accuracy), with the highest increase observed when other variables are at their highest values. A similar effect is observed for a 1 SD unit increase in the proportion of DE genes. Though a 1 SD unit increase in fold change leads to an crease in the Log odds, as sample size and proportion of DE genes, its effect is highest when the other variables are at their lowest values. While on the average a 1 SD unit increase in the genes' variances, pairwise correlations of non-DE and DE genes will lead to a decrease in the accuracy, these effects become very severe when other variables are at their highest values. For very low values of other variables, a 1 SD unit increase of pairwise correlations between DE genes could even lead to an increase (a positive effect) on the accuracy as was previously observed and illustrated diagrammatically by. A similar effect is observed for a 1 SD unit increase in the genes' variances at very low values of other variables. These varying effects, indicate the complex interactions between the study factors and hence illustrate why classification functions will perform differently on different datasets. Lastly, the bottom-right panel of the table shows that all classification functions will perform reasonably well if the predictive factors are at their average values (0 SD) with PLR1, PLR12, LDA and QDA having outstanding performances. For extremely small valueswhere both PLR1 and PLR12 fail when other variables are fixed at-2SD and otherCorr or DECorr is varied from-1SD to 2SD. It must be noted however that the combination of all other variables simultaneously being at 2, or at 2, is highly unlikely.
LDA
ApplicationTo evaluate the predictive ability of the here presented random effects regression model on real-life data, eight Affymetrix geneClassification in gene expressionexpression datasets of the 25 non-cancerous datasets described in one of our previous studies () were used. These datasets were selected to include a variety of Array platforms, both class-balance and class-imbalance, number of DE probesets, as well as various sample sizes. Three of these datasets were preprocessed without filtering while the other five were preprocessed and filtered as described by. We quantified the data characteristics studied and presented onas follows:(i) sampSize, by counting the samples in the study, (ii) propDE, by ranking the probesets using limma () and computing the proportion of DE probesets based on a log 2 fold change cutoff of 1 if the number of DE is !10 or 0.5 otherwise, (iii) variance, was determined as the mean of the variances of all the probesets, (iv) log2FC, computed as the mean log 2 fold changes of the DE probesets, (v) deCorr as the mean of the elements of the upper-(lower-) triangular of the correlation matrix of the DE probesets and (vi) otherCorr, was computed as the standard deviation (SD) of the elements of the upper-(lower-) triangular of the correlation matrix of non-DE probesets. This matrix was computed from all non-DE probesets if they were less than 20 000 or a sample of 20 000 from these non-DE probesets otherwise. These data characteristics were standardized using the mean and SD of the respective variables from the simulated data. And our model was used to predict the accuracies for all classification functions in each dataset (Supplementary). We then built and evaluated classifiers using the classification functions by splitting the data into 2 3 learning set and 1 3 test set with stratification and a 3-fold inner cross-validation on the learning set for parameters optimization. This step was repeated a hundred times, each time predicting the accuracies of classification functions on the learning set using the random effects model and also recording the expected (observed) accuracies on the test set. These predicted and observed accuracies over the 100 repetitions are respectively presented on Supplementaryand B. To compare the predicted to observed accuracies, and considering that we are interested in the ordering of performance (i.e. determining an optimal function for a given data), we used the ranked base Spearman correlation between the average predicted accuracies and the average observed accuracies. The results of this comparison for each dataset are presented on. The positive correlation values on this figure indicate agreement between our predicted and observed accuracies. Though these correlations are not very high in some datasets, our model more or less determined an optimal classification function for all the datasets except for UC7 where Ridge regression and SVM emerged first instead of fourth as predicted (i.e. 87.5% sensitivity). Nevertheless, the model was able to rule out on which classification(s) will perform poor on a given dataset, with approximately 100% certainty. As expected, the performance of the functions deteriorate on CF (small sample size and low proportion of DE probesets), Dia2 (high class-imbalance and small fold changes), UC2 (low proportion of DE probesets and small fold changes), UC3 (large variances and high correlations) and UC7 (low proportion of DE probesets). Fromand Supplementaryand B, one sees that except on the UC3 data, our model's accuracies are less than or equal to observed accuracies. The model performs well on dataset with large sample sizes and balanced classes (UC2, UC3 and UC5). It attained its lowest performance on Dia2 where there is high class-imbalance and hence few samples of the small class in the learning set and on HIV2 and CF datasets with small sample sizes.
DiscussionWe hypothesized that the performance of classification functions on gene expression data depends on sample size, proportion of DE genes, genes' variances, log 2 fold changes between DE genes and magnitude of the pairwise correlation within DE genes and non-DE genes, and showed their association to the accuracies of ten often used and clinically relevant classification functions using simulations. Additionally, we built a predictive model to determine an optimal classification function among the studied functions using the simulation results. An application of the predictive model on eight non-cancerous real-life gene expression datasets predicted optimal function(s) for seven out of the eight and was able to rule out function(s) that will perform poor on almost all the datasets. This model may serve as a guide for choosing a classification function for a given gene expression data. Classification functions have been shown to perform differently across gene expression datasets () and data characteristics have been shown to differ across datasets and are associated to the performance of classification functions (). While sufficient knowledge is available on the properties of most classification functions and procedures to build class prediction models using gene expression data have been outlined by, little is known about data characteristics that accounts for the variability in the performance of classification functions and how to use these characteristics to choose an optimal classification function for a specific dataset. As such, most researchers adhere to specific classification function(s) or randomly choose a classification for their class prediction models irrespective of the disease or data under study. A common practice is to evaluate several classification functions and select the one with smallestmisclassification error but this leads to selection bias (). In this study, we outlined data characteristics together with clinically relevant and often used classification functions and investigated their effects on classification performance using simulation studies. Based on these simulation studies, we provided a guide for choosing an optimal classification function for a specific dataset using the data's characteristics and the studied classification functions through a linear random effects predictive model. As a metamodel one would expect it to explain close to 100% of the variance in the simulated data but our predictive model accounts for approximately 70% of the variability in the simulated data. The remaining 30% unexplained variance may be associated to sampling variability stemming from the several (192) random covariance matrices used to generate both learning and test sets as well as the different learning and test sets generated at each iteration. Although we used different classification functions and evaluated these functions using accuracy, our simulation results confirm the findings of Kim and Simon (2011) that classifiers tend to have poor performance on highly correlated data. Our results also agree with those ofthat correlations, the absolute log 2 fold changes and the number of DE probesets are associated to the accuracy of a class prediction model. In addition, these results specify clearly the directions of the association and point out the effects of other data characteristics like sample size, genes' variances that were not previously identified.Most importantly, we have provided a predictive model that can serve as a guide to choose a classification function for a given dataset and its application on eight real-life datasets (both filtered and unfiltered) indicated a good predictive ability of the model. Although our model was reasonably good in its prediction on real-life data, we want to point out that it might have failed in some datasets because of the following reasons: (i) most of the eight non-cancerous datasets had small sample sizes and splitting these datasets to learning and test sets yielded even smaller sample sizes of the learning sets and hence might have led to poor estimates of the characteristics under study and (ii) the observed accuracies might not be the true accuracies because of the few Bootstrap samples. It could have been better if we had the means to perform several Bootstraps but due to the small sample sizes, the number of independent Bootstrap samples is limited. The fact that our predictions were most often slightly lower than the observed accuracies for almost all classification functions might indicate the general trend that the performance of a model usually decreases on an independent dataset. Hence, our model's predictions might reflect expected accuracies on independent datasets. In the simulated data, we assumed exponential and normal distributions for the variances and pair-wise correlation of non-DE genes respectively. These distributional assumptions might be violated in some datasets. As such, it will be worth trying different distributions. Also, we used accuracy as a measure of evaluation by minimizing the loss function but in clinical applications, probabilities are more informative than simple yes or no predictions because they quantify theuncertainty of a prediction (). As such, it is worth evaluating these data characteristics on probabilistic classification functions where by the log-likelihood function is optimized, this might possibly provide a predictive model that will be most useful in clinical applications. Despite these limitations, our model was found to work well with data containing reasonably large and balanced sample sizes (n ! 30). As such, our results apply to balanced class data. For data with class-imbalance some classification functions will have deteriorating performance, for which several solutions are proposed. However, this topic is outside the focus of the current study. In summary, our results serve as a guide to use data characteristics to choose an optimal classification function for a given dataset.
at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
V.L.Jong et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
