Motivation: Cancer biology is a field where the complexity of the phenomena battles against the availability of data. Often only a few observations per signal source, i.e. genes, are available. Such scenarios are becoming increasingly more relevant as modern sensing technologies generally have no trouble in measuring lots of channels, but where the number of subjects, such as patients or samples, is limited. In statistics, this problem falls under the heading 'large p, small n'. Moreover, in such situations the use of asymptotic analytical results should generally be mistrusted. Results: We consider two cancer datasets, with the aim to mine the activity of functional groups of genes. We propose a hierarchical model with two layers in which the individual signals share a common variance component. A likelihood ratio test is defined for the difference between two collections of corresponding signals. The small number of observations requires a careful consideration of the bias of the statistic, which is corrected through an explicit Bartlett correction. The test is validated on Monte Carlo simulations, which show improved detection of differences compared with other methods. In a leukaemia study and a cancerous fibroblast cell line, we find that the method also works better in practice, i.e. it gives a richer picture of the underlying biology.
INTRODUCTIONIn this article, we develop a likelihood ratio (LR) test that aims to detect small concordant changes in a collection of related signals under two experimental conditions. Under certain conditions, it can be shown that the LR test is the most powerful test in many practical testing problems and potentially offers deeper insight into complex phenomena, e.g. the biological processes underlying cancer development. Exact calculation, however, is not always easy. Previous work () used LR tests for evaluating changes in a single signal in the area of microarray analysis. Our approach here differs from theirs by considering multiple concordant changes. () suggested to use Hotelling's T 2 statistic for this purpose, but this is only a viable alternative when the number of observations exceeds the number of channels. This is * To whom correspondence should be addressed. in many modern genomic applications not the case. Alternatives are ANCOVA approaches (), but compared with the Hotelling T 2 test they lose some power for correlated channels. Our approach is further motivated by recent work that developed a global gene expression estimation method for testing association with clinical outcome (), although that test either makes asymptotic assumptions for calculating significance, which may be too liberal, or uses permutation, which may not be optimal. The test we propose takes into account the 'magnitude and uncertainty' of the changes. In the field of microarray analyses, two-stage methods have been proposed that look for over-represented functional classes, e.g. as defined by gene ontology (GO), among differentially expressed genes (). Although these methods are very powerful and simple, they tend to look at clearly differentially expressed genes among which patterns are to be found. These methods are exploratory in nature and often require an arbitrary cutoff for the second stage analyses. Our method is more a higher level exploratory method, which is more sensitive to concordant small changes and does not require arbitrary dichotomies.
MODEL FOR RELATED SIGNALSOur aim is to devise a model for the expression of replicates of m-related signals measured across two different conditions, say x and y. This is a situation that is common in many high-frequency sensing data, such as in astronomy, geography and finance. The use of traditional mixed-effect models has also proved popular within functional genomics. () proposed a simple mixed model combined with two-stage effect estimation for detecting individually differentially expressed genes. Their model allows for systematic nuisance effects; however, the estimation of the effects of interest, i.e. the signals, is performed one-by-one using individual gene-specific variances. These individual gene variances ignore the fact that there is something common about the underlying measurements. Others, such as () and (), have proposed models with a common variance, stressing that one can borrow strength from measurements in the other channels, but ignoring that there can still be orders of magnitude difference between measurements. Ordinary mixed-effect models are therefore not suited for modelling sets of related, but possibly quantitatively different signals. Instead, we need to think about a model that data adaptively can move between a model that assumes individual variances for each of the signals, to one that takes a common variance. For thispurpose, we propose a hierarchical model, in which the variance term is modelled explicitly. Initially, we assume that after some suitable transformation, the signals can be considered to be normally distributed with a common variance across the conditions, but potentially varying across the signal sources,describe the average signal strength under the two conditions for channel j. Although in some cases the number of subjects per condition are the same, we do not assume this here. Let n x j and n y j be the number of replicates for gene j of condition x and y, respectively. We assume that channels measure similar kinds of quantities. This is for instance the case in microarray experiments, where each 'channel' is a gene-specific probe. In order to gain strength from the presence of 'similar' signals, we shall assume the unknown variance  2 j of channel j to be related to the variance of the other channels on the measuring device, in that all come from a 'common' inverse gamma distribution,  2 j  1 (,). The within-channel variation for m channel can be represented as a two-layer hierarchical directed acyclic graph (DAG), shown in. The full log-likelihood can be written aswhere for notational simplicity, we introduce the quantities
INFERENCEThe aim is to test whether the two conditions are same or differ in one or more channels. Traditionally, one considered series of univariate tests to tackle this question. However, modern measurement instruments typically have lots of parallel channels. Besides the difficulty of inferentially combining lots of tests, the fundamental problem is that univariate tests ignore the information, resulting from the similarity of the channels. Therefore, here we consider a single test, which tests the joint equality of all the average signal strength in all the channels across the two conditions,Asymptotically, under certain regularity conditions, the likelihood ratio statistic provides the most powerful test. However, when each of the signal sources has only a small number of observations, the distribution of such statistic can deviate heavily from its asymptotic  2 distribution. The Bartlett correction is a first-order correction of likelihood ratio test statistic and involves rescaling the test statistic to have the correct  2 mean under the null hypothesis with a finite sample. The model we have described is a highly parametric model. However, inference based on the model is not very much affected by violation of these parametric assumptions. In particular, the likelihood ratio statistic will, even for skewed distributions such as log-normal distribution, be approximately  2 distributed if n  30. For larger m the likelihood ratio statistic will become more and more normally distribution, irrespective of the number of observations n. In realistic scenarios, such as microarray studies where the groups are defined through GO terms or KEGG categories, the group size m is typically at least 20. For a moderate number of observations, e.g. n = 38 such as in the leukaemia example in Section 4.3.2, this means that even if the underlying data are not normally distributed the likelihood ratio test is still expected to work. Often data in measurement channels are strictly positive and therefore right-tailed. In such cases, it is obviously possible to transform the data beforehand, e.g. by some power transformation or the log-transformation.
Likelihood ratio statisticIn general, the logarithm of the likelihood ratio test statistic is defined aswhere 0 are the hypothesized values under H 0. We consider separately the maximization of the likelihood under H 0 and the full parameter space.Expressions for finding the maximum log-likelihood l 0 and l 1 as a function of ,  and  can be found by setting the partial derivatives of Equation (1) to zero, i.e.Copyedited
E.Wit and D.J.G.Bakewellfor the unconstrained solution l 1 , whereas for the constrained solution l 0 , we havFurthermore,   l(,, ) = 0 results inand   l(,, ) = 0 leads towhere the 'digammawher  j is defined with respect to the constrained and unconstrained estimatesestimates estimates x j andand and y j for l 0 and l 1 , respectively. The procedure for finding the suprema of the log-likelihood l(), i.e. l 0 and l 1 , is an iterative process. The initialization step finds the approximate location ofof of MLE as  0 by maximizing (8) over some grid {0,d 1 ,...,d max }, where, e.g. d max  = 10 10. Then, we progressively refine the starting point using NewtonRaphson (NR),Computationally less demanding, but leading to the same results, is iteratively solving Equation (7) for  by fixing  =  ( k ) using NR and then replacing  byby by( k+1 ). The NelderMead (NM) simplex method (), that does not require the explicit use of partial derivatives, provides a robust alternative to NR iteration. Using the same starting values  0 , the NM yielded excellent agreement with NR. In cases where the maximum likelihood for  achievesachieves achieves MLE =  max , then it is clear from Equation (6) that als  tends to be large. Practically, this means that the distribution of  2 j is close to being degenerated in a single point around its mean / 1. In other words, each channel has the same signal variance. By lettingletting letting , Equation (6) simplifies to  = nm / m j=1  j , so that j {1,...,m} the channel variance estimate degenerates inwhich is the usual pooled ML estimate of the variance. In order to be able to evaluate the values of l 0 and l 1 within the likelihood ratio test statistic, we require the value of log-likehood under these estimates. Applying an asymptotic expansion for the gamma function () to Equation (1), the log-likelihood at the maximum is given by,
Bartlett correctionFor large samples, n = n x +n y , the distribution of (x,y) is approximately distributed like a  2 q distribution, where q = dim dim 0 is the difference in the number of free parameters in and 0. We define the Bartlett correction aswith l 0 and l 1 defined in Equations (2) and (3). By defining the small sample likelihood ratio statistic as BC (x,y) = (x,y)/BC, we achieve that precisely the Bartlett corrected likelihood ratio statistic has E H 0 { BC } = m as would be expected for a  2 m distribution. Calculation of these two expectations in general is very involved. If we use the characterization for lim
l() defined in Equation(9), we can get an explicit approximate expression for the Bartlett correction,Copyeditedx ij. Since for small values of m, the supremum of the likelihood in Equations (2)and (3) are actually found at such degenerate , the approximation in Equation (11) can be exact in certain cases. The log expressions are normally distributed, so that under the assumption of no differential expression and in the degenerate case  2 j =  2 , we have that x j and B(x,y) is the Beta function. The expected value in Equation (13) can be found by applying a transformation theorem (), whereas subsequent approximations use the following expansion of the psi function, (z) = ln(z)1/2z +O(z 2 ) (, p. 259),Given that (z +1) = (z)+1/z (, p. 260), it can be shown that Equation (16) is a lowerbound for Equation (15), which results in an equality if and only if m = 2. Similarly, Equation (17) is an upperbound for Equation (15). Using these three approximations, we can order the three forms of approximately Bartlett-corrected likelihood ratio statistic:The corresponding p-values, therefore, have the reverse ordering, making the simple n/(n2) correction the most conservative of the three test statistics.
Evaluation of the Bartlett correctionIn this section, we test the small sample behaviour of the Bartlett correction. We compare the three forms of the Bartlett correction with Monte Carlo simulations of the same value. We also check theshows the comparison between the three forms of the Bartlett correction. The column with the Monte Carlo approximations should be seen as the (approximately) true values that the final three columns try to approximate. Importantly, the Bartlett correction that we propose is independent of  and , given that it is based on the asymptotic approximation (9), in particular, on large values of . Therefore, it is not very surprising that the first approximation (15) does an excellent job in capturing the Bartlett correction for  = 10 000. However, for large values of  and , each of the channels is forced to have a similar variance and simpler methods would be available. On the other hand, the approximation (15) is somewhat liberal for small values for  and . The somewhat more conservative and very simple upperbound (17) of the Bartlett Correction results in better agreement with Monte Carlo runs in that case. For small m, the latter lies closer to the upperbound and for large m closer to its lowerbound. Therefore, we conclude that for cases in which there is some channel variance heterogeneity (i.e. small to moderate , relative to  ), the simple Bartlett-Correction approximation, n/n2, does do an excellent job as first-order correction of the likelihood-ratio statistic.
Dependent channelsThe Bartlett-corrected likelihood ratio test (BC-LRT) we proposed in this section makes some allowance for dependence between the channels. In fact, the common variance distribution induces some dependence on the measurements within the same channel. However, conditionally on the variance the data from the individual channels are assumed to be independent. This may be unproblematic in many practical circumstances, especially when m is small and the channels show only small correlation. However, in many circumstances the dependence between the channels may be substantial. For example, voxels on a fMRI scan or messenger RNA (mRNA) data from genes with a common transcription factor will show high interdependence. In such cases, we should make allowance for the fact that the information that comes from the various channels cannot be considered m pieces of separately supporting evidence. In this section, we describe how this impactsThe value m refers to the number of genes in the GO-class, whereas m * is the number of independent channels as estimated from the data according to Equation (18). the likelihood-ratio statistic and how we can accommodate this in the test. Crucially, as the likelihood ratio statistic, conditionally on  and  is a sum of channel data, dependence between the channels will not affect the mean of the likelihood ratio statistic. Therefore, as the Bartlett is a mean-value correction, it, conditional on  and , is also not affected by the dependence. Clearly, the shape of the distribution is affected. In the extreme case, if the data in a particular group consisted of m identical copies, the likelihood ratio statistic for sufficient sample size n would be a rescaled  2 1 variable under H 0 , in fact m 2 1 distributed, rather than a  2 m random variable. The following shows a practical guide to adjust the likelihood ratio statistic in the case of dependence between the variables. The idea is to estimate the number of independent variables by the number of channels needed to explain at least, say, 95% of the correlation in the data. This is done by considering eigenvalues of the observed correlation matrix and calculating the number of eigenvalues to exceed 95% of the total sum. Notice that if n = n x +n y < m, the rank of the correlation matrix is less than full rank and this method will always conservatively suggest dependence, whereas the data could be fully independent. In fact, when n < m the method work, but will give conservative P-values. If one has additional information that the channels are stochastically independent and the number of observations n is of the same order as or smaller than m, it would be better not to use the correction. Consider the following two examples. In two groups of size m = 4 channels, the observed correlation matrices are, respectively, given as1. Consider data x and y from, 2 conditions respectively, with n x and n y replicates across m channels.
E.Wit and D.J.G.Bakewell
Calculate the observed mm correlation matrix3. Let  1 ,..., m be the eigenvalues of R.
Define the number of independent variables m * asthe smallest value for which the relative sum of eigenvalues exceeds 95%, i.e.5. Calculate likelihood ratio test statistic with the most conservative Bartlett correction (17) and transform this into a reduced test statistic: *Under H 0 , we have that approximately *Then it is clear that there are really only two independent channels in both cases. This is consistent with fact that in both cases ( 1 + 2 )/( 1 +...+ 4 ) = 0.995 > 0.95. We also apply this to the leukaemia example discussed in Section 4.3.2.shows a subset of the 235 GO terms under consideration and how the potential dependence affects the power of the test.
APPLICATIONThis section applies the method both to simulated and real data. It is important to note that the computational performance of the method does not deteriorate with larger number of observations or channels. In fact, the numerical maximization described in Section 3.1 converges slower with a small number of channels, since the optimum is more likely obtained for a degenerate channel variance (, ), which is computationally more expensive.
Simulation studyIn this section, we ascertain two aspects of the Bartlett-corrected test, to wit (i) its nominal coverage probability and (ii) its power. We consider two series simulations to test either aspect of the test. We would like to compare the method with possible alternatives. The Hotelling T 2 test () is a natural alternative to our method as it is similarly multivariate and under full channelvariance inhomogeneity (close to) the optimal test. One important disadvantage of the Hotelling T 2 test is that it is only defined for a few more observations than channels, i.e. n > m+2. This means that for the simulations n x +n y = n = 4,m = 2 and n x +n y = n = 4,m = 5 no Hotelling T 2 alternative can be calculated. This limits the use of the Hotelling T 2 , alternative in sparse data situations. In theseof the BC-LRTs, in comparison with idk corrected univariate t-tests (n = 4,m = 2 and n = 4,m = 5) and the Hotelling T 2 alternative (n = 12,m = 2 and n = 12,m = 5) based on 300 simulations cases, it is possible to test the composite null hypothesis through a series of univariate tests, joined through multiple testing correction, such as a step-down idk correction, i.e. P idk = 1(1P (1) ) m , where the P (1) is the smallest observed P-value (). We would like to point out that, despite the small sample sizes, the t-tests are the optimal tests to use 'from a univariate point of view'.
Is the method unbiased?In this section, we show that the method does not systemically give low P-values. This is essential, otherwise it would lead to an unfair comparison to the other methods. A way to test whether there is bias is to apply the method in the case where there are no differences between the channels. In that case, the resulting P-value should be an uniformly distributed value between 0 and 1. The ideal line inshould therefore be a straight line going from (0,0) to (1,1). In the first set of simulations, we consider 300 draws from the null model, according to Equation (2), in which  = 3 and  = 1/3. Furthermore, we vary the number of observations (n = n x +n y ) at two levels, 4 and 12, and the number of simultaneous channels (m) at 2 and 5. These are challenging conditions for inference due to a large variance heterogeneityas a result of a small and a small number of observations n. The aim is to see whether the method gives indeed rise to approximate uniform P-values. The plots inshow the results for each of the four scenarios for the raw LR test, for two version of the BC-LRT and for Hotelling T 2 test. What we can see is that the most conservative Bartlett correction (17), i.e. the simple n/n2, is very close to the nominal coverage probability in each of the simulations. Notice that for two channels m = 2, there is no difference between the Equations (15) and (17) and therefore only the latter is shown. The Hotelling T 2 test naturally achieves the nominal coverage probabilities by the very definition of a T 2 distribution.
Power of method
E.Wit and D.J.G.Bakewell(b) (a)channels show activity in the presence of channel heterogeneity ( = 3 and  = 1/3). We perform a total of 600 simulations, whereby half of the null hypotheses are true and the other half false with effect size  x  y = 0.5. We plot the receiver operating curve for the simulation in. From this we can clearly see that the BCLRT has significant power. However, in cases where the Hotelling T 2 test can be used, i.e. if the number of observations are somewhat larger than than the number of channels, to wit n > m+2, then this test achieves a higher power. This is due to the fact that in these simulation there are so few channels m: it is impossible to 'borrow' much 'strength' across the channels present. A second simulation shows the dependence of the power of the likelihood ratio test on the variance heterogeneity and the numbers of channels present. We look at m = 30 channels with the same average variance as above, but with two levels of variance heterogeneity, i.e., we can see how in these cases the likelihood ratio test beats the Hotelling T 2 test. Moreover, the more variance homogeneity, the larger the power of the test. The power of Hotelling T 2 test does not depend on the variance heterogeneity and only a single line has been plotted. As before, the simulation is based on 600 iterations, whereby 300 under H 0 and 300 under H 1 (here, effect size is taken  x  y = 0.1).
Comparison with other methodsIt is crucial to compare the performance of the test with other standard methods for combining P-values across joint hypotheses. Effectively there are two approaches. Traditionally, the most common method is to combine univariate P-values into a single group P-value that represents the overall significance of those group of test. Fisher's combined probability test () was proposed early on as a way to test a single (joint) hypothesis through m independent P-values. Under the null hypothesis each P-value was independently uniform and, therefore, minus the sum of the log-transformed P-values would be  2 distributed with 2m degrees of freedom. Despite its easy applicability, this P-value combination method is not very often applied in a bioinformatics context. More commonly, P-values of several individual tests are 'combined' through some multiple testing procedure to explore whether something, and what, is happening. Of those the Bonferroni correction is most famous, but the idk correction is, in the case of independent P-values, more sensitive. On the other hand, Hotelling T 2 approach was a model-based approach for testing a joint hypothesis directly. This is similar to the likelihood ratio test we proposed and the Global test ().
Two microarray applicationsMicroarray gene expression experiments typically measure the behaviour of a large number of genes with a relatively small number of independent biological samples. The small number of independent samples limits the statistical inference that can be made about the behaviour of individual genes. Consequently, the microarray analyst is faced with bewildering lists of differentially expressed (DE) genes that are typically full of false positivesa direct result of the well-known 'small-n-large-p' dimensionality problem: only a few samples available and many genes to consider. At the same time biologists are using microarrays to understand processes involving the collective action of a number of genes, often organized as a complex or pathway (). To address these needs and remedy the problem of dimensionality, we consider applying the BC-LRT to detect differentially expressed pre-assigned groups of genes. GO () is a bioinformatics initiative to unify the representation of genes and gene products across the whole biological spectrum. At the leaves of this directed acyclic graph or tree-like representation are theindividual genes, whereas higher up in the tree so-called annotation terms unify groups of genes. All the genes that fall under a particular annotation term typically relate to some functional property of these genes and are therefore ideal candidates for a group-wise analysis.
Likelihood ratio test for related sparse signals
Comparison of cancerous and normal fibroblast cellsWe focus on a study by Nighean Barr (), which aimed to study differences in expression in cancerous and normal fibroblast cells. The fibroblast tissue used was created in vitro from two cell linesone cancerous and one normal. From each of these two cell lines, four separate replicates were obtained. Pairs of cancerous and normal replicates were then hybridized to four twochannel cDNA arrays, resulting in eight observations. The presence of a slide effect requires some sort of correction. The simplest possible correction, i.e. pairing the data, would reduce the number of independent samples to four. However, the availability of thousands (9216) of gene expressions per slide means that we can use a flexible slide correction model (df  20) and still obtain an effective number of observations 8420/49216 = 7.998 close to 8. This correction has been applied before analyzing the data. We focus on 94 groups of genes identified by various GO terms. The group or channel size, m, varies from 4 to 178. It is clear that in most of these cases it is impossible to apply a Hotelling T 2 test as m exceeds n. We report the top 17 of the 94 GO terms in. From Table 3, we can see that only 3 of the 17 GO terms were declared differentially expressed by all four methods, i.e. the idk adjusted t-tests, Global test (), Fisher's combined probability test () and our likelihood ratio test with the most conservative Bartlett correction. These GO terms are cystatin, gap junction protein (GJP) and claudin. Below, we refer to GO terms as negative or positive DE, i.e. expression levels for the cancerous condition are, respectively, mainly less or more than for normal. Only three GO terms are negative DE: cystatin, GJP and laminin. Cystatin (also known as stefin) genes are responsible for inhibiting cysteine proteases, i.e. enzymes that degrade proteins by hydrolysis and are typically down regulated in skin cancer (), concurring with our finding. GJP is a family of membrane proteins (connexin genes) that enables trafficking, through channels, of small ions and metabolites between neighbouring cells. Important for cellcell communication and coordination, GJPs maintain tissue homeostasis. Cancer is a genetic disease involving aberrant cell behaviour so that DE of GJP, that results in a loss of coordination and homeostasis, is consistent with disruption by tumour activity. Human skin consists of a thin epidermal layer attached by a basement membrane (BM) to a dermal layer of fibroblast-dominated connective tissue (). Laminin is in important constituent of BM and the direction of DE is tissue-and tumourspecific, e.g. a gene for laminin 332 is positive and negative DE for squamous and basal cell carcinomas, respectively (). Claudin, a family of cell-membrane tight junction proteins, is also tissue-specific (). Laminin GO is detected by our LRT-BC at the 5% level whereas at best it is marginal by Fisher's, Global or idk tests. A roughly similar pattern of detection occurs for spectrin, an intracellular scaffold protein that maintains cell membrane and cytoskeletal integrity, and fibulin, an extracellular matrix (ECM) protein secreted by cells. Cyclin orchestrates the cell cycle by directing cyclin-dependent kinase activity. Production and subsequent degradation of cyclin by proteolysis are critical for cell cycle control. Cyclin genes with pronounced DE are involved with S, G2 and mitosis phases. Proteasomes are cylindrical protein structures within each cell that contain active sites where proteolysis occurs. Up-regulation of proteasome therefore indicates increased proteolysis so 'new can be made from old' and is consistent with down-regulation of inhibitors (e.g. cystatin). Similar trends occur for cyclin, cell division and mitochondria, that supplies ATP and is involved with many cell-signalling pathways; so the GO terms indicate tumour cell proliferation and growth  a hallmark of cancer (). None of the four GO terms is detected by the idk or Global tests at the 5% significance level, and Fisher's test is marginal for cyclin and helicase. Tumoru's exhibit unstable genomes and this is shown by DE of enzymes. Helicase, for example, drives the unwinding of DNA or RNA helices into separate stands and is crucial for replication, transcription, etc. Polymerases are involved in copying and reading of DNA and RNA and often work co-operatively with helicase motor proteins. Only the LRT-BC test confirms DE of the two GO terms; the other three tests exhibit, at best, marginal DE. Similar results are found for the translocase gene family involved with moving molecules across membranes and chromatin remodelling. Vascular endothelial growth factor (VEGF) is a signalling protein produced by cells that stimulates angiogenesis needed by cells to access oxygen and nutrients from blood. DE of VEGF occurs in premalignant neoplastic lesions as well developed tumours () and it is noteworthy this angiogenic factor is detected only by the LRT-BC and Fisher's test. Of further interest is the expression of insulin-like growth factor (IGF) GO, a complex of proteins that cells use to communicate. Fibroblast dermal cells, for example, communicate survival factor IGF-1 to neighbouring epithelial keratinoctes () and the reduction of IGF-2 signal in tumoarigenic pathways has been shown
E.Wit and D.J.G.Bakewellto reduce tumour growth (). Upregulation of IGF GO, therefore, confirms fibroblast skin cancer growth and proliferation. Importantly, only the LRT-BC detects IGF GO, the other tests being marginal at best. Fibroblasts are stromal cells that typically produce collagen for ECM connective tissue. Positive DE for keratin in the fibroblast cancer suggests a possible mesenchymal-to-epithelial transition (MET) and EMT since keratin can be expressed by fibroblasts recruited into cancer stroma ()shows the LRT-BC detecting many more GO terms than the other three tests and reveals important cancer hallmarks, e.g. tumour cell proliferation, growth, resistance to cell death, angiogenesis and possibly trans-differentiation and invasiveness. Altogether, they indicate the development of a tumour micro-environment and suggest a follow-up study. LRT-BC has been shown in a previous section to be more sensitive and unbiased. It presents, therefore, a balanced and complete picture of significant GO activity of skin cancer fibroblasts.
LeukaemiaIn leukaemia gene expression study,considered 38 bone marrow samples obtained from acute leukaemia patients at the time of diagnosis. Of these 38 samples, 11 were from acute myeloid leukaemia (AML) patients and 27 from acute lymphoblastic leukemia (ALL) patients. RNA prepared from bone marrow mononuclear cells was hybridized to high-density oligonucleotide microarrays, produced by Affymetrix and containing probes for 6817 human genes. For each gene, the experimenters obtained a quantitative expression level. Samples were subjected to a priori quality control standards regarding the amount of labelled RNA and the quality of the scanned microarray image. After preprocessing, the dataset consisted of 3051 genes and 38 tumour mRNA samples. The aim of this study was to obtain a automatically derived class predictor to determine the class of new leukaemia cases. Although it is possible to create such a predictor with a large number of individual genes, in a second stage one is interested in a biological explanation of the difference between the two types of leukaemia. In this analysis, we considered 235 GO classes, each one with at least five genes present on the microarray. For each GO class, we calculated P-values according to our LRT with the most conservative Bartlett correction and Goeman's Global test. A summary of the results are shown in, whereas a detailed list of results for all GO classes is given in the Supplementary Materials. From this two important features become clear: (i) although the Global testand the LRT pick up the same signal, the LRT is, in general, more powerful and (ii) the larger the group size m, the more powerful the LRT test becomes. In fact, the correlation between the difference in log P-value for the two methods and the group size is 0.55. This feature, i.e. more power when m increases, is exactly what we call 'borrowing strength' in the title of this article. There are important GO classes that the global test would have missed in this case. For example ATP-dependent helicase activity (GO:0008026) was non-significant at the 5% level for the Global test (not even considering multiple testing), but was highly significant (P-value < 0.001) for the LRT. This class of eight genes drives the unwinding of a DNA or RNA helix and seems to be important in distinguishing between ALL and AML cases.
CONCLUDING REMARKSA property of many modern measurement techniques is the ability to measure simultaneously large numbers of features. Often it is of interest to test for concordant changes in particular subgroups of these features. One can think of groups of genes, so-called pathways, in genomic data or groups of labelled pixels in remote sensing data as part of astronomical images, fMRI brain scans or geographic surveys. Typically measurements are roughly on the same scale, but assuming a common variance is too restrictive. Sequential univariate tests with a 'multiple testing correction', such as the idk-like correction, are commonly used in such situations. Although they are simple, they are not particularly powerful to detecting small concordant changes in many channels. Multivariate tests, such as Hotelling's T 2 test, are traditionally used to deal with testing movement in multiple dimensions, but are not suited when the number of dimensions (m) exceeds the number of observations (n). This is typically the case in modern multichannel data. Other methods considered in this article are Fisher's method of combining P-values and Goeman's Global test. In the case of real applications, both methods work reasonably well, but do not achieve the same power as the likelihood ratio test with the most conservative Bartlett correction.
Likelihood ratio test for related sparse signalsThe advantage of the likelihood ratio test proposed in this article consists 'borrowing information', i.e. sharing of variance information across the measurement channels. We have shown in a practical example how the power increases when the number of features goes up. Moreover, its ability to detecting small but concordant changes across a large number of signal sources makes it preferable over more commonly used univariate tests. Ultimately, this sensitive test gives us a richer picture of the underlying biology, as we have shown in the comparison of normal and cancerous fibroblast cells.
ACKNOWLEDGEMENTS
The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
