Motivation: Multiple sequence alignments (MSAs) are usually scored under the assumption that the sequences being aligned have evolved by common descent. Consequently, the differences between sequences reflect the impact of insertions, deletions and mutations. However, non-coding DNA binding sequences, such as transcription factor binding sites (TFBSs), are frequently not related by common descent, and so the existing alignment scoring methods are not well suited for aligning such sequences. Results: We present a novel multiple MSA methodology that scores TFBS DNA sequences by including the interdependence of neighboring bases. We introduced two variants supported by different underlying null hypotheses, one statistically and the other thermo-dynamically generated. We assessed the alignments through their performance in TFBS prediction; both methods show considerable improvements when compared with standard MSA algorithms. Moreover, the thermodynamically generated null hypothesis outper-forms the statistical one due to improved stability in the base stacking free energy of the alignment. The thermodynamically generated null hypothesis method can be downloaded from http://sourceforge.net/ projects/msa-edna/
INTRODUCTIONWith the advent of third-generation sequencing technology, the rate of genome sequencing is likely to continue to outstrip the capacity for experimental analysis of gene function and regulation. In addition, there is a similar growth in the availability of transcription factor binding site (TFBS) sequences through genome-wide ChIP-seq methods. Thus, there is a continued need for more accurate computational methodologies. Particularly important are multiple sequence alignment (MSA) programs, such as ClustalW () and Dialign (), that are widely used to identify sections of protein or DNA sequences that share similar consensus. These alignment programs generally work under the assumption that the sequences being aligned are evolutionarily related and that they have been derived from a common ancestor, undergoing changes because of insertions, deletions and substitutions. This assumption is then built into scoring matrices, which are used to find the best possible alignment, such as BLOSUM () and PAM (), as well as for gap penalties. However, non-coding DNA regions, especially TFBSs, are rather more conserved and not necessarily evolutionarily related (), and may have converged from noncommon ancestors. Thus, the assumptions used to align protein sequences and DNA coding regions are inherently different from those that hold for TFBS sequences. Although it is meaningful to align DNA coding regions for homologous sequences using mutation operators, alignment of binding site sequences for the same transcription factor cannot rely on mutation operators. Similarly, the evolutionary operator of point mutations can be used to define an edit distance for coding sequences, but this has little meaning for TFBS sequences because any sequence variation has to maintain a certain level of specificity for the binding site to function. It has been long known that interdependencies between neighboring DNA bases have a significant impact on DNA topology. For example, the thermodynamic properties of base stacking interactions have been extensively measured and are commonly used in computational methods for DNA secondary structure prediction (). This was illustrated in work discussing the effect of DNA flexure on the binding site affinity (). Compensating mutations between neighboring DNA bases have been long known (). Consideration of the codependencies between DNA bases in a TFBS have been proved to be efficient in modeling binding site specificity (), also suggesting the possibility of coevolution between the DNA bases (). We have shown in previous work () using mutual information analysis that there are dependencies between neighboring and distant positions of the TFBSs; many of the distant interactions reflect the palindromic nature of TFBSs. The issues of effective MSAs for DNA binding sites become particularly important for supervised TFBS prediction. Supervised TFBS prediction algorithms take as input a set of known TFBS sequences, known as a 'training set', and use these to build an expected model of the TFBS that can be used to identify further sequences (). These can be contrasted with de novo binding site prediction methods that analyze upstream regions *To whom correspondence should be addressed. of genes to find overrepresented motifs () and which are not the subject of this article. We previously demonstrated that the use of a nonoptimal alignment for training can seriously disrupt TFBS prediction efficacy, even as compared with using a block alignment (). Moreover, effective supervised TFBS prediction relies on a suitably large training set of known binding sites, and so predictions can only be made for 'global' regulators. Most transcription factors only regulate a small number of genes, and supervised prediction of those 'non-global' regulators has been particularly hard. One solution would be to use binding sites from paralogous or orthologous transcription factors, as interdependence patterns are similar across related transcription factors (). To do so, it becomes necessary to produce MSAs for those binding sites that capture TFBS specificity. In this article, we aim to include the interdependencies in the MSA to provide a better representation for the DNA binding site specificity. We introduce a dinucleotide representation method of the TFBS sequence that captures the interdependencies. We then introduce two alignment cost metrics methodologies: the first is using a statistical approach similar to the one used to calculate BLOSUM (); the second makes use of base stacking thermodynamics. We specifically used a Boltzmann distribution centered on the change in base stacking free energy as the null hypothesis for dinucleotide substitutions. To validate our work, we compare both methods against each other, against other commonly used alignment methods (ClustalW, Dialign) and against our previously published method using block alignments. There is a good range of benchmark alignments to test protein MSA methods including BAliBASE (), OXBench (), SABmark () and SMART (). For DNA coding regions,have developed a DNA reference benchmark based on the tertiary structure of encoded protein. However, no such benchmark alignments are available for non-coding DNA sequences. A score is often used to detect the accuracy of the MSA using the homology in the resulting alignment (). The approach we take is to test the alignments for their capacity to act as a training set for a supervised first-order Hidden Markov Model (HMM) () to predict known TFBSs.
METHODS
Dinucleotide substitution matrixThe substitution matrix considered in this case is a 16  16 matrix representing the dinucleotide substitution rates for the TFBS. The dinucleotides are symbolized using the alphabet A to P to represent alphabetically each possible pair of neighboring nucleotides AA through to TT in alphabetical order. A DNA sequence is translated into our new alphabet in an overlapping manner. For instance, the sequence ACA would be represented in our new alphabet by the sequence BE: B for AC and E for CA. Every sequence in the training set is converted into this new alphabet for alignment with the other sequences. The hypothesis behind this conversion is that the sequences are now forced into an interdependent representation of the binding site rather than an independent one. This representation captures the heart of the single nucleotide mutation effect on neighboring base interactions. For instance, a single point mutation (transition) of the cytosine to thymine in this sequence ACA 4 ATA would result in a change of two neighboring base interactions AC 4 AT and CA 4 TA, and so would be represented as two position mutations in dinucleotide representation BE 4 DM. When considering stacking interactions, this can be used to represent the change in free energy of both interactions. A substitution matrix is highly specific for each TFBS set. The substitution matrix is then computed for each TFBS set. A problem is that to construct such a substitution matrix a valid alignment is required. This is inherently recursive, as obtaining a correct substitution matrix requires a valid MSA, and the MSA requires an optimized substitution matrix. This recursive nature of the problem has led us to devise a simple recursive algorithm similar to the expectation maximization algorithm devised by (). Initially, we define a random substitution matrix with costs drawn from independent uniform random variables between 0 and 100 (see Supplementary Methods). This is then used to generate the first alignment. In the maximization step, the resulting alignment is used to calculate the substitution matrix using the log-odds cost matrixWhere O i,j is the observed probability of aligning dinucleotide i with dinucleotide j and E i,j is the expected probability (null hypothesis) of aligning dinucleotide i with dinucleotide j. In the expectation step, the resulting substitution matrix is then used again to compute a new sequence alignment with optimum alignment cost. In all our implementations of this algorithm, it has converged; this is likely to be because of the analogy with EM algorithms (). The final substitution matrix is selected through another iterative process optimizing gap penalties, as explained later. The final dinucleotide substitution matrix is then used to align the transcription factor known binding sites. The observed probability O i,j is constructed using the following equationsWhere Q i,j is given by Equation (3)Where N i x is the number of dinucleotides of type i at position x. Thus, Q i,j represents the number of times each dinucleotide as been aligned to each other dinucleotide across the whole alignment. The log odds ratio optimization process is controlled by the null hypothesis used in the denominator of the equation. The null hypothesis in the log odd scoring controls the substitution cost as in Equation (1). Hence, in the algorithm devised, a dinucleotide alignment cost is rewarded if the observed cost is better than the expected cost and penalized if it is worse. Accordingly, we have assessed two different null hypothesis distributions, statistical and thermodynamical, with associated cost matrices. These have been denoted as SDNMSA for the statistically generated null hypothesis and EDNA for the thermodynamically generated null hypothesis.
Statistically generated null hypothesisThe first null hypothesis is a purely statistical hypothesis representing the expected independent joint distribution of the dinucleotides, which is generated following the equations as given in (Where E i is the expected probability dinucleotide iFrom Equations 2 and 5, a statistical cost matrix is then generated usingWhere S i,j is the statistical cost of substituting dinucleotide i with dinucleotide j and K is a scaling constant.
Thermodynamically generated null hypothesis (EDNA)The second null hypothesis is thermodynamical, and it assumes that the dinucleotide substitutions associated with a TFBS set are distributed using Boltzmann distributions, and that the joint distribution between two dinucleotides (e.g. representing a single base substitution) is energetically independent. The parameters for the Boltzmann distributions are derived from the dinucleotide stacking free energies (, b, c, d;). First, a Boltzmann distribution is constructed for the existing dinucleotides in the training set:Where G i is the stacking free energy of the dinulceotide i, T is temperature (298 K) and K B is the Boltzmann Constant equal to 0.00198721 kcal/ mol/K. Next, the expected values for the null hypothesis distribution are constructed as a joint distribution assuming independence:The thermodynamical cost of substituting dinucleotide i with dinucleotide j can be constructed from Equations 2 and 8:Where K is a scaling constant (set to 100). Reversing the sign of the odd score function is a pure technicality of the alignment program, which requires low-cost values for expected substitutions and high cost for unexpected ones. Hence, reversing the sign would penalize the negative odd score and reward the positive ones.
Gap penaltiesAn affine gap penalty function is used with different sets of penalties for gaps within the sequence (internal) and for prefix and suffix gaps (). Thus, the total gap penalty is given byWhere is the internal gap open penalty, is the internal gap extension penalty, D i is the length of internal gap i, 0 is the terminal (external) gap open penalty, 0 is the terminal gap extension penalty and D j 0 the length of the terminal gap extension j. The gap penalty has to be tightly optimized with the cost matrix optimization process. Therefore, the gap penalty constants are optimized iteratively as fractions of the average of the substitution matrix calculated as shown in Equation 11Where m c is the average of the cost matrix previously optimized. The acceptance function " for a chosen penalty is calculated for every computed G as followsWhere is the number of columns in the alignment for which the highest frequency of symbols is greater than as given by Equation (13), L A is the length of the alignment and L B is the initial length of the binding site block alignment (ungapped)Where P i is the number of the most frequent symbols in column i, and the homology percentage is the maximal symbol frequency observed in the block alignment given byThe proposed alignment mainly operates by shifting the binding sites relative to each other, preferring terminal gaps over internal gaps, due to the steric constraints of proteinDNA interactions. Accordingly, the internal gap open/extension penalties (, ) are fixed at the high value of 5. The terminal open/extension gaps, on the other hand, are optimized by fixing ' at a 0.1 and increasing the extension terminal gap penalty (') iteratively until no gaps can be added to the sequence. In other words, the opening of a terminal gap is always allowed, although the extension is optimized. The best terminal extension gap penalty is defined as the one that corresponds to higher ().
ValidationAs a validation, we tested the ability of a supervised first-order HMM (), trained using alignments from each of these methods, to predict known TFBS. Accordingly, we considered 18 global transcription factors in Escherichia coli K12 (Supplementary). For each factor, we used the RegulonDB 7.0 () known binding sites, as these can be considered as a highly reliable 'gold standard'. We aligned the binding sites for each transcription factor using the four different alignment methods compared in this article. We used the alignments to build first-order HMM for each factor (see Supplementary Methods). We used a leave-one-out cross-validation to determine the likelihood of each binding site within the TFBS set. Finally, we constructed a negative binding set using a moving window of the same length of the binding site in the intergenic regions in E.coli genome, asA non-independent energy-based multiple sequence alignment described previously (). The collective predictive power of each alignment is then assessed using area under receiver operating characteristic (ROC) curve ().
IMPLEMENTATIONEDNA is implemented as an extension of the open source MSA program, Opal (). Our alignment methodology makes profound changes in the alphabet, substitution matrix computation and gap penalties optimization. We have optimized the specific alignment parameters in Opal. EDNA is currently available at source forge, which is currently open for use by the public (http://sourceforge.net/projects/msaedna/). Example data, namely, the sequences from the binding sites used in this manuscript, are included with the code.
RESULTS
EDNA conserves the thermodynamic stability of MSAThis method is intended to primarily optimize the thermodynamic stability of the alignment. In other words, find the correct alignment where a dinucleotide substitution is possible thermodynamically without disrupting the free energy of neighboring interactions. Hence, we have measured the thermodynamic stability of each of the alignments for each of the TFBSs considered in the study (). It can be seen that the thermodynamic stability is greatly improved even for the statistical dinucleotide alignment SDNMSA, with further improvement for the thermodynamically based alignment EDNA. As an illustrative example, greater detail is shown for the AraC TFBS alignments (). In general, we noticed that ClustalW shows better free energy conservation than Dialign. SDNMSA and EDNA, on the other hand, showed an even better conservation than ClustalW; however, EDNA showed the most conserved column count. Dialign produced a highly unstable alignment with only a small stable core. Similarly, ClustalW not only produced an alignment with high stability in the core columns but also a terminal stability is noticed. SDNMSA showed, on the other hand, a much better conservation of the free energy across the alignment, with conservation on both sides and the core. Finally, EDNA showed a similar but better conservation to SDNMSA with more columns conserved.
Dinulecotide-based alignments outperform independent column alignments for TFBS predictionDinucleotide representation of the binding site outperformed independent position alignments methods, notably Dialign and ClustalW, as shown inand summarized in. The area under ROC curves (AUCs) for EDNA range between 81 and 99% (); many of the AUCs for Dialign are considerably worse, with the lowest, MetJ, at 28%, and the AUCs for ClustalW are also considerably worse, with MetJ at 36%. Overall, EDNA outperforms Dialign (P  0.001 paired Wilcoxon test), with larger AUCs for 14 of the 18 TFs and equal AUCs for the other 4 TFs. Notably, these TFs, CRP, GlpR, FNR and LexA all have highly conserved binding sequences (Supplementary), and both methods perform equally well (98 or 99% AUC). EDNA outperforms ClustalW (P  0.0008 paired Wilcoxon test), with greater AUCs for 15 of the 18 TFs. The AUCs for FNR and LexA are equal; ClustalW outperforms EDNA for TyrR, but the AUCs for the two methods are 97 and 98%, respectively, so there is minimal material difference between the two. EDNA also outperforms our previously published method using a block alignment, ungapped likelihood under a positional background (ULPB) () (P  0.0005). The range of AUCs for ULPB is considerably better than those using Dialign and ClustalW, with the lowest being 67% for MetJ. EDNA outperforms ULPB for 16 of the 18 TFs. Theexceptions are NtrC for which both report an AUC of 99%, and SoxS for which EDNA reports 93% and ULPB 95%.
Thermodynamic multiple alignments outperforms statistically based multiple alignment methodThe thermodynamically based method EDNA outperforms the statistically based method SDNMSA (). The AUCs for SDNMSA fare considerably better than those for the column alignments, with the lowest, for LRP, being 70% (). Overall, EDNA outperforms SDNMSA (P  0.006 paired Wilcoxon test). EDNA outperforms SDNMSA for 10 of the TFs studied, and they have equal AUCs for the other 8 TFs, all of which are in the range 9199%.
DISCUSSIONWe have developed a new methodology for MSAs of non-coding DNA sequences that uses an alignment of dinucleotides. Two variants have been described, using two different null hypotheses, one statistically driven and the other thermodynamically driven. These have been compared with other alignment programs, exemplified by ClustalW and Dialign, which are designed with a null hypothesis derived from the evolution of peptide sequences. They have been evaluated by assessing TFBS prediction for 18 global regulators from E.coli K12. The ROC curves for the first-order supervised HMM prediction using the dinucleotide alignment demonstrated better alignment than the current alignment tools, irrespective of the null hypothesis used. Between the two variants, the use of thermodynamic-driven null hypothesis proved to be statistically better. The Boltzmann distribution derived provides a base stacking interaction dinucleotide distribution for each TFBS set studied. An independent thermodynamic joint distribution of dinucleotides represents the random distribution of dinucleotide thermodynamic alignment. The log odd scoring system in this case would provide a negative or 0 score if the observed dinucleotides alignment joint probability is less than or equal to the random joint probability, and positive otherwise. Accordingly, scoring the observed joint distribution versus this null hypothesis distribution would indicate the odds of randomly aligning any dinucleotide pair under thermodynamic hypothesis. The higher these odds are, the less favored they are in the alignment. This would indicate that the resulting alignment from EDNA would favor to align dinucleotides that are more thermodynamically dependent than independent.
Thermodynamic null hypothesis is not governed by rarity of the dinucleotideAnother major advantage of using a thermodynamically driven null hypothesis is that it is not governed by the rarity of the substitutions in the binding site training set (). Instead, it provides a consistent behavior for all the binding sites, expecting that the rarity of an alignment event be governed by unfavorable stacking free energy. Thus, EDNA avoids problems resulting from under-sampling in a set of TFBS, which may be particularly valuable when building alignments for non-global regulators.
Base stacking interaction-driven convergenceIn general, the method provides a good performance for most binding sites relative to alternative methods. Nevertheless, false positives are still observed in some binding sites, including Lrp, SoxS, IHF and MetJ. For well-conserved binding sites, such as LexA, ArgR, FNR, Fur, GlpR and PhoP, all methods show good performance; this can be attributed to the small solution space providing a set of constraint alignment solutions. On the other hand, the predictions for the less-conserved binding sites, such as AraC, ArcA, CpxR, FlhDC, TyrR and MetJ, have been particularly enhanced by our new method (Supplementary). This may reflect the importance of thermodynamic interactions between neighboring DNA bases, which have been described well by this method.
Higher-order matricesThe fact that we have used a first-order substitution matrix to capture the binding site might be on its own providing an advantage for the alignment, as we account for the substitutions and aligning blocks of two bases rather one. It could be argued that higher-order matrices might provide further improvements. However, a major problem with such matrices is the complexity involved; a trinucleotide matrix would require a 64  64 substitution matrix with 44000 parameters, which would be challenging to compute and optimize, and most MSAs are likely to be under-sampled in this context. Moreover, the base stacking free energies used have only been experimentally determined for first-order interactions,and not for higher-order interactions (, b, c, d).
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
R.A.Salama and D.J.Stekel at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
