We present a new C implementation of an advanced Markov chain Monte Carlo (MCMC) method for the sampling of ordinary differential equation (ODE) model parameters. The software MCMC_CLIB uses the simplified manifold Metropolis-adjusted Langevin algorithm (SMMALA), which is locally adaptive; it uses the parameter manifold's geometry (the Fisher information) to make efficient moves. This adaptation does not diminish with MC length, which is highly advantageous compared with adaptive Metropolis techniques when the parameters have large correlations and/or posteriors substantially differ from multivariate Gaussians. The software is standalone (not a toolbox), though dependencies include the GNU scientific library and sundials libraries for ODE integration and sensitivity analysis. Availability and implementation: The source code and binary files are freely available for download at http://a-kramer.github.io/mcmc_clib/. This also includes example files and data. A detailed documentation, an example model and user manual are provided with the software.
INTRODUCTIONModeling intracellular reaction networks by ordinary differential equations (ODES) has become a standard approach in systems biology. ODEs provide a quantitative and dynamic description of the underlying biochemical processes. However, parameter estimation for these models is a challenging task, as typically only few data points are available, and the respective optimization problem is ill-posed. That is, the data do not contain sufficient information to identify parameter values uniquely. In a statistical Bayesian framework, this results in high correlations of parameters and/or multiple modes in the posterior distribution. Standard sampling algorithms frequently fail when facing those problems. The Metropolis algorithm in its original form (), for example, is elegant and easy to implement but uses an isotropic distribution to suggest the next set of parameters and struggles with complicated posteriors. In the case of strongly correlated parameters, some directions are highly prohibitive because of low likelihoods, whereas others are permissive because of low sensitivity of the model output in these directions, i.e. flat likelihoods. This results in highly auto-correlated samples and extremely slow convergence rates. Adaptive variants of Metropolis that try to amend this problem do exist; they use diminishing adaptation that can deal with strong correlations of sampling variables in some cases. One such implementation is described in. The adaptive Metropolis algorithm adapts to the posterior by estimating its covariance from previously sampled points. While this can drastically increase sampling efficiency for densities that are similar to Gaussians, it struggles with more complex cases. To date, sampling techniques that put more computational effort into the proposal step are often implemented in high-level programming languages only and so are not efficient computationally. This prompted our efforts to provide a highly efficient specialized tool that spends most of its running time on the solution of initial value problems. We introduce MCMC_CLIB, an implementation of SMMALA (simplified manifold Metropolis-adjusted Langevin algorithm), a sampling algorithm, described in Girolami and Calderhead, 2011 that has been shown to outperform simpler sampling algorithms.
APPROACHOur approach is motivated by modeling practices in systems biology, where model parameters k are typically required to be of fixed sign for model stability. Otherwise, the model structure is fairly generic. Although the software will always supply k =exp  k 40 to the model (see Availability and Implementation section), the framework can be manipulated to accommodate sign switching. Using for sampling, i.e. sampling in logarithmic space, has the benefit of easily covering several orders of magnitude. An important novel feature is the ability of the implemented likelihood function to deal with relative data like Western blots in arbitrary units.
ModelingWe assume an ODE model given in state-space representation:with additive Gaussian noise d jk 2 R m $N 0; S jk . We assume the relation between state and measurement to be linear, characterized by C 2 R mn (it may, however, be scaled by an unknown parameter, i.e. data may be in arbitrary units). Index k=1;. .. ; n E enumerates different experiments (described by different inputs u k 2 R q , e.g. drug dosage), whereas j marks different measurement time instances. For simplicity, we assume *To whom correspondence should be addressed.Using Bayesian learning, we construct the posterior distribution pujD / L D upu with prior distribution pu$N ;   that is implemented as a multivariate Gaussian distribution with mean and covariance matrix .
METHODSThe original and the SMMALA algorithms are comprehensively described in, respectively, and we only give a glimpse of its structure here.
The simplified manifold MALA algorithm SMMALAThe Markov chain (MC) is a discrete time approximation of the SDEwhere Vu;D=log pujD and dbt is a Wiener process. The natural metric Gu of the parameter space is the Fisher information; chol denotes the Cholesky decomposition, a function that returns the Cholesky factor. We can write the MC proposal u ! u 0 with step size 2 R + as pu 0 ju=N yu; ; Mu;  5This step proposes a parameter vector from a Gaussian distribution with mean and covariance that take information about the local shape of the posterior distribution via the Fisher matrix and the gradient into account. The metric tensor G is calculated using sensitivity analysis (S y : =dCx=du) of the solution to the initial value problem,shows a flowchart of the typical MCMC_CLIB usage. Input The ODE model has to be provided as an XML file in VFGENS vf format, together with a data file that contains measurement time points and a data set D. Additionally, this data file also specifies sample size, initial step size, desired acceptance rate and the parameters and  1 for the prior distribution. Sampling algorithm To evaluate the likelihood function, ODEs are numerically integrated using CVODES with fixed relative and absolute tolerance. For further details, we refer to the supplement in the Availability and Implementation section (manual.pdf and documentation.pdf). Output The user can select a text or binary file output, which contains the results of the sampling procedure: the sampled log-parameters and their log-posterior values. The sample can be further processed with other numerical software (e.g. GNU OCTAVE, MATLAB).
Implementation details
Test results for the provided demoFor demonstration purposes, we supply a model with 11 state variables, 26 parameters and 4 input variables. The data, obtained in a real experiment, are given in arbitrary units, stem from n E =4 experiments and are only meaningful in relation to the reference experiment (also included). The initial conditions are unknown stable steady states of the unperturbed system. At t = 0, a constant disturbance, parametrized by u, is activated and perturbs the system indefinitely. Then measurements are taken. The activation is modeled by a logistic function centered at t = 0. Performance of the sampling algorithm is summarized in. Further insight into this example is provided as part of the software package. The analysis reveals that not all data points can be fitted equally well by the model; a considerable uncertainty remains for several parameters and consequently model predictions. Hence, we consider this as a realistic test case, which covers many of the difficulties that emerge in current research.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
A.Kramer et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
