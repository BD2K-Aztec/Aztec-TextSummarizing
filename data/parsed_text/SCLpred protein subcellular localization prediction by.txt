Knowledge of the subcellular location of a protein provides valuable information about its function and possible interaction with other proteins. In the post-genomic era, fast and accurate predictors of subcellular location are required if this abundance of sequence data is to be fully exploited. We have developed a subcellular localization predictor (SCLpred), which predicts the location of a protein into four classes for animals and fungi and five classes for plants (secreted, cytoplasm, nucleus, mitochondrion and chloroplast) using machine learning models trained on large non-redundant sets of protein sequences. The algorithm powering SCLpred is a novel Neural Network (N-to-1 Neural Network, or N1-NN) we have developed, which is capable of mapping whole sequences into single properties (a functional class, in this work) without resorting to predefined transformations, but rather by adaptively compressing the sequence into a hidden feature vector. We benchmark SCLpred against other publicly available predictors using two benchmarks including a new subset of Swiss-Prot Release 2010_06. We show that SCLpred surpasses the state of the art. The N1-NN algorithm is fully general and may be applied to a host of problems of similar shape, that is, in which a whole sequence needs to be mapped into a fixed-size array of properties, and the adaptive compression it operates may shed light on the space of protein sequences. Availability: The predictive systems described in this article are publicly available as a web server at
INTRODUCTIONWith the recent advances in high-throughput sequencing technology, there has been a rapid increase in the availability of sequence information. To fully exploit, this information sequences need to be annotated quickly and accurately, which has led to the development of automated annotation systems. A major step toward determining the function of a protein is determining its subcellular localization (SCL). Knowledge of the location of the protein sheds light not only on where it might function but also what other proteins it might interact with, as, in order to interact, proteins must inhabit * To whom correspondence should be addressed. the same location or physically adjacent compartments, at least temporarily. There is a growing gap between the number of proteins that have reliable SCL annotations and the number of known protein sequences. Experimental approaches to SCL prediction are timeconsuming and expensive, whereas computational methods can provide fast and increasingly accurate localization predictions. There are various different mechanisms by which a protein is directed to a particular location in the cell, and there are many possible compartments in which eukaryotic proteins may be located. Some nuclear proteins have a nuclear localization signal (NLS), which may occur anywhere in the sequence (). Most secreted, mitochondrial and chloroplastic proteins have N-terminal cleavable peptides (SP, mTP and cTP), but many proteins have no known motif (), and many are known not to have N-terminal peptides (). Even in these cases, the information contained in a protein sequence may be sufficient to predict the protein's location in the cell, given that residue and k-residue frequencies correlate with locations (). There are many methods for the prediction of SCL that can be roughly divided into two groups: homology or knowledgebased, which rely on similarity to another sequence of known location, or other known information about the sequence or similar sequences, for example WoLF PSORT () or SherLoc (); and de novo or ab initio, sequencebased methods, which may use evolutionary information in the form of multiple sequence alignments (MSAs), but do not depend on similarity to sequences of known location, for example BaCelLo (). We predict SCL for eukaryotes only, which we divide into animals, plants and fungi. There are many potential classes of subcellular localization, and different prediction systems sometimes use different class subdivisions, ranging from 3 () up to more than 10 classes (). Here, similarly to BaCelLo (), to which we directly compare our results, we consider four subcellular localizations for animals and fungi and five for plants: nucleus, cytoplasm, mitochondrion, chloroplast and secreted. In a first series of tests, we adopt essentially the same experimental setting as in () and (), to which we compare our predictor. We then take a further step by developing new, redundancy reduced training and testing sets starting with Swiss-Prot ReleasePage: 2813 28122819
Protein subcellular localization prediction2010_06 () and benchmark SCLpred on these sets against six state-of-the-art, publicly available predictors of SCL: BaCelLo, LOCtree, SherLoc, Protein Prowler, TARGETp and WoLF PSORT, which we briefly describe in the following sections. BaCelLo: BaCelLo () uses a hierarchy of binary support vector machines (SVMs) to predict SCL for eukaryotes into four classes for animals and fungi and five for plants: secreted, cytoplasm, nucleus, mitochondrion and chloroplast. BaCelLo is trained on a non-redundant set of sequences from SwissProt 48. Predictions are made from the full sequence, from the N-and C-terminal regions and evolutionary information. BaCelLo is available at http://gpcr.biocomp.unibo.it/bacello/. LOCtree: LOCtree () uses binary SVMs to predict SCL. Three versions of the predictor are available, for plants, non-plants and prokaryotes. For prokaryotes, predictions are dived into three classes: secreted, periplasm and cytoplasm. For eukaryotes, predictions are divided into six classes: extracellular space, nucleus, cytoplasm, chloroplast, mitochondrion and other organelles. LOCtree is trained on a redundancy reduced subset of Swiss-Prot 40. Predictions are made from the full sequence, a 50-residue N-terminal region, predicted secondary structure and the output of SIGNALp (for eukaryotes). LOCtree is available at http://www.predictprotein.org/. SherLoc: SherLoc () uses SVM that integrate sequence and text-based features. There are three predictors (animal, fungi, plant) which predict 10 locations for animals and fungi: cytoplasm, endoplasmic reticulum, extracellular, Golgi, lysosome, mitochondrion, nucleus, peroxisome, plasma membrane, vacuole and an extra class, chloroplast, for plants. The predictors are trained on sequences extracted from Swiss-Prot 42. http://wwwbs.informatik.uni-tuebingen.de/Services/SherLoc/. TargetP: TargetP () uses a feed-forward neural network for the prediction of plant and non-plant SCL into three and four classes, respectively, based on the N-terminal sequence. The prediction is based on the presence of a chloroplast transit peptide (cTP), a mitochondrial targeting peptide (mTP) or a secretory pathway signal peptide (SP). TargetP is available at http://www.cbs.dtu.dk/services/TargetP/. Protein Prowler: Protein Prowler () is based on the ideas behind TargetP and trained on a subset of Swiss-Prot 37 and 38. The predictor uses neural networks and SVMs specialized for the prediction of plants or nonplants and predicts into the following classes: secretory pathway, mitochondrion, chloroplast and other. Protein Prowler is available at http://pprowler.itee.uq.edu.au/. WoLF PSORT: WoLF PSORT () is a version of the PSORT family of SCL predictors for the prediction of eukaryotic proteins based on their sequence. Based on a number of features (residue composition, presence of known sorting signal and target peptides, etc.), WoLF PSORT uses a k-nearest neighbor classifier, comparing these features to other Swiss-Prot-annotated proteins, resulting in a ranked list of up to 12 possible locations: chloroplast, cytosol, cytoskeleton, endoplasmic reticulum, extracellular, Golgiapparatus, lysosome, mitochondrion, nuclear, peroxisome, plasma membrane and vacuolar membrane. WoLF PSORT is available at http://wolfpsort.org/.
MATERIALS AND METHODS
DatasetsThe first dataset that we use to train and test SCLpred is the dataset used by, respectively, have a 'SUBCELLULAR LOCATION'. We remove membrane proteins and sequences that have nonexperimental qualifiers (Potential, Probable, By similarity), leaving 16 406, 3339 and 7116 sequences, respectively. We internally redundancy reduce each of these sets using an all-against-all BLAST () search (with e = 10 3 ) removing any sequence with a hit with >30% sequence identity to any other sequence in the set. All the sequences added to SwissProt earlier than 2009 in the set are used as a training set (2010_06 training set). Sequences added to Swiss-Prot in 2009 or later are used for testing, as these sequences have <30% sequence similarity to any sequences used to train any of the predictors tested in this article. We refer to as the 2009+ test set.shows the number of sequences per class for each of the three kingdoms in these new training (2010_06 training set) and test sets (2009+ test set). The BaCelLo datasets are available on the BaCelLo website: http://gpcr.biocomp.unibo.it/bacello/dataset.htm and the SCLpred datasets are available upon request from the authors. MSAs are extracted from uniref90 () from February 2010 containing 6 464 895 sequences. The alignments are generated by three runs of PSI-BLAST with parameters b = 3000 (maximum number of hits) and e = 10 3 (expectation of a random hit).
Predictive architecture: N1-NNWe call the model that we describe in this work N-to-1 Neural Network or N1-NN. The model is based and on our framework to design Neural Networks for structured data (). The aim of the model is to map a sequence of variable length N into a single property or fixed-width array of properties. Other models transform/compress the sequence into a fixed number of descriptors (or into descriptors of pairwise relations between sequences) beforehand, and they then map these descriptors into the property of interest. These descriptors are Page: 2814 28122819. In some cases whole sections of the sequence are directly taken into account (again, typically the termini, where some signals are to be found), but even in this case the size of this section needs to be fixed and decided beforehand. In N1-NN, instead, we do not compress all the information of a sequence into a handful of predefined features (e.g. k-mer frequencies, sequence length, etc.). Rather, we decide beforehand only how many features we want to compress a sequence into. If these features are stored in a vector
C.Mooney et al., and if we represent the i-th residue in the sequence as r i , then f is obtained as:where N (h) is a non-linear function, which we implement by a two-layered feed-forward Neural Network with h non-linear output units (the sequenceto-feature network). N (h) is replicated N times (N being the sequence length), and k is a normalization constant. The feature vector f is obtained by combining information coming from all windows of 2c+1 residues in the protein. If c = 20, as in all the tests in this article, the motifs have a length of 41 residues. The feature vector f thus obtained is mapped into the property of interest o (for instance, cellular component class), as follows:where N (o) is a non-linear function that we implement by a second twolayered feed-forward neural network (the feature-to-output network). The whole neural network (the cascade of N replicas of the sequence-to-feature vector network and one feature-to-output network) is itself a feed-forward neural network, and thus can be trained by gradient descent via the backpropagation algorithm. As there are N copies of N (h) for a sequence of length N, there will be N contributions to the gradient for this network, which are added together. A graphical representation of N-to-1 NN is shown in. The feature vector f is a compression of the sequence into h real-valued descriptors. These descriptors are automatically determined/learned in order to minimize the output error, hence to be most informative to predict the property of interest. Although there is a daunting number of possible motifs of length 2c+1, the model does not need to count them or represent them all. Only a relatively small number of free parameters is available to represent all the motifs in a sequence. This prevents overparametrization and model fitting problems that arise when one counts frequencies of n-mers as soon as n > 23. If training is successful, only (soft) motifs relevant to the task at hand are represented in f. Thus, f is effectively a compressed version of the sequence into a fixed-size array. The compression is property driven, meaning that different predictive targets generally induce different representations of a sequence. The number of free parameters in the overall N1-NN can be controlled by: the number of units in the hidden layer of the sequence-to-feature networkthe number of hidden units in the feature-to-output networknetwork (only three represented for simplicity) process all the (overlapping) motifs of a predefined length in a sequence. The vectorial outputs f k of these networks are added up, and the resulting feature vector f is input to the N (o) network to produce the localization prediction.the number of hidden states in the feature vector f , which is also the number of output units in the sequence-to-feature network, N f. Given that only one instance of the sequence-to-feature network (i.e. only one set of free parameters) is replicated for all positions in the sequence, and there is only one feature-to-output network, the overall number of free parameters N p of the N1-NN is:where N i is the size of the input vector representing one residue (including its context) and N o is the number of output classes. The number of free parameters can be controlled by N H f , N f and N H o , while N o is governed by the property being predicted, and N i depends on the input representation and, importantly, by the size of the motifs being consideredTraining: for each training experiment (i.e. training on the BaCelLo training set and training on the 2010_06 training set), we implement three predictors, one for each of the three kingdoms of animals, fungi and plants. Each training is conducted by 10-fold cross-validation, i.e. 10 different sets of training runs are performed in which a different tenth of the overall set is reserved for testing. The 10 tenths are roughly equally sized, disjoint and their union covers the whole set. For each training, the 9/10 of the set that are not reserved for testing are split into a validation set (1/10 of the overall set) and a proper training set. Given that some classes are far less numerous than others, in order to rebalance the training set we repeat the number of instances in the various classes until we have roughly the same number of examples in each of them. Examples in the testing and validation sets are not replicated. The training set is used to learn the free parameters of the network by gradient descent, while the validation set is used to monitor the training process. For each different architecture, we run three trainings, which differ only in the training versus validation split. Excluding different validation sets ensures that the resulting models are different, which yields larger gains when ensembling them. During preliminary experiments (run on the BaCelLo plant training set split into 2/3 for training and 1/3 for testing), we tested N H o values of 6, 8 and 10, which all yielded similar performances. When choosing a motif size, we considered that the average size for known signal peptides in eukaryotes is 20 residues (), and 3540 is an upper size bound for most known signals and NLS ().
Protein subcellular localization predictionIt should be noted that, since all (overlapping) motifs of length 2c+1 are considered by an N-to-1 NN, it is not strictly necessary for 2c+1 to cover all motif sizes, because signal larger than 2c+1 is still input to an N-to-1 NN as all its overlapping substrings of length 2c+1, although this may lead to the loss of some positional information. During preliminary experiments, we tested c values of 10 and 15, which performed marginally less well than c = 20. We kept N H f and N f fixed at 10 in all experiments. During the final cross-validations, we use exactly the same architecture for all sets and all kingdoms, in which N H f = N f = N H o = 10 and c = 20. All trainings are also identical in that the weights in the networks are updated every 10 examples (proteins) and 2000 epochs of training are performed, which brings the training error to near zero in all cases. In all cases, we save networks at epochs 1800, 1900 and 2000, ensemble average them and evaluate them on the corresponding test set. Saving the models that perform best on validation yields very similar results. The final results for each 10-fold cross-validation (different kingdoms, BaCelLo and 2010_06 training sets) are the average of the results on each test set. When testing on an independent set from the one used during training (BaCelLo for training and BaCelLo_2008 for testing, 2010_06 for training and 2009+ for testing), we ensemble-combine all the models from all cross-validation folds of the best architecture. Training is performed by gradient descent on the error, which we model as the relative entropy between the target class and the output of the network. The overall output of the network [output layer of N (o) ()] is implemented as a softmax function, while all internal squashing functions are implemented as hyperbolic tangents. The examples are shuffled between epochs. We use a momentum term of 0.9. Although this does not significantly affect the final results, it speeds up overall training times by a factor 10. The learning rate is kept fixed at 0.2 throughout the training. Training one model on a state of the art core took between 8 h and 4 days, depending on the size of the training set. Predicting the localization of an average-sized protein from the sequence and MSA takes less than a second, in fact running BLAST to generate MSA is far costlier (minutes) than obtaining the actual prediction from an ensemble of N-to-1 NN. Evaluating performance: to evaluate the performance of SCLpred against other predictors, we use the following global indices:where: @BULLET z ij : the number of sequences of class i predicted to be in class j. @BULLET e ij : the number of sequences of class i expected to be predicted in class j by chance. @BULLET N: the number of sequences. @BULLET K: the number of classes.To measure performances for a given class i we use:We emphasize performances based on GC [seefor more details], as this index minimizes the effect of class sizes. For some of the experiments, we extract performances of other predictors from the literature, hence not all indices are reported at all times.
RESULTS AND DISCUSSIONIn previous tests, BaCelLo () was shown to outperform the following publicly available methods for the prediction of the subcellular localization: LOCtree (), PSORT II (), SubLoc (), ESLpred (), LOCSVMpsi (), SLP-local (), Protein Prowler (), TARGETp (), PredoTar () and pTARGET (). In, we show the performance of SCLpred compared with BaCelLo on the BaCelLo training set (). Both predictors are assessed by 10-fold cross-validation on the same set. Overall SCLpred is far more accurate for animals (Q 82% versus 74% and GC 72% versus 67%) and fungi (Q 75% versus 70% and GC 67% versus 66%) while the accuracy for plants (Q) is the same (68%), but GC is still considerably higher for SCLpred (63% versus 59%).shows the accuracy of the same version of SCLpred tested on the BaCelLo_2008 test dataset fromcompared with the other five SCL predictors tested on the same dataset. Notice that two of the predictors (Protein Prowler and TARGETp) use a different class assignment ('easier' as comprised by fewer classes) and are thus not directly comparable to SCLpred. The results refer to versions of the various predictors that were trained on datasets extracted from Swiss-Prot release 48 or earlier. Since the BaCelLo_2008 test set Page: 2816 28122819. Results for SCLpred, trained on the BaCelLo training set from Swiss-Prot 48 (), compared with BaCelLo (), LOCtree (), WoLF PSORT (), Protein Prowler () and TARGETp (Tested on the BaCelLo_2008 test set (see text). Results for the predictors other than SCLpred from. Results in italics are for predictors using a fewer classes, hence not directly comparable to SCLpred. For these predictors 'Other' is the class of proteins that cannot be classified as mitochondrion, secreted or chloroplast based on the presence of a known SP, mTP or cTP. Deviations are 2 for both GC and Q for Fungi and 1 for Plant and Animal. These are for our predictor (SCLpred). We have no access to the raw data for the other predictors as these are obtained from the literature and were not reported. is extracted from Swiss-Prot release 54 and redundancy reduced against Swiss-Prot 48, there is no significant overlap between the training sets of any predictors in the table and the BaCelLo_2008 test set. For animals we obtain a Q of 85% and GC of 81%, higher than the second best predictor that is directly comparable (WoLF PSORT, with 81 and 75%, respectively). SCLpred also performs better than the two predictors that are not directly comparable on the two classes that are common (mitochondrion and secreted). On fungi, SCLpred has the best Q (60% versus BaCelLo's 59%) and the second best GC (57% versus WoLF PSORT's 59%). On plants, SCLpred has by far the highest GC (58% versus BaCelLo's 46%) and the joint highest Q (76%, again with BaCelLo). It should be noted that BaCelLo was optimized for balanced class accuracies (), that is, to maximize average class sensitivity (nQ measure). Based on nQ, SCLpred still outperforms BaCelLo on both the BaCelLo and BaCelLo_2008 set for animal proteins (by 2.3 and 6.2%, respectively), BaCelLo fares better on fungi (by 4.5 and 2%), while on plants BaCelLo does better on the BaCelLo training set (by 4.6%) and SCLpred on the BaCelLo_2008 test set (by 2.2%). Overall BaCelLo shows a more balanced sensitivity across classes than SCLpred, although in the case of animal proteins this is at a lower average level. We repeat the experiments on a new training set extracted from the 2010_06 release of Swiss-Prot, which is approximately twice the size of the BaCelLo set for all three kingdoms. The accuracy of this new version of SCLpred is shown in. On animal and fungi, overall performances are lower, in absolute value, to those obtained on the BaCelLo set. We attribute this to the more balanced nature of the 2010_06 training set, which is thus intrinsically 'harder'. Assigning proteins randomly to classes with a probability proportional to class frequencies yields a Q measure 3% higher on theproteins as fixed-size arrays) induced by different output targets (functional classes, protein folds/families), to determine whether they are satisfactory representations toward protein comparison, and whether they yield insights into the structure of the protein space. SCLpred is available as part of our web servers for protein sequence annotation. Up to 32 768 residues can be handled in a single submission. The servers are freely available for academic users at http://distill.ucd.ie/distill/. Predictions are obtained by an ensemble of all models trained on the 2010_06 training set (as in). Linux binaries and the benchmarking sets are freely available for academic users upon request.
C.Mooney et al.
Protein subcellular localization prediction
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
.67 0.71 on the BaCelLo set compared with 2010_06 for animals (44.9% versus 35.9%) and 14.4% higher for fungi (59.3% versus 44.9%). Moreover, in both kingdoms the class which is overrepresented in the 2010_06 set compared with BaCelLo is cytoplasm (26.8% versus 16.9% of the examples for animal, 34.7% versus 17.6% of the examples for fungi), which in all out tests is the hardest to predict. Hence not only is 2010_06 more challenging because of its distribution of examples, but also because it contains a higher proportion of difficult instances. On plants, Q is higher on the 2010_06 training than on the BaCelLo training set (71% versus 68%) while GC is lower (58% versus 63%). This is the result of a larger chloroplast class (which is well predicted) in 2010_06, and of the mitochondrion class being only 7% of the 2010_06 set (versus 14% in BaCelLo), which results in infrequent predictions for this class. While the improvement on the much larger chloroplast class dominates in terms of Q measure, the reduction of performances on mitochondrion dominates with respect to GC, which weighs all classes equally. Overall it should be noted that, because of different class composition, it is hard to compare Q and GC measures across different datasets, and different predictors should always be ranked on the same dataset, as we do throughout this article. We then test the version of SCLpred trained on the 2010_06 set on the 2009+ dataset (a subset of Swiss-Prot 2010_06 with <30% sequence similarity to the training set, described in Section 2.1). We compare its accuracy with BaCelLo, SherLoc, WoLF PSORT, Protein Prowler and TARGETp (Table 6). Results for TARGETp and Protein Prowler are based on three class predictions for animals and fungi, and four for plants, whereas for WoLF PSORT and SherLoc prediction is possible for more four/five classes. For WoLF PSORT, we count any proteins predicted as 'vacu', 'lyso', 'E.R.', 'golg' or 'plas' as secreted, and any 'cyto', 'cysk', 'cyto_nucl' as cytoplasmic and any 'nucl' or 'cyto_nucl' as nuclear. For SherLoc, any sequences predicted as 'extracellular', 'ER', 'vacuolar', 'peroxisomal', 'Golgi' or 'plasma' are counted as secreted. On 2009+, SCLpred again performs best of all predictors. On animals, Q is 89%, more than 20% better than the second best directly comparable predictor (BaCelLo, with 66.3%), and over 10% better than predictors using one less class. GC, at 79%, is also 10% higher than BaCelLo, and higher than that of the two predictors with one less class. On fungi, both Q and GC (72% and 69%) are the highest of all four class predictors, and similar to those obtained by the three class predictors. On plants, again Q (at 80%) is by far the highest (SherLoc in this case being the second best five class predictor at 68%), and GC (66%) is at least 9% higher than all other five class predictors, and only lower than Protein Prowler's (69%) which tackles the simpler four class problem. In this case, SCLpred also outperforms BaCelLo by nQ on all three kingdoms. 4 CONCLUSION AND FUTURE WORK As the amount of sequence information churned out by experimental methods keeps expanding at an ever-increasing pace, it is crucial to develop and make available fast and accurate computational methods to make sense of it. SCL prediction is a step toward bridging the gap between a protein sequence and the protein's function and can provide information about potential proteinprotein interactions and insight into possible drug targets and disease processes. As different SCL predictors are specialized for prediction into different classes and number of classes, and as some predictors are more accurate than others at prediction into any one class, this information can be exploited to lead to more accurate overall consensus predictions, especially if the predictors are diverse in their behavior. In this article, we have developed a new method for SCL prediction (SCLpred) based on a novel Neural Network architecture (N1-NN). The architecture can map a sequence of any length into a set of individual properties for the whole sequence. We have developed three kingdom specific predictors for animals, fungi and plants and predict into four classes for animals and fungi (nucleus, cytoplasm, mitochondrion and the secreted) and an additional fifth class for plants (chloroplast). We have trained SCLpred in 10-fold cross-validation on large non-redundant subsets of annotated proteins from Swiss-Prot 2010_06 and benchmarked it against five other state-of-the-art SCL prediction servers on an independent set of recently annotated proteins. SCLpred performs favorably on these benchmarks, often by consistent margins, and we expect that its prediction accuracy will continue to improve with frequent retrainings to take advantage of larger, more diverse, datasets of annotated proteins as they become available, and as our understanding of the underlying biological mechanisms improves. We expect larger datasets to be especially beneficial to our models, as these incorporate information from the whole sequence and normally have a higher number of free parameters than the alternatives. In this work, we have used the primary sequence and multiple sequence alignments as inputs to the network. Additional residuelevel information may be included, such as predicted secondary structure, solvent accessibility, location of binding sites, etc. Incorporating diverse information into the input to SCLpred is one of our future directions of investigation, as is the inclusion of putative homology to 'templates' or proteins of known localization/structure [e.g. by techniques similar to those in Mooney and Pollastri (2009)]. In this work, we predict subcellular localizations into a small number of classes (four for animal and fungi, five for plants), to allow the comparison of our novel algorithms against a a number of existing predictors, and direct comparison against BaCelLo in particular, which has been shown as one of the best-performing ab initio systems to date. We are currently testing our methods on a wider set of localization classes, as well as different functional tasks. A further direction of research is studying the space of f vectors (i.e. compressed, property-driven representations of whole
