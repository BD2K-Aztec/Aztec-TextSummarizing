Motivation: It is well known that data deficiencies, such as coding/rounding errors, outliers or missing values, may lead to misleading results for many statistical methods. Robust statistical methods are designed to accommodate certain types of those deficiencies, allowing for reliable results under various conditions. We analyze the case of statistical tests to detect associations between genomic individual variations (SNP) and quantitative traits when deviations from the normality assumption are observed. We consider the classical analysis of variance tests for the parameters of the appropriate linear model and a robust version of those tests based on M-regression. We then compare their empirical power and level using simulated data with several degrees of contamination. Results: Data normality is nothing but a mathematical convenience. In practice, experiments usually yield data with non-conforming observations. In the presence of this type of data, classical least squares statistical methods perform poorly, giving biased estimates, raising the number of spurious associations and often failing to detect true ones. We show through a simulation study and a real data example, that the robust methodology can be more powerful and thus more adequate for association studies than the classical approach. Availability: The code of the robustified version of function lmekin() from the R package kinship is provided as Supplementary Material.
INTRODUCTIONGenetic association studies aim to identify genetic polymorphisms that cause phenotypic variation for a trait of interest, or that are in linkage disequilibrium with the causative genetic variant. We focus our analysis on biallelic genetic variants, such as single-nucleotide polymorphisms (SNPs). In this case, the unit of analysis can be regarded as a three-category variable. For instance, a SNP with alleles A (adenine) and G (guanine) has categories AA, AG and GG. We are interested in using a number of genotyped SNPs in a gene, or region, to detect the genetic factors underlying a quantitative * To whom correspondence should be addressed. trait of interest that does not follow simple Mendelian patterns of inheritance. The most straightforward and still more favoured approach in association studies, though raising multiple testing problems (), is to perform a single SNP test for every genotyped SNP via regression or analysis of variance (ANOVA) methods [, used a linear model to test the association between SNPs in eight candidate genes and agespecific growth rate in the Artic charr;, used mixed linear models to test for association between 57 SNPs from 20 candidate genes and some wood properties in Pinus taeda;) used a mixed random effects linear model to test for the association between a collection of SNPs and some Teosinte traits;, used a linear model to test for the association between 151 SNPs from 57 candidate genes and several traits of boar). Though the single SNP approach may be considered if we are looking for a single causal variant, it is not very efficient when the SNPs have limited LD with that causal variant, meaning smaller power. Moreover, quantitative traits are usually controlled by several and sometimes many genes. Thus, a joint analysis of SNPs may be more adequate, being much more informative than single-SNP analysis (). However, it also may lose power due to the usually large number of degrees of freedom involved. Ideally, one should make use of the information provided by multiple SNPs, capturing as much of the genetic variance as possible, without raising the degrees of freedom too much () and thus not compromising power. Note that the joint analysis of SNPs (multiple-SNP approach) can only be applied to situations where the number of explanatory variables is much smaller than the number of individuals, therefore implying that in a genome-wide association study context a preliminary step of dimension reduction is necessary. There is an extensive literature on how two specific data problemsLD and population structure (PS)may affect both the power to detect true associations as well as the number of false positives, therefore distorting the conclusions when testing for association between a quantitative trait and a set of candidate SNPs in a population-based study (). We also find in literature many methods for overcoming these problems (). Another frequent data problem, which may have the same sort of undesirable effects, is non-normality and/or presence of outliers in the phenotypic data. This problem is far less studied
V.M.Loureno et al.than LD or PS. For instance, the review paper by, treats non-normality in one sentence where the only mentioned remedy is a transformation of the original trait values. However, transformation may not be sufficient to solve all the problems caused by non-normality () and frequently raises interpretation issues. For many real-life datasets, the distribution of the quantitative traits is not normal and often shows heavy tails, which in turn tend to make regular observations look like outliers. This is mainly the reason why non-normality and outlier presence are usually associated. In such scenarios, the classical approach, whose likelihood-based inference leans on the normality assumption and is known to be non-robust to small model deviations (), may be inappropriate, having low statistical efficiency if the tails are symmetric and large bias if the tails are asymmetric. This leads to tests with unreliable level and low power and to confidence intervals with also unreliable level and large expected interval length. We emphasize the fact that, contrary to some statements in the literature dating back to Box, 1953, the ANOVA F-test is not robust against non-normality (). Robust methods are designed to be resistant to influent factors such as outlying observations, non-normality and other model misspecifications (). Moreover, if the model verifies the classical assumptions, robust methods provide results close to the classical ones. Therefore, the use of robust methods has been advocated for inference in the linear and mixed linear model setup (). Also, we already see some applications in genetic association studies (), which show increasing concern with the violation of model assumptions and growing interest in using methods that are capable of coping with them. In this article, we propose that Huber M-estimation is used together with adapted Wald-type tests (Section 2) to assess trait/SNP associations. The performance of the proposed approach, is compared with both the classical and two non-parametric methods in terms of type I error rate and power in a simulation study under several contamination settings (Sections 3 and 4). Finally, we present a real data example (Section 5) and discuss the results obtained (Section 6).
BACKGROUND STATISTICAL METHODSWe describe the general multiple linear regression model aswhere n > p is the number of observed individuals. This model appropriately rewrites to Y = X +, where Y = (Y 1 ,...,Y n ) T is the (n1) vector of the response variable, X is the (np) design matrix,  = ( 0 ,..., p1 ) T are the unknown parameters and  = ( 1 ,..., n ) T is a vector of non-observable independent errors with expectation E() = 0 and covariance matrix var() =The least squares (LS) estimate of  is obtained by minimizing the residual sum of squares,where X i@BULLET = (1,X i1 ,...,X ip1 ). If X has rank p  n, the result is  LS = (X T X) 1 X T Y , with covariance matrixIn the classical approach, we have   N(0, 2 I n ) and  LS is also the maximum likelihood estimate (MLE) of . A general linear hypothesis concerning  is of the form H 0 : H = 0, where H is a known qp matrix with q  p. The general test for testing this hypothesis is to reject H 0 , at the level , if F  F  , where P H 0 (F  F  ) =  and and  are the unrestricted and restricted MLE of , respectively. We also know that, under H 0 , (np)F/qF q,np. We are interested in the following two testing situations: (i) H 0 :  1 =  =  p1 = 0  H = 0 where H =[0 I p1 ]; (ii) {H 0k :  k = 0} k=1,...,p1 {H = 0} where now H is a 1p vector of nulls with 1 at position k +1. In (i),and, under H 0 , (np)F/(p1)F p1,np. In (ii), for each H 0k test, we have q = 1 and so, again under H 0 , (np)FF 1,np. In practice, for each biallelic SNPs, there are two dummy variables in the regression model, that is, q = 2 and (np)F/2F 2,np (under H 0 ). As to the robust approach, the normality condition on the error distribution is relaxed to a quasi-normality condition and the estimators are obtained by methods other than ML. There are many robust regression methods in the literature but, since in the context of genetic association studies there are no outliers in the explanatory variables, we can restrict our attention to M-estimators, which have good computational and efficiency properties (). In the M-regression approach, the estimates are the solutions to the following minimization problem,where  is an appropriate function andand and is a robust estimate of . It is easy to verify that when (x) = x 2 , we have the LS/ML situation described above. Differentiating (5) for every  j , and equating to zero we get the p equations,...,p1.Although (5) and(6) are not always equivalent, (6) is useful in the search of solutions to (5). Moreover, considering the weights, where W is a diagonal matrix with elements W i. This shows that (6) can be solved by iteratively reweighted least squares (IRWLS). Choosing a robust estimator within the class of M-estimators is not always an easy task. We have considered the  function proposed bysince it is known that this function leads to efficient estimators under general conditions and provides a unique solution to (6):Other choices are available in the literature, e.g. Tukey's biweight, but do not guarantee a unique solution to (6) and one needs good initial estimates of the parameters to assure the convergence of the corresponding algorithm to the optimal solution. Using Huber's , the resulting M-estimator of  is efficient (for both normal and non-normal data) and it is robust against outliers in the response variable, which is precisely the situation we may find in practice. Library MASS in R has a model-fitting function rlm() in the conditions described above. By default, it uses -Huber with tuning constant b = 1.345 and both  and the scale parameter  are estimated by the IRWLS procedurePage: 817 815821
Robust methods in association studieswith initial estimates of  and  given by the LS estimate and the rescaled MAD, respectively. As to the robust tests for the general linear hypothesis, taking  = H, we considered the robust Wald-type statisticwhere  is an estimate of the covariance matrix of . SinceThe two non-parametric methods selected for comparison were rank transform (RT;), which was recently used in genetics () and a Wilcoxon based (WIL;), used in QTL analysis ().
SIMULATION STUDY
The simulation modelWe simulate N biallelic genes on one pair of chromosomes of an F2 population. We start by simulating the genotype for the first gene. Afterwards, each new gene genotype is simulated based on the previous gene genotype, assuming no crossover interference and a recombination fraction r between both genes, randomly taken from the uniform distribution, U(0,0.5) (, for further details on the simulation of an F2 population genotype). If a quantitative trait is assumed to be controlled by a number N of genes, without epistatic interactions, it can be simulated bywhere y j is the trait value for the j-th individual in the population, (x (2i1)j ,x (2i)j ) are the dummy variables associated to the additive and dominance effects for the N genes, respectively, (a i ,d i ) are the additive and dominance effects for each gene,  is the overall mean for the trait and  j is the random error for the j-th individual, j = 1,...,n. The dummy variables are coded (x (2i1)j ,x (2i)j ) = (1,0), (x (2i1)j ,x (2i)j ) = (0,1), and (x (2i1)j ,x (2i)j ) = (1,0), for genotypes AA, Aa and aa, respectively. In order to use (10), we need to specify some parameters underlying the simulation model. These are as follows: Heritability (broad sense): heritability is the proportion of phenotypic variation in a population that is attributable to genetic variation among individuals. It therefore quantifies the importance of the genetic effects to the trait value and is defined as the ratio of genotypic to phenotypic variancewhere  2 G and  2 e are the variance components associated with the genetic effects and the residual error, which may include undetected genetic effects, environmental effects and the random effects. From Equation (11) and for a certain value of H 2 ( = 0,1) we have the relationsGenetic, additive and dominance variances: the genetic variance can be decomposed asthe additive and dominance variances, respectively. Under model (10), the additive and dominance variances between genes are given by ()where r ij represents the recombination fraction between genes i and j. From the genotype data simulation, the values of r 1 ,.
..,r N1 coincidewith r 12 ,r 23 ,...,r (N1)N. We also have r ii = 0  i. In order to obtain the remaining values of the recombination fractions between genes, and since recombination fractions are not additive, r 1 ,...,r N1 can be converted to map distances d * 1 ,...,d * N1 via Kosambi's or Haldane's map function. Map distances are additive so, with the values of d * 1 ,...,d * N1 , the distances between all genes can be calculated, and through the inverse process the recombination fractions between all genes can thus be obtained. The relative importance of the additive and dominance variances can be quantified by their ratio and so we may consider, for simulation purposes,Additive and dominance effects: Assuming the relative sizes of the effects to be given by,...,N, the additive and dominance effects can be obtained from(13) with a little algebra:For a fixed value of H and  2 G = 1, and using Equation (12), we have  j = z e = z (1H 2 )/H 2 for each individual j = 1,...,n, where z is a random observation from the N(0,1) distribution. Finally, for a fixed value of t, the additive and dominance variances can be calculated from(14) and from there all the additive and dominance effects (15). The quantitative trait is then simulated for the n individuals.
Simulation settingsThe variation of quantitative traits is only shortly explained by genetic factors and thus we considered a trait heritability of 30%, i.e. H 2 = 0.3. Other parameters fixed were n = 500,  = 50 and t = 0.6. Additionally, for the purpose of the simulation, we considered independent SNPs, that is r ij = 0.5  i =j. This assures that the SNPs are in HardyWeinberg equilibrium (HWE) and that there is no LD between pairs of SNPs. Also, we considered the relative additive and dominance effects of the genesPage: 818 815821
V.M.Loureno et al.
SIMULATION RESULTSWe adjusted in R the additive model described in (1) via the usual linear regression (lm()) and the robust linear regression (rlm()) with the MHuber estimator, in the two simulation settings described. Function lm() was also used for the rank transform method, i.e. the original quantitative traits were replaced by their ranks. As to the Wilcoxon methodology, we used the R function wwest) and thus all methods are comparable, allowing for the use of the same multiple testing corrections. We used Bonferroni correction in order to control the FWER at the 5% level. In the (ii) under H 0 , there were no surprises, i.e. all methods were able to control the FWER at the 5% level, whatever the contamination, gross, intermediate or smooth. However, in the (iii) situation under H 0 , we noticed that in the smooth contamination setting, the FWER of the classic approach surpasses 5% most of the times, while the robust approach always keeps it around that threshold (). This tendency of the classic approach towards inflated type I error rates was also observed in the intermediate contamination setting and accompanied by the Wilcoxon approach also in the smooth and intermediate contamination settings (see results in the Supplementary Material). As to the rank transform approach, it only failed to control for the FWER at the desired level once. Under H 1 (and Supplementary Material), all methods show very good power to detect association between the SNPs and the quantitative trait, in the following order: Cls>WIL>Rob>RT and with relatively small differences. Even using Bonferroni correction, we had over 99% power in all methods, up to 5 SNPs, and still over 76% power when we took the model with 10 SNPs (0% contamination). With the introduction of contamination, as expected, the power of all methods decreases as the contamination level increases. However, the robust method shows much higher power to detect associations than the classic method and higher power than the rank transform approach, being neck to neck with the Wilcoxon approach. At the worst scenario, 10 SNPs in association with the trait and 10% gross outliers, the robust method, with rank transform and Wilcoxon close behind, has a power over 52% to detect those associations, while the usual model stays under 1%. Even in the smooth contamination case, the power of the classical method is only 23.5% versus over 48% for all the other approaches. Moreover, we must stress out that there are relevant power losses even in cases where the residual deviations from normality are not evident. See, for example, inthe 10 SNP model with 2% smooth contamination, where there is 13% power loss from the classic approach relative to the robust one but whose residuals do not look different from normal (see Q-Q plots in the Supplementary Material). If we now analyseand the correspondent table from theSupplememtary Material, we see that the robust method detects in general more SNPs than the classical and rank transform procedures. Although that difference may not look substantial in the smooth contamination setting, it is quite evident in the 10 SNP simulation for 5 and 10% gross contamination. If compared to the Wilcoxon approach, in the 10 SNP simulation Wilcoxon comes off better than the robust approach but with a maximum difference of only 0.08.and the correspondent table from the Supplementary Material show the results for the 10 SNP simulation scenario. Note that the global robust test, as well as the rank transform and Wilcoxon global tests, keep a 100% power in all simulation settings, whereas the classical power falls down to 15.21% at the 10% gross contamination setting.
EXAMPLEAs an example of application, we downloaded the data of, and www.panzea.org), with respect to the quantitative trait FERL (length of the female and hermaphroditic portions of the basalmost ear on the lateral branch), getting information on 61 SNPs from 22 Page: 819 815821candidate genes, chosen based on their possible effects on the trait under study, given their known mutant phenotype in maize or other plants. In that paper, Weber studied the association between these SNPs and this Teosinte (maize's wild ancestor) trait, among others, by adjusting the mixed linear model (as presented by),
Robust methods in association studieswhere y is the vector of phenotypic values, v is a vector of fixed effects regarding population structure (inferred via PowerMarker,),  is the fixed effect for the candidate SNPs, u is a vector of random effects relative to recent coancestry, e a vector of residuals, P is a matrix of the 10 significant principal components [as suggested byand discussed by], S a vector of the SNPs genotypes and I an identity matrix. The structure assumed for the variances is as follows: var(u) = 2KV g and var(e) = IV R , where K is the Kinship matrix, which quantifies the proportion of shared alleles, V g =  is the genetic variance and V R =  2 is the residual variance. The pertinence of model (16) in this study is justified by the systematic sources for spurious associations found under the simple model,non-uniformity of the P-values and high type I error rates () observed while testing 498 randomly genotyped SNPs one at a time (). The adequacy of the full model to test under the null hypothesis is also clear, with only  5.4% by chance significant associations in both the classical and the robust methodologies. Here, we configured SNP as a numerical covariate. However, when we are not under the null, the codification of SNP as factor is more adequate in order to capture eventual dominance effects, unless of course the effects in the model are only additive, which may not always be the case. In these circumstances, we will therein consider SNP coded as factor. We started by rewriting model (16) asis known, and is positive semi-definite, taking its Cholesky decomposition, leads toThe ML estimates of  and  2 may then easily be obtained. Usually, the matrix V is unknown and  0 is estimated by ML, restricted maximum likelihood (REML) or some other method. We should note that Weber used the SAS Proc Mixed routine with REML, and we used ML from the R function lmekin() in package kinship, which may justify the slight differences between both results. Once an estimate of V , V , has been obtained, it is used in (19) so that the estimates of  and  2 can be calculated. Inference for the fixed-effects terms, i.e. the tests H 0 : H = 0 of q  p fixed-effects, were conducted by the usual F statistic with degrees of freedom estimated by the Satterthwaite approximation (as in).
Single SNP analysisWe performed the classical analysis described above for each of the 61 candidate SNPs with routine lmekin() from the R package kinship. The robust analysis was performed with a robustified version of this instruction (Supplementary Material). This single SNP analysis allowed for the detection of nine associations in the classic analysis and eight in the robust analysis at the 5% level (i.e. P  0.05), all associations being significant after correction for multiple testing via false discovery rate (FDR) (q  0.1),. In both analysis, we have the six SNPs identified by. Moreover, with the exception of zagl1.6, all other detected SNPs are common between approaches. Also, (i) from the identified SNPs, SNPs PZD00073.5 and PZD00073.8 are not independent since they are in high LD (rdominance; (iii) from the R 2 values, we acknowledge that all individual effects are small, ranging from 0.8% to 2.7% in the classic analysis and from 0.3% to 2.8% in the robust analysis, therefore explaining only a small fraction of the phenotypic variation. This could be because the marker assayed is not the causative site but is in LD with the causative site giving an underestimate R 2 of the real effect, the trait may have low heritability or the associations may be due to alleles of small effect. We additionally performed the ShapiroFrancia (SF) normality test on both the residuals from the classic and robust approaches and observed that they all failed the normality assumption showing heavy tails, specially on the left side of the distribution. Although it is not a pre-requisite in the robust analysis, residual normality is one of the classical assumptions. This violation indicates that either the classical analysis normality assumption we have made is not realistic or the model adjusted is not good. Either way, one should take care with false association detection (possibly the case of SNP zagl1.6 detected in the classical methodology but not in the robust) and possible reduction in power.
Multiple SNP analysisWe further investigated the previous results in a joint analysis of the SNPs detected above. This multiple SNP analysis (), after correcting for multiple testing with the conservative Bonferroni correction (bold P-values), left us with only two significant SNPs in the classic analysis (PZD0006.1 and PZD00022.3) and three significant SNPs in the robust analysis (PZD0006.1, PZD00022.3 and ba1.9). If we consider the FDR at level 10% (bold q-values), as in, then both methods detect SNPs PZD00006.1, PZD00022.3, ba1.9 and PZB00049.7. The classic analysis additionally detects SNP zagl1.6. In order to evaluate the models adequacy, we plotted the conditional residuals plots (), and performed the SF test of normality, verifying, again the non-normality of both the classical (p 0.034) and the robust residuals (p 0.003) at the 5% level. The plots inalso show heavier tails on the left of the distributions (normal QQ-plots) and the presence of possible outlying observations (conditional residuals plots). To conclude, we removed the outlying observations identified by the robust analysis, namely, TAMex0344/0719/0775/0802/0805/ 0807/0821/1534, and re-run the classical analysis without them (,*). We now had p 0.9325 for the SF normality test. Also, we observed a reduction in the number of SNPs detected by the classical methodology from 5 to 3, where zagl1.6 no longer appears as a significant association, reinforcing the idea that it was really a false positive. It might be expected though, that the classical analysis without the outlying observations would produce the same results as the robust analysis with all observations. However, SNP PZB00049.7 now misses significance by merely 0.0092. Thishappens because the robust analysis gave the outlying observations weights between 0.36 and 0.55, whereas the classical approach without the outliers actually corresponds to giving zero weight to the outliers and one to the remaining observations. We have seen in the simulation study that, when testing under the alternative hypothesis with no contamination, the classical methodology can be in some cases slightly more powerful than the robust. However, we argue that removing outliers to achieve normality is not acceptable since they may contain some information on the model. This is also sustained in the simulation contamination setting, where we acknowledged the robust approach to be actually much more powerful than the classic approach. We, therefore, have reasons to believe that the SNPs identified in the robust analysis should be the ones further investigated for association with FERL. Moreover, SNP PZB00049.7 that missed significance in the final classical analysis may well be a false negative.
DISCUSSIONIn Sections 3 and 4, we compared the performance of classic, robust and non-parametric methodologies in association studies in a particular simulation frame. We showed that under the null, without contamination, the four methods considered have control of the FWER at the desired level. We acknowledged not only the tendency of both the classic and Wilcoxon approaches towards inflated type I error rates, but also that the robust approach proposed (Huber M-regression plus Wald-type tests to assess association) is not as sensitive to outlier contamination as the classical approach and is more powerful than the rank transform approach to detect SNP/trait associations. Despite the fact that the Wilcoxon approach kept close to the robust methodology in terms of power, its tendency to inflated type I error rates and computational issues indicate that for an association study involving a small number of independent SNPs to be tested (as in the simulation study), the robust multiple SNP linear model is preferable over the remaining approaches. In Section 5, we applied the classic and robust methodologies to a published real dataset. This dataset was not in the conditions of the simulation study and was therefore treated accordingly (). Results showed the presence of outliers and therefore the non-normality of
Robust methods in association studiesresiduals. Having these outliers removed and re-running the classical analysis, still the robust methodology proved to be more adequate. It is clear that the disregard of the non-normality in the classical approach may lead to the use of suboptimal estimators and hence inaccurate conclusions, i.e. spurious associations and/or false negatives. Furthermore, it is well known that the ordinary LS estimator is quite sensitive to outliers and long-tailed distributions and that it has poor efficiency relative to many robust estimators when the errors are not normally distributed. We also argue that the removal of outlying observations on statistical grounds alone is not advisable: (i) with classical methods the 'true outliers' are not always visible due to masking and swamping effects; (ii) it may be easy to identify gross outliers but it is not easy to separate those from mild outliers and these from regular data; (iii) outliers may still contain some relevant information. Plugging-in robust estimators to classical approaches reveals to be a proper way of addressing the problem. Since most genetic association studies in plants and animals are on economically important traits, too many false positive associations can incur in time and money losses. It is, therefore, compelling that a good compromise is achieved between true and spurious associations. We believe that this work enlightens a new pathway in achieving that goal.
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
i = 2,...,N so that in this way every gene in the model would have an important, though unequal, contribution to the trait value. As to the number of SNPs in the model, we took N = 2,3,4,5 and 10. A percentage, 2,5 and 10%, of data contamination (outliers) was also considered. The contamination was generated from a normal distribution N( c , 2 c ), where  c was obtained from a uniform distribution U(1,5) and  c from U(80,90), U(60,70) and U(55,60), corresponding, respectively, to gross, intermediate or smooth contamination. Association tests were run 10 000 times. In order to compare the modelling approaches, one needs first to investigate how they control the family wise error rate (FWER) at a prespecified level. On the other hand, a better performance of one over another means POWER. Hence, we have the following two testing settings: Under the null hypothesis: Under the null hypothesis, we looked at three distinct situations: (i) traits were simulated independently from the SNPs genotypes out of a N(50,1) distribution; (ii) traits were simulated independently from the SNPs genotypes out of a N(50,1) and a percentage of outliers (2, 5 and 10%) was then introduced in the trait values; (iii) a percentage of the traits (98, 95 and 90%) were simulated independently from the SNPs genotypes, that is under H 0 , but using the normal contaminant distributions, whereas the remaining traits (2, 5 and 10%) were generated from model (10), that is under H 1. Under the alternative hypothesis: Under the alternative hypothesis two situations were considered: (i) the traits were simulated according to model (10); (ii) the traits were simulated from model (10) and a percentage (2, 5 and 10%) of outliers was introduced.
