Motivation: Structural information of macromolecular complexes provides key insights into the way they carry out their biological functions. Achieving high-resolution structural details with electron microscopy requires the identification of a large number (up to hundreds of thousands) of single particles from electron micrographs, which is a laborious task if it has to be manually done and constitutes a hurdle towards high-throughput. Automatic particle selection in micrographs is far from being settled and new and more robust algorithms are required to reduce the number of false positives and false negatives. Results: In this article, we introduce an automatic particle picker that learns from the user the kind of particles he is interested in. Particle candidates are quickly and robustly classified as particles or non-particles. A number of new discriminative shape-related features as well as some statistical description of the image grey intensities are used to train two support vector machine classifiers. Experimental results demonstrate that the proposed method: (i) has a considerably low computational complexity and (ii) provides results better or comparable with previously reported methods at a fraction of their computing time. Availability: The algorithm is fully implemented in the open-source Xmipp package and downloadable from http://
INTRODUCTIONElectron Microscopy (EM) is a key tool to study the structure and function of biological macromolecules at a mediumhigh resolution. Single particle analysis is an EM modality in which multiple copies of the same macromolecule are simultaneously imaged into a single micrograph. Particles from several hundreds or even thousands of micrographs are commonly employed in a structural study. The standard data processing workflow of single particle reconstruction includes: particle selection, particle alignment, particle classification, three-dimensional (3D) reconstruction and model refinement (). Different views of a specimen are required for the 3D reconstruction of a complex, but these views suffer from low signal-to-noise ratio (SNR) (due to low-dose imaging) (), low contrast (due to close to focus conditions), and image deformations (due to the microscope aberrations). It is generally accepted that high resolution can only be achieved with thousands of projection images, so that the 3D reconstruction algorithm can compensate for these challenging imaging conditions. In particular, there is generally a direct relationship between the number of selected particles and the maximum achievable resolution (). Manually identifying that number of particles is not just time consuming and laborious, but also an error-prone process. A robust automatic particle picker (APP) algorithm is, therefore, indispensable to enhance the technique's throughput. As the selection of several thousands of particles from low-dose micrographs is the first and one of the crucial steps towards a highresolution reconstruction, a large amount of effort has been made by researchers to develop accurate methods for APP. These methods have been classified into groups by different authors (). Among them, the classification by, suggests a general categorization into template matching-based and feature-based approaches. Template matching-based methods () calculate the cross-correlation (or any other measure of similarity) between a set of templates and a micrograph image to seek for particle candidates. Templates are obtained either from different projections of an initial 3D volume () or from a number of manually picked particles (). Instead of using all templates (either from different projections of an initial 3D volume or from a number of manually picked particles), which severely increases the processing time, other alternatives can be employed, such as eigenimages of templates () or some form of an average of each template cluster (). In feature-based approaches, particles are sought through the calculation of some prominent geometric and/or statistical features of the particle images (). Feature-based methods can be reference-free or learning-based. In the former, features corresponding to particles are known to fall within a certain region of the feature space and, therefore, no training is necessary and the algorithm can start picking particles straightaway; in the latter, however, a set of particles and nonparticles are required to train a classifier which is then able to distinguish between particles and non-particles based on the training features. Although reference-free methods require less effort from the user, they are of limited applicability, because the space region corresponding to particle features has to be known a priori. We previously introduced a feature-based APP method () that learns features from the user selected particles via a continuous learning phase (the algorithm is available in the open-source package Xmipp 2.4). In a manual picking step, a small dataset of particle and non-particle images is formed to train an ensemble naive Bayesian classifier. Once the classifier is trained, it suggests new particles in a new micrograph. The user supervises this result by discarding the wrongly picked particles and identifying the disregarded ones. This feedback information is then submitted to the classifier which is updated to accommodate this new information. This semi-automatic picking is continued on several micrographs until the user is satisfied by the results. At this point, the trained classifier carries out the selection of the particles in the remaining micrographs in a fully automatic way. In this article, we introduce an APP method that follows the general learning structure of, but major improvements are made to increase speed and accuracy. Our new feature vector is completely different from the previous method and consists of a number of geometrical and statistical features; it is robust to noise, very fast to compute and most of its features are rotationally invariant. Instead of Naive Bayesian (NB), we now use a support vector machine (SVM) as the base classifier due to its interesting properties, like high generalization capabilities and small training/classification time. In order to reduce the number of false positives, two SVM classifiers are used: one for discriminating between particle candidates and non-particle objects and the other for checking if a particle candidate recognized by the first classifier is a real particle or not. In contrast to, which explores a big search space for particles, the proposed method limits the search space to the peaks obtained from the cross-correlation of the micrograph with some pre-computed templates. Templates are generated during the manual picking step by clustering [the clustering algorithm is described in] the hand-picked particles and selecting the average of each cluster. As the correlation with all orientations of the template noticeably increases the computation time, a rotationally averaged template can be used instead. The algorithm has been successfully tested on three experimental datasets and compared with our previous algorithm (which had, in turn, been compared with other approaches), resulting in a more than an order of magnitude decrease in computing time while achieving even better performance.
METHODSThere are three crucial steps in the proposed algorithm: identifying initial possible locations of particles, locally characterizing the image by means of a low-dimensional feature vector and classifying each location as particle or non-particle. In this section, we go through the details of each step.
Identification of possible particle centresIn principle, each pixel of a micrograph has the potential to be the centre of a particle. In practice, checking each pixel is rather time consuming, and so the number of possible candidates to evaluate must be reduced. Therefore, we cross-correlate the input micrograph with suitable templates to make an initial guess about the candidate particle positions. In this way, not only the search space is noticeably reduced, but also the rate of false positives. During the training phase, we cluster manually picked up particles into a few number of classes using an algorithm similar to the one described in. The number of clusters can be increased or decreased dynamically to facilitate user's ongoing control on the quality of the templates. These templates are correlated with the input micrograph at all possible orientations (at each location of the correlation map, we should keep the maximum observed correlation for all templates and all orientations). If the particle is relatively globular, the calculation of the correlation map can be further accelerated by substituting the template by its rotational average, avoiding in this way the need to correlate with all possible orientations. Local maxima of the correlation map are possible candidate locations for being at the centre of a particle. We further reduce this candidate set by ranking local maxima according to their correlation values and keeping only a portion of the local maxima with a value higher than a small threshold. This portion may not be the same for all datasets, and basically depends on the density of particles within the micrographs. We should note that the cross-correlation with a template in our approach is just a first step to have a set of possible candidates, but that in no way do we rely on this cross-correlation for further processing of these candidates into particles and non-particles. In fact, the chosen threshold for local maxima is small enough such that even particles that were related to excluded templates could still be detected at this initial step (). In most practical cases we have found that the use of only one template is generally enough. For instance, in, only a single class representative has been azimuthally averaged and cross-correlated with the micrograph, to identify the local maxima. In this figure, pickingAs can be seen, at the location of each particle a local maximum with a specific energy can be observed. Still, this behaviour depends on the practical case at hand, and consequently, the number of templates is dynamic, so if the result is not satisfactory the user can request for more templates.
Feature extractionWe compute a feature vector at each location previously identified as a candidate for particle centre. This vector is used by the classifier in the next step to distinguish between particles and non-particles. Two properties have been sought for the feature vector: being robust to noise (due to the low SNR in the micrographs) and being rotationally invariant (to save computational time and avoid having to look for the particles in all possible orientations). We use a feature vector that is robust to noise and most of its features are rotationally invariant. it is made of three subsets of features: the first two feature subsets are sensitive to the particle shape, whereas the third one encodes the particle grey intensities. The first and third feature subsets are rotational invariant but not the second one.
Particle shape description at different frequencies Micrographsare submitted to a filter bank with N h raised cosine band pass filters to decompose them into several sub-band images in order to being able later on to extract features associated to particular frequencies (). The filtered micrograph by the kth (k  1,. .. , N h ) filter, M k r (i.e. kth subband image) is computed bywhere FT 1 is the inverse Fourier transform and MR and H k R show the Fourier transform of the micrograph and kth Fourier filter in the filter bank, respectively. R is the two-dimensional (2D) spatial frequency, and r is the spatial coordinate within the image. Each raised cosine band pass filter has a particular width  R and a decay of R (see the last image in column B of). The low-pass filter in the filter bank is defined by a transfer function given bywhere R is the modulus of R. The kth band pass filter (k  2, 3,. .. , N h ) is defined by the transfer function H k R defined asLet us concentrate now on a given particle within a micrograph.where I k 0 r,  is shifted over I k r,  by r (, where r p is the radius of the particle), and by  (180 , 180 ) along r and , respectively (see column D in). In particular, for each k, we calculate kk 0 for k 0  k (autocorrelation; k  1,. .. , N h ), k 0  k  1 (k  1,. .. , N h  1) and k 0  k  2 (k  1,. .. , N h  2). The autocorrelation of a given band is related to the particle shape, and the cross-correlation between sub-bands reveals the linear relationships between the shapes at two different frequency bands. We can see inan example of how these cross-correlation functions can, indeed, distinguish between particles and non-particles. From this figure, it is clear that cross-correlation functions for particles (first row) are quite different from those for nonparticles (second row). It is worth mentioning that, for the sake of speed up, we just consider the 2D projections and not the original 3D object. According to this, cross-correlation functions are not rotationally invariant in 3D, and therefore we need to have enough projections from different orientations to fully cover the projection space. The cross-correlation functions of the training set of particles for a particular combination kk 0 are highly redundant and can be easily compressed using principal component analysis (PCA) (). There are as many kk 0 images as candidate particles, which is a number in the many thousands. This is the set from which the PCA basis is extracted (there are, therefore, as many PCA's as all combinations of k and k 0 ). For each kk 0 image, we keep its projection onto the first N b PCA vectors as features to be used during the classification step. Note that the PCA basis is calculated for the kk 0 cross-correlations of the training dataset of particles. This means that the kk 0 cross-correlations of non-particles will be poorly represented by this PCA basis. In this way, the dimensionality reduction itself is presumed to have a positive impact on the classification accuracy. An example of these bases can be seen in, where four eigenvectors are shown for each cross-correlation function.
Particle shape descriptionin a particle-adapted rotational invariant subspace Ponce and Singer (2011) proposed to calculate an image basis that is adapted to the kind of images being studied and their in-plane rotations. They do this by calculating the PCA of a set of images and all their possible rotations. We apply this principle to extract some features from boxed particles or particle candidates [this idea is partially the same as in. Given the training particles provided by the user, we first align them into a few templates using a process similar to that described in. Then, we compute the PCA basis associated to these templates () to form a rotational invariant subspace (the basis of this subspace is able to reproduce any 2D rotation of particle images). We keep the first N rb projection coefficients onto this basis as part of the feature vector. Note that the subspace that is spanned by these vectors is rotationally invariant, but not the basis itself. Therefore, particles with different 2D orientations have different coefficients but still can be efficiently approximated using this basis. Again, this basis has been especially designed to represent good particle images; consequently, non-particles will be poorly represented by the basis helping the classifier to perform its task.
Particle intensityDesired particles are assumed to follow a specified pattern of intensity distribution once the boxed image is normalized to have zero mean and unit power (). For instance, particles in the carbon region are normally discarded. To capture the intensity features desired by the user we calculate the mean, the SD as well as N i equidistributed deciles of the intensity histogram.
Feature vectorWe collect all these features into a feature vector that characterizes the boxed image to be classified. The vector size is 3N b N h  1  N rb  N i  2. By default, we suggest to use N b  4, N h  6 ( R  0:025, R  0:02 in digital frequencies normalized to 0.5), N rb  20 and N i  9. This produces a feature vector of dimension 91. In practice, we have observed that these choices provide generally good results on all the tested datasets. The values of the parameters depend on both properties of the micrographs and particles. For instance, for a low defocus set of micrographs, we need a higher value of N h to extract the information of higher frequencies, whereas for smaller particles we should increase N b and N rb to be able to regenerate them from the basis.
ClassificationDistinguishing between particles and non-particles is, in general, a complex task due to the low contrast and low SNR present in the micrographs. Additionally, the problem is complicated by having to distinguish between particles and damaged particles, contaminated particles, partly formed particles, etc., which, in general we will denote as 'errors'. We use non-linear, binary support vector machines (SVMs) () as classifiers due to their good performance in other classification problems [it is particularly used successfully in APP by;, their robustness to noise and their speed. The general idea of the SVM classifier is to find an optimum hyperplane in an n-dimensional space by which two different classes are distinguishable. LIBSVM () is an efficient and widely used implementation of the SVM. This package suggests a variety of kernels to perform the non-linear classification. We use this package with a radial basis function (RBF) kernel to gain a high accuracy in our classification (see Supplementary material for an introduction to SVM). We use two SVM classifiers. The first classifier is responsible to discriminate between particle objects and any other kind of objects (non-particles and errors). Because of the similarity between errors and particles, the output of the first classifier is not so accurate, and some errors are labelled as particles. Therefore, to reduce the false positives to a feasible extent, the second classifier is dedicated to just focus on distinguishing particles from errors. This strategy was already used by.shows the behaviour of the classifier for three types of objects. As can be seen, the first classifier passes the error, but thesecond classifier rejects it. In fact, the learning process follows the same steps as in the algorithm by. First, the user picks all particles from the first micrograph and they are clustered in order to construct particle templates. All non-selected locations are assumed to correspond to non-particles and some of them will be later used to train the classifiers. This manual step can be continued with more micrographs to expand the training set. It is worth pointing out that none of the classifiers are trained at this point. Once the training set is large enough (empirically, at least 30 training particles are required), the process enters a supervised phase. The rotationally invariant subspace as well as the PCAs for the polar, sub-band cross-correlations are calculated on the training particles. Then, feature vectors are calculated for the manually selected particles and nonparticles. Finally, the first classifier is trained using this data. At this point, the algorithm tries to automatically pick the next micrograph in the list of micrographs that was not previously manually picked. After suggesting possible particle locations, the user can correct the results by adding those missed particles (false negatives) and removing wrongly picked particles (false positives). After being corrected by the user, the first classifier is retrained using all previous information plus the new set of false negatives and false positives. The second classifier is trained to distinguish between all particles known so far and the set of false positives. This process can be repeated several times on more micrographs till the performance of the classifiers is not further improved by user corrections. This ongoing learning process is particularly interesting because the classifiers carefully adapt to the user's preferences. When the user is satisfied with the performance of the classifier during the semi-supervised phase, he can go to a fully automatic particle picking mode, in which all micrographs that have not been picked yet are automatically picked (in parallel). At the end of this process, the user can supervise the result and eliminate wrongly picked particles or add missed particles.
RESULTSWe applied our APP method on three datasets to assess the speed and accuracy of the proposed algorithm for micrographs with different contrast, density and particle shape and size. These) corresponding to a given template, (a) Given particle template. (b) First 20 eigenvectors of the template that generate a rotational invariant subspace datasets are: KLH (keyhole limpet haemocyanin), adenovirus and helicase. All the experiments were done with the same fixed parameters (N b  4, N h  6, N rb  20 and N i  9) on a single core of a CPU Intel Core i5, 64 bits, 2.53 GHz (of a standard laptop with 4 gigabytes of RAM). To show the results in a quantitative way, we have used the performance metrics introduced by Langlois and Frank (2011). If TP is the number of true positives, FP the number of false positives and FN the number of false negatives, then these metrics are defined asshows the fraction of picked particles by the algorithm that is real particles, and recall indicates the fraction of true particles that are picked by the algorithm. F-measure is a harmonic mean to summarize both recall and precision.
KLH datasetThis dataset was produced as a general benchmark for APP by. It includes 82 micrographs of KLH particles. A Phillips CM200 TEM was used to record the micrographs on a 2 K  2 K CCD Tietz camera at a magnification of 66 000 and a voltage of 120 kV. The sampling rate at this magnification was 2.2 A  /pixel. In the 3DEM Benchmark site (http://i2pc.cnb.csic.es/3dem benchmark) the dataset has been split into two datasets: one with 30 micrographs for training and another one with 50 micrographs for testing. We set the size of particles to 200 pixels, and processed 10% of the local maxima of the correlation map. To evaluate the connection between the number of particles and accuracy metrics, we calculated precision, recall and F-measure for the test dataset after training the classifier with different numbers of true particles of the training dataset.shows how these three parameters change as the number of training particles is increased. As can be seen in this figure, precision, recall and F-measure of the algorithm are increased if we keep training the classifier with more and more particles. Training with more than 173 particles (13 micrographs), a precision range ofand a recall range ofare achievable. F-measure ranges as a summary of precision and recall in the interval. Since there is a trade-off between recall and precision, F-measure looks smooth, and changes in a small range. Training the algorithm with 39 manually picked particles took 3.5 s. After training, the algorithm needs 1 s to suggest new particles on each new micrograph and 2.5 s to retrain after being corrected by the user. Picking particles from 50 micrographs of the test dataset took 51 s, without any parallelization (however, current Xmipp implementation can benefit from multiple CPUs by concurrently picking different micrographs). According to the 3DEM benchmark, our previous APP method was capable of selecting particles with an average time of 47 s, precision rate 80.94% and recall rate 68.59%. For the new algorithm, the precision and recall rates are reported as 85.16% and 74.81%, respectively, and the average processing time for each micrograph is 1 s. Therefore, the proposed algorithm is very fast and produces accurate results compared with our previous algorithm () as well as with the reported results in the APP challenge ().shows the result of the algorithm for one micrograph of the test dataset with nine particles. In this figure, nine green squares show the selected objects, from which one long particle with '' symbol is a false positive. One particle, marked with '' symbol, is missed from the final result.
Adenovirus datasetIn this experiment we examined the reliability of our method by means of a set of micrographs () that has lower contrast but higher density and a larger particle size than the KLH dataset ().There is one false positive and one false negative, which are identified by  and , respectivelyAdenovirus type 2 is an icosahedral mammalian virus with a molecular mass of 150 MDa, and consists of genomic DNA and structural proteins. Samples were vitrified in liquid ethane, and a FEI Tecnai G2 FEG microscope at working voltage of 200 kV was used to analyse the samples. Micrographs were recorded on a film at a magnification of 5000, and digitized in a Zeiss Photoscan TD scanner using a step size of 7 mm, which provided a sampling rate of 1.4 A  (particle size at this sampling rate is 600 pixels). True particles for this dataset were manually identified by PerezPerez. Like in the previous experiment, the original 305 micrographs are divided into a training dataset with 30 micrographs and a test dataset with 275 micrographs (these two datasets can be obtained from http://i2pc.cnb.csic.es/3dembenchmark/). The sizes of the micrographs are not the same, but we know that 40005height515 000 and 40005width512 000 pixels. The dimensions of the micrographs were internally reduced by a factor of 4. We examined 90% of the local correlation maxima, and assumed that one template was enough for the cross-correlation. We used 334 particles and 690 non-particles from the first 14 micrographs to fully train the classifier. The classifier was used to automatically pick the particles of the test dataset. This experiment resulted in picking 9216 particles with precision rate 92.17% and recall rate 90.24%.shows the result of the automatic picking for one micrograph of this dataset. As it can be seen in this figure, the algorithm picks 115 particles from this micrograph and particles close to or inside the carbon parts are rejected. The average processing time per micrograph was 34 s, which is more than for the KLH dataset because it is more dense (90% of the local maxima are checked) and the size of the micrographs and particles were twice and three times larger than the KLH dataset ones, respectively.
Helicase datasetIn this section we work with the complex of a helicase and its loading factor with a total molecular mass of 0.39 MDa (V.A. et al., unpublished data). This dataset consists of 160 micrographs of size 4046  4046. The CCD of a JEOL JEM-2200FS microscope with magnification 50 000 and voltage 200 kV provided digital micrographs with pixel size 2.16 A . The rather dense micrographs of this dataset (the average number of particles in each micrograph is 572) () as well as the small particle size make them particularly difficult. Additionally, contrast is especially low due to the cryo-EM conditions. This in-house dataset is not published yet, and no benchmark is available in order to check the accuracy quantitatively, therefore the result is given qualitatively. We set the size of particles to 100 pixels, and processed 90% of the local maxima to ensure that no particle was missed. To train the classifier, 1037 positive and 2510 negative samples were extracted from the first two micrographs, and then for the third micrograph the algorithm automatically picked 874 particles in 50 s.shows the result for the third micrograph of this dataset, qualitatively in agreement with user expectations.
DISCUSSION AND CONCLUSIONIn this article, we proposed an APP algorithm that identifies particles from electron micrographs more accurately and faster compared to our previous method by, already one of the best performing methods. A set of robust shape-related and statistical features are extracted from particle candidate locations (that are distinguished by cross-correlation between the templates and micrograph) and a two-stage classifier decides if each feature vector is a particle or not. Like in, the learning process of the classifier is continuous to grab the features of the user desired particles, and the second stage of the classifier is used to concentrate just on making distinctions between particles and errors, which are very similar to particles. Experimental results for three datasets show that this algorithm is able to select particles accurately even from low-contrast and highly dense micrographs. The feature vector includes three types of features. This feature vector is robust to noise and has a high discrimination power, so that the classifier can distinguish between particles and nonparticles. Having two types of features for shape description at the same time helps us to reduce the probability of producing partially similar feature vectors for particles and non-particles. For instance, in case of the KLH dataset, the descriptors at different frequencies are not adequate to properly distinguish between side and top views of the particle. On the other hand, the calculated shape descriptors from rotational invariant subspace are not sufficient to separate errors very similar to the side views of the particle. As a summary, these two types of features conspire efficiently to decrease the false positive rate. Statistical features are also important to catch the properties of the intensity distribution, and prevent the APP from picking particles from the carbon parts of micrographs. The role of this type of feature is clearer for helicase and adenovirus datasets, where more particles lie in dark areas of the micrographs. In Supplementary material, the classification power of the individual features is assessed in depth. To decrease false positives as much as possible, a two stage SVM classifier is used to classify the feature vectors: the first stage to separate particles and non-particles, and the second stage to remove errors (very similar to particles) from the output of the previous stage. Each SVM component of the classifier is capable of performing the separation with an accuracy of 490% (see Supplementary material). The second component of the classifier plays an important role in reducing the false positive rate (e.g. 15% of wrongly selected particles were discarded by the second classifier in the KLH dataset). There are six parameters that can be set by the user: number of sub-bands, PCA basis, rotational PCA basis, consecutive subbands to be correlated, templates and percentage of local maxima in correlation map to keep. Although these parameters help the user to achieve the highest possible accuracy, they can result in complexity. To moderate this complexity, the value of the first four parameters is set by default, so that the user can focus on adjusting the last two parameters (number of templates and percentage of local maxima to keep). Regarding the particularities of the datasets, KLH is a dataset with highly contrasted particles, but there are two sources of errors that make it challenging. First, a few background objects and also noisy top views that present features similar to the ones of the particles. Second, KLH can polymerize to some degree. The second classifier is in charge of removing these polymerized particles, and keeping, at the same time, the recall rate at a reasonable level. For the adenovirus dataset, our algorithm achieved a better accuracy than for the KLH dataset. The reason is that, although the micrographs are denser and have samples with lower contrast, the similarity between particles and non-particles is not as high as in the KLH dataset. The second classifier rejects just 3% of the outputs of the first one. Difficult cases in this dataset are those particles located on carbon areas, which are efficiently distinguished by statistical features. The speed of the algorithm was examined for the three datasets. According to the density of micrographs, it can go from 1 s to 50 s. Most computations are related to feature extraction, especially the shape descriptors. In order to eliminate this bottleneck and improve the speed even more, in our implementation particle candidates are divided between different threads. In addition to this, the automatic selection of particles from micrographs is performed in parallel. In this way, our APP is extremely fast and can be executed in a very short time. This algorithm is included in Xmipp 3.0 and downloadable from http://xmipp.cnb.csic.es. The APP is accessible through the protocols described by
The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
V.Abrishami et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Automatic selection of particles from electron micrographs at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
