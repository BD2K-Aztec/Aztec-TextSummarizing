Motivation: Electron cryo-microscopy can be used to infer 3D structures of large macromolecules with high resolution, but the large amounts of data captured necessitate the development of appropriate statistical models to describe the data generation process, and to perform structure inference. We present a new method for performing ab initio inference of the 3D structures of macromolecules from single particle electron cryo-microscopy experiments using class average images. Results: We demonstrate this algorithm on one phantom, one synthetic dataset and three real (experimental) datasets (ATP synthase, V-type ATPase and GroEL). Structures consistent with the known structures were inferred for all datasets. Availability: The software and source code for this method is available for download from our website: http://compbio.cs.toronto .edu/cryoem/ Contact:
INTRODUCTIONSingle particle electron cryo-microscopy has achieved remarkable success in determining 3D structures of macromolecules at resolutions close to atomic scales (). Some of the protein structures recently determined at high resolutions using this modality include 15 bacteriophage at 4.5 , GroEL at 4.2  and cytoplasmic polyhedrosis virus and rotavirus at 3.8  (). These experiments used a large amount of data, knowledge of the symmetry of these structures, and an approximate initial model of the structure to achieve this high resolution. However, successful 3D inference of asymmetric structures and symmetric structures with unknown symmetries still remains a challenging task in the absence of an initial model. In this article, we present a general method for creating an initial * To whom correspondence should be addressed. model (typically referred to as ab initio 3D inference) that can be used to create high-resolution structures. Before we describe our method, we first briefly describe the typical data analysis steps in performing 3D inference using single particle images. The reader is referred elsewhere for a detailed review of the various analysis steps in the data-processing pipeline (). Electron micrographs are electron-optic images of the biological sample recorded on a charge-coupled device or on film. These images contain projections of hundreds to thousands of copies of the macromolecule in different and unknown orientations (which may or may not be uniform over the space of rotations). However, the projections are convoluted with the point spread function of the microscope and corrupted by noise. Before a 3D structure can be estimated, these large micrograph images must be preprocessed to select the subsections of the images that correspond to the projections of the individual macromolecules (or particles) under study. Extensive research has been conducted on achieving this goal, called particle picking, using different techniques (). Once these particle images have been extracted, 3D structure determination may be performed in one of the two main ways, depending on the availability of a prior low-resolution model of the macromolecule. With an initial model, the particle images that have been picked are used to generate a higher resolution model through a process called refinement. To do so, the orientations of the noisy particle images with respect to the initial model are first determined, and then these orientations are used to generate a high-resolution model. Several methods have been developed to determine the orientations of particle images with respect to a model. In the projection matching technique (), projections of the model at different orientations are computed and the particle images are matched to these projections to determine their orientation. Frealign () uses a signal-to-noise ratio weighted correlation coefficient between the Fourier transform of the particle images and the corresponding central slice of the Fourier transform of the 3D model to determine the orientation of particles. Maximum likelihood-based procedures have also been developed for aligning particles to multiple 2D () and 3D references (). These methods have the added advantage that the refinement of the 2D references or 3D models is performed simultaneously with the alignment.
Bayesian method for structure inference in electron cryo-microscopyIn the absence of an initial model, the noisy particle images are first classified into groups, and a representative image, called a class average image, is built for each group from its members. These class average images are assumed to capture the more frequently observed views of the structure. Over 25 years of research has gone into methods for creating high signal-to-noise class average images. These include multivariate statistical analysis (MSA; van), the 'alignment through classification' method (), Bayesian modeling with Gibbs sampling (), and maximum likelihood methods using single () and multiple references (). The higher signal-to-noise class average images can then be oriented relative to each other by the common lines method () or by angular reconstitution (van), and 3D structure determination can be performed using the Fourier Slice Theorem. The inference of a correct structure from the class averages is complicated by the existence of spurious models, which may explain the observed data. Experimental and analytical methods have been developed to make this inference more robust. In the Random Conical Tilt method (), the alignment problem is simplified by acquiring micrographs as tilt pairs. The tilt geometry is used to provide constraints, which make alignments more robust. Tomography () and the orthogonal tilt-method () similarly improve robustness through the use of multiple images taken at different tilts. In some data analysis methods (), an assumption of known symmetry is used to prevent incorrect models from being generated by the 3D reconstruction. However, these are, by their nature, restricted to symmetric systems where the symmetry is previously known. Recently, a new method was proposed for ab initio 3D reconstruction, which bypasses the use of class averages by using particle images directly (). This method performs 3D reconstruction by iteratively refining an initial model built from particles that were assigned random orientations. In this article, we present a probabilistic method for performing ab initio inference of a 3D model from class average images. Our method defines a prior probability distribution of orientations of particles in the micrographs, and a probability distribution of errors and attempts to fit model parameters that best explain the observed data. In practice, this probabilistic method works analogously to back-projection () in that it starts with a random model that is iteratively refined. However, it does so under a formal probabilistic model that allows for prior information to be incorporated in a principled manner. In addition, our method attempts to avoid local minima in the iterative procedure. These local minima can cause methods to produce non-optimal solutions. We address this point by using a deterministic annealing () procedure in the model fitting process. We assume higher variance in the data at the start of the model fitting process and then slowly reduce the assumed variance to move towards a correct, globally optimal model. This method is similar to that ofin that an observed image is modeled as a multivariate Gaussian with a mean described by the projection of the 3D model. However, these methods were meant for 2D refinement using a reference image (), alignment and 2D averaging with multiple reference images (), and refinement using multiple 3D reference models (), while our method is used for ab initio 3D structure inference. As these methods start with approximate solutions (e.g. initial 3D models), which are refined, these methods only need to find local solutions. However, ab initio inference needs to find a global solution, for which we use deterministic annealing. Our use of annealing to find a global minima is not unique in this problem space. Ogura () employed simulated annealing to perform 3D inference from class averages using a modified cross correlation score. Similarly,used genetic algorithms with a common-lines-based score to find global minima in the space of orientations of class average images. However, our approach is different from these approaches; they use annealing to find a single optimal orientation of images while we probabilistically average over (i.e. marginalize out) the orientation variables in our model. Our contribution, thus, lies in the application of annealing to find a good minima with a probabilistic model, the use of a Bayesian prior for the distribution of Coulomb density in space, and for incorporation of knowledge of orientations into the model fitting. We show the application of our method to determine 3D structures from class averages of five systemsan asymmetric phantom structure with projections in random orientations, a synthetic dataset with projections of Ribosome in random orientations, two asymmetric, experimentally observed structures (bovine mitochondrial ATP synthase and V-type ATPase) with projections along the equatorial axis and an experimentally observed symmetric structure (GroEL) with projections generally along the top and side views.
METHODSIn this section, we describe the model used, the method used to fit the model to the data, and the datasets.
Bayesian model for electron cryo-microscopy dataOur model has two main components: the parameterization used to represent Coulomb density in space and the probability model to calculate the likelihood of the observed data. These will now be discussed in turn.
Data modelWe assume that the 3D space over which we are trying to determine the Coulomb density is a cube of side N, centered at the origin, which is split into N 3 voxels by dividing each side (corresponding to x,y and z directions) into N equal length intervals. The Coulomb density at a vertex of arbitrary index, i, where 1  i  N 3 , is the parameter  i. Thus, the unknown parameters for the model are  =  1 , 2 ,..., N 3 . The Coulomb density (interchangeably referred to as intensity) at any point x,y,z in the cube is obtained by tri-linear interpolation over the vertices of the voxel the point lies in. That is,corresponds to the coordinates of the i-th vertex, and N (x,y,z) is the set of vertices of the voxel containing the point (x,y,z). It can be seen that the projection of this model onto a plane through the origin, described by a rotation matrix, R can be calculated asPage: 2408 24062415the Coulomb density and to compute the projection of a macromolecule. The density of a macromolecule is modeled over a 3D grid that is composed of N 3 voxels. The Coulomb density of a point in the interior of a grid is computed as a tri-linear interpolation over the densities at pixels on the vertices of the voxel it belongs to. The intensity at a point in a projection taken at rotation, R, is the line integral over the grid along the ray perpendicular to the plane of projection, defined by rotation matrix R. As a result, the projection of the model on a point can be computed as a weighted linear sum of contributions of pixels at the borders of voxels. The weights,
N.Jaitly et al., from a pixel, i, in the model, can be calculated by ray-tracing.where c i x,y;R is the contribution of vertex i to pixel x,y in the projection, P  ,R (note that the c i 's do not depend on the parameters  ) 1. The c i 's can be calculated by taking the line integral along the line parallel to the z-axis (defined by the rotation matrix, R) that passes through pixel x,y in the image (see).
Probability modelwhere  is assumed to be the SD of the noise. Marginalizing over rotation matrices, R, with prior probability distribution p(R), we can compute the probability of observing an image, I, asNote that in our model, we assume that all images are centered correctly and translations do not need to be marginalized out. In our experience, the centering of images performed by software packages used to create class average images alleviates the need to do so. However, this assumption can easily be changed and translation added into the model above. Then, given M images, the probability of observing them issince the probability of observing an image is independent of the probability of observing the other images given the model. We can now add priors, p  , for our parameters. Given images, I 1..M , the posterior probability for the parameters,  , isThe prior we use penalizes changes in intensity between neighbouring grid points. More formally, the probability of a set of values  =  1 , 2 ,.
.., N 3 iswhere  is a smoothing parameter that controls how strongly changes are penalized. This prior can encourages smooth densities when the observations are inconsistent and effectively interpolates the density when data is missing or ambiguous.
Model fittingStarting with an artificially high noise parameter, , a constant smoothing parameter  (described in the previous section), a constant L 1-regularization parameter, , and a random initial model, the model is fit to the data using a Quasi-Newton optimization ( and details of the optimization method are described in the next section). Subsequently, the noise is reduced deterministically, and the fit model is used as the initial seed in the next iteration of data fitting. The cycle of deterministic annealing is repeated as long as the model inferred at the end of a cycle is better than the model inferred at the end of the previous cycle. Seefor an overview of the process.
Quasi-Newton optimization with orthant-wise limited-memory QuasiNewtonThe model is fit by a limited memory Quasi-Newton optimization method called orthant-wise limited-memory Quasi-Newton (OWL-QN) method () using a C implementation available at http://www.chokkan.org/software/liblbfgs/. This method uses the gradient of a function to perform a Quasi-Newton minimization of the function under L 1 regularization. L 1 regularization is a standard penalty function in model fitting that attempts to shift the values of the parameters towards 0, and thus prevents them from taking spurious values in order to explain the data, effectively reducing the degrees of freedom. In this manner, only parameters that are strongly supported by the data are able to achieve large absolute values. The amount of penalty applied is controlled by the regularization parameter , with higher values of  causing a larger penalty, and hence a larger shift of values towards 0. In our model, we attempt to minimize the negative log posterior probability function of the parameters  with OWL-QN in order to find the model that generates the observed data with the highest probability.. Overview of our method for structure inference. The final structure is inferred by multiple cycles of deterministic annealing. In each cycle, the noise parameter, , is annealed from a high value to a target value of noise over multiple steps. In each of these steps, a MAP estimate of the parameters is found and used as the starting model for the subsequent step. In the first cycle of annealing, the target noise value is an intermediate value; in subsequent cycles, the target noise value is chosen from the difference between the data and the projections from the model inferred in the previous cycle.
Page: 2409 24062415
Bayesian method for structure inference in electron cryo-microscopyThe derivative of the log of the posterior probability, logp  |I 1..M , w.r.t.  i , required by OWL-QN can be calculated asThe individual terms in the sum in Equation (8) can be calculated using Equations (2) and (4)is the Gaussian normalization constant and N p the number of pixels in an image. The derivative of the prior p  w.r.t.  i is elementary and is not shown here. It can be seen that no obvious closed form solution exists for the likelihood [] and the gradients [] because of the difficulty in performing integration over rotation matrices, R. Hence, these must be calculated numerically by using an approximation to the integral over rotations. To perform this numerical approximation, we sample Q rotations, R 1..Q , from the distribution p R that represents our prior belief of how rotations are distributed for a dataset, and compute the averages over these rotations, i.e.Note that we choose to use a single set of rotations for all observed images. Without prior information about the orientations of the molecules, this distribution can be uniform. Alternatively, this sampling can also be tailored to reflect the actual distribution of orientations of a molecule, such as rotations only along a side/equatorial view or rotations that are distributed along the top and side/equatorial views. Such a biased sampling of rotations may occur for structures where mass is distributed more along one direction (the dominant axis) than others. We show examples of all these rotation schemes in our datasets below. In the above approximations, the most computationally demanding step is the calculation of the projection, P(R, ), of the model at a given rotation, R. However, because the same set of rotations are shared for each observed image each projection of the model needs to be calculated only once and can be used to calculate the individual terms for all the images, I 1..M inand (12). Thus, for each rotation matrix, the calculation is linear in the number of pixels per image and the number of images. The rotation matrices used for the approximations can be generated using prior beliefs. For a uniform distribution of rotations, we divide the unit sphere into equal sections using geodesic grids (), and then for each point on this sphere, we perform rotations around the new z-axis. Note that we did not create rotations by taking periodic values of ,  and  Eulerian angles because this leads to non-uniform sampling, with more rotation matrices being sampled on the poles of a viewing sphere. For the equatorial/side view distribution of rotations, we choose a grid of equally spaced  angles between 0 and 360, with  angle equal to 90 and  angle equal to 90 (the macromolecules were assumed to be oriented correctly in the plane of the image), under the ZYZ Euler angle convention. Similarly, for distributions consisting of top and equatorial/side views, we added rotations with equally spaced  angles between 0 and 360,  angles equal to 0, and  angles equal to 0 and 180 to the set of rotations generated for equatorial/side view distributions. Thus, by computing the derivatives of the approximation we are able to perform OWL-QN optimization of the negative likelihood function to obtain approximate MAP estimates for 's, for a given noise parameter, , prior parameter, , and L 1 regularization parameter, . The values of  and  were set at 10 4 and 10 4 , respectively, for all the datasets.
Deterministic annealingThe noise parameter is annealed exponentially from a high initial value of 20 down to a low final value of 0.3 over 20 steps in the first round of optimization. A value of 20 was assumed to be a reasonable starting point because it was much higher than the maximum intensity in any pixel in any of the images, which were scaled linearly such that the maximum intensity was a value of 1. Similarly, 0.3 was assumed to be a reasonably low noise value to anneal to in the first iteration. In subsequent rounds of optimization, the parameter is re-annealed from 20 down to a final value, which is the SD of noise calculated from the model fitting in the last round of optimization. The cycle of annealing
N.Jaitly et al.is stopped when the improvement in the log likelihood of the best model of an annealing cycle over the log likelihood of the best model of the previous cycle is less than a user specified value, (typically chosen to be 1 for our test cases). Seefor a flowchart representation of the process.
DATASETS AND RESULTS
Datasets
Phantom structureThe phantom structure used in this article was first described in Baker and Rubinstein (2008). It consists of a long cylindrical core with a peripheral helical coil that runs along the side of the cylindrical object, and a handle-like structure on one side. Fifty noiseless projections of size 3232 were created along random orientations, which were then used for inference. For this article, to reflect the uniform distribution of projections in space, the approximations to Equations (6) and (8) were calculated using uniformly distributed rotations as described earlier.
ATP synthaseThe ATP synthase dataset used here was described in. In that study, 5984 particles extracted from the experimental data were used to create 20 class average images with MSA (van). Of these, nine reasonable class average images were selected using the presence of mirror-image pairs as an indicator of the validity of a class average. We down-sampled these images from 128128 to 3232 for computational speed. The class average images all shared only a single common-line because the dominant orientations of ATP synthase represent views along the side (equatorial axis). As a result, the typical approach of angular reconstitution based on common-lines was not applicable, and an alternative approach based on registration points was used for structure determination. Thus, this is an interesting structure for any general ab initio inference method. For this dataset, to reflect the fact that the class average images were known to have been side/equatorial views, we computed the approximations to Equations (6) and (8) along rotations generated from an equatorial distribution as described in Section 2.2.1.). Of these, seven class average images that appeared qualitatively cleaner than the rest (on the basis of noise) were chosen to perform the ab initio inference. As with ATP synthase, we downsampled these images to 3232 to reduce the computational time required for inference of the 3D structure. The GroEL dataset is of interest to us because it represents a symmetric structure, but we do not explicitly use symmetry in the inference. To reflect our belief in the orthogonality of the two sets of views, we computed the approximations to Equations (6) and (8) along orientations that represented a mix of top and side views (without assigning any of the images to any particular view). The rotation matrices for these views were generated as described in Section 2.2.1.
GroEL
RibosomeSynthetic data for Ribosome was downloaded from the Xmipp website (). The dataset contained 20 000 images of size 130130. We used these images to create 50 class average images using Xmipp, as we had done for the GroEL particles. These were down-sampled to 3232. For the Ribosome dataset, no assumption was made of the orientations, similar to the phantom dataset above.
Bayesian method for structure inference in electron cryo-microscopy
V-type ATPaseThe V-type ATPase dataset used here was described in Lau and Rubinstein (2010). In that study 19 825 particles extracted from experimental data were used to create class averages with MSA (van). Of these, 10 class average images were selected using the presence of mirror-image pairs. Similar to the above datasets these class averages were down-sampled to 3232. We used the same rotation prior (equitorial views) that was used for ATP synthase.shows the inferred structures for the five test datasets at different points of the annealing cycles. Animations of the smoothed inferred structures and corresponding views of original structures can be seen in the .avi files in the Supplementary Material.
Results
Phantomdataset: a structure that was qualitatively comparable to the original phantom structure (on the basis of the presence of cylindrical stem, peripheral helix and handle) was obtained towards the last two iterations of the first cycle, see. The earlier iterations presented a spherical object with intensity decaying away from the center of the model. This corresponds to a model built with images assigned almost equal probability in all orientations. It is possible that the annealing could have been performed in fewer steps as most of the intermediate steps changed the structure qualitatively little.
ATP synthase A structurethat had the main qualitative features of a central and a peripheral stalk of ATP synthase was inferred by the 20th iteration of the first annealing cycle, as shown in. Earlier iterations showed a model that appeared rotationally symmetric around the main axis as a result of the assignment of equal probabilities to all equatorial views. Further refinement of the model was achieved in the subsequent annealing cycles. A detailed comparison ofand 4 reveals that the model inferred by our approach has the opposite handedness compared to the model from.
GroELGroEL also produced a qualitatively accurate model by the end of the first cycle of annealing, which can be seen in. In addition, it took four annealing cycles to converge to the final model. The final model has the important characteristics of GroEL, including a central channel and an approximate 7-fold symmetry. The 7-fold symmetry is more obvious from the side views than it is from the top view. (See Supplementary Material animation file GroELTopAndSide.avi). Interestingly, some of the intermediate models for GroEL exhibit better symmetry, probably indicating some overfitting in the later iterations and cycles (compare iterations 10 and 15 against iteration 20 in). The algorithm's discovery of the 7-fold symmetry arises without its explicit specification in the inference.
RibosomeRibosome produced a qualitatively accurate model by the end of the first cycle of annealing. However, in the subsequent cycles, the model looks less similar to the reference model, and the measured resolution falls (possible explanations are discussed in Section 4).As was seen with ATP synthase, earlier iterations produced more cylindrically symmetric models, while the later iterations resulted in a model similar to that seen in Lau and Rubinstein (2010).
V-type ATPase:
ATP synthase and GroEL under no assumptions of rotationsStructure determination was also performed for ATP synthase and GroEL by sampling rotations from a uniform distribution rather from than their known distributions. Structures inferred with this method can be seen as animated structures in the Supplementary Material (see files ATPUniform.avi and GroELUniform.avi). It can be seen that GroEL produces a reasonable structure while the ATP reconstruction lacks some detail. This is probably because only 9 class averages were available for ATP synthase, while 50 class averages were used for GroEL.
Assessment of resolutionWe attempted to measure the resolution of the models through the use of the Fourier Shell Correlation (FSC) criterion (). The typical approach of dividing the available set of particle images into two sets was not applicable here because we had only a small set of class average images. Instead, we used Page: 2412 24062415
N.Jaitly et al.the models available from previous reconstructions (or the known models) as reference models, and computed the FSC curves between our inferred models and these reference models. This method is appropriate, because we expect our algorithm to generate lowresolution models compared to the high resolutions these models were reconstructed at. At FSC of 0.3, resolutions of 48.9, 40.7, 41.8, 96.1 and 50.6  were calculated for the phantom structure, ATP synthase (under a prior of equitorial views), GroEL (under a prior of top and side views), ribosome and V-type ATPase, respectively (FSC curves can be found in the Supplementary Material). Here, we assumed a pixel size of 11.2 to account for the down-sampling from particle sizes of 2.8 . The model inferred after the first cycle of annealing with ribosome was significantly better (43.8 ) than the model inferred after the last cycle of annealing with the ribosome. The animated file provided for ribosome shows the model inferred after the first cycle. Possible reasons for the low quality of model inferred for ribosome in the final cycle are suggested in the Section 4.
Run timeExperiments were performed on a Linux Ubuntu 9.04 64-bit OS with a 2.66 GHz Intel Core  i7 processor. Threedimensional inference for ATP synthase, GroEL and V-type ATPase under the assumed equitorial and top view priors took 2 h for four to six cycles of annealing. Inference for the phantom dataset, ribosome, and GroEL under no assumptions of rotations, with 50 class averages each, took approximately a week of computation for four to six cycles of annealing (with each cycle taking one to two days). ATP synthase without any assumptions of rotations and nine class averages took 6 days.
DISCUSSIONWe have shown above that our method performs reasonably well in discovering 3D models from class average images for five different systems. Here, we give insight into our motivations for the choices made in different components of the algorithm and provide some guidance on how ab initio models may be created for novel systems using our method.
Data and likelihood modelThe first design decision was the choice of the representation used to express an arbitrary structure. We are attempting to create a representation for a structure with continuous Coulomb density using a finite number of parameters. The resulting representation needs to possess some intuitive properties, such as gradual transition of Coulomb density from one vertex to another in a voxel. Our use of tri-linear interpolation permits us to represent the continuous Coulomb density of the protein structure using a finite number of parameters. In addition, the Coulomb density transitions smoothly between vertices within a voxel (although the derivative of the intensity at the edge of a voxel may be discontinuous) and the projection in any direction takes into account the length of the ray (or amount of matter) passing through a voxel in a given orientation. In contrast,uses blobs that are a specific generalization of Kaiser-Bessel functions (), centered at the edges of the voxels. These basis functions are spherically symmetric functions with limited support, and have the advantage that their projections can be computed easily and rapidly. One possible disadvantage that blobs may have is that in a representation with blobs centered on voxel edges, densities can arise which have values in the middle of the voxel that are lower or higher than values at any of the edges; tri-linear interpolation on voxels does not suffer from this property. A formal comparison of a pixel-and a blob-based representation suggests that a pixel-based representation may offer better resolution (). However, in practice, blobs perform as well as pixel-based representations, at much faster speeds, as shown in. In future work, we will explore that advantage of using this alternative representation. Our tri-linear model also allows for easy transition to higher resolution models using a previously fit, low-resolution model. For example, if a model was fit using a 323232 grid, it could be used as a seed for a higher resolution model with a 128128128 grid. This is done by re-sampling the 323232 grid using the tri-linear interpolation on internal points. One theoretical disadvantage of our representation is that the derivative of the density is discontinuous along the edges of voxels. It is unclear if other interpolation methods (e.g. tri-quadratic) would give rise to significantly different results.
Prior distribution and regularizationThe other main component of our model is the prior probability distribution representing our belief about the likelihood of different densities. The prior probability distribution is important because the space of possible 3D densities is generally going to be underconstrained given the relatively small number of observations (i.e. class averages) available for a typical dataset. For example, with the ATP synthase dataset we had nine class average images with 1024 pixels each, while we had approximately 32 000 unknown parameters. To handle this sparsity of information, the prior captures our beliefs about the plausible distributions of the Coulomb density, providing additional constraints in the model fitting. Further, any choice of prior must not be so restrictive that valid models are penalized. A suitable assumption in modeling a physical structure is that the amount of change in Coulomb density from one voxel to another is low over the complete set of voxels. The prior we chose is based on this assumption. Our value of  = 10 4 performed well for all five systems, leading to solutions that retain important features without excess noise. Increasing  causes the model to have a lower resolution, as high-frequency changes in intensity of voxels are removed, while decreasing it causes the final model to have more spurious changes of intensity from one voxel to the next. It may be that different structures require different values of  for correct inference. A value of  that would be appropriate for a large set of structures may be chosen by an analysis of the structures available in the EMDB (http://www.ebi.ac.uk/pdbe/emdb/). Typically, spurious models fit with only a few class averages will have a large number of voxels assigned small density values. This is because such models can satisfy the constraints that are enforced by the observed pixels of the few class averages, without many significant violations. Conversely, such models rarely have very large values because they would lead to projections that significantly violate some constraints. Thus, we also enforce the assumption that voxels are effectively zero unless there is strong evidence to the contrary, thereby limiting the degrees of freedom of the model.
Page: 2413 24062415
Bayesian method for structure inference in electron cryo-microscopyWhile we do not explicitly model this as a prior, it is taken into account in the fitting process with the use of an L 1 regularization. Our chosen value of  = 10 4 (see Section 2.2.1) seems to perform reasonably for all five systems. Higher values of  cause solutions to be sparse and also affect resolution. With more images, a higher value of  may be used because the data is able to overpower the prior. It must be pointed out here that the ability to use the same parameters across all datasets relies strongly on our normalization procedure, which normalizes class average images by scaling the maximum intensity over all images down (or up) to 1.
Model fittingPage: 2414 24062415
N.Jaitly et al.regularization that favors solutions with large number of zero valued voxels. Thus, our reported resolutions should only be interpreted as crude approximations. Our model does, however, allow for a comparison between the accuracy of different models on the basis of the posterior probability of a model, which reflects the consistency between the experimental data and the model. Given two inferred structures, it is possible to state which one is better on the basis of higher posterior probability. However, this approach requires that the assumptions made by the model are valid. For example, we referred to the need to recreate class average images for ATP synthase to ensure that each of the class average images had the same noise SD. As such, proper model comparison may only be possible when our method creates models from raw particles where these assumptions are more likely to be valid. We conclude by pointing out that the model we have defined permits sophisticated modifications in the future, by the virtue of its probabilistic nature. Such modifications include modern sampling techniques that allow for better sampling of the space of orientations as the model fitting proceeds, and annealed sampling procedures that explore modes more efficiently than our deterministic annealing technique. These techniques could additionally be applied to raw particles rather than class averages. Moreover, with an improved instrument noise model, it should be possible to compare 3D structures inferred from different methods just on the basis of the likelihoods. Lastly, even though we did not enforce symmetry for any of the structures in this article, our probabilistic method does allow us to incorporate prior knowledge of symmetry through the use of a prior that is symmetric. We have released the source code of our program at http:// compbio.cs.toronto.edu/cryoem/ to allow others to apply this software to their own data. A parallel implementation is being developed to apply the method to raw particle images.
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
We follow the convention that matrices and vectors are in bold font e.g.,P  ,R , while their individual elements e.g.P x,y;,R are not in bold font.
The use of a OWL-QN for fitting data presents several advantages. It is well known that Quasi-Newton methods are able to, at least partially, account for the curvature in parameter space by using lowrank approximations to the Hessian of the parameter space (Nocedal and Wright, 2006). This leads to faster optimization as compared to a simple approach such as gradient descent. The use of linesearches in these procedures also obviates the need for specifying parameters, such as learning rate, which additionally complicate any fitting procedure that depends on gradient descent. The additional advantage of using OWL-QN over other Quasi-Newton procedures, such as L-BFGS, is that it specifically incorporates L 1-regularization in the optimization process. Probably, the most important component for the success of this method is the use of deterministic annealing of the noise parameter, . It was seen in our experiments that starting with a random model and a value of  close to the real value gave arbitrary, incorrect models. This is because small values of  created many steep, local optima in the objective function, causing the model to remain close to its initial value. In this scenario, the parameters changed very little from the starting parameters, getting stuck in a local minima which were poor in explaining the data. In contrast, a high values of  results in a smoother objective function and fewer local minima, allowing the space of models to be more easily explored. As the  is reduced, more and more features of the real object emerge, and correct orientations contribute more to the model. An exponentially decaying annealing schedule allows us to reach an average structure quickly and spend more time in the refinement stages with lower . In the first cycle of annealing, we anneal the noise estimate to an intermediate value, since an accurate estimate of the noise is not available at the start of this cycle (a value of 0.3 worked reasonably for all five of our test cases). We then compute the root mean square deviation between the projections of the models and the images at the end of the cycle and use it as the target noise SD to anneal down to in the subsequent cycle. As a result, by the end of our training we are annealing to the expected, true noise value. Other than allowing us to recover from local minima, the repeated cycles of annealing are, thus, also useful in annealing to a correct value of noise. The above method of estimation of final noise is a local estimate, because of the inter-dependence between the final model inferred and the value of noise used in the inference. When the number of class averages is low, and a low value of variance is used, overfitting can result, which reinforces an even lower variance in the next iteration or cycle. As a result, it was seen that the final GroEL model inferred in the last cycle of annealing had less obvious 7-fold symmetry, than some of the models in the intermediate annealing cycles. Similarly, the ribosome structure inferred at the end of the first cycle is more accurate than that inferred at the end of the second and subsequent cycles.
