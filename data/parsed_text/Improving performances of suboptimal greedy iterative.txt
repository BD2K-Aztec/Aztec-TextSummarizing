Motivation: Biclustering gene expression data is the problem of extracting submatrices of genes and conditions exhibiting significant correlation across both the rows and the columns of a data matrix of expression values. Even the simplest versions of the problem are computationally hard. Most of the proposed solutions therefore employ greedy iterative heuristics that locally optimize a suitably assigned scoring function. Methods: We provide a fast and simple pre-processing algorithm called localization that reorders the rows and columns of the input data matrix in such a way as to group correlated entries in small local neighborhoods within the matrix. The proposed localization algorithm takes its roots from effective use of graph-theoretical methods applied to problems exhibiting a similar structure to that of biclustering. In order to evaluate the effectivenesss of the localization pre-processing algorithm, we focus on three representative greedy iterative heuristic methods. We show how the localization pre-processing can be incorporated into each representative algorithm to improve biclustering performance. Furthermore, we propose a simple biclustering algorithm, Random Extraction After Localization (REAL) that randomly extracts submatrices from the localization pre-processed data matrix, eliminates those with low similarity scores, and provides the rest as correlated structures representing biclusters. Results: We compare the proposed localization pre-processing with another pre-processing alternative, non-negative matrix factorization. We show that our fast and simple localization procedure provides similar or even better results than the computationally heavy matrix factorization pre-processing with regards to H-value tests. We next demonstrate that the performances of the three representative greedy iterative heuristic methods improve with localization pre-processing when biological correlations in the form of functional enrichment and PPI verification constitute the main performance criteria. The fact that the random extraction method based on localization REAL performs better than the representative greedy heuristic methods under same criteria also confirms the effectiveness of the suggested pre-processing method. Availability: Supplementary material including code implementations in LEDA C++ library, experimental data, and the results are available at
INTRODUCTIONClustering refers to the process of organizing a set of input vectors into clusters based on similarity specified according to some predefined distance measure. In many cases, it is more desirable to simultaneously cluster the dimensions as well as the vectors themselves. This special instance of clustering, referred to as biclustering, was introduced by Hartigan (). Although traditional one-way clustering provides valuable information with regards to a global perspective, extracting local substructures via biclustering may help build intuition on both dimensions of the data. In addition to the areas such as data mining and pattern recognition, biclustering has found many applications in bioinformatics, specifically in microarray analysis, drug activity analysis and motif detection (). In gene expression analysis, data are assumed to be arranged in a matrix, where each gene corresponds to a row and each condition to a column. After reordering rows and columns of the matrix, a bicluster can then be defined as a submatrix with significant correlation among its data values. Such a submatrix is likely to group together genes that exhibit similar behavior over a subset of experimental conditions. One of the early approaches for biclustering expression data is that of Cheng and Church (). A meansquared residue score is defined and the algorithm greedily inserts/removes rows and columns to arrive at a certain number of biclusters achieving some predefined residue score. Order preserving submatrix () is another greedy, iterative algorithm that finds a statistically significant bicluster at each iteration. Maximum similarity biclusters (MSB;) starts by constructing a similarity matrix based on a reference gene. A greedy strategy of removing rows/columns iteratively is employed to provide the maximum similarity bicluster in polynomial time. Large average submatrices (LAS;) is a recently proposed algorithm which tries to extract large average submatrices according to a Bonferroni-based significance score. Several graph-theoretical approaches have also been suggested.provided a divide-and-conquer algorithm, Bimax (Prelic), that runs on discretized binary data. In SAMBA (), the data matrix is viewed as a bipartite graph where the genes/conditions constitute the layers of the bipartite graph and edges in the graph correspond to thePage: 2595 25942600
Improving biclustering heuristics via localizationexpression changes. The goal is to find out heavy bicliques inside the graph. A similar model is constructed in () where crossing minimization in unit-weight bipartite graphs is used as a means to extract bicliques corresponding to biclusters in the data matrix. The model is generalized to weighted bipartite graphs in (). Many other biclustering algorithms including xMOTIFs (), ISA (), coupled two-way clustering () have also been suggested; see the survey of Madeira et al. for further details of various other biclustering methods (). Once a model is determined, be it a matrix of real values corresponding to relative abundance of mRNA, a discretized binary matrix or a bipartite graph model of the data matrix, the problem becomes that of globally optimizing a suitable scoring function defined under the terms including residue (), similarity (), order-preserving submatrix (), maximal biclique (). NP-hardness of even the simplest versions of all these seemingly similar optimization problems makes the task quite challenging (). An approach common to most of the existing algorithms, therefore, is to apply a greedy iterative heuristic that aims at locally improving the suitable scoring function under the defined model. Following this observation and the common observation that a 'good' initial configuration is especially important in the success of greedy iterative heuristics for solving optimization problems (), we present a pre-processing method called localization that provides an appropriate initial configuration by placing rows/columns exhibiting similar patterns in nearby locations within the data matrix. Although the majority of the proposed solutions for biclustering consists of iterative, greedy local optimization heuristics, no suitable pre-processing algorithm has been suggested for an improvement of these methods. We extract only one such method, non-singular non-negative matrix factorization (nsNMF) proposed by. Though not presented originally as a pre-processing method per se, but rather as a biclustering method in itself, we employ nsNMF for our comparisons as a possible alternative pre-processing method. This is plausible since reordering rows and columns so as to gather similar entities in close proximity is also a common goal of nsNMF. We show experimentally that our proposed pre-processing method of localization provides similar or even better results than the computationally heavy nsNMF. In addition, we show that when pre-processed with our localization method, performances of the greedy iterative biclustering heuristics improve. The algorithms under consideration are SAMBA (), MSB () and LAS (). Furthermore, we provide a biclustering method that simply uses localization followed by a random extraction of submatrices from the provided initial configuration. We show that this method when applied on real data outperforms the tested greedy heuristics according to certain evaluation criteria.
METHODS AND ALGORITHMSGiven an input data matrix M where rows correspond to genes, columns correspond to conditions, and each data entry is a real value corresponding to the expression change of the gene under the specified condition, the idea behind localization is to first group together rows and columns of M in suchSort P c using ordering defined by. end for /* Fix row set R, order column set C */ Switch R and C, and repeat the above procedure. until no change in row/column ordering or enough iterations a way that correlated rows/columns are 'localized', that is rows/columns exhibiting similar patterns are placed in nearby locations within M. In order to formalize this notion, first a decision has to be made with regards to where in matrix M to collect entries with similar patterns. Gathering such patterns around the main diagonal of M seems like a natural choice. Let a i,j denote the real valued data entry at row i and column j of M. Formally, the goal of localization is to reorder the rows and columns of M in such a way as to minimize i,j a i,j |ij|. We note that such an optimization in 01 matrices corresponds to row/column permutations that bring non-zero entries as close to the diagonal as possible. This optimization problem has received considerable attention from a wide range of application areas including graph drawing, VLSI layout and numerical analysis under different names such as bipartite linear arrangement, minimum-1-sum, bandwidth sum, edge sum, wirelength minimization; see;for nice surveys on the topic.
Algorithm 1 Localization
The localization algorithmSince the optimization goal determined for localization is computationally hard in general, it is of interest to establish connections with other wellstudied layout problems. In, it has been theoretically shown that the bipartite linear arrangement and the problem of minimizing crossings in a bipartite drawing is closely related and that the relation is approximation preserving: given an f (.) approximation for one of the problems, where f is some function on the input size, it also provides an approximation for the other with the same ratio. This connection has also been verified in practice through various experimental evaluations (). Based on these results we present our localization method that aims at minimizing:Note that considering the partitions of a bipartite graph as rows and columns, and the weights of the edges as the real valued entries in the data matrix, minimizing weighted edge crossings in the bipartite graph is equivalent to minimizing Equation (1) in a data matrix. A pseudocode of the localization method is provided in Algorithm 1. Assuming a fixed arrangement of columns, first the rows are reordered so as to reduce the sum in Equation (1). Then the rows are kept fixed and the columns are reordered using the same procedure. This is continued until no further improvements can be made or a predetermined number of iterations is reached. For rearranging one of the dimensions while
C.Erten and M.Szdinler(a) ( b)fixing the other, we adopt our method presented inwhich has been suggested in a different context with a similar optimization goal. For completeness, we provide the proof of the following lemma in the Supplementary Material.With a straightforward implementation, the running time of each iteration of the repeat loop is O(|R||C|+|R|log|R|+|C|log|C|), where |R|,|C| denote the sizes of the rows and the columns of the data matrix respectively. The loop is iterated until the minimization of Equation (1) converges or a constant number of iterations is achieved. We note that in all the experiments conducted, the method converges after a small constant number of iterations. In most practical settings |C|=O(|R|), that is the number of conditions is usually much smaller than the number of genes and |C|=(log|R|). Therefore, under these settings, the running time of the proposed method is linear in terms of the input size. Heatmap visualizations of the Yeast expression data both before and after localization are provided in. Theshows the original gene expression data matrix where the data values are assigned colors for visualization purposes. Theshows the gene expression matrix after the localization method is applied. The right block gathering homogeneous colored subblocks together as compared to the left block is a visual clue that the localization method does 'localize' the matrix by grouping together the submatrices with correlated gene expression data values.
Improving biclustering heuristics via localizationThe most natural heuristic candidates to apply localization as a preprocessing step for further improvement are those based on iteratively improvin some locally optimum solution in a greedy fashion. We identify three such algorithms, briefly describe each, and comment on possible modifications necessary for pre-processing with our localization method. The SAMBA () algorithm identifies biclusters using an exhaustive enumeration method. First, a bipartite graph model of the gene expresssion matrix is created. The set of genes and the set of conditions correspond to the two partitions and there exists an edge between a gene condition pair if the expression level of the gene changes significantly under the given condition. Weights are assigned to the edges and non-edges in the bipartite graph in a way that the weight of a subgraph corresponds to its statistical significance. Once the weights are assigned the problem then turns into that of extracting maximum weight complete subgraphs. The subgraph construction phase is followed by an iterative local improvement procedure of growing/shrinking the composed subgraphs. Localization pre-processing becomes affective at this phase by localizing the vertices corresponding to coexpressed genes and thus providing a better chance for local improvement at each iteration. MSBs () are yet another algorithm we envision improvement via localization pre-processing. In the original description of the algorithm, first a similarity matrix for a given reference gene is constructed. Each entry in the similarity matrix descibes how similar the expression value of a gene under a certain condition is to the expression value of the reference gene under the same condition. It is shown that a greedy iterative strategy of removing the least similar row or column from the similarity matrix yields the MSB. To generalize this approach for the extraction of all the biclusters in the expression matrix a subset of reference genes are needed. This is achieved via randomly selecting a subset of genes and designating them as reference. The new algorithm Randomized Maximum Similarity Biclusters (RMSBs) is the resulting algorithm. As each output bicluster is computed based on the given reference gene it is important to carefully construct the set of references. We extend this biclustering algorithm by pre-processing the data matrix with localization and selecting a reference gene subset by considering genes at constant intervals gene i ,gene i+ ,gene i+2 ,...gene i+k. It should be intuitive that selecting a gene from each local neighborhood in the localized matrix would yield better references than selecting them in random. LAS proposed byis yet another recent algorithm for which we utilize the localization pre-processing for further improvement. Assuming a Gaussian null model for the data, the significance score of a submatrix is defined using a Bonferroni-corrected P-value based on the size and the average of the data values in the submatrix. The main goal then is defined as that of extracting the submatrix with maximum score. As with the previous algorithms this optimization goal is computationally hard to achieve. Therefore, rather than solving it to the optimum a greedy iterative heuristic is proposed for the search procedure. The heuristic starts out with a random initial submatrix. Fixing alternatively the column (row) set, the set of rows (columns) locally optimizing the significance score is searched iteratively until convergence. Pre-processing the LAS algorithm with localization and modifying the greedy search heuristic to start the iterations with a submatrix consisting of rows and columns within a local neighborhood rather than a random submatrix yields an improvement in the local search results.
Random extraction after localizationIn order to demonstrate the power of the localization procedure, we provide a simple algorithm that takes as input the localized matrix M L , extracts local submatrices randomly from M L , evaluates the significance (statistical and/or biological) score of each submatrix, and finally reports those with high scores. Given minimum and maximum row counts, and a row increment amount respectively denoted with min r , max r , inc r the set of possible row sizes are min r +k inc r , for all non-negative integers k where k inc r  max r. The set of possible column sizes is defined analogously. The dimension of each extracted local submatrix is a member of the cartesian product of these two sets. For each possible dimension we extract a pre-specified number of submatrices randomly and eliminate those with low significance. The rows (columns) of the submatrix are all consecutive in M L. We evaluate the significance of each extracted submatrix with two types of scorings. First is the H-value test (). Let the set of rows and columns of the extracted submatrix be denoted respectively with I, J. The residue R of the entry (i,j) iswhere a iJ is the mean of row i, a Ij is the mean of column j and a IJ is the mean of the submatrix. H-value of the submatrix is then defined as,The submatrices with H-value lower than a given threshold  are subject to a second test when biological validation data in the form of GO associations
Improving biclustering heuristics via localizationthrough sources such as FuncAssociate () is avaliable. Given such associations it is easy to determine the ratio of the number of genes associated with some category to the total number of genes in the extracted submatrix. If the highest ratio of the submatrix is lower than a threshold the submatrix is eliminated, otherwise it is reported as a bicluster with high significance. In what follows random extraction after localization is referred to as REAL if only H-value scoring is applied. When biological significance is also evaluated with available GO associations we refer to the algorithm as REAL GO .
DISCUSSION OF RESULTSThe localization algorithm and the bicluster extraction algorithm REAL are implemented in C++ using the LEDA Library (). The codes for the algorithm implementation and experimentation are available as part of the Supplementary Materials. The implementations of the algorithms subject to improvement via localization, SAMBA, MSB and LAS are from the cited sources of the algorithms. Each of these algorithms are evaluated both with and without localization pre-processing. In the former case, the algorithms are denoted with SAMBA * , MSB * and LAS * . We experiment on two real datasets, Saccharomyces cerevisiae (Yeast) from) and A.thaliana from. We note that our localization algorithm requires non-negative real values in the input matrix. To achieve this, we apply a normalization procedure on the input expression matrix by first adding the minimum value to each entry and then taking the logarithm of the data entries in the resulting matrix. The logarithm transformation is a standard transformaton in microarray data analysis to achieve distributions that are closer to normal distributions (). We emphasize that to have a fair comparison, right after localization we preserve the original values of the expression matrix before we apply the algorithms SAMBA, MSB and LAS, though localization may have changed the order of the rows and the columns.
Structural and statistical evaluationWe first evaluate the structural and statistical significance of biclusters output by each algorithm REAL, REAL GO , MSB, SAMBA, LAS and the localization pre-processed counterparts of the last three. The parameter settings of each of these algorithms and those of their localization pre-processed counterparts are the same as the default settings suggested in the original descriptions of the methods. The results of our evaluations applied on both A.thaliana and the Yeast gene expression datasets are presented in. We apply REAL on the Thaliana dataset and REAL GO on the Yeast dataset. The parameter  is set to 75 for REAL and to 0.12 for REAL GO. The parameter settings for each algorithm are the same in this and the following experimental subsections. In addition to structural information including maximum and minimum bicluster sizes, average dimensions and the bicluster count we also provide evaluations based on statistical measures of similarity. Candidate measures commonly used in literature are H-value (), Hv-value (), average correlation value (ACV) (). All these measures have similar definitions and provide similar results in our evaluations, and we only provide the results of H-value evaluations.At the A.thaliana 734x69 major row, the best performers in terms of H-values are the algorithms REAL and MSB * . MSB * interestingly provides several constant-valued biclusters as its Hvalue average is 0. Localization pre-processing helps MSB provide more constant biclusters than the original algorithm. Comparing SAMBA and SAMBA * , the localization pre-processed SAMBA * provides more homogeneous biclusters in terms of H-value. LAS and LAS * are among the worst ones. This is because of the high variance of the dataset. Also, due to the scoring scheme of the LAS algorithm both the original algorithm and the localization preprocessed modification provide relatively small biclusters. When we compare the original and localized runs of the algorithms on the Yeast 2993x173 dataset, we note a slight increase in the H-values of MSB * and SAMBA * as compared to their unlocalized counterparts. However the localization pre-processed versions of the algorithms provide a larger number of biclusters in each case which maybe the cause of this discrepancy.
Comparison with localization alternativesWe compare the performance of our localization algorithm with another potential pre-processing method, the non-smooth nonNegative Matrix Factorization (nsNMF) proposed by. This method is not originally proposed as a preprocessing method per se, but rather as a full-fledged biclustering method. However since the main idea involves reordering the rows and the columns of the data matrix so as to gather correlated structures in close proximity, it serves as a potential benchmark 1 method for pre-processing. In nsNMF, the input data matrix M is first converted to a product of two matrices W and H with sizes |R|f and f |C|, respectively. Each of the f columns of W is called a factor andconstitutes a basis experiment. Each row of H constitutes a basis gene. Given a certain factor, that is a column of W , the rows of M can be sorted based on it. Similarly, the columns of M are sorted according to the corresponding row, that is the basis gene of H. It is assumed that each such sorting of the original data matrix provides correlated genes/conditions localized in a portion of the matrix. For our evaluations we assume the parameter settings employed in the original paper. For nsNMF we produce results for four factors. Therefore we evaluate six matrices in total:
C.Erten and M.SzdinlerThe last four correspond to the results of nsNMF with basis factorizations shown as the corresponding subscripts. We present 3D plots inand b which summarize the results of our experiments performed on the Yeast expression data () and the A.thaliana expression data () respectively. We use the H-value test described in Equation 3 to compare the localization performances of the six alternative matrices. Each point on the x-axis corresponds to a specific gene size g s where g s {10,20,30,...,100} for the Yeast expression data experiment and g s {5,10,15,...,50} for the A.thaliana expression data experiment. Each point on the y-axis corresponds to a specific condition size c s where c s  {5,10,15,...,50} for the Yeast data and c s {3,6,9,...,30} for the Thaliana data. For each pair (g s ,c s ), we randomly pick a submatrix with g s consecutive rows and c s consecutive columns from each of the six matrices starting at exactly the same indices and compute an H-value of the submatrix. The random choices are repeated 1000 times, an average H-value is computed for each alternative and is plotted at the z-axis. Analyzing the results of our experiments on the Yeast dataset, H-value test seems to favor M L , the matrix pre-processed with our Localization procedure over M,M 1 ,M 2 , the original input matrix, and the resulting matrices of nsNMF with basis factorizations 1 and 2, respectively. We note that low H-value implies more correlation between the entries of a given submatrix. The factorizations with basis 3 and 4 seem to provide H-values slightly better than that of localization. With regards to the A.thaliana dataset experiments H-value results of localization is better than the rest in almost all the cases, except for the case where g s and c s are small. In this exceptional case, the H-value test is not stable due to the large variance in the original dataset. As alternative statistical significance measures the six matrices are also evaluated based on variances and the average pearson correlation coefficient (PCC) ratios. Such an evaluation reveals the intuition behind the nsNMF method and its contrast with the localization. Detailed 3D plots and a discussion of these results emphasizing the main conceptual differences between nsNMF and localization can be found in the Supplementary Material. Although they do not provide an explicit running time, the algorithm of Carmonaseems to require (|R| 2  |C|f I conv ) time, where I conv is the number of iterations necessary for convergence to local optimum. In practice, the constant in the suggested bound is large and the running time is much worse. The fact that the greedy iterative biclustering methods themselves require large amounts of computing time, necessitates a pre-processing algorithm for improvement of such methods take much less proportion of the necessary CPU time. This makes nsNMF a much less appealing pre-processing alternative. Our localization algorithm provides better or comparably similar results to those of nsNMF with only an almost linear running time.
Functional enrichment evaluationWe next evaluate the biclustering results of the algorithms under study based on biological significance. Based on gene functionality a gene-to-category association is created for the Yeast dataset in von. Using these associations, we are able to evaluate an enrichment ratio for each bicluster. Categories of genes are identified in a manner similar to. There are 13 pre-identified categories. Given a bicluster the ratio of the number of genes specified in a category to the number of genes in the bicluster provides a possible enrichment value for that category. For a specific category, we choose the highest enrichment value among all biclusters as the enrichment ratio of the category. This is a ratio between 0 and 1, see. For this experiment, we do not evaluate small biclusters that contain less than 20 genes. We first compare the results of each greedy iterative heuristic method with those of its localization pre-processed counterpart. MSB * provides better enrichment ratio than MSB in 10 categories. MSB has better ratio than MSB * in only one category and they have a tie in two of the categories. These are the expected results as the MSB algorithm greedily selects the maximum similarity bicluster based on a random reference gene, whereas with the localization pre-processing each reference gene is selected from a representative local neighborhood that already satisfies a certain degree of similarity. The contrast is also apparent in the comparison Page: 2599 25942600of SAMBA with its localized counterpart SAMBA * . The localization pre-processed version provides a better functional enrichment ratio than the original algorithm in seven categories, whereas the original algorithm beats the modified version in five categories. There is a draw in one category. A similar contrast results when comparing LAS versus LAS * . As a result as far as the functional enrichment is concerned, the localization pre-processed counterparts of all three greedy iterative biclustering methods provide more significant biclusters than the original methods themselves. We note that with this measure the best performer among all the algorithms, localization pre-processed or not, is the simple random extraction algorithm applied after localization, that is REAL GO. It provides better enrichment ratio than the rest of the six algorithms in eight categories. The category of Transport and Sensing is especially notable as REAL GO provides a bicluster with all the genes associated with that category giving rise to a functional enrichment ratio of exactly one for that category. This is a major indication of success especially given that the average number of genes in a bicluster produced by REAL GO is almost 24, see. Both the localization pre-processing and the GO association validation of the randomly extracted structures contribute to this achievement. We note that the results presented inare verified when the enrichment ratios are calculated based on Bonferroni-corrected P-values as well. Details of this evaluation can be found in the Supplementary Material.
Improving biclustering heuristics via localization
Protein protein interactions evaluationThere is usually a natural interconnection between the information contained within expression data and protein-protein interaction (PPI) networks (). The validity of high-level structures such as biclusters is usually cross-checked via low-level biological data in the form of protein interactions (). Conversely, to improve the accuracy and the coverage of predicted protein interactions it is usually necessary to integratecomplementary data sources including gene expression (). Therefore, our next evaluation is based on validating extracted biclusters with known proteinprotein interactions from literature. We make use of two separate sources of PPI network data for the yeast, one from Ben-Hur and Noble (2005) with 23 702 interacting gene pairs and the other fromwith 11 833 interacting gene pairs. As for the A.thaliana dataset evaluations we use the PPI network with 24 418 interactions and 11 706 genes from De. For the rest of the evaluations, these data sources are denoted with PPI 1 , PPI 2 , PPI 3 , respectively. For each bicluster extracted using a specified algorithm, we define the hit ratio of the bicluster as the ratio of the number of interactions among the gene pairs in the bicluster to the square of the total number of genes in the bicluster. Extracting the subnetwork of genes of the bicluster from an appropriate PPI network, the hit ratio in other words is a measure of the density of this subnetwork. Therefore, given an algorithm and a PPI network source it is easy to compute the hit ratios of all the biclusters produced by the algorithm. We compute an average  of the hit ratios per algorithm for our evaluations, see. As far as the proteinprotein interactions of the given source are concerned, the higher the average hit ratio, the more correlated the gene groups in output biclusters are. With regards to PPI 1 network data localization pre-processing improves the results of the MSB and the SAMBA algorithms, but fails to improve those of LAS. An increase in the average hit ratio of the produced biclusters is assumed to be an improvement in this case. Localization pre-processing is verified even more strongly in the PPI 2 network data, as all the modified versions of the algorithms provide better hit ratios than their original counterparts. We note that going from PPI 1 to PPI 2 the average hit ratios decrease as the former is more dense than the latter. As for the PPI 3 network data as the network is the sparsest among all three networks the average hit ratios are all quite low. However, it is still easy to verify the improvement created by localization pre-processing. Overall, three network evaluations MSB * and the REAL GO algorithms are among the best performers as far as the average hit ratios of the produced biclusters are of concern.
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
Another feasible alternative would be running the algorithms under consideration on several randomly permuted instances of the input and picking the 'best' output instance rather than employing a pre-processing of some sort. We show this alternative does not improve the biclustering results by running each algorithm on 50 randomly ordered instances of the S.cerevisiae dataset. Details can be found in the Supplementary Material.
