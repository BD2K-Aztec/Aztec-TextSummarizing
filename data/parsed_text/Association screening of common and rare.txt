Motivation: This article extends our recent research on penalized estimation methods in genome-wide association studies to the realm of rare variants. Results: The new strategy is tested on both simulated and real data. Our findings on breast cancer data replicate previous results and shed light on variant effects within genes. Availability: Rare variant discovery by group penalized regression is now implemented in the free program Mendel at
INTRODUCTIONGenome-wide association studies (GWASs) have enjoyed varying degrees of success in the past decade (). The failure of single nucleotide polymorphism (SNP)-based studies to explain a substantial fraction of trait variation is hardly surprising given the tendency of selection to drive even weakly deleterious mutations to extinction. There are several candidates for the missing dark matter of genetic epidemiology. Among these are: (i) copy number variants (CNVs); (ii) polygenes of small effect; (iii) interactions between genes and between genes and environment; (iv) epigenetic effects; and (v) rare variants. Rare variants are currently attracting the most attention. CNVs are subject to the same selective forces as SNPs. The sole benefit of discovering polygenes of small effect is the insight these provide into biochemical pathways and genetic networks. Detecting interactions is problematic unless they are large or sample sizes are very large. Epigenetic effects and parent-of-origin effects are clearly important in certain settings and deserve more study. In view of the recent striking advances in large-scale sequencing (), the search for rare variants is apt to be the most promising route to disease gene discovery. Statistical methods must evolve to meet the challenges of sequence data. Most current analysis methods are predicated on the common disease common variant (CDCV) hypothesis, which * To whom correspondence should be addressed.postulates that common diseases are caused by common variants of small to modest effect. The competing common disease rare variant (CDRV) hypothesis postulates that common diseases are caused collectively by multiple rare variants of moderate to large effect. Macular degeneration is cited as an example supporting the CDCV hypothesis (). Because macular degeneration onset is typically late in life, it has a small impact on Darwinian fitness. The CDRV hypothesis receives support from traits such as low plasma levels of HDL cholesterol (), cystic fibrosis (), colorectal adenomas (), familial breast cancer () and schizophrenia (). The distinction between the two hypotheses is less sharp than proponents might suggest in the heat of argument. There is a spectrum of deleterious allele frequencies within many disease genes, and special circumstances of human history may favor one hypothesis over the other, depending on the diseases and populations studied (). It makes good statistical sense to consider all predictors (SNP variants and environmental covariates) in concert. Because rare disease predisposing alleles may be present in only a handful of patients, the traditional variant-by-variant approach is doomed to low power. A remedy is to group variants by gene or pathway membership. Once this is done, the strongest marginal signal is assessed by a weighted sum test () or by a groupwise test exploiting the multivariate and collapsing strategies of Li and Leal (2008). Multiple testing remains a major concern. The current article extends our recent research on penalized estimation methods in GWAS () to the realm of rare variants. This approach to association mapping has several advantages: (i) it applies to both ordinary and logistic regression; (ii) it is parsimonious and very fast; (iii) it offers a principled approach to model selection when the number of predictors exceeds the number of study participants; and (iv) it handles interactions gracefully. Our current software relies on lasso penalties and forms part of the Mendel package (). Here, we discuss how to incorporate group penalties that make it easier for related predictors to enter a model once one of the predictors does. For example, one could group all SNPs within a single gene or within several genes in the same pathway. We will argue that a mixture of group penalties and single-predictor penalties tends to work best in practice and constitutes a good alternative to forced collapsing. When we pass to penalized estimation, model selection is emphasized over hypothesis testing. The lasso penalty is one of the best continuous variable selection mechanisms known for highdimensional models. The term 'lasso' stands for the least absolute shrinkage and selection operator. Unfortunately, the lasso is too stringent for rare variants. Shifting some of the lasso action to a group Euclidean penalty makes it easier for weak or low-frequency predictors to enter a model. Retention of a partial lasso penalty still discourages inclusion of neutral mutations within disease susceptibility genes. The mixed penalty tactic is apt to be most successful when a disease gene harbors a borderline-rare variant with substantial risk. Note that there is no reason to omit common variants in the model selection framework. Hence, a strength of mixed penalties is that they can be applied without choosing between the CDCV and CDRV hypotheses. Once the model selection perspective assumes center stage, multiple testing problems recede. They reappear in replication, but in a more benign form because the number of genes and SNPs of interest drop dramatically. This niche at the intersection between statistics and genetics is undergoing rapid evolution. Our prior experiences applying lassopenalized ordinary regression to microarray data () and lasso-penalized logistic regression to GWAS data () were very encouraging. Since we embarked on the current rare variant research, important work has appeared by a number of authors. The recent technical report ofintroduces a mixture of group and lasso penalties in ordinary regression. Earlierconsidered logistic regression with a pure group penalty. Both of these papers fall outside the arena of GWAS. The latter paper also employs different algorithms for optimization. Croiseau and Cordell (2009) applied logistic regression with a pure group penalty to a North American rheumatoid arthritis consortium dataset. However, they treat each SNP as a separate group, whereas we group SNPs by gene or pathway. To our knowledge, there have been no published papers on generalized linear models with mixed group and lasso penalties, certainly none focused on association mapping and rare variants. The remainder of the article is organized as follows. Section 2 describes our statistical approach and optimization algorithms. It introduces the lasso and group Euclidean penalties, and shows how they can be implemented in linear and logistic regression. The coordinate descent algorithms covered are exceptionally quick and permit optimal tuning of the penalty constant by cross-validation. Section 2 also presents an efficient method for simulating samples under the CDRV model. Section 3 applies the mixed penalty method to two simulation examples. Section 4 analyzes a breast cancer dataset that is small enough to allow comparison to traditional model selection. The discussion highlights some strengths and weaknesses of model selection with mixed penalties and suggests potentially helpful extensions.
METHODS
Lasso and group-penalized regressionLasso-penalized linear regression () is applied to high-dimensional regression problems with tens to hundreds of thousands of predictors. Estimates are derived by minimizingwhere y is the response vector, X the design matrix,  the vector of regression coefficients, z 2 = ( j z 2 j ) 1/2 the Euclidean ( 2 ) norm and z 1 = j |z j | the taxicab ( 1 ) norm. The sum of squares yX 2 2 represents the loss function minimized in ordinary least squares; the 1 contribution  1 is the lasso penalty function. Its multiplier >0 is the penalty constant. The lasso shrinks the estimates of the regression coefficients  j toward 0. An alternative ridge penalty  2 2 also shrinks parameter estimates, but it is not effective in reducing the vast majority of them to 0. For this reason the lasso penalty is preferred to the ridge penalty. Both lasso and ridge regressions are special cases of the bridge regression (). The constant  can be tuned to give any desired number of predictors. In this sense, lasso-penalized regression performs continuous model selection. The order predictors enter a model as  decreases is roughly determined by their impact on the response. Exceptions to this rule occur for correlated predictors. Logistic regression is handled in a similar manner. Instead of equating the loss function to a sum of squares, we equate it to the negative loglikelihood. The loglikelihood itself can be written aswhere n is the number of responses,  = (,) the parameter vector and the success probability p i for trial i is defined byHere, the response y i is 0 (control) or 1 (case), x t i the i-th row of the design matrix X and  an intercept parameter. In practice, statisticians also include the intercept in the ordinary regression model. It can be accommodated by taking the first column of X to be the vector 1 whose entries are identically 1. Because the intercept is felt to belong to any reasonable model, the lasso and ridge penalties omit it. To put the regression coefficients on an equal penalization footing, all predictors should be centered around 0 and scaled to have approximate variance 1. There is a parallel development of lassopenalized regression for generalized linear models (). In each case, the objective function is written asas the difference between the loglikelihood and the lasso penalty. Because we now maximize f (), we subtract the penalty. In some applications, it is natural to group predictors (). This raises the question of how to penalize a group of parameters. The lasso penalty and the ridge penalties separate parameters. If a parameter enters a model, then it does not strongly inhibit or encourage other associated parameters entering the model. Euclidean penalties have a more subtle effect. Suppose G denotes a group of parameters. Consider the objective functionwith a Euclidean penalty on each group. Here,  G is the subvector of the regression coefficients corresponding to group G. In coordinate ascent, we increase f () by moving one parameter at time. If a slope parameter  j is parked at 0, when we seek to update it, its potential to move off 0 is determined by the balance between the increase in the loglikelihood and the decrease in the penalty. The directional derivatives of these two functions measure these two opposing forces. The directional derivative of L() is thefor movement to the right and the negative score  for movement to the left. An easy calculation shows that the directional derivative of  G 2 is  in either direction at  j = 0 when  i = 0 for all i  G with i = j. In this case note that  G 2 =| j |. If  G = 0, then the partial derivative of  G 2 with respect to  j is  j /| G 2. Hence, the directional derivatives both vanish at  j = 0. In other words, the local penalty around 0 for each member of a group relaxes as soon as the regression coefficient for one member moves off 0. Euclidean group penalties run the risk of selecting response-neutral predictors. As soon as one predictor from a group enters a model, it opens the door for other predictors from the group to enter the model. For this reason, Page: 2377 23752382
Penalized regression for GWASwe favor a mixture of group and lasso penalties in ordinary regression. In our genetics context, lasso penalties keep the pressure on for neutral mutations to be excluded, even if they occur in causative genes or pathways. There is no need to group SNPs that occur outside coding or obvious regulatory regions. However, it seems reasonable in the absence of other knowledge to penalize all SNPs equally. This suggests that all Euclidean penalties have the same scale and that the sum of the group and lasso scales for each SNP be the same. Thus, if SNP j belongs to group G, it should experience penalty  E  G 2 + L | j |. If it belongs to no group, it should experience penaltyImposition of lasso and Euclidean penalties has further advantages. In addition to enforcing model parsimony and selecting relevant parameters, both penalties improve the convergence rate in minimizing the objective function. Because the penalties are convex, they also increase the chances for a unique minimum point when the loss function is non-convex. As we demonstrate, both kinds of penalties are compatible with coordinate descent, which is by far the fastest optimization method in sparse regression.
AlgorithmsCoordinate descent/ascent has proved to be an extremely efficient algorithm for fitting penalized models in high-dimensional problems (). Traditional algorithms such as Newton's method and scoring are not computationally competitive. Cyclic coordinate descent/ascent optimizes the objective function one parameter at a time, fixing the remaining parameters. Block relaxation generalizes cyclic coordinate descent by cycling through disjoint blocks of parameters and updating one block at time.use block relaxation to fit logistic regression. The extreme efficiency of cyclic coordinate descent/ascent in high-dimensional problems stems from the low cost of the univariate updates and the fact that most parameters never budge from their initial value of 0. Here, we present cyclic coordinate descent for linear and logistic regression with mixed lasso and group penalties.
Logistic regression with cases and controlsIt is well known that the logistic loglikelihood (1) with success probabilities (2) has score vector and observed information matrixFor the intercept derivatives, recall that the relevant coordinate of x i is 1. The penalized loglikelihood augmented by group and lasso penalties becomeswhere G ranges over all groups. When  L = 0, f () incorporates a pure group penalty (). When  E = 0, f () incorporates a pure lasso penalty (). In penalized maximum likelihood estimation, coordinate ascent is implemented by replacing the loglikelihood by its local quadratic approximation based on the relevant entries of the score and observed information. The penalty contribution is likewise approximated locally by a quadratic in the parameter being updated. For the intercept parameter , the penalty can be ignored, and Newton's update amounts to.To update a slope parameter  j , we commence maximization at 0. If the directional derivatives to the right and left are both negative, then no progress can be made, and  j remains at 0. Otherwise, maximization is confined to the left or right half-axis, whichever shows promise. Because the objective function is concave, the two directional derivatives cannot be simultaneously positive. If  j belongs to group G, then the two first two partial derivatives areThe lack of continuity of the first partial derivative at the point  j = 0 does not prevent the directional derivatives from being well defined. The Newton's update of  jalmost always converges within five iterations. At each iteration one should check that the objective function is driven uphill. If the ascent property fails, then the simple remedy of step halving is available.
Ordinary regression with a quantitative traitThe objective function to be minimized isThe Newton update of the intercept is the obvious averageTo implement Newton's method for a slope parameter  j belonging to group G, one employs the first and second partial derivativesWith these derivatives in place, the 1D Newton's update (3) is pertinent. Once again iteration is confined to the left or right half-axis, provided either passes the directional derivative test.
Selection of tuning constantsIn principle, cross-validation can be invoked to determine the optimal values of  L and  E. As we show in our simulations, setting them equal works well. Given a fixed ratio of the two penalties, the total penalty  =  L + E can be adjusted to deliver a predetermined number of genes or SNP variants. Because the number of non-zero predictors entering a model is a generally a decreasing function of , a bracketing and bisection strategy is effective in finding a relevant  (). Of course, the smaller the number of predictors desired, the faster the overall computation proceeds. If computing time is not a constraint, it is helpful to optimize the objective function over a grid of points and monitor how new predictors enter the model as  decreases.Page: 2378 23752382
H.Zhou et al.
Simulation algorithmsFor the sake of simplicity, we adopt the rare variant model of Li and Leal (2008). They postulate that any of v variants can independently cause the disease under consideration. If I i is the indicator of disease attributed to variant i, then the sum S = v i=1 I i captures the essence of the model. An individual is affected if and only if his/her value of S satisfies S  1. Thus, an individual could have multiple mutations, each one sufficient to cause the disease. Ignoring genetic details for the moment, let k i = Pr(I i = 1). These prevalences plus the independence of the indicators I i completely determine the Poisson-binomial distribution characterizing S. The discrete density of S can be computed recursively from the probabilities k i (). Once the discrete density is available, we sample from the conditional distribution Pr(S = j | S  1). Obviously, all I i = 0 whenever S = 0. Finally, given a positive value j of S, one can sample from the conditional Poisson-binomialin an efficient sequential manner (). This brief account omits many details that are fully supplied in the cited reference and the table labeled Algorithm 1. The most suspect assumption in the model is the independence of the disease indicators I i , which rules out linkage disequilibrium for closely spaced variants. The remaining genetics assumptions are more defensible. Let G i be the genotype at variant i. Designate the normal allele by a i and the high-risk allele by A i. If the latter has frequency p i , then under HardyWeinberg equilibrium the three genotypes a i /a i = 0, A i /a i = 1 and A i /A i = 2 have frequencies (1p i ) 2 , 2p i (1p i ) and p 2 i , respectively. Denote the penetrance of the genotype G i = j at variant i by f ij = Pr(I i = 1 | G i = j). The prevalence attributed to variant i amounts to k iA dominant model takes f i1 = f i2 , and a recessive model takes f i0 = f i1. For purposes of discussion, the wild-type penetrance f 0 = 1 v i=1 (1f i0 ) is the probability that a person with no high-risk alleles is affected. The variant-specific relative risks (RRs) are defined by the ratios  ij = f ij /f 0. To simulate genotypes in a case/control study, we first simulate the disease indicators I i , assuming case status (S  1) or control status (S = 0). Conditional on the indicators I i , the genotypes G i are independent. Sampling G i given I i is a simple application of Bayes rule taking into account the various genotype probabilities and penetrances. Our entire simulation scheme is summarized in Algorithm 1. Computation of the discrete density of S requires v 2 /2 operations but only needs to be done once. Simulating each case requires 3v operations and each control v operations. It takes <2 s on a standard laptop to simulate 10000 SNPs for 500 cases and 500 controls.
ANALYSIS OF SIMULATED DATAOur first simulation example compares mixed group and lasso penalties to pure lasso and pure group penalties in association testing.shows the solution paths of a simulation example with 500 cases and 500 controls at various mixes of lasso and group penalties for three genes. Gene 1 (red) contains one common causal variantAlgorithm 1 Given MAFs p 1 ,...,p v and variant specific penetrances f ij for i = 1,...,v and j = 0,1,2, simulate D cases and N controls Calculate genotype frequencies under HWE: Pr(G i = j)Calculate the lower triangular probability table Q(0 : v,0 : v) via recursion
for each control dogenes, each with 5 rare variants. Across the simulations, the MAF is uniformly distributed from 0.1% to 1%. For i = 1,...,5, gene i has i causal rare variants. Therefore, the model has 15 causal rare variants dispersed over 5 genes and 35 neutral rare variants dispersed over 10 genes. All neutral variants have RR 1. The wild-type penetrance f 0 is set at 0.01.reports the receiver operating characteristic (ROC) curves calculated from selected variants and genes, with the proportion of the lasso penalty  L / set at 0 (pure group penalty), 0.5 and 1.0 (pure lasso penalty). Each point of the ROC curves records the true and false positive rates of the selected variants (first row) or genes (second row) at a specific  value. Inspection of these graphs shows that the performance of the mixed group and lasso penalties dominates that of the pure lasso penalty in variant selection. Note how the green ROC curves are shifted toward the upper left. The effects on gene selection is not clear-cut. The second and third scenarios (columns) support our contention that penalized regression with mixed penalties performs better when any of the causal variants is relatively common or has a high RR in groups.
APPLICATION TO FAMILY CANCER REGISTRY DATAGermline mutations in genes from various DNA repair pathways, most notably BRCA1, BRCA2 and ATM, have been shown to dramatically increase the risk of familial breast cancer but do not explain all of the risk (). Based on a candidate gene study of the double-strand break repair (DSBR) pathway, we have identified SNPs from genes involved in DSBR (XRCC4, XRCC2, NBS1, RAD21, TP53, BRIP1, ZNF350) that are associated with risk of familial breast cancer in single SNP analyses (). Identifying group effects from this pathway can be helpful in understanding factors that modulate an individual's risk of developing breast cancer. We wish to identify group effects by gene and apply here mixed group and lasso-penalized regression.contains 10 neutral rare variants. All neutral rare variants have MAF 1% and RR 1. The wild-type penetrance f 0 is set at 0.01. The pure lasso penalty ( L / = 1) picks up significant variants (common and rare) sequentially. The pure group penalty ( L / = 0) picks up the genes (groups) 1, 2 and 3 sequentially. The mixed group plus lasso penalty ( L / = 0.75 or 0.50) achieves a good compromise between the two.The first row is for variants and the second row for genes. MAFs of all variants are uniform between 0.1% and 1%. Neutral variants have RR 1. Column 1: RRs of causal variants are uniform between 1.2 and 5. Column 2: RRs of causal variants are uniform between 1.1 and 2, except one RR is set to 10 in each causal gene. Column 3: MAF of one variant is set to 5% in each causal gene. The true positive rate (sensitivity) is the proportion of causal variants/genes correctly identified, while the false positive rate (1-specificity) is the proportion of neutral variants/genes identified as causal.
Page: 2379 23752382
Penalized regression for GWASPage: 2380 23752382Family Cancer Registry: data are taken from genotype samples of participants enrolled in the UCLA Family Cancer registry. To be eligible, individuals must have a personal or family history of either a known cancer genetic susceptibility, such as a mutation in BRCA1 or BRCA2, or a family history containing at least two first or second degree relatives who are afflicted with the same primary cancer. This enriched sample of participants allows for the identification of factors that modulate risk of breast cancer. Data analysis has to be fairly subtle because of the way in which the participants were enrolled. Analysis: we performed penalized logistic regression with the dependent variable, breast cancer status (affected versus unaffected) coded as a binary outcome. We limited our sample to 399 Caucasian participants because other ethnic groups were too small to fully characterize and provide little power to detect differences. There were 196 affected and 203 unaffected individuals. Age was used as a covariate in our analysis. The well-known association of age with breast cancer was confirmed in our previous analysis (). We imputed missing data for covariates using the mean value for continuous variables and the most frequent category for categorical variables. SNPs were excluded from our analysis if genotype call rates were <75%. Missing SNPs were imputed using the SNP imputation option of the Mendel 10.0 software (). 148 SNPs from the DSBR pathway were grouped by gene. These 17 genes included BRCA1, BRCA2, BRIP1, ATM, RAD50, RAD51, RAD52, RAD54L, RAD21, TP53, NBS1, XRCC2, XRCC4, XRCC5, MRE11A, ZNF350 and LIG4. Some genes carried large numbers of SNPs (e.g. BRCA2 had 19 SNPs), and some genes had only one SNP for analysis. SNPs were analyzed under additive models. Penalized regression was performed under varying proportions of lasso and group penalties. Analysis under a dominant model leads to similar conclusions (data not shown). Results: although most of the SNPs in this dataset are common, 4 have MAFs <1%, 5 have MAF between 1% and 5% and 13 have MAF between 5% and 10%.plots the selection trajectories for groups of SNPs and demonstrate the ability of mixed group and lasso-penalized regression to select SNPs within a gene as a group. As the total penalty grows, SNPs are selected either singly or as groups. In the case of the pure lasso, SNPs enter the model singly, and in the case of the pure group penalty, genes enter the model with their full sets of SNPs. In the mixed cases, we see that either single SNPs or sets of SNPs grouped by gene enter the model. When a group enters in the mixed cases, it need not contain all of the SNPs in that gene. Age was the first predictor selected in all models as expected. The content and order of selection of the top four gene-defined groups under varying proportions of lasso to total penalty are shown in, while the XRCC4 SNPs fall in different haplotype blocks. Many of these SNPs are found to be associated with familial breast cancer in single SNP analyses. In marginal analysis, 14 SNPs have P < 0.05. Ten of these are also selected by the mixed penalty method with  L / = 0.250.5 (boldfaced in). It seems biologically reasonable that these SNP sets should be among the first predictors selected after age. SNP rs4986763 from gene BRIP1 is present in all models. This SNP was found to be significant in previous single SNP analyses. However, it was only highly significant after excluding individuals who were known to be BRCA1 and BRCA2 positive. RAD52 was not found to be significant in previous single SNP analyses (), suggesting it may modulate the effects of other SNPs. In a follow-up study (, possible interaction of RAD52 with other genes is investigated.
H.Zhou et al.
Penalized regression for GWAS
DISCUSSIONThe results of this article suggest that mixed group and lasso penalties outperform lasso penalties alone, especially when both common and rare variants are present. Our simulated examples clearly demonstrate this fact. Our analysis of the breast cancer data is more ambiguous because we do not know the truth that nature hides. In our view, the focus in genetic epidemiology should be on both SNP and gene discovery. The connections between penalized regression and Bayesian analysis are obvious. One could argue the case for passing to a full Bayesian assault on association testing. This has already been accomplished for marginal analysis of SNPs (Wellcome Trust). Although it is tempting to construct multi-predictor Bayesian methods, the computational costs are apt to be high. Penalized estimation and model selection achieve many of the same goals at a fraction of the computational cost.Mixed penalties help us sort through the confusion of causal genes and neutral variants within them. Even though mixed penalties improve both false positive and false negative rates, we are not suggesting that mixed penalties are a panacea. However, the gradual accumulation of incremental improvements in statistical methods will make a substantial difference. The statistical tools showcased here form part of the next release of the Mendel statistical genetics package. Mendel is available for free in Linux, MacOS and Windows versions at http://www.genetics.ucla.edu/software. It takes <5 s on a standard desktop computer to complete all single SNP analyses and lasso estimation on the family cancer registry data. Our companion paper (Zhou,H. et al., unpublished data) discusses Mendel syntax and output conventions. Geneticists and statisticians wanting to judge for themselves the virtues of mixed penalties are welcome to use Mendel on their own data.
The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org 2375 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
