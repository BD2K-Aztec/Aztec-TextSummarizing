Motivation: Over the past decade, several biclustering approaches have been published in the field of gene expression data analysis. Despite of huge diversity regarding the mathematical concepts of the different biclustering methods, many of them can be related to the singular value decomposition (SVD). Recently, a sparse SVD approach (SSVD) has been proposed to reveal biclusters in gene expression data. In this article, we propose to incorporate stability selection to improve this method. Stability selection is a subsampling-based variable selection that allows to control Type I error rates. The here proposed S4VD algorithm incorporates this subsampling approach to find stable biclusters, and to estimate the selection probabilities of genes and samples to belong to the biclusters. Results: So far, the S4VD method is the first biclustering approach that takes the cluster stability regarding perturbations of the data into account. Application of the S4VD algorithm to a lung cancer microarray dataset revealed biclusters that correspond to coregulated genes associated with cancer subtypes. Marker genes for different lung cancer subtypes showed high selection probabilities to belong to the corresponding biclusters. Moreover, the genes associated with the biclusters belong to significantly enriched cancer-related Gene Ontology categories. In a simulation study, the S4VD algorithm outperformed the SSVD algorithm and two other SVD-related biclustering methods in recovering artificial biclusters and in being robust to noisy data. Availability: R-Code of the S4VD algorithm as well as a documentation can be found at http://s4vd.r-forge.r-project.org/. Contact: m.sill@dkfz.de
INTRODUCTIONClustering methods belong to the most commonly used statistical tools in the analysis of high-dimensional datasets. If additional information about the sample class labels is lacking, other types of analysis like supervised classification methods or testing for differentially expressed genes cannot be performed. In this case, unsupervised clustering allows to reveal unknown structures that are * To whom correspondence should be addressed. possibly hidden in the gene expression data matrix. These structures may be characterized by groups of genes that are coregulated by a common transcription factor and thus belong to the same pathway or samples that share a similar gene expression pattern. One disadvantage of commonly used clustering algorithms like hierarchical clustering or k-means clustering is that the cluster assignment of objects are based on the complete feature space, e.g. in case of clustering the samples, the resulting clusters are derived with respect to all genes. But groups of genes may only be coregulated within a subset of the samples and samples may share a common gene expression pattern only for a subset of genes. Such clusters that exist only in a subspace of the feature space can hardly be detected by these classical one-way clustering algorithms. To find such clusters, other clustering concepts are needed. In the past decade, the concept of biclustering has emerged in the field of gene expression analysis. Biclustering which is also known as coclustering or two-way clustering describes the simultaneous clustering of the rows and the columns of a data matrix. The first biclustering algorithm, the so-called Block Clustering, has been developed by. Cheng andproposed the first biclustering algorithm for the analysis of high-dimensional gene expression data. Since then, many different biclustering algorithms have been developed. Currently, there exists a diverse spectrum of biclustering tools that follow different strategies and algorithmic concepts. Among others, popular algorithms are the Coupled Two-Way Clustering (CTWC) by, Order Preserving Sub Matrix (OPSM) algorithm by, the Iterative Signature Algorithm (ISA) by, the Plaid Model byand the improved Plaid Model (), SAMBA by, biclustering by non-smooth non-negative matrix factorization by, the Bi-correlation clustering algorithm (BCCA) by Bhattacharya and De (2009) and factor analysis for bicluster acquisition (FABIA;).developed a fast divide-and-conquer algorithm (Bimax) and conducted a systematic comparison of different biclustering algorithms.published an article on validation indices for the evaluation of biclustering results and the comparison for biclustering algorithms. Comprehensive reviews about the concept of biclustering and the different biclustering approaches have been written byand Van. In a more theoretical review,emphasized the mathematical concepts behind several biclustering algorithms and pointed out that the SVD represents a capable tool for finding biclusters. Furthermore, most existing biclustering algorithms use the SVD directly or have a strong association with it. To keep track of the huge diversity, regarding the mathematical properties of the existing biclustering algorithms,suggest to relate new and existing biclustering algorithms to the SVD. A major drawback of many biclustering methods is that they rely on random starting seeds and thus are inconsistent and results may vary even when the algorithm is applied to the same dataset. As often in unsupervised clustering it is difficult to judge the biclustering results regarding their stability. For one-way clustering, several resampling approaches to validate the stability of the clustering results are known, e.g. multiscale bootstrap hierarchical clustering () and consensus clustering (). In case of biclustering, similar methods that take the stability of the results into account are not yet available. Recently,proposed a sparse SVD (SSVD) method to find biclusters in gene expression data. Singular vectors of an SVD are interpreted as regression coefficients of a linear regression model. The SSVD algorithm alternately fits penalized regression models to the singular vector pair to obtain a sparse matrix decomposition. The sparseness of the resulting singular vectors strongly depends on the choice of the penalization parameter. In this article, we propose to choose the penalization parameters by stability selection (), which is a subsampling procedure that can be applied to penalized regression models to select stable variables. In addition, stability selection offers the possibility to control Type I error rates (), e.g. the per-family error rate (PFER) or the per-comparison wise error rate (PCER). Applying the new combined algorithm, the sparse SVD algorithm with nested stability selection (S4VD) to a lung cancer gene expression dataset reveals biclusters that represent lung cancer subtypes characterized by relevant sets of coregulated genes. In a simulation study, we compare the S4VD with the SSVD algorithm as well as the improved Plaid Model () and the ISA ().
METHODS
SVD and biclusteringLet X = (x ij )  R pn be the gene expression matrix with indices i = 1,...,p and j = 1,...,n. The number of genes p is usually by multiple greater than the number of samples n. The SVD of X can be written as:where r is the rank of X and the columns of the matrix U = (u 1 ,...,u r ) are the orthonormal left-singular vectors and the columns of V = (v 1 ,...,v r ) are the orthonormal right-singular vectors. The elements of the diagonal matrix D are the corresponding positive singular values d 1  d 2  ...  d r > 0. Thus, the SVD is the sum of rank of one matrices d k u k v T k , herein after also called SVD-layers. According to, biclustering can be related to the SVD by considering an idealized data matrix. This matrix has a block diagonal structure where each block represents a bicluster and the elements outside these blocks are equal to zero:where X k , k = 1,...,r are submatrices of X. If we decompose X by the SVD, then each submatrix X k will be associated with a singular vector pair (u k ,v k ) such that the non-zero coefficients in u k represent the rows that belong to X k and the non-zero coefficients v k represent the columns that belong to X k. In the presence of noise and if the data matrix has no block diagonal structure, the SVD will still be able to detect the rows and columns of the submatrices as the prominent coefficients in the singular vector pair. These properties make the SVD a practical method for biclustering.
The SSVD algorithmA sparse SVD method for biclustering high-dimensional gene expression data has been proposed by. The idea is to interpret the singular vectors of a regular SVD as regression coefficients of a linear regression and use sparsity-inducing penalties to obtain sparse singular vector pairs. According to, the first SVD-layer gives us the best rank-one approximation of X with respect to the squared Frobenius norm, i.e.where  2 F indicates the squared Frobenius norm, which is the sum of squared elements of the matrix.showed how this rank-one approximation can be related to linear regression. Suppose u 1 is fixed, then the minimization of (3) with respect to (d 1 ,v 1 ) is equivalent to a minimization with respect t v 1 = (d 1 v 1 ). Accordingly, the loss function can be written as:where y = (x T 1 ,...,x T n ) T  R pn with x j being the j-th column of X. Then the minimization of (4) can be interpreted as least squares problem with y as the response vector, I n u 1 as the design matrix and thevthe thev 1 as vector of regression coefficients. The least squares estimator ofvof ofv 1 is:In the same way, we can derive the least squares estimator for the product of the first left singular vector multiplied with the first singular valueuvalue valueu 1. So without loss of generality with v 1 fixed, the minimization of (3) with respect t u 1 = (d 1 u 1 ) is given by the minimization of:where z = (x 1 ,...,x p ) T  R pn with x T i being the i-th row of X. Here z is the response vector and (I n v 1 ) is the design matrix. Finally, the least squares estimator ofuof ofu 1 is given by:In order to obtain sparse singular vector pairs,suggest to find the first SVD-layer that minimizes the Frobenius norm subject to sparsity-inducing penalty termswhere  u 1 and  v 1 are tuning parameters. Possible penalty functions are the adaptive lasso penalties (). The corresponding penalized function is given by:where w 1,i and w 2,j are weights that can be chosen according to, e.g. for w 1,i = w 2,j = 1 we obtain the lasso penalty. Thus, the penalty functions
Sparse SVD incorporating stability selectionare weighted sums of the absolute values of the elements of the first singular vector pair. Fixing u 1 and using the adaptive lasso penalty, the minimization of (8) becomes:To solve this penalized regression and estimate the sparse right singular vector,proposed an algorithm that incorporates a simple component-wise thresholding rule. The component-wise minimizer of (10) is:This is the well-known soft threshold estimator proposed by. ThenThenThen, is an estimate for the product of the first right singular vector multiplied with the first singular value. In order to get an estimate for the first sparse right singular vector, we have to update the first singular value. The first update of d 1 is d 1,v 1 ====== v 1 and accordingly the estimated sparse singular vector becomesvbecomes becomesvThe penalized regression for the left singular vector can be solved in the same way. For fixed v 1 and with the adaptive lasso penalty, the loss function of (8) becomes:The component-wise minimizer of (12) is:The updated singular value is d 1,u 1 ====== u 1 , withwithwithFinally, the estimated sparse left singular vector isisisThe degree of sparsity, which is defined as the number of non-zero coefficients in the singular vector pair, depends on the choice of the penalty parameters.proposed to choose the optimal degree of sparsity by computing the complete penalization path and apply the penalty parameter that minimizes the Bayesian information criterion (BIC). In the SSVD algorithm, the two regressions with the corresponding parameter tuning are alternated until convergence is reached, which is if either<<, where >0 is an arbitrary convergence threshold. After convergence the final estimate of the first singular value of the sparse SVD-layer isdisThe next sparse rankone approximation can be obtained by subtracting the sparse SVD-layer and applying the SSVD method to the residual matrix
The SSVD algorithm1. Apply the standard SVD to X. Let {d 1 ,u 1 ,v 1 } denote the first SVD triplet.2. Update:minimizes the BIC. LetLetLet(b) SetSetSetIn practice, we observed that choosing the regularization parameters according to the BIC results in singular vector pairs with a relative low degree of sparsity. In addition, the SSVD algorithm does not offer a stopping criterion and so the choice of the number of SVD-layers is arbitrary.
Stability selectionIn this article, we propose to choose the penalization parameters and to control the degree of sparsity of the resulting SVD-layers using stability selection (). The idea of stability selection is to combine resampling with variable selection methods, e.g. penalized regression models. For each variable, its probability of being selected is estimated by resampling the data and calculating relative frequencies of being selected. Meinshausen and Bhlmann (2010) provide a theoretical framework for controlling Type I error rates of falsely selecting variables based on the maximum of these selection probabilities over the range of regularization parameters. Suppose we want to infer the true set of non-zero coefficients in the left singular vector S u 1 = i : u 1,i = 0 . The set of possible penalization parameters that can be applied within the adaptive lasso regression isGiven any  u 1 , the estimated setSset setS u 1 u 1 can be written as a function of the samples J = {1,...,n}, e.g.u 1 (J). If J *  J is a subsample drawn without replacement, then the estimated selection probability is: The selection probability can be estimated by calculating the relative selection frequencies of i with regard to all subsamples. Given an arbitrary threshold  thr  (0.5,1) and the set of penalization parameters u 1 , the set of non-zero coefficients estimated with the stability selection is:According to, the value of  thr has a negligible influence and they recommend to choose values in the range ofbe the union of the estimated sets of selected coefficients with regard to all  u 1  u 1. Then the average number of selected coefficients is qdenote the set of zero coefficients, then the number of falsely selected coefficients with stability selection is given by
|. Following Theorem 1 inMeinshausen and Bhlmann (2010), the expected number of falsely selected coefficients is bounded by:Interpreting Equation (16), the expected number of falsely selected coefficients decreases by either reducing the average number of selected coefficients q u 1 or by increasing the threshold  thr. Supposing that  thr is fixed, the stability selection controls the desired error level of E(V u 1 ) as long as the average number of selected coefficients is less then e u 1 , where) is an upper bound for the average number of selected coefficients that can be controlled by reducing the length of the regularization path u 1. In multiple testing, the expected number of falsely selected variables is also known as PFER and if divided by the total number of the variables, it will become the PCER (). The stability selection allows to control these Type I error rates.
Page: 2092 20892097
M.Sill et al.
The SSVD algorithm with nested stability selectionHere, we propose to replace the BIC-based penalty parameter selection of the SSVD algorithm by the stability selection. This combined approach allows to control the expected number of falsely selected non-zero coefficients in the singular vector pair and therefore the degree of sparsity of the resulting SVD-layers. Furthermore, the error control also serves as stopping criterion for the improved SSVD algorithm and determines the number of reasonable layers. We aim to estimate the left singular vectorvectorvector 1 and at the same time infer the true set of non-zero coefficients S u 1. For each possible  u 1 , we draw subsamples and estimate the selection probabilitiesuprobabilities probabilitiesu 1 i. Given a threshold  thr and the desired Type I error E(V u 1 ), the regularization region u 1 is defined so that q u 1  e u 1. Then the estimated set of non-zero coefficients is:To estimatestimat estimatu 1 , we apply the component-wise minimizer ofwith the smallest penalization value of the regularization path  minLike in the original SSVD approach, the first update of the singular value isThe estimated sparse singular vector isisis 1 =   u 1 /d 1,u 1. Without loss of generality, we estimate the sparse right singular vectorvvector vectorv 1 and infer the respective set of non-zero coefficients). Consequently, the estimated set of non-zero coefficients in the right singular vector is:Given the smallest parameter of the penalization path  min
v 1, the components ofvof ofv 1 are:Finally letletletthe updated first singular value is d 1,v 1 =  v 1 and estimated sparse singular vector isvis isvThese two penalized regression models with the nested stability selection are alternated until convergence, e.g. that is if either<<, where >0. After convergence, the estimated singular value isdis isd) u 1,i and the components ofvof ofv 1 becombecomis an indicator function. The high degree of sparsity of the resulting SVD-layers may lead to a poor matrix factorization that might induce noise to the residual matrix when subtracted from the data matrix. Like for multivariate regression models, this can be seen as a trade-off between a high degree of sparsity and hence interpretability for the cost of losing prediction power. Regarding the sequential fitting procedure of the S4VD algorithm, the acceptance of a poor matrix approximation might induce noise into the residual matrix. This induced noise may perturb the fitting process for subsequent biclusters. In order to avoid a propagation of errors induced by a poor matrix approximation, we propose to apply the regular SVD to the submatrix defined by the stable subsets of rows and columns identified with the S4VD algorithm. According to, the rank-one SVD approximation of this submatrix is the best rank-one approximation of the submatrix with respect to the Frobenius norm. The next bicluster can be detected by subtracting this rank-one approximation of the submatrix from the corresponding submatrix of the input data matrix and applying the S4VD algorithm to the resulting residual matrix.Alternatively non-overlapping biclusters can be detected by excluding either the rows or the columns (or both) that correspond to the non-zero coefficients in the singular vector pair and apply the S4VD method to the submatrix. By incorporating the stability selection, a stopping criterion can be defined. If in any iteration an estimated set of non-zero coefficients is an empty set, the sequential fitting of sparse rank-one layers will be interrupted. Due to the element of resampling the S4VD algorithm will not necessarily converge to the exact same result when applied to the same dataset. However, the element of resampling also allows to take the bicluster stability into account by controlling the Type I error levels of falsely assigning rows and columns. As demonstrated by the simulations presented in Section 3, the S4VD algorithm shows a better performance in revealing the true bicluster structure and is more robust to noise.) v 1,j. 4. To obtain the next layer, apply steps 13 to the residual matrix after subtracting the rank-one approximation derived by applying a regular SVD to the submatrix defined bySby byS stable
Stop steps 14 if eitherSeither eitherS stableThe subsampling steps of the stability selection makes the S4VD algorithm computationally very demanding. To reduce the computation time, we implemented the pointwise error control suggested by Meinshausen and Bhlmann (2010). To examine how the pointwise error control reduces the computation time, the runtimes of the SSVD algorithm, S4VD algorithm and S4VD algorithm with the pointwise error control have been compared. In the first part of the simulation study described in the Section 3 at a noise level of 0.5, the mean runtime of the SSVD algorithm was 33.9 s, for the S4VD it was 181.7 s and for the S4VD with the pointwise error control it was 5.8 s. The simulations have been carried out using a notebook with an IntelCore2 Duo Processor T7700 2.4 GHz and 4 GB DDR2 SDRAM. Details about the pointwise error control and boxplots of the runtimes are shown in the Supplementary Material.
RESULTSIn order to demonstrate that the here proposed S4VD algorithm is able to find biologically relevant biclusters, we applied it to a knownthat the heatmap shows only those genes that have been selected in at least one bicluster. The rectangles indicate the genes and samples that correspond to the three biclusters (the rectangle on the left side corresponds to Bicluster 1, the two rectangles in the middle correspond to Bicluster 2 and the two rectangles on the right side to Bicluster 3). lung cancer gene expression dataset (). Furthermore, to examine the influence of increasing levels of noise regarding the performance of the S4VD algorithm, we performed a simulation study. The S4VD algorithm was compared with the SSVD method, the improved Plaid Model () and the ISA (). The ISA and the Plaid Model are known to be closely related to the SVD.
Page: 2093 20892097
Sparse SVD incorporating stability selection
Evaluation of the lung cancer datasetHere we analyzed the same subset of the lung cancer gene expression data set () that was used byto illustrate the SSVD algorithm. This dataset contain 56 samples and gene expression values of 12 625 genes measured using the Affymetrix 95av2 GeneChip. The samples comprise 20 pulmonary carcinoid samples (Carcinoid), 13 colon cancer metastasis samples (Colon), 17 normal lung samples (Normal) and 6 small cell lung carcinoma samples (SmallCell).applied the SSVD method to this gene expression matrix and decomposed it into the first three sparse SVD-layers. For each of the resulting SVD-layers, the degree of sparsity was relatively low, e.g. for the three singular vectors that correspond to the samples, the number of non-zero coefficients were 55 for the first two and 47 for the third. The singular vectors that correspond to the genes contained 3205, 2511 and 1221 non-zero coefficients. Scatterplots of the sample singular vectors showed a clear grouping of the samples into the different cancer subtypes. In addition,formed 27 gene sets according to the sign of the coefficients in the three gene singular vectors. The mean expression profiles of these gene sets showed clear differences between the cancer subtypes. However, despite these results a direct interpretation of each singular vector pair is not possible. To obtain SVD-layers with a higher degree of sparsity that can be interpreted as single biclusters, we applied the S4VD algorithm controlling a PCER of 0.5 for falsely selecting coefficients in the sample singular vector and a PCER of 0.01 for falsely selecting coefficients in the gene singular vector. Furthermore, we did not allow the samples to overlap, e.g. each sample is assigned to only one bicluster. Therefore, after a sparse SVD-layer is fitted, we exclude the corresponding columns from the data matrix and applied the S4VD algorithm to the resulting submatrix. According to the stopping criterion of the S4VD algorithm, three biclusters have been obtained and are shown in the heatmap in. The first bicluster corresponds to a subset of 550 genes and a subset of 28 samples including 14 Normal samples and 14 Carcinoid samples. The second bicluster comprises 12 Colon samples and one falsely assigned Carcinoid sample together with a subset of 506 genes. The third bicluster consists of 6 SmallCell samples and 344 genes. All other samples and genes have not been assigned to any bicluster. To illustrate that the selected genes represent genes that are associated with the cancer subtypes, we performed a geneset enrichment analysis (). Tables of all significantly enriched Gene Ontology (GO) terms (p < 0.01) as well as a description of the analysis can be found in the Supplementary Material.identified several possible marker genes for the different cancer subtypes. A list of eight of these genes together with the corresponding selection probabilities with respect to the three biclusters are shown intissue and thus have high selection probabilities for the first bicluster. This coincides with the GO analysis, e.g. 2 of the 62 GO terms that are significantly enriched by the genes corresponding to the first bicluster are TGF receptor signaling pathway (GO:0007179) and response to retinoic acid (GO:0032526). Integrin,6 as well as v-myc (c-myc) are usually overexpressed in colon cancer. These genes have high selection probabilities with respect to the second bicluster. In addition, among the 61 significantly enriched GO terms corresponding to the second bicluster is the term endothelial cell migration (GO:0043542), which coincides with the fact that the associated samples correspond to colon cancer metastases. The cell-cycle inhibitor protein p18 and thymosin- are markers for small cell carcinomas and show high selection probabilities in the third bicluster. Among the 97 GO terms significantly enriched in the third bicluster are many cell cycle-associated terms, e.g. cell division (GO:0051301), mitotic spindle organization (GO:0007052) and cell cycle checkpoint (GO:0000075). Furthermore, for the first bicluster as well as for the third bicluster the GO term positive regulation of Notch signaling pathway (GO:0045747) is significantly enriched. Alterations of the Notch signaling cascade are known to be associated with several human cancer types.
Simulation studyIn the first part of the simulations, we generated 100 artificial data matrices comprising p = 1000 genes and n = 100 samples, where each entry of the data matrix is set to 0. In each dataset, we randomly assigned 100 genes and 10 samples to a bicluster that shows constant upregulated gene expression represented by a value of 1 in the data matrix. Normally distributed noise N(0, 2 ) was added to each entry of the data matrix. We examined different noise levels in the range of  = (0,0.1,...,1). In the second part of the simulation study, 100 data matrices of the same dimension were generated. This time four biclusters were included where each consists of 100 genes and 10 samples. Constant up-and downregulation was represented by values of 1,1,0.5 and 0.5. For both scenarios, the performance of the S4VD algorithm was examined in comparison to the original SSVD algorithm, the improved Plaid Model (PM;) and the ISA (). Since the SSVD algorithm does not include a stopping criterion, we considered only the first SVD-layer as result in the first scenario and the first four SVD-layers as the biclustering result in the second scenario. The clustering results were validated by application of an external validation index based on the Jaccard coefficient. In addition, the stability of the clustering results was assessed through the average proportion of falsely selected rows and columns. Details on the validation indices, the remaining biclustering algorithms and their relation to the SVD are provided in the Supplementary Material.
Scenario 1The simulation results of the first scenario are shown in. For low noise levels up to  = 0.3, all biclustering algorithms except the SSVD show an almost perfect performance with relevance and recovery scores mostly equal to one and no falsely selected rows and columns. For noise levels of 0.1 to 0.7, all biclusters proposed by the SSVD algorithm are too large and on average a proportion around 0.015 of the rows and 0.012 of the columns are falsely assigned. This results in relevance and recovery scores around 0.8. In case of larger noise levels, the SSVD algorithm often fails to converge and thus the relevance scores and the number of falsely assigned rows and columns approach zero. For noise levels above 0.3, the first bicluster detected by the Plaid Model usually consists of a strict subset of those rows and columns that belong to the true artificial bicluster in the data. Thus, the performance of the Plaid Model regarding the relevance and the recovery decreases with noise. Furthermore, the algorithm starts to fit the noise and proposes a number of further small biclusters. This explains why the relevance score is inferior compared with the recovery score. Most of these small biclusters correspond to parts of the true artificial bicluster and hence the proportions of falsely assigned rows and columns are close to zero. Beginning with a noise level of 0.5, the ISA proposes an increasing number of biclusters of which only one shows a strong agreement with the true bicluster. Even after applying the additional filtering functions available in the isa2 R-package (), some nonsense biclusters remain. Thus, both scores start to decrease with noise but are superior to the Plaid Model. The number of falsely assigned rows and columns increases with the noise level indicating that some of the detected biclusters correspond to fitted noise. Regardless of the noise level, the S4VD algorithm always detects a single bicluster that agrees with the true bicluster. For noise levels above 0.6, the proposed bicluster becomes smaller and represents only a part of the true bicluster. Therefore, both scores start to decrease with noise but are superior to that of all other biclustering methods considered in the simulation study.
Scenario 2The results of the second part of the simulation study are shown in.
M.Sill et al.The Plaid Model algorithm in some cases perfectly revealed the hidden structure, but in other situations depending on the randomly chosen starting values and the noise level, the algorithm falsely assigns rows and columns to the biclusters. The stopping criterion of the algorithm depends on a permutation test that can fail to reject unimportant biclusters that correspond to noise. On the other hand for higher noise levels, the permutation test also tends to reject biclusters early in the fitting process so that only three or less biclusters are detected. Thus, the resulting relevance and recovery scores are highly variable and decrease with noise. Regarding low noise levels, the SSVD algorithm mostly identifies the correct biclusters but usually falsely assigns some additional rows and columns. This behavior maintains for higher noise levels, but additionally the number of correctly identified biclusters becomes less. For noise levels above 0.7, both the SSVD algorithm and the Plaid Model mostly do not detect any of the artificial biclusters and hence the average proportions of falsely assigned rows and columns approach zero. The performance of the ISA decreases due to an increasing number of identified irrelevant biclusters, starting with noise levels above 0.2. For noise level 0.5, the medians of both similarity scores are around 0.5, and the relevance scores show a high variability. For noise levels above 0.5, the two embedded biclusters generated to have a constant up-and downregulation of 0.5 and 0.5 are masked by noise, and hence, the ISA as well as the S4VD algorithm tend to miss these clusters. This results in a slight increase of their relevance scores while the recovery scores decrease. Moreover, the relevance scores for both algorithms show a high variability at noise level 0.6. In summary, the S4VD algorithm outperforms all other biclustering algorithms considered in the simulation study regarding the relevance and the recovery of the artificial biclusters for all simulation scenarios. Furthermore, due to the stability selection the S4VD algorithm rarely assigns false rows and columns to the proposed bicluster and does not detect any additional nonsense clusters. Thus for all simulation scenarios, the average proportions of falsely assigned rows and columns are close to zero.
DISCUSSION AND CONCLUSIONIn this article, we propose a new biclustering algorithm that combines the SSVD algorithm suggested bywith the stability selection of Meinshausen and Bhlmann (2010). In brief, the model selection-based parameter tuning of the penalized regression models of the SSVD algorithm is replaced by a subsampling-based variable selection that controls Type I error rates. The S4VD approach here presented allows to control the degree of sparsity of the resulting SVD-layers by choosing desired Type I error levels. The stability selection estimates the selection probabilities of the rows and columns to belong to a bicluster. Depending on the chosen Type I error levels, the results are robust biclusters represented by rows and columns that have high selection probabilities. If the noise level is getting too high, the stopping criterion leads to an interruption of the S4VD algorithm preventing from fitting additional SVD-layers that correspond to noise. So far, the S4VD method is the only biclustering approach that takes the cluster stability regarding perturbations of the data into account. We applied the S4VD algorithm to evaluate a lung cancer microarray dataset and showed that the resulting biclusters represent tumor subclasses together with coregulated genes. Marker genes for the different tumor subclasses showed high selection probabilities in the respective biclusters. In addition, a gene set enrichment analysis revealed that the genes associated with identified biclusters belong to significantly enriched cancerrelated GO terms. In a simulation study, the S4VD algorithm was compared with the SSVD algorithm, the improved Plaid Model () and the ISA (). The S4VD algorithm showed the best performance regarding the recovery of biclusters and was more robust to noisy data compared with the other methods. The subsampling steps of the stability selection make the S4VD algorithm computationally very demanding. However, an improvement that strongly reduces the computation time is presented in the Supplementary Material.
Conflict of Interest:none declared.
The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com
at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from
