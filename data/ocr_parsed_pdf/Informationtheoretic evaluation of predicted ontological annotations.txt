BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

W.T.Clark and P.Radivojac

 

ontology to probabilistically model, via a Bayesian network, the
prior distribution of protein experimental annotation. We then
apply our metric to three protein function prediction algorithms
selected to highlight the limitations of typically considered evalu—
ation metrics. We show that our metrics provide added value
to the current analyses of the strengths and weaknesses of com—
putational tools. Finally, we argue that our framework is prob—
abilistically well founded and show that it can also be used to
augment already existing evaluation metrics.

2 BACKGROUND

The issue of performance evaluation is closely related to the
problems of measuring similarity between pairs of graphs or
sets. First, we note that a protein’s annotation (experimental or
predicted) is a graph containing a subset of nodes in the ontology
together with edges connecting them. We use the term leaf node
to describe a node that has no descendants in the annotation
graph, although it is allowed to have descendants in the ontol—
ogy. A set of leaf terms completely describes the annotation
graph.

We roughly group both graph similarity and performance
evaluation metrics into topological and probabilistic categories
and note that a particular metric may combine aspects from
both. More elaborate distinctions are provided by Guzzi et a].
(2012) and Pesquita et a]. (2009). Topological metrics rely on the
structure of the ontology to perform evaluation and typically use
metrics that operate on sets of nodes and/ or edges. A number of
topological measures have been used, including the Jaccard and
cosine similarity coefficients (the cosine approach initially maps
the binary term designations into a vector space), the shortest
path—based distances (Rada et al., 1989) and so forth. In the
context of classiﬁer performance analysis, two common 2D met—
rics are the precision/recall curve and the Receiver Operating
Characteristic (ROC) curve. Both curves are constructed based
on the overlap in either edges or nodes between true and pre—
dicted terms and have been widely used to evaluate the perform—
ance of tools for the inference of GO annotations. They can also
be used to provide a single statistic to rank classiﬁers through the
maximum F—measure in the case of precision/recall curve or the
area under the ROC curve. The area under the ROC curve has a
limitation arising from the fact that the ontology is relatively
large, but that the number of terms associated with a typical
protein is relatively small. In practice, this results in speciﬁcities
close to one, regardless of the prediction, as long as the number
of predicted terms is relatively small.

Although these statistics provide good feedback regarding
multiple aspects of a predictor’s performance, they do not
always address node dependency or the problem of unequal spe—
ciﬁcity of functional annotations found at the same depth of the
graph. Coupled with a large bias in the distribution of terms
among proteins, prediction methods that simply learn the prior
distribution of terms in the ontology could appear to have better
performance than they actually do.

The second class of similarity/performance measures is prob—
abilistic or information—theoretic metrics. Such measures assume
an underlying probabilistic model over the ontology and use a
database of proteins to learn the model. Similarity is then as—
sessed by measuring the information content of the shared terms

in the ontology but can also take into account the information
content of the individual annotations. Unlike with topological
measures where updates to the ontology affect similarity between
objects, information—theoretic measures are also affected by
changes in the underlying probabilistic model even if the struc—
ture of the ontology remains the same.

Probabilistic metrics closely follow and extend the method—
ology laid out by Resnik (1995), which is based on the notion
of information content between a pair of individual terms. These
measures overcome biases related to the structure of the ontol—
ogy; however, they have several drawbacks of their own. One
that is especially important in the context of analyzing the per—
formance of a predictor is that they only report a single statistic,
namely, the similarity or distance between two terms or sets of
terms. This ignores the tradeoff between precision and recall that
any predictor has to make. In the case of Resnik’s metric, a
prediction by any descendant of the true term will be scored as
if it is an exact prediction. Similarly, a shallow prediction will be
scored the same as a prediction that deviates from the true path
at the same point, regardless of how deep the erroneous predic—
tion might be. Although some of these weaknesses have been
corrected in subsequent work (Jiang and Conrath, 1997; Lin,
1998; Schlicker et al., 2006), there remains the issue that the
available probabilistic measures of semantic similarity resort to
ad hoe solutions to address the common situation where proteins
are annotated by graphs that contain multiple leaf terms (Clark
and Radivojac, 2011). Various approaches have been taken,
including averaging between all pairs of leaf terms (Lord et al.,
2003), finding the maximum among all pairs (Resnik, 1999) or
finding the best—match average, but each such solution lacks
strong justiﬁcation in general. For example, all—pair averaging
leads to anomalies where the exact prediction of an annotation
containing a single leaf term 14 would be scored higher than the
exact prediction of an annotation containing two distinct leaf
terms 14 and v of equal information content, when it is more
natural to think that the latter prediction should be scored
higher. Finally, certain semantic similarity metrics that incorpor—
ate pairwise matching between leaf terms tacitly assume that the
objects to be compared are annotated by similar numbers of leaf
terms. As such, they could produce undesirable solutions when
applied to a wide range of prediction algorithms such as those
outputting a large number of predicted terms.

3 METHODS

Our objective here is to introduce information-theoretic metrics for eval-
uating classiﬁcation performance in protein function prediction. In this
learning scenario, the input space X represents proteins, whereas the
output space 31 contains directed acyclic graphs describing protein func-
tion according to GO. Because of the hierarchical nature of GO, both
experimental and computational annotations need to satisfy the consist-
ency requirement, i.e. if an object X e X is assigned a node (functional
term) v from the ontology, it must also be assigned all of the ancestors of
v up to the root(s). Therefore, the task of a classiﬁer is to assign the best
consistent subgraph of the ontology to each new protein and output a
prediction score for this subgraph and/or each predicted term.

We only consider consistent subgraphs as descriptions of function and
simplify the exposition by referring to such graphs as prediction or an-
notation graphs. In addition, we frequently treat consistent graphs as sets
of nodes or functional terms and use set operations to manipulate them.

 

i54

ﬁm'spzumofpmjxo'sopeuuopuorq/ﬁdnq

Information -theoretic evaluation

 

We now proceed to provide a deﬁnition for the information content of
a (consistent) subgraph in the ontology. Then, using this deﬁnition, we
derive information-theoretic performance evaluation metrics for compar-
ing pairs of graphs.

3.1 Calculating the information content of a graph

Let each term in the ontology be a binary random variable and consider a
ﬁxed but unknown probability distribution over X and 31 according to
which the quality of a prediction process will be evaluated. We shall
assume that the prior distribution of a target can be factorized according
to the structure of the ontology, i.e. we assume a Bayesian network as the
underlying data generating process for the target variable. According to
this assumption, each term is independent of its ancestors, given its par-
ents and, thus, the full joint probability can be factorized as a product of
individual terms obtained from the set of conditional probability tables
associated with each term (Koller and Friedman, 2009). Here, we are only
interested in marginal probabilities that a protein is experimentally asso-
ciated with a consistent subgraph Tin the ontology. This probability can
be expressed as

MD=HHmmm m
VET
where v denotes a node in a graph and 73(v) is the set of parent nodes of v.
Here, Equation (1) can be derived from the full joint factorization by ﬁrst
marginalizing over the leaves of the ontology and then moving towards
the root(s) for all nodes not in T.

The information content of a subgraph can be thought of as the
number of bits of information one would receive about a protein if it
were annotated with that particular subgraph. We calculate the informa-
tion content of a subgraph T in a straightforward manner as

l
KT) =10gm

and use a base 2 logarithm as a matter of convention. The information
content of a subgraph Tcan now be expressed by combining the previous
two equations as

. l
"D {MW

veT

=me

veT

where, to simplify the notation, we use ia(v) to represent the negative
logarithm of Pr(v|73(v)). Term ia(v) can be thought of as the increase,
or accretion, of information obtained by adding a child term to a parent
term, or set of parent terms, in an annotation. We will refer to ia(v) as
information accretion (perhaps information gain would be a better term,
but because it is frequently used in other applications to describe an
expected reduction in entropy, we avoid it in this situation).

A simple ontology containing ﬁve terms together with a conditional
probability table associated with each node is shown in Figure 1A.
Because of the graph consistency requirement, each conditional probabil-
ity table is limited to a single number. For example, at node I; in the
graph, the probability Pr(b 2 Ha = l) is the only one necessary because
Pr(b = Ola = l) = l — Pr(b 2 Ha =1) and because Pr(b 2 Ha = 0) is
guaranteed to be 0. In Figure 1B, we show a sample dataset of four
proteins functionally annotated according to the distribution deﬁned by
the Bayesian network. In Figure 1C, we show the total information con-
tent for each of the four annotation graphs.

3.2 Comparing two annotation graphs

We now consider a situation in which a protein’s true and predicted
function is represented by graphs T and P, respectively. We deﬁne two
metrics that can be thought of as the information-theoretic analogs of

 

Pr(d|bc) = 1/2

Pr(elc) = ‘A i(ac) 2

Fig. 1. An example of an ontology, dataset and calculation of information
content. (A) An ontology viewed as a Bayesian network together with a
conditional probability table assigned to each node. Each conditional
probability table is limited to a single number owing to the consistency
requirement in assignments of protein function. Information accretion
calculated for each node, e.g. ia(e) = —logPr(e|c) = 2, are shown in
gray next to each node. (B) A dataset containing four proteins whose
functional annotations are generated according to the probability distri-
bution from the Bayesian network. (C) The total information content
associated with each protein found in panel (B); e.g. i(ace) = ia(a)+
ia(c) + ia(e) = 2. Note that i(ab) = l and i(abcde) = 4, although proteins
with such annotation have not been observed in part (B)

recall and precision and refer to them as remaining uncertainty and mis-
information, respectively.

DEFINITION l. The remaining uncertainty about the protein’s true anno-
tation corresponds to the information about the protein that is not yet
provided by the graph P. More formally, we express the remaining un-
certainty (ru) as

ru(T, P) = Z ia(v)
veTiP
which is simply the total information content of the nodes in the ontology
that are contained in true annotation T, but not in the predicted anno-
tation P. In a slight abuse of notation, we apply set operations to graphs
to manipulate only the vertices of these graphs.

DEFINITION 2. The misinformation introduced by the classiﬁer corres-
ponds to the total information content of the nodes along incorrect
paths in the prediction graph P. More formally, the misinformation is
expressed as

mi(T, P) = Z ia(v),
vePiT
which quantiﬁes how misleading a predicted annotation is.

Here, a perfect prediction (one that achieves P = T) leads to
ru(T, P) = 0 and mi(T, P) = 0. However, both ru(T, P) and mi(T, P)
can be inﬁnite in the limit. In practice, though, ru(T, P) is bounded by
the information content of the particular annotation, whereas mi( T, P) is
only limited by the particular annotations a predictor chooses to return.

To illustrate calculation of remaining uncertainty and misinformation,
in Figure 2, we show a sample ontology where the true annotation of a
protein T is determined by the two leaf terms t1 and t2, whereas the pre-
dicted subgraph P is determined by the leaf terms 17 1 and [72. The remaining
uncertainty ru(T, P) and misinformation mi(T, P) can now be calculated
by adding the information accretion corresponding to the nodes circled in
gray.

Finally, this framework can be used to deﬁne the similarity between
the protein’s true annotation and the predicted annotation without rely-
ing on identifying an individual common ancestor between pairs of leaves
(this node is usually referred to as the maximum informative common

 

i55

ﬁm'spzumofpmjxo'sopnuuoonIq/ﬁdnq

:39\Ewowsmoaﬁmowoxmoagoﬁsambwﬁ

Information -theoretic evaluation

 

3.4.1 Information—theoretic weighted formulation The deﬁnition of
information accretion and the use of a probabilistic framework deﬁned
by the Bayesian network enables the straightforward application of in-
formation accretion to weight each term in the ontology. Therefore, it is
easy to generalize the deﬁnitions of precision and recall from the previous
section into a weighted formulation. Here, weighted precision and
weighted recall can be expressed as

Z ia(v)
wpr(T, P (1')) = %
\‘EP(T)
and
Z ia(v)
wrc(T, PU» = WE+M

Weighted precision wpr(r) and recall wrc(r) can then be calculated as
weighted averages over the database of proteins, as in Equations (4)
and (5).

4 EXPERIMENTS AND RESULTS

In this section, we ﬁst analyze the average information content in
a dataset of experimentally annotated proteins and then evaluate
performance accuracy of different function prediction methods
using both topological and probabilistic metrics. Each experi—
ment was conducted on all three categories of the GO:
Molecular Function (MFO), Biological Process (BPO) and
Cellular Component (CCO) ontologies. To avoid cases where
the information content of a term is inﬁnite, a pseudo—count of
one was added to each term, and the total number of proteins in
the dataset was incremented when calculating term frequencies.

4.1 Data, prediction models and evaluation

We first collected all proteins with GO annotations supported by
experimental evidence codes (EXP, IDA, IPI, IMP, IGI, IEP,
TAS, IC) from the January 2011 version of the Swiss—Prot data—
base (29 699 proteins in MFO, 31608 in BPO and 30 486 in
CCO). We then generated three simple function annotation
models: Naive, BLAST and GOtcha, to assess the ability of per—
formance metrics to accurately reﬂect the quality of a predicted
set of annotations. In addition to these three methods, we gen—
erated another set of ‘predictions’ by collecting experimental an—
notations for the same set of proteins from a database generated
by the GO Consortium released at about the same time as our
version of Swiss—Prot. This was done to quantify the variability
of experimental annotation across different databases using the
same set of metrics. In addition, this comparison can be used to
estimate the empirical upper limit of prediction accuracy because
the observed performance is limited by the noise in experimental
data. All computational methods were evaluated using 10—fold
cross—validation.

The Naive model was designed to reﬂect biases in the distri—
bution of terms in the dataset and was the simplest annotation
model we used. It was generated by ﬁrst calculating the relative
frequency of each term in the training dataset. This value was
then used as the prediction score for every protein in the test set;
thus, every protein in the test partition was assigned an identical
set of predictions over all functional terms. The performance of
the Naive model reﬂects what one could expect when annotating
a protein with no knowledge about that protein.

The BLAST model was generated using local sequence identity
scores to annotate proteins. Given a target protein sequence x,
a particular functional term v in the ontology, and a set of
sequences SV 2 {51,52, ...} annotated with term v, we determine
the BLAST predictor score for function v as max{sid(x,s):
s e S}, where sid(x,s) is the maximum sequence identity re—
turned by the BLAST package (Altschul et al., 1997) when the
two sequences are aligned. We chose this method to mimic the
performance one would expect if they simply used BLAST to
transfer annotations between similar sequences.

The third method, GOtcha (Martin et al., 2004), was selected
to incorporate not only sequence identity between protein se—
quences but also the structure of the ontology (technically,
BLAST also incorporates structure of the ontology but in a rela—
tively trivial manner). Speciﬁcally, given a target protein x, a
particular functional term v, and a set of sequences
SV 2 {51,52, ...} annotated with function v, one ﬁrst determines
the r—score for function v as rv = e — Z‘Yes‘log(e(x,s)), where
e(x,s) represents the E—value of the alignment between the
target sequence x and sequence 5, and e=2 is a constant
added to the given quantity to ensure all scores were above 0.
Given the r—score for function v, i—scores were then calculated by
dividing the r—score of each function by the score for the root
term iv = rv/rrom. As such, GOtcha is an inexpensive and robust
predictor of function.

4.2 Average information content of a protein

We first examined the distribution of the information content per
protein for each of the three ontologies (Fig. 3). We observe a
wide range of information contents in all ontologies, reaching
over 128 bits in case of BPO (which corresponds to a factor of
128 in the probability of observing particular annotation graphs).
The distributions for MFO and CCO show unusual peaks for
low information contents, suggesting that a large fraction of an—
notation graphs in these ontologies are low quality. One such
anomaly is created by the term ‘binding’ in MFO that is asso—
ciated with 72% of proteins. Furthermore, 41% of proteins are
annotated with its child ‘protein binding’ as a leaf term, and 26%
are annotated with it as their sole leaf term. Such annotations,
which are clearly a consequence of high—throughput experiments,
present a signiﬁcant difﬁculty in method evaluation.

Previously, we showed that the distribution of leaf terms in
protein annotation graphs exhibits scale—free tendencies (Clark
and Radivojac, 2011). Here, we also analyzed the average
number of leaf terms per protein and compared it with the in—
formation content of that protein. We estimate the average
number of leaf terms to be 1.6 (std. 1.0), 3.0 (std. 3.6) and 1.6
(std. 1.0) for MFO, BPO and CCO, respectively, and calculate
Pearson correlation between the information content and the
number of leaf terms for a protein (0.80, 0.92 and 0.71). Such
high level of correlation suggests that proteins annotated with a
small number of leaf terms are generally annotated by shallow
graphs. This is particularly evident in the case of ‘protein bind—
ing’ annotations that can be derived from yeast—2—hybrid experi—
ments but provide little insight into the functional aspects
of these complexes when only viewed as GO annotations.
We believe the wide range of information contents coupled

 

i57

ﬁm'spzumofpmjxo'sopnuuoonIq/ﬁdnq

:39\Ewowsmoaﬁmowoxmoagoﬁsambwﬁ

an?kgogmomammowoio~&o:3m7.omm\

 

W.T.Clark and P.Radivojac

 

Table 1. Performance evaluation of several information-theoretic and topological metrics

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Molecular Function Biological Process Cellular Component
Lord et a]. (2003) Max Threshold ru mi Max Threshold ru mi Max Threshold ru mi
GOtcha 2.34 0.47 6.34 3.20 1.95 0.40 23.36 11.90 1.80 0.36 5.88 4.58
BLAST 1.61 0.43 4.69 27.90 1.40 0.43 16.73 139.57 1.27 0.38 4.42 37.24
Naive 0.46 0.09 9.56 4.23 0.63 0.01 10.35 504.88 0.75 0.07 5.81 16.34
Lin (1998) Max Threshold ru mi Max Threshold ru mi Max Threshold ru mi
GOtcha 0.44 0.52 6.67 2.67 0.26 0.46 24.43 9.40 0.41 0.50 6.71 2.76
BLAST 0.22 0.43 4.69 27.90 0.16 0.43 16.73 139.57 0.23 0.40 4.78 30.45
Naive 0.37 0.30 10.39 0.21 0.12 0.12 24.92 23.14 0.26 0.31 8.98 1.32
Schlicker et a]. (2006) Max Threshold ru mi Max Threshold ru mi Max Threshold ru mi
GOtcha 0.29 0.51 6.60 2.76 0.23 0.42 23.73 10.99 0.30 0.43 6.31 3.56
BLAST 0.17 0.44 4.83 25.39 0.14 0.43 16.73 139.57 0.18 0.43 5.26 23.26
Naive 0.14 0.30 10.39 0.21 0.08 0.12 24.92 23.14 0.13 0.31 8.98 1.32
Jiang and Conrath (1997) Min Threshold ru mi Min Threshold ru mi Min Threshold ru mi
GOtcha 5.74 0.75 8.20 1.27 8.38 0.98 30.88 1.22 4.83 0.76 8.21 1.19
BLAST 6.34 1.00 10.62 0.43 8.39 1.00 31.31 1.40 5.20 1.00 10.16 0.35
Naive 6.19 0.63 10.53 0.13 8.24 0.50 31.75 0.07 5.01 0.61 10.13 0.08
Jaccard Max Threshold ru mi Max Threshold ru mi Max Threshold ru mi
GOtcha 0.57 0.46 6.29 3.32 0.31 0.34 22.24 15.24 0.56 0.43 6.31 3.56
BLAST 0.37 0.50 5.74 14.72 0.19 0.50 19.68 76.98 0.34 0.43 5.26 23.26
Naive 0.46 0.30 10.39 0.21 0.17 0.19 27.53 9.22 0.47 0.31 8.98 1.32
me Max Threshold ru mi Max Threshold ru mi Max Threshold ru mi
GOtcha 0.72 0.43 6.12 3.68 0.49 0.32 21.84 16.69 0.73 0.43 6.31 3.56
BLAST 0.64 0.48 5.42 17.89 0.49 0.50 19.68 76.98 0.63 0.45 5.57 19.42
Naive 0.60 0.29 9.87 1.44 0.33 0.19 27.53 9.22 0.64 0.33 9.22 0.80
S; Min Threshold ru mi Min Threshold ru mi Min Threshold ru mi
GOtcha 7.11 0.47 6.34 3.20 26.14 0.43 23.91 10.56 7.23 0.46 6.48 3.21
BLAST 9.13 0.77 8.25 3.90 29.89 0.88 28.28 9.69 9.08 0.78 8.51 3.15
Naive 9.98 0.10 9.72 2.80 29.00 0.22 27.67 8.72 8.79 0.21 7.71 4.95

 

 

 

Note: For each measure, the decision threshold was varied across the entire range of predictions to obtain the maximum or minimum value (shown in column 1). The threshold
at which each method reached the best value is shown in column 2. Columns 3 and 4 show the remaining uncertainty (m) and misinformation (mi) calculated according to the
Bayesian network. Each semantic similarity metric was calculated according to the relative frequencies of observing each term in the database.

5 DISCUSSION

In this work, we propose an information—theoretic framework
for evaluating the performance of computational protein func—
tion prediction. We frame protein function prediction as a struc—
tured—output learning problem in which the output space is
represented by consistent subgraphs of the GO graph. We
argue that our approach directly addresses evaluation in cases
where there are multiple true and predicted (leaf) terms asso—
ciated with a protein by taking the structure of the ontology
and the dependencies between terms induced by a hierarchical
ontology into account. Our method also facilitates accounting
for the high level of biased and incomplete experimental anno—
tations of proteins by allowing for the weighting of proteins
based on the information content of their annotations. Because

we maintain an inforInation—theoretic foundation, our approach
is relatively immune to the potential dissociation between the
depth of a term and its information content, a weakness of
often—used topological metrics in this domain such as precision/
recall or ROC—based evaluation. At the same time, because we
take a holistic approach to considering a protein’s potentially
large set of true or predicted functional associations, we resolve
many of the problems introduced by the practice of aggregating
multiple pairwise similarity comparisons common to existing se—
mantic similarity measures.

Although there is a long history (Resnik, 1999) and a signiﬁ—
cant body of work in the literature regarding the use of semantic
similarity measures (Guzzi et al., 2012; Pesquita et al., 2009), to
the best of our knowledge, all such metrics are based on single

 

i60

ﬁm'spzumol‘pmjxo'sopnuuoonIq/ﬁdnq

Information -theoretic evaluation

 

statistics and are unable to provide insight into the levels of re—
maining uncertainty and misinformation that every predictor is
expected to balance. Therefore, the methods proposed in this
work extend, modify and formalize several useful information—
theoretic metrics introduced during the past decades. In addition,
both remaining uncertainty and misinformation have natural in—
forInation—theoretic interpretations and can provide meaningful
information to the users of computational tools. At the same
time, the semantic distance based on these concepts facilitates
not only the use of a single performance measure to evaluate
and rank predictors but can also be exploited as a loss function
during training.

One limitation of the proposed approach is grounded in the
assumption that a Bayesian network, structured according to the
underlying ontology, will perfectly model the prior probability
distribution of a target variable. An interesting anomaly with
this approach is that the marginal probability, and subsequently
the information content, of a single term (i.e. consistent graph
with a single leaf term) calculated from a Bayesian network
does not necessarily match the relative term frequency in the data—
base (instead, the conditional probability tables are estimated as
relative frequencies). Ad hoe solutions that maintain the term
information content are possible but would result in sacriﬁced
interpretability of the metric itself. One such solution can be ob—
tained via a recursive deﬁnition ia(v) = i(v) — Zuepm ia(u) and
ia(root) = 0, where i(v) is estimated directly from the database.

Finally, rationalizing between evaluation metrics is a difﬁcult
task. The literature presents several strategies where protein se—
quence similarity, proteiniprotein interactions or other data are
used to assess whether a performance metric behaves according
to expectations (Guzzi et al., 2012). In this work, we took a
somewhat different approach and showed that the demonstrably
biased protein function data can be shown to provide surprising
results with well—understood prediction algorithms and conven—
tional evaluation metrics. Thus, we believe that our experiments
provide evidence of the usefulness of the new evaluation metric.

ACKNOWLEDGEMENT

The authors thank Prof. David Crandall for his comments on the
manuscript, Prof. Iddo Friedberg for stimulating discussions
about semantic similarity measures and four anonymous
reviewers for their suggestions that improved the quality of this
study.

Funding: This work was supported by the National Science
Foundation grant DBI—0644017 and National Institutes of
Health grant R01 LM009722—06A1.

Conflict of Interest: none declared.

REFERENCES

Alterovitz,G. et a]. (2010) Ontology engineering. Nat. Biotechnol., 28, 1287130.

Altschul,S.F. et a]. (1997) Gapped BLAST and PSI—BLAST: a new generation of
protein database search programs. NM?I€i(? Acids Res., 25, 338973402.

Ashburner,M. et a]. (2000) Gene ontology: tool for the uniﬁcation of biology. The
gene ontology consortium. Nat. Genet, 25, 2&29.

Clark,W.T. and Radivojac,P. (2011) Analysis of protein function and its prediction
from amino acid sequence. Proteins, 79, 208(rZO96.

Guzzi,P.H. et a]. (2012) Semantic similarity analysis of protein data: assessment with
biological features and issues. Brief. Bioinform., 13, 5697585.

Jiang,J.J. and Conrath,D.W. (1997) Semantic similarity based on corpus statistics
and lexical taxonomy. In: Proceedings of the International Conference on
Research in Computational Linguistics. Taiwan. pp. 1%33.

Koller,D. and Friedman,N. (2009) Probabilistic Graphical Models. The MIT Press,
Cambridge, MA.

Lin,D. (1998) An information—theoretic deﬁnition of similarity. In: Proceedings
of the 15th International Conference on Machine Learning. Morgan Kaufmann,
San Francisco, CA, pp. 2967304.

Lord,P.W. et a]. (2003) Investigating semantic similarity measures across the Gene
Ontology: the relationship between sequence and annotation. Bioiry’ormatics, 19,
127571283.

Martin,D.M. et a]. (2004) GOtcha: a new method for prediction of protein function
assessed by the annomtion of seven genomes. BMC Bioinformatics, 5, 178.
Pesquita,C. et a]. (2009) Semantic similarity in biomedical ontologies. PLoS

Comput. Biol., 5, e1000443.

Rada,R. et a]. (1989) Development and application of a metric on semantic nets.
IEEE Trans. Syst. Man Cybern., 19, 17730.

Radivojac,P. et a]. (2013) A large—scale evaluation of computational protein func—
tion prediction. Nat. Methods, 10, 2217227.

Rentzsch,R. and Orengo,C. (2009) Protein function predictionithe power of
multiplicity. Trends Biotechnol., 27, 21(k219.

Resnik,P. (1995) Using information content to evaluate semantic similarity in a tax—
onomy. In: Proceedings of the 14111 International Joint Conference on Artiﬁcial
Intelligence, Morgan Kaufmann, San Francisco, CA, pp. 448453.

Resnik,P. (1999) Semantic similarity in a taxonomy: an information—based measure
and its application to problems of ambiguity in natural language. J. Artif. Intel].
Res., 11, 957130.

Robinson,P.N. and Bauer,S. (2011) Introduction to Bio—Ontologies. CRC Press,
Boca Raton, FL, USA.

Schlicker,A. et a]. (2006) A new measure for functional similarity of gene products
based on gene ontology. BMC Bioiry’ormatics, 7, 302.

Sharan,R. et a]. (2007) Network—based prediction of protein function. Mo]. Syst.
Biol., 3, 88.

Verspoor,K. et a]. (2006) A categorization approach to automated ontological func—
tion annotation. Protein Sci., 15, 15444549.

 

i61

ﬁm'spzumol‘pmjxo'sopnuuoonIq/ﬁdnq

