Bioinformatics, 31 (24), 2015, 3881—3889

doi: 10.1093/bioinformatics/btv483

Advance Access Publication Date: 26 August 2015
Original Paper

 

 

Genome analysis

Fast and accurate approximate inference of
transcript expression from RNA-seq data

James Hensman1'*"’, Panagiotis Papastamoulisz'*"’, Peter Glaus3,
Antti Honkela4 and Magnus Rattray2'*

1Sheffield Institute for Translational Neuroscience (SlTraN), Sheffield, UK, 2Faculty of Life Sciences, 3School of
Computer Science, The University of Manchester, Manchester, UK and 4Helsinki Institute for Information
Technology (HIIT), Department of Computer Science, University of Helsinki, Helsinki, Finland

*To whom correspondence should be addressed.
TThe authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.
Associate Editor: Ivo Hofacker

Received on January 23, 2015; revised on August 3, 2015; accepted on August 7, 2015

Abstract

Motivation: Assigning RNA—seq reads to their transcript of origin is a fundamental task in transcript
expression estimation. Where ambiguities in assignments exist due to transcripts sharing se—
quence, e.g. alternative isoforms or alleles, the problem can be solved through probabilistic infer—
ence. Bayesian methods have been shown to provide accurate transcript abundance estimates
compared with competing methods. However, exact Bayesian inference is intractable and approxi—
mate methods such as Markov chain Monte Carlo and Variational Bayes (VB) are typically used.
While providing a high degree of accuracy and modelling flexibility, standard implementations can
be prohibitively slow for large datasets and complex transcriptome annotations.

Results: We propose a novel approximate inference scheme based on VB and apply it to an exist—
ing model of transcript expression inference from RNA—seq data. Recent advances in VB algorith—
mics are used to improve the convergence of the algorithm beyond the standard Variational Bayes
Expectation Maximization algorithm. We apply our algorithm to simulated and biological datasets,
demonstrating a significant increase in speed with only very small loss in accuracy of expression
level estimation. We carry out a comparative study against seven popular alternative methods and
demonstrate that our new algorithm provides excellent accuracy and inter—replicate consistency
while remaining competitive in computation time.

Availability and implementation: The methods were implemented in R and C++, and are available
as part of the BitSeq project at github.com/BitSeq. The method is also available through the BitSeq
Bioconductor package. The source code to reproduce all simulation results can be accessed via
github.com/BitSeq/BitSquB_benchmarking.

Contact: james.hensman@sheffield.ac.uk or panagiotis.papastamoulis@manchester.ac.uk

or Magnus.Rattray@manchester.ac.uk

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 IntrOducuon alleles of the same genes or from closely related homologous genes,

RNA—seq is a technology with the potential to identify and quantify and consequently they may share much of their primary sequence.

all mRNA transcripts in a biological sample (Mortazavi et (11., Currently, popular RNA—seq technologies generate short reads that
2008). Some of these transcripts come from different isoforms or must be aligned to the genome or transcriptome to quantify
©The Author 2015. Published by Oxford University Press. 3881

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/Iicenses/by/4.U/), which permits
unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

112 ﬁle'slvumofqutxo"sotJBuiJOJutotq/ﬁduq 11101} popcolumoq

91oz ‘Og anﬁnv 110 ::

3882

J. Hensman et aI.

 

expression levels. In some cases the observed reads could originate
from several different transcripts and there may be few reads that
are useful to distinguish these transcripts. It is therefore a challeng—
ing statistical problem to uncover the expression levels of closely
related transcripts. A recent assessment confirms this by showing
significant variability between results obtained using different com—
putational pipelines (SEQC/MAQC—III Consortium, 2014).
Probabilistic latent variable models, in particular mixture models
(Jiang and Wong, 2009; Glaus et (11., 2012; Katz et (11., 2010; Li and
Dewey, 2011; Li et (11., 2010; Nariai et (11., 2013; Trapnell et (11.,
2013; Turro et (11., 2011) provide a popular and effective approach
for inferring transcript expression levels from RNA—seq data. Such
models can be used to deconvolve the signal in the read data, assign—
ing reads to alternative, pre—defined transcripts according to their
probability of originating from each. The term mixture model derives
from the interpretation of the data as being derived from a mixture
of different transcripts, the mixture components, with each read orig—
inating from one component. Although reads originate from only
one component they may map to multiple related components, result—
ing in some ambiguity in their assignment. Transcript expression lev—
els are model parameters (mixture component proportions) that have
to be inferred from the mapped read data. Due to their probabilistic
nature these models can fully account for multiple mapping reads,
complex biases in the sequence data, sequencing errors, alignment
quality scores and prior information on the insert length in paired—
end reads. Mixture models have been successfully applied to infer the
proportion of different gene isoforms or allelic variants in a particu—
lar sample (Jiang and Wong, 2009; Katz et (11., 2010; Turro et (11.,
2011), for inferring gene and isoform expression levels (Li et (11.,
2010; Li and Dewey, 2011; Mortazavi et (11., 2008; Roberts and
Pachter, 2013; Trapnell et (11., 2013) and for transcript—level differen—
tial expression calling (Glaus et (11., 2012; Trapnell et (11., 2013).
Inference in latent variable models such as these can be carried out
by maximum likelihood (ML) or Bayesian parameter estimation. In
ML the choice of parameters that maximizes the data likelihood is ob—
tained through a numerical optimization procedure. In the case of
mixture models a popular choice of algorithm is the Expectation
Maximization (EM) algorithm, as first applied to this model and ex—
pressed sequence tag data by Xing et al. (2006) and later to RNA—seq
data by Li et al. (2010). For Bayesian inference the most popular ap—
proach is Markov chain Monte Carlo (MCMC) and for the case of
mixture models a Gibbs sampler is most often used (Glaus et (11.,
2012; Katz et (11., 2010; Li and Dewey, 2011). An advantage of
Bayesian inference is that one obtains a posterior probability over the
model parameters rather than just a point estimate. This provides a
level of uncertainty in the inferred transcript expression levels as well
as information about the covariation between estimates for closely
related transcripts. The uncertainty information can be usefully propa—
gated into downstream analysis of the data, e.g. calling differentially
expressed transcripts from replicated experiments (Glaus et (11., 2012).
A Bayesian method, BitSeq, was proposed in which inference
was carried out using a collapsed Gibbs sampler (Glaus et (11.,
2012). The method was shown to perform well, especially for the
task of inferring the relative expression of different gene isoforms
and for ranking transcripts according to their probability of being
differentially expressed between conditions. However, for typical
modern RNA—seq datasets with hundreds of millions of read—pairs
the Gibbs sampler can be inconveniently slow, creating a computa—
tional bottleneck in applying a Bayesian approach. As the volume of
data continues to grow and gene models are becoming more com—
plex as more alternative transcripts are discovered, more efficient

inference algorithms are required so that Bayesian methods can be
used to provide practical computational tools.

An alternative approach to Bayesian inference is to use determin—
istic approximate inference algorithms such as Variational Bayes
(VB) (reviewed in Bishop, 2006). While MCMC algorithms are at—
tractive due to their asymptotic approximation guarantees, VB often
provides a much faster method to obtain a good approximation to
the posterior distribution. For models where Gibbs sampling can be
applied there is typically a closely related VB Expectation
Maximization (VBEM) algorithm. In this contribution, we show
how VB can be used to massively speed up inference in the BitSeq
model for transcript expression—level inference. We show that the
mean transcript expression level estimates are very close to those ob—
tained with MCMC. We use a recent formulation of VB (Hensman
et (11., 2012) which is shown to provide a greater speed up when
compared with a more standard VBEM algorithm. Our new algo—
rithm is implemented in the most recent version of the BitSeq, allow—
ing the method to be applied to much larger RNA—seq datasets in
equal computing time.

An alternative VB method, TIGAR, was recently proposed for the
same problem using a standard VBEM algorithm (Nariai et (11., 2013).
The assumptions made in our approximation are similar to those used
in TIGAR, but the empirical comparisons herein show that our pro—
posed method performs better in terms of computation time and
required memory, while also providing improved accuracy on real and
simulated data. The improvement in terms of reduced computational
cost is due to our adoption of a novel VB method. Furthermore, we in—
vestigate the effects of the variational assumption in this problem, and
compare empirically to results using the gold standard, MCMC.

The article is organized as follows. In Section 2, we review
the original BitSeq probabilistic model and describe our new infer—
ence algorithm, BitSquB, explaining the principles underlying our
improved optimization scheme. In Section 3, we benchmark our
new method against the original BitSeq algorithm and six popular
alternative methods using realistic simulated data and real human
RNA—Seq data. We consider accuracy in terms of expression estima—
tion, relative with—gene transcript proportions and between—replicate
consistency. We also compare the computation time required for all
methods and compare the new VB algorithm to more standard
MCMC and VBEM inference algorithms.

2 Methods

Our probabilistic model of RNA—seq follows Stage 1 of Glaus et al.
(2012), and is similar to that used by RSEM. We summarize our n0—
tation in Table 1. The probabilistic model is shown using standard
directed graphical notation in Figure 1. Here we have focused on the

Table 1. Summary of notations

 

N Number of reads in the dataset

M Number of transcripts in the transcriptome
r,, The nth read

R The collection of reads

T The transcriptome

Tm The mth transcript

0m Proportion of transcript Tm in the sample
znm Binary: znm = 1 if read 71 comes from transcript m
2,, Allocation vector of the nth read

Z Collection of all allocation vectors

¢nm Approximate posterior probability of znm = 1
yum Re—parameterization of ¢nm

 

112 ﬁle'slvumofquixo"soticuiJOJutotq/ﬁduq 11101} popcolumoq

91oz ‘Og isnﬁnv 110 ::

Fast and accurate approximate inference of transcript expression

3883

 

 

an T

0” (I) y
I
06

n=1...N

 

 

 

Fig. 1. Graphical model of the RNA-seq mixture problem. Given a known
Transcriptome T and some observed reads R, the inference problem is for 9
through the latent variables Z

mixture part of the analysis, assuming that the model which associ—
ates reads to transcripts [i.e. p(r,, I Tm)] is known. Following BitSeq
(Glaus et (11., 2012), we compute this part of the model a priori,
with parameters estimated from uniquely aligned reads. We consider
RNA—seq assays independently, computing an approximate poster—
ior for the transcript proportions 0 in each assay. Subsequent ana—
lysis such as differential expression can be done using the estimated
distributions of each assay.

2.1 The generative model

Transcript fragment proportions

The generative model for an RNA—seq assay is as follows. We as—
sume that the experiment produces of collection of RNA fragments,
where the abundance of fragments derived from transcript Tm in the
assay is 0,". Fragments are then sequenced in these proportions, so
that the prior probability of any fragment corresponding to tran—
script Tm is 0,". Introducing a convenient allocation vector 2,, for
each read, we can write

N M
17(Zl0) : H H02“, (1)

n:1 m:1

where znm E {0, 1} is a binary variable which indicates whether the
nth fragment came from the mth transcript (znm : 1) and is subject
to XIII/1:0 znm : 1. We use Z to represent the collection of all alloca—
tion vectors. We note that both 0 and Z are variables to be
inferred, with 0 the main object of interest. 0 can be transformed
later into some more convenient measure, for instance reads per
kilobase of length per million sequenced reads (RPKM) (Mortazavi
et (11., 2008), though it is more convenient from a probabilistic
point of view to work with 0 directly. The variables Z are some—
times known in the machine learning literature as latent variables.
Although not of interest directly, inference of these variables is es—
sential to infer 0.

Read model

An important part of the model is the likelihood term p(r,,ITm)
which is the probability of generating the nth read from the mth
transcript. Writing the collection of all reads as R : {rn}nN:1, the
likelihood given a set of alignments Z is

N M
p(RlT,Z) = H H Manner. (2)

n:1 m:1

where Tm represents the mth transcript and T represents the tran—
scriptome. The values of p(r,,ITm) for all alignments can be com—
puted before performing inference in 0 since we are assuming a
known transcriptome. For paired—end reads, the mates originate

from a single fragment and their likelihood is inferred jointly.

Denoting r” : (r21) , r9), the likelihood of alignment is computed as

Panic.» : P<Ile>P<plLTm> H P(r£:>lseqm,,), <3>
i:1,2

where l is the length of a fragment, p is its position and seqmjp
denotes the underlying reference sequence. The fragment length
distribution can be pre—defined or inferred empirically. The
position likelihood, P(le,Tm), can be either uniform or account
for different biases using an empirical model as in Glaus et al.
(2012). The last term, 11:12 P0”)? Iseqmlp) describes the probability
of observed read sequences based on quality scores and base dis—
crepancy between read and reference. For detailed description of
the alignment likelihood estimation please refer to Glaus et al.
(2012).

Identifying noisy reads

Our model is similar to previous work (Glaus et (11., 2012), but does
not contain a variable identifying reads as belonging to a ‘noise’
class. To circumvent the explicit formulation of a model with this
variable, we introduce a ‘noise transcript’ which we append to the
list of known transcripts. The generative probability of any read
from this transcript, p(r,,ITo), is again calculated according to the
model described in Glaus et al. (2012). Due to the conjugate rela—
tionships between the variables in our model and those of Glaus et
al. (2012), the models are the same, subject to a slight reformulation
of the prior parameters.

Prior over0

The final part of our model is to specify some prior belief in
the vector 0. To make our approximations tractable, it is necessary
to use a conjugate prior, which in this case is a Dirichlet

distribution
r a" M 0
17(0) 2 M(—) H 0:2"4 (4)
H rm.) “:1
m:1

where «Om represents our prior belief in the values of 0m and
56° : XIII/1:1 «Om. We use a weak but proper prior «Om : 1; m : 0 .. .
M which corresponds to a single ‘pseudo—count’ read (or read—pair)
for each transcript.

2.2 Approximate inference

We are interested in computing the posterior distribution for the
mixing proportions, p(0 I RT) 0( ZZp(R I T, Z)p(Z I 0)p(0). For
very small datasets, it is possible to perform exact Bayesian inference
in this model, however for any realistically sized problem, exact in—
ference is impossible due to the combinatorial explosion of the num—
ber of possible solutions. Our proposed solution is to use a collapsed
version of Variational Bayes (VB). VB involves approximating the
posterior probability density of all the model parameters with an—
other distribution q,

t1(0) Z) w PULZIRIT) (5)

The approximation is optimized by minimising the Kullback—Leibler
(KL) divergence between q(0,Z) and p(0,ZIR, T) (Bishop, 2006).
To make the VB approach tractable, some factorizations need to be
assumed in the approximate posterior. In the case of the current

112 ﬁle'slvumofquixo'soticuiJOJutotq/ﬁduq 11101} pQPBOIIIAAOG

9103 ‘Og isnﬁnv uo ::

3884

J. Hensman et al.

 

model, we assume that the posterior probability of the transcript
proportions factorizes from the alignments:

11(0) Z) = q(0)q(Z)- (6)

Further factorizations in q(Z) occur due to the simplicity of the
model, revealing q(Z) : Hull q(z,,). We write the approximate dis—
tribution for q(Z) using the parameters (pm:

N M
q(Z)=HH 2%- (7)

n:1 m:1

We need not introduce parameters for 11(0) since it will arise impli—
citly in our derivation in terms of <1).

The objective function

Approximate inference is performed by optimization: the param—
eters of the approximating distribution are changed so as to mini—
mize the KL divergence. Whilst the KL divergence is not
computable, it is possible to derive a lower bound on the marginal
likelihood, maximization of which minimizes the KL divergence (see
e.g. Bishop, 2006). Here we derive a lower bound which is depend—
ent only on the parameters of q(Z), with the optimal distribution for
q(0) arising implicitly for any given q(Z). First we construct a lower
bound on the conditional log probability of the reads R given the
transcript proportions 0 and the known transcriptome T:

1np(RIT, 0) :1nIp(RIZ,T)p(ZIO)dZ

N M (8)
I ZZ¢nm(IHP(rnITm) +1ﬂ0m -1n¢>nm)

n:1m:1

: £1 

where the first line follows from Jensen’s inequality in a similar fash—
ion to standard VB methods. We have denoted this conditional
bound £1 (0), which is still a function of 0. To generate a bound on
the marginal likelihood, p(RIT), we need to remove this depend—
ence on 0 which we do in a Bayesian fashion, by substituting £1 (0)
into the following Bayesian marginalization:

FWD 2 Ip<RlT,0>p<o>d 0
(9)
zIexp{£1(0)}p(0)d 0 .

Solving this integral and taking the logarithm gives us our final
bound which equates to

M:

N
1nP(RIT)Z£ = Z ¢>nm(lnP(ranm) -1n¢>nm)
n:1

l l
._.

m

M
+ln m0) — ln m0 + N) — Zﬂn my") — ln m; + (2%)),
m:1

(10)

where (2)," : 221:1 (pm and we also have that the approximate pos—
terior distribution for 0 is a Dirichlet distribution with parameters
atom + 

2.3 Optimization

Having established the objective function as a lower bound on the
marginal likelihood, all that remains is to optimize the variables of
the approximating distribution q(Z,0). The dimensionality of this

optimization is rather high and potentially rather difficult.
Optimization in standard VB is usually performed by an EM like al—
gorithm, which performs a series of convex optimizations in each of
the factorized variables alternately. In our formulation of the prob—
lem, we only need to optimize the parameters of the distribution
q(Z), which we do by a gradient—based method. Taking a derivative
of (10) with respect to the parameters (1) gives

3% 21np<rn le) —1n¢>,m — 1 + w; + in"), (11>

where 1/1 is the digamma function. To avoid constrained optimiza—
tion we re—parameterize (1) as y:

67m

2 : em!

m’:1

and it is then possible to optimize the variables y using a standard
gradient—based optimizer.

2.4 Geometry

Information geometry concerns the interpretation of statistical ob—
jects in a geometric fashion. Specifically, a class of probability distri—
butions behaves as a Riemannian manifold with curvature given by
the Fisher information. Amari (1998) showed that the direction of
the steepest descent on such a manifold is given by the natural
gradient:

vb: G-lvc, (13)

where G is the Fisher information matrix. Since we are performing
optimization of the distribution q(Z), we can make use of the nat—
ural gradient in computing a search direction (Honkela et (11., 2010).
For our problem, we assume that the N ><M matrix Z has been
transformed into a NM vector, and the Fisher information corres—
ponding to yum, ynlm, is given by

(pm — gm, ifn : n’andm : m’
GIm,n,m’,n’I : —¢>nm¢>nm,, ifn : n’ butm 9E m’ (14)
0, otherwise.

We note that this structure is block—diagonal, and that each block
can be easily inverted using the Sherman—Morrison identity, giving
an analytical expression for G‘1V£, and thus making the natural
gradient very fast to compute (see Hensman et al. (2015) for more
details). One can draw comparisons with a Newton method, where
G would be replaced with a Hessian, though in the proposed case
the system is much cheaper to compute.

The optimization of the variational parameters then proceeds as
follows. Following random initialization, a unit step is taken in the
natural gradient direction. Subsequent steps are subject to conjugate
gradients (Honkela et (11., 2010). If the conjugate gradient step
should fail to improve the objective we revert to a VBEM update,
which is guaranteed to improve the bound. For more details, see
Hensman et al. (2012).

2.5 Truncation

The optimization described above has N X M free parameters for
optimization, one to align each read to each transcript. However,
for most read—transcript pairs, p(r,, I Tm) will be negligibly small. We
follow Glaus et al. (2012) in truncating the values of p(r,, I Tm) to
zero for reads which do not suitably align. Examining the objective

112 ﬁle'slvumofquixo'soticuiJOJutotq/ﬁduq 11101} pQPBOIIIAAOG

9103 ‘Og isnﬁnv uo ::

Fast and accurate approximate inference of transcript expression

3885

 

function (10) we see that we can also set (pm to zero for these trun—
cated alignments (using the convention that 01n(0) : 0) and thus
also yum : —00 for the same. This truncation dramatically reduces
the computational load of our algorithm, reducing the dimensional—
ity of the optimization space as well as reducing the number of oper—
ations needed to compute the objective.

2.6 The approximate posterior

Having fitted our model, we may wish to propagate the posterior
distribution through a second set of processing, for example to iden—
tify differentially expressed transcripts as in BitSeq stage 2 (Glaus et
111., 2012). Whilst it may be desirable to solve both stages together in
a Bayesian framework, the size of the problem generally forbids this,
therefore we propose the use of either a moment—matching or sam—
pling procedure to propagate 11(0) through further analysis. The ap—
proximate posterior 11(0) is a Dirichlet distribution, whose
marginals have the following useful properties:

, «z. + <1»,
covtéizflé’rzl29211123511111516MG (1%)

with C : (51° + N)_2(&° + N + 1)". This approximate posterior is
somewhat inﬂexible, in that it cannot express arbitrary covariances
between the transcripts. This arises from the factorizing assumption
amongst the assignment of reads to transcripts: reads are assigned
independently in the variational method and their dependence can—
not be modelled. This is reﬂected in the results section where we
show empirically that the VB approximation leads to an underesti—
mation of the variance. Nonetheless, this simplifying assumption
leads to very accurate expression estimates much faster than
MCMC.

3 Results and discussion

The proposed BitSquB algorithm was compared with Cufﬂinks
(Trapnell et 111., 2010), RSEM as well as the corresponding MCMC
sampler RSEM—PME (Li and Dewey, 2011), BitSeqMCMC (Glaus
et111., 2012), eXpress (Roberts and Pachter, 2013), Casper (Rossell
et 111., 2014), Sailfish (Patro et 111., 2014), Tigar2 (Nariai et 111.,
2014) and Kallisto (Bray et 111., 2015). We note that both MCMC
samplers (RSEM—PME and BitSeqMCMC) use similar collapsed
Gibbs algorithms but are initialized differently: RSEM—PME starts
from the ML solution found by RSEM while BitSeqMCMC starts
from a random initialization and therefore requires more iterations
to find a good solution.

We used two main ways for benchmarking: analysis on synthetic
data allowed comparison with a known ground truth under a var—
iety of generative scenarios; analysis on high—quality replicated
human data focused on inter—replicate consistency following the
evaluation of Rossell et 111. (2014). We find BitSquB to have excel—
lent inter—replicate consistency and accuracy, closely approximating
the original MCMC algorithm, while also being competitive with
other methods in terms of run—time. We subsequently analyze in
more detail the approximation to the posterior used in the BitSquB
method. For comparison with other methods, we used default set—
tings where appropriate: both MCMC sampling methods use 1000
posterior samples as default. However, this number refers to effect—
ive samples (Gelman et111., 2003) in BitSeqMCMC and not to single
iterations as in RSEM—PME. We turned off creating of unnecessary
output files in RSEM. The experiments were conducted on a four

core workstation. All the details of the experiments can be found at
the aforementioned URL.

3.1 Inference accuracy on synthetic data
RNA—seq reads from M : 48 009 transcripts of the UCSC/hg19 tran—
scriptome annotation (Kent et 111., 2002) were simulated using the
Spanki software (Sturgill et 111., 2013). The expression is evaluated
in three different measures: transcript expression accuracy (Theta),
transcript within—gene relative proportion accuracy (WGE—True)
and inter—replicate consistency (WGE—Inter). The first two measures
(Theta and WGE—TRUE) compare the resulting estimates against the
ground—truth. On the other hand, WGE—Inter compares the consist—
ency of within—gene estimates across independent repetitions of the
same experiment. This implies that an algorithm yielding constant
estimates independent of any data could achieve WGE—Inter : 0,
but it would obviously do very poorly on WGE—True. Thus, a good
score on WGE—Inter is necessary but not sufficient for a method to
perform well in practice. For further details of the evaluation meas—
ures see supplementary material (Section 5).

A ground truth was generated using four different models of
transcript expression, according to the following scenarios:

1. estimated expression levels from real data using BitSeqMCMC
(z 56 million reads per replicate)

2. randomly selected expression levels according to a uniform dis—
tribution deﬁned on the set (10, 200) (z 7.8 million reads per
replicate)

3. a high—dimensional mixture of Poisson Generalized Linear mod—
els, which was recently used to model the heterogeneity in RNA—
seq datasets (Papastamoulis et 111., 2014b) (z 5.5 million reads
per replicate)

4. estimated expression levels from real data using RSEM (z 18
million reads per replicate)

For each scenario five replicates are generated according to a
Negative Binomial model. Full details of the four scenarios are
described in the Supplementary Material. Finally, the resulting
reads—per—kilobase (RPK) values were fed into Spanki. Next, the
simulated reads were aligned to the reference annotation using
Bowtie2 and/or Tophat2. In particular, BitSeq, RSEM, eXpress and
Tigar require transcriptomic alignments so Bowtie2 (version 2.0.6)
(Langmead and Salzberg, 2012) was used, while Cufﬂinks and
Casper work with genomic alignments using Tophat2 and Bowtie2.
On the other hand, Sailfish and Kallisto produce their own align—
ments using k—mers mapping and pseudo—alignments, respectively.
The corresponding mapping rate for genomic or transcriptomic
alignments was 96%. The same amount of reads pseudo—aligned
when using Kallisto, whilst Sailfish mapped a smaller portion of
k—mers (z 63%).

Figure 2 displays the mean absolute error (MAE) according to
the three criteria, after performing the following normalization:

Z MAE)? 2 1,

memerhods

V6 6 {Theta,WGE — Inter,WGE — True}, to make all criteria
equally weighted for each scenario. Moreover, the ‘Theta’ and
‘WGE—True’ metrics were averaged across the five replicates, while
‘WGE—Inter’ was averaged across all ten combinations of pairs of
replicates. The methods were ranked with respect to their average
across the three criteria. RSEM—PME, BitSeqMCMC and BitSquB
are ranked as best when considering all three criteria. RSEM has
similar accuracy in terms of the ground truth expression (Theta

112 /310'S[BHJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”K1111] 11101} popcolumoq

9103 ‘01; anﬁnv uo ::

3886

J. Hensman et al.

 

 

 

 

 

Criterion Dataset 0 O
o Theta O scenarioi (bitseq)
q: _ A WGE—True o scenario2 (random)
0 + WGE—lnter O scenario 3 (GLM mixture)
I scenario4(RSEM)
x— ”. _
e 0
Li
2
:1
g A
n A
<
c (\I _ o
g o o o o
E
0 +
+ +A++ 0+
F + + + + + ++ Afr A+A ++
o' — AA + + o 7&3 A31: A¢°A A
+§A A A0 A A A
+A + t + oo o 0
A0 $0 + o A + o
o 0 O o
o 0 0 0 o 0 o
I I I I I I I I I I
rsemPME bitseqMCMC bitsquB rsem casper eXpress kallisto sailfish cufflinks tigaIQ

Fig. 2. Ranking of methods for five replicates of simulated RNA-seq reads. WGE-lnter: inter-replicate consistency of within gene estimates, WGE-True: within
gene estimates compared with the true values and Theta: estimated relative transcript expression compared with the true values. Scores have been normalized
to unity per dataset. Alternative normalizations are available in supplementary material (Supplementary Fig. 89)

and WGE—True) but has lower inter—replicate consistency (WGE—
Inter). Conversely, Casper achieves good performance with
respect to inter—replicate consistency (WGE—Inter) but is less accur—
ate in comparison to the ground truth values (WGE—True and
Theta). The ranking of methods with respect to run—time is shown in
Figure 3. Note that the run—time calculation excludes the align—
ment procedure, but includes all other computations (including
computing alignment probabilities in BitSeq’s case). An exception is
made for Sailfish and Kallisto, where alignment is not required,
making these by far the fastest methods. Timings which include
the time required for alignment are provided in Supplementary
Figure S12.

The plots of inter—replicate consistency between pairs of replicates
are shown in the supplementary material (Figs. 2, 4, 6 and 8). As seen
there, Kallisto, RSEM, Sailfish, Tigar2, Cufﬂinks and eXpress, pro—
duce estimates close to the boundary of the parameter space. This is
also obtained for RSEM—PME except for scenario 2. This behaviour is
avoided when using BitSeqMCMC, BitSquB and Casper.

The accuracy of BitSquB is very close to the two sampling
methods BitSeqMCMC and RSEM—PME, but it is consistently faster
that these approaches, being about 10 times faster than
BitSeqMCMC and 2 times faster than RSEM—PME on average
(RSEM—PME is significantly faster than BitSeqMCMC because is
uses many fewer iterations of MCMC). BitSquB has similar speed
to the Cufﬂinks method in most cases whilst exhibiting much better
accuracy.

We conclude that the proposed VB algorithm is competitive in
speed while exhibiting both high accuracy and good inter—replicate
consistency.

3.2 Replicate consistency in human data

A recent study (Rossell et111., 2014) used the mean absolute error be—
tween pairs of replicates of the same ENCODE experiment to assess
the accuracy of transcript expression estimation methods. For this
purpose, the relative within gene expression estimates are used
(WGE—Inter). Here, we provide an extended version of this analysis
to benchmark against BitSeqMCMC and six other methods.

100.0
500 scenario 1: bitseq (280 M reads)
scenario 4: RSEM (86 M reads)

scenario 2: random (39 M reads)

' scenario 3: GLM mixture (27 M reads
1.0
0.5 I
0.1

Fig. 3. Run-time in hours (log-scale) for four synthetic data samples with five
replicates per sample. The total number of simulated reads is shown in

run—time (hours)
01
O

kallisto
sailfish
casper
eXpress
cufflinks
bitsquB
rsem
rsemPME
bitseqMCMC
tigar2

parenthesis

In total, five ENCODE datasets (Tilgner et 111., 2012) consisting
of 2 X 76 bp reads were selected, corresponding to the following
pairs of replicates: (SRR307897, SRR307898), (SRR307901,
SRR307902), (SRR307907, SRR307908), (SRR307911,
SRR307912), (SRR307915, SRR307916). All methods were applied
assuming the same UCSC/hg19 transcriptome annotation as in the
previous section. According to the alignment rates shown in
Figure 4a, all methods work with almost the same number of
mapped reads when Bowtie2 is used. This is not the case for
Bowtie1 which for some reason fails on this dataset.

Figure 4b illustrates the ranking of methods in terms of the MAE
criterion, averaged across the five datasets. We conclude that
BitSeqMCMC has best inter—replicate consistency, closely followed
by BitSquB, while Casper comes next. Sailfish, RSEM, Tigar2 and
Cufﬂinks exhibit almost two times larger MAE, while eXpress is al—
most 2.5 times worse according to this measure. Based on these five
samples there is a partial order: BitSeqMCMC >— BitSquB >-
{Casper, Kallisto} > RSEM — PME > {RSEM, Sailfish} > {Cufﬂinks,
Tigar2} >— eXpress, where >— denotes ‘is better in every experiment’.
Excluding BitSeq (MCMC and VB) and Casper, we see that many
methods produced estimates close to the boundary of the parameter
space, as seen in Figure 5. This means that many transcripts are esti—
mated as weakly or non—expressed in one replicate while being

112 /310'S[BHJDO[pJOJXO'SOIIIBHIJOJIIIOIq/ﬂdnq won popcolumoq

9103 ‘01; anﬁnv uo ::

Fast and accurate approximate inference of transcript expression

3887

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

2 80
E 1:1 I——'—I I——‘—I . #1
E 60 I i | i_,_i
g 40
E: 20
< c .
acme ‘ 90,106 2 ,(uowe ‘ Dome '1 saws“ “(yew
60“”? I 60“”? i
0.07 $
0.06 '
g 005 . i =1 — I;I
E “’4 ; $ ': I
0.03 :b 1:":
0.0
0 e» a) » \o E . » n m . (z . s s
“sewed “was” Gas? was ﬁeme we (56 we want 614.165
1;; _|_ :
s 8.0
O
.C
V I:-
E 2.0 . I E .
._—_ = ——
g 05 — —— g
‘- =
“(rem saws“ Sweat case“ warm“ wde (ﬁam‘e’aNV‘J‘E @1000 mad

as

Fig. 4. Five ENCODE pairs of replicates. (a) Alignment rates for transcriptome mapping (Bowtie1 and Bowtie2), genome mapping (Tophat 2.0.9 with Bowtie1 and
Bowtie2), k-mers mapping (Sailfish) and pseudo-alignments (Kallisto). (b) Ranking of methods in terms of the Mean Absolute Error. (1:) Run-time in hours (log-

scale) with 24.6 M (mapped) reads per sample

bitsquB: MAE = 0.0299

bitseqMCMC: MAE = 0.0279

1.0
1.0
1.0

replicate 2
replicate 2
replicate 2

   

0.0 0.2 0.4 0.6 0.8
0.0 0.2 0.4 0.6 0.8
0.0 0.2 0.4 0.6 0.8

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

replicate 1 replicate 1
sailfish: MAE = 0.0519 rsem: MAE = 0.0534

    

1.0
1.0
1.0

L

replicate 2
replicate 2
replicate 2

0.0 0.2 0.4 0.6 0.8
0.0 0.2 0.4 0.6 0.8
0.0 0.2 0.4 0.6 0.8

 

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

replicate 1 replicate 1

casper: MAE = 0.0366

 

0.0 0.2 0.4 0.6 0.8 1.0

tigar2: MAE = 0.0566

 

0.0 0.2 0.4 0.6 0.8 1.0

kallisto: MAE = 0.0368

rsemPME: MAE = 0.0398

    

1.0
1.0

replicate 2
replicate 2

 

0.0 0.2 0.4 0.6 0.8
0.0 0.2 0.4 0.6 0.8

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

replicate 1 replicate 1 replicate 1
eXpress: MAE = 0.071

cufflinks: MAE = 0.0585

1.0
1.0

replicate2
0.0 0.2 0.4 0.6 0.8

replicate2
0.0 0.2 0.4 0.6 0.8

   

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

replicate 1 replicate 1 replicate 1

Fig. 5. Scatterplots of within gene estimates for one pair of replicates (SRR307907 and SRR307908) from the ENCODE data. The blue color corresponds to a

smoothed color density representation of the scatterplot

highly expressed in the other. This problem appears to affect meth—
ods using ML estimation (RSEM, Sailfish, Cufﬂinks, eXpress) or
Bayesian methods using a very weak prior (Tigar2). Casper ensures
consistency with a strong prior, but this may degrade the accuracy
of absolute estimates relative to BitSeq because of stronger regulari—
zation. We note that Casper uses MAP parameter estimation, find—
ing the mode of the posterior distribution, while the BitSeq methods
estimate the mean of the posterior distribution. Using the posterior
mean may avoid spurious values where the mode is a long way from
the mass of the posterior without the need for an overly strong prior.
Finally, note that the coherency of inter—replicate consistency esti—
mates in our simulation study (Supplementary Figs S2, S4, S6 and
S8) with the one reported here.

The run—time for each method is displayed in Figure 4c.
BitSquB is comparable to the fastest methods (except for Kallisto
which is by far the fastest method) while being ranked as second in

terms of the MAE criterion. We conclude that BitSquB offers
perhaps the best trade—off in accuracy and runtime on these
datasets.

Finally, we mention that the BitSeqMCMC performance here is
in stark contrast with the performance reported in Rossell et 111.
(2014). The reason for this is that in Rossell et 111. (2014) reads
were aligned using Bowtie1 whereas we are using Bowtie2. As seen
in Figure 4a, Bowtie1 can exhibit very low alignment rates for
these samples. Interestingly, this behaviour is not present when
Bowtie1 is combined with Tophat for genome mapping. The low
alignment rates of Bowtie1 means that methods have available
only a tiny fraction of the useful data, leading to less accurate re—
sults. This explains the weak agreement of same transcript esti—
mates between pairs of replicates reported for BitSeq in Rossell et
111. (2014) and is a reminder that it is very important to check the
alignment rates.

112 /310'S[BHJDO[pJOJXO'SOIIIBHIJOJIIIOICI/ﬂdnq won popcolumoq

9103 ‘01; anBnV uo ::

3888

J. Hensman et al.

 

3.3 Analysis of the variational Bayes approximation

To examine the properties of the variational approximation, we
focused on ENCODE dataset SRR307907 (Tilgner et 111., 2012).
This contained 30.8 million reads, each 76 bp. The reads were again
mapped to the same UCSC/hg19 reference transcriptome resulting
in 23.7 million mapped reads.

Our main potential concern in using the VB method is the qual—
ity of approximation to the posterior. Figure 6a shows a comparison
of the variational posterior with a ground truth computed by
MCMC with a very large sampling time. We conclude that the VB
method consistently provides very accurate estimates of the poster—
ior mean across the whole range of expression levels. The estimates
of posterior variance are less consistent and for a fraction of tran—
scripts the variances are underestimated (Fig. 6b). It appears that VB
only estimates the Poisson variance associated with random sam—
pling of reads (Fig. 6d), whereas the true posterior variance is larger
for some transcripts due to the uncertainty in assigning multi—
mapping reads (Fig. 6c). If estimation of the expression level is all
that is required, then it would seem that the VB method suffices.
However, downstream methods which make use of uncertainty in
the transcript quantification [such as the differential expression ana—
lysis proposed in BitSeq stage 2 (Glaus et111., 2012)] may suffer from
the poor approximation in terms of posterior variance. This can po—
tentially be addressed by augmenting the VB method with a more
accurate approximation as done in a recent study that proposed a

     

m (a) (b)

> .

E a   E «a

O .-/ a

‘1’ a: u v

C V—'

g v '1") N

g, o 2 o

_ 0 2 4 a e 10 12 14 0 2 4 a

log—mean—count MCMC

 

 

   

(d)
o E
2 ﬂ --"
<21 0 E i/
v— n m '
a . i
I 8’ V
c) _
2 o 0
02468101214 02468101214

log—mean MCMC log—mean VB

Fig. 6. A comparison of the first two moments of the approximate posterior
expression in counts per transcript: (a) posterior mean (R2 correlation is
0.999) (b) posterior standard deviation: the VB method significantly under-
estimates the posterior variance (172). (c), (d) posterior mean-variance relation-
ship in MCMC and VB respectively. Shading represents the number of
transcripts in each region

 

 

.
+ + MCMC-collapsed
0 O VB2VBEM

X X VB-proposed

  
   

 

 

 

Average RMSE

o
a“

 

 

 

Minutes

Fig. 7. Convergence comparison of Collapsed MCMC with standard VB algo-
rithm and VB with Fletcher-Reeves conjugate gradient optimization.
Expression estimates obtained by very long run of MCMC are used as a
ground truth and average root mean square error over 10 runs was calcu-
lated, two standard deviations are used as error bars. The VB methods with
several randomized initial conditions showed negligible differences in
convergence

new VB algorithm with improved variance estimates and a
tighter lower bound on the log—marginal likelihood (Papastamoulis
et111., 2014a).

3.4 Convergence comparison

We further investigate convergence properties of MCMC and VB in
terms of mean expression. RNA—seq data was obtained from
ENCODE experiment SRX110318, run SRR3 87661, generating
124.8 million 76 bp read—pairs. We mapped the reads using Bowtie 2
to a reference transcriptome using 8713 transcripts of chromosome
19 from Ensembl human cDNA, release 70 (Flicek et111., 2013).

As the true expression levels are unknown, we used a long run of
MCMC as the ground truth for mean expression estimates. Running
the inference methods for a certain number of iterations, we record
the run time and calculate Root Mean Square Error (RMSE) of esti—
mated expression. The convergence of our variational method
(BitSquB) and the original Gibbs sampling procedure
(BitSeqMCMC) is shown in Figure 7. We also include a standard
implementation of VB (similar to Nariai et 111. (2013)) but using the
BitSeq model (denoted VBEM). It is straightforward to derive this
algorithm from our VB algorithm derivation since standard VBEM
is obtained as a special case of steepest descent VB learning
(Hensman et 111., 2012). Our implementation of VB converges first
in about 2min. Surprisingly, some runs of collapsed MCMC con—
verge to better estimates even faster than standard VB, which takes
around 10 min. However, as MCMC is a stochastic method, an esti—
mate that is consistently better than the results obtain by VB is only
obtained after 900 min.

4 Conclusion

We have presented a new Variational Bayes method for inference of
transcript expression from RNA—seq data. Building on previous
work in BitSeq, we have presented a fast approximate inference
method. The mean of the posterior distribution of expression levels
was very well estimated in substantially less time than the original
MCMC algorithm. The method is therefore suitable when point esti—
mates of expression are sufficient, especially if time and computa—
tional resources are limited. We have compared both the original
BitSeq algorithm and our new method with the majority of
available methods for transcript expression estimation and conclude
that BitSquB is highly competitive both in terms of expression esti—
mation and run—time. We also note that an existing VBEM algo—
rithm implementation, TIGAR, does not provide a significant
improvement over Gibbs sampling in terms of computational
time in our examples, as well as having a very high memory
requirement.

The newest method considered here, Kallisto, is found to be ex—
tremely fast and perform with very good accuracy compared with
other ML approaches. This speed—up is achieved through avoiding
full alignment and simplifying the likelihood computation through
using a pseudo—alignment approach. However, the method still pro—
duces estimates at the boundary in our between—replicate compari—
sons similar to all ML methods. It would therefore be
very interesting to apply a Bayesian algorithm, such as the fast
VB method proposed here, using the same likelihood model as
Kallisto.

Finally, we suggest some areas for future development. The
fast and consistent convergence of the VB method makes it useful
for quick examination of the data before the Gibbs sampler is run.
Further, since it provides an excellent approximation to the mean

112 /310'S[BHJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} popcolumoq

9103 ‘01; anBnV uo ::

Fast and accurate approximate inference of transcript expression

3889

 

of the posterior, it could be used to e.g. reduce the burn—in time for
the Gibbs sampler, or as the initial stage of a more sophisticated
approximating technique, as in Papastamoulis et al. (2014a).

Acknowledgements

We thank three anonymous reviewers for useful comments which have greatly
improved the article and Lior Pachter for his blog comments on a preliminary
version of this article which led us to include more realistic data simulation
scenarios.

Funding

J.H. was supported by an MRC fellowship and BBSRC award BB/H018123/
2, PP. and M.R. by BBSRC award BB/JOO9415/1 and MR by EU FP7 award
“RADIANT” (grant no. 305626). P. G. was supported by the Engineering and
Physical Sciences Research Council [EP/P505208/1]. A.H. was supported by
the Academy of Finland [259440 to A.H.].

Conﬂict of Interest: none declared.

References

Amari,S. (1998) Natural gradient works efﬁciently in learning. Neural
Comput., 10, 251—276.

Bishop,C. (2006) Pattern Recognition and Machine Learning. Springer, New
York, NY.

Bray,N. et al. (2015) Near-optimal RNA-Seq quantiﬁcation. arXiv
(q-hio.QM), arXiv:1505.02710v2.

Flicek,P. et al. (2013) Ensembl 2013. Nucleic Acids Res., 41, D48—D55.

Gelman,A. et al. (2003) Bayesian Data Analysis. 2nd edn. Chapman 86 Hall,
CRC Press LLC, Florida, US, Texts in Statistical Science.

Glaus,P. et al. (2012) Identifying differentially expressed transcripts from
RNA—seq data with biological variation. Bioinformatics, 28, 1721—1728.

Hensman,J. et al. (2012) Fast variational inference in the conjugate exponen-
tial family. Adv. Neural Inf. Process. Syst. (NIPS).

Hensman,J. et al. (2015) Fast nonparametric clustering of structured time-
series. IEEE Trans. Pattern Anal. Mach. Intell., 37, 383—393.

Honkela,A. et al. (2010) Approximate Riemannian conjugate gradient learn—
ing for ﬁxed—form variational Bayes. ]. Mach. Learn. Res., 11, 3235—
3268.

Jiang,H. and Wong,W.H. (2009) Statistical inferences for isoform expression
in RNA—seq. Bioinformatics, 25, 1026—1032.

Katz,Y. et al. (2010) Analysis and design of RNA sequencing experiments for
identifying isoform regulation. Nat. Methods, 7, 1009—1015.

Kent,W. et al. (2002) The human genome browser at UCSC. Genome Res., 6,
996—1006.

Langmead,B. and Salzberg,S.L. (2012) Fast gapped—read alignment with
Bowtie 2. Nat. Methods, 9, 357—359.

Li,B. and Dewey,C.N. (2011) RSEM: accurate transcript quantiﬁcation from
RNA-Seq data with or without a reference genome. BMC Bioinformatics,
12,323.

Li,B. et al. (2010) RNA—Seq gene expression estimation with read mapping un—
certainty. Bioinformatics, 26, 493—500.

Mortazavi,A. et al. (2008) Mapping and quantifying mammalian transcrip—
tomes by RNA—Seq. Nat. Methods, 5, 621—628.

Nariai,N. et al. (2013) TIGAR: transcript isoform abundance estimation
method with gapped alignment of RNA-Seq data by variational Bayesian in—
ference. Bioinformatics, 18, 2292—2299.

Nariai,N. et al. (2014) TIGAR2: sensitive and accurate estimation of tran—
script isoform expression with longer RNA-Seq reads. BMC Genomics, 15.

Papastamoulis,P. et al. (2014a) Improved variational Bayes inference for tran-
script expression estimation. Stat. Appl. Genet. Mol. Biol., 13, 203—216.

Papastamoulis,P. et al. (2014b) On the estimation of mixtures of Poisson regres-
sion models with large number of components. Comput. Stat. Data Anal., 93,
97—106.

Patro,R. et al. (2014) Sailﬁsh enables alignment-free isoform quantiﬁcation from
RNA-seq reads using lightweight algorithms. Nat. Biotechnol., 32, 462—464.

Roberts,A. and Pachter,L. (2013) Streaming fragment assignment for real-
time analysis of sequencing experiments. Nat. Methods, 10, 71—73.

Rossell,D. et al. (2014) Quantifying alternative splicing from paired-end
RNA-sequencing data. Ann. Appl. Stat., 8, 309—330.

SEQC/MAQC—III Consortium. (2014) A comprehensive assessment of RNA—
seq accuracy, reproducibility and information content by the sequencing
quality control consortium. Nat. Biotechnol., 32, 903—914.

Sturgill,J. et al. (2013) Design of RNA splicing analysis null models for post
hoc ﬁltering of Drosophila head RNA—Seq data with the splicing analysis kit
(Spanki). BMC Bioinformatics, 14, 320.

Tilgner,H. et al. (2012) Deep sequencing of subcellular RNA fractions shows
splicing to be predominantly co—transcriptional in the human genome but in—
efﬁcient for lncRNAs. Genome Res., 22, 1616—1625.

Trapnell,C. et al. (2010) Transcript assembly and quantiﬁcation by RNA-Seq
reveals unannotated transcripts and isoform switching during cell differenti—
ation. Nat. Biotechnol., 28, 516—520.

Trapnell,C. et al. (2013) Differential analysis of gene regulation at transcript
resolution with RNA—seq. Nat. Biotechnol., 31, 46—53.

Turro,E. et al. (2011) Haplotype and isoform speciﬁc expression estimation
using multi-mapping RNA—seq reads. Genome Biol., 12, R13.

Xing,Y. et al. (2006) An expectation-maximization algorithm for probabilistic
reconstructions of full—length isoforms from splice graphs. Nucleic Acids
Res., 34, 3150—3160.

112 /310'S[BHJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} popcolumoq

9103 ‘01; anBnV uo ::

