ORIGINAL PAPER

Vol. 26 no. 5 2010, pages 640—646
doi: 1 0. 1093/bioinformatics/btq001

 

Gene expression

Advance Access publication January 15, 2010

A censored beta mixture model for the estimation of the
proportion of non-differentially expressed genes

Anastasios Markitsisl and Yinglei Lai2’*

1Department of Statistics and 2Department of Statistics and Biostatistics Center, The George Washington University,

Washington DC. 20052, USA
Associate Editor: David Rocke

 

ABSTRACT

Motivation: The proportion of non-differentially expressed genes (:10)
is an important quantity in microarray data analysis. Although many
statistical methods have been proposed for its estimation, it is still
necessary to develop more efficient methods.

Methods: Our approach for improving 710 estimation is to modify
an existing simple method by introducing artificial censoring to
P-values. In a comprehensive simulation study and the applications
to experimental datasets, we compare our method with eight existing
estimation methods.

Results: The simulation study confirms that our method can
clearly improve the estimation performance. Compared with the
existing methods, our method can generally provide a relatively
accurate estimate with relatively small variance. Using experimental
microarray datasets, we also demonstrate that our method can
generally provide satisfactory estimates in practice.

Availability: The R code is freely available at http://home.gwu
.edu/~y|ai/research/CBpi0/.

Contact: ylai@gwu.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on November 25, 2008; revised on December 23, 2009;
accepted on December 30, 2009

1 INTRODUCTION

Microarray technology is a powerful tool for studying complex
diseases (Mootha et al., 2003) and for assessing the effects of
drugs (Salvatore et al., 2008) at the molecular level. It is an
experimental method by which thousands of genes can be printed on
a small chip and their expression can be measured simultaneously
(Lockhart et al., 1996; Schena et al., 1995). It can be used to detect
changes in gene expression between normal and abnormal cells,
which enables scientists to detect novel disease-related genes (Singh
et al., 2002). Many statistical methods have been developed for
this purpose (Cui and Churchill, 2003). Although other advanced
genomics technologies, such as RNA sequencing (Nagalakshmi
et al., 2008; Wilhelm et (11., 2008), have been developed, microarrays
have been continuously used for broad biomedical studies (Cancer
Genome Atlas Research Network, 2008). Furthermore, since the
structures of data from different genomics technologies are basically

 

*To whom correspondence should be addressed.

similar, methods for analyzing microarray data can also be useful
for analyzing other similar genomics data.

Performing statistical tests for a large number of genes raises
the need for an adjustment for multiple hypothesis testing (MHT).
A widely used method to address this issue is the false discovery
rate (FDR; Benjamini and Hochberg, 1995) that evaluates the
proportion of false positives among claimed positives. FDR control
is less stringent than the traditional family-wise error rate (FWER)
control such as the Bonferroni correction, and provides more
power for discovering differentially expressed genes. However,
estimating FDR involves the estimation of 710, the proportion of
non-differentially expressed (null) genes [(1—7:0) corresponds to
the proportion of differentially expressed genes]. A reliable estimate
of 710 is also of great importance to the sample size calculation for
microarray experiment design (Jung, 2005; Wang and Chen, 2004).

A variety of methods have been proposed for estimating 710. Storey
and Tibshirani (2003) proposed qvalue. This method uses the ordered
P-values and a cubic spline, and estimates 710 as the value of the ﬁtted
spline at a value close to 1. Pounds and Morris (2003) suggested
BUM, a ‘beta-uniform’ mixture model with the estimate of 710 being
the value of the ﬁtted model at 1. convest, a method introduced by
Langaas et al. (2005), utilizes a non-parametric convex decreasing
density estimation method and gives the value of the density at 1 as
an estimate of 710. A histogram-based method has also been proposed
(Mossig et al., 2001; Nettleton et al., 2006). The above methods
usually provide conservative estimates of 710; in other words, they
are expected to give positively biased 710 estimates. This has been
considered an advantage, since it protects against overestimating the
number of differentially expressed genes.

Many other methods have also been proposed for estimating 710.
Lai (2007) proposed a non-parametric moment-based method
coupled with sample-splitting to achieve the identiﬁability and
obtained a closed-form formula for 710. Scheid and Spang
(2004) presented the successive exclusion procedure (SEP),
which successively excludes genes until the remaining u-values
(transformed P-values) are sufﬁciently close to a uniform
distribution U [0, 1]. SEP estimates 710 by J /m, where J is the
estimated number of null genes, and m is the total number of genes.
Guan et al. (2008) estimated the marginal density of P-values using
a Bernstein polynomial density estimation, and gave a closed-form
expression for their 710 estimator. Liao et al. (2004) obtained an
estimate of 710 through Bayesian inference from a mixture model,
which requires the distribution of P-values from non-null genes to
be stochastically smaller than that from null genes. In addition to
the above methods, there are still many other proposed methods

 

640 © The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org

112 /3.Io's[Bumo[pJOJXO'soiwuiJOJuioiqﬂ:duq moi; papeolumoq

9103 ‘{g anﬁnv 110::

Proportion of non -differentially expressed genes

 

for estimating 710 (Broberg, 2005; Dalmasso et al., 2005; Jiang
and Doerge, 2008; Lu and Perkins, 2007; Pounds and Cheng,
2004, 2006). Furthermore, no can also be estimated through a
normal mixture model based on the z-scores obtained from P-values
(McLachlan et al., 2006).

In this study, to improve 710 estimation, we propose a simple
method, which is a modiﬁcation of BUM. The novelty of our
method is the introduction of artiﬁcial censoring to P-values so that
an improved estimation can be achieved. Our motivation is based
on the observation that a well-ﬁtted BUM curve for the empirical
P-value distribution may not be optimized for estimating 710. In the
following sections, we ﬁrst introduce the statistical background and
our method; then, we present the evaluation and comparison results
from our simulation and application studies. Finally, we give some
brief discussion to conclude our study.

2 METHODS

2.1 Detection of differential expression

In a typical microairay experiment, the gene expression in two groups of
cells can be compared. On a microarray chip, a large number of genes can
be monitored simultaneously, which provides researchers with measurement
for each gene in each group. For example, to assess genes” involvement in
tumor growth, the expression of tens of thousands of genes can be measured
in normal and cancerous cells. Depending on the number of microairay
chips available, multiple measurements for the expression of each gene are
obtained.

For each gene, let it] and 11.; be the true mean intensities, in groups 1 and 2,
respectively. To determine whether the gene is differentially expressed, the
null and alternative hypotheses are:

H01M1=M2 Versus Harm ##2-

A commonly used test statistic is the Student’s t-test (assuming equal
variances). A positive is claimed when H0 is rejected in favor of Hg, and a
negative when H0 is not rejected. A positive means that the gene is declared
differentially expressed; a negative means that the gene is declared non-
differentially expressed.

If we knew the true state of each gene (i.e. whether it is truly differentially
expressed or not), then the results of testing In genes simultaneously could
be classiﬁed into four categories (each denoted by the random variable in
parentheses): true positives (S), false positives (V), true negatives (U) and
false negatives (T). Table 1 gives an illustration. Ideally, one would like to
minimize V and T, and maximize S and U.

The probability Pr(V > 0) is called the FWER. In MHT, strong control is
deﬁned as maintaining the FWER below a speciﬁed level at. The traditional
strong-control method is the Bonferroni procedure; that is, rejecting each H0
corresponding to a P-value less than oz/m. However, in microarray studies,
oz/m is typically so small that it is unlikely that many null hypotheses will
be rejected. A widely used alternative is to control the FDR, the expected
proportion of false positives (V) among the claimed positives (R=V+S)

Table 1. Numbers of true/false null hypotheses and negatives/positives in
the situation of MHT

 

 

True null False null Total
Negative U T m — R
Positive V S R
Total mg m — mg m

 

(Benjamini and Hochberg, 1995):

V
FDR=E(Q), where Q: E when R> 0, and Q20 otherwise.

Other versions of FDR have also been proposed: Tsai et al. (2003)
considers the estimation of four other FDR versions. In general, controlling
FDR provides higher statistical power for discovering differentially
expressed genes. Let m0 = U + V denote the total number of true null
hypotheses, and nozmo/m denote the proportion of true null hypotheses
(i.e. the proportion of non-differentially expressed genes; so the proportion
of differentially expressed genes is 1 — 710). Suppose that a researcher rejects
Hg for each gene with a P-value less than a prespeciﬁed level at. To estimate
the corresponding FDR in this situation, Storey (2002) proposed
mfrooz

NO!)
where fro is an estimate of no, and r(oz) is the observed number of positives.
From this equation, it is clear that the accuracy of an FDR estimate depends
on the estimation of 710, which is the parameter of interest in this study.

 

F/D\R(cx) =

v

2.2 The beta-uniform mixture model

Pounds and Morris (2003) have proposed the beta-uniform mixture (BUM)
model. It assumes the following model for the marginal distribution of
P-values:
f(P)=J/+(1—J/)‘¥l7a_l~

where0<p§1, 0<y<1 and 0<oz<1.

Based on this simple model, Pounds and Morris (2003) have proposed the
following estimate of 710:

f(1)=JA/+(1-JA/)5l~

where f/ and 61 are the MLE estimates.

2.3 Our approach

To represent the marginal distribution of P-values, BUM uses a mixture
of the uniform distribution U [0, 1] (also Beta(1, 1)) and a Beta distribution
Beta(oz, 1) with 0 < at < 1. However, BUM is too simplistic to achieve a robust
performance in practice. Let p: {171,p2,...,p,,,} be the observed P-values.
Under the independence assumption, the log-likelihood is given by

Ill Ill

L<mlp>=Zlogifm>i=Zlogty+u —y>ap§‘“].

1’21 [’21
BUM estimates the ﬁtted model curve by maximizing this log-likelihood.
As p—> 0, f(p)—> 00. Clearly, the smaller a p, is, the larger its contribution
will be to the log-likelihood. Therefore, to optimize the ﬁtted curve, BUM
places more weight on smaller P-values. However, no is our focus and is
estimated by f (1), which depends more on the P-values close to 1. To solve
this problem, we propose the following censored beta mixture model.

2.3.1 A censored beta mixture model To improve BUM, we artiﬁcially
censor the P-values that are less than a cut-off point A. These P-values
are considered ‘indistinguishable’. In other words, even though the actual
P-values less than A are available, we do not use those values; our model
only uses the number of such P-values. (We do not consider P-values < A as
missing data). In this way, we aim to reduce the effect of very small P-values.
Then, we have the mixture model:

f0?)=yg1(p)+(1— y)g2(p).
where
_ censored 0 517 < A
g1 _ 1 A :p s 1
is a left-censored uniform distribution U [0, 1] and,
{censored 0 517 < A
2 = 41—1
0117 A SP 5 1
is a left-censored Beta(oz, 1) distribution (0 < at < 1). Figure 1 provides an
illustration of this model.

 

641

112 /3.Io's[BumoIpJOJxosoiwuiJOJuioiq”:duq uioii papeolumoq

9103 ‘{g isnﬁnv 110::

A.Markitsis and Y.Lai

 

 

gzlp)

 

 

 

   

Fig. 1. (A) Graph of g1 , a censored uniform distribution U [0, 1]. (B) Graph
of g2, a censored beta distribution Beta(ot, 1).

REMARK 1. Note that although we do not assume a speciﬁc form for the
density off(p) in [0,A), we know that Pr(0 5p < A|g1)=A and Pr(0 5p
< A|g2) 2A“. The marginal probability is Pr(0 5p < A): yA + (1 — y)A“.

REMARK 2. In this study, we assume that A is given as 0.05, which
is conventionally considered small (e.g. a threshold value for declaring
statistical signiﬁcance in practice). It is theoretically true that selecting a
A less than the minimum P-value is equivalent to using BUM. Furthermore,
as pointed out by a reviewer, selecting a large A is very similar to using
qvalue or the histogram methods.

2.3.2 Estimating model parameters Our model is a special case of the
two-component mixture model in Ji et al. (2005). It consists of a censored
Beta(1, 1) (equivalent to U [0, 1]), and a censored Beta(ot, 1). Therefore, we
can use the ExpectationiMaximization (EM) algorithm (McLachlan and
Krishnan, 2008) to estimate the parameters y and at. Following Ji et al.
(2005), we augment the data by introducing the latent indicator variables zi,
1 5 i 5 m (where m is the total number of genes) deﬁned as:

_ 0 if p, belongs to the component g1,
_ 1 if p, belongs to the component g2.

Let z: {zl ,zg, . . . ,z,,,}. The log-likelihood of our model given the
‘complete’ data {p,z}, is:
Ill
L(y.a|p.z)=log:H[(Vg1)““((1 —y>g2>:i]
i:l
To maximize the log-likelihood with respect to y and at, given the
‘complete’ data, we take the partial derivative of the above equation with
respect to y and set it equal to zero, and then do the same for at. Solving
these two equations, we obtain the following maximum likelihood estimates
of y and at to be used in the M-step of the EM algorithm:
? 2 22:10 —zi>,
m .
A _ _ Zing/2,513
10g0‘)2i:05p,-<Azi + Zing/2515108070 ’

In the E-step of the EM algorithm, we need to update the expected values
of the {zi}. Given the current estimates of y and at, we can compute z; =
E(z,»|p,f/,6t). Since each z, is an indicator variable, z; is the conditional
probability (at each iteration of the algorithm) that p, belongs to component
g2. Hence, we have the following formulas:

 

0 For each censored P-value, that is, if 0 5 p, < A,
Z, _ [(1—W]
’ [WHl -J9)A‘3‘l’
0 For each non-censored P-value, that is, if A 5 p, 5 1,
Z._ [(1—t>&pi‘“1

’_ [19+(1—J?)ap‘§“]’

To start the EM algorithm, we select an initial value for y; in general,
we can use ym) :05, unless we have some empirical estimate of no to use

E0)=1—y(0) for 1 Sifm. With

instead. Then, we initialize {zi} by setting z
{zgm}, we can obtain y“) and at“), the estimates of y and at after the ﬁrst

iteration. The convergence of EM algorithm is declared when
W -y"‘“)| <6

where ya” is the estimate of y at the end of the k-th iteration, and 8 is a
prespeciﬁed threshold (6:1 X 10‘6 in this study). When the EM algorithm
converges, let f/ and 0! be the estimates of y and at, respectively. Then, the
estimate of no is given by:

ﬁo=f(1)=;>+(1—;>)&.

REMARK 3. As suggested by a reviewer, it is necessary to consider multiple
initial values for BUM. In our simulation study, for BUM 's parameters
(a and A), we use a = A = min(2 >< mean ofall P-values, 0.9) and 5pairs of
randomly simulated numbers from U [0, 1]. Although our method is robust to
diﬁ‘erent initial values in this study, we still suggest that multiple initial values
may be necessary to achieve a reliable estimate of no in certain situations
(e.g. nowl). Furthermore, although the required computing time of our
method is much longer than that of BUM (and several of other methods), it
is still aﬂordable with a general computer.

2.3.3 Conﬁdence interval Since the above EM algorithm does not provide
us with any closed formulas of estimates, it is difﬁcult to derive the theoretical
conﬁdence interval (CI) for the estimated no. Therefore, we use the bootstrap
procedure (Efron, 1979) to obtain a CI for no (we set B = 500 for both
application studies):

(1) Select a random sample of m P-values from {pl,p2,...,p,,,} with
replacement and equal probabilities;

(2) Apply the EM algorithm to the sample generated in Step 1 and obtain
a resampling estimate of no;

(3) Repeat Steps 1 and 2 B times to obtain the resampling distribution
of no;

(4) For a 100(1 —Ol)% CI for no, ﬁnd the (ot/2)-th and (1—ot/2)-th
quantiles of the resampling distribution.

REMARK 4. A key assumption for bootstrapping P-values in the construction
of CIs is that the observed P-values are independent. However, since genes
are correlated in a expression dataset, a bootstrapped CI for no should be
considered as an approximation in practice. This issue has been discussed
in Allison et al. (2002).

3 RESULTS

3.1 Simulation studies

3.1.] Simulation conﬁguration We simulate gene expression data
to evaluate the performance of our method. We also select several
existing methods for a comparison study. BUM (Pounds and Morris,
2003) has to be included since it is the foundation of our method.
Based on the consideration of the popularity and research history of
the statistical methods for estimating no, the following methods are
selected (notation in parentheses): (CB) our method; (BUM) Pounds
and Morris (2003); (H) the histogram-based method (Mosig et al.,
2001; Nettleton et al., 2006); (Q) qvalue (Storey and Tibshirani,
2003); (L) the method proposed by Liao et al. (2004); (S) the
method proposed by Scheid et al. (2004); (C) convest (Langaas et al.
2005); (RDM) the method proposed by Lai (2007); (G) the method
proposed by Guan et al. (2008). The notations deﬁned above are
used in Figure 2. (We have actually performed a simulation study

 

642

112 /3.Io's[Bumo[p.IOJxosoiwuiJOJuioiq”:duq uioii papeolumoq

9103 ‘{g isnﬁnv 110::

Proportion of non -differentially expressed genes

 

 

        

 

 

 

 

 

 

 

 

 

A B C
6\ (6+6) RMSE (T) vs. no \ (18+18) RMSE (T) vs. no RMSE (T) vs. no
0 N. ’ ' - x
(\i _ to
O in O. -
Q _ o
o
o
O’
(\l
in g _ O. -
8 ' o' O
(\l  _  _
g --0- CB - — G O
— BUM H ‘ - RDM
C L S
I I I I I I I I I I I I
0.2 0.4 0.6 0.8 0.2 0.4 0 6 0 8 0.2 0.4 0 6 0 8

Fig. 2. Simulation results: gene expression data are simulated based on a independence structure. RMSE in log-scale of the estimates from different methods

with different sample sizes considered: n1 =n2 :6 (A), 18 (B) and 30 (C).

to compare many more methods. However, due to the page limit, it
is difﬁcult to present all the results. The exclusion of other methods
does not change our conclusion.)

For each dataset, we simulate expression observations for 5000
genes. In reality, genes work together in complicated gene networks.
To study the impact of correlation among genes on different
methods, we generate data with the assumption that genes interact in
blocks (‘networks’) of equal size. We also assume that within each
block, the correlation among any pair of genes is the same, and equal
to ,0. We perform simulations for different sample sizes (n1,n2=
6,10,18,30 and 50); correlation strength (,0=0,0.3,0.5,0.7 and
0.9); and number of blocks (b: 100,200 and 500) [or, equivalently,
number of genes per block (g), = 50,25 and 10)].

REMARK 5. It is well-known that the sample size has an important
impact on the estimation of no. As pointed out by one reviewer; the
power of an a level test is the cumulative distribution function of
the P-value evaluated at a. Since the power depends on the sample
size, so does the distribution of P-values. Therefore, any non-trivial
transformation of P-values (including no estimators) depends on the
sample size. For example, Pounds and Cheng ( 2005 ) have showed
that when an estimate of the minimum of the assumed marginal
distribution of P-values [ e. g. f (1) for BUM] is used for estimating
no, the estimator can be expressed as a function of the sample size.

Given a value of no (0. 1 , 0.2, . . . , 0.9), a corresponding number of
blocks are set to consist entirely of differentially expressed genes,
with the remaining blocks consisting entirely of non-differentially
expressed genes. For example, to generate a dataset with no :07
for the {b= 100, g1, :50} conﬁguration, we simulate 30 blocks with
differentially expressed genes, and 70 blocks with non-differentially
expressed genes. For each block, we use the covariance matrix
2 = (1 — ,0)I+ pE of size g), X gb, where I is the identity matrix and
E is a matrix of ones. (Note that 2 is also the correlation matrix
since all genes have unit variances.) Then, for each conﬁguration
mentioned above, we perform the following:

(1) Simulate a gene expression dataset with 5000 genes.

(a) For a block of non-differentially expressed genes,
generate observations from a multivariate normal
distribution N (0, 2) for both sample groups.

(b) For a block of differentially expressed genes, generate
observations from a multivariate normal distribution
N(0,2) for one sample group. Then, generate
observations from a multivariate normal distribution
N (a, 2) for the other group (where a is a random vector,
with elements coming from a uniform distribution
U[0.5,1.5]).

(2) Apply the two-sample Student’s t-test to the proﬁle of each
gene and obtain 5000 theoretical P-values.

(3) Use different methods to estimate 710.

3.1.2 Criteria for evaluation and comparison We repeat
the above steps B=100 times for different values of 710
(01,02, ...,0.9). For each value of no and each method, we compute
the bias, standard deviation (SD) and root mean squared error
(RMSE) as follows:

' Bias=Zf=1(ﬁo.—no)/B.
' 5D=\/Z?=1(ﬁo, —Z?=1no,/B)2/(B_ 1),
° RMSE=JW,

where no, is the i-th estimate of 710. These criteria are used to
evaluate the estimation performance of different methods and the
impact of different A.

 

3.1.3 Comparison of diﬁ’erent methods In all the results, the
patterns in RMSE, bias and SD are very similar for all cases sharing
the same sample size and correlation strength. In other words, the
block size g), in our conﬁguration does not substantially affect the
patterns in RMSE, bias and SD. In the Supplementary Materials,
we present the simulation results based on 200 blocks with 25
genes in each block and different correlation values (,0). In the
following, we discuss the simulation results based on the simple
independence structure (,0: 0), which is representative of the other
results. The simulation results are presented for samples sizes 6+6,

 

643

112 /3.IO'S[1211anI’pJOJXO'SOIJBLUJOJIIIOIq”Idllq uioii papeolumoq

9103 ‘{g isnﬁnv 110::

A.Markitsis and Y.Lai

 

18+ 18 and 30+ 30. [In order to show a clear comparison among
different methods, we use a log-scale for the y-axis in RMSE and
SD graphs (with the option ‘log=“y”’ in the R-ﬁinction ‘plot’),
and the cube root of Bias is actually used as the y-axis in the Bias
graphs. All these comparison plots are given in the Supplementary
Materials. However, in the following, we only give the RMSE-based
comparison plots due to the page limit.]

When n1 =n2 :6 (Fig. 2A), all the no estimation methods show
an overall decreasing pattern of RMSE as the value of no increases.
BUM gives the lowest RMSE when no > 0.4; but its RMSE is among
the worst when 710 < 0.3, where RDM gives the lowest RMSE. Our
method always gives a competitive low RMSE when no > 0.1.

When the sample size increases to n1 =n2 = 18 (Fig. 2B), BUM
shows an unstable performance: it gives a relatively high RMSE for
all the values of 710, except at 710 = 0.2 and 0.9. The beneﬁt of using
our method is more apparent: its RMSE is lower than those of all the
other methods, for all the values of 710 except for no :02 (where
BUM’s RMSE is the lowest).

For n1=n2=30 (Fig. 2C), BUM’s RMSE displays a concave
parabola pattern, and is always relatively high. Our method has the
lowest RMSE for all no.

The ﬁgures in the Supplementary Materials also conﬁrm a
satisfactory performance in Bias and SD from our method. In
general, most methods’ bias decreases as the sample sizes and the
value of no increase. However, BUM quickly becomes the most
negatively biased (which explains the observed large RMSE of BUM
although the SD of BUM is among the smallest). A strongly negative
bias leads to an undesirable overestimation of the number of truly
differentially expressed genes. On the other hand, our method’s bias
becomes negligible as the sample sizes increase. Most methods’ SD
increases as the value of no increases and decreases as the sample
sizes increase.

In general, when the simulation results based on different
dependent structure (independent, weakly/strongly dependent) are
compared (Supplementary Materials), the higher the correlation, the
higher becomes the SD. (The bias, on the other hand, remains mostly
unaffected by the increase in correlation.) However, our results
show that the increase in SD induced by positive correlation among
test statistics does not render the existing 710 estimation methods
inappropriate.

3.1.4 Choice of A The above reported simulation conﬁguration
can also be used to understand the effect of A. We simulate data with
different sample sizes 6+ 6, 18+ 18 and 30+ 30 and compare the
performance of our model for A in the set {0.01 , 0.03, 0.05, ,0.25}.
The ﬁgures in the Supplementary Materials shows that no single
value of A can be identiﬁed to minimize RMSE in a wide range of
710. Furthermore, the RMSE patterns can change signiﬁcantly when
the sample sizes are changed. It is clear that a relatively large A
(e.g. A =0.25) is not a good choice. However, a relatively small A
(e.g. A =0.01) is also not an appropriate choice. In our simulation
study, we have observed that A = 0.05 is always a reasonable choice
to achieve an overall satisfactory performance.

3.2 Applications to experimental data

We ﬁrst consider the following two published experimental
microarray datasets for our applications. The ﬁrst dataset contains
22 283 gene expression proﬁles from kidney biopsies of 19 kidney

transplant subjects with cyclosporine-based immuno-suppression
and 22 kidney transplant subjects with sirolimus-based immuno-
suppression. The second dataset consists of 12488 gene expression
proﬁles from pancreatic T regulatory (three subjects) and T effector
cells (ﬁve subjects). Both datasets are publicly available in the Gene
Expression Omnibus (GEO) database (Barrett et al., 2007) with
accession numbers GSE1743 (Flechner et al., 2004) for the ﬁrst
(renal) dataset and GSE1419 (Chen et al., 2005) for the second
(T cell) dataset.

Theoretical P-values based on the corresponding t-distributions
are calculated for each dataset (two-sample Student’s t-test is
used for detecting differential expression). The true value of no
is unknown in the applications. Therefore, to compare different
methods in each application, we obtain B=500 bootstrap estimates
of no (see Section 2.3.3 for details) and construct a boxplot for the
estimate from each method. Such a boxplot is useﬁil to understand
general CIs for an estimate.

Based on our simulation study, convest has consistently showed
a relatively low RMSE. BUM should be considered since it is the
foundation of our method. Therefore, for simplicity, we use boxplots
to compare our method with BUM and convest. (The exclusion of
other methods does not change our conclusion.) Figure 3 shows
the P-value histograms and the estimates from these three different
methods. For both datasets, the P-value distribution curves ﬁtted
by our method are close to the corresponding P-value histograms.
Theoretically, n0 cannot be higher than the marginal P-value
distribution. This has been brieﬂy discussed in one of our previous
publications (Lai, 2007).

For the renal dataset, our method gives an estimate of 710 0.256
with a relatively tight CI (95% CI: 0.252—0.261). convest gives
a higher estimate 0.278 with a wider CI (95% CI: 0.263—0.293),
whereas BUM gives the highest estimate 0.321 although a slightly
tighter CI (95% CI: 0.318—0.325). Notice that only the estimate from
our method is under the whole P-value histogram. For the T—cell
dataset, our method gives an estimate of 710 0.605 with relatively
tight CI (95% CI: 0.586—0.626), whereas BUM gives a slightly
lower estimate 0.598 and slightly tighter CI (95% CI: 0.583—0.616).
Both estimates are under the whole P-value histogram. However,
convest still gives a higher estimate 0.638 and wider CI (95% CI:
0609—0671).

We also use another experimental dataset to illustrate that our
method (also BUM) does not always yield satisfactory estimation
results. The third application is based on a dataset with 22 283 gene
expression proﬁles from small airway tissues (ﬁve non-smokers
versus six smokers). This dataset is also publicly available in
GEO with accession number GSE3320 (Harvey et al., 2007). The
estimation results are also given in Figure 3. A clear ‘bumped’
shape can be observed in the P-value range [0.15, 0.35], which
causes the problematic estimation results from our method and BUM
(these beta distribution based models do not allow any ‘bumped’
shapes). Our ﬁtted model curve is not close to the P-value histogram.
Although the CI from our method (95% CI: 0899—0921) and BUM
(95% CI: 0897—0935) are clearly tighter (the one from our method
is the tightest) than that from convest (95% CI: 0.850—0.902), both
estimates from our method (0.909) and BUM (0.907) are clearly
higher than the right end portion of P-value histogram. convest
provides a more reasonable estimate 0.878 for this application,
although the difference among the estimates from different methods
is quite small.

 

644

112 /3.IO'S[1211anI’pJOJXO'SOIJBLUJOJIIIOIq”Idllq uioii papeolumoq

9103 ‘{g isnﬁnv 110::

Proportion of non -differentially expressed genes

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

_ P—value histograms
w _ N __I N —o—
0') 0') _ E
d O _._
w ' o _ o
0') 0') _
c; _ c; —'—:
<1- — E
w (D
N N _
N - I
_’[—I _‘   - u ’
0 0 E .
O_l IIIIl—l‘I—I—I‘r—rrl:l.w:   cEstlmates
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 o 8 1.0 N s c
m. _ _ i
“" P—value histograms
O _ I
m. _ I
no to I
m to. no. - .
N _ O — o I
Q _ _ O E
N
2- 1 a a. 2 4; :
_ _— O O ' I I
°" — E ' i
F -—— a . II - - E =
m, _ ’_’ T ’ — I I 0
Q _ O 0 Estimates
0
o o o 2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 N s c
8 — 8
N - _‘ _ P—value histograms F‘ F‘ '
S ' —’ I_I J_l Ln — Ln 0
CD _, CD _
c5 0
w. _ —<:— i
O
o I—’ o I: = .
no _ c» c» _ . , .
0 c5 J c5 —o— + I
.. _ 
0 LO LO '
°°. °°. - I
N o o _._
c; ' 8
Q _ 8 8 _ Estimates
0 . . . . . . c5 . . c5 . .
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 N B C

Fig. 3. Application results: histograms of P-values and boxplots of bootstrap estimates of no. The P-values are calculated based on three experimental
datasets: the renal data (upper panel), the T-cell data (middle panel) and the smoke data (lower panel). In the histograms (left panel), the gray curves represent
the ﬁtted censored beta mixture models (the dashed parts are artiﬁcially censored). The zoomed-in histograms (middle panel) are also shown to compare the
estimate of no from different methods. The gray solid, dashed and dotted lines represent the estimates from our method, BUM and convest, respectively. In
the boxplots (right panel), N = our method, B = BUM and C = convest. The numbers in gray color are the estimates of no based on the original data.

Therefore, in practice, we suggest to check the histogram shape
before applying any statistical methods for estimating no. If the
histogram shape is roughly decreasing, then we expect satisfactory
estimation performance from our method (and BUM in certain
situations). If the histogram shape is not regular, then we may
consider some non-parametric method like convest or the moment-
based method (Lai, 2007).

4 DISCUSSION

Microarrays have been widely used in biological and medical
studies. An accurate estimate of the proportion of differentially
expressed genes is important in false positive control and experiment
design. Therefore, the improvement of existing estimation methods
still remains important.

Our proposed method for estimating 710 provides an effective
solution. Although it is arbitrary, the choice of A=0.05 provides

an overall satisfactory performance. In our simulation study, the
advantage of using our method is clear in the cases of moderate
and large sample size (18+18 and 30+30). In these cases, our
method outperforms (w.r.t. RMSE) the other methods considered
in this study. In the case of small sample, BUM has a satisfactory
performance. Our method may be improved if an efﬁcient method
for the automatic selection of A can be developed. This issue will be
pursued in our future research.

Although none of the no estimation methods mentioned above
considers the effect of gene networks and interactions, dependence
among genes is still a difﬁcult issue in microarray data analysis
(Efron, 2007). However, as investigated by Benjamini and
Yekutieli (2001), methods that are based on the independence
assumption perform quite well in general situations of weak positive
dependence, and a positive dependency structure is common in
many situations. A satisfactory performance under weak positive
dependence has also been conﬁrmed in our simulation studies.

 

645

112 /3.IO'S[1211anI’pJOJXO'SOIJBLUJOJIIIOIq”Idllq uioii papeolumoq

9103 ‘{g isnﬁnv 110::

A.Markitsis and Y.Lai

 

ACKNOWLEDGEMENTS

We thank Professors Joseph Gastwirth and Qing Pan for their helpful
comments and suggestions. We also thank the Associate Editor and
anonymous reviewers for their insightful and helpful comments and
suggestions.

Funding: National Institutes of Health (DK-075004 to Y.L.).

Conﬂict of Interest: none declared.

REFERENCES

Allison,D.B. et al. (2002) Amixture model approach for the analysis of microarray gene
expression data. Comput. Stat. Data Anal, 39, 1720.

Barrett,T. et al. (2007) NCBI GEO: mining tens of millions of expression proﬁlcsi
database and tools update. Nucleic Acids Res., 35, D7607D765.

Benjamini,Y. and Hochberg,Y. (1995) Controlling the false discovery rate: a practical
and powerful approach to multiple testing. J. R. Stat. Soc. Ser B, 57, 2897300.
Benjamini,Y. and Yekutieli,D. (2001) The control of the false discovery rate in multiple

testing under dependency. Ann. Stat, 29, 116571188.

Broberg,P. (2005) Acomparative review of estimates of the proportion unchanged genes
and the false discovery rate. BM C Bioinformatics, 6, 199.

Cancer Genome Atlas Research Network (2008) Comprehensive genomic
characterization deﬁnes human glioblastoma genes and core pathways. Nature,
455, 106171068.

Chen,Z. et al. (2005) Where CD4+CD25+ T reg cells impinge on autoimmune diabetes.
J. Exp. Med., 202, 138771397.

Cui,X. and Churchill,GA. (2003) Statistical tests for differential expression in cDNA
microarray experiments. Genome Biol., 4, 210.

Dalmasso,C. et al. (2005) A simple procedure for estimating the false discovery rate.
Bioinformatics, 21, 669668.

Dudoit,S. et al. (2003) Multiple hypothesis testing in microarray experiments. Stat. Sci.,
18, 717103.

Efron,B. (1979) Bootstrap methods: another look at the jackknife. Ann. Stat, 7, 1726.

Efron,B. (2007) Correlation and large-scale simultaneous signiﬁcance testing. J. Am.
Stat. Assoc., 102, 937103.

Flechner,S.M. et al. (2004) De novo kidney transplantation without use of calcineurin
inhibitors preserves renal structure and function at two years. Am. J. Transplant, 4,
177671785.

Guan,Z. et al. (2008) Nonparametric estimator of false discovery rate based on Bernstein
polynomials. Stat. Sin., 18, 9057923.

Harvey,B.G. et al. (2007) Modiﬁcation of gene expression of the small airway epithelium
in response to cigarette smoking. J. Mol. Med., 85, 39753.

Jiang,H. and Doerge,R.W. (2008) Estimating the proportion of true null hypotheses for
multiple comparisons. Cancer Inform, 6, 25732.

Ji,Y. et al. (2005) Applications of beta-mixture models in bioinforrnatics.
Bioinformatics, 21 , 211872122.

Jung,S-l-I. (2005) Sample size for FDR-control in microarray data analysis.
Bioinformatics, 21, 309773104.

Lai,Y. (2007) A moment-based method for estimating the proportion of true null
hypotheses and its application to microarray gene expression data. Biostatistics,
8, 7447755.

Langaas,M. et al. (2005) Estimating the proportion of true null hypotheses, with
application to DNA microarray data. J. R. Stat. Soc. Ser B, 67, 5557572.

Liao,J.G. et al. (2004) A mixture model for estimating the local false discovery rate in
DNA microarray analysis. Bioinformatics, 20, 269442701.

Lockhart,D. et al. (1996) Expression monitoring by hybridization to high-density
oligonucleotide arrays. Nat Biotechnol., 14, 167571680.

Lu,X. and Perkins,D.L. (2007) Re-sampling strategy to improve the estimation of
number of null hypotheses in FDR control under strong correlation structures. BM C
Bioinformatics, 18, 157.

McLachlan,GJ. et al. (2006) A simple implementation of a normal mixture approach
to differential gene expression in multiclass microarrays. Bioinformatics, 22,
160871615.

McLachlan,GJ. and Krishnan,T. (2008) The EM algorithm and extensions, 2nd edn.
John Wiley & Sons, Inc., Hoboken, New Jersey, pp. 18726.

Mootha,V.K. et al. (2003) PGC-la-response genes involved in oxidative phos-
phorylation are coordinately downregulated in human diabetes. Nat Genet, 34,
2677273.

Mosig,M.O. et al. (2001) A whole genome scan for quantitative trait loci affecting
milk protein percentage in Israeli-Holstein cattle, by means of selective milk DNA
pooling in a daughter design, using an adjusted false discovery rate criterion.
Genetics, 157, 168371698.

Nagalakshmi,U. et al. (2008) The transcriptional landscape of the yeast genome deﬁned
by RNA sequencing. Science, 320, 134471349.

Nettleton,D. et al. (2006) Estimating the number of true null hypotheses from a
histogram of p values. J. Agric. Biol. Environ. Stat, 11, 3377356.

Pounds,S. and Morris,S.W. (2003) Estimating the occurrence of false positives and false
negatives in microarray studies by approximating and partitioning the empirical
distribution of p-values. Bioinformatics, 19, 123671242.

Pounds,S. and Cheng,C. (2004) Improving false discovery rate estimation.
Bioinformatics, 20, 173771745.

Pounds,S. and Cheng,C. (2005) Sample size determination for the false discovery rate.
Bioinformatics, 21, 42634271.

Pounds,S. and Cheng,C. (2006) Robust estimation of the false discovery rate.
Bioinformatics, 22, 197971987.

Salvatore,P. et al. (2008) Detrimental effects of Bartonella henselae are counteracted by
L-arginine and nitric oxide in human endothelial progenitor cells. Proc. Natl Acad.
Sci. USA, 105, 94279432.

Scheid,S. and Spang,R. (2004) A stochastic downhill search algorithm for estimating
the local false discovery rate. IEEE Trans. Comput. Biol. Bioinform., 1, 987108.

Schena,M. et al. (1995) Quantitative monitoring of gene expression patterns with a
complementary DNA microarray. Science, 270, 467470.

Singh,D. et al. (2002) Gene expression correlates of clinical prostate cancer behavior.
Cancer Cell, 1, 2037209.

Storey,J.D. (2002) A direct approach to false discovery rates. J. R. Stat. Soc. B, 64,
479498.

Storey, JD. and Tibshirani,R. (2003) Statistical signiﬁcance for genomewide studies.
Proc. Natl Acad. Sci. USA, 100, 944e9445.

Tsai,C-A. et al. (2003) Estimation of false discovery rates in multiple testing: application
to gene microarray data. Biometrics, 59, 107171081.

Wang,S-J. and Chen,J.J. (2004) Sample size for identifying differentially expressed
genes in microarray experiments. J. Comput. Biol., 11, 7144726.

Wilhelm,B.T. et al. (2008) Dynamic repertoire of a eukaryotic transcriptome surveyed
at single-nucleotide resolution. Nature, 453, 123971243.

 

646

112 /3.IO'S[1211anI’pJOJXO'SOIJBLUJOJIIIOIq”Idllq uion papeolumoq

9103 ‘{g isnﬁnv 110::

