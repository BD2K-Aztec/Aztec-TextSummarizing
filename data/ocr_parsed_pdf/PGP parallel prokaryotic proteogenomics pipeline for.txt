BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

A.Tovchigrechko et al.

 

as experimental evidence. One example is the Mycobacterium
tuberculosis H37Rv genome (ftp://ftp.ncbi.nih.gov/genomes/
Bacteria/Mycobacterium_tuberculosis_H37Rv_uid57777/NC_
000962. gbk) containing the CDS attributes /
experiment=“EXISTENCE: identiﬁed in proteomics study”.

2 ARCHITECTURE AND IMPLEMENTATION
2.1 Parallelization strategy

In the present work, our main goal was to make the same pipe—
line protocol portable across different parallel execution envir—
onments that users are likely to encounter. The original
algorithm is embarrassingly parallel for the most part. It pro—
cesses each spectrum ﬁle independently throughout all computa—
tionally intensive stages of the algorithm. There is a global
synchronization point in the middle to build a histogram of all
scores for P—value computation. Thus, the pipeline corresponds
to a distributed workﬂow where multiple serial processes are
executed concurrently following a dependency graph deﬁned
by required input and output ﬁles. This model is compatible
with a wide variety of execution environments such as standalone
multicore machines, HTC clusters and, with extra effort, MPI
clusters. The original unreleased implementation used HTC
model tied into VICS and SGE.

We have now achieved the portability across execution envir—
onments by generating and running the same workﬁow under
the Makeﬂow engine (http://nd.edu/~ccl/software/makeﬂow/)
(Thrasher et al, 2010) that provides parallel execution on mul—
tiple types of batch queuing systems as well as on standalone
multicore nodes. On MPI clusters, Makeﬂow uses ‘glide—in’ ap—
proach that we describe in PGP software manual. In short, the
‘glide—in’ approach emulates an HTC cluster inside a single large
MPI job.

It will be also trivial to deploy our pipeline behind any Web
services front—end such as Galaxy (Giardine et al, 2005) or
Taverna (Wolstencroft et al, 2013). Each run of the pipeline
appears to the caller as a single command—line invocation of
the entry point script that exits only once it finishes executing
its parallel workﬂow. Backend options (batch queue or local
multicore) are passed through the command arguments. No per—
manently running server components are used by Makeﬂow.
Deployment in Galaxy, for example, would be the same as de—
ployment of a simple serial tool, requiring creation of a single
XML tool description ﬁle.

2.2 Installation and execution

Newly developed installation procedure and documentation are
part of the source code repository. The step—by—step installation
and usage manual (also shown on the landing page at BitBucket)
covers the execution environments, speciﬁc examples of conﬁg—
uration ﬁles for each environment and instructions for adapting
these ﬁles to new compute clusters.

The manual also covers sample run—time parameters for dif—
ferent environments, Quick Start instructions for testing the pipe—
line on a small dataset included in the repository and example of
interpreting the pipeline’s output to discover a novel gene. The
automated conﬁguration and build procedure is driven by
CMake (http://www.cmake.org/). Our installation procedure
builds its own local copy of the Makeﬂow and several prote—
omics tools from (http://proteomics.ucsd.edu).

Funding: National Science Foundation awards (BF—0949047 and
1048199), and XSEDE allocation (DEB100001) on the Texas
Advanced Computing Center Ranger. The funders had no role
in the study design, data collection and analysis, decision to pub—
lish or preparation of the manuscript.

Conﬂict of Interest: none declared.

REFERENCES

Aziz,R.K. et ul (2008) The RAST Server: rapid annotations using subsystems tech—
nology. BMC Genomics, 9, 75.

Chapman,B. et ul (2013) Plant Proteogenomics: from protein extraction to im—
proved gene predictions. Metliods Mol Biol, 1002, 2677294.

Giardine,B. et ul (2005) Galaxy: a platform for interactive large—scale genome ana—
lysis. Genome Res., 15, 145171455.

Kumar,D. et ul (2013) Proteogenomic analysis of Bradyrhizobium japonicum
USDA110 using GenoSuite, an automated multi—algorithmic pipeline. Mol
Cell. Proteomics, 12, 338873397.

Markowitz,V.M. et ul (2008) The integrated microbial genomes (IMG) system in
2007: data content and analysis tool extensions. Nucleic Acids Res., 36,
D5287D533.

Risk,B.A. et ul (2013) Peppy: proteogenomic search software. J. Proteome Res., 12,
301973025.

Sanders,W.S. et ul (2011) The proteogenomic mapping tool. BM C Bioiiy’ormutics,
12, 115.

Thrasher,A. et ul (2010) Taming complex bioinformatics workﬂows with weaver,
makeﬂow and starch. In: 2010 5th Workshop on Workﬂonn‘ in Support of Large—
Scale Science (WORKS). IEEE, pp. 176.

Venter,E. et ul (2011) Proteogenomic analysis of Bacteria and Archaea: a 46 or—
ganism case study. PLoS One, 6, e27587.

Wolstencroft,K. et ul (2013) The Taverna workﬂow suite: designing and executing
workflows of web services on the desktop, web or in the cloud. Nucleic Acids
Res., 41, W5577W561.

 

1470

ﬁm'spzumofpmﬂo'sopnuuopnorq/ﬁdnq

