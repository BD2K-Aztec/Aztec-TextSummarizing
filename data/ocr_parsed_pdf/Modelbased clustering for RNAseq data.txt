BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Y.Si et al.

 

performance was not evaluated for RNA—seq data analysis.
Studies of clustering algorithms with microarray data revealed
that heuristic algorithms performed worse than model—based al—
gorithms (Yeung et al., 2001). Surprisingly, there has been few
published statistical research to examine cluster analysis of
RNA—seq data, although it is urgently needed due to the huge
amount of data being generated. Model—based algorithms for
microarray data are based on ﬁnite mixture of normal distribu—
tions and cannot be directly applied to RNA—seq data that are
discrete counts and often skewed. RNA—seq data have been mod—
eled using Poisson (Bullard et al., 2010; Marioni et al., 2008) or
negative binomial (NB) distributions (Robinson et al., 2010).
Witten (2011) describes a hierarchical clustering method to clus—
ter samples (experimental units) based on the RNA—seq data of
all genes within each sample using Poisson model and dissimi—
larity measure based on likelihood ratio statistics. Often the case,
as in Li et a]. (2010), clustering gene expression proﬁles is of
interest. In this article, we aim to cluster genes based on the
differential expression patterns across treatments using model—
based statistical methods. In other words, we are interested in
grouping genes that share the same or similar expression fold—
changes with respect to the mean expression level across all treat—
ments. To do this, we derive model—based clustering algorithms
for cluster genes based on either Poisson or NB models for
RNA—seq data, and we evaluate the performance of the model—
based approach and heuristic algorithms including the K—means
method to cluster genes.

We describe the Poisson and NB distributions in Section 2
and show how our model—based clustering method handles
both probability models in a unified fashion. We present an
expectation—maximization (EM) algorithm for estimating the
model parameters and cluster membership in Section 3.1. In add—
ition, a model—based initialization algorithm is proposed in
Section 3.2 to reduce the dependence on the initialization. We
also describe two stochastic versions of EM algorithms in Section
3.3 that are intended to reduce the chance of being trapped at
local solutions. A model—based hierarchical algorithm is pro—
posed in Section 3.4 to generate a hierarchical structure of the
clusters and allow more ﬂexibility of choosing cluster numbers.
In Section 4, we simulate data and compare the proposed
method with others using three commonly used criteria: sensitiv—
ity, speciﬁcity and mutual information (MI) (Booth et al., 2008;
Strehl and Ghosh, 2002; Woodard and Goldszmidt, 2011). In
Section 5, we apply the model—based method to the data from
Li et a]. (2010) and evaluate our results by comparing the clusters
with gene annotations. We summarize in Section 6 that our
results from extensive simulation studies and an analysis of an
RNA—seq dataset all show that our proposed method outper—
forms alternative methods, namely, the K—means algorithm and
self—organizing map (SOM) (Ressom et al., 2003; Tamayo et al.,
1999).

2 MODEL

Let Ngil- denote the count of reads mapped to gene g for replicate
j of treatment 1' for g: 1, ---,G;i= 1, ---,I; j: 1, ---,n,~,
where G is the total number of genes of interest, I is the
number of treatment groups and 11,- is the number of replicates

for treatment 1'. Two discrete probability distributions have been

proposed to model RNA—seq data. The Poisson distribution has
been shown to be appropriate for the RNA—seq data when only
technical replicates are included (Bullard et al., 2010; Marioni
et al., 2008). When there are biological replicates, RNA—seq data
may exhibit more variability than expected with a Poisson dis—
tribution, i.e. the overdispersion phenomenon (Anders and
Huber, 2010). The NB model proposed by Robinson and
Smyth (2008) originally for serial analysis of gene expression
data allows overdispersion and has been applied to RNA—seq
data analysis (Anders and Huber, 2010; Robinson et al., 2010).
We consider both distributions in this article.

2.1 Poisson distribution

Suppose Ngi, follows a Poisson distribution with mean lg!) that is
parameterized as follows:

log ligi/ = ngj + org + ﬁg (1)

with ELI ﬁg 2 0. The offset term 5g” is a normalization factor
that may depend on the gene length and library of a sample such
as the total number of mapped reads of a library. Once estimated
from data, the normalization factor is often treated as known in
the model (Bullard et al., 2010; Marioni et al., 2008; Robinson
and Oshlack, 2010). The parameter ag represents the geometric
mean expression level of gene g across all treatments; ﬂgi meas—
ures the expression level of gene g in treatment 1' relative to the
overall mean expression. To cluster gene expression proﬁles, we
are interested in clustering the vectors ﬁg 2 (ﬂgl, - - - , ﬂgl) for all
G genes.

2.2 Negative binomial distribution

For the NB model, we adopt the parametrization in Robinson
and Smyth (2008) by modeling the variance as

Var(Ngl~/) = ,1,” + (ﬁg/1;, (2)
where ligi/ is the same as in (1) and (ﬁg is a dispersion parameter.
Compared with Poisson model, an extra parameter, (ﬁg, is intro—
duced for each gene. Robinson and Smyth (2008) described sev—
eral methods to estimate (ﬁg. In this article, we estimate (ﬁg by the
quasi—likelihood method. To simplify the algorithm, we treat (ﬁg
as known on its estimation because our numerical studies
showed this strategy produced similar clustering results to
those based on the true (ﬁg values (see Section 4.3). With this
strategy, the unknown parameters are the same for the Poisson
and NB models, and thus we denote the likelihood function for
both models by ﬂNglag, ﬁg) for gene g where N = {Ngij}.

3 MODEL-BASED CLUSTERING

Model—based clustering methods assume that data are generated by
a mixture of probability distributions where each component cor—
responds to one cluster. Extensive research has been done in model—
based clustering with multivariate normal mixture distributions.
See, for example, Fraley and Raftery (2002) for an excellent
review. In this section, we describe model—based clustering for
RNA—seq data with the probability models introduced in Section 2.

The algorithms described later in the text aim to cluster gene
expression proﬁles, which is desired in practical application.

 

198

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

Model-based clustering for RNA-seq data

 

Consequently, genes within the same cluster have similar expres—
sion proflles (denoted by ﬁg in our notation), but may have dif—
ferent overall mean expression levels (indicated by org). However,
it is straightforward to make changes in the algorithm if the goal
is to cluster according to both the overall expression levels and
the expression proﬁles, Gig + ﬁg.

Suppose there are K clusters and let Mk 2 (am, - - - , um)
denote the center of cluster k with ELI am 2 0 for k =
1, ---,K. The likelihood of the mixture model for gene g is
kakf(Ng|ag,ﬂg = Mk), where f(Ng|oig,ﬂg 2 uk) is the likeli—
hood if gene g belongs to the kth cluster and pk is the mixing
proportion with pk 3 0 and 21:1 pk = 1. The likelihood func—
tion can be based on a Poisson model or NB model as described
in Section 2. Taking all genes together, the likelihood is as
follows:

L = HZpMNglwg, ﬁg = Mk) (3)
g k

Note that we assume independence among genes, which is likely
not true in real situations. However, it is difficult, or impossible,
to model and estimate the correlation among tens of thousands
of genes with only several replicates and no prior knowledge
about the relationship among genes. Thus, for simplicity, we
take the independence assumption as in previous model—based
cluster analysis for microarray studies (Yeung et al., 2001).

3.1 Model-based clustering with the
expectation-maximization algorithm (MB-EM)

The EM algorithm has been widely applied to model—based clus—
tering with multivariate normal mixture distributions (Fraley
and Raftery, 2002). McLachlan (1997) describes an EM algo—
rithm to fit overdispersed univariate count data in Poisson re—
gression and logistic regression setting. Here, we derive an EM
algorithm (Algorithm 1) for clustering RNA—seq gene expression
proﬁle with a mixture of Poisson or NB models. Let ng = 1 if
gene g belongs to the kth cluster and ng = 0 otherwise. The EM
algorithm views the cluster memberships Z: {ng : g = 1, ---,
G; k = 1, - - - , K} as missing data and proceeds by iteratively cal—
culating the conditional expectations of Z and updating the
estimates for model parameters until convergence:

Algorithm 1: MB-EM Algorithm.

(i) Initialization: Set p21) according to prior knowledge about
the cluster size. If no such information is available, let

p21) 2 l/K for k: l, ---,K. Choose K
nil), ---,u(1;) with ELI/L2.) = 0for k: 1, ---,K as the
initial set of cluster centers. See Algorithm 2 for one way

VECIOI’S

to choose these nil). Obtain the initial values of
(1(1) 2 {(122 :g: l, ---,G;k= l, ---,K} by maximizing
ﬂNgIOtgk, a?) with respect to 0th for each combination of
gene g and cluster k.

(ii) E—step: Calculate the conditional expectation of ng given
data and parameters estimated from the mth step

(M(m),11(m),0l(m)), where MW) = mg") I k = 1, 

_ . . . A )
k_ l, ---,K}. To szmplify the notation, we use 

to denote the conditional expectation E(ng|N,/x(m),
p(m>,a(m>)

A m) _ plWngagﬁl #5:")
k _—.
g Eplmnzvglagtln’")

(iii) M—step: Update the parameter estimates by

My“) : argmax :22? log f<Ng|ag£),/zk)
[Eh/{i=0} g

l A
p2m+ ) : ZZEZVG
g

(4)

and

org“) 2 argmaxf<Ng|oigk, MENU)
051:]:
where Z2? is obtained from from step (ii).
(iv) Return to step ( ii ) or stop the iteration change of the total
log—likelihood is small.
(v) For each g = 1, - - - , G, assign gene g to cluster k if
k = argmax, ZgI, where 2g, is obtained after the conver—
gence of aforementioned steps.

Note that Algorithm 1 not only assigns gene g to cluster k
but also provides a measure of the uncertainty in the assignment
by 1—2gk. If clustering based on Gig + ﬁg is preferred, then
we do not estimate agk but estimate wk together with uk
and corresponding calculations in step (i}(iii) can be easily
modiﬁed.

3.2 Initialization

It is well known that initialization of the cluster centers impacts
both the speed of convergence and the outputs of the EM algo—
rithm (Fraley and Raftery, 2002; Hall et al., 1999; Park et al.,
2005). To tackle this problem, Arthur and Vassilvitskii (2007)
proposed to pick the initial cluster centers from observations in
a specific way such that they are well separated from each other
with respect to some distance measure. Following this idea,
rather than choosing K genes uniformly at random from
all genes and using their expression proﬁles as the initial cluster
centers, we only choose one cluster center uniformly at random
and then set the additional centers gradually by selecting
genes based on the distance between each gene and each of
the selected centers. Here, the distance is measured by likelihood
function.

Algorithm 2: Model-based Initialization for Cluster Centers.

(i) Choose one gene randomly from all genes, and set the initial
center for cluster 1, #31), to be the maximum likelihood
estimate (MLE) of ﬁg of the selected gene.

(ii) Given m center(s), M11), ---,/x§n1) for l f m<K,
selected from previous steps, calculate the measure of the
distance, dgl, between each gene g and each previously

 

199

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

Y.Si et al.

 

selected cluster center a)” by

d 1 maxageR, Zﬁgpo f(Ng|aga ﬁg)
g/ = Og—
maxageR f(Ng|aga ﬁg : ME”)

for g: 1, ---,G; l: 1, ---,m. Then randomly select a
gene with probability qg = dé/ 23:10? for dg =min
{dg1, ---,dgm} and set a new center [4,141 as the MLE of
ﬁg for the selected gene in this step.

(iii) Repeat step ( ii ) until K cluster centers are obtained.

By the definitions of dg and qg in step (ii) of Algorithm 2, a
gene is more likely to be selected if it is far away from all existing
centers. Hence the K centers chosen by this algorithm are
expected to be separated better than a set of centers that are
randomly selected. Our simulation study shows that this
algorithm improves the performance of EM algorithm
(Section 4.4).

3.3 Other algorithms for model-based clustering

The EM algorithm does not guarantee global optimal solutions.
Several stochastic algorithms have been proposed to reduce the
risk of being trapped in local solutions. We describe two in this
subsection and will examine their performances in our analysis.
Both algorithms modify Equation (4) to calculate Z22?) in step (ii)
of Algorithm 1.

(a) According to the deterministic annealing (DA) algorithm
described in Rose (1998), the cluster in the m th iteration
step is updated by

UL"
171:") MM; lag), MW]

1/ rm
BMWmﬁMW]

Am):

(5)

(b) The classiﬁcation expectation maximization (CEM) algo—
rithm with simulated annealing (SA) proposed by Celeux
and Govaert (1992) updates the estimate of ng by

l/rm
( ) ( ) ( )
2W)_ [afﬂAgm;,ur>]
gk 2 ("1qu law) (no) l/rm
l P/ g gl ’1‘!

Both algorithms use the annealing procedure with a sequence
of preselected annealing rates (‘temperatures’, rm) decreasing to
zero from a positive number. Apparently, when fixing rm 2 1,

(6)

both algorithm updates the values of Z2? the same way as the
EM algorithm. Hence, Algorithm 1 can be viewed as a special
case with a constant annealing rate rm 2 1. As rm —> 00, we
always get Z1? 2 pk for DA algorithm and 1 /K for SA algo—
rithm, which means that genes are assigned to each cluster totally
randomly. On the other hand, as rm —> 0 the randomness is
gradually lost and we ﬁnally get ng = 0 or 1, i.e. a hard cluster
solution. Hence, rm determines the amount of randomness added
in each step while searching for solutions. To apply these algo—
rithms, we follow the suggestions of Rose (1998) and use
Tm+1 2 0.91m with 1'1 2 2.

For the SA algorithm proposed in Celeux and Govaert (1992),
another difference from the EM algorithm (Algorithm 1) is that,
before updating parameter values in the M—step, each gene is
assigned to a cluster based on one random draw from a multi—

nomial distribution with probabilities Z22?) as calculated by
Equation (6).

3.4 Model-Based Hybrid-Hierarchical Clustering
Algorithm

So far, we have assumed that the number of clusters, K, is pre—
determined. For a real data analysis, this quantity often needs to
be estimated. There are different methods that can be applied to
estimating K. For instance, choose the K that minimizes the
Akaike information criterion (AIC) for the mixture model.
Alternatively, instead of choosing a single value of K for the
clustering analysis, we can build a hierarchical tree of clusters.
The hierarchical structure of the clusters provides information
about the relationships of clusters and allows ﬂexibility of
obtaining different number of clusters by cutting the tree at
different levels.

There can be tens of thousands of genes from RNA—seq data,
and treating each gene as the smallest cluster at the bottom of the
tree requires intensive computation. To speed up the calculation,
we propose to use agglomerative (bottom—up) strategy starting
with K0 clusters, where K0 is a number relatively large to allow
enough resolution but far less than the number of genes, G. The
initial K0 clusters can be obtained by the model—based clustering
algorithms described in the previous subsections. In each of the
following steps, two clusters are merged if the ‘distance’ between
them is the smallest among all possible pairs. Finally after K0 — 1
steps, all genes belong to a single cluster and the hierarchical tree
is built up. Such an algorithm has been called hybrid—hierarchical
(HH) clustering algorithm (Vaithyanathan and Dom, 2000;
Zhong and Ghosh, 2003). Here, the term ‘hybrid’ is used to
point out that the HH algorithm combines the starting steps
that obtain K0 clusters using non—hierarchical methods and the
merging steps that are similar to ordinary hierarchical clustering.

After the mth (0 g m<K0) merging step, we denote the
K0 — m clusters by disjoint sets g1, g2, - - - , gKWn, and calculate
the distance between two clusters, say g1, and g,, by Equation:

n f(Ng|a,(gk)’/Lk) llf(Nglag)aM/)
£6 1

9/1
de=m“ ,
Fl “Nightly, Mam)
Keg/(U91

 

(7)

where Gig“) and Mk maximize the likelihood f (Nglaig, Mk), and MW)
is the center of the cluster formed by merging g, and g,. This
distance is the reduction of total log—likelihood from before to
after the mergence. Obviously, merging clusters with the minimal
distance defined in (7) aims to achieve the maximum log—likeli—
hood in each step (Fraley, 1999; Meila and Heckerman, 2001).

4 SIMULATION STUDY

We conducted simulation studies to compare model—based clus—
tering methods with other methods, including K—means and
SOM, which have been popularly used in microarray data ana—
lysis and could also be applied to analyzing RNA—seq data.

 

200

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

Model-based clustering for RNA-seq data

 

We ﬁrst describe how data were generated in Section 4.1 and
present the criteria used to evaluate the clustering performance in
Section 4.2. Then we check the validity of treating the estimated
dispersion parameter (ﬁg as known for NB models in Section 4.3
and evaluate the model—based initialization algorithm (Algorithm
2) versus random initialization in Section 4.4. Finally, in Section
4.5, we compare our proposed algorithms with others.

4.1 Data simulation

We considered an experiment with three treatment groups and
three replicates for each treatment group. This is a case easily
encountered in real data analysis. Suppose that there were K = 7
different expression patterns across three treatments and the
cluster centers were characterized by Mk 2 W61“ where 17,, deter—
mined the magnitude of gene expression changes across treat—
ments and 6k 2 (6k1,6k2,6k3) described the pattern of changes
for cluster k, for k = 1, ---,K. A larger 17,, means larger dis—
tances between the centers and better separation of clusters.
The distinct proﬁles characterized by (6k1,6k2, 61,3) are listed as
follows:

 

Cluster k| 1 2 3 4 5 6 7
61d —1 —l 0 0 l l 0
(3kg 0 l —l l —l 0 0-
61,3 1 0 l —l 0 —l 0

For the ﬁrst cluster, the expression of genes increases from the
ﬁrst treatment group to the second one and increases further for
the third treatment group. For the second cluster, the expression
increases from ﬁrst treatment group to the second one but then
decreases for the third group. Note that the last cluster has a
mean proﬁle identically zero and this cluster corresponds to the
group of genes that are non—differentially expressed across treat—
ments. Although only identified differentially expressed genes are
typically included in the cluster analysis, there could be false
positives on the list of identiﬁed genes. For the simulation
study, we included this cluster of non—differentially expressed
genes to make our simulation more general and did not expect
this to affect the relative ranking of the evaluated methods.
RNA—seq data for G: 10000 genes were simulated for
each dataset according to the following regime. For each
g: 1, ---,G, 22 2 {22k :k: 1, ---,7} was drawn independ—
ently from a multinomial distribution with equal probabilities,
where Zg, = 1 means gene g belongs to cluster k and ng = 0
otherwise. Given ng = 1, the gene expression proﬁle was simu—
lated according to ﬁg 2 uk + 6g, where Mk 2 nﬂdk as described
earlier in the text and 6g 2 (egl, eg2,eg3) added ﬂuctuation
around cluster center Mk speciﬁcally for gene g. We sampled egi
for i = 1,2, 3 from nMnEN(0,0.22), where 116 controlled the level
of ﬂuctuation relative to the cluster center W61,“ The overall
mean expression level Gig was drawn from naN(4, 1), where no,
controlled the magnitude of average expression level. The disper—
sion parameter (ﬁg was simulated from n¢Gamma(0.75, 2), where
Gamma(0.75, 2) is a gamma distribution with mean 0.75/2 and
variance 0.75/22. Changing the value of 11¢ allowed different
levels of dispersion. Specially, 11¢ = 0 corresponds to the
Poisson model, which is the limiting case of NB model as the
dispersion approaches zero. The normalization factor sgi, was

generated from N(0, 1). Given these parameters, the gene expres—
sion count Ngi, was generated from the NB model with expect—
ation exp(sg[; + Gig + ﬁg) and dispersion (ﬁg.

Once the dataset was simulated, we treated all parameters
except sgi, as unknown to resemble a real experiment. The
values of 11M, 116, no, and 11¢ were varied to create different simu—
lation settings, and 100 datasets were independently simulated
for each setting.

To test the robustness of our model, we also simulated data
according to a generalized linear mixed model (GLMM)
Ng,~,«~NB(exp(sg,~,« + Gig + um + 6g + ygij), (ﬁg). Here, we added a
random effect ygi/ to the expected expression, where ygi/ is speciﬁc
for each combination of gene and sample. ygij was drawn from a
normal distribution nMnEN(0, 0.12). With this GLMM model, we
have overdispersed data compared with the NB model that we
assume in (3). The results based on data simulated from both
models [GLMM and the NB model with expectation
exp(sg[/ + Gig + ﬂgi)] are similar, and our conclusions are the
same. So we only present the results based on the NB model.

4.2 Assessment of performance

We assessed the performances of different clustering approaches
by comparing the resulting partitions with the original partition
of genes deﬁned by Z0 = {22 : g = 1, - - - , 10000}. A better per—
formance is indicated by more agreement between the two par—
titions. The following three statistics were used to evaluate the
agreement. For all the three statistics, higher values indicate
better performance.

(1) Pairwise sensitivity: the proportion of pairs of genes (ob—
jects) that are clustered together among all pairs that had
the same original assignment (Booth et al., 2008; Woodard
and Goldszmidt, 2011).

Pairwise specificity: the proportion of pairs of genes (ob—
jects) that are clustered to different groups among all pairs
that had different original assignment (Booth et al., 2008;
Woodard and Goldszmidt, 2011).

(3) Normalized mutual information (NMI): MI is used in in—
formation theory to measure the amount of information
one random variable contains about another, or equiva—
lently, the reduction in the uncertainty of one due to the
knowledge of the other. Here, MI is used to quantify
the shared information between the true partition and
the clustering result. See Strehl and Ghosh (2002) for the
explicit formula for calculation using the contingency table
formed by the two partitions. MI value is high if there is
strong dependence (more shared information) between the
two partitions, and is close to zero otherwise. Because
there is no upper bound for MI, its normalized version
ranging from 0 to 1 is often desirable for easier comparison
(Strehl and Ghosh, 2002).

(2

v

4.3 Validation of estimating dispersion parameters

We estimated the dispersion parameters (ﬁg and treated them as if
they were true values when applying the model—based clustering
algorithms. However, it is challenging to obtain good estimates
of dispersion parameters due to the small number of replicates in

 

201

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

Y.Si et al.

 

RNA—seq data. To examine the impact of the estimated param—
eters on cluster analysis, we compared the model—based clustering
methods using estimated values for (ﬁg versus that using the input
(true) values used to simulate the counts.

Figure 1a plots the values of sensitivity, speciﬁcity and NMI
for different clustering approaches over a range of m values used
to simulate RNA—seq data, whereas other parameters 11M, no, and
11¢ were ﬁxed at 1. As shown in Figure 1a, when K: 7 and at the
same level of 115, the MB—EM algorithms using true and esti—
mated dispersions perform indistinguishably as shown in
Figure 1a. In practice, the true number of clusters is unknown,
and we might apply a different number in cluster analysis, say
K: 10. Still, the clustering results from using true and estimated
dispersions are almost the same. We also varied parameters
1105,11,, and 11¢ one at a time while keeping others fixed at 1 to
generate RNA—seq datasets. The difference between using true
and estimated dispersions were small at most of the parameter
settings (see Supplementary Fig. S1). Consequently, all results
presented later were obtained using estimated dispersion param—
eters just like how we analyze real data.

It is worth pointing out that we cannot conclude that the re—
sults for K: 10 are better than that for K: 7, though the speci—
ﬁcity scores for the former are higher. Comparing the sensitivity
or speciﬁcity scores is not meaningful when the numbers of clus—
ters are different. For an extreme example, the sensitivity will
always be 1 when K: 1 because all gene pairs that had
the same original assignment will be clustered together.
Similarly, when choosing K as high as 10000, the speciﬁcities
will always be 1.

4.4 Comparison of initialization algorithms

In Figure 1b, we compared the initialization effects on the MB—
EM clustering results. Our proposed model—based algorithm
(Algorithm 2) and random initialization were examined.
Though initialization using true cluster centers is not applicable
in practice, we also included it in the comparison as a gold stand—
ard to evaluate the other two initialization methods. Figure 1b
clearly illustrates that the model—based initialization performs
much better than random initialization by giving higher evalu—
ation statistics for all parameter settings in simulation. In many
cases, the model—based approach generated results similar to
those when the true cluster centers were applied for initialization.
Results for other simulation settings are presented in
Supplementary Figure S2.

4.5 Comparison of MB cluster algorithms with others

We proposed EM algorithm (Algorithm 1) to perform model—
based clustering. However, it is possible that the resulting parti—
tion from EM algorithm is not a global optimum. Hence, two
stochastic versions, DA and SA algorithms, are described in sec—
tion 3.3 to reduce such risk. In this section, we compare these
slightly differing algorithms, whereas all three were initialized
with the same set of cluster centers chosen by Algorithm 2.
First, we did cluster analysis with the true number of clusters,
K: 7. Figure 1c and Supplementary Figure S3 suggest that all
three algorithms perform almost the same. We also analyzed the
same datasets with K: 10 (Fig. 1c and Supplementary Fig. S4).
Interestingly, Supplementary Figure S4 shows that the SA

algorithm typically achieves the highest sensitivity, whereas the
DA algorithm gains in terms of speciﬁcity. If practitioners are
more interested in sensitivity, getting groups of genes with similar
proﬁles, then the SA algorithm is recommended. If separating
genes with different proﬁles is more of interest, then DA algo—
rithm can be applied.

We also compared the proposed algorithms with K—means and
SOM, two methods that have been popularly applied to micro—
array analysis and can potentially be applied for RNA—seq data.
To cluster gene expression proﬁles, K—means and SOM were
applied to cluster the MLEs obtained based on the NB model,
i.e. the mean proﬁle of normalized RNA—seq data across repli—
cates for each gene. Plots in Figure 1c and d and Supplementary
Figures S3 and S4 show that, evaluated by all three criteria, the
model—based algorithms perform obviously better than K—means
and even better than SOM. Note that our simulation settings
include Poisson model, which is a special case when the disper—
sion parameter is set to be zero. We also did more simulations
with Poisson model and the results are similar to what are shown
here.

4.6 Choosing the number of clusters

One important question in the implementation of model—based
cluster analysis for real data is to choose the number of clusters,
K. Here, we evaluated the AIC. For given K, we can calculate the
likelihood L by (3) and the AIC by —2(logL — np), where
np : G(K + 1) + K] — 1 is the number of parameters in the
model. A low value of AIC indicates a better clustering result.
As shown in Figure 2a, the AIC identiﬁed the true number of
clusters being optimal.

5 REAL DATA ANALYSIS

Li et al. (2010) studied the maize leaf transcriptome using
Illumina Genome Analyzer 2. The dataset quantiﬁes transcript
abundance of four sections along a leaf developmental gradient,
with two biological replicates for each section. Using generalized
linear model analysis based on NB distribution, we found that
12631 genes were differentially expressed across the four sec—
tions. Li et al. (2010) normalized the count data by calculating
the values of reads per kilobase of exon model per million
mapped reads (RPKM), a popular quantification method pro—
posed by Mortazavi et al. (2008). In this section, on log—trans—
form and mean—center the RPKM values for each gene, we
obtained the log fold change estimates of the expressions relative
to the average expression of each gene. To these log fold change
estimates, we applied both the K—means, which has been used in
Li et al. (2010), and the SOM algorithms. We also present results
from the model—based clustering algorithms for the untrans—
formed count data based on NB model. One advantage of the
model—based approaches is that the Poisson or NB model can
handle genes with low counts easily. When sequencing depth is
low, there may be many genes with low counts or zero counts in
some replicates or treatment groups. However, this will induce
problems in the log—transformation, which is typically done
before applying K—means method. The following numerical
results also show that our proposed method provides better clus—
ters than both K—means and SOM algorithms.

 

202

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

53x\Ewogmoizmnnw.oxmoHEoﬁsiwbHQ

 
 

 

~—‘\
\

-~
‘

 

 

 
 

Y.Si et al.

 

 

AIC

m
V
A
V:

I
3000000
I

1100000 1150000 1200000

 

 

 

 

 

2000000
I

 

4 6 B 10 12 14 10 20 30 40 50

Number ol Clusters (K) Number of Clusters (K)

Fig. 2. Number of clusters. (21) The clustering results in the simulation
study were evaluated by the AIC. Under the simulation setting
nu : n“ : nE : no, : 1, 100 independent datasets were simulated.
Results are averaged over the 100 datasets, and the length of the vertical
bar at each point is the standard error of the mean of the score. (b) The
clustering results for the maize data were evaluated by the AIC (see
Section 5 for real data analysis)

As we expect that the genes within the same functional cat—
egory have correlated expression patterns and thus more likely to
be grouped together, a clustering result can be evaluated by
checking its concordance with the functional categories. Gene
annotations were obtained from Mapman as described in Li
et al. (2010). Excluding categories that contain <5 or >500
genes, we ended with 306 non—overlapping categories with a
total of 5002 genes. Because these annotations are independent
to the clustering processes, the evaluation is not biased toward
any clustering method and data model.

We first used K: 100 to cluster genes using both our model—
based method and the K—means method. The reason that we
chose K: 100 is because we presume that more clusters can
give better resolution of expression trends to the grouped genes
with the 306 Mapman categories. We are interested in genes that
show monotonic expression proﬁles along the leaf gradient, and
we found that genes in clusters 14, 18 and 21, which are the three
biggest clusters resulting from our model—based method, show a
monotonic decreasing pattern from base to tip, which may help
us to discover the biology that distinguishes base from other
sections (Supplementary Table in excel file). We found that 23
genes in cell wall functional category according to Mapman an—
notation are grouped into cluster 21. However, these genes are
scattered around different clusters obtained from the K—means
method. The cell wall functional category totally includes 165
genes. We noticed that in model—based method, the cell wall
related genes are enriched in cluster 14 (15 genes in cluster 14)
and 18 (15 genes in cluster 18), in addition to cluster 21 (23 genes
in cluster 21), which all represent the higher gene expression in
base. However, these genes were scattered into 23 clusters ob—
tained from K—means method, and there is no cluster identiﬁed
by K—means that includes >10 genes from this gene category.
Only by looking at these three clusters from model—based
method, we can clearly conclude that there was an active cell
wall metabolism at the basal part of developing leaf, which is
not easy to detect using the K—means method. In addition, cell
organization and DNA synthesis/chromatin structure pathways
were also enriched in cluster 21 in model—based method, which
suggested active cell construction and DNA replication in the
leaf base, and this is consistent with the active cell wall metab—
olism in the basal part of leaf. All these biological events were

v
A

5'
v

 

     

 

 

 

 
 

 

 

 

 

 

 

 

‘ E _
0'
N 53
S ‘ o' '
(u
9 _ S '
d
m — [UK—Means ; -
g ' -»- [2)SOM o — (1)HH(NB)
 [3)MB—EMINB)  ' .’ "' (2)HH(Euclidean)
w -- » [4)MB—DA(NB) ,o’ »- ~ (3)HH(Pealson)
: - —— [5)MB—SA(NB) g _ ' "" (4)HH(WGCNA)
I I I I I ‘3‘ I I I I
20 40 60 80 100 40 50 80 100

Number of Clusters Number 01 Clusters

Fig. 3. Clustering results for the maize dataset. (a) We compared our
proposed model-based algorithms (EM, DA and SA) with the K—means
and SOM methods. (b) MB-HH is compared with hierarchical clustering
based on Euclidean distance, Pearson correlation and similarity function
in WGCNA. They all start from 100 clusters obtained using their corre-
sponding distance measures

easily identified by the model—based method, but not the
K—means method.

To obtain a more quantitative analysis, we measured the con—
cordance between clustering results and gene functional cate—
gories by NMI. We performed cluster analysis with
K: 10,15,20, ---, 100 clusters for all five methods, including
SOM, K—means and the three model—based algorithms.
Figure 3a shows that the model—based algorithms outperform
SOM and K—means for all K values in terms of NMI. We then
applied the HH clustering as described in Section 3.4, starting
from K0 : 100 clusters obtained using the corresponding dis—
tance measures. We also applied hierarchical clustering using
average linkage based on Euclidean distance, Pearson correlation
and the adjacency (similarity) function in weighted gene co—ex—
pression network analysis (WGCNA) proposed by Zhang and
Horvath (2005). Our proposed HH method generated higher
NMI scores (Fig. 3b) than the other three hierarchical methods.
Examples of the clustering results for K: 20 and hierarchical
structures for the model—based hybrid—hierarchical clustering al—
gorithm (MB—HH) clusters are plotted in Figure 4 and
Supplementary Figures S5 and S6. These plots show that the
EM algorithms result in much cleaner expression patterns than
the clusters obtained from either K—means or SOM algorithm.

We also used the AIC criterion based on NB models, similarly
as in Section 4.6, to decide the number of clusters. We found
K: 15 is the optimal number of clusters by AIC (Fig. 2b).

6 DISCUSSION

In this article, we derived clustering algorithms based on ﬁnite
mixture of Poisson or NB models. We proposed an EM algo—
rithm with model—based initialization, and show this initialization
method greatly improves the performance of the EM clustering.
Compared with heuristic algorithms such as K—means method,
our method has the following advantages: First, we build our
approach of clustering RNA—seq data based on more appropri—
ate probabilistic models such as Poisson and NB distributions.
Owing to the nature of RNA—seq technology, the observed count
data are discrete and skewed. Poisson model has been shown to
fit well to data without biological replicates (Marioni et al., 2008)
and NB model to data with biological replicates (Anders and

 

204

ﬁm'spzumol‘pmjxo'sopeuuowrorq/ﬁdnq

East. NEED:
\«Cﬁo : kam
Aymwwfxms :2 :
APEX: AQZK
mag/fem fwﬁﬁ

