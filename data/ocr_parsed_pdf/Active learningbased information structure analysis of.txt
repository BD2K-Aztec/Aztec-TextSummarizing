BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Applications for information structure analysis

 

biomedical abstracts earlier by Guo et al. (2011b) and it has been
applied to various text classiﬁcation tasks (e.g. Brinker, 2006;
Esuli and Sebastiani, 2009; Hoi et al., 2006; Lewis and Gale,
1994; Novak et al., 2006; Silva and Ribeiro, 2007), achieving
performances close to that of fully supervised learning. In this
study, we investigate its performance with one of the most widely
used ML models7Support Vector Machines (SVM). In com—
parison with the related work on biomedical abstracts
(Guo et al., 2011b), we deal with the complexity of full articles
by performing a more detailed linguistic analysis (from sentence
splitting to feature extraction), by making use of features that
take context information into account, and by introducing
a wider range of active learning strategies (e.g. Query—by—
Committee).

Trained on 500 labeled sentences (6% of the corpus) our
method yields impressive accuracy of 82% on full articles, just
2% lower than that of fully supervised learning. In addition, we
introduce two novel applications where the idea is to use AZ to
support real—life literature review in biomedicine. We apply them
to the area of cancer risk assessment, and use them for task—
based evaluation of our AZ approach. The ﬁrst application
focuses on question answering. We investigate whether biomed—
ical researchers ﬁnd relevant information faster from AZ—anno—
tated than from unannotated full articles. The results show that
the AZ—annotations accelerate the process by 27730%, making
literature review considerably faster. The second application
focuses on customized summarization. We use the application
to create customized summaries for the conclusions of full art—
icles as many biomedical scientists are particularly interested in
these. The results show that sentences extracted from the
Conclusion zone are significantly more similar to gold—standard
summaries, with 4% higher ROUGE—1 F—score, than those
extracted from the Discussion section of articles. In sum, our
investigation shows that active learning can yield highly accurate
results for AZ of full—text articles and that its performance is high
enough to beneﬁt real—life tasks in biomedicine.

We make our AZ—annotated corpus available with this article,
together with our novel applications so that they can beneﬁt
further research in this area where publicly available resources
are scarce.

2 METHODS

2.1 Annotated corpus

We developed a corpus of 50 biomedical articles (consisting of 8171 sen-
tences and 234619 words) from a set of biomedical journals [e.g.
Carcinogenesis, Toxicological Sciences, Journal of Biological Chemistry,
among others (http://www.cl.cam.ac.uk/~yg244/l2bioinfo/readme.txt)],
and annotated them according to the AZ scheme that describes the
rhetorical progression of scientiﬁc text. The scheme was originally
introduced by Teufel and Moens (2002) who applied it to computational
linguistics articles. We used the version that Mizuta et al. (2006) adapted
for biology articles, with minor modiﬁcations concerning zone names:
Background (BKG), Problem (PROB), Method (METH), Result
(RES), Conclusion (CON), Connection (CN), Difference (DIFF) and
Future work (FUT). Zones BKG, PROB, METH, RES, CON
and FUT refer to the background of the study, the research question,
the methods used, the results obtained, the conclusions drawn and the
future directions, respectively. CN and DIFF refer to related work that is
consistent or inconsistent with authors’ work.

Table 1. Distribution of sentences (shown in percentages) in abstracts,
full articles and their individual sections in the AZ-annotated corpus

 

 

Text BKG PROB METH RES CON CN DIFF FUT

Abstract 14.6 10.9 15.8 37.5 21.0 7 7 0.2

Article 16.9 2.8 34.8 17.9 22.3 4.3 0.8 0.2
Introduction 74.8 13.2 5.4 0.6 5.9 0.1 7 7
Methods 0.5 0.2 97.5 1.4 0.2 0.2 0.1 7

Results 4.0 2.1 11.7 68.9 12.1 1.1 0.1 7
Discussion 16.9 1.1 0.7 1.5 63.5 13.3 2.4 0.7

 

We developed a tool that allows users to open an article in the Firefox
browser and to annotate the sentences with AZ categories. A biomedical
expert annotated all the 50 articles sentence by sentence so that each
sentence was assigned to a single zone (a practice followed by most vari-
ations of AZ annotation).

Table 1 shows the distribution of sentences in abstracts, full articles
and their individual sections in the annotated corpus. As section names
vary from article to article, we grouped similar names before calculating
the statistics. For instance, sections Case presentation, Experimental pro-
cedures, Materials and Methods were merged into Methods. Within full
articles, METH is the most frequent category, accounting for 34.8% of
the data, followed by CON, RES and BKG, accounting for 22.3, 17.9,
and 16.9% of the data, respectively. The four low-frequency categories
are FUT, DIFF, PROB and CN, covering 024.3% of the data each.
Full articles include a larger number of zones than abstracts, e. g. CN and
DIFF. Our statistics also show that although there is one major zone in
each section (e.g. BKG for Introduction), 2.5736.5% of the sentences still
belong to other categories, demonstrating that the scheme and the anno-
tations are informative. We measured the inter-annotator agreement
between the biomedical expert and a computational linguist on 15
articles. According to Cohen’s kappa (Cohen, 1960), annotators are in
a good agreement with K = 0.83.

2.2 Machine learning for AZ

In academic writing, zones tend to appear in sequential order. For
example, BKG is usually followed by PROB, and RES is followed by
CON. Therefore a natural approach to AZ would be a sequence model
such as Conditional Random Fields, which takes into account transition
probabilities. However, in weakly supervised (and in particular active)
learning, diversity of the selected labeled data is important. This is difﬁ-
cult to achieve with a sequence model where a selected sequence is an
entire article that may include hundreds of sentences but tends to be
limited in vocabulary and structures. Moreover, recent work has shown
that non-sequence models actually perform better than sequence models
in identifying information categories in full articles and short abstracts
(Guo et al., 2011b; Liakata et al., 2012). Hence we base our active learn-
ing approach on SVM7the most widely used non-sequence model7in
this work.

2.2.] S VM and active learning SVM aims to find the maximum-
margin hyperplane that separates the classes:

I!
ﬁlling %HWHZ+CZ§1’
1r. 7. [:1
Subject to y,»(w - X,» — b) 2 l — Si, Si 2 0,

where X,- is a data point and y,- is its label, w is a normal vector to the
hyperplane, S,» is a slack variable that measures the degree of misclassiﬁ-
cation for X,- and C is the penalty term. The parameters can be learned
using the SMO algorithm (Platt, 1999b). We used LIBSVM (Chang and
Lin, 2011) with linear kernel for our experiments.

 

ﬁm'spzumofpmjxo'sopnuuopnorq/ﬁdnq

Y.Guo et al.

 

 

Algorithm 1 Pool-based active learning

 

Require: labeled set L, unlabeled pool U, query strategy Q(-), query batch
size B, stopping criterion C
while !C do
//training on the current L
train(L)
for b = 1 to B do
//query the most informative instance
X* = arg min Q(x)

//move  labeled query from U to L
L = L U <X*,label(x*)>
U = U — x*
end for
end while

The idea of active learning is to reduce annotation cost by iteratively
selecting the most informative instances to be labeled and used as training
data for the next iteration. Algorithm 1 shows how pool-based active
learning works. The following query strategies were used for SVM-
based active learning:

Least conﬁdent sampling (Lewis and Gale, 1994) queries the instance
whose label the current model is the least conﬁdent about:

 

arg min Q(x) = arg min P(y* lx; 6).
er er

For SVM, a monotonic function (sigmoid function) was used to trans-
form the distance between data points and hyperplanes into posterior
probabilities (Platt, 1999a) and, because there were more than two classes,
the probabilities were combined by pairwise coupling (Wu et al., 2004).

Margin sampling (Scheffer et al., 2001) is another uncertainty sampling
strategy that queries the instance with the smallest margin between the
posteriors of the two most likely labelings:

arg min Q(x) = arg min P(yl*|x; 6) — P(y2* lx; 6),
er er

Query-by-Bagging (Abe and Mamitsuka, 1998) is a variation of the
Query-by-Committee strategy (Seung et al., 1992) for discriminative or
non-probabilistic models. It maintains a committee of models that are
trained on a portion of the current labeled data but that represent com-
peting hypotheses. The most informative instance is the one about which
the committee members disagree the most. Although this method has
been shown to work well, no previous work has applied this method to
information structure analysis, so we decided to compare it with other
query strategies in this work.

2.2.2 Features Each sentence was represented by features that had
proved promising in related works (Guo et al., 2011b; Merity et al.,
2009; Mullen et al., 2005; Teufel and Moens, 2002):

Section. Normalized section names (Introduction, Methods, Results,
Discussion).

Location (i/ii/iii). Each article/section/paragraph was divided into 10
equal parts. Location was deﬁned by the parts where the sentence begins
and ends.

Reference (i/ii). The number of citations/referred tables and ﬁgures in a
sentence (0, 1 or more).

Word. All the words in the corpus (a word feature equals 1 if it occurs
in the sentence and 0 if it does not; the rest of the features were deﬁned in
a similar way).

Bi-gram. Any combination of two adjacent words in the corpus.

Verb. All the verbs in the corpus.

Verb Class. Sixty verb classes obtained by spectral clustering (Sun and
Korhonen, 2009).

Tense and Voice. Tense and voice indicated by the part-of-speech
(POS) tag of main verbs and auxiliary verbs, e.g. havelVBZ belVBN
_|VBN indicates present perfect tense, passive voice. Previous work

such as (Guo et al., 2011c; Liakata et al., 2012) made use of the POS
tags of main verbs directly as features. In this work, we take into account
not only the main verbs but also the chain of corresponding auxiliary
verbs for a more accurate tense and voice analysis.

Grammatical relation (GR). Subject (ncsubj), direct object (d0l7j), indir-
ect object (iobj) and second object (obj2) relations involving verbs, e.g.
(ncsubj observed difference obj).

Noun (i/ii). The subjects/objects appearing with any verbs in the corpus
(extracted from GRs).

An elaborate sentence splitter and tokenizer was developed to deal
with complex biomedical terms and various types of citations in full-
text articles. The C&C POS tagger and parser (Curran et al., 2007)
trained on biomedical literature were used for extracting syntactic fea-
tures. We lemmatized the lexical items for all syntactic features using
Morpha (Minnen et al., 2001), and removed the words, bi-grams and
GRs with fewer than two occurrences.

2.2.3 Context information A well-written scientiﬁc article aims for a
logical ﬂow of ideas and connections (cohesion) between sentences.
Therefore, the context of a sentence can be a good indicator of its infor-
mation category. For instance, we do not know if the sentence ‘The most
consistent demographic variable inﬂuencing the MN frequency was age,
with MN frequency increasing significantly with age (citation).’ belongs
to CN or DIFF before we see the following sentence: ‘However, our
results indicated that there was no significant increase in MN frequency
among older workers compared with younger workers. . .’. Also, (Teufel
and Moens, 2002) showed that the label of the preceding sentence can
be an important feature for AZ with fully supervised learning. However,
because the labels of the surrounding sentences are not available in
weakly supervised learning, we used the features of both the target and
the surrounding sentences for active learning-based AZ.

2.2.4 Evaluation We evaluated the accuracy, precision, recall and
F-score of SVM with full or light supervision. We also plotted pairwise
receiver operating characteristic (ROC) (Landgrebe and Duin, 2007)
curves for a more detailed comparison between these methods. All results
were averaged across lO-folds using cross-validation to avoid conﬁrm-
ation bias. Each fold was used once as test data and the remaining 9-folds
as training (labeled and unlabeled) data.

2.3 Novel AZ-based applications for supporting
biomedical literature review

Many real-life tasks in biomedicine require review of scientiﬁc literature.
We developed two novel applications where the idea is to use AZ to
support literature review via question answering and summarization.
We used the applications to support the literature-intensive task of
cancer risk assessment of chemicals.

2.3.1 Question answering The information that is interesting to risk
assessment can be as general as Is it a study of humans, animals or cells? or
as speciﬁc as Do they refer to any supporting studies? Although section
names (e. g. Methods) can be an indicator of information of interest, many
sections are rich in information (e.g. in the Discussion section, references
to related work are usually mixed with the interpretations of new results),
and relevant information may be difﬁcult to ﬁnd. Guo et al. (2011a, c)
have shown that it is easier for biomedical researchers to ﬁnd relevant
information in AZ-annotated than unannotated abstracts. In this study,
we investigated whether AZ annotations can speed up the process of
reviewing full-text articles.

We developed an application where scientists can deﬁne and ﬁll in a
questionnaire relevant for their literature review, and which highlights,
for each question, the information in a scientiﬁc article, which is most
likely to answer the question according to our AZ annotation. We used

 

ﬁm'spumol‘pmjxo'sopeuuopnorq/ﬁdnq

".15 mi L. [1.1L (rt-1L 51nd
nsntont demographic
Hummer . mu‘ IthllH nuhm Lil 111m Il1ur-L‘ u l.‘ I'Iu nl‘ﬂ'll R.

mun litmulc un'LI mull.

quun , .
.ltii'u'bellip..

u m .\-'ll\ h'uqu-L‘nL

' 1 lin‘ (hm.

 

/310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

Y.Guo et al.

 

Table 3. Accuracy and F-scores for individual zones

 

 

 

Method Acc F-score

BKG PROB METH RES CON CN DIFF FUT
Baseline 0.77 0.69 7 0.93 0.80 0.71 7 7 7
Full supervision 0.84 0.80 0.40 0.95 0.88 0.80 0.50 7 0.11
Random selection 0.79 0.75 0.20 0.93 0.82 0.75 0.23 7 7
Active learning 0.82 0.77 0.42 0.94 0.86 0.77 0.36 7 7

 

The baseline did not use any labeled data. Fully supervised learning used ~7350 labeled sentences, and random selection and active learning used 500. We report results for the

best—performing active learning method: least conﬁdent sampling.

active learning use only 500 labeled sentences, selected randomly
or according to a particular query strategy. Because the perform—
ances of the three query strategies are similar to one another,
Table 3 shows the results for least conﬁdent sampling only.

Given the distribution of sentences in each section
(see Table 1), it is not surprising that the baseline performs so
well, with an accuracy of 0.77 and F—score ranging from 0.69 to
0.93 for the four major zones (BKG, METH, RES and CON).
However, we show that ML can identify also minor categories in
each section that enables a more detailed information structure
analysis. As shown in Table 3, fully supervised learning outper—
forms the baseline signiﬁcantly with 7% higher accuracy and so
does active learning with 5% higher accuracy. Notably, the ac—
curacy of active learning is just 2% lower than that of fully
supervised learning, which needs substantial labeled data for
training.

The highest F—score is observed for the METH zone, which
makes sense because 97.5% of the Methods section belongs to
this zone. Although CON is the second largest zone in the
corpus, its F—scores are 779% lower than those for RES, prob—
ably because the Discussion section is usually a more complex
mixture of CON and other zones than the Results section. The
low—frequency categories such as DIFF and FUT are not identi—
ﬁed owing to the lack of training data, but unlike the baseline
method, ML does find two more categories, CN and PROB. We
further looked into zones where the achievement of active learn—
ing is the most significant (e.g. PROB) or trivial (e.g. CON), as
illustrated by the pairwise ROC curves in Figure 2. We can see
that in both cases, active learning performs as well as full super—
vision, whereas random selection hardly identiﬁes a low—fre—
quency category.

Table 4 shows the accuracy of weakly supervised learning
when 5&500 labeled sentences are used. Active learning has a
clear advantage over random selection as the amount of labeled
data increases. When 25(P500 sentences are labeled, it performs
signiﬁcantly better than random selection with 273% higher
accuracy [with P< 0.001 in McNemar’s test (McNemar, 1947)].
The three query strategies perform similarly. Least conﬁdent
sampling performs slightly better than the other two with 1%
higher accuracy when 500 sentences are labeled.

3.2 Question answering and customized summarization

Table 5 shows the time it took experts A and B to answer ques—
tions using (i) unannotated articles, (ii) articles that highlight

 

   

 

 

 

 

 

1.0
0.8—
3' 0 6_ 3'
IE ' IE
.2 _/_" .2 /
2 I f' — -Full 2 / — -Fu||
$ 0 4' 17" - - - Random $ 0-4' /' - - - Random
_ y." — Active / ' — Active
0 2_ /. — - Reference 0 2_ /. — - Reference
. :/ :/
0‘0 I I I I I 0-0 I I I I I
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1 -Specificity 1 -Specificity

Fig. 2. ROC curves of full supervision, random selection and active
learning for PROB versus BKG (left) and CON versus BKG (right).
BKG is the most frequent category coming along with PROB/CON in
Introduction/Discussion

Table 4. Accuracy of weakly supervised learning with 5(P500 labeled
sentences

 

Method 50 100 150 200 250 300 400 500

 

Random selection 0.73 0.76 0.77 0.78 0.78 0.78 0.79 0.79
Active learning
Least conﬁdent 0.73 0.76 0.77 0.79 0.80 0.80 0.81 0.82
Margin 0.73 0.76 0.78 0.79 0.80 0.80 0.81 0.81
Query-by-bagging 0.73 0.77 0.78 0.78 0.79 0.79 0.80 0.81

 

zones annotated by humans and (iii) articles that highlight
zones learned by SVM through active learning, as well as the
percentage of time savings using AZ annotations. Because ques—
tions 378 are all related to the RES zone, we sum up the time it
took to answer those questions. Looking at the total time spent
on all the questions, experts found the information of interest
significantly faster from AZ—annotated articles (with P ranging
from 0.002 to 0.019) than from unannotated ones. They used 3&
42% less time when examining manually annotated articles (ii)
and 27730% when examining automatically annotated articles
(iii). The experts were in a good agreement and >89% of their
answers were the same. Looking at the results for individual
questions, AZ annotations are particularly useful for answering

 

ﬁm'spzumol‘piqxo'sopeuuopnorq/ﬁdnq

Applications for information structure analysis

 

questions related to PROB, CON and CN (Q1, Q9, Q10), but
not those related to METH or FUT (Q2, Q12) because the
METH zone overlaps with the Methods section to a large
extent, and the FUT zone is rarely seen in the corpus.

Table 6 shows examples of customized summaries focusing on
the conclusions of an article, where the zone—based summary was
extracted from the CON zone (learned through active learning)
and the section—based summary from the Discussion section.
Comparing the two auto summaries against the gold standard,
we can see that the zone—based summary is considerably more
accurate: it shares three sentences (in bold) with the gold stand—
ard, whereas the section—based summary only shares one (the rest

Table 5. The time (measured in seconds) it took experts A and B to
answer questions for (i) unannotated articles, (11) articles with highlighted
zones annotated by human and (111) articles with highlighted zones
annotated through active learning

 

Article Q1 Q2 Q%Q8 Q9 Q10 Q11 Q12 Total

 

A
(1) 35.8 21.4 125.4 44.2 21.6 26.8 18.6 293.8
(11) 16.4 21.4 82.2 29.6 11.8 15.2 12.2 188.8
(1)411) 54% 0% 34% 33% 45% 43% 34% 36%
(111) 20.4 15.4 99 33.4 16.8 11.6 10.2 206.8
(1)4111) 43% 28% 21% 24% 22% 57% 45% 30%
B

(i) 69.6 12 124 157 27.4 33.4 16.8 440.2
(11) 11.6 13 106 50.2 21 30.2 22.4 254.4
(1)411) 83% 78% 15% 68% 23% 10% 733% 42%
(111) 40.8 13.2 95.8 118.4 20.2 15.4 16.2 320

(1)4111) 41% 710% 23% 25% 26% 54% 4% 27%

 

(1)411) and (1)4111) refer to the percentage of time savings when using AZ
annotations.

Table 6. Examples of customized summaries focusing on the conclusions

contains background information or related work). Table 7
shows averaged ROUGE scores for zone—based and section—
based summaries. Zone—based summaries are significantly more
similar to the gold standard with 475% higher F—scores and
8710% higher precision than section—based summaries. The
recall scores for zone— and section—based summaries are similar,
suggesting that both schemes can retrieve equal amount of rele—
vant information. The differences in precision and F—score indi—
cate that zone—based summaries are more compact and precise
than section—based summaries.

4 DISCUSSION AND CONCLUSIONS

We have introduced an AZ—annotated corpus of 50 full biomed—
ical articles, twice as big as the one developed by Mizuta et al.
(2006) for biology articles, and the first publicly available
AZ—annotated corpus of scientiﬁc articles. Using this corpus,
we have investigated, for the ﬁrst time, whether a weakly
supervised approach is realistic for information structure analysis

Table 7. Averaged ROUGE scores for customized summaries: ROUGE-
1 for word co-occurrence, ROUGE-2 for bi-gram co-occurrence (any pair
of adjacent words) and ROUGE-SU4 for skip-bi-gram co-occurrence
(any pair of words in the same order as they appear in sentences)

 

 

 

ROUGE Zone-based Section-based

Recall Precision F-score Recall Precision F-score
ROUGE-1 0.49 0.60 0.51 0.50 0.50 0.47
ROUGE-2 0.32 0.37 0.33 0.30 0.29 0.28
ROUGE-SU4 0.33 0.39 0.34 0.32 0.31 0.29

 

 

Gold standard summary Zone-based summary

Section-based summarya

 

We found an elevated risk of colorectal

To overcome these limitations, we studied a

OCs have previously been associated with increased

cancer associated with high levels of
mono-ortho PCBs 28 and 118. In our
population, high body burdens of
mono-ortho PCBs 28 and 118 were
associated with an elevated risk of colo-
rectal cancer. Nevertheless, higher serum
levels of PCB-28 were associated with
increased risk of colon cancer in this
study. In our population, p,p’-DDE also
increased risk for tumors with wild-type
K-ras but not when this oncogene was
mutated. In conclusion, these results
suggest that exposure to mono-ortho
PCBs is associated with an increased risk
of colorectal cancer.

population not occupationally exposed
using serum OC levels as exposure
markers. We found an elevated risk of
colorectal cancer associated with high
levels of mono-ortho PCBs 28 and 118. In
our population, high body burdens of
mono-ortho PCBs 28 and 118 were asso-
ciated with an elevated risk of colorectal
cancer. These OCs are among the most
toxic of the PCBs, together with non-
ortho PCBs. The role of other OCs in
colorectal cancer risk may be more
complex. In conclusion, these results sug-
gest that exposure to mono-ortho PCBs is
associated with an increased risk of colo-
rectal cancer.

risk of colorectal cancers in studies of
occupationally exposed individuals

(Acquavella et al., 1996; Soliman et al., 1997;
Wilkinson et al., 1997). We found an elevated risk of
colorectal cancer associated with high levels of mono-
ortho PCBs 28 and 118. Many studies that also used
plasma OC concentrations as exposure markers
have reported mixed results for breast cancer (Calle
et al., 2002) and non-Hodgkin lymphoma (Cantor
et al., 2003; De Roos et al., 2003; Rothman et al.
1997) but increased risk of pancreatic cancer
(Hoppin et al., 2000; Porta et al., 1999; Slebos et al.,
2000). Some studies on breast cancer have also re-
ported increased risk being limited to mono-ortho
PCBs (Aronson et al., 2000; Demers et al., 2002;
Lucena et al., 2001).

 

"References cited in this column are from an example in our dataset.

 

1445

ﬁm'spzumofpiqxo'sopeuuopnorq/ﬁdnq

Y.Guo et al.

 

of full scientific articles. We have shown that an approach based
on active learning performs well with an accuracy of 82% when
using 500 labeled sentences. Nearly as good as fully supervised
learning (accuracy 84%), our results are promising, especially
considering the high linguistic and informational complexity of
full articles. We have also introduced two novel task—based evalu—
ations of AZ that involve doing literature review via question
answering and customized summarization. In our question an—
swering experiment, researchers find relevant information from
AZ—annotated articles 27742% faster than from unannotated
articles, regardless of whether manual or automatic annotations
are used. In the customized summarization experiment, sentences
extracted from a particular zone are signiﬁcantly more similar to
gold standard summaries, with 8710% higher precision, than
those extracted from a particular section. Both experiments
show that active learning—based AZ can support biomedical lit—
erature review.

Recent studies have shown promising results on weakly
supervised learning of information structure of biomedical ab—
stracts (Guo et al., 2011a, c). We have focused on full—text articles
whose information structure is considerably more complex than
that of abstracts. Despite the challenges, we have reported pro—
mising results in our direct and task—based evaluations.

In the future, we intend to improve this work in several direc—
tions. First, it may be useful to develop a more fine—grained
scheme for analyzing the information structure of full—text art—
icles. Reﬁning the existing scheme will offer an opportunity for a
more detailed analysis of the article’s information structure,
which is likely to benefit also real—life applications such as litera—
ture review. With respect to the weakly supervised learning, there
has been a lot of recent work on integrating declarative know—
ledge/constraints into standard ML models for improved per—
formance, such as Generalized Expectation (Mann and
McCallum, 2010) and Posterior Regularization (Bellare et al.,
2009). The power of these methods has been demonstrated for
a variety of natural language processing tasks, in particular for
text classiﬁcation (Druck et al., 2008). Thus, we plan to investi—
gate these methods for AZ on full articles and evaluate their
usefulness in real—world applications. We are also interested in
developing domain adaptation technologies for porting of AZ
across the sub—fields of biomedicine and other areas of science.

Funding: The work reported in this article was funded by the Royal
Society (UK), Swedish Research Council, EPSRC (UK) grant EP/
G051070/ 1 and EU grant 7FP—ITC—248064. Y.G. was funded
by the Cambridge International Scholarship. I.S. was funded
by the Swedish Governmental Agency for Innovation System.

Conﬂict of Interest: none declared.

REFERENCES

Abe,N. and Mamitsuka,H. (1998) Query learning strategies using boosting and
bagging. In: Proceedings of the Fifteenth International Conference on Machine
Learning. San Francisco, CA, USA, pp. 179.

Bellare,K. et al. (2009) Alternating projections for learning with expectation
constraints. In: Proceedings of the Twenty—Fifth Conference on Uncertainty
in Artificial Intelligence. UAI ’09 AUAI Press, Arlington, Virginia, USA,
pp. 43750.

Berger,A. and Mittal,V.O. (2000) Query—relevant summarization using FAQS. In:
Proceedings of the 38th Annual Meeting on Association for Computational

Linguistics. Association for Computational Linguistics, Hong Kong,
pp. 2947301.

Brinker,K. (2006) On active learning in multi—label classiﬁcation.
In:Spiliopoulou,M., Kruse,R., Borgelt,C., Nurnberger,A. and Gaul,W. (eds)
From Data and Information Analysis to Knowledge Engineering. Springer—
Verlag, Berlin/Heidelberg, pp. 20(r213.

Chang,C—C. and Lin,C.—J. (2011) LIBSVM: a library for support vector machines.
ACM Trans. Iritell. Svst. Technol., 2, 1727.

Cohen,J. (1960) A coefﬁcient of agreement for nominal scales. Educ. Psycho].
Meas., 20, 37416.

Cohen,K.B. et al. (2005) Corpus design for biomedical natural language processing.
In: Proceedings of the ACL—ISM B Workshop on Linking Biological Literature,
Oiitologies and Databases: Mining Biological Semantics. Association for
Computational Linguistics, Detroit, Michigan, pp. 38745.

Cohen,K.B. et al. (2010) The structural and content aspects of abstracts
versus bodies of full text journal articles are different. BMC Bioinformatics,
11, 492.

Curran,J.R. et al. (2007) Linguistically motivated large—scale nlp with c&c and
boxer. In: Proceedings of the ACL 2007 Demonstrations Session. Association
for Computational Linguistics, Prague, Czech Republic, pp. 33736.

Druck,G. et al. (2008) Learning from labeled features using generalized expectation
criteria. In: Proceedings of the 31st Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval. ACM, Singapore,
Singapore, pp. 5957602.

Esuli,A. and Sebastiani,F. (2009) Active learning strategies for multi—label text clas—
sification. In: Proceedings of the 31st European Conference on IR Research on
Advances in Information Retrieval. Springer—Verlag, Toulouse, France,
pp. 1027113.

Guo,Y. et al. (2010) Identifying the information structure of scientiﬁc abstracts:
an investigation of three different schemes. In: Proceedings of BioNLP.
Association for Computational Linguistics, Uppsala, Sweden, pp. 997107.

Guo,Y. et al. (2011a) A comparison and user—based evaluation of models of textual
information structure in the context of cancer risk assessment. BMC
Bioinformatics, 12, 69.

Guo,Y. et al. (2011b) A weakly—supervised approach to argumentative zoning of
scientiﬁc documents. In:Proceedingsof the 2011 Conference on Empirical
Methods in Natural Language Processing. Association for Computational
Linguistics, Edinburgh, United Kingdom, pp. 2737283.

Guo,Y. et al. (2011c) Weakly supervised learning of information structure of
scientiﬁc abstracts7is it accurate enough to beneﬁt real—world tasks in biomedi—
cine? Bioinﬁ)rmatics, 27, 317973185.

Hoi,S.C.H. et al. (2006) Large—scale text categorization by batch mode active learn—
ing. In:Proceedingsof the 15th international conference on World Wide Web.
ACM, Edinburgh, Scotland, pp. 6337642.

Landgrebe,T.C.W. and Duin,R.P.W. (2007) Approximating the multiclass ROC
by pairwise analysis. Pattern Recogn. Lett., 28, 174771758.

Lewis,D.D. and Gale,W.A. (1994) A sequential algorithm for training text classi—
ﬁers. In:Proceedingsof the 17th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval. Springer—Verlag New
York, Inc., Dublin, Ireland, pp. 3712.

Liakata,M. et al. (2010) Corpora for the conceptualisation and zoning of scientiﬁc
papers. In:Proceedingsof LREC '10. European Language Resources Association
(ELRA), Valletta, Malm.

Liakata,M. et al. (2012) Automatic recognition of conceptualisation zones
in scientiﬁc articles and two life science applications. Bioinformatics, 28,
99171000.

Lin,C.—Y. (2004) ROUGE: a package for automatic evaluation of summaries. In:
Text Summarizatioii Branches Out: Proceedings of the ACL—04 Workshop.
Association for Computational Linguistics, Barcelona, Spain, pp. 74781.

Mani,I. and Bloedorn,E. (1998) Machine learning of generic and user—focused
summarization. In: Proceedings of the Fifteenth National/Tenth Conference
on Artificial Intelligence/limovative Applications of Artificial Intelligence.
American Association for Artiﬁcial Intelligence, Madison, Wisconsin, USA,
pp. 82(F826.

Mann,G.S. and McCallum,A. (2010) Generalized expecmtion criteria for
semi—supervised learning with weakly labeled data. J. Mach. Learn. Res., 11,
9557984.

Mann,H.B. and Whitney,D.R. (1947) On a test of whether one of two random
variables is stochastically larger than the other. Ann. Math. Stat., 18, 50760.
McNemar,Q. (1947) Note on the sampling error of the difference between correlated

proportions or percentages. Psychometrika, 12, 1537157.

 

1446

ﬁre'spzumofproyo'sopeuuoprrorq/pdnq

Applications for information structure analysis

 

Merity,S., Murphy,T. and Curran,J.R. (2009) Accurate argumentative zoning with
maximum entropy models. In: Proceedings of the 2009 Workshop on Text and
Citation Analysis for Scholarly Digital Libraries. Association for Computational
Linguistics, Suntec, Singapore, pp. 19726.

Minnen,G. et al. (2001) Applied morphological processing of English. Nat. Lang.
Eng., 7, 2077223.

Mizuta,Y. et al. (2006) Zone analysis in biology articles as a basis for information
extraction. Int. J. Med. Inform, 75, 468487.

Mullen,T. et al. (2005) A baseline feature set for learning rhetorical
zones using full articles in the biomedical domain. SIGKDD Explor. Newsl.,
7, 52758.

Novak,B., Mladeni,C.D. and Grobelnik,M. (2006) Text classiﬁcation with active
learning. In: Spiliopoulou,M., Kruse,R., Borgelt,C., Nurnberger,A. and
Gaul,W. (eds) From Data and Information Analysis to Knowledge Engineering.
Springer—Verlag, Berlin/Heidelberg, pp. 398405.

Platt,J.C. (1999) Probabilistic outputs for support vector machines and comparisons
to regularized likelihood methods. In: Alexander,J.S., Peter,B., Bernhard,S. and
Dale,S. (eds) Advances in Large Margin Classiers. MIT Press, Cambridge, MA,
USA, pp. 61774.

Platt,J.C. (1999b) Using analytic QP and sparseness to speed training of support
vector machines. In: Proceedings of the 1998 Conference on Advances in Neural
Information Processing Systems [1. MIT Press, Cambridge, MA, USA,
pp. 5577563.

Ruch,P. et al. (2007) Using argumentation to extract key sentences from biomedical
abstracts. Int. J. Med. Inform, 76, 1957200.

Scheffer,T., Decomain,C. and Wrobel,S. (2001) Active hidden Markov models
for information extraction. In: Proceedings of the 4th International Conference
on Advances in Intelligent Data Analysis. Springer—Verlag, London, UK,
pp. 3097318.

Schuemie,M.J. et al. (2004) Distribution of information in biomedical abstracts
and full—text publications. Bioinformatics, 20, 259772604.

Seung,H.S., Opper,M. and Sompolinsky,H. (1992) Query by committee. In:
Proceedings of the Fifth Annual Workshop on Computational Learning Theory.
ACM, Pittsburgh, Pennsylvania, USA, pp. 2877294.

Shatkay,H. et al. (2008) Multi—dimensional classiﬁcation of biomedical text: Toward
automated, practical provision of high—utility text to diverse users.
Bioinformatics, 24, 208G2093.

Silva,C. and Ribeiro,B. (2007) Combining active learning and relevance vector
machines for text classiﬁcation. In: Proceedings of the Sixth International
Conference on Machine Learning and Applications. IEEE Computer Society,
Washington, DC, USA, pp. 13(kl35.

Sun,L. and Korhonen,A. (2009) Improving verb clustering with automatically
acquired selectional preference. In: Proceedings of EMNLP. Association for
Computational Linguistics, Singapore, pp. 638447.

Teufel,S. (2005) Argumentative Zoning for improved citation indexing. In:
Shanahan,J.G., Qu,Yan and Wiebe,Janyce (eds) Computing Attitude and
Aﬂect in Text: Theory and Applications. Springer, Dordrecht, The
Netherlands, pp. 1597170.

Teufel,S. and Moens,M. (2002) Summarizing scientiﬁc articles: experiments with
relevance and rhetorical status. Comput. Linguist, 28, 409445.

Teufel,S., Siddharthan,A. and Batchelor,C. (2009) Towards domain—independent
argumentative zoning: evidence from chemistry and computational linguistics.
In: Proceedings of EMNLP. Association for Computational Linguistics,
Singapore, pp. 149371502.

Wilcoxon,F. (1945) Individual comparisons by ranking methods. Biometrics Bull., l,
8&83.

Wu,T.—F. et al. (2004) Probability estimates for multi—class classiﬁcation by pairwise
coupling. J. Mach. Learn. Res., 5, 97571005.

 

ﬁre'spzumofproyo'sopeuuoprrorq/pdnq

