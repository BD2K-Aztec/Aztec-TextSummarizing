Bioinformatics, 32(10), 2016, 1518—1526

doi: 10.1093/bioinformatics/btwO23

Advance Access Publication Date: 21 January 2016
Original Paper

 

 

Systems biology

MS-REDUCE: an ultrafast technique for
reduction of big mass spectrometry data for
high-throughput processing

Muaaz Gul Awan1 and Fahad Saeed1'2'*

1Department of Electrical and Computer Engineering and 2Department of Computer Science, Western Michigan
University, Kalamazoo, MI 49008, USA

*To whom correspondence should be addressed.
Associate Editor: Jonathan Wren

Received on 27 October 2015; revised on 28 December 2015; accepted on 12 January 2016

Abstract

Motivation: Modern proteomics studies utilize high—throughput mass spectrometers which can
produce data at an astonishing rate. These big mass spectrometry (MS) datasets can easily reach
peta—scale level creating storage and analytic problems for large—scale systems biology studies.
Each spectrum consists of thousands of peaks which have to be processed to deduce the peptide.
However, only a small percentage of peaks in a spectrum are useful for peptide deduction as most
of the peaks are either noise or not useful for a given spectrum. This redundant processing of non-
useful peaks is a bottleneck for streaming high—throughput processing of big MS data. One way to
reduce the amount of computation required in a high—throughput environment is to eliminate non—
useful peaks. Existing noise removing algorithms are limited in their data—reduction capability and
are compute intensive making them unsuitable for big data and high—throughput environments. In
this paper we introduce a novel low—complexity technique based on classification, quantization and
sampling of MS peaks.

Results: We present a novel data—reductive strategy for analysis of Big MS data. Our algorithm,
called MS—REDUCE, is capable of eliminating noisy peaks as well as peaks that do not contribute to
peptide deduction before any peptide deduction is attempted. Our experiments have shown up to
100>< speed up over existing state of the art noise elimination algorithms while maintaining com—
parable high quality matches. Using our approach we were able to process a million spectra in just
under an hour on a moderate server.

Availability and implementation: The developed tool and strategy has been made available to
wider proteomics and parallel computing community and the code can be found at https://github.
com/pcdslab/MSREDUCE

Contact: fahad.saeed@wmich.edu

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 lntroductlon has proved to be the most Widely used. MS based proteomics

Mass spectrometry (MS) is an analytical chemistry technique which
is used for determining the type and amount of constituents of a
mixture. MS has found its application in the field of biomedical re—
search. Among all the applications of MS in biology and medicine
(Finehout and Lee, 2003) protein identification and quantization

(Aebersold and Mann, 2003) is very frequently used for profiling of
exosomes (Pisitkun et (11., 2004), toxicological screening (K, 2013),
evolutionary biology (Zhao et (11., 2012) and numerous other appli—
cations (Awan and Saeed, 2015). Wide variety of computational
techniques such as estimation of false positive rates (Du et (11.,

©The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1518

9103 ‘Og JSanV 110 seleﬁuv soc] ‘BtHJOJtIBQ 30 AJtsreAtuf] 112 /310'S[BIIJHO[pJOJXO'SOTJBLUJOJIITOTCI”Zduq 11101} popcolumoq

MS—REDUCE

1519

 

2008), protein quantification from large datasets (Hoffert et (11.,
2006), phosphopeptide filtering (Jiang et (11., 2010), phosphoryl—
ation site assignments (Saeed et (11., 2013b), spectrum—to—peptide
matching (Eng et (11., 1994), (Perkins et (11., 1999) and denovo pep—
tide identification (Dancik et (11., 1999) are required to make this
MS data useful.

With the introduction of modern mass spectrometers such as
Thermo Orbitrap, thousands of spectra can be generated in just a
single run of experiment (AS et (11., 2014). An MS2 spectrum con—
sists of mass—to—charge ratio and associated intensities for each peak
depicting their abundance in the sample under consideration. On an
average total number of peaks for one spectrum may range up to
4000 (Awan and Saeed, 2015) and for 60k human proteins the num—
ber of distinct peaks that need to be compared is close to 240 million
(assuming that there is no redundancy). This number is just for a sin—
gle human proteome and with projects like Peptide Atlas the number
of distinct human observations are close to 35 000 which makes the
total number of peaks equal to 8.4 X 1012. Note that this number
does not include other species, distinct experimental conditions or
novel post—translational modifications which exponentially increases
the number of peaks that needs to be processed.

The current computational analysis techniques have not been de—
signed for such massive datasets. The current peptide identification
techniques (e.g. Sequest, Mascot; Eng et (11., 1994; Perkins et (11.,
1999) assume that each peak that is encountered is useful in making
peptide deductions. This leads to processing much more number of
peaks than are necessary to make a peptide deduction (Awan and
Saeed, 2015; Ding et al., 2009; Mujezinovic et (11., 2006; Saeed
et 111., 20133). The processing of peaks that are noise and/0r do not
contribute in deduction of peptides makes the processing of these
large datasets time consuming. We assert that in order to process big
MS data we should be able to eliminate noisy peaks and the peaks
that do not contribute to peptide deduction before an in—depth ana—
lysis of the spectra. This will clearly result in faster processing of the
MS/MS spectra and will save overhead for peptide searches by
reducing the number of peaks to be analyzed. Only processing the
peaks that are useful rather than performing intensive per—peak—
computations will result in tremendous time and space—advantages.
To the best of authors knowledge there is no algorithm available
which can perform the noise removal function without performing
an in—depth analysis on spectra. Further, we are not aware of any
procedure that can eliminate non—noisy and yet non—essential peaks
that do not contribute to peptide deduction.

In this paper we introduce a novel algorithm, called MS—
REDUCE, for ultrafast reduction of MS/MS data in pre—processing
stage. The proposed algorithm is a low—complexity procedure based
on random sampling, approximate classification and quantization
making it highly scalable with increasing number of spectra.
Further, user defined reduction ratio makes it suitable for a variety
and sizes of MS datasets. Our experiments show peptide deduction
accuracy of up to 95% with reduction in the data size of up to 70%.
Our results also indicate that we are able to process 1 000 000 spec—
tra in under 1 h on a sequential machine making it highly efficient
for big datasets. Comparable reduction tools took over 3 days for
the same dataset on a similar machine.

2 Literature review

Spectral pre—processing has become an essential part of the MS based
proteomics in recent years. Most of the spectral pre—processing tech—
niques have a common objective i.e. to improve the reliability of the

peptide to spectral matches assigned by a peptide search engine such
as Sequest or Mascot. Some of the pre—processing methods that allow
better identification of peptides include spectral clustering (Saeed
et 111., 20133), noise reduction in spectra (Ding et al., 2009), quality
assessment of spectra (Bern et (11., 2004) and precursor charge deter—
mination (Wu et (11., 2008). The prime objective of these techniques is
to reduce the noise level in spectra which leads to better identification
of peptide using standard search engines. It is also shown by several
studies that reduction in data can also speedup the process of peptide
identification in peptide search engines (Ding et al., 2009). Below we
will emphasize on the application of the existing work for reduction
of Big Mass Spectrometry data. Note that we are not aware of any
method that allows elimination of peaks that are not noise and may
not contribute to peptide identification; with or without significant
processing of the data.

In the literature several noise reducing or spectral denoising algo—
rithms are available. These algorithms identify the noisy peaks in a
spectrum, depending upon the approach each algorithm uses these
peaks are then either removed or their intensity is decreased to a cer—
tain value. Mujezinovic et al. (2006) presented the MS Cleaner soft—
ware for removing the unwanted peaks from the spectra to facilitate
the peptide search engines. Their technique provided an added ad—
vantage of data reduction. They made use of numerical analysis and
signal detection approach to form four different algorithms. Each al—
gorithm looked for multiply charged ions, isotopic clusters of peaks,
periodic background noise and detection of non—interpretable spec—
tra. However, these methods are deemed to be too compute—inten—
sive to be used as a big data pre—processing application especially for
high—throughput put environments e.g. the authors report compute
time per spectrum of 0.255 while treating 53 944 spectra. Their re—
sults show a total reduction of 15% to 39% in raw data. The same
authors presented an upgrade of MS Cleaner software, a version 2.0
in 2010 (Mujezinovic et (11., 2010). The improved software employs
a new algorithm for screening the interpretable spectra. It detects
the peptide ladder sequence using a fixed number of most intense
peaks from each spectrum. With this upgrade they claim to have
reduced the data to up to 80%. Time per spectrum for newer version
has been stated about 0.02—0.08 5 per spectrum depending upon the
dataset used.

The method presented in Ding et al. (2009) consists of two steps.
In first; a peak intensity adjustment takes place based upon scores ob—
tained from five different features. In the later stage a morphological
reconstruction filter is employed to remove the noisy peaks based
upon their adjusted intensity in the previous stage. This algorithm is
able to reduce up to 69% of data but is extremely compute intensive
to be used for high—throughput or parallel processing. Our experi—
ments show a computational time of around 3 days for 1 000 000
spectra. Two other similar algorithms can be found in Zhang et al.
(2008) and Gentzel et al. (2003). Like previously discussed algorithms
they also suffer from huge number of per—peak calculations. The im—
plementation of Gentzel et al. (2003) takes approximately 1.7 5 per
spectrum and will take hours to process a million spectra.

A quality assessment technique for spectra has been presented in
Lin et al. (2012). The authors estimate the probability of a spectrum
being a high quality one by treating this problem as a constraint 0p—
timization problem. Their results show that a total of 63—74% of
low quality spectra were removed while losing 9—10% of high qual—
ity spectra in the process. In Na and Paek (2007) a new feature has
been introduced for assessment of spectral quality which is based on
cumulative intensity normalization. The results show a removal of
about 60% spectra with a loss of losing 2% of high quality spectra.
Some other spectral quality assessment algorithms have been

9103 ‘Og JSanV 110 seleﬁuv soc] ‘BtHJOJtIBQ 30 AJtsreAtuf] 112 /310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOICI”Zduq 11101} pQPBOIII/lAOG

1520

M. G.Awan and F. Saeed

 

presented in Tabb et al. (2003), Bern et al. (2004), Purvine et al.
(2004) and Ding et al. (2011). All of these algorithms take different
approaches towards assessing the spectra. However, most of the
approaches are compute intensive which makes them impractical
and ineffective for evaluation of big datasets.

3 Proposed MS-REDUCE algorithm

In this paper we present a highly efficient dimensionality reduction
technique which allows massive reduction in number of peaks per
spectra and in turn decrease the overall amount of data that needs to
be processed. Our work builds upon a random sampling strategy
that we presented earlier (Awan and Saeed, 2015). The proposed al—
gorithm, apart from being more accurate than previous strategies,
has very low—computational complexity which makes it ideal for big
data computations. Our classification and sampling strategy allows
us to determine useful peaks before any peptide deduction calcula—
tions. Also in each stage calculations are performed on only a hand—
ful of peaks from each spectrum regardless of the size of individual
spectrum. This makes processing for each spectrum a constant time
operation resulting in linear—time algorithm. Here we formally intro—
duce the problem. Notations will be introduced and defined wher—
ever they occur first throughout the paper.

Definition 1: Let there be N number of spectra
S : {51,52, . . . ,5N}. If length of spectrum s, is 1, then each spectrum
can be represented as a series of peaks i.e. 5,- : {171,172,173, . . . 47;}.
Where p is a peak in a spectrum.

Definition 2: If 5;- denotes a spectrum after being processed by
MS—REDUCE and the size of the processed spectrum be I;- then R is
the reduction factor such that R : /l,-) >i< 100 for each spectrum.

Each spectrum 5 in S needs to be reduced to obtain 5’ such that
both 5 and 5’ correspond to the same peptide with a high confidence
value. Note that there may be cases where 5 and 5’ do not correspond
to a same peptide, in that case if the peptide match for 5’ has a confi—
dence value better than the threshold value to qualify for a high con—
fidence hit then that counts as a correct hit. For example a raw
spectrum 5 might correspond wrongly to a peptide A but after being
reduced using MS—REDUCE its noise level may get lowered and the
reduced spectrum 5’ may correspond correctly to another peptide B
or vice versa. The correctness of match is determined using quality
assessment method discussed in Section 5.3.

MS—REDUCE exploits the fact that about 90% of peaks in a spec—
trum are noisy or are not required for peptide deduction (Mujezinovic
et (11., 2010). The sampling technique is dependent on the level of
noise and intensity variation in a given spectrum. The algorithm com—
prises of a three stage pipeline. Each spectrum streams throught it
while discarding the peaks that cannot pass through the last stage.
The three stages of the pipeline are (i) Spectral Classification, (ii) Peak
Quantization and (iii) Weighted Random Sampling. Figure 1 shows
the proposed three stage pipeline for MS—REDUCE algorithm.

The spectral classification module is the first stage in the pipeline
of MS—REDUCE. The main objective of this module is to determine
an estimate of a spectrum’s noise level. To this end, we present a
novel metric, called Spectral Intensity Spread that allows us to bring
about an approximate classification of spectra according to their
noise level. The Intensity Spread of a spectrum roughly estimates
how diverse the intensities of different peaks are. The module makes
this plain assumption that larger the value for Intensity Spread,
more noisy the spectrum is Wells et al. (2011 ).

MS/MS
Spectra

Data Reduction/Spectral
Denoising/Spectral Assessment
Algorithms

Reduced MS/
MS data

Peptide Matching Algorithms

o t b
e.g. SEQUEST, MASCOT a a a“

Peptide
Spectral
Matches

Post processing algorithms for
quality analysis and protein
deduction.

 

Fig. 1. Figure showing the pipeline for MS-REDUCE algorithm

Once a spectrum has been assigned a class based on its estimated
noise level, it is sent forward to the Spectral Quantization module.
Here the spectrum is quantized into several levels along the intensity
axis. The number of quantization levels depends on the class of spectra
that was assigned in previous stage. A noisier spectrum is quantized
into larger number of quanta. This module distributes the peaks into
different groups based upon their intensity levels thus making it much
simpler and faster to access peaks based on their intensity levels.

The quantized spectra is sent into the last module, where possible
signal peaks are retained using random peak sampling on the
quanta. The number of peaks to be retained are calculated based on
the user defined reduction factor R. Weighted sampling rates are cal—
culated for each quantum such that the sum of peaks gathered from
each level equals to the percentage of peaks required. Here sampling
rate is defined as the percentage of peaks to be retained in one
quantization level. We give details of each module below.

3.1 Spectral classification

Most pre—processing algorithms process the spectra without any re—
gards to the quality of spectra i.e. spectra with better signal to noise
ratio are processed in the same way spectra with poor S/N ratio.
This results in a wastage of resources as a lot of redundant work is
performed for the spectra already having higher S/N ratio. This
module takes care of this issue by classifying spectra on the basis of
approximate noise content in them.

3.1.1 Intensity spread
The classification is performed by comparing each spectrums
Intensity Spread with the Average Intensity Spread of the dataset.

9103 ‘Og tsnﬁnv 110 seleﬁuv soc] ‘BtHJOJtIBQ JO AttsreAtuf] 112 /310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOICI”Zduq 11101} papeolumoq

MS—REDUCE

1521

 

More formally:

Definition 3: Let N be the total number of spectra in set S then S
: {51.52.53, . . . ,5,,} here si represents one spectrum. Then the inten—
sity spread for spectrum 5,- can be calculated as:

V,- : Max10Avg(5,-) — Min10Avg(5,-) (1)

where V,- is the Intensity spread of the spectrum i and Max10Avg(5,-)
and Min10Avg(5,-) present the average of ten most and least intense
peaks of the spectrum respectively.

Similarly Average Intensity Spread for a dataset can be calculated
as:

N
(Max1 0Avg(5,-) — Min1 0Avg(5,-)
1

Vavg :

 

N

where

Vavg : Average Intensity Spread

N : number of spectra in set S

For each incoming spectrum the Intensity Spread value is calcu—
lated. As seen in Eq. (2), this calculation requires only twenty peaks
from each spectrum regardless of its size.

3.1.2 Classification

Spectra are classified in four different classes depending upon how
much above or below the Vavg their value of V lies. Details regarding
the choice of number of classes can be found in Section 4.1 of supple
mentary materials. Classes are named in increasing numerical order;
higher classes contain spectra with larger value of V and vice versa.
Threshold values for V to be assigned to a particular class are deter—
mined based on each dataset’s Vavg. More formally the threshold
values for each class can be defined as follows:

Definition 4: Let x denote a class then for x : {1, 2, 3}
1 1
Sx:{5il(x—1)*Z*Vm,g£W§x*Z*Vavg} (3)

and for x : {4}:

s, 2 {5,1}. V...vg < V.} (4)

where Sx : Class x containing spectra assigned to it.

It can be seen in Figure 2 how spectrum 1 and 2 have very differ—
ent range of intensities yet they have similar spectra spread hence
have been assigned the same class. Algorithm 1 in supplementary ma
terials presents a pseudo code for the classification module.

3.2 Spectral quantization

In our proposed algorithm quantization of spectra takes place along
the intensity axis. The intensity of a peak is simply compared with
the upper and lower level of a quanta, if it lies within the limit, the
peak is assigned to that quantum. This process provides us with dif—
ferent bins, each containing peaks of intensities within a specific
range. The advantage of quantization is exploited in the following
step, where useful peaks are just picked out from their quanta and
added to the final reduced spectrum. Thus preventing the need of
performing per peak computations.

3.2.1 Quantization levels
Number of quantization levels is chosen such that those spectra hav—
ing wide Intensity Spread are quantized into larger number of levels

_Eiaa?1

w ./A' ./,:'

 

Intensityl
IMEIEIV

   W.

rm':

CInHZ

 

Interns“!I

 

"Wellan
g __
§
§

 

"11121163?

 

 

 

 

Fig. 2. Figure depicts a visual representation of classification stage. The
shaded regions present the Spectral Spread (V), larger the shaded area larger
the value of Vand noisier the corresponding spectrum is considered

while those having a narrow spread are processed using smaller
number of quantization levels. Our in house experiments suggest
that a spectrum with a smaller intensity spread yields no improve—
ment if processed using larger number of quantization levels while
increasing the processing time. In order to save time and space re—
sources we use the smallest possible number of quantization levels
necessary to perform the computation. Similarly the spectra with
wider Intensity Spread needs more number of quantization levels to
achieve similar accuracy. Classes 1, 2, 3 and 4 are assigned 5, 7, 9
and 11 levels of quantization respectively. These values have been
chosen based upon an empirical study, details of which can be found
in Section 4.2 of supplementary materials. The quantization process
can be formally defined as:

Definition 5: Let nx be the maximum number of quantization
levels for class x then we can have n1:5,n2 : 7,n3 : 9 and
n4 : 11. q,,- represents the quantum / of spectrum i. Then following
equations are calculated for each spectrum 5,-, for each quantum /
from 1 till nx.
for j < nx

 

(1.7-2m”— 1) * M10A(st):llz7ll : L9 M10A<si>} <5>

n "x

for/:nx

 

(1.7-2m” "x135 M10A<si> : llpll} <6>
where j : quantization level under consideration
qii : ith quantization level of ith spectrum
nx : number of quantization levels for class x
 : intensity of peak 17
M10A(5,-) : Average Intensity of 10 most intense peaks of 5,-

Eqs. (5) and (6) are computed for each value of nx ranging from
1 till nx. The quantum number assigned to each peak represents cer—
tain characteristics e.g. quantum 1 is the lowest and it contains the
least intense peaks, similarly the quantum number 11 would be the
highest for class 4 spectra and would contain the most intense peaks.
The quanta are equally spaced rather than being of irregular spread
because about 90% of the data is redundant so the probability that

9103 05 tsnﬁnv uo sopﬁuv soc] ‘BIIIJOJIIBD JO AJtSJQAtuf] 112 /310'S[BIIJHO[pJOJXO'SOTJBLUJOJIITOTCI”Zduq 11101} papeo1umoq

1522

M. G.Awan and F. Saeed

 

th
5 Quantization Level
4 Quantization Level

3 Quantization Level

nd
2 Quantization Level

st
1 Quantization level

Intensity

 

   

mi:

Fig. 3. Figure represents quantization of a class I spectrum with five different
quantization levels. The red colored peaks are the most intense and belong to
the fifth quantum while the light blue colored are the least intense and have
been binned into the lowest quantum

any outlier (if any) will affect the quality of the hits is extremely
small. Algorithm 2 in supplementary materials shows a pseudo code
for this module.

3.3 Weighted random sampling

Rather than dealing with each peak, this step deals with quanta of
peaks. Here this assumption is made that each peak within one
quantization level has an equal probability of being a useful peak.
Also because of the presence of more high intensity peaks, probabil—
ity of finding a useful peak is greater in the higher quanta (Havilio
et al., 2003). In order to determine the number of peaks to be
sampled from one quantum, sampling weights are determined as ex—
plained below (Fig. 4).

3.3.1 Weights calculation

First an estimate of number of peaks to be retained is calculated
based upon the user defined reduction factor. Then a recursive
method estimates the sampling weights for each quantum such that
they satisfy the following equation:

"x

2(ng * qt) 2 p’ (7)

i:1

 

where
x,- : sampling rate for quantization level i

Intensity

t
E

_ _ _ _ _ '  m
- Random Sampling
-'|_ma—--
. 1 
..'_.._. |l_l|. .'_ l_  a
mi:

Fig. 4. Figure presents a visual representation of the random sampling mod-
ule. In this figure the top most quantum is assigned a weight of 100% while
the fourth quantum is assigned a weight of 50%. Peaks from all other quanta
are discarded owing to their zero sampling rate

 

In}:

q,- : ith quantization level
Hqill : number of peaks at ith quantization level
17’ : number of peaks required to satisfy the reduction factor

Peaks are taken starting from the highest quantization level and
continuing with lower levels until the required number of peaks is
reached. If there are more peaks at a given quantization level than
are needed to reach the required number of peaks, the sufficient
peaks are chosen at random from that quantization level. Formally
this can be presented by Eqs. (8) through (10):

case 1: llqnxll I P’

100, if i: nx.
xi 2 . (8)
0. otherw1se.
 2:11am > p’
. _ , _ I
 if ,2 
xi : Will (9)
0. otherwise.
case 3: Default
100. if 17’ — llqill > liq/mil-
xi: p’jleqI-ll (10>
’:’+1 . otherwise.
llqill

Figure 4 shows an example of weighted random sampling being
performed on a class I spectrum. In the right half of the figure a
reduced spectrum can be observed, it can be noticed that among the
two peaks from fourth quantum only one appears in the final spec—
trum because of 50% sampling rate. This one peak is chosen totally
at random.

4 Experimental datasets and method

We made use of 13 datasets to carry out performance and speed
evaluation of MS—REDUCE algorithm. The details of the datasets
have been provided in Supplementary Materials.

5 Performance evaluation

We carried out performance evaluation of MS—REDUCE in two
phases. In first part we evaluate the time complexity and the speed
up achieved in comparison to some of the existing algorithms. In the
second part we perform the quality assessment experiments. This
tests the quality of the peptide matches obtained after performing
data reduction using MS—REDUCE. We also compare the quality
assessment results of MS—REDUCE with the existing algorithms as

9103 05 isnﬁnv uo sopﬁuv soc] ‘BIIIJOJIIBD JO 1015191011 {1 112 /310'S[Buln0prOJXO'SOiJBLUJOJIIiOiq/ﬂduq 11101} pep1201umoq

MS—REDUCE

1523

 

well as investigate the improvement achieved above the previous
random sampling approach (Awan and Saeed, 2015).

5.1 Time complexity

Time complexity of the algorithm can be formulated by observing
the working of each module closely and summing up the individual
complexities of the modules. Theoretical time complexity for MS—
REDUCE comes out to be O(N). The step by step calculations to ob—
tain this result can be found in supplementary materials.

In order to verify this linear time complexity over datasets vary—
ing from conventionally sized to the modern big datasets we repli—
cated UPS2 dataset several times to obtain datasets of desired sizes.
We formed ten datasets with each subsequent set having 100 000
more spectra. The MS—REDUCE has been designed keeping in mind
the challenges of big datasets from proteomics so it makes sense to
use such huge datasets to perform time related experiments. For all
the experiments discussed from here onwards we made use of a
Linux based server with 24 CPUs, each operating at 1200 MHz.

Figure 5 shows the time taken by MS—REDUCE to process each
datasets explained above. Currently the MS—REDUCE has been de—
veloped only as a single threaded program. To compensate for back—
ground tasks and other time delays we performed the experiment on
each dataset about ten times and averaged the time taken. For these
experiments we set the user defined reduction factor to 50 and 90.

It can be observed from Figure 5 that MS—REDUCE has a linear
time complexity with respect to the number of spectra processed
which is in agreement with our theoretical computational complex—
ity. It can further be observed that a varying reduction factor does
not significantly affect the running time efficiency of the algorithm.
A reduction factor as described before determines the amount of
data to be retained by the algorithm. It can be observed that the al—
gorithm was able to retain its linear trend while being run with dif—
ferent values of Reduction Factor.

5.2 Speed comparison
We compared the processing speed of MS—REDUCE with the

denoising algorithm presented in Ding et al. (2009). In order to

MS-REDUCE Timing Plot

 

 

    

 

 

 

 

.106
l l l
2.5 e e
A 2 e e
m
'c
x:
o
8
g 1.5 e e
E
d.)
a 1 e e
H
+ Reduction Factor 10
0.5 e —I— Reduction Factor 30
+ Reduction Factor 60
0 ‘ ‘ —-k— Reduction Factor 90
0.2 0.4 0.6 0.8 1
.106
Number of Spectra

Fig. 5. Figure showing a graph between processing time of MS-REDUCE and
the number of spectra processed at reduction factors of 10, 30, 60 and 90. The
horizontal axis represents the number of spectra while the vertical axis repre-
sents time in milliseconds

compare the speed we define two metrics here. One is the conven—
tional speed up calculation method while the other is spectra per se-
cond or SPS. Following equations describe both these metrics:

S : Tether/Treduce  

Where S is the speed up obtained, Tome, is the processing time of
other algorithm under consideration while deuce is the time taken
by MS—REDUCE.

SPS : Spectra /Time (12)

Eq. (11) presents the conventional way of calculating speed up
and the Eq. (12) presents spectra per second metric. Larger number
of spectra per seconds would mean a faster processing rate for the
algorithm. Here we will refer the algorithm in Ding et al. (2009) as
De—Noising Algorithm.

5 .2. 1 Comparison with De-noising algorithm

Both the algorithms were operated in similar environments for this
study. As it was explained before, De—Noising algorithm makes use
of four different scoring techniques to perform peak adjustments
and then undesirable peaks are filtered out using a morphological
filter.

Table 1 shows the results from timing experiments performed
for comparing the time taken by the De—Noising Algorithm and MS—
REDUCE. The columns two and three show the processing time for
algorithms in milliseconds. The De—Noising algorithm takes almost
three days to process 1 million spectra. Poor scalability of such algo—
rithms with increasing size of the datasets renders them unsuitable
for high—throughput environments. The table shows MS—REDUCE
takes around 47 min to process a million spectra thus achieving an
average speed up of 100.

5.3 Quality assessment

In this section we investigate the quality of the peptide matches ob—
tained from spectra that have been processed by MS—REDUCE. First
we present the quality improvements achieved over the previous
technique and then we compare the results with the two similar al—
gorithms described before.

Figure 6 presents procedure for assessing the quality of peptide
matches obtained after the application of MS—REDUCE algorithm.
The raw spectra are fed into the MS—REDUCE or any other algo—
rithm under observation. The processed spectra are then sent to
the Tide (Diament and Noble, 2011) search engine of Crux toolkit

Table 1. Speed achieved over the Denoising Algorithm

 

 

SPeCtra Tdenoise  Treduce  Speed up
9.61 X 104 2.35 X 107 2.25 X 105 103
1.92 X 105 4.41 X 107 4.50 X 105 96
2.89 X 105 6.49 X 107 6.78 X 105 94
3.85 X 105 8.60 X 107 9.03 X 105 94
4.81 X 105 1.09 X 108 1.12 X 106 95
5.78 X 105 1.31 X 108 1.75 X 106 83
6.74 X 105 1.55 X 108 2.0 X 106 86
7.71 X 105 1.76 X 108 2.05 X 106 94
8.67 X 105 1.97 X 108 2.29 X 106 94
9.63 X 105 2.20 X 108 2.47 X 106 98
1.06 X 106 2.43 X 108 2.81 X 106 99

 

Table showing comparison between run runtimes of the De-Noising
Algorithm denoted by Tdmm-s, and MS-REDUCE denoted by dem.

9103 ‘01; isnﬁnv uo sopﬁuv soc] ‘BIIIJOJIIBD JO 10151911111 {1 112 /310'S[BIIJHO[pJOJXO'SOiJBLUJOJIIiOiq”Zduq 111011 popco1umoq

1524

M. G.Awan and F. Saeed

 

(Park et (71., 2008). Tide provides with the peptide spectral matches
(PSMs) and decoy peptide matches based on a decoy database.
These two datasets are then sent to the post processing tool known
as the percolator (Kall et (71., 2007). The percolator computes a
statistical confidence value based upon the PSMs and the decoy
database matches which serve as a false discovery rate (FDR) and
assigns it to each PSM. We calculated the number of PSMs for
same FDR threshold obtained by using the datasets which had
been treated by the test algorithm. Using this information we were
able to calculate a percentage of high quality PSMs obtained by the
processed spectra with respect to the number of high quality PSMs
obtained using the raw spectra. This experiment was repeated
for FDR values of 1%, 3%, 5%, 7% and 9%. We are taking FDR
of 5% as a nominal value, so in the following experiments be—
cause of limited space we will only be presenting the results for
FDR of 5%.

5.3.1 Comparison with random sampling of peaks

We performed the above explained experiment on all the thirteen
datasets which have been explained in the supplementary materials
and plotted the results for each dataset. The results are for FDR
value of 5% but the results are extendible to other FDR values.
Figures 7 and 8 present the results for quality assessment experi—
ments performed on MS—REDUCE and and the random peak sam—
pling method (Awan and Saeed, 2015) using three HCD datasets
and the UPS2 dataset. Results for remaining datasets can be found
in supplementary materials (Figs 8, 9 and 10).

The graphs have been plotted by varying the value of reduction
factor for MS—REDUCE and Sampling rate of random peak sam—
pling approach from 10% to 90%. The 100% presents the un—
treated raw dataset. MS—REDUCE presents significant
improvement over the random sampling approach. For some data—
sets percentage matches are nearing 90% with a data reduction

Raw MS
Spectra

Test Algorithm

Decor
Matches

Percolator Percolator

FDR Filtering

FDR Filtering

Quality Check Ted Data

 

Fig. 6. Figure shows flow of quality assessment experiments. The Test
Algorithm shown in top right corner is replaced by the algorithm under ob-
servation i.e. MS-REDUCE, MSCleaner 2.0 and Denoising Algorithm

rate of only 20%. The results are also shown to be consistent for a
given fragmentation type (HCD or CID) with MS—REDUCE doing
a bit better for HCD due to better S/N ratio for HCD datasets
(Saeed et (71., 2013c).

5.3.2 Comparison with conventional algorithms

We also compared the quality of peptide matches for the data pro—
cessed by MS—REDUCE with that processed by conventional noise
reducing algorithms. The approach taken for these experiments was
also the same as presented in Figure 6. Figure 9 shows quality assess—
ment plots of De—Noising Algorithm, MSCleaner 2.0 and perform—
ance of MS—REDUCE at reduction factors of 30, 60 and 90. MS—
REDUCE out performs MSCleaner 2.0 for all datasets except UPS2
while operating at nearly all the values of reduction factors. It out

 

   

 

 

 

l l l l l
100 e e
80 e e
.G
3
g 60 7 7
d.)
.°<3°
:
o 40 7 L
8
9.1
— MS—REDUCE
20 T — Random Sampled
o HCD—DSl
O 7 + HCD-D82
1 J J El HCD—D83

 

 

20 40 60 80 100
Reduction Factor

Fig. 7. Quality assessment plots for HCD-D81, HCD-D82 and HCD-D83 data-
sets. The red colored plots represent the performance of MS-REDUCE while
the blue colored plots represent the Random Sampled data. Three datasets
have been differentiated using different symbols. The X—axis contains
Reduction Factor i.e. amount of data retained. And the y—axis shows the per-
centage of accurate peptide hits obtained from the processed data

 

 

 

 

100 e e

80 e e
.r:
3

g 60 7 7
d.)
E“
a

d.)  7 7
8
Ga

20 e e

0 7 + REDUCE
1 J J —I— Random—Peak

 

 

2O 40 60 80 100
Reduction Factor

Fig. 8. Quality assessment plot for UPSZ dataset. The red plot represents the
performance of MS-REDUCE while the blue plot represent the Random
Sampled data. The X—axis contains Reduction Factor i.e. amount of data re-
tained. And the y—axis shows the percentage of accurate peptide hits obtained
from the processed data

9103 ‘01; isanV uo sopﬁuv soc] ‘BIIIJOJIIBD JO 10151911111 {1 112 /310'S[BIIJHO[pJOJXO'SOiJBLUJOJIIiOiq”Zduq 111011 popco1umoq

MS—REDUCE

1525

 

Quality Assessment

 

   

 

 

 

1 1 1 1 1 1 1 1 I I I I I I I
100 7 ,
90 7 7
80 7 7
70 7 7
9“;
:6
E 60 7 ,
u
.52“
1:
1:3 50 7 ,
32
40 7 ,
30 7 7
20 +De-Noising Algorithm 7
+ MSCleaner 2.0
+ MS-REDUCE-30
10 + MS-REDUCE-60 *
+ MS-REDUCE-90 i

 

 

CID-D53
CID-D54
CID-D55 7
CID-D56 7
UPSZ 7

CID DSl
CID-D52
HCD-D52 7
HCD-D53 7
HCD-D54 7
HCD-D55 7
HCD-D56 7

55
‘33
8
:1:

Fig. 9. Figure showing quality assessment plots for De-Noising Algorithm
Algorithm, MSCleaner 2.0 and MS-REDUCE. Quality Assessment plots for dif-
ferent Reduction Factor of MS-REDUCE can be observed. In the legend a nu-
merical value with MS-REDUCE represents its reduction factor. X—axis
contain the labels for the experimental datasets while Y—axis represents the

percentage of peptide matches obtained from each dataset after being pro-
cessed by each algorithm

performs De—Noising Algorithm while operating around a reduction
factor of 60. Note that other both algorithms are compute—intensive
and take much longer time as compared to MS—REDUCE to produce
comparable results as shown in Figure 9.

6 Conclusion

Analysis of high—throughput MS based proteomics data is an essential
task in systems biology. Data from multiple experiments can scale
from million to a billion spectra and this data volume can easily reach
tera— to peta—byte level. The Big Data from modern mass spectrom—
eters creates scaling problems for existing software designed for much
smaller datasets. Although these algorithms are useful for interpret—
ation of simple spectra, the search and match routine becomes compu—
tationally intractable for complex peptides. The big data volume that
one gets from these high—throughput machines is enormous and low
scalability of conventional tools cannot keep up with the rate of data
generation. Hence dimensionality reduction techniques that can re—
duce the number of peaks that needs to be processed are essential for
fast and efficient processing of MS data for system— wide studies.

In this paper we presented a novel dimensionality reduction tech—
nique, called MS—REDUCE, for pre—processing big MS datasets. To
our knowledge, the proposed strategy is first attempt at data reduc—
tion of MS data for high—throughput environments. Our low—com—
putational cost strategy is based on classification, quantization and
sampling of MS data peaks. An approximate classification of spectra
followed by a quantization step results in binning of peaks. Each
quantum of a spectrum contains peaks within a particular intensity
range. Then a random sampling step is performed on these bins to
obtain the peaks which form the final reduced spectrum. Our strat—
egy is linear in time complexity with increasing number of spectra
which is confirmed by our experiments. We also show that MS—
REDUCE can process up to a million spectra in 47 min as compared

to the De—Noising Algorithm, which processes the same number of
spectra in about 3 days. We performed rigorous testing of the algo—
rithm using experimental datasets and compared its performance
with two of the existing algorithms. The implemented software will
be available for free academic use at the author’s webpages.

Funding

This work was partially funded by National Science Foundation grant NSF
CCF-1464268.

Conﬂict of Interest: none declared.

References

Aebersold,R. and Mann,M. (2003) Mass spectrometry-based proteomics.
Nature, 422, 198—207.

Awan,M.G. and Saeed,F. (2015). On the sampling of big mass spectrometry
data. In: Proceedings of the 7th International Conference on Bioinformatics
and Computational Biology, BICOB pp. 143—148.

Bern,M. et al. (2004) Automatic quality assessment of peptide tandem mass
spectra. Bioinformatics, 20, i49—i54.

Dancik,V. et al. (1999) De novo peptide sequencing via tandem mass spec—
trometry. I. Comput. Biol., 6, 327—342.

Diament,B.I. and Noble,W.S. (2011) Faster sequest searching for peptide iden—
tiﬁcation from tandem mass spectra. I. Proteome Res., 10, 3871—3879.

Ding,I. et al. (2009) A novel approach to denoising ion trap tandem mass spec—
tra. Proteome Sci., 7.

Ding,I. et al. (2011) Svm—rfe based feature selection for tandem mass spectrum
quality assessment. Int. I. Data Min. Bioinf., 5, 73—88.

Du,X. et al. (2008) Linear discriminant analysis—based estimation of the false
discovery rate for phosphopeptide identiﬁcations. I. Proteome Res., 7,
2195—2203.

Eng,I.K. et al. (1994) An approach to correlate tandem mass spectral data of
peptides with amino acid sequences in a protein database. I. Am. Soc. Mass
Spectrom., 5, 976—989.

Finehout,E. and Lee,K. (2003) An introduction to mass spectrometry applica-
tions in biological research. Biochem. Mol. Biol. Edna, 32, 93—100.

Gentzel,M. et al. (2003) Preprocessing of tandem mass spectrometric data to
support automatic protein identiﬁcation. Proteomics, 3.

Havilio,M. et al. (2003) Intensity-based statistical scorer for tandem mass
spectrometry. Anal. Chem, 75, 435—444.

Hebert,A.S. et al. (2014) The one hour yeast proteome. Mol. Cell Proteomics,
13, 339—347.

Hoffert,I.D. et al. (2006) Quantitative phosphoproteomics of vasopressin-sen—
sitive renal cells: regulation of aquaporin—2 phosphorylation at two sites.
Proc. Natl. Acad. Sci. USA, 103, 7159—7164.

Iiang,X. et al. (2010) Classiﬁcation ﬁltering strategy to improve the coverage
and sensitivity of phosphoproteome analysis. Anal. Chem, 82, 6168—6175.

Kall,L. et al. (2007) Semi—supervised learning for peptide identiﬁcation from
shotgun proteomics datasets. I. Proteome Res., 4, 923—925.

Lin,W. et al. (2012) An unsupervised machine learning method for assessing
quality of tandem mass spectra. Proteome Sci., 10, 1—8.

Linnet,K. (2013) Toxicological screening and quantitation using liquid chro-
matography/time-of—ﬂight mass spectrometry. I. Foren. Sci. Criminol., 1, 1.

Mujezinovic,N. et al. (2006) Cleaning of raw peptide ms/ms spectra:
Improved protein identiﬁcation following deconvolution of multiply
charged peaks, isotope clusters, and removal of background noise.
Proteome Sci., 6, 5117—5131.

Mujezinovic,N. et al. (2010) Reducing the haystack to ﬁnd the needle: im—
proved protein identiﬁcation after fast elimination of non—interpretable pep-
tide ms/ms spectra and noise reduction. BMC Genomics, 11, 1—8.

Na,S. and Paek,E. (2007) Quality assessment of tandem mass spectra based on
cumulative intensity normalization. I. Proteome Res., 5.

Park,C.Y. et al. (2008) Rapid and accurate peptide identiﬁcation from tandem
mass spectra. I. Proteome Res., 7, 3022—3027.

9103 ‘01; JSanV 110 sopﬁuv 50'] ‘1211110111123 10 10151910111] 112 /310's112u1n0[p101x0'so1112111101u101q//:d1111 111011 pep1201um0q

1526

M. G.Awan and F. Saeed

 

Perkins,D.N. et al. (1999) Probabioity-based protein idenitiﬁcation by search—
ing sequence database using mass spectrometry data. Electrophoresis, 20,
35 5 1—35 67.

Pisitkun,T. et al. (2004) Identiﬁcation and proteomic proﬁling of exosomes in
human urine. Proc. Natl. Acad. Sci. USA, 101, 13368—13373.

Purvine,S. et al. (2004) Spectral quality assessment for high—throughput tan—
dem mass spectrometry proteomics. OMICS: I. Integr. Biol., 8, 255—265.
Saeed,F. et al. (2013a) Cams—rs: clustering algorithm for large-scale mass spec—
trometry data using restricted search space and intelligent random sampling.

IEEE/ACM Trans. Comput. Biol. Bioinf., 11, 128—141.

Saeed,F. et al. (2013b) An efﬁcient dynamic programming algorithm for phos-
phorylation site assignment of large-scale mass spectrometry data. IEEE
Int. Conf. Bioinf Biomed. Workshops (BIBMW), 7, 618—625.

Saeed,F. et al. (2013c) Phossa: fast and accurate phosphorylation site assign-
ment algorithm for mass spectrometry data. Proteome Sci., 11, 514.

Tabb,D.L. et al. (2003) Similarity among tandem mass spectra from proteomic
experiments: Detection, signiﬁcance, and utility. Anal. Chem., 75.

Wells, G. et al. (2011). Why use signal—to-noise as a measure of ms perform-
ance when it is often meaningless? Technical report, Agilent Technologies.
Wu,F.X. et al. (2008) An approach to assessing peptide mass spectral quality

without prior information. Int. I. Funct. Inf. Person. Med., 1, 140—155.
Zhang,I. et al., (2008) Peakselect: preprocessing tandem mass spectra for bet-
ter peptide identiﬁcation. Rapid Commun. Mass Spectrom., 22, 1203—
1212.
Zhao,B. et al. (2012) Cphos: a program to calculate and visualize evolutionar—
ily conserved functional phosphorylation sites. Proteomics, 12, 3299—3303.

9103 ‘01; JSanV 110 sopﬁuv 50'] ‘1211110111123 10 10151910111] 112 /310's112u1n0[p101x0'so1112111101u101q//:d1111 111011 pep1201um0q

