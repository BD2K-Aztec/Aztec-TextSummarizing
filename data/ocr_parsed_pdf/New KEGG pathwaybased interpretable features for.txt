Bioinformatics Advance Access published July 31, 2016

Bioinformatics, 2016, 1—8

doi: 10.1093/bioinformatics/btw363

Advance Access Publication Date: 17 June 2016
Original paper

 

 

Data and text mining

New KEGG pathway-based interpretable
features for classifying ageing-related
mouse proteins

Fabio Fabris* and Alex A. Freitas

School of Computing, University of Kent, CT2 7NF Canterbury, Kent, UK

*To whom correspondence should be addressed.
Associate Editor: Jonathan Wren

Received on February 9, 2016; revised on May 12, 2016; accepted on June 1, 2016

Abstract

Motivation: The incidence of ageing—related diseases has been constantly increasing in the last
decades, raising the need for creating effective methods to analyze ageing—related protein data.
These methods should have high predictive accuracy and be easily interpretable by ageing experts.
To enable this, one needs interpretable classification models (supervised machine learning) and
features with rich biological meaning. In this paper we propose two interpretable feature types
based on Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways and compare them with
traditional feature types in hierarchical classification (a more challenging classification task regard—
ing predictive performance) and binary classification (a classification task producing easierto inter—
pret classification models). As far as we know, this work is the first to: (i) explore the potential of
the KEGG pathway data in the hierarchical classification setting, (i) use the graph structure of
KEGG pathways to create a feature type that quantifies the influence of a current protein on another
specific protein within a KEGG pathway graph and (iii) propose a method for interpreting the classi—
fication models induced using KEGG features.

Results: We performed tests measuring predictive accuracy considering hierarchical and binary
class labels extracted from the Mouse Phenotype Ontology. One of the KEGG feature types leads
to the highest predictive accuracy among five individual feature types across three hierarchical
classification algorithms. Additionally, the combination of the two KEGG feature types proposed in
this work results in one of the best predictive accuracies when using the binary class version of our
datasets, at the same time enabling the extraction of knowledge from ageing—related data using
quantitative influence information.

Availability and Implementation: The datasets created in this paper will be freely available after
publication.

Contact: ff79@kent.ac.uk

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 IntrOducuon this type of research is clear: it is projected that the economical value

Ageing—related diseases are affecting an increasing number of peo— of adding 2.2 extra healthy years to the human population is $7.1
ple. At the same time, delaying ageing in humans seems to be more trillion dollars over 50 years in the United States alone (Goldman
and more plausible in the not so distant future. Biologists can al— et (11., 2013).

ready extend the lifespan of several animal species such as the fruit One of the aims of ageing—research is to treat ageing as a Whole,
ﬂy and the mouse. The potential economical benefit of investing on reducing the incidence of many different ageing—related diseases at

©The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1

91oz ‘Og isnﬁnv 110 salaﬁuv soc] ‘BtHJOJtIBQ 30 AJtSJQAtuf] 112 /310'S[Buln0prOJXO'SOTJBLUJOJIITth/ﬂduq 11101} papeolumoq

F. Fabris and A. A. Freitas

 

the same time, instead of focusing on individual diseases. This ap—
proach promises to be much more effective than the current ap—
proach of treating individual diseases and has the potential of
stopping the trend of increasing costs of treating ageing—related dis—
eases (Goldman et al., 2013). One way to study the ageing process
holistically is to use data mining algorithms to find connections be—
tween genes or proteins that are known to be ageing—related and
other genes or proteins that have unknown function using the ever
increasing freely accessible biological data.

Two databases of interest for ageing experts are the Kyoto
Encyclopedia of Genes and Genomes (KEGG) (Kanehisa et al.,
2016) and the Mouse Phenotype Informatics (MPI) (Eppig et al.,
2015) databases. The MP1 database contains, among other data, the
definition of an ontology of ageing—related terms that describe the
phenotype of several allele—mutations. The KEGG database also
contains several types of information about genes and proteins,
including pathway information presented in a graphical way that
allows biologists understanding the interactions of proteins in com—
plex biological processes.

We address two types of classification (supervised machine
learning) problems: binary classification, where instances (proteins)
are annotated with the presence or absence of a class (indicating if
the protein is ageing related); and hierarchical classification; where
the classes to be predicted (protein functions) are organized into a
hierarchy, where more generic functions are ancestors of more spe—
cific functions (Silla Jr. and Freitas, 20113). Note that this is a more
complex but more rewarding problem than conventional classifica—
tion, since the latter ignores hierarchical relationships among
classes. We address the hierarchical classification task because the
terms in the MPI ontology are hierarchical, and address the binary
classification task because it produces classification models easier to
interpret, from an ageing—biology perspective. In both tasks, it is cru—
cial to describe each instance (protein) by a set of features (protein
properties) that has both good predictive power and rich biological
meaning.

The contributions of this work are three—fold: (i) the integration
of data from the MP1 and KEGG databases to create a new numer—
ical KEGG feature type with rich biological meaning. This new fea—
ture type quantifies how an instance (protein) inﬂuences other
proteins, the idea being that proteins that inﬂuence other proteins in
a similar way have similar function; (i) the new investigation of the
use of the binary KEGG feature type in the context of hierarchical
classification; and (i) proposing a method for interpreting classifica—
tion models generated using KEGG features.

The construction of specially tailored, meaningful features for
specific problems is part of the feature engineering process (Forman,
2002; Yepes et al., 2015). The objective is to introduce carefully
crafted features for the type of problem being addressed. In the bio—
informatics field, it has been common to use features that are easily
extractable from the protein sequence or from some database con—
taining several protein properties. These features, although valuable,
often lack the preciseness an expert needs to reach a meaningful bio—
logical conclusion. This works differs from current practice by creat—
ing a new KEGG pathway—based feature type that encodes precise
and meaningful relations between proteins.

This paper is organized as follows: Section 2 describes how we
built our ageing—related datasets, including the proposed KEGG fea—
tures. Section 3 reports the predictive power of our features across
hierarchical and binary classification algorithms and the interpret—
ation of a binary classification model using some of the proposed
features. Finally, in Section 4 we discuss the results of our work and
draw conclusions.

2 Methods

2.1 Creation of the ageing datasets using the mouse
genome informatics dataset

To study the biological aspects of ageing/longevity using hierarchical
classification algorithms, we have built 7 datasets containing fea—
tures extracted from the proteins encoded by the genes in the
Phenotypes and Mutant Alleles section of the Mouse Genome
Informatics (MGI) database. The MGI provides the two primary
sources of data of our datasets: (i) the definition of the Mammalian
Phenotype Ontology (MPO), the source of class labels to be pre—
dicted, and (i) a list of genotypes annotated with the phenotypes pre—
sent in the MPO, the source of the features (predictors).

The MP0 is organized as a DAG (Directed Acyclic Graph),
where each node represents a phenotype (an ontology term) and
each edge an ‘IS—A’ relation between phenotypes. Because of the
structured organization of the class labels, this is a hierarchical clas-
sification problem, where the class labels of the instances are organ—
ized in a graph, usually a DAG or tree. The nodes of the graph
represent class labels and edges are ‘IS—A’ relationships among class
labels. This structural organization means that if an instance is
annotated with a given (specific) class label, it is implicitly annotated
with all ancestor (more generic) class labels.

The MP0 contains 10 907 terms in total, and 113 terms under
the term MP:001076 8 (ageing/mortality) part of the hierarchy, our
research focus. We consider only the 113 ageing—related terms as
class labels for our study, and discard the others. Considering all
10 907 terms would generate classification models more focused on
predicting non—ageing—related terms, generating models with less
interest for the biology of ageing. After further discarding MPO
terms with less than 10 instances, we end up with 81 MP0 terms,
the hierarchical class labels to be predicted.

With the class hierarchy defined, we must create our instances.
In the MGI database, 11 532 genotypes are annotated with at least
one of the 113 mortality/ageing—related ontology terms. Each geno—
type is formed by a list of allele—mutations. Each allele—mutation
contains (among other information) one or more protein—encoding
genes, which in turn are associated with particular mutations.
Therefore, using the MPO hierarchy we can associate a protein (in—
stance) with one or more phenotypes (hierarchical classes). Figure 1
shows these relations graphically.

Note that our instances are proteins encoded by standard genes,
not gene mutations, because, as discussed later, information about
proteins is much richer and precise than information about gene
mutations.

However, choosing to use proteins as instances (instead of gene
mutations) has the disadvantage of risking annotating the same pro—
tein with contradictory MPO terms. This may happen because two
different mutations on the same gene may have contradictory ef—
fects. E.g. one mutation may over—express the protein encoded by a

rﬁ
Phenotype annotates fOI‘IIled by Allele-
Genotype

(Our Classes) Mutation
M}

 

 

   

Iindirects annotates

 

 

 

 

 

 

contains
. (ﬂ . .
Protem (our assoc1ated Wlth Gene
. Gene _
Instances) encodes Mutation
M1

 

Fig. 1. Relationships among MPI elements and the instances in our datasets.
Filled edges represent relationships present in the MGI database. The dashed
edge represents the indirect relation that we use for our datasets. Note that
we ignore mutation information

9103 ‘Og isnﬁnv uo salaﬁuv soc] ‘BtHJOJtIBQ JO AJtSJQAtuf] 112 /310'S[Buln0prOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

New KEGG pathway—based interpretable features for classifying ageing—related mouse proteins 3

 

gene, while some other mutation on the same gene may under—
express that protein, possibly leading to opposite MPO terms being
associated with the same gene with different mutations. Since this
mutation information is not available for the classifier, these appar—
ent contradictions (opposite MPO terms annotating the same in—
stance) may reduce classification performance and interpretability.
However, we consider this compromise acceptable since the lack of
information about particular gene mutations makes the use of classi—
fication algorithms considering gene mutations as instances inviable.

Following this approach, we merged the annotations associated
with the same gene, keeping all MP0 terms that were associated with
the different mutations of that gene. After this step, the 11 532 gene—
mutations were reduced to 5045 genes (without mutation information)
keeping all annotations associated with different gene mutations.

The next step is to retrieve the Entrez Id (unique gene identifier)
for each one of the 5045 genes associated with the mortality/ageing
phenotypes. Genes without an Entrez Id were discarded, further
reducing the number of instances to 4575. Finally, we retrieved the
UniProt Id associated with each Entrez Id., using the UniProt ID
Mapping Tool. This gives us information about the protein product
associated with each gene. Genes having the same UniProt Id were
discarded, leaving us with the final number of 3886 proteins (in—
stances), each instance linked with one protein and a list of mortal—
ity/ageing phenotypes (MPO terms used as class labels).

For the list of 3886 proteins, we derive five datasets, each with a
different feature type: numeric features, protein motifs features,
Protein—Protein Interaction (PPI) features and two types of KEGG
pathway features, explained later. We brieﬂy describe next these
features.

2.1.1 Numeric dataset

We extracted the following numeric features from the amino acid se—
quence of the proteins, described by Salama and Freitas (2013); Silla
Jr. and Freitas (2011b): ‘Amino Acid Composition’ (21 features, 20
from standard amino acids and one for Selenocysteine),
‘Composition’ (3 features), ‘Transition’ (3 features), ‘Distribution’
(15 features), and ‘Z—Values’ (15 features), ‘Sequence Length’, and
‘Molecular Weight’, totalling 59 features.

2.1.2 Protein motif dataset

The binary motif features represent the presence or absence of a
motif in the amino acid sequence of the protein. A motif is a tem—
plate describing sequences of amino acids that occur recurrently in
proteins. Motifs serve as a high—level representation of a protein and
it is expected that proteins sharing some specific motifs share similar
functions. We have used the same motif features studied in Silla Jr.
and Freitas (2011b): Interpro, Pfam, Prosite and PRINTS. We have
considered the motifs occurring in at least 1% of proteins (instances)
in the dataset, to avoid classifier overfitting, resulting in a total of 95
motif features.

2.1.3 Protein-protein interaction (PPI) dataset

This type of binary feature indicates whether or not an ageing—
related protein interacts with each of a set of other proteins (which
may or may not be ageing—related proteins). Interacting partners of
one protein often give away hints of its function (Sharan et al.,
2007). We have used the BioGrid (http://thebi0grid.0rg) database to
extract PPIs and have only considered features representing interact—
ing partners occurring in 1% or more instances in the dataset, to
avoid classifier over—fitting. This resulted in a total of 13 PPI
features.

2.1.4 KEGG pertinence (KEGGP) pathway dataset

KEGG pathways are directed—graph representations of interactions
between several types of biological products (e.g. genes or proteins).
To build our KEGG pathway features we have parsed the KGML
representations of the mouse KEGG pathways under the condition
that at least 1% of our instances must be present in the pathway in
other for the pathway to be considered. This generated a total of
221 KEGGP features.

We have created two pathway feature datasets. The first, simi—
larly to the PPI and motif datasets, contains binary features inform—
ing the pertinence of each instance (protein) into several KEGG
pathways. We call this dataset KEGGP (KEGG Pertinence) from
now on. Pertinence features based on KEGG pathways have already
been explored in other works involving data mining, e.g. (Jungjit
et al., 2014; Keerthikumar et al., 2009).

2.1.5 KEGG inﬂuence (KEGGI) pathway dataset

In this dataset, the KEGG pathway features represent the relative in—
ﬂuence of an instance (the reference protein) on the other proteins
that are downstream in relation to the instance in some KEGG path—
way. This feature quantifies an inﬂuence that an instance (reference
protein) has on the downstream proteins of a KEGG pathway, the
idea being that proteins that have a common inﬂuence on a set of
downstream proteins share similar function. Consider that one
ageing—related protein affects a set of downstream proteins in a given
way. If another protein affects the downstream proteins in a similar
way, then it is likely that that protein is also ageing—related.

The use of complex KEGG—based pathway features for data min—
ing has been proposed in other works: Zhang and Wiemann (2009)
proposed a software tool to construct a graph—based model of
KEGG pathways. Xia and Wishart (2010) used graph—based KEGG
features for metaholomics analysis. Chen et al. (2010) used charac—
teristics extracted from the KEGG pathway graph to classify the
pathways into ‘biologically meaningful’ or not. Breitkreutz et al.
(2012) correlated the complexity of cancer—related KEGG pathways
to patient survivability. Despite being previously used for different
goals, as far as we know, this paper is the first work proposing com—
plex KEGG—based features for the classification of protein functions.

The inﬂuence score for a given protein 17 has the minimum value
of 0.0 when the reference protein (Pref) does not influence p at all,
because 17 is not ‘downstream’ of (i.e. cannot be reached from) Pref.

Figure 2 shows an example of the calculation of the proposed ‘in—
ﬂuence’ score for a hypothetical instance (reference protein) and a
set of downstream proteins. Proteins P1, P2 and P6 have a score of
0.0, since they are not downstream of Ref.

 

 

 

 

 

 

 

 

Fig. 2. Example of score values (5) for five downstream proteins
(P3,P4,P5,P7,P8) in relation to a reference protein Pref. Diamond-shaped
nodes represent proteins that are parent of some downstream protein but are
not downstream protein themselves

91oz ‘Og isnﬁnv uo salaﬁuv 50'] ‘BtHJOJtIBQ JO AJtSJQAtuf] 112 /310'S[Buln0prOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

F. Fabris and A. A. Freitas

 

The score of a given protein p that is downstream of Pref has the
maximum value of 1.0 if, when Pref is removed from the pathway,
the downstream protein p becomes unreachable from the proteins
that are not downstream proteins of Pref. The biological meaning
that we want to capture is that a knockout on Pref would nullify the
standard behaviour of the downstream protein p. Proteins P3 and
P7, in Figure 2, have a score of 1.0 since if Pref is removed from the
pathway, proteins P3 and P7 will be disconnected from the KEGG
pathway graph defined by the set of proteins that are not down—
stream proteins.

If the score of a given protein p that is downstream of Pref has a
value of 0.5, it means that Pref accounts for half of the influence that
the downstream protein p receives. Removal of Pref would not nul—
lify completely the standard behaviour of the downstream protein p,
because there would be one more protein (which is not downstream
in relation to Pref) that also affects p, therefore the inﬂuence of Pref
on p is 50%. Protein P5, in Figure 2, has a score of 0.5 because if
one removes protein Pref from the graph, protein P5 would still be
reachable from protein P2, which is not a downstream protein.

In practice, to calculate the value of the features for each in—
stance, we need to build two sets of proteins: the first, the down-
stream proteins, comprises the proteins that are downstream of the
current instance, Pref. The second set, the non-downstream parent
proteins, contains the proteins that are not downstream of Pref but
are the parents of a protein that is downstream of Pref—e.g. proteins
P2 and P6 in Figure 2. Finally, for each downstream protein, the in—
ﬂuence score is equal to 1 / (1 + page“), where pagect is the number of
non—downstream parent proteins that have an effect (direct or indir—
ect) on the downstream protein. We consider that a non—
downstream protein has an effect on the downstream protein if the
non—downstream proteins can reach the downstream protein.

To illustrate these concepts in detail lets us consider protein Pg
(see Figure 2), which is in the set of downstream proteins of Pref.
Because both non—downstream parent proteins affect P8 (both P2
and P6 can reach P8) the value of the inﬂuence score for P8 is
1/(1 + 2) : 0.3.

This gives us a set of downstream protein scores for the instance.
We repeat this procedure for every available KEGG pathway. If the
same downstream protein occurs more once in the same pathway,
we keep the highest score. We discard the features (downstream pro—
teins) with value> 0.0 in less than 1% of the instances, totalling
1331 features. We call this KEGG pathway dataset KEGGI (KEGG
Inﬂuence) from now on.

2.1.6 Combined datasets

We have created two datasets by combining some feature types. The
first dataset was created by joining all five feature types into a single
dataset. We call it the ‘ALL’ dataset. The goal of creating this data—
set is to investigate if joining feature types from different domains
increases the overall predictive performance of the classifiers.

We have also joined the KEGGI and KEGGP datasets to create a
new dataset called ‘KEGGPI’. The KEGGPI dataset combines two
similar feature types with complementary characteristics: while the
KEGGP feature type provides the coarse—grained information about
the pertinence of a protein in a KEGG pathway, the KEGGPI feature
type encodes the fine—grained information of the inﬂuence of a pro—
tein in a KEGG pathway. We expect that by combining these two
feature types, with different strengths, will result in models with su—

perior predictive performance.

2.2 Hierarchical classification algorithms

Interpretability is a desirable characteristic when designing features
for classification (Freitas, 2013) as long as predictive power is not
sacrificed. In order to check if our newly proposed KEGG features
have at least comparable predictive power in relation to the other
three types of features (numeric, PPI and motifs) used in (Fabris and
Freitas, 2014), we use the following hierarchical classification
algorithms.

2.2.1 The predictive clustering tree (PCT) algorithm

The PCT algorithm (Struyf et al., 2005) creates a decision tree by
finding binary splits that recursively divide the training data in two
disjoint clusters until a given threshold that measures the quality of
the split is not met. In the testing phase, the algorithm finds which
cluster (leaf node of the tree) the instance belongs to using the tree
induced in the training phase and then returns a class probability
vector that represents the probability of the instance belonging to
each one of the hierarchical classes. This class probability vector is
calculated by first creating a binary vector for each instance in the
cluster. The i—th position of this binary vector has the value ‘1’ if the
instance is annotated with the i—th class label and ‘0’ otherwise. The
PCT ’5 final class probability vector is the average of the binary vec—
tors of the instances in the cluster. Note that the class probability
vectors are guaranteed to be consistent with the class hierarchy (the
computed probability of each child class is always smaller than or
equal to the probability of its parent classes).

2.2.2 The hierarchical dependence network using non-structural
relationships (HDN-NSR) algorithm

A dependence network is a Probabilistic Graphical Model (PGM)
where nodes represent random variables (features or classes) and
directed edges represent dependencies among variables (Heckerman
et al., 2001).

The Hierarchical Dependence Network (HDN) algorithm
(Fabris and Freitas, 2014) is a type of PGM that uses the Gibbs
Sampling algorithm to predict the probability of a protein belonging
to each one of the hierarchical classes. The HDN algorithm uses the
relationship given by the class hierarchy to create the edges of the
Dependency Network. The main advantage of the HDN algorithm
is that, contrary to most PGMs (e.g. Bayesian Networks), it allows
for loops in the graph—representation of the dependencies among the
random variables.

This work uses the HDN—NSR variation of the HDN algorithm
(Fabris and Freitas, 2015), which uses a more sophisticated proced—
ure to find the relationships between the classes of the hierarchy. It
has been shown that HDN—NSR has overall better predictive per—
formance than HDN and greater potential for finding relations that
were not initially present in the class hierarchy, possibly useful for
biologists studying the ageing process.

The HDN—NSR algorithm uses a standard classification algo—
rithm with probabilistic outputs to estimate the probability of each
hierarchical class. We have chosen to use the SVM (Support Vector
Machine) classifier due to its high predictive power, and we applied
the F—test feature selection method (Hall, 1999) to reduce the feature
space. To train the classifier for each class c,- we consider as positive
examples the instances annotated with class c,- or any of its descend—
ants, and as negative examples the complementary set of instances.

After we run the HDN—NSR classifier we limit the value of the
probability of each class to the minimum probability of its parents,
to maintain the classification consistence across the class hierarchy.

9103 ‘Og isnﬁnv uo sajaﬁuv 50'] ‘BtHJOJtIBQ JO AJtSJQAtuf] 112 /310'S[Buln0prOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

New KEGG pathway—based interpretable features for classifying ageing—related mouse proteins 5

 

2.2.3 The local hierarchical classifier (LHC) algorithm

The LHC algorithm is a collection of ﬂat binary classification algo—
rithms trained to predict independently each one of the classes in the
hierarchy. We have again used the SVM algorithm as a base classi—
fier applying the F-test feature selection algorithm prior to training
the algorithms. We have also used the same strategy to define the
positive and negative examples that we used for the HDN—NSR
algorithm.

Usually, when using the LHC approach in the testing phase, the
top—down strategy is applied: first, the highest—level classes (exclud—
ing the root node) are predicted. Then, the algorithm recurses to the
children of each positively predicted class, until no positive predic—
tions are made or a leaf node is reached. As we are dealing with
probabilistic classifications instead of crisp classifications, we apply
the same procedure to guarantee prediction consistence used in the
HDN—NSR algorithm, limiting the probability of each class to the
probability of its parents.

2.3 Measures estimating the predictive performance of
hierarchical classification algorithms

We have used three measures of predictive accuracy (AU(WC),
WW and AUPRC (Vens et al., 2008)) based on the Area Under
the Precision Recall Curve (A UPRC). In the ﬂat classification con—
text this measure works by constructing a PR curve (a plot of the
classifier’s precision as a function of its recall) thresholding the out—
put (class probability) of the classifier. Each threshold is associated
with a value of precision and recall, corresponding to a point in the
PR space. To obtain a single performance measure from the curve,
we calculate the area under the curve using a trapezoidal approxi—
mation (Boyd et al., 2013). A perfect classifier would have an
AUPRC of 1.0. For more detail on how this measure is calculated,
see (Vens et al., 2008).

2.4 Interpreting the classification model induced using
the KEGG pathway features

Initially we generated a classification model using the KEGG fea—
tures and all the 81 hierarchical classes in the MPO dataset.
However, this led to results that were difficult to interpret, because
the ageing—related proteins are much less common (85 out of 3886)
than the mortality—related proteins. For this reason, the classification
models focused on discriminating the mortality—related classes much
more than the ageing—related classes.

The high class imbalance of the original hierarchical class dataset
(only 2% of instances belong to the ‘ageing’ class) is detrimental for
classifiers predicting the ‘ageing’ class, and consequently for inter—
preting the models. To tackle this problem, in another experiment,
we have introduced two simplifications for generating interpretable
models: (i) we have joined all ageing—related classes into a single
ageing—related class and all mortality—related classes into a single
non—ageing—related class, transforming the hierarchical classification
problem into a binary classification problem and (ii) we have under—
sampled the mortality—related proteins to a 1/1 ratio of instances be—
tween the two classes in the training set.

Also, instead of using the PCT algorithm for generating the clas—
sification models, we used conventional algorithms for binary classi—
fication that generate interpretable models. Namely, we have used
the J48 algorithm, which generates a decision tree, the Decision
Table (DT) algorithm, which generates a table with a set of condi—
tions that must be satisfied for an instance to be classified as ageing—
related, the PART algorithm, that builds several C4.5 decision trees,
extracting rules from the ‘best’ leafs and the JRip algorithm, which

builds a rule list by incrementally growing and pruning the model
until a given stopping criterion is met. All four binary classification
algorithms are available in the Weka data mining tool (Hall et al.,
2009).

3 Results

3.1 Predictive accuracy results for hierarchical
classification (with 81 classes)

Table 1 shows the predictive accuracy results of the three algorithms
we have tested in the seven hierarchical datasets (5 different feature
types and 2 combined feature types). Note that in this work we are
interested mainly in comparing datasets (feature types), not algo-
rithms. So, Table 1 shows, for each accuracy measure, the average
rank of each dataset. The average rank is calculated by first assign—
ing a rank varying from 1 (highest predictive accuracy) to 7 (lowest
accuracy) to each dataset for each combination of classification al—
gorithm and measure. Next, for each measure, the values displayed
in the ‘Avg. Rank’ rows of Table 1 are calculated by averaging the
ranks of each dataset across algorithms. The best (smaller) average
rank for each accuracy measure is highlighted in boldface.

For each combination of hierarchical algorithm and accuracy
measure in Table 1, we have applied the paired t-test with the
Hochherg correction (Demsar, 2006) for multiple comparisons
(using the individual results on the 10 folds of the cross—validation
process) to check if the predictive accuracy of the model induced
using the best dataset in the row is statistically significantly different
from the accuracy of the model induced by the same classification
algorithm using the other dataset. Statistically significant results are
marked with a dagger (1').

Note that the ‘ALL’ dataset is statistically significantly better
than all other 6 datasets when using LHC and HDN—NSR. When
using the PCT algorithm, the best dataset is KEGGP. Overall, con—
sidering all 3 algorithms, the best (smallest) average rank was ob—
tained by the ‘ALL’ dataset for the AU(W) and AUPRCW
measures, while the combined KEGGPI dataset had the best rank
for the m measure.

We can also observe that the rank of the KEGGPI feature type
was better than the rank of the KEGGP feature type only when using
the W measure. We can explain this behavior by analysing the
bias of the AUPRC measure. This measure weighs all hierarchical
classes equally, including those with relatively few proteins. So, clas—
sification models that use a wider range of features types (that can
better predict more classes) are favored in relation to models which
are better at predicting hierarchical classes with more instances
using more general feature types.

To find out which individual feature representation is the best,
we removed the combined datasets and performed a second statis—
tical analysis. In this second study, the KEGGP feature type is al—
ways either statistically significantly better than all other feature
types or is in the group of statistically equivalent feature types that
include the best feature type.

It is also import to note that although the KEGGI feature type
carries more complex information than the KEGGP feature type
overall, the latter produces more accurate models. In fact, we have
observed that PCT models generated using the KEGGP feature type
have substantially more splits than the ones generated using KEGGI
features. This is due to the smaller number of non—zero feature val—
ues present in the KEGGI dataset, which culminates in hierarchical
classes with too few instances with non—zero feature values for a
good classifier to be induced.

9103 ‘Og isnﬁnv uo sajaﬁuv 50'] ‘BtHJOJtIBQ JO AJtSJQAtuf] 112 /310'S[Buln0prOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

6 F. Fabris and A. A. Freitas

 

Table 1. Predictive accuracies of the three hierarchical classification algorithms over the seven used datasets. Due to lack of space, the
HDN-NSR algorithm is referred to simply as ‘HDN’

 

Dataset (feature type)

 

Mea. Alg. KEGGP KEGGI KEGGPI PPI Motifs Num. ALL
AU(P—C) PCT 0.715 0706* 0.714 0709* 0710* 0.711 0.714
LHC 0716* 0709* 0714* 0709* 0710* 0718* 0.722
HDN 0718* 0710* 0715* 0710* 0711* 0718* 0.721
Avg. Rank 2.3 6.7 3.3 6.3 5.0 2.7 1.7
AUPRCW PCT 0.556 0.547 0.556 0544* 0545* 0537* 0545*
LHC 0551* 0536* 0541* 0541* 0539* 0551* 0.566
HDN 0546* 0526* 0544* 0529* 0536* 0552* 0.563
Avg. Rank 2.7 5.7 3.0 5.7 5.3 3.7 2.0
AUPRC PCT 0.146 0141* 0144* 0136* 0136* 0128* 0135*
LHC 0141* 0141* 0143* 0130* 0134* 0134* 0.147
HDN 0138* 0129* 0139* 0124* 0131* 0134* 0.147
Avg. Rank 2.3 4.3 2.0 6.3 4.7 5.7 2.7

 

Numbers in boldface represent the top result in the row. Boldfaced ranks represent the best (smaller) ranks. Daggers (1) denote t-tests results rejecting the null
hypothesis of equivalence between the best feature type (in boldface) and the current feature type, concluding that the models generated using the best feature

type are statistically superior (or = 0.025). Due to lack of space, the HDN-NSR algorithm is referred to simply as ‘HDN’.

Table 2. AUROC measure results for the classification algorithms on the binary class dataset described in Section 2.4

 

 

Alg. Dataset (feature type)

KEGGP KEGGI KEGGPI PPI Motifs Numeric ALL
J48 0.584 0.505 0.584 0.508 0.507 0.495 0.566
DT 0.605 0.533 0.593 0.518 0.518 0.484 0.541
JRip 0.556 0.523 0.563 0.520 0.505 0.514 0.461
PART 0.581 0.499 * 0.583 0.504 * 0.485 * 0.543 0.585
Avg. Rank 1.8 4.8 1.8 4.5 6.0 5.8 3.5

 

Boldface numbers highlight the best result. Daggers (1) next to a result indicate statistically worse results than the best result for the algorithm in the row, ac—
cording to a paired t-test using the Hochberg step—up correction (Demsar, 2006) (a = 0.05). The rank in boldface indicates the best (smallest) rank.

3.2 Results for binary classification

3.2.1 Predictive accuracy analysis

We tested 4 well—known algorithms that generate interpretable clas—
sification models from binary class datasets: J48, Decision Table
(DT), JRip and PART; all available in the Weka data mining tool
(Hall et al., 2009). Table 2 shows the Area Under the ROC
(Receiver Operating Characteristic) curve (AUROC) measure results
obtained by the four classification algorithms for the seven used
datasets. The rankings of the feature types are calculated in the same
way as described in Section 3.1.

The AUROC measure informs us the quality of the probabilities’
ranking given by the classification model. That is, the AUROC
measure has the maximum value of 1.0 if, for all ageing—related class
instances, this class’ probability estimated by the model is higher
than the estimated probabilities assigned to the non—ageing—related
instances. A random classifier is expected to have an AUROC meas—
ure of 0.5.

By analysing Table 2 we can conclude that few feature types
have AUROC values statistically significantly worse (or : 0.05) than
the best performing feature type. This happened in three cases
(shown by a dagger (1')), all when using the PART algorithm: when
comparing the ‘ALL’ dataset against the Motif, PPI and KEGGI
datasets.

The combined KEGGPI and KEGGP feature types had the best
(smallest) joint predictive accuracy rank across the four classifica—
tion algorithms in Table 2. If one is interested only in predictive
accuracy, one could use just the KEGGP feature type instead of
the combined KEGGPI feature type. However, when model

interpretation is important (as it is the case here) using the KEGGPI
feature type has the advantage of providing additional, more precise
information, while maintaining predictive accuracy. Therefore, the
KEGGPI feature type is useful for the binary classification problem
we are studying.

3.2.2 Interpreting results for binary classification

We have interpreted the model generated for the KEGGPI feature
type induced by the DT (Decision Table) algorithm from the binary
class dataset described in Section 2.4. This choice of classification al—
gorithm/dataset was made for three reasons: the KEGGPI and
KEGGP datasets were tied as the best dataset in Table 2; the
KEGGPI dataset comprises the KEGGP and KEGGI feature types,
so we can interpret both at the same time; and the DT algorithm
had the best predictive performance across the seven feature types
(winning in 4 out of 7 feature types).

In Table 3 we show the classification rules created by the DT al—
gorithm for predicting ageing—related protein functions. A rule is a
set of feature values that a protein must have to be classified as ei—
ther ageing related or non—ageing related. The first rule of Table 3
means: if a protein is not present in the KEGG pathways in columns
2—8; and the protein inﬂuences protein P11440 (Cyclin—dependent
kinase 1), present in pathway mmu04110 (Cell cycle); then the pro—
tein is likely to be ageing related. The last two columns of this table
show the coverage (number of instances classified by the rule) and
the accuracy (percentage of correctly classified instances) of each
row (rule). Note that the rule containing the ‘yes’ condition for the
KEGGI feature type (first row) had the best accuracy (28%) with

9103 ‘01; isnﬁnv uo so1a§uv soc] ‘BIIIJOJIIBD 10 AJLSJQAtuf] 112 /310'S[Buln0prOJXO'SOIJBLUJOJIIIOIq/ﬂduq 111011 papeo1umoq

New KEGG pathway—based interpretable features for classifying ageing—related mouse proteins 7

 

Table 3. Classification rules that predict the ‘ageing’ class, gener-
ated by the DT algorithm using the KEGGPI dataset

 

P11440_04110 4151 5168 4660 4380 4350 4917 3420 Cov. %Ac.

 

> 0 No No No No No No No 21 28
= 0 Yes No No No No No No 1 3 9 6

= 0 No Yes No No No No No 5 9 1 2
= 0 No No Yes No No No No 27 1 5
= 0 No No No Yes No No No 2 1 1 0
= 0 No No No No Yes No No 49 1 0
= 0 No No No No No Yes No 1 3 2 3
= 0 No No No No No No Yes 20 20

 

The ﬁrst column presents a KEGGI feature; its name shows the Uniprot Id
of the protein that is being potentially inﬂuenced by an instance and (after the
‘_’) the Id of the KEGG pathway where the inﬂuence can occur. The next 7
columns show binary KEGGP features, indicating whether or not an instance
belongs to the corresponding KEGG pathway. The last two columns show the
coverage and the % accuracy of each rule. Due to lack of space, we have sup-
pressed the ‘mmuO’ preﬁx in the KEGG pathways ids. Each row shows the
conditions that must be satisﬁed for a protein to be predicted as ’ageing—
related’. The selected KEGG pathways are: mmu04110 (Cell cycle),
mmu04151 (PI3K—Akt signaling pathway), mmu05168 (Herpes simplex infec-
tion), mmu04660 (T cell receptor signaling pathway), mmu04380 (Osteoclast
differentiation), mmu04350 (TGF-beta signaling pathway), mmu04917
(Prolactin signaling pathway) and mmu03420 (Nucleotide excision repair).

good coverage (21 instances). At first glance, an accuracy of 28%
seems small, but recall that only 2% of instances are ageing—related,
so an accuracy of 28% is actually a 14—fold increase in relation to
the prior probability of the ‘ageing—related’ class.

In Figure 3 we show how we can use the information given in
column 1 (from the KEGGI feature type) to interpret the rules cre—
ated by the DT. This figure shows part of the KEGG pathway
mmu04110 and highlights the inﬂuence of several proteins on the
protein P11440 (CDK1). Our results suggest that if a reference pro—
tein Pref has any influence in CDK1 (feature value > 0), then Pref is
more likely to be ageing—related.

Note that the highlighted proteins are, at the same time, in—
stances and features for other proteins. For example, the instance
representing protein ‘Chk1,2’ (reference protein) influences protein
CDK1 (feature) according to our score. Therefore, this instance
(‘Chk1,2’) has a non—zero value in the feature associated with inﬂu—
ence on protein CDK1. At the same time, CDK1 is also an instance,
having features with a non—zero value in the set of proteins it inﬂu—
ences. Note that not all proteins in the KEGG pathway are instances
in our ageing—dataset, i.e. not every protein associated with a feature
is an instance.

In contrast, the KEGGP feature type provides a different type of
information; the conditions involving this feature merely inform the
user if a protein (an instance) is present in a KEGG pathway or not.
E.g. the seventh row of Table 3 (second most accurate rule) informs
us that if a protein is in the KEGG pathway mmu04917 (Prolactin
signaling pathway), is not present in the other selected pathways
and has no inﬂuence on protein P11440 in pathway mmu04110, it
is likely ageing—related. This suggests that proteins in the ‘Prolactin
signaling pathway’ may have some inﬂuence on the ageing process,
so some other proteins present in the same pathway could be candi—
dates for further investigation.

4 Discussion

We have presented the construction of two KEGG feature types for
the classification of ageing—related protein functions engineered

DNA damage checkpoint

    

 

 

Fig. 3. Graphical representation of the KEGG pathway mmu04110 (Cell cycle)
with highlighted interesting proteins and interactions. The protein complex
highlighted with a solid black line contains the protein ‘Cyclin-dependent kin-
ase 1' (CDK1), which occurs in the feature selected by the DT algorithm (see
Table 3). The highlighted dashed grey proteins represent ageing-related pro-
teins that influence CDK1. The highlighted solid grey proteins represent non-
ageing-related proteins that influence CDK1. Solid grey edges represent all
possible influence paths from a solid grey protein to CDK1. Dashed grey
edges represent all possible influence paths from the dashed grey protein to
CDK1

specifically with the goal of predicting protein function using inter—
pretable features.

The advantages of using the KEGG—derived features types are
two—fold: (i) the good predictive performance of the models induced
using these two features together and (ii) the improved interpret—
ation potential of using richer features to represent the instances. In
fact, the KEGG pathway seems to be a very appropriate database to
use when interpretation is required, since it is focused on integrating
not only biological data from several sources, but also concepts
about the data (Kanehisa et al., 2011).

The downside of relying on such rich source of data is that, in
order to compute values of the KEGGP and KEGGI features for an
instance, the corresponding gene or protein must first be character—
ized into some KEGG pathway, which involves laborious wet—lab
experimentation. So, an uncharacterized protein represented only by
its amino acid sequence cannot be classified using KEGGP and
KEGGI features (nor using PPI and Motif features). In fact, in this
scenario, out of the five feature types used in this work, only the
‘Numeric’ feature type could be used, which is arguably the most
difficult to interpret due to its low level of abstraction.

The KEGG Pertinence (KEGGP) feature type, used for the first
time for hierarchical classification in this work, had the best per—
formance according to our statistical analysis compared to three
other feature types and the KEEGI feature type, a new KEGG fea—
ture type proposed here.

The combined KEGGPI dataset (using both KEGGP and KEGGI
features) had the best mean rank on the binary class dataset, tied
with the KEGGP feature type. Although the KEGGP feature type
has a simpler interpretation, if a richer, more precise model

9103 ‘01; isnﬁnv uo so1a§uv soc] ‘BIHJOJIIBD 10 AnsJQAtuf] 112 /310'S[Buln0prOJXO'SOlJBLUJOJIIlOlq”Zduq 111011 papeo1umoq

F. Fabris and A. A. Freitas

 

interpretation is desired (as it is the case here), the combined
KEGGPI feature type is more suitable, as it contains both the easier
to interpret KEGGP feature type (at a higher level of abstraction)
and the KEGGI feature type (with finer—grain information). To illus—
trate this point, we have shown how the KEGGPI feature type can
be used for generating biological knowledge using the Decision
Table algorithm, which generates interpretable classification
models.

We have also contrasted the interpretation of the KEGGI feature
type with the interpretation of the simpler KEGGP feature type and
concluded that the complementary nature of these two feature types
provides a good range of biological information: the KEGGI feature
type presents more precise information to the user, enabling a richer
interpretation of the classification model: it quantifies the inﬂuence
of a current (reference) protein on another specific protein in a given
KEGG pathway. On the other hand, the KEGGP feature type tells
the user the higher—level information of which KEGG pathways are
important for discriminating between ageing and non—ageing related
proteins.

Acknowledgements

We thank Dr. J. P. de Magalhaes for valuable discussions about the early
phase of this work.

Funding

This work was supported by Capes, a Brazilian research-support agency
[grant number 0653/13-6 to F.F.].

Conﬂict of Interest: none declared.

References

Boyd,K. et al. (2013) Area under the precision—recall curve: point estimates
and conﬁdence intervals. Mach. Learn. Knowl. Discov. Datah., 8190,
451—466.

Breitkreutz,D. et al. (2012) Molecular signaling network complexity is corre-
lated with cancer patient survivability. Proc. Natl. Acad. Sci. U. S. A., 109,
9209—9212.

Chen,L. et al. (2010) Analysis of protein pathway networks using hybrid prop-
erties. Molecules, 15, 8177—8192.

Demsar,J. (2006) Statistical comparisons of classiﬁers over multiple data sets.
]. Mach. Learn. Res., 7, 1—30.

Eppig,J.T. et al. (2015) The mouse genome database (MGD): facilitating
mouse as a model for human biology and disease. Nucleic Acids Res., 43,
D726—D736.

Fabris,F. and Freitas,A.A. (2014). Dependency Network Methods for
Hierarchical Multi—label Classiﬁcation of Gene Functions. In: Proc. 2014
IEEE Comp. Intel. and Data Mining, pp. 241—248.

Fabris,F. and Freitas,A.A. (2015 ). A novel extended hierarchical dependence
network method based on non—hierarchical predictive classes and applica-
tions to ageing—related data. In: Proc. 27th IEEE Intl. Conf. on Tools with
Artiﬁcial Intel., pp. 294—301.

Forman,G. (2002) Feature engineering for a gene regulation prediction task.
ACM SIGKDD Explor. Newslett., 4, 106—107.

Freitas,A.A. (2013) Comprehensible classiﬁcation models — a position paper.
ACM SIGKDD Explor. Newslett., 15, 1—10.

Goldman,D.P. et al. (2013) Substantial health and economic returns from
delayed aging may warrant a new focus for medical research. Health
Affairs, 32, 1698—1705.

Hall,M. et al. (2009) The Weka data mining software: an update. SIGKDD
Explorations Newsletter, 1 1, 10—18.

Hall,M.A. (1999). Correlation-based feature selection for machine learning.
Ph.D. thesis, The University of Waikato, New Zealand.

Heckerman,D. et al. (2001) Dependency networks for inference, collaborative
ﬁltering, and data visualization. ]. Mach. Learn. Res., 1, 49—75.

Jungjit,S. et al. (2014). Extending multi—label feature selection with kegg path-
way information for microarray data analysis. In: 2014 IEEE Conf. on
Comp. Intel. in Bioinfo. and Comp. Biology, Honolulu, Hawaii. IEEE, pp.
1—8.

Kanehisa,M. et al. (2011) KEGG for integration and interpretation of large—
scale molecular data sets. Nucleic Acids Res., 40, D109—D114.

Kanehisa,M. et al. (2016) Kegg as a reference resource for gene and protein an-
notation. Nucleic Acids Res., 44, D45 7—D462.

Keerthikumar,S. et al. (2009) Prediction of candidate primary immunodeﬁ-
ciency disease genes using a support vector machine learning approach.
DNA Res., 16, 345—351.

Salama,K.M. and Freitas,A.A. (2013). ACO—Based Bayesian Network
Ensembles for the Hierarchical Classiﬁcation of Ageing—Related Proteins.
In: Evoltionary Computation, Machine Learning and Data Mining in
Bioinformatics, volume 7833 of LNCS, pp. 80—91.

Sharan,R. et al. (2007) Network-based prediction of protein function. Mol.
Syst. Biol., 3, 1.

Silla,C.N Jr. and Freitas,A.A. (2011a) A survey of hierarchical classiﬁcation
across different application domains. Data Mining Knowl. Discov., 44,
31—72.

Silla,C.N Jr. and Freitas,A.A. (2011b) Selecting different protein representa-
tions and classiﬁcation algorithms in hierarchical protein function predic-
tion. Intell. Data Anal., 15, 979—999.

Struyf,J. et al. (2005). Hierarchical Multi—classiﬁcation with Predictive
Clustering Trees in Functional Genomics. In: Progress in Artiﬁcial Intel.,
volume 3808 of LNCS, pp. 272—283.

Vens,C. et al. (2008) Decision trees for hierarchical multi—label classiﬁcation.
Mach. Learn., 73, 185—214.

Xia,J. and Wishart,D.S. (2010) Metpa: a web-based metabolomics
tool for pathway analysis and visualization. Bioinformatics, 26,
2342—2344.

Yepes,A.J.J. et al. (2015 ) Feature engineering for medline citation categoriza-
tion with mesh. BMC Bioinformatics, 16, 113.

Zhang,J.D. and Wiemann,S. (2009) Kegggraph: a graph ap-
proach to kegg pathway in r and bioconductor. Bioinformatics, 25,
1470—1471.

9103 ‘01; JSanV 110 so1a§uv 50’] ‘1211110111123 10 A11519A1uf] 112 /310's112u1n0fp101x0'so1112111101u101q//:d1111 111011 pap1201umoq

