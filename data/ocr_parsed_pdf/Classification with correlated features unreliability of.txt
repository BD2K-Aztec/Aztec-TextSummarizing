ORIGINAL PAPER

Vol. 27 no. 14 2011, pages 1986-1994
doi:10. 1 093/bioinformatics/btr300

 

Data and text mining

Advance Access publication May 16, 2011

Classification with correlated features: unreliability of feature

ranking and solutions
Laura Tolosi* and Thomas Lengauer

Department of Computational Biology and Applied Algorithmics, Max—PIanck—Institute for Informatics, Saarbr cken,

Germany
Associate Editor: John Quackenbush

 

ABSTRACT

Motivation: Classification and feature selection of genomics or
transcriptomics data is often hampered by the large number of
features as compared with the small number of samples available.
Moreover, features represented by probes that either have similar
molecular functions (gene expression analysis) or genomic locations
(DNA copy number analysis) are highly correlated. Classical model
selection methods such as penalized logistic regression or random
forest become unstable in the presence of high feature correlations.
Sophisticated penalties such as group Lasso or fused Lasso can
force the models to assign similar weights to correlated features
and thus improve model stability and interpretability. In this article,
we show that the measures of feature relevance corresponding to
the above-mentioned methods are biased such that the weights of
the features belonging to groups of correlated features decrease
as the sizes of the groups increase, which leads to incorrect model
interpretation and misleading feature ranking.

Results: With simulation experiments, we demonstrate that Lasso
logistic regression, fused support vector machine, group Lasso and
random forest models suffer from correlation bias. Using simulations,
we show that two related methods for group selection based on
feature clustering can be used for correcting the correlation bias.
These techniques also improve the stability and the accuracy of
the baseline models. We apply all methods investigated to a breast
cancer and a bladder cancer arrayCGH dataset and in order to
identify copy number aberrations predictive of tumor phenotype.
Availability: R code can be found at: http://www.mpi-inf.mpg.de/
~|aura/Clustering.r.

Contact: laura.tolosi@mpi-inf.mpg.de

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on December 20, 2010; revised on April 19, 201 1; accepted
on May 10,2011

1 INTRODUCTION

The accelerated development of microarrays and, more recently,
of high-throughput sequencing techniques affords genome-wide
measurements of molecular changes in the cell that have an impact
on cancer onset and progression. High-resolution experiments
targeting gene expression, DNA copy number or DNA methylation
in tumors can be the basis for discovering patterns predictive of
diagnosis, prognosis and therapy selection (Hicks et al., 2006;

 

*To whom correspondence should be addressed.

Ma et al., 2007; Mikeska et al., 2007; van’t Veer et al., 2001).
Machine-leaming techniques for classiﬁcation and feature selection
are often used for automated identiﬁcation of variables associated
with particular tumor phenotypes. In this article, we are concerned
with two widely discussed aspects of microarray classiﬁcation:
handling high dimensionality and ill conditioning.

The high dimensionality of microarray-based experiments
contrasting to the small number of samples easily leads to overﬁtting.
Regularized linear models such as logistic regression with ridge
(Hastie et al., 2001) or Lasso penalty (Tibshirani, 1996) are popular
solutions to ﬁtting sparse models in which only a small subset of
features plays a role. More sophisticated penalties for sparse model
selection are discussed by Zou and Li (2008).

The problem of ill conditioning refers to the existence of groups
of highly correlated features. The high correlations often have
a biological basis, for example if the correlated features relate
to the same molecular pathway (coregulated genes in expression
data), are in close proximity in the genome sequence (neighboring
genes in copy number data) or share similar methylation proﬁle
(consecutive CpG dinucleotides in CpG islands). Methods using
simple penalties like Lasso typically discard most of the correlated
features: only one or a few arbitrary representatives from every
group of correlated features enter the model, provided they are
relevant for the outcome. As a consequence, the models become
unstable: small changes in the training set result in dramatic changes
in the selected subset of features. If the purpose of feature selection
includes biological interpretation of the model, then stability must
be ensured. A successful approach used in many recent articles is
that of selection of groups of features. For example, the group Lasso
model (Meier et al., 2008) consists of Lasso selection of predeﬁned
groups of features. The fused support vector machine (Rapaport
et al., 2008) combines a Lasso and a ﬁised penalty for enforcing
similar weights on correlated features, this way performing group
discovery and group selection simultaneously. Another approach to
group selection adopted in a large class of methods uses clustering
procedures to discover feature groups, compute super features to
summarize every cluster and apply feature selection on the set of
super features. For example, in Park et al. (2007), the features are
grouped with a hierarchical clustering procedure and the cluster
centroids are used for training linear models. The Metagene method
(Huang et al., 2003a, b) consists of k-means clustering of the
features, followed by computing the principal components of the
clusters, called metagenes, which are used for model training. J ager
et al. (2003) use fuzzy clustering to determine groups of features and
then select a limited number of representatives from each cluster for
training SVM models. Yu et al. (2008) search for dense groups of

 

1986 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /3.Io's[Bumo[pJOJXO'sorwuiJOJurorqﬂ:duq 11101} papeolumoq

9103 ‘Og anﬁnv uo ::

Classification with correlated features: unreliability of feature ranking and solutions

 

features by kernel density estimation. The pelora method (Dettling
and Buhlmann, 2004) performs supervised grouping of features, by
iteratively updating the groups such that the accuracy of a penalized
logistic regression model is increased.

A non-parametric model often used in microarray classiﬁcation
is the random forest (Breiman, 2001; Diaz-Uriarte and Alvarez de
Andres, 2006; Fang et (11., 2008). In a recent study, Strobl et al.
(2008) observe that correlated variables are used interchangeably
in the decision trees of the random forest models. The authors
analyze the consequence of this phenomenon by simulating artiﬁcial
datasets containing few correlated variables with different predictive
values. They notice that the less relevant variables often replace the
predictive ones (due to correlation) and thus receive undeserved,
boosted importance. Strobl et al. (2008) introduce a new variable
importance measure that better reﬂects the predictive power of
each feature within a correlated group. In contrast to the study by
Strobl et al. (2008), we assume that the correlated features in a
group share the same predictive value (due to a common underlying
biological event) and we investigate how correlation affects the
feature importance given by random forest.

The main contribution of this article is to raise awareness of
a speciﬁc effect involving feature correlation in several of the
methods mentioned above, that can misguide model interpretation.
We observed that the Lasso penalized logistic regression, the group
Lasso, the fused SVM and the random forest report feature weights
which are affected by a type of bias which we call correlation bias.
Speciﬁcally, the features which belong to larger groups of correlated
features receive smaller weights, proportional to the group size, due
to a shared responsibility in the model. Therefore, if the group is
large enough, all features may appear irrelevant, even if they yield
high correlations with the outcome. This effect is expected in the
case of the sparse Lasso logistic regression, but is surprising in the
case of group Lasso and fused SVM, which are speciﬁcally designed
to afford selection at group level and improved model interpretation.
Moreover, such bias has not been reported on random forest models
previously.

We show using simulations that correlation bias exists and affects
several widely used classiﬁcation models for microarray data. We
also show that group selection based on feature clustering such as
provided by the method presented in Park et al. (2007) can be
successﬁilly used for removing the correlation bias. We test and
compare the methods investigated on two biological datasets.

2 METHODS

We consider only binary classiﬁcation problems. Let (xi, yi), i = 1, ...,N be N
i.i.d. observations of a p-dimensional vector x, e R” and a response variable
y,»e{0,1}. Denote be=(x1, ...,xN)e’RNX/’ the input matrix and ye{0,1}N
the binary outcome. In general. we will use small letters to refer to samples
x1,...,xN and capital letters to refer to features X1 ,  of the input matrix
X. In the manuscript. we will use the notion feature importance to refer to
the measures of feature relevance commonly used for model interpretation.
such as feature weights in linear models or variable importance in random
forest.

2.1 Classiﬁcation methods

Logistic regression is a popular method for classiﬁcation of biological data.
It models the logarithm of the posterior probabilities of the classes as linear
functions of the input features. The parameters WERP of the model are
estimated by maximizing the log-likelihood L(w;X, y) over the observations

in the training set. Model sparsity is obtained by adding a Lasso penalty A
[see Equation (1)]. which can be optimized with cross-validation. Feature
importance is given by the model weights wLLR:

p
ma=argm5erw:X.y>—i;IwiI (1)
,2

In what follows. we will call this model Lasso logistic regression (LLR).
In our experiments. we used the R package glmnet (Friedman and Hastie
and Tibshirani. 2010) for training LLR models.

Logistic group Lasso (Meier et al.. 2008) uses the logistic regression
model with a more specialized penalty. which takes into account some natural
grouping of the features. Assume there are G groups of predictors and each
group must be entirely included in the model by receiving non-zero weights
or be discarded as irrelevant. The group penalty is a combination of a Lasso
penalty acting at the group level and a ridge penalty on the predictors within
each group. If [g is the index of the features belonging to group g. then the
weights of the logistic group Lasso (GL) model are given by:

G
wat=argmng(w:X.y>—AZIIWI, i2 (2)
g2]

We use the R package grplasso (by Lukas Meier) for training GL models
and cross-validation for estimating the optimum penalty A.

The fusea1 S VM (Rapaport et al.. 2008) has been proposed for the special
case that the features can be ordered such that neighboring features are
expected to be correlated. This is the case in data on copy number aberrations.
where the features are genomic sites ordered by position in the genome.
Fused SVM (FSVM) is a linear support vector machine model with two
supplementary penalties: a Lasso penalty for model sparsity and a fused
penalty. which acts as a smoother of the weights. in such a way that weights
of neighboring features are forced to be similar. The weights of the model
wFSVM are obtained by minimizing a penalized hinge loss. as follows [see
Rapaport et al. (2008) for details]:

N p p
WFSVM=ZIgH3nZ[1—YiWTXil+ +AZ lWil +MZ lWi —Wi—ll (3)
1’21 [’21 i:2

The optimization problem given by System 3 can be solved by a
linear program. We implemented this method using Matlab and the CVX
optimization toolbox (Grant and Boyd. 2008). Cross-validation for both
penalty parameters A and p. is necessary. which makes ﬁtting an FSVM
model slower than ﬁtting the other methods.

Random forest (RF) models (Breiman, 2001) are non-parametric and
non-linear models. attractive due to their interpretability. They are based
on averaging over a large collection of decision trees. each trained on a
separate bootstrap sample of the input set. The aggregate model has lower
variance and is less susceptible to overﬁtting than a single decision tree.
Gini Importance (GI) and Variable Importance (VI) are two measures of
feature relevance that can be computed based on the RF model. We use
the R package randomForest (Liaw and Wiener. 2002) for training RF
models. There are two parameters that inﬂuence the performance of RF:
the number ntree of trees in the collection and the number mtry of variables
considered for each tree split. In our experiments. we use the recommended
value mtry = x/ number of features and we select the optimal value for ntree
Via cross-validation. Diaz-Uriarte and Alvarez de Andres (2006) evaluate the
performance of RF models for various parameter settings in 10 real-world
learning instances. Their results suggest that the default value of mtry affords
either optimal or close to optimal performance.

2.2 Correlation bias

In the classiﬁcation of high-dimensional data containing (large) groups of
correlated features. the requirements of model sparsity and of retrieving of
all predictive features are in direct competition. In applications in which
assessment of feature importance is the main objective. models that give

 

1987

112 /3.Io's[Bumo[pJOJXO'soneuiJOJurorqp:duq wort papeolumoq

9103 ‘Og anﬁnv uo ::

L.Tolosi and T.Lengauer

 

priority to the latter requirement should be preferred. In what follows. we
formulate three key properties that we believe a classiﬁcation model should
meet in order to be a good instrument for assessment of feature importance.

Assume that two independent biological events P1 and P; (e. g. the deletion
of a chromosome arm can be an event) inﬂuence the binary phenotype Y (e. g.
tumor stage ) and let us denote the magnitude of their effects on Y with E(P1)
and E(P2). respectively and assume that E(P1)>E(P2). Assume that. by
means of an experimental technology. variables associated with each of the
two events are measured (e. g. all genes located within a deleted chromosome
arm). Let us denote with U1,...,Uq the variables associated with P1 and
with V1 ,  V,,. the variables associated with P2. p, q 3 1. Consequently.
{Umsisq and {VJ-hﬂ-S}, form two groups of correlated variables. Assume a
classiﬁcation model M is used to predict Y from a set of N observations
on features U1,..., Uq, V1,..., Vp and this model assigns importance values

to features: wi , Hamlin/12%,. w2 Without losing generality. assume all the

 p.
importance values are pOSitive and a larger value indicates a more predictive

feature. The following three properties should hold:

(1) The importance values of the correlated features are similar: wl %
wiw...%wtl] and W1 wwgw...%wg.

(2) The importance of the variables reﬂect the magnitude of the effect
of the corresponding process on the outcome: w; Z wjzﬂi: 1..q,Vj =
1..p.

(3) The importance of the variables {wilhsisq and {wlzhsisp does
not depend on the corresponding group sizes. namely (1 and p.
respectively.

We require that property (i) holds because. in absence of a true model. it
is wise to give fair chances to all correlated variables for being considered as
causative for the phenotype. In this case. supplementary evidence from other
sources should be used for identifying the causative variable from a correlated
group. Property (ii) is based on the assumption that E(P1) > E(P2) and hence
any of the features {Ui}i5q contributes more to the outcome than any of the
features {VJ-bar Thus. the property ensures a fair ranking of the variables.
which is important in applications because often only a few top ranking
groups are considered for further investigation. Property (iii) demands that
the importance of the features does not change as more evidence (more
variables) about the corresponding events is added to the data.

In this article. we show that in classiﬁcation problems with groups of
correlated features. the assignment of feature importance by LLR. RF. GL
and FSVM does not meet requirements (ii) and (iii). Speciﬁcally. the reported
feature importance varies with the sizes of the correlated groups of features
and results in biased feature ranking. In the context of the example above.
the feature weights {w} } 195,, and {W12} 15,5], depend on the values of q and
p. respectively. in a way that larger group size leads to smaller importance
values. As a consequence. ifq is much larger than p. variables {Ui}lsisq can
falsely appear less predictive than variables {Vj} 13-5], and P2 is considered
more relevant than P1.

In sparse models like LLR. correlated features are generally discarded in
favor of a single representative. Instability of feature importance is a known
issue in such models (lager et al.. 2003; Park et al.. 2007). and it is easy to
observe that the larger the group. the smaller the chance of each particular
variable within the group is to be selected by the model. Therefore. under
repeated perturbations of the training set. the average weights of the features
decrease as the size of the group increases. In the case of FSVM. the weights
of correlated features are forced to be equal (or similar). Consequently. if the
group of correlated features becomes larger. the common weights need to be
decreased. in order to accommodate all features in the model and not Violate
the Lasso penalty. This rescaling of the weights is possible without decreasing
the accuracy of the model. since correlated features provide only redundant
information. In the Supplementary Material. we show how the interaction
between the two penalties of FSVM can cause correlation bias. (see Example
of correlation bias. Supplementary Material.) A similar effect can be observed
in GL models. In RF. the correlation bias is caused by the bootstrap sampling
of the observations and by the sampling of the features at each node of the

trees. which causes correlated features to be used interchangeably in the tree
components.

In this article. we say that models that do not meet requirements (ii) and
(iii) are affected by correlation bias.

2.3 Methods for reducing correlation bias based on FC

Intuitively. a good strategy for reducing the correlation bias is to group
the correlated features prior to model ﬁtting and derive corresponding
feature representatives as a summary of each group. The importance of
the original features can be deﬁned as the importance of the corresponding
representatives. A very simple and intuitive approach is described by Park
et al. (2007): average-linkage hierarchical clustering with Euclidean distance
is performed on the features. Cluster centroids are used afterwards for
training linear regression models. The features are standardized to mean
zero and standard deviation one before clustering. such that the Euclidean
distance is equivalent with the correlation distance between features. The
optimal number of clusters is selected in a supervised manner. by Visiting
each level of the clustering dendrogram and estimating the model accuracy by
means of cross-validation. Park et al. (2007) make use of the monotonicity
of the penalized loss and propose a path-following algorithm for efﬁcient
search for optimal parameters of the method. In this article. we apply the
same approach for clustering features. in combination with LLR and RF
models. Since RF are non-parametric models. an efﬁcient path-following
algorithm for simultaneous optimization of the number of feature clusters
and number of trees in the RF is not available. Therefore. the cross-validation
procedure is slower than the one presented in Park et al. (2007). For a
dataset with a few thousands of features. it becomes inefﬁcient to train
a model for all levels of the clustering dendrogram. The running time of
such a procedure is Zidﬂk). where C(k) is the running time required
by the classiﬁcation algorithm to ﬁt datasets with k features and and p is
the total number of features. Using a univariate ﬁltering for eliminating
irrelevant features can help in applications. in which relatively small number
of features are expected to be relevant. For example. the sure independence
screening method (Fan and LV. 2008) ensures that all predictive variables
survive the ﬁltering procedure. However. such ﬁltering would not provide
with substantial dimension reduction in applications like classiﬁcation of
copy number aberrations. because the number of predictive features can be
very large (e.g. all probes covering several chromosomes). Therefore. in
this study. we propose a cheaper. unsupervised alternative for estimating the
number of clusters. based on silhouette values (Rousseeuw. 1987).

For simplicity. we will refer to the method by Park et al. (2007) as
supervisea1 feature clustering (FC-Sup). By feature clustering (FC). we mean
the alternative approach using silhouette values for estimating the number of
clusters. We use both methods in combination with LLR and RF and compare
them with the baseline methods and with the group-penalty methods GL and
FSVM.

2.4 Model evaluation

We compare the performance of classiﬁcation methods on training sets
with (large) groups of correlated features. In particular. we analyze the
measures of feature relevance provided by the models investigated and seek
for evidence of correlation bias. Additionally. we report and discuss the
prediction accuracy of the models (estimated Via 10-fold cross-validation)
and the stability of the respective feature importance measures.

The stability of feature importance is deﬁned as the variability of
feature weights under perturbations of the training set. When the goal of
classiﬁcation is to select the most relevant features. small modiﬁcations in
the training set should not lead to considerable changes in the set of important
covariates. When the true distribution of the training set is not known.
stability can be inferred Via repeated sampling from the available training
observations. In Kalousis et al. (2005). classical 10-fold cross-validation
is used in order to create 10 overlapping training sets and model stability is
estimated by comparing the 10 resulting feature weightings. For this purpose.

 

1988

112 /3.IO'SIBIIJI’IOprOJXO'SOIJBLUJOJIIIOIq”Idllq uroii papeo1umoq

9103 ‘0g anﬁnv uo ::

Classification with correlated features: unreliability of feature ranking and solutions

 

the authors propose several measures of similarity between two vectors of
feature weights. For our purposes, the Pearson’s correlation coefﬁcient is
most suitable. Overall model stability is given by the average of all pairwise
Pearson correlations between feature weight vectors provided by the models
ﬁtted on the 10 variations of the training set. The stability score has a value
between —1 and +1. with higher values for more stable models.

3 DATA

3.1 Simulated data

Simulation A: we generated datasets with N: 100 samples and p=250
features. The features are divided into three groups: G1. G2 and R. Group R
has 50 features and the cardinality of group G2 is 2OO—|G1|, for different
values of |G1| e{100,120, 140,160,180}. The features in each group are
mutually correlated and any two features belonging to different groups are
independent. The features in group G1 are generated from the prototype
vector U , which is sampled from a mixture of two Gaussians with equal
probabilities: g0 =N(0,0.2) and g1 =N(1,0.3). The particular choice for a
Gaussian mixture stems from gene copy number data. where g0 corresponds
to those samples with normal copy number and g1 indicates aberrations
(copy number gains, in this case). We generate features U U1GI1 with
the following procedure: randomly select 20% of the components of U
and alter them by adding Gaussian noise N(0,0.5), then repeat |Gl | times.
The features generated this way are correlated with U and with each other
and resemble segmented copy number data, which are piecewise constant
with occasional changes. Using the same algorithm. we generate a prototype
vector V independent from U and corresponding features V1,  V1021. which
form group G2, and then repeat the procedure to simulate group R. Last, we
generate a binary outcome y with the following linear classiﬁcation rule:

_ 1, if5U+4V—(5U+4V)+8>0,
_ 0, otherwise.

where 8~N(0,0.1) and 5U+4V denotes the average of 5U+4V. From
the simulation parameters, it follows that features in group G1 are most
relevant to the outcome (being correlated to U ), features in group G2 are
less predictive and features in group R are irrelevant. Table 1 shows the
within group and between group average correlations, summarized over 100
simulated datasets.

Simulation B: we generate more complex artiﬁcial datasets by considering
10 groups of predictive features G1 ,  Gm and 20 groups of irrelevant
features, R1, ...,R20. The number of samples is N: 100. Each of the groups
G2 to G10 and R1 to R20 contains 10 correlated features and the cardinality
of group G1 takes. in turn, one of the values {10, 50, 100,200}. The groups of
correlated variables G1 ,  G10,R1 , ...,R20 are generated from the prototype
variables U1 ,  U10, V1 ,  V20, respectively. The simulation procedure is
similar to that of Simulation A. with different parameters: we alter all
components of the corresponding prototype vector by adding Gaussian noise
N(0,0.6). The binary outcome y is given by the linear rule:

_ 1, ifIOU]+9U2+-"+1U10—(10U1+9U2+"'+1U10)+8>0,
_ 0, otherwise.

The groups G1 to Gm are ordered decreasingly by their relevance to the
outcome. In Simulation B, the within-group correlations are smaller than
in Simulation A and the number of features is larger, which makes the
identiﬁcation of the groups of features more difﬁcult. In Supplementary
Figure S 1. we show the average correlations between features and the average
correlations between features and the outcome variable. for each group.
summarized over 100 simulations.

3.2 Real data

Bladder tumors: we tested our methods on a set of 98 CGH arrays
measuring copy number aberrations in bladder tumors. The experimental

Table 1. Pairwise Pearson’s correlation between features, averaged within
and between groups

 

 

G1 G2 R V
G1 0.86 0 —0.02 0.63
G2 0 0. 86 0 0.42
R —0.02 0 0. 87 0

 

The last column shows the average correlation between each feature group and the
outcome y.

settings and data have been described in Blaveri et al. (2005). DNA copy
number has been measured for 2142 probes distributed over all autosomes.
The correlation between adjacent probes is very high (median 0.82), see
Supplementary Figure S2. We considered two binary classiﬁcation problems.
by tumor grade and by tumor stage. For grade classiﬁcation, we used 19
samples with low grade (Grade 1) and 77 samples with high grade (Grade 2 or
3). For stage classiﬁcation, 84 samples were grouped into two classes: stage
Ta (29 samples) and stage T2+ (55 samples). We excluded the intermediary
stage T1 [as in Rapaport et al. (2008)]. For each classiﬁcation scenario, we
train RF. LLR, FSVM and GL models. We also ﬁt RF and LLR in combination
with FC and FC-Sup. For each model, we report accuracy (using 10-fold
cross validation), area under the curve (AUC) and feature importance.

Breast tumors: In Climent et al. (2007), 185 early stage breast tumors were
analyzed using arrayCGH technology (UCSF Hum Array 2.0). Copy number
aberrations are measured for 2369 BAC probes (chromosomes X and Y
excluded). High correlations between neighboring probes are observed. with
median value of 0.69 (Supplementary Fig. S3). The authors of the study use
statistical tests and report signiﬁcant associations between certain genetic
alterations and ER status (oestrogen receptor) and PR status (progesterone
receptor) of the tumors. Using the methodology introduced here, we identify
genetic lesions which help discriminate between ER positive and ER negative
tumors, and PR positive and PR negative tumors, respectively. In the cohort.
there are 60 ER negative and 101 ER positive tumors, and 65 PR negative and
96 PR positive tumors. For all models considered. we report classiﬁcation
accuracy and AUC (using 10-fold cross-validation) and feature importance.

4 RESULTS

4.1 Simulated data

Simulation A: we evaluated the performance of RF, LLR, FSVM
and GL with respect to the criteria described in Section 2.4. Figure 1
summarizes the importance values assigned to features from the
three groups G1, G2 and R. The average feature weights over 100
simulations are shown for each chosen cardinality of group G1,
with indication of standard deviation added. The correlation bias is
clearly demonstrated by the decreasing importance of group G1 as
its cardinality increases and conversely, the increasing relevance of
G2 as its cardinality decreases.

In the case of RF, when the number of features in group G1 is
larger than 140 (|G1|/|G2| >2.3), the ranking of the groups given
by GI and VI is incorrect in that features in G2 falsely appear most
relevant for the model (Fig. 1a and b). On average, the same effect
is observed in LLR models (depicted in Fig. 1c).

In the case of FSVM and GL, the correlation bias is noticeable
even if G1 = 120 (|G1|/|G2| = 1.5) (Fig. 1e). In the context of the
formal example from the Supplementary Material (see Example
of correlation bias in Supplementary Material), note that our

 

1989

112 /3.IO'SIBIIJI’IOprOJXO'SOIJBLUJOJIIIOIq”Idllq uroii papeo1umoq

9103 ‘0g anﬁnv uo ::

L.Tolosi and T.Lengauer

 

 

    
    
  

  
 
   
   

 

 

 

 

 

 

 

(a) (b) o (C)
“I
_ 1:51:100 V. _ — mace ° — mace
In, — ieII:120 ° — maze m — maze
o — lGI|:140 — name —, — name
— iGII=iso . — iG.I=Isu o — iG.I=Isu
_ 1:51:130 — ls.|=iau o — ls.|=iau
3 3I 3
In
.— 0.
._ 0' ' o
d
o
0.
G1 (32 R Gt G2 R Gt G2 R

 

— iGI|=1°°
— lGI|=120
— IGII=140
— lGI|=150
— iGI|=180

0.08

 

 

 

 

 

 

 

0.04

 

 

 

 

 

 

 

5

4

 

 

3

 

 

 

 

2

 

 

 

O

 

     

Fig. 1. Average importance of features for classiﬁcation of data from
Simulation A. The importance is averaged over groups G1, G2 and R. (a) RF
(GI); (b) RF (VI); (c) LLR; (d); GL (e); FSVM (f); RF-FC (g); LLR-FC (h)
RF-FC-Sup; (i) LLR-FC-Sup.

experimental results agree with the set of conditions (6) (a = 5, b = 4,
q: 120, r=80).

We used FC and FC-Sup as preprocessing step before training
RF and LLR models. The FC procedure almost always discovered
the correct number of groups of features (three). FC-Sup more often
than not overestimates the number of groups. Moreover, the RF
models select a larger number of groups than the LLR. Figure 2a
shows a summary of the selected number of feature groups by FC
and FC-Sup. Importantly, both FC and FC-Sup succeed in removing
the correlation bias, which is evident from Figure 1f—i.

The prediction accuracy of all models is summarized in Table 2.
All linear models outperform RF, which is expected because the
simulations are based on a linear model. With FC, both baseline
methods RF and LLR achieve higher accuracy. FC-Sup always
outperforms FC, which is surprising, given that FC discovers the
true number of feature groups and FC-Sup does not. Most probably,
the classiﬁcation is improved if each group is ﬁirther split into
several subgroups. This is possible because the feature groups are
not spherical, but rather elongated, thus a single centroid is not the
best representation of the group. As the cardinality ratio |G1|/|G2|
increases, the accuracy of FSVM and GL decreases and thus the
LLR with FC becomes signiﬁcantly better than FSVM and GL.

Table 3 shows the stability estimates for the various models. The
RF are most unstable, probably due to their increased complexity
and thus tendency to overﬁtting. As expected, the LLR are the most
unstable among the linear models. FC improves dramatically the
stability of RF and LLR. FC-Sup is always more stable than the
baseline methods. Interestingly, FC is more stable than FC-Sup. This
is the case because the grouping of the features by FC is driven only

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(a) FC RF—FC—Sup LLR—FC—Sup
' ' T - + T i ' a .
I 3 ' I a 0 ° °
0_ o_ I I I I o_ °
‘9 °° . 4. : . T °° ° : ° ° .
a- a— I 5 ' : a- , E a g
. I ' .
8- 8- I - 8- a g g 3
a— a— I] l] a— T t = i i
l:| I:I £. .£. Iii
o_ I I I I I o_ I I I I I o_ I I I I I
100 140 180 100 140 150 100 140 130
(b) FC RF—FC—Sup LLR—FC—Sup
' ' J. I T I ' s .
o_ o_ I I —.— o_ a g
a: w . 8 | I an o I u
' o I I g o
O_ O_ | l I 0.
‘° ‘° n I n ' "’ i I : i
o_ o_ . D o. . I ' .
" " __ I:I |_. ‘* ;1 CI CI
8— a— e :' Lr‘ : a— + i i +
. , . :
°' . . . . °' . T T T °' . i T .
10 50 100 200 10 50 100 200 10 50 100 200

Fig. 2. Boxplot summarizing the number of feature groups selected by FC
and FC-Sup (in combination with RF and LLR) for (a) simulation A and (b)
simulation B. The red horizontal line shows the true number of groups. On
the x-axis, the cardinality of G1 is given.

Table 2. Accuracy of classiﬁcation models on data from Simulation A

 

 

{011 100 120 140 160 180

RF 0.913:i:0.03 0.906:i:0.02 0.911:i:0.02 0.912:i:0.02 0.901 21:0.03
RF—FC 0.915:i:0.03 0.921:i:0.03 0.916:i:0.03 0.921:i:0.02 0.915:i:0.02
RF—FC—Sup 0.928:i:0.03 0.930:i:0.02 0.925:i:0.02 0.924:i:0.02 0.922:i:0.02
LLR 0.940:i:0.03 0.941:i:0.02 0.939:i:0.02 0.940:i:0.02 0.938:i:0.02

LLR—FC 0.966:l:0.01 0.966:l:0.02 0.987:l:0.02 0.970:l:0.02 0.964:l:0.02
LLR—FC—Sup 0.972:l:0.01 0.972:l:0.02 0.973:l:0.02 0.973:l:0.01 0.969:l:0.01
FSVM 0.967:l:0.02 0.966:l:0.02 0.963:l:0.02 0.965:l:0.02 0.951:l:0.02
GL 0.970:l:0.01 0.965:l:0.02 0.959:l:0.02 0.941:l:0.02 0.884:l:0.03

 

The values are averaged over 100 simulations.

Table 3. Stability of feature importance of classiﬁcation models on data
from Simulation A

 

 

|G1| 100 120 140 160 180

RF 0.56:l:O.14 O.52:l:0.17 0.55:l:O.16 0.55:l:O.16 0.55:l:O.18
RF-FC 0.99:1:001 0.99:1:001 0.99:1:001 0.99:1:001 1.00:l:0.01
RF-FC-Sup 0.88:l:O.14 0.89:l:O.14 O.86:l:0.15 0.88:l:O.15 0.90:1:O.14
LLR O.72:l:0.08 O.72:l:0.08 0.71:l:0.08 0.73:1:0.08 0.75:1:0.07

LLR-FC 1.00:l:0.00 1.00:l:0.00 1.00:l:0.00 1.00:l:0.00 1.00:l:0.00
LLR-FC-Sup O.87:l:0.21 O.86:l:0.20 0.91:l:O.17 0.91:l:O.18 0.93:1:0.14
FSVM 0.96:1:003 0.98:1:002 0.96:1:0.07 0.95:1:0.04 0.95:1:0.04
GL 0.95:1:0.02 O.94:l:0.02 O.94:l:0.01 O.94:l:0.01 0.91:l:0.02

 

The scores are averaged over 100 simulations.

by the features themselves, while in the case of FC-Sup, the outcome
also plays a role. The results show that FSVM and GL are also very
stable models.

Simulation B: Figure 3 clearly demonstrates the correlation bias
affecting RF (GI) (Fig. 3a), RF (VI) (Fig. 3b), LLR (Fig. 3c), GL
(Fig. 3d) and FSVM (Fig. 3e) models. Most dramatically, in the
case of FSVM, as the cardinality of group G1 increases to 200

 

1 990

112 /3.IO'SIBIIJI’IOprOJXO'SOIJBLUJOJIIIOIq”Idllq uroii papeo1umoq

9103 ‘0g isnﬁnv uo ::

Classification with correlated features: unreliability of feature ranking and solutions

 

 

 

(a) (b)

0.5

.03
11.11

  
 

 

 

_ 1131:2130

 

 

 

—1—1—1—1—1—1—1—1—1—1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1—
GI 63 65 G7 69 R1 R4 R7 R11 R15 R19

(c) (d)

 

 

 

 

 

 

 

 

 

 

m
d — [31:10 a _ — |G‘|=ID
a — 61:50 ° — 161:50
;_ _ mam, . — iGII:Iuo
_ 1:31:200 e — 161:200
3 - d _ R
a _
O a
S ‘ g -
61 GS GS G7 69 R1 R4 R7 R11 R15 R19 51 63 65 G7 6! R1 R4 R7 R11 R15 R19
(e) (0

 

 

 

 

 

 

      

0.00 0.05 0.10 0.15

 

 

llllllllllllllllllllllllllllll llllllllllllllllllllllllllllll
GI Gs G5 (37 Ge R1RA R7 R11 R15 R19 G1 53 55 G7 135 R1 R4 R7 R11 R15 R19

(g) (h)

— £122 :- - a2:
— 161500 "’ ' — BlPIUO
_ 1131:2110 ' - — 1131:2110

” ' m

11111111111111111111.1111”...
61 Ga (35 G7 69 R1 R4 R7 R11 R15 R19

(0

 

 

   
    
 
 

 

 

 

 

 

 

 

 

 

 

1111111111111111111111.1111...
61 Ga 65 67 Ga R1 R4 R7 R11 R15 R19

 

_ [34:10
— iGII:sn
— IGII:100
_ [34:20!)

 

 

 

 

0.0 05 ID 15 2.0
1

 

11111111111111111111.1111”...
61 Ga (35 G7 69 R1 R4 R7 R11 R15 R19

Fig. 3. Average importance of features for classiﬁcation of data from
Simulation B. The importance is averaged over groups G1,  G10,R1 , ...,R20
(a) RF (GI); (b) RF (VI); (c) LLR; (11) GL; (e) FSVM; (f) RF-FC; (g)
LLR-FC; (h) RF-FC-Sup; (i) LLR-FC-Sup.

features, the features in G1 appear almost irrelevant. When the size
of G1 exceeds 100 features, the GL model selects only group G1 and
disregards all other predictive groups. As in the case of Simulation
A, FC and FC-Sup succeed to remove the correlation bias (Fig. 3f—
i). FC in general ﬁnds the true number of groups (30), but as the
number of correlated features in G1 increases, the number of groups
is sometimes underestimated (Fig. 2b). FC-Sup often selects a larger
number of feature groups.

The accuracy of the models is given in Table 4. The models show
similar relative performance as seen in Simulation A: FC and FC-
Sup always outperform the baseline models and FC-Sup slightly
outperforms FC. LLR always outperforms RF, probably due to the
underlying linear model. The FSVM and GL lose accuracy as the
size of the group G1 increases.

Supplementary Table S1 shows the stability scores of all models.
FC and FC-Sup improve the stability of the baseline models, with
FC being more stable than FC-Sup.

4.2 Real data

Bladder tumors: by applying FC to the bladder data, we obtained
113:l:11 feature groups with grade labeling and 140:l:31 groups
with stage labeling. The varying number of groups corresponds to
the 10 training sets of the cross-validation procedure. When applied
with RF, FC-Sup method ﬁnds 107 groups (with grades labeling)
and 20 groups (with stages labeling), respectively. When applied

Table 4. Accuracy of classiﬁcation models on data from Simulation B

 

 

|G1| 10 50 100 200

RF 0.741:l:0.05 O.744:l:0.03 0.724:l:0.04 0.714:l:0.04
RF-FC 0.754:l:0.05 0.758:l:0.05 0.758:l:0.05 0.753:l:0.05
RF-FC-Sup 0.756:l:0.05 0.758:l:0.05 0.758:l:0.05 0.761:l:0.05
LLR O.777:l:0.06 O.787:l:0.05 0.783:l:0.05 0.788:l:0.05
LLR-FC 0.857:l:0.03 0.874:l:0.03 0.868:l:0.04 0.863:l:0.03
LLR-FC-Sup 0.870:l:0.03 0.890:l:0.03 0.883:l:0.03 0.880:l:0.03
FSVM 0.894:l:0.03 0.898:l:0.03 0.891:l:0.03 0.889:l:0.04
GL 0.828:l:0.05 0.739:l:0.04 0.693:l:0.05 0.690:l:0.05

 

The values are averaged over 100 simulations.

with LLR, FC-Sup partitions the set of features into smaller number
of groups: 24 (with grade labeling) and 19 (with stage labeling).

In both classiﬁcation scenarios (with grades and stages labeling),
the LLR with FC-Sup yields best accuracy. RF with FC and FC-Sup
improve the baseline model in the case of tumor grade prediction
but decrease it slightly when tumor stage is predicted.

The stability scores of the prediction models (Table 6) lead to
the same conclusions as the experiments on the simulated data:
the RF and LLR models are most unstable; however, using FC
and FC-Sup results in signiﬁcant improvements. FC-Sup is more
unstable than FC.

Supplementary Figures S4 and S5 show the feature importance
reported by all methods investigated. For comparison purposes,
to the set of prediction methods analyzed, we added a univariate
measure of feature relevance, consisting of t-test p-values (log-
transforrned) (Supplementary Fig. S4i). A t-test was applied to
each feature independently, in order to evaluate the signiﬁcance
of the difference between the means of the two classes. We do
not perform multiple testing correction because we use the log-
transforrned p-values as scores, and not as indicators of relevance.
Concerning interpretability, FC and FC-Sup with LLR and RF or
FSVM are the better models, reporting clear groups of features with
identical weights. GL is also suitable for ﬁnding relevant groups;
however, there is high variance among the weights within groups.

In the absence of a true model, it is difﬁcult to show how
correlation bias affects classiﬁcation models. However, in the case of
classiﬁcation with stage labeling, we speculate that correlation bias
is observable in the feature importance given by FSVM. A large
group of 175 correlated features on chromosome 7 is ranked ﬁfth
(w.r.t. absolute value of the weights) by FSVM (Fig. 4a). However,
a large subgroup of this group of features located toward the short
arm of the chromosome is indicated as most relevant by RF with
EC, LLR with EC, LLR with FC-Sup, as well as by univariate t-tests
(Fig. 4b—e). It is possible that the lower rank of this group of features
in the FSVM model can be caused by the correlation bias. The FSVM
includes the entire group of correlated features, at the price of lower
average weights. In order to verify this hypothesis, we constructed
a new dataset, by assigning one feature representative to each group
with identical weights in the FSVM model. The representatives are
computed by averaging over the corresponding group. All features
with null weights were excluded. This procedure essentially uses
FSVM for discovering groups of correlated features and computes
the centroid of each group selected by the FSVM model. The
resulting reduced dataset has 11 features. We trained and evaluated

 

1991

112 /3.IO'SIBIIJI’IOprOJXO'SOIJBLUJOJIIIOIq”Idllq uroii papeo1umoq

9103 ‘0g isnﬁnv uo ::

L.Tolosi and T.Lengauer

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(a)
V-
8
0
g_ I I I I III III I |||||||||||
123457891113161922
(b) _
°._
N

o lllllll lllllllllllllll
12345 7 89 11 13161922
(0 _
“I-
“I:
0—
°._ I'l— I
0 lllllll III‘I—IIIIIIIIIII
12345 7 89 11 13161922
(d)
“I-
60.:
O
a:
o—
<:__ l_
0 lllllll lllllllllllllll
12345 7 89 11 13161922
(e)
9w:
‘3‘. v-
D. _
.9“.—
_8' _
lo-
123457891113161922
(f)
E» :
$§_
2°.— _, l-l
Eg“ -" I . 1_I._‘,-’
(so-- I_l
° I I I I I III
2 6 7 9101213 21
(g) D
Em.-
mo
3v:— 1—}
a, 1—
§Irz_ -| 1-1
0° 1__'
B l_'
mg— I I I I I I II‘Tb
2 6 7 9101213 21

Fig. 4. Feature relevance (in absolute value) by different classiﬁcation
models on the bladder dataset with stage labeling. The features are sorted
according to genomic position and the chromosomes are shown along the
x-axis. (a) FSVM; (b) RF-FC; (c) LLR-FC; (d) LLR-FC-Sup; (e) t-test; (f)
FSVM (only relevant groups); (g) reduced FSVM.

an FSVM model on the new dataset, this time without ﬁised penalty
[/i=0 in Equation (3)], since most probably there are no ﬁirther
groups to be discovered. We call the new model reduced FSVM. In
Figure 4f, the weights of the features in the original FSVM model
are represented (in absolute value and only the regions with non-
zero weights). Figure 4g shows the weights of the reduced FSVM
(in absolute values), extended for convenience so as to be aligned to
the original weights. The representative feature corresponding to the
group located on chromosome 7 receives highest absolute weight,
which could indicate that the correlation bias has been removed.

Breast tumors: FC method identiﬁes 130:l:37 groups of features
with ER labeling and 127 :l: 32 groups of features, with PR labeling.
FC-Sup in combination with RF selects an optimal partitioning into

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(a)

In._

@—

0.:

N—

“2:

a I IIIIII III I IIIIIIIIIII
123457891113161922
(b)

Irz_

In:

0
||I||I| III I |IIII||||||
123457891113161922

0(0

01 _

°_

m-

3:

8- ||

6— I I IIII III I IIIIIIIIIII
123457891113161922

NI_(d)

0.: 

g:_‘— I I'l—l—l_l_l_l—l—l'l_l_l'l

 

 

I II
123457891113161922

Fig. 5. Feature relevance (in absolute value) by different classiﬁcation
models on the breast dataset with PR labeling. The features are sorted
according to genomic position and the chromosomes are shown along the
x-axis. (a) RF-FC; (b) RF-FC-Sup; (c) LLR-FC; (d) LLR-FC-Sup.

195 groups (with ER labeling) and 163 groups (with PR labeling),
respectively. Table 7 summarizes the accuracy of the different
algorithms on ER and PR classiﬁcation. FC improves the accuracy
of the RF in both cases and of LLR in the case of PR labeling, but
decreases slightly the accuracy of the LLR model when ER status is
predicted. In the case of ER classiﬁcation, all RF and LLR models
outperform FSVM and GL, however, by a small margin. In the case
of the PR classiﬁcation, FSVM performs best.

The models investigated have similar stability scores as in the
case of bladder tumors: RF and LLR are most unstable, FC with
LLR and RF have increased stability, comparable to that of GL and
FSVM models and FC-Sup is less stable than FC (Supplementary
Table S2).

The feature importance reported by the various methods
investigated in general conﬁrms the ﬁndings reported in the original
study (Climent et al., 2007) (Supplementary Figs S6 and S7).
An interesting aspect is shown in Figure 5: FC and FC-Sup in
combination with RF and LLR select a group of features in
chromosome 13 as highly relevant for classiﬁcation of PR status.
In the original study (Climent et al., 2007), none of these features
were reported signiﬁcant, based on univariate association with the
outcome (corrected t-test p-value). Allelic loss at chromosome 13
is known to be associated with poor prognosis in breast cancer,
due to the loss of the tumor suppressor gene BRCA2, located in
this region. Associations with low progesterone content have been
reported previously in the literature (Eiriksdottir, 1998), which we
conﬁrm in our study.

5 DISCUSSION

We have shown that several widely used classiﬁcation algorithms
can generate misleading feature rankings when the training datasets

 

1 992

112 /3.io's112111110[p.101x0'soiwu1101uioiq”:duq uroii papeo1umoq

9103 ‘0g isnﬁnv uo ::

Classification with correlated features: unreliability of feature ranking and solutions

 

Table 5. Performance of different classiﬁers on the bladder data

 

 

 

Grades Stages

Acc AUC Acc AUC
RF O.792:l:0.02 0.827 0.833:l:0.03 0.882
RF-FC 0.833:l:0.01 0.878 0.810:I:0.02 0.882
RF-FC-Sup 0.833:l:0.01 0.885 0.810:I:0.03 0.884
LLR 0.823:l:0.01 0.800 0.798:I:0.01 0.821
LLR-FC O.854:l:0.02 0.838 0.774:l:0.01 0.757
LLR-FC-Sup 0.865:l:0.01 0.771 O.845:I:0.02 0.873
FSVM O.813:l:0.02 0.642 0.810:I:0.05 0.780
GL 0.833:l:0.02 0.775 0.833:l:0.04 0.780

 

Table 6. Stability of feature importance of classiﬁcation models on the
bladder data

 

 

Grades Stages
RF 0.55:1:0.03 0.60:I:0.03
RF-FC O.80:l:0.04 0.83 :I:0.04
RF-FC-Sup O.78:l:0.06 O.69:l:0.12
LLR O.61:l:0.12 0.66:I:O.11
LLR-FC O.86:l:0.04 0.87:I:0.08
LLR-FC-Sup O.72:l:0.11 O.66:l:0.18
FSVM O.75:l:0.16 0.88:1:0.05
GL O.72:l:0.13 0.87:I:0.09

 

Table 7. Performance of different classiﬁers on the breast data

 

 

 

ER status PR status

Acc AUC Acc AUC
RF 0.658:I:0.01 0.664 0.677:I:0.02 0.673
RF-FC O.670:I:0.06 0.635 0.682:I:0.08 0.660
RF-FC-Sup 0.665:I:0.02 0.663 0.671:I:0.02 0.667
LLR 0.683:I:0.03 0.692 0.683:I:0.03 0.733
LLR-FC O.671:l:0.02 0.718 O.689:I:0.04 0.723
LLR-FC-Sup O.696:l:0.02 0.676 0.714:l:0.01 0.691
FSVM 0.658:I:0.02 0.660 0.745:I:0.02 0.800
GL 0.658:I:0.01 0.692 0.702:I:0.02 0.698

 

contain large groups of correlated features. This can confound
model interpretation, since large groups of predictive features can
be masked and falsely appear irrelevant. Such an effect is likely to
occur because variables relating to a biological process or genomic
location of high interest (w.r.t. a phenotype) are overrepresented
in the probes set of microarray-based experiments. In this article,
we have described the correlation bias and have shown that it
affects random forest, Lasso logistic regression, group Lasso and
ﬁised SVM models. We used two artiﬁcial datasets based on linear
models to show that the expected importance of the features in
a correlated group decreases as the size of the group increases.
We also illustrated the correlation bias caused by the combination
of ﬁised and Lasso penalties by means of a theoretical example,

which considers the particular case of two groups of correlated
features. We showed that correlation bias can be reduced using a
group-selection algorithm which combines feature clustering with
any classiﬁcation method. We tested two methods for estimating the
number of clusters, based on a unsupervised (FC) and supervised
approach (FC-Sup), respectively.

We showed using simulated data experiments that FC and FC-
Sup successfully remove the correlation bias, improve the stability
of feature importance and increase the accuracy of the baseline
methods. FC-Sup outperforms FC in terms of accuracy, but FC is
faster and has higher stability. The classiﬁcation of the real data
shows that FC dramatically increases the model interpretability
and stability of feature importance. Moreover, in ﬁve out of eight
classiﬁcation tasks, FC improved the accuracy of the baseline
models. FC-Sup improves the accuracy of the baseline models in
six out of eight classiﬁcation tasks. FC-Sup used in combination
with Lasso logistic regression yields highest accuracy in three out
of four cases.

Using hierarchical clustering of the features and then computing
cluster centroids using the average is certainly not the only solution
for identifying and summarizing groups of correlated features.
Depending on the distribution of the features in the sample space,
methods using principal component as cluster centroid [as in Huang
et al. (2003a)] or even several representatives [as in Jager et al.
(2003)] may yield better performance.

ACKNOWLEDGEMENT

We would like to thank J6rg Rahnenfiihrer, Adrian Alexa, Hiroto
Saigo and Konstantin Halachev for their helpful discussions.

Funding: German National Genome Research Network (NGFNplus)
(01GS08100 to L.T.).

Conﬂict of Interest: none declared.

REFERENCES

Blaveri,E. et al. (2005) Bladder cancer stage and outcome deﬁned by array based
comparative genomic hybridization. Clin. Cancer Res, 11 (19 Part 1), 701277022.

Breiman,L. (2001) Random forests. Mac/1. Learn, 4S , 5732.

C1iment,J. et al. (2007) Deletion of chromosome llq predicts response to anthracycline-
based chemotherapy in early breast cancer. Cancer Res, 67, 8187826.

Dettling,M. and Biihlmann,P. (2004) Finding predictive gene groups from microarray
data. J. Multivar Anal, 90, 103131.

Diaz-Uriarte,R. and Alvarez de Andrés,S. (2006) Gene selection and classiﬁcation of
microarray data using random forest. BMC Bioinformatics, 7, 3.

Eiriksdottir,G (1998) Mapping loss of heterozygozity at chromosome 13q: loss at
13q12-q13 is associated with breast tumor progression and poor prognosis. Eur
J. Cancer, 34, 207672081.

Fan,J. and Lv,J. (2008) Sure independence screening for ultrahigh dimensional feature
space. J. R. Stat. Soc. Ser B Stat. Methodol, 70, 8497911.

Friedman,J. et al. (2010) Regularization paths for generalized linear models via
coordinate descent. J. Stat. Softwr, 33, 1722.

Grant,M. and Boyd,S. (2008) Graph implementations for nonsmooth convex programs.
Recent Advances in Learning and Control, Vol. 371, Springer, pp. 957110.

Hastie,M. (2001) The Elements of Statistical Learning. Springer, NY.

Hicks,J. (2006) Novel patterns of genome rearrangement and their association with
survival in breast cancer. Genome Res, 16, 146571479.

Huang,E. et al. (2003a) Gene expression predictors of breast cancer outcomes. Lancet,
361, 159071596.

Huang,E. et al. (2003b) Gene expression phenotypic models that predict the activity of
oncogenic pathways. Nat. Genet, 34, 223230.

J'ager,J. et al. (2003) Improved gene selection for classiﬁcation of microarrays. Pac.
Sympos Biocomput., 8, 53764.

 

1 993

112 /3.io's112111110[p.101x0'soiwu1101uioiq”:duq 111011 papeo1umoq

9103 ‘0g isnﬁnv uo ::

L.Tolosi and T.Lengauer

 

Kalousis,A. et al. (2005) Stability of feature selection algorithms. In ICDM ‘05
Proceedings, pp. 2187225.

Liaw,A. and Wiener,M. (2002) Classiﬁcation and regression by randomForest. R News,
2, 18722.

Ma,S. et al. (2007) Supervised group Lasso with applications to microarray data
analysis. BMC Bioinformatics, 8, 60.

Meier,L. et al. (2008) The group lasso for logistic regression. J. R. Stat. Soc. B, 70,
53771.

Mikeska,T. et al. (2007) Optimization of quantitative MGMT promoter methylation
analysis using pyrosequencing and combined bisulﬁte restriction analysis. J. Mol.
Diagn., 9, 3687381.

Pang,H.et al. (2008) Building pathway clusters from Random Forests classiﬁcation
using class votes. BM C Bioinformatics, 9, 87.

Park,M.Y. et al. (2007) Averaged gene expression for regression. Biostatistics, 8,
2127227.

Rapaport,F. et al. (2008) Classiﬁcation of arrayCGH data using fused SVM.
Bioinformatics, 24, 137571382.

Rousseeuw,P. (1987) Silhouettes: a graphical aid to the interpretation and validation of
cluster analysis. J. Comput. Appl. Mat/1., 20, 53$5.

Strobl,C. et al. (2008) Conditional variable importance for random forests. BMC
Bioinformatics, 9, 307.

Tibshirani,R. (1996) Regression shrinkage and selection via the Lasso. J. R. Stat. Soc.
B, 58, 2677288.

van’t Veer,L.J. (2001) Gene expression proﬁling predicts clinical outcome of breast
cancer. Nature, 415, 5307536.

Yu,L. et al. (2008) Stable feature selection via dense feature groups. In Proceedings of
the 14th ACM KDD‘08.

Zou,H. and Li,R. (2008) One-step sparse estimates in nonconcave penalized likelihood
models. Ann. Stat., 36, 150971533.

 

1 994

112 /3.io's112111110[p.101x0'soiwu1101uioiq”:duq 111011 papeo1umoq

9103 ‘0g isnﬁnv uo ::

