BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Comparisons with real data

 

the articles they have to review. A bug in one of the competitive
algorithms may perhaps be discovered if the referee spends a few
hours or days checking the authors’ code. This requires that all
codes are made available (which is currently not always the case;
see Hothorn and Leisch (2011) for a recent study on this topic)
and that the referee has much time to spend on this unpaid re—
viewing task (this is also unlikely). As far as other aspects such as
optimization of the method’s characteristics are concerned, there
is in our understanding no way even for the most careful referee
to identify them with certitude. Based on all these facts, we
stressed the importance of neutral comparison studies (i.e. com—
parisons that are not part of an article suggesting a new algo—
rithm) in a previous publication (Boulesteix et al., 2013a).

The second pitfall is that applications intended as representa—
tive comparisons are in practice often underpowered, as docu—
mented in Boulesteix et al. (2013b) in the special case of
supervised learning. By underpowered, we mean that the per—
formance (or the difference between the performances of two
algorithms) is so variable across datasets that more datasets
would be needed to draw conclusions from a statistical testing
perspective. In a literature survey with focus on supervised clas—
sification, Boulesteix et al. (2013a) found that the median
number of datasets considered in comparison studies included
in articles on new algorithms was only five. In the statistical
framework of Boulesteix et al. (2013b), assuming a signiﬁcance
level of 0.05, a power of 0.8 and a ‘typical value’ of 7% for the
standard deviation of the performance difference over the data—
sets, this number of datasets would only allow to detect as sig—
niﬁcant a difference in error rates of >9.5%7a big difference!
Thus, simply conducting any comparison study is not sufﬁcient
to provide guidance: the comparison study also has to have
enough power; a condition that is almost never fulﬁlled in
practice.

The third pitfall is that it is difﬁcult to draw datasets at
random within a deﬁned ﬁeld of interest. This problem is char—
acterized by lack of literature and deﬁnition problems. It should
be addressed in future research. Most importantly, it may be
related to the overoptimistic bias mentioned earlieriboth be—
cause authors might tend to underreport the results obtained
with datasets that are not favorable to their new algorithm
(Yousef1 et al., 2010) and because they might choose datasets
that are somehow interrelated and yield a distorted picture of
the performance of the new algorithm in the whole field of
interest.

Ideally, representative comparisons that aim to yield general
conclusions for an application ﬁeld should roughly follow the
rules considered as standard in many substantive fields, for ex—
ample in clinical research, whereby in our metaphor datasets play
the role of patients, methods play the role of treatments and
applications fields play the role of populations. This implies,
among others, that one (i) considers power issues while designing
a comparison study, especially while determining the number of
datasets (Boulesteix et al., 2013b); (ii) the selection of the datasets
follows systematic and well—documented criteria in the vein of
inclusion criteria used in trials; (iii) subgroup analyses are
planned previously and interpreted cautiously; (iv) the main out—
come (e.g. the error rate in the case of supervised learning) is
clearly defined; (v) the datasets are selected at random or at least
potential selection biases are discussed; and (vi) dropout

(datasets that are disregarded in the course of the study) and
its reasons are well—documented.

Note that these recommendations would address the second
pitfall but only partially the ﬁrst one. Following our metaphor,
the first pitfall in clinical research would be that inventors of a
new treatment tend to be overoptimistic regarding its efﬁciency
for many reasons, a fact that is widely recognized in medical
research. In our context, the six recommendations outlined ear—
lier should be completed by a seventh one to better address the
ﬁrst pitfall, say, ‘deﬁne the method at the beginning of the study
and do not adapt it depending of its results on the considered
datasets’, to avoid overf1tting of the new algorithm on the con—
sidered datasets (Jelizarow et al., 2010; Rocke et al., 2009).

Considering the three pitfalls discussed above, it is clear that
designing a representative comparison study is an extremely dif—
ﬁcult and time—intensive task that can and should probably not
be performed in all articles presenting new methods. Indeed,
many real data applications presented in articles on new methods
are meant as examples. This is what we denote as illustrative
comparisons. They may demonstrate the use of the new
method and point to speciﬁc aspects, such as software implemen—
tation (possibly including exemplary code as additional f11e), pre—
liminary data preparation, parameter/variant choice or
computation time. They might also show in which form the re—
sults are obtained and how to interpret them.

The main concern related to illustrative comparisons is that
their results are sometimes (wrongly) interpreted in the literature
and discussed as if they were representative. Typically, conclu—
sions are drawn on the superiority of a method (most often the
new method; see Boulesteix et al. (2013a) for an empirical study
on this topic) based on a too small number of datasets that ac—
tually do not allow to draw such conclusions from a testing
perspective. Coming back to our metaphor, it would be as if a
team of medical scientists established the superiority of a new
treatment based on only 11: 2 or n: 3 patients. It just does not
make sense. Similarly, it does not make sense to make conclu—
sions such as ‘method A performs better than method B for
datasets with the characteristics XYZ’ if the study is based on,
say, one dataset with this characteristic and one without. It
would be as if a medical team said that treatment A is more
efﬁcient than treatment B for male patients just because it was
the case for the considered male patient but not for the con—
sidered female patient.

Note that the term ‘illustrative’ does not necessarily imply that
the comparison criteria are qualitative. But it implies that these
comparisons and criteria are interpreted as examples, and not as
representative of the considered ﬁeld. In particular, differences in
performance should not be interpreted in terms of guidance for
the choice of the algorithmieven if the results of illustrative
comparisons may provide information on the order of magni—
tude of the relative performance of the considered algorithms
and tell us whether, roughly speaking, the ‘new algorithms be—
have as expected in real data settings’.

The main difference between the two types of comparisons is
the way in which datasets are selected that essentially affects their
interpretation. In representative comparisons, selection is ideally
performed at random within the deﬁned field, and their number
is chosen by taking power issues into consideration, whereas in
illustrative comparisons datasets are selected because they are

 

2665

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

A.-L.Boulesteix

 

interesting to better present the new method. For example, in an
illustrative application it is acceptable to select two extreme data—
sets, say (in the case of supervised learning), a dataset where the
response is easy to predict and a more difﬁcult dataset.

Note that other important aspects of the new algorithmsi
such as ease of use, speed, conceptual simplicity or generalizabil—
itywan be adequately addressed in an article even in the ab—
sence of representative comparison study. However, it is
important to note that (i) these aspects are of no consequence
if the performance turns out to be bad, and (ii) some of them
(such as ease of use and speed) also depend on the considered
dataset. The above considerations on the selection of example
datasets may thus also be relevant to aspects of the algorithms
other than performance, even if our classiﬁcation into illustrative
and representative studies primarily focuses on performance.

3 CONCLUSION

In conclusion, we agree that comparisons of new algorithms to
existing algorithms are important. As Smith et a]. (2013), we
make a plea for more comparison studies on real datasets in
bioinformatics literature. Suggesting an algorithm without even
running it on a real dataset is just unacceptable. Going back to
our parallel between medical and computational sciences, it
would be as if a physician described a new therapy without
even showing that it was successful on a few patients. The appli—
cation of the new algorithm to, say, at least two distinct example
datasets with different characteristics should be a minimum non—
negotiable requirement for publication. Generally, we also think
that in research articles applying bioinformatics algorithms to
obtain substantive results, the results should not be based on a
new novel algorithm that is scarcely described, applied only to
the dataset of interest and not compared with any other method.

We also believe that both approaches discussed in this letteri
illustrative and representativeimay make sense, but that they
are completely different and should be reported differently. Each
approach implies a different way to report the results and a dif—
ferent way to design the experiment. Reporting and design
should be consistent. In practice, the most frequent violation of
this principle is when algorithm developers ‘feel’ that their new
method might be better than existing methods based on an
underpowered comparison study and report their results as if
the comparison was representative. The design of a representa—
tive comparison study is so difﬁcult and time consuming and the
risk of bias in favor of the new method is in practice so important
that most applications presented in articles on new methods
should probably be seen as illustrative applications. In our
letter, we discussed a few conditions that a good representative
application should in our view fulﬁll to be really representative.
However, we believe that much more effort is needed to deﬁne
precise criteria and guidelines. In particular, the neutrality issue
addressed in Boulesteix et al. (2013a) should be carefully taken
into account.

Referring to the sentence ‘these evaluations ought to be pri—
marily provided in the novel algorithm publications themselves’
(Smith et al., 2013), we again point out that representative com—
parisons (as deﬁned in our letter) can hardly be performed within

an article on a new method, especially because of the well—known
optimistic bias in favor of the new method (see also the editorial
by Rocke et al., 2009). Addressing this general issue related to
scientiﬁc methodology and publication practice will probably
need a long time and much coordinated effort from all par—
tiesiresearchers, editors and reviewers. This problem is also
connected with publication bias and publication of negative re—
search ﬁndings (Boulesteix, 2010).

In the meantime, we believe that authors and readers should
interpret illustrative comparisons as suchiwithout implicitly
assuming that they give information about what would happen
on other datasets, and that journals might give more attention to
‘neutral’ comparison studies entirely devoted to the comparison
task itself. This would give a motivation to potential authors of
such comparison studies: if they know that their work will not
only provide useful information to other scientists but also have
good chance to be published in a high—ranking journal, they will
be more likely to conduct such a study. Motivating potential
authors of comparison studies by publishing more of these stu—
dies is certainly easier than motivating overloaded reviewers to
spend hours investigating the possible bias of a comparison study
included in an article on a new method.

To conclude, in this letter we tried to clarify some aspects left
unaddressed by Smith et a]. (2013). The exact definition of re—
quirements for the publication of new algorithms, however,
cannot be formulated within this modest framework. Such guide—
lines should be the result of coordinated efforts of consortia
involving a large number of scientists, similarly to the teams
working on reporting guidelines in medical sciences such as the
EQUATOR network (Altman et al., 2008).

ACKNOWLEDGMENT

I thank Manuel Eugster for helpful comments.

Conﬂict of Interest: none declared.

REFERENCES

Altman,D.G. et a]. (2008) EQUATOR: reporting guidelines for health research.
Lancet, 371, 114%1150.

Boulesteix,A.L. (2010) Over—optimism in bioinformatics research. Bioiiy’ormuticx,
26, 4374139.

Boulesteix,A.L. et al. (2013a) A plea for neutral comparison studies in computa—
tional sciences. PMS One, 8, e61562.

Boulesteix,A.L. et al. (2013b) A statistical framework for hypothesis testing in real
data comparison studies. Technical Report, Department of Statistics (LMU),
No. 136, http://epub.ub.uni—muenchen.de/14324/ (5 September 2011, date last
accessed).

Hand,D.J. (2006) Classiﬁer technology and the illusion of progress. Stat. Sci., 21,
1714.

Hothorn,T. and Leisch,F. (2011) Case studies in reproducibility. Brief. Bioinform.,
12, 2887300.

Jelizarow,M. et a]. (2010) Over—optimism in bioinformatics: an illustration.
Bioinfornmticx‘, 26, 199071998.

Rocke,D.M. et a]. (2009) Papers on normalization, variable selection, classiﬁcation
or clustering of microarray data. Bioinformuticx, 25, 7017702.

Smith,R. et a]. (2013) Novel algorithms and the beneﬁts of comparative validation.
Bioinfornmticx‘, 29, 158371585.

Youseﬁ,M.R. et a]. (2010) Reporting bias when using real data sets to analyze
classiﬁcation performance. Bioinformuticx, 26, 68776.

 

2666

ﬁm'spzumofpmjxo'sopeuuopnorq/ﬁdnq

