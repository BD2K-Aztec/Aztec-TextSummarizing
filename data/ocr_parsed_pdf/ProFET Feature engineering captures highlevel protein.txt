Bioinformatics, 31 (21), 2015, 3429—3436

doi: 10.1093/bioinformatics/btv345

Advance Access Publication Date: 30 June 2015
Original Paper

 

 

Sequence analysis

ProFET: Feature engineering captures high-level
protein functions
Dan Ofer and Michal Linial*

Department of Biological Chemistry, Institute of Life Sciences, The Edmond J. Safra Campus, The Hebrew
University of Jerusalem, Givat Ram, 91904, Israel

*To whom correspondence should be addressed.
Associate Editor: John Hancock

Received on March 4, 2015; revised on May 3, 2015; accepted on May 29, 2015

Abstract

Motivation: The amount of sequenced genomes and proteins is growing at an unprecedented
pace. Unfortunately, manual curation and functional knowledge lag behind. Homologous inference
often fails at labeling proteins with diverse functions and broad classes. Thus, identifying high—level
protein functionality remains challenging. We hypothesize that a universal feature engineering ap—
proach can yield classification of high—level functions and unified properties when combined with
machine learning approaches, without requiring external databases or alignment.

Results: In this study, we present a novel bioinformatics toolkit called ProFET (Protein Feature
Engineering Toolkit). ProFET extracts hundreds of features covering the elementary biophysical
and sequence derived attributes. Most features capture statistically informative patterns. In add—
ition, different representations of sequences and the amino acids alphabet provide a compact,
compressed set of features. The results from ProFET were incorporated in data analysis pipelines,
implemented in python and adapted for multi—genome scale analysis. ProFET was applied on 17 es—
tablished and novel protein benchmark datasets involving classification for a variety of binary and
multi—class tasks. The results show state of the art performance. The extracted features’ show
excellent biological interpretability. The success of ProFET applies to a wide range of high—level
functions such as subcellular localization, structural classes and proteins with unique functional
properties (e.g. neuropeptide precursors, thermophilic and nucleic acid binding). ProFET allows
easy, universal discovery of new target proteins, as well as understanding the features underlying
different high—level protein functions.

Availability and implementation: ProFET source code and the datasets used are freely available at
https://github.com/ddofer/ProFET.

Contact: michall@cc.huji.ac.il

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1lntr°ducu°n detection (Edgar and Sjolander, 2004; Karplus et (11., 1998).

9103 ‘Org rsnﬁnv 110 salaﬁuv soc] ‘BTIIJOJTIBD 30 AJtSJQAtuf] 112 /310'S[BIIJHO[pJOJXO"SOTJBLUJOJIITOTCIHIdllq 11101} popcolumoq

The most used approaches in protein classification rely on distance
measures between sequences based on various alignment methods
(e.g. Smith—Waterman, BLAST). With the growth in the amounts
and diversity of protein sequences, more sophisticated methods have
been introduced (e.g. PSSM, Profile—Profile, HMM—HMM)
(Jaakkola et (11., 2000; Soding, 2005). These methods are based on
multiple sequence alignments for improving remote homologs

Incorporating 3D—structure as a seed for the statistical models fur—
ther improved the quality of protein domains and families (e.g.
Pfam) (Finn et (11., 2014; Sonnhammer et (11., 1997). Currently, there
are ~27 000 such models (InterPro, Mulder and preiler, 2007)
that cover 83% of all sequences in UniProtKB (2014_10). Function
assignment is gained from mapping InterPro models to Gene
Ontologies (i.e. InterPr02GO). An alternative model—free approach

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3429

3430

D. Ofer and M.Linial

 

was proposed (Portugaly et (11., 2002). The assessment of large—scale
automatic protein functional annotations (Radivojac et (11., 2013;
Rost et (11., 2003) and the contribution of alternative approaches to—
ward this task have been extensively discussed (e.g. Valencia, 2005).

Despite the strength of the model—based methods, in many in—
stances the local sequence—based methods fail to reliably assign a
function (Rentzsch and Orengo, 2009). This is best demonstrated by
the limitation in classifying proteins by their 3D—folds (Greene et (11.,
2007; Todd et (11., 2005). Notably, the classification of some biolo—
gical niches is especially suited for feature representation. For ex—
ample, routine annotation tools fail to confidently assign function
for bioactive peptides and short proteins (Naamati et (11., 2009). A
number of previous studies focus on feature extraction from whole
protein sequences (Cao et (11., 2013; Ding and Dubchak, 2001;
Dubchak et (11., 1995; Nanni et (11., 2014; van den Berg et (11., 2014)
as a starting input for machine learning (ML) approaches. Structural
benchmark from SCOP and CATH (Lewis et (11., 2013) are fre—
quently used to assess the predictive ML methods. Specialized pre—
dictors have been presented for structural tasks including secondary
structure, solvent accessibility, stability, disordered regions, domains
and more (Cai et (11., 2001; Cheng et (11., 2005; Ding and Dubchak,
2001). ML approaches have proven suitable to classify protein prop—
erties beyond their 3D—structure. SVMProt was tested on preselected
50 functional families from Pfam (Chou and Cai, 2003). Naive bio—
physical features classification outperformed simple sequence—based
methods for a number of protein families (Varshavsky et (11., 2007).
However, the most likely advantage of the feature and pattern—based
ML approach is toward high—level functionality (e.g. Pe’er et (11.,
2004). Examples for such predictions include protein—protein inter—
actions (Bock and Gough, 2001; Cheng and Baldi, 2007), discrimi—
nating outer membrane proteins (Gromiha and Suwa, 2005),
membrane topology (Nugent and Jones, 2009), subcellular localiza—
tion (Hua and Sun, 2001) and more. The strongest features learned
by the ML classifiers often expose biologically important motifs
(Leslie et (11., 2004).

In this study, we focus on the ability of elementary biophysical
features together with a rich set of engineered representation of pro—
teins to classify high—level protein functions. These features are
suited for both supervised and unsupervised classification. Our goal
is to illustrate the importance of ProFET (Protein Feature
Engineering Toolkit) as a ‘one size fits all’ framework for represent—
ing whole protein sequence. We present a universal, modular work—
ﬂow for protein function classification: (i) feature generation and
extraction from primary sequences (ProFET). (ii) Application of the
extracted features in a ML framework for binary or multi—class par—
tition. (iii) Presentation of discriminative classification power. (iv)
Identification of patterns and features that underlie the successful
classification (‘Feature Selection’).

2 Methods

2.1 Protein databases and datasets

In gathering the protein sets in this study, we used datasets made
available by (i) custom sets gathered from public databases such as
UniProtKB (Wu et (11., 2006) and SCOPe (Fox et (11., 2014) and (ii)
benchmarks extracted from publications. For both resources, we
applied CD—Hit and USearch (Edgar, 2010) to remove redundant se—
quences according to a predefined % of sequence identity. As a rule,
we used only classes that contain a minimal number of samples per
group (typically 40, after redundancy removal). Sequences with un—
known amino acid (AA), errors or sequences that are shorter than

30 AA were removed. We included in the analysis the most recent
SCOP classification (2.05, 71015 PDB entries pre—filtering) as some
literature—based benchmarks from SCOP were outdated (Chandonia
et (11., 2004).

2.1.1 Specialized protein functions

' Neuropeptide precursors (NPPs): The keyword ‘neuropeptide’ is
acquired from SwissProt (SWP) and UniRef90 representatives.
We removed proteins that contain the terms ‘fragment’ and
‘receptor’.

' Ribosomal proteins: Acquired from SWP and partitioned to
Archea, Bacteria and Eukarya. Redundancy ﬁlter was set to
20—40% identity (according to the set size).

' Thermophilic proteins: The ThermoPred benchmark dataset (Lin
and Chen, 2011) was used, with a further redundancy removal
(at 40% identity threshold).

2.1.2 Cellular localizations

' LocTree3 benchmark (Yachdav et (11., 2014) for Eukarya and
Bacteria were used. Filtered at 40% identity within each class.

' Mammalian subcellular localization: Protein—organelle pairs are
acquired from SWP.

' Uncultured bacterium. Sequences extracted from UniProtKB and
mapped to keyword annotations for major cellular compart—
ments (membrane, cytoplasm, ribosome). Filtered at 50% iden—
tity according to UniRef clusters.

2.1.3 Structural-based classifications

' SCOPe (Release 2.05, February 2015) (Fox et (11., 2014). Classes
and folds were deﬁned by SCOP, with 25% or 10% sequence
identity ﬁlter (8514 and 6721 sequences, respectively).

' SCOPe (Release 2.05, February 2015) ‘selected class’ deﬁned by
the SCOP class (marked a—k), with classes c,d removed. We also
apply as a benchmark classes ‘a,b,f,g’ at 25 % sequence identity
ﬁlter. Classes a,b,c,f,g were tested following redundancy removal
at extremely low identity level (10%). The classes that were not
included had small number of folds in each.

2.1.4 Nucleic acids binding proteins

' DNA—binding proteins. Benchmark dataset from DNA binder
(Kumar et (11., 2009).

' RNA—binding proteins. Benchmark dataset from BindN (Wang
et (11., 2010).

2.1.5 Viral properties and classes

' Virus—host pairs: Acquired from SWP. The set include all viral
proteins partitioned by the kingdom of the hosts. Redundancy ﬁl—
tration (at 40% identity) was performed on the viral proteins but
not on the hosts.

' Capsids: Compilation of two sets of all viral capsid proteins
annotated by SWP: (i) Classes according to host type. (ii) Classes
according to viral replication mode.

The datasets and sequences used are all freely provided online:

https://github.com/ddofer/ProFET.

9103 ‘Org rsnﬁnv uo salaﬁuv 50'] ‘BIIIJOJIIBD JO ArtsmAtuf} 112 /310'S[BIIJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”Idllq 11101} popcolumoq

Feature engineering workflow

3431

 

2.2 Features

All features extracted by ProFET are directly derived from the pro—
tein sequence and do not require external input (Saeys et (11., 2007).
The software packages required for ProFET are part of the scientific
Python distribution. Properties relying on external predictors (e.g.
the 3D structural fold, secondary structure) are not included by de—
fault. However, users can trivially add additional features via the
‘FeatureGen’ script. ProFET can also generate a pre—defined set of
default features for consistency in evaluation and ease of use, call—
able from the command—line.

The features that are described below can be restricted to a seg—
ment of a protein (e.g. each individual third of a sequence). We sup—
port two versions for a subsequence analysis: (i) relative portions
and (ii) fixed lengths. The activation of global feature extraction
combined with segmental consideration is advantageous. It is moti—
vated by the atypical composition of different segments of numerous
protein classes, e.g. the signal peptides, ﬂexible N—terminal linker re—
gions, C—terminal portions of membranous kinases and GPCR recep—
tors, disordered regions and more.

The categories of features currently implemented in ProFET are
as follows.

2.2.1 Biophysical quantitative properties

i. Molecular weight (in Da)
ii. Sequence length (in AA)
iii. pH(I), the isoelectric point
iv. Net Charge at various pH(I)s.
v. Aromaticity the relative frequency of Phe, Trp, Tyr.
vi. Instability index, an estimate for the stability of a protein in
vitro (Gasteiger et (11., 2003).

vii. GRAVY (Grand Average of Hydropathy), the sum of hydrop—
athy values of all AA, divided by the number of AA in the ana—
lyzed sequence (Kyte and Doolittle, 1982).

viii. Aliphatic index, the relative volume occupied by aliphatic side
chains (Ala, Val, Ile and Leu) (Gasteiger et (11., 2003).

Most of these properties were based on the Expasy proteomics collec—

tion (Gasteiger et (11., 2003). The important of these elementary global

features has been previously validated (Varshavsky et (11., 2007).

2.2.2 Letter-based features

i. AA composition (single or di—peptide)

ii. Overlapping K—mers.

iii. ‘Mirror’ K—mers. It accounts for K—mers of various combin—
ations of ‘grouped’ AA. For example, lysine—arginine appear—
ance (KR) is grouped together with RK.

iv. Reduced AA alphabets. Grouping of AA secures a compact rep—
resentation. We include a large number of such alphabets from
various sources (Murphy et (11., 2000; Peterson et (11., 2009) and
some novel alphabet representations of size 14 and 8 (Ofer_14
and Ofer_8, respectively). For the 14 AA representation, the
grouping is for KR, TS and LIVM. For the 8 AA representation,
the grouping is for FYW, ALIVM, RKH, DE and STNQ. The
other AA remain in the uncompressed representation.

2.2.3 Local potential features

i. Potential post—translational modiﬁcation (PTM) sites. We
included motifs implemented as regular expressions, including
those for ‘known short motif’ dibasic cleavage model (X—X—Lys—
[Lys or Arg], X—X—Arg—Arg, Arg—X—X—[Lys or Argl; Where X

denotes any AA (Southey et (11., 2006; Veenstra, 2000). Others
include N—glycosylation and Asp or Asn hydroxylation sites. We
included Cysteine spacer motif that captures the tendency of Cys
to appear in a minimal window (Naamati et (11., 2009).
Additional PTM motifs collected from ELM (Dinkel et (11.,
2012) were not implemented.

ii. Potential Disorder (FoldIndex). Local regions of disorder are
predicted using the naive FoldIndex (Prilusky et (11., 2005) and
TDP—IDP methods (Campen et (11., 2008; Klus et (11., 2014).
FoldIndex predicts the disorder as a function of the hydrophobic
potential and net charge.

2.2.4 Information-based statistics

These features aim to capture the non—randomly distribution of each

AA in the sequence, based on the concept of information entropy.
The information—based features used are:

i. Total entropy per letter, as a whole
ii. The binary autocorrelation
iii. Autocorrelation with Selected letters. For example, K, R or C is
denoted as ‘1’ and the rest as ‘0’. Lag is then computed. For de—
tails, see Ofer and Linial (2014).

2.2.5 AA scale-based features

AA propensity scales map each AA to a quantitative value that rep—
resents physicochemical or biochemical properties, such as hydropa—
thicity or size. These scales can then be used to represent the protein
sequence as a time series, typically using sliding windows of differ—
ent sizes and to extract additional features.

ProFET includes a wide array of scales, ranging from the estab—
lished propensities for hydrophobicity and ﬂexibility/B—factors
(acquired from Expasy), to ‘optimal’ and maximally independent
derived scales (Atchley et (11., 2005; Georgiev, 2009).

Features derived from these scales include:

i. Averages for the sequence as a whole, for different window
sizes.
ii. Quartile averages (e.g. top 25%).
iii. Maximum and minimum values for a given scale and window—
size along the entire sequence.
iv. Autocorrelation.

2.2.6 Transformed CTD features (Dubchak et 411., 1995)

We implemented the Dubchak and ProFEAT CTD features (hydro—
phobicity, normalized Van der Waals volume, polarity, polarizabil—
ity, charge, secondary structure and solvent accessibility
hydrophobicity, normalized Van der Waals volume, polarity, polar—
izability, charge, secondary structure and solvent accessibility)
(Dubchak et (11., 1995; Li et al., 2006). Code from Spice (van den
Berg et (11., 2014) and (Cao et (11., 2013) was also integrated. An add—
itional subdivision of disorder propensity was adapted from
Composition Profiler (Vacic et (11., 2007): 1:‘ARSQEGKP‘,
2:‘ILNCFYVW’ and 3:‘DHMT’.

The features used are:

i. Composition (C) is the number of AA of a particular property
divided by the total number of AA.
ii. Transition (T) is the number of transitions from a particular prop—
erty to different property, divided by (total number of AA — 1).
iii. Distribution (D) captures is the chain length within which the
ﬁrst 25%, 50%, 75% and 100% AA of a particular property
are located.

9103 ‘Org rsnﬁnv uo salaﬁuv 50'] ‘BIIIJOJIIBD JO ArtsmAtuf} 112 /310'S[BIIJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”Idllq 11101} popcolumoq

3432

D. Ofer and M.Linial

 

2.3 Evaluation

The power of any of the predictor proposed is tested by several rou—

tinely used evaluation methods. We measure the performance for

the binary and multiclass tasks with the same metrics: F1 score (the

weighted average of the precision and recall) and Accuracy (Acc).
These parameters are defined as:

- F1:2"'TP/(2TP+FP+FN)
- Ac:(TP+TN)/(TP+TN+FP+FN)

TP represents the number of the correctly recognized proteins.
FP, the number of proteins wrongly identified and EN the number of
proteins missed. Performance is evaluated using cross—validation.

Specifically, multiple rounds of randomized stratified cross valid—
ation (‘Stratified Shufﬂe Split’), with 18% holdout for each iteration
(unless mentioned otherwise). Features were filtered prior to cross
validation and testing using a simple univariate filter for statistical
significance (a g 0.01, Bonferroni multiple testing family wise
error rate corrected; analysis of variance one—way F—test). This pre—
filtering step at the cross validation phase had a negligible impact on
the overall performance (not shown).

2.4 Feature selection

A wide array of methods for supervised and unsupervised feature se—
lection can be applied to identify the best features, implemented
with the superlative Scikit learn toolkit (Abraham et (11., 2014).
These include wrapper methods—Random Feature Elimination
(Ozcift, 2012), model—based filtering [e.g. support vector machine
(SVM) classifiers with a L1 Loss penalty, for sparse coefficients],
statistical filtering, stability selection, PCA, etc.

In the test cases, we used the RFE method, combined with an
underlying non—linear ensemble of classifiers (Random forests). The
underlying principle is iterative fitting of the classifier on the data,
with the weakest features being pruned at each of the iterations
(Abraham et (11., 2014). We examined the selected features, and the
model classification performance with the reduced set of features,
and show novel, interpretable features, as well as excellent retained
performance.

3 Results

3.1 ProFET outline

We introduce two test cases to illustrate the potential of ProFET to
provide a generic platform for analyzing the basis of high—level func—
tionality in proteins.

Classifying thermophile proteins was used as a test case for a bin—
ary classification of functionality that is not explicitly derived from
the sequence. Classifying neuropeptide (NP) hormone precursors
serves to assess the classification of poorly studied protein niche
(Karsenty et (11., 2014). We generalize the approach to a range of
from subcellular localization to viral phylogeny tasks (see Section
2.1.1—2.1.5). In all the illustrated cases, ProFET was used as a gen—
eric framework for feature extraction and prediction. External
information that is often available (e.g. the family PSSM, GO anno—
tation, structural prediction and disorder predictors) was not
included.

The workflow is composed of modular sections (Fig. 1)

1. ProFET: Feature extraction from any protein sequences.
Extracted features can be analyzed independently (suitable for
ML analysis or unsupervised tasks) or discriminativer (i.e. seek—
ing contrast between groups of proteins).

2. Model Selection: The features are used to train and tune differ—
ent ML models. For any given performance metric (e.g. preci—
sion), the optimal model and hyper—parameters are selected.

3. Performance Report: Classiﬁcation performance is measured for
a given model and dataset, using cross—validation.

4. Feature Selection: Informative features are selected and their im—
portance measured using different methods. These methods in—
clude the statistical signiﬁcance, wrapper methods, model—based
selection, stability selection and more.

5. New sequences can be predicted using a trained ML model. This
can be applied via the feature extraction pipeline or with a se—
lected smaller subset of the selected features.

3.2 ProFET workflow—case studies
We selected three datasets to illustrate the performance of ProFET
and its workﬂow (Fig. 1).

3.2.1 Positive—negative protein sets

Set 1: Thermophiles are proteins that function under high tempera—
ture. Given the extreme environmental conditions, we expect to de—
tect biophysical signatures in these proteins underlying their
thermostability. We used a benchmark dataset of 915 thermophilic
and 793 non—thermophilic (Mesophile) proteins that were further fil—
tered to insure < 40% sequence identity between sequences within
each group (Lin et (11., 2005).

Set 2: NPPs are pre—pro—polypeptide precursors of NPs. These
are secreted proteins. Routine sequence alignment—based methods
are insufficient to identify the immensely diverse NPs. In compiling
a dataset, we used as a negative set a collection of proteins with
Signal peptides, which lacked validated TMD (and therefore, most
likely to be secreted). We keep the same (atypical) range of lengths
to match the labeled NPPs. Both the positive and negative datasets
have Signal peptides confirmed and cleaved using SignalP (Petersen
et (11., 2011). The negative (non—NPP) dataset was filtered using
Usearch (Edgar, 2010), so that proteins in the negative set shared no
sequence similarity (cutoff of 10% identity was applied). The final
dataset held 2309 negatives and 1269 NPPs. Note that in the case of
NPPs, we expect many unidentified NPP peptides among the pro—
teins in the negative set.

Set 3: Uncultured bacteria account for ~25 0 K proteins in
UniProtKB (Wu et (11., 2006). We restricted the test to those having
GO annotations for ‘ribosome’, ‘membrane’ or ‘cytoplasm’. Proteins
were filtered for redundancy according to UniRef50 classification,

—»
_
1

Fig. 1. The ProFET framework: merging machine-learning protocols, cross-

Input

 
 
 
 
 
 

   
 
 
  
   

Feature
Extraction

Machine
Learnrng

   

New Sequences

Cross
Validation

validated tuning, feature selection and prediction

9103 05 JSanV uo so1a§uv soc} ‘BIIIJOJIIBD JO AJtSJQAtuf] 112 /310's1au1n0fp101x0"sotwurJOJutotq/ﬁduq r1101} papao1umoq

Feature engineering workflow

3433

 

leaving 15 995 sequences, 59.2% of them being ‘membrane’
proteins.

3.2.2 Classification results

For all three sets (as in Section 3.2.1), we obtained almost perfect
classification. Classification was performed using a random forest
classifier, implemented in Scikit learn (see Section 2). Figure 2A
shows the results of the classifications for the set of the
Thermophilic proteins and the NPPs as confusion matrices. Results
were derived from 10—fold stratified cross validation. In both sets,
the number of missed classified (FN and FP) is below 5% for the
NPPs (63 and 110 proteins as FN and FP, respectively). For the set
of the thermophiles, the missed classifications of the FN and FP
reach 6% and 10%, respectively. Figure 2B shows the performance
as receiver operating characteristics curves. Performance was meas—
ured using an automatically tuned SVM with a radial basis function
(RBF) kernel, with 15—fold stratified cross validation. The perform—
ance was very high with a FP rate of 0.1 and the AUC for both tests
reaching 0.97 (out of a maximum of 1.0).

Uncultured bacteria comprise a set of poorly characterized pro—
teins (Set 3). We trained proteins that mapped three main compart—
ments in bacterium (membrane, cytoplasm, ribosome, total of
15 995 sequences). The localization performance for the multi—class
task is very convincing (tested via 12 rounds of stratified shufﬂe split
cross—validation). The F1 score is 0.917 (:0.01 SD); accuracy is
0.916 (:0.01 SD).

We further used a combination of ML approach with naive PSI—
Blast search. We activated PSI—Blast (three iterations, default param—
eters) on sets 1 and set 2 (Thermophiles and NPPs sets). The most

Conius on matr'x — Thermo h'les .
I I p I Confusron matrix — Neuropeptides

   

 

  

3 M1 50 EL
 a m 2
a in?
T: B
E E 3
«2 a a
E 1’8 g,
.5
Mesophile Thermophiles
Prad'cmd Class Predrcled class
B ROG (Receiver operating characteristic) Curves
1.0 u   ’ ,
I
’ I
0.8 r I
2 , ’
a? , ’
2 0.6 ’ I I
‘_ f
.8; « ’
o 0'4 r ’ m“ Neuropeptide Precusors
E , ’ Mean ROG (area = 0.9?)
02 a ’ Than'nophiles
’ a ’ — Mean Roc (area = 0.9?)
— - Luck
0.0
0.0 0.2 0.4 0.6 0.3 1.0

False Positive Rate

 

Fig. 2. Performance results for the two datasets used. (A) Confusion matrix of
the classifier performance. Results were derived from 10-fold stratified cross-
validation. The number of FF and FF is shown forthermophiles (left) and NPs
(right). (B) AUC (area under receiver operating characteristics curve)

significant E—value was used for each sequence as an approximate
distance matrix. We then trained a K—nearest—neighbors classifier
and recorded the performance. We also used an unsupervised, clus—
tering approach (spectral clustering and K—means) and compared
these clusters to the ‘true’ labels.

Clustering performance was significantly lower than reported
(Fig. 2). The best results for the Psi—Blast test were obtained from
Spectral clustering model. For the NPP set (total of 3370 proteins),
the F1 score is 0.56. The similar analysis for the Thermophile/
Mesophile proteins reached F1 score of 0.29 (total of 1708 pro—
teins). To make sure that the poor performance is not dependent on
the choice of the ML methodology, we repeated the analysis for a
classification by K—nearest neighbors classifier (k : 1 or 2). The data
were split 80/20 into evaluation and hold—out sets, and the best par—
ameters on the evaluation set were determined by 4—fold cross valid—
ation. For the NPP and Thermophile sets, the accuracy on the
‘evaluation set’ was 62.8% (:0.16 SD) and 48.9% (:0.03 SD), re—
spectively. The F1 score for the hold—out sets were 0.61 and 0.44 for
sets 2 and 1, respectively.

3.3 Post—training feature selection

In addition to the success of the predictors, interpretability of the
features that best contributed to the performance is a crucial know—
ledge. Several methods for feature selection can be applied to iden—
tify a minimal set of such features. We applied a combination of
Random Forests (an ensemble of decision tree classifiers) with the
Random Feature Elimination wrapper method.

In each of the iterations, the weakest features are removed and
the model is then retrained with the remaining features, until the
preselected desired amount of features remains. Performance of the
reduced feature set is measured using new splits of the training data
and cross validation. Recall that the initial set of (default) generated
features included 771 features. The F—test filter reduced the number
of features to 453 and 544 features for the Thermophiles and NPP
sets, respectively.

3.3.1. Thermophilic proteins—informative features
We note the importance of AA composition, particularly of charged
and polar AA groups. Of further importance are features involving
glutamic acid (E) and glutamine (Q), and the organizational entropy
of E and Q. The relevance of these AA was reported (Lin and Chen,
2011; Zhang and Fang, 2007). We note that merely using the AA
composition would not have captured many of these features.

The classification performance (F1 score) with just 15 features
reached 99.53% of that obtained using all statistically significant
features (F1 score : 0.906; 453 features).

3.3.2 NPPs feature—informative features

As opposed to the features dominating the test case of thermophilic
proteins, in the case of the NPPs, a smaller set of features dominates,
mainly relating to the normalized frequency of putative NPP cleavage
sites, according to the ‘known motif’ model. Further properties of the
basic residues Lys (K) and Arg (R) repeat themselves by virtue of en—
tropy, binary autocorrelation (6/15 features) and more. Additional
features include protein size (Mw and length) and to a lesser extent
some ‘structural’ properties, such as ﬂexibility (‘Flex_min’), and sec—
ondary structural propensities—reﬂecting the importance of availabil—
ity of the putative cleavage sites and atypical composition of the
putative peptides.

9103 05 JSanV uo so1a§uv 50’] 0211110111123 JO [(1151910qu 112 /310's1au1n0fp101x0"sothJJOJutotq/ﬁduq r1101} papao1umoq

3434

D. Ofer and M.Linial

 

Classification performance: (F1 score of the positive class) with
just 15 features was 95.85% of that obtained using all statistically
significant features [0.945 (T 0.01); 544 features].

Figure 3 shows the types of the 15 strongest features for the two
test cases. Selected features are ranked by relative importance to the
classifier. Feature titles are self—explanatory. For example,
‘ofer14KC’ specifies the reduced AA alphabet ofer14 (see Section
2.2.2) for grouping of KC in the reduced representation.

3.4 Benchmarks’ performance
The workﬂow applied to our test cases (Section 3.2) was systematic—
ally applied to all the datasets. Each set was measured using 15—fold
randomized stratified cross—validation. For each iteration, a fraction
of the data (18%) is randomly set apart. The framework’s automat—
ically selected the performance of the classifier. The term ‘Dummy —
by majority’ applies to a classifier that always picks the majority
class. Altogether, we present 15 additional datasets (in addition to
the NPPs and Thermophilic proteins). For 76% of the datasets, the
accuracy and F1 Scores are above 80%, while for 35%, the accuracy
is > 90% (Fig. 4).

The classification performance for DNA and RNA binding pro—
teins meets the state of the art results obtained by special purpose

 

a. E

L; E

e §

i E

3- ﬂ

E IIIII gill

‘5 I.- z _  II!IIT_
i'i'iiiiiéiitiiiﬁ Writrtfaitirt

 

“'4-§;"§'§ :- 1H ENE :-
“raw-g  5:"- ~
_ g g a a. 1 iii: if

I: i

Fig. 3. Top 15 informative features that dominate the successful classification
of thermophilic proteins and NPPs

I A“; ':F1-score

     

predictors (Wang, etal., 2010). This specialized predictor for DNA
and RNA binding proteins relies on the specific evolutionary infor—
mation (e.g., PSSMs) combined with Support Vector Machine
(SVM) (Wang, etal., 2010). 72.42% Accuracy is reported for DNA
binding proteins using a random forest model and extensive feature
selection (Kumar et al., 2009).

We used the same benchmark data to directly assess the perform—
ance. We show (Fig. 4) that our platform reaches a classification
success of 0.72 and 0.79 for DNA and RNA binding proteins, re—
spectively. We conclude that excellent performance is achieved by
using the default setting of the ProFET workﬂow.

Five of the benchmarks (Supplementary Table S1, Fig. 4) concern
structural SCOP datasets, at the class or fold level. The classification
success varies according to the tasks. For example, the success for
the SCOP ‘selected class’ is very high (0.82—0.9), whereas the per—
formance for the fold classification is much lower (0.62—0.65). Note
that SCOP 25% and SCOP 10% tasks use the same dataset (SCOPe
version 2.05). These sets differ only by the degree of redundancy re—
moval. We found similar levels of accuracy for both sets. The per—
formance (accuracy, F1 score) for all 17 analyzed datasets with
respect to the Dummy—majority classifier is shown (Supplementary
Table S1).

4 Discussion

The main drawbacks in existing sequence—based methods are (i)
some functions cannot be detected by sequence—based methods; (ii)
current statistical models mostly capture local patterns rather than
high—level function and (iii) rare sequences or those that have very
few homologs cannot be successfully used for inference or construc—
tion of good statistical model.

4.1 Compact representations

In this study, we introduce ProFET as a feature extraction platform
that can serve many classification tasks. ProFET was compiled as a
ﬂexible tool for any size of protein sequence. Our platform adds to
previous studies that use quantitative feature representations for

I Fl-soore Dummy 11w majority}

     

   

   

at“

0gb a _ x _ \
-.  aw
3“ _ or°‘ a

at" a“?

Fig. 4. Classification performance by the Accuracy and F1 score for 17 datasets using the ProFET combined with the ML scheme. Results and SD for the Accuracy
(Acc, dark blue) and the F1-score (light blue, middle) are shown. Dummy predictor is a default classifier for the largest class in the dataset (rightmost, coloured

pink)

9103 05 JSanV uo so1a§uv 50’] 0211110111123 JO [(1151910qu 112 /310'S[BHJnOprOJXO'SOIJ’BLUJOJIIIOICI”K1111] r1101} papao1umoq

Feature engineering workflow

3435

 

sequences. The communality in these methods is the transformation
step in which the protein sequences are converted to hundreds or
thousands of features, many of them elementary biochemical and
biophysical properties, while others are statistically derived (e.g. fre—
quency of AA and dipeptides).

ProFET includes many novel additions for the elementary repre—
sentation. For example, features that are based on a reduced alpha—
bets, entropy, high performance AA scales, binary autocorrelation,
sequence segmentation, mirror k—mers and more. Many of these fea—
tures not only improved performance while allowing a compact rep—
resentation but also expose statistical importance properties in
proteins (Fig. 3). The advantage of using reduced alphabet has been
noted for 3D—structure representation (Bacardit et (11., 2009) and
more (Weathers et (11., 2004).

ProFET results were the input for ML approaches allowing a
rigorous assessment of performance and reaches state of the art re—
sults. Recovering the classification success by a small set of top fea—
tures argues for the power of a compact representation for
understanding the features that dominate any specific tasks.

4.2 The user perspective
Several conclusions can be drawn from the results of the classifica—
tion tasks (Fig. 4):

A. Protein centric analysis: Feature engineering methods pre—
sented in this study should be considered a baseline approach for
whole protein rather than protein domains. Most of our knowledge
from 3D structure and evolution relies on the properties of domains
within proteins. We propose the feature engineering as a comple—
mentary approach to the domain—centric one.

B. ‘One size fits all’: Features that are included in ProFET are
highly relevant to a broad range of proteins. This is in contrast to
methods that customize features for a specific task. The ProFET
pipeline provides a default set of features that is suitable for many
classification tasks. Therefore, ProFET eliminate the need to dupli—
cate the effort for feature extraction.

C. Flexibility of use: Our presented pipeline accepts a single se—
quence, combined files, multiple files or a directory. It automatically
labels the input into classes (if desired) and normalizes the features
(if desired). Thus, any user can use ProFET to set the desired com—
bination of features, representations and normalization. From the
point of view of the user, several considerations were taken:

' Our pipeline handles FASTA ﬁles and stores them as labeled CSVs.

' We use state of art, open source, freely available python data sci—
ence tools (such as Pandas, scikit—learn, biopython) (Cock et (11.,
2009).

' Easy to add new features using a standardized format.

' Our framework includes details on the features as part of the
data pipeline so results are interpretable.

' Our code is available for academic and non—commercial use,
under the GNU 3 license.

We provide a large collated resource for feature extraction.
Thanks to the modular design of ProFET, adding and tinkering with
features is trivial. Users of ProFET can decide to focus, remove or
expand any subset of the features (e.g. k—mer lengths). ProFET
allows tuning of any number of parameters in the feature generation
pipeline, e.g. the AA scales to use and the elementary window size
for extracting properties. In addition, features can be extracted lo—
cally from the N’ terminals or the C’—terminals or from an arbitrary
segment of the protein.

In summary, the approach presented here is suitable and power—
ful for application towards modern approach for ML especially in
the emerging field of Deep Learning and unsupervised learning of
feature representations. These features can easily experimented with
allowing additional applications of biological insight to the task of
feature engineering.

Acknowledgements

We thank Michael Doron for extensive collaboration, aid and programming
expertise in setting up the framework. Nadav Rappoprt supported Psi—Blast
comparisons. We thank Nadav Rappoport, Nadav Brandes and Kerem
Wainer for fruitful discussions. The project is part of the ELIXIR
infrastructure.

Conﬂict of Interest: none declared.

References

Abraham,A. et al. (2014) Machine learning for neuroimaging with scikit-
learn. Front. Neuroinform., 8, 14.

Atchley,W.R. et al. (2005 ) Solving the protein sequence metric problem. Proc.
Natl. Acad. Sci. USA, 102, 6395—6400.

Bacardit,J. et al. (2009) Automated alphabet reduction for protein datasets.
BMC Bioinformatics, 10, 6.

Bock,J.R. and Gough,D.A. (2001) Predicting protein—protein interactions
from primary structure. Bioinformatics, 17, 455—460.

Cai,Y.D. et al. (2001) Support vector machines for predicting protein struc-
tural class. BMC Bioinformatics, 2, 3.

Campen,A. et al. (2008) TOP—IDP—scale: a new amino acid scale measuring
propensity for intrinsic disorder. Protein Pept. Lett., 15, 95 6—963.

Cao,D.S. et al. (2013) propy: a tool to generate various modes of Chou’s
PseAAC. Bioinformatics, 29, 960—962.

Chandonia,J.M. et al. (2004) The ASTRAL Compendium in 2004. Nucleic
Acids Res., 32, D189—D192.

Cheng,J. and Baldi,P. (2007) Improved residue contact prediction using sup-
port vector machines and a large feature set. BMC Bioinformatics, 8, 113.
Cheng,J. et al. (2005) SCRATCH: a protein structure and structural feature

prediction server. Nucleic Acids Res., 33, W72—W76.

Chou,K.C. and Cai,Y.D. (2003) Prediction and classiﬁcation of protein subcel-
lular location—sequence—order effect and pseudo amino acid composition.
]. Cell. Biochem, 90, 1250—1260.

Cock,P.J. et al. (2009) Biopython: freely available Python tools for com-
putational molecular biology and bioinformatics. Bioinformatics, 25,
1422—1423.

Ding,C.H. and Dubchak,I. (2001) Multi—class protein fold recognition using sup-
port vector machines and neural networks. Bioinformatics, 17, 349—35 8.

Dinke1,H. et al. (2012) ELM—the database of eukaryotic linear motifs.
Nucleic Acids Res., 40, D242—D251.

Dubchak,I. et al. (1995) Prediction of protein folding class using global
description of amino acid sequence. Proc. Natl. Acad. Sci. USA, 92,
8700—8704.

Edgar,R.C. (2010) Search and clustering orders of magnitude faster than
BLAST. Bioinformatics, 26, 2460—246 1.

Edgar,R.C. and Sjolander,K. (2004) COACH: proﬁle—proﬁle alignment of pro-
tein families using hidden Markov models. Bioinformatics, 20, 1309—1318.

Finn,R.D. et al. (2014) Pfam: the protein families database. Nucleic Acids
Res., 42, D222—D230.

Fox,N.K. et al. (2014) SCOPe: structural classiﬁcation of proteins—extended,
integrating SCOP and ASTRAL data and classiﬁcation of new structures.
Nucleic Acids Res., 42, D304—D309.

Gasteiger,E. et al. (2003) ExPASy: the proteomics server for in-depth protein
knowledge and analysis. Nucleic Acids Res., 31, 3784—3788.

Georgiev,A.G. (2009) Interpretable numerical descriptors of amino acid space.
]. Comput. Biol., 16, 703—723.

9103 05 JSanV uo so1a§uv 50’] 0211110111123 JO [(1151910qu 112 /310'S[BIIJnOprOJXO'SOIJ’BLUJOJIIIOICI”Zduq r1101} papao1umoq

3436

D. Ofer and M.Linial

 

Greene,L.H. et al. (2007) The CATH domain structure database: new proto—
cols and classiﬁcation levels give a more comprehensive resource for explor-
ing evolution. Nucleic Acids Res., 35, D291—D297.

Gromiha,M.M. and Suwa,M. (2005 ) A simple statistical method for discrimi—
nating outer membrane proteins with better accuracy. Bioinforrnatics, 21,
961—968.

Hua,S. and Sun,Z. (2001) Support vector machine approach for protein sub-
cellular localization prediction. Bioinforrnatics, 17, 721—728.

Iaakkola,T. et al. (2000) A discriminative framework for detecting remote
protein homologies. I. Comput. Biol., 7, 95—114.

Karplus, K. et al. (1998) Hidden Markov models for detecting remote protein
homologies. Bioinforrnatics, 14, 846—85 6 .

Karsenty,S. et al. (2014) NeuroPID: a classiﬁer of neuropeptide precursors.
Nucleic Acids Res., 42, W182—W186.

Klus,P. et al. (2014) The cleverSuite approach for protein characterization:
predictions of structural properties, solubility, chaperone requirements and
RNA-binding abilities. Bioinforrnatics, 30, 1601—1608.

Kumar,K.K. et al. (2009) DNA-Prot: identiﬁcation of DNA binding proteins
from protein sequence information using random forest. I. Biomol. Struct.
Dyn., 26, 679—686.

Kyte,I. and Doolittle,R.F. (1982) A simple method for displaying the hydro-
pathic character ofa protein. I. Mol. Biol., 157, 105—132.

Leslie,C.S. et al. (2004) Mismatch string kernels for discriminative protein
classiﬁcation. Bioinforrnatics, 20, 467—476.

Lewis,T.E. et al. (2013) Genome3D: a UK collaborative project to annotate
genomic sequences with predicted 3D structures based on SCOP and CATH
domains. Nucleic Acids Res., 41, D499—D507.

Lin,H. and Chen,W. (2011) Prediction of thermophilic proteins using feature
selection technique. I. Microbiol. Methods, 84, 67—70.

Lin,C. et al. (2013) Hierarchical classiﬁcation of protein folds using a novel en—
semble classiﬁer. PloS One, 8, e5 6499.

Lin,K. et al. (2005) A simple and fast secondary structure prediction method
using hidden neural networks. Bioinforrnatics, 21, 152—15 9.

Mulder,N. and preiler,R. (2007) InterPro and InterProScan: tools for
protein sequence classiﬁcation and comparison. Methods Mol. Biol., 396,
5 9—70.

Murphy,L.R. et al. (2000) Simpliﬁed amino acid alphabets for protein fold rec—
ognition and implications for folding. Protein Eng., 13, 149—152.

Naamati,G. et al. (2009) ClanTox: a classiﬁer of short animal toxins. Nucleic
Acids Res., 37, W363—W368.

Nanni,L. et al. (2014) An empirical study of different approaches for protein
classiﬁcation. ScientiﬁcWorldIournal, 2014, 236717.

Nugent,T. and Iones,D.T. (2009) Transmembrane protein topology prediction
using support vector machines. BMC Bioinformatics, 10, 159.

Ofer,D. and Linia1,M. (2014) NeuroPID: a predictor for identifying neuropep—
tide precursors from metazoan proteomes. Bioinforrnatics, 30, 931—940.

Ozcift,A. (2012) Enhanced cancer recognition system based on random forests
feature elimination algorithm. I. Med. Syst., 36, 25 77—25 85 .

Pe’er,I. et al. (2004) Proteomic signatures: amino acid and oligopeptide com—
positions differentiate among phyla. Proteins, 54, 20—40.

Petersen,T.N. et al. (2011) SignalP 4.0: discriminating signal peptides from
transmembrane regions. Nat. Methods, 8, 785—786.

Peterson,E.L. et al. (2009) Reduced amino acid alphabets exhibit an im—
proved sensitivity and selectivity in fold assignment. Bioinforrnatics, 25,
135 6—1362.

Portugaly,E. et al. (2002) Selecting targets for structural determination by nav—
igating in a graph of protein families. Bioinforrnatics, 18, 899—907.

Prilusky,I. et al. (2005) FoldIndex: a simple tool to predict whether a given
protein sequence is intrinsically unfolded. Bioinforrnatics, 21, 3435—3438.

Radivojac,P. et al. (2013) A large—scale evaluation of computational protein
function prediction. Nat. Methods, 10, 221—227.

Rentzsch,R. and Orengo,C.A. (2009) Protein function prediction—the power
of multiplicity. Trends Biotechnol., 27, 210—219.

Rost,B. et al. (2003) Automatic prediction of protein function. Cell. Mol. Life
Sci., 60, 2637—2650.

Saeys,Y. et al. (2007) A review of feature selection techniques in bioinfor-
matics. Bioinforrnatics, 23, 2507—25 17.

Soding,I. (2005) Protein homology detection by HMM-HMM comparison.
Bioinforrnatics, 21, 951—960.

Sonnhammer,E.L. et al. (1997) Pfam: a comprehensive database of protein do—
main families based on seed alignments. Proteins, 28, 405—420.

Southey,B.R. et al. (2006) NeuroPred: a tool to predict cleavage sites in neuro-
peptide precursors and provide the masses of the resulting peptides. Nucleic
Acids Res., 34, W267—W272.

Todd,A.E. et al. (2005) Progress of structural genomics initiatives: an analysis
of solved target structures. I. Mol. Biol., 348, 1235—1260.

Vacic,V. et al. (2007) Composition Proﬁler: a tool for discovery and visualiza-
tion of amino acid composition differences. BMC Bioinformatics, 8, 211.
Valencia,A. (2005 ) Automatic annotation of protein function. Curr. Opin.

Struct. Biol., 15, 267—274.

van den Berg,B.A. et al. (2014) SPiCE: a web-based tool for sequence—based
protein classiﬁcation and exploration. BMC Bioinformatics, 15, 93.

Varshavsky,R. et al. (2007) When less is more: improving classiﬁcation of pro-
tein families with a minimal set of global. In: Giancarlo,R. and
Hannenhalli,S. (eds.) Algorithms in Bioinformatics: 7th International
Workshop, WABI. Springer, Philadelphia, PA, pp. 12—24.

Veenstra,I.A. (2000) Mono- and dibasic proteolytic cleavage sites in in—
sect neuroendocrine peptide precursors. Arch. Insect Biochem. Physiol, 43,
49—63.

Wang,L. et al. (2010) BindN + for accurate prediction of DNA and RNA—binding
residues from protein sequence features. BMC Syst. Biol., 4(Suppl 1), S3.

Weathers,E.A. et al. (2004) Reduced amino acid alphabet is sufﬁcient to accur-
ately recognize intrinsically disordered protein. FEBS Lett., 5 76, 348—352.

Wu,C.H. et al. (2006) The Universal Protein Resource (UniProt): an expand—
ing universe of protein information. Nucleic Acids Res., 34, D187—D191.

Yachdav,G. et al. (2014) PredictProtein—an open resource for online predic-
tion of protein structural and functional features. Nucleic Acids Res., 42,
W337—W343.

Zhang,G. and Fang,B. (2007) LogitBoost classiﬁer for discriminating thermo-
philic and mesophilic proteins. I. Biotechnol., 127, 417—424.

9103 05 JSanV uo so1a§uv 50’] 0211110111123 JO [(1151910qu 112 /310'S[BIIJnOprOJXO'SOIJ’BLUJOJIIIOICI”Zduq r1101} papao1umoq

