Bioinformatics, 31 (14), 2015, 2364—2370

doi: 10.1093/bioinformatics/btv156

Advance Access Publication Date: 19 March 2015
Original Paper

 

 

Bioimage informatics

Automatic determination of NET (neutrophil
extracellular traps) coverage in fluorescent
microscopy images

Luis Pedro Coelho1'*"’, Catarina Patoz, Ana Friaesz, Ariane Neumann3,
Maren von Kockritz-Blickwede3, Mario Ramirezz and
Joao André Garrico2

1Unidade de Biofisica e Expressao Genética, Instituto de Medicina Molecular and 2Instituto de Microbiologia,
Instituto de Medicina Molecular, Faculdade de Medicina, Universidade de Lisboa, Portugal and 3Department of
Physiological Chemistry, University of Veterinary Medicine Hannover, Germany

*To whom correspondence should be addressed.
TPresent address: European Molecular Biology Laboratory (EMBL), Heidelberg, Germany
Associate Editor: Robert Murphy

Received on July 9, 2014; revised on February 10, 2015; accepted on February 16, 2015

Abstract

Motivation: Neutrophil extracellular traps (NETs) are believed to be essential in controlling several
bacterial pathogens. Quantification of NETs in vitro is an important tool in studies aiming to clarify
the biological and chemical factors contributing to NET production, stabilization and degradation.
This estimation can be performed on the basis of fluorescent microscopy images using appropriate
labelings. In this context, it is desirable to automate the analysis to eliminate both the tedious pro—
cess of manual annotation and possible operator—specific biases.

Results: We propose a framework for the automated determination of NET content, based on visu—
ally annotated images which are used to train a supervised machine—learning method. We derive
several methods in this framework. The best results are obtained by combining these into a single
prediction. The overall ('22 of the combined method is 93%. By having two experts label part of the
image set, we were able to compare the performance of the algorithms to the human interoperator
variability. We find that the two operators exhibited a very high correlation on their overall assess—
ment of the NET coverage area in the images (Ff2 is 97%), although there were consistent differ—
ences in labeling at pixel level (02, which unlike R2 does not correct for additive and multiplicative
biases, was only 89%).

Availability and implementation: Open source software (under the MIT license) is available at
https://github.com/luispedro/Coelh02015_NetsDetermination for both reproducibility and applica—
tion to new data.

Contact: luis@luispedro.org

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 IntrOducuon circulating in the blood are recruited to the site of inﬂammation or

Neutrophils are important effectors of the innate immune system infeCtiona contribUting t0 Pathogen Clearance through PhagOCYtOSiSa
in mammals, constituting the first line of defense against many PrOdUCtiOH 0f highly tOXiC reaCtiVe oxygen Species and release 0f
microbial pathogens. Upon tissue injury and infection, neutrophils granule proteins with antimicrobial activity (Mayadas et (11., 2014).

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 2354

112 /3.10'speum0fp1q1x0"sotJBurJOJutotq/ﬁduq 11101} popcolumoq

91oz ‘Og anﬁnv 110 ::

Automatic determination of NET coverage

2365

 

In recent years, it has been shown that some inﬂammatory medi—
ators, as well as a wide range of microbes including bacteria, fungi
and protozoa, can stimulate neutrophils to undergo a distinctive
form of cell death designated NETosis (Yipp and Kubes, 2013).
This process leads to the extracellular release of fibers of DNA asso—
ciated with histones, granule proteins and peptides—neutrophil
extracellular traps (NETs). These are capable of entrapping micro—
bial pathogens and mediate their extracellular killing. It is also
becoming increasingly clear that NET formation is linked to mul—
tiple neutrophil—mediated pathologies and that it is implicated in
vascular injury and thrombosis (Mayadas et (11., 2014). Since the
first identification of NETs by Brinkmann et al. (2004), this has
become an area of active research, with many studies relying on the
in vitro production and quantification of NETs as means to further
understand the mechanisms involved in their formation, as well as
their importance in pathogen clearance and in the development of
multiple diseases (Buchanan et (11., 2006; Kessenbrock et (11., 2009;
Marin—Esteban et (11., 2012; Neumann et (11., 2014; Wartha et (11.,
2007; Yost et (11., 2009).

The spectroﬂuorometric quantification of NET content based on
DNA release using DNA intercalating dyes has been frequently used.
However, this can be confounded by the necrotic release of DNA, and
by the inﬂuence of peptides present in NETs, such as LL—37, which
can affect the binding of DNA to such dyes (Neumann et (11., 2014).
The quantification of the area covered by NETs on ﬂuorescent mi—
croscopy images is a suitable alternative, but visual quantification, be—
sides being tedious and labor—intensive, has reproducibility problems
and can present interoperator differences. In order to avoid these pit—
falls, an automatic quantification method is needed. A method for the
semiautomatic quantification of in vitro NET formation in micros—
copy images has been recently proposed, based on the calculation of
the percentage of neutrophils that are under NETosis, corresponding
mostly to a cell death program that leads to chromatin decondensa—
tion and NET formation (Brinkmann et (11., 201 3). This method is use—
ful for evaluating NET induction by different stimuli, but is not
suitable for NET degradation experiments. In these experiments, NET
production is chemically induced, so that virtually all neutrophils are
activated, and the ability of degrading NETs is then evaluated based
on the quantification of the DNA fibers.

Preliminary analysis showed that threshold—based methods,
which Brinkmann et al. (2013) successfully used to detect NETs,
were not able to quantify the area of the slide that was covered in
NETs (which requires a more precise quantification).

We thus developed a more sophisticated method based on a
supervised machine learning approach that has shown good results
in other areas of bioimage informatics (Boland and Murphy, 2001;
Boland et (11., 1998; Conrad and Gerlich, 2010; Glory and Murphy,
2007; Loo et (11., 2007; Nanni et (11., 2010) including (most similar
to our work) in the segmentation of ﬂuorescent images (Nattkemper
et (11., 2002). These methods have even been reported to outperform
human evaluation on some tasks (MacLeod et (11., 2010;
Nattkemper et (11., 2003). In particular, we used a supervised learn—
ing approach whereby we learn a regression model to determine
NET fraction in local regions based on numeric features computed
from the pixels in that region. Finally, all the regions in an image are
aggregated for a combined prediction. We present three different im—
plementations of this generic framework based on particular choices
for splitting the image into regions and different feature sets.

We also show that we can combine the outputs of the different
implementations to obtain a single combined result, which outper—
forms all the individual predictions, as evaluated by cross
validation.

Part of our data was annotated by two different experts enabling
us to quantify interoperator variation. We observe that the two ex—
perts have large disagreements on a pixel—by—pixel level, with smaller
disagreements on the global appreciation of the image. However, we
also document a systematic bias between the two experts, highlight—
ing the potential pitfalls of relying on human visual classification.

2 Methods

2.1 Data acquisition

Human blood—derived neutrophils were stimulated with 25 nm of
phorbol 12—myristate 13—acetate (PMA) (Sigma) for 4h to induce
nearly 100% NET formation, as previously described (von
Kockritz—Blickwede et (11., 2010). The cells were then incubated for
1 h at 37°C and 5% C02 with purified bacterial nucleases or with
the supernatant of bacterial cultures for NET degradation. NET
samples were immunostained using a mouse monoclonal anti—H2A—
H2B—DNA complex (Losman et (11., 1992) followed by a secondary
goat—anti—mouse Alexa—Fluor—488 antibody and embedded in
ProlongGold + DAPI (Invitrogen) to counterstain DNA (von
Kockritz—Blickwede et (11., 2010). Images were acquired using a Leica
TCS SP5 confocal microscope with a HCX PL APO 40 X 0.75—1.25
oil immersion objective or a Zeis LSM 710 confocal microscope with
an EC Plan—Neoﬂuar 40 X 1.30 oil immersion objective. For each
preparation, randomly selected images were acquired. For most prep—
arations, at least three images were collected.

2.2 Data labeling

NETs were labeled visually using Image] (Schneider et (11., 2012). A
total of 88 fields were labeled from 30 different slides. A subset of
the data (37 fields) was labeled independently by two experts, that
were blinded to each other’s classification. Figure 1 shows an ex—
ample image, including manual segmentation by both operators.

2.3 Method evaluation
The result of our algorithm is an estimate f of the fraction of the
image area that is covered by NETs, which must be compared with

the human—annotated value  For evaluation, we used the Q2 meas—
ure, defined by Wold (1982) as:

Z (yi — 37192
Q2 : 1 —  (1)
2 (3’2" — 7192
z
where )7,- is the prediction for y,- obtained by cross—validation (i.e. by
evaluating a model which had not been trained using 31,-), and y, is the
average value across all the data. This is similar to the familiar R2 coef—
ficient of determination (and is referred to as cross—validated R2 by
some authors), but, unlike computing R2 between prediction and re—
sponse, it penalizes additive and multiplicative biases in the prediction.
We used a cross—validation scheme whereby all the fields from
the same slide were held—out for testing, while the fields from the
other slides are used for training.

2.4 Software

Software was implemented in Python, using the mahotas library for
image processing and feature computation (Coelho, 2013) and scikit
learn for regression (Pedregosa et (11., 2011). The whole dataset,
code, and the learned models are available for download under the
MIT license at https://github.com/luispedro/Coelho2015_Nets
Determination. A tutorial on how to apply the methods to other

112 /3.10'speum0fp1q1x0"sotJBurJOJutotq/ﬁduq 11101} pQPBOIII/lAOG

9103 ‘Og anﬁnv 110 ::

2366

L.P.Coelho et al.

 

DNA H2A-H2E3-DNA

Merged (first)

Merged (second)

 

Fig. 1. Example image including comparison between human operators. Neutrophils were induced to form NETs followed by exposure to degrading agents
derived from bacterial cultures. NETs were visualized using an Alexa 488-labelled antibody against H2A-HZB-DNA complexes in combination with DAPI to stain
the nuclei; white lines depict human operator segmentation. The first operator marked 8.7% of the area as NETs, the second operator 5.8%. Scalebar corresponds
to 50 um. Images have been contrast stretched for publication, original data is available for download

data is available at that location as well. In the case of application to
images with very similar characteristics (same tagging strategy and
comparable acquisition apparatus and settings) to our own, the
model may be applicable directly. For other data, it may be more
appropriate to fit a new model with our software.

3 Results

3.1 A framework for automated NET quantification with
three proposed implementations
We present a generic framework for NET quantification, based on
human—labeled data. As input, this framework takes in a set of
images, consisting of DNA and histone channels, and a human
labeling of these images where regions of interest, namely NET
areas, have been annotated. The framework will then produce
a model which can be applied to new images to produce an
estimate of the NET area in those images. We describe several
methods which fall into this framework and finally we demonstrate
that combining the methods obtains a better result than any single
method.

The framework is illustrated in Figure 2 and consists of the
following steps:

1. partition the image into regions and compute numeric
descriptors (features) in each region,

2. based on the human—labeled data, learn a regression from
features to estimate the fraction of area covered by NETs in each
region. Estimates for regions of the same image are aggregated
to form a raw image estimate.

3. Finally, apply a linear correction to this raw estimate to correct
for biases in the previous steps.

The ﬁrst step is motivated by the fact that NETs are present in dis—
crete regions in the images and not throughout the image. In order to
characterize these images by machine—learning methods, we compute
numeric features from each region. Previous work had used similar
techniques in cell segmentation problems (Chen et al., 2011; Sommer
et al., 2011). We used regression to estimate the fraction of regional
NET coverage, instead of classifying the regions into NET containing
versus NET empty, because preliminary results showed that develop—
ing a segmentation method to obtain good agreement with manually
drawn boundaries would be nontrivial and an unnecessarily hard
step. Partitioning of the image can be achieved by oversegmentation
or be defined geometrically (e.g. using a regular grid).

. _.

From each reglon, we compute a feature vector x,. We learn a re—
gression function r, which returns an estimate of the local NET
fraction:

371' : 1135i)- (2)

This function is estimated from the training data. In our imple—
mentation, we used random forest regression (Breiman, 2001) for
this step (using 100 trees). This nonlinear method has the advantage
of providing out—of—band estimates for the training data, which are
comparable to what would have been obtained by cross—validation.
These estimates are used below.

A raw image prediction is obtained by computing the average
NET fraction over all areas. This average is weighted by the relative
size of each area:

A m-
fl'aW : I 

7
251'
i

where )7,- is the estimate for region 1' and s,- is the size of the region
(in number of pixels).

The image prediction is obtained by a linear correction to the
raw estimate

fcorrected : ﬁfraw + u‘ 

The parameters [3 and at were learned by least—squares minimiza—
tion on the training data using the out—of—band estimates for ﬂaw.
This step corrects biases in the first regression (Wolpert, 1992) (in the
results section, we present the empirical evidence for the necessity of
this step). This differs from the generic stacking estimator which uses
estimates from cross—validation, but when using random forests,
out—of—band estimates are similar to cross—validated estimates and can
be obtained at a very low computational cost (Breiman, 2001).

Finally, the fraction estimates are clipped to values between zero
and one.

In what follows, we describe three different implementations of
this framework, which differ in the way in which the image is bro—
ken up and the features computed.

3.1. 1 Oversegmented regions
We oversegmented the Alexa channel with seeded watershed initial—
ized with regularly spaced seeds. For each region, for both the Alexa

112 £10spatian[plogxo'soutzuuoguioiq//:dnq 1110.1} papmjumoq

9103 ‘Og anBnV uo ::

Automatic determination of NET coverage

2367

 

0 Image Partition
"

 

 

Fig. 2. Framework overview. (1) Input images are (explicitly or implicitly) partitioned into images (depicted is the result of applying the technique described in

Section 3.1.1). (2) A supervised approach estimates a local fraction for each region, which is summarized to a first global estimates by a weighted average.

(3) This estimate is finally corrected for biases with a linear regression

and the DAPI channel, we computed the following features (a total
of 77 features were used):

1. the size of the region in pixels,

2. Haralick texture features for each of the channels (Haralick
et al., 1973),

3. Sobel features.

4. Haralick texture features of the Sobel ﬁltered version of each of
the channels,

5. Pearson correlation between channel intensities,

6. overlap features.

The inclusion of Sobel filtering was motivated by the observation
that NETs are fibrous. Therefore, we reasoned that an edge—enhanc—
ing filter would be appropriate. The other features were included as
they have been previously been shown to produce good results in
similar bioimage problems (Newberg et al., 2009).

Two overlap features were used: the fraction of pixels above
threshold in either channel and the fraction above threshold in both
channels. The mean intensity value was used as the threshold.

Regions that were smaller than 16 pixels were removed from con—
sideration as the statistics computed on them can be unreliable due to
the small number of pixels. Additionally, this step reduces the size of
the input of the regression module, speeding up the process. The
removed regions represent, on average, less than 4% of the total area
of the image, therefore the impact of excluding these regions is small.

3.1.2 Regular sampling of filtered images (pixel regression)

Similar to what is done in Ilastik, a machine—learning based segmen—
tation framework (Sommer et al., 2011 ), we compute several filtered
versions of the image. Then, we sample from these filtered images
on a regular grid (each sample point is 8 pixels away from the next
and points close to the border are discarded. This results in 3844
samples from each 512 X 512 image). In particular, from each chan—
nel, we computed the following filtered versions of the image
(in total, 1 8 features are gathered at each sampling point):

1. Gaussian ﬁltered images with different standard deviations
((724, 8, 12 and 16)
2. difference of Gaussians (between a : 16 and a : 8)

Sobel ﬁltered
4. thresholded image using both the mean pixel value and mean

(N

pixel value plus two standard deviations as thresholds

5. a version of the image where pixel values were converted to z—
scores (i.e. mean subtracted and normalized by the estimated
standard deviation).

From these images, pixel values were sampled in a regular grid,
forming a feature vector 35. To obtain an estimate of the local NET
fraction at each pixel, we considered the human—labeled NET image
as a binary image and blurred it with a Gaussian filter (a : 8 pixels).
Thus, at each pixel, we have a weighted average of the NET fraction
in the local region. These values are sampled at the same locations
as the features to form the regression labels.

3.1.3 Dense local features
Local features have recently been shown to perform well in other
bioimage informatics machine learning problems (Coelho et al.,
2013; Liscovitch et al., 2013), including segmentation (Song et al.,
2013). In our work, we used dense sampling (Nowak et al., 2006),
whereby feature descriptors are computed in a regular grid (as
above, we used an 8 pixel separation between sampling points,
while discarding areas close to the image border). We compute
SURF features (Bay et al., 2008) computed at four different scales
(namely setting 1, 2, 4 and 8 pixels as the initial scale).

As in the case of pixel regression, we associated a local NET frac—
tion with each feature vector by smoothing the binary map with a
Gaussian filter.

3.2A combination of methods outperforms any single
method

Figure 3 shows the errors committed by the individual models
(errors estimated by cross—validation). We can see that the local—fea—
ture based methods show correlated errors, but this is not the case
for the other methods. These data suggest that a better prediction
could be obtained by combining the methods. We thus combine all
methods by averaging the raw predictions of the methods before
applying Equation (4).

Figure 4 summarizes the results of this study. Interestingly, the
performance of all the methods is similar, with the exception of
SURF features using the largest scale (8 pixels).

The results also clearly demonstrate the value of the linear cor—
rection. SURF features with a scale of 1 was the worst method be—
fore the correction, but the best (tied with pixel regression) after
correction. This is explained by the fact that this model had a clear

112 /3.10'spzu.mo[p10}xo"sotJBurJOJutotq/ﬁduq 11101} pQPBOIII/IAOG

9103 ‘Og anBnV uo ::

2368

L.P.Coelho et al.

 

   

   

 

 |

a II“ I'll- .

 0'59 Ill.--  

% 0.63 0.17 ,1.'.

a _-I||||III ."I . ' . '

E 0.74 0.18 0.85   
m .Il l-I :H 

E

g 0.71 0.22 0.71 0.92 II'llI

E

s 0.61 0.23 0.62 0.58 0.63 
” hll II.

pixel regions surf{1) surl(2} surftd) suri(8)

Fig. 3. Error correlations. For each cell above the diagonal, we depict a scatter
plot of errors: each dot represents an image and we show the difference to
the reference. If predictions exactly matched the references, all points would
collapse to a single point in the centre The diagonal shows histograms depict-
ing the density. Below the diagonal, the Pearson correlation is displayed. The
method identified as regions is described in Section 3.1.1, the one as pixel in
Section 3.1.2 and the SURF approach in Section 3.1.3

multiplicative bias (see Supplementary Fig. S1). The overall Q2
obtained by combining the methods was 93% (compared with 90%
for the best performing single prediction).

In Figure 5, we plot the estimated results versus the underlying
gold standard of human classification (the estimates were obtained
by cross—validation). We can see that the estimates are predictive of
the underlying value throughout the whole range of possible values.

3.3 Per pixel agreement between operators is low, but
image—level agreement is high

Part of the data was independently labeled by two human operators
(The operators were CP and AN.). Thus, we are able to quantify the
amount of variance and bias in human annotation.

We found that, on a pixel—by—pixel level, the two labelings pre—
sented substantial differences. In fact, the Pearson correlation be—
tween the two binary pixel assignments was 0.68 (corresponding to
47% explained variance). Reporting explained variance of binary
variables may be misleading (Abelson, 1985), but the Jaccard Index
is only, on average, 0.67 (this measure ranges from 0 to 1, with 1
denoting perfect agreement). See Figure 1 for a comparison of the
two operators on a single image.

However, the fraction of the area assigned to NETs by each of the
operators showed a much higher correlation (R2 is 97%). At the same
time, we observed a systematic difference in labeling since one of the
experts consistently reported a lower value. Reﬂecting this, the Q2
measure (which penalizes for this additive error) is only 89% (see
Fig. 6), slightly below the value obtained by the automated prediction.

3.4 Cross—validation per slide is a stricter measure of
performance than cross—validation per field

As described above, several fields were acquired from each
microscope slide. When presenting the results above, we used a

Itlll

 

III] 87 89

 

surf{8_‘I surlH} pier 500(2) surltt} regions avg
Method

Fig. 4. Results for all of the methods. Methods are ordered by the quality of
their corrected predictions

LU —

0.8 —

    

Automatic prediction

 

’0‘ .0
o

 

I I I I I I
0.0 [1.2 0.4 0.6 0.8 1.0
Human labeled

Fig. 5. Final combined prediction. Each dot represents an image, plotting the
underlying gold standard on the horizontal axis and the cross-validated pre-
diction on the vertical axis. Prediction were obtained by cross-validation, i.e.
the NET fraction of each image is estimated from a model which was trained
on data not including that particular image nor any image from the same
slide

cross—validation scheme whereby we held out a slide at a time for
testing and trained on the rest of the data. To support our choice,
we also considered an alternative schedule whereby a single field is
left out at a time. Thus, each field is evaluated by a model which is
obtained from training data which includes other fields from the
same slide.

The results show that such approach would lead to over—inﬂated
performance measures. For instance, when the combined method is
evaluated in this manner, we obtain a Q2 value of 95% (higher than
the 93% we obtained in stricter testing approach). Similarly small
but positive differences are observed when evaluating all the
individual methods (see Supplementary Table S1).

4 Discussion

We present a framework for automated quantification of NET area
in fluorescent microscopy images. Based on this generic framework,
we implemented three specific methods. Follow—up work may make
different choices of implementation and arrive at different methods.
The use of different feature sets, different oversegmentation methods
or a different regression methodology are all potential avenues to

112 /310's1cu1noip101x0"sotJBurJOJutotq/ﬁduq 11101} pap1201umoq

9103 ‘0g anBnV uo ::

Automatic determination of NET coverage

2369

 

1.0 -

R2 : 97%

0.8 —

0. 0 -

0.x] —

0.2 —

 

 

0.0 —

Human labeler 2 {fraction assigned to NET)

 

I I I I I I
0.0 0.2 0.4 0.6 0.8 1.0
Human labeler 1 {fraction assigned to NET)

Fig. 6. Comparison between different human operators. Each circle repre-
sents a labeled image, the solid line shows the diagonal (hypothetical perfect
agreement)

explore whilst staying within the generic framework we propose in
this work.

The final output of our method is a single fraction measurement
which does not directly relate back to pixel assignments. While the
ideal method would compute a perfect segmentation, from which
the NET fraction would be trivially estimated, it is not necessary to
obtain this segmented image in order to solve a biologically relevant
question. A single fraction estimate can provide enough information
for the user. This is in accordance with work in computer vision
which had previously shown that to get accurate cell counts, it is not
necessary to completely segment individual cells (Lempitsky and
Zisserman, 2010).

The fact that we only aimed to obtain single overall estimate
from each image, instead of pixel assignments, allowed us to build a
stacked estimator from the different methods to obtain a final pre—
diction, even though these do not share the same partitions of the
image (Wolpert, 1992). On our data, the gains from combining dif—
ferent predictions is at least partially explained by the fact that the
errors of the different methods are only weakly correlated.

Previous work by Coelho et al. (2013) had demonstrated the
need for careful delineation of data for cross—validation in the con—
text of subcellular location determination. In this work, we re—
inforce the point in a different context, namely automated
determination of NET content, by demonstrating that when differ—
ent images from the same sample are used for training and testing,
the measured performance is increased relative to validation from
data from different samples. This is likely due to overfitting to
match very particular aspects of each slide as opposed to generaliz—
able features of NETs.

We also empirically investigated and measured the agreement
between different human evaluators. The results require a nuanced
interpretation. The pixel—level agreement is very low, but the two
experts have very high correlation in their evaluations of the images.
This is another instance where pixel—level measures are misleading, a
phenomenon we and others had already investigated in the case of
nuclear segmentation (Coelho et al., 2009; Yang—Mao et al., 2008).
The general conclusion is that for evaluation of automated tech—
niques, the evaluation measure should be as close as possible to the
underlying biological goal (in our case, the fraction of the sample
covered by NETs) and not an intermediate result (the exact locations
of such areas).

The high correlation between human operators was hiding a sys—
tematic bias in quantitative results. One of the operators systematic—
ally reported a lower value. The automated method was more
consistent with the data that it was trained on, resulting in a lower
Q2 value.

Although the samples were prepared using the same protocol,
the image acquisition equipment and parameters were not the same
(see Section 2). Despite these differences, the proposed algorithm
was able to classify equally well images obtained with either condi—
tion, suggesting that it is insensitive to any potential specificities
introduced.

The automated method has a lower cost, perfect reproducibility,
and results in a faster processing of the images than human annota—
tion. Automated analysis opens up the possibility of multistudy
comparisons by consortia of independent laboratories, which can be
trained with their data and assessed by the method proposed. This
can result in more robust training sets that can accommodate more
significant differences in sample preparation protocol and data ac—
quisition conditions. In its current form, the robustness of the pro—
posed automated process, when combined with automated image
acquisition, can facilitate the analysis in a single laboratory of num—
bers of fields and samples that would be prohibitive using manual
annotation. Such large—scale analysis will certainly improve confi—
dence in the determination of NET area, providing more accurate
and reproducible analyses than is currently available from human
annotation. Our data is available for others in the community.

Funding

L.P.C. was supported by Fundagao para a Ciencia e a Tecnologia (grant
PTDC/SAU-GMG/l15652/2008). M.v.K.-B. was supported in part by DFG
grant KO 3552/4—1. A.N. was supported by a fellowship of the Akademie fur
Tiergesundheit (AfT) and the PhD programme ‘Animal and Zoonotic
Infections’ of the University of Veterinary Medicine Hannover, Germany.
A.F. received a Short-Term Fellowship 2011 by the Federation of European
Biochemical Societies (FEBS) and an ESCMID Travel Grant for Training in a
Foreign Institution 2011 by the European Society of Clinical Microbiology
and Infectious Diseases (ESCMID).

Conﬂict of Interest: none declared.

References

Abelson,R.P. (1985) A variance explanation paradox: when a little is a lot.
Psychol. Bull., 97, 129—133.

Bay,H. et al. (2008) Speeded—up robust features (SURF). Comput. Vis. Image
Understand, 110, 346—359.

Boland,M.V. and Murphy,R.F. (2001) A neural network classiﬁer capable of
recognizing the patterns of all major subcellular structures in ﬂuorescence
microscope images of hela cells. Bioinformatics, 17, 1213—1223.

Boland,M.V. et al. (1998) Automated recognition of patterns characteristic of
subcellular structures in ﬂuorescence microscopy images. Cytometry, 33,
366—375.

Breiman,L. (2001). Random forests. Mach. Learn., 45, 5—32.

Brinkmann,V. et al. (2004) Neutrophil extracellular traps kill bacteria.
Science, 303,1532—1535.

Brinkmann,V. et al. (2013) Automatic quantiﬁcation of in vitro net formation.
Front. Immunol., 3, doi: 10.3389/ﬁmmu.2012.

Buchanan,J.T. et al. (2006) DNase expression allows the pathogen group a
streptococcus to escape killing in neutrophil extracellular traps. Curr. Biol.,
16, 396—400.

Chen,C. et al. (2011) A pixel classiﬁcation system for segmenting biomedical
images using intensity neighborhoods and dimension reduction. In:
Biomedical Imaging: From Nano to Macro, 2011 IEE International
Symposium on, pp. 1649—1652.

112 /310'S[BIIJnOprOJXO'SOIJBLUJOJIIIOICI”Zduq 11101} pap1201umoq

9103 ‘0g isanV uo ::

2370

L.P.Coelho et al.

 

Coelho,L.P. (2013) Mahotas: Open source software for scriptable computer
vision. I. Open Res. Softw, 1.

Coelho,L.P. et al. (2009) Nuclear segmentation in microscope cell images: a
hand—segmented dataset and comparison of algorithms. In: IEEE
International Symposium on Biomedical Imaging: From Nano to Macro,
2009.1SBI’09, pp. 518—521.

Coelho,L.P. et al. (2013) Determining the subcellular location of new proteins
from microscope images using local features. Bioinformatics, 29,
2343—2349.

Conrad,C. and Gerlich,D.W. (2010) Automated microscopy for high-content
rnai screening. I. Cell Biol., 188, 453—461.

Glory,E. and Murphy,R.F. (2007) Automated subcellular location determin—
ation and high—throughput microscopy. Dev. Cell, 12, 7—16.

Haralick,R.M. et al. (1973) Textural features for image classiﬁcation. IEEE
Trans. Syst. Man Cyhernet., 3, 610—621.

Kessenbrock,K. et al. (2009) Netting neutrophils in autoimmune small-vessel
vasculitis. Nat. Med., 15, 623—625.

Lempitsky,V.S. and Zisserman,A. (2010) Learning to count objects in images.
In: NIPS, vol. 1, p. 2.

Liscovitch,N. et al. (2013) FuncISH: learning a functional representation of
neural ish images. Bioinformatics, 29, i36—i43.

Loo,L.—H. et al. (2007) Image-based multivariate proﬁling of drug responses
from single cells. Nat. Methods, 4, 445—45 3.

Losman,M.I. et al. (1992) Monoclonal autoantibodies to subnucleosomes
from a mrllmp (—)+l+ mouse. oligoclonality of the antibody response and
recognition of a determinant composed of histones h2a, h2b, and dna. I.
Immunol., 148,1561—1569.

MacLeod,N. et al. (2010) Time to automate identiﬁcation. Nature, 467,
154—155.

Marin-Esteban,V. et al. (2012) Afaldr diffusely adhering escherichia coli strain
c1845 induces neutrophil extracellular traps that kill bacteria and damage
human enterocyte—like cells. Infect. Immun., 80, 1891—1899.

Mayadas,T.N. et al. (2014) The multifaceted functions of neutrophils. Annu.
Rev. Pathol., 9, 181—218.

Nanni,L. et al. (2010) Local binary patterns variants as texture descriptors for
medical image analysis. Artif. Intell. Med., 49, 117—125.

Nattkemper,T.W. et al. (2002) A neural network architecture for automatic
segmentation of ﬂuorescence micrographs. Neurocomputing, 48, 35 7—36 7.

Nattkemper,T.W. et al. (2003) Human vs. machine: evaluation of ﬂuorescence
micrographs. Comput. Biol. Med., 33, 31—43.

Neumann,A. et al. (2014) Novel role of the antimicrobial peptide ll-37 in the
protection of neutrophil extracellular traps against degradation by bacterial
nucleases. I. Innate Immun.

Newberg,I.Y. et al. (2009) Automated analysis of human protein atlas im—
munoﬂuorescence images. In: Proceedings IEEE International Symposium
on Biomedical Imaging: From Nano to Macro, vol. 5193229, pp.
1023—1026.

Nowak,E. et al. (2006) Sampling strategies for bag-of—features image classiﬁ—
cation. In: Computer Vision—ECC V2006. Springer, Berlin, pp. 490—503.
Pedregosa,F. et al. (2011) Scikit-learn: machine learning in Python. I. Mach.

Learn. Res., 12, 2825—2830.

Schneider,C.A. et al. (2012) NIH image to Image]: 25 years of image analysis.
Nat. Methods, 9, 671—675.

Sommer,C. et al. (2011) ilastik: Interactive learning and segmentation toolkit.
In: IEEE International Symposium on Biomedical Imaging: From Nano to
Macro, pp. 230—233.

Song,Y. et al. (2013) Region—based progressive localization of cell nuclei in
microscopic images with data adaptive modeling. BMC Bioinformatics, 14,
173.

von Kockritz-Blickwede,M. et al. (2010) Visualization and functional
evaluation of phagocyte extracellular traps. Methods Microbiol., 37,
139—160.

Wartha,F. et al. (2007) Capsule and d—alanylated lipoteichoic acids protect
streptococcus pneumoniae against neutrophil extracellular traps. Cell.
Microbiol., 9, 1162—1171.

Wold,H. (1982) Soft modeling: the basic design and some extensions. In:
Systems Under Indirect Observation, vol. 2, pp. 5 89—5 91.

Wolpert,D.H. (1992) Stacked generalization. Neural Netw., 5, 241—25 9.

Yang—Mao,S.—F. et al. (2008) Edge enhancement nucleus and cytoplast contour
detector of cervical smear images. IEEE Trans. Syst. Man Cyhernet. Part B,
38, 353—366.

Yipp,B.G. and Kubes,P. (2013) Netosis: how vital is it? Blood, 122,
2784—2794.

Yost,C.C. et al. (2009) Impaired neutrophil extracellular trap (NET) forma-
tion: a novel innate immune deﬁciency of human neonates. Blood, 113,
6419—6427.

112 /310'S[BIIJnOprOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} pap1201umoq

9103 ‘0g isanV uo ::

