BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Genome compression

 

implementations are not yet available, whereas the works that
have been implemented (Claude et al., 2010; Gagie et al., 2012;
Kreft and Navarro, 2013; Makinen et al., 2010) are tested on
relatively small collections, not exceeding ~1 GB.

In this article, we try to answer the question how well a
collection of genomes of the same species can be compressed,
when knowledge of the possible variants is given. The cited
works of Christley et a]. (2009) and Pavlichin et a]. (2013)
are so far the only attempts to compress a (single) genome
sequence with a variant database. In this work, we take two
large collections of genomes (H.sapiens and A.thaliana) and try
to exploit cross—sequence correlations in the variant loci. Our
solution is a specialized Ziv—Lempel—style compressor, where
the input sequences are basically formed by binary ﬂags denot—
ing if successive variants from the database were found in the
individuals. This approach appears highly successful, allowing
to store the human collection in 432 MB (395KB per individ—
ual) and the plant collection in 110MB (142 KB per individ—
ual). We point out that the general idea of exploiting common
features for improved compression is known for some other
NGS tasks, including compression of (both mapped and un—
mapped) reads (Bonﬁeld and Mahoney, 2013; Cox et al., 2012;
Hach et al., 2012; Jones et al., 2012; Popitsch and Haeseler,
2013).

In the next section, we present the input data and the general
idea of our approach. Then, we show some details of the pro—
posed compression algorithm. Finally, we evaluate the compres—
sor. The last section concludes the article.

2 MATERIALS AND METHODS
2.1 Datasets

Large collections of genomes of the same species in public repositories are
nowadays often represented as one reference genome and multiple variant
ﬁles. There are several formats for storing the variants, e.g. variant call
format (VCF) (Danecek et al., 2011) used in the lOOOGP, general feature
format (GFF) used in the PGP. These formats are much more compact
than, e.g. FASTA (with raw genomic sequences), yet large collections
may still require hundreds of gigabytes of storage.

We use two large datasets in the experiments. The publicly available
database of Phase 1 of the lOOOGP contains data for 1092 human indi-
viduals. The genomes are in VCF ﬁles, one ﬁle for each chromosome, and
so there are 24 ﬁles in total. These ﬁles contain the information about
each variant [SNP, insertion (INS), deletion (DEL) and structural variant
(SV)] that was found in at least one genome in the dataset. The genomes
are phased, i.e. there is information on which of the two chromosomes of
each pair (or on none/both) each variant is found.

Similar information about variants is present in the lOOlGP for
A.thaliana. For this collection, we have 775 haploid sequences, each con-
sisting of seven chromosomes. These data were scattered, and we gather
them from four ‘subprojects’.

Our research goal concerns only genome collection compression [simi-
larly as in the works of Christley et a]. (2009) and Pavlichin et a]. (2013)],
and VCF ﬁles usually contain much more information than needed to
recover the DNA sequences (e.g. in FASTA format). We ignore the
non-essential VCF ﬁelds, i.e. keep only the information on which positions
the changes in each genome may be found. For the lOOOGP dataset, it
meant removing extra ﬁelds from the original VCF ﬁles. For the lOOlGP
dataset, we directly converted the available data to a stripped VCF
(sub)format, which we call VCF minimal (VCFmin). We point out that
these are valid VCF ﬁles. As a side note, let us remark that the GFF data

Table 1. Characteristics of the datasets used in the experiments

 

H .sapiens data

 

 

Variant types Total Average found on
23 23
chromosome chromosomes
pairs
No. of SNPs 38 267 471 3 701254 2 553 479
No. of l-symbol insertions 392 515 97 023 67 586
No. of 2-symbol insertions 81 462 27 325 19223
No. of longer insertions 103 918 35 502 25 205
No. of l-symbol deletions 393 748 94031 65 503
No. of 2-symbol deletions 166 554 38 316 25 795
No. of longer deletions 305317 64907 43 186
No. of SVs 14422 746 462
Total no. of variants 39 725 407 4059 104 2 800 439

 

A.thaliana data

 

 

Variant types Total Average found on
5 chromosomes,
chloroplast,
mitochondria

No. of SNPs 13 123 220 552 825

No. of l-symbol insertions 261428 2271

No. of 2-symbol insertions 35 005 249

No. of longer insertions 5652 39

No. of l-symbol deletions 1046 670 29 194

Total no. of variants 14471 975 584 579

 

(http: //evidence.personalgenomes.org/guide_upload_and_annotated_ﬁle_
formats) used in the large-scale PGP (Ball at al., 2012) are compatible with
that of ours stored in VCFmin.

The basic dataset characteristics are presented in Table 1 (URLs and
other technical descriptions of the data, including preprocessing details
for the 1001 dataset, are given in the Supplementary Material).

2.2 The compression algorithm

Our tool, Thousands Genomes Compressor (TGC), assumes that the
input data are in VCFmin form. Such textual ﬁle consists of rows, one
per each variant. A single row contains the following data:

0 Description of a variant (position, type and information about the
changes to the reference genome),

0 Evidence of occurrence of the variant in each single genome.

The VCFmin ﬁles can be compressed quite efﬁciently by general tools,
but much better results are possible. The biggest hurdle for a generic
compressor is the ‘non-locality’ of the VCFmin format, i.e. the genomes
are stored in columns, so the occurrences of the successive variants of the
same genome are at long distances. This means that if two genomes have
similarities, the compressor must ﬁnd and encode their similar (identical)
areas, which are far away and are relatively short (the description of the
occurrence of a variant in a single genome takes a few bytes). The main
idea of our algorithm is to transform the input data in a way to increase
the locality and lengths of similar (identical) genome areas. To this end,

 

2573

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

S.Deorowicz et al.

 

13957 DEL 1
13980 SNP C
30923 SNP T
46402 INS TGT
47190 INS A

\OOOQONUI'

1000 776646 SNP A
1001 776769 SV 15112 TGA
1002 776814 SNP G

Fig. 1. Excerpt of the variant database. Successive columns contain vari-
ant id, chromosomal location, type and change: (i) substituting symbol
for SNPs, (ii) inserted symbols for INSs, (iii) number of removed symbols
for DELs and (iv) number of removed symbols and (optionally) symbols
to insert for SVs

we transform the VCFmin ﬁle containing N genomes to the following
ﬁles:

0 A single database of variants containing only the basic information
about each variant (see Fig. 1); multiple variant alleles are also
supported,

0 2N (for diploid) or N (for haploid genomes) bit-vectors. Value 1 at
some jth position in this vector means that jth variant in the database
is found in the genome. To reduce the space, these bit-vectors are
packed into byte-vectors (8 bits in a byte). A byte is then the smallest
processing unit in the compression scheme.

For example, if the lOOOth variant in the database is 776646 SNP A,
and the lOOOth bit for some genome is 1, then we know that an SNP
occurs in this genome at position 776646 and the resulting nucleotide
there is A. The collection of the database of variants and the byte vectors
is later called as variant database+byte vectors (VDBV) format. Using
the dense byte vectors has a few advantages:

0 Processing compact input is usually faster and less memory demand-
ing than with more ‘bloated’ input;

0 Identical patterns of successive variants in different genomes are rep-
resented with repeating byte sequences (which can be easily handled
with standard dictionary compression techniques);

0 Same variants in different genomes have the same positions, and thus
encoding the repetitions of the common patterns in successive gen-
omes is cheaper.

The resulting VDBV representation is already well compressible with
generic tools, especially 7z, but universal solutions neglect some existing
redundancy; in particular, they are not ‘aware’ about the aligned repeti-
tions between genomes in our byte vectors. To exploit this, we devised a
specialized compressor loosely based on the LZSS algorithm (Storer and
Szymanski, 1982). Each vector is processed from left to right. At every
analyzed position k, we look for the longest match (identical byte sub-
string) starting at kth position in any previously processed byte vector.
The match position restriction is obvious, as we look for common haplo-
types, and matches elsewhere in the vectors are accidental and thus un-
likely to be long. (Moreover, with the restriction, the matches need fewer
bits to encode.) The already processed data from the vectors are indexed
using two hash tables (HTs) to make the search faster; one HT is for
searching for long matches and if none such is found then the other HT is
used (some more details on the hash scheme are given in the
Supplementary Fig. S2).

Similarly as for the classical LZSS algorithm, at each position we can
ﬁnd a match of length at least mm] (minimal match length, set

experimentally to ﬁve in our implementation) or a literal (if no sufﬁciently
long match can be found). A match is described as a triple (1, vid, Zen),
where vid is the id of the vector in which we found the longest match,
and Zen is the match length. A literal is represented with a pair (0,bv),
where bv is the value of the byte at position k in the byte vector. The ﬁrst
ﬁelds (ﬂags), 0 or 1, distinguish between literals and matches. After pro-
cessing a match, we shift to (k + len)th position, and in a case of literal
to (k + l)th position.

The sequence of pairs and triples is then compressed using an arith-
metic coder (Salomon and Motta, 2010) (We use a popular and fast
arithmetic coding variant by Schindler (http://www.compressconsult.
com/rangecoder/), also known as a range coder.) Broadly speaking,
arithmetic coder encodes each symbol occurrence on (in general, frac-
tional) number of bits related to the probability of the symbol occurrence.
These probabilities are estimated on the basis of already encoded sym-
bols, i.e. if a symbol has occurred frequently, the corresponding prob-
ability is high and the number of bits spent for encoding it is small.
We note that using Huffman coding instead for our data would result
in a few percent compression loss.

There are several contextual models for compressing various ﬁelds,
which means that different by-products of our scheme are handled
based on different collected statistics (more details are given in the
Supplementary data). The ﬂags are compressed in one model. For the
literals, another model is used, but before passing their byte values to
the entropy coder, we do some trick. The byte value of the ﬁrst literal
after a match (1, vid,len) is ‘xored’ (if its value is not 0) with the byte
following the repetition in vidth byte vector, i.e. the byte of index k + Zen.
We know these two byte values cannot be equal (otherwise the match
could be extended by at least 1 byte). It was found experimentally that it
is more likely to have a decreased number of resulting set bits in bv after
the ‘xor’ operation than to have it increased (even if in most cases, the
number of set bits is unchanged). In this way, the distribution of the bv
values gets more skewed, which helps the compression.

Contextual compression models are usually more practical if they are
not too large, and this is the reason why numbers from a broad interval
are often split before being processed by a statistical model (this approach
is used, e.g. in gzip, bzip2). Following this rule, in our scheme, the length
of the match is stored in two parts, both compressed with an entropy
coder. First, we compress the binary logarithm of the match length
(rounded to an integer) and then the remaining bits needed to recover
its length. More precisely, the ﬁrst part is [logZUen — mm] + UL, and
if [en — mm] 2 2, then the value of [en — mm] — 211°g2(1""""m1+1) ’1 is
encoded.

Similarly, the vid field is split into two (byte) parts: hid/256] and
vid — 256 hid/256], both encoded with an entropy coder.

The compression of variant database (excerpt of which is presented in
Fig. 1) is rather straightforward. The main idea is to compress each
variant type separately: SNP, insertion, deletion and SV. Thus, for each
variant, we ﬁrst store its type. The variants positions are then differen-
tially encoded, i.e distances between consecutive SNPs, consecutive
DELs, etc, are stored. Then, for SNP, we store the substituting symbol.
For INS, we store its length and the symbols to insert. For DEL, only its
length is stored. For SV, we encode the deletion length, the insertion
length and ﬁnally (if necessary) the inserted symbols. All the values are
encoded using a variant of arithmetic coding with appropriate contextual
models (details can be found in the Supplementary data).

3 RESULTS

As mentioned earlier, for the evaluation of the proposed com—
pression algorithm, we use two datasets of 1092 H.sapiens and
775 A.thaliana genomes. In all experiments, the data are pro—
cessed chromosome by chromosome. This approach, typical in
the genomic compression literature (see, e.g. Deorowicz and

 

2574

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

Genome compression

 

Grabowski, 2011; Pavlichin et al., 2013; Wandelt and
Leser, 2012), reduces the memory footprint, speeds up computa—
tions and improves the compression ratio for the generic
algorithms.

There are several tools for compressing genomic sequences
in FASTA format. Unfortunately, the amount of our test
data, ~6.8 TB of raw sequences if converted to FASTA, is
so huge that the running times of some of those compressors
would be counted in months. Thus, we started from a prelimin—
ary test in which we evaluated the most powerful as well as
the most recent tools for only two human chromosomes (14
and 21) and also two plant chromosomes (1 and 4). The results
are presented in Table 2. This and all further experiments were
performed on a computer equipped with four 4—core 2.4GHz
AMD Opteron CPUs with 128—GB RAM running Red Hat
4.1.2—46 linux.

Two of the presented compressors may be considered fast with
regard to the compression speed: ABRC (Wandelt and Leser,
2012) with ~100 MB/s (run with 8 threads) and GDC—normal
(Deorowicz and Grabowski, 2011) with ~40 MB/s speed (only a

Table 2. Evaluation of genome compressors for four sets: 2184 sequences
of H.sapiens chromosome 14 (237.6 GB) and chromosome 21 (106.5 GB),
775 sequences of A.thaliana chromosome 1 (23.9 GB) and chromosome 4
(14.6 GB)

 

 

Data 72 RLZ GReEn ABRC GDC GDC
Normal Ultra
H .sapiens
Chromosome 14 1068 270 218 472 674 2455
Chromosome 21 1561 269 211 460 685 2397
A.thaliana
Chromosome 1 242 86 64 67 154 254
Chromosome 4 234 80 59 61 141 230

 

Note: The values are compression ratios (rounded to the nearest integer) of the
collection, understood as the original data size divided by the compressed size.

serial implementation exists), while the others are by about one
[GDC—ultra (Deorowicz and Grabowski, 2011) and RLZ
(Kuruppu et al., 2011)] or about two (7z) orders of magnitude
slower. Interestingly, the only generic compressor in the tests, 7z,
is the second best in the compression ratio (after GDC—ultra), but
its compression speed is low (0.4 MB/s).

The memory available for the compression has a major impact
on the compression ratio. For example, 7z was run with its max—
imum setting, 1GB for its LZ—buffer (translating to >10 GB of
total memory use), yet it ﬁt only a small part of the input for
H.sapiens data: <10 individuals (each of size ~108 MB) for
chromosome 14 dataset and 21 individuals (each of size
~48 MB) for chromosome 21 dataset. This, supposedly, was
the main reason for which its compression ratio in the latter
case is signiﬁcantly higher. The hypothesis is indirectly conﬁrmed
by the results for much shorter A.thaliana chromosomes, for
which more individuals ﬁt the 1—GB LZ—buffer and the compres—
sion ratios are close to GDC—ultra.

In the next experiment, we compared a few well—known generic
compressors (gzip, bzip2, 7z) on VCFmin input ﬁles (Table 3).
Compressed sizes (in megabytes) and compression times are
reported for selected chromosomes and the collections in total.
Surprisingly perhaps, the best compression was obtained by
bzip2 compressor.

Table 4 is similar, but now the inputs are in our temporary
‘dense’ representation, VDBV. Here, the generic compressors are
compared against our proposal, TGC. Two simple observations
can be made: (i) the more compact of these two input represen—
tations, VDBV, is clearly more appropriate for the best generic
compressor, 7z, both from the point of compression ratio and
from speed; (ii) TGC is signiﬁcantly better than VDBV+7z in
both measured aspects, which demonstrates that designing a
specialized compression algorithm was a worthy goal in this case.

The results are summarized in Table 5. For comparison pur—
poses, we also show the sizes of the raw sequences as well as the
compression results of the best compressor working on such rep—
resentation, GDC—ultra. We resigned, however, from presenting
the compression and decompression times of GDC—ultra, as it
works on a completely different representation than VCFmin,

Table 3. Evaluation of universal compressors for variant data of 1092 (H.sapiens) and 775 (A.thaliana) samples

 

 

 

Data VCF VCFmin VCFmin + gzip VCFmin + bzip2 VCFmin + 7z
size size size c-time size c-time size c-time
H .sapiens
Chromosome 1 93 087 13 249 306.6 321 138.9 6259 144.2 2003
Chromosome 11 58 671 8351 196.5 205 89.0 3960 91.5 1400
Chromosome 21 16 065 2286 55.9 58 25.7 1090 26.9 467
Complete 1 223 470 173 505 4066.0 4114 1849.2 82128 1936.8 27 352
A .thaliana
Chromosome 1 7 4945 111.3 99 63.3 2596 86.4 1002
Chromosome 4 7 3386 76.3 67 43.7 1608 60.0 700
Complete 7 20 755 466.8 414 265.0 10107 362.8 4100

 

Note: The input data are in VCF format without any non—essential ﬁelds (VCFmin). For comparison, the sizes of the original VCF ﬁles from the 1000GP are given. All sizes
are in megabytes. Columns ‘c—time’ conmin compression time in seconds. The values marked in bold indicate best compression. The extended version of this table (with results

for all chromosomes) can be found in Supplementary Table Sl.

 

2575

ﬁm'spzumol‘pmﬂo'sopeuuopnorq/ﬁdnq

S.Deorowicz et al.

 

Table 4. Evaluation of universal compressors and the proposed algorithm (TGC) for variant data stored in the intermediate VDBV format

 

 

 

 

Data VCFmin VDBV VDBV + gzip VDBV + bzip2 VDBV + 7z TGC
size size c-time size c-time size c-time size c-time size c-time
H.sapiens
Chromosome 1 13 249 890.6 136 368.9 311 326.3 251 55.9 1070 32.3 690
Chromosome 11 8351 560.1 86 238.7 200 194.8 160 33.1 598 20.2 392
Chromosome 21 2286 153.0 25 66.8 54 39.3 44 10.4 107 6.3 96
Complete 173 505 11679.9 1819 4813.6 4075 3907.0 3396 733.8 11 176 431.6 8364
A.thaliana
Chromosome 1 4945 405.8 64 117.3 121 102.9 107 34.7 395 26.1 231
Chromosome 4 3386 281.4 42 79.7 92 67.9 72 23.7 271 18.2 120
Complete 20 756 1722.5 270 489.0 526 424.1 453 143.5 1,673 109.7 815

 

Note: All sizes are in megabytes and times are in seconds. The ‘VDBV c—time’ column contains the conversion times from VCFmin format to VDBV format. In the remaining
columns titled ‘c—time’, total compression time is given (i.e. ‘VDBV + 7z c—time’ denotes the sum VCFmin—to—VDBV conversion time and 7z compression time; ‘TGC c—time’ is
the total TGC processing time, comprising the conversion to VDBV and the actual compression). Note that the variant database (part of the VDBV representation) is of size
933 MB for H.supiens and 320 MB for Alkalimm. After compressing by TGC, their sizes (included in ‘TGC size” column) are ~51.0 MB and 12.5 MB, respectively. The values

marked in bold indicate best compression. The extended version of this table (with results for all chromosomes) can be found in Supplementary Table 52.

Table 5. Summary of the results of the universal, specialized and proposed compressors

 

 

Data Total size Size per individual Ratio to Ratio to Compression Decompression
[MB] [MB] VCFmin raw time [s] time [s]

H.sapiens
Raw 6 669 797 6107.873 7 7 7 7
GDC-ultra 2952 2.703 58.8 2259 7 7
VCFmin 173 505 158.887 7 7 7 7
VDBV 11680 10.696 14.9 571 1819 1730
VCFmin + 72 1937 1.774 89.6 3444 27 352 1951
VDBV + 72 734 0.672 236.4 9088 11 176 1936
TGC 432 0.395 402.0 15 453 8364 2067

A.thaliana
Raw 94047 121.351 7 7 7 7
GDC-ultra 384 0.495 54.1 245 7 7
VCFmin 20 756 26.782 7 7 7 7
VDBV 1723 2.223 12.1 55 270 286
VCFmin + 72 363 0.468 57.2 259 4100 239
VDBV+7z 144 0.185 144.6 655 1673 316
TGC 110 0.142 189.1 857 815 363

 

Note: There are two columns conmining ratios: ‘Ratio to raw” tells how many times the compressed ﬁle is smaller than the genome sequences in FASTA format. ‘Ratio to
VCFmin” is the compression ratio according to the size of VCFmin ﬁles. For GDC—ultra, compression and decompression times are not given, as this compressor uses a
different input form than others and such a comparison would be irrelevant. The values marked in bold indicate best compression.

which we use in the article. The compression ratios of GDC can
be treated as a reference point.

The most important ‘numbers’ from this summary are the
average sizes of genomes in the most compact, TGC, represen—
tation. The obtained 395 KB (for the human data) is more than
six times smaller than offered by the best so far genome
sequence, GDC—ultra, compressor. Also, the very recent paper
by Pavlichin et a]. (2013), working on a representation similar to
our VCFmin, reports more than six times larger ﬁles. When ex—
pressing the compression ratios in relation to the raw genome
sequence sizes, it means that our algorithm squeezes H.sapiens
genomes ~15 500 times and A.thaliana ~850 times.

In the last experiment, we compared TGC against SpeedGene
(Qiao et al., 2012), an algorithm for efﬁcient storage of SNP
datasets. For an honest comparison, we restricted the 1000GP
set of variants to SNPs only. SpeedGene requires the input data
to be in LINKAGE format, in which there is no distinction be—
tween chromosomes in each pair, i.e. for each SNP it describes
only whether no SNP is found in a genome, one SNP (on any
chromosome) is found, two SNPs are found (on both chromo—
somes) or the status of SNP is unknown. Thus, we changed our
algorithm slightly and instead of processing each single chromo—
some, we joined chromosomes of each pair, and obtained vectors
of ‘dibits’, i.e. bit pairs. These vectors are then transformed into

 

2576

ﬁm'spzumol‘pmﬂo'sopeuuopnorq/ﬁdnq

Genome compression

 

Table 6. Comparison of the ways of storing the dataset of SNPs from the
1000GP

 

 

Data No. of VCFmin LINKAGE/ SpeedGene TGC
SNPs (SNP only) PLINK

Chromosome 1 2 896 960 12 687 791.8 180.5 42.4

Chromosome 11 1 827 284 8003 498.8 115.3 26.9

Chromosome 21 497 824 2180 135.9 33.0 8.0

Complete 38 267 471 167 569 10 444.4 2426.4 564.8

 

Note: VCFmin means a simpliﬁed VCF with SNP calls only that spends only 4 bytes
for each genotype. All sizes are in megabytes. The values marked in bold indicate
best compression.

byte vectors (each byte contains four consecutive dibits). In this
way, our tool can compress these byte vectors without any
change. Table 6 presents the compressed sizes obtained by
SpeedGene and TGC. We show the results for three chromo—
somes, as well as for the complete genome. As one can see,
TGC reduces the dataset size more than four times better than
SpeedGene.

4 DISCUSSION

We examined the possibility of obtaining much better compres—
sion ratios of genomic collections than from existing tools, when
additional knowledge is given. The knowledge was the informa—
tion about the possible variants in genomes and the occurrence
of these variants in specific genomes. This helps a lot in com—
pression of genomic sequences, as all input sequences are per—
fectly aligned and the task of ﬁnding repetitions in data (usually
the most important and time—consuming task handled by data
compression algorithms) becomes rather simple. We should men—
tion that in theory such perfect alignments can be found by com—
pression algorithms, but the computational burden would be
enormous. Thus, compression tools usually make some heuristic
decisions when comparing the sequences in the hope that they do
not lose too much.

The success of our algorithm was possible not only because
of the variant database, but also because we searched for cross—
correlations between individuals. In other words, for each indi—
vidual, similarities to any other previously processed individual
(i.e. runs of repeating variants) can be found. In principle,
the available memory may be a limiting factor but process—
ing the collection on the chromosome level resulted in
<2.5 GB memory use for the larger (human) of the tested data—
sets. In the future, when much more genomes are available, we
may need to re—address the memory issue though, possibly
via working on blocks smaller than whole chromosomes, or
trying to re—order the sequences in a way to maximize local
similarities.

In the compression method design, we sometimes traded com—
pression ratio for reduced memory requirements, e. g. some
(rather minor) improvements in compression would be possible
owing to higher—order contextual modeling. Probably, a more
practical approach is to make use of more biological knowledge;
the very recent work of Pavlichin et al. (2013) gives new insight,
which might be possible to use in our scheme, but we leave it for
future work.

Why such experiments can be interesting? Although accurate
and efﬁcient analyses of such huge (several terabytes in raw
format) genomic collections remain a major challenge, we believe
that the mere compressibility of human genomes (e. g. as a
‘lower bound’ for memory requirements of future algorithms
and tools) is a question worth investigating. For example, our
compressed collection takes ~430 MB, so including also a com—
pressed reference genome (at most 700 MB) requires ~1.1 GB of
space, which seems quite modest. Naturally, running efficient
queries over such data is another matter (clearly with some over—
head in space use), but our results suggest this is not impossible.

The information kept in VCF or genome variation format
(GVF) files is often more detailed (e. g. may include quality
scores) than what our tool preserves. Although clearly efﬁcient
compression methods for such data are also needed, we do not
anticipate a possibility to obtain similar compression ratios to
TGC, unless a (strongly) lossy mode is used. Unfortunately, we
cannot see a way to easily adapt our compression techniques for
such data.

TGC allows extracting an arbitrary chromosome (or a whole
genome) from the compressed collection, yet this solution is
simple and rather slow. Making this extraction faster, or (even
better) allowing for quick access to position—restricted arbitrary
snippets of the genomes in the collection, is an important task left
for future work. Clearly, there must be some space—time tradeoffs
for such functionalities. A somewhat related functionality will be
to add or remove an individual genome to/from the collection.
Currently, changing the archive content requires recompressing
the collection from scratch.

The performed experiments showed that even the best genomic
sequence compressor, GDC—ultra, is signiﬁcantly (up to seven
times) poorer in compression ratio than what can be obtained
with extra knowledge. The main conclusions from our work are:

0 Modern genomic sequence compressors cannot come close
in compression ratio to the proposed algorithm basically
because of two, not fully independent, reasons: (i) (almost)
all of them ignore external knowledge (variant information),
and (ii) working on consensus sequences is extremely re—
source—consuming and keeping full statistics needed for effi—
cient compression is practically impossible for a large
collection even on a 128—GB machine.

0 Even huge human genome databases can be stored in rela—
tively small space, as the data size of a single individual is
only 395KB on average. When extrapolated, this would
mean that modern 2—TB hard drive is sufﬁcient to store
the genomes of ~5 million humans, size of a large city.

ACKNOWLEDGEMENTS

We thank the authors of SpeedGene for providing us with the
source codes of their tool, used in the experiments, and the
anonymous reviewers for helpful suggestions.

Funding: The work was partially supported by the Polish Ministry
of Science and Higher Education under the project [DEC—201 1/
01 /B/ST6/06868] (SD) and by the European Union from the
European Social Fund [UDA—POKL.04.01.01—00—106/09] (A.D.).

Conﬂict of Interest: none declared.

 

2577

ﬁm'spzumol‘pmﬂo'sopeuuopnotq/ﬁdnq

S.Deorowicz et al.

 

REFERENCES

The 1000 Genome Project Consortium. (2012) An integrated map of genetic
variation from 1092 human genomes. Nature, 491, 5&65.

Ball,M.P. et al. (2012) A public resource facilitating clinical use of genomes.
Proc. Natl Acad. Sci. USA, 109, 1192(711927.

Bonﬁeld,J.K. and Mahoney,M.V. (2013) Compression of FASTQ and SAM format
sequencing data. PLoS One, 8, e59190.

Cox,A.J. et al. (2012) Large—scale compression of genomic sequence databases with
the Burrows—Wheeler transform. Bioinformatics“, 28, 141?]419.

Cao,M.D. et al. (2007) A simple statistical algorithm for biological sequence com—
pression. In: Proceedings“ of the Data Compression Conference IEEE Computer
Society Press. IEEE Computer Society, Washington, DC, USA, p. 4352.

Christley,S. et al. (2009) Human genomes as email attachments. Bioinformatics“, 25,
2747275.

Claude,F. et al. (2010) Compressed q—gram indexing for highly repetitive biological
sequences. In: Proceedings“ of the 10th IEEE Conference on Bioinformatics“ and
Bioengineering. Philadelphia, Pennsylvania, USA, pp. 8(791.

Danecek,P. et al. (2011) The variant call format and VCFtools. Bioinformatics“, 27,
215(72158.

Deorowicz,S. and Grabowski,S. (2011) Robust relative compression of genomes
with random access. Bioinformatics“, 27, 297972986.

Do,H.H. et al. (2012) Fast relative Lempel—Ziv self—index for similar sequences. In:
Snoeyink,J. et al. (ed.) Proceedings“ of the Joint International Conference
on Frontiers in Algorithmics“ and Algorithmic Aspects in Information and
Management ( FA W—AAIM ). Springer, Beijing, China, pp. 2917302.

Gagie,T. et al. (2012) A faster grammar—based self—index. In: Dediu,A.H. and Martin—
Vide,C. (eds) Proceedings“ of the 6th International Conference on Language and
Automata Theory and Applications“. Springer, A Coruﬁa, Spain, pp. 24(7251.

Gagie,T. et al. (2011) Faster approximate pattern matching in compressed repetitive
texts. In: Asano,T. et al. (ed.) Proceedings“ of the 22nd International Symposium
on Algorithms“ and Computation. Springer, Yokohama, Japan, pp. 653%62.

Hach,F. et al. (2012) SCALCE: boosting sequence compression algorithms using
locally consistent encoding. Bioinformatics“, 28, 305173057.

Jones,D.C. et al. (2012) Compression of next—generation sequencing reads aided by
highly efﬁcient de novo assembly. Nucleic Acids“ Res., 40, e171.

Kreft,S. and Navarro,G. (2013) On compressing and indexing repetitive sequences.
Theor. Comput. Sci., 483, 1157133.

Kuruppu,S. et al. (2011) Optimized relative Lempel—Ziv compression of genomes.
In: Reynolds,M. (ed.) Proceedings“ of the ACSC Australasian Computer
Science Conference. Australian Computer Society, Inc., Sydney, Australia,
pp. 91798.

Levy,S. et al. (2007) The diploid genome sequence of an individual human. PLoS
Biol., 5, e254.

Makinen,V. et al. (2010) Storage and retrieval of highly repetitive sequence
collections. J. Comput. Biol., 17, 2817308.

Manzini,G. and Rastero,M. (2004) A simple and fast DNA compressor. Software
Pract. Ex., 34, 139771411.

Pavlichin,D. et al. (2013) The human genome contracts again. Bioinformatics, 29,
219972202.

Pinho,A.J. et al. (2011) On the representability of complete genomes by multiple
competing ﬁnite—context (Markov) models. PLoS One, 6, e21588.

Pinho,A.J. et al. (2012) GReEn: a tool for efﬁcient compression of genome
resequencing data. Nucleic Acids“ Res., 40, e27.

Popitsch,N. and von Haeseler,A. (2013) NGC: lossless and lossy compression of
aligned high—throughput sequencing data. Nucleic Acids“ Res., 41, e27.

Qiao,D. et al. (2012) Handling the data management needs of high—throughput
sequencing data: SpeedGene, a compression algorithm for the efﬁcient storage
of genetic data. BMC Bioinformatics“, 13, 100.

Salomon,D. and Motta,G. (2010) Handbook of data compression. Springer, London.

Storer,J.A. and Szymanski,T.G. (1982) Data compression via textual substitution.
J. ACM, 29, 9287951.

Wandelt,S. and Leser,U. (2012) Adaptive efﬁcient compression of genomes.
Algorithms“ Mol. Biol., 7, 30.

 

2578

ﬁm'spzumol‘pmﬂo'sopauuopnotq/pdnq

