BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

H. Wu et al.

 

because more reads are generated from longer transcripts
(Oshlack and Wakeﬁeld, 2009). Further, as coverage depends
on sequencing efﬁciency as well as expression level, genes with
modest counts are not necessarily expressed at low levels. Thus,
these may still be of interest even if we want to focus on genes
that are above a certain level of expression.

Another issue often overlooked in existing methods of sample
size determination for DE experiments is the wide application of
empirical Bayes approach in DE detection (Anders and Huber,
2010; Robinson et al., 2010; Smyth, 2004; Tusher et al., 2001; Wu
et al., 2013). Because of the limited sample size in many experi—
ments, the gene—speciﬁc biological variance is often estimated
with some shrinkage by borrowing strength across genes. This
helps stabilizing the variance estimates and leads to better rank—
ing of true DE genes, but in the meantime also creates
dependency among genes, which affects the validity of some
error—control procedures in multiple testing. Though all methods
report type I error (either as P—value, FDR/q—value or both), the
type I error may be computed from a parametric test of which
assumptions are not all met, and the reported FDR is often
obtained via simple conversion from nominal P—values using
BenjaminiiHochberg methods (Anders and Huber, 2010;
Robinson et al., 2010). The resulting nominal error rate can be
rather different from actual error rate (Wu et al., 2013).

Finally, the ﬂexibility of sequencing experiments gives scien—
tists more freedom in experimental design: for the same amount
of sequencing, one may choose to seek deeper coverage of a small
collection of samples, or to obtain more samples with modest
coverage. This is an additional factor not encountered in
microarrays.

There are several methods for calculating the sample size for
RNA—seq data in recent literature. These include methods for
single—gene differential expression analysis based on likelihood
ratio or Wald test (Fang and Cui, 2011), or on score test from
negative binomial model (Hart et al., 2013). These methods,
however, are not directly applicable to simultaneously testing
thousands of genes profiled from one RNA—seq experiment be—
cause they do not come with a procedure that deals with multiple
comparisons. Li et al. (2013b) proposed an analytical method
based on Poisson model to determine sample size for both
single gene and multiple gene comparisons with adjustment for
FDR. This method has been further extended to negative bino—
mial model in Li et al. (2013a). However, to make calculations
attainable, the authors suggested setting a common value for
parameters including fold change, dispersion, and average read
count for all the genes. In reality, these parameters vary a lot
between genes, and this method is not ﬂexible enough to fully
capture the complex characteristics of RNA—seq data. Although
one can choose common conservative values for these param—
eters, it will overestimate the sample size and increase the cost of
the experiments.

We argue, because of the complexities of RNA—seq experi—
ments, it is no longer feasible to rely on one simple power
versus sample size curve while treating all other factors as ﬁxed
input and holding strong assumptions such as exchangeability
between genes and equating nominal error rate as actual error
rate. We advocate prospective power evaluation in the context of
RNA—seq, i.e. evaluating power in a comprehensive manner
under various scenarios of sample size and sequencing depth.

We use the word ‘prospective’ to emphasize our choice to
assess and visualize power in multiple forms and maintain its
high—dimensional nature, instead of specifying a ﬁxed level of
one particular form of power to determine the sample size. We
demonstrate that, in addition to the sample size and the other
usual suspects in power analysis (namely, effect size and within—
group variance), there are other factors (such as the distribution
of mean expression level) and other choices (such as sequencing
depth and gene ﬁltering) that inﬂuence the power of DE detec—
tion. We propose a simulation—based power evaluation, as the
accumulation of RNA—seq data allows us to construct in silico
datasets that well resemble real RNA—seq data, and the increas—
ing computing efﬁciency allows us to evaluate actual error rate.
Moreover, we demonstrate that conditional power, i.e. power
stratified by coverage or biological variation, is more informative
than overall (marginal) power in both experimental design and
analysis plan.

2 METHODS

We propose to evaluate how experimental design affects power com-
pletely based on simulation. Our proposed method consists of two sep-
arate components. First, we provide a ﬂexible semi-parametric simulation
module that generates count tables resembling actual RNA-seq data in
many aspects: marginal distribution of average expression, marginal dis-
tribution of biological dispersion, conditional relationship between dis-
persion and expression level, etc. Then, in a separate component, we
evaluate power and error rates on the simulated dataset, emphasizing
the concepts of stratiﬁed power and false discovery cost (FDC). We
keep these two components separate so a user may choose an entirely
different simulation scheme and still apply the same power assessment
tools.

2.1 Data generation

We provide a negative binomial model-based simulation scheme, for
using the power evaluation part of our method. The negative binomial
model is the most widely used model for RNA-seq count data for its
simplicity, ﬂexibility and interpretability. It can be seen as a gamma-
Poisson mixture, with the gamma layer capturing biological variation
conditioning on covariates, and the Poisson distribution capturing the
sequencing counting error. Let Ygi be the observed count for gene g,
replicate i, we assume that Ygi~NB(s,»p,g, ¢g). Here, Mg and Q, represent
the mean and dispersion for gene g, respectively. 3,- represents the normal-
izing factor, such as the library size. We begin by simulating a baseline
expression level Hg for each gene. This can be drawn from a parametric
distribution, or re-sampled non-parametrically using the empirical aver-
age expressions estimated from an existing dataset. Unless there is a good
justiﬁcation for the choice of a parametric distribution of a transcrip-
tome, we recommend re-sampling, as coverage is important in detecting
DE and the dynamic range for RNA-seq is rather wide. This is one major
difference from microarray, where often the mean expression (in log
scale) can be simulated as 0 without loss of generality, as it does not
affect DE detection.

Next we simulate a dispersion parameter Q, that captures a gene’s
biological variation. This dispersion parameter, referred to as the squared
biological coefﬁcient of variance (Anders and Huber, 2010; McCarthy
et al., 2012), is closely related to the standard deviation in log transformed
microarray data, which represents the biological variation of gene expres-
sion between replicates (Wu et al., 2013). Again, the parameter Q, can be
drawn from a parametric distribution or re-sampled based on empirical
sample dispersions from a real dataset of the user’s choice. An important
option here is provided: Q, can be drawn independently, or a functional

 

234

ﬁm'spzumofpmjxo'sopnuuopnorq/ﬁdnq

PROPER

 

relationship between Q, and Mg can be preserved as suggested by previous
data. Though there is no simple biological explanation for the depend-
ence between biological variation and mean expression, this trend has
been reported in many studies (Anders and Huber, 2010; Robinson
et al., 2010).

In the third step, we set the effect sizes. This is the most difﬁcult as-
sumption to make, as we rarely know the amount of differential expres-
sion that is biologically relevant, nor do we know the proportion of genes
with that level of difference. In the literature, several settings have been
used. The ﬁrst is a mixture: let zg be the indicator that gene g is differ-
entially expressed, the proportion of gene with DE is P(zg = 1) = in. We
have the effect size ﬁg satisfying ﬁglzx:0=0 and ﬁglzx:1~N(0,az). As
there is a point mass at zero, i.e. P(ﬁg=0)=l — in, we refer to this
as a zero-inﬂated normal distribution for ﬁg. Another option is to
allow ﬁglzxﬂ be uniform over a user-deﬁned range. Moreover, we can
also choose to investigate the power of detecting speciﬁc effect sizes, by
setting ﬁg at multiple constant levels with the greatest point mass at 0,
reﬂecting the general assumption that DE is present in only a small subset
of genes in most experiments.

Many genes with 2,; = 1 are by deﬁnition differentially expressed, but
may not be biologically interesting, as lﬁgl is small or even essentially 0.
We should expect little power detecting these genes. Thus, we may be
interested in deﬁning DE of interestian indicator 2; =1 if lﬁgl >A or
|/3g|/\/¢Tg >A, and investigate the power of detecting these genes. We let
the user decide the ‘meaningful effect size’. The user can also simply
provide a vector of )3, with paring indicators whether each ﬁg is con-
sidered a true positive.

2.2 DE detection

After generating the simulated read counts, existing software developed
for count-based RNA-seq is applied to detect DE genes. We implemented
interface for calling edgeR (Robinson et al., 2010), DESeq (Anders and
Huber, 2010) and DSS (Wu et al., 2013). Users can deﬁne other DE
detection methods and plug into the procedure. Each method reports
test statistics, P-values and FDR for all genes. These results are used
for downstream power assessment. The simulations (data generation
and DE detection) are performed under different sample sizes (number
of replicates in each biological condition). Each simulation is repeated for
a number of times, and the power assessments are averaged to provide the
ﬁnal results.

2.3 Power assessment and visualization

We consider genes that can potentially fall into three categories: (i) non-
DE where the null hypothesis ﬁg = 0 is true; (ii) with low DE that is not
biologically relevant; and (iii) with DE high enough that we are most
interested in identifying. The total number of genes in each categories
are represented by GO, Gm and GM, respectively. Let Dg be the decision on
gene g (g= 1, . . . , G), with Dg = 1 declaring DE (discovery) and Dg = 0
declaring non-DE, we summarize the decisions in Table 1, where V rep-
resents the total number of type I errors. Though any gene with ﬁg 7A 0 is
differentially expressed, thus failing to discover it is a type II error, we
argue that we care less about a gene with low DE that does not achieve
a user-deﬁned relevance level. The power we care about is the ability of
detecting genes in the third category, i.e. power associated with 5],. We
call it the targeted power. In the rest of this article, we will focus on the
assessment of the targeted power. In the software, we provide options to
deﬁne biologically interesting genes by lﬁgl or lﬁgl/M. For illustration
purpose, we focus only on results from the former deﬁnition throughout
this manuscript.

The family-wise type I error rate is P( V> 0), and the FDR is E[V/R].
We introduce a concept that we referred to as FDC, deﬁned as E[V/Sb].
The interpretation is straightforward: for every discovery that we care
about (2; =1 when Dg = 1), the expected number of false discoveries.

Table 1. DE detection and potential errors

 

 

 

zg 2; Discovery? Total
(Dg = 1) (Dg = 0)
,Bg = 0 0 V G0 — V G0
0< lﬁgl S A 1 0 5.1 G1“ — 5.1 G1“
lﬁgl >A 1 1 S], G11; — Sb Glb
Total R G — R G

 

Thus, FDC represents the cost of false discoveries we expect to identify
each true discovery we aim for. We are still testing the null ﬁg = 0. If we
called a gene with 0< lﬁgl 5 A as DE, it is not a false discovery, but
simply that we would not mind as much if we fail to discover it.

Statistical power in gene expression experiments has complex mean-
ings. The family-wise power’, that is the probability of detecting all true
DE genes, can be small in most studies, especially when many DE genes
have small magnitude of differences or low baseline expression levels.
This means that P(Zg ngg = 2g 2g) = P(S,, = G],,&Su = Gm) is often
small. However, it is rarely the goal to detect every single DE gene in
an RNA-seq experiment. We may be interested in the proportion of true
DE genes detected: when there are a small set of true DE genes, we may
wish to detect the majority of these. If the tests for DE are independent,
the expected proportion is the same as average power:
E[Zg Zng/Zg Zg] :  + Sl>)/(Glu + Glb)]-

In other cases, especially in hypothesis-generating experiments, we may
simply aim for a number of leads, even if that is a small proportion of all
DE genes. That is, the power of interest is E[Zg ngg], or the expected
number (as opposed to proportion) of true discoveries. Finally, as men-
tioned above, regardless of proportion or absolute number of discoveries,
we may only care about the power of detecting DE of a certain size, i.e. of
a medical or a biological relevance.

With these considerations, we advocate comprehensive power evalu-
ation by visualizing its relationship with a number of factors, including
but not limited to sample size, instead of pre-specifying a desired power
level, as we recognize that power in a high-throughput setting could have
more than one deﬁnition. We refer to this as prospective power evaluation,
in contrast to sample size determination with preselected power definition
and level. We compute the following quantities from each simulation
when discoveries are made with a user-deﬁned type I error control (at
a nominal P-value or FDR/q-value) and a user-deﬁned magnitude of
relevant effect size A. We report the averages of these quantities from a
number of simulations as our empirical values for error rates and power.

0 Empirical marginal type I error rate:
Zoga — 2g)/(G — 22g): V/Go
g g

0 Empirical marginal FDR:
2g Dg(1 — Zg)
2g DE

0 Empirical marginal targeted power: the proportion of biologically
meaningful DE genes detected at the nominal type I error

ZDgzz/Zz;
g g

= V/R

If one is interested in detecting DE of any size, deﬁning A = 0 will reduce
the targeted power to the classical deﬁnition of average power.

 

235

ﬁm'spzumofpmjxo'sopnuuopnorq/ﬁdnq

H. Wu et al.

 

0 Empirical marginal FDC:
ZDEU _ 29/: DgZ;
g g

0 Empirical stratiﬁed targeted power by coverage: for genes with aver-
age coverage: (T’g = Z, Ygi/N) in the stratum (aj,aj+1]

* *
Z Dgzg/ Z 2g
g:a/<Yx§a/+l g:a/<Yx§a/+l

0 Empirical stratiﬁed targeted power by dispersion: for genes with dis-
persions in the stratum (bj,bj+1]

Z Dgzz/ Z 

g:b/<03N§b/+l g:b/<$>N§b/+l

Empirical stratiﬁed FDC, FDR and type I error rate by coverage or
dispersion: similar to the deﬁnition of the stratiﬁed targeted power.

For experimental design, we provide a comprehensive view of the stat-
istical power as a function of not only sample size, but also coverage,
biological dispersion, proportion and magnitude of DE. We also provide
the empirical error rates to alert the user that the nominal type I error,
either in the form of raw P-value or FDR, may not be valid. Our stra-
tiﬁed view of both type I error and power shows the gain and loss in
different subsets of a transcriptome, aiding the investigators in both ex-
perimental design (choosing number of samples and sequencing depth,
for example) and analysis plan (setting ﬁlters and choosing a reasonable
control for error rate).

2.4 Power reassessment

One challenge in the sample size determination in high-throughput ex-
periment is the choice of type I error control. Classical sample size cal-
culation often assumes a valid test is available, which is often the case
when asymptotic properties can be assumed in large samples. In high-
throughput experiments with multiple testing, FDR is often a preferred
choice over family-wise type I error for its balance between false positive
and power. However, we face a 2-fold difﬁculty here. First, there is no
conventional guidance for FDR cutoff, as the level of acceptable FDR
often depends on the number of discoveries. With 10 total discoveries, a
20% FDR may be reasonable, but this may be considered too high if the
total discoveries reach 100. Second, many DE analysis methods report an
FDR that is a rough estimate and relies on assumptions such as inde-
pendence and exchangeability. Thus, the reported FDR may not reﬂect
the actual FDR. Therefore, we often want to evaluate power at several
nominal FDR levels, and assess the power as well as the validity of error
control.

For each simulation study, we thus keep all settings for data gener-
ation, and save the necessary simulation results, including the nominal P-
value, reported FDR and observed average expression and dispersion.
When we would like to reassess the power under a different choice of type
I error control, desired effect size, or choose a different stratiﬁcation of
genes, we do not need to rerun the entire simulation. That greatly reduces
the computational burden.

2.5 Implementation

We implemented the proposed methods in an open-source R package
PROPER, standing for PROspective Power Evaluation for RNAseq.
The software is currently available at

http:/ /web1 .sph.emory.edu/users/hwu30/PROPER.html, and being pre-
pared to submit to Bioconductor (Gentleman et al., 2004). A vignette is
distributed with the package, which contains detailed instruction and
examples of using the package, interpreting the results and an example
of sample size justiﬁcation for grant proposal.

The computational efﬁciency of PROPER depends on the DE detec-
tion software and the scale of the simulation. For the ones presented in
Section 3 (50 000 genes, ﬁve different sample sizes and using edgeR), each
simulation takes around 10s on a MacPro laptop with 2.7Ghz i7 CPU
and 16G RAM, which translates to 17min for 100 simulations.

3 RESULTS
3.1 Simulation setup

To illustrate the power evaluation in various forms, we generated
results using two public datasets as our basis for simulation. The
Cheung data (Cheung et al., 2010) quantiﬁes the expressions of
lymphoblastoid cell lines from 41 CEU individuals in
International HapMap Project (The International HapMap
Consortium, 2003). The samples are from unrelated individuals,
and the expressions show large biological variations overall. The
Bottomly data (Bottomly et al., 2011) includes 21 striatum sam—
ples from two strains of inbred mice (C57BL/6J and DBA/2J).
The expressions in this dataset show much smaller biological
variations. These two datasets, one involving a random sample
from a human population and the other involving animals from
model organisms, represent experiments with large and small
biological variations. Most of the other datasets we examined,
including almost all datasets on reCount (Frazee et al., 2011) and
80% of experiments in Barcode (McCall et al., 2011), have bio—
logical variation that falls between these two examples.

In all simulations, we use 50 000 genes and assume 5% of them
are DE. For each simulation, the read counts are generated ac—
cording to the steps described in Section 2. To be specific, the
baseline expression level Mg and the dispersion parameter (ﬁg are
resampled independently from the real data. The effect size is set
to be 0 for non—DE genes, and is randomly sampled from normal
distribution N(0, 1.52) for DE genes. This choice of the effect size
is only for illustration purpose. The software provides an option
for user—deﬁned effect sizes. In practice, we recommend users
obtain effect sizes from historical data under similar biological
context.

Under each simulation scenario, we evaluate power at repli—
cate numbers 2, 3, 5, 7 and 10. We apply edgeR for DE detection
to identify DE genes, then compare the results with the truth to
evaluate both type I error control and various metrics of power.
The results presented below are averaged >100 simulations. The
aim of our method is not to compare performance of different
DE analysis methods, which often depends on simulation setting.
We choose edgeR as the illustrative method for its popularity and
speed. For all results presented below, we use A=0.5 to deﬁne
biologically meaningful DE genes.

3.2 Simulation results with independent mean and
dispersion

As an overall summary, we present a table that compares the
marginal targeted power as well as the actual type I error rate at
the user—speciﬁed control of nominal type I error. The measure—
ment of power in the form of the proportion of true DE genes
identiﬁed and the average number of DE genes identiﬁed are
both provided.

Table 2A is an example using Cheung data as the source for
simulation, at a nominal FDR at 0.1. As expected, the targeted

 

236

ﬁm'spzumofpmjxo'sopnuuopnorq/ﬁdnq

PROPER

 

Table 2. Marginal targeted power analysis results from simulations when
DE are declared with nominal FDR 0.1

 

N FDRn FDRo power ETD ﬁFD FDC

 

C heung data

2 0.10 0.59 0.17 66.02 95.93 1.45
3 0.10 0.48 0.27 107.00 100.75 0.94
5 0.10 0.31 0.41 165.26 73.73 0.45
7 0.10 0.22 0.49 205.10 58.19 0.28
10 0.10 0.15 0.58 244.62 45.06 0.18
Bottomly data
2 0.10 0.28 0.53 343.70 136.64 0.40
3 0.10 0.24 0.62 407.79 130.35 0.32
5 0.10 0.15 0.72 482.79 85.16 0.18
7 0.10 0.11 0.77 519.17 67.61 0.13
10 0.10 0.08 0.80 547.04 53.71 0.10

 

N: number of replicates in each group. FDRn: nominal FDR. FDRo: observed
FDR. ETD: average number of true discoveries. ﬁm: average number of false
discoveries.

power increases with sample size. In a classical multiple testing
situation with exchangeable tests, one would expect that a valid
control of FDR would mean that the ratio of true and false
discoveries is maintained as sample size increases, and the in—
crease in targeted power results from more true discoveries.
However, Table 2 shows that the actual FDR is quite different
from the nominal FDR. Under the same nominal FDR control,
we obtain less false discoveries and more true discoveries as
sample size increases, thus the actual FDR decreases. Overall,
the nominal FDR mostly underestimates the true FDR in our
simulation settings. The FDC also decreases with larger sample
size, meaning that it is cheaper to detect DE genes when there are
more replicates. Table 2B shows the result from a similar simu—
lation based on Bottomly data, which are from inbred animals
with much smaller biological variances. Results show that the
DE detection is easier in these data: the powers are considerably
higher and the FDCs are lower under the same sample size.
These imply that compared with the Cheung data, it would re—
quire less replicates here to achieve the same level of power.

FDR is often a preferred measure of type I error over the raw
P—value, owing to concerns of excessive multiple testing and the
over—conservativeness of Bonferroni correction. However, there
is no conventional cutoff of FDR as the classical signiﬁcance
level of 005/001 for P—values. An acceptable FDR may
depend on the number of discoveries. Thus, we let the user
reevaluate the targeted power at a different nominal FDR
level. The summary tables for nominal FDR at 0.2 are provided
in the Supplementary Table Sl.

At the ﬁrst glance, the targeted power from the Cheung data—
based simulation appears rather low: only 0.58, when there are
10 replicates in each group. This is disappointing especially when
we observe that the actual FDR can be higher than the nominal
FDR. However, we strongly recommend viewing the stratiﬁed
targeted power as shown in Figure 1. Here the genes are stratiﬁed
by the average counts. Clearly larger sample size leads to better
power at all strata, as expected. But for all sample sizes, including

 

 

 

E": "'1‘

 

 

 

 

 

 

 

Fig. 1. Top: Histogram of genes stratified by average counts. Open histo-
gram is for total number of genes and blue histogram is the counts of DE
genes. Bottom: Targeted power stratiﬁed by average counts, under
different sample sizes. Results are averaged from 100 simulations based
on Cheung data. n: the number of replicates in each group in a two-class
comparison

11 = 10, there is little power for the genes with low coverage
(average counts up to 10). This is not surprising because even
if there is true DE, when the expression level is so low (that is, at
current sequencing depth, only a few reads from the gene are
sequenced), the Poisson counting error shadows the real biolo—
gical difference and we do not have high probabilities of detect—
ing these DE genes while controlling for FDR. When the average
counts become moderately large (average counts greater than
10), the gain of targeted power is signiﬁcant with increased
sample size. For example, the stratified targeted power for
genes with read counts between 10 and 20 increases from 0.33
to 0.73 when the number of replicates increases from 3 to 10.
Moreover, for this simulation, the stratified targeted power in—
creases sharply past the first stratum, but further increases are
modest after the average count goes beyond 20.

When the average targeted power is the goal and the stratified
targeted power varies a lot as seen in Figure l, we may decide to
simply ﬁlter out genes with low counts: we give up the possibility
of detecting DE in this stratum knowing there is little power, but
at the same time we avoid making any false discovery as well.
For the rest of the genes, we can achieve a much higher marginal
power, as seen in Figure 2. It shows that if one discards genes
with <10 average counts, the marginal targeted power will in—
crease to 0.8 (from 0.58) when the sample size is 10, using FDR
<0.l to deﬁne DE genes. The signiﬁcant gains in power after
ﬁltering are achieved from two sources: (i) reduced size of the

 

237

ﬁm'spzumofpmjxo'sopnuuopnorq/ﬁdnq

H. Wu et aI.

 

 

 

 

 

 

 

 

 

Fig. 2. Marginal targeted power versus sample sizes, with and without
filtering out genes with average counts lower than 10, averaged from 100
simulations based on ('heang data

true positive set, and (ii) reduced number of simultaneous tests.
These simulation results demonstrate that filtering genes with
low counts make it easier to achieve signiﬁcance for the remain-
ing genes. The software package allows users to specify strata.
Additional results from using a different set of strata are pro-
vided in the Supplementary Section S6 and Supplementary
Figure S7. We recommend users explore the results under differ-
ent strata to choose proper stratiﬁcation and filtering strategy.

To maximize our gain, that is, to obtain the most true discov-
eries with the same cost of false discovery, we recommend view-
ing “false discovery cost’ plot (Fig. 3) for the choice of filtering.
The FDC has a simple interpretation: at the current cutoff for
declaring DE, the expected number of false discovery (cost) for
each true discovery. For example, Figure 3 shows that when
there are three replicates in each group, to detect every true posi-
tive gene with average counts between 0710, one can expect to
detect 1.3 falseipositive findings. Overall, these results show that
using more replicates will decrease FDC for all strata, and genes
with greater expression levels have lower associated FDC (so it is
‘cheaper’ to detect the highly expressed DE genes).

The stratified visualization of targeted power, as seen in
Figures 1 and 3 above, sends a rather different message than
the marginal targeted power. This demonstrates that we should
not consider power as a single numeric value. We recommend
viewing several other ﬁgures simultaneously, especially when
the power we target is not the average power. For example,
in hypothesis-generating studies, our goal may be identifying a
number of leads for further study. In this case, the number of
discoveries, rather than the proportion of discoveries, is more
important. We show the average number of true discoveries in
each stratum of average gene counts in Figure 4. Results based
on the two different targeted power deﬁnitions, as seen in Figures
1 and 4, are seemingly contradictory at the ﬁrst glance. This is
because genes are not evenly distributed across the strata. The
actual number of discoveries is a product of the total number of
true DE genes and the average power, thus a higher value in
either quantity can increase the number of discoveries. Though

 

 

 

 

 

 

 

 

 

 

Fig. 3. False discovery cost stratified by average counts, under different
sample sizes, averaged from 100 simulations based on ('heang data

 

 

 

 

 

 

 

 

 

 

Fig. 4. Number of true discoveries stratified by average counts, under
different sample sizes, averaged from 100 simulations based on ('heang
data

the sensitivity of a DE detection is low in certain strata, a small
fraction of a big collection of genes can still yield a considerable
number. For example, in Figure 1, genes in the first stratum has
power several folds lower than other strata, but there are >4000
genes in this stratum, 200 of which have DE. Thus a small sen-
sitivity at ~25% can still lead to >40 discoveries when n : 10, as
shown in Figure 4. RNA-seq data reflect the wide dynamic range
of expression levels across the transcriptome, and often a large
fraction of genes are covered with modest counts. Thus, it is
common that power in absolute number of discoveries versus
power in fraction of true DEs discovered may send different
messages. Whether one should focus on the fraction of true
DE genes (Figure l) or the actual number of true DE genes
detected (Fig. 4) depends on the purpose of the experiment. If
one aims to recover most of the transcriptomic response to a
treatment, the average power is a better guide. If one aims to
identify a number of hits in a hypothesis generating exercise to

 

238

/810'SIEumo_fpm}Xo'sotJEmJOJUtotW/zduq

PROPER

 

Table 3. Effect of changing sequencing depth on marginal targeted power
in simulations based on Cheung data at nominal FDR 0.1

 

 

Relative coverage 2 reps 3 reps 5 reps 7 reps 10 reps
0.2 0.13 0.22 0.34 0.43 0.51
0.5 0.15 0.25 0.38 0.47 0.56
1 0.17 0.27 0.42 0.49 0.58
2 0.19 0.30 0.45 0.54 0.62
5 0.22 0.34 0.49 0.58 0.66
10 0.24 0.36 0.52 0.61 0.69

 

lead further study, the actual number of true DE genes identiﬁed
is more useful. We leave this judgement to the users.

Realizing that genes with low coverage have low power of DE
detection and high FDC, we may consider increasing the sequen—
cing depth as an alternative to increasing the sample size. Using
the same amount of resources (total number of sequencing
reads), which choice benefits us more? We provide a table that
compares the targeted power at various sequencing depth, so the
user can decide on a desirable combination of sequencing depth
and sample size. Table 3 shows the result based on Cheang data
at deeper and shallower sequencing. In this example, increasing
the sequencing depth does not help as much as increasing the
number of replicates. Speciﬁcally, using ﬁve replicates in each
group and double the coverage depth in Cheang data produces
marginal targeted power of 0.45. Using the same number of total
reads, one can double the sample size (to 10 replicates per
group), and use the same coverage depth as in Cheang data.
That will provide a marginal targeted power of 0.58, greatly im—
proved compared with the other strategy. These results agree
with the conclusion in Liu et al. (2014), i.e. using more replicate
is more beneﬁcial than sequencing deeper. In real applications,
we suggest the users reproduce the table based on their simula—
tion choices, especially when the simulation is based on a differ—
ent dataset, as both the baseline expression level and the
distribution of DE magnitudes can have strong impact on stat—
istical power.

All figures presented above are based on the Cheang data. The
same set of ﬁgures for the Bottomly data is provided in the
Supplementary Materials (Supplementary Fig. S2). Owing to
smaller biological variations in inbred animals, the DE detection
is easier in Bottomly data under similar effect sizes and sequen—
cing depths. We observe, as expected, higher powers and lower
FDCs for DE detection in Bottomly data. The general conclu—
sions from the analyses are otherwise consistent with the Cheang
data.

For all results presented above, we use A=0.5 to define bio—
logically meaningful DE genes. Users may choose different A
values to deﬁne DE genes. We present a set of results from
using A = 1 in Supplementary Materials (Supplementary Table
S2, Supplementary Figs S3 and S4). As expected, greater values
of A lead to better targeted power because the effect sizes are
larger. On the other hand, this also leads to decreased number of
true discoveries, which could be undesirable if the primary goal
of DE detection is to generate a set of target genes. It is advisable

 

 

Fig. 5. Simulation results based on the Cheung data, with dispersioni
mean dependency

for users to try different options based on these simulation results
and select proper experimental design and analysis plan.

We also provide functionality for computing the power—related
metrics stratiﬁed by biological coefﬁcient of variation (through
dispersion). Those results are provided in the Supplementary
materials (Supplementary Figs S5 and S6). In general, genes
with greater dispersion have lower power and higher FDC,
and larger sample size helps DE detection.

Furthermore, we compared the power assessment results from
PROPER and ssize . fdr, the R package based on method by
Liu and Hwang (2007) for microarray data. In general, we found
that ssize . fdr over—estimates power (Supplementary Section
7 and Supplementary Fig. S8). That is because ssi ze . fdr does
not take into account the sequencing depth information, and
assumes that the power of detecting DE genes only depends on
the effect sizes. The comparison demonstrate that power calcu—
lation method developed for microarray data is not applicable
for RNA—seq data and may lead to erroneous results.

3.3 Simulation results with dispersion—mean dependency

Although its biological explanation remains elusive, dependency
between (1),, and Mg has been reported in many studies (Anders
and Huber, 2010; Robinson et al., 2010), with low expression
genes often associated with higher dispersion. We performed
simulation when the dependence of dispersion and mean expres—
sion is preserved based on the Cheang data. To be speciﬁc, we
ﬁrst estimate Mg and (1),, for all genes, then sample Mg and (1),, in
pairs, thus their correlation is preserved in simulation. There is a
strong negative correlation between Mg and (ﬁg, e.g. genes with
higher expressions show lower biological variations.

Figure 5 shows the power analysis results. Both targeted
power and FDC increase sharply as average count increases.
Compared with the results in Section 3.2, the dependence of
targeted power and FDC on average count is stronger: the
sharp increase retains even after the average count goes
beyond 20. This is because genes with lower counts now suffer

 

239

ﬁm'spumot‘pmjxo'sopauuopnorq/ﬁdnq

H. Wu et al.

 

from higher dispersion, in addition to under higher inﬂuence of
Poisson counting error. In contrast, highly expressed genes bene—
ﬁt from lower dispersion. In situations like these, filtering out
genes with low counts may provide even more beneﬁt. Moreover,
in the presence of dispersionimean dependence, it will be even
more difﬁcult to derive a sample size formula analytically, so the
proposed method will be more important and practically useful.

4 DSCUSSON

Statistical power and sample size determination are the
most common questions we face in experimental design.
In high—throughput experiments that involve a large number of
in—exchangeable tests, statistical power is not as tractable as in
classical hypothesis testing. We demonstrate that in a RNA—seq
study, more factors affect the sample size determination in add—
ition to the effect size and variance, including the distribution of
the baseline expression level (what proportion of genes have high
coverage in the sequencing), the distribution of the biological
variation and the proportion of genes having DE. Asking a
biologist to provide speciﬁc numbers for all the above factors,
and/or to conﬁrm that a particular parametric distribution is
reasonable for some parameter, seems unrealistic. On the other
hand, assuming the overall behavior of a factor to resemble that
in some existing dataset eases the communication. Thus, we
prefer semi—parametric simulation settings as described in
Section 2.

The definition of power itself can vary in RNA—seq experi—
ments: we may be interested in average marginal power as the
proportion of all DE genes identiﬁed, or targeted power as the
proportion of DE genes identiﬁed from a subset of genes, or we
may be interested in the number instead of proportion of DE
genes identified. For these reasons, we advocate sample size de—
cision based on a comprehensive evaluation of statistical power
as well as actual type I error, over a range of sample sizes, based
on simulation studies. We refer to this as prospective power
evaluation, as compared with ﬁxing one set of assumptions on
effect sizes/type I error control/expression level/sequencing depth
and then compute a minimum sample size to achieve a certain
level of power, for a particular type of power. The user visualizes
the relationship between various types of power and sample size,
expression level and biological variation, and understands the
cost of false discovery in different strata of genes. The power
evaluation thus assist the decision on sequencing depth, analysis
plan (ﬁltering or not, choice of nominal error rate), and then
based on these decisions, the user can select a sample size that
provides acceptable power.

Filtering certainly comes with sacrifice: we discard the power
completely on the genes we ﬁlter out. But the power evaluation
allows us an informative decision: we would know how much
power we give up, and make this decision before real data are
analyzed, so we reduce the number of tests, hence not having to
adjust for the tests never performed.

The fact that statistical power depends on the baseline expres—
sion level and the dispersion level has several consequences. The
ﬁrst consequence is that power for A = 0 (i.e., lﬂgl >0) is often
biased toward highly expressed genes. Sometimes it may be bene—
ﬁcial to ﬁlter out genes with counts too low, as discussed above.
The second consequence is that simulation results based on one

RNA—seq dataset may not be generalizable to experiments invol—
ving another tissue/cell type with a different expression distribu—
tion across genes. For this reason we provide options using
several public RNA—seq datasets as simulation sources. We
also let the user substitute with their choice of baseline
expression.

One way of increasing power is to increase sequencing depth.
This is apparent from the stratified power plot: when we se—
quence deeper, genes with average counts in lower strata will
move to higher strata and be associated with higher sensitivity
at the same type I error control. However, based on Figure 1,
there is a sharp increase of power when the genes average count
goes >10, but remains relatively ﬂat thereafter. If there are many
genes whose expression level is lower than but near 10, increasing
the sequencing depth may help, but there is little gain on DE
detection sensitivity for those genes that already have high
power. Thus, the impact of sequencing depth also depends on
the expression pattern of the transcriptome under study. If the
transcriptome consists of a smaller fraction of the genes with
similar level of expression, then with modest depth, most of
the genes may already reside in middle expression strata with
acceptable power.

Funding: H.W. was partially supported by the National Center
for Advancing Translational Sciences of the National Institutes
of Health under Award Number UL1TR000454. Z.W. was par—
tially supported by the National Science Foundation award
DB11054905 and the National Institutes of Health under
Award Number R01 GM067862.

Conﬂict of interest: none declared.

REFERENCES

Anders,S. and Huber,W. (2010) Differential expression analysis for sequence count
data. Genome Biol., 11, R106.

Bottomly,D. et a]. (2011) Evaluating gene expression in c57bl/6j and dba/2j mouse
striatum using RNA—seq and microarrays. PloS One, 6, e17820.

Cheung,V.G. et a]. (2010) Polymorphic cis—and trans—regulation of human gene
expression. PLoS Biol., 8, e1000480.

Djebali,S. et a]. (2012) Landscape of transcription in human cells. Nature, 489,
1017108.

Fang,Z. and Cui,X. (2011) Design and validation issues in RNA—seq experiments.
Brief. Bioinformatics, 12, 28(F287.

Frazee,A.C. et a]. (2011) Recount: a multi—experiment resource of analysis—ready
rna—seq gene count datasets. BMC Bioiiy’ormaties, 12, 449.

Gentleman,R.C. et a]. (2004) Bioconductor: open software development for com—
putational biology and bioinformatics. Genome Biol., 5, R80.

Hansen,K.D. et a]. (2011) Sequencing technology does not eliminate biological
variability. Nat. Biotechnol, 29, 5727573.

Hart,S.N. et a]. (2013) Calculating sample size estimates for RNA sequencing data.
J. Comput. Biol., 20, 97(F978.

Li,C.—I. et al. (2013a) Sample size calculation based on exact test for assessing dif—
ferential expression analysis in RNA—seq data. BMC Bioiiy’ormaties, 14, 357.

Li,C.—I. et al. (2013b) Sample size calculation for differential expression analysis of
RNA—seq data under poisson distribution. Int. J. Comput. Biol. Drug Des., 6,
3587375.

Liu,P. and Hwang,J.G. (2007) Quick calculation for sample size while controlling
false discovery rate with application to microarray analysis. Bioiiy’ormaties, 23,
7397746.

Liu,Y. et a]. (2014) RNA—seq differential expression studies: more sequence or more
replication? Bioiiy’ormaties, 30, 3017304.

McCall,M.N. et a]. (2011) The gene expression barcode: leveraging public data
repositories to begin cataloging the human and murine transcriptomes.
Nucle‘u? Acids Res, 39 (Suppl. 1), D10117D1015.

 

240

ﬁm'spumot‘pmjxo'sopnuuopnorq/ﬁdnq

PROPER

 

McCarthy,D.J. et a]. (2012) Differential expression analysis of multifactor RNA—seq
experiments with respect to biological variation. Nucleic Acids Res, 40,
428841297.

Mormzavi,A. et a]. (2008) Mapping and quantifying mammalian transcriptomes by
RNA-seq. Nat. Methods, 5, 6217628.

Oshlack,A. and Wakeﬁeld,M.J. (2009) Transcript length bias in RNA—seq data
confounds systems biology. Biol. Direct., 4, 14.

Robinson,M.D. et a]. (2010) edger: a bioconductor package for differential expres—
sion analysis of digital gene expression data. Bioinformatics, 26, 1397140.

Smyth,G.K. (2004) Linear models and empirical bayes methods for assessing dif—
ferential expression in microarray experiments. Stat. Appl Genet. Mol Biol, 3,
3.

The International HapMap Consortium. (2003) The international hapmap project.
Nature, 426, 7897796.

Tusher,V.G. et a]. (2001) Signiﬁcance analysis of microarrays applied to the ionizing
radiation response. Proc. Natl Acad. Sci. USA, 98, 51 1&5121.

Wu,H. et a]. (2013) A new shrinkage estimator for dispersion improves differential
expression detection in RNA—seq data. Biostatistics, 14, 2327243.

 

241

/3.IO'S[BIImO[p.IOJXO'SOIJBLUJOJIIIOIq/ﬂduq

