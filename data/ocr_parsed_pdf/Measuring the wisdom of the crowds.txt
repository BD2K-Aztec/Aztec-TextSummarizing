Bioinformatics, 3115), 2015, 745—752

doi: 10.1093/bioinformatics/btu715

Advance Access Publication Date: 29 October 2014
Original Paper

 

 

Data and text mining

Measuring the wisdom of the crowds in
network-based gene function inference

W. Verleyen, S. Ballouz and J. Gillis*

Stanley Institute for Cognitive Genomics, Cold Spring Harbor Laboratory, Woodbury, NY 11797, USA

*To whom correspondence should be addressed.
Associate Editor: Jonathan Wren

Received on 1 May 2014; revised on 13 October 2014; accepted on 23 October 2014

ABSTRACT

Motivation: Network—based gene function inference methods have proliferated in recent years, but
measurable progress remains elusive. We wished to better explore performance trends by control—
ling data and algorithm implementation, with a particular focus on the performance of aggregate
predictions.

Results: Hypothesizing that popular methods would perform well without hand—tuning, we used
well—characterized algorithms to produce verifiably ’untweaked’ results. We find that most state—
of—the—art machine learning methods obtain ’gold standard’ performance as measured in critical
assessments in defined tasks. Across a broad range of tests, we see close alignment in algorithm
performances after controlling for the underlying data being used. We find that algorithm aggrega—
tion provides only modest benefits, with a 17% increase in area underthe ROC (AUROC) above the
mean AUROC. In contrast, data aggregation gains are enormous with an 88% improvement in
mean AUROC. Altogether, we find substantial evidence to support the view that additional algo—
rithm development has little to offer for gene function prediction.

Availability and implementation: The supplementary information contains a description of the
algorithms, the network data parsed from different biological data resources and a guide to the

source code (available at: http://gillislab.cshl.edu/supplements/).

Contact: jgillis@cshl.edu

 

1 Introduction

High-throughput genomic data often relies on computational meth-
ods for functional inference and interpretation. Improving our
knowledge of gene function in otherwise uncharacterized genes is
one major task to which these computational methods are put
(Eisenberg et (11., 2000). While this is often called gene function pre-
diction when being treated as a machine learning problem, essen-
tially the same methods underlie a variety of important biomedical
applications, such as candidate disease gene prioritization
(Tranchevent et (11., 2011). Like many methods in machine learning,
a major concern for function inference methods is the degree to
which their performance is robust and generalizes from benchmark
tasks to novel data (Bousquet and Elisseeff, 2002; Domingos, 2012).
To some extent, systematic problems with generalizing past gene

function prediction to future performance have been recognized by
the field. These have led to critical assessments of function predic-
tion, e.g. the critical assessment of Mus musculus gene function pre-
diction (MouseFunc) (Pena—Castillo et (11., 2008) and the critical
assessment of function annotation (CAFA) (Radivojac et (11., 2013),
where groups compete to predict gene function. These assessments
are intended to provide field-wide benchmarks for comparative per-
formance and thereby help determine which research directions may
be more fruitful. Generally speaking, gene function prediction meth-
ods rely on (i) prior function assignments for genes, (ii) data to char-
acterize genes and (iii) an algorithm which learns the data features
associated with the previous function assignments. Novel gene-
function mappings are then predicted based on the learned data fea-
tures; this can be done either in a gene-centric or function-centric

(C7 The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 745

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

746

W. Verleyen et al.

 

way. Because individual laboratories approach this problem in quite
different ways, characterizing the field overall in critical assessment
has been difficult; it is hard to know what factors in one laboratory’s
implementation drives results. Indeed, even in critical assessments,
the same laboratory may submit slightly altered versions of the same
software and find their performance changes dramatically (Hamp
et al., 2013).

We solve the over—training problem addressed by critical assess—
ments in an alternate way; by using verifiable implementations it
can be checked that there are no adjustments or tweaks. Normally,
‘fine—tuning’ algorithms and data to work appropriately are neces—
sary to obtain reasonable performance, but this is precisely what
may contribute to poor generalization. Based on performance trends
in previous critical assessments, we hypothesized that well—
developed machine learning approaches would perform at a high
level if using gold—standard data resources. That the performance as—
sessment is not tuned to obtain artificially high performance can be
ensured by using pre—existing and verifiable tests, i.e. the critical
assessments. By having an in—house representative of the field as a
whole, we are able to conduct additional experiments with greater
control.

One of our principal interests is in exploring how aggregation
improves performance. That it generally does so has been a finding
common to previous assessments of function prediction and net—
work inference methods and is a frequent expectation for machine
learning in general (Gillis and Pavlidis, 2013a,b; Marbach et al.,
2012; Opitz and Maclin, 1999). However, the factors central to this
effect have not been well characterized, likely because most
laboratories will have an individualized approach which makes
well—controlled comparison difficult. To overcome this, we will first
benchmark a set of machine learning algorithms based upon data
resources available from MouseFunc to establish they are represen—
tative samples of high—performing methods. Then, we will systemat—
ically apply these algorithms on various types of Saccharomyces
cerevisiae data as a sample of ‘field—wide’ properties and investigate
how their combination improves performance and on what factors
this depends.

2 Methods

2.1 Network and gene annotation data

For the gene network data, each gene’s interaction profile or set of
gene interactions was treated as the feature data associated with that
gene. We had access to the MouseFunc networks and data resources
(Pena—Castillo et al., 2008). These included protein annotation from
Pfam (Finn et al., 2006) and InterPro (Mulder et al., 2005), pro—
tein—protein interaction (Brown and Jurisica, 2005 ), phenotype
ontology from the mouse genome informatics database (Smith et al.,
2005), co—expression (Siddiqui et al., 2005; Su et al., 2004; Zhang
et al., 2004), phylogenetic profile (O’Brien et al., 2005), disease data
for M.musculus (Hamosh et al., 2005). For the S.cerevisiae data, we
generated networks from four types of biological data: (i) protein—
protein interaction, (ii) sequence similarity, (iii) co—expression and
(iv) semantic similarity or shared pathways data (e.g. Mistry and
Pavlidis, 2008), each parsed in a specific manner. The protein—
protein interaction network was constructed from the BioGRID
database (version 3.2.103) (Chatr—Aryamontri et al., 2013). We gen—
erated a binary network of only the physical interactions between
the yeast species, with no further filtering on experimental type. For
the sequence similarity, we generated a weighted network using the
protein sequences from the Saccharomyces Genome Database

(Cherry et al., 2012) (orf_trans_all.fasta/31—5—2013). The sequence
hits were calculated with psiBLAST (Altschul et al., 1997) and the
negative log10 E—value of the psiBLAST score (and thresholding to
zero all values below 1 after transformation), previously shown to
be a simple and reliable measure for sequence similarity (Enright
et al., 2002; Hawkins et al., 2009), was used to weight the inter—
actions in the network. We used an aggregate co—expression net—
work, generated from 30 yeast microarray experiments on the
Affymetrix Yeast Genome S98 Array (GPL90), characterizing 5457
genes with unique probes across 966 samples. Briefly, we down—
loaded each expression dataset from the Gene Expression Omnibus
then used the Spearman correlation coefficient as weight edges be—
tween gene pairs in the co—expression network, and the aggregate
was generated by summing the individual experiments (Gillis and
Pavlidis, 2011a,b). The final network was then rank standardized.
The semantic similarity network was generated based upon the bio—
logical pathways of S.cerevisiae in the kyoto encyclopedia of genes
and genomes (KEGG) database (Ogata et al., 1999). In short, the
weight of each interaction in the network was taken as the Jaccard
index of two genes sharing biological pathways, according to their
appearance in the corresponding KEGG pathways. We used an over—
lap/intersect of 5455 genes for S.cerevisiae. As a gene annotation set,
we used the gene ontology (GO) annotations from April 13, 2013.
We propagated over the GO structure, and then filtered for GO
groups on size such that each remaining group had between 20 and
1000 associated genes while excluding inferred from electronic an—
notation, a range that generally gives stable performance (Gillis and
Pavlidis, 2011a,b). We were left with 1313 GO groups to be used in
the remaining analyses. Note that while we use the term ‘gene func—
tion prediction” throughout, there is generally no difference from the
methods, data and analysis present in protein function prediction.

2.2 Algorithms and parameter settings

We benchmarked six machine learning algorithms on the
MouseFunc data: (i) logistic regression (Fan et al., 2008), (ii) ran—
dom forest (Breiman, 2001), (iii) support vector machine with a
stochastic gradient descent solver (Su et al., 2004), (iv) passive
aggressive (Crammer et al., 2006), (v) GeneMANIA (Mostafavi
et al., 2008) and (vi) neighbor voting (Gillis and Pavlidis, 2011a,b).
The first four algorithms are implementations from the scikit—learn
machine learning framework (Pedregosa et al., 2011) with
GeneMANIA and neighbor voting having matlab implementations
which we used. The first five algorithms are comparatively sophisti—
cated methods, while neighbor voting serves as a simple but useful
baseline. We then applied these algorithms to the four yeast data
networks described previously. These methods were chosen based
on their relative prominence within the machine learning literature,
their similarity to methods used in Mousefunc and their continued
popularity in gene function prediction research.

For the MouseFunc benchmarking, we used the default param—
eters for each of the algorithms. Logistic regression was used with
dual formulation regularized with L2—norm penalization for the
LIBLINEAR library, classes are weighted inversely proportional to
their frequency; the regularization strength (C: 1.0) and tolerance
(tol: 0.001) were kept to their default settings. The random forest
classifier was used with 512 decision trees; the Gini impurity was
used to split the decision tree whereby the maximum number of fea—
tures for each individual decision trees was the square root of the
total number of features. The stochastic gradient descent solver for
the support vector machine uses the hinge loss function with
L2—norm penalization; we used 10 iterations over the training data,

/310‘s112umo[p10}xo‘sopcuHOJIItotq/ﬁdnq

o_

3'!

Hall, 1998

an?kgogmomammowoio~&o:3m7.omm\

nu
1..

A BE 9.5th m9;

3
_.H..



4.
.U

m
.6
.e
.v
.m
S
0
D.
e
:6
a
F

D
00
G

«U
5

$.58 00 «a

Performance (AUROC)

 

Pm lidis, 2013.11)

Pm lidis (71 al., 2002

Gillis and

o.)
LJ
t:
:9.
C
L.
E
E
CL

r“ ..
Ea
+_-r'~
ta
0::
Ur _
MS.

[16
Performance GeneMANIA
(AUROCJ

 

/310'S[BIIJHOIPJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

an?kgogmomammowoio~&o:3m7.omm\

e
.L
n
m
ml;
C
.mO
r.
ER
FU
em
.9
m
m...
A

GOmD$ 00% S
EmEmbocQE, EmEms Equ
A mucmpEouth B mucmEuQqum

 

Fig. 3

Fig. 3

Performance 1;.
improvement

H
53%
cp—
EC:
CCU

2.".
F39
“ta
ESE

(AUROC)

L.)
C)
Q:
:3

(:1

C.

:3

M

D

Fig. 4

L15 0.?
Average performance (AUROC)

 

2011.11)

Figure

Gillis and Pmlidis,

/310'S[BIIJHOIPJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

Measuring the Wisdom of the crowds

751

 

which GO—slim effectively estimated general trends was very high.
Mean performances for any algorithm varied only slightly if averaged
over all of GO or just GO slim, with the change in mean performance
never exceeding an AUROC of 0.01. Thus, while the hierarchy as a
whole may be useful at various other stages in algorithm implementa—
tion, using GO—slim is probably a reasonable approximation for per—
formance assessment between algorithms.

4 Discussion

There is a simple and consistent trend in our results: even algorithms
of wholly distinct conception and design give quite similar results (in
all ways) on the same data. This may seem an obvious finding—one
cannot get blood from a stone—but it is, in fact, contrary to much
of the published literature.

Summarizing our points of departure from the previous litera—
ture, the most similar previous analysis to ours concluded that ‘it’s
the machine that matters”, even where similarly using default imple—
mentations (Wang and Marcotte, 2010). Similarly, it is the consen—
sus that algorithm aggregation alone can provide substantial
benefits on the closely related task of network inference (Marbach
et al., 2012), which would normally suggest that algorithms vary
enough to benefit from aggregation. Supported by these general
findings, more focused assessments have also concluded that simply
aggregating algorithms on the same data offers enormous benefits,
again, on tasks closely related to but not identical to function predic—
tion, e.g. mutation prioritization (ManChon et al., 2014).

While we draw superficially contrary conclusion, our results are
actually surprisingly consistent with these claims. Within the range
of performance of our algorithms on a given dataset, aggregation
has a very large influence. Thus, in any competition in which data is
held constant, we too, would suggest aggregation as a potentially
useful strategy. It is only by examining variation across data re—
sources that we see how comparatively modest these benefits are.

In support of this finding, in critical assessments where devel—
opers may alter their data choice, high—performing methods have
tended to use large and diverse datasets, as in the top—performing
CAFA method, where data integration was one of the chief areas of
novelty (Cozzetto et al., 2013). This suggests that comparative as—
sessment of data (or even its integration) may be more fruitful than
the current algorithmic focus. In fact, it would then be important to
hold methods somewhat constant. Fortunately, our analysis suggests
this is rather easy and that even where methods differ in basic design
and conception, they are quite similar in performance (relative to
variation caused by data). Because of this, downstream results can
be assumed to be the property of the data being studied, rather than
the inference method being used to study it. While it is true that
good experimental design can ensure over—training does not occur, if
a custom method is necessary to get a dataset to work or the custom
method works unusually well, we suggest there is a heavy onus to
show it does not reflect accidental over—training in some way.

Of course, another reason for our performance trends is that all
our algorithms performed well. If some performed badly, then the
machine could be said to matter more. We believe there may be a
subtle selection bias at play here which is difficult to evaluate for—
mally. Our function inference task was more computationally chal—
lenging than what might be considered a typical assessment for a
targeted function or multiple functions in a targeted dataset. While
our algorithms are not customized, many methods we initially tried
simply did not operate readily in the large data and prediction space
we wished to assess. This may well have selected for general—purpose

methods or even just higher—quality ones. Likewise, this may explain
why our performance is close to carefully tailored methods in crit—
ical assessment; the task is so broad that feature selection provides
few benefits. Our observation was that when algorithms worked at
all, they worked well, but some failed catastrophically. It is possible
the divide in data interpretation is between methods that basically
work and those which basically fail—and we simply seen combin—
ations of this. Certainly, this is supported by our observation in the
sequence similarity data that what worked, worked the same, and
what failed, failed the same.

An encouraging finding from our work is the generally positive
relationship between the learnability of a function and the degree to
which aggregation is of value. Aggregation is of no likely value for
combinations of random data/methods (AUROC 0.5) and also of no
value for perfect output (AUROC 1.0). Despite seeing good overall
performance, apparently we are still not in the realm where
performance ceilings are a limiting factor; the overall trend is still
dominated by the addition of signal yielding better aggregate per—
formance (rather than the exhaustion of additional signal diminish—
ing aggregation’s value). This, combined with the high level of
improvement from data aggregation overall, suggests that we have
not come close to maximizing performance gains attainable from
additional data.

Acknowledgements

Sara Mostafavi and Quaid Morris generously provided the Matlab implemen-
tation of GeneMANIA.

Funding

].G., W.V. and S.B. were supported by a grant from T. and V. Stanley. No
funding source played any role in the design, in the collection, analysis and in-
terpretation of data; in the writing of the manuscript; and in the decision to
submit the manuscript for publication.

Conﬂict of Interest: Home declared.

References

Altschul,S.F. et al. (1997) Gapped BLAST and PSI-BLAST: a new generation
of protein database search programs. Nucleic Acids Res., 25, 3389—3402.
Bousquet,O. and Elisseeff,A. (2002) Stability and generalization.  Mach.

Learn. Res., 2, 499—526.

Breiman,L. (2001) Random forests. Mach. Learn., 45, 5—32.

Brown,K.R. and Jurisica,I. (2005) Online predicted human interaction data-
base. Bioinformatics, 21, 2076—2082.

Chatr—Aryamontri,A. et al. (2013) The BioGRID interaction database: 2013
update. Nucleic Acids Res., 41, D816—D823.

Cherry,].M. et al. (2012) Saccharomyces Genome Database: the genomics re-
source of budding yeast. Nucleic Acids Res., 40, D700—D705.

Cozzetto,D. et al. (2013) Protein function prediction by massive integration of
evolutionary analyses and multiple data sources. BMC Bioinformatics, 14
(Suppl. 3), S1.

Crammer,K. et al. (2006) Online passive-aggressive algorithms.  Mach.
Learn. Res., 7, 551—585.

Domingos,P. (2012) A few useful things to know about machine learning.
Commun. ACM, 55, 78—87.

Eisenberg,D. et al. (2000) Protein function in the post-genomic era. Nature,
405, 823—826.

Enright,A.]. et al. (2002) An efﬁcient algorithm for large—scale detection of
protein families. Nucleic Acids Res., 30, 1575—1584.

Fan,R.E. et al. (2008) LIBLINEAR: a library for large linear classiﬁcation.
]. Mach. Learn. Res., 9,1871—1874.

ﬁm'srcumol‘pquo'sopcuuowtotq/ﬁdnq

752

W. Verleyen et al.

 

Finn,R.D. et al. (2006) Pfam: clans, web tools and services. Nucleic Acids
Res., 34, D247—D251.

Gillis,]. and Pavlidis,P. (2011a) The impact of multifunctional genes on “guilt
by association” analysis. PLoS One, 6, e17258.

Gillis,]. and Pavlidis,P. (2011b) The role of indirect connections in gene net-
works in predicting function. Bioinformatics, 27, 1860—1866.

Gillis,]. and Pavlidis,P. (2013a) Assessing identity, redundancy and confounds
in Gene Ontology annotations over time. Bioinformatics, 29, 476—482.

Gillis,]. and Pavlidis,P. (2013b) Characterizing the state of the art in the com-
putational assignment of gene function: lessons from the ﬁrst critical assess-
ment of functional annotation (CAFA). BMC Bioinformatics, 14(Suppl. 3),
S15.

Hall,M.A. (1998) Correlation-based Feature Selection for Machine Learning.
The University of Waikato, New Zealand.

Hamosh,A. et al. (2005) Online Mendelian Inheritance in Man (OMIM), a
knowledgebase of human genes and genetic disorders. Nucleic Acids Res.,
33, D514—D517.

Hamp,T. et al. (2013) Homology-based inference sets the bar high for protein
function prediction. BMC Bioinformatics, 14 (Suppl. 3), S7.

Hawkins,T. et al. (2009) PFP: automated prediction of gene ontology
functional annotations with conﬁdence scores using protein sequence data.
Proteins, 74, 566—582.

ManChon,U. et al. (2014) Prediction and prioritization of rare oncogenic mu-
tations in the cancer kinome using novel features and multiple classiﬁers.
PLoS Comput. Biol., 10, e1003545.

Marbach,D. et al. (2012) Wisdom of crowds for robust gene network infer-
ence. Nat. Methods, 9, 796—804.

Mistry,M. and Pavlidis,P. (2008) Gene Ontology term overlap as a measure of
gene functional similarity. BMC Bioinformatics, 9, 327.

Mostafavi,S. et al. (2008) GeneMANIA: a real-time multiple association net-
work integration algorithm for predicting gene function. Genome Biol., 9
(Suppl. 1), S4.

Mulder,N.]. et al. (2005) InterPro, progress and status in 2005. Nucleic Acids
Res., 33, D201—D205.

O’Brien,K.P. et al. (2005) Inparanoid: a comprehensive database of eukaryotic
orthologs. Nucleic Acids Res., 33, D476—D480.

Ogata,H. et al. (1999) KEGG: kyoto encyclopedia of genes and genomes.
Nucleic Acids Res., 27, 29—34.

Opitz,D. and Maclin,R. (1999) Popular ensemble methods: an empirical
study.]. Artiﬁ Intell. Res., 11, 169—198.

Pavlidis,P. et al. (2002) Learning gene functional classiﬁcations from multiple
data types. ]. Comput. Biol., 9, 401—411.

Pedregosa,F. et al. (2011) Scikit-learn: machine learning in python.  Mach.
Learn. Res., 12, 2825—2830.

Pena-Castillo,L. et al. (2008) A critical assessment of Mus musculus gene
function prediction using integrated genomic evidence. Genome Biol.,
9(Suppl. 1), S2.

Radivojac,P. et al. (2013) A large-scale evaluation of computational protein
function prediction. Nat. Methods, 10, 221—227.

Siddiqui,A.S. et al. (2005) A mouse atlas of gene expression: large-scale digital
gene-expression proﬁles from precisely deﬁned developing C57BL/6] mouse
tissues and cells. Proc. Natl Acad. Sci. USA, 102, 18485—18490.

Smith,C.L. et al. (2005) The Mammalian Phenotype Ontology as a tool for
annotating, analyzing and comparing phenotypic information. Genome
Biol., 6, R7.

Su,A.I. et al. (2004) A gene atlas of the mouse and human protein-encoding
transcriptomes. Proc. Natl Acad. Sci. USA, 101, 6062—6067.

Tranchevent,L.C. et al. (2011) A guide to web tools to prioritize candidate
genes. Brief. Bioinform., 12, 22—32.

Wang,P.I. and Marcotte,E.M. (2010) It’s the machine that matters: predicting
gene function and phenotype from protein networks.  Proteomics, 73,
2277—2289.

Zhang,W. et al. (2004) The functional landscape of mouse gene expression.
]. Biol., 3, 21.

ﬁm'srcumol‘pquo'sopcuuopttotq/ﬁdnq

