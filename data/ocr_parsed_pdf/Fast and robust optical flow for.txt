ORIGINAL PAPER

Vol. 29 no. 3 2013, pages 373—380
doi:10. 1093/bioinformatics/bts706

 

Bioimage informatics

Advance Access publication December 14, 2012

Fast and robust optical flow for time-lapse microscopy

using super-voxels

Fernando Amat1 '*, Eugene W. Myers2 and Philipp J. Keller1 '*

1Howard Hughes Medical Institute, Janelia Farm Research Campus, Ashburn, VA 20147, USA and 2Max Planck Institute
of Molecular Cell Biology and Genetics, 01307 Dresden, Germany

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: Optical flow is a key method used for quantitative motion
estimation of biological structures in light microscopy. It has also been
used as a key module in segmentation and tracking systems and is
considered a mature technology in the field of computer vision.
However, most of the research focused on 2D natural images,
which are small in size and rich in edges and texture information. In
contrast, 3D time-lapse recordings of biological specimens comprise
up to several terabytes of image data and often exhibit complex object
dynamics as well as blurring due to the point-spread-function of the
microscope. Thus, new approaches to optical flow are required to
improve performance for such data.

Results: We solve optical flow in large 3D time-lapse microscopy
datasets by defining a Markov random field (MRF) over super-voxels
in the foreground and applying motion smoothness constraints be-
tween super-voxels instead of voxel-wise. This model is tailored to
the specific characteristics of light microscopy datasets: super-voxels
help registration in textureless areas, the MRF over super-voxels effi-
ciently propagates motion information between neighboring cells and
the background subtraction and super-voxels reduce the dimension-
ality of the problem by an order of magnitude. We validate our ap-
proach on large 3D time-lapse datasets of Drosophila and zebrafish
development by analyzing cell motion patterns. We show that our
approach is, on average, 10xfaster than commonly used optical
flow implementations in the Insight Tool-Kit (ITK) and reduces the
average flow end point error by 50% in regions with complex dynamic
processes, such as cell divisions.

Availability: Source code freely available in the Software section at
http://Janelia.org/lab/keller-lab.

Contact: amatf@janelia.hhmi.org or kellerp@janelia.hhmi.org
Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on July 30, 2012; revised on November 16, 2012; accepted
on December 7, 2012

1 INTRODUCTION

Automated computational techniques are essential for the
quantitative analysis of cellular dynamics using time-lapse light
microscopy. For example, to quantitatively reconstruct the
development of large multi-cellular organisms such as entire
Drosophila and zebrafish embryos, tens of thousands of cells
need to be segmented and tracked at high spatial resolution

 

*To whom correspondence should be addressed.

(McMahon et al., 2008; Tomer et al., 2012) (Fig. 1).
Such analyses are of fundamental importance to understanding
the development of biological tissues, to reconstructing func-
tional defects in mutants and disease models and to quantita-
tively dissecting the mechanisms underlying the cellular building
plan of entire complex organisms (Keller et al., 2008). However,
many computational challenges are encountered when perform-
ing key tasks, such as image registration, cell segmentation and
cell tracking, in complex microscopy datasets (Khairy et al.,
2008; Li et al., 2007; Lou et al., 2011; Preibisch et al., 2010;
Rubio-Guivernau et al., 2012).

Optical ﬂow computation is one of the central tasks used to
perform quantitative motion estimation of biological structures
in time-lapse light microscopy, from the subcellular level to the
tissue scale (Abramoff and Viergever, 2002; Buibas et al., 2010;
Delpiano et al., 2011; Roberts et al., 2010). Optical ﬂow is
deﬁned as the vector field capturing the motion of brightness
patterns between adjacent volumes in time (Horn and Schunck,
1981; since our examples are 3D images, we use the term
‘Volume’ to refer to the datasets used in optical flow computa-
tion. However, our approach and code work also for 2D
images). On the cellular level, optical ﬂow information can the-
oretically be obtained from single-cell tracking data. However,
comprehensive and accurate cell tracking in complex multi-
cellular organisms is currently an open research problem
(Tomer et al., 2012, Lou et al., 2011). Here, optical flow methods
can be useful for analyses of group dynamics, which do not
require single-cell resolution, or, conversely, as the first module
in a larger cell tracking framework. In this latter scenario, the
ﬂow information informs the tracking algorithm and helps
improving results for regions exhibiting complex or fast cell
dynamics.

Optical ﬂow computation has been the object of decades of
research, and it is considered a mature technology in many com-
puter Vision applications (Baker et al., 2011). However, most
approaches have been tested in relatively small 2D natural
images, which are dense and rich in edges and texture informa-
tion. The Middlebury database (Baker et al., 2011) used as a
benchmark in the computer Vision community is a good example
of these types of images. Fluorescence microscopy volumes of
biological structures are qualitatively very different from natural
images (Fig. 1). They are sparse (in datasets similar to Figure 1,
80795% of voxels are background; throughout the text, we use
the term ‘Voxel’ to generically refer to each intensity value in a
dataset independent of the dimensionality of the data) and con-
tain relatively textureless objects, which typically appear blurred

 

© The Author 2012. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which
permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

112 /310's112u1n0fp10}x0"sotJBurJOJutotq/ﬁduq 11101} papaolumoq

9103 ‘Og anﬁnV uo ::

F.Amat et al.

 

 

Fig. 1. (A) Rendering of 3D volume obtained with SiMView light-sheet
microscopy (Tomer et al., 2012). Each of the objects represents a single
cell nucleus marked by a ﬂuorescent reporter in a Drosophila embryo.
Dimensions are 602 x 1386 x 110 voxels per volume (0.4 x 0.4 x 2.0 um3
voxel size). The embryo is ~550 um long and 200 pm in diameter.
(B) Optical slices of the volume visualized in (A). (C) Enlarged View of
two superimposed consecutive time points. Multiple motions, such as cell
divisions and cell migration, occur in the same volume

owing to the point spread function of the microscope and the
characteristics of commonly used ﬂuorescent labeling strategies.
Moreover, neighboring objects with similar appearance and mul-
tiple motions in the same volume are very common. Finally,
microscopy volumes tend to be much larger than natural
images, which demands computationally efﬁcient approaches.
Here, we present a new algorithm for optical ﬂow estimation
tailored to large ﬂuorescence light microscopy 3D time-lapse
datasets as the one shown in Figure 1. The key idea is to
deﬁne a model that takes into account the specific characteristics
of time-lapse microscopy data. In particular, we deﬁne a Markov
random field (MRF) over super-voxels to improve registration in
textureless areas, propagate motion information efﬁciently be-
tween neighboring structures and speed up computations by
reducing the complexity of the problem.

1.1 Optical ﬂow techniques

In this paragraph, we highlight some of the fundamental insights
introduced over the past few decades. We refer the reader to
Baker et a]. (2011) for a recent comprehensive review. First,
Lucas and Kanade (1981) proposed a local approach by solving
the optical ﬂow independently in small rectangular regions that
partition the entire volume. This approach produces a sparse
ﬁeld because it is ill posed for large regions with uniform

appearance. In contrast, Horn and Schunck (1981) introduced
a global method, where the ﬂow is calculated at each voxel in-
stead of a rectangular region by introducing smoothness con-
straints between adjacent voxels as a regularization strategy.
This approach produces a dense ﬁeld, but it cannot resolve
motion discontinuities. Black and Anandan (1996) introduced
robust metrics, instead of the traditional L2 norm, to improve
results in motion discontinuity boundaries and regions with in-
tensity changes between volumes. Bruhn et a]. (2005) merged the
beneﬁts of all of these previous approaches in a combined local-
to—global approach, where a robust Horn and Schunck formula-
tion was solved at different spatial scales, effectively incorporat-
ing the beneﬁts of the approach by Lucas and Kanade. Other
relevant insights are the application of different weights to each
of the smoothness terms to add robustness against motion dis-
continuities, the detection of occluded regions (Ayvaci et al.,
2010) and the application of a smoothing ﬁlter to the ﬂow
after each iteration of the optimization procedure (Sun et al.,
2010; Thirion, 1998) to improve accuracy. Over the years,
there has also been progress on real-time optical ﬂow, especially
with recent Graphics Processing Unit (GPU) implementations
(W erlberger et al., 2009). Unfortunately, software incorporating
the most recent advances is not publicly available, and it is not
clear whether some of these techniques can be scaled to large 3D
datasets according to the timing reported in the benchmarks by
Baker et al., (2011).

Most biomedical optical ﬂow applications tend to implement
and report results using similar methodologies to the ones ex-
plained earlier in the text without tailoring them to the charac-
teristics of the data. For example, Pock et a]. (2007) presented a
total variation (TV)-L1 optical ﬂow model for clinical datasets.
However, even with the use of image pyramids to solve the prob-
lem efﬁciently, this approach was still slow for large 3D datasets,
and it did not always outperform the Insight ToolKit (ITK) im-
plementations. ITK is a multi-threaded C++ library for N-di-
mensional image registration and segmentation, and it is the
most common baseline for comparing the performance and ac-
curacy of new algorithms in the bioimaging domain. Many
recent articles use similar strategies to target speciﬁcally
time-lapse light microscopy datasets (Delpiano et al., 2011;
Lombardot et al., 2008; Pizarro et al., 2011), which demonstrate
the general interest in applying optical ﬂow to the type of data-
sets presented in this article. In the following sections, we present
an optical ﬂow formulation speciﬁcally tailored to solving optical
ﬂow for 3D time-lapse microscopy volumes. We show that our
method is 10 x faster and reduces the average ﬂow end point
error (EE) by 50% for complex dynamic processes, such as cell
divisions, with respect to optical ﬂow algorithms available in the
ITK library.

2 APPROACH

First, we use a conservative foreground/background segmenta-
tion to consider only useful pixels. Background removal avoids
the optical ﬂow ambiguity in large uniform uninformative re-
gions of the volume and improves computational efficiency.
Second, we use a region-based approach to improve performance
in the textureless objects. Glocker et a]. (2008) proposed a similar
approach by dividing the image in a rectangular grid. However,

 

374

112 /310's112u1n0fp10}x0"sotiizuiJOJutotq/ﬁduq 11101} pQPBOIII/lAOG

9103 ‘Og isnﬁnV uo ::

Optical flow for time-lapse microscopy

 

as shown in Table 3, rectangular grids do not adapt well to
sparse signals and degrade performance, as a single rectangular
region can contain two objects with different dynamics. Prinet
et a]. (2006) and Xu et a]. (2008) also presented region-based
approaches to optical ﬂow. However, their segmentation as-
sumptions cannot be applied to light microscopy images owing
to the lack of edge and color information. Therefore, we use
recent advances in fast super-voxel generation (Achanta et al.,
2012) to group ﬂows into small subsets. We combine the fore-
ground/ background mask with non-adjacent super-voxel regions
to generate a volume partition graph over the set of super-voxels.
Then, all smoothness constraints are taken between neighboring
super-voxels instead of adjacent voxels, which effectively propa-
gate motion information between close-by cellular structures
with similar motions. This graphical model effectively captures
speciﬁc characteristic of time-lapse light microscopy data.
Recently, Gkamas and Nikou (2011) also used super-voxels for
optical ﬂow estimation. They added super-voxels to the
combined local-to-global framework to establish disconnected
motion boundaries between different objects in dense natural
images, which is opposite to the strategy in our MRF model
for microscopy images, showing that time-lapse microscopy
image should be treated differently. Aside from robustness, the
model for optical ﬂow presented here allows us to speed up the
optimization by an order of magnitude. Finally, we show how
standard procedures, such as robust metrics and multi-scale op-
timization schemes, are also effective in the microscopy imaging
domain to improve performance. Our combined framework thus
improves and extends optical ﬂow to the application of
large-scale time-lapse ﬂuorescence light microscopy images.
Figure 2 summarizes the steps described in the next subsections.

3 METHODS

Given two N-dimensional images of the same size, 1’ (source volume) and
1’“ (target volume), our ﬁnal goal is to estimate a motion ﬁeld v1, for
each voxel p to register the target volume to the source volume.

3.1 Image model

When most objects present in the volume are textureless and similar to
each other, single voxels are not very informative. In other words, just
trying to match single intensities leads to poor solutions. Most optical
ﬂow approaches try to guide the registration in textureless areas by
imposing a smoothness constraint between adjacent voxels.
Unfortunately, microscopy volumes tend to contain many background
voxels, which also misguide the smoothness constraint. Thus, we need
better partitioning of the volume to improve optical ﬂow.

 

Input Foreground Su DEF-VDXBI
Images estimation generation

 

 

 

Image graph Optical flow
+ panitlcn [MRFJ _.i over MRF FF”

Fig. 2. Block diagram representing the pipeline described in this article to
estimate optical ﬂow. Optical ﬂow is performed over a set of super-voxels
in the volume foreground, and the smoothness constraints are imposed
between neighboring (and possibly non-adjacent) super-voxels instead of
between connected voxels. This approach guides the registration process
of neighboring nuclei with similar dynamics to a better solution than
previous approaches

 

 

 

First, we generate a foreground/background mask (Fig. SE) to ignore
voxels containing no information in the volume. This mask can be as
simple as an intensity threshold or any other existing background detec-
tion method. Aside from removing non-informative voxels, the mask also
helps speed up convergence, as it reduces the number of motion vectors v],
we need to estimate. Data sparsity is problematic and advantageous at
the same time, as it precludes the imposition of standard smoothing con-
straints but it allows a reduction in the size of the problem in the ﬂow
calculation.

Once we have a set of foreground voxels, we want to apply the intu-
ition from Xu et a]. (2008) that region-based optical ﬂow helps in tex-
tureless areas. Unfortunately, segmentation techniques tend to be
computational costly in large 3D biomedical volumes, and color infor-
mation is often not available. The connected components of foreground
regions contain multiple cells (Fig. 3B), so we cannot use them directly for
segmentation. Moreover, cellular structures change shape in a non-rigid
manner from one time point to another. Thus, it is not advisable to
segment full objects into a single region. Otherwise, the motion model
would be too complex. We take advantage of recent advances by Achanta
et a]. (2012) to generate fast super-voxels based on intensity and geomet-
ric distance in the volume. Simple linear iterative clustering (SLIC)
super-voxels segment each nucleus into a small number of regions while
usually respecting the boundaries between different objects (Fig. 3C).
Thus, we can expect that all voxels within a super-voxel should have
similar motion. Results in Table 3 show that super-voxels outperform
ﬁxed-size rectangular regions similar to Lucas and Kanade (1981), as
rectangles can sometimes lie in the middle of two objects with different
dynamics and degrade performance.

The super-voxels form a partition of the elements in the volume fore-
ground. The final step needed to model the volume is to connect neigh-
boring super-voxels to capture common dynamics between regions. We
will deﬁne an edge between two super-voxels if their centers of mass are
below a distance threshold dmax. This deﬁnition forms an MRF (or
equivalently a partition graph) over the foreground voxels (Fig. 3D),
where we can directly impose smoothness constraints to calculate optical
ﬂow. This setup is necessary because often two regions with coherent
dynamics are completely disconnected by background voxels, so trad-
itional voxel-based regularizations are not as effective.

3.2 Optimization model

Most approaches in optical ﬂow use the brightness constancy assumption

Vp EIvI, s.t. II” = 11"?” withp, v], E [RN (1)

to pose optical ﬂow as the following optimization problem:

argmnVl,---aVIPI ZPD(I;’ _  + A: Z Wl’v’lpc(v1’ _ v”) (2)
pEP

pEP qu(p)

where P is the set of voxels in the volume, N(p) are adjacent neighboring
voxels in the volume (using 2N or 3” — 1 connectivity) and p0 and pg are
robust cost functions such as Huber penalty, L1, TV or Lorentzian (Black
and Anandan, 1996). The ﬁrst sum term in Equation (2) with pD can be
considered a unary potential or data term, in which we want to match the
intensity between two volumes. In this context, robust metrics are im-
portant to allow ﬂuctuations in the volume intensity. However, this term
by itself does not offer enough constraints for the motion ﬁeld vp. Thus,
the second term in Equation (2), referred to as the pairwise potentials or
smoothness term, is incorporated to regularize the solution. Here, robust
metrics are important to allow for discontinuities in the ﬂow ﬁeld between
different objects in the scene (Black and Anandan, 1996). Finally, it is
common to adapt the smoothness term at the pixel level by deﬁning a
weight WP”, based on edge intensity, effectively reducing the importance
of the smoothness constraint in areas of possible motion discontinuities.

 

375

112 /310's112u1n0fp10}x0"sotiizuiJOJutotq/ﬁduq 11101} papBOIH/noq

9103 ‘Og isnﬁnV uo ::

F.Amat et al.

 

 

Fig. 3. Step for constructing an MRF over the super-voxels on the volume foreground to partition the volume and perform robust optical ﬂow. (A) 2D
slice of raw data from Figure 1. We show only a slice to simplify the visualization, but the method is implemented in 3D. (B) Outline of the foreground
mask obtained with a trained classiﬁer in Ilastik (Sommer et al., 2011). Some connected components correspond to multiple nuclei. (C) Slice of 3D SLIC
(Achanta et al., 2012) super-voxels calculated over the foreground. Super-voxels respect object boundaries of nuclei in the same foreground connected
component. (D) Edges added between neighboring super-voxels to generate an MRF. Each node V,- represents a super-voxel in panel C. This is the ﬁnal
volume partition model where we perform optical ﬂow. We impose the smoothness conditions over entire super-voxels instead of voxelwise

Robust metrics alone and voxel-wise smooth ﬂow assumptions are not
enough to handle the challenges present in microscopy volumes: given the
sparsity, the lack of distinct features between objects and the multiple
dynamics in a single volume, the energy terms deﬁned in Equation (2) are
not strong enough to guide the optimization process to the right min-
imum, as shown in Section 4. Using Equation (2) as a model and the
MRF over super-voxels constructed in Section 3.1, we can deﬁne a new
optimization problem:

argrnithuwvw Z pr (11’, —  + 4: Z WR,sPC(Vs — VR) (3)

$6111 pES SEIIi REN(S)

where ER is the set of super-voxels in the graph partition, and we calculate
a single translation vs for each region. The modiﬁcation to the data term
helps further regularizing the solution in textureless regions to guide the
optimization to the right solution. Moreover, we have reduced the dimen-
sionality of the search by several orders of magnitude (|ER| < < |P|). In this
case, we decided not to use global afﬁne transformation models, as they
do not ﬁt the large variability in cell dynamics. In contrast, we determined
experimentally that a local afﬁne model was not necessary to capture
those dynamics, so we introduced a compromise with a local translational
ﬂow ﬁeld for each super-voxel. Finally, we adapted the concept from
Equation (2) of adaptively adjusting the weight wR,S of the smoothness
constraint between connected regions in the graph. However, we cannot
use edge information because regions may not be adjacent to each other.
In our case, we deﬁne wR,S as follows:

_ d“ 2 vol(R)+voI(S)
wR,s—exp{-0-5(m)  (4)

where dR, S is the distance between the center of masses of super-voxels R
and S, and vol(R) is the number of voxels contained in region R.
Intuitively, the ﬁrst term decreases interaction between super-voxels if
regions are far apart, and the second term decreases interaction if they
do not represent large sets of voxels.

Even with this region-based regularization, the data term is still not
powerful enough to always return the right solution (Table 1), as most of
the objects in the volume look very similar (Fig. 1). In our case, the term
N(S) connects entire neighboring regions (not only adjacent voxels),
which agrees with the assumption that we have multiple cells with
common dynamics in some areas. By connecting non-adjacent
super-voxels, the smoothness constraint is imposed much more efﬁciently
over non-connected objects with similar dynamics.

Setting the correct value for N(S) is crucial to achieve good ﬂow esti-
mations. In our case, the size of N(S) is controlled by the parameter dim,
which deﬁnes the maximum distance (in voxels) between two region cen-
troids to consider them neighbors or not. Intuitively, we have reduced the
complexity of N(S) to one parameter per node that controls how global

or local we expect object dynamics to be. We can determine an appro-
priate value for the dmx parameter by qualitatively experimenting on
different volumes or testing against some ground truth (Sun et al.,
2008). Tables 1 and 2 show that it is possible to ﬁnd a single value that
works well across very different motion regimens. However, if the user
has a priori information of cell division locations or group motion, it is
straightforward to locally set the appropriate dmax for each region to
improve accuracy results.

3.3 Implementation details

To generate super-voxels, we use the available source code for SLIC
super-voxels (Achanta et al., 2012). Achanta et a]. (2012) is appealing,
as we can control the expected size of each super-voxel and its complexity
is linear in the number of voxels, making it a reasonable choice for large
3D volumes. Even if the volume consists of grayscale data, the generated
super-voxels (Fig. 3C) still respect most object boundaries. Since we have
a foreground mask, we tested two approaches: (i) ﬁrst calculate
super-voxels over the entire volume and then apply the mask; or
(ii) ﬁrst apply the mask and then calculate super-voxels only in the fore-
ground. Empirically, both approaches provide similar results, so we use
the second approach because it is faster.

To solve the optimization in Equation (3), we use the Limited memory
Broyden, Fletcher, Goldfarb and Shanno quasi-Newton method made
available by Byrd et a]. (1994). In particular, pp and pg are both defined
with the Huber cost function (Huber, 1981). Even though the Huber cost
function has a discontinuous second derivative, Li (1995) proved that the
function is regular enough to converge using quasi-Newton methods. We
use ﬁve-point ﬁnite difference along each dimension as well as tri-linear
interpolation to compute derivatives with subvoxel accuracy at any point
in the target volume. We ﬁlter the raw data with a small Gaussian
(a = 1.5) in each direction to smooth the gradient calculations. Finally,
as suggested in previous studies, we use a Gaussian pyramid on the vol-
umes to produce a coarse-to-ﬁne solution of the ﬂow. This pyramid not
only helps avoiding local minima in the optimization to resolve larger
displacements, but also speeds up convergence (Table 3). We also down-
sample the foreground/background mask and the super-voxels
accordingly. All these calculations are performed using a scale parameter
along each dimension, as it is common in microscopy volumes to have
anisotropic sampling along different axes.

4 RESULTS

We evaluate our approach in scanned light-sheet microscopy
datasets. Light-sheet microscopy provides exceptionally high
imaging speeds while minimizing the energy load on the

 

376

112 /310's112u1n0fp10}x0"sotJBuiJOJutotq/ﬁduq 11101} papBOIH/noq

9103 ‘Og isnﬁnV uo ::

Optical flow for time-lapse microscopy

 

Table 1. Stability and importance of parameter dmax to improve accur-
acy, for the test region without cell divisions

 

 

Method EE EE EE EE AUC
90%ile 95%ile 99%ile 100%ile
None 0.79 0.89 1.01 2.19 0.77
Our, climax: 10 0.13 0.34 1.86 2.27 0.93
dmax=25 0.12 0.15 0.39 1.51 0.98
dmax=40 0.13 0.16 0.34 0.48 0.97
ITK-demon 0.19 0.29 0.45 0.58 0.97
ITK-curvature 0.41 0.55 0.83 1.34 0.89

 

Each entry in the table is equivalent to a data point in the plots from Figure 5A. EE
X % ile indicates the XI11 percentile of the list ofEE errors for all nuclei in the ground
truth annotation.

Table 2. Stability and importance of parameter dmax to improve accu-
racy, for the test region with cell divisions

 

 

Method EE EE EE EE AUC
90%ile 95%ile 99%ile 100%ile
None 0.93 1.03 1.35 1.47 0.76
Our, aim: 10 0.40 0.51 0.76 1.23 0.93
dmax = 25 0.47 0.56 0.84 1.20 0.92
aim,x = 40 0.48 0.58 0.78 1.08 0.92
ITK-demon 0.86 0.98 1.28 1.39 0.84
ITK-curvature 0.81 0.91 1.28 1.58 0.82

 

Each entry in the table is equivalent to a data point in the plots from Figure 5B. EE
X % ile indicates the XI11 percentile of the list ofEE errors for all nuclei in the ground
truth annotation.

Table 3. Resulting accuracy when not using some of the modeling and
implementation techniques explained in Sections 3 and 3.3, for the test
region with cell divisions (Fig. 4C)

 

Method EE EE EE EE AUC Time (s)
90%ile 95%ile 99%ile 100%ile

 

None 0.70 0.93 1.03 1.35 0.80 0
Default 0.47 0.56 0.84 1.20 0.92 185
Pyramid levels=2 0.46 0.57 1.03 1.11 0.92 178
Pyramid level: 1 0.70 1.01 1.37 1.61 0.88 320
L2 0.51 0.62 0.89 1.21 0.91 181
Voxel-based 0.94 1.05 1.39 1.45 0.81 1754
SLIC step=3 0.85 0.98 1.31 1.19 0.84 191
SLIC step=7 0.49 0.61 1.11 1.76 0.92 169

Grid step: 3 0.93 1.04 1.29 1.41 0.83 170
Grid step: 5 0.84 0.97 1.34 1.42 0.86 161
Grid step=7 0.69 0.78 1.31 1.51 0.88 153
Watershed 0.45 0.55 0.85 1.40 0.92 174

 

The most signiﬁcant improvement is obtained by moving from a voxel—based regis—
tration to a super—voxelibased registration. However, all elements described in this
article improve optical ﬂow accuracy. The default method refers to our method with
the parameters deﬁned in Section 4.2. Section 1.3 in the Supplementary Material
contains a full description of implementation decisions involved in the deactivation
of algorithmic modules for each row in this table.

biological specimen, and has thus emerged as an essential tool for
life sciences. This combination of capabilities is invaluable for
live imaging applications and enables quantitative imaging of
cellular dynamics throughout the development of complex or-
ganisms such as entire Drosophila and zebrafish embryos (Fig.
1 and Videos in the Supplementary Material). Light-sheet micro-
scopes often produce terabytes of image data per specimen,
which need to be analyzed with efﬁcient computational
approaches.

We tested our approach in two different biological model
systems using previously published datasets of Drosophila
(Tomer et al., 2012) and zebraﬁsh (Keller et al., 2008). Two
Videos are included in the Supplementary Material to show the
complete results of the optical ﬂow estimation and how it allows
analyzing different motion patterns for different groups of cells.
Each volume of the Drosophila dataset consists of
602x 1386 x 110 voxels (179 MB in UINT16), and each pair
of time points was processed in 3min with our method (all
Central Processing Unit (CPU) running times reported in this
article were determined on a workstation with Intel® Xeon®
X5690 CPU with 3.47GHz clock rate). In total, we processed
50 time points (9 GB of data) following a cell division wave in
early development.

Each volume of the zebrafish dataset consists of
1064 x 1034 x 500 voxels (379 MB in UINT16), and each pair
of time points was processed in 9 min with our method. In total,
we processed 220 time points (83 GB of image data) to follow
epiboly and the formation of the body axis.

Additional evaluation of the proposed and baseline methods
using synthetic data is provided in the Supplementary Material.
We simulate ﬂuorescent nuclei images with different types of
motion (linear, cell division and Brownian), different
signal-to-noise ratios, different cell densities and different photo-
bleaching settings to show that our method is applicable to dif-
ferent types of ﬂuorescence microscopy techniques and cell
dynamics.

4.1 Baseline and ground truth

We compare our results with two common implementations of
optical ﬂow for 3D biomedical volumes available in the ITK
(Ibanez et al., 2003). Lombardot et a]. (2008) discussed these
implementations in the context of time-lapse light microscopy
for organism development at single-cell resolution. In particular,
we use the multi-scale ITK-demon optical ﬂow, which imple-
ments a multi-scale version of Thirion’s demon algorithm
(Thirion, 1998), as our first baseline. The algorithm has complex-
ity 0(|P|), where |P| is the number of voxels in the volume, and
solves Equation (2) with pc(r) 2 pl) (r) = r2. The second baseline
is a modiﬁcation of the ITK-demon algorithm using regulariza-
tion of the second derivative of the ﬂow instead of the ﬁrst order,
which has been shown to provide better convergence properties
for certain types of volumes (Fischer and Modersitzki, 2004).
This algorithm has complexity 0(|P|IoglPl), and we will refer
to it as ITK-curvature throughout the text. Both implementa-
tions are written in C++ using multi-threaded and multi-scale
techniques for efﬁcient handling of large biomedical datasets.
To quantitatively assess performance, we manually segmented
nuclei in two different regions of adjacent time points in the

 

377

112 /310's1au1nofp101x0'sopauuowtotq/ﬁduq 111011 papao1umoq

9103 ‘0g1sn8nv uo ::

F.Amat et al.

 

 

 

 

 

 

 

 

 

 

 

A450 B450 C
400 466 -\ - I 35“ x. x. x is, '
‘~\\‘::~:x‘*i‘iiil 182900100111  \\\ \
\‘1\\\‘ ‘11“‘1 “"N \‘W‘ 11"”1‘ 5 \

300 1‘ L \xx‘.‘ 01. \" \ i i' .11‘ I' 300 L“ \\\ or. \ 3 I ’1 i 7‘ I. \ N \ b
E‘ \\Q\\\\ 1.1! L‘t'lstl E \‘_\\\\ \3\ I‘L‘ll‘ 25034\—\_.
a: 250 \ ‘1 x b \ V I 1 l ‘1 $250 \\\ ‘L. x t \‘l L . J ’1 100 200
 \\\:.‘\\‘ ‘\ ‘ Ll‘ l’.  \\\:u“~\ 1‘ '11 iii ‘1 "(plxelsl
:20” *, tit :200 *1 .1’1 35" x. \ h H‘

\ u . r I \ 1 1' 1

150 ‘,‘.I--- “,1; 150 “.‘,I 1”}: :13 \\\

‘ 1 1 1 ‘ :4 l J ‘ . 1I 14 1' :; ’t )1 $1 \ \ \

100 , ~‘.' * ‘ ‘ {a 1,1 ’ 100 1.1' l'.’ (a 2: ’ é :\ \ \\

50 . . i ; 1 ' 50 1 . ' : . - 3“ K \ \

250—‘I—‘h—‘t—L
G 1 1 1 1 1 1 1 G 1 1 1 1 1 1 100 I (pixe|s) 200
0 50 100 150 200 250 300 350 U 50 100 150 200 250 300 350
1 (pixels) x (pixels)
D35o- _ _ .I _ , 1 E350' — . I. F 250
300- . A)“ 1 ,_ H 1 1r - 300, I ‘ .‘ E
- ' ' ~ - I \ _
, i/I .. 44K" 1“ 1 . 2 I ,.._ ._’\ -. a;
250- (1 1__ L .“\_\1_\ 1‘6 \ : 1,; 250- (3‘: :33 {1‘ h \ I "'1 K : I .3
‘7 t”! \r \ #1,! :9 ‘7 \ 4" 1 .__\ . ;
/ —.§V ~ ‘4. I h ._.,
ﬁzoo-qilL/‘fgf/(éf‘fe,_} $200-$111:I_A,27 g 1", 
E150 \i r‘1‘ ‘ ‘K‘LI‘L.’ ' k - -" T L E150 .‘,‘ ’1‘ Ft”:- Lgf | - N r1 "“ q
'a 1 '1 -1'. ‘ J 1‘ . 'a 1 '4 em" . ‘ J ' ’
u :l {I'- ,1 I- _, 4.," ‘ ---r '1, J — A ,_ ‘1 _‘1 2
77100 ii, .4! ’A’_"‘. .' ‘ "'1 ‘ p. 7.100. ‘57.}: .".'/'f.‘ ‘ ". . ‘ ‘— K‘
x' f“ 1.- ‘ u ,5 3* r.- ' x .- e
H --’ ’ 1 r “r .9 r \ E
50 1'11-‘ - vi. 511- urr-’ . '-:“‘ ﬂ
1 i’ . ‘ r ' i 1. ' 1' a f . ' ‘l " = “1 E.
\1 I ' 1 1 (i I, 1 I" 1111’ r r ‘ \1 \ 1 ‘ I ' ‘;
0 1H *1 'r J ‘1 11l 0- (‘311‘1’ “11"1':1
' ' ' 153300 4m
1110 200 300 400 500 100 200 300 400 500 I (ﬁlm's)
3: (pixels) x (pixels)

Fig. 4. (A) Motion ﬁeld (black: ground truth, red: estimate by our approach) projected on the Xtilane for a subregion of the volume in Figure 1 with
smooth ﬂow. Each arrow corresponds to a nucleus centroid. (B) Same as (A) for motion ﬁeld estimated by the baseline method multi-scale ITK-demon
(blue). (C) Enlarged subregion of (A) and (B). (D) Same as (A) for a subregion where cells are dividing, which translates into non-smooth dynamics for
neighboring nuclei. Our approach is still able to predict the correct motion for 99% of the nuclei. Supplementary Movie Sl shows the raw data and the
output of our optical ﬂow algorithm side by side for the entire time series. (E) Same as (B) for the subregion presented in (D). The complex dynamics
complicate setting a global motion smoothing parameter that works for all nuclei at the same time. (F) Enlarged subregion of (D) and (E). Most of the
ITK ﬂow (blue) results as zero because it cannot adapt to the complex motion pattern

Drosophila dataset using the software package Imaris (Bitﬂow).
Each region represents different dynamic regimens (Fig. 4). We
then manually assigned correspondences between segmented
nuclei to calculate the displacement (Fig. 4). Given that the
nuclei are textureless, we cannot assign unique voxel-to-Voxel
correspondences, and thus, our ground truth evaluates center
of mass displacement for each nucleus. We use the ﬂow EE
metric

EE(p) = (5)

 

deﬁned by Otte and Nagel (1994) to measure accuracy. Here 1) is
the center of mass for a given nucleus, vaT is the ground truth
ﬂow at centroid p and v; is the estimated ﬂow for each individual
algorithm. v; is estimated as the mean ﬂow of all voxels con-
tained within the segmentation mask for each nucleus in the
ground truth. Because a nucleus is usually split in several
super-voxels, this estimation can be seen as a weighted average
of the calculated optical ﬂow for each super-voxel proportional
to its size. Section 1.1 in the Supplementary Material contains
statistics on the accuracy of the ground truth vaT.

Once we have EE(p) for all nuclei, we can compute different

statistics to compare accuracy of different methods. Figure 5

displays the full cumulative distribution of errors, while
Tables 1, 2 and 3 display different figures of merit, summarizing
the information in the cumulative distribution. In particular, we
show several percentiles of the EE(p) distribution and the area
under the curve (AUC). This last figure of merit is typically used
in computer Vision applications with precision-recall curves, as it
summarizes the entire distribution in a single number. We nor-
malize the maximum AUC to 1 to simplify the comparison.

4.2 Results in light-sheet microscopy data

For the purpose of a quantitative performance analysis, we se-
lected two regions from two consecutive time points in the
Drosophila dataset and performed a ground truth annotation
for both of them. Figure 5A shows comparative results for the
first test region between time points 39 and 40. This region com-
prises 214 cells with an average diameter of 11 voxels moving all
in the same direction, although at different speeds. In this ex-
ample, the motion between cells is coherent, and thus, smooth-
ness constraints are sufﬁcient in most voxels to compensate for
lack of texture. In this case of simple dynamics, our method has
an average EE of 0.07, whereas the best ITK method has an
average EE (normalized by nuclei diameter) of 0.10. However,
tested on the same hardware, our implementation is consistently
10 x faster. In particular, it takes 3 min to converge for each 3D

 

378

112 /310's1au1nofp101x0'sopauuowtotq/ﬁduq 111011 papao1umoq

9103 ‘0g1sn8nv uo ::

Optical flow for time-lapse microscopy

 

Cumutative dislrib ution

 

 

Relative flowr error (flow error 1' nucleus diameter]

0 0.5 ‘I 1.5 2

.1

5'3
on

f3
m

   
 

.‘3
4:.

—- No registration

— Our method
—Multi—3cale ITK-curvature ;
—' Multi—scale ITK-demon 1

Cumulative dlstrib ution

0.2

 

 

0 0.5 ‘I 1.5 2
Relative flowr error [flow error i nucleus diameter)

Fig. 5. Optical ﬂow results for light-sheet microscopy using different methods. See text for details on ground truth deﬁnition. X -axis represents the BE
for each nucleus centroid normalized by the equivalent diameter of each nucleus. As a rule of thumb, values < 0.5 are considered good for most
quantitative applications, whereas values > 1.0 are not good. Values between 0.5 and 1.0 are acceptable, but ﬂow tracking has a higher error rate.
Method labeled as ‘None’ represents the original displacement without ﬂow estimation. Panel A shows results on data from Figure 4A and B. Panel B
shows results on data from Figure 4D and E. Our method improves accuracy over all baselines in both scenarios, on average, by 23%

volume, whereas both ITK algorithms require ~30min for the
same task. One of the main reasons for the speed improvement is
the dimensionality reduction achieved with super-voxels. As an
example, in this particular stack, there were 1 117 920 foreground
voxels, which resulted in 19 274 super-voxels, reducing the size of
the optimization problem ~60—fold.

Figure 5B shows a very different scenario from the same stack:
in this part of the embryo, nuclei are synchronously dividing, and
the motion ﬁeld transitions very rapidly from smooth to
non-smooth. In total, we performed a ground truth annotation
for 309 cells with an average diameter of 10 voxels between time
points 38 and 39. In this case, our method has an average EE
(normalized by nuclei diameter) of 0.16, and the best ITK method
has an average EE of 0.32. Figure 4A and Table 2 also show that
~1% of the nuclei are assigned to the wrong location using our
method (Supplementary Fig. S1 shows an enlarged View of the
location exhibiting the largest error). This error is due to the fact
that neighboring nuclei divide synchronously and two daughters
from different mother cells collide, causing the MRF to pull one
of them to the wrong location. This region of the volume pushes
the limits of optical ﬂow, as touching neighboring objects do not
have a coherent motion and suffer displacements larger than the
object size.

Tables 1 and 2 show the stability of parameter N(S) in
Equation (3). The accuracy results change gradually with the
value of dmax, and this allows us to use the same value for all
regions and still outperform other approaches. The only excep-
tion is 1% of the nuclei in the ﬁrst test region, which need an
increase in the smoothness constraint to be guided to the correct
location, especially at the edges of the MRF (Fig. 4A). In our
case, we used dmax = 25 voxels for both the Drosophila and
zebraﬁsh dataset, which is slightly more than the expected near-
est neighbor distance between adjacent nuclei (23 isotropic
Voxels). This result indicates that, in general, superior results
are obtained by directly considering motion information between
neighboring cells in the smoothness term, which cannot be
achieved with the usual pixel-wise regularization approaches.
However, Table 2 also shows that in extreme cases of incoherent

motion, such as during cell division, we could beneﬁt from redu-
cing dmax to 10 voxels. In this particular case, a cell division
detector (Huh et al., 2011) could be used to detect such events
and locally adjust the value of dmax. Supplementary Tables S1
and S2 in the Supplementary Material present a more
detailed analysis by decomposing the accuracy results in
Table 2 between dividing and non-dividing nuclei. An extended
accuracy analysis using synthetic data is provided in the
Supplementary Material, which further supports the conclusions
of this section.

Table 3 shows that all elements introduced in Sections 3 and
Section 3.3 are necessary to obtain the best accuracy and per-
formance. In particular, a region-based (SLIC super-voxels in
our case) and a multi-scale approach (of at least two levels) are
critical to deﬁne an appropriate data term and to avoid local
minima in Equation (3), respectively. Moreover, the use of
super-voxels that adapt to the sparse signal instead of fixed-size
rectangular-like regions [as suggested by Glocker et a]. (2008)]
improves accuracy as long as the super-voxels have a minimum
size. As the table entry using watershed shows, any oversegmen—
tation method producing reasonable super-voxels adapted to the
sparse data could be used within this framework.

All results discussed in this section were obtained with ﬁxed
parameters. For our method, we use ll 2 800, three levels in the
pyramid and dmax = 25. For Huber penalty, we use 8D 2 40,
which indicates intensity values are well preserved between
frames, and 8c 2 3. Finally, for the SLIC super-voxels, we use
STEP 2 5 and m = 10 [see Achanta et a]. (2012) for details]. For
both ITK implementations, we performed an optimal parameter
search using the ground truth to obtain the best performance.
Additionally, we use three pyramid levels for their multi-scale
scheme and applied the foreground mask filter for a fair com-
parison. Finally, we tested ITK algorithms on the raw stacks and
on cubic interpolated stacks to generate isotropic sampled voxels
to confirm that anisotropy in the data along the z-axis was not
compromising performance. The ﬁnal results (data not shown)
were undistinguishable, so we performed all comparisons with
the anisotropic data because execution time was shorter.

 

379

112 /310's1au1nofp101x0'sopauuowtotq/ﬁduq 111011 papao1umoq

9103 ‘0g1sn8nv uo ::

F.Amat et al.

 

5 DISCUSSION

We developed and tested a new model for optical ﬂow tailored to
microscopy volumes, in which a large fraction of the objects are
textureless and similar in appearance. Moreover, the information
in the volume tends to be sparse because many voxels do not
contain any information and cellular dynamics can be very vari-
able. A key idea in our approach is to generate a volume parti-
tion graph over the foreground voxels, and to perform optical
ﬂow directly on that model instead of computing it at the voxel
level. This model is tailored to the speciﬁc characteristics of
time-lapse light microscopy datasets, as it provides the regular-
ization needed to solve optical ﬂow robustly for these types of
volumes. At the same time, our method reduces the complexity
of the problem by an order of magnitude, which is an invaluable
advantage when working with large 3D datasets.

In Section 4.1, we showed that the method might fail in some
extreme cases for ~1% of the nuclei, when neighboring nuclei
move in opposite directions. In those scenarios, we are left only
with the data term to determine the correct ﬂow. Thus, a possible
future direction would be to use different features or point de-
scriptors in the volume intensity to increase robustness of the
data term (Brox and Malik, 2011; Liu et al., 2008). It is also
possible to constrain the ﬂow ﬁeld to a diffeomorphism, as
two objects cannot originate from the same source point.
Finally, if a faster implementation is required, it is straightfor-
ward to parallelize the computation of the data term in Equation
(3) for each super-voxel using GPU technology. At the moment,
this operation takes ~40% of the time for each function evalu-
ation in the quasi-Newton method, and it is thus a primary can-
didate for code optimization.

ACKNOWLEDGEMENTS

We would like to thank Kristin Branson for many helpful dis-
cussions and comments on the manuscript and the members of
the Myers and Keller labs for helpful feedback, in particular
Raju Tomer for the Drosophila dataset.

Funding: This work was supported by the Howard Hughes
Medical Institute.

Conflict of Interest: None declared.

REFERENCES

Abramoff,M.D. and Viergever,M.A. (2002) Computation and visualization of
three—dimensional soft tissue motion in the orbit. IEEE Trans. Med. Imaging,
21, 29(r304.

Achanta,R. et al. (2012) SLIC superpixels compared to state—of—the—art superpixel
methods. IEEE Trans. Pattern Anal. Mach. Intell., 34, 2274w2282.

Ayvaci,A. et al. (2010) Occlusion detection and motion estimation with convex
optimization. In NIPS’IO. MIT Press, Cambridge, pp. 103108.

Baker,S. et al. (2011) A database and evaluation methodology for optical ﬂow. Int.
J. Comput. Vis., 92, 131.

Black,M.J. and Anandan,P. (1996) The robust estimation of multiple motions:
parametric and piecewise—smooth ﬂow ﬁelds. Comput. Vis. Image Underst.,
63, 757104.

Brox,T. and Malik,]. (201 1) Large displacement optical ﬂow: descriptor matching in
variational motion estimation. IEEE Trans. PAMI, 33, 5007513.

Bruhn,A. et al. (2005) Lucas/Kanade meets Horn/Schunck: combining local and
global optic ﬂow methods. Int. J. Comupt. Vis., 61, 2117231.

Buibas,M. et al. (2010) Mapping the spatiotemporal dynamics of calcium signaling
in cellular neural networks using optical ﬂow. Ann. Biomed Eng, 38,
252072531.

Byrd,R.H. et al. (1994) A limited memory algorithm for bound constrained
optimization. SIAM J. Sci. Comput., 16, 119071208.

Delpiano,J. et al. (2011) Performance of optical ﬂow techniques for motion analysis
of ﬂuorescent point signals in confocal microscopy. Mach. Vis. Appl., 23,
6757689.

Fischer,B. and Modersitzki,]. (2004) A uniﬁed approach to fast image registration
and a new curvature based registration technique. Linear Algebra Appl., 380,
1077124.

Gkamas,T. and Nikou,C. (2011) Guiding optical ﬂow estimation using superpixels.
In International Conference on Digital Signal Processing. Corfu, Greece, pp. 1%.

Glocker,B. et al. (2008) Dense image registration through MRFs and efﬁcient linear
programming. Med. Image Anal., 12, 7317741.

Horn,B.K.P. and Schunck,B.G. (1981) Determining optical ﬂow. Aritiﬁcal Intell.,
17, 1857203.

Huber,P.J. (1981) Robust Statistics, 1st edn. Wiley—Interscience, New York.

Huh,S. et al. (2011) Automated mitosis detection of stem cell populations in
Phase—Contrast microscopy images. IEEE Trans. Med. Imaging, 30, 583596.

Ibanez,L. et al. (2003) The ITK Software Guide: The Insight Segmentation and
Registration Toolkit. Kitware Inc., Albany, NY.

Keller,P.J. et al. (2008) Reconstruction ofzebraﬁsh early embryonic development by
scanned light sheet microscopy. Science, 322, 106571069.

Khairy,K. et al. (2008) Detection of deformable objects in 3D images using
Markov—Chain monte carlo and spherical harmonics. In MICCAI.
Springer—Verlag, Berlin/Heidelberg, pp. 107571082.

Li,G. et al. (2007) 3D cell nuclei segmentation based on gradient ﬂow tracking.
BMC Cell Biol, 8, 40.

Li,W. (1995) Numerical estimates for the huber M—Estimator problem.
Approximation Theory, 8, 1710.

Liu,C. et al. (2008) SIFT ﬂow: dense correspondence across different scenes. In
ECC V. Springer—Verlag, Berlin/Heidelberg, pp. 28736.

Lombardot,B. et al. (2008) Evaluation of four 3d non rigid registration methods
applied to early zebraﬁsh development sequences. In MIAAB MICCAI. New
York.

Lou,X. et al. (2011) Deltr: digital embryo lineage tree reconstructor. In 2011 IEEE
International Symposium on Biomedical Imaging: From Nano [0 Macro. Chicago,
IL, USA, pp. 155771560.

Lucas,B.D. and Kanade,T. (1981) An iterative image registration technique with an
application to stereo vision. In IJCAI. Morgan Kaufmann Publishers, San
Francisco, pp. 674—679.

McMahon,A. et al. (2008) Dynamic analyses of drosophila gastrulation provide
insights into collective cell migration. Science, 322, 15431550.

Otte,M. and Nagel,H. (1994) Optical ﬂow estimation: advances and comparisons.
In ECC V. Springer—Verlag, Berlin/Heidelberg, pp. 51760.

Pizarro,L. et al. (2011) Towards dense motion estimation in light and electron mi—
croscopy. In ISBI. Institute of Electrical and Electronics Engineers (IEEE),
Washington D.C., pp. 193971942.

Pock,T. et al. (2007) A duality based algorithm for TV—Ll—optical—ﬂow image regis—
tration. MICCAI, 10 (Pt. 2), 5117518.

Preibisch,S. et al. (2010) Software for bead—based registration of selective plane
illumination microscopy data. Nat. Methods, 7, 4184119.

Prinet,V. et al. (2006) MRF modeling for optical ﬂow computation from
multi—structure objects. In ICIP. pp. 109371096.

Roberts,T. et al. (2010) Estimating the motion of plant root cells from in vivo
confocal laser scanning microscopy images. Mach. Vis. Appl., 21, 9217939.
Rubio—Guivernau,J.L. et al. (2012) Wavelet—based image fusion in multi—view

three—dimensional microscopy. Bioinformatics, 28, 2387245.

Sommer,C. et al. (2011) Ilastik: interactive learning and segmentation toolkit. In
ISBI. pp. 233233.

Sun,D. et al. (2008) Learning optical ﬂow. In ECC V. Springer—Verlag, pp. 83797.

Sun,D. et al. (2010) Secrets of optical ﬂow estimation and their principles. In
CVPR. pp. 243272439.

Thirion,]. (1998) Image matching as a diffusion process: an analogy with maxwell’s
demons. Med. Image Anal., 2, 2437260.

Tomer,R. et al. (2012) Quantitative high—speed imaging of entire developing em—
bryos with simultaneous multiview light—sheet microscopy. Nat. Methods, 9,
7557763.

Werlberger,M. et al. (2009) Anisotropic Huber—L1 optical ﬂow. In BM VC. London.

Xu,L. et al. (2008) A segmentation based variational model for accurate optical ﬂow
estimation. In ECC V. Springer—Verlag, Berlin/Heidelberg, pp. 671$84.

 

380

112 /810's1au1n0[p101x0'sopauumutotq/ﬁduq 111011 pap201um0q

9103 ‘0g1sn8nv 110 ::

