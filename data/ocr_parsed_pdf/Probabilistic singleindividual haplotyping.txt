BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

V.Kuleshov

 

majority of statistical methods do not use the partial phase in—
formation provided by long reads, and are not applicable to our
setting. A notable exception is a recent method called Hap—Seq
(He et al., 2012); without its statistical component it reduces to
the well—known exact exponential—time algorithm mentioned
above (He et al., 2010).

Also, there exists an extensive literature on the SIH problem
from the perspective of combinatorial optimization (Lippert
et al., 2002). Research in this field is aimed at optimizing com—
binatorial objectives such as minimum fragment removal, min—
imum SNP removal or MEC. This research is of a more
theoretical nature and aims at providing a rigorous theoretical
understanding of the SIH problem (Lippert et al., 2002).

3 RESULTS

3.1 Overview of PROBHAP

PROBHAP is based on a new exact dynamic programming solu—
tion for the SIH problem, which makes it more accurate than
many existing methods. Its main drawback is a higher computa—
tional cost: its worst—case running time increases exponentially
with the read coverage. Fortunately, modern long read technol—
ogies cover the genome at a relatively low depth (Duitama et al.,
2012; Kitzman et al., 2010), making it possible to apply our al—
gorithm to such data. In cases when the coverage is extremely
high, PROBHAP also uses a preprocessing heuristic to merge simi—
lar reads (see Section 4). In our experience, PROBHAP handles
long read coverages of up to 20x; however, it is not appropriate
for higher coverage short read datasets.

The output of PROBHAP is a set of haplotype blocks in the
format of RefHap and HapCut. In addition, PROBHAP also pro—
duces at each position three confidence scores that can be used to
identify locations where the phasing results are less accurate. The
posterior score represents the probability of correctly determining
the phase of a SNP with respect to the first SNP in the block. The
transition score represents the probability of correctly determin—
ing the phase of a SNP with respect to the previous one. Finally,
the emission score is often helpful in ﬁnding sequencing errors
and other issues with the underlying data.

Whenever the transition score is too low, we suggest breaking
the haplotype block at a position. Whenever the posterior or the
emission scores are low, we suggest leaving that position
unphased.

3.2 Comparison methodology

We compared PROBHAP to three state—of—the art algorithmsi
RefHap (Duitama et al., 2010), FastHare (Panconesi and
Sozio, 2004) and DGS (Panconesi and Sozio, 2004) as well as
to HapCut (Bansal and Bafna, 2008), a historically important
phasing package, and to MixSIH (Matsumoto and Kiryu, 2013),
the only method that we know that produces confidence scores.
Previous studies (Duitama et al., 2012; Geraci, 2010) have iden—
tified the above methods as being the current state—of—the—art in
single—individual haplotype phasing.

Note that we do not compare our method to HapSeq (He
et al., 2012) because this package additionally uses population—
based statistical phasing techniques to improve accuracy. We
also do not consider previously proposed exact dynamic

programming methods (He et al., 2010), as they do not scale
to long reads: their running time increases exponentially in the
number of variants in a read, and some of the reads in our
datasets have >50 variants.

The heuristics we consider work as follows. In brief, FastHare
sorts the input reads, and then traverses this ordering once,
greedily assigning each read to its most probable chromosome
given what has been seen so far. The DGS method is equally
simple: it iterates until convergence between assigning each frag—
ment to its closest chromosome, and recomputing a set of
consensus haplotypes. The RefHap and Hapcut algorithms con—
struct a graph based where each vertex is either associated with a
position (HapCut) or with a sequencing read (RefHap); then, the
algorithms approximately solve a MaxCut problem on this
graph.

We test the above methods on a long read dataset from
HapMap sample NA12878 that was produced using a fosmid—
based technology (Duitama et al., 2012). The long reads have an
average length of ~40 kb and cover the genome at a depth of
~3><. This dataset is a standard benchmark for SIH algorithms
(Duitama et al., 2012; Matsumoto and Kiryu, 2013) in part
because HapMap sample NA12878 has also been phased mul—
tiple times based on the genomes of its parents. In this work, we
take the trio—phased variant calls from the GATK resource
bundle (DePristo et al., 2011); these provide accurate phase at
1 342091 heterozygous variants that are also present in the long
read dataset.

We measure performance using the concept of a switch error
(Browning and Browning, 2011). A switch error is said to occur
when the true parental provenance of SNPs on a haplotype
changes with respect to the previous position. For example, if
the true SNP origins of a phased block can be written as MMFF,
then we say there is a switch error at the third position. In this
analysis, we differentiate between two types of switches: a long
switch corresponds to an inversion that lasts for more than one
position (e.g. MMFF); a short switch, on the other hand, affects
only a single position (e. g. MMFM). Switch accuracy is deﬁned
as the number of positions without switch errors, divided by the
number of positions at which such errors could be measured.
Long switch accuracy is deﬁned accordingly in terms of long
switch errors. We also measure accuracy in terms of switches
per megabase (Sw./Mb).

Finally, a block N50 length of x signiﬁes that at least 50% of
all phased SNPs were placed within blocks containing x SNPs or
more. The percentage of SNPs phased was defined as the number
of SNPs in blocks of length two or more, divided by the total
number of SNPs.

3.3 Results

Given comparable phasing rates and N50 block lengths,
PROBHAP produced haplotype blocks with more accurate long—
range phase: the long—range switch error of PROBHAP was 11%
lower than that of the second best algorithm, RefHap (Table 1).
In addition, PROBHAP also produced 6% fewer short switch
errors than RefHap.

Note that long switch accuracy is substantially more import—
ant than short switch accuracy, as it drastically changes the
global structure of haplotypes. Short switch errors, on the

 

i380

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

an?kgogmomammowoio~&o:3m7.omm\

 

V.Kuleshov

 

Table 3. Example of a 2 X 4 phasing matrix M, in which two reads cover
three positions each

 

 

1 2 3 4
Read 1 0 l 0 7
Read 2 7 1 0 0

 

computing such scores. Nonetheless, it phases chromosome 22 in
just under a minute; the total time for phasing a human genome
was under 30 minutes.

4 METHODS
4.1 Notation

Formally, an instance of the SIH problem is defined by a pair of n X m
matrices M, Q, whose columns correspond to heterozygous positions
(indexed by j= 1, . . . ,m), and whose rows correspond to reads (indexed
by i = 1, . . . , n). We refer to M as the phasing matrix; its entries take
values in the set {0, 1, —}. These values indicate the allele carried by a
read at a given position: for example, M,-,~ = 0 signiﬁes that read i covers
position j and carries allele 0 at j. A value of 7 indicates that read i did not
cover position j. See Table 3 for an example of a 2 X 4 phasing matrix.

The n X m matrix Q e [0, 1]"X'" is referred to as the q-score matrix; it
encodes the probability of observing a sequencing error at a given pos-
ition in a read. Such scores are available on virtually all sequencing
platforms.

A solution to an instance of the SIH problem consists of a pair of
vectors h e {0, 1}"7 and r e {0, 1}". The former determines the subject’s
haplotypes: at each genomic position j, it speciﬁes an allele hj e {0, 1}. We
consider only one haplotype, as the second is always the complement h of
the first. The second vector r 6 {0,1}" indicates the true provenance
r,» e {0, 1} of each read i (i.e. whether i was obtained from the ‘maternal’
or the ‘paternal’ copy; because we do not have information to deter-
mine which copy comes from which parent, we refer to them as 0, 1).
We also use

M) h,- ifr,»=0
'l‘i : _
J  lfl‘l*=l

to denote alleles on the haplotype from which read i originated.

Next, let Po(i)= {leij 75 —} denote the set of positions covered by
read i. Let also Hi: {hjlmin P00) 5 j 5 max Po(i)} be the set of haplo-
type variables spanned by read i and let Rj= {rilmin Po(i) 5
j 5 max Po(i)} be the set of read provenance variables spanning a pos-
ition j. We will use this notation to simplify several expressions through-
out the article. In particular, if position j is spanned by, say, reads 2, 3,
then we will use the notation maxR/ f(RJ-)=maxr,n f(r2, r3) and

ZR/ﬂRj) = erlﬂrz, r3).

4.2 Probabilistic model

We deﬁne the probability P(r, h, 0) over haplotypes h e {0, 1}'", assign-
ments of reads r e {0, 1}" and observed data 0 e {0,1,—}"X'" to be a
product of factors

P(r.h.a>71‘[ 1‘1 Holy-Inhj)1‘[P(ri>1‘[P(hj>w
[71 j=1

i= 1j:jePo(i) —

where

P(0ij|l‘i,  = Q1]  01] # hJOl)
l —  1f Olj=hj(l‘i)
is the probability of observing the allele on the j—th position in read i, and
the factors P(r,») and P(hj) are priors that we leave as uniform, except for
P(h1 =0): 1. This last choice eliminates the ambiguity stemming from
the fact that a solution h can be always replaced with its complement h; it
resolves this ambiguity by always choosing the solution with h] = 0.
Finally, note that the r and h variables are hidden, while the 0 variables
are observed; the observed values are deﬁned by the matrix M.

The dependency structure of P can be represented in terms of a
Bayesian network whose topology mirrors the two-dimensional structure
of the matrix M. See Figure 3 for the Bayesian network associated with
the phasing matrix in Table 3, which we gave earlier as an example.

4.3 Maximum likelihood haplotypes

We determine maximum-likelihood haplotypes h* = arg max,7 log P(o =
M |h) using the belief propagation algorithm, also known as max-sum
message passing over a junction tree (Koller and Friedman, 2009). In
brief, this algorithm involves groups of variables passing each other in-
formation about their most likely assignment; a well-known special case
of this method is the Viterbi algorithm for hidden Markov models
(HMMs).

4.3.] Definition of max—sum message passing We start by brieﬂy
deﬁning the max-sum message passing algorithm for graphical models.
Readers familiar with the subject may skip this subsection.

DEFINITION 1. Let P be a probability over a set of variables
X={X1,...,X,,} that is a product of k factors P=1—L:1 ¢i(X,»), with
each factor Q» being defined over a subset of variables X,» g X. A junction
tree T over P is a tree whose set of nodes is a family of subsets
C = {C 1, . . . , Cm}, with Cj g X and that satisfies the following properties:

(1) For each factor $1», there is a cluster c( i) such that X l» g C4,»).

(2) (Running intersection) IfX e C,» andx e Cj, then X 6 CA. for all Ck
on the unique path from C,- to C) in T.

Given this deﬁnition, we now deﬁne max-sum message passing. We
restrict our deﬁnition to the case when the junction tree Tis a path, which
is going to be the case for our model.

DEFINITION 2. Let P be a probability distribution as in Definition 1. Let T
be a junction tree over clusters C / for j= l, . . . ,m connected into a path and
ordered by j, with Cm serving as the root. The max-sum message from C / to
Cj+1 is a function M / defined over the variables in Cj ﬂ Cj+1 as

44jom Cj+1): max < Z 10% ¢i(Xi)+]Wj7l(Cj7l ﬂ CD)

GAG/H i:c(i)=j
with the additional definition that M0 E 0.

The max-sum message passing algorithm recursively computes the
above messages and determines that max XP(X) is

mCax < Z log¢.(X.)+Mm,1(Cm,1no»).

"' i:c(i)=m

The actual assignment that maximizes P can be found by storing the
variable assignments that maximize each M /. Unfortunately, proving
the correctness of this algorithm is beyond the scope of this article. For
a complete discussion that holds for arbitrary junction trees, we refer the
reader to a textbook on graphical models (Koller and Friedman, 2009).

 

i382

ﬁm'spzumofpmjxo'sopeuHOJmorq/ﬁdnq

55,2kgogmoddmmowoxwoa‘oﬁsambmﬁ

V.Kuleshov

 

P(h1) that the initial values equal F[h1 =0, R1] =P(R1) and
F[h1 =1,R1]=0; in addition, B[hm, Rm]: 1.
It is easy to show by induction that

FIhjv RjI = P(01:j~ hr Rf) (4)

BIth RjI :P(0j+1:thj~ Rib (5)

where 0k:1={o,j|k 51' 5 1}.

4.4.3 Computing conﬁdence probabilities From (4), (5), we can
now easily compute conﬁdence scores. One such score is the posterior
probability P(hj|01:m). It represents the probability that h was deter-
mined correctly with respect to h 1 and can be computed as
 = ZR/  Rj|01:m), where

P(hj~ RjI01:m)=P(01:jv hjv Rj)P(0j+l:thjv Rj)/P(01:m)-

Next, the transition probability P(hj|hj,1,01:m) represents the prob-
ability of consecutive SNPs being phased correctly; it can be used to
detect potential errors like the one shown in Table 4. We compute this
value using the identity P(hj|hj,1, 01:m)=P(hJ-,hj,1|01:m)/P(hj,1|01:m),
where the denominator is the posterior probability and the numerator
is computed as
ZR/,R/,l P(hj~ hj71~ Rb R171~ 0km)

P(01:m)
: EM P(01+1:mlhjv RI)T(hJ" Rh 01')
P(01:m) i

P(hjv hj7l I01:m):

where

T011» R1» 01-) = Z P(0,-Ih,-, Rj)P(h,-)P(R,-)P(0t,-. h,-. R.)

17),. ,RH

Additionally, we found that the emission probability P(0j|thJ-) was
usede in detecting errors in the data. Computing this value only involves
the expression P(0j|thJ-) = HIV-61300:) P(0ijIriv hf)-

Finally, note that in general, one can compute any set of probabilities
P(h,‘.|h1, 0m) in the model. However, this involves doing potentially up
to a full run of message passing.

4.5 A merging heuristic

The exact dynamic programming algorithm described above is practical
for coverages of up to 1(P12><. For deeper or for highly uneven cover-
ages, we propose a simple preprocessing heuristic. The heuristic consists
in reducing the coverage by repeatedly merging reads that are likely to
come from the same haplotypes until there are no reads that we can
conﬁdently merge.

To determine whether to merge reads k, l, we consider the ratio

I—IjePo(k)ﬂPo(l) (PW/tr 0v 0)P(01j~ 1v 0)+P(0kjv 1, 0)P(01j. 0. 0))
njepoaprom (P(0;g-, 0, 0)P(01j, 0, 0) + P(0kj, l, 0)P(01j, l, 0)) i

where P(o,‘.j, X, y) is shorthand for P(okj, rt. =X, hj= y). Intuitively, the
denominator is associated with the likelihood that the two reads come
from the same haplotype and the numerator is associated with the like-
lihood that the reads’ origins are different. Both terms are estimated by a
heuristic formula that decomposes over each position. If reads k, l are
merged, then position j of the resulting new read is assigned the allele that
has the highest q-score in the initial reads k, l (i.e. arg maxk‘1{Q,q-, Qlj});
the q-score at that position is set to the difference of the initial reads’
q-scores (i.e. |ij — Qljl).

In practice, one may select a conﬁdence threshold for (6) and only
merge reads that are below this threshold. We found empirically a
value of 1 — 10’9 to work well.

(6)

 

4.6 A post-processing heuristic

In addition, PROBHAP admits an extra post-processing heuristic for ad-
j usting the optimal haplotypes h*. This heuristic was initially proposed for
the algorithm RefHap; PROBHAP currently uses it by default, although it
can be disabled. The heuristic starts with the optimal read assignments r*
and determines at each position j a pair of sets

SW = {i|(r,» =0 ﬂ My: 0) U (ri=1 ﬂ Mlj=1)}
Sf] = {i|(r,» =0 ﬂ My: 1) U (ri=1 ﬂ M,»j=0)}.
It then outputs a new haplotype hnew deﬁned as
0 if ISj.0I>ISj.1I
hyw 7 1 if ISj.0I<ISj.1I
— otherwise.

We found that this heuristic increases the short switch accuracy of
PROBHAP on the NA12878 dataset; the long switch accuracy remains
the same. We suggest using this heuristic in settings where the quality
scores may not be well calibrated.

5 DISCUSSION: THEORETICAL ASPECTS

Interestingly, the probabilistic framework of PROBHAP general—
izes the SIH formalism on which most existing methods are based.
This allows us to easily derive well—known exact dynamic pro—
gramming algorithms as special cases of the variable elimination
algorithm for graphical models. More interestingly, the variable
elimination algorithm with different variable orderings results in
novel exact algorithms that are far more efficient than existing
ones.

5.1 Generalizing the SIH framework

In its standard formulation, the SIH problem consists in ﬁnding a
haplotype h that minimizes the MEC criterion:

MEC(h, M)

=Zmin|:/Z [(Mii=hj)7 Z “Mi/2%)]

i: 1 Z/EPOU) 12/6 P00)

where I : {True, False} —> {0, 1} is the indicator function, and the
remaining notation is the same as deﬁned in the Section 4. The
MEC measures the total number of positions within all the reads
that need to be corrected to make the reads consistent with a
haplotype h.

It is easy to show that the MEC objective can be recovered as a
special case of our framework. Indeed, if we deﬁne the factors
¢(o[,«, r,~, h,) (which we have previously set to P(o,:,«|r,~, h,)) in a way
that

exp (1) if 0v 7’5 ll/(ri)

(l) 01", Vi, h‘ =
( I, .1) exp (0) if og=hj("i)t

then log P(M, r, h) equals MEC(h, 114), although P is no longer a
probability.

Thus, our dynamic programming algorithms can also produce
exact solutions to the MEC objective, and just as interestingly,
they can produce conﬁdence probabilities associated with the
MEC.

 

i384

ﬁre'spzumofpmjxo'sopeuHOJmorq/ﬁdnq

Probabilistic single-individual haplotyping

 

5.2 Rederiving existing SIH algorithms

Interestingly, we can easily recover an existing dynamic program—
ming algorithm (He et al., 2010) for the MEC as a special case
of variable elimination in our graphical model. Indeed, con—
sider the junction tree deﬁned by n variable clusters
C,~= {r,~. h). oijlj e Po(i)} connected into a path ordered by i. If
we assume for simplicity that the data have no contained reads,
then the message from cluster i71 to cluster i during a run of
max—sum message passing with C" as the junction tree root
equals precisely

M(Hfﬂf+ 1) = maxmax 2 log P(0,"/|}",', h/)+M(Hiilmf) ,

i H;\i+1 jzhjeHi
(7)

where HW+1=Hiﬂ H,~+1 and H,~\,~+1=H,~\H,~+1. This is essen—
tially the well—known dynamic programming recursion (He et al.,
2010) we were looking to find.

Unfortunately, the time to compute the above recursion in—
creases exponentially in the length of the reads, which is precisely
the data we want to use for phasing.

5.3 Deriving novel SIH algorithms

Fortunately, as we have seen, we can derive from our framework
exact algorithms that are suitable for long read data.
Interestingly, these methods are in a sense dual to equation (7):
the structure of the probabilistic model P is entirely symmetric in
r, h. If we reverse h and r in Section 4, we obtain recursion (7).

Potentially, our framework allows deriving other exact algo—
rithms by defining alternative junction trees for the max—sum
message passing algorithm. One way to do this involves using
minimizing their tree—width using some well—known heuristics
(Koller and Friedman, 2009). Because the running time max—
sum message passing is exponential in the tree—width of a junc—
tion tree, this would lead to much faster running times.

6 CONCLUSION

In summary, we have introduced a new single—individual phasing
algorithm, PROBHAP, that offers an 11% improvement in accur—
acy over the current state—of—the—art method, RefHap. In add—
ition, it is one of the only methods to provide the user with
conﬁdence scores at every position; these confidence scores can
be used to prune positions whose phase is uncertain and thus
substantially increase the overall accuracy.

The advances behind PROBHAP are made possible by framing
the phasing problem within a probabilistic graphical models
framework. This framework makes it particularly easy to
reason about the problem; in fact, all our algorithms are special
cases of standard procedures for optimizing graphical models.

On the theoretical side, this work generalizes the MEC criter—
ion used by existing methods. Our approach allows us to obtain
existing algorithms as special cases of well—known optimization

procedures, and also easily derive new, more efﬁcient algorithms;
it may thus serve as a foundation for further algorithmic insights.

ACKNOWLEDGEMENT

We thank Sivan Berovici for important suggestions regarding the
model deﬁnition, as well as Dmitry Pushkarev and Michael
Kertesz for helpful discussions. This research was partly done
at Moleculo Inc.

Funding: This work was partly funded by NIH/NHGRI grant
T32 HG000044.

Conﬂict of Interest: none declared.

REFERENCES

Bansal,V. and Bafna,V. (2008) HapCUT: an efﬁcient and accurate algorithm for the
haplotype assembly problem. Bioinformatics, 24, i1537i159.

Bansal,V. et al. (2008) An MCMC algorithm for haplotype assembly from whole—
genome sequence data. Genome Res., 18, 133(71346.

Browning,S.R. and Browning,B.L. (2011) Haplotype phasing: existing methods and
new developments. Nat. Rev. Genet., 12, 7037714.

DePristo,M.A. et al. (2011) A framework for variation discovery and genotyping
using next—generation DNA sequencing data. Nat. Genet., 43, 4914198.

Duitama,J. et al. (2010) ReFHap: a reliable and fast algorithm for single individual
haplotyping. In: Proceedings of the First ACM International Conference on
Bioinformatics and Computational Biology. ACM, New York, NY, USA,
pp. 163169.

Duitama,J. et al. (2012) Fosmid—based whole genome haplotyping of a HapMap
trio child: evaluation of Single Individual Haplotyping techniques. Nucleic Acids
Res., 40, 204172053.

Geraci,F. (2010) A comparison of several algorithms for the single individual SNP
haplotyping reconstruction problem. Bioinformatics, 26, 221772225.

Gusﬁeld,D. (2001) Inference of haplotypes from samples of diploid populations:
complexity and algorithms. J. Comput. Biol., 8, 3057323.

He,D. et al. (2010) Optimal algorithms for haplotype assembly from whole—genome
sequence data. Bioinformatics, 26, i18¥i190.

He,D. et al. (2012) Hap—seq: an optimal algorithm for haplotype phasing with im—
putation using sequencing data. In: RECOMB'IZ: Proceedings of the 16th
Annual international conference on Research in Computational Molecular
Biology. Springer—Verlag, Berlin.

Kaper,F. et al. (2013) Whole—genome haplotyping by dilution, ampliﬁcation, and
sequencing. Proc. Natl Acad. Sci. USA, 110, 555275557.

Kim,J.H. et al. (2007) Diploid genome reconstruction of Ciona intestinalis and
comparative analysis with Ciona savignyi. Genome Res., 17, 110171110.

Kitzman,J.O. et al. (2010) Haplotype—resolved genome sequencing of a Gujarati
Indian individual. Nat. Biotechnol, 29, 59%3.

Koller,D. and Friedman,N. (2009) Probabilistic Graphical Models: Principles and
Techniques — Adaptive Computation and Machine Learning. The MIT Press,
Cambridge, MA.

Lippert,R. et al. (2002) Algorithmic strategies for the single nucleotide polymorph—
ism haplotype assembly problem. Brief. Bioinformatics, 3, 23731.

Matsumoto,H. and Kiryu,H. (2013) MixSIH: a mixture model for single individual
haplotyping. BMC Genomics, 14 (Suppl. 2), SS.

Panconesi,A. and Sozio,M. (2004) Fast hare: a fast heuristic for single individual
snp haplotype reconstruction. In: Jonassen,I. and Kim,J. (eds) Algorithms in
Bioinformatics. Springer, Berlin Heidelberg, pp. 26(7277.

Peters,B.A. et al. (2012) Accurate whole—genome sequencing and haplotyping from
10 to 20 human cells. Nature, 487, 19(kl95.

Voskoboynik,A. et al. (2013) The genome sequence of the colonial chordate,
Botryllus schlosseri. eLife, 2, e00569.

 

i385

ﬁle'spzumofpmjxo'sopeuuopnotq/pdnq

