BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

MITSU: stochastic EM for TFBS motif discovery

 

p(X,Z|6) over observed variables X and latent variables Z,
governed by parameters 6, the deterministic EM algorithm
(Dempster et al., 1977) maximizes the likelihood function
p(X|6) with respect to 6. This likelihood function is intractable
directly, so two steps are iteratively applied until some conver—
gence criteria are reached to maximize the likelihood function.
An initial estimate of the parameters is made, then the E—step
calculates the expected value of the log likelihood function, with
respect to the distribution of Z conditional on X under the
current estimate of the parameters 6”):

9(97 90)) = [Ez|x,9“‘ [1nP(X7 Z19” (1)

In the context of motif discovery, this can be viewed as calculat—
ing the probability for each width—w subsequence in the dataset
that it is an occurrence of the motif, or equivalently estimating
the position of occurrences of the motif within the input dataset.
The M—step then evaluates a new estimate of the parameters by
maximizing the expected value of the log likelihood function:

6(’+1)= argronax 9(6, 60)) (2)

In the context of motif discovery, this can be viewed as reesti—
mating the model parameters given the current estimates for the
motif position within the input dataset.

Stochastic variations of the EM algorithm first use Monte
Carlo methods to draw a set of samples {2(1), . . .,z(m)} from
the current approximation to the conditional predictive distribu—
tion p(Z|X, 60)), before replacing the integral in the E—step of the
EM algorithm Equation (1) with a ﬁnite sum over the drawn
samples. The modiﬁed E—step is thus

1 M
Q,+ [(9, 90>) % Mn; 1nP(X, Z(’”)|9) (3)

The M—Step then requires maximizing the Q function as before.
This particular variation on the EM algorithm is known as the
Monte Carlo EM (MCEM) algorithm (Wei and Tanner, 1990).

Stochastic EM (Celeux et al., 1995) can be viewed as a special
case of MCEM, where only one sample is drawn at each iter—
ation. In this case, the latent variables Z characterize which one
of the mixture components is responsible for each point in the
dataset, effectively making a ‘hard’ assignment of data points to
mixture components, rather than the probabilistic weightings
used by the EM algorithm. In the context of motif discovery,
this would assign each data point to either the motif model or the
background model. Formally, the sampling step (S—step, analo—
gous to the E—step in EM) of the sEM algorithm replaces the
computation of the Q function in the E—step by the simpler com—
putation of p(Z|X, 6m) and simulation of a ‘pseudosample’ z“).
The update step (U—step, analogous to the M—step in EM) up—
dates the model parameters 6“) on the basis of the ‘pseudo—com—
plete sample’ {X, 20)}, in the same way as normal.

As noted above, one of the reasons for stochastic variations of
EM being generally more successful than EM is that they have
the ability to avoid insigniﬁcant local maxima of the likelihood
function. This is achieved by choosing whether to accept or reject
the new set of proposed model parameters in the U—step of the
algorithm. Through this accept/reject mechanism, there is a non—
zero probability of accepting new model parameters with a lower

likelihood than the current parameters at each iteration of the
algorithm (Celeux et al., 1995). In contrast, deterministic EM is
guaranteed not to decrease the likelihood and so may become
trapped in local maxima or saddle points of the likelihood
function.

One signiﬁcant limitation of the SEAM algorithm is that only
the OOPS model is implemented. Bi (2007) suggests that the
OOPS model may be extended to the two—component mixture
(TCM) model (which is unconstrained with regard to the distri—
bution of motif occurrences) by ﬁrst discovering a motif using
the OOPS model, then scanning the input sequences to discover
further occurrences. However, this strategy may not be statistic—
ally robust. In this article, we take an approach that extends the
OOPS model naturally to the ‘zero or one occurrences per
sequence’ (ZOOPS) model, based on the original model deﬁn—
itions. We then continue this extension to a model that allows an
arbitrary number of motif occurrences in each input sequence,
using a previously described cutting heuristic.

3 METHODS
3.1 A sEM density for the OOPS model

The idea underlying existing algorithms for motif discovery, which im-
plement stochastic variants of EM (Bi, 2007, 2009), is to replace the
computation and maximization of Q(6, 60)) by the much simpler compu-
tation of p(Z,;J-= llXi, 6”), drawing a number of samples Z") (S-step),
followed by an update to 6 based on the pseudo-complete samples (X ,Z(’))
(U-step). A suitable density to represent an input sequence Xi is required.
We begin by conﬁrming that the density used by Bi (2007) to represent an
input sequence using the OOPS model is consistent with the OOPS model
derived by Bailey and Elkan (1994).

We generalize the expression introduced by Bailey and Elkan (1994) to
deﬁne the expectation of the missing data for position j in sequence 1'
using the OOPS model as follows:

17(XiIZiJ : L 9m)

4’} $142.,- = 1m a”) = L (4)

Hwi

Z p(XiIZ.»J=1.6<”)
[=1

where Li is defined as the length of input sequence 1', and w is deﬁned as
the motif width. Although Bi (2007) uses slightly different notation, we
conﬁrm that the deﬁnition used is equivalent to that of Bailey and Elkan
(1994). Deﬁning k as the set of nucleotides, that is, k e {A, C, G, T}, the
conditional probability of sequence 1' given the hidden variables is deﬁned
in both methods as follows:

p(X.»|Z.»,,- =1,6)é

H ﬁ glam-)4) 11/1 ﬂame-NH =k> (5)
01‘» rule

leAL/AcA 1r: 1k=A

This may be viewed as the product of two terms: the ﬁrst calculating the
probability of the background positions and the second calculating the
probability of the motif positions.

Here, we generalize the expressions used by Bailey and Elkan (1994) to
deﬁne the joint (log) likelihood function for the OOPS model as follows:

1np(X,Z|9)é

N Lrw+1 1 
Zi-l X,»Z,~=l,0 +Nl —

[Z]:  ‘1 HM I ‘1 ) nLi—w-‘rl

Again, despite notational differences, this can be shown to be equivalent
to the expression as deﬁned by Bi (2007).

 

i311

ﬁre'spzumofpmJXO'sopeuuopuorq/ﬁdnq

A.M.Kilpatrick et al.

 

To define a suitable density to represent an input sequence, Bi (2007)
substitutes Equation (5) into Equation (4); cancelling the ‘background’
terms and taking logs for efﬁciency results in the expression,

4’} =p(Z.:,- =1IXi,e<'>)=

1  T 91’}. (7)
map  1(Xi,j+171:/()ln @

1: 1k = A ,k
where <I>(i) is a normalizing factor such that

L;7u’+l

2 12(21): 1|)4.6<”)=1
j=1

Discussion of the sEM S- and U-steps is deferred to the following section,
where they are presented in the context of the ZOOPS model.

3.2 Extending sEM to the ZOOPS model

Here, we follow a similar method to derive an expression representing a
sequence in the ZOOPS model. The ZOOPS model, introduced by Bailey
and Elkan (1994), assumes that the input sequences contain ‘zero or one
occurrences per sequence’. The ZOOPS model requires an indicator vari-
able that denotes whether a particular input sequence contains a motif oc-
currence. Here, the indicator variable Qi is deﬁned as Qié  Z”.
That is, Q = 1 if sequence 1' contains a motif occurrence and d otherwise.
The conditional likelihood for a sequence containing a motif occurrence
remains the same [Equation (5)]. The conditional likelihood for a sequence
that does not contain a motif occurrence is now deﬁned as follows:

LI T
p(Xi|Qi=0, H 93891-10 

l=1k=A

Deﬁning an additional variable y as the prior probability of a motif occur-
ring in a sequence and assuming a uniform prior distribution for motif
occurrences within a sequence, it follows that the prior probability of a
position in sequence ibeing a motif start site is

V
Zi.=19=
17(J  L

— 9
i—w-l-l ()

For simplicity, the model parameters are now collected and denoted as
¢ = (6, y). It is noted that the model parameters now include the prior prob-
ability of a sequence containing a motif occurrence, in addition to the motif
and background models from the OOPS model. It can be shown that the log
likelihood function for the complete data in the ZOOPS model can be
generalized as follows:

N 1471.41
1np(X,ZI¢)-Z< Z zijlnmxilzw-Le)

1:1 j=1

N
+20 — Qi)1np(X.IQi=o,e)
F1 (10)

N
V

‘1' i —

ngnLi—w-l-l

N
+Z(1—Qi)1n(1—y)
i=1

The expectation of the missing data for the ZOOPS model is therefore

(I)
170412211: 1w emu—y—

 

Zv? : i’lf‘l’l 11
U 17(XilQi =0, 9<'>)(1 _ yam. ( )
Lruv+1 (I) yo)
Xi 21’ =1, 9 —
EN | .1 )Li_w+1

[=1

It can be shown that substituting Equations (5) and (8) into Equation (1 l)
as required, then cancelling terms yields

 

I)_

ﬁﬁ<9wk)l(xt/+wl:k)
_’ y
w=1k=A 99" 12
(Li—w-l-l)(l—y(’))+ ( )
L;fl:ﬁﬁ<9wk)l()ﬁl+wl—k) }
1:1 w=1k=A 901*

our expression representing a sequence in the ZOOPS model.

The S-step of the sEM algorithm is implemented as described previ-
ously (Bi, 2007), drawing a sample j,“ from Equation (12) for each input
sequence ie {1, ...,N}. The U-step of the sEM algorithm requires the
construction of a proposal model 6’ based on the samples from the S-
step. The parameter updates provided by Bi (2007) are altered here to
account for the fact that not every sequence may contain a motif occur-
rence. The expected values of the Q variables are used to weight the
samples from each sequence 1'. Here we deﬁne the parameters of our

proposal model as
N

Z I(X.;,-,»+..y,1 = k) Q?” + m
at... =  (13)
Z Z I(X.;,-,»+..y,1 = k) Q?” + 5
i=1 k=A

for w e {1, . . ., W} and k e {A, C, G, T}. The parameters of the back-
grOLTlnd model are not updated, but could be reestimated if required. )3 =

>:A [BA is a vector of pseudocounts, equivalent to a Dirichlet prior
distribution. We also require an update for the other parameter y. It
can be shown that the proposal value for the fraction of sequences con-
taining a motif occurrence is just that, based on the values of Q?” calcu-
lated in the S-step:

/ 1 N
y = NZ Q?” (14)
1

As in SEAM (Bi, 2007), the Metropolis algorithm is used to decide
whether to keep our updated parameters. The energies of the current and
proposal models, G(¢(’)) and G(¢’), respectively, are calculated (how this
is done is described in Section 3.4) and the change in energy calculated:

AG = CW”) — G(¢’)- (15)
The Metropolis ratio is deﬁned as
aM(¢’, W) = min {1, exp(—AG)} (16)

A random number u ~ Unif[0, l] is drawn and the parameters updated to
the proposal parameters only if u is less than or equal to the Metropolis
ratio, that is,

/ ~ / (t)
6(!+1)::6uxk v If“: €¥M(¢~¢ )7 
ink (I) .
6M, otherw1se,
for we {l,..., W}andke {A,C,G,T}and
' / (I)
W1): V, lquaM(¢.¢ ), (18)
y"), otherwise.

3.3 Removing the ZOOPS constraint

The ZOOPS model still enforces constraints on the distribution of motif
occurrences; it is assumed that each input sequence contains at most one

 

i312

ﬁre'spzumofpmJXO'sopeuuopnorq/ﬁdnq

MITSU: stochastic EM for TFBS motif discovery

 

occurrence of a motif. However, there are many biological examples of
promoter sequences that contain multiple copies of the same TFBS
(Bembom et al., 2007). This is the primary motivation for the TCM
model introduced by Bailey and Elkan (1994), which allows an arbitrary
number of non-overlapping motif occurrences in each input sequence.

The likelihood function for the TCM model is more computationally
complex than those for the OOPS and ZOOPS models. As a result, exact
methods based on the TCM model have been avoided in favour of more
tractable approximations (Bembom et al., 2007). The TCM model pro-
posed by Bailey and Elkan (1994) uses a derived dataset consisting of all
overlapping subsequences of width w from the original dataset. Some
proportion of these subsequences are motif occurrences; the remainder
are background. While the subsequences in this derived dataset are ne-
cessarily overlapping, the likelihood function is based on a sample of
independent sequences (Bembom et al., 2007). An additional smoothing
step is required to reduce the degree to which two overlapping subse-
quences can both be assigned to the motif component of the model.

Keles et a]. (2003) propose an alternative cutting heuristic, which in-
volves deriving a different dataset from the original, then applying the
ZOOPS model to each of the derived sequences. The main advantages of
this method are that no additional steps are required to deal with the
assumption of independence, and the approximation to the likelihood
function is improved. This method is improved by Bembom et a].
(2007) and we implement a similar method here. Brieﬂy, the original
dataset is cut into subsequences of a given length U, such that each sub-
sequence contains the first (w 1) positions of the next subsequence. The
ZOOPS model is then applied to this derived dataset. The previous stu-
dies implementing this heuristic have shown that the method is fairly
robust with respect to the choice of cut length U but have suggested
that this parameter may be optimized using cross-validation (Bembom
et al., 2007; Keles et al., 2003). Here, the cut heuristic is implemented as
an inner loop within the motif discovery algorithm (Section 3.5). The
ZOOPS model is applied to derived datasets with varying values of U,
and the parameter settings that yield the highest energy value are returned
as the best motif model. We show in Section 4.3 that the cut heuristic in
combination with the ZOOPS model successfully allows discovery of
multiple copies of the same motif within a single input sequence, in the
context of motif discovery using sEM.

3.4 Deﬁning an energy function

The original energy function used in the SEAM algorithm (Bi, 2007)
becomes problematic when used with the cut heuristic used to implement
discovery of multiple motifs within a single input sequence. The main
problem stems from the fact that the energy function

T W T
 9(1le eo‘k‘i‘z Z 9]"le 911A») 
k=A

j=1k=A

is scaled by the number of input sequences N; this is assumed to be
constant in the SEAM algorithm and means that energies cannot be
compared between datasets with differing values of N. Using the cutting
heuristic means that the value of N may double, or triple, depending on
the cut length (U). A way of fairly comparing motif energies is required.
We are further interested in the properties of the energy function, par-
ticularly how it varies with changing motif conservation and varying
values of y. Here, we propose a modiﬁcation to the original energy func-
tion such that

1 T W T
 = —N  901‘» 111 901‘» ‘1‘ Z Z 9]"le 911A») 
y k=A j=1 k=A

This modiﬁed energy function is maximized with a perfectly conserved
motif occurring in each input sequence, and the yN factor cancels in the

case of datasets derived by the cut heuristic. It can be shown that the
following useful properties hold:

(1) If two motifs are perfectly conserved, the motif with the higher
number of occurrences will have a higher energy.

(2) Given two motifs of equal prevalence and unequal motif conser-
vation, the motif discovery algorithm will tend to discover the
motif with the higher energy (equivalently, the higher motif
conservation).

(3) All else being equal, a higher proportion of sequences containing a
motif occurrence will yield a higher energy.

We adopt this modiﬁed energy function in MITSU but note that other
alternative energy functions may be possible; because the sEM accept/
reject mechanism is based on a difference of energies, substituting other
energy functions based on the model entropy should have little effect on
this mechanism.

3.5 MITSU algorithm
The pseudocode of MITSU is given as follows:

 

procedure MITSU algorithm
create Markov background model
for w = Wm,” to Wan do
for cut length in {set of cut lengths}do
for n random seeds do
for-y: 1/JNto 1 by><2do
run sEM on cut dataset using ZOOPS model at width w:
until convergence do
S-step (Equation 12)
U-step (Equations 13718)
end
end
end
return the best motif model over n random seeds & varying y
end
return the best motif model over all cut lengths
end
estimate most likely width 1% using MCOIN
return motif model and list of predicted sites for W
end MITSU algorithm

 

Although satisfactory convergence results for sEM and related algo-
rithms have been obtained (Diebolt and Robert, 1990, 1994), designing a
stopping rule for sEM is challenging; Jank (2005) notes that a simple
deterministic stopping rule may be triggered by what is a chance ﬂuctu-
ation stemming from the S-step of the algorithm. Following the recom-
mendations of Booth and Hobert (1999), we implement a deterministic
stopping rule for several iterations to reduce the chance of a premature
stop. After each iteration, the Euclidean distance between the previous
and current motif models is calculated. If this distance is below a given
threshold for three successive iterations, the algorithm is deemed to have
converged; we choose the threshold here as 103. Stochastic EM generally
takes longer to converge than deterministic EM (on tests with the CRP
dataset used in Section 4.3, deterministic EM was approximately ﬁve
times faster than MITSU, based on testing 1000 random seeds).
However, as noted above, sEM usually converges faster than full stochas-
tic methods. We accept this longer running time in exchange for increased
accuracy in terms of predicted motif occurrences. We compare the con-
vergence of MITSU with that of deterministic EM in Section 4.2.

Motif occurrences are predicted using a Bayes-optimal classiﬁer that
has been described previously by Bailey and Elkan (1994). Following the
ZOOPS model, we predict at most one motif occurrence per sequence in

 

i313

ﬁre'spzumoiproyo'sopeuuopnorq/ﬁdnq

A.M.Kilpatrick et al.

 

Table 1. Realistic synthetic data: classiﬁcation results

 

 

 

Conservation (mean bits/col) Deterministic EM SEAM MITSU

sSn sPP V AUC sSn sPP V AUC sSn sPP V AUC
2.00 0.84 0.25 0.99 1.00 1.00 7 0.70 0.74 0.97
1.49 0.26 0.07 0.98 0.93 0.93 7 0.90 0.97 1.00
1.08 0.02 0.01 0.96 0.49 0.49 7 0.68 0.77 0.99
0.76 0.00 0.00 0.94 0.09 0.09 7 0.17 0.19 0.94
0.51 0.00 0.00 0.93 0.06 0.06 7 0.07 0.08 0.93

 

Note: sSn, sPPVand AUC for ﬁve collections of realistic synthetic data with varying levels of motif conservation. Best results are printed in bold. In these tests, motif discovery

was carried out only at the known motif width.

Table 2. Escherichia coli data: classiﬁcation results

 

 

 

Conservation(mean bits/col) Deterministic EM SEAM MITSU

sSn sPPV AUC sSn sPPV AUC sSn sPPV AUC
‘High’ (1.36) 0.81 0.22 0.96 0.67 0.67 7 0.54 0.75 0.98
‘Low’ (0.78) 0.63 0.41 0.96 0.65 0.65 7 0.57 0.71 0.97
Overall (1.13) 0.74 0.30 0.96 0.66 0.66 7 0.55 0.73 0.98

 

Note: sSn, sPP V and AUC for 20 datasets created using previously characterized Ecoli TFBS sequences. Best results are printed in bold. In these tests, motif discovery was

carried out only at the experimentally determined motif width.

the cut dataset; the cut heuristic means that more than one occurrence per
sequence may be predicted when these predictions are mapped back to
the original dataset.

4 RESULTS AND DISCUSSION

Here, we summarize and discuss the results of a number of tests
that illustrate the advantages of a sEM—based approach for motif
discovery and the performance advantages of MITSU in particu—
lar. Algorithmic performance is assessed through mean site—level
sensitivity (sSn), mean site—level positive predictive value (sPPV)
and the area under the receiver operating characteristic (ROC)
curve (AUC). These measures are commonly used to assess the
performance of motif discovery algorithms, for example, in the
studies of Hu et a]. (2005) and Tompa et a1. (2005). Following
these studies, a predicted motif site is deﬁned as a true—positive
result if it overlaps the true site by at least a quarter of the motif
width.

4.1 Stochastic EM outperforms deterministic EM

MITSU was evaluated quantitatively using a mixture of realistic
synthetic and previously characterized real data. Datasets were
constructed as described previously (Kilpatrick et al., 2013).
Brieﬂy, ﬁve large data collections each consisting of 1000 datasets
were constructed using synthetic motifs of varying conservation
and realistic Escherichia coli background sequence extracted from
the EcoGene database (Rudd, 2000). A sixth data collection con—
sisting of 20 datasets was constructed using known Ecoli TFBS

sequences extracted from RegulonDB (Gama—Castro et al., 201 1).
Finally, a data collection consisting of nine datasets was con—
structed using known TFBS motif sequences from diverse pro—
karyotic species. These motif sequences were discovered by ChIP
methods. Background sequences for these datasets were con—
structed using synthetic data, altering the probability of choosing
each nucleotide to reﬂect the species GC—content as required.
Tables 173 summarize the results of the tests on these data collec—
tions. For comparison, we also include the results of a determin—
istic EM—based motif discovery algorithm (Kilpatrick et al., 2013)
and SEAM. AUC results are not available for SEAM, as con—
structing a ROC curve requires ordering all subsequences accord—
ing to their probability of being a motif occurrence. This is not
possible in SEAM as a result of the method of prediction used.

4.1.] Realistic synthetic data Based on the results on realistic
synthetic data shown in Table 1, we note that 5511 and sPPV
decrease with decreasing motif conservation for all three tested
algorithms. We have noted this behaviour previously in deter—
ministic EM (Kilpatrick et al., 2013) and attribute the decrease in
5511 to fewer sites being predicted overall and the decrease in
sPPV to the background sites better matching the motif sites
as conservation decreases, leading to an increase in the number
of false—positive results.

We note that, in the majority of tests, the results of MITSU
outperform those of both the deterministic EM algorithm and
SEAM, particularly with regard to 5511 and sPP V. The increased
performance at lower levels of motif conservation is particularly
notable. The success of MITSU is attributable to making fewer,

 

i314

ﬁm'smumoipiqxo'sopeuuopnorq/ﬁdnq

MITSU: stochastic EM for TFBS motif discovery

 

Table 3. Diverse prokaryotic data: classiﬁcation results

 

 

 

Conservation (mean bits/col) Deterministic EM SEAM MITSU
S Sn Sppv AUC sSn sPP V AUC sSn sPP V AUC
0.99 0.75 0.67 0.99 0.86 0.86 7 0.88 0.92 1.00

 

Note: sSn, sPP V and AUC for nine datasets created using real prokaryotic data detemiined through ChIP experiments.

discovery was carried out only at the experimentally detemiined motif width.

but more accurate, predictions. The predictions made are gener—
ally more cautious; previous false—positive predictions are now
more likely to be classiﬁed as true—negative predictions. This sig—
niﬁcant reduction in the number of false—positive predictions ex—
plains the large increase in the sPPV values.

We also note that the 5511 and sPPVresults for the sEM—based
algorithms are less biased. The results for the deterministic EM
algorithm, particularly at high levels of motif conservation, are
skewed towards increasing 5511; that is, fewer false—negative pre—
dictions were made at the expense of having more false—positive
predictions. SEAM and MITSU are more unbiased in this
respect, producing fewer false predictions in general.

4.1.2 Escherichia coli and prokaryotic ChIP data Tables 2 and 3
present the results of tests on previously characterized E.c0li
TFBS sequences and TFBS sequences from diverse prokaryotes
determined by ChIP experiments, respectively. The general trend
remains the same: both sSn and sPPV decrease with decreasing
motif conservation. We have reported previously that determin—
istic EM—based motif discovery achieves better classiﬁcation re—
sults on previously characterized E.c0li data than could be
expected given realistic synthetic data of a similar conservation
(Kilpatrick et al., 2013). Again, we attribute this improvement in
performance to the differences in motif structure. Whereas the
conservation of the synthetic motifs used here is independent of
position, Eisen (2005) notes that real TFBS motifs with low
mean conservation often have clusters of well—conserved pos—
itions; we believe that differences in the distribution of high
and low conservation across true motifs in comparison with syn—
thetic motifs explains the improvement in performance on real
data. We note a similar trend here with the results of SEAM and
MITSU, particularly at lower levels of motif conservation.

As with the realistic synthetic data, MITSU is shown to in—
crease sPPV by making fewer, more accurate, predictions
(Table 2). We note that the 5511 values are decreased to lower
than the corresponding values from deterministic EM and (to a
lesser extent) SEAM. This is a side effect of predicting fewer sites
overall: ‘borderline’ predictions that may have been classiﬁed as
true—positive results previously are now classed as false—negative
results owing to the more cautious predictor. However, as with
the realistic synthetic data results, we note that the 5511 and sPPV
values for MITSU are now less biased. Although MITSU uses a
Bayes—optimal classiﬁer for site prediction, the results of the
E.c0li tests here suggest that a better balance between 5511 and
sPPV may be achieved with a different predictor. However, we
note that the complexity of the computational problem and the

a

an
a

an

5
on

True positive rate

True positive rate
I
u :1

2

ulﬁ
2
2

on 0| 02 as on 05 on (H in us 04 05
False positive rate False positive rats

Fig. 1. ROC curves (plotted for 05sFPR50.5) for the E.coli TorR
motif discovered by the deterministic EM algorithm (left) and MITSU
(right). Curve colour illustrates the threshold of p(Z,;J-= 1|XU. 6), from
highest (red) to lowest (blue)

wide structural variety of TFBS motifs may mean that it is not
possible to improve on all measures in all cases.

MITSU is shown to be particularly effective in cases where
the deterministic EM—based algorithm returned poor results.
Figure 1 displays ROC curves for the E.c0li TorR motif as dis—
covered by both the deterministic EM and MITSU algorithms.
This motif was poorly discovered by the deterministic EM algo—
rithm (5511 = 0.10, sPPV = 0.03, AUC = 0.83); however,
MITSU increases performance over all measures (5511 = 0.30,
sPPV = 0.50, AUC = 0.98). As noted above, the signiﬁcant im—
provement in sPPVis attributable to predicting fewer sites over—
all, reducing the number of false—positive results. In this case, the
improvement in 5511 is a result of an improved motif model,
which better fits the known occurrences. Sequence logos repre—
senting the motifs discovered by both algorithms are shown in
Figure 2. Similar improvements in performance are also seen for
the E.c0li FruR and RscB motifs.

Table 3 shows that for the diverse prokaryotic motifs, MITSU
outperforms deterministic EM and SEAM in terms of all three
performance measures. We note that the increase in sPPV is
most signiﬁcant. This result may be of particular interest to
biologists, as it means that fewer false—positive results are pre—
dicted: sites which are predicted now are therefore more likely to
be true TFBS occurrences. As with the E.c0li motifs above, we
notice signiﬁcant increases in performance for motifs that were
relatively poorly discovered by deterministic EM, for example,
the E.c0li CRP and RutR motifs and the Bacillus subtilis Sp00A
motif.

Further tests were carried out in which the MCOIN heuristic
was used to determine the most likely motif width from a range
of plausible widths (:l:4 bp of the experimentally determined

 

i315

Best results are printed in bold. In these tests, motif

ﬁm'smumoipiqxo'sopeuuopnorq/ﬁdnq

 

:39\Ewowsmoaﬁmowoxmoagoﬁsambwﬁ

 

 

 

deterministic EM

:1 MITSU

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

:39\Ewowsmoaﬁmowoxmoagoﬁsambwﬁ

A.M.Kilpatrick et al.

 

REFERENCES

Bailey,T.L. and Elkan,C. (1994) Fitting a mixture model by expectation maximization
to discover motifs in biopolymers. Proc. Int. Conf. Intel] Syst. Mol Biol, 2, 28736.

Bailey,T.L. et a]. (2010) The value of position—speciﬁc priors in motif discovery
using MEME. BMC Bioinformatics, 11, 179.

Bembom,O. et a]. (2007) Supervised detection of conserved motifs in DNA se—
quences with cosmo. Stat. Appl. Genet. Mol Biol, 6, Article 8.

Bi,C. (2007) SEAM: a stochastic EM—type algorithm for motif—ﬁnding in biopoly—
mer sequences. J. Bioinform. Compat. Biol, 5, 47777.

Bi,C. (2009) A Monte Carlo EM algorithm for de novo motif discovery in biomo—
lecular sequences. IEEE/ACM Trans. Compat. Biol. Bioinform., 6, 3707386.
Booth,J.G. and Hobert,J.P. (1999) Maximizing generalized linear mixed model like—
lihoods with an automated Monte Carlo EM algorithm. J. R. Stat. Soc. B

Methodol, 61, 2657285.

Celeux,G. et a]. (1995) On stochastic versions of the EM algorithm. Rapport de
Recherche—Institat National de Recherche en Informatiqae et en Aatomatiqae, No
2514.

Dempster,A. et a]. (1977) Maximum likelihood from incomplete data via the EM
algorithm. J. R. Stat Soc B Methodol, 39, 1738.

Diebolt,J. and Robert,C. (1990) Bayesian estimation of ﬁnite mixture distributions:
part II, sampling implementation. Technical Report, 1]]. Laboratoire de
Statistique Theorique et Appliquee, Universite Paris VI.

Diebolt,J. and Robert,C. (1994) Estimation of ﬁnite mixture distributions through
Bayesian sampling. J. R. Stat Soc B Methodol, 56, 3637375.

Eisen,M. (2005) All motifs are NOT created equal: structural properties of tran—
scription factor—DNA interactions and the inference of sequence speciﬁcity.
Genome Biol, 6, P7.

Gama—Castro,S. et a]. (2011) RegulonDB version 7.0: transcriptional regulation of
Escherichia coli K—12 integrated within genetic sensory response units (Gensor
Units). Nucleic Acids Res., 39 (Suppl. 1), D987D105.

Hu,J. et a]. (2005) Limitations and potentials of current motif discovery algorithms.
Nacle‘w Acids Res., 33, 489971913.

Jank,W. (2005) Stochastic variants of EM: Monte Carlo, Quasi—Monte Carlo and
more. Proc. Am. Stat. Assoc.

Keles,S. et a]. (2003) Supervised detection of regulatory motifs in DNA sequences.
Stat. Appl. Genet. Mol Biol, 2, Article 5.

Kilpatrick,A.M. et a]. (2013) MCOIN: a novel heuristic for determining transcrip—
tion factor binding site motif width. Algorithms Mol Biol, 8, 16.

Lawrence,C.E. and Reilly,A.A. (1990) An expectation maximization (EM) algo—
rithm for the identiﬁcation and characterization of common sites in unaligned
biopolymer sequences. Proteins, 7, 41751.

Lawrence,C.E. et a]. (1993) Detecting subtle sequence signals: a Gibbs sampling
strategy for multiple alignment. Science, 262, 2087214.

Rudd,K.E. (2000) EcoGene: a genome sequence database for Escherichia coli K—12.
Nacle‘w Acids Res., 28, 60—64.

Spivakov,M. et a]. (2012) Analysis of variation at transcription factor binding sites
in Drosophila and humans. Genome Biol, 13, R49.

Stormo,G.D. and Hartzell,G.W. (1989) Identifying protein—binding sites
from unaligned DNA fragments. Proc. Natl Acad. Sci. USA, 86,
118371187.

Tompa,M. et a]. (2005) Assessing computational tools for the discovery of tran—
scription factor binding sites. Nat. Biotechnol, 23, 1377144.

Wei,G.C. and Tanner,M. (1990) A Monte Carlo implementation of the EM algo—
rithm and the poor man’s data augmentation algorithms. J. Am. Stat. Assoc.,
85, 6997704.

Whitﬁeld,T. et a]. (2012) Functional analysis of transcription factor binding sites in
human promoters. Genome Biol, 13, R50.

Yip,K. et a]. (2012) Classiﬁcation of human genomic regions based on experimen—
tally determined binding sites of more than 100 transcription—related factors.
Genome Biol, 13, R48.

 

i318

ﬁm'sreumoipiqxo'sopizuuopiioiq/pdnq

