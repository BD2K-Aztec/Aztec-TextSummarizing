Biainfarmatics, 32, 2016, i322—i331
doi: 10.1093/bioinformatics/btw269
ISMB 2016

 

Faster and more accurate graphical model
identification of tandem mass spectra using
trellises

Shengjie Wang1, John T. Halloranz, Jeff A. Bilmes1'2'* and
William s. Noble1'3'*

1Department of Computer Science and Engineering, 2Department of Electrical Engineering and 3Department of
Genome Sciences, University of Washington, Seattle, WA 98195, USA

*To whom correspondence should be addressed.

Abstract

Tandem mass spectrometry (MS/MS) is the dominant high throughput technology for identifying
and quantifying proteins in complex biological samples. Analysis of the tens of thousands of
fragmentation spectra produced by an MS/MS experiment begins by assigning to each observed
spectrum the peptide that is hypothesized to be responsible for generating the spectrum. This as—
signment is typically done by searching each spectrum against a database of peptides. To our know—
ledge, all existing MS/MS search engines compute scores individually between a given observed
spectrum and each possible candidate peptide from the database. In this work, we use a trellis, a
data structure capable ofjointly representing a large set of candidate peptides, to avoid redundantly
recomputing common sub—computations among different candidates. We show how trellises may
be used to significantly speed up existing scoring algorithms, and we theoretically quantify the ex—
pected speedup afforded by trellises. Furthermore, we demonstrate that compact trellis representa—
tions of whole sets of peptides enables efficient discriminative learning of a dynamic Bayesian net—

 

work for spectrum identification, leading to greatly improved spectrum identification accuracy.

Contact: bilmes@uw.edu or william—noble@uw.edu

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

A critical problem in medicine and biology is accurately identifying
the proteins in a complex mixture, such as a drop of blood.
Solutions to this problem have many important applications, includ—
ing the early detection of diseases and congenital defects (Walters
et al., 1996). The most widely used high throughput technology to
identify proteins in complex mixtures is tandem mass spectrometry
(MS/MS), whose output is a collection of tens of thousands of frag—
mentation spectra, each of which ideally corresponds to a single gen—
erating peptide. The core problem in the interpretation of MS/MS
data involves identifying the peptide responsible for generating each
observed spectrum, which we call the spectrum identiﬁcation
problem.

The most accurate methods to solve the spectrum identification
problem employ a database of peptides (reviewed in Nesvizhskii,
2010). Given an observed spectrum, peptides in the database are
scored, and the top scoring peptide is assigned to the spectrum. The
pair consisting of an observed spectrum and a peptide sequence is
referred to as a peptide-spectrum match (PSM).

©The Author 2016. Published by Oxford University Press.

In this work, we show how trellises may be used to make this
database search significantly more efficient and accurate. A trellis is a
data structure capable of representing an exponential size collection
of strings in polynomial space. Trellises have been used to speed up in—
ference in hidden Markov models (Huang and Soong, 1991; Jelinek,
1997; Young et al., 1997), dynamic Bayesian networks (DBNs) and
dynamic graphical models (DGMs; Ji et al., 2006). In the context of
MS/MS, we use a trellis to compactly represent the collection of can—
didate peptides associated with an observed fragmentation spectrum,
i.e. peptides whose masses are close to the observed peptide mass
associated with the spectrum. Using the trellis allows for the sharing
of computation across candidate peptides. We describe how to apply
trellises to any scoring function expressible as a DGM. This includes
linear scoring functions such as the SEQUEST XCorr (Eng et al.,
1994), the score functions employed by XlTandem (Craig and Beavis,
2004), Morpheus (Wenger and Coon, 2013), MS—GF+ (Kim and
Pevzner, 2014) and OMSSA (Geer et al., 2004), as well non—linear
methods such as our recently proposed DBN for Rapid Identification
of Peptides (DRIP; Halloran et al., 2014).

i322

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.U/),
which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact

journals.permissions@oup.com

112 /310'slcumofp1q1xo"soiJBuiJOJuioiq/ﬁduq 11101} popcolumoq

91oz ‘Og anﬁnv uo ::

Trellises

i323

 

Table 1. Notation used in this article

 

Symbol Function

 

x observed spectrum of length Tx, x, is a (m/z, intensity) pair
t index along m/z axis for DGM expansion, t = 0, . . . , T — 1
T DGM frame unrolling amount and number of peaks in

observed spectrum
.1! , J!" precursor mass, precursor mass of spectrum x
K W‘ precursor charge, precursor charge of spectrum x

be

number of bins of m/z axis quantization (i.e. typically 2000
for low—resolution data)

number of theoretical peaks in some peptide

9? binned and processed observed spectrum of length B
9?, difference observed spectrum (used in XCorr).

y a peptide

My length of y

M

9

binned sparse theoretical vector of length B (dot product
with binned observed spectrum: (97, ﬂ)

length My sequence of increasing incremental m/z values of
fragment ions of )1 (used for trellis)

set of strings y E Y to be compressed into a trellis.

vector of peaks indices of y of length T

subsequence of °y from position t1 to t2

vector of peaks types of y of length T

a 3020 .<\ 2.
b"

n,-, n,, n, trellis nodes

1 trellis links

A trellis DGM transition variable

N trellis DGM node variable

L trellis DGM link variable

2, observed child variable for use in a DGM (i.e. E, = 1 always)

 

Note: x and its variations denote the observed spectrum for different meth—
ods/contexts. y and its variations denote the theoretical spectrum for different
methods/contexts.

For common MS/MS application settings, we prove and quantify
the extent to which, in expectation, determining the top scoring data in—
stance in a trellis is substantially more efficient than scoring each data
instance individually. We then demonstrate empirically that trellises
provide a significant reduction in the computational costs of the XCorr
and DRIP scoring functions, ranging from 12— to 16—fold speedups in
the low—resolution and high—resolution datasets examined here.

Next, we show that trellises may be used to discriminatively
train DBNs for MS/MS, leading to significantly improved peptide
identification accuracy. In particular, we modify the DRIP model,
which was originally trained via maximum likelihood estimation, to
instead employ discriminative training via maximum mutual infor—
mation (MMI) estimation (Povey, 2003).

Maximum likelihood estimation maximizes the log—likelihood
given a set of high—confidence PSMs, whereas MMI estimation
maximizes the conditional log—likelihood between the high—
confidence PSMs and a large ‘background’ collection of alternative
PSMs. Thus, MMI estimation encourages learned parameters which
both explain the high—confidence labels well and discriminate
against the background labels. However, MMI estimation is costly
because it requires computing denominator quantities over whole
sets of peptides before being able to take a single gradient step in the
parameter space; this is infeasible if considering each peptide in the
set individually. We demonstrate that using trellises renders the dis—
criminative training procedure feasible. Furthermore, we present
empirical evidence that this discriminative approach significantly
improves identification accuracy relative to the generatively trained
DRIP model (Section 7.2), resulting in 11.2% and 6.2% more cor—
rect identifications at a stringent false discovery rate (FDR) of 0.5%
for two datasets. The general trellis—based discriminative training

procedure used herein is applicable to any DGM for any class of
problem, but we are unaware of any previous work that does this.

The article is organized as follows. In Section 2, we formally de—
fine the spectrum identification problem and introduce the two data—
base search scoring functions, XCorr and DRIP. We then define
trellises in Section 3, which we utilize to compress the theoretical
spectra of candidate peptides to be scored during database search.
We show how graphical models may be used to efficiently traverse
elements in a trellis (Section 3.3), enabling easy combination with
any scoring function represented as a graphical model. Thus, in
Section 4, we show how the vastly different scoring functions,
XCorr and DRIP, may be represented as graphical models. The com—
bination of trellises with these graphical model scoring functions
(Section 5) allows for significantly faster database search (Section
7.1). The highly compressed trellises allow feasible discriminative
training for DRIP (Section 6), providing significantly more accurate
peptide identification accuracy (Section 7.2).

2 Database search in tandem mass
spectrometry

We describe the spectrum identification problem as follows. Given
an observed spectrum x (check Table 1 for notations in the paper)
with precursor m/z J!" and precursor charge (6", and given a data—
base D, we wish to find the peptide y E D responsible for
generating x. Using the precursor m/z and charge, we constrain
the set of peptides to be scored by setting a mass tolerance threshold,
to, such that we score the set of candidate peptides
D(.//Z",‘€",D,w) :{yzy ED, lm(y)/‘€x —.//Z") S to}, where m(y) de—
notes the mass of peptide y. Denoting an arbitrary scoring function
as f(y,x), the spectrum identification problem, for a given x, in—
volves finding:

V E argmax f(yrx)- (1)

yED(.Il",‘K",D,u/)

We study two very different database search algorithms:
SEQUEST (Eng et al., 1994) and DRIP (Halloran et al., 2014).
SEQUEST begins by quantizing and preprocessing the observed
spectrum x into a vector 3?. Given a candidate peptide y, a theoret—
ical spectrum y (typically, a sparse vector) is constructed from y
with length equal to that of 3?. This yields the XCorr score function:

75
XCorr(x,y) : yT (a? — &T—755€r> : PTJAC’, (2)
where 3?, denotes the vector shifted by 17 m/z units. XCorr is thus a
foreground minus a background inner product, and is hence linear.

Our recently proposed DBN—based scoring function, DRIP
(Halloran et al., 2014), constructs a potentially non—linear alignment
between the theoretical and observed spectra. A peptide’s theoretical
spectrum is given, and hidden variables are used to represent two
MS/MS alignment phenomena: insertions, which are observed peaks
that do not match a theoretical peak, and deletions, which are theor—
etical peaks that do not match an observed peak. The most probable
alignment, i.e. sequence of insertions and deletions, is calculated via
the max—product (or the Viterbi) algorithm (Bilmes, 2010), and pep—
tides are scored using the log—likelihood of the most probable align—
ment. A key advantage of DRIP over most other scoring functions is
that the alignment costs are automatically deduced using a machine
learning method such as maximum likelihood estimation or (in the
present paper) conditional likelihood.

112 /310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOTCI”Idllq 11101} popcolumoq

91oz ‘Og isanV uo ::

i324

S. Wang et aI.

 

For these two scoring functions, we define their corresponding
DGMs in Section 4. We note that the DGM used to model XCorr
may be used to model any linear MS/MS scoring function, including
functions employed by many commonly used search algorithms (e.g.
XlTandem, the base scores for MS—GF+ and OMSSA, and
Morpheus). By combining these DGMs with trellises, we efficiently
model the scoring of all candidate peptides in Equation (1) within a
single data structure (Section 3), affording efficient discriminative
training for MS/MS (Section 6) and significantly faster database
search (Section 7.1).

3 Trellises

Trellises are powerful structures capable of representing an expo—
nentially large set of sequential data hypotheses compactly and effi—
ciently, often using only linear space. For example, natural language
dictionaries can be stored in trellises for more efficient querying; in
speech recognition, trellises constructed out of the top phoneme (or
word, or sentence) hypotheses (e.g. N—best lists) can be used to re—
score and select the best hypothesis much more efficiently than a
simple linear max computation over all N scores (Dyer et al., 2008;
Huang and Soong, 1991; Jelinek, 1997; Young et al., 1997). In this
section, we first formally define a trellis, then show how to effi—
ciently construct a trellis given a large (exponential) set of strings,

ttle

 

and lastly show how to apply the constructed trellises to MS/MS
scoring.

A trellis over an alphabet 2 is a directed graph G : (¥, 1L, n5, n,),
where ¥ is the node set, L is the link set, and n5, n, 6 ¥ denote the
source and target nodes, respectively. To avoid confusion, we use
the terms ‘vertex’ and ‘edge’ for graphical model graphs (Section
3.3), and we use ‘node’ and ‘link’ for trellis graphs. Every link I 6 L
is a tuple (n1,n2, or(l)), where n1, n2 are the from—node, and to—node
respectively, and «(1) E 2 is the alphabet element encoded in 1. Each
path in the trellis from n, to n, represents a sequence. Thus, letting
P(G, n5, nt) denote the set of paths from n, to n,, every
p : 11,12, . . . ,lipi, p E P(G,n5,nt) represents a sequence of charac—
ters, or a string, over alphabet 2: «(11),or(lz),...,or(lipi). Also, let
P(G, 11,12), where 11,12 6 IL, denote the set of paths starting with 11
and ending with 12.

For a set of strings Y : {331,332, . . . ,ym}, the trellis representa—
tion of the strings G(Y) can be extremely compact because the elem—
ents of Y might be highly redundant. For example, if )9,- and )3,- share
some substring in common, then we can merge the common parts
into a sequence of common links. Figure 1A shows a trellis over four
character strings, with the shared substrings collapsed into common
trellis links to reduce redundancy.

The common links of trellises not only reduce memory require—
ments when representing a set of strings but can also speed up com—
putations over the encoded string set, sometimes quite significantly.

sea 2 1 8 243 3 1 4 33 1
‘ 2:
201
_ EAKK 275 329 0 /
(A) (B)

G = {{ns, m}, 12), 72.5, m} // initialize an empty
trellis ;
Assign an ordering in E, and sort Y
lexicographically;
for g] E Y do

Split :1} into preﬁx g[1 : i — 1] and sufﬁx (1) "ac" '> G (2) “ad” '> G c

 : end], where y[1 : i — 1] is the longest W . a Q A

preﬁx that is consistent within G;

Traverse G with preﬁx 7][1 : i — 1] stopping at

node n’ ; (4) "bd" -> G c

Build a chain structure C out of the sufﬁx (5i DFA Minimization

 : end], where each link on the chain a

corresponds to one character;
Merge G into G by merging the start node of C
with n’ and the end node of G with m;

end

Run DFA minimization on G; Return G;

(C) (D)

 

G
-Q-Q~

 

Fig. 1. (A) A trellis encoding ‘seattle', ‘seafood', ‘kungfu' and ‘tofu'. n5,no,n1, and n, are trellis nodes, and every arrow corresponds to a trellis link, e.g.
(n5,no,'sea'). (B) An example of a simple trellis for MS/MS scoring functions, consisting of the theoretical peaks (discretized b/y-ions) for three peptides: ‘ELAK',
‘EALK' and ‘EAKK'. Every edge corresponds to the m/z value of a fragment ion rounded to the nearest integer. Three colored paths from source node nS to sink

node n, correspond respectively to three peptides. The observed spectrum is charge +2 with low-resolution fragment ions. (Cl Trellis construction algorithm that

takes as input a set of strings Y and the corresponding alphabet 2 and returns a trellis representation of Y. (D) Sample trellis construction for strings: ‘ac', ‘ad',

‘bc' and ‘bd'.

112 /3.10'spzu.rnofp1q1xo"soiJBMJOJutoiq//:d11q 11101} popcolumoq

9103 ‘Og isnﬁnv uo ::

Trellises

i325

 

For this article, we focus on the task of DGM inference with the
Viterbi algorithm. Trellises allow us to reduce the state space of the
Viterbi algorithm and to apply smart pruning strategies more effect—
ively, achieving orders of magnitude reductions in computation time
(Section 7.1).

3.1 Trellis construction

Constructing the optimal trellis from the input set of strings Y over
alphabet 2 is a difficult problem. The objective of the ‘optimal’ trel—
lis is task dependent. For example, for natural language dictionary
queries to be computationally optimal, the trellis should have a min—
imal number of nodes. For data compression, trellises are often
stored as a node list and a link list, where each entry of the link list
records the starting/ending node and possibly some additional fea—
tures. The optimal trellis should, thus, be minimal in overall size, so
both the nodes and links matter. Moreover, some tasks do not re—
quire the trellis to be an ‘exact’ representation of the input strings.
For a set of strings Y, let  be a trellis representation, and for a
trellis G, let 'll‘(G) be the set of strings represented by the trellis. We
define an exact trellis to be one where precisely  : Y. For
our task of speeding up DGM training/inference for MS/MS data—
base search, our objective is to construct a trellis  : (¥, lL, n5, n,
) that is exact but where UL] is minimized.

Constructing the optimal trellis is a hard problem, as we can
think of a trellis as a non—deterministic finite automaton (NFA;
Hopcroft et al., 2001), and it has been proven (Schnitger and
Gramlich, 2005) that NFA minimization in terms of the number of
states/transitions (trellis nodes/links) is NP—hard to approximate
within a constant factor. We have hence developed a heuristic algo—
rithm (Fig. 1C) that is similar to the determinize—minimize proced—
ure of Watson (1993) but that is specialized to sets of MS/MS
theoretical spectra. The resulting trellis  is the minimum state
deterministic finite automaton (DFA; Hopcroft et al., 2001) of the
given language of peptides Y.

An example run of this algorithm for the strings ‘ac’, ‘ad’, ‘bc’
and ‘bd’ is depicted in steps (1)—(5) of Figure 1D. The for loop,
which merges prefixes of input strings, constructs a DFA out of Y.
Minimization on the constructed DFA can be thought of as a process
that merges nodes which share the same suffixes. Both merging pre—
fixes and suffixes reduce the number of links in the trellis, making
the algorithm a powerful heuristic in practice. The complexity of the
algorithm is bounded by the DEA minimization step. With
Hopcroft’s algorithm (Hopcroft, 1971), the running time is
O(]2]]Y]lmaxlog(]Y]lmax)), where lmax : maxiiEY 

3.2 Trellises for MS/MS scoring functions

To speed up the scoring of a set of candidate peptides, we construct
a trellis, GP, consisting of the set of theoretical peaks from the candi—
date peptides to be scored (i.e. peptides whose masses lie within the
specified precursor mass tolerance window). The alphabet 2,, for G,
contains all possible peak m/z values (in Thomsons) discretized ac—
cording to the resolution of the dataset (e.g. for low—resolution frag—
ment ion spectra, the values are rounded to the nearest integers).
Figure 1B gives an example of a trellis constructed over the theoret—
ical spectra of three peptides.

For each observed spectrum, a trellis is thus constructed contain—
ing the theoretical peaks of all peptide candidates within the corres—
ponding mass window. Each trellis then becomes a ‘database’ to
search for the best peptide candidate match. All trellises (one for
each 1 Th mass bin) are pre—computed and stored during a database
indexing step prior to search. The complexity of construction is

bounded by the DFA minimization step as stated above
(O(]2]]Y]lmaxlog(]Y]lmax))), and for trellises of theoretical peptide
peaks, ]2] is the number of distinct peak m/z values and is a constant
based on the resolution of the data,  is the number of theoretical
peptides in the querying mass window, and lmax is the number of
theoretical peaks of the longest peptide candidate in the mass
window.

We next show how MS/MS trellis traversal can be expressed and
implemented with DGMs (Section 3.3). This enables the combined
use of trellises with both linear and non—linear MS/MS scoring func—
tions expressed by DGMs (Section 4), thereby allowing significantly
faster database search (Section 5.1) and efficient generative and dis—
criminative training for improved identification accuracy (Section
7.2).

3.3 Traversing trellises using DGMs

A graphical model compactly represents the factorization properties
of a family of probability distributions defined over a set of random
variables. In a graphical model’s graph, vertices represent random
variables and edges denote allowable direct interaction between
variables. A Bayesian network is one type of graphical model that
uses directed acyclic graphs. DGMs are defined over temporal se—
quences where each element in the sequence (called a frame) is repre—
sented by a repeated set of vertices and edges. DGMs provide a great
deal of modeling power and ﬂexibility, offering a calculus with
which to construct widely varying and potentially very complex
models to reason about the underlying data while providing strat—
egies to maintain tractable inference. DBNs are DGMs where the
graphs are directed and acyclic.

As in Ji et al. (2006), we can use DGMs to traverse over a trellis.
At frame t, we use three vertices to access the trellis: a trellis—node
vertex V,, a trellis—link vertex L,, and a transition vertex A,.
Intuitively, V, corresponds to a node in the trellis, L, corresponds to
a link in the trellis and A, controls the traversal of the trellis. V,_1,
V,, and A, determine the set of possible values for L,, with each value
of L, corresponding to one character in the encoded strings.

Our trellises can be represented as a DGM structure (Fig. 2).
Values V, : n,- and A,+1 : d (d 2 0) determine the allowable set of
trellis nodes VH1 E {n,- E N]E|p E P(G,n,-,n,-), ]p] : d} (VH1 has 0
probability for values not in the allowable set, and has the same
probability for all values in the allowable set). Also, values V, : n,-,
VH1 : n,-, and A,+1 : d together determine the allowable set of
links L,“ E {l E lL]Elp E P(G,n,-,n,-), ]p] : d,p[d — 1] : 1}. Thus,
L,+1 is a random variable corresponding to all links that go into n,-
and can be reached from n,- with a path of length d. If d: 0 (i.e. a

Chuck to unroll for

Prolo ue
9 frames t = 1, T —

E'lo e
2 pr gu

A : :A,_

 
 
 
  

V0 '

Lt Contains the data to access.

L

 
  
 

Fig. 2. DBN for traversing a trellis. L, corresponds to the set of links being tra-
versed, which contains the data to access. The value of L, is decided based on
the previous node V,,1, the current node V,, and the transition A,.

112 /3.10'spzu.rnofp1q1xo"soiJBMJOJutoiq//:d11q 11101} popcolumoq

9103 ‘Og isanV uo ::

i326

S. Wang et al.

 

zero length path connecting two nodes), then the algorithm stays at
the current node and the link stays put as well, i.e. L,+1 : L,. In the
simplest case, A, is binary (so A, 6 {0,1}), so that if A,+1 : 0, the al—
gorithm stays at the current node and link, and if A,“ : 1, L,“
may be the outgoing link incident to n,-, and VH1 the set of corres—
ponding destination trellis—nodes for those outgoing links. Taking
Figure 1A as an example, suppose V, : {n0, n1} and A,+1 : 1, then
VH1 : {n,} and L,+1 : {’ttle’, ‘food’, ‘fu’}. The ‘FIRST_NODE’
vertex is observed to be the node value n, and is used only for initial—
izing the time—dependent structure.

The complexity for constructing the conditional probability table
(CPT) to store Pr(L,]V,, VH1, A,+1), which is required for traversing
the trellis as described above, is ]{(l1,l2) : l1,l2 6 IL, Elp E
P(G, l1,l2), ]P] S dmax}], where dmax is the largest value A, may take
(i.e. the maximum number of deletions). The CPT is likely quite sparse
because many paths do not exist. The value of dmax can vary based on
the underlying DGM. If only link—by—link traversal is desired, then
dmax : 1. Setting dmax : oo encodes all subsequences of the data in—
stance in the trellis. The CPT can be constructed online to save mem—
ory. Our implementation in the Graphical Model Toolkit (GMTK)
(Bilmes and Zweig, 2002) efficiently supports sparse trellis CPTs.

A trellis DGM representation is applicable to any DGM that
accesses data in a sequential manner. Rather than accessing data in
the traditional way, the trellis representation, moreover, offers two
major benefits over accessing sets of sequences in the traditional
way, namely: (i) various pruning and approximate inference strat—
egies can be applied locally that speed up the underlying DGM sig—
nificantly, since pruning causes many trellis paths to be removed
simultaneously; (ii) compressed trellis representation makes prac—
tical certain expensive learning methods that requires access to the
entire set of data, such as discriminative training. Below, we show
how we take advantage of both of these benefits in our trellis/DGM—
based peptide—spectrum score functions.

4 Graphical model MS/MS scoring functions

4.1 Linear scoring via graphical models
Many MS/MS scoring functions, including XCorr, XlTandem, the
base scores for MS—GF+ and OMSSA, and Morpheus (Wenger and
Coon, 2013), are linear in the theoretical and observed spectra (i.e.
they constitute a dot product between two preprocessed vectors cor—
responding to the theoretical and the observed spectrum). Using vir-
tual evidence (Bilmes, 2004; Pearl, 1988), where the conditional
distributions of observed child variables may be unnormalized non—
negative scores, we may easily represent any MS/MS linear scoring
function in a graphical model as the log—likelihood of a mixture—like
model. This can then be combined with the aforementioned trellis
constructs. In this section, we first describe linear score functions
and then, in Section 4.2, show how a non—linear score function is
also compatible with trellises.

Let E, be an observed child variable that is always observed to be
a fixed and known value (e.g. unity). Such a child is known as ‘vir—
tual evidence’ since it may be used to impart a soft version of evi—
dence into a model as follows: given a distribution Pr(y,) over a
random variable y,, the construct Pr(Z, ]y,)Pr(y,) is a function of only
y, but is a generalization of evidence for 31,. For example, if
Pr(Z,]y,) : 5(3), : it), where 5 is a Kronecker delta, then this would
be the same as y, being observed at value 37,. If, on the other hand, Pr
(Z,]y,) is a non—negative vector (indexed by y,) of real values, this im—
parts virtual evidence for different values of 3),. Another construct we
utilize (Bilmes and Zweig, 2002) is that of ‘switching parents’ and

‘switching weights’. Let a, be what is known as a switching parent,
and consider a conditional distribution Pr(Z,]y,,a,). When a, is a
switching parent, the current value of a, determines the subset of
other parents of 2, that are active. For example, a,: 3 might say that
Z, is no longer dependent on y,. The construct of switching weights,
moreover, allows a, to determine an exponential weight of the distri—
bution. That is, Pr(2,]y,,a,) o< (Pr(Z,]y,))w“‘ , where Pr(2,]y,) is some
locally normalized y,—dependent distribution on 5,, and aid, is some
non—negative a,—dependent constant weight. More details of these
constructs are given in Bilmes (2004).

Virtual evidence, along with switching weights, allows us to ex—
press any linear (i.e. dot—product) MS/MS score within a DGM in a
way that is ideally suited to DGM—expressed trellises. Given a pep—
tide y, recall that its binned theoretical spectrum, y, is a length—B
sparse vector that corresponds to the positions, along the binned m/z
axis, where there are peaks in a theoretically derived spectrum. Let):
be an increasing—order sorted vector of indices from 0 to B — 1, that
is : (0, 1,. . . ,B — 1). Also, recall that 3?, is the length—B processed
observed spectrum that is utilized in a dot—product scoring function
(37,?) (for example, Equation (2) in the case of XCorr).

Most MS/MS linear scoring functions use different weights de—
pending on the type of theoretical peak. For instance, in XCorr, b—
and y—ions are each assigned weight 50, and the neutral losses of am—
monia, water and carbon monoxide are each assigned weight 10.
These weights are the unique values in the vector y and are then
multiplied by the corresponding processed observed spectrum 37, as
expressed by the dot—product (373). Let a : (ao,a1, . . . ,aB_1) be a
length—B vector of peak type indices, so there are B peak types indi—
ces, one for each peak. Each a, is integer valued and takes on values
a, E {0, 1, . . . ,k} for [2 peak types. Since y is sparse, some of the pos—
itions along the binned m/z are empty, and we encode the empty
condition of position t as a,: 0. Moreover, let it! : (we, W1, . . . ,wk)
be a fixed vector of peak type non—negative weights, one weight for
each of the [2 possible peak types, and where we : 0. That is, the
value of w(a,) depends on the ion type of the theoretical t—th theoret—
ical peak (i.e. whether it is a b—/y—ion or a neutral ioss), or if the peak
(P(3’o)5(3’1)7~wPO’B—i» :
(w(ao),w(a1), . . . ,w(a3_1)). Now for t : 1,. .. ,B, define a virtual
evidence factor so that logPr(Z,]  : 3?’ Then with virtual evi—

is non—present. Therefore,

dence, and switching weights, we produce a probability model as

follows;
0 B—1 0
Pr“°‘3‘1’y°rB-17“°rB-1> = Hp<ut>p<at>p<ztly,,at> (3>
t:0
B—1 0 o
°< H17631701,)(Pr(z,1y,))w<ai> (4i
t:0

When it is the case that the current theoretical peptide is observed,
then both yo;3_1 and ao;B_1 are also observed, and if at these
observed values we set p61,) : p(a,) : 1, we, therefore, get that
logPr(Zo;B_1, 30101344034) : (37,?) + const. All of the above can
be expressed with a graphical model of length T: B (Fig. 3A). If a
sequencer over a trellis determines the vector variables 31017;, and
ao,T_1 (which are no longer observed), then the specifics of the dot—
product be directly controlled and used via a trellis, and all of the
aforementioned benefits of trellises become applicable (see Fig. 4B
and details in Section 5).

4.2 Non—linear scoring via graphical models
A linear model is not the only one that can be used with trellises
with a DGM. DRIP is a model that (potentially non—linearly) aligns

112 /310's1rzumofp10}xo"soiJBMJOJutoiq//:d11q 11101} popco1umoq

9103 ‘0g isanV uo ::

Trellises

i327

 

  
  
 
 
  

Chuck to unroll for -
A Pr°|°gue frames t = 1  T — 2 Epllogue
_ _ I — — l _ _
70 an r Y: ‘1: lyT—l ‘11—1
I I
I I
I I
I I
_ I _ I _
20 I Z, | z-r_1
I I
Theoretiml

MAX_THEO_INDEX
THEO_INDEX

Spectrum

 

THEOJ’EAK

_-
0111
0

iT—latbifé-‘ui‘i

Fig. 3. (A) The graphical model representation of linear MS/MS scoring func-
tions. (B) DRIP template. Shaded vertices are observed variables, while un-
shaded vertices are hidden variables. Black edges correspond to
deterministic functions of parent variables, red edges correspond to condi-
tional Gaussian distributions and blue edges represent switching parents.

A Prologue Chuck to unroll for

E 1
frames 1: = 1,...,T — 2 pl ogue

     
  
 
 
    
 
 

FIRSLNODE

A '1
LT—l

DRIP? v

_
6%.

   
  

FIRST7NOD E
TRE_NODE

An

70
XCorr DEN Part

Fig. 4. (A) The DRIP trellis model. The trellis DBN (Trellis Part) is attached to
the DRIP DBN (DRIP Part) by taking the input from 6,from DRIP (green cone),
which controls the traversal of theoretical peaks, and outputting L, for DRIP to
score (pink cone), which is the m/z values of theoretical peaks. The DRIP DBN
structure remains unchanged (the part with green background is unchanged
from Fig. 38). (B) The graphical model representation of linear MS/MS scoring
functions incorporated with trellis structure. Trellis Part (pink background) is
attached to the linear MS/MS function graphical model (green background),
which remains unchanged.

an unquantized observed spectrum with a peptide’s theoretical spec—
trum. In DRIP, the theoretical spectrum is represented by hidden
variables, as are constructs corresponding to insertions (spurious
observed peaks) and deletions (missing theoretical peaks). An

instantiation of the random variables in DRIP thus correspond to an
alignment between the theoretical and observed spectra, where an
alignment corresponds to the sequence of theoretical peaks used to
score observed peaks as well as the sequences of insertions and dele—
tions. During exact probabilistic inference, all possible alignments
between the theoretical and observed spectra are considered, and
the most probable alignment is used to score a peptide. We next de—
scribe particulars of how DRIP aligns spectra, and in Section 5 we
show how it naturally combines with trellises.

The DRIP DGM template is displayed in Figure 3B, where the
middle frame (the chunk) is dynamically expanded to fit the length
of the current observed spectrum; that is, like in Figure 3A, DRIP
considers the observed spectrum as the temporal sequence being
modeled. Let n" be the number of frames and t be an arbitrary frame
number. Each frame of the model corresponds to a single observed
peak, with observed variables 6;” and 6;“ corresponding to the t-th
m/z value and intensity, respectively. Parent to these variables, the
Bernoulli random variable i, denotes whether an observed peak is an
insertion (in which case, we score these observations using a con—
stant penalty) or not (in which case, we score these observations
using a Gaussian centered along the m/z access—a Gaussian cen—
tered near the current frame’s theoretical peak will of course score
much higher than a Gaussian centered farther away).

To score a sequence of observed peaks using a set of theoretical
peaks (which corresponds to a particular theoretical spectrum), a se—
quencer, expressed using a set of hidden random variables, probabilis—
tically traverses through the spectrum from left to right, and this
corresponds to the blue shaded portion of Figure 3B. Let 113’ be a vec—
tor containing a theoretical peptide’s fragment ions, sorted in increas—
ing order. We index into elements of this vector via the random
variable K,, which denotes the element index of 113’ in a particular
frame. The non—negative, discrete random variable 5, indicates the
number of theoretical peaks we skip when advancing in the model by
one frame; hence, Pr(K,+1 : i]K, : j) : lag-+51}. Hence, the number
of deletions that occur after frame t is 5, — 1. The observed variable
W, 5,’s parent, ensures that 5, does not increment past the number of
remaining theoretical peaks in the current theoretical peptide.

We note that despite the use of probability in DRIP to traverse
through a theoretical peptide’s spectrum to achieve an alignment
with an observed spectrum, only one theoretical peptide is con—
sidered at a time to control the sequencer. In the next section, we de—
scribe trellises which mitigate this limitation.

5 Connecting trellises with graphical model
MS/MS scoring functions

Figure 4A shows the DGM for DRIP that uses trellises. The vertex
5,, which controls the number of deletions, behaves similarly to the
transition random variable A, in the trellis representation in DGMs,
and we feed the value of 5, into A, (green cone in Fig. 4A). As 5,
ranges from 0 to the maximum length of the candidate peptides, all
subsequences of a peptide are encoded in the trellis.

The value of L,, which contains the set of m/z values of b/y—ions
stored as trellis links given values of V, and A,, is fed into the
‘THEO_PEAK’ vertex 113’ (K,) (pink cone in Fig. 4A) for peptide scor—
ing (Section 4). In general, the trellis variant of DRIP scores all the
candidate peptides all together unlike standard DRIP, which scores
candidate peptides separately and one by one. Thus, the trellis acts
like a database for querying theoretical peaks, and it does not affect
the mechanism of the underlying DGM but it does allow joint
decoding and reuse of common computational patterns.

112 /310's1rzumofp10}xo"soiJBMJOJutoiq//:d11q 11101} popco1umoq

9103 ‘0g isanV uo ::

i328

S. Wang et al.

 

Similar to DRIP, candidate theoretical peaks for XCorr can be
represented as a trellis and accessed via the node, transition, link
and index variables, as in the ‘Trellis Part’ of Figure 4B (the transi—
tion is always 1 for XCorr). However, unlike DRIP, XCorr requires
that a theoretical peak be weighted differently depending on the cor—
responding ion type, i.e. whether it is a b/y—ion or neutral loss. To
traverse two sets of sequences simultaneously within the same trel—
lis—the sequence of theoretical peaks as well as the sequence of the—
oretical peak ion types—an extra bit is appended to each theoretical
peak in the trellis. This bit denotes the value of the ion type variable,
which acts as a switching parent and changes the weight of the the—
oretical peak (Section 4). This mechanism may be easily extended to
include any variable number of ion types.

5.1 Speeding up graphical model inference with
trellises

Utilizing trellises within DGMs means the state space for accessing all
the data instances is much smaller compared with accessing each in—
stance separately. Intuitively, consider a ‘simple trellis’ that contains ]
 disjoint paths from n, to n,, where each path corresponds to one
data instance (e.g. the ‘simple trellis’ in Fig. 1D). The state space for
the simple trellis is no different than accessing each data instance sep—
arately. By constructing the compact trellis, redundant structures in
the simple trellis get merged into shared links so that the state space is
greatly reduced [step (5) in Fig. 1D]. Depending on the data, the state
space reduction can be quite significant. In general, as datasets get
larger, we expect more shared structures since there is more tendency
for redundancy. Hence, the size of a trellis will grow sublinearly with
the size of the input data. For the task of peptide identification in
mass spectrometry, there are often thousands of candidate peptides
within a certain mass window for one spectrum having the potential
of being compressed. Moreover, trellises can be even more effective
when post—translational modifications or sequence variants are con—
sidered (which otherwise greatly increase the number of separate pep—
tide candidates).

Along with trellises, approximate inference algorithms are ef—
fective at significantly reducing the state space of DGMs, decreas—
ing runtime but without keeping the most probable sequence from
being inferred. One such method, ideally suited for trellis infer—
ence, is k-heam pruning (histogram pruning in Ney et al. (1994), a
heuristic where only the [2 most probable hypotheses (or states) at
each time frame are retained and all other hypotheses are pruned
away (i.e. no longer modeled). While k—beam pruning may be used
in DGMs without trellises, utilizing k—beam pruning with trellises
is significantly more effective since we are pruning hypotheses
from the joint representation shared by all sequences. For example,
the hypotheses of the original DRIP model only consists of a single
peptide’s alignments between an observed spectrum so that, when
using beam pruning, poor alignments are pruned away early on.
With trellises, beam pruning induces a competition among all pep—
tide hypotheses, where peptide candidates which align poorly with
the observed spectrum are pruned away early, so we end up scoring
only a subset of the candidates. Note that, while we use k—beam
pruning in this work, other beam pruning methods or forms of ap—
proximate inference may also be sped up using trellises, because
trellises only alter the representation of the underlying sequential
hypothesis space.

5.1.1 Optimal pruning bounds for Viterbi decoding in trellises
Here we prove under generally applicable assumptions that, in ex—
pectation, a small beam width may be used with impunity when

jointly modeling multiple sequences (in our case, theoretical spectra)
in a single trellis, compared to performing approximate inference
with beam pruning independently on each sequence. This pruning
results in a substantial computational reduction while ensuring that
we have not pruned the most probable hypothesis.

Define a hypothesis h : h1, . . . ,h, : hi, to be a sequence of
instantiated random variables for frames 1 . . . , t in a DGM. For
frame t’, let h; be the most probable hypothesis we are trying to
infer, m be the number sequences represented in the trellis, n,: be the
number of hypotheses with higher probability than h:,, and r,: be the
total number hypotheses in the trellis without pruning at frame t’.

Theorem 1: Suppose m and n,: are large, r,: > n,:, and the n,:
hypotheses with higher probability than hf, are uniformly distributed
from the m sequences in the trellis. In expectation, performing inference
on each sequence independently requires Q(m n,:  more beam
width to ensure h* is not pruned compared to inference in a trellis.

The proof of Theorem 1 is provided in Section 1 of the
Supplementary Appendix. Though a uniform distribution is assumed
over the hypotheses, such a distribution is biased in practice for
many applications, and the trellis may be even more efficient, be—
cause we may more aggressively prune using the k—beam strategy
while still preserving the top hypothesis. This theoretically quantifies
the expected speedup using trellises to score MS/MS candidate pep—
tides as opposed to scoring candidate peptides individually (in
Section 7.1, we demonstrate this speedup empirically).

6 Training DRIP with trellises

In Halloran et al. (2014), generatively training the DRIP model’s
Gaussian parameters was shown to significantly increase perform—
ance. Here we extend this framework to discriminative training,
using trellises to make such training tractable. For the overall train—
ing procedure, assume that we have a collection, C, of N i.i.d. pairs
(xi,yi), where 3ci is an observed spectrum and yi the corresponding
peptide we have strong evidence to believe generated xi. Let 0 be the
set of parameters for which we would like to learn (in our case,
DRIP’s Gaussian parameters). For generative training, we then wish
to find 0* : argmaxgzizii Pr(xi]yi, 0), i.e. we wish to maximize
DRIP’s likelihood with respect to the parameters to be learned,
achieved via the expectation—maximization algorithm (Dempster
et al., 1977).

A much more difficult training paradigm is that of discriminative
training, where we not only wish to maximize the likelihood of a set
of parameters, but would also like to simultaneously minimize a par—
ameterized distribution defined over a set of alternative hypotheses.
In our case, this alternative set consists of all candidate peptides
within the precursor mass tolerance not equal to y‘, i.e. all incorrect
explanations of x‘. More formally, our discriminative training criter—
ion is that of MMI estimation (Povey, 2003). Defining the set of can—
didate peptides for 3ci within precursor mass tolerance to as
C" : D(.//Zx,‘€x,D,u/) and the set of all training spectra and high—
confidence PSMs as X and y, respectively, the MMI objective func—

tion we maximize with respect to 0 is

~ 1 N Pr(3c‘]yi 0)

I X; :— l 

0( y) N; 0gE:Pr(3c‘,3c]0)
356Ci

1 N . . .
= NEGOgPrOc‘Iy‘, 0) - logz Pr(x‘.yl0)), (5)
i:1 yECi

112 /310's1rzumofp10}xo"soiJBMJOJutoiq//:d11q 11101} popco1umoq

9103 ‘0g isanV uo ::

Trellises

i329

 

where we call Mn(xi,yi,0) : logPr(xi]yi, 0) the numerator model
and Md(yi,0) : logZyECrPr(xi,y]0) the denominator model. Note
that the numerator model is our objective function for generative
training.

Intuitively, Equation (5) is maximized by learning parameters
which increase the numerator model (what is done in generative
training) and/or decrease the denominator model. Thus, while gen—
erative training only learns parameters based on the high—confidence
PSMs in the numerator, discriminative training learns parameters
which also discourage the ensemble of PSMs in the denominator
model (which are incorrect matches). We solve maximize Equation
(5 ) with respect to 0 using stochastic gradient ascent.

In stochastic gradient ascent, we calculate the gradient of the ob—
jective function with regards to a single training instance,
ng9(xi;yi) : V9M,,(xi,yi, 0) — VgMd(xi, 0), where the gradients of
Mn and Md are vectors typically referred to as Fisher scores. We up—
date the parameters 0 using the previous parameters plus a damped
version of the objective function’s gradient, iterating this process
until convergence. In practice, we begin the algorithm by initializing
00 to a good initial value, i.e. the output of generative training, and
the learning rate 11,- is updated with n,,, : (ﬂfl. Intuitively, the
gradients move in the direction maximizing the difference between
the numerator and denominator models, encouraging improvement
for the numerator while discriminating against the incorrect labels
in the denominator.

Discriminative training is computationally expensive. The de—
nominator model requires calculating the gradients of all candidate
peptides C", which can be infeasible for many tasks. A further chal—
lenge in DRIP’s case is that it is difficult to constrain the model to
consider valid peptides only, because the distance between subse—
quent theoretical peaks can take on any value. Trellises address both
of these challenges. The denominator model of the discriminative
training for DRIP is exactly the same as the DRIP trellis model
(Section 5). Furthermore, the trellis of all possible labels can be very
compact; together with different strategies to speed up graphical
models with trellises discussed in the previous session, discriminative
training with trellises is highly efficient. In practice, we use a trellis
constructed from a decoy set, which contains permutations of the
target peptides as the denominator. As the denominator set grows
larger, trellises make the computational cost grow sub—linearly so
that discriminative training with trellises is efficient. Our experimen—
tal results show that discriminative training positively influences
performance (Section 7.2).

7 Results

A significant challenge in evaluating the quality of a spectrum identi—
fication algorithm is the absence of a ‘ground truth’ dataset where
the generating peptide is known for each observed spectrum. We,
therefore, follow the standard approach of using decoy peptides
(which in our case correspond to randomly shufﬂed versions of each
real target peptide) to estimate the number of incorrect identifica—
tions in a given set of identified spectra. In this work, targets and
decoys are scored separately and used to estimate the number of
identified matches at a particular FDR, i.e. the fraction of spectra
improperly identified at a given significance threshold. We estimate
FDR using the target—only variant of target—decoy competition, T—
TDC (Keich et al., 2015). Because the FDR is not monotonic with
respect to the underlying score, we instead use the q-valne, defined
to be the minimum FDR threshold at which a given score is deemed
to be significant. Since datasets containing more than 10% incorrect

identifications are generally not practically useful, we only plot q—
values in the range [0, 0.1].

7.1 Faster graphical model identification of MS/MS
spectra using trellises

In Section 5.1.1, the expected performance of Viterbi decoding with
beam pruning using trellises was proven to be significantly more effi—
cient than considering each sequence independently. We first veri—
fied that DRIP’s performance, in terms of the number of spectra
identified at a fixed FDR threshold, is equivalent with and without
use of the trellis (Supplementary Appendix Fig. 1). We then investi—
gate the improved inference speed using trellises. To do so, we ran—
domly select and search 200 spectra each from three datasets
(described in Supplementary Appendix Section 2): yeast and worm,
both acquired using low—resolution precursor scans (:3 Th toler—
ance) and low—resolution fragment ions; and Plasmodinm, acquired
using a high—resolution precursor scan (:50 ppm tolerance) and
high—resolution fragment ions. Further details regarding these data—
sets and search settings may be found in Section 2 of Supplementary
Appendix.

For all methods, we use the same graphical model inference en—
gine, GMTK (Bilmes and Zweig, 2002). Experiments were carried
out on a 3.40 GHz CPU with 16G memory. For each experiment,
the lowest CPU time out of three runs is recorded, and we report the
relative CPU time of methods using trellises to those without. For
the XCorr mixture model (Section 4), a fixed k—beam for all frames
was used. Per spectrum, the trellis—inferred top PSM scores were
exactly the same as computing each XCorr individually and deter—
mining the top PSM.

We test DRIP trellis with two beam pruning strategies (some dis—
cussion of beam pruning with trellises can be found in Section 5.1),
and we compare the results against DRIP using the beam pruning
settings of Halloran et al. (2014). The trellislme pruning uses
k—beams that are dynamic across time frames, with wider beams for
the early part and narrower beams later on. The trellisspeed pruning
also uses a dynamic k—beam but with a narrower beam, followed by
another pruning strategy that removes all hypotheses whose score
falls below some fraction of the currently top scoring hypothesis
while building up the inference structures. Timing tests show that
DRIP runs 7—15 times faster using trellises versus without trellises
(Fig. 5A).

The absolute timing numbers of all the searches implemented
using the graphical model engine are relatively high. XCorr (without
a trellis) takes ~2s per spectrum to search the low—resolution data—
sets (yeast and worm) and ~0.5 5 per spectrum to search the high—
resolution Plasmodinm dataset. Without a trellis, the more complex
DRIP model takes ~10 5 per spectrum searching the low—resolution
datasets and ~2s searching the high—resolution dataset. Constant
factor improvements for these models may be accomplished by opti—
mized C ++ implementations. For instance, a highly optimized im—
plementation of XCorr (McIlwain et al., 2014) takes 0.024s per
spectrum searching the high—resolution Plasmodinm dataset (min—
imum time over three runs) with the same compute environment
and search settings used in our timing tests. Thus, we expect that an
optimized trellis XCorr implementation would search the same spec—
tra (under the same settings and environment) in roughly 0.0024s
per spectrum.

We note that trellises can speed up many scoring algorithms sim—
ply by changing the preprocessing of data; the trellis approach is ag—
nostic to the underlying scoring algorithm and, therefore,
compatible with any method that makes the underlying scoring

112 /310's1rzumofp10}xo"soiJBMJOJutoiq//:d11q 11101} popco1umoq

9103 ‘0g isanV uo ::

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

i330 S. Wang et al.
A B C 7
. 10
l:lXCorr trellis A 6
-DRIP irerrrsmed  g
o -DRIP trellisbn” 8 8 8 5
°\ .— c,
E B 6 E 4
E z t
a '= E
 E 4 a 3
t s e 2
a: .. ...
8 — Disc. Train § — Disc. Train
(7)- 2 — Gen. Train U) 1 — Gen. Train
— Hand Train — Hand Train
Yeast-1 Worm-1 Plasmodium 0 0.004 0.008 l0.012 0.016 0. 20 0 0.004 0.008 l0.012 0.016 0. 20
q-va ue q-va ue

Fig. 5. (A) Percent running time of XCorr trellis, DRIP trellisbase, and DRIP trellisspeed relative to the running time of the original model without a trellis. (B)
Discriminative training improves performance for the worm dataset. (C) Similar to (B), but for the yeast dataset.

algorithm more efficient. We also note that our use of a
prototyping language (GMTK) to express the above methods, while
slower than highly optimized and specialized C implementation,
allows the exploration of vastly different models (linear XCorr and
non—linear DRIP) without needing to re—implement each model from
scratch.

7.2 More accurate graphical model identification of MS/
MS spectra using trellises

As detailed in Section 6, we use a set of high confidence, charge +2
PSMs and their corresponding peptide database to discriminativer
train the DRIP model, using trellises in the denominator model. We
compare the discriminatively trained DRIP model to generatively
trained and hand—tuned DRIP models, as well as the methods
described in Section 2. The discriminativer trained DRIP model
identifies more spectra at a 1% FDR threshold relative to the gener—
atively trained and hand—tuned models (Fig. 5 ). Note that the dis—
criminatively trained model employs the trellislme pruning strategy;
thus, the model yields an increase in accuracy as well as an approxi—
mately 7—fold speedup. Comparisons against other search engines
(MS—GF+, XCorr, XCorr P—value and XITandem) may be found in
Supplementary Appendix Section 3. We further note that, while MS/
MS scoring methods that afford efficient parameter learning are
scarce, any DBN may be discriminatively trained using trellises in
the same fashion as DRIP. Because several widely used MS/MS algo—
rithms may be expressed as DBNs (shown in Section 5), they may
also adapt this overall training procedure to their benefit.

8 Discussion

We have proven that, for many practical settings, the expected run—
time when using a trellis with beam pruning for Viterbi decoding is
significantly faster than considering sequences independently. We
have also empirically shown that trellises may be used to signifi—
cantly improve the speed and accuracy of peptide identification in
tandem mass spectrometry. Using the DRIP model and a DBN im—
plementation of XCorr, we have shown how to apply trellises to
dramatically speed up inference (6— to 15—fold), both for low—
resolution and high—resolution precursor mass spectra. We further
note that the algorithmic speedup afforded by using trellises may, in
future work, be combined with previous work on improving XCorr
runtime, which focused on constant—factor improvements (Diament
and Noble, 2011). We also note that trellises constructed from pep—
tides with variable modifications can be potentially more efficient as
variable modifications produce a lot redundancy among peptides.
Our MS/MS trellises support traversing all subsequences of a
data instance, allowing ‘jumps’ over whole subsequences, a novel

feature in contrast to traditional trellises (such as those used for
speech recognition), which only allow data instances to be sequen—
tially traversed. As in DRIP, where jumps correspond to deletions,
this feature may be used to model noise or missing data.
Furthermore, the jump feature enriches the hypothesis space repre—
sentation, allowing more sophisticated models to be expressed and
evaluated efficiently. With this feature and the ability to compactly
represent entire sets of peptides, we have extended DRIP’s learning
framework to discriminative training, significantly improving its
performance relative to previous training strategies.

In future work, we plan to further explore using trellises for im—
proved MS/MS identification. Using trellises to efficiently evaluate
and score peptides beyond those in the given database, we will inves—
tigate ways to take thresholds with respect to DBN—based scoring
functions to compute P—values, similar to the P—value calculations
done via dynamic programming by MS—GF+ and XCorr P—value. By
improving score calibration, we expect this approach to greatly im—
prove DRIP’s performance.

Funding

This work was supported by National Institutes of Health awards
ROIGMO96306 and P41GM103533.

References

Bilmes,I. (2004) On virtual evidence and soft evidence in Bayesian networks.
Technical report UWEETR-2004—0016. Electrical Engineering, University
of Washington.

Bilmes,I. (2010) Dynamic graphical models. IEEE Signal Proc. Mag., 27, 29—42.

Bilmes,I. and Zweig,G. (2002) The Graphical Models Toolkit: an open source
software system for speech and time-series processing. In: Proceedings of the
IEEE International Conference on Acoustics, Speech, and Signal Processing,
Orlando, US.

Craig,R. and Beavis,R.C. (2004) Tandem: matching proteins with tandem
mass spectra. Bioinformatics, 20, 1466—146 7.

Dempster,A.P. et al. (1977) Maximum likelihood from incomplete data via
the EM algorithm. I. Roy. Stat. Soc. B, 39, 1—22.

Diament,B. and Noble,W.S. (2011) Faster sequest searching for peptide identi—
ﬁcation from tandem mass spectra. I. Proteome Res., 10, 3871—3879.

Dyer,C. et al. (2008) Generalizing word lattice translation. Technical report,
DTIC Document, LAMP—TR—149.

Eng,I.K. et al. (1994) An approach to correlate tandem mass spectral data of
peptides with amino acid sequences in a protein database. I. Am. Soc. Mass
Spectrom., 5, 976—989.

Huang E.—F. and Soong, F.K. (1991) A tree—trellis based fast search for ﬁnding
the n—best sentence hypotheses in continuous speech recognition. In:
International Conference on Acoustics, Speech, and Signal Processing,
Toronto, Canada, IEEE, Vol. 1, pp. 705—708.

112 /310's1rzumofp10}xo"soiJBMJOJutoiq//:d11q 11101} popco1umoq

9103 ‘0g isanV uo ::

Trellises

i331

 

Geer,L.Y. et al. (2004) Open mass spectrometry search algorithm. I. Proteome
Res., 3, 958—964.

Halloran,I.T. et al. (2014) Learning peptide-spectrum alignment models for
tandem mass spectrometry. In: Uncertainty in Artiﬁcial Intelligence (UAI),
Quebic City, Quebec Canada. AUAI.

Hopcroft,H. (1971) An n log n algorithm for minimizing states in a ﬁnite au-
tomaton. Technical report, DTIC Document, STAN-CS-71-190.

Hopcroft,I.E. et al. (2001) Introduction to automata theory, languages, and
computation. ACM SIGACTNeu/s, 32, 60—65.

Ielinek,F. (1997) Statistical Methods for Speech Recognition. Cambridge,
MA, US, MIT Press.

Ii,G. et al. (2006) Graphical model representations of word lattices. In: IEEE/
ACL Workshop on Spoken Language Technology (SLT), Palm Beach,
Aruba.

Keich,U. et al. (2015) Improved false discovery rate estimation procedure for
shotgun proteomics. I. Proteome Res., 14, 3148—3161.

Kim,S. and Pevzner,P.A. (2014) Ms—gf+ makes progress towards a universal
database search tool for proteomics. Nat. Commun, 5, 5277.

McIlwain,S. et al. (2014) Crux: rapid open source protein tandem mass spec—
trometry analysis. I. Proteome Res., 13, 4488—4491.

Nesvizhskii,A.I. (2010) A survey of computational methods and error rate esti-
mation procedures for peptide and protein identiﬁcation in shotgun prote—
omics. I. Proteom., 73, 2092—2123.

Ney,H. et al. (1994) Improvments in beam search. In: Proceedings the
International Conference on Spoken Language Processing, Yokohama, Japan.

Pearl,I. (1988) Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann, San Francisco, CA, USA.

Povey,D. (2003) Discriminative training for large vocabulary speech recogni—
tion. PhD Thesis, Cambridge, UK, University of Cambridge.

Schnitger,G. and Gramlich,G.(2005) Minimizing NFA’s and regular expressions.
In: 22nd International Symposium on Theoretical Aspects of Computer
Science, Stuttgart, Germany, Berlin, Springer, Vol. 3404, pp. 399—411.

Walters,M.C. et al. (1996) Bone marrow transplantation for sickle cell disease.
New Engl. I. Med., 335, 369—376.

Watson,B.W. (1993) A taxonomy of ﬁnite automata minimization algorithms.
Comput. Sci. Note, 44.

Wenger,C.D. and CoonJJ. (2013) A proteomics search algorithm speciﬁcally de-
signed for high—resolution tandem mass spectra. I. Proteome Res., 12, 1377—1386.

Young,S. et al. (1997) The HTK Book, 2.1 edn. Cambridge, UK, Entropic
Labs and Cambridge University.

112 /310'S[Buln0prOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} popco1umoq

9103 ‘0g isanV uo ::

