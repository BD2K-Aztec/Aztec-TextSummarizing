BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

A.Kramer et al.

 

     

 

   

model
prior

Fig. 1. Flowchart of the usage structure

independent noise with standard deviation aim for each experi—
ment, each output variable and each time point, such that Em =
diag(0.,-/() contains variances of outputs yil-k (i = l, . . . , m). Given
data D = {y,-,-/(, 03/)“, the likelihood reads

 _ (3)

LD (0) = exp
 k 0727i?

Using Bayesian learning, we construct the posterior distribu—
tion p(0|D) o< LD(0)p(0) with prior distribution p(0)~N(u. E)
that is implemented as a multivariate Gaussian distribution
with mean u and covariance matrix E.

3 METHODS

The original and the SMMALA algorithms are comprehensively
described in Girolami and Calderhead, 2011 and Calderhead, 2012,
respectively, and we only give a glimpse of its structure here.

3.1 The simplified manifold MALA algorithm SMMALA
The Markov chain (MC) is a discrete time approximation of the SDE

d0 = éG’lwm V(0; D)dt + chol(G’1(0))db(t), (4)

where V(0;D)= log (p(0|D)) and db(t) is a Wiener process. The natural
metric G(0) of the parameter space is the Fisher information; chol de-
notes the Cholesky decomposition, a function that returns the Cholesky
factor. We can write the MC proposal 0 —> 0’ with step size 6 e R+ as

I’m/la) =N(v(0~ 6) M(0~ 6)) (5)
v(0. e) = 0 + g 0’1 (0m V(0; D) (6)
M(0, e) = e2 G’1(0). (7)

This step proposes a parameter vector from a Gaussian distribution
with mean and covariance that take information about the local shape
of the posterior distribution via the Fisher matrix and the gradient
into account. The metric tensor G is calculated using sensitivity analysis
(5,. : =dCx/d0) of the solution to the initial value problem,

6(0)= Z Sy(t';0. nifzjg} S,(t.;o. uk). (g)
jk

3.2 Implementation details

Figure 1 shows a ﬂowchart of the typical MCMC_CLIB usage.

Input The ODE model has to be provided as an XML ﬁle in VFGENS vf
format, together with a data ﬁle that contains measurement time points
and a data set D. Additionally, this data ﬁle also speciﬁes sample size,
initial step size, desired acceptance rate and the parameters u and 3’1 for
the prior distribution.

Sampling algorithm To evaluate the likelihood function, ODES are nu-
merically integrated using CVODES with ﬁxed relative and absolute toler-
ance. For further details, we refer to the supplement in the Availability and
Implementation section (manual .pdf and documentation .pdf).

a

 

SMMALA

posterior sample

   
 

e26

       

 

Llllllll

 

bmhbbLonN

WHIIIII

~9-8-7 ~6-5-4-3-2-1

 

Table 1. Sample properties for the provided example model

 

 

Property Value
Sample size N =1 >< 10"
Target acceptance 0.500
Observed acceptance 0.498
Auto-correlation length rimnl 340 :l: 30
Sampling time tS 74h

Effective sampling speed v (5.50 :l: 0.49) >< 10’3 s’1

 

Note: The effective sampling speed is deﬁned as v:N/(2rm1_.1t\).

Output The user can select a text or binary ﬁle output, which con-
tains the results of the sampling procedure: the sampled log-parameters
and their log-posterior values. The sample can be further processed with
other numerical software (e.g. GNU OCTAVE, MATLAB).

3.3 Test results for the provided demo

For demonstration purposes, we supply a model with 11 state variables,
26 parameters and 4 input variables. The data, obtained in a real experi-
ment, are given in arbitrary units, stem from nE = 4 experiments and are
only meaningful in relation to the reference experiment (also included).
The initial conditions are unknown stable steady states of the unper-
turbed system. At t = 0, a constant disturbance, parametrized by u, is
activated and perturbs the system indeﬁnitely. Then measurements are
taken. The activation is modeled by a logistic function centered at t = 0.

Performance of the sampling algorithm is summarized in Table 1.
Further insight into this example is provided as part of the software
package. The analysis reveals that not all data points can be ﬁtted equally
well by the model; a considerable uncertainty remains for several param-
eters and consequently model predictions. Hence, we consider this as a
realistic test case, which covers many of the difﬁculties that emerge in
current research.

Funding: AK. and NR. acknowledge support by the German Research
Foundation (DFG) within the Cluster of Excellence in Simulation
Technology (EXC 310/1) at the University of Stuttgart.

Conﬂict of interest: none declared.

REFERENCES

Calderhead,B. (2012) Differential geometric MCMC methods and applications. PhD
Thesis, Department of Computer Science, University of Glasgow, Glasgow, UK.

Girolami,M. and Calderhead,B. (2011) Riemann manifold Langevin and
Hamiltonian Monte Carlo methods. J. R. Stut. Soc. B, 73, 1237214.

Haario,H. et ul. (2006) DRAM: efﬁcient adaptive MCMC. Stut. Comput., 16,
3397354.

Hastings,W.K. (1970) Monte Carlo sampling methods using Markov chain and
their applications. Biometriku, 57, 977109.

Metropolis,N. et ul. (1953) Equation of state calculations by fast computing ma—
chines. J. C/zem. Plzy.s‘., 21, 108771092.

 

2992

/3.IO'S[BIImOfp.IOJXO'SOlJBLUJOJIIlOlq/ﬂduq

