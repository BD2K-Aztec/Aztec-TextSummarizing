BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

D.A.duVerIe et al.

 

In the rest of this article, section 2 ﬁrst outlines our general
approach for adapting existing path regularization techniques to
work with patterns of discretized input features instead of single
continuous values. Section 3 details the mathematical basis for
our algorithm and illustrates its application to proportional
hazard models using Cox’s partial likelihood as loss function
(with further detailed proofs as Supplementary Material).
Finally, section 4 presents qualitative and quantitative results
obtained by applying our method to different survival datasets.

2 APPROACH

2.1 Zl-penalized maximum likelihood estimation

A common defining feature to many major regression models,
such as generalized linear models (GLM) or previously men—
tioned Cox model, is the use of a loss function to ﬁt the param—
eters of otherwise analytically intractable problems. Adding an (1
penalty term to the original loss criterion results in the typical
estimation problem:

130) = argllininﬁﬂy; X13) + A llﬂ “1) (1)

where £ denotes the log—likelihood function with respect to the
given data (X, y), [3 is the vector of coefﬁcients that needs to be
estimated and A the regularization parameter.

For values of A tending towards infinity, all coefﬁcients in [3
will be forced to 0, whereas as A decreases, more coefficients will
have non—null values (i.e more predictor variables will be used in
the model estimation).

2.2 Regularization path-following algorithm

Among various methods for solving Zl—regularized problems
similar to (l), the use of so—called ‘regularization path—following’
algorithms (Hastie et al., 2005; Park and Hastie, 2007) is of par—
ticular interest for their ability to ﬁnely control the number of
active variables in the model, regardless of the dimensionality of
the input. The general idea behind path—following is to study
variations of the A parameter in the space of [3 coefﬁcient
values (see Fig. l): by decreasing the value of A, starting from
the maximum Am,” for which [3 is non—null, we can find a se—
quence of all discrete values of A, for which new coefﬁcients of [3
change between null and non—null (corresponding to a particular
predictor variable exiting or entering the regression model).

ﬁ(A3) @
[392). '\‘”> - mi)
[694).
3 A1
( )' 'ﬁOw)

Fig. 1. Schematic representation of the regularization path in the space of
)3. Successive values of [3(Ak) can be approximated using 

The resulting sequence of Ak and associated optimal [3(Ak)
allow us to model the data at varying levels of sparsity.

Park and Hastie (2007) suggested a path—following algorithm
for Zl—regularized GLM that uses a predictor—corrector approach
to efﬁciently ﬁnd all Ak and the coefficients of the model asso—
ciated with each level of regularization. If we deﬁne the ‘active
set’, AM, as the set of non—null indices in the coefﬁcient vector
[3(Ak), their algorithm can be deﬁned as a loop over four main
steps:

(1) Predict: Starting with a known [3(Ak,1) and Ak: the next
target value of A, estimate [3(Ak) using a piecewise linear
approximation of [3, under the assumption that A remains
unchanged.

(2) Correct: Solve the associated convex optimization problem
to ﬁnd the exact value of [3(Ak) (using the linear approxi—
mation as a warm start).

(3) Update active set: By confronting the new values of [3 to
the optimality conditions of the problem, update A (i.e.
add/remove predictors from the model). Repeat step 3 if
necessary to adjust [3.

(4) Decrement A: Analytically ﬁnd the exact value of AMI, at
which the active set will next change.

It is worth noting that, when an Zl—regularized model is fitted
to high—dimensional small sample data, sparse models are usually
selected (based on some model selection criteria). Therefore, we
do not really have to compute the ‘entire’ regularization path
(from A0 to 0). The algorithm is usually terminated for a value
of A where the size of the active set A is still much smaller than
the input dimension.

Because steps 1 and 2 only use variables in the current active
set A, they can be performed at little computing cost for values
of A where |A| remains much smaller than the number of vari—
ables. Steps 3 and 4 require solving simple equations for each
possible input variable (in linear time of the input’s dimension).

In their work, Park and Hastie (2007) showed that, along with
GLM, their algorithm could also easily be applied to the Cox
proportional hazards model. In fact, it can be shown that their
results hold for any loss—based model ﬁtting task, provided a loss
function that exhibits certain mathematical properties (see sec—
tion 3 and Supplementary Material).

2.3 Finding combinatorial covariates

When the linear model is extended to combinatorial interaction
terms, the input dimension increases exponentially because of the
combinatorial explosion of gene interactions. Of the steps enum—
erated in section 2.2, the predictor and corrector steps only deal
with the small subset of covariates currently in the active set A,
and therefore do not need to be changed. On the other hand,
updating the active set in step 3 and ﬁnding the next value of A at
which an update event will occur in step 4, both potentially re—
quire examining a number of feature combinations that grows
exponentially with the order of the interactions considered.
One practical approach to dealing with issues of combinatorial
explosion and computational complexities in steps 3 and 4 is to
take advantage of the input’s structure to efﬁciently explore its
space. By discretizing our input (gene expressions or other

 

3054

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

Discovering combinatorial interactions

 

clinical data) and considering all possible sets of such binary
variables, we can use itemset mining techniques (Saigo et al.,
2007; Uno et al., 2004) to preserve the computational efﬁciency
of the path—following algorithm despite a high dimensional input.

We show that step 3 can be reduced to a weighted itemset
mining problem, easily solvable using existing optimization tech—
niques (see Methods section 3.1.3), whereas step 4 requires sol—
ving a particular form of fractional programming problem, for
which we developed an efﬁcient pruning approach (see Methods
section 31.4). Our method can therefore overcome those com—
putational complexity issues, and identify complex interactions
(between two or more factors) that contribute to the response
model, at varying degrees of sparsity (controlled by the penaliza—
tion component).

2.4 Application to Cox proportional hazards model

We applied our modiﬁed version of the path—following algorithm
to the Cox proportional hazards model, where patient survival
(or any timed event) is used as a response, allowing for missing
data because of right censorship. To estimate this model, we seek
to maximize a so—called log partial likelihood function (see
Methods section 3.2) for a given set of data. As predictors, we
use discretized values of the gene expression levels (see section
4.1).

3 METHODS

In this section, we give a quick overview of the path-following algorithm
ﬁrst presented by Park and Hastie (2007) and the necessary changes to
work on combinatorial interactions:

3.1 Path-following algorithm
Let J(ﬁ) be the criterion from (1):

103) 1= -£(y: Xﬂ)+AllﬂII1 (2)

In the regularization path, we consider the optimal parameter vector [i
as a function of the regularization parameter A, and represent the optimal
parameter vector at A as ﬁ(A). We can write the optimality condition as
follows:

3103)
HUW»), A) 5: W lpzpm = 0 (3)

Our goal is to compute the path of solutions of (3) for all the A. If we
only consider the range of A where the active set A does not change
(noting ﬁA: the restriction of [f to the active set A), the partial change
of the optimality condition (3) with respect to A must satisfy:

mom _ 11 E3134 _

— 4
3A 3A my BA ( )

3.1.1 Predictor step In each predictor step, we assume that the cur-
rent active set, A, does not change. In the k-th predictor step, we use a
linear approximation to predict [f with the current active set:

3&0»)
3A

 

340%“) 5= ﬂAO‘k) + (M41 - M») lAzAk (5)

3.1.2 Corrector step We also assume that the active set A does not
change during each corrector step. Any convex optimization algorithm
can be used to minimize the penalized loss function (2). The use of

13 A(Ak+1) as an initial starting point ensures that an optimal solution
can be found in a small number of iterations.

3.1.3 Active set update After each corrector step, it is necessary to
identify all new features that should enter A. If we consider the set 73 of
all possible patterns, up to a given length, of binarized input features (e. g.
‘gene A over-expressed and gene B under-expressed) and assign each such
pattern an index value, for any (3 e {1, . . . , |73l}, we note xg 6 IE" (where n
is the total number of observations) the indicator vector for the matching
pattern. Our goal is to identify such values of (3 that contribute to mini-
mize the loss function (2), and for which the matching value of the par-
ameter vector [i should be non-null (noted as ﬁg being ‘active’ and (3 being
in the ‘active set’ A).
With the feature notation X := {Xij}i.j> we deﬁne:

3L3 "
w,» :2 ——, C :2 wixi 6
(WM 1  g ( )

Assuming strong complementarity slackness, we obtain the following
result (see Supplementary Material for detailed proof):

THEOREM 1.
ﬁg is active <:> |Cg| = A (7)

Therefore, if |Cg| : AH] after the corrector step, (3 (and its associated
parameter ﬁg) must be added to the active set A.

If (3 were an easily enumerable feature (such as in the case of single gene
expression level), it would be computationally feasible to exhaustively
enumerate all values of Cg for all possible 6. In our case, however, (3
can match an arbitrarily long pattern drawn from the power set of all
binarized features; the number of such features grows exponentially with
the maximum size of the patterns, making the problem highly impractical
for sets of >2 or 3 items. However, as long as Cg can be rewritten as linear
sums of Xl’g, finding all such (3 can be accomplished in reasonable time,
using frequent itemset enumeration techniques.

Because the values w,- in the linear sum deﬁned in (6) do not depend on
(3 (and are constant for A e [Ak+1,A,‘.]), finding all items Cg : AM] is
equivalent to ﬁnding all itemsets with weighted support above AM] (the
symmetrical problem of also ﬁnding {Cg| — Cg : Ak+1} is then trivial). To
solve this problem, we use the LCM program (http://research.nii.acjp/
~uno/codes.htm) (Uno et al., 2004), which provides an exhaustive enu-
meration of frequent itemsets in guaranteed polynomial time per itemset.

If any variable is added to the active set A, or removed (indices
{(3 6 Al ﬁg 2 0}), we go back to the corrector step (where the new
values of Cg are ﬁrst recomputed). These two steps are repeated until
the active set does not change, thus guaranteeing that the solutions are
optimal.

3.1.4 Step length To determine the optimal step length (the minimal
value by which the regularization parameter must be decreased in order
for the active set to change), we need to solve a similar problem, this time
involving the ratio of two separate frequent itemset mining optimization
problems.

If we deﬁne the step length:

AM» = M41 — M»

the minimum decrement of A for which the active set A changes (a vari-
able is added or removed), it can be shown (see Supplementary Material
for detailed proof) that:

 

 

THEOREM 2.
+, AA» — Ck AA» + 
AAA: 2 _  dz _ ll 3 _dl _Z1 3 Ammiuctivca AA:

 

3055

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

D.A.duVerIe et al.

 

where minJr is the smallest striCtly positive value, dg :=  and Ammwcm

are obtained by:
. . age ’1
Anvniuctive =    lAzAk) :| 

We note that Ammﬂgm only depends on the variables in the active set
and can be easily computed. On the other hand, much like in section
3.1.3, exhaustively computing the values of the ﬁrst two expressions in (2)
for all (3 in A is not computationally feasible given the dimension of our
input.

We designed an exploratory approach using bounds on each sub-
problem to efﬁciently prune the search tree and drastically reduce the
number of solutions explored.

First, we observe that both expressions can be rewritten as optimiza-
tion problems of the form:

+ Kp + Zpixil

111%] Kg +  inil (9)
where Vi : p,», q,» e [R only depend on the variables in the active set A (and
can therefore be easily computed) and [(1), Kg]: constant terms
(l—M, — 1J1», 1})-

We consider a relaxed form of (9), known as unconstrained fractional
(P1 programming, problem (Hammer et al., 1968) and frequently encoun-
tered in the ﬁelds of scheduling or database query optimization (Hansen
et al., 1990):

Kp + ZPiXi

= min —
wit-elm“ M, + Z ini
i

$1 (10)

where n is the number of non-zero values for the itemset (3 being con-
sidered. {pi}, e R" and {of}, e R".

Although the general form of this problem is shown to be NP-hard (by
association to the well-known NP-complete subset sum decision problem),
it has an easy polynomial solution (Boros and Hammer, 2002; Hammer
et al., 1968) if certain conditions hold.

With the following notation, separating positive and negative terms in
the sums ofp,- and q,-:

ViaI’i =17? —p.» 1171,1790: 13? == 217%: 132 == prxw

Viaqi = q? — q? 1511,5190: 511: Zqixw: i2 == Zqi’xw
we have the following result:

THEOREM 3. For a given itemset B, it is not neCessary to explore any super-
sets of E  either of the following Conditions holds:

(Kg — a; Z 0) /\ (d): 2 Curmin)

(Kg + a; 5 0) /\ (d): 2 Curmin)

where curmin is the current minimum value found by the algorithm up
until itemset E.

A much faster (0(1)), albeit slightly weaker, pruning condition can
also be obtained (see proof in Supplementary Material):

THEOREM 4. For a given itemset B, it is not neCessary to explore any super-
sets of E  either of the following Conditions holds:

_ "* ~+
(K41 — 51; Z 0) /\ [<Kp  Z Curmin) \/  5 0)]

 

 

Kg] + qg K41 _ q;
-+ _ C

(K, +£1g+ : 0) /\  2 Curmin) v (KP 1.” s 0)]
K11 _ 41 Kt! + W

Although this pruning-based method loses some of its efﬁciency as the
regularization parameter A decreases and the model becomes less sparse,
for the range of values of AA. treated, it remains well within the reach of
standard computing equipment (under a minute on a single 3.2 GHz CPU
core).

3.2 Application to Cox proportional hazards model

To demonstrate the potential of our method, we applied it to the Cox
model. This model uses survival data of the general form {(x,», y,», 8,»)};':1,
where x,» 6 ER"1 is the vector of risk factors, for instance gene expression
levels. In practice the x,» used by our method is vector of binary indicators
of under- or over-expression (possibly in combination); y,»>0 is the time
observed (survival until an event or censoring); 8,» 6 {0,1} is a binary
variable indicating whether an event has taken place (8,» = 1) or the ob-
servation was right censored (8,» = 0).

The Cox regression model (Cox, 1972) for the hazard of death at time t
can be expressed as:

110) = h0(1) CXPMTX) (11)
where h0(t) is the baseline hazard function, [f 6 ER"1 is the vector of par-
ameters and X = {X 1, . . . , Xd} is the vector of risk factor variables with

corresponding sample value of x,» for the i-th sample.

However, it is not necessary to know mm to infer the regression par-
ameters, thanks to the use of the log partial likelihood function of the
Cox model (Tibshirani et al., 1997), deﬁned as:

£0?) = Z (Ext—104 Z exp<ﬂij))) (12)

i:5,-:l fry/2y;

Refer to the Supplementary Material for the exact computation of the
criterion Cg (6) in the case of the Cox proportional model.

3.3 Gathering synthetic candidates

To extract as many interaction candidates as possible, while avoiding the
risk of overﬁtting the data, we repeatedly run the path-following algo-
rithm on a randomly chosen subset of the input. It has been shown
(Meinshausen and Bﬁhlmann, 2010) that the use of such sampling
method with regularized methods of variable selection provides a good
estimator of the original data. On each run of the algorithm, we keep
feature combinations that show a signiﬁcantly improved predictive power
over the linear models (likelihood ratio test P-value <0.01). We aggre-
gate all such combinations and rank them by KaplaniMeier test P-value
to produce a list of candidate interactions positively or negatively affect-
ing the timed outcome.

As could be expected, a few combinations will tend to reoccur multiple
times across successive iterations of the algorithm, whereas a large
number only occurs once or twice. We hypothesized and veriﬁed a pos-
teriori (see Supplementary Material) that combinations with low number
of occurrences might be overﬁtting a particular iteration’s training subset
and have poor generalization power. We therefore set an additional
screening thresholds on the list of interactions, keeping only those that
occur in at least four (out of 100) iterations. This threshold value was
selected as giving the best compromise between ratio of false positives and
overall number of interactions found (see details in Supplementary
Material).

Independent testing shows remarkable stability of the list of selected
interactions for a large-enough number of iterations. With our chosen
occurrence and P-value thresholds, the final list of variables sees little
change after ~50 iterations (see plot in Supplementary Material). This
trend is also conﬁrmed when using an independent test: none of the rarely
occurring combinations added in later iterations turn out to be signiﬁcant
in the test subset. For our experiment, we therefore set the total number

 

3056

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

Discovering combinatorial interactions

 

of total iterations to 100, a value that once again seems to offer a good
compromise between exhaustivity and the risk of false discovery.

4 EVALUATION

4.1 Datasets

To test our method, we used two datasets publicly available:
survival studies of neuroblastoma (Oberthuer et al., 2006) and
breast cancer (Van De Vijver et al., 2002) patients. In both stu—
dies, complementary DNA microarray assays of gene expression
(10 163 probes for 9878 unique genes and 24 158 probes for
23031 unique genes, respectively), along with (right—censored)
survival data, were available for n = 251 and n = 295 patients,
respectively. In both cases, after setting aside a test subset (25%
of all instances), the algorithm was iteratively applied on rando—
mized subsets of the training data (95%) in a method similar to
the leave—one—out procedure (Kearns and Ron, 1999).

For each study, gene expression data were normalized across
arrays using standard methods (Yang and Thorne, 2003), then
discretized in two binary classes depending on their distance to
the mean ([1) using a threshold proportional to the standard
deviation (0): genes that are over—expressed (expression value
above [1 + 60, where 6 is a thresholding parameter, set to 1.5
in this instance) or under—expressed (below It — 60).

To compare the higher—order interactions found by our method
with a linear combination search, we ran the original Park and
Hastie (2007) algorithm on the same training datasets and ranked
the resulting variables found by the order in which they entered
the regularized model. These ranks appear in the result tables
under the column ‘single—variable rank’ (‘NA’, standing for ‘not
applicable’, indicates a variable that did not appear in any of the
models ﬁtted by the single—variable version of the algorithm before
one of its default termination conditions were reached).

4.2 Analysis of breast cancer data

The list of interactions found for Van De Vijver et al. (2002) (see
Table 1) not only features a large number of genes strongly
associated with breast cancer prognosis in the medical literature,
such as SLC2A3 (Sternlicht et al., 2006), CA9 (Span et al., 2003),
RAB6B (van’t Veer et al., 2002), BBC3 (Cobleigh et al., 2005) or

Table 1. Interaction results for Van De Vijver et al. (2002)

KIAA0882 (Abba et al., 2005), many of which do not appear at
all in single—variable model ﬁts (see single—variable ranks); it also
features interesting examples of synthetic interactions: e.g. the
KaplaniMeier plot for the interaction between BBC3 and
KIAA0882 (Fig. 2) shows perfect prediction of survival of all
test samples (P<0.0003), compared with the much less signiﬁ—
cant plot for BBC3 alone (P = 0.03), whereas a strong synthetic
effect can be observed with BBC3 over—expressed (logrank
P—value: 0.008, see plots in Supplementary Material).

Despite the overall small number of samples and difﬁculties to
obtain good generalization power from such small training and
test subsets, these results hold fairly well in test. Logrank
P—values computed over an independent test subset for all se—
lected combinations show 6 of 9 (66.7%) to be signiﬁcant
(P<0.05), with 4 combinations (44%) still signiﬁcant after
Bonferroni correction for multiple—hypotheses testing.

4.3 Analysis of neuroblastoma data

The even smaller number of samples for Oberthuer et al. (2006)
makes it difﬁcult to obtain good generalized results (Table 2);
however, the single interaction validated on the test subset (out
of four interactions in total selected by our algorithm) not only
shows strong predicting power on both subsets, but also involves
two sequences strongly tied to breast cancer in literature. Locus
BC046178 is associated with CENPW (previously known as
C6orf173 or CUG2), a well—studied oncogene associated with
apoptotic behaviours in tumour cells (Lee et al., 2007, 2010).
Probe Hs458148 is a match for multiple genes including

dn.BB03‘dn.KIAA0882

mummxtm amllcs annulus:
p-val: 0 000215 D-wl: u mass prvnl n momz

. uvmsmnuvkimlm . upBBCJlﬂ) wmm my
00 . magnum-4mm; 00 . magnum M . unnnnuzmi

a 5 m 15 o 5 10 IS n s m 15

Fig. 2. KaplaniMeier plots for genes BBC3 and KIAA0882 (separately
and in combination) in data used by Van De Vijver et al. (2002) (using
test subset independent from training data used to compute Table 1)

 

Gene combination LR test P-value

Logrank P-value

No. of occurrences Test logrank P-value Single-variable rank

 

up.SLC2A3 * up.CA9 0.00153 0.000175
dn.Contig56307 * up.RAB6B 0.00168 0.000392
dn.BBC3 * dn.KIAA0882 5.21e-05 0.00043
up.KIAA0964 * up.SLC2A3 0.000254 0.00132
up.GADD153 * up.SLC3lAl 0.0147 0.0022
dn.Contig4l887_RC * dn.KIAA0252 0.0151 0.00387
up.RAD51C * up.TIMELESS 0.0367 0.0168
up.TGFBI * up.ITGA5 0.0195 0.0298
dn.Contig4l887_RC * up.UGT8 0.000172 0.0329

65 0.003432472 NA NA
15 009744396 NA NA
22 0.0002761196 NA NA
23 0.04875811 NA NA

5 0.2596054 540 NA
13 0.01261452 NA NA

4 0.001651706 NA NA
11 0.2221538 NA NA
51 0.003772726 NA NA

 

Note: Selected feature combinations, ranked by KaplaniMeier P—value. Bonferroni—signiﬁcant KaplaniMeier test P—values are in bold (correction factor: m = 71). Total
variables found with single—variable model: 585. Combinations of two genes (or more) are indicated by the symbol ‘*’, while ‘up.’ and ‘dn.’ preﬁxes indicate up— and down—

regulated genes, respectively.

 

3057

ﬁm'spzumofpmﬂo'sopeuuoyutotq/ﬁdnq

D.A.duVerIe et al.

 

Table 2. Interaction results for Oberthuer et al. (2006)

 

Gene combination LR test P-value

Logrank P-value

No. of occurrences Test logrank P-value Single-variable rank

 

up.BC046178 * up.Hs458148.20 0.0131 2.16e-07
dn.THC1529413 * up.Hsl72998.2 0.0199 0.00142
dn.I_32339l9 * up.USPl 0.0164 0.00561
dn.U9298l * dn.SLCl4A2 0.0147 0.0369

36 0.01003018 NA 67

20 0.3228081 NA NA

61 0.2413684 NA 89
9 0.1264266 NA NA

 

Note: Selected feature combinations, ranked by KaplaniMeier P—value. Bonferroni—signiﬁcant KaplaniMeier test P—values are in bold (m = 48). Total variables found with

single—variable model: 474. Using same notations as Table l.

RPL10: a ribosomal protein—coding gene that has been found to
be over—expressed in breast cancer tumours (Nagai et al., 2004).
Although Hs458148 could also match other genes, its expression
values in this dataset are highly correlated (Pearson’s coefﬁcient:
0.63) with two other probes exclusively matching RPL10.

4.4 Model validity and computation time

Although our goal is primarily not to create a predictor, but to
gather input feature combinations (with promising synthetic le—
thality properties, in the case of cancer studies), we could still
confirm that the model estimates produced by our method were
sound and consistent with previous methods. Separating the ori—
ginal dataset in a training (75%), model—selection (12.5%) and
test (12.5%) subsets and running nested cross—validation (100
iterations at the training level, each evaluated over 100 partition—
ing of the model—selection and evaluation subsets), we were able
to compare the average log partial likelihood for both our algo—
rithm and that of Park and Hastie (2007) (who use a Zl—penalized
path—following algorithm that only selects single variables, here—
after referred to as single—variable algorithm or single—variable
model), both on the test subset.

Using the breast cancer survival data from Van De Vijver et al.
(2002), our algorithm gave a mean log partial likelihood of
—121.00 (SD: 27.56) compared with —117.10 (SD: 26.85) for
the single—variable algorithm by Park and Hastie (2007), both
signiﬁcantly (P<2.2e — 16) higher than the null model
(—123.28, SD: 27.88), where no variables are used. With both
algorithms, a large variance in the cross—validated results and
overall middling performances are to be expected due to the
small sizes of training, model—selection and testing subsets
along with the typically high level of noise in microarray data.
However, as the validation of the results in section 4.2 shows,
there is still enough signal to detect meaningful covariates.

Additionally, we ran our algorithm on a randomized version
of the breast cancer data, where survival data had been shufﬁed
so as to no longer match its particular gene expression data.
Using the same experimental set—up as described in 4.1, the al—
gorithm produced only two signiﬁcant interactions (P<0.05):
one of which only occurred once (and therefore would not be
selected under normal conditions), whereas the other, with a
P—value of 0.03, was no longer significant after Bonferroni cor—
rection (correction factor: 36) for multiple—hypotheses testing.
This is to be contrasted with the multiple Bonferroni—signiﬁcant
interactions found in regular data (see section 4.2).

Computing time, although consistently longer for our algo—
rithm was still within reasonable distance of the single—variable
version: with similar termination conditions and the same input
data, a single run of our path—following algorithm took on aver—
age <5 min (281 s:1:83 s) on a quad—core 3.2GHz CPU, com—
pared with a little under a minute for Park and Hastie (2007)
(36 s :1: 6 s).

5 CONCLUSION

In this article, we presented an algorithm to follow the regular—
ization path of any Zl—regularized linear model ﬁtting, using
combinatorial interactions as covariates. Although the path—fol—
lowing method has been applied to microarray data in the past
(Park and Hastie, 2007), it was until now only able to deal with
single—valued features, ignoring possible higher—order effect of
gene interactions.

Our method makes uses of existing frequent itemset mining
techniques and novel imports from fractional programming to
avoid the intractability issues of combinatorial input and pro—
duce a regression model of accuracy and run time comparable
with the linear case. By running multiple iterations of the algo—
rithm on subsampled datasets, we can produce ordered lists of
candidate interactions with strong predicting power.

The interactions found by applying our method to cancer
study survival data include many genes that could not be
found through linear models, yet show up in literature as
strongly tied to these conditions, conﬁrming the crucial import—
ance of taking interaction effects into account to detect some of
the weaker signal in gene expression data. Although most sig—
niﬁcant interactions found by our method on experimental data
were limited to two or three genes, there are no theoretical limi—
tations to the size of interactions that can be searched, at no
particularly higher computational cost, setting this method
apart from other recent work on penalized selection of inter—
actions in high—dimensional data (Bien et al., 2012).

The strong noise inherent to gene expression microarray likely
prevents the detection of weaker signals between more than three
genes, making it an attractive prospect to work with less noisy
types of data where larger interactions might be detectable. In the
future, we plan to extend our ﬁeld of application to a wider range
of biomedical data, such as the identiﬁcation of SNP interactions
(Schwender and Ickstadt, 2008), as well as leverage our model’s
ability to deal with heterogeneous input, for example by

 

3058

ﬁm'spzumofpmﬂo'sopeuuoyutotq/ﬁdnq

Discovering combinatorial interactions

 

including a wide range of clinical data in addition to the large—
scale numeric data.

ACKNOWLEDGEMENTS

The authors would like to thank Hiroshi Mamitsuka and
Timothy Hancock, of Kyoto University, for their helpful feed—
back and suggestions.

Funding: Grant—in—Aid for J SPS Fellows (No. 24—02709 in part to
D.dV.); MEXT KAKENHI (No. 23700165 to IT); Grant-in-
Aid for Scientific Research on Innovative Areas (No. 23110002
to K.K.) from the Ministry of Education, Culture, Sports,
Science and Technology (MEXT) of Japan; Grant—in—Aid (No.
20390092 to K.K.; 24590376 to Y.M.-T.) from MEXT; and
funds from the Global COE program, MEXT, to Nagoya
University. FIRST program and J ST ERATO Minato Project
(in part to K.T.).

Conﬂict of Interest: none declared.

REFERENCES

Abba,M. et al. (2005) Gene expression signature of estrogen receptor a status in
breast cancer. BMC Genomics, 6, 37.

Bien,J. et al. (2012) A lasso for hierarchical testing of interactions. arX iv preprint
arXiv,1211.1344.

Boros,E. and Hammer,P. (2002) Pseudo—boolean optimization. Discrete Appl.
Math, 123, 1557225.

Bovelstad,H. et al. (2007) Predicting survival from microarray data a comparative
study. Bioinformatics, 23, 208(F2087.

Cobleigh,M.A. et al. (2005) Tumor gene expression and prognosis in breast cancer
patients with 10 or more positive lymph nodes. Clin. Cancer Res., 11,
862378631.

Cox,D. (1972) Regression models and life—tables. J. Roy. Stat. Soc. Ser. B, 34,
1877220.

Dudoit,S. et al. (2002) Comparison of discrimination methods for the classiﬁcation
of tumors using gene expression data. J. Am. Stat. Assoc., 97, 77787.

Ghosh,D. (2003) Penalized discriminant methods for the classiﬁcation of tumors
from gene expression data. Biometrics, 59, 99271000.

Gui,J. and Li,H. (2005) Penalized cox regression analysis in the high—dimensional
and low—sample size settings, with applications to microarray gene expression
data. Bioinformatics, 21, 300173008.

Hammer,P. et al. (1968) Boolean methods in operations research and related areas.
Vol. 5, Springer—Verlag, New York.

Hanahan,D. and Weinberg,R.A. (2000) The hallmarks of cancer. cell, 100, 57770.

Hansen,P. et al. (1990) Boolean query optimization and the 0—1 hyperbolic sum
problem. Ann. Math. Artif. Intel]., 1, 977109.

Hastie,T. et al. (2005) The entire regularization path for the support vector machine.
J. Mach. Learn. Res., 5, 1391.

Kaelin,W. (2005) The concept of synthetic lethality in the context of anticancer
therapy. Nat. Rev. Cancer, 5, 689498.

Kearns,M. and Ron,D. (1999) Algorithmic stability and sanity—check bounds for
leave—one—out cross—validation. Neural Comput., 11, 142771453.

Lee,S. et al. (2007) Molecular cloning and functional analysis of a novel oncogene,
cancer—upregulated gene 2 (cug2). Biochem. Biophys. Res. Commun., 360,
633439.

Lee,S. et al. (2010) Cancer—upregulated gene 2 (cug2) overexpression induces apop—
tosis in skov—3 cells. Cell Biochem. Funct., 28, 461468.

Lin,D. and Wei,L. (1989) The robust inference for the cox proportional hazards
model. J. Am. Stat. Assoc., 84, 10741078.

Meinshausen,N. and Biihlmann,P. (2010) Stability selection. J. Roy. Stat. Soc. Ser.
B, 72, 417473.

Nagai,M.A. et al. (2004) Gene expression proﬁles in breast tumors regarding the
presence or absence of estrogen and progesterone receptors. Int. J. Cancer, 111,
8927899.

Oberthuer,A. et al. (2006) Customized oligonucleotide microarray gene expressioni
based classiﬁcation of neuroblastoma patients outperforms current clinical risk
stratiﬁcation. J. Clin. Oncol., 24, 507075078.

Park,M. and Hastie,T. (2007) Ll—regularization path algorithm for generalized
linear models. J. Roy. Stat. Soc. Ser. B, 69, 659477.

Saigo,H. et al. (2007) Mining complex genotypic features for predicting HIV—l drug
resistance. Bioinformatics, 23, 245572462.

Schwender,H. and Ickstadt,K. (2008) Identiﬁcation of SNP interactions using logic
regression. Biostatistics, 9, 1877198.

Span,P. et al. (2003) Carbonic anhydrase—9 expression levels and prognosis in
human breast cancer: association with treatment outcome. Br. J. Cancer, 89,
2717276.

Sternlicht,M.D. et al. (2006) Prognostic value of pail in invasive breast cancer:
evidence that tumor—speciﬁc factors are more important than genetic variation
in regulating pail expression. Cancer Epidemiol. Biomarkers Prev., 15,
210772114.

Tibshirani,R. et al. (1997) The LASSO method for variable selection in the Cox
model. Stat. Med, 16, 3857395.

Tibshirani,R. et al. (2002) Diagnosis of multiple cancer types by shrunken centroids
of gene expression. Proc. Natl. Acad. Sci. USA, 99, 65674572.

Uno,T. et al. (2004) An efﬁcient algorithm for enumerating closed patterns in trans—
action databases. In: Discovery Science. Springer, Heidelberg, pp. 57759.

Van De Vijver,M. et al. (2002) A gene—expression signature as a predictor of survival
in breast cancer. N Engl. J. Med, 347, 199972009.

van’t Veer,L.J. et al. (2002) Gene expression proﬁling predicts clinical outcome of
breast cancer. Nature, 415, 533536.

Yang,Y. and Thorne,N. (2003) Normalization for two—color cDNA microarray
data. In: Lecture Notes—Monograph Series. Institute of Mathematical Studies,
Beachwood, pp. 403418.

 

3059

ﬁm'spzumol‘pmjxo'sopeuuopnotq/pdnq

