BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Performance reproducibility index

 

The typical analysis proceeds in the following fashion: (i) based
on the data, a feature set is chosen from the 30 000; (ii) a classiﬁer
is designed, with feature selection perhaps being performed in
conjunction with classiﬁer design and (iii) the classiﬁcation
error is measured by some procedure using the same sample
data upon which feature selection and classiﬁer design have
been performed. Given no lack of reproducibility owing to la—
boratory issues, if the error estimate is sufﬁciently deemed small
and a follow—on study with 1000 independent data specimens is
carried out, can we expect the preliminary error estimate on a
sample of 50 to be reproduced on a test sample of size 1000?
Since the root—mean—square (RMS) error between the true and
estimated errors for independent—test—data error estimation is
bounded by (2ﬁ)’l, where m is the size of the test sample
(Devroye et 0]., 1996), a test sample of 1000 insures
RMS 3 0.016, so that the test—sample estimate can be taken as
the true error.

There are two fundamental related questions (Dougherty,
2012): (i) Given the reported estimate from the preliminary
study, is it prudent to commit large resources to the follow—on
study in the hope that a new biomarker diagnostic will result? (ii)
Prior to that, is it possible that the preliminary study can obtain
an error estimate that would warrant a decision to perform a
follow—on study? A large follow—on study requires substantially
more resources than those required for a preliminary study. If the
preliminary study has a very low probability of producing repro—
ducible results, then there is really no purpose in doing it. We
propose a reproducibility index that simultaneously addresses
both questions posed earlier. Our focal point is not that inde—
pendent validation data should be usedithis has been well
argued, for instance, in the context of bioinformatics to avoid
overoptimism (Jelizarow et 0]., 2010); rather, the issue addressed
by the reproducibility index is the efﬁcacy of small—sample pre—
liminary studies to determine whether a large validating study
should be performed. We set up a simulation study on synthetic
models that emulate real—world scenarios and on some real data—
sets. We calculate the reproducibility index for different distribu—
tional models (and real datasets) and classiﬁcation schemes.

We consider two other scenarios: (i) multiple independent pre—
liminary studies with small samples are carried out and only the
best results (minimum errors) reported and (ii) multiple classiﬁ—
cation schemes are applied to the preliminary study with small
samples and only the results (minimum errors) of the best class—
ﬁer are reported. A decision is made for a large follow—on study
because the reported errors show very good performance.
Youseﬁ et a]. (2010, 2011) show that there is a poor statistical
relationship between the reported results and true classiﬁer per—
formance in these scenarios, namely, there is a potential for sig—
niﬁcant optimistic ‘reporting bias’ or ‘multiple—rule bias’. These
two biases can substantially impact the reproducibility index.

2 SYSTEMS AND METHODS

We deﬁne a classiﬁer rule model as a pair (\II, E), where W is a
classiﬁcation rule, possibly including feature selection, and E is a
training—data error estimation rule on a feature—label distribution
F. Given a random sample 8,, of size n drawn from F, the de—
signed classiﬁer is 1//,, = \II(S,,). The true error of 1//,, is given by
8,, = P(1//,,(X) 75 Y), where (X, Y) is a feature—label pair. The

7-!

error estimation rule E provides an error estimate, 5,, = E(S,,),
for 1//,,. To characterize reproducibility, we postulate a prelimin—
ary study in which a classiﬁer, 1//,,, is designed from a sample of
size n and its error is estimated. We say that the original study is
reproducible with accuracy ,0 2 0 if 8,, f 5,, + ,0. One could re—
quire that the true error lies in an interval about the estimated
error, but our interest is in whether the proposed classiﬁer is as
good as claimed in the original study, which means that we only
care whether its true performance is below some tolerable bound
of the small—sample estimated error.

Given a preliminary study, not any error estimate will lead to a
follow—on study: the estimate has to be sufﬁciently small to mo—
tivate the follow—on. This means there is a threshold, ‘17, such that
the second study occurs if and only if 5,, g ‘L'. We define the
reproducibility index by

R,,(p, r) = P(s, : 5n + plén s 1)-

R,,(,0, 1') depends on the classiﬁcation rule, \II, the error estima—
tion rule, E, and the feature—label distribution F. Clearly,
,01 g ,02 implies R,,(,01,r) g R,,(,02,r). If ||s,, — §,,|| g ,0 almost
surely, meaning P(||s,, — é,,|| g ,0) = 1, then R,,(,0, T) = 1 for all
T. This means that, if the true and estimated errors are sufﬁ—
ciently close, then, no matter the decision threshold, the repro—
ducibility index is 1. In practice, this ideal situation does not
occur. Often, the true error greatly exceeds the estimated
error when the latter is small, thereby driving down R,,(,0,r)
for small ‘17.

We are interested in the relationship between reproducibility
and classiﬁcation difﬁculty. Fixing \II and E makes R,,(,0, T) de—
pendent on F. If we parameterize F, and call it F(6), then we can
write the reproducibility index as R,,(,0,r; 6). If we select 6 so
there is a 1—1 monotonic relation between 6 and the Bayes
error, shay, then there is a direct relationship between reprodu—
cibility and intrinsic classiﬁcation difﬁculty, R,,(,0, T; shay).

If we know the joint distribution between the true and esti—
mated errors, then the entire analysis can be done analytically,
for instance, in the case of the 1D Gaussian model with linear
discriminant analysis (LDA) classiﬁcation and leave—one—out
(LOO) error estimation (Zollanvari et 0]., 2010). Fixing the vari—
ances and letting the means be zero and 6 gives the desired scen—
ario. We can now analytically derive the reproducibility index
R,,(,0, T; 6). Unfortunately, there are very few distributions for
which the joint distribution between the true and estimated
errors is known. If not, then we use simulation to compute
R,,(,0, 1'; 6).

2.1 Analysis
To analyze the reproducibility index as a function of ‘L' and tie
this to the joint distribution of the true and estimated errors, we
expand R,,(,0, T) to obtain

1

1 P(511< Suiﬂff) -
P(s,, 7,055,, 5r)

RH(/0a 1) E

Consider the special case in which ,0 = 0, and let r(r) denote
the probability fraction in the denominator. Then we have the
upper bound R,,(0, T) g  To gain insight into this bound,
we postulate a geometrically simple model whose assumptions
are not unrealistic. First, we assume that the linear regression of

 

2825

ﬁm'spzumofpmjxo'sopeuuopnoiq/ﬁdnq

M.R.Yousefi and E.R.Dougherty

 

P(én < an < 1') //

«a t
I
I
I
I
I

Hg<g<ﬂ

 

 

 

6]
t
3“» v

Fig. 1. A single-level cut of the joint distribution and corresponding
probabilities

8,, on é, is a horizontal line, which has been observed approxi—
mately in many situations (Hanczar et 01., 2007, 2010). This
means that 8,, and §,, are uncorrelated, which have also been
observed for many cases. Then, let us approximate the resulting
joint distribution by a Gaussian distribution with common mean
u (unbiased estimation) and ,0 = 0 (according to our assump—
tions). Finally, let us assume that the standard deviation of 8,,
is less than the standard deviation of 5,, as is typically the case
for cross—validation. Letting f(s,,,§,,) denote the joint Gaussian
density,

_ for 5"f(s,,é,)dg.dg,

_ f0” 0‘5"f(s,,, é,)ds,,dé,,.

r(I)

Figure 1 shows a pictorial of a single—level cut of the joint
distribution, along with the horizontal regression line and the
8,, = 5,, axis. Relative to the level cut, the dark gray and light
gray regions correspond to the regions of integration for the
numerator and denominator, respectively, of r(r). It is clear
that for small r,r(r) becomes large, thereby making R,,(0,r)
small.

2.2 Synthetic model

A model is adopted for generating synthetic data, which is built
upon parameterized multivariate distributions, each representing
a class of observations (phenotype, prognosis condition, etc.).
The model is designed to reflect a scenario in which there are
subnetworks (pathways) for which genes within a given subnet—
work are correlated but there is no (negligible) correlation be—
tween genes in different subnetworks. The situation is modeled
by assuming a block covariance matrix (Hanczar et 01., 2010;
Hua et 0]., 2005; Yousefi et 01., 2010, 2011).

Sample points are generated from two equally likely classes,
Y: 0 and Y :1 with d features. Therefore, each sample point
is speciﬁed by a feature vector X 6 Rd and a label Y 6 {0,1}.
The class conditional densities are multivariate Gaussian
with f(x|Y = y) ~ Nd(u,,022y), for y=0, 1, where
m, = [0,0,0, ...,0]T and u, = [0,0,0, ...,0]T are d><1
column vectors (for d = 1, we have Mo = 0 and m = 6), and

Table 1. Four microarray real datasets used in this study

 

Dataset Dataset type Featureisample size

 

Yeoh et a]. (2002) Pediatric ALL
Zhan et a]. (2006) Multiple myeloma 54 6137156/78
Chen et a]. (2004) HCC 10237775/82
Natsoulis et a]. (2005) Drugs response on rats 84917120/61

50777149/99

 

2,, is a d X d block matrix with off—diagonal block matrices
equal to 0 and l X l on—diagonal block matrices 2m being 1 on
the diagonal and ,0, off the diagonal.

Three classes of Bayes optimal classiﬁers can be defined de—
pending on ,00 and ,01. If the features are uncorrelated, i.e.
,00 2 0120, the Bayes classiﬁer takes its simplest form: a
future point is assigned to the class to which it has the closest
Euclidian distance. When ,00 = ,01 75 0, the Bayes classiﬁer is a
hyperplane in R", which must pass through the midpoint be—
tween the means of two classes. If ,00 7E ,01, the Bayes classiﬁer
takes a quadratic form, and decision surfaces are hyperquadrics.

2.3 Real data

We consider four microarray real datasets, each having more
than 150 arrays: pediatric acute lymphoblastic leukemia (ALL)
(Yeoh et 01., 2002), multiple myeloma (Zhan et 01., 2006), hepa—
tocellular carcinoma (HCC) (Chen et 01., 2004), and a dataset for
drugs and toxicant response on rats (Natsoulis et 01., 2005). We
follow the data preparation instructions reported in the cited
articles. Table 1 provides a summary of the four real datasets.
A detailed description can be found in the Supplementary
Materials.

2.4 Classiﬁer rule models

Three classiﬁcation rules, two linear and one non—linear, are con—
sidered: LDA, linear support vector machine (L—SVM) and
radial basis function SVM (RBF—SVM). Three error estimation
methods are considered: 0.632 bootstrap, LOO and 5—fold
cross—validation (5F—CV). In total, we have nine classiﬁer rule
models.

LDA is a plug—in rule for the optimal classiﬁer in a Gaussian
model with common covariance matrix. The sample means and
pooled sample covariance matrix obtained from the data are
plugged into the discriminant. Assuming equally likely classes,
LDA assigns a sample point x to class 1 if and only if
(X — 11055719 — I11) 5 (X — ﬁafﬁilC’C — Ila), Where lly iS
the sample mean for class ye {0,1}, and i) is the pooled
sample covariance matrix. LDA usually provides good results
even when the assumptions of Gaussianity with common covari—
ances are mildly violated.

Given a set of training sample points, the goal of support
vector machine classiﬁer is to ﬁnd a maximal margin hyperplane.
When the data are not linearly separable, one can introduce
some slack variables in the optimization procedure allowing for
mislabeled sample points and solve the dual problem. This clas—
siﬁer is called L—SVM, which is essentially a hyperplane in the
feature space. Alternatively, using a transformation the data can

 

2826

ﬁre'spzumofpmJXO'sopeuuowrorq/ﬁdnq

Performance reproducibility index

 

be projected into a higher—dimensional space, where it becomes
linearly separable. One can avoid using the transformation and
work with a kernel function that is expressible as an inner prod—
uct in a feature space. The equivalent classiﬁer back in the ori—
ginal feature space will generally be non—linear (Boser et 0]., 1992;
Cortes and Vapnik, 1995). When the kernel function is a
Gaussian radial basis function, the corresponding classiﬁer is
referred to as RBF—SVM.

In general, the 0.632 bootstrap error estimator can be
written as,

éboot : 0-3685resub + 0-632§zeroa

where 5,551,]3 and 5zero are the resubstitution and bootstrap zero
estimators. The resubstitution uses the empirical distribution by
putting mass 1 /n on each of the 11 sample points in the original
data. A bootstrap sample is made by drawing 0 equally likely
points with replacement from the original data. A classiﬁer is
designed on the bootstrap sample, and its error is calculated by
counting the misclassiﬁed original sample points not in the boot—
strap sample. The basic bootstrap zero estimator is the expect—
ation of this error with respect to the bootstrap sampling
distribution. This expectation is usually approximated by a
Monte—Carlo estimate based on a number of independent boot—
strap samples (between 25 and 200 is typical, we use 100).

In 5F—CV, the sample 8,, is randomly partitioned into ﬁve
folds 82"), for i: 1, 2,. . ., 5. Each fold is held out of the classiﬁer
design process in turn as the test set, a (surrogate) classiﬁer 1/12’) is
designed on the remaining sets 8,, \ 8g), and the error of 1/12’) is
estimated by counting the misclassiﬁed sample points in 82']. The
5F—CV estimate is the average error counted on all folds. Beside
the variance arising from the sampling process, there is ‘internal
variance’ resulting from the random selection of the partitions.
To reduce this variance, we consider 5F—CV with 10 repetitions,
meaning that we also average the cross—validation estimates of
10 randomly generated partitions over S,,. LOO error estimation
is a special case of cross—validation with n folds, where each fold
consists of a single point. Therefore, LOO has no internal vari—
ance since there is only a single way to partition the data into n
folds. With small samples, cross—validation tends to be inaccurate
owing to high overall variance (Braga—Neto and Dougherty,
2004) and poor correlation with the true error (Hanczar et 0].,
2007).

2.5 Simulation design

For the synthetic data, we assume that the features have multi—
variate Gaussian distributions as described in Section 2.2. We
choose ([6 {1,2,5, 10, 15}, ]=d if d<5 and [:5 if d: 5. We
also assume that a = 0.6 and the pair {,00, ,01} takes three differ—
ent values: {0, 0}, {0.8, 0.8} and {0.4, 0.8}. For ﬁxed 0, {00,01}
and d, we choose 6 so that the Bayes error equals some desired
values; specifically, from 0.025 to 0.4 (or the maximum possible
value depending on {,00, ,01}) with increments of 0.025. This will
define a large class of different distribution models in our simu—
lation. From each distribution model, we also generate random
samples of size 30, 60 and 120 (half from each class) to emulate
real—world problems, where only a small number of sample
points are available. Due to the large number of simulations in
this study, we have limited the dimension of the cases studied;

however, the reproducibility index is not limited by dimension
and, owing to the increased estimation variance, one can expect
that, with larger dimensions, reproducibility can be expected to
be even more problematic in such circumstances.

For each model, we generate 10000 random samples. For
each sample, the true and estimated error pairs of all classiﬁer
rule models are calculated. The true errors of the designed
classiﬁers are found exactly if analytical expressions are avail—
able. Otherwise, they are calculated via a very large inde—
pendent sample (10000 points) generated from the same
distribution model. For each ,0 e {0.0005,0.01,0.05,0.1},
‘L' e {0, 1/60,2/60, ..., 0.5} and classiﬁer rule model, we empir—
ically calculate R,, (,0, T; shay) from 10 000 true and estimated error
pairs.

The real—data simulation is essentially the same as for the syn—
thetic data, except that each real dataset now serves as a
high—dimensional distribution model. Thus, there is a need for
feature selection, which is part of the classiﬁcation rule. Another
difference is in calculating the true error: at each iteration, n = 60
sample points are randomly picked for training, and a
feature—selection step is carried out where d=5 features with
highest t—scores are selected. Then a classiﬁer is designed and
its error estimated. The remaining held—out sample points are
used to calculate the true error.

3 RESULTS AND DISCUSSION

The complete set of simulation results can be found in the com—
panion website of this article, including graphs for the joint dis—
tributions and reproducibility indices for different distribution
models, real datasets and classiﬁer rule models. Here, we provide
some results that represent the general trends observed in the
simulations.

3.1 Joint distribution

The joint distributions between 5,, and 5,, are estimated with a
density estimation method that uses bivariate Gaussian kernels.
Here we present the results for only two synthetic distribution
models with d = 5 features and different sample sizes. For the
ﬁrst model, the class—conditional covariance matrices are equal
and the features are uncorrelated. The target Bayes error is set to
0.2, being equivalent to 6 = 1.0. The results are shown for LDA
and 5F—CV in Figure 2(aw). For the second model, the
class—conditional covariance matrices are assumed to be unequal
and the features are correlated ({,00, ,01} = {0.4,0.8}). The target
Bayes error is 0.1, which results in 6 = 0.82. Figure 2(dif) shows
the corresponding graphs when RBF—SVM and LOO are used.
Each plot also includes the regression line (dotted) and a small
circle, indicating the sample mean of the joint distribution. Lack
of regression and correlation, slightly high—bias and very
high—variance for the estimated error are evident for small
sample sizes. These graphs, which are consistent with ones in
previous studies (Dougherty et 0]., 2010; Hanczar et 0]., 2007),
show a resemblance to Figure 1, indicating that our analysis in
Section 2.1 is suitable for the synthetic model.

The expected true errors for different classiﬁcation rules
applied to different real datasets are listed in Table 2. Similar
to the synthetic data, the joint distributions for the real data are

 

2827

ﬁre'spzumoiproyo'sopeuuowrorq/ﬁdnq

  

     

   
 
  
     

  
  
   

A “

 

 
   
   

    

6&ng

§

  

 

/3.10'
8112an
OFPJO
JXO'souBmJ
. OJuror
. .9// 'd
- 1111

 
 

\\“§t\\
$‘\\\\\\\‘\\\
WW»
~‘\®‘\\‘s“‘\\“\‘°§‘°‘ ‘5‘”
‘Q‘ ‘29:“ ‘ &\
y, ““va‘v‘\¢\‘\‘
V #va “‘9‘?

“ ‘

\

    

  
 
 
   
  

     

 

   
 
   

\i
\\\\\“\\\\\
\\\\\\“ \\\\\ \\\
\

\ \\\

1 \
73W“ xv *1?“
V ‘5‘???»

     

I

I
I
’I

&
s~“\\“
¢~W
-\\\\\“‘
s~“‘§\\“\\\‘ kw“
‘~‘\\ «9‘ ““‘
«f WW8“

a 9&2
‘2
\‘9

$838? we‘ve:

I
I;

I

9

I

I

4, X?
I «M
A

 

‘2 \4

 

/3.IO'S[BIIJI’10[pJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

        
   

/3.IO'S[BIIJI’10[pJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

M.R.Yousefi and E.R.Dougherty

 

classiﬁcation rule is LDA, Figure 10a shows that n must exceed
82. As another example, the graph in Figure 10d shows that, for
r=0.8, p = 0.05, r = 0.15 and RBF-SVM, n>60.

3.6 Concluding remarks

Performance reproducibility is an epistemological issue: What
knowledge is provided by a study? Ultimately, we are led back
to the core epistemological issue in biomarker prediction, accur—
acy of the error estimate. To the extent that the estimated clas—
sifier error differs from the true error on the feature—label
distribution, there is lack of knowledge at the conclusion of the
ﬁrst study. If there is virtually no reproducibility, then there is
virtually no knowledge. Thus, there is no justiﬁcation for a large
study based on the preliminary study. Indeed, why proceed with
the preliminary study if there is no reason to believe that its
results will be reproducible? The issue of reproducibility should
be settled before any study, small or large. The proposed repro—
ducibility index provides the needed determination.

Ultimately, the reproducibility index depends on the accuracy
of the error estimator, and if we judge accuracy by the RMS,
then the deviation variance of the estimator plays a crucial rule
since RMS 2 Vardev[5,,] + Bias2[5,,], where the bias and devi—
ation variance are defined by Bias[5,,] = E[5,, — 5,,] and
Vardev[5,,] = Var[5,, — 5,,], respectively. When the bias is small,
as in the case of LOO,

RMS 8 \/ Vardev[5,,]

 

= \/Var[5,,] + Var[5,,] — 2p Var[5,,]Var[5,,]

where ,0 is the correlation coefﬁcient between the true and esti—
mated errors. As we see in Figures 2 and 3, Var[5,,] tends to be
large and ,0 tends to be very small or even negative (Braga—Neto
and Dougherty, 2010; Hanczar et 0]., 2007). This large variance
and lack of positive correlation results in lack of reproducibility
for small samples.

Let us conclude with some remarks concerning validation,
which, in our particular circumstance, means validation of the
classiﬁer error from the original small—sample study. For com—
plex models, such as stochastic dynamical networks, validation
of the full network is typically beyond hope, and one must be
content with validating some characteristic of the network, such
as its steady—state solution, by comparing it to empirical obser—
vations (Dougherty, 2011). As for how close the theoretical and
the corresponding empirical characteristic must be to warrant
acceptance, closeness must be deﬁned by some quantitative cri—
terion understood by all. The intersubjectivity of validation res—
ides in the fact that some group has agreed on the measure of
closeness (and the requisite experimental protocol), although
they might disagree on the degree of closeness required for ac—
ceptance (Dougherty and Bittner, 2011). In the case of classiﬁ—
cation (as noted in the Introduction), when applying a classiﬁer
on an independent test set, the RMS possesses a distribution—free
bound of (2ﬁ)’l. Agreeing to using the RMS as the closeness
criterion and using this bound, one can determine a test sample
size to achieve a desired degree of accuracy, thereby validating
(or not validating) the performance claims made in the original
experiment.

The situation is much more subtle when using the RMS on the
training data. In very few cases are any distribution—free bounds
known and, when known, they are useless for small samples. To
obtain useful RMS bounds, one must apply prior distributional
knowledge. There is no option. Given prior (partial) distribu—
tional knowledge, one can determine a sample size to achieve a
desired RMS (Zollanvari et 0]., 2012). Furthermore, given a prior
distribution on the uncertainty class, one can ﬁnd an exact ex—
pression for the RMS given the sample, meaning that one can
use a censored sampling approach to sample just long enough to
achieve the desired RMS (Dalton and Dougherty, 2012a). Prior
knowledge can also be used to calibrate ad hoc error estimators
such as resubstitution and LOO to gain improved estimation
accuracy (Dalton and Dougherty, 2012b). One might argue
that assuming prior knowledge carries risk because the know—
ledge could be erroneous. But if one does not bring sufﬁcient
knowledge to an experiment to achieve meaningful results, then
he or she is not ready to do the experiment. Pragmatism requires
prior knowledge. The prior knowledge is uncertain, and our for—
mulation of it must include a measure of that uncertainty. The
more uncertain we are, the less impact the knowledge will have
on our conclusions. In the case of the reproducibility index, we
have introduced a few criteria by which one can decide whether,
in the framework of this uncertainty, a desired level is achieved.
A key point regarding uncertainty in the context of reproduci—
bility is that, should the prior distribution on the uncertainty
class be optimistic, it may result in carrying out a second study
without sufﬁcient justiﬁcation but it will not lead to an over—
optimistic conclusion because the conclusion will be based on
the independent larger follow—on study in which the prior know—
ledge is not employed. This is far better than basing the decision
to proceed with a large independent study on a meaningless error
estimate.

ACKNOWLEDGEMENT

The authors thank the High—Performance Biocomputing Center
of TGen for providing the clustered computing resources used in
this study; this includes the Saguaro—2 cluster supercomputer,
partially funded by NIH grant 1S10RR025056—01.

Conﬂict of Interest: none declared.

REFERENCES

B0ser,B.E. et a]. (1992) A training algorithm for optimal margin classiﬁers. In
COLT '92: Proceedings of the Fifth Annual Workshop on Computational
Learning Theory. ACM, New York, pp. 1447152.

B0ulesteix,A.—L. (2010) Over—optimism in bioinformatics research. Bioinformaticx',
26, 4374139.

B0ulesteix,A.—L. and Slawski,M. (2009) Stability and aggregation of ranked gene
lists. Brief. Bioinform., 10, 55(r568.

Braga—Net0,U.M. and D0ugherty,E.R. (2004) Is cross—validation valid for
small—sample microarray classiﬁcation? Bioinformaticx', 20, 37¢380.

Braga—Net0,U.M. and D0ugherty,E.R. (2010) Exact correlation between actual and
estimated errors in discrete classiﬁcation. Pattern Recognit. Lett., 31, 4074113.

Castaldi,P.J. et a]. (2011) An empirical assessment of validation practices for mo—
lecular classiﬁers. Brief. Bioinform., 12, 1897202.

Chen,X. et a]. (2004) Novel endothelial cell markers in hepatocellular carcinoma.
Modern Pathol., 17, 119871210.

Cortes,C. and Vapnik,V.N. (1995) Support—vector networks. Mach. Learn, 20,
2737297.

 

2832

ﬁre'spzumoiproyo'sopnuuowrorq/ﬁdnq

Performance reproducibility index

 

Dalton,L.A. and D0ugherty,E.R. (2011a) Bayesian minimum mean—square error
estimation for classiﬁcation erroriPart I: Deﬁnition and the Bayesian MMSE
error estimator for discrete classiﬁcation. IEEE Trans. Signal Process., 59,
1157129.

Dalton,L.A. and D0ugherty,E.R. (2011b) Application of the Bayesian MMSE error
estimator for classiﬁcation error to gene—expression microarray data.
Bioinﬁ)rmatics, 27, 182271831.

Dalton,L.A. and D0ugherty,E.R. (2012a) Exact MSE performance of the Bayesian
MMSE estimator for classiﬁcation erroriPart II: Consistency and performance
analysis. IEEE Trans. Signal Process., 60, 258872603.

Dalton,L.A. and D0ugherty,E.R. (2012b) Optimal MSE calibration of error esti—
mators under Bayesian models. Pattern Recognit., 45, 230872320.

Devroye,L. et a]. (1996) A Probabilistic Theory of Pattern Recognition.
Springer—Verlag, New York.

D0ugherty,E.R. (2011) Validation of gene regulatory networks: scientiﬁc and infer—
ential. Brief Bioinform., 12, 2457252.

D0ugherty,E.R. (2012) Prudence, risk, and reproducibility in biomarker discovery.
BioEssays, 34, 2777279.

D0ugherty,E.R. and Bittner,M.L. (2011) Epistemology of the Cell: A Systems
Perspective on Biological Knowledge. John Wiley, New York.

D0ugherty,E.R. et a]. (2010) Performance of error estimators for classiﬁcation.
Curr. Bioinform., 5, 53$7.

D0ugherty,E.R. et a]. (2011) The illusion of distribution—free small—sample classiﬁ—
cation in genomics. Curr. Genomics, 12, 3337341.

Fisher,R.A. (1925) Statistical Methods for Research Workers. Oliver and Boyd,
Edinburg.

Hanczar,B. et a]. (2007) Decorrelation of the true and estimated classiﬁer errors in
high—dimensional settings. EURASIP J. Bioinform. Syst. Biol., 2007, 12.

Hanczar,B. et a]. (2010) Small—sample precision of ROC—related estimates.
Bioinﬁ)rmatics, 26, 8227830.

Hua,J. et a]. (2005) Optimal number of features as a function of sample size for
various classiﬁcation rules. Bioinformatics, 21, 150971515.

Hothorn,T. and Leisch,F. (2011) Case studies in reproducibility. Brief. Bioinform.,
12, 2887300.

Ioannidis,J.P.A. (2005) Why most published research ﬁndings are false. PLoS Med,
2, e124.

Jelizarow,M. et a]. (2010) Over—optimism in bioinformatics: an illustration.
Bioinformatics, 26, 199(F1998.

Li,Q. et a]. (2011) Measuring reproducibility of high—throughput experiments. Ann
App]. Stat., 5, 175271779.

Natsoulis,G. et a]. (2005) Classiﬁcation of a large microarray data set: algorithm
comparison and analysis of drug signatures. Genome Res., 15, 72¢736.

Ray,T. (2011) FDA’s Woodcock says personalized drug development entering
‘long slog” phase. Pharmacogen. Rep., http://www.genomeweb.com/mdx/fdas—
woodcock—says—personalized—drug—development—entering—long—slog—phase
(26 October 2011, date last accessed).

Sabel,M.S. et a]. (2011) Proteomics in melanoma biomarker discovery: great poten—
tial, many obstacles. Int. J. Proteom., 2011, 8.

Yeoh,E.J. et a]. (2002) Classiﬁcation, subtype discovery, and prediction of outcome
in pediatric acute lymphoblastic leukemia by gene expression proﬁling. Cancer
Cell, 1, 1337143.

Youseﬁ,M.R. et a]. (2010) Reporting bias when using real data sets to analyze
classiﬁcation performance. Bioinﬁ)rmatics, 26, 6&76.

Youseﬁ,M.R. et a]. (2011) Multiple—rule bias in the comparison of classiﬁcation
rules. Bioinformatics, 27, 167571683.

Zhan,F. et a]. (2006) The molecular classiﬁcation of multiple myeloma. Blood, 108,
20232028.

Zhang,M. et a]. (2008) Apparently low reproducibility of true differential expression
discoveries in microarray studies. Bioinformatics, 24, 205772063.

Zhang,M. et a]. (2009) Evaluating reproducibility of differential expression discov—
eries in microarray studies by considering correlated molecular changes.
Bioinformatics, 25, 166271668.

Zollanvari,A. et a]. (2010) Joint sampling distribution between actual and estimated
classiﬁcation errors for linear discriminant analysis. IEEE Trans. Inform.
Theory, 56, 784~804.

Zollanvari,A. et a]. (2012) Exact representation of the second—order moments for
resubstitution and leave-one—out error estimation for linear discriminant analysis
in the univariate heteroskedastic Gaussian model. Pattern Recognit., 45,
9087917.

 

2833

ﬁre'spzumoiproyo'sopnuuowrorq/ﬁdnq

