BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

S.Zhe et al.

 

(Lasserre et al., 2006). For the conditional component, we use a
graph Laplacian matrix to encode information of each network
(e. g. a pathway) and incorporate it into a sparse prior distribu—
tion to select individual networks. For the generative component,
we use a spike and slab prior distribution to choose relevant
nodes (e.g. genes) in selected networks. For this hybrid model,
we do not impose the hard consistency constraints used by
Stingo et a]. (2011). Furthermore, the prior distribution of our
model does not contain intractable partition functions. This en—
ables us to give a full Bayesian treatment over model parameters
and develop an efﬁcient variational inference algorithm to obtain
approximate posterior distributions for Bayesian estimation. As
described in Section 3, our inference algorithm is designed to
handle both continuous and discrete outcomes.

Simulation results in Section 4 demonstrate superior perform—
ance of our method over alternative methods for predicting
continuous or binary responses, as well as comparable or im—
proved performance for selecting relevant genes and pathways.
Furthermore, on real expression data for diffuse large B cell
lymphoma (DLBCL), pancreatic ductal adenocarcinoma
(PDAC) and colorectal cancer (CRC), our results yield meaning—
ful biological interpretations supported by biological literature.

2 MODEL

In this section, we present the hybrid Bayesian model, NaNOS,
for network and node selection. First, let us start from the clas—
sical variable selection problem. Suppose we have N independent
and identically distributed samples D = {(x1,t1), ...,(xN, IN)},
where x,~ and t,- are the explanatory variables and the response
of the i—th sample, respectively. The explanatory variables can be
various biomarkers, such as gene expression levels or single—nu—
cleotide polymorphisms. Following the tradition in variable
selection, we normalize the values of each variable so that its
mean and standard deviation are 0 and 1, respectively. The
response can be certain phenotype or disease status. We aim to
predict the response vector t: [t1, ..., IN]T based on the
explanatory variables X = [x1, ...,xN]T and to select a small
number of variables relevant for the prediction. Because the
number of variables (e.g. genes) is often much bigger than the
number of samples, the prediction and selection tasks are statis—
tically challenging.

To reduce the difﬁculty of variable selection, we can use
valuable information from networks, each of which contains
certain variables as nodes and represents their interactions.
For example, biological pathways cluster genes into functional
groups, revealing various gene interactions. Based on
M networks, we organize the explanatory variables x,~ into
M subvectors, each of which comprises the values of explanatory
variables in its corresponding network. If a variable (i.e. a gene)
appears in multiple networks (i.e. pathways), we duplicate its
value in these networks. Note that networks here are exchange—
able with graphs; we can use them to represent not only
biological pathways but also linkage disequilibrium structures
for genetic variation analysis.

Our model is a Bayesian hybrid of conditional and generative
models based on a general framework proposed by
(Lasserre et al., 2006). The conditional component selects
individual networks via ‘discriminative’ training, the generative

component chooses relevant nodes in the selected networks and
the two models are glued together through a joint prior
distribution, so that the selected networks can guide node selec—
tion and, in return, the selected nodes can inﬂuence network
selection.

Speciﬁcally, for the conditional model, we use a Gaussian data
likelihood function for the continuous response

N
p(tlx,w, r) = HN(z.~|x,~Tw, 1") (1)
i=1

where w are regression weights, each of which represents the
contribution of the corresponding node to the response, and
‘L' is the precision parameter. For the unknown variance 1', we
assign an uninformative diffuse Gamma prior, Gam(r| g, h) with
g = h 2 10*.

For the binary response, we use a logistic likelihood

N

p(t|X,W) = 1—[0‘(x;rw)li[1 _ 0(XTW)]lit,~ 
i=1

where t,~ e {0, 1}, w are classifler weights and a(-) is the logistic
function [i.e. 001) = (1 + exp(—y))’l]. Based on the M networks,
we partition w into M groups, so that w = [w1, . . . , WM]T where
wk are the weights for the explanatory variables in the k—th
network.

To incorporate the topological information of a network, we
use its normalized Laplacian matrix representation. Specifically,
given an adjacent matrix Gk that represents the edges (i.e. inter—
actions) between nodes in the k—th network, the normalized
Laplacian matrix Lk is defined as

1 i=j and deg(i) 75 0
. . l . . . .
Lk(l,j) = —m 175] and Gk(laj) 7’5 0
0 otherwise

where deg(i) = 2/ Gk(i, j) is the degree of the i—th node in the
k—th network.

Based on the graph Laplacian matrices, we design the follow—
ing mixture prior over wk to select relevant networks:

P(Wk|0lk) = NON/cw,51L21)a"N(Wk|0,521/0170“ (3)

where wk is a binary variable indicating whether the k—th network
is selected, 51 >52, 52 B 0 and Ik is an identity matrix. We set the
hyperparameters 51 and 52 based on cross—validation (CV) in our
experiments. To make sure Lk is strictly positive—deflnite, we add
a diagonal matrix 10’le to Lk. In (3), Lk captures the
correlation information between nodes in the k—th network.
Note that if we replace Lk by Ik in the slab component, the
prior (3) becomes a simple generalization of the classical spike
and slab prior (George and McCulloch, 1997) for group
selection. When wk 2 1, the k—th network is selected and the
elements of wk are encouraged to be similar to each other due
to the Laplacian matrix Lk; when wk 2 0, because 52 is close to
zero, the corresponding Gaussian prior prunes wk. We use a
Bernoulli prior distribution to reﬂect the uncertainty in
wk, p(ak) = (uk)°‘"(1 — uk)l’°‘" where uk 6 [0,1] is the selection
probability. Without any prior preference over selecting or prun—
ing the k—th network, we assign a uniform prior over uk:
p(uk) = 1 [i.e. p(uk) = Beta(uk; a, b) where a = b = 1].

 

1988

ﬁm'spzumofpmﬂo'sopeuuopuorq/ﬁdnq

Joint network and node selection for pathway-based genomic data analysis

 

To identify relevant nodes, we introduce a latent vector Wk in
the generative model for each network k, which is tightly linked
to wk as explained later. We use a spike and slab prior:

[7/1
PWV/cI/Bk) = UNOI’k/‘IQ r1)ﬁ"’N(ﬂ’/q|0, Uri/3'”
' 1

/:

[7/1 ~ ~ 17 (4)
= nNmIW/m1’1)ﬁl"N(0IW/<i,1’2) ’3'”
i=1

: [Kola/h ﬂ/()

where pk is the number of nodes in the k—th network, r2 8 0 and
m,- is a binary variable indicating whether to select the j—th node
in the k—th network. We give m,- a Bernoulli prior,
1203/”) = (V/(/)ﬁ"’(l — Viv/)l’ﬁ’”, and a uniform prior over vkj:
p(v/(,) = 1 (i.e. p(v/(,) = Beta(v/(,-|c,d) where c = d: 1). As
shown above, the spike and slab prior p(Vv/(|/3k) has the same
form as p(0|W/(, 54.), which can be viewed as a generative
modeliin other words, the observation 0 is sampled from Wk.
This view enables us to combine the sparse conditional model for
network selection with the sparse generative model for node
selection via a principled hybrid Bayesian model.

Speciﬁcally, to link the conditional and generative models
together, we introduce a prior on M:

[KW/(WW) : N(W/<Iw/<> AI) 

where the variance A controls how similar Wk and wk are in our
joint model. For simplicity, we set A = 0 so that
p(Vv/(|wk) = 6(Vvk — wk) where 6(f) = 1 if f=0 and 6(f) = 0
otherwise. The graphical model representation of the joint
model is given in Figure 1.

The network and node selections are consistent with each
other in a probabilistic sense. If a network is pruned, all its
node are removed. Because wk 2 Wk is enforced by the prior
6(Vvk — wk), when wk 2 0, wk 2 0 implies Wk 2 0. As a result,
the spike component in (4) will be selected for all the nodes in
the k—th network (i.e. m,- = 0 for j: 1, ...,pk) with a higher
probability than the slab component. On the other hand, it is
easy to see that if one or multiple nodes in a network are selected,
then this network will be selected too. Note that if a node

Ta,b c,d

@ 
Lie
51, 82 0— Wk @ r1, r2

 

 

 

 

 

 

 

 

 

 

 

 

 

Fig. 1. The graphical model representation of NaNOS

appears in multiple networks and is selected, our model will
not force all the networks that contain this node to be chosen.
The reason is that we duplicate the value of this node in the
networks and treat their corresponding regression or classiﬁca—
tion weights as separate model parameters.

3 ALGORITHM

In this section, we present the variational Bayesian algorithm for
model estimation. Speciﬁcally, we develop the variational
updates to efficiently approximate the posterior distribution of
weights w, the network—selection indicators 0:, the node—selection
indicators [3, the network— and node—selection probabilities u and
v and the precision parameter ‘E for regression. Based on the
posteriors of a and [3, we can decide which networks and
nodes are selected.

For regression, based on the model speciﬁcation in Section 2,
the posterior distribution of our model is

p(w) W) a) ﬂ) u) V)  
1
= 2/\/(t|xW, I’II)Gamma(r)~

an/c Iak  I wk  I Wk a 5k) Bern(a/( I Mk) Beta(u/( ) ‘ 
k

1'1 Bern(/3/q| v/(,)Beta(vi,-)
i

where [)(W/(IOi/J and p(0|Vv/(, A) are deﬁned in (3) and (4),
p(VV/<|wk) 26(Vvk —wk) and Z is the normalization constant.
For classiﬁcation, the posterior distribution is similar to (6),
except that we replace the Gaussian likelihood (l) by the logistic
function (2) and remove the precision parameter ‘E and its prior
for regression in (6).

Classical Markov chain Monte Carlo methods can be applied
to approximate the posterior distribution. However, given the
high dimensionality of the parameters (e.g. w and a), it would
take a long time for a sampler to converge. In practice, it is even
difﬁcult to judge the sampler’s convergence. Thus, we resort to a
computationally efﬁcient variational approximation to (6).

Speciﬁcally, we approximate the exact posterior
distribution in (6) by a factorized distribution: Q(6) =
Q(w)Q(ai)Q(/3)Q(u)Q(v)Q(t), where 6 denotes all the latent vari—
ables. Note that, for classiﬁcation, we do not have Q,(t).
Because we set p(€'v|w) = 6(Vv — w), we do not need a separate
distribution Q(Vv). To solve Q(6), we minimize the Kullback—
Leibler (KL) divergence between the exact and approximate
posterior distributions of 6:

Q(9) d6

KL(Q(9)||p(9lt,X)) = / Q(9) 1H7

p(9lt, X) (7)

Applying coordinate descent for the minimization of (7), we
obtain efﬁcient updates for the variational distributions as
described in the following sections. The updates are iterative:
we update one of the variational distributions at a time while
having all the other variational distributions ﬁxed, and iterate
these updates until convergence. Because these updates
monotonically decrease the value of the KL divergence (7),
which is lower bounded by zero, they are guaranteed to converge
in terms of the KL value (Bishop, 2006).

 

/810'spzum0fp10}x0'sopBLuJOJuioiq”:duq

S.Zhe et al.

 

3.1 Regression

The variational distributions for regression have the following
forms:

Q(W) = N<w1m, 2) (8)

Q(w) = 1'1, yzka — rift") (9)

Q03) = 1'1, H, (mi/W111 — new” (10)

Q0!) e< 1'1, (um/1’10 — a)“ (11)

Q0) e< 1'1, H, (vi/WWI — vii/>511" (12)

Q0) = Wig: 1%) (13)

Their parameters are iteratively updated as follows:

2 = (A + (t)xTX)*l m = (1)2XTt (14)
szzyk+a 131:1—yi+b (15)
Ek/Z’lk/‘I’C (7k/21—77k/+d (16)

yk = 1/(1+ exp((ln(1— up) — (1n uk) +I%ln:—l
1 1 2 (17)

_%1n|Lk| +£tr<<ka75><ng —g1k))

me; = 1/(1 + exp ((1110 — vii» — an vi.)

18)
1 r1 1 2 1 1 (
+§lng+§<m~v> WEI)»

~_ 1 T T T 1 T T
h—h+§tt—th+§Zixi(ww )x,~ (l9)
- N

g=g+— (20)

2

where A = adiagayiLim + adiagaa — yi)Ii}i)+adiag(n> +
ﬁdiag(1 — 11) [note that diag({ykLk}k) is a block—diagonal ma—
trix], (-) means expectation over the corresponding variational
distribution, and the required moments in the above equations
are

(wa) = 2 + mmT (t) = g7}?

(111 14k) = Milk) — 114510 (1110 — 1%)) = 111(5k)— IKE/e)

(111 Vki) = 114517) — 71%;) (1110 — Wei» = 11/(dki) — 71%;)
where 1//(x) = Axhi l"(x), ék = 5k + 13k and ii, = 51., + (71,.

3.2 Classiﬁcation

Compared with regression, the classiﬁcation task is more
challenging. Because of the logistic function (2), we cannot dir—
ectly solve the variational distribution Q(w). Therefore, we use a

lower bound proposed by (Jaakkola and Jordan, 2000) to replace
the logistic function in the joint distribution:
0@)'(1 — 00))”
2t — 1 —§ (21)
2 e(s)exp (% —f(€)((2t — 1>2y2 — 52))

where f(x) = ﬁtanh(§/2), and 5 is a variational parameter. Note
that the equality is achieved when 5 2 (2t — 1)y. Because the
logarithm of the lower bound (21) is quadratic in y, it essentially
converts the logistic function into a Gaussian form so that the
variational inference becomes tractable.

Combining the maximization of the lower bound (21) with the
minimization of the KL divergence (7), we obtain the variational
updates for classiﬁcation. They are the same as those for the
regression task, except for that Q(w) = N(w|m, 2), now we have

,1 1
2 _ (A + 2Zif(§,)x,xi) m _ 2 2x (2t 1) (22)
where A is the same as in the regression.
In addition, maximization of the lower bound of the logistic
function gives the update for the variational parameter 5,:

5,2 = x,.T(wa)x,~. (23)

3.3 Computational cost

The computational cost of the proposed algorithm is dominated
by (14) for regression and (22) for classiﬁcation. For both cases,
it takes 0(p3) for matrix inversion to obtain 2 and 0(Np + p2)
to obtain m for each iteration. Thus, the total cost is 0(1)3 + Np)
and, for most applications where p>N, it simplifies to 0(p3).

4 EXPERIMENTS

In this section, we apply NaNOS to synthetic and real gene
expression data to select pathways (i.e. networks) and genes
(i.e. nodes), and provide biological analysis of our results. We
also compare NaNOS with alternative methods, including lasso
(Tibshirani, 1996), elastic net (Zou and Hastie, 2005), group
lasso (Jacob et al., 2009; Yuan and Lin, 2007), the network—con—
strained regularization approach [Li and Li (2008), henceforth
‘LL’] and the sparse Bayesian model with the classical spike and
slab prior (George and McCulloch, 1997). For lasso and elastic
net, we used the Glmnet software package (www—stat.stanford.
edu/~tibs/glmnet—matlab/). For group lasso, we treat each path—
way as a group. To handle genes appearing in multiple pathways
(i.e. groups), we ﬁrst duplicated their expression levels for each
grouﬁas suggested by (Jacob et al., 2009)7and then used the
SLEP software package (www.public.asu.edu/~jye02/Software/
SLEP/) for group lasso estimation. For the spike and slab
model, we implemented variational inference similar to our
updates in Section 3. Just as NaNOS, all these software packages
use the Gaussian likelihood for regression and the logistic likeli—
hood for classiﬁcation. We used the default conﬁguration of
these software packages for the maximum number of iterations,
initial values and the threshold for convergence. To tune regu—
larization weights in lasso, group lasso and the LL approach, we
conducted thorough 10—fold CV on training data (i.e. not using
the test data) using a large computer cluster. The CV grids on the

 

1990

ﬁm'spzumofpmjxo'sopeuuopuorq/ﬁdnq

Joint network and node selection for pathway-based genomic data analysis

 

free parameters are summarized here: for lasso, at = [0 : 0.01 : 1];
for elastic net, at = [0 : 0.01 : 1] and ﬂ = [0 : 0.01 : 1]; for
group lasso (both regression and logistic regression),
at = [0 : 0.01 : 1]; and for the LL approach, )11 = [1 : 25 : 300]
and )12 = [1 : 25 : 300] (we also did a second—level CV after we
pruned the range of )11 and )12 values based on the ﬁrst—level CV).
Finally, for NaNOS, the CV grids are 51 = r1 = [0.1, 1, 3] and
52 = r2 = [10’3,10’4,10’5,10’6].

On the synthetic data for which we knew the true relevant
pathways, we also compared NaNOS with a popular tool for
gene set enrichment analysis (GSEA) (Mootha et al., 2003;
Subramanian et al., 2005). We treated each pathway as a set,
used GSEA’s default conﬁguration and applied its suggested
criterion false discovery rate (FDR) <25% to discover enriched
pathways. We then identiﬁed all the genes in these enriched path—
ways as target genes. Because GSEA cannot provide predictions
on responses t, we did not include it for comparison on the real
data.

4.1 Simulation studies

We first compare all the methods on synthetic data in the
following three experiments.

Experiment 1. We followed the ﬁrst and second data gener—
ation models used by Li and Li (2008). Specifically, we simulated
expression levels of 200 transcription factors (TFs), each control—
ling 10 genes in a simple tree—structured regulatory network, and
assumed that four pathwaysiincluding all of their genesihave
effect on the response t. We sampled the expression levels of each
TF from a standard normal distribution, xTF ~ N(0, 1) and the
expression level of each gene that this TF regulates from
N (07):”, 0.51). This implies a correlation of 0.7 between the
TF and its target genes.

For the ﬁrst model with the continuous response, we designed
a weight vector for each pathway, ,0 = [1, ﬂ, . . . , ﬁ],

corresponding to the TF and 10 genes it regulates, and then
sampled t as follows:
w =  _ 510a 310a _ 310a 0T]T
t = Xw + e
where 6 ~ N(0, of.) and 0 is a vector of all zeros.
The second model is the same as the first one, except that the

genes regulated by the same TF can have either positive or nega—
tive effect on the response t. Speciﬁcally, we set

—1 —1 —1 1 1
1a a a ,—,...,— _
V10 V10 V10 V10 V10
\—,—z

7

’0:

For the first and second models, the noise variance was set to be
03 = (2;11§)/4 so that the signal—to—noise ratio was 12.85 and
7.54, respectively.

For the binary response, we followed the same procedure as
for the continuous response to generate expression proﬁles X and
the parameters w. Then we sampled t from (2).

For each of the settings, we simulated 100 samples for training
and 100 samples for test. We repeated the simulation 50 times.
To evaluate the predictive performance, we calculated the

prediction mean—squared error for regression and the error
rate for classiﬁcation. To examine the accuracy of gene and
pathway selection, we also computed sensitivity and
speciﬁcity and summarized them in the F1 score, F1 2 2>
(sensitivity >< speciﬁcity)/ (sensitivity + speciﬁcity). The bigger
the F1 score, the higher the selection accuracy.

All the results are summarized in Figure 2, in which the error
bars represent the standard errors. For all the settings, NaNOS
gives smaller errors and higher F1 scores for gene selection than
the other methods, except that, for classiﬁcation of the samples
from the second data model, NaNOS and group lasso obtain the
comparable F1 scores. All the improvements are signiﬁcant under
the two—sample t—test (P<0.05). We also show the accuracy of
group lasso, GSEA and NaNOS for pathway selection in
Figure 5. Again, NaNOS achieves significantly higher selection
accuracy. Because the LL approach was developed for regression,
we did not have its classiﬁcation results. While the LL approach
uses the topological information of all the pathways, they are
merged together into a global network for regularization. In con—
trast, using a sparse prior over individual pathways, NaNOS can
explicitly select pathways relevant to the response, guiding the
gene selection. This may contribute to its improved performance.

Experiment 2. For the second experiment, we did not require
all genes in relevant pathways to have effect on the response.
Speciﬁcally, we simulated expression levels of 100 TFs, each
regulating 21 genes in a simple regulatory network. We sampled
the expression levels of the TFs, the regulated genes and their
response in the same way as in Experiment 1, except that we set

1 1

= 1,—,...,—,0,...,0
'0 V21 V21 “11
\—,—z

10

for the first data generation model and

—1_1_1_11 100(24)
’0— amamamama---a¢2’—aﬁ%
\—,—z

7

for the second data generation model. Note that the last 11 zero
elements in ,0 indicate that the corresponding genes have no effect
on the response t, even in the four relevant pathways.

The results for both the continuous and binary responses are
summarized in Figures 3 and 5. For regression based on the first
data model, NaNOS and LL obtain the comparable F1 scores;
for all the other cases, NaNOS signiﬁcantly outperforms the
alternative methods in terms of both prediction and selection
accuracy (P< 0.05).

Experiment 3. Finally, we simulated the data as in Experiment
2, except that we replaced x/2—1 in the denominators in (24) with
21, to obtain a weaker regulatory effect of the TF. Again, as
shown in Figures 4 and 5, NaNOS outperforms the competing
methods signiﬁcantly.

4.2 Application to expression data

Now we demonstrate the proposed method by analyzing
gene expression datasets for the cancer studies of DLBCL
(Rosenwald et al., 2002), CRC (Ancona et al., 2006) and

 

1991

ﬁre'spzumofpmjxo'sopeuuowrorq/ﬁdnq



[TIM m 1111'! 1:111
[1| .|II|II|| IIIIIIIII

 

 

 

 

El
EIGLasso
I:]NaNOS

 

SEA

/3.10's112um0fp10}x0"sorwuiJOJurorqﬂ:duq

 

Endoplasmic
Reticulum

Securin

2
H

G

T . t.
factor

EMT

/3.10's112um0fp10}x0"sorwuiJOJurorqﬂ:duq

an?kgogmomammowoio~&o:3m7.omm\

ANaNOS
ENet

   

Joint network and node selection for pathway-based genomic data analysis

 

selected, the model has a Gaussian prior over its value (i.e.
weight) that is equivalent to a [2 regularizer (as in ridge regres—
sion) and does not shrink the value of the selected variable as [1
penalty would do. By contrast, lasso or elastic net, with a ﬁxed
mixture weight, has sparsity penalty over both pruned and se—
lected variables, which can greatly shrink the values of selected
variables and hurt predictive performance.

Second, NaNOS incorporates correlation structures encoded
in pathways for variable selection. Speciﬁcally, it uses pathway
structures into the extended spike and slab prior distribution to
explicitly model the detailed relationships between correlated
genes. In contrast, lasso and elastic net do not use this valuable
correlation information in their models. By comparing prediction
accuracies of NaNOS when 0 and 100% edges are removed from
pathways (Fig. 8), we can see that the detailed correlation infor—
mation captured by the pathway topology can greatly improve
modeling quality.

Third, NaNOS has the capability of selecting both relevant
pathways and genes due to its two—layer sparse structure. By
contrast, with 11/12 penalty, group lasso encourages the selection
of all the genes in chosen pathways, leading to dense estimation.
This may be undesirable in practice and deteriorate the predictive
performance of group lasso. NaNOS enhances the ﬂexibility of
group lasso by conducting sparse estimation at both the pathway
(or group) and gene levels. Meanwhile, our Bayesian estimation
effectively avoids overﬁtting, a problem often plaguing ﬁexible
models.

NaNOS has been applied to joint pathway and gene selection
in this article. Inspired by the seminal works in (Chuang et al.,
2007; Frohlich et al., 2006; Srivastava et al., 2008; Zycinski et al.,
2013), we can use NaNOS in a variety of biomedical applications
where there are abundant high—dimensional biomarkers of indi—
vidual samples and other information sourcesifor example, the
gene ontology (GO) and proteiniprotein interaction networks
informationithat capture correlation in the high—dimensional
space. Here we discuss two approaches to apply NaNOS when
we have only GO or other group information without network
topology. The ﬁrst approach is to compute some distance or
similarity scores between genes based on the GO information
[e.g. following the approach by Srivastava et al. (2008)] and
then estimate the network topology based on a network learning
method, for example, graphical lasso (Friedman et al., 2008).
With the estimated network topology, we can compute the
graph Laplacian matrices and apply NaNOS to select genes
and groups of genes. The second approach is to directly use
the group membership information in NaNOS by replacing the
graph Laplacian matrices with identity matrices. This approach
becomes useful when we even do not have any information avail—
able to learn the network topology. As shown in Figure 8, even
when all the edges were removed and we had only group infor—
mation, NaNOS still outperformed the second best method, elas—
tic net, in terms of prediction accuracy.

Funding: This work was supported by NSF IIS—0916443, NSF
CAREER Award IIS—1054903, and the Center for Science of
Information (CSoI), an NSF Science and Technology Center,
under grant agreement CCF—0939370.

Conﬂict of Interest: none declared.

REFERENCES

Agesen,T. et al. (2012) ColoGuideEx: a robust gene classiﬁer speciﬁc for stage II
colorectal cancer prognosis. Gut., 61, 156(%1567.

Amiot,L. et al. (1998) Loss of HLA molecules in B lymphomas is associated with an
aggressive clinical course. Br. J. Haematol., 100, 655%63.

Ancona,N. et al. (2006) On the statistical assessment of classiﬁers using DNA
microarray data. BM C Bioinformatics, 7, 387.

Badea,L. et al. (2008) Combined gene expression analysis of whole—tissue and micro—
dissected pancreatic ductal adenocarcinoma identiﬁes genes speciﬁcally overex—
pressed in tumor epithelia. Hepatogastroenterologv, 55, 201672027.

Bishop,C.M. (2006) Pattern Recognition and M acltine Learning ( Information Science
and Statistics). Springer—Verlag New York, Inc., Secaucus, NJ.

Chalkias,A. et al. (2011) Patients with colorectal cancer are characterized by
increased concentration of fecal hb—hp complex, myeloperoxidase, and secretory
IgA. Am. J. Clin. Oncol., 34, 5617566.

Chuang,H. et al. (2007) Network—based classiﬁcation of breast cancer metastasis.
Mo]. SVst. Biol, 3, 140.

Cycon,K. et al. (2009) Alterations in CIITA constitute a common mechanism
accounting for downregulation of MHC class II expression in diffuse large
B—cell lymphoma (DLBCL). Exp. Hematol., 37, 18¢194.

Dupire,S. and Coifﬁer,B. (2010) Targeted treatment and new agents in diffuse large
B cell lymphoma. Int. J. Hematol., 92, 12724.

Friedman,J. et al. (2008) Sparse inverse covariance estimation with the graphical
lasso. Biostatistics, 9, 4324141.

Frohlich,H. et al. (2006) Kernel based functional gene grouping. In: International
Joint Conference on Neural Networks. IEEE Computer Society, Los Alamitos,
CA, USA, pp. 358073585.

George,E.I. and McCulloch,R.E. (1997) Approaches for bayesian variable selection.
Statistica Sinica, 7, 3397373.

Giaginis,C. et al. (2009) Clinical signiﬁcance of MCM—2 and MCM—5 expression in
colon cancer: association with clinicopathological parameters and tumor prolif—
erative capacity. Dig. Dis. Sci., 54, 2827291.

Gordon,K. et al. (2009) Bone morphogenetic proteins induce pancreatic cancer cell
invasiveness through a Smadl—dependent mechanism that involves matrix
metalloproteinase—Z. Carcinogenesis, 30, 238r248.

Jaakkola,T.S. and Jordan,M.I. (2000) Bayesian parameter estimation through vara—
tional methods. Stat. Comput., 10, 25737.

Jacob,L. et al. (2009) Group lasso with overlap and graph lasso. In: Proceedings
of the 26th International Conference on Machine Learning. New York,
pp. 433440.

Kameda,K. et al. (1999) Expression of highly polysialylated neural cell
adhesion molecule in pancreatic cancer neural invasive lesion. Cancer Lett.,
137, 2017207.

Keleg,S. et al. (2003) Invasion and metastasis in pancreatic cancer. Mol. Cancer,
2, 14.

Krantz,S. et al. (2012) Contribution of epithelial—to—mesenchymal transition
and cancer stem cells to pancreatic cancer progression. J. Surg. Res., 173,
1057112.

Lasserre,J. et al. (2006) Principled hybrids of generative and discriminative models.
In: IEEE Computer Society Conference on Computer Vision and Pattern
Recognition. Vol. 1, IEEE Computer Society, Washington, DC, USA,
pp. 87794.

Lee,S. et al. (2011) Clinicopathologic characteristics of CD99—positive diffuse large
B—cell lymphoma. Acta. Haematol, 125, 1677174.

Li,C. and Li,H. (2008) Network—constrained regularization and variable selection
for analysis of genomics data. Bioinformatics, 24, 117?1 182.

Li,F. and Zhang,N. (2010) Bayesian variable selection in structured high—dimen—
sional covariate space with applications in genomics. J. Am. Stat. Assoc., 105,
120271214.

Menssen,A. et al. (2007) c—MYC delays prometaphase by direct transactivation of
MAD2 and Bule: identiﬁcation of mechanisms underlying c—MYC—induced
DNA damage and chromosomal instability. Cell Cycle, 6, 3397352.

Mootha,V.K. et al. (2003) PGC—lot—responsive genes involved in oxidative phos—
phorylation are coordinately downregulated in human diabetes. Nat. Genet., 34,
2677273.

Rizzo,A. et al. (2011) Intestinal inflammation and colorectal cancer: a double—edged
sword? World J. Gastroenterol., 17, 309273100.

Rosenwald,A. et al. (2002) The use of molecular proﬁling to predict survival after
chemotherapy for diffuse large—B—cell lymphoma. N. Engl. J. Med., 346,
193771947.

 

1 995

ﬁre'spzumofpmjxo'sopeuuowrorq/pdnq

S.Zhe et al.

 

Sakai,N. et al. (2012) CXCR4/CXCL12 expression proﬁle is associated with tumor
microenvironment and clinical outcome of liver metastases of colorectal cancer.
Clin. Exp. Metastasis, 29, 1017110.

Shields,M. et al. (2012) Biochemical role of the collagen—rich tumour microenviron—
ment in pancreatic cancer progression. Biocltem. J., 441, 5417552.

Srivastava,S. et al. (2008) A novel method incorporating gene ontology information
for unsupervised clustering and feature selection. PLoS One, 3, 12.

Stingo,F.C. and Vannucci,M. (2010) Variable selection for discriminant analysis
with Markov random ﬁeld priors for the analysis of microarray data.
Bioinformatics, 27, 4957501.

Stingo,F.C. et al. (2011) Incorporating biological information into linear models: A
Bayesian approach to the selection of pathways and genes. Ann. Appl. Stat., 5,
197872002.

Subramanian,A. et al. (2005) Gene set enrichment analysis: a knowledge—based ap—
proach for interpreting genome—wide expression proﬁles. PNAS, 102,
15545715550.

Terol,M. et al. (1999) Expression of beta—integrin adhesion molecules in
non—Hodgkin’s lymphoma: correlation with clinical and evolutive features.
J. Clin. Oncol., 17, 186971875.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. R Stat.
Soc., B, 58, 2677288.

Toiyama,Y. et al. (2010) Loss of tissue expression of interleukin—10 promotes the
disease progression of colorectal carcinoma. Surg. Today, 40, 4&53.

Toiyama,Y. et al. (2012) Evaluation of CXCL10 as a novel serum marker for pre—
dicting liver metastasis and prognosis in colorectal cancer. Int. J. Oncol., 40,
56(k566.

Vermeulen,K. et al. (2003) The cell cycle: a review of regulation, deregulation and
therapeutic targets in cancer. Cell Prolif, 36, 1317149.

Wang,Q. et al. (1998) Altered expression of cyclin D1 and cyclin—dependent kinase 4
in azoxymethane—induced mouse colon tumorigenesis. Carcinogenesis, 19,
200172006.

Wei,Z. and Li,H. (2007) A Markov random ﬁeld model for network—based analysis
of genomic data. Bioinformatics, 23, 153771544.

Wei,Z. and Li,H. (2008) A hidden spatial—temporal Markov random ﬁeld model for
network—based analysis of time course gene expression data. Ann. Appl. Stat., 2,
4087429.

Weinel,R. et al. (1992) Expression and function of VLA-Otg, «13, -()t5 and ialpltaﬁ—
integrin receptors in pancreatic carcinoma. Int. J. Cancer, 52, 8277833.

Yuan,M. and Lin,Y. (2007) Model selection and estimation in regression with
grouped variables. J. R Stat. Soc., B, 68, 49767.

Zou,H. and Hastie,T. (2005) Regularization and variable selection via the elastic
net. J. R Stat. Soc., B, 67, 3017320.

Zycinski,G. et al. (2013) Knowledge Driven Variable Selection (KDVS) a new ap—
proach to enrichment analysis of gene signatures obtained from high—through—
put data. Source Code Biol. Med., 8, 2.

 

1996

ﬁre'spzumofpmJXO'sopeuuowrorq/pdnq

