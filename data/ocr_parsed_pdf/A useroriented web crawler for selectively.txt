ORIGINAL PAPER

Vol. 30 no. 1 2014, pages 104—1 14
doi:10. 1 093/bioinformatics/btt5 71

 

Data and text mining

Advance Access publication September 29, 2013

A user-oriented web crawler for selectively acquiring online

content in e-health research

Songhua Xu*'i, Hong-Jun YoonT and Georgia Tourassi
Biomedical Science and Engineering Center, Health Data Sciences Institute, Oak Ridge National Laboratory, One Bethel

Valley Road, Oak Ridge, TN 37830, USA

Associate Editor: Igor Jurisica

 

ABSTRACT

Motivation: Life stories of diseased and healthy individuals are abun-
dantly available on the Internet. Collecting and mining such online
content can offer many valuable insights into patients’ physical and
emotional states throughout the pre-diagnosis, diagnosis, treatment
and post-treatment stages of the disease compared with those of
healthy subjects. However, such content is widely dispersed across
the web. Using traditional query-based search engines to manually
collect relevant materials is rather labor intensive and often incomplete
due to resource constraints in terms of human query composition and
result parsing efforts. The alternative option, blindly crawling the whole
web, has proven inefficient and unaffordable for e-health researchers.
Results: We propose a user-oriented web crawler that adaptively ac-
quires user-desired content on the Internet to meet the specific online
data source acquisition needs of e—health researchers. Experimental
results on two cancer-related case studies show that the new crawler
can substantially accelerate the acquisition of highly relevant online
content compared with the existing state-of-the-art adaptive web
crawling technology. For the breast cancer case study using the full
training set, the new method achieves a cumulative precision between
74.7 and 79.4% after 5h of execution till the end of the 20-h long
crawling session as compared with the cumulative precision between
32.8 and 37.0% using the peer method for the same time period. For
the lung cancer case study using the full training set, the new method
achieves a cumulative precision between 56.7 and 61.2% after 5 h of
execution till the end of the 20-h long crawling session as compared
with the cumulative precision between 29.3 and 32.4% using the peer
method. Using the reduced training set in the breast cancer case
study, the cumulative precision of our method is between 44.6 and
54.9%, whereas the cumulative precision of the peer method is be-
tween 24.3 and 26.3%; for the lung cancer case study using the
reduced training set, the cumulative precisions of our method and
the peer method are, respectively, between 35.7 and 46.7% versus
between 24.1 and 29.6%. These numbers clearly show a consistently
superior accuracy of our method in discovering and acquiring user-
desired online content for e-health research.

Availability and implementation: The implementation of our user-
oriented web crawler is freely available to non-commercial users via
the following Web site: http://bsec.ornl.gov/AdaptiveCrawler.shtmI.
The Web site provides a step-by-step guide on how to execute the
web crawler implementation. In addition, the Web site provides the

 

*To whom correspondence should be addressed.
7‘The authors wish it to be known that, in their opinion, the ﬁrst two
authors should be regarded as Joint First Authors.

two study datasets including manually labeled ground truth, initial
seeds and the crawling results reported in this article.

Contact: xus1@ornl.gov

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on April 25, 2013; revised on September 25, 2013; accepted
on September 26, 2013

1 INTRODUCTION

The Internet carries abundant and ever enriching user-generated
content on a wide range of social, cultural, political and other
topics. Life stories of patients are no exception to this trend.
Collecting and mining such personal content can offer many
valuable insights on patients’ experiences with respect to disease
symptoms and progression, treatment management, side effects
and effectiveness, as well as many additional factors and aspects
of a patient’s physical and emotional states throughout the whole
disease cycle. The breadth and depth of understanding attainable
through mining this voluntarily contributed web content would
be extremely expensive and time-consuming to capture Via trad-
itional data collection mechanisms used in clinical studies.
Despite the merits and rich availability of user-generated pa-
tient content on the Internet, collecting such information using
conventional query-based web search is labor intensive for the
following two reasons. First, it is not clear what are the right
queries to use to retrieve the desired content accurately and com-
prehensively. For example, a general query such as ‘breast cancer
stories’ would pull up over 182 million results using Google web
search wherein only a selected portion, usually small (such as
< 0.1%), of the whole search result set may meet the researcher’s
speciﬁc needs. Manually examining and selecting the qualiﬁed
search results require extensive human effort. Second, clinical
researchers have speciﬁc requirements regarding the user-
generated disease content they need to collect. Query-based
search engines cannot always support such requirements. Let
us assume that a researcher wants to collect the personal stories
of two groups of female breast cancer patients, those who have
had children and those who have not. With much manual effort,
the researcher might be able to obtain some stories of the ﬁrst
group, but so far no off-the-shelf general purpose search engine
that we are aware of allows users to retrieve information that
does not carry undesirable content (i.e. the support of negative
queries). Given the steadily growing volume of patient-generated
disease-speciﬁc online content, it is highly desirable to minimize

 

104 © The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e—mail: journals.permissions@oup.com

112 /310'slvu1nofp103xo'sopBHJJOJutotq/ﬁduq 11101} pappolumoq

91oz ‘Og isanV uo ::

A user-oriented web crawler for e-health research

 

the manual intervention involved in source acquisition and sub-
sequent mining processes. Although an extensive collection of
automatic or largely automatic text mining algorithms and
tools exists for analyzing social media content, limited efforts
have been dedicated to developing automatic or largely auto-
matic content acquisition tools and methods for obtaining
online patient-generated content meeting certain e-health re-
search needs and requirements. To meet this challenge in the
e-health research community as well as the broader bioinfor-
matics communities, we propose a user-oriented web crawler,
which can acquire user-generated content satisfying particular
content requirements with minimum intervention. We use
cancer as the case study to demonstrate the value and impact
of the proposed web crawling technology.

2 RELATED WORK

There is an extensive number of published studies reporting
methods for adaptive web crawling (e.g. Aggarwal, 2002;
Aggarwal et al., 2001; Almpanidis and Kotropoulos, 2005;
Almpanidis et al., 2005; Badia et al., 2006; Chakrabarti et al.,
2002; Chung and Clarke, 2002; Gao et al., 2006; Menczer et al.,
2004; Pant and Srinivasan, 2005, 2006; Zhuang et al., 2005). In
an early article, Aggarwal et a]. (2001) contributed the pioneering
idea of adaptively crawling the World Wide Web with the algo-
rithmic guidance of using arbitrary predicates. In a separate art-
icle, the authors discussed the design of a learning crawler for
topical resource discovery, which can apply learned knowledge in
one crawling session for new crawling tasks (Aggarwal, 2002).
Chakrabarti et a]. (2002) proposed to accelerate focused web
crawling by prioritizing unvisited uniform resource locators
(URLs) in a crawler through simulating a human user’s behav-
iors to identify and locate links meeting his/her information
needs. Chung and Clarke (2002) proposed a topic-oriented col-
laborative crawling strategy that partitions the whole web into
multiple subject topic-oriented areas and assigns one crawler for
each partitioned area. Menczer et a]. (2004) developed a frame-
work for systematically evaluating topical speciﬁc crawling algo-
rithms using multiple metrics. Pant and Srinivasan (2005)
compared different classiﬁcation schemes used for building adap-
tive crawlers that can learn from their past performance where
the crawling process is simulated as a best-ﬁrst graph search
activity over the web. Almpanidis and Kotropoulos (2005)
demonstrated the efﬁcacy of combining text and link analysis
for improving the web page collection productivity of focused
crawlers. The same authors also adopted latent semantic index-
ing in a focused crawling effort to produce a vertical search
engine. Zhuang et a]. (2005) studied the problem of how to
launch focused crawling efforts for harvesting missing docu-
ments in digital libraries.

Micarelli and Gasparetti (2007) overviewed methods for
focused web crawling with an emphasis on approaches equipped
with adaptive crawling capabilities. Babaria et a]. (2007) proposed
a method that tackles the focused web crawling problem as a large
scale ordinal regression problem. Their crawler was subsequently
supported by scalable ordinal regression solvers. Barbosa and
Freire (2007) proposed an adaptive crawler that can efﬁciently
discover hidden web entry points through web page content
topic mining, link prioritization and exploration-based link

Visitation. De Assis et a]. (2008) explored the impact of term se-
lection in conducting genre-aware focused crawling efforts. Guan
et al. (2008) explored how to use online topical importance esti-
mation to efﬁciently and effectively guide the execution of focused
crawlers. Chen et a]. (2009) developed a cross-language focused
crawling algorithm by applying multiple relevance prediction stra-
tegies. Batsakis et a]. (2009) showed that by jointly analyzing web
page content and link anchoring text, focused crawlers can better
reach relevant pages. Ahlers and B011 (2009) introduced an adap-
tive geospatially focused crawler that can efﬁciently retrieve online
documents relating to location information. Dey et a]. (2010)
introduced a focused web crawler for obtaining country-based
ﬁnancial data. Furuse et a]. (2011) extended the Hyperlink-
Induced Topic Search algorithm proposed by Kleinberg (1999)
and introduced a new method to ﬁnd related web pages with
focused crawling techniques. The key feature of their algorithm
was a mechanism to Visit both forward and backward links from
seed web pages for constructing an extended neighborhood graph
to conduct focused web crawling. Liu and Milios (2012) intro-
duced two probabilistic models for focused web crawling, one
based on maximum entropy Markov model and the other based
on linear-chain conditional random ﬁeld. Fu et a]. (2012) pro-
posed to use opinion information to guide focused web crawling
efforts for constructing a sentimentally aware web crawler.

Compared with all the crawling efforts surveyed earlier in the
text and other similar pieces of work that cannot be included in
this article due to space limitations, our proposed adaptive web
crawler is characterized by three novel features. First, existing
web crawlers rely heavily on the link structures of web graphs to
determine the crawling priorities, under the assumption that rele-
vant web pages are well interconnected. However, medical topic
forums and blogs tend to be highly scattered with sparse or no
links among them. Recognizing the challenge that this particular
problem poses, we propose a new crawler that leverages a third-
party search engine to massively and aggressively harvest
candidate target crawling links, coupled with a parallel crawler
navigation module that performs elaborate user-oriented crawl-
ing utility prediction and utility-driven crawling priority deter-
mination. Second, due to the critical importance of crawling
utility prediction, our crawler carefully balances the time cost
between repeatedly training a capable crawling utility predictor
using a dynamically identiﬁed machine learning method and the
actual time spent on crawling the web. Last but not least, our
new crawler is equipped with an autonomous query composition
and suggestion capability, built on content-based mining of ex-
emplar search results. Compared with existing topic-based
focused crawlers, the new crawler performs its function without
a predeﬁned topic ontology. Therefore, the crawler can be
applied to efﬁciently and effectively acquire any content that
matches the user’s needs. This function enables users to harvest
relevant content more comprehensively without the manual
effort of composing explicit queries.

3 ESTIMATING WEB PAGE UTILITY SCORES AS
FEEDBACKS FOR A WEB CRAWLER

To develop a self-adaptive web crawler, one key system module
is a feedback component that is capable of estimating the utility

 

105

112 /310'S[BHJDOIPJOJXO"SOIJBHIJOJIIIOIq/ﬂdnq 11101} pappolumoq

91oz ‘Og isanV uo ::

S.Xu et al.

 

score of an arbitrary web page. The estimated web page utility
scores can then guide the web crawler to make optimized crawl-
ing decisions. Below, we describe how we develop this predictive
feedback module for web page utility score estimation. For easy
reference and understanding, we list the key symbols and math-
ematical notations used in the algorithm description in the
Appendix, which is available as a Supplementary File.

For an arbitrary web page wp and certain user information
need £2, we aim to develop a predictive model d3 that is capable
of determining the utility score of wp according to £2. The derived
score is denoted as @(wp, £2) 6 [0, 1], where the higher the score
is, the more useful the web page is considered. To construct the
predictive model d), we follow a supervised learning-based ap-
proach. Features extracted from wp as the input for d3 include
words or word phrases detected from (i) the content words in the
main body of an HTML file, (ii) words in the heading and sub-
titles of an HTML file and (iii) the anchor text embedded in an
HTML ﬁle, including the URL(s) associated with the anchor
text. To extract words in the main body of an HTML file, we
use the Boilerpipe Java library introduced in Kohlschutter
(2011). To obtain the heading and subtitles of an HTML ﬁle,
we implement an HTML parser that extracts all the text enclosed
in the HTML blocks of <h?id=“. . .”>. . .</h?> where ? stands
for an integer number in the range of [1, 6]. For example, from
the HTML block <hlid:“sectiontitle”>Breast Cancer</hl>, we
can extract the heading text of ‘Breast Cancer’. Similarly, to
obtain the anchor text, we implement an HTML text parser
that collects the annotation text associated with hypertext links
embedded in an HTML file. Following the above procedure, we
derive the aforementioned three sets of the text from wp, which
are, respectively, denoted as T1(wp), T2(wp) and T3(wp).

We then apply the Rapid Automatic Keyword Extraction al-
gorithm (RAKE) proposed by Rose et a]. (2012) to identify a set
of key words or phrases from each one of the text sets,
T1(wp), ---, T3(wp), prepared in the above. The results are, re-
spectively, denoted as kwLJ-(wp) (i = 1,2, 3; j = l, -- - ,n,), where
n,- denotes the number of distinct key words extracted by the
RAKE algorithm from the text set T,(wp) and the subscript j in
the notation kwLJ-(wp) indexes these key words individually. To
train the web page utility estimator @(wp, £2) following a super-
vised learning—based procedure, our method also requires a set of
manually labeled samples. For this purpose, we collect all the de-
tected key words from web pages in wp and denote them as
kw = {kwffjlwpk e wp} where kwffj is a short notation for
kwLJ-(wpk). To train the utility estimator @(wp, £2), we present a
selected collection of web pages wp = {wpk} and solicit human
experts’ manual ratings for these web pages according to the in-
formation quality measurement criterion £2. Each human-labeled
utility score, denoted as $(wpk, £2), is a rational number in the
range of [0, 1]. The higher the score value is, the better the quality
of the web page is as considered by the human evaluator.

Given the substantial imbalance between the number of candi-
date key words that may be used as features for wp and the
available human-labeled training samples, to train the utility es-
timator @(wp, £2), we ﬁrst apply a feature selection procedure to
reduce the amount of candidate key word features. This screening
process consists of two steps. In the ﬁrst step, we eliminate all the
key words whose support values are below a certain empirically

chosen threshold, which is set to ﬁve in all our experiments. After
the infrequent key word ﬁltering step, we denote the set of re-
maining key words as E. In the second step of the key word
reduction process, we examine the odd ratios of key words with
respect to a human-labeled training set. Speciﬁcally, for each can-
didate key word j e N and a given threshold 1.’ e [0, 1], we
ﬁrst derive the key word’s odd ratio 1//(kwffj, £2, 1', wp) with respect
to the labeled training set as follows:

kwf.C ., £2, 1', w _ , l

I“ ’1 p) 1001(ka j: WP)P10(kWIfJ-a WP) ( )
’F.
1,],
tain the key word j and whose human-labeled utility score is

where p11(kw wp) is the number of web pages in wp that con-

above the threshold, i.e. p11(kwffj,wp) = |{wpx e wplkwffj e
wpx, 43(pr, 9) Z Ill; 1710061“

Lj’
in wp that contain the key word kwffj and whose human-labeled

wp) is the number of web pages

utility score is below the threshold, i.e. 1010(ka j, wp) = |{wpx e
wplkwffj e wpx, @(wpx, £2)<t}|.
p01(kwffj,wp) and 1000(ka j, wp), Which are counterparts for
P11 and pm with the only difference being that the web

pages considered now do not contain the key word kwffj. That

Similarly, we deﬁne

is, P01(kWIfj-:WP) = l{wpx e Wplkwffﬁéwpx, érwpx, s2) 2 In and
p00(kwffj,wp) = |{wpx e wplkwffﬁwpx, $(wpx, £2)<t}|. We then
rank all the candidate key words kwffj e N in a descendant
order according to their respective odd ratios derived from (1).
When training the web page utility estimation model using a
speciﬁc machine learning method, we progressively admit key
words as features into the model one-by-one until the testing
performance of the trained model as obtained through 10-fold
cross-validation declines from the peak testing performance by
>5%. We then retrospectively remove all the key word features
admitted after the model achieves its peak performance moment.

In our experiments, we explored the following machine learn-
ing methods when developing the web page utility estimator
@(wpx,£2): Gaussian processes for regression, isotonic regres-
sion, least median squared linear regression, linear regression,
radial basis function network, additive regression, bagging, re-
gression by discretization and stacking. The implementations of
all the above methods were provided by the Weka package (Hall
et al., 2009). According to our experimental results, we empiric-
ally found that the additive regression method achieves the best
performance in all our experiments as measured by the 10-fold
cross-validation scheme.

4 ADAPTIVE WEB CRAWLER FOR ACQUIRING
USER-DESIRED ONLINE PATIENT CONTENT

Given a web page utility estimator d>(wp,(, £2) trained from a set
of human-labeled example web pages {$(wpk, £2)}, we can then
develop a user-oriented web crawler that is capable of adaptively
acquiring relevant web pages that satisfy the user information
requirement £2. Figure 1a illustrates the overall architecture of
our adaptive web crawler, the design of which will be discussed in
this section. Figure 1b provides a companion computational data
flow of the crawler design with more technical details.

 

106

112 /310'S[BHJDOIPJOJXO"SOIJBHIJOJIIIOIq/ﬂdnq 11101} pappolumoq

91oz ‘Og isanV uo ::

A user-oriented web crawler for e-health research

 

 
   
 
   
  
 

Initial Manual
Search

Initial crawling
seeds

Crawling link pool

 

  
   
   
     
 
       
  
  
      
    
  
 

Expand and
update crawling
links

     

 

 

F

Light-weighted Train full-text
crawling link

utili estimation

Select the next
crawilng link

we beige utility
estimator

   
 

 

 
  
 
   

   
 
   
  
 
 
  
   
  
  
 

Download the
webs-age ol the
selened link

Apply lull-tent
webpage ul'llitv
measurement

 

 

Aggregate all

 
  
  
   
  

  
  
  

F t “23:11:” downloaded
will: [[awﬁrg link WEIJIJEBES and
thelr utility

utility estimator “are;

(bl

Crawling
seeds
‘-"

U={ur}

Information need 52
Iﬂlllal man Ual Search

Good Bad
example examples

       
     

 

 

3 Apply webnage utilitv
estimation and ranking

 

 

 

       

Priority list

A(“I = {"Taunllqzn ' 

 

Train lull-text web page
unlilv estimator

1’19}
W
/ "J i) Webpage “11(th Webuage utility
dawnluad ’ I measurement
ll!

Feature
HIIB(UDH

 

 

 

 

Randomized selection of
the next crawling link

 

 

 

 

 

 

 

 

  
   

 

(11(Hp(lll),ﬂ)

if!
i—-
/ Flue} } ASEFEEBIE m Aggregate
rESuIls

lRe-llrain Iigl’m

weighted
I’IW.) web page
utility

estimator

 

 

 

 

 

 

 

 

 

 

 

 

{Cpl "pl",  E2)

 

 

 

 

Fig. 1. The overall architecture (a) of our adaptive web crawler design
and its companion computational data ﬂow (b)

4.1 Crawler design I: a naive crawler design

A straightforward construction of a web crawler, which is denoted
as Cl, is to plug the trained utility estimator @(wpk, £2) at the end
of the crawling pipeline. Using this simple design, the crawler ex-
pects a human user to supply a series of queries {Q1, Q2, - - - , Qn},
on which Cl then executes each query either sequentially or in
parallel to obtain a collection of search result web pages {wpx}.
For each obtained search result page wpx, Cl applies the trained
estimator @(wpx, £2) to derive the page’s utility score. Only pages
that carry utility scores above a user-speciﬁed threshold 1' will be
output as the ﬁltered crawling results. One of the key shortcom-
ings associated with the design of Cl is that Cl requires human
users to provide a set of manually composed queries. As discussed
in the beginning of this article, manual query composition poses
non-trivial challenges for human end users. Preparing a set of

queries that precisely expresses the user search intent and also
ensures a comprehensive and diverse coverage over the user de-
manded topics are well beyond the reach of most human search-
ers. To overcome this limitation, we introduce a heuristic query
generator, which is described later in this article. In addition, we
further equip the crawler with an adaptive feature to speedup its
execution. To differentiate from the ﬁrst version of the crawler Cl ,
we name the adaptive version of the crawler as 62, the design of
which will be described in the next section.

4.2 Crawler design 11: a user-oriented web crawler design

4.2.] Problem Statement Given a list of search result URL
links u = {u1,u2, ---, uz}, the goal of an adaptive crawler is to
adaptively and dynamically determine a priority list
A(u) 2 015(1), um), - - - , u,;(z)} where these links shall be crawled.
Here 8() is a ranking function over all the candidate web pages to
be crawled and the notation of um) represents the URL of the
i—th web page that the crawler Visits since the beginning of a
crawling session. As discussed previously, exhaustively down-
loading all the links may take a long time wherein many links
may not be relevant to the end user’s need. Hence in practice,
given a priority list A(u) and a certain amount of downloading
time that a user can afford, the crawler will only download the
header part of the prioritized results until all available time is
used up. Determining a truly optimal ranking function 8() re-
quires the full knowledge regarding the utilities of web pages
pointed to by these links, which is well beyond the reach of
any runtime algorithm. Therefore, we have to rely on some heur-
istics to construct the ranking function.

Given a web page’s URL link u,, once the crawler actually
downloads the actual web page wp(u,-) that is pointed to by the
link, we can measure the utility of the individual search result
page @(wp(u,~), £2) by applying the trained utility estimation func-
tion d>(-, -). The problem now reduces to whether we can predict
the value of @(wp(u,~), £2) based on all the available information
regarding the link u,. A typical search engine will return a high-
light snippet about u,, including ui’s URL, a running head text
and a brief piece of selected text from the search result. Using
text features F(u,-) extracted from the above three types of infor-
mation, our method wants to predict the value of @(wp(u,~), £2).
In our current implementation, F(u,-) includes all the individual
non-stop words in the above snippet text of the web page wp(u,~).
The prediction function is stated as follows:

T(WWI), 9) I F(ul) —> ¢(Wp(ul)a 9) (2)

The key difference between the function T(wp(u,~), £2) and
@(wp(u,~), £2) is that d>(wp(u,-), £2) is estimated using text features
extracted from the full text of the web page wp(u,-) after the web
page is downloaded, whereas T(wp(u,~), £2) is estimated using text
features extracted from the snippet text of the web page wp(u,~)
before the web page is downloaded. It should be noted that to
obtain a sample pair of (F(u,-), @(wp(u,~), £2)) for training the pre-
diction function T, there is a penalty in term of the link Visitation
time, which is non-trivial in many practical scenarios as analyzed
at the beginning of this article. Therefore, from the runtime effi-
ciency perspective, it is desirable to use T(wp(u,~), £2) instead of
@(wp(u,~), £2) if the error can be tolerated. Let 1//(T, t) be the pre-
diction error of the trained predictor T at a given time moment t.

 

107

112 /310'S[Buln0lpJOJXO'SOIJBLUJOJIIIOICI”K1111] 11101} papeolumoq

91oz ‘Og isanV uo ::

S.Xu et al.

 

With the progression of a crawler’s execution in any crawling
session, more training examples will be accumulated, which help
train a more accurate predictor. Lastly, it is noted that the predic-
tion accuracy of T(wp(u,~), £2) is affected by the amount of training
samples available. Because in this work, we assume the time dur-
ation of deriving the value of @(wp(u,~), £2) is negligible once the
web page wp(u,-) is downloaded.

4.2.2 Objective formulation We can now formulate the optimal
URL link Visitation planning task as the following dynamic sched-
ule optimization problem. Let us consider a pool of candidate
URLs bl 2 {ul, uz, - - - ,} to be crawled. The link u,- has the utility
score of @(wp(u,~), £2), if its pointing destination web page has been
downloaded, which costs time T(u,-) to access. In this work, we do
not consider the time of applying the trained estimator d>(-, -) onto
the web page with the URL u,- because all the candidate machine
learning methods we considered for this predictive modeling task
are able to yield the prediction values almost instantly once they
are trained. For the time duration T(u,-), it is primarily the re-
sponse time of the Web site where the target link u,- points to.
The link u,- has a lightweight utility prediction score
T(wp(u,~), £2, tj) at time t], which we assume will take an ignorable
amount of time to assess, as evaluating the function of
T(wp(u,~), £2, tj) does not require downloading the web page
wp(u,-). The above prediction has its relative prediction error inter-
val of [—1//k(wp(u,-), tj), 1//k(wp(u,-), 11)]. In our current implementa-
tion, we derive multiple error intervals for each estimation, which
are differentiated by the subscript k. The estimation conﬁdence
for the relative error interval of [—1//k(wp(u,-), tj), 1//k(wp(u,-), 11)] is
denoted as nk(wp(u,-), 1) e [0, 1], the higher the score is, the more
confident the estimation is. To measure the relative error intervals
and their corresponding conﬁdence, each time when a candidate
predictive model is trained as allowed by the available training
data, we then test the model on the testing dataset using the leave-
one-out evaluation scheme. Instead of aggregating all the testing
errors into some overall error metric, we compute the
90%,80%, ---,10%,5% percentile error intervals [—1//k(wp
(m), t1), WWI)th on = 1, -10) where wkrwprul), r.) is
assigned as the relative error of the k—th percentile error value
derived in the aforementioned procedure. Its corresponding
confidence value nk(wp(u,-), tj) is assigned as the corresponding
percentile value, which indicates the likelihood that the true
error will indeed fall into the estimated error interval.

Given the above notations, we can quantitatively state the goal
of our optimization problem as follows: given a certain amount
of time tx permissible for a crawler, the problem goal is to ﬁnd
an optimal URL Visitation trajectory 12(10, 1,) = (um), um),
- - - , Llama) that maximizes the total utility score of all the
URLs Visited since the beginning moment of a crawling session
to and ends by the time period tx wherein the URL sequence
015(1), um), - - - , u5(nx)} encompasses all the web pages the crawler
manages to Visit under the Visitation trajectory V(tx) within the
given time duration of [t0,tx]. One final touch regarding the
problem statement is that the URL pool Ll itself is a dynamically
growing set. Because each time when the web page wp(u,~)
pointed to by the URL u,- is Visited, the crawler may discover
new URLs from wp(u,-), which will then be extracted and added
into the pool Ll. For reference, we denote the snapshot of the
pool of candidate URLs awaiting to be crawled at the time

moment of t,- as Ll(t,-). At the beginning of a crawling session,
i.e. at the initial time moment to, no web pages have been
crawled. The corresponding candidate URL pool L100) is the
pool of seed web page URLs for launching the crawler.
Formally, we can express the optimization objective as follows:

maXiijBl/(lo,lx)=(us(1),“uuamegUO’ tX’ VUO’ tx» (3)

in which the objective function is deﬁned as follows:

QUO, tx: V00, Ix» =  ¢(Wp(u5(o)a 9) SUbjeCt t0 3
i=1

T (“5(0) 5 tx — to;
g (4)
[71
Man) 6 “(Z T(u5(/)))(i=1, ma")
j=l

Note that in (4), the ﬁrst constraint, ELI T(u,;(,)) 5 IX — to, en-
sures that visiting all the URLs along the Visitation trajectory
V(to, tx) will not take the total length of the allocated time. The
second constraint um) 6 [AZ]: T(u,;(,-))) guarantees that at any
moment when the crawler executes the Visitation trajector
V(to, 5,), which we assume to be the moment after the (i — 1)-
th URL in V(t0, tx) is Visited but before the i—th link is to be
Visited, the next URL the crawler is going to Visit shall only
come from the current candidate URL pool (AZ: T(u,g(,)))
wherein Z: T(u,;(,)) is the corresponding time stamp for the
moment. Also, by deﬁnition, 8() is a ranking function, which
implies that i #j => 8(1) 75 8(1).

It shall be noted that the target function g(to,tX,V(t0,tX))
deﬁned in (4) cannot be directly used in the actual optimization
process during runtime because as mentioned earlier, to obtain
the information @(wp(u,;(,)), £2), the crawler ﬁrst needs to Visit the
URL um), which would incur the cost of link Visitation time
T(u,;(,)). Expecting to have the full knowledge of @(wp(u,;(0), £2)
for all 115(08 involved in the optimal planning process is imprac-
tical because this would require the crawler to Visit every link in
the candidate URL pool, which is highly undesirable. Taking
into account the considerable amount of time cost for ‘know-
ledge acquisition’ in terms of the time required for downloading
the web page wp(u,;(,)) to derive the value of @(wp(u,;(0), £2), we
revise the objective function goo, tx,V(t0, 1,,» in (4) and formu-
late a new objective function 3(a), IX, 12(10, 1,,» that can be eval-
uated computationally on the ﬂy. For simplicity, we use the short
notation of T,- to denote  T(u,;(,-)), which indicates the time
moment immediately after the ﬁrst if 1 URLs have been Visited
by the crawler in a crawling session:

maXimiZevoo, z,)=(u5(1,,  u5(,,,.,)g(loa tx: V00, Ix» =

10
n, k; 17k(WP(us(0)aTi)(1 — 1/1k(WP(us(0)a TDWWPWM): 9: Ti)
10

 

i=1
subject to :

2 Hum) 5 IX — to; use) 6 “(Tl-Xi = 1, "3")-
i=1

(5)

 

108

112 /310'S[Buln0lpJOJXO'SOIJBLUJOJIIIOICI”K1111] 11101} papeolumoq

9103 ‘Og isnﬁnv uo ::

A user-oriented web crawler for e-health research

 

To understand the design of (5), for a given utility estimate for a
web page T(wp(u,;(,)), £2, T,), for its k—th relative error interval
[—1//k(wp(u,s(,)), 7}),1//k(wp(u,g(,)), T,-)], the corresponding lowest
utility estimate is (1 — 1//k(wp(u,;(,)), T,))T(wp(u,;(,)), £2, T,) with
the estimate conﬁdence being nk(wp(u,;(,)), Ti). Please note the
above estimate gives a conservative measure regarding the utility
scores harvested from crawled web page as the actual relative
error may not be as high as the maximum value, 1//k(wp(u,;(,)), T,),
in the error interval. By averaging such estimates for all 10 error
intervals, we derive a conservative estimation of the conﬁdence-
modulated utility score for the i-th web page crawled. Adding up
all nx web pages the crawler acquires along the Visitation trajec-
tory, we derive the overall conﬁdence-modulated utility score for
all the web pages acquired by the crawler during the period of
[10,tx] under the most conservative estimate.

4.2.3 Problem resolution The above formulated optimization
problem is apparently computationally intractable because the
three terms involved in the objective function nk(wp(u,g(0), Ti),
1//k(wp(u,;(,)), Ti) and T(wp(u,;(,)), £2, T,) as well as the candidate
URL pool bl(T,-) are all functions of the time consumed up to a
certain intermediate step, i.e. T,, in the simultaneous crawler
online execution and dynamic planning process. Others may
want to use dynamic programing to derive an optimal solution
for the problem. However, given the typically large number of
steps involved in the planning process it is commonly expected
for a crawler to encounter and gather tens of thousands or even
millions of web pages during one run. Consequently, the cost of
executing a dynamic programing procedure can quickly grow
computationally unaffordable.

To derive the solution in a computationally affordable way, we
hence turn to an approximation algorithm-based approach,
which can be executed efﬁciently in practice. Let t, be a
moment when the algorithm needs to probabilistically choose a
crawling target. At this moment, the algorithm chooses from the
then candidate URL pool bl(tz) a link u,- with the probability of

p,(tz) : % where 1],-(t2) is defined as follows:

“jeuozl '

10
glitz) = Z TIk(Wp(ul), tz)(1 — Illk(wp(ul), tz))T(Wp(ul), 9, tz)- (6)
k=l

Note that before we acquire a sufﬁcient number of samples of
@(wp(u,~, £2)), we cannot train a reliable model to serve as
T(wp(u,~, £2)) because the training data required of T(wp(u,~, £2))
is in the form of pairs of (F(u,-), @(wp(u,-), £2)) (see the deﬁntion of
(2) in Section 4.2.1). Therefore, we assign a uniformly random
distribution over all candidate URLs that are currently available.
That is, we set p,(tz) =  In our current implementation, we
use (6) to assign probability distributions of {p,-(tz)} when the
crawler has acquired >1000 web pages in a crawling session.

4.3 Workﬂow of prototype web crawler

Putting all the algorithmic modules together, we constructed a
user-oriented adaptive web crawler. The overall operating pro-
cedure of the pipeline is as follows. Given a speciﬁc user web
crawling need £2, the user first conducts a few brief web query
efforts wherein he/she would compose a few queries to search the
web. Within the returned search results, the user then selectively

identiﬁes a few ‘good’ search result web pages {wpfood} that sat-
isfy the need £2 as well as a few ‘bad’ search result web pages
{ij9ad} that fail to satisfy the need. Given such initial set of
positive and negative examples, the method then trains the pre-
dictive model @(wpx, £2) for determining the utility score of an
arbitrary search result web page wpx using the method intro-
duced earlier in Section 3. Given the trained web page utility
assessment model @(wpx,£2), we then launch our advaptive
web crawler following the design introduced at Section 4.2.

During the online execution of the web crawler, according to
the utility scores @(wpx, £2) derived from the currently down-
loaded web pages {wpx}, we can then train the lightweighted
web page utility estimation function T(wp(u,~), £2, tz), which can
be deployed to estimate the utility of a web page wp(u,-) without
requiring the web crawler to ﬁrst download the page, thus offer-
ing a tremendous time saving for the crawling procedure. To
balance two alternative options, (i) to derive a most accurate
estimation model T(wp(u,~), £2, tz) by making use of all available
training samples through timely model retraining and (ii) to save
some model training time for the actual web crawling process, we
adopt the tradeoff solution of only retraining the model of
T(wp(u,~), £2, tz) when there are 10% additional training samples
available. Note that after each round of retraining, we will ac-
quire a new version of the functions nk(wp(u,;(,)), Ti),
1//k(wp(u,;(,)), Ti) and T(wp(u,;(,)), £2, T,), which allows the crawler
to update its crawling strategy according to (6).

It shall also be noted that during any moment of the crawler’s
execution process, an end user can always label additional search
results as being satisfactory or not with respect to the informa-
tion acquisition need £2. Such interactive user labeling process

allows an end user to dynamically expand the training sets of

{wplgood} and {ijydd}. Once these two training sets are expanded,

the model of d>(wpx, £2) will be updated, which in turn will lead
to an updated model of T(wp(u,~), £2, t2).

5 EXPERIMENTAL RESULTS

To explore the potential of our proposed web crawler, we con-
ducted two crawling tasks where we, respectively, collected pa-
tient-generated online content regarding two cancer research
topics: one on breast cancer patients and their lifestyle choices
and the other on lung cancer patients with history of smoking.
Both crawling tasks are relevant for addressing epidemiological
type questions by analyzing online personal stories of cancer
patients who meet the speciﬁc selection criteria imposed by the
cancer researcher. For the breast cancer study, 133 positive and
875 negative exemplar search results were manually collected by
a human researcher to initialize the crawling system. These
sample results were mostly collected by manually searching es-
tablished cancer-related forums such as the American Cancer
Society’s cancer survivor network forum ACS (2013). To assess
the performance advantage of the new crawler with respect to the
state-of—the—art, we compared our crawler with one of the leading
adaptive web crawlers proposed by Barbosa and Freire (2007) as
a peer crawling method. We implemented the prototype system
of the peer method according to the design and all technical
details disclosed in their original publication. In our comparative

 

109

112 /310'S[Buln0lpJOJXO'SOIJBLUJOJIIIOICI”K1111] 11101} papeolumoq

9103 ‘Og isnﬁnv uo ::

S.Xu et al.

 

experiments, the same set of example search results is used to
train and initialize the peer crawling method.

The runtime performance of both crawlers is reported in
Figure 2 for both case studies. The ﬁgure shows the total
amount of web pages crawled (Raw Volume), the amount of
web crawling results obtained by a crawler that are estimated
relevant to the current crawling objective wherein the relevance
estimation is performed by the crawler’s built-in self-assessment
capability (Gross Volume) and the amount of satisfactory search
results obtained as judged by the human end user (Net Volume).
In addition, we also show the temporal precision (Precision) and
cumulative precision (Cumulative Precision) of either crawler
throughout the whole crawling process as measured Via a sam-
pling-based human evaluation procedure. Owing to the large
volume of crawling results produced, it is prohibitively expensive
to ask a cancer researcher to manually evaluate the quality of
each individual result for deriving the precision of either crawler.
Therefore, we used a sampling-based manual evaluation proced-
ure wherein the researcher manually evaluated the quality of
every other 50 crawling results using binary labels (1: relevant,
0: irrelevant) regarding the relevance of the sampled result.
Comparing between (a) and (b) in Figure 2, we can see that
the peer method obtains a slightly larger raw crawling volume
than our crawler, which is possibly caused by the more sophis-
ticated decision logic used by our method. This qualitative dif-
ference is weakly reversed when it comes to the gross crawling
volume, suggesting that our method works better in guiding itself
to encounter more useful web pages than the peer approach.
More importantly, the precision attained by the new crawler,

 

 

 

 

 

 

 

(a) A?» _ 
T o 3—9 A *’f
/ ‘q z \ X .. __‘ . _ / x _ A!
o I'x_.x_\*_’g__.x--x--i’--X- grit % g‘ggﬁ-‘tugl-x-nQ-ag a
8 _ .4" V +’ \ I
I' a \
8 ,+ I _ w c
‘E’ *‘l ° ° '3
= 21’ ._
_ 0
g ‘ ’1'” _ v 2
ﬂ" 0 n.
G
O
o — _ N.
a c
a _ _ G.
I I I I T
Time (hours) 5 10 15 20
-l- Raonlume — Gross Volume - Net Volume
-0- Precision  Cumulative Precision
C
( )§ _ /+ - 3
8 "I’
0 2V
_ .a I\ A“ _ a;
9/ \ Aj’l’ 51 a
g I \ ,4', I \
/ I
a g T 0 like ’ _+/\ I ‘\ I \ to e
v / I \ — ' o
E T. / T9 \ /at-ft-W'buﬂqM-Ix-"xi-x-‘x-ux T '6
,x-' \ ,0 v
> 8 ‘x“x/ ,1,» \ ’ b/ ‘ o n.
a _ \ d +» \'
a b- +1 a N
_ T o'
O O
_ T o"

 

 

 

Time (hours) 5 1o 15 20

-l— Raonlume
—0- Precision

— GrossVolume - NetVolume

"X- Cumulative Precision

Fig. 2. Performance comparison between our new crawling method (a) and the peer method by Barbosa and Freire (2007) (b) for collecting life stories of

both in terms of its temporal precision and cumulative precision
as evaluated by the human searcher, is consistently superior to
that of the peer method. Consequently, the net crawling amount
obtained by our proposed crawler is substantially more abun-
dant than that of the peer method. Table 1 reports the quanti-
tative difference between the two methods in terms of their net
crawling volumes.

To further assess the performance advantage of our web craw-
ler, we performed a secondary experiment for the breast cancer
case study by asking our adaptive crawler to repeat the search
using a substantially reduced number of exemplar search results.
Specifically, the crawler was initialized using only 67 positive and
438 negative search results. The results are also shown in the
figure, which demonstrate a similar qualitative performance dif-
ference between the two methods in comparison: (i) the peer
method encounters more web pages than our crawler in that it
obtains a larger raw crawling volume than the new method; (ii)
the gross volume of the two methods is comparable, with a slight
advantage achieved by the new crawler; (iii) the precision of the
new crawler is substantially higher than that of the peer method,
leading to a substantially larger net volume of results crawled by
our approach. Table 1 shows a quantitative comparison between
the two adaptive web crawling methods in terms of their effect-
iveness and time efficiency of harvesting user-desired online
content for two cancer case studiesione on breast cancer and
the other on lung cancer research using both an enriched set and
a reduced set of user-labeled exemplar search results to train each
crawler, respectively. This table reports the number of distinct
web pages crawled that are relevant to the specific information

 

 

 

 

 

 

      
 

_ °.
(b)  -
§ _ ,+’ a
a A” ' °
.1-
4’
«P
a ' ’* ' 3 
7:, 3 v‘I'JI'o / _q E
> g - /o 4.! 40’ x\ I *\ a £3 _ :I E
T tgx;I§-\§;-y$=%-*'¥'*" 9gp" ‘3‘ I """/"\‘x'; "I
a x \ I \Y N
_ ’4: a _ 6
° _ _ c:
I I I I T
Time (hours) 5 10 15 20
-l- Raonlume — GrossVolume - NetVolume
-0- Precision - ~x - Cumulative Precision
(d) c.
g **+ _ "'
° ‘ 21'
D
ID ,1! w
)Il-T —
— d, o
o ’+’
o E T +T+ — ‘° =
a v ,+’ ° g
:1 + _
a - +’ 8
> 9 fl" 9 9‘9 T g E
O r o I ‘9
° T r-g. ,‘l‘ \ / \ ’ \
a \-x—-’x-..x.,lx-er-x--§~x--.x-I-x-\{x-;R(-x--;l-..x,{x---x---X N
7 +,\+, \ I o \ I , \ I x — o-
+, a a ‘d b
a . . n - - - . . . . - - - - . . . . - - - . . . . - - - - . . . - - - - - . . . n - - - - . . . . . - - . . . . . . . . . . . . . . . - . . .7 a
T T 9'

 

 

 

Time (hours) 5 1o 15 20

-l— Raonlume
—0- Precision

— Gross Volume - NetVolume

-->t- Cumulative Precision

breast cancer patients. Both crawling methods were initialized using the same set of labeled samples. To explore the inﬂuence of the training sample size,
we randomly selected 50% of the available positive and negative training samples and repeated the aforementioned comparable analysis (c and d)

 

110

112 /310's1eu1nolp101xo"sothJJOJHtotq/ﬁduq 111011 papeo1umoq

9103 ‘0g isnﬁnv uo ::

A user-oriented web crawler for e-health research

 

Table 1. Quantitative comparison between the performance of our craw-
ler and that of the peer method (Barbosa and Freire, 2007) in terms of the
net volumes of user-desired online content harvested by each crawler for
progressively extended periods of crawling time

 

Comparison Cumulative crawling time (hour)

 

Method 1 6 1 1 16 20

 

(a) Crawling for the breast cancer study (enriched training set)

Peer crawler 225 1021 2403 2904 3607

Our crawler 506 3085 5732 8691 10628

Rate (our/peer) 2.25 3.02 2.39 2.99 2.95
(b) Crawling for the breast cancer study (reduced training set)

Peer crawler 562 1 125 1565 1964 2403

Our crawler 284 1569 3184 4830 5800

Rate (our/peer) 0.51 1.39 2.03 2.46 2.41
(c) Crawling for the lung cancer study (enriched training set)

Peer crawler 166 883 1620 2325 3263

Our crawler 404 3124 5286 6810 8130

Rate (our/peer) 2.43 3.54 3.26 2.93 2.49
(d) Crawling for the lung cancer study (reduced training set)

Peer crawler 107 1104 2021 2643 3404

Our crawler 168 1409 2285 3302 4881

Rate (our/peer) 1.57 1.28 1.13 1.25 1.43

 

Note: See more detailed explanations about the comparative experiments for per—
formance benchmarking in the text.

needs and requirements of either study, referred to as ‘net vol-
umes’, after executing each adaptive crawling process for pro-
gressively extended periods of time, namely after 1, 6, 11, 16 and
20h of crawling. To derive the end user-evaluated net volumes of
online content obtained up to each snapshot moment of a crawl-
ing process, we adopt the aforementioned selective sampling-
based manual evaluation strategy. As mentioned earlier in the
text, both our adaptive web crawler and the state-of—the—art peer
crawler were trained and initialized using the same set of seed
URLs and user-labeled exemplar search results for capturing and
understanding the type and scope of online content desired by
e-health researchers in either crawling experiment. Instead of re-
porting the raw volumes of web content acquired by the two
crawlers, respectively, we choose to compare the net volumes
of selectively acquired online content because the latter volumes
more truthfully indicate the amount of acquired web content
relevant and useful for e-health researchers in either study. The
last row of each subtable also reports the rate of harvesting user-
desired online content by our crawler with respect to the harvest-
ing rate of the peer crawler at each crawling snapshot moment.
In both comparative studies, the adaptive crawler is consistently
superior to the peer method. In addition, the comparative study
using a reduced set of user-labeled sample web search results
further supports the advantage of the new crawler. Namely,
our new crawler can be initialized with a small set of exemplar
search results for quick launch, which is still capable of obtaining
superior crawling performance.

Similarly, for the lung cancer case study, 73 positive and 700
negative web pages from sites such as the Lung Cancer Support
Community were manually collected as exemplar search results
to initialize the system. For comparison purposes, we further

conducted a peer crawling session by using a reduced set of
human labels consisting of 50 positive and 400 negative sample
web pages. The runtime performance of our crawler for the new
crawling task under the two initialization conditions is reported
in Figure 3 and Table 1. Similar to the breast cancer case study,
the new crawler demonstrates clear advantage over the peer
method for the lung cancer case study as well.

As illustrated in Figure 3, for the lung cancer study, the pre-
cision attained by the proposed new crawler, both in terms of its
temporal precision and cumulative precision according to the
evaluation by the human end user, is consistently superior to
that of the peer method. Benefited by this prevailing advantage
of the new crawler in more precisely locating and acquiring
online content relevant to the speciﬁc crawling needs, the net
crawling amount by the new crawler consistently surpasses that
of the peer method for both crawling sessions using the enriched
and reduced training sets of user-labeled exemplary search re-
sults. This conclusion can also be quantitatively veriﬁed by the
comparative rate of harvesting speeds between the two crawling
methods as reported in the last rows of the subtables (b) and
(d) in Table 1.

We further observe that when trained using the enriched set of
user-labeled exemplary search results, our crawler obtains a
roughly comparable rate of raw crawling volumes as the peer
method. Yet, when trained using the reduced set of user-labeled
search results, for the initial 10 h of crawling, our crawler exhibits
a slower rate of harvesting the raw crawling volume than the peer
method. However, as the crawling time keeps increasing, our
crawler gradually catches up with the peer crawler in terms of
the raw crawling volume. At the end of the 21 h of crawling, the
raw volumes of results obtained by both methods become highly
comparable.

For the initial slower rate of acquiring the raw crawling
volume, recalling the early conclusion that the new crawler con-
sistently sustains a superior crawling precision than the peer
method, it seems that the proposed new crawler favors precision
rather than the raw crawling volume as compared with the peer
crawler. That is, we hypothesize that the new crawler may choose
to spend more time in executing its adaptive web crawling logics
to determine on—the-ﬂy a web crawling plan for the current
crawling task rather than the alternative strategy of performing
more operations of raw web crawling with a less deliberated
adaptive crawling plan. Consequently, the new crawler may
exhibit a slower rate of web crawling than the peer method.
As veriﬁed by the performance evaluation results reported in
Figure 3 and Table 1, such prioritized execution of the planning
process for adaptive web crawling turns out to yield more effect-
ive overall return in terms of the net volume of crawling results
obtained. Such tactical crawling strategy determination may not
be necessary or evident when the amount of available user-
labeled exemplary training samples is abundant; yet when the
samples are scarce or less informative, extra planning in the
adaptive crawling process may be more important for the pro-
posed crawler compared with the peer method.

For the second phase of the gradual speedup of the proposed
adaptive crawler, a potential reason is that when more online
content has been crawled and thus becomes available for retrain-
ing the lightweighted crawler navigation model T(wp(u,~), £2, 12),
the utility of frequent model retraining declines. Thus, by shifting

 

111

112 /310'S[Buln0lpJOJXO'SOIJBLUJOJIIIOIq/ﬂduq 111011 papeo1umoq

9103 ‘0g isnﬁnv uo ::

S.Xu et al.

 

 

A
0)
V
00
\
+
I
10

’0 Ark X"\

80000
4
,5:
i

*1
‘1‘ 1’
1’2“
j;

i;

a

a?

X'

1‘
)l
0'6

Volume

40000
|

 

 

 

0
0.0

 

Time (hours) 5 10 15 20

-l— Raonlume
—0- Precision

— Gross Volume - Net Volume

--x- Cumulative Precision

 

A
0
V
a

I
1 0

80000
I
‘4:
3k
n 6

0.4

Volume
, ,l
73

ads
I
x
Re

40000
I

0.2

 

 

 

0

 

Time (hours) 5 10 15 20

—l- Raonlume
—0- Precision

— Gross Volume - Net Volume

- -x - Cumulative Precision

Precision

Precision

 

 

 

 

 

 

O
<b> s - ,. - g
3 ﬂue
_ ,‘I‘ _ a
+z‘l‘ O
O +,
o r
O — I... w =
o g ’4. _ d o
E 2* 9 'n
3 _ + .5
o ,y I \ v 2
> o .+’ I 8 n-
g _  <:.\. As. 4x.\..x’.x---x-\-‘lr--§><§ a
3 090‘ s; x y - g-s-II- \Xf .ér \ r
b ‘d _ N
_ O
2i"
o — 4"? - - . . . . - - - - . . . c . - - - - . . . . . - - - . . . . . - - - . . . c . . - - . . . . - - - - . . . - - - -- _ Q-
I I I I T
Time (hours) 5 1o 15 20
-l— RachIume — Gross Volume - NetVolume
—0- Precision --x- CumulativePrecision
_ °.
(d) 1, ,+ ,
)F/‘l‘
_ W
a Aral“ 6
° — a!
g 21’
0 ‘° )1! _ to g
E. 1,!“ ° :g
g ° ,4” _ s. 2
8 T I“ a +’+ A ° T
° Kn __ 1. .—.. __ _ . _._ u
v ‘3) ’k\*, ‘V * *3 *“"';'§"*--Is-:s-;*--x a
+r+ g \°,e \\ , e.\ _ O.
I / o
z‘l‘
° - — 2

 

 

 

 

 

I I |
Time (hours) 5 1o 15 20

—l- Raonlume
—0- Precision

— Gross Volume - NetVolume

--x- Cumulative Precision

Fig. 3. Performance comparison between our new crawling method (a) and the peer method by Barbosa and Freire (2007) (b) for collecting personal
stories of lung cancer patients with history of smoking. Both crawling methods were initialized using the same set of labeled samples. To explore the
inﬂuence of the training sample size, we randomly selected a subset of the available positive and negative training samples and repeated the aforemen-

tioned comparable analysis (c and d)

more time from the model retraining to actual web crawling
operations, the crawler can obtain the raw volume faster. A
second potential reason for the accelerated rate of the raw crawl-
ing volume is that after the crawler has accumulated a critical
mass amount of online content, such an intermediate result set
may lead to a more effective T(wp(u,-),£2,tz) for guiding the
crawler to Visit web pages with fast link Visitation time.

6 DISCUSSION

Comparing the design of our newly proposed adaptive crawler
with that of the peer method presented in Barbosa and Freire
(2007), there exist four key aspects of similarities between the two
approaches as follows: (i) both methods are designed with an
online learning capability, which automatically learns on—the-
ﬂy to capture characteristics of promising crawling paths or des-
tination web regions that lead to the most rewarding discoveries
of online content relevant to the current crawling needs,; (ii) both
methods are equipped with a randomized link Visitation strategy
where the likelihood of selecting a certain link from the candidate
URL pool for Visit in the next crawling step is probabilistically
modulated by the estimated reward that the link can lead to the
discovery of relevant content, (iii) both methods are equipped
with some self-assessment and self-critic module that can autono-
mously determine the relevance of any harvested web page with
respect to the crawling needs for selective output of crawling
results and (iv) both methods extract text features and select
the most reliable subset of candidate features to construct the
predictive estimator on the crawling utility of a link.

The key differences between the two methods include (i) to
forecast the utility of a link it), our method introduces a light-
weighted learner T(wp(u,~), £2) that predicts the utility score
before crawling the web content pointed to by u,- according to
the information provided by the snippet associated with it). Such
snippet is always available for search results returned by a typical
search engine (such as Google and Yahoo). The snippet includes
ui’s URL, a running head text of u,- and a brief piece of selected
text from the search result associated with u,. In comparison, the
adaptive link learner introduced in the peer method only exam-
ines the text information encoded in a link’s URL when perform-
ing the link utility estimation. The extended scope of textual
information available from the snippet of a link allows our pre-
dicting function to be able to estimate the link utility more ac-
curately (as conﬁrmed by the experimental results), which results
in better accuracy in targeted web crawling. (ii) In addition to the
lightweighted learner T(wp(u,~), £2), our method also carries a
more powerful web page utility assessment function @(wp, £2)
that measures the utility of a web page wp with respect to the in-
formation need £2 after the web page is crawled. Based on the
output of the function @(wp,£2), we dynamically retrain the
function of T(wp(u,~), £2) so that the particular machine learning
model selection and conﬁguration are optimized on—the-ﬂy ac-
cording to all the cumulative quality assessment scores produced
by the function d>(wp,£2) since the beginning of the current
crawling session. This novel two-tier online learning framework,
which was not present in the peer method, allows our method to
be able to train a more tailored and task-optimized link utility
predictor T(wp(u,-), £2) in an autonomous fashion. (iii) The peer

 

112

112 /310'S[Buln0lpJOJXO'SOIJBLUJOJIIIOIq/ﬂduq 111011 papeo1umoq

9103 ‘0g isnﬁnv uo ::

A user-oriented web crawler for e-health research

 

method uses a specialized classiﬁer for recognizing online search-
able forms as a critic to judge the relevance of their online crawl-
ing results. This feedback mechanism is only applicable for the
particular crawling needs to discover web pages of searchable
forms as hidden entries to online databases. In contrast, when
assessing the utility of a crawled online content web page, our
method comprehensively considers the content words in the main
body of an HTML file, words in the heading and subtitles of an
HTML file and the anchor text embedded in an HTML file.
Beneﬁted by this more generic design of the self-assessment
mechanism, which is coupled by a corresponding more advanced
text feature extraction, selection and predictive modeling imple-
mentation, our adaptive crawler is able to detect online content
meeting a much wider spectrum of users’ needs for a more gen-
eric scope of applications. (iv) Our new crawler design explicitly
models the confidence and reliability of its link utility prediction
performance during the execution of online web crawling and
considers such uncertainty during its planning of adaptive stra-
tegies for the current crawling task. This uncertainty modeling
feature is missing from the design of the peer method. (V) Our
crawler design explicitly models the time required to access a
given web page for balancing the time spent between developing
more carefully planned crawling strategies versus executing more
operations of web page harvesting with less deliberated crawling
strategies. Such feature is also absent from the design of the peer
method.

To better understand the behavior characteristics of the two
crawling methods, we conducted some further investigative ana-
lysis to comparatively examine crawling results obtained by each
crawler for the two case studies reported in this article. First, we
examined the overlap between crawling results harvested by the
two crawlers using three metrics, which assess the amount of
common content between two crawling result sets S1 and S2
on the levels of key words, documents and sites, respectively.
For the key word-level overlap between S1 and S2, we ﬁrst
extracted the key words from each result set using the RAKE
algorithm (Rose et al., 2012). Next, we ranked the two lists of key
words according to each key word’s term frequency-inverse
document frequency value among its corresponding crawling
result set. We then counted the number of common key words
included in the top-ranked key word lists of S1 and S2 as the key
word-level overlap between S1 and S2. Figure 4 reports results of
the estimated key word-level overlap between the two sets of
crawling results where both crawlers are trained with the full
set of example search results. Figure 4a shows that the key
word-level overlap remains roughly in the same value range (be-
tween 60 and 80%) even when estimated using different sizes of
top-ranked key words. We then chose a representative window
size (300) for the top-ranked key words and estimated the key
word-level overlap throughout the entire crawling sessions. The
results are shown in Figure 4b, according to which we can see the
overlap between the two crawling result sets also stably remains
within the same value range of 60 and 80% during the progres-
sion of the crawling processes. For the document-level overlap,
we use the cosine distance to measure pairwise document simi-
larity. We then estimated the overlap between S1 and S2 as the
number of documents in S1 and S2 that can find at least one
counterpart document in the other crawling result set with which
their pairwise document similarity is no <90%, divided by the

 

A
0)
V
80%
I

Overlap
70%
1

 

 

 

 

60%
|

100 200 300 400 500
Number of top-ranked keywords

—9- Breast Cancer -+- Lung Cancer

 

80%
|

Overlap
70%
1

 

 

 

60%
|

 

Time (hours)

—9— BreastCancer -+- Lung Cancer

Fig. 4. Estimated key word-level overlap between crawling results ob-
tained by our adaptive crawler and the peer crawler for the two case
studies (using the full set of training examples in each crawling session)

total number of documents in S1 and S2 (without duplicate
document removal). For the breast cancer study, the estimated
document-level overlap is 69.5%; for the lung cancer case study,
the estimated document-level overlap is 66.2%. Lastly, we also
estimate the site-level overlap between the two crawling result
sets. For the breast cancer study, the estimated site-level overlap
is 33.7%; for the lung cancer case study, the estimated site-level
overlap is 27.3%. We suspect the reason for the reduced numbers
in the estimated overlap scores is because the site-level overlap
estimate focuses on the physical URL positions of the acquired
web pages rather than the actual content embedded in these web
pages. Therefore, site-level overlap estimate presumably under-
estimates the actual content overlap between the two crawling
result sets. Overall, according to the above analysis of overlap-
ping content between the two crawling result sets, we acknow-
ledge the beneﬁt of simultaneously running both crawlers to
acquire online content in a complementary fashion, as only a
partial body of the harvested materials is commonly acquired
by both crawling methods.

To understand the content similarities and differences between
online materials harvested by the two crawlers, we further gen-
erated for each case study three lists of top-ranked key words,
including key words that appear in both sets of crawling results,
referred to as ‘common keywords’ and key words that appear
only in crawling results acquired by one of the two crawlers,
referred to as ‘unique keywords’. By manually reading through
these three lists of top-ranked key words saliently embodied in
the crawling results, we empirically notice that the common key
words cover overwhelmingly generic non-technical terms asso-
ciated with cancer, such as ‘cancer’, ‘therapy’, ‘surgery’, ‘treat—
ment’, ‘tumor’ and ‘diagnosis’. For key words unique to the peer
crawling method, they primarily include technical terms asso-
ciated with cancer, such as ‘invasive breast cancer’, ‘ultrasound’,
‘discharge’, ‘Vivo’, ‘ducts’, ‘lobules’, ‘mortality’, ‘ductal carcin-
oma’ and ‘inhibitors’. Finally, key words unique to our adaptive
crawler are mostly associated with lifestyle aspects of a patient,
and fewer key words describing the disease itself. In summary,

 

113

112 /310's112u1n0[p101x0'so1112111101u101q//:d1111 111011 pap1201um0q

9103 ‘0g15n8nv 110 ::

S.Xu et al.

 

the peer method appears to collect online content richer in med-
ical terms, whereas the adaptive crawler appears to be able to
harvest cancer patient stories that expose more abundant details
in lifestyle and emotionally related matters. It is noted the latter
type of crawling results acquired by our crawler agrees better
with the true information collection needs of e-health researchers
in both case studiesione about gathering life stories of breast
cancer patients and the other about smoking of lung cancer pa-
tients. We speculate this better alignment of crawling results with
researchers’ information acquisition needs is achieved by the
more content-sensitive adaptive crawling mechanism of our
new crawler, beneﬁted by its more advanced selective online con-
tent detection and acquisition algorithm proposed in this article.

7 CONCLUSION

We propose a user-oriented web crawler that can adaptively ac-
quire social media content on the Internet to meet the speciﬁc
online data source acquisition needs of the end user. We evalu-
ated the new crawler in the context of cancer epidemiological
research with two case studies. Experimental results show that
the new crawler can substantially accelerate the online user-
generated content acquisition efforts for cancer researchers
than using the existing state-of—the-art adaptive web crawling
technology.

ACKNOWLEDGEMENTS

This manuscript has been authored by UT—Battelle, LLC, under
Contract No. DE-AC05 000R22725 with the US. Department
of Energy. The United States Government retains and the pub-
lisher, by accepting the article for publication, acknowledges that
the United States Government retains a non-exclusive, paid-up,
irrevocable, world-wide license to publish or reproduce the pub-
lished form of this manuscript, or allow others to do so, for
United States Government purposes. This study was approved
by the Oak Ridge site-wide Internal Review Board.

Funding: National Cancer Institute (lR01CA170508—01).

Conﬂict of Interest: none declared.

REFERENCES

ACS. (2013). American Cancer Society: Cancer Survivors Network, Atlanta, GA,
USA.

Aggarwal,C.C. (2002) Collaborative crawling: mining user experiences for topical
resource discovery. In: Proceedings of the Eighth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. KDD’02, pp. 423428.

Aggarwal,C.C. et al. (2001) Intelligent crawling on the World Wide Web with
arbitrary predicates. In: Proceedings of the IOth International Conference on
World Wide Web. WWW’01. pp. 967105.

Ahlers,D. and Boll,S. (2009) Adaptive geospatially focused crawling. In:
Proceedings of the I8th ACM Conference on Information and Knowledge
Management. CIKM’09. pp. 445454.

Almpanidis,G. and Kotropoulos,C. (2005) Combining text and link analysis for
focused crawling. In: Proceedings of the Third International Conference on

Advances in Pattern Recognition — Volume Part I. ICAPR’05. Springer—Verlag,
Berlin, Heidelberg, pp. 2787287.

Almpanidis,G. et al. (2005) Focused crawling using latent semantic indexing: an
application for vertical search engines. In: Proceedings of the 9th European
Conference on Research and Advanced Technology for Digital Libraries.
ECDL’05. Springer—Verlag, Berlin, Heidelberg, pp. 402413.

Babaria,R. et al. (2007) Focused crawling with scalable ordinal regression solvers.
In: Proceedings of the 24th international conference on Machine learning.
ICML’07. pp. 57764.

Badia,A. et al. (2006) Focused crawling: experiences in a real world project. In:
Proceedings of the I5th International Conference on World Wide Web. WWW’06.
pp. 104371044.

Barbosa,L. and Freire,J. (2007) An adaptive crawler for locating hidden web entry
points. In: Proceedings of the I6th International Conference on World Wide Web.
WWW’07. ACM, New York, NY, pp. 441450.

Batsakis,S. et al. (2009) Improving the performance of focused web crawlers. Data
Know]. Eng, 68, 100171013.

Chakrabarti,S. et al. (2002) Accelerated focused crawling through online relevance
feedback. In: Proceedings of the IIth international conference on World Wide
Web. WWW’02. pp. 1487159.

Chen,Z. et al. (2009) A cross—language focused crawling algorithm based on mul—
tiple relevance prediction strategies. Comput. Math. Appl., 57, 105771072.
Chung,C. and Clarke,C.L.A. (2002) Topic—oriented collaborative crawling. In:
Proceedings of the Eleventh International Conference on Information and

Knowledge Management. CIKM’02. pp. 34—42.

de Assis,G.T. et al. (2008) The impact of term selection in genre—aware focused
crawling. In: Proceedings of the 2008 ACM symposium on Applied Computing.
SAC’08. pp. 115871163.

Dey,M. et al. (2010) Focused web crawling: a framework for crawling of country
based ﬁnancial data. In: Proc. IEEE International Conference on Information and
Financial Engineering (ICIFE). pp. 4097412.

Fu,T. et al. (2012) Sentimental spidering: leveraging opinion information in focused
crawlers. ACM Trans. Inf. Syst., 30, 24:1724:30.

Furuse,K. et al. (2011) An extended method for ﬁnding related web pages with
focused crawling techniques. In: Proceedings of the I5th International
Conference on Knowledge—Based and Intelligent Information and Engineering
Systems — Volume Part II. KES’ll. Springer—Verlag, Berlin, Heidelberg, pp.
21730.

Gao,W. et al. (2006) Geographically focused collaborative crawling. In: Proceedings
of the I5th International Conference on World Wide Web. WWW’06. pp.
2877296.

Guan,Z. et al. (2008) Guide focused crawler efﬁciently and effectively using on—line
topical importance estimation. In: Proceedings of the 31st Annual International
ACM SI GIR Conference on Research and Development in Information Retrieval.
SIGIR’08. pp. 7577758.

Hall,M. et al. (2009) The weka data mining software: an update. ACM SIGKDD
Exp. Newslett., 11, 1&18.

Kleinberg,J.M. (1999) Authoritative sources in a hyperlinked environment.
J. ACM, 46, 604432.

Kohlschutter,C. (2011) The Boilerpipe library: boilerplate removal and fulltext
extraction from html pages. In: Google Code Base.

Liu,H. and Milios,E. (2012) Probabilistic models for focused web crawling. Comput.
Intel]., 28, 2897328.

Menczer,F. et al. (2004) Topical web crawlers: evaluating adaptive algorithms.
ACM Trans. Internet Technol., 4, 378419.

Micarelli,A. and Gasparetti,F. (2007) The Adaptive Web: Adaptive Focused
Crawling. Springer—Verlag, Berlin, Heidelberg, pp. 2317262.

Pant,G. and Srinivasan,P. (2005) Learning to crawl: comparing classiﬁcation
schemes. ACM Trans. Inf. Syst., 23, 430—462.

Pant,G. and Srinivasan,P. (2006) Link contexts in classiﬁer—guided topical crawlers.
IEEE Trans. Know]. Data Eng, 18, 1077122.

Rose,S.J. et al. (2012) Rapid automatic keyword extraction for information retrieval
and analysis. US patent 8,131,735 B2. July 2, 2009.

Zhuang,Z. et al. (2005) What’s there and what’s not?: focused crawling for missing
documents in digital libraries. In: Proceedings of the 5th ACM/IEEE—CS Joint
Conference on Digital libraries. JCDL’05. pp. 3017310.

 

114

112 /310's112u1n0[p101x0'so1112111101u101q//:d1111 111011 pap1201um0q

9103 ‘0g1sn8nv 110 ::

