BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

S.Chatterjee et al.

 

may still require several days to process a dataset in a desktop
environment. Given this challenge, considerably faster methods
based on different convex optimization strategies have been re—
cently proposed (Koslicki et al., 2013; Meinicke et al., 2011). In
particular, sparsity—based techniques, mainly compressive sen—
sing—based algorithms (Candes, and Wakin, 2008), are used for
estimation of bacterial community composition in (Amir and
Zuk, 2011; Koslicki et al., 2013; Zuk et al., 2012). However,
(Amir and Zuk, 2011) used sparsity—promoting algorithms to
analyze mixtures of dye—terminator reads resulting from Sanger
sequencing, with the sparsity assumption that each bacterial
community comprises a small subset of known bacterial species,
the scope of the work thus being different from methods in—
tended for high—throughput sequence data. The Quikr method
of (Koslicki et al., 2013) uses a k—mer—based approach on 16S
rRNA sequence reads and has a considerable similarity to the
method (SEK: Sparsity Exploiting K—mers—based algorithm)
introduced here. Explained brieﬂy, the Quikr setup is based
on the following core theoretical formulation: given a reference
database D= {(11, ,dM} of sequences and a set S= {51, . . . , 5,}
of sample sequences (the reads to be classiﬁed), it is assumed that
there exists a unique  for each 5,, such that s, =  In general, all
reference databases and sample sets consist of sequences with
highly variable lengths. In particular, the lengths of reference
sequences and samples reads are often different. Violation of
the assumption leads to sensitivity in Quikr performance accord—
ing to our experiments. Another example of fast estimation is
called Taxy (Meinicke et al., 2011), which addresses the effect of
varying sequence lengths (W ommack et al., 2008). Taxy uses a
mixture model for the system setting and convex optimization
for a solution. The method referred to as COMPASS (Amir
et al., 2013) is another convex optimization approach, similar
to the Quikr method, that uses large k—mers and a divide—and—
conquer technique to handle large resulting training matrices.
The currently available version of the Matlab—based
COMPASS software does not allow for training with custom
databases, so a direct comparison with SEK is not yet possible.

To enable fast estimation, we adopt an approach where the
estimation of the bacterial community composition is performed
jointly, in contrast to the read—by—read analysis used in the RDP
classiﬁer. Our model is based on kernel density estimators and
mixture density models (Bishop, 2006), and it leads to solving an
under—determined system of linear equations under a particular
sparsity assumption. In summary, the SEK approach is imple—
mented in three separate steps: off—line computation of k—mers
using a reference database of 16S rRNA genes with known taxo—
nomic classiﬁcation, online computation of k—mers for a
given sample and then final online estimation of the relative
frequencies of taxonomic units in the sample by solving an
under—determined system of linear equations.

2 METHODS

2.1 General notation and computational resources used

We denote the non-negative real line by R+. The EP norm is denoted
by ||.||p, and [E[.] denotes the expectation operator. Transpose of a vector/
matrix is denoted by (.)’. We denote cardinality and complement of a set
8 by |8| and S, respectively. In the computations reported in the remain-
der of the article, we used standard Matlab software with some instances

of C code. For experiments on mock community data, we used a Dell
Latitude E6400 laptop computer with a 3GHz processor and 8GB
memory. We also used the cvx (Boyd and Vandenberghe, 2004) convex
optimization toolbox and the Matlab function lsqnonneg() for a least-
squares solution with non-negativity constraint. For experiments on
simulated data, we used standard computers with an Intel Xeon x5650
processor and an Intel i7-4930K processor.

2.2 k—mer training matrix from reference data

The training step of SEK consists of converting an input labeled database
of 16S rRNA sequences into a k-mer training matrix. For a ﬁxed k, we
calculate k-mers feature vectors for a window of ﬁxed length, such that
the window is shifted (or slid) by a fixed number of positions over a
database sequence. This procedure captures variability of localized
k-mer statistics along 16S rRNA sequences. Using bp as the length unit
and denoting the length of a reference database sequence d by L,,, and
further a ﬁxed window length by L”, 5 L,, and the ﬁxed position shift by
LP, the total number of subsequences processed to k-mers is close to
Lh’ff‘ﬂ. The choice of L”, may be decided by the shortest sample sequence
length that is used in the estimation, assuming the reads in a sample set
are always shorter than the reference training sequences. In practice, for
example, we used L... = 450 bp in experiments using mock communities
data. The choice of LF is decided by the trade-off between computational
complexity and estimation performance.

Given a database of reference training sequences D = {dl,...,dM}
where d,“ is the sequence of the mth taxonomic unit, each sequence d,“
is treated independently. For dm, the k-mer feature vectors are stored
column-wise in a matrix Xm e [RAP/V“, where Nm % L%j. From the
training database D, we obtain the following full training” matrix

X=[X1X2 ...,XM] e Rik”,
E[X1X2 ...xN],

M I:
where "1:1 Nm =N, and x,, e Ri“ denotes the nth k-mers feature
vector in the full set of training feature vectors X.

2.3 SEK model

For the mth taxonomic unit, we have the training set
_ 4"an
Xm7[xm1xm2...mem] ER+ .

where we used an alternative indexing to denote the 1th k-mer feature
vector by xml. Letting x and Cm denote random k-mer feature vectors and
mth taxonomic unit, respectively, and using Xm, we ﬁrst model the con-
ditional density p(x|Cm) corresponding to mth unit by a mixture density
as follows:

Nm

polo.) = 2 am] pmrxlxmz. em». (1)
[=1

where 01m; 2 0. 21"] am; = 1, xml is assumed to be the mean of distribu-
tion pm,, and 6),"; denotes the other parameters/properties apart from the
mean. In general, 17,“, could be chosen according to any convenient para-
metric or non-parametric family of distributions. In biological terms, am,
reﬂects the ampliﬁcation of a variable sequence region and how probable
that is in a given dataset with a sufﬁcient level of coverage. The approach
of using training data xml as the mean of 17,“, stems from a standard
approach of using kernel density estimators [see Section 2.5.1 of
(Bishop, 2006)].

Given a test set of k-mers (computed from reads), the distribution of
the test set is modeled as follows:

M
pm = 2pm..) mom),
m=1

 

2424

ﬁm'spzumofpmﬂo'sopnuuopnorq/ﬁdnq

SEK

 

where we denote probability for taxonomic unit m (or class weight) by
p(Cm), satisfying 2;: 1 p(Cm) = 1. {p(C,,,)};,‘f:1 is the composition of taxo-
nomic units. The inference task is to estimate p(Cm) as accurately and
fast as possible, for which a ﬁrst order moment matching approach is
developed. We ﬁrst evaluate the mean of x under p(x) as follows:

[E[x]

=/xp(x)dx e Riff“
[‘4
= Zp(cm)fxp(xlcm)dx
m=1

M N,"
= 2mm] x 2am; pmrxlxmz. omndx
m = 1 =

[—1

Nm

[VI
=  “ml/X 17m[(X|Xm[~ G)n1[)dx
m=1 [=1

[‘4 Nm
= ZP(Cm)Zam[Xm[-
m =1 [=1

. . . =1 .
Introducmg a new 1ndex1ng nén(m, l) = 21:] NJ- +1, we can wr1te
N
[E[x] = Z V" x" = X14

"=1

where
V=1V1V2~wVNIIERIXX1~ (2)
ynéJ/Mmj) =p(Cm)am[v

with the following properties

"(m~Nm) Nm

2 yn=p(cm>Zaml=p(cm>.

n(m,1) [=1

N
Zyn=llylli =1.
"=1

In our approach, we use the sample mean of the test set. The test set
consists of k-mers feature vectors computed from reads. Each read is
processed individually to generate k-mers in the same manner used for
the reference data. We compute sample mean of the k-mer feature vectors
for test dataset reads. Let us denote the sample mean of the test dataset
by p, e Rf“, and assume that the number of reads is reasonably high
such that p, % [E[x]. Then we can write

p.%Xy.

Considering that model irregularities are absorbed in an additive noise
term 11, we use the following system model

M=Xy+nenﬁx1 (3)

Using the sample mean u and knowing X, we estimate y from (3) as
f/éﬁ/l f/Z ...,]>N]’ e [IR/XX] followed by estimation ofp(Cm) as

"(WNW
13(cm)= Z n.
n(m,1)
The estimation f/ e [IR/XX] must satisfy the following constraints

[20.
N M
||y||1=Zyn=ZP(Cm)=1
"=1 m=1

In (4), f/ z 0 means Wt, 12,, z 0. We note that the linear setup (3) is under-
determined as 4k<N (in practice 4]“ <<N) and hence, in general, solving

(4)

(3) without any constraint will lead to inﬁnitely many solutions. The
constraints (4) result in a feasible set of solutions that is convex and
can be used for ﬁnding a unique and meaningful solution.

We recall that the main interest is to estimate p(Cm), which is achieved
in our approach by ﬁrst estimating y and then p(Cm). Hence, y represents
an auxiliary variable in our system.

2.4 Optimization problem and sparsity aspect

The solution of (3), denoted by )9, must satisfy the constraints in (4).
Hence, for SEK, we pose the optimization problem to solve as follows:

Pick] :[=argmin IIM—XJ/llzwyzowllyllﬁl (5)
V

where ‘ + ’ and ‘1’ notations in ng‘l refer to the constraints f/ e [ij and
llf/lll = 1, respectively. The problem ng‘l is a constrained least squares
problem and a quadratic program (QP) solvable by convex optimization
tools, such as cvx (CVX). In our assumption 4k<N, and hence the
required computation complexity is 0(N3) (Boyd and Vandenberghe,
2004).

The form of P 5:13 bears resembance to the widely used LASSO method
from general sparse signal processing, mainly used for solving under-
determined problems in compressive sensing (Candes and Wakin, 2008;
Chatterjee et a[., 2012). LASSO deals with the following optimization
problem [see (1.5) of (Effron et a[., 2004)]:

 : 19121550: arg min “FL _ XVIIZv  S T
V

where r 6 [13+ is a user choice that decides the level of sparsity in f/lasso;
for example, I = 1 will lead to a certain level of sparsity. A decreasing 1'
leads to an increasing level of sparsity in LASSO solution. LASSO is
often presented in an unconstrained Lagrangian form that minimizes
{llu — Xyllg +A||y||1}, where A decides the level of sparsity. ng‘l is not
theoretically bound to provide a sparse solution with a similar level of
sparsity achieved by LASSO when a small r<l is used.

For the community composition estimation problem, the auxiliary
variable y deﬁned in (2) is inherently sparse. Two particularly natural
motivations concerning the sparsity can be brought forward. Firstly, con-
sider the conditional densities for taxonomic units as shown in (1).
Regarding the conditional density model for a single unit, a natural hy-
pothesis for the generating model is that the conditional densities for
several other units will induce only few feature vectors, and hence am,
will be negligible or effectively zero for certain patterns in the feature
space, leading to sparsity in the auxiliary variable y (unstructured sparsity
in y). Secondly, in most samples only a small fraction of the possible
taxonomic units is expected to be present, and consequently, many p(Cm)
will turn out to be zero, which again corresponds to sparsity in y (struc-
tured block-wise sparsity in y) (Stojnic, 2010). In practice, for a highly
under-determined system (3) in the community composition estimation
problem with the fact that y is inherently sparse, the solution of ng‘l
turns out to be effectively sparse because of the constraint |y||1 = 1.

2.5 A greedy estimation algorithm

For SEK we solve ng‘l using convex optimization tools requiring com-
putational complexity 0(N3). To reduce the complexity without a signiﬁ-
cant loss in estimation performance we also develop a new greedy
algorithm based on orthogonal matching pursuit (0MP) (Tropp and
Gilbert, 2007); for a short discussion of OMP with pseudo-code, see
also (Chatterjee et a[., 2012). In the recent literature, several algorithms
have been designed by extending 0MP, such as, for example, the back-
tracking-based OMP (Huang and Makur, 2011), and, by a subset of the
current authors, the look-ahead OMP (Chatterjee et a[., 2011). Because
the standard OMP uses a least-squares approach and does not provide
solutions satisfying constraints in (4), it is necessary to design a new
greedy algorithm for the problem addressed here.

 

2425

ﬁm'spzumofpmJXO'sopnuuopnorq/ﬁdnq

S.Chatterjee et al.

 

The new algorithm introduced here is referred to as OMPstk‘l, and its

pseudo—code is shown in Algorithm 1. In the stopping condition (step 7),
the parameter v is a positive real number that is used as a threshold, and the
parameter I is a positive integer that is used to limit the number of iterations.
The choice of v and I is ad hoc, depending mainly on user experience.

 

Algorithm 1: 0MP;1

Input:

1: X, it, v, I;

Initialization:

1:r0<—p,,80<—@,i<—0;

Iterations:

1: repeat

2: i <— i + 1; (Iteration counter)
r,- <— index of the highest positive element of X’rH;

8i <— 5H U Ti; IV (182" = i)
r.» e argﬁmin HM — Xsrinz. r.» 2 0; (X5. e Rt“)

 

0‘ 

: r,» <— p, — X5471»; (Residual)
7: until ((I Hth — 1| s v) or (i: 1))
Output:

1: 1? 6 R11, satisfying 195; =17,» and 195; =0.

2: f/ <— WVHI (Enforcing llf/lll =1)

 

 

Compared with the standard 0MP, the new aspects in OMPSICk‘1 are as
follows:

o In Step 3 of Iterations, we only search within positive inner product
coefﬁcients.

o In Step 5 of Iterations, a least-squares solution )7,» with non-negativity
constraint is found for ith iteration via the use of intermediate vari-
able ﬁi 6 WI 1. In this step, X5, is the sub-matrix formed by columns
of X indexed in 8,». The concerned optimization problem is convex.
We used the Matlab function lsqnonneg() for this purpose.

In Step 6 of Iterations, we ﬁnd the least squares residual r,-.

In Step 7 of Iterations, the stopping condition provides for a solution
that has an El norm close to one, with an error decided by the
threshold v. An unconditional stopping condition is provided by
the maximum number of iterations I.

o In Step 2 of Output, the El norm of the solution is set to one by a
rescaling.

The computational complexity of the OMPSICk‘1 algorithm is as follows.
The main cost is incurred at Step 5 where we need to solve a linearly
constrained QP using convex optimization tools; here we assume that the
costs of the other steps are negligible. In the ith iteration X5, 6 [RfXi and
i << 4]“, and the complexity required to solve Step 5 is 0(4ki2) (Boyd and
Vandenberghe, 2004). As we have a stopping condition i 5 I, the total
complexity of the OMPSICk‘1 algorithm is within 0(1 X 4" 12) = 0(4kI3). We
know that optimal solution of ng‘l using convex optimization tools re-
quires a complexity of 0(N3). For a setup with I < 4" << N, we can have
O(4"I3) << 0(N3), and hence the OMPSICk‘1 algorithm is typically much
more efﬁcient than using convex optimization tools directly in a high-
dimensional setting. It is clear that the OMPSICk‘1 algorithm is not allowed
to iterate beyond the limit of I; in practice, this works as a forced con-
vergence. For both OMPSICK‘1 and ngl, we do not have a theoretical
proof on robust reconstruction of solutions. Further, a natural question
remains on how to set the input parameters v and I. The choice of par-
ameters is discussed later in Section 3.4.

2.6 Overall system ﬂowchart

Finally, we depict the full SEK system by using a ﬂowchart shown in
Figure 1. The ﬂowchart shows main parts of the overall system and
associated off-line and online computations.

Reads Reference

(test sequences) (training sequences)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

PM or 0MP“

sek sek

 

 

 

 

17

Computing composition
proportion p (C m )

User choice:

 

Window length L“;
Window shift LP
k of k-mers

 

 

 

I I
' l - S :
: k-mers : : k-mers g I
: generation : : generation ‘5 :
I I I Ch '
I l I I E '
I I I o '
I I I 0 I
I I I o :
: Sample meanp : : E I
: g computation : : Xmamx :L‘ :
:‘g I : creation ‘5 :
I S I“ i I '
I Cu I ‘ . . . . . . . . . . . . . . --.'
I E . . . :
. 8 Finding *7 nsmg I X
I

o
I .E I
I 1: :
:o :
I I
I l
I I
I I
I I
I I
I I
I I

 

 

 

Composition proportion

Fig. 1. A ﬂowchart of full SEK system

2.7 Mock communities data

For our experiments on real biological data, we used the mock microbial
communities database developed in (Haas et a[., 2011). The database is
called even composition Mock Communities (eMC) for chimeric
sequence detection where the involved bacterial species are known in
advance. Three regions (V1=V3, V3=V5, V6=V9) of the 16S rRNA gene
of the composition eMC were sequenced using 454 sequencing technology
in four different sequencing centers. In our experiments, we focused on
the V3=V5 region datasets, because these have been earlier used for evalu-
ation of the BeBAC method [see Experiment 2 of (Cheng et a[., 2012)].

2. 7.1 Test dataset (Reads) Our basic test dataset used under a var-
iety of different in silico experimental conditions is the one used in
Experiment 2 of BeBAC (Cheng et a[., 2012). The test dataset consists
of 91 240 short length reads from 21 different species. The length of reads
has a range between 450 and 550 bp, and the bacterial community com-
position is known at the species level, by the following computation per-
formed in (Cheng et a[., 2012). Each individual sequence of the 91240
read sequences was aligned (local alignment) to all the reference se-
quences of reference database Dialog“ described in the Section 2.7.2 and
then each read sequence is labelled by the species of the highest scoring
reference sequence, followed by computation of the community compos-
ition referred to as ground truth.

2. 7.2 Training datasets (Reference) We used two different data-
bases (known and mixed) generated from the mock microbial community
database (Haas et a[., 2011). The ﬁrst database is denoted by Dialog“ and
it consists of the same M = 21 species present among the reads described
in Section 2.7.1. The details of the Dialog“ database can be found in
Experiment 2 of (Cheng et a[., 2012). The database consists of 113 refer-
ence sequences for a total of 21 bacterial species, such that each reference
sequence represents a distinct 16S rRNA gene. Thus, there is a varying
number of reference sequences for each of the considered species. Each
reference sequence has a length of ~1500 bp, and for each species, the
corresponding reference sequences are concatenated to a single sequence.
The ﬁnal reference database Dialog“ then consists of 21 sequences where
each sequence has a length of ~5000 bp.

To evaluate inﬂuence of new species in reference data on the perform-

ance of SEK, we created new databases denoted by DlnoCk (E). Here E

mixed

 

2426

ﬁm'spzumot‘pmJXO'sopnuuopnorq/ﬁdnq

SEK

 

represents the number of additional species included to a partial database
created from Dﬁ‘rfgxn, by downloading additional reference data from the
RDP database. Each partial database includes only one randomly chosen
reference sequence for each species in Dialog“ and hence consists of 21
reference sequences of a length of ~1500 bp. For example, with E = 10,
10 additional species were included in the reference database and conse-
quently DEEQEUO) contains 16S rRNA sequences of M = 21 + 10 = 31

species. Several instances of DiggccﬂE) were made for each ﬁxed value of
E by choosing a varying set of additional species and we also increased E
from 0 to 100 in steps of 10. Note that, in Diggcg‘JE), the inclusion of only
single reference sequence results in reduction of biological variability for

each of the original 21 species compared with Dialog“.

2.8 Simulated data

To evaluate how SEK performs for much larger data than the mock
communities data, we performed experiments for simulated data
described below.

2 .8 .1 Test datasets (Reads) Two sets of simulated data were used to
test the performance of the SEK method. First, the 216 different simu-
lated datasets produced in (Koslicki et al., 2013) were used for a direct
comparison with the Quikr method and the RDP’s NBC. See [(Koslicki
et al., 2013), Section 2.5] for the design of these simulations.

The second set of simulated data consists of 486 different pyrosequen-
cing datasets constituting >179M reads generated using the shotgun/
amplicon read simulator Grinder (Angly et al., 2012). Read-length distri-
butions were set to be one of the following: ﬁxed at 100 bp, normally
distributed at 450 :l: 50 bp, or normally distributed at 800 :l: 100 bp. Read
depth was ﬁxed to be one of 10K, 100K or 1M total reads. Primers were
chosen to target either only the V1=V3 regions, only the V6=V9 regions or
else the multiple variable regions V1=V9. Three different diversity values
were chosen (10, 100 and 500) at the species level, and abundance was
modeled by one of the following three distributions: uniform, linear or
power-law with parameter 0.705. Homopolymer errors were modeled
using Balzer’s model (Balzer et al., 2010), and chimera percentages
were set to either 5 or 35%. As only amplicon sequencing is considered,
copy bias was used, but not length bias.

2.8.2 Training datasets (Reference) To analyze the simulated data,
two different training matrices were used corresponding to the databases
Dsman and Dmge from (Koslicki et al., 2013). The database Dsman is iden-
tical to RDP’s NBC training set 7 and consists of 10 046 sequences cover-
ing 1813 genera. Database D131.ge consists of a 275 727 sequence subset of
RDP’s 16S rRNA database covering 2226 genera. Taxonomic informa-
tion was obtained from NCBI.

3 RESULTS

3.1 Performance measure and competing methods

As a quantitative performance measure, we use variational dis—
tance (VD) to compare between known proportions of taxo—
nomic units p=[p(C1), p(C2), . . . ,p(CK)]T and the estimated
proportions p=[[3(C1), f)(C2),...,[3(CK)]T. The VD is defined
as follows:

VD=0.5 >< ||p—p||1 e[0. 1]

A low VD indicates more satisfactory performance.

We compare performances between SEK, Quikr, Taxy and
RDP’s NBC, for real biological data (mock communities data)
and large—size simulated data.

3.2 Results for mock communities data

Using mock communities data, we carried out experiments where
the community composition problem is addressed at the species
level. Here we investigated how the SEK performs for real bio—
logical data, also vis—a—vis relevant competing methods.

3.2.] k—mers from test dataset In the test dataset, described in
Section 2.7.1, the shortest read is of length 450 bp. We used a
window length LW = 450 bp and refrained from the sliding—the—
window approach in the generation of k—mers feature vectors.
For k = 4 and k = 6, the k—mers generation took 21 and 48 min,
respectively.

3.2.2 Results using small training dataset In this experiment, we
used SEK for estimation of the proportions of species in the test
set described in Section 2.7.1. Here we used the smaller training
reference set kanocfvlin described in Section 2.2. The experimental
setup is the same as shown in Experiment 2 of BeBAC (Cheng
et al., 2012). Therefore, we can directly compare it with the
BeBAC results reported in (Cheng et al., 2012). SEK estimates
were based on 4—mers computed with the setup Lw = 450 bp and
LF = 1bp. The choice of LF = 1bp corresponds to the best case
of generating training matrix X, with the highest amount of vari—
ability in reference k—mers. Using kanocfvlin, the k—mers training
matrix X has the dimension 44 x 121412. For the use of SEK
in such a high dimension, the QP P5451 using cvx suffered of
numerical instability, but OMPSE;l provided results in 3.17s,
leading to a VD = 0.0305. For OMPS:1;1, v and I in algorithm
1 were set to 10’5 and 100, respectively; the values of these two
parameters remained unchanged for other experiments on mock
communities data presented later. The performance of SEK
using OMPSng;l is shown in Figure 2, and compared against the
estimates from BeBAC, Quikr and Taxy. The Quikr method
used 6—mers and provided a VD = 0.4044, whereas the Taxy
method used 7—mers and provided a VD = 0.2817. The use of
k = 6 and k = 7 for Quikr and Taxy, respectively, is chosen ac—
cording to the experiments described in Koslicki et al. (2013) and
Meinicke et al. (2011). Here Quikr is found to provide the least
satisfactory performance in terms of VD. BeBAC results are
highly accurate with VD = 0.0038, but come with the require—
ment of a computation time in the order of >30 h. On the other
hand OMPSE;l had a total online computation time around
21 min that is mainly dominated by k—mers computation from
sample reads for evaluating a; given pre—computed X and u, the
central inferenece (or estimation) task of OMPSE;l took only
3.17s. Considering that Quikr and Taxy also have similar
online complexity requirement to compute k—mers from sample
reads, OMPSE;l can be concluded to provide a good trade—off
between performance and computational demands.

3.2.3 Results for dimension reduction by higher shifts The
L1) = 1bp leads to a relatively high dimension of X, which is
directly related to an increase in computational complexity.
Clearly, the LF = 1bp shift produces highly correlated columns
in X, and consequently it might be sufﬁcient to use k—mers fea—
ture vectors with a higher shift without a considerable loss in
variability information. To investigate this, we performed an ex—
periment with a gradual increase in LP. We found that selecting

Lp = 15bp results in an input X e fogosz, which the cvx—based

 

2427

ﬁm'spzumot‘pmJXO'sopnuuopnorq/ﬁdnq

O1. A.baumann11
02. A.odontolyticus
03. Beereus

O4. B.vulgatus

05. C.beuer1nck11
06. D.rad10durans
07. E.0011

08. E.faeca113

O9. H.py10r1

10. L.gasser1

11. L.monocytogenes

12. M.Sm11h11
.N.memngitidis

14. Faeries

15. P. aerugmosa
. Rsphaeroides

17. Sagalactiae

18. S.aureus

19.8.ep1derm1dis

20. Smutans

21. S.pneumomae

 

/310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

 

 RDP's NBC _ Ouikr(D1dm)

RDP's NBC
Medlan=612 hours

Quikr (1)1.ugu) 

\chizurltv : mimics

/310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

S.Chatterjee et al.

 

 

 

 

 

0.8

0.6
D
> 0.4

0.2

0.0

Phylum Class Order Family Genus
Taxanomic Rank
"' SEK (Dsmall)

Median=3.3 seconds

SEK (Dsmall)
Median=5.3 minutes

0 20 40 60 80 100
Minutes

QuikT (Dsmall) ’

 

 

 

Fig. 5. For simulated data: comparison of SEK (OMng‘l) with Quikr on
the second set of simulated data. (21) VD error averaged over all 486
simulated datasets versus taxonomic rank for SEK and Quikr trained
using DsmalL (b) Algorithm execution time for SEK and Quikr trained
using DsmalL Whiskers denote range of the data, vertical black bars des-
ignate the median and the boxes demarcate quantiles

SEK somewhat experienced decreasing performance as a func—
tion of diversity: at the genus level, SEK gave a mean VD of
0.467, 0.579 and 0.603 for the simulated datasets with diversity
10, 100 and 500, respectively.

3.4 Remarks on parameter choice and errors

In SEK, we need to choose several parameters: k, L..., LP, 1) and 1.
Typically an increase in k leads to better performance with the
fact that a higher k always subsumes a lower k in the process of
generating k—mers feature vectors. The trend of improvement in
performance with increase of k was shown for Quikr Koslicki
et al. (2013) and we believe that the same trend will also hold for
SEK. For SEK, the increase in k results in exponential increase
in row dimension of X matrix and hence the complexity and
memory requirement also increase exponentially. There is no
standard approach to ﬁx k, except a brute force search. Let us
now consider choice of L... and Lp. Our experimental results
bring the following heuristic: choose L... to match the read
length of sample data. On the other hand, choose Lp as small
as possible to accommodate a high variability of k—mers infor—
mation in X matrix. A reduction in Lp results to a linear increase

in column dimension of X. Overall users should choose k, L... and
LF such that the dimension of X remains reasonable without
considerable loss in estimation performance. Finally, we consider
1) and I parameters in Algorithm 1 that enforce sparsity, with the
aspect that computational complexity is 0(4k13). In general,
there is no standard automatic approach to choose these two
parameters, even for any standard algorithm. For example, the
unconstrained Lagrangian form of LASSO mentioned in section
2.4 also needs to set the parameter A by user. For Algorithm 1,
0< v< 1 should be chosen as a small positive number and I can
be chosen as a fraction of row dimension of X that is 4", of
course with the requirement that I is a positive integer. Let us
choose I= Ln x 41"] where 0<n§ 1. In case of a lower k, the
system is more under—determined and naturally the enforcement
of sparsity needs to be slackened to achieve a reasonable estima—
tion performance. Hence for a lower k, we need to choose a
higher 17 that can provide a good trade—off between complexity
and estimation performance. But, for a higher k, the system is
less under—determined, and to keep the complexity reasonable,
we should choose a lower 11. For mock communities date, we
used k = 4 and I = 100, and hence n= 14$ B 0.4, and for simu—
lated data, we used k = 6 and I = 409, and hence n= :0? B 0.1.
Further, it is interesting to ask what are the types of errors
most common in SEK reconstruction. In general, SEK recon—
structs the most abundant taxa with remarkable fidelity. The less
abundant taxa are typically more difﬁcult to reconstruct and at
times each behavior can be observed: low frequency taxa missing,
miss—assigned or their abundances miss—estimated.

 

4 DISCUSSION AND CONCLUSION

In this article, we have shown that bacterial compositions of
metagenomic samples can be determined quickly and accurately
from what initially appears to be incomplete data. Our method
SEK uses only k—mer statistics of fixed length (here k~4,6) of
reads from high—throughput sequencing data from the bacterial
16S rRNA genes to ﬁnd which set of tens of bacteria are present
out of a library of hundreds of species. For a reasonable size of
reference training data, the computational cost is dominated by
the pre—computing of the k—mer statistics in the data and in the
library; the computational cost of the central inference module is
negligible, and can be performed in seconds/minutes on a stand—
ard laptop computer.

Our approach belongs to the general family of sparse sig—
nal processing where data sparsity is exploited to solve under—
determined systems. In metagenomics, sparsity is present on sev—
eral levels. We have used the fact that k—mer statistics computed
in windows of intermediate size vary substantially along the 16S
rRNA sequences. The number of variables representing the
amount of reads assumed to be present in the data from each
genome and from each window is thus far greater than the
number of observations, which are the k—mer statistics of all
the reads in the data taken together. More generally, although
many bacterial communities are rich and diverse, the number of
species present in, for example, the gut of one patient, will almost
always be only a small fraction of the number of species present
at the same position across a population, which in turn will only
be a small fraction of all known bacteria for which the genomic
sequences are available. We therefore believe that sparsity is a

 

2430

ﬁm'spzumot‘pmJXO'sopnuuopnorq/ﬁdnq

SEK

 

rather common feature of metagenomic data analysis that could
have many applications beyond the ones pursued here.

The major technical problem solved in the present article stems
from the fact that the columns of the system matrix X linking
feature vectors are highly correlated. This effect arises both from
the construction of the feature vectors, i.e. that the windows are
overlapping, and from biological similarity of DNA sequences
along the 16S rRNA genes across a set of species. An additional
technical complication is that the variables (species abundances)
are non—negative numbers and naturally normalized to unity,
although in most methods of sparse signal processing there are
no such constraints. We were able to overcome these problems
by constructing a new greedy algorithm based on OMP modiﬁed
to handle the positivity constraint. The new algorithm, dubbed
OMP;1;1, integrates ideas borrowed from kernel density estima—
tors, mixture density models and sparsity—exploiting algebraic
solutions.

During the article preparation, we became aware that a similar
methodology (Quikr) has been developed by Koslicki et al.
(2013). Although there is a considerable similarity between
Quikr and SEK, we note that Quikr is based only on sparsity—
exploiting algebraic solutions, while SEK further exploits the
additional sparsity assumption of non—uniform ampliﬁcations
of variable regions in 16S rRNA sequences. We hypothesize
that the improvement of SEK over Quikr is mainly because of
the superior training method of SEK. The comparison between
the two methods reported above in Figures 2, 4 and 5 shows that
SEK performs generally better than Quikr. The development
of two new methodologies independently and roughly simultan—
eously reﬂects the timeliness and general interest of sparse pro—
cessing techniques for bioinformatics applications.

Funding: Erasmus Mundus scholar program of the European
Union (Y.L.), by the Academy of Finland through its Finland
Distinguished Professor program grant project 129024/Aurell (to
E.A.), ERC grant 239784 (to J .C.) and the Academy of Finland
Center of Excellence COIN (to EA. and J .C.), by the Swedish
Research Council Linnaeus Centre ACCESS (to E.A., M.S.,
L.R., SC. and M.V) and by the Ohio Supercomputer Center
and the Mathematical Biosciences Institute at The Ohio State
University (to D.K.).

Conﬂict of interest: none declared.

REFERENCES

Amir,A. et al. (2013) High—resolution microbial community reconstruction by inte—
grating short reads from multiple 16S rRNA regions. Nucleic Acids Res, 41, e205.

Amir,A. and Zuk,O. (2011) Bacterial community reconstruction using compressed
sensing. J. Comput. Biol, 18, 17234741.

Angly,F.E. et al. (2012) Grinder: a versatile amplicon and shotgun sequence simu—
lator. Nucleic Acids Res, 40, e94.

Balzer,S. et al. (2010) Characteristics of 454 pyrosequencing data=enabling realistic
simulation with ﬂowsim. Bioinformatics, 26, i42(%i425.

Bishop,C.M. (2006) Pattern Recognition and Machine Learning. Springer.

Boyd,S. and Vandenberghe,L. (2004) Convex Optimization. Cambridge University
Press.

Cai,Y. and Sun,Y. (2011) Esprit—tree: hierarchical clustering analysis of millions of
16s rRNA pyrosequences in quasilinear computational time. Nucleic Acids Res,
39, e95.

Candes,E.J. and Wakin,M.B. (2008) An introduction to compressive sampling.
IEEE Signal Proc. Mag., 25, 21=30.

Chatterjee,S. et al. (2011) Look ahead orthogonal matching pursuit. Acoustics,
Speech and Signal Processing (ICASSP), 2011 IEEE International Conference.
pp. 40244027.

Chatterjee,S. et al. (2012) Projection—based and look—ahead strategies for atom
selection. IEEE TransSignal Process, 60, 634=647.

Cheng,L. et al. (2012) Bayesian estimation of bacterial community composition
from 454 sequencing data. Nucleic Acids Res, 40, 52435249.

CVX. A system for disciplined convex programming. http://cvxr.com/cvx/. 2013.

Edgar,R.C. (2010) Search and clustering orders of magnitude faster than blast.
Bioiip’ormatics, 26, 246(F246l.

Effron,B. et al. (2004) Least angle regression. Ann. Statist., 32, 407499.

Haas,B.J. et al. (2011) Chimeric 16s rRNA sequence formation and
detection in sanger and 454—pyrosequenced pcr amplicons. Genome Res, 21,
494=504.

Huang,H. and Makur,A. (2011) Backtracking—based matching pursuit method for
sparse signal reconstruction. IEEE Signal Process. Lett., 18, 391=394.

Huson,D.H. et al. (2007) Megan analysis of metagenomic data. Genome Res, 17,
377=386.

Koslicki,D. et al. (2013) Quikr: a method for rapid reconstruction of bacterial
communities via compressive sensing. Bioinformatics, 29, 2096=2102.

Meinicke,P. et al. (2011) Mixture models for analysis of the taxonomic composition
of metagenomes. Bioinﬁ)rmatics, 27, 16184624.

Mitra,S. et al. (2011) Analysis of 16s rRNA environmental sequences using megan.
BMC Genomics, 12 (Suppl. 3), S17.

Ong,S.H. et al. (2013) Species identiﬁcation and proﬁling of complex microbial
communities using shotgun illumina sequencing of 16s rRNA amplicon se—
quences. PLoS One, 8, e60811.

Stojnic,M. (2010) lg/ll—optimization in block—sparse compressed sensing and its
strong thresholds. IEEE J. Sel. Top. Signal Process, 4, 35(k357.

Tropp,J.A. and Gilbert,A.C. (2007) Signal recovery from random meas—
urements via orthogonal matching pursuit. IEEE Trans. Inf. Theory, 53,
46554666.

von Mering,C. et al. (2007) Quantitative phylogenetic assessment of microbial com—
munities in diverse environments. Science, 315, 112(r1130.

Wang,Q. et al. (2007) Naive bayesian classiﬁer for rapid assignment of rrna
sequences into the new bacterial taxonomy. Appl. Environ. Microbiol, 73,
52615267.

Wommack,K.E. et al. (2008) Metagenomics: read length matters. Appl Environ
Microbiol, 74, l45¥l463.

Zuk,O. et al. (2012) Accurate Profiling of Microbial Communities From Massively
Parallel Sequencing Using Convex Optimization. Vol. LNCS 8214. Chan],
Switzerland, Springer.

 

2431

ﬁm'spzumot‘pmJXO'sopnuuoprrorq/pdnq

