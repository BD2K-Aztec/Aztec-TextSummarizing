ORIGINAL PAPER

Vol. 27 no. 24 2011, pages 3348-3355
doi:10. 1 093/bioinformatics/btr560

 

Sequence analysis

Advance Access publication October 8, 2011

KABOOM! A new suffix array based algorithm for clustering

expression data
Scott Hazelhurstlaii and Zsuzsanna Lipt k2’*

1Wits Bioinformatics, School of Electrical and Information Engineering, University of the Witwatersrand,
Johannesburg, Private Bag 3, 2050 Wits, South Africa and 2Universit degli Studi di Salerno, Dipartimento di

Informatica, Via Ponte don Melillo, f, 84084 Fisciano, Italy
Associate Editor: John Quackenbush

 

ABSTRACT

Motivation: Second-generation sequencing technology has
reinvigorated research using expression data, and clustering such
data remains a significant challenge, with much larger datasets and
with different error profiles. Algorithms that rely on all-versus-all
comparison of sequences are not practical for large datasets.
Results: We introduce a new filter for string similarity which has
the potential to eliminate the need for all-versus-all comparison in
clustering of expression data and other similar tasks. Our filter is
based on multiple long exact matches between the two strings, with
the additional constraint that these matches must be sufficiently far
apart. We give details of its efficient implementation using modified
suffix arrays. We demonstrate its efficiency by presenting our new
expression clustering tool, wcd-express, which uses this heuristic.
We compare it to other current tools and show that it is very
competitive both with respect to quality and run time.

Availability: Source code and binaries available under GPL at
http://code.google.com/p/wcdest. Runs on Linux and MacOS X.
Contact: scott.hazelhurst@wits.ac.za; zsuzsa@cebitec.uni-bielefeld.de
Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on June 15, 2011; revised on August 26, 2011; accepted
on September 20, 2011

1 INTRODUCTION

The Clustering of expressed sequence tags (ESTs) and other
gene expression data continues to be a major Challenge in
bioinformatics. The emergence of new sequencing technologies such
as pyrosequencing, collectively referred to as second-generation
sequencing (Pop and Salzberg, 2008; Robison, 2010), has recently
reinvigorated studies using expression data. Second-generation
sequencing provides the opportunity to study the transcriptomes of
organisms for which good quality genomes are not known. However,
new computational Challenges have emerged, with much larger
datasets, shorter sequence length and new error proﬁles (Schwartz
and Waterman, 2009).

In expression Clustering, we start with a large set of CDNA
sequences, typically 105 or more, which have been derived from
transcriptomic data in a laboratory process (commonly, these
sequences are referred to as ESTs). The goal is to ﬁnd a partitional
Clustering such that sequences derived from the same gene are

 

*To whom correspondence should be addressed.

members of the same Cluster. Expression Clustering can broadly be
divided into two Classes: (i) Clustering for which a reference genome
is known (supervised Clustering) and (ii) Clustering for which a
reference genome is not known (also called ab initio or de novo
Clustering). In this article, we focus on the latter Class.

Typically, single linkage Clustering is used for expression data:
if two sequences are similar, their Clusters are merged. Within this
approach, different similarity measures can be used.

Traditionally, edit distance/alignment has been used to deﬁne
similarity between sequences. However, alignment-free measures
are increasingly being adopted, such as q-gram distance (Ukkonen,
1992) or d2 (Torney et al., 1990). These deﬁne similarity between
sequences with respect to the multiplicity of substrings (subwords)
of a ﬁxed, usually small, length. Because of effects such as
alternative splicing, in expression Clustering typically a local
similarity of a predeﬁned length is sought. For two sequences of
length m to be regarded as similar, it sufﬁces to ﬁnd a pair of similar
windows. Using subword-based measures, it is possible to compute
the maximum similarity between all pairs of windows of a ﬁxed
length in time 0(m2) [similarly, computation of an optimal local
alignment score takes 0(m2) time].

EST Clustering algorithms that use subword-based distance
measures rather than alignment methods have proved successful
(Hazelhurst et al., 2008; Kalyanaraman et (11., 2002; Miller
et (11., 1999). However, with the new and much larger datasets,
computation time is still an issue: given n EST sequences, with
average length m, computing all pairwise similarities requires
@(nzmz) time. For real datasets this is prohibitive, at least without
massive parallelism.

Much work has gone into breaking these complexity limits.
Filtering heuristics have been very successful. They test two strings
in linear time to see whether they are likely to be similar, before a
more expensive comparison is done. In practice, these heuristics
have sped up Clustering by orders of magnitude. However, the
algorithms still remain quadratic in the number of sequences.

In this article, we introduce the KABOOM ﬁlter, which greatly
reduces the number of candidate pairs without compromising on
Clustering quality. This heuristic passes a pair of sequences if they
share a given number of common words (substrings) of a given
length, occurring at least a given distance apart. We also give details
of its efﬁcient implementation, which uses a modiﬁed sufﬁx array.

Contribution: our contribution is 2-fold:

(1) We introduce a new heuristic ﬁlter for sequence similarity.

 

3348 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /3.Io's[BumoIpJOJxosouBurJOJurorq”:duq 11101} papBOIH/lAOG

9103 ‘Og anﬁnv uo ::

KABOOM

 

(2) We demonstrate a method which, at least for real-world
data, implements this heuristic and eliminates the need for
all-versus-all comparison.

The computation of our heuristic runs in time which depends
on the multiplicities of words of the given length within the set to
be clustered. Although in the worst case, the computation of our
heuristic could take quadratic time in n, the number of sequences,
the cost on real-life data is signiﬁcantly subquadratic. On the other
hand, the ﬁltration rate of the KABOOM heuristic is impressive: on
all our experimental data, the number of pairwise comparisons was
reduced to far below 1%.

We present extensive experimentation results which show how
the KABOOM heuristic has sped up our previous EST clustering
tool, WCD (Hazelhurst, 2008; Hazelhurst et al., 2008), without
compromising clustering quality. WCD uses the subword based
dissimilarity measure d2 to Cluster sequences, shown to be
competitive with or even superior to many existing tools, both with
respect to running time and clustering quality (Hazelhurst et al.,
2008), where we reported comparison studies with ESTate (Slater,
2000), xsact (Malde et al., 2003), PaCE (Kalyanaraman et al., 2002)
and the assembler tool CAP3 (Huang and Madan, 1999). Here, we
compare our new tool, called WCD-EXPRESS, to WCD and to two
other EST clustering tools, TGICL (Pertea et al., 2003) and the
recent tool PEACE (Rao et al., 2010), which Claims to have better
performance than WCD. WCD-EXPRESS outperforms them both with
respect to run time and produces at least as good quality. We show
applicability to Sanger style data as well as to 454 and Illumina data
(second-generation sequencing). We believe that the new ﬁlter has
the potential to achieve similar speedups for other tools which rely
on pairwise comparison of a set of sequences.

Related work: EST clustering tools based on common words
include PaCE (Kalyanaraman et al., 2002), xsact (Malde et al.,
2003), QUASAR (Burkhardt et al., 1999), more recently our tool
WCD (Hazelhurst et al., 2008) and PEACE (Rao et al., 2010). Several
of these use sufﬁx trees or sufﬁx arrays for ﬁnding common words.

PaCE and WCD both explore a heuristic of ﬁnding candidate
matches based on a shared common word to avoid all-versus-all
comparison. The basic idea is to choose a length k and to do a
ﬁill comparison only on those pairs of sequences that share at least
one word of length k. These pairs can be found efﬁciently using a
sufﬁx array or sufﬁx tree, which allows to identify all sequences that
contain a given k-word.

The KABOOM heuristic adapts this idea to sequence pairs which
share several exact matches. Our approach is closest to that of
xsact (Malde et al., 2003), which also uses multiple common
words as a criterion for similarity. Unlike in KABOOM, xsact
uses variable size common words, which must occur in the same
order in both strings. However, the biggest difference is in the
implementation. The authors only report results on one fairly small
dataset; in our earlier work, we found the tool was very slow on
large sets, and it did not scale (Hazelhurst et al., 2008). This article
shows how a related approach can be efﬁciently implemented and
integrated in a well-established EST clustering system.

2 PRELIMINARIES

We start with some formal deﬁnitions. A string (or word or sequence)
over a ﬁnite alphabet 2 is a ﬁnite sequence s= s1 ...s,, of characters

from 2={A,C,G,T}. 2* is the set of all strings over 2. For a
string s=s1...s,, e 2*, we denote by |s| its length n. For two strings
s,w62*, where s=s1...sn and w=w1...wk, w is a substring, or
subword of s, denoted wEs, if there exists an index 1 f i 3n such
that si...si+k_1 =w1 ...wk. Such an index i is called an occurrence
of w in s. A string of length k is also referred to as a k-word.

2.1 Clustering based on pairwise string similarity

Given a set S of strings over {A, C ,G,T} (sequences derived from
expression data), we want to ﬁnd a clustering, i.e. a partition, of S
such that two sequences end up in the same Cluster if and only if
they have been derived from the same gene. Since it is difﬁcult to
detect when two sequences have been derived from the same gene,
we formally state our problem as:

Problem statement (Expression Clustering). Given a set S of
sequences over the alphabet 2 = {A,C,G, T}, compute a clustering
C={C1,...,Cr} oszuchthat (i) Ule C,- =S, andforigéj, GHQ:
(11; and (ii) for all s, t ES, if s and t are similar according to the given
similarity or dissimilarity measure, then s and t are in the same
cluster.

This is the deﬁnition for ab initio, partitional, single-linkage
clustering. For deﬁnitions of other types of clustering, see e.g. (Jain
et al., 1999). Condition (i) states that C is a partition of S.
Dissimilarity measures commonly used for string comparison in
EST clustering include the edit distance (Levenshtein, 1966),
q-gram distance (Ukkonen, 1992) and d2 (Torney et al., 1990).
Usually, one decides on a threshold 6 e R+, and then two sequences
s,t are said to be similar if d(s,t)§6, where d is the dissimilarity
measure.

In our tool, we use the dissimilarity measure d2. We denote by
mult(w,s) the number of occurrences of the word w in s.

Deﬁnition 2.1 (Windowed d2). Let s,t62*, Let q (the word
size) and A (the window length) be two positive integers.
Then d2(s,t)=min{Zwezq (mult(w,s’)—mult(w,t’))2 : s’EsJ’E
t,|s’|=|t’|=A}.

For a string x, let multq(x) denote the vector of length |2|q
containing the multiplicities of all q-words in x, for some ﬁxed
enumeration of 2‘1. If |s|=|t|=k, then d2(s,t) is the Euclidean
distance squared (hence the name) of multq(s) and multq(t). In
general, the two sequences s and t need not have the same length.
When at least one of the two strings s,t is shorter than A, then the
parameters 6 and A are scaled accordingly.

The measure d2 has been shown to be well suited for Sanger-style
expression clustering (Miller et al., 1999), using q :6 as word size.
This article and that of (Rao et al., 2010) are the only ones to have
examined the use of d2 with second-generation data.

A closely related dissimilarity measure employed for EST
clustering is the q-gram distance (Burkhardt et al., 1999; Ukkonen,
1992), deﬁned as dq_gr(s, t) = Zwezq |mult(w, s) — mult(w, t)|.

2.2 Filters based on common words

Because edit distance or windowed d2 is quadratic in the length of
the two strings, often ﬁlters are used: a similarity relation that the
pair is ﬁrst Checked for before d(s,t) is computed.

Our previous tool, WCD, uses the H -ﬁlter. It passes two words s, t
if they have a certain number of q—words in common. For reasons

 

3349

112 /3.Io's[BumoIpJOJxosouBurJOJurorq”:duq 11101} papeolumoq

9103 ‘Og anﬁnv uo ::

S.Hazelhurst and Zs.Lipt k

 

of efﬁcient implementation, the word count is asymmetric: if a word
appears x times in s and y times in t, then we add y to the count.

Deﬁnition 2.2 ((H,q)-Similarity). Let H ,6] be positive integers.
For s,te2*, we say that t is (H,q)-similar to s if it has at least
H occurrences of the q-words that are substrings of s [i.e. if
ngss‘w‘zqmult(w,t)3H].

Typically, q B 6, H B 70. This similarity relation is asymmetric, as
the common substrings of s and t are counted with their multiplicity
in t (and not in s). For the right Choice of H, the (H ,q)-ﬁlter is a
true, non-heuristic ﬁlter for d2, as stated in the following lemma.
The proof can be found in the Supplementary Material.

Lemma 1. Let q,6 and A be given. Set H=A—q+1—6/2. If
d§(s,t)§ 6, then s is (H,q)-similar to t, t is (H,q)-similar to s.

This ﬁlter has probably good performance. Its computation is linear
in the length of s and t; however, every pair has to be inspected
separately. Therefore, this ﬁlter has @(nzm) run time.

In an attempt to avoid an all-versus-all comparison, Hazelhurst
et a1. (2008) and Kalyanaraman et al. (2002) use variants of the
k-word similarity ﬁlter:

Deﬁnition 2.3 (k-Word Similarity). Let k be a positive integer. For
s,t e 2*, we say that s and t are k-word-similar if they have at least
one substring of length k in common.

The k-word ﬁlter can be implemented efﬁciently using a sufﬁx
array. Its ﬁindamental problem is the Choice of k. If k is too large,
then many pairs with high similarity will be missed; if k is too
small, then a quadratic number of candidate pairs will be found.
The heuristic is particularly sensitive to the sequence error rate—as
it increases, the largest common word that two sequences must share
in order to approximately match becomes smaller.

In our previous work, we found setting k=27 was reasonably
effective for many datasets. But the approach was fragile: for some
datasets, k=27 was too big (particularly for shorter sequences) or
too small (particularly for sequences with repeats).

We now introduce the KABOOM ﬁlter, which is an extension of
the k-word ﬁlter, and combines its efﬁcient implementation with the
good ﬁltering properties of the H -ﬁlter.

2.3 The KABOOM ﬁlter

The idea behind the (k,a, ﬂ)-multiword ﬁlter (pronounced
KABOOM) is to generalize the k-word approach: rather than
requiring that two sequences share a relatively long common word,
we require that they share several shorter common words. We
claim that this has two advantages (which we substantiate through
experimentation later): using multiple words allows us to test for
longer regions of high similarity rather than short regions of exact
similarity; and this is more likely to be biologically relevant.

While we present the KABOOM ﬁlter in the context of using
the d2 dissimilarity measure and the WCD tool, the ﬁlter could in
principle be used by any clustering algorithm.

The KABOOM ﬁlter deﬁnes a pair of sequences to be similar
if they share a ﬁxed number a of common words (substrings) of a
ﬁxed length k, and in addition, the ﬁrst and the last must occur at
least a ﬁxed distance 13 apart. This last condition is introduced in
order to avoid too many overlapping matches. We shall count the

substrings with multiplicities, i.e. if s has x occurrences of w and t
has y, then we shall add xy to our count. [This count is known as D2
in the literature (see e.g. Rahmann and Rivals, 2000; Reinert et al.,
2009), not to be confused with d2]. Note that k refers to the word
length used in the KABOOM ﬁlter, while q is the length of the word
used in the computation of d2: typically k > q.

Deﬁnition 2.4 (( k, a, ﬂ)-Multiword Similarity). Let k, a, 13 be positive
integers. For s, t e 2*, let

Common(s,t)={(i,j)|si...si+k_1=0...0+k_1}

be the set of pairs of occurrences of common k-words of s and t. We
say that s and t are (k,a,ﬂ)-similar if

(1) |Common(s,t)| 3a;
(2) El (i1,j1),(i2,j2)eCommon(s, t) s.t. i2—i1 313; and
(3) a (i’, .13).(i’2.j’2>ecommon(s. z) stir/1:13.

Experiments showed that for Sanger-style data, conservative
values of these parameters are k = 16, a = 3, ﬂ = 45 (for d2-threshold
of 6:40), while for 454-type data, conservative values are k: 16,
at = 3, ﬂ = 16 (for 6 = 60): even though we do not have a theoretical
guarantee, with these parameters, the ﬁlter produces negligibly few
false negatives, i.e. pairs (s, t) which are not (k,a,ﬂ)-similar but
whose d2 score is below the threshold 6.

Symmetric and asymmetric implementation: if we drop requirement
(3), we get an asymmetric KABOOM-heuristic requiring distance 13
only in one of the two sequences.

3 ALGORITHM

In this section, we detail our algorithm for ﬁnding all pairs of
sequences that are (k,a, ﬂ)-similar.

The sufﬁx array of a string s is an array of length |s| which lists
the indices i according to the lexicographical order of the sufﬁxes
starting at position i (Manber and Myers, 1993). By the properties of
the sufﬁx array, for any non-empty substring w E s, all occurrences
of w are listed contiguously in the sufﬁx array, as all sufﬁxes preﬁxed
by w are contiguous in the lexicographic order.

Let sa be the sufﬁx array of the sequence data. Fix k > 0. A k-block
of sa is a maximal subarray of the sufﬁx array where the ﬁrst k
characters of the corresponding entries in the text are equal. We
deﬁne the modiﬁed sujﬁx array to be the array sa’ such that the
indices within each k-block of the sufﬁx array sa are reordered
in ascending order, and invs to be the inverse mapping of sa’,
i.e. invs[sa’ [i]] =i. The modiﬁed sufﬁx array sa’ has the following
property, whose proof is immediate.

Lemma 2. For each w with |w| =k, and every pair of occurrences
i;:éil ofw in s, i<il <=> invs[i] <invs[i’].

As a small example, consider a set of seven sequences: aaa,
aacggt, gttaaagt, tcggt, gttat, cgg and acggt. Let k=3. The
sequences are concatenated together (the @ symbol represents a
sequence break Character) and the sufﬁx array constructed. Figure 1
gives a detail (lines 15—26) of the sufﬁx array sa (left), the sufﬁx
that starts at the corresponding position in the text (centre) and the
modiﬁed sufﬁx array sa’ . The Supplementary Material contains the
full table.

 

3350

112 /3.Io's[BumoIpJOJXO'sorwurJOJHrorq”:duq mot} papeo1umoq

9103 ‘0g15n8nv uo ::

KABOOM

 

 

 

 

 

 

 

 

 

 

aaa@aacggt@gttaaagt@tcggt@gttat@cgg@acggt@

i sa Text from sa[i] sa’
15 16 agt@tcggt@gttat@cgg@acggt@ 16
16 29 at@cgg@acggt@ 29
17 32 cgg@acggt@ 6
18 37 cggt@ 21
19 6 cggt@gttaaagt@t. . .gt@ 32
20 21 cggt@gttat@cgg@acggt@ 37
21 34 g@acggt@ 34
22 33 gg@acggt@ 33
23 38 ggt@ 7
24 7 ggt@gttaaagt@t. . .t@ 22
25 22 ggt@gttat@cgg@acggt@ 38
26 39 gt@ 8

 

 

 

Fig. 1. Detail of example sufﬁx array and modiﬁed sufﬁx array (k=3).

Each k-block is ruled-off: e.g. rows 17—20 and 23—25 are k-
blocks. For Clarity, we show all 3-words of the text; however, in
the implementation, only those are considered which are substrings
of one of the sequences si, i.e. those that do not contain @.

The k-block in our example from positions 17 to 20 inclusive
corresponds to all occurrences of the k-word cgg. The array sa
stores the elements of this k-block, 32, 37, 6 and 21, according to the
lexicographic order of the corresponding sufﬁxes, while the column
sa’ shows the entries within the block sorted.

We create the modiﬁed sufﬁx array from the sufﬁx array by
scanning through it and re-ordering it, but it could probably be done
more efﬁciently by modifying a sufﬁx array generation algorithm.

3.1 The algorithm

An outline of the modiﬁed clustering algorithm is shown below.
Detailed pseudo-code can be found in Algorithm 1. The main loop
of the algorithm considers each sequence sl- in turn. While processing
sequence si, we record each sequence sj containing common k-
words with si, where j > i. For each such sequence sj, we compute
count[j] := |Common(sl-,sj)|, as well as the leftmost and rightmost
occurrences of such words in sl- and in sj. The variables lindIU]
and rindIU] (lindJ[j] and rindJUD store the current leftmost
and rightmost positions in sl- (sj) of all common k-word of sl- and sj.

for all sequences sl- do

M <— ((1
for all words w E sequence sl- do

M <—MU{j:j> i,w Esequence sj}
for allj EM where count(i,j) 3a /\ rindIU] — lindIU] 3
ﬂ /\ rindJU] — lindJU] 3 13 do

if sim(sl-,sj) then

merge(cluster(i), cluster0))

In more detail: when processing sequence si, we consider each
(overlapping) k-word in si. Let w be a k-word in sequence sl- that
starts at position r. Using the inverse of the modiﬁed sufﬁx array invs,
we ﬁnd where this word occurs in sa’, say at position p=invs[r].
Then we ﬁnd all occurrences of w in other sequences sj, where
j > i, by looking at each entry 11’ >1) within the same k-block. By
Lemma 2, this gives all occurrences to the right of the current

occurrence of w. With this information, we update our records of
which sequences sj share a k-word with si, and also where, in
both sl- and sj, the leftmost and the rightmost of these common
k-words are.

Once all k-words in sequence sl- have been processed, we know
which sequences sj, for j > i, share a k-word with si, how many k-
words are shared and the various leftmost and rightmost positions.
Sequence sl- is a candidate sequence with each of those sequences
satisfying the condition of the KABOOM ﬁlter. A small example is
given in the Supplementary Material.

WCD-EXPRESS then compares each candidate pair ﬁrst with the
heuristics described by Hazelhurst (2008) and then, if necessary,
using d2. However, any dissimilarity ﬁinction (such as edit distance)
could be used. Note that every pair (si,sj), i< j, is considered only
once, namely in the iteration for si.

 

Algorithm 1 Algorithm WCD-EXPRESS (with KABOOM)
fori<—0ton—1 d0

r <— 0; M <— ((1;

While r3 length(sl-)—k d0
1) <— invs[r] + 1; {1): index in sa’}
While (seqnumber[sa ’ [12]] = i /\ not(newblock[p]))
do

p++; {skip ﬁirther matches within si}
while not(newblock[p]) d0
j <— seqnumber[sa ’ [[2]];
ifjgéM then
lindIU] <— r; rindIU] <— r;
between sl- and sj seen}
M <—MU {1'};
else
rindIU] <— r;
lindJU] <— min(sa ’ [p],lir1dJ[j]);
rindJU] <— max(sa ’ [p],rir1dJ[j]);
countU]++; {countz # common words of sl- and sj}
p++;
r++;

{compute d2(si, sj) for j passing ﬁltering phase}

for alljeM do
if (i,j) are (k,a,ﬂ) similar then

if d2(si,sj)§6 {distance of sl- and sj is small} then
merge(cluster(i), cluster0));

reset lindI,rindI, lindJ,rindJ,cour1t,M;

 

{r: index in string si}

{jz current sequence}

{ﬁrst common word

 

3.2 Analysis of algorithm

Suppose there are n sequences of average length m in set S. For
Sanger-style sequencing m may be between 300 and 700. For real
datasets n is unlikely to be <104 and may be as large as 106 (so
m < n, and in large datasets m2 < n). Let k be the word length used
in the sufﬁx array algorithm step. Typical values of k will be in the
range 12—20. We make the following deﬁnitions:

c : number of candidate pairs found by WCD.

cK: no. of candidate pairs found by the KABOOM ﬁlter.

cX: number of candidate pairs found by WCD-EXPRESS,
(using the KABOOM ﬁlter and all other heuristics).

p : number of distinct k-words occurring in S.

11w: number of occurrences of the k-word w in S.

 

3351

112 /3.Io's[BumoIpJOJXO'sorwurJOJHrorq”:duq mot} papeo1umoq

9103 ‘0g15n8nv uo ::

S.Hazelhurst and Zs.Lipt k

 

y: %, ratio of number distinct words to total number of
k-words. (1 /y: average no. occurrences of k-words.)
E: [17 Z 11%,, ave. of squares of no. of occurrences.
lWl=k

The previous version of WCD needs @(mnz) time for the heuristics
and @(cmz) time for computing the d2 or edit distance of those
pairs of sequences that have been passed by the heuristics. In WCD-
EXPRESS, in the KABOOM-step, for each word w in the ﬁle, we do
0(nw) work. Each distinct word w appears 17w times, so the total
work done is 0(Z‘w‘zk nw2)=0(yEmn). Another way of looking
at this is that there are mn (non-distinct) words, and for each we do
yE work on average.

WCD-EXPRESS then applies all of WCD’s heuristics on the pairs
passed, which run in linear time. Thus, the total amount of work done
by WCD-EXPRESS is @(yEmn+cKm+cXm2). Since WCD-EXPRESS
applies all of WCD’s heuristics, cX 3c. In practice, c60(n). Thus,
provided yE <n, WCD-EXPRESS will run faster than WCD. However,
since the constant factors due to memory behaviour are likely to
be substantially larger, for practical purposes, it is important that
yE << n. The experimental results reported later demonstrate that in
practice WCD-EXPRESS runs substantially faster than WCD and that
indeed yE << n.

For the generation of the sufﬁx array, any of a large number
of sufﬁx array construction algorithms can be used, several of
which run in linear time. In our current implementation, we use
the mkESA tool (Homann et al., 2009), which employs the Deep-
Shallow algorithm (Manzini and Ferragina, 2004), one of several
super-linear algorithms that have been shown to perform better
in practice. See (Puglisi et al., 2007) for a survey of sufﬁx array
construction algorithms.

WCD-EXPRESS requires substantial amount of RAM to store both
the sufﬁx array and its inverse: @(mnlogmn) amount of RAM.

4 IMPLEMENTATION

The algorithm is implemented as an extension to the WCD clustering
tool (Hazelhurst et al., 2008). The code is implemented in C.

Pre-processing: pre-processing requires building a sufﬁx array of
the data ﬁle and its reverse complement. We use the mkESA tool
(Homann et al., 2009) to create the sufﬁx array, which suited our
needs well and performed excellently.

Clustering: the WCD-EXPRESS program performs clustering as
presented in Algorithm 1. However, it is important to note that the
call to the d2 algorithm is preceded by the use of ﬁltering heuristics
described in Hazelhurst (2008); Hazelhurst et al. (2008).

5 RESULTS

This section compares WCD-EXPRESS to the previous version of
WCD, and to two other systems, TGICL (Pertea et al., 2003) and
PEACE (Rao et al., 2010), evaluating the impact of the KABOOM
heuristic and the overall performance of WCD-EXPRESS.

All experiments were done on an Intel E5335 (2 GHz; dual quad-
core processor with 4MB of L2 cache per processor and 16GB
of RAM; single thread; Scientiﬁc Linux 5.4, gcc 4.1.2, 0-2 for
WCD-EXPRESS and WCD). We used the asymmetric implementation
of WCD-EXPRESS, which initial experimentation showed was slightly
faster (but which is controlled by a compiler-switch).

Table 1. Comparison of new and old versions of WCD

 

 

Dataset # seqs Size yE WCD-EXP WCD Speed-up
(k) MB (S) (S)
A076941 77 32 17 100 578 5.7
A208 484 208 102 1240 23 983 19.3
C10 126 56 355 511 4512 8.8
chlamy 190 100 139 1000 5989 6.0
Drosophila 25 86 68 184 1542 8.4
ecHuman 17 11 171 135 496 3.7
pubcot 30 17 34 65 222 3 .4
ricinus 58 40 162 840 1518 1.8
xen 233 137 63 855 9298 10.8

 

Experimental results on different sets of EST datasets. # seqs is the number of sequences
in thousands; size is the number of megabases. E is the average square of the frequency
of the distinct words; y is the ratio of the number of distinct words to the total number
of words. WCD-EXPRESS is the time our new algorithm takes including pre-processing;
WCD is the time our previous version of WCD takes. All times in seconds are rounded
to the nearest second. For both versions of WCD, the same parameters were used. The
sensitivity of WCD-ExrkEss with respect to WCD is over 0.999 in all cases.

5.1 Data

The experiments use both real and synthetic Sanger-style and
second-generation data. The data are described in the Supplementary
Material. In summary, A686904 is a set of 686904 Arabidopsis
thaliana sequences from GenBank, while other datasets of the form
Ax are subsets of this; ecHuman is the EasyCluster reference set
(Picardi et al., 2009); chlarny a set of Chlamydomonas reinhardtii
sequences used to test PEACE (Rao et al., 2010). The C-series is a set
of synthetic data ﬁles, generated using the ESTsim tool (Hazelhurst
and Bergheim, 2003); the metasim ﬁles are synthetic ﬁles simulating
454 and Illumina style sequences using the tool MetaSim (Richter
et al., 2008).

5.2 Impact of the KABOOM ﬁlter

Table 1 shows the improvement in running time from WCD to WCD-
EXPRESS, gained by adding the KABOOM ﬁlter. We use a number of
Sanger-style datasets using the same parameters for both tools. The
running times for WCD-EXPRESS include the generation of the sufﬁx
array. In all cases, there is a large speed-up, and the speed-up is
larger with larger datasets. Recall from Section 3.2 that the success
of the algorithm depends on how yE compares to the number of
sequences. As can be seen from Table 1, on all our datasets yE is
two to three orders of magnitude smaller than n.

Next, we explore the impact of the KABOOM ﬁlter with different
word sizes on the quality of the clustering and running times.
Figure 2 shows the time taken and quality of the clustering of
different datasets as the sufﬁx word length k changes, on different
data ﬁles (both real and synthetic). H is the parameter of the
(H, q)-ﬁlter (Section 2.2). For quality, we measure sensitivity of the
clustering with respect to a base case of sufﬁx array word length of
10 (a very conservative value). The other clustering parameters are
chosen extremely conservatively to avoid masking too aggressive
values of other parameters. These results show that choosing a sufﬁx
word length of 16 does not signiﬁcantly decrease sensitivity but that
for word length at least 12 the running time improves dramatically.

 

3352

112 /3.Io's[BumoIpJOJXO'sorwurJOJHrorq”:duq uror} papeo1umoq

9103 ‘0g isnﬁnv uo ::

KABOOM

 

Time °/
10 (°)

Vva-é vv“;

|0.8

0.6

 

 

 

Fig. 2. The effect of word size on the quality and time. The graph on the
left shows a set of mouse (Sanger) data and the graph on the right shows the
MetaSim 454 data. The left y-axis shows time (as a percentage of the time
taken when k = 10), and the right y-axis shows sensitivity.

5.3 Quality comparison with other tools

Given a clustering C of a set S, quality is measured using validity
indices, which compare C to a known clustering D. These give a
numerical value in the range [0, l] (1 being best value), in terms of
the number of pairs (x, y) which are put into the same cluster by both
C and D [true positives (TP)]; those which C clusters together but D
does not [false positives (FP)]; those which neither clusters together
[true negatives (TN)]; and those which D clusters together but C
does not [false negatives (FN)]. Common indices include sensitivity
SE = TP/(TP + FN), positive predictive value PPV = TP/(TP + FP)
and the Jaccard index II = TP/(TP + FN + FP).

Table 2 compares the running times and quality of WCD-EXPRESS
and PEACE on several datasets with known clusterings (the ﬁrst
three are Sanger-style data, the others second-generation). WCD-
EXPRESS and PEACE have very similar quality scores, and WCD-
EXPRESS consistently outperforms PEACE with respect to run-time.

A complication in comparison is that PEACE ﬁlters out low-
complexity sequences in a pre-processing step, while the WCD
tools do not. Therefore, for Table 2 we adopt a pre-step before
calling WCD-EXPRESS of ﬁltering out the same sequences that PEACE
does. Note that our results differ from those quoted in Rao et al.
(2010) because we adopt a different methodology for dealing with
the ﬁltered sequences. See the Supplementary Material for a full
discussion. Since TGICL incorporates ﬁltering into its clustering
step in a different way, a direct comparison with WCD-EXPRESS was
not possible. However, we note that (Rao et al., 2010) report a
comparison of TGICL with PEACE and with WCD and found the
clustering quality to be competitive.

The clusterings computed by WCD-EXPRESS and WCD are
essentially the same. Note that the scores for 454 and Illumina data in
Table 2 differ from those reported in (Rao et al., 2010). The reason is
that the default parameters of WCD were optimised for Sanger-style
data, while for short read sequences, other values are appropriate.
(For details see Supplementary Material.)

As shown in our earlier work, the choice of parameters has a very
important effect on the quality of the results (Zimmermann et al.,
2004). In most published work, tools are compared based on one set
of parameters. This kind of study is limited, since it only proves that
with one set of parameters one tool performs better or worse than
another tool with its own set of parameters.

Table 2. Quality and runtime comparison of WCD-EXPRESS (column labelled
WCD) and PEACE on datasets where an ideal clustering is known

 

 

 

Sensitivity Jaccard Index Time (s)

WCD PEACE WCD PEACE WCD PEACE
A076941 0.932 0.930 0.473 0.477 100 951
chlamy 0.949 0.949 0.513 0.513 907 8823
ecHuman 0.996 0.998 0.707 0.630 50 147
metasim454 0.793 0.714 0.765 0.689 16 66
metasimIllum 0.444 0.368 0.398 0.364 19 1975

 

The ﬁrst three datasets are Sanger style data (average length 500); the second two
are 454 (average length 240) and Illumina data (average length 60). See text and
Supplementary Material for details.

We stress that it is unlikely that any tool has a set of universally
optimal parameters. The range of different sequencing technologies
and quality of data means that different modelling parameters
will be required to cluster optimally. This makes computational
performance more important. A fast tool is very helpful since it
gives an experimenter the ability to cluster the same dataset with
different parameters and investigate the stability of the clustering
with respect to the various parameters.

We also ran tests that showed that for reasonable values of k, very
high levels of sensitivity can be obtained. For Sanger-style data with
k = 16, even for an aggressive 6:60, WCD-EXPRESS gives less than
a 0.1% FN rate. For 454 data, with k = 14, and aggressive clustering
values we get less than a 0.5% FN rate. As expected, performance
declines with shorter sequence length; however, second-generation
sequence length is increasing as the technology improves.

5.4 Computational cost comparisons with other tools

Memory usage: the use of the sufﬁx array and inverse creates
signiﬁcant memory requirements for WCD-EXPRESS. The current
implementation requires ~25 bytes of RAM per byte of input so
that a 200 MB input ﬁle requires ~5 GB of RAM to run effectively.
This makes WCD-EXPRESS much more expensive than WCD (which
required ~0.3 bytes of RAM per byte of input).

TGICL’s memory footprint is very small as the input ﬁle is broken
into chunks. PEACE’s memory usage pattern varies. PEACE’s
usage is 2—10 times less than WCD-EXPRESS’s, with the difference
diminishing on larger data (2.1 GB for the 200 MB input ﬁle
mentioned above). A positive feature of WCD-EXPRESS’s memory
usage is that memory is allocated as the data are read in—thus even
on a long run, usage will be known after a minute or so. PEACE’s
memory usage, on the other hand, may increase throughout its
execution.

The Supplementary Material presents experimental evidence and
discusses memory use in more detail.

Running times: the tables and ﬁgure below show computational
costs of WCD-EXPRESS, PEACE and TGICL on a variety of data. As
discussed, TGICL and PEACE do ﬁltering while WCD-EXPRESS does
not. The Supplementary Material shows that this has a profound
effect on quality and performance. If tool A ﬁlters and B does not, it
may appear that A is faster than B, or vice-versa when the root cause
is that the data to be clustered are effectively very different. In the
experiments below, we adopt a ﬁltering pre-step for WCD-EXPRESS.

 

3353

112 /3.Io's[BumoIpJOJXO'sorwurJOJHrorq”:duq uror} papeo1umoq

9103 ‘0g isnﬁnv uo ::

S.Hazelhurst and Zs.Lipt k

 

 

 

     

 

 

 

 

(a) 40000
wexp +
wcd5 
30000 PEACE 
E
20000 - Hg
'—
10000 -
0-1—w-l" - 4
25 50 75 100 125 150 175 200
Data size (MB)
(b) 8000
wexp + I
TGICL  x 
6000 - "
4000- g '1‘...
1— 
2000 -
0

 

 

 

 

25 50 75 100 125 150 175 200
Data size (M B)

Fig. 3. Performance on the Arabidopsis dataset series on an Intel E5335
Xeon processor. We compare WCD-EXPRESS with PEACE and TGICL on
separate graphs to make the differences clearer and because different ﬁltering
is done by TGICL and PEACE. Note the scales on the y-axes differs.
The slight jink in TGICL’s performance curve was re-tested several times.
(a) PEACE versus WCD and WCD-EXPRESS. (b) TGICL versus WCD-EXPRESS.

 

12500
10000
EE
7500 w
E
5000 I—
2500

      

 

 

0 25 50 75 100 125 150 175
Data size (MB)

Fig. 4. Comparison of performance WCD-EXPRESS versus TGICL on subsets
of a large set of Human 454 ESTs.

To be fair, we do this differently in the comparisons with TGICL and
PEACE, as these tools ﬁlter differently (see Supplementary Material
for details). We emphasize that the times reported for WCD-EXPRESS
take into account all pre-processing time, including ﬁltering and
construction of the sufﬁx array.

In Figure 3, we show how the running times scale as datasets grow
larger. The Arabidopsis data is a set of real ESTs. Figure 4 shows the
difference between WCD-EXPRESS and TGICL on subsets of Human
454 ESTs of different sizes. See the Supplementary Material for
additional experimentation.

Table 3. Running times on different datasets (top part Sanger-style, bottom
second-generation)

 

Dataset No. of Size Time (s)
sequences (K) (Mb)

WCD PEACE WCD TGICL

 

 

A076941 77 32 100 951 166 282
A208 484 208 1078 36611 1620 7111
C10 126 56 135 1800 278 448
chlamy 190 100 907 8823 900 4577
drOSte 83 45 141 1130 247 431
ecHuman 17 11 50 147 115 233
pubcot 30 17 43 124 85 150
ricinus 58 40 783 923 555 1085
xen 233 137 496 7032 1065 2218
metasim454 25 6 16 66 25 46
hsubl28 355 130 1023 25706 964 11788
metaIllum 150 9 19 1975 25 121
soybean 882 173 1626 167590 4012 39556
SRR019551 335 87 876 21595 3204 63524

 

All times rounded to the nearest second. See text for details.

Table 3 shows performance of WCD-EXPRESS on a range
of different datasets. In summary, the results show that WCD-
EXPRESS outperforms PEACE, and for very large datasets does
so substantially. WCD-EXPRESS is also much faster than TGICL.
Additional experimentation and more detailed results (including
pre-processing costs) are presented in the Supplementary Material.

6 CONCLUSION AND FUTURE RESEARCH

This article has introduced a new algorithm for ﬁnding candidate
pairs for clustering gene expression data. The idea is that two
sequences are candidates for comparison if they share a many
common k-words, where the leftmost and rightmost words start at
least 13 away from each other. This heuristic can be implemented
very efﬁciently using a modiﬁed sufﬁx array and its inverse.

The experimental results show that the algorithm is very effective.
For reasonable values of word length, substantial improvement in
compute performance is achieved without degradation in quality of
clustering. There are a number of areas for future work:

Improvement of the algorithm implementation: there is scope for
improving the current implementation, using less memory and
improving cache behaviour. We currently have an experimental
version of our tool that uses about half the amount of memory, with
an approximate 15% run-time penalty.

Parallelization: at this point, WCD-EXPRESS is not parallelized (WCD
has both pthreads and MP1 parallelization). Parallelization should be
straightforward, though given the overall cost of pre-processing, for
large-scale parallelization, sufﬁx array construction and sequence
ﬁltering must both be parallelized. A number of implementations of
parallel sufﬁx array construction are available, including in mkESA,
which can be used directly.

Improving clustering quality: the experiments show that the quality
of clustering is very dependent on parameters used. The most
important lesson is that in practice, bioinformatists should run their

 

3354

112 /3.Io's[BumoIpJOJXO'sorwurJOJHrorq”:duq uror} papeo1umoq

9103 ‘0g isnﬁnv uo ::

KABOOM

 

tools several times with different parameters to evaluate the stability
of their results. Work is needed to separate out the effect of the
parameters used and the underlying algorithms. This is also likely
to be affected by the error models of the underlying sequencing
technology (e.g. the homopolymer problem for 454 data).

ACKNOWLEDGEMENTS

We thank F. Cicalese for his help and support, the referees for helpful
comments, and Dr D]. Rao for help with PEACE.

Funding: Anderson-Capelli Fund; South African National Research
Foundation (grant number IFR2010052500011); SA Centre for
High Performance Computing (to S.H.); European Union’s Seventh
Framework Programme (FP7/2007-2013); Marie Curie Intra-
European Fellowship for Career Development (PIEF-GA-2010-
274778 to Zs.L.).

Conﬂict of Interest: none declared.

REFERENCES

Burkhardt,S. et al. (1999) q-gram based database searching using a sufﬁx array
(QUASAR). In Proceedings of the Third Annual International Conference on
Research in Computational Molecular Biology (RECOMB), ACM, New York,
pp. 77783.

Hazelhurst,S. (2008) Algorithms for clustering EST sequences: the wcd tool. South
African Comput. J., 24, 154271546.

Hazelhurst,S. and Bergheim,A. (2003) ESTsim: a tool for creating benchmarks for EST
clustering algorithms. Technical Report TR-Wits-CS—2003-I. School of Computer
Science, University of the Witwatersrand, Johannesburg, S.A.

Hazelhurst,S. et al. (2008) An overview of the wcd EST clustering tool. Bioinformatics,
24, 154271546.

Homann,R. et al. (2009) mkESA: enhanced sufﬁx array construction tool.
Bioinformatics, 25, 108471085.

Huang,X. and Madan,A. (1999) CAP3: a DNA sequence assembly program. Genome
Res, 9, 8687877.

Jain,A.K. et al. (1999) Data clustering: a review. ACM Comput. Surv., 31, 26L323.

Kalyanaraman,A. et al. (2002) Parallel EST clustering. In Proceedings of IEEE
Conference High Performance Computational Biology. IEEE Computer Society.

Levenshtein,V. (1966) Binary codes capable of correcting deletions, insertions, and
reversals. Soviet Phys. Doklady, 10, 7077710.

Malde,K. et al. (2003) Fast sequence clustering using a sufﬁx array algorithm.
Bioinformatics, 19, 122171226.

Manber,U. and Myers,E.W. (1993) Sufﬁx arrays: a new method for on-line string
searches. SIAM J. Comput., 22, 9357948.

Manzini,G. and Ferragina,P. (2004) Engineering a lightweight sufﬁx array construction
algorithm. Algorithmica, 40, 33750.

Miller,R. etal. (1999) Acomprehensive approach to clustering of expressed human gene
sequence: the sequence tag alignment and consensus knowledge base. Genome Res.,
9, 114371155.

Pertea,G. et al. (2003) TIGR gene indices clustering tools (TGICL): a software system
for fast clustering of large EST datasets. Bioinformatics, 19, 6517652.

Picardi,E. et al. (2009) EasyCluster: a fast and efﬁcient gene-oriented clustering tool
for large-scale transcriptome data. BMC Bioinformatics, 10 (Suppl. 6), $10.

Pop,M. and Salzberg,S. (2008) Bioinforrnatics challenges of new sequencing
technology. Trends Genetics, 24, 1427149.

Puglisi,S.J. et al. (2007) A taxonomy of sufﬁx array construction algorithms. ACM
Comput. Surv., 39, 1731.

Rahmann,S. and Rivals,E. (2000) Exact and efﬁcient computation of the expected
number of missing and common words in random texts. In Proceedings of the
I I th Annual Symposium Combinatorial Pattern Matching (CPM 2000), Vol. 1848
of Lecture Notes in Computer Science, Springer, pp. 375387.

Rao,D. et al. (2010) PEACE: parallel environment for assembly and clustering of gene
expression. Nucleic Acids Res., 38, W7377W742.

Reinert,G et al. (2009) Alignment-free sequence comparison (I): Statistics and power.
J. Comput. Biol., 16, 161571634.

Richter,D. et al. (2008) MetaSim 7 a sequencing simulator for genomics and
metagenomics. PLoS One, 3, e3373.

Robison,K. (2010) Editorial: next generation sequencing. Brief Bioinformatics, 11,
455456.

Schwartz,D.C. and Waterman,M.S. (2009) New generations: Sequencing machines and
their computational challenges. J. Comput. Sci. Technol., 25, 379.

Slater,G. (2000) Algorithms for the Analysis of Expressed Sequence Tags. PhD Thesis,
University of Cambridge, Cambridge, UK.

Torney,D. et al. (1990) Computation of d2: a measure of sequence dissimilarity. In
Bell,G and Marr,T. (eds) Computers and DNA. Addison-Wesley, Boston, Mass, pp.
1097125.

Ukkonen,E. (1992) Approximate string-matching with q-grams and maximal matches.
Theor. Comput. Sci., 92, 1917211.

Zimmermann,J. et al. (2004) A method for evaluating the quality of string dissimilarity
measures and clustering algorithms for EST clustering. In Proceedings of the 4th
IEEE International Symposium BioInformatics and BioEngineering (BIBE 2004 ).
IEEE Computer Society, pp. 3017309.

 

3355

112 /3.Io's[BumoIpJOJXO'sorwurJOJHrorqp:duq uror} papeo1umoq

9103 ‘0g isnﬁnv uo ::

