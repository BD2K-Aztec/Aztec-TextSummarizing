BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

E.Behnam and A.D.Smith

 

2 METHODS

We ﬁrst describe a geometric representation we use for raw metagenomic
sequencing data. Then we introduce the two indexing strategies: a rando-
mized hashing strategy based on locality-sensitive hashing [LSH; Gionis
et a]. (1999)] and the regular nearest neighbor graph. Finally, we show
how these two indexing strategies are integrated.

2.1 Geometric interpretation of metagenomes

A metagenome is the set of genomes for a population of organisms
existing in some deﬁned space at a deﬁned time. This is commonly a
complex mixture of microorganisms interacting to form a microbial com-
munity with behaviors related to the relative frequencies of different mi-
crobes in the community. Our window into this population is by sampling
parts of these genomes via sequencing. Following a WMS experiment, the
totality of information resides in the unprocessed sequenced reads from a
given sample (e.g. in FASTQ ﬁles). Our goal is to extract information
from these data, but to avoid any kind of sequence alignment, assembly
(Luo et al., 2012) or assignment of OTUs (Porter and Beiko, 2013). We
instead use representations from alignment-free sequence comparison
(Vinga and Almeida, 2003). Speciﬁcally, the set of sequenced reads is
transformed into a k-mer count vector. This is a d=4"-dimensional
vector, where coordinate 1' gives the frequency of occurrence in the meta-
genome for the ith k-mer.

The assumption underlying the use of alignment-free methods in meta-
genomics is that each microorganism contributes a distinct ﬁngerprint
when represented by a k-mer count vector, while similar microbial se-
quences should give vectors that are closer in this d—dimensional space.
This also depends on the measure used for comparing these k-mer count
vectors.

Assume metagenome M, represented as a set of reads, contains N
possible k-mers, which includes all sliding windows of length k in all
sequenced reads. We centralize the count of each k-mer to reduce the
effect of background noise, as suggested in Kantorovitz et a]. (2007).
If each read were a random sequence, letting [7,, denote the probabil-
ity of drawing letter a e A when generating M, then for any
2:2122...zk 6 AA,

k
1731—1172),
j=1

so Np: is approximately the expected number of occurrences of z in M.
The centralized k-mer count vector is as follows:

M(Z)=Xz - N172,

where X: is the total number of occurrences of z in M. This simple
representation accomplishes the transformation from the set of reads
into a vector in d= 4" dimensional space. We treat these vectors as
points and hereafter, when we refer to a metagenome, we assume it is
represented as a point in this space.

In addition to expressing biologically meaningful relationships, appro-
priate distance measures provide a statistically meaningful foundation for
alignment-free sequence comparison. Metric distances are especially ap-
pealing, as they allow algorithms to take advantage of the triangle in-
equality and infer some distance relationships implicitly. Here, we use the
angle between two metagenomes as our distance measure. Dot products
are computed as usual, the norm of point X is ||X|| = m, and the angle
between points X and y is 6,}, =arccos X - y/(||X||||y||).

2.2 Indexing metagenomic points with LSH for angles

LSH is among the best known indexing methods for high-dimensional
data (Gionis et al., 1999; Lv et al., 2007), and provides a framework for
understanding and analyzing a large class of randomized dimension-
reduction techniques. Intuitively, a family of hash functions is ‘locality

sensitive’ if any of its members hashes closer points to the same bucket
with higher probability. We use an LSH family for which the measure of
proximity is the angle between points. Suppose u 6 [Rd is a random unit
vector. For any X 6 [Rd deﬁne

l ifX-uZO.

hu(X) = (1)

0 otherwise.

Then, for any two points X and y, it is straightforward to show that

Pr (hu(X) : hu(y)) : 1 _ gay/717 
where 0 5 6X}, 5 71. Let U={u1, . . . , ur} be a set of random vectors with
norm one and deﬁne the hash function as follows:

HU(X) = (hul (X), . . . , hur(X)). (3)

Equation (2) suggests that under the r-bit binary embedding induced
by H U, the ‘locality’ of points is likely to be preserved. In other words, the
closer the two points in R“, the more likely are they to share the same
mapping in the Hamming space, that is, the codomain of H U.

This function, which is locality sensitive when distances are measured
as angles for pairs of points, was ﬁrst introduced in a theoretical work by
Goemans and Williamson (1995) before its ﬁrst use in the context of LSH
by Charikar (2002). Subsequently, this function has been applied in dif-
ferent contexts including natural language processing (Ravichandran
et al., 2005) and computational biology (Behnam et al., 2013).

To achieve our desired accuracy during the query process, we index
points in multiple iterations, with each iteration generating a new hash
table. Therefore, the ﬁrst step in our indexing procedure can be summar-
ized as follows:

0 Generate U as a set of r random unit-length vectors in ER“ [using
an established method (Muller, 1959)].

o For each metagenomic point X, ﬁnd H U,(X) and store the result in
a hash table T,-.

0 Repeat the above two steps L times, producing the set
T: {T1, . . . , TL} of hash tables.

Although LSH provides a simple indexing scheme and guarantees sub-
linear query complexity under certain conditions (Gionis et al., 1999), its
use in large databases has been hindered because a large number of hash
tables can be required to overcome its approximate nature, which is crit-
ical for applications that must guarantee query accuracy. Our starting
point to address this problem in the metagenomics context was to ask
how could we keep some of the useful information from each hash table,
without explicitly using all the hash tables or explicitly storing them. The
solution was to augment the indexing structure of Amordad by incorpor-
ating a proximity graph. This graph encodes important distance relation-
ships that have accumulated through the application of many distinct
hash functions, allowing all but a small number of hash tables to be
discarded. We show how this graph is constructed and later examine its
use in the query process.

2.3 Supplemental indexing with a developing nearest
neighbor graph

The family of hash functions deﬁned in Equation (1) corresponds to
hyperplanes in R“, each partition the space of metagenomes. Consider
the geometric loci of all points that are hashed to the same bucket as the
query point. This ‘query region’ is the intersection of r half-spaces,
deﬁned by r hyperplanes, that contain the query point. The query
region is likely to contain the actual nearest neighbor point, but even if
it does not, it is reasonable to expect that this point remains close to the
query region. We grow this region by searching the neighborhood of all
points inside it. Amordad encodes spatial information about the proxi-
mities of data points using a graph, and employs the graph when

 

2950

ﬁm'spzumol‘pmJXO'sopeuuopuorq/ﬁdnq

an?kgogmomammowoio~&o:3m$.omm\

Insertion/ Deletion

 

m ANNG

E.Behnam and A.D.Smith

 

metagenomic associations in large—scale applications, and (ii)
demonstrating the favorable technical aspects of our database
engine with particular emphasize on scalability.

3.1 Dataset and implementation details

We obtained WMS data from the European Nucleotide Archive
(ENA) (Leinonen et al., 2011), including ~3.5TB of compressed
FASTQ files from 133 projects (08/15/13). We extracted 5—mers
from each sample and obtained 5073 real metagenomic count
vectors in d=1024—dimensional space. We augmented this set
with an additional 106 randomly generated metagenomes to in—
crease the scale of our evaluations. Each simulated metagenome
contained N = 107 i.i.d. reads of length 100. On initiation of the
query, the graph and the queue are read into memory (these
could be memory residents on a dedicated server). Because of
space constraints, we do not load all count vectors but instead an
index of their location on disk.

For comparison purposes in evaluating accuracy, we imple—
mented the brute—force (quadratic time) nearest neighbor graph
construction and the linear time scan for exact results from
queries. The implementation of the database engine and asso—
ciated utilities was done in C+ + and with the assistance of the
Boost Graph Library (http://www.boost.org/libs/graph). Our
Amordad implementation includes multi—threaded query pro—
cessing. However, all evaluations described here were done
using a single core of a 2.4GHz Xeon CPU and 16GB of
memory.

Building Amordad is a multi—stage process, involving the con—
struction of hash tables and the nearest neighbor graph. The
overall time to build the database with 106 count vectors was
44.9 h. We also investigated the memory requirement for build—
ing the database. Initial construction of the nearest neighbor
graph required the largest memory among all of the programs
included in Amordad software package. When examining
L = 200 hash tables to construct a 20—NNG representing 106
metagenomes, the peak memory was 10.4GB.

3.2 Assigning significance to query responses

Assigning significance to query results is essential, but there
should be no expectation that actual distances between metagen—
omes (for any reasonable representation) follow a simple statis—
tical distribution. To gain insight into how these distances can be
interpreted in the context of Amordad queries, we examined the
distribution of distances for relatively small sample of available
metagenomes.

Figure 2 shows the empirical distribution of distances between
the 5073 metagenomes from the 133 projects. Denoting F as the
cumulative function of this distribution, for any query point y
with response X, we deﬁned F(6xy) as the signiﬁcance score of the
query answer. This score shows the relative ranking of the dis—
tance between X and y among all of the distance measurements.
If F(6xy) is large, the relatedness between X and y might be ques—
tionable. In fact, if X and y are two unrelated metagenomes, the
centralization of count vectors and the linearity of expectations
assert that E(6x.‘,) =rr/ 2. This important observation suggests
that any meaningful 6x). must belong to the tail of the empirical
distribution.

 

 

 

 

 

(l 7rI/6 7TI/3 7rI/2 27rI/3 57rI/6
angle

Fig. 2. Probability density function of angles resulted from real
metagenomes

We decided to deﬁne the maximum proximity radius as the
maximum distance between the query point and the retrieved
data point such that the response is assumed relevant to the
query. We denote the maximum proximity radius by 60‘ and
for a query y, we ﬁlter out any response X if 6W>60. In our
evaluations, a liberal threshold is F1(0.2), which results in
60 =0.987 radians or 60 8 57°. A more conservative cut—off is
F1(0.1) which is equivalent to 60 =0.752 radians or approxi—
mately 43°. The conservative and liberal cutoff thresholds are
respectively selected as the first and second deciles of the empir—
ical cumulative distribution of distances within the database.

3.3 Evaluation of the graph effect on the query space
requirement

We can view the m—NNG as having two effects: increasing ac—
curacy when using a fixed number of hash tables or reducing the
number of hash tables required to achieve a particular accuracy.
We conducted a series of experiments to compare the perform—
ance of the various instances of Amordad with the basic LSH
algorithm when they are both using the same sets of hash tables.
For LSH alone, we exclude the graph. The evaluation metric
used here is average recall on a set of queries. Given a query y,
denoting the t nearest neighbors as N,(y), if R(y) is the set of
points reported by the algorithm, then

$021110): |N2(y) ﬂ RUN/WAYN-

The detailed experimental set up is as follows: We randomly
selected 10 of 133 projects and regarded their 94 samples as the
query set. Different instances of Amordad were then constructed
considering all other samples as database points and m—NNG for
m e {1, 10, 20}. Next, using 1 g L g 200 hash tables, we com—
pared the average recall value of different instances of Amordad
with the basic LSH algorithm. For simplicity, we only considered
the ﬁrst nearest neighbor (Le. t = 1), and performed experiments
separately for both the conservative and liberal—maximum prox—
imity radius. If the distance between any query point and its
nearest neighbor exceeded the maximum proximity radius in
an experiment, it was discarded from the query set. As hash
functions are randomly generated, we repeated each experiment

 

2952

ﬁm'spzumofpmjxo'sopeuuopuorq/ﬁdnq

The Amordad database engine for metagenomics

 

0.9

0.8

recall

0.7

0.6

0.5

 

0 40 80 120 160 200
number of hash tables

0.9

0.8

recall

0.7

0.6

0.5

 

0 40 80 120 160 200
number of hash tables

Fig. 3. Average recall for a query set. showing (A) the conservative and (B) liberal maximum proximity radius

 

A 0.55- ,a
0.50- ,x
6 ,’
S ,’
§  ‘1
Z x
E 0.40- ,'
1’;
0.35- 

 

 

 

16 17 18 19 20

log of database size

 

9

m 1000— 
C I
.9 ,.'
§ /
a 900— ,’
E 5
8 
G.) I
'31) 800- x
g x.

700— 5’

 

 

 

16 17 18 19 20

log of database size

Fig. 4. (A) Average wall—time and (B) required distance computations per query as a function of (log) database size

10 times with different set ofhash functions and reported average
recall.

Based on the results presented in Figure 3. for a ﬁxed accur-
acy. Amordad requires considerably fewer hash tables than LSH.
When the graph is constructed for m : 10 or m : 20. Amordad
already exhibited no query errors in any experiment. However.
LSH is not able to reach perfect recall even when L : 200 hash
tables are used under the liberal-maximum proximity radius.

3.4 Measuring query time in a large-scale database

We conducted a set of experiments to investigate the scalability
of the Amordad engine. Similar to the previous experiment. we
randomly selected 10 projects and used their metagenomes (183)
for queries. All nd : 4890 remaining metagenomic points were
used. along with My simulated points. to construct the different
instances of the database. We set the value of 11X such that 715+
I'll/:21. for 1' increasing from 16 to 20. Then. we created an in-
stance of the database with n points as follows. We set the
number of hash functions to 1' :log n. We used the conservative
threshold 90:0.752 to determine the number of hash tables

required by the LSH to bound the probability of missing a near-
est neighbor below a:0.05. Finally. we constructed the graph
with 10 outgoing edge from each node.

To evaluate the relation between the query time and the
number of points in the database. we calculated the average
wall-time and number of angle computations required for re-
sponding to all queries. As shown in Figure 4. the query time
grows almost linearly with log n. This establishes the scalability
of Amordad. validating its ability to efficiently process queries in
large database instances. In each experiment conducted here. the
query accuracy was 100%. again demonstrating the effect of the
graph to drive the probability of query error to virtually zero.

3.5 Relationships captured in the nearest neighbor graph
Our original purpose for including the m-NNG in the design of
Amordad was to eliminate the space that would otherwise be
required to explicitly use enough hash tables to ensure the desired
accuracy. But it can also be leveraged as a persistent and adap-
tive knowledge base that links close points and therefore permits
the detection of biologically similar metagenomes. We expect

 

2953

[310'sp2umofpmjxo'sopeuHOJIIrorq/ﬁdnq

E.Behnam and A.D.Smith

 

Table 1. Top 10 most similar pairs of metagenomes from different projects, with scope of corresponding projects, distance between metagenomic points

(i.e. the angle in degrees) and associated signiﬁcance scores

 

 

Accession I Accession II Project scope I Project scope II Distance Relative ranking
ERP001737 ERP001736 Sea water Sea water 40 0.0016
ERP001736 ERP001227 Sea water North sea water 4.60 0.0020
SRP009476 ERP001736 Sea water Sea water 5.30 0.0023
ERP001068 SRP013944 Low and neutral PH soil Forest soil 5.30 0.0023
SRP021115 ERP001736 Marine matter Sea water (different depth) 5.40 0.0023
SRP009476 SRP021115 Sea water Marine matter 5.50 0.0024
SRP017582 ERP001568 Oil sands Polluted water 6.50 0.0028
ERP001737 SRP021115 Sea water Marine matter 6.60 0.0029
SRP016067 ERP002469 Gut microbiotaa Gut microbiotab 6.80 0.0030
SRP019913 SRP021115 Red sea water Marine matter 7.30 0.0033

 

"From a cohort of normal and atherosclerosis samplestetagenomes are sampled from European women with normal, impaired and diabetic glucose control.

many edges in the graph will link between biological replicates,
the precise deﬁnition of which might depend on the goals of the
project. For example, a cohort representing sea water samples
taken at slightly different depths, or gut microbiota samples ob—
tained at slightly different times. Recovering biological replicates
within a cohort is an easy task under our framework and could
be potentially useful. However, we claim our approach can clus—
ter inherently similar metagenomes together even when the meth—
odologies for obtaining, preparing and sequencing samples vary.

To demonstrate this type of relationship captured within
Amordad, we constructed the least informative graph (i.e. the
1—NNG) using metagenomes from the 133 projects. We identiﬁed
any edge connecting metagenomes that belong to two distinct
projects. Among the 194 such edges, 58 showed the signiﬁcance
score <0.01. Table 1 shows the top 10 most similar pairs of
samples (projects are listed by accession number from ENA).
To avoid redundancy, any pair of projects is reported at most
once in the table (Supplemetary Material contains a complete
list). In general, we expect the number of applications that can
beneﬁt from mining this graph is extremely broad.

4 DISCUSSION

We presented Amordad, a database engine for whole—metagen—
ome sequencing data. Amordad uses alignment—free principles,
representing metagenomes as points in a high—dimensional geo—
metric space. LSH is used to efﬁciently index the database points.
To improve accuracy and efﬁciency beyond what is practical
from LSH alone, Amordad augments this indexing with a regu—
lar nearest neighbor graph. The randomness in Amordad is con—
tinually refreshed by resampling new random hash functions.
Although only a fixed number of hash functions is alive within
Amordad at any given time, those that are extinct have contrib—
uted to optimizing connectivity in the graph, and thus continue
to assist queries even if they are no longer used for hashing.
Results from a series of experiments have demonstrated this
approach to have a signiﬁcant effect on query efﬁciency and
accuracy.

The ability to cope with the rapid accumulation of data was
a central design goal. LSH is a well—known approach to index

high—dimensional data, but it is also a randomized method.
A major drawback is the number of hash tables one must main—
tain to provide guarantees on the accuracy of queries, imposing
time and space constraints. This is a well—known problem, and
solutions have been proposed to overcome this pitfall (Lv et al.,
2007; Panigrahy, 2006). These solutions mostly rely on the fact
that each hash function (i.e. each bit in our scheme) provides a
ranking based on the proximity of points to a certain query.
Therefore, even if the closest neighbor is not hashed to the
same bucket as the query, it is highly probable that buckets
close to the query bucket contain the nearest neighbor.
Consequently, one may avoid generating many hash tables at
the cost of checking more buckets from each table. This clever
strategy has a direct effect on the indexing memory consumption
(Lv et al., 2007). However, as the distance between the query and
its nearest neighbors increases, there is a rapid growth in the
number of buckets that must be checked. This presents a chal—
lenge in our applications because of the inherent diversity of
microbial communities in many circumstances. In fact, if we
view a set of biologically related metagenomes as a cluster in
high—dimensional space, the diameter of some complex clusters
like human gut microbiota (Arumugam et al., 2011), might be
large enough to make the process of searching close buckets to
the query bucket time—consuming.

Our approach, augmenting the LSH indexing with a graph
structure, avoids having to explicity search additional buckets,
and depends on transitivity of relationships encoded in the
graph. Intuitively, if the underlying distance measure between
metagenomes satisﬁes the triangle inequality, the graph should
guide the search in the most appropriate directions. Additionally,
the LSH provides a means of streamlining the database main—
tenance process. Hash tables are maintained in a ﬁxed size queue
that is continually updated, and each new hash table contributes
to reﬁning edges in the graph. As a consequence, we are able to
keep the graph in a near ideal state without requiring explicit
operations to maintain the graph after metagenomes are added
to (or removed from) the database. This design decision acknow—
ledges that in the near future large (and distributed) metagenome
databases will be updated frequently, and efﬁciency of queries
will be more important to users of Amordad.

 

2954

ﬁm'spzumot‘pmjxo'sopeuuopuorq/ﬁdnq

The Amordad database engine for metagenomics

 

It is clear that many applications of Amordad will involve
maintaining large metagenomic data clusters so that new meta—
genomes can be understood through their neighborhoods in the
graph. From this perspective, the graph becomes the primary
structure, and the LSH serves simply to find the right neighbor—
hood without exhaustive search.

Finally, we address some limitations of this work. We repre—
sent each metagenome by a feature vector that included the
frequencies of all k—mers for a fixed k. We did not use feature
selection, and were bound to relatively small values of k. Many
of the k—mers were almost certainly irrelevant in our experiments
on real data. More ﬂexibility could be obtained if a metagenome
is represented by a ‘bag of features’ (Salton, 1991) composed of a
variety of words (i.e. subsets of k—mers for varying k). From
statistical point of View, this is equivalent to shifting the para—
digm from a full Markov chain of order k toward a variable
length Markov chain (Biihlmann and Wyner, 1999). The main
challenge is to ﬁnd a parsimonious algorithm for feature extrac—
tion at large scale. More important than irrelevant features, how—
ever, are features representing technical artifacts, for example, of
the sequencing experiments.

Funding: This work was supported by the National Institute of
Health [P50 HG002790].

Conﬂict of interest: none declared.

REFERENCES

Arumugam,M. et al. (2011) Enterotypes of the human gut microbiome. Nature, 473,
1744180.

Behnam,E. et al. (2013) A geometric interpretation for local alignment—free sequence
comparison. J. Comput. Biol, 20, 471485.

Beis,J.S. and Lowe,D.G. (1997) Shape indexing using approximate nearest—neigh—
bour search in high—dimensional spaces. Conference on Computer Vision and
Pattern Recognition, Puerto Rico, pp. 100071006.

Biihlmann,P. and Wyner,A.J. (1999) Variable length Markov chains. Ann. Stat., 27,
48(F513.

Chan,C.X. and Ragan,M.A. (2013) Next—generation phylogenomics. Biol. Direct, 8,
14.

Charikar,M.S. (2002) Similarity estimation techniques from rounding algorithms.
In: Proceedings of the Thiry—Fourth Annual ACM Symposium on Theory of
Computing. ACM, Las Vegas, Nevada, pp. 38(%388.

Daniel,R. (2005) The metagenomics of soil. Nat. Rev. Microbiol, 3, 470478.

Dong,W. et al. (2011) Efﬁcient k—nearest neighbor graph construction for generic
similarity measures. In: Proceedings of the 20th International Conference on
World Wide Web. ACM, Hyderabad, India, pp. 577586.

Gionis,A. et al. (1999) Similarity search in high dimensions via hashing. In: VLDB.
Vol. 99, pp. 5187529.

Goemans,M.X. and Williamson,D.P. (1995) Improved approximation algorithms
for maximum cut and satisﬁability problems using semideﬁnite programming.
J. ACM, 42, 111571145.

Grabherr,M.G. et al. (2011) Full—length transcriptome assembly from RNA—Seq
data without a reference genome. Nat. Biotechnol, 29, 644452.

Huson,D.H. et al. (2011) Integrative analysis of environmental sequences using
megan4. Genome Res, 21, 155271560.

Jannink,J. (1995) Implementing deletion in B+—trees. ACM Sigmod Rec., 24,
33738.

Kantorovitz,M. et al. (2007) Asymptotic behavior of k—word matches between two
uniformly distributed sequences. J. Appl. Proba17., 44, 7887805.

Le Chatelier,E. et al. (2013) Richness of human gut microbiome correlates with
metabolic markers. Nature, 500, 5417546.

Leinonen,R. et al. (2011) The european nucleotide archive. Nucleic Acids Res., 39,
D287D31.

Luo,R. et al. (2012) SOAPdenovoZ: an empirically improved memory—efﬁcient
short—read de novo assembler. GigaScience, l, 18.

Lv,Q. et al. (2007) Multi—probe LSH: efﬁcient indexing for high—dimensional simi—
larity search. In: Proceedings of the 33rd international conference on Very large
data bases. VLDB Endowment, Vienna, Austria, pp. 9507961.

McHardy,A.C. et al. (2007) Accurate phylogenetic classiﬁcation of variable—length
DNA fragments. Nat. Methods, 4, 63772.

Meyer,F. et al. (2008) The metagenomics rast serveria public resource for the
automatic phylogenetic and functional analysis of metagenomes. BM C
Bioiiﬁ’ormatics, 9, 386.

Miller,G.L. et al. (1997) Separators for sphere—packings and nearest neighbor
graphs. J. ACM, 44, 1729.

Muller,M.E. (1959) A note on a method for generating points uniformly on
n—dimensional spheres. Commun. ACM, 2, 19720.

Nalbantoglu,O.U. et al. (2011) RAIphy: phylogenetic classiﬁcation of metage—
nomics samples using iterative reﬁnement of relative abundance index proﬁles.
BMC Bioinﬁ)rmatics, 12, 41.

Panigrahy,R. (2006) Entropy based nearest neighbor search in high dimensions. In:
Proceedings of the seventeenth annual ACM—SIAM Symposium on Discrete
Algorithm. ACM, Philadelphia, PA, pp. 118(r1195.

Porter,M.S. and Beiko,R.G. (2013) SPANNER: Taxonomic assignment of
sequences using pyramid matching of similarity proﬁles. Bioiiy’ormatics, 29,
185871864.

Qin,J. et al. (2010) A human gut microbial gene catalogue established by metage—
nomic sequencing. Nature, 464, 5945.

Qin,J. et al. (2012) A metagenome—wide association study of gut microbiota in type
2 diabetes. Nature, 490, 5540.

Ravichandran,D. et al. (2005) Randomized algorithms and NLP: using locality
sensitive hash function for high speed noun clustering. In: Proceedings of the
43rd Annual Meeting on Association for Computational Linguistics. Association
for Computational Linguistics, Ann Arbor, MI, pp. 622429.

Salton,G. (1991) Developments in automatic text retrieval. Science, 253,
9744980.

Samet,H. (2006) Foundations of Multidimensional and Metric Data Structures.
Morgan Kaufmann, Burlington, MA.

Song,K. et al. (2014) New developments of alignment—free sequence comparison:
measures, statistics and next—generation sequencing. Brief. Bioiiy’ormatics, 15,
343753.

Turnbaugh,P.J. et al. (2007) The human microbiome project. Nature, 449,
8044810.

Tyson,G.W. et al. (2004) Community structure and metabolism through reconstruc—
tion of microbial genomes from the environment. Nature, 428, 3743.

Vinga,S. and Almeida,]. (2003) Alignment—free sequence comparisonia review.
Bioiiﬁ’ormatics, 19, 5137523.

Wooley,J.C. et al. (2010) A primer on metagenomics. PLoS Comput. Biol, 6,
e1000667.

 

2955

ﬁm'spzumot‘pmjxo'sopeuuopuorq/pdnq

