BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Interaction-based feature selection and classification for high-dimensional biological data

 

gene, it may be necessary to consider the gene jointly with others
in the same functional module such as a pathway. Secondly, the
genes may interact with each other in inﬂuencing the response.
Thirdly, the effect of genes on the response may be highly
nonlinear.

To address these challenges, the proposed method extracts
different types of information from the data in several stages.
In the ﬁrst stage, we select variables with high potential to form
inﬂuential variable modules when combining with other vari—
ables. In the second stage, we generate highly influential variable
modules from variables selected in the first stage so that each
variable interacts with others in the same module to produce a
strong effect on the response Y. The third stage combines clas—
sifiers, each constructed from one module, to form the classiﬁ—
cation rule.

The overarching idea is that since the feature selection problem
involving module, interaction and nonlinearity is too compli—
cated to be reduced to one single optimization problem based
on a model equation, we break the problem into smaller ones. As
the nonlinearity, interaction and module effects can be ad—
equately accommodated within a smaller problem, we then
solve each smaller problem and put the solutions together to
form the final one. Our method provides a ﬂexible framework
to analyze high—dimensional data for classiﬁcation purposes. It is
model—free and considers variable interaction explicitly, which
aligns well with the systems—oriented biological paradigm.

LASSO related methods were developed for grouped variables
(Yuan and Lin, 2006; Zou and Hastie, 2005). Comprehensive
reviews of feature selection methods are available from machine
learning literature (Dash and Liu, 1997; Guyon and Elisseeff,
2003; Liu and Yu, 2005) and bioinformatics (Saeys et al., 2007).

2 APPROACH: TWO BASIC TOOLS

To shed light on the effectiveness of our method, we provide
preliminary illustration via a toy example. The purpose of the
toy example is to demonstrate that two basic tools adopted by
our method can elicit interaction information difﬁcult for other
methods.

2.1 An inﬂuence measure

For easy illustration, we assume that the response variable Y is
binary (taking values 0 and l) and all explanatory variables are
discrete. Consider the partition 7% generated by a subset of k
explanatory variables {X b] , - - - , X bk }. If all variables in the subset
are binary then there are 21" partition elements; see the ﬁrst para—
graph of Section 3 in (Chernoff et al., 2009). Let n1(j) be the
number of observations with Y :1 in partition element j. Let
n1(j) = n, x 711 be the expected number of Y :1 in element j
under the null hypothesis that the subset of explanatory variables
has no association with Y, where n, is the total number of ob—
servations in element j and n1 is the proportion of Y: 1 obser—
vations in the sample. The inﬂuence measure of (Lo and Zheng,
2002), henceforth L2, is deﬁned as

1(an“‘aXb/.)= 2 [111(1) — 1110312-

./E 73k

The statistic I equals the sum of squared deviations of Y—fre—
quency from what is expected under the null hypothesis. Two
properties of I make it useful. First, the measure I does not re—
quire one to specify a model for the joint effect of {Xbl , - - - , ka}
on Y. It is designed to capture the discrepancy between the con—
ditional means of Y on {X b] , - - - , X bk} and the marginal mean of
Y whatever the conditional distribution may be. Secondly, under
the null hypothesis that the subset has no inﬂuence on Y, the
expected value of I remains non—increasing when dropping vari—
ables from the subset. The second property makes I critically
different from the Pearson’s X2 statistic whose expectation de—
pends on the degrees of freedom and hence on the number of
variables used to deﬁne the partition. To see this, we rewrite I in
its general form when Y is not necessarily discrete

I= Dim — Yr,
1673A

where  is the average of Y—observations over the jth partition
element and Y is the overall average. Under the same null, it is
shown (Chernoff et al., 2009) that the normalized I, I/na2 (02
denotes the variance of Y), is asymptotically distributed as a
weighted sum of independent X2 random variables of one
degree of freedom each such that the total weight is less than
one. This very property provides the theoretical basis for the
following algorithm.

2.2 A backward dropping algorithm

The backward dropping algorithm (BDA) is a greedy algorithm
to search for the variable subset that maximizes the I—score
through stepwise elimination of variables from an initial subset
sampled in some way from the variable space. The details are as
follows.

(1) Training set: Consider a training set {021, x1), - - - , (yn, x,,)}
of n observations, where x,~ = (x1,~, ---,xp,~) is a p—dimen—
sional vector of explanatory variables. Typically p is very
large. All explanatory variables are discrete.

(2) Sampling from variable space: Select an initial subset of k
explanatory variables 5;, = {X},], ---,X,,,_,}, b = 1, ...,B_
(3) Compute I—score: I(S;,) = I;  — Y)2
.16 Ir
(4) Drop variables: Tentatively drop each variable in S}, and
recalculate the I—score with one variable less. Then drop
the one that gives the highest I—score. Call this new subset
g, which has one variable less than Sb.

(5) Return set: Continue the next round of dropping on
5; until only one variable is left. Keep the subset that
yields the highest I—score in the whole dropping process.
Refer to this subset as the return set R1,. Keep it for future
use.

If no variable in the initial subset has inﬂuence on Y, then
the values of I will not change much in the dropping
process; see Figure lb. On the other hand, when inﬂuential vari—
ables are included in the subset, then the I—score will increase
(decrease) rapidly before (after) reaching the maximum; see
Figure la.

 

2835

ﬁm'spzumofpmﬂo'sopeuuopnoiq/ﬁdnq

H. Wang et al.

 

2.3 A toy example

To address the three major challenges mentioned in Section 1,
the toy example is designed to have the following characteristics.

(a) Module effect: The variables relevant to the prediction of
Y must be selected in modules. Missing any one variable in
the module makes the whole module useless in prediction.
Besides, there is more than one module of variables that
affects Y.

(b) Interaction effect: Variables in each module interact with
each other so that the effect of one variable on Y depends
on the values of others in the same module.

(c) Nonlinear effect: The marginal correlation equals zero be—
tween Y and each X —variab1e involved in the model.

Let Y, the response variable, and X = (X 1, X2, - - - , X 30), the ex—
planatory variables, all be binary taking the values 0 or 1. We
independently generate 200 observations for each X ,~ with
P{X,~ = 0} = P{X,~ = 1} = 0.5 and Yis related to X via the model

Y_ X1 + X2 + X3 (modulo2) with probability0.5 (1)
_ X4 + X 5 (modulo2) with probability0.5

The task is to predict Y based on information in the 200 x 31
data matrix. We use 150 observations as the training set and 50
as the test set. This example has 25% as a theoretical lower
bound for classiﬁcation error rates because we do not
know which of the two causal variable modules generates the
response Y.

Table 1 reports classiﬁcation error rates and standard errors
by various methods with ﬁve replications. Methods included are
linear discriminant analysis (LDA), support vector machine
(SVM), random forest (Breiman, 2001), LogicFS (Schwender
and Ickstadt, 2008), Logistic LASSO, LASSO (Tibshirani,
1996) and elastic net (Zou and Hastie, 2005). We did not include
SIS of (Fan and Lv, 2008) because the zero correlation

 

 

 

 

 

 

 

 

a b
(g) _ 7,0 (2) o _
o o 0 _
a S: - o 1, a 3
8 o ‘ 0’ § o '
C _
‘3 8 _ o g 8
E _¢mmmlﬂ‘§p E _anm:mn1wﬁpzsp°
_ o — o —
I I I I I I I I I I
5 10 20 5 10 20
dropping round dropping round
Dropping with information Dropping without information

Fig. 1. Comparison of variable subsets with/without information using
BDA

Table 1. Classiﬁcation error rates for the toy example

mentioned in (c) renders SIS ineffective for this example. The
proposed method uses boosting logistic regression after feature
selection.

To assist other methods (barring LogicFS) detecting inter—
actions, we augment the variable space by including up to
3—way interactions (4495 in total). Here the main advantage of
the proposed method in dealing with interactive effects becomes
apparent because there is no need to increase the dimension of
the variable space. Other methods need to enlarge the variable
space to include products of original variables to incorporate
interaction effects.

For the proposed method, there are B: 5000 repetitions in
BDA and each time applied to select a variable module out of
a random subset of k: 8. The top two variable modules, identi—
fied in all ﬁve replications, were {X4, X5} and {X1, X2, X3} due to
the strong interaction effect within them. Here the advantage of
the proposed method in handling the module effect is clearly
demonstrated because variables are always selected in modules.
In summary, the proposed method correctly selected the two
causal modules of variables and thus yields the lowest test
error rate. Moreover, by comparing the train and test error
rates in Table 1, we observe that all methods except for the
proposed one suffer from overfitting. We also tested several
other models. The general message is that LASSO and related
methods work well for linear models with signiﬁcant marginal
effects while our method performs better for nonlinear models
with module and interaction effects.

3 METHODS

The proposed method consists of three stages (Fig. 2). First, we screen
variables to identify those with high potential to form inﬂuential modules
when combining with other variables. Secondly, we generate highly in-
ﬂuential variable modules from variables selected in the ﬁrst stage, where
variables of the same module interact with each other to produce a strong
effect on Y. The third stage combines the variable modules to form the
classiﬁcation rule.

We have shown in Section 2 that BDA can extract useful information
from the data about module and interaction effects. However, how to
determine the input to BDA and how to use the output from BDA
require entirely new methods. Unless one can properly manage the
input to and output from BDA, the strength of BDA as a basic tool
cannot be fully realized. In this regard, the innovation of the proposed
method manifests itself in three ways. First, because direct application of
BDA in high-dimensional data may miss key variables, we propose a
two-stage feature selection procedure: interaction-based variable screen-
ing and variable module generation via BDA. Since the quality of vari-
ables is enhanced by the interaction-based variable screening procedure in
the ﬁrst stage, we are able to generate variable modules of higher order
interactions in the second stage. These variable modules then serve as
building blocks for the ﬁnal classiﬁcation rule. Secondly, we introduce
two filtering procedures to remove false-positive variable modules.
Thirdly, we put together classiﬁers, each based on one variable module,

 

 

Method LDA SVM Random forest LogicFS Logistic LASSO LASSO Elastic net Proposed
Train error 0.14 :i: 0.03 0.00 :i: 0.00 0.00 :i: 0.00 0.13 :i: 0.02 0.23 :i: 0.05 0.27 :i: 0.06 0.27 :i: 0.06 0.21:1:0.01
Test error 0.47 :i: 0.02 0.50 :i: 0.01 0.44 :i: 0.04 0.34 :i: 0.04 0.45 :i: 0.03 0.48 :i: 0.04 0.48 :i: 0.04 0.24:1:0.03

 

 

2836

ﬁm'spzumofpmjxo'sopeuuopuoiq/ﬁdnq

Interaction-based feature selection and classification for high-dimensional biological data

 

 

Fig. 2. Method ﬂowchart

to form the ﬁnal classiﬁcation rule. These new ideas produce signiﬁcantly
better results than applying BDA directly.

We believe that the analysis of high-dimensional data with module,
interaction and nonlinear effects cannot be effectively resolved within one
single optimization problem based on a model equation. We must extract
information in several stages, each aims for a speciﬁc type of information
from the data, then combining information from each to achieve the ﬁnal
goal. Thus dividing our method into stages, each with a speciﬁc goal, is
part of the method and not just a convenient way to present the method.
We use the data from van’t Veer (2002) as a running example. The back-
ground of the data is given in Section 4.

The inﬂuence measure I works best for discrete variables. If some
explanatory variables are continuous, we ﬁrst convert them into discrete
ones for feature selection purpose (once variable modules have been
generated, we use the original variables to estimate their effects). This
pre-processing step induces an information tradeoff: information loss due
to discretizing variables versus information gain from robust detection of
interactions by discretization. By the classiﬁcation error rates reported in
Section 4, we demonstrate that the gain from robust detection of inter-
actions is much more than enough to offset possible information loss due
to discretization.

In this article, we use the two-mean clustering algorithm to turn the
gene expression level into a variable of two categories, high and low. As
an additional piece of evidence supporting the proposed pre-processing
step, we have tried more than two categories; e.g. three categories of high,
medium and low. The empirical results show that the more categories
used the worse classiﬁcation error rates. Using more categories is sup-
posed to reduce information loss due to discretization. Hence the empir-
ical result suggests that for the kind of problems studied in this article,
robust detection of interaction in feature selection is much more import-
ant than avoiding information loss by using the original variables.

3.1 Interaction-based variable screening

When the number of variables is moderate as in the toy example, we can
directly apply BDA to generate variable modules without doing variable
screening ﬁrst. However, when the number of variables is in the thou-
sands or more, directly applying BDA may miss key variables and we
need to do variable screening before BDA. Since we would like to screen
for both module and interaction effects, the screening is not on individual
variables but rather on variable combinations and thus it is interaction-
based. However, the dimensionality of variable combinations grows ex-
ponentially with the size of the combinations. For example, with 5000
variables we have over 10 million pairs and over 20 billion triplets.
Computational constraints arise. While screening all triplets provides in-
formation up to 3-way interactions, the computation cost is more than
1000 times that for pairs. In light of computational resource consider-
ations, one must decide on the order of interaction to screen for.

3.1.1 Determine the threshold for inﬂuence scores Suppose that
it is decided to screen for 3-way interactions. We then obtain I-scores, one

 

 

 

(Du) -
L -—o I
8‘— I
w .
I E
I
u5 9— ;
\_
‘1’ i
g .
o O 5
._I.n . .
tbd— .
iE .!
'C ..|
1: ‘0
E °- W
No 0

 

010 20 30 40 50 60

# of triplets in thousands

Fig. 3. 2nd difference of I-scores for every 1000 triplets

for each triplet. Now the job is to ﬁnd a cut-off value for I-scores so that
triplets with scores higher than the cut-off value are selected for further
analysis and those with lower scores are discarded. This is a common
issue for feature selection. Generally speaking, there are two approaches:
controlling the size of the selected variable subset (Fan and Lv, 2008;
Guyon et al., 2002;) or controlling the false discovery rate (Benjamini and
Hochberg, 1995). Here, we offer a new approach based on the 2nd dif-
ference of the scores, which works as follows.

First, order the triplets from high to low according to their respective
I-scores. Then go through the ordered triplets and record the I-score for,
say, every one thousandth triplets. That is, record the scores for 1st,
lOOlst, 2001st, . . . triplets. Typically, the second difference (The ﬁrst dif-
ferences of a sequence a], 02, - - - , are the successive differences
a] — 02, 02 — a3, - - - and the second differences are the successive differ-
ences of the ﬁrst difference sequence.) of the aforementioned sequence of
scores declines sharply in the beginning and then settles down around
zero. We will choose a cut-off value corresponding to when the 2nd dif-
ference is near zero for the ﬁrst time. Figure 3, obtained from the van’t
Veer dataset, reveals that the second difference of I-score ﬂattens out
after 21 thousand top scored triplets, which are retained for further
analysis.

The rationale for the 2nd difference procedure is very simple: if we
lower the cut-off value just a little, we will allow a lot more variables to be
included and we know most of them are false positives. A calculation
similar to the local false discovery rate of (Efron et al., 2001) gives almost
the same result.

3.1.2 Determine the threshold for retention frequency After
determining the cut-off value for the high-scored triplets, we face a related
yet different issue. Since the I-score is shared by variables in the same
triplet and different triplets may overlap, we have a set of I-scores instead
of one for each variable in the high-scored triplets. We use the retention
frequency to select variables from the high-scored triplets.

The retention frequencies usually show big drops in the beginning and
small differences after a certain point. See Figure 4, which is based on the
top 21 thousand triplets obtained from Figure 3. We select the top 138
high-frequency variables because the later ones differ little as the 1st dif-
ference indicates. Moreover, retention frequency ties (lst-difference zeros)
occur much more frequently after the top 138 variables.

We did sensitivity analysis on the cut-off values for both I-scores and
retention frequencies. The cut-off values do not affect later analysis re-
sults if they are changed up to 10%. In Figure 3, if we use 19723 thou-
sands triplets, the ﬁnal result is basically the same as that based on 21
thousand triplets. In Figure 4, using between 110 and 150 high-frequency
variables yields the same ﬁnal result.

High-frequency variables have high potential to form inﬂuential vari-
able modules. This is because they yield high I-scores when combined
with other variables and do so frequently. Usually there are only a mod-
erate number of high-frequency variables. In the three microarray

 

2837

ﬁm'spzumol‘pmjxo'sopeuuopuoiq/ﬁdnq

H. Wang et al.

 

 

>
O
E o
3 ‘—
0'
a.)
.2 co
m
C
OJ (0
O)
“5
w v
0
C
2 N j
a) I ll
% 0 "ill illl llili II‘III‘III "li‘l'i"‘l“|i
E
0 50 100 200
#ofgenes

Fig. 4. 1st difference of retention frequency from top 21 000 triplets

datasets that we analyzed, the variable screening procedure described
above reduces the number of variables from thousands to below 1507
a >95% reduction in the number of variables! This drastic reduction of
variables facilitates the generation of variable modulesithe next stage of
the proposed method.

3.2 Variable module generation via BDA

We now apply BDA to the high potential variables obtained from the
previous stage to generate variable modules. There are two quantities to
be determined before applying BDA.

3 2.] Calculate the initial size for BDA The initial size refers to
the size of initial variable subsets subjected to backward dropping. The
initial size depends on the number of training cases available. If the initial
size is too large, then most partition elements induced by the initial subset
of variables contain no more than one training case. Hence, dropping is
basically random and one can start with a smaller subset and achieve the
same result with less computing time. The minimum requirement on the
initial size is that at least one partition element containing two or more
observations. Using Poisson approximation (Supplementary Material),
we can calculate the expected number of partition elements with two or
more observations. Then the minimum requirement is met if the initial
size k satisﬁes

n2/2n1k,1 3 1, (2)

where n is the number of training cases and mm is the number of par-
tition elements induced by a subset of k—l variables. Thus (2) provides an
upper bound for the initial size. Suppose that there are 150 training cases
and all variables are binary. Since 13 variables induce a partition with
213 = 8192 elements and 1502/(2 - 8192)> l, we can choose the initial size
to be 14, the largest integer satisfying (2). In Supplementary Material,
another inequality gives the condition for at least one partition element
containing three or more observations and it suggests an improved upper
bound of 10. An argument in the next paragraph leads to a lower bound.
In practice, any initial size between the upper and lower bounds can be
used.

3.2.2 Calculate the number of repetitions in BDA The number
of repetitions in BDA is the number of variable subsets subjected to
backward dropping, which are randomly sampled from those variables
retained after the previous stage. The number of repetitions in BDA de-
pends on the number of training cases as well. The number of training
cases determines the size of variable subsets that can be supported by the
training set. For example, if we have 150 training cases, then they can
support a subset of size 5 assuming all explanatory variables are binary.
The reason is as follows. As a rule of thumb (Agresti, 1996), the
x2 approximation is adequate if the averaged number of observations

per partition element is at least 4. Since each subset of size 5 has
25 = 32 partition elements, each partition element on the average con-
tains 150 / 32>4 training cases. Hence, variable subsets of size 5 are ad-
equately supported by a training set of 150 (it is also the lower bound for
the initial size). In this case, we would like to make sure that the number
of repetition in BDA is sufﬁcient so that the quintuplets are covered
rather completely. Here we encounter a variation of coupon-collecting
problem.
Let p be the number of variables and let k be the initial size. Then there

are ’5’ quintuplets from p variables and each repetition in BDA can cover

g quintuplets. Consider the following coverage problem. There are a
total of ’5’ urns and each corresponds to a quintuplet. Each time 
balls, the number of quintuplets contained in an initial subset of size k,
are randomly placed into urns so that each ball is in a different urn. In
Supplementary Material, it is shown that we are expected to have

é~i<§>/<’§>110g<2> 

repetitions in BDA for a complete coverage of quintuplets from p
variables.

The preceding result does not take into account that each time we do
not place one ball but a cluster of balls into urns. It is known that clus-
tering increases the proportion of vacant urns (Hall, 1988). Hence the
expected number of repetitions to cover all quintuplets is larger than 13
(the exact result is an open problem). We propose 213 as an upper bound.
In the simulated examples, this upper bound is quite sufﬁcient and after
running 213 repetitions in BDA we do not miss any key variables.
Applying the 213 upper bound to the van’t Veer data, we use 1.5 million
repetitions in BDA to cover all quadruplets of 138 selected genes using
the initial size 11.

3.2.3 Two filtering procedures The return sets generated from
BDA will undergo two ﬁltering procedures to reduce between-return-set
correlation and false positives, respectively. The ﬁrst procedure is to ﬁlter
out return sets with overlapping variables. Since the return sets will be
converted into classiﬁers and it is desirable to have uncorrelated classi-
ﬁers, we shall keep only one of those return sets containing common
variables. This can be done by sorting the return sets in decreasing
order according to the I-scores and then remove those having variables
in common with a higher-scored one. This procedure has another remark-
able effectiit reduces the number of return sets from tens of thousands
to a few dozens, which greatly simpliﬁes the subsequent analysis. For
example, in one of the cross validation (CV) experiments of van’t Veer
data, the number of return sets with I-score above 300 reduced from 110
283 to 29 after this ﬁltering procedure.

The return sets after removing overlap ones are then subjected to a
forward adding algorithm to remove false positives. See Supplementary
Material for details. Very often, the error rates are much improved after
the ﬁltering procedures. The return sets retained after the two ﬁltering
procedures are the variable modules that we will use to build the ﬁnal
classiﬁcation rule.

3.3 Classiﬁcation

After variable modules have been generated, we then construct classiﬁers,
each based on one variable module. Since the number of variables in one
module is quite small (275 typically), the traditional setting of large n
small p prevails, and most existing classiﬁcation methods, including those
in Table 1 such as LDA related methods, SVM related kernel methods,
logistic regression and different versions of LASSO etc. can be employed.

3.3.1 Construct the classifier The classiﬁer used in this article is
logistic regression. In the logistic-regression classiﬁer, we include all inter-
action terms from a variable module. Thus a module of size 4 would give
rise to 16 terms including up to 4-way interaction as the full model.

 

2838

ﬁle'spzumol‘pmjxo'sopeuuonuoiq/ﬁdnq

Interaction-based feature selection and classification for high-dimensional biological data

 

We can then apply Akaike information criterion (AIC) to select a sub-
model. A sample output of logistic regression from R programming lan-
guage is shown in Supplementary Exhibit S1.

3.3.2 Combine the classzﬁers The logistic regression classiﬁers,
each based on one variable module, needs to be combined to form the
ﬁnal classiﬁcation rule. Methods that combine classiﬁers are referred to
as ensemble classiﬁcation methods in the literature. Dietterich (2000) gave
reasons for using ensemble classiﬁcation methods. Two of them ﬁt the
current situation well: (i) Since the sample size is only modest, many
classiﬁers ﬁt the data equally well. (ii) The optimal classiﬁer cannot be
represented by any one classiﬁer in the hypothesis space.

In this article, we employ the boosting method (Freund and Schapire,
1997) to combine classiﬁers. The boosting algorithm for variable modules
is included in Supplementary Exhibition S2. The ﬁnal classiﬁcation rule is
such that interactions among variables are allowed within each compo-
nent classiﬁer but not among variables in different classiﬁers. As the
classiﬁers are added one by one to the classiﬁcation rule via the boosting
algorithm, we expect the error rates for the training set to decrease after
each addition to reﬂect continuing improvement of ﬁt to the training
sample. However, the error rates for the test sample obtained by sequen-
tially adding classiﬁers do not necessarily decrease and can be used to
detect overﬁtting because information from the test sample is not used in
constructing the classiﬁcation rule via the boosting algorithm.

4 RESULTS

4.1 Classiﬁcation based on van’t Veer’s data

The ﬁrst dataset comes from the breast cancer study of (van’t
Veer et al., 2002). The purpose of the study is to classify female
breast cancer patients according to relapse and non—relapse clin—
ical outcomes using gene expression data. Originally, it contains
the expression levels of 24 187 genes for 97 patients, 46 relapse
(distant metastasis <5 years) and 51 non—relapse (no distant
metastasis :5 years). We keep 4918 genes for the classiﬁcation
task, which were obtained by (Tibshirani and Efron, 2002).

In (van’t Veer et al., 2002), 78 cases out of 97 were used as the
training set (34 relapse and 44 non—relapse) and 19 (12 relapse
and 7 non—relapse) as the test set. The best error rates (biased or
not) on this particular test set in the literature is around 10%.
Our method yields a perfect error rate on the test set of van’t
Veer (Fig. 5).

Since it is better to cross validate the error rates on other test
sets as well, the literature offers such error rates by a wide variety
of methods. The cross—validated error rates of the van’t Veer data
are typically around 30%. Some papers reported error rates sig—
niﬁcantly lower than 30%. However, after careful investigation,
we found all of them suffer from feature selection bias and/or
turning parameter selection bias (Zhu et al., 2008). Some of them
used leave—one—out cross validation (LOOCV). On top of the two
kinds of biases mentioned, LOOCV has the additional problem
of much larger variance than, say, 5—fold CV, because the esti—
mates in each fold of LOOCV are highly correlated. A summary
is in Table 2. The details are given in Supplementary Table S2.

The proposed method yields an average error rate of 8% over
10 randomly selected CV test samples. To be more specific, we
run the CV experiment by randomly partitioning the 97 patients
into a training sample of size 87 and a test sample of 10, then
repeated the experiment ten times. Since it has no tuning param—
eter and selects features without using any information

 

 

 

 

 

 

 

 

Train T951
07
g — o F ‘ °
‘— ‘o I
000
a) O)
2 .. z — 000
E 9 <1- \
L L 00
9 E as \
E a E — 00000
w 03
I: —oo nnnnnnnnnnnnnnn : — 0000
O | | | O | l |
5 10 ‘15 5 10 15

# of variable modules used # of variable modules used

Fig. 5. Error rates of our method for the test set of van’t Veer

Table 2. Classiﬁcation error rates by various methods on van’t Veer data

 

 

Method Test set lO-fold CV
Literature“ 0.31 &0.632 0.2194129
Proposed 0 .00 0.08

 

"Performance of other methods in the literature, by the same validation procedures
used in this article. A full list of literature results can be found in Supplementary
Table 52.

whatsoever from the test samples, the proposed method is free
from both types of biases. The error rates of the 10 training and
test samples are shown in Figure 6.

In all 10 CV experiments, the error rates on the test sample
generally decline as more classiﬁers are added to the classiﬁca—
tion rule. Since the classiﬁcation rule is constructed without using
any information from test samples, this indicates that the pro—
posed method does not have overfitting problems.

4.2 Biological significance of features selected

To see whether or not the identiﬁed genes are biologically mean—
ingful, we examine the gene modules obtained from the training
set of van’t Veer. There are 18 gene modules containing 64 genes
yielding a perfect error rate on the 19 test cases (Fig. 5). Among
the 64 genes, we found that 18 of them have been reported as
breast cancer associated (genes having at least one piece of mo—
lecular evidence from the literature) by G2SBC database while
others have protein folding functions. Such a result is signiﬁcant,
considering our method does not use any prior biological infor—
mation and selects genes based on statistical analysis only.

The G2SBC database contains 2166 genes and 903 of them are
among the 4918 genes we adopted from (Tibshirani and Efron,
2002). Thus the proportion of disease—associated genes in our
gene pool is 903/4918 = 0.184, whereas the proportion of
disease—associated genes in our classiﬁcation rule is 18/64:
0.281. This result is signiﬁcant with P—value 2.3%. With a sizable
proportion of replicated genes, it is not unreasonable to expect
some of the remaining protein folding genes which have not been
reported are new risk factors of breast cancer.

The gene modules and component genes’ biological functions
can be found in Supplementary Table S3. We will elaborate on
two gene modules here (Table 3). The ﬁrst one has the highest
I—score among all 18 modules. It consists of 5 genes, 2 of them,

 

2839

ﬁle'spzumol‘pmjxo'sopeuuonuoiq/ﬁdnq

 

00

GCV 1 GCV 2 GCV 3 GCV 4 GCV 5

WWW

W :..; ..a
GCV 7 GCV 8 GCV 9 GCV 10

   

/310'S[BHm0[pJOJXO'SOIJBIIIJOJIIIOIq/ﬂdnq

Interaction-based feature selection and classification for high-dimensional biological data

 

 

 

 

 

 

 

 

 

Train Test
<1-
g — o
o
.“3 g _ 
S co 9 q. o oo
.5 Q —oooooooooooooooo B Q —o \o_
L. O L- ‘r o oo
:1) a) _ \
V ooot
‘2 - 000
I I I o I
5 10 15 5 10 15

# of variable modules used # of variable modules used

Fig. 7. Error rate paths of our classiﬁcation rule for Golub dataset

leukemia (AML). The dataset was analyzed in the same way as
that for (Sotiriou et al., 2003). Our classiﬁcation rule consists of
52 genes in 16 modules and correctly classiﬁes all test cases. That
is, the error rate is zero (Fig. 7). The names and biological func—
tions of these 52 genes are listed in Supplementary Table S4.
The dataset appears to have strong marginal effects and a few
existing methods yield very low classiﬁcation error rates when
applied to this dataset (Fan and Lv, 2008; Zou and Hastie, 2005).
One reason for including this dataset is to show that even though
designed to detect interaction and module effects, the proposed
method would not miss signiﬁcant marginal effects either.

5 CONCLUSON

To deal with the tremendous complexity created by interactions
among variables in high—dimensional data, the proposed method
provides a ﬂexible framework to extract different types of infor—
mation from the data in three stages. First, variables are selected
with high potential to form inﬂuential modules. The dimension
of the data is drastically reduced in the first stage. Secondly,
highly inﬂuential variable modules are identiﬁed from variables
selected in the ﬁrst stage. Since there is only a small number of
variables in each variable module, the interaction and module
effects can be accurately estimated by existing methods
well—suited for low or moderate dimensional data. The third
stage constructs classiﬁers and combining them into one ﬁnal
classiﬁcation rule. The prediction errors of the proposed
method outperform all other methods that ignore interactions.
It also has the advantage in identifying relevant genes and their
modules. In summary, our article is intended to send three mes—
sages: (i) classiﬁcation rules derived from the proposed
method will enjoy substantial reduction in prediction errors;
(ii) inﬂuential variable modules with scientiﬁc relevance are iden—
tiﬁed in the process of deriving the classiﬁcation rule and
(iii) incorporating interaction information into data analysis
can be very rewarding in generating fruitful scientiﬁc knowledge.

DATABASES

(1) Gene—to—system breast cancer database: http://www.itb
.cnr.it/breastcancer/php/browse.php#molecular_top

(2) NCBI gene database: http://www.ncbi.nlm.nih.gov/gene

(3) Breast cancer database: http://www.breastcancerdatabase
.org/

Funding: Hong Kong Research Grant Council (642207 and
601312 in part to I. H.); NIH (R01 GM070789,
GM070789—0551 and NSF grant DMS—0714669 in part to
S—H. L. and T. 2.).

REFERENCES

Agresti,A. (1996) An Introduction to Categorical Data Analysis. Wily, New York.

Beketic—Oreskovic,L. et al. (2011) Prognostic signiﬁcance of carbonic anhydrase IX
(CA—IX), endoglin (CD105) and 8—hydroxy—2/—deoxyguanosine (8—OHdG) in
breast cancer patients. Pathol. 0ncol. Res., 17, 593%03.

Benjamini,Y. and Hochberg,Y. (1995) Controlling the false discovery rate—a prac—
tical and powerful approach to multiple testing. JRSS B, 57, 2897300.

Breiman,L. (2001) Random forests. Mach. Learn., 45, 532.

Carlborg,O. and Haley,C.S. (2004) Epistasis: too often neglected in complex trait
studies. Nat Rev. Genet, 5, 618r625.

Chernoff,H. et al. (2009) Discovering inﬂuential variables: a method of partitions.
Ann. App]. Stat, 3, 133571369.

Cordell,H.J. (2009) Detecting gentLgene interactions that underlies human diseases.
Nat Rev. Genet, 10, 3924104.

Dash,M. and Liu,H. (1997) Feature selection for classiﬁcation. Intel. Data Anal., 1,
1317156.

Dietterich,T.G. (2000) Ensemble methods in machine learning. In Kittler,J. and
Roli,F. (eds.) First International Workshop on Multiple Classifier Systems,
Lecture Notes in Computer Science.(.). Springer, New York, pp. 1715.

Efron,B. et al. (2001) Empirical Bayes analysis of a microarray experiment. JASA,
96, 115171160.

Fan,J. and Lv,]. (2008) Sure independence screening for ultrahigh dimensional fea—
ture space (with discussion). J. R. Stat. Soc. Ser. B, 70, 8497911.

Freund,Y. and Schapire,R. (1997) A decision—theoretic generalization of online
learning and an application to boosting. J. Comput Sys. Sci., 55, 1197139.
Golub,T.R. et al. (1999) Molecular classiﬁcation of cancer: class discovery and class

prediction by gene expression monitoring. Science, 286, 5317537.

Guyon,I. et al. (2002) Gene selection for cancer classiﬁcation using support vector
machine. Mach. Learn., 46, 3894122.

Guyon,I. and Elisseeff,A. (2003) An introduction to variable and feature selection.
JMLR, 3, 115771182.

Hall,P. (1988) The Theory of Coverage Process. Wily, New York.

Khan,A.I. et al. (2011) Negative epistasis between beneﬁcial mutations in an evol—
ving bacterial population. Science, 332, 119371196.

Kooperberg,C. et al. (2010) Structures and assumptions: strategies to harness gene x
gene and gene x environment interactions in GWAS. Stat. Sci., 24, 4724188.

Liu,H. and Yu,L. (2005) Toward integrating feature selection algorithms for clas—
siﬁcation and clustering. IEEE Tran. Know] Data Eng, 17, 4917502.

Lo,S.H. and Zheng,T. (2002) Backward haplotype transmission association algo—
rithmia fast multiple—marker screening method. Hum. Her., 53, 1977215.
Moore,J.H. and Williams,S.M. (2009) Epistasis and its implication for personal

genetics. Am. J. Hum. Gen., 853, 3097320.

Perou,C.M. et al. (2000) Molecular portraits of human breast tumours. Nature, 406,
7477752.

Pinheiro,C. et al. (2011) GLUTl and CAIX expression proﬁles in breast cancer
correlate with adverse prognostic factors and MCTl overexpression. Histol.
Histopathol., 26, 127971286.

Saeys,Y. et al. (2007) A review of feature selection techniques in bioinformatics.
Bioinformatics, 23, 250772517.

Schwender,H. and Ickstadt,K. (2008) Identiﬁcation of SNP interactions using logic
regression. Biostatistics, 9, 1877198.

Shao,H. et al. (2008) Genetic architecture of complex traits: large phenotypic effects
and pervasive epistasis. Proc. Natl Acad. Sci. USA, 105, 1991319914.

Sotiriou,C. et al. (2003) Breast cancer classiﬁcation and prognosis based on gene
expression proﬁles from a population—based study. Proc. Natl Acad. Sci. USA,
100, 10393710398.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. R. Stat.
Soc. Ser. B, 58, 2677288.

Tibshirani,R. and Efron,B. (2002) Pre—validation and inference in microarray. Stat.
Appl. Genet. Mol. Biol., 1, Article 1.

 

2841

ﬁle'spzumol‘pmjxo'sopeuuonuoiq/ﬁdnq

H. Wang et al.

 

van’t Veer,L.J. et al. (2002) Gene expression proﬁling predicts clinical outcome of
breast cancer. Nature, 415, 533536.

Yuan,M. and Lin,Y. (2006) Model selection and estimation in regression with
grouped variables. J. R. Stat. Soc. Ser. B, 68, 49%7.

Zawistowski,J.S. et al. (2002) KRITl association with the integrin—binding protein
ICAP—l: a new direction in the elucidation of cerebral cavernous malformations
(CCMl) pathogenesis. Hum. Mol. Genet, 11, 3897396.

Zhang,J. et al. (2001) Interaction between kritl and icaplalpha infers perturbation
of integrin betal—mediated angiogenesis in the pathogenesis of cerebral cavern—
ous malformation. Hum. Mol. Genet, 10, 295372960.

Zhang,H. et al. (2006) Gene selection using support vector machine with non—
convex penalty. Bioinformatics, 22, 88785.

Zhu,J.X. et al. (2008) On selection bias with prediction rules formed from gene
expression data. J. Stat. Plann. Infer., 138, 37¢386.

Zou,H. and Hastie,T. (2005) Regularization and variable selection via the elastic
net. J. R. Stat. Soc. Ser. B, 67, 3017320.

Zuk,O. et al. (2011) The mystery of missing heritability: genetic interactions create
phantom heritability. Proc. Natl Acad. Sci. USA, Early Edition, 1%.

 

2842

/810'sleum0prOJxo'sopBuIJOJuioiq”:duq

