ORIGINAL PAPER

Vol. 29 no. 20 2013, pages 2625-2632
doi:1 0. 1093/bioinfonnatics/btt436

 

Systems biology

Advance Access publication July 29, 2013

Near-optimal experimental design for model selection

in systems biology

Alberto Giovanni Busett01‘2'*

Sotiris Dimopoulosz'4'5, Cheng Soon OngB, Jdrg Stelling‘“5 and Joachim M. Buhmann

, Alain HauserS, Gabriel Krummenacher‘, Mikael Sunnéker2'4‘5,

1,2

1Department of Computer Science, EI'H Zurich, 2Competence Center for Systems Physiology and Metabolic Diseases,
3Department of Mathematics, ETH Zurich, 4Department of Biosystems Science and Engineering, ETH Zurich,
5Swiss Institute of Bioinformatics, Zurich, Switzerland and 6National lCT Australia, Melbourne, Australia

Associate Editor: Igor Jurisica

 

ABSTRACT

Motivation: Biological systems are understood through iterations of
modeling and experimentation. Not all experiments, however, are
equally valuable for predictive modeling. This study introduces an
efficient method for experimental design aimed at selecting dynamical
models from data. Motivated by biological applications, the method
enables the design of crucial experiments: it determines a highly
informative selection of measurement readouts and time points.
Results: We demonstrate formal guarantees of design efficiency on
the basis of previous results. By reducing our task to the setting of
graphical models, we prove that the method finds a near-optimal
design selection with a polynomial number of evaluations. Moreover,
the method exhibits the best polynomial-complexity constant approxi-
mation factor, unless P=NP. We measure the performance of the
method in comparison with established alternatives, such as ensem-
ble non-centrality, on example models of different complexity. Efficient
design accelerates the loop between modeling and experimentation: it
enables the inference of complex mechanisms, such as those control-
ling central metabolic operation.

Availability: Toolbox ‘NearOED’ available with source code under
GPL on the Machine Learning Open Source Software Web site
(mloss.org).

Contact: busettoa@inf.ethz.ch

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on May 6, 2013; revised on July 10, 2013; accepted on
July 24, 2013

1 INTRODUCTION

At the present level of development, investigations in biology
require setting up complicated and expensive experiments
(Kitano, 2002). Advances in measurement techniques prompted
the recent growth of detailed mathematical models, which cap-
ture biological phenomena at different levels of detail. However,
the employment of novel measurement techniques by itself is
insufﬁcient to achieve high predictive power. Experimental
design provides the necessary guidance to determine crucial
observations. Often, in fact, an important task is the selection
of the most informative experiments. In systems biology,

 

*To whom correspondence should be addressed.

dynamical models express causeieffect relations between inter-
acting components (Kitano, 2002). Designing optimal experi-
ments for parameter estimation is challenging, but also well
studied. At present, there already exist conclusive results and
ready-to-use procedures (Bandara et al., 2009; Faller et al.,
2003). In contrast, modern research often consists of discrimi-
nating between alternative models (Box and Hill, 1967; Kuepfer
et al., 2007), a task for which several questions remain open
(Faller et al., 2003; Kreutz and Timmer, 2009; Myung
and Pitt, 2009). Design optimization for the selection of dynamic
models proves especially challenging in the presence of non-
linear behavior (Balsa—Canto et al., 2008; Kitano, 2002).
In classical statistics, ensemble non-centrality constitutes the ref-
erence technique to design experiments for model selection
(Atkinson and Fedorov, 1975; Ponce De Leon and Atkinson,
1991; Skanda and Lebiedz, 2012). Recently, Bayesian tech-
niques have been applied with success to neuroimaging and
biochemical modeling (Busetto et al., 2009; Daunizeau et al.,
2011; Kramer and Radde, 2010; Liepe et al., 2013; Steinke
et al., 2007). Existing methods are primarily limited by compu-
tational bottlenecks, as optimization is often practically
intractable.

This study introduces an efﬁcient method to design inform-
ative experiments for selecting biological dynamical systems.
Building on previous results (Krause and Guestrin, 2005),
we go beyond current limitations by constructing a method
that yields near-optimal combinations of time points and
measurable readouts. Formal efﬁciency guarantees of the
method are proved by reduction to a well-studied general setting
(Feige, 1998; Krause and Guestrin, 2005; Nemhauser et al.,
1978). The method is generally applicable and has been primarily
motivated by questions arising from the biological domain. We
empirically evaluate the performance of the method with models
of glucose tolerance and cell signaling. We apply the method
to address challenging open problems of biological and medical
relevance.

The manuscript is organized as follows. We start by introdu-
cing relevant facts and notions to be used in the rest of the article.
Theoretical results are followed by empirical evaluation
and numerical comparison with competing techniques. Finally,
the method is evaluated and veriﬁed with glucose tolerance and
cell signaling. Further details are presented in the Supplementary
material.

 

© The Author 2013. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which
permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

112 /310's112u1n0[p1q1x0"sotwurJOJutotq/ﬁduq wort popcorn/hog

9103 ‘Og anﬁnv uo ::

A.G.Busetto et al.

 

2 BACKGROUND

We distinguish three entities: the studied system, the researcher
and the measurement apparatus. The system is modeled by the
researcher, who learns from the data and designs experiments by
tuning the measurement apparatus. In this study, learning and
reasoning follow the rules of probability theory (Baldi and Itti,
2010). Let admissible conﬁgurations of the system be called
states x(t) e X g IR". States are time-varying representations
evolving over time t e T g IR. We deﬁne the ‘true model’
as f *, the function that governs the evolution of the system.
Modeling with systems of ordinary differential equations
(ODEs), we have

 

diff) = *(x(t),6) (1)
with a certain known initial condition x(t0). The function f *
deﬁnes how infinitesimal state increments depend on current
states and parameters 6 6 O Q Rd of the system. In biochemical
and physiological applications, each state component quantiﬁes
molecules, concentrations or other physiological measures. In
practice, parameters consist of acceptable values for reaction
rates and other kinetic constants (Kitano, 2002; Zhong et al.,
2012). Calculating the trajectory of the system in Equation (1)
from a certain starting point is an initial value problem (IVP).
The ‘true model’ f * and its parameters are unknown to the
researcher.

The goal of modeling is to select the most predictive model,
and to estimate parameters and initial conditions. In this study,
model selection is inference, that is deductive learning from data.
The lack of knowledge of the researcher is not absolute. First, the
researcher has access to a set of candidate models, which we call
the hypothesis class 7-". We denote a generic candidate model as
f e 7-". The ‘true model’ is not necessarily a candidate model
available to the researcher. Let us call the scenario in which
f* e .7: as realizable, and non-realizable otherwise. This study
considers both realizable and non-realizable scenarios. Second,
the researcher benefits from previous experiments, published re-
sults and domain knowledge. All these pieces of information
form the a priori knowledge, that is the prior probability p(f).
Such probability is deﬁned over the candidate models before
observing the data.

Experimental measurements consist of readouts

Kb) 3: [111(0): ---:yn(ti)]Te IR" (2)

obtained through sampling. Sampling can be performed at
arbitrary time points t1, ..., ts. We denote the range of indexes
for time points as S 2: {1, . . . ,s} and the range for the readout
variables as N 2: {1, ...,n}, such that the index pair
(1‘, j) e S x N refers to the individual measurement

iji) I: iji) + 817: (3)

whose noise is denoted by 8,]. Noise terms are independent
random variables sampled from known distributions Nij.
Individual measurements can be grouped into datasets

Yﬂ;:{yj(t,-)GIR”:(i,j)err§8x/\/}, (4)

whose elements are deﬁned by the indexes in experiment 71, which
is, more generally, a multiset. Adopting the Bayesian viewpoint,

a priori a posteriori

belief state belief state

I I update
0 (tow gain)

"I 53 V
f f 3
update
0 .11- ,mi in o
f E f .5 ti 5“ ti 5‘
Fig. 1. This example compares probability updates for four models.
The updates are induced by two different datasets. On the top, both
initial and ﬁnal belief states are uninformative: the update yields low
information gain. This is in contrast to the bottom plot, which shows
a highly informative update: starting from an uninformative prior, the

posterior concentrates the probability mass on a single model (Busetto,
2012)

high uncertainty high uncertainty

0-11.
gigs?

probability
probability

"Inger?
"has! 7

high uncertainty low uncertainty

probability
probability

the researcher performs inference by calculating the probability
of the models given the data, as visualized in Figure 1. The pos-
terior probability is related to priors and likelihood through
Bayes’ rule

likelihood prior
posterior

WA
WI? ) :P(Yn|f) 170‘) : p(Y,,|f)p(f)
7' P<Yn> gamma)

 

(5)

Probabilities are revised and updated for each model in .7: as
more evidence is accumulated. The likelihood function p( Yﬂl f)
is the probability of generating a speciﬁc instance of the data
with a candidate model. By construction, measurements are
conditionally independent given the model, and hence the likeli-
hood factorizes as

pour): Fl PCYjUth) (6)

(i,j)err

for given 6. Because of conditional independence, posteriors
from previous inference are priors for subsequent experiments.
This property is useful when single experiments do not yield
sufﬁcient evidence, but sequences might provide conclusive re-
sults. In practice, this advantage might prove essential to select
predictive models 0(u et al., 2010). Here, the primary aim is to
select models, not parameters. Nonetheless, it is useful to assume
a certain degree of uncertainty regarding the parameters. The
model posterior is such cases obtained by marginalizing over
the parameters

17(Ynlf, (9)190”, 6)

pm.) d6 (7)

p(len) = pmei Yoda = 

 

2626

112 /310'S[BHJHOIPJOJXO'SOIJ’BLUJOJIIIOICI”Zduq uteri pQPBOIIIAAOG

9103 ‘Og isnﬁnv uo ::

Near-OED for model selection in systems biology

 

Note that models with alternative parameter values and initial
conditions can be treated as alternative models. The probability
of each state follows the drift equation

317060))
at

where V- denotes the divergence operator. The equation deter-
mines the evolution over time of the state uncertainty.
Conceptually, it constrains the probability of observing a certain
state in the future on the basis of the dynamical properties of the
system. Equation (8) can be extended to the parameter space to
perform inference (Busetto and Buhmann, 2009a; Busetto et al.,
2009). Figure 1 schematically illustrates Bayesian inference with
two updates from prior to posterior probabilities. In the
example, the hypothesis class consists of |.7-'| = 4 models.
Informative probability distributions exhibit ‘narrow’ peaks, as
they concentrate substantial mass on few models. The smaller the
subset of models, the higher is the informativeness, as the data
discard all other candidates. In contrast, ‘ﬂat’ distributions indi-
cate high uncertainty and no preference for a speciﬁc selection of
models. This intuition is formalized by information theory,
which offers Shannon entropy as a fundamental measure of
uncertainty (Cover and Thomas, 2012).

For the purpose of learning, the researcher is not only inter-
ested in the uncertainty expressed by probabilities at a speciﬁc
point in time. In contrast, the aim is to maximize the information
gain, that is the additional amount of valuable information pro-
vided by new data. Figure 1 illustrates the concept with two
examples. In the update on the top, the information gain is
low because the posterior is almost identical to the prior. In
contrast, the update on the bottom shows an informative pos-
terior obtained from an uninformative prior. Hence, the infor-
mation gain is high: the assimilated dataset yields a substantial
decrease in uncertainty. At this point, the question is how to
measure the gain in information. The gain yielded by a dataset
is given by the relative entropy (also known as KullbackiLeibler
divergence) between prior and posterior probabilities (Baldi and
Itti, 2010; Liepe et al., 2013)

 

= V - [f(x(t), 9)p(x(t))1 (8)

 

DKL[p(fl Yr!) || 1909] = Zp(f| Y,) iog/(f' Y”) (9)

M 170‘)

In the context of modeling, the relative entropy has a precise
interpretation based on the analogy between learning and com-
munication. The information gain corresponds to the expected
number of extra bits that are lost if the dataset Y,r is neglected.
As highlighted by the example, information gain is thus a data-
dependent quantity. The example in Figure 1 shows that high
gain is obtained when probabilities strongly revise the belief of
the researcher, that is when extraordinary evidence is incorpo-
rated. Because it depends on the future outcome Y,r of the
experiment, the gain is a quantity unknown a priori to the
researcher. Nonetheless, prior probabilities and likelihoods are
enough to predict its value in expectation. Formally, information
gain can be maximized in expectation, where the expectation is
taken over all possible outcomes of the experiment. To reﬂect the
a priori information and the known properties of the models,
information gain is weighted according to the respective meas-
urement probabilities.

3 THEORETICAL RESULTS

The objective of our experimental design is to maximize the in-
formation gain in expectation, that is the mutual information

1(Y7'nf) = [EYNIDKLII’UI Yr!) || P031] (10)

for the experiment 71' g S x N. The task of optimal design is

select 71* e arg max I(Y,,,f) (11)

TISSXNZITIIfK

The budget K e N is determined by the researcher and constrains
the maximum number of allowed measurements (Busetto et al.,
2009). In practice, the design always selects the maximum
allowed number of measurements, thus justifying the choice of
a limited budget. The incorporation of extra measurements, in
fact, invariably adds non-negative contributions to the informa-
tion obtained from the experiment. As an objective, mutual in-
formation measures the expected ability of a model to predict the
data. Such an objective is not only appealing to intuition, but
also theoretically justiﬁed (Cover and Thomas, 2012), and
strongly supported by evidence (Baldi and Itti, 2010). The intro-
duced method for optimal design jointly selects with 71' two
aspects of the design: time points (when to measure) and read-
outs (what to measure).

The method starts by solving the IVP for each candidate
model in 7-". Then, it proceeds with the optimization, which con-
sists of maximizing the objective with the maximum budget of K
measurements (Busetto, 2012). The experimental outcomes
are averaged and weighted to estimate the expected information
gain of the particular experiment under evaluation. For compu-
tational efﬁciency, optimization is performed greedily: observa-
tions are incrementally added to construct the near-optimal
approximation 7? of the optimal design 71*. Given p( f ), K,
x(t0), 6, and by initializing 710 = 0, the process of optimization
proceeds as follows. Iterating over k from 1 to K,

71k = 711,21 U arg max I(Y,,U{(,-,j)},f) (12)

(i,j)e$><N\rrk,1

The procedure yields the final approximation 7": = 71K of 71*. The
formal worst-case performance guarantees for the method are
obtained on the basis of previous results for submodular opti-
mization in the context of active learning (Feige, 1998; Krause
and Guestrin, 2005; Nemhauser et al., 1978). The proof is based
on a reduction to the more general setting of graphical models
(Krause and Guestrin, 2005), which in turn builds on previous
approximation bounds for submodular optimization (Feige,
1998; Nemhauser et al., 1978).

THEOREM. The greedy method that selects up to K informative
readouts and time points to discriminate dynamical systems
yields the near-optimal design 7? such that

Ice/020$) max 1(me) (13)

e TISSXNZITIIfK
with a polynomial number of evaluations of the objective; more-
over, such a constant approximation factor is the best in polyno-
mial time, unless P: NP.

Informally, the theorem states the following: selecting the op-
timal experiment might be hard, and yet it is possible to easily

 

2627

112 /310's1izu1n0fp101x0"sotwurJOJHtotq/ﬁduq mot} p9p1201um0q

9103 ‘0g isnﬁnv uo ::

A.G.Busetto et al.

 

select experiments that are provably near-optimal. It is worth
noting that the yielded information is always guaranteed to be
at least (1 — e’l)>63% of the optimal value, that is the total
experimentally achievable information. Furthermore, the empir-
ical results introduced in the next section demonstrate that in
practice, it is possible to achieve even better results in cases of
concrete interest. From the computational point of view, each
evaluation of the information gain requires the calculation of the
posterior, which in turn requires the integral solutions of the
systems of ODEs. For non-linear systems, closed-form solutions
are typically unavailable (or might not even exist), thus one has
to numerically approximate the solutions. Calculating the pos-
terior is, however, as tractable as filtering for system identiﬁca-
tion. For efficiency, Sequential Monte Carlo (SMC) methods
and unscented Kalman filtering may be used to perform approxi-
mate inference (Doucet and Tadié, 2003). Whereas the former
technique is more general and able to deal with arbitrary multi-
modal distributions (Busetto and Buhmann, 2009b), the latter is
particularly advantageous in the case of unimodal distributions.
Approximate Bayesian computation might further extend the
scope of applicability of the method (Sunnaker et al., 2013b).
For further details and comparison of SMC and filtering
approaches, see ‘Comparison of Different Methods for
Uncertainty Propagation’ in Supplementary Material.

4 EMPIRICAL AND APPLIED RESULTS

This section reports empirical and applied results in the domain
that motivated this study: systems biology (Busetto, 2012; Hauser,
2009; Krummenacher, 2010). First, we verify the introduced
method on the Bergman glucose tolerance model. We perform
frequency and time point selection, showing that near-optimal
solutions yield tight approximations of the global optimum (and
provide similar designs, too). Second, we identify the most inform-
ative readouts to elucidate the pathway for Target-of—Rapamycin
(TOR) signaling from hundreds of candidate models. Third,
results are compared with other established design techniques.
The results are particularly relevant to experimentalists interested
in understanding metabolic control operation.

4.1 Dynamics of glucose tolerance

The Bergman glucose tolerance models constitute the ﬁrst sys-
tematic attempt aimed at explaining the role of insulin in the
degradation of blood glucose (Bergman et al., 1979). This class
of phenomenological models aims at identifying the mechanisms
involved in reduced glucose tolerance in patients suffering from
diabetes mellitus. Bergman’s models constitute a set of empirical
models, regarded as the conventional reference for modeling glu-
cose homeostasis (Kovacs et al., 2010). The models are highly
predictive, well understood and non-linear. Figure 2 highlights
the different structural properties of the models, and Figure 3
exempliﬁes their glucose dynamics.

Figure 4 shows the normalized information yielded by glucose
sampling frequencies in the range between 0 and 1 samples/min.
More than 90% of the experimentally available information is
already reachable at the uniform sampling frequency of 1/300
Hz (0.2min_1). Also with respect to growing frequency, the
mutual information follows a law of diminishing returns and,

NH, .\liirIvl 0131-? and Drilillllt‘li‘r‘

{': =ii.(: - in

I’Jit A'i =i' !.[lll.L[||l,“I|]'
0.1:}! :33; I III

I (E Z :i.(.‘ _ all + it,

' {hi—b. ,r—Mitiil'.) IIJ
tl_, _ it} _ £13.") -- Lil: iii 3
‘ {Ii—l" —||.l‘..\.l

1.2 1“ i'; —:).(.‘—(i-_-_\' +11.
l l" ‘- _ -
I A 4m +i
k ii, = 7i, = (i;:1ii::i.7:ii lU'
‘ ri_. = it-ylc, = viii): lJ-Si in -

1' t
—- ' i1, : i‘ z 5.07:0.71:
1,

l ri,_—i-, _t—iillttllli) II]

in i.“ i'.‘ — ((11 _\'\(,' . ii,
l - r . . l
, , _\ —ra,..\ +th
1' ‘ . X — list-.-

tli - TI-I'l+III'1]_{’-I[III:II,UT:I Ill '-’
ti.- 7 —i. 7 -:—‘.).lil': l,.3[|‘,- iii "-‘
' — .t, ti: — I‘:ili':—-.j -— rm): Ln'zn lU 
fl. = i!” =l12:0.Tl

 

 

 

 

 

 

H -‘ i-i

inor
‘ .
1:
“j
[n i ipli

Fig. 2. Insulin-dependent models of glucose metabolism (Bergman et al.,
1979). P is the hepatic glucose production rate. I is the plasma insulin
concentration; its time course is not determined by the ODEs, but sup-
plied to the models. I’ is the insulin concentration in a compartment
remote from plasma. Models IV and V assume a constant production
rate of glucose (G); in model VI, this rate is assumed to be dependent on
insulin concentration. Model VI also accounts for the disappearance of
glucose into peripheral tissues (‘periph.’)

(‘niiu-iiii-iiiiori int-Jill
'33“

 

 

 

.\I\tlr\ll . - \. .\lrtl‘l [1'
leU k: I x " .3 I I V rIl' U: Hi 'iii
in] ‘ C‘\\ " t — e — ‘Iﬂm'nm—T
\ .. 7 ___.: 1,
int: _ _ _ - \ ‘7— — _,- Mu,“
i  t _ _ _ _ _ l ' “M
iii - _,,‘
. _ Mr"
: -i._"
t,_ Mini-i \= 3‘3 Mini-H1 t
2till e, - -
\ ’\ i I-

I’m!

\~,\ .
[EH1 \ .\ - \ ' ,-—
_ \L‘ _ 7 , _ _ \S‘Hl: __ L—__ I ._ 1 i _n "l‘ Iii iii lil‘

,ill \ -k_ _ —7—'  _ _ _ _ .

_ 7 - iiim- |miiij

 

 

 

Ill] '20 til it _ ‘ EU ll] till
Tmu- iu-ini
Fig. 3. On the left, mean values (solid lines) and standard deviation of the
distributions approximated by the unscented transform (dashed lines) of
the glucose measurements predicted by models I, IV, V and VI. On the
right, the mutual information (normalized by the entropy) for each time
point

1 .Aar n *- '+C—.’)-’3—C—é-’9’—€J
'_ ﬁjﬁvmxmﬁﬁ

9369‘"

0.9 — —  d — — — — — — — — — — — — — — — — — — — — —

0.8 r;
0.? 

0.6 rh

Normalized information

0.5 .‘

0‘4 l I I l I I I I I I
0 0,1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Measurement frequency [1lrriin]

Fig. 4. For the identiﬁcation of glucose tolerance dynamics, 90% of the
experimentally available information (dashed line) can be obtained with a
sampling frequency of 0.2 minTl. Higher sampling rates yield negligible
contributes to physiological modeling. Standard errors are too small to be
drawn (81.44> 10’2 bits)

 

2628

112 /310's1izu1n0fp101x0"sotwurJOJHtotq/ﬁduq mot} p9p1201um0q

9103 ‘0g isnﬁnv uo ::

Near-OED for model selection in systems biology

 

consistent with the expectation from Figure 3, it grows rapidly
and then saturates. The theoretical maximum of log2 |.7-'| = 2 bits
is rapidly approached for frequencies above 1/400 Hz
(0.15min_1). We also consider the case in which a glucose injec-
tion is performed as a physiological intervention. We measure
the information at each individual time point to find the most
informative time interval. In fact, it is possible to consider the
heuristic approach of measuring with a sample frequency that is
local rather than uniform and constant. The most informative
region does not coincide with the beginning of the glucose deg-
radation, but rather with the initial transition towards the steady
state, as visible in Figure 3; the maximum of the information is
reached at approximately 30 min from the injection. After the
tipping point, the informativeness decreases while the system
ﬁnally reaches the steady state. After that, residual information
comes exclusively from the heterogeneous steady levels of
glucose. Information is estimated with unscented propagation,
which outperforms linear and SMC approximations (details in
the Supplementary Material). For standard errors of 10’2 nats
(81.44- 10’2 bits), the unscented approximation is between 40
and 400 times faster than that obtained with particles (which
require storage and update of at least 104 samples).

By selecting quintuplets from a pool of 20 time points, it is
possible to estimate how close the near-optimal design is to the
optimal. Optimal solutions are calculated by exhaustive search,
which is extremely time-consuming, as it requires the evaluation

of  > 104 experiments. Table 1 compares optimal and near-

optimal designs for K = 3,4, 5. Notably, near-optimal solutions
are effectively indistinguishable from the optimal ones in all
cases. Not only the yielded information is practically the same
(below error tolerance), but also the selections differ by a single
sample over K.

As a consequence, optimal and near-optimal design exhibit
indistinguishable probability of selecting the ‘true model’ from
the data. For all practical purposes, the near-optimal selections
are optimal.

4.2 TOR pathway

The TOR pathway is a highly conserved cell signaling structure,
whose mammalian homolog is implicated in cancer, cardiovas-
cular diseases, autoimmunity and metabolic disorders (Kuepfer
et al., 2007). For budding yeast, a set of 18 elementary extensions
have been previously proposed in combination with a consensus
core model (Kuepfer et al., 2007). The elementary extensions
incorporate a set of additional reactions. Combined with the
core model, they represent putative mechanistic conﬁgurations
of the biochemical system.

The core model consists of experimentally validated molecular
interactions from inhibition of TOR kinases to the activation of
protein phosphatase 2A (PP2A). In principle, the elementary ex-
tensions are not mutually exclusive (Raman and Wagner, 2011).
In the evaluation, the hypothesis class .7: consists of 200 model
prototypes. Each hypothesis corresponds to a system of ODEs
with heterogeneous model complexity (from individual reactions
to interlocked non-linear feedback). All 24 shared chemical spe-
cies are considered measurable quantities for the experimental
design. Readout selection is performed with a maximum of

Table 1. Expected information gain for subsets of measurement time
points of different cardinality K for the insulin-dependent models of
glucose metabolism

 

 

K I? (near-optimal) 21* (optimal) I( Y;,, f) I( Yni , f)
3 {31, 34, 37} {34, 37, 40} 1.0004 :l: 0.004 1.0009
4 {13,31,34, 37} {10, 34, 37,40} 1.0910:l:0.004 1.0940

5 {10,31,34,37,40} {10,34,37,40,43} 1.1564:l:0.016 1.1585

 

Note: The measurement time points are selected from <6O/3) candidates from the
tons p

set S = {1,4, ...,60}. Optimal and near—optimal solu actically coincide.

Mutual information

  

 

IIIIIIIIlII IIIIIIlI
°°q1§3063333r¥ar$$$°395
ﬁNodogﬂgﬁa..o§:oﬂciﬁao
a.,"..<.. :1 a" a n
signagaesetgtetﬁgiﬁeg
llmlmlagﬁl—&%§ml EEgmiiﬂagmi
ndﬁagal E'nI—n. ii: 0. Ian.
were *3' i 5 £33
E‘Eeu 3 ea it; .. it...
i—ﬁ '13 FE i- I- '_§_I-
g. i— i—
._

Fig. 5. Expected information gain for increasingly large sets of selected
measurements (green), each consisting of jointly selected species and time
points. Online and ofﬂine bounds appear in blue and red, respectively

s: 50 regularly spaced time points in a relative time scale from
0 to 1.4 [time units of (Kuepfer et al., 2007)]. Uniform spacing has
been chosen for simplicity of description; the design method is
directly applicable to any distribution of the time points. In this
setting, the number of candidate experiments amounts to
IS x NI 2 1200. In Figure 5, the expected information gain is
plotted as a function of the incremental design 71k as in
Equation (12), together with bounds showing tightness of
approximation. The ofﬂine bound is calculated by multiplying
for the approximation factor e/ (e — 1) B 1.58 and is thus avail-
able a priori. The online bounds, in contrast, are iteratively calcu-
lated by using submodularity to bind the additive improvements
of the objective from the current selection. The bound is

1(Yrr*sf) 5 1(anf) + i 8w, 
[:1

where the incremental value is 8,, :=I(Y,,U{w}, f) — I(Y,,, f) for
each of the top q measurements w not considered yet (Krause
and Guestrin, 1999/2007). The optimal information value is,
hence, always between the achieved objective and the bound.
Whereas ofﬂine bounds are trivial to compute, online bounding
requires few additional calculations, but is often preferable be-
cause it yields tighter bounds. Both bounds are useful to predict

 

2629

112 /310's1cu1n0fp101x0"sotwuiJOJHtotq/ﬁduq mot} p9p1201um0q

9103 ‘0g1sn8nv uo ::

A.G.Busetto et al.

 


E?
r‘t
g2
:15

 

Fig. 6. Diagram representing the individual mean mutual information
over time for each chemical species in the core of the TOR pathway
(Kuepfer et al., 2007). Information is measured in bits and also visualized
with colors ranging from blue to red

quasi-plateaux of information due to saturation effects, and to
evaluate the quality of the optimized (but not necessarily opti-
mal) design (Krause and Guestrin, 1999/2007; Minoux, 1978).
Tap42pP—PP2A exhibits the highest information content and is
thus the best discriminative candidate. Such a species is the com-
plex between PP2A and the phosphorylated protein Tap42p, an
essential protein of the TOR signaling pathway (Diivel et al.,
2003). The species is known for its central role, and yet there
exist substantial uncertainty regarding its precise interactions in
the biochemical network (Kuepfer et al., 2007). The information
associated with each species is represented by Figure 6, which
overlays the diagram of the core model with the mean informa-
tion over time.

The theorem states that the method dominates all other efﬁ-
cient techniques in terms of information yield. For completeness,
we also assess the performance with respect to the empirical suc-
cess rate, an external score. This measure is consistent with the
research goal of finding the best model and allows the comparison
of the greedy approach with the available non-Bayesian alterna-
tive, that is ensemble non-centrality (Atkinson et al., 2007).

We evaluate the method in two benchmark scenarios: realiz-
able and non-realizable. The success rate is the ratio of successful
selections over 103 runs. Model selection is considered successful
when the best model is selected a posteriori from the data
through the designed experiment. In the realizable scenario, the
best model is the true model f *, because this model is available as
a candidate. In the non-realizable scenario, however, the ‘true
model’ is not a candidate because f *¢ 7-". The best model then is
the closest one to the ‘true model’ in terms of predictive power
measured as relative entropy. In each test run, the method selects
noisy readouts from the TOR models. In turn, each candidate
model is assumed to generate data with additive independent
normal noise (standard deviation corresponding to half of the
concentration). On the left of Figure 7, near-optimal design
achieves a substantially higher success rate compared with
ensemble non-centrality. The evaluation highlights one of the

 

 

 

 

 

 

 

 

Realizable Non-realizable
' ' 0.15- -
1 “it + ~ m
|
0.9 r I
l
0.8 - ,,I,,,
1' "r
_ @<
0-7 i 0.1- > -
l ' '
g 0.6 I - g I
in w |
g 0.5 + e@ 7 g J_
8 S
m 0.4 r m
03 _ 0.05- -
0.2 I - _l_
| 7 _|_
0.1 I may
0 t , z z z z z z z m; z
i . 0- . . -
EIG ANC EIG EUD

Fig. 7. Comparison of success rates for the identiﬁcation of the TOR
pathway. Rates range from 0 (complete lack of success) to 1 (complete
success). Realizable (f * e .7-') and non-realizable (f *¢.7-') scenarios appear
on left and right plots, respectively. Expected information gain, ensemble
non-centrality and sum of Euclidean distances are, respectively, abbre-
viated as EIG, ANC and EUD. The plot on the right offers the inter-
pretation of relative success with respect to chance (dashed horizontal
line), as the maximal rate achievable for a given sample size is unknown

main practical disadvantages of ensemble methods: the huge
computational demands. Precisely, parameter fitting is the com-
putational bottleneck: the step is repeated for all tested param-
eter conﬁgurations against what is assumed to be the correct
model. Each iteration of cost minimization requires numerical
solutions of non-linear ODEs, testing every model combination.
This procedure is so resource-intensive that the hypothesis class
has to be limited to only four models with two unknown param-
eters and two unknown initial conditions. The exact computa-
tional complexity of the ensemble non-centrality is unknown.
However, it heavily relies on non-linear optimization, which is
generally considered hard or even intractable (Nelles, 2001). It is
possible, nonetheless, to calculate the number of non-linear

optimization tasks involved, which follows 0(|_7-'|2 If: p),

where p is the number of samples employed for the approxima-
tion of the integral solution. In contrast, the greedy approach is
bounded by 0(Kns) evaluations for the objective, which in turn
relies on the solution of |.7-'| uncertainty propagation equations
such as Equation (8). Combining ﬂow propagation and Bayesian
learning can be performed with the unscented Kalman filtering,
which requires the solution of 2(n + d) + 1 individual IVPs,
where d is the number of free parameters in 6. This number is
approximately proportional to the expected degree of the net-
work, which follows a Zipf distribution, making it independent
of network size (Szallasi et al., 2006). Detailed analysis and com-
parison with other filtering approaches is reported in
Supplementary Material.

The analysis proceeds with the non-realizable scenario, which
captures the fact that hypothesis classes are mere approximations
of reality. Ensemble non-centrality is not directly applicable in
this case because it assumes that the true model is among the
candidates (and performs selections with respect to it). Taking

 

2630

112 /310's1cu1n0fp101x0"sotwuiJOJHtotq/ﬁduq mot} p9p1201um0q

9103 ‘0g1sn8nv uo ::

Near-OED for model selection in systems biology

 

the best approximation as the correct model, one maintains the
same objective function based on the average residual sum of
squares. Results are reported on the right of Figure 7 for all
200 models and 50 time points. Calculations have been
performed with the submodular optimization toolbox for
Matlab (Krause, 2010). As in the realizable scenario, the intro-
duced approach yields signiﬁcantly higher success rates. In con-
trast to the realizable case, success should be seen as a relative
quantity, as the ﬁnite sample size induces an unknown scaling for
the maximal rate of practical success. The results also highlight
that multiple models achieve comparable predictive power and
are, thus, difﬁcult to exactly discriminate from the data.

5 CONCLUSION

In a complex field in which noisy data and expensive experiments
constitute the norm, it is crucial to guide experimentation
through rational design. Here, our main contribution is the intro-
duction of a method that guarantees high informativeness with a
polynomial number of evaluations of the information objective.
The main motivation of this study is biological, but it is worth
noting that the presented results for readout and time point
selection are applicable to general dynamical systems. As a con-
sequence of previous results from submodular optimization
(Feige, 1998; Krause and Guestrin, 2005; Nemhauser et al.,
1978), we could prove that the greedy method exhibits the best
constant approximation factor (unless P = NP) to design experi-
ments for the selection among alternative dynamical systems.

This study proves that entirely rational selections can be made
a priori with efﬁciency and solely on the basis of the accumulated
domain knowledge. Reported results show that near-optimal
experiments are effectively optimal in the application to glucose
tolerance. The method outperforms the available alternatives in
terms of empirical success rate, as shown for TOR modeling.

In a practical application, we used the method presented here
in a study revealing nuclear phosphorylation as the key control
mechanism for the transcription factor Msn2 on stress release in
Saccharomyces cerevisiae (Sunnaker et al., 2013a). By optimiza-
tion of Equation (10), the experimental design was targeted to
enable informative selection among 12 models representing
various hypothetical mechanisms for the short-term Msn2 dy-
namics. In this application, the combination of experimental
design and model selection led to identiﬁcation, and prediction,
of previously unknown and potentially generic principles for
transcription factor dynamics (Sunnaker et al., 2013a).

A distinct but relevant question remains open: how to reliably
identify the parameters of the candidate models? This issue
goes beyond the scope of this study, as it strictly belongs to
the domain of system identiﬁcation (Busetto and Buhmann,
2009a). At the same time, it is an aspect that deserves special
attention, as design and modeling are part of the same hypothe-
tico-deductive process. We conclude that the introduced method
may be useful to guide intuition through quantitative indicators
and thus accelerate scientiﬁc discovery.

ACKNOWLEDGEMENTS

The authors thank Andreas Krause, Andreas Wagner, Karthik
Raman, Elias Zamora-Sillero, Kay Henning Brodersen, Jean

Daunizeau, Heinz Koeppl, Elias August, Volker Roth, Marcus
Hutter and Simonetta Scola for insightful discussions and helpful
comments, and mloss.org.

Funding: This project was ﬁnanced with a grant from the Swiss
SystemsX.ch initiative (projects YeastX and LiverX), evaluated
by the Swiss National Science Foundation.

Conflict of Interest: none declared.

REFERENCES

Atkinson,A.C. and Fedorov,V.V. (1975) Optimal design: experiments for discrimi—
nating between several models. Biometrika, 62, 2897303.

Atkinson,A. et al (2007) Optimum Experimental Designs, with SAS. Vol. 34, Oxford
University Press, Oxford.

Baldi,P. and Itti,L. (2010) Of bits and wows: a Bayesian theory of surprise with
applications to attention. Neural Netw., 23, 649$66.

Balsa—Canto,E. et al (2008) Computational procedures for optimal experimental
design in biological systems. IET Syst. Biol, 2, 1637172.

Bandara,S. et al (2009) Optimal experimental design for parameter estimation of
a cell signaling model. PLoS Comput. Biol, 5, e1000558.

Bergman,R.N. et al (1979) Quantitative estimation of insulin sensitivity. Am. J.
Physiol, 236, E6677E677.

Box,G.E.P. and Hill,W.J. (1967) Discrimination among mechanistic models.
T echnometrics, 9, 57771.

Busetto,A.G. (2012) Information theoretic modeling of dynamical systems.
PhD Thesis, Department of Computer Science, ETH Zurich, Zurich,
Switzerland.

Busetto,A.G. and Buhmann,J.M. (2009a) Stable Bayesian parameter estimation for
biological dynamical systems. In: International Conference on Computational
Science and Engineering, CSE 2009. Vol. 1. IEEE, pp. 1487157.

Busetto,A.G. and Buhmann,J.M. (2009b) Structure identiﬁcation by optimized
interventions. In: Journal of Machine Learning Research Proceedings of the
International Conference on Artﬁcial Intelligence and Statistics. Clearwater
Beach, FL, pp. 49756.

Busetto,A.G. et al (2009) Optimized expected information gain for nonlinear
dynamical systems. In: Proceedings of the 26th Annual International
Conference on Machine Learning. ACM, pp. 977104.

Cover,T.M. and Thomas,J.A. (2012) Elements of Information Theory. Wiley—
Interscience, New York.

Daunizeau,J. et al (2011) Optimizing experimental design for comparing models of
brain function. PLoS Comput. Biol, 7, e1002280.

Doucet,A. and Tadic,V.B. (2003) Parameter estimation in general state—space
models using particle methods. Ann. Inst. Stat. Math., 55, 4094122.

Diivel,K. et al (2003) Multiple roles of Tap42 in mediating rapamycin—induced
transcriptional changes in yeast. Mol Cell, 11, 146771478.

Faller,D. et al (2003) Simulation methods for optimal experimental design in
systems biology. Simulation, 7‘), 7177725.

Feige,U. (1998) A threshold of ln(n) for approximating set cover. J. ACM, 45,
634—652.

Hauser,A. (2009) Entropy—based experimental design for model selection in systems
biology. Master’s Thesis, Department of Computer Science, ETH Zurich,
Zurich, Switzerland.

Kitano,H. (2002) Computational systems biology. Nature, 420, 20(r210.

Kovacs,L. et al (2010) New principles and adequate robust control methods for
artiﬁcial pancreas. In: Computational Intelligence in Engineering. Springer,
Berlin, pp. 75786.

Kramer,A. and Radde,N. (2010) Towards experimental design using a Bayesian
framework for parameter identiﬁcation in dynamic intracellular network
models. Procedia Comput. Sci., 1, 164571653.

Krause,A. (2010) SFO: a toolbox for submodular function optimization. J. Mach.
Learn. Res., 11, 114171144.

Krause,A. and Guestrin,C. (2005) Near—optimal nonmyopic value of information in
graphical models. In: T wenty—ﬁrst Conference on Uncertainty in Artiﬁcial
Intelligence. p. 5.

Krause,A. and Guestrin,C. (2007) Near—optimal observation selection using sub—
modular functions. Vol. 7. AAAI Press, Vancouver, BA.

Kreutz,C. and Timmer,J. (2009) Systems biology: experimental design. FEBS J.,
276, 92%942.

 

2631

112 /310's1eu1nofp1o1xo'sotieuiJOJHtotq/ﬁduq mot} papeo1umoq

9103 ‘0g1sn8nv uo ::

A.G.Busetto et al.

 

Krummenacher,G. (2010) Large—scale experimental design toolbox for systems
biology. Master’s Thesis, Department of Computer Science, ETH Zurich,
Zurich, Switzerland.

Kuepfer,L. et al (2007) Ensemble modeling for analysis of cell signaling dynamics.
Nat. Biotechnol, 25, 100171006.

Liepe,J. et al (2013) Maximizing the information content of experiments in systems
biology. PLoS Comput. Biol, 9, e1002888.

Minoux,M. (1978) Accelerated greedy algorithms for maximizing submodular set
functions. In: Optimization Techniques. Springer, Berlin, pp. 234e243.

Myung,J.I. and Pitt,M.A. (2009) Optimal experimental design for model discrimin—
ation. Psychol. Rev., 116, 499.

Nelles,O. (2001) Nonlinear System Identﬂication. Springer, Berlin.

Nemhauser,G.L. et al (1978) An analysis of approximations for maximizing
submodular set functions. Math. Programs, 14, 2657294.

Ponce De Leon,A.C. and Atkinson,A.C. (1991) Optimum experimental design for
discriminating between two rival models in the presence of prior information.
Biometrika, 78, 6017608.

Raman,K. and Wagner,A. (2011) Evolvability and robustness in a complex signal—
ling circuit. Mol BioSyst., 7, 108171092.

Skanda,D. and Lebiedz,D. (2010) An optimal experimental design approach
to model discrimination in dynamic biochemical systems. Bioinformatics, 26,
9397945.

Steinke,F. et al (2007) Experimental design for efﬁcient identiﬁcation of gene
regulatory networks using sparse Bayesian models. BMC Syst. Biol, 1, 51.
Sunnaker,M. et al (2013a) Automatic generation of predictive dynamic models
reveals nuclear phosphorylation as the key Msn2 control mechanism. Sci.

Signal, 6, ra4l.

Sunnaker,M. et al (2013b) Approximate Bayesian computation. PLoS Comput.
Biol, 9, e1002803.

Szallasi,Z. et al (2006) System Modeling in Cell Biology: From Concepts to Nuts and
Bolts. MIT Press, Cambridge, MA.

Xu,T.—R. et al (2010) Inferring signaling pathway topologies from mul—
tiple perturbation measurements of speciﬁc biochemical species. Sci.
Signal, 3, ra20.

Zhong,Q. et al (2012) Unsupervised modeling of cell morphology dynamics for
time—lapse microscopy. Nat. Methods, 9, 7117713.

 

2632

112 /310's1eu1nofp1o1xo'sotieuiJOJHtotq/ﬁduq mot} papeo1umoq

9103 ‘0g1sn8nv uo ::

