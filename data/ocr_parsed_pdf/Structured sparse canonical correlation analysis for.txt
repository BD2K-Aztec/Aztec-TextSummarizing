Bioinformatics, 32(10), 2016, 1544—1551

doi: 10.1093/bioinformatics/btiA/033

Advance Access Publication Date: 21 January 2016
Original Paper

 

Bioimage informatics

Structured sparse canonical correlation analysis
for brain imaging genetics: an improved
Grathet method

Lei Du1, Heng Huangz, Jingwen Yan1, Sungeun Kim1,
Shannon L. RisacherI, Mark lnlow3, Jason H. Moore",
Andrew J. Saykin1 and Li Shen1'* for the Alzheimer’s Disease
Neuroimaging Initiative'r

1Department of Radiology and Imaging Sciences, Indiana University, Indianapolis, IN, USA, 2Department of
Computer Science 81 Engineering, The University of Texas at Arlington, Arlington, TX, USA, 3Department of
Mathematics, Rose-Hulman Institute of Technology, Terre Haute, IN, USA and 4Institute for Biomedical
Informatics, School of Medicine, University of Pennsylvania, Philadelphia, PA, USA

*To whom correspondence should be addressed.

Associate Editor: Robert Murphy

TData used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) data-
base (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of
ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investi-
gators can be found at: http://adni.|oni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.

Received on 21 September 2015; revised on 29 December 2015; accepted on 16 January 2016

Abstract

Motivation: Structured sparse canonical correlation analysis (SCCA) models have been used to
identify imaging genetic associations. These models either use group lasso or graph—guided fused
lasso to conduct feature selection and feature grouping simultaneously. The group lasso based
methods require prior knowledge to define the groups, which limits the capability when prior
knowledge is incomplete or unavailable. The graph—guided methods overcome this drawback by
using the sample correlation to define the constraint. However, they are sensitive to the sign of the
sample correlation, which could introduce undesirable bias if the sign is wrongly estimated.
Results: We introduce a novel SCCA model with a new penalty, and develop an efficient optimiza—
tion algorithm. Our method has a strong upper bound for the grouping effect for both positively
and negatively correlated features. We show that our method performs better than or equally to
three competing SCCA models on both synthetic and real data. In particular, our method identifies
stronger canonical correlations and better canonical loading patterns, showing its promise for re—
vealing interesting imaging genetic associations.

Availability and implementation: The Matlab code and sample data are freely available at http://
www.iu.edu/~shenlab/tools/angsccal.

Contact: shenli@iu.edu

Supplementary information: Supplementary data are available at Bioinformatics online.

 

©The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

 

91oz ‘Og JSanV 110 salaﬁuv 50'] ‘BtHJOJtIBQ 30 AJtSJQAtuf] 112 /3.10'speum0[p1q1x0"sothJJOJutotq/ﬁduq 11101} popcolumoq

An improved Grathet method

1545

 

1 Introduction

Sparse canonical correlation analysis (SCCA) (Chen and Liu, 2012;
Chen et (11., 2012; Du et (11., 2014; Lin et (11., 2013; Parkhomenko
et (11., 2009; Witten et (11., 2009), is a powerful bi—multivariate ana—
lysis technique (Vounou et (11., 2010). It has recently become a popu—
lar method in brain imaging genetics studies to identify
bi—multivariate associations between single nucleotide polymorph—
isms (SNPs) and imaging quantitative traits (QTs).

SCCA was initially proposed by Witten et al. (2009) and Witten
and Tibshirani (2009) in the analysis of gene expression data. This
first SCCA model introduced the £1—n0rm (lasso) term into the trad—
itional CCA model to make both canonical loadings sparse. The
penalized matrix decomposition (PMD) technique was used to solve
this sparse learning problem. For a group of correlated features,
lasso tends to randomly select only one feature from the group, and
often cannot recover all the relevant and correlated features. Witten
et al. (2009) and Witten and Tibshirani (2009) also proposed the
fused lasso based SCCA, which takes into account the spatial correl—
ation among features. Thus, neighboring features tend to be selected
together to help discover regional structures.

In order to accommodate other types of structures in the data,
several structured SCCA methods (Chen et (11., 2013; Chen and Liu,
2012; Chen et (11., 2012; Du et (11., 2014, 2015; Lin et (11., 2013;
Witten et (11., 2009; Witten and Tibshirani, 2009; Yan et (11., 2014)
arise recently. We group these SCCA methods into two kinds ac—
cording to their distinct regularization terms. One kind used the
group lasso penalty, and the other kind used the graph/network—
guided fused lasso penalty to conduct feature selection and feature
grouping. The first kind, i.e. the group lasso based SCCA, required
prior knowledge to define the group structure. Lin et al. (2013)
incorporated the priori knowledge into the SCCA model with a
group lasso regularizer, where the same PMD technique was used to
identify non—overlapping group structure. Du et al. (2014) proposed
SZCCA using group lasso, and incorporated both the covariance ma—
trix information and the priori knowledge information to discover
group—level bi—multivariate associations. The KG—SCCA (Yan et (11.,
2014) was an extension of SZCCA (Du et (11., 2014), which also em—
ployed the group lasso to constrain one canonical loading. This type
of SCCA methods may not be useful when the biological priori
knowledge is incomplete or unavailable. Of note, it is a hard task to
provide precise prior knowledge in real biomedical studies.

The second kind of structured SCCA methods use graph/
network—guided fused lasso penalties. These methods can perform
well on any given priori knowledge. In case the prior knowledge is
not available, these methods can also work via using the sample cor—
relation to define the graph/network constraint. Chen et al. (2013)
proposed ssCCA using a graph—guided fused [z—norm penalty for
one canonical loading of the taxa based on their relationship on a
phylogenetic tree. Chen et al. (2012) proposed a network—guided
fused lasso based SCCA which penalized every pair of features by
the [l—norm of (u,- — 14,-). It could be viewed as an extension to the
fused lasso based SCCA without demanding the features being
ordered. Du et al. (2015) proposed GN—SCCA which penalizes the
[z—norm of (u,- — 14,-). These two SCCA methods could only handle
the positively correlated features. Chen and Liu (2012) proposed an
improved network—structured SCCA (NS—SCCA) by incorporating
the sign of the sample correlation within features. NS—SCCA penal—
ized the [l—norm of (u,- — sign(p,-,-)u,-) to tune a similar weight value
for u,- and u,- if p,,- > 0, or dissimilar if p,,- < 0. The aforementioned
KG—SCCA (Yan et (11., 2014) employed [z—norm of (u,- — sign(p,-,-)u,-)
on one canonical loading. Most of these SCCA methods used the

data—driven correlation as the network constraint, while some incor—
porated prior knowledge to define the network constraint (Chen
and Liu, 2012; Yan et (11., 2014). In the data—driven mode, they were
dependent on the sign of the pairwise sample correlation to identify
the hidden structure pattern. Unfortunately, this can introduce add—
itional estimation bias since the sign of the correlations can be
wrongly estimated due to possible graph/network misspecification
caused by noise (Yang et (11., 2012).

We focus on the data—driven mode in this paper. We first propose
a novel structured penalty using the pairwise difference of absolute
values between features, which is an improved Grathet penalty
(Grosenick et (11., 2013). Then we introduce our novel structured
SCCA model coupled with an effective SCCA algorithm, i.e. SCCA
using the absolute value based Grathet (AGN—SCCA). Our contri—
butions are summarized as follows. (i) The new regularizer penalizes
the difference between the absolute values of the coefficients no matter
whether their correlations are positive or negative. Thus it could tune
both positively and negatively correlated features to have similar
weights despite the correlation signs. (ii) AGN—SCCA could reduce es—
timation bias due to its independence to the signs of sample correl—
ation, and thus has better performance and generalization ability than
those methods dependent on sample correlation signs. (iii) We provide
a quantitative upper bound for the grouping effect of AGN—SCCA and
prove that the algorithm is guaranteed to converge fast. (iv) On both
synthetic and real imaging genetic data, AGN—SCCA yields higher or
comparable correlation coefficients, and generates more accurate and
cleaner patterns than three competing methods, i.e. L1—SCCA (CCA
with lasso) (Witten et (11., 2009; Witten and Tibshirani, 2009) FL—
SCCA (CCA with fused lasso) (Witten et (11., 2009; Witten and
Tibshirani, 2009) and NS—SCCA (Chen and Liu, 2012).

2 Methods

In this paper, we use the boldface lowercase letter to denote a vector,
and use the boldface uppercase one to denote a matrix. Ini repre—
sents the 1th row of matrix M. We use X : {x1; . . . ;x”} g RP and
Y : {y1;  ;y”} 2 IR" to denote the SNP data and the QT data
originating from the same population. The SCCA model proposed
in (Witten et (11., 2009; Witten and Tibshirani, 2009) can be defined
as follows:

min —uTXTYV (1)

u,v

st llulli : 1. llvlli : 1. llull1 : c1. llvll1 : c2. where llull1 : c1 and
Hvlll S 62 are constraints for controlling the model sparsity, and
typical constraints include lasso (Chen et (11., 2012; Parkhomenko
et (11., 2009; Witten et (11., 2009; Witten and Tibshirani, 2009) and
fused lasso (Witten et (11., 2009; Witten and Tibshirani, 2009).

2.1 The new penalty
Grosenick et al. (2013) have extended the traditional elastic net reg—
ularizer to a more general form, which is named Grathet, i.e.

IIUIIGNzlluTMu+l31IIuII1 (2)

where M is a matrix, and (A1, [31) are tuning parameters. Note that
Grathet becomes the elastic net if M : I (Grosenick et (11., 2013).
Typical Grathet studies (Du et (11., 2015; Grosenick et (11., 2013)
have M : L, where L is the Laplacian matrix of a graph. Let g be
the graph formed by our sample correlation matrix A. Let D be a di—
agonal degree matrix with the following diagonal entries:
D(i, i) : ZI- A(i,/). The Laplacian matrix L is defined as L : D — A

91oz ‘Og JSanV 110 salaﬁuv soc] ‘BtHJOJtIBQ 30 AJtSJQAtuf] 112 /310'S[Buln0prOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} popcolumoq

1546

L.Du et aI.

 

(Grosenick et al., 2013). When M : L, the Grathet term can be
transferred and written as:

llullGN : 11 2 mm — unz + ﬁlllulh- 13>
(1,069

It is easy to see that this penalty only puts emphasis on the posi—
tively correlated features, and does not take into consideration the
negatively correlated features. To address this issue, we introduce a
novel penalty which uses the pairwise difference between absolute val—
ues instead, i.e. Z  — Iu,-I)2. SCCA requires two penalties, one for
each canonical loading. Thus, we propose the following new penalties:

IIuIIAGN : Alzwiﬂ
IIVIIAGN : 4L22:14’141

where um,- and urgi- depend on the pairwise sample correlation of X
and Y respectively. [31 I Iul I1 and [32)le )1 are used to control the model
sparsity.

In accordance to the form of Grathet, we rewrite the penalty
and call it absolute value based Grathet penalty,

2
Mil - lull) +b’1llull1.

 

2
vii - IViI) +b’2llvll1-

 

T

llullAcNZ/hlul L1lul+ﬁ1llull17 (5)
T

IIVIIAGN Zizlvl Llel +b’2llvll1-

where L1 and L2 are Laplacian matrices of the correlation matrices
of X and Y respectively.

The main motivations for proposing  IAGN are as follows.
First, if we have some priori knowledge, e.g. the pathway informa—
tion about genetic markers, each pairwise penalty encourages 
and  to be similar. This makes sure that the two markers have a
high probability to be selected together if they are connected on the
graph. Second, if the priori knowledge is unavailable, every pairwise
term will be imposed to encourage  z  for both positively and
negatively correlated features based on the strength of their sample
correlations, which will be supported by Theorem 1. Third, genetic
(or imaging) markers in the same pathway (or brain circuitry) could
play different roles for a specific disease. That is, some markers
could be significant, while others could be irrelevant. Therefore, we
impose lasso to assure additional sparsity.

2.2 AG N—SCCA model and proposed algorithm
By imposing the novel Grathet penalty into a CCA model, we ob—
tain our AGN—SCCA model.

min —uTXTYv (6)

u,v
2 2
5t IIXUII2 S LIIYVllz S LIIUIIAGN S 511IIVIIAGN S 62-

Note that we utilize  S 1 and  S 1, which embraces
the covariance structure of the data in our model. The strength of
this strategy has been demonstrated by our prior S2CCA work (Du
et al., 2014).

Using Lagrange multiplier method, we define the Lagrangian [3
below,

£(u,v, F) : —uTXTYv + £1IuITLlluI +ﬂllulll
2 2
12 T [32 V1 2 V2 2 (7)
+7le Lzlvl+7llvll1+7llXull2 +7llell2

where F : {2, [1, y} > 0 are the Lagrange multipliers, which are also
called dual variables.

According the Lagrange duality, the Lagrangian can represent
problem Eq. (6) as the following unconstrained one,

17 2 H3551 rpgglxulvll") (8)

Now that there is no constraint term in Lagrangian [3, it is easy

to solve Eq. (8) than Eq. (6). Given the optimal dual variables I“,

we could obtain the solution by taking derivative regarding u and v

respectively, and let both of them be zero.
813 813
a — 07 E — 07 

However, the new proposed penalty is non—differentiable at zero
value owning to the [l—norm term and the absolute value based
Grathet term. Thus we use the sub—gradient in Eq. (9) instead, and
obtain the following (if  : 0, the ith element of diagonal matrix

. . . . 1 .
D1 Is not avallable. So we regularlze the element In D1 as Z—m (C 15

a very small positive number) when  : 0. Then the objective

function regarding u is [3*(u) : §:1( — u,-x,TYv + 

Why/u?+C+%(/u%+{+%llxiuilli). It is easy to prove

that [3*(u) will reduce to problem (7) regarding u when {—> 0.
Those (11,-) : 0 can also be regularized by the same strategy),

(1113, + [111), + leTX)u : XTYV, (10)

(12132 + [12132 + yZYTY)v : YTXu, (11)
where D1 is a diagonal matrix with the ith element as  (i E [1, p]),
and D2 is a diagonal matrix with the jth element as  (j E [141]).
A 11
D1 is a diagonal matrix with the [nth element as % ([21 E [1,pl),
1

where LI“ is the [nth row of the Laplacian matrix L1. Similarly, D2

 

l!
is the diagonal matrix with the k2th element as  ([22 E [1,ql),

and LI;2 is the k2th row of the Laplacian matrix L2.

Both D1 and D1 depend on u; and both D2 and D2 depend on v.
Since u and v are unknown, we propose an effective iterative algo—
rithm called AGN—SCCA to solve this problem. Algorithm 1 shows
the pseudocode. In each iteration, the algorithm first fixes v to calcu—
late u and then fixes u to calculate v. This procedure repeats until it
converges.

Computational analysis. Step 4 and Step 7 are the key steps of
Algorithm 1. To assure the efficiency, we solve a system of linear
equations with quadratic complexity to update u and v other than
computing the matrix inverse with cubic complexity. Step 10 is a
simple operation to rescale the results. So, the whole procedure is ef—
ficient and runs fast. Moreover, the algorithm is guaranteed to con—
verge, as shown in Theorems 2 and 3.

2.3 The grouping effect analysis

It is important to investigate the grouping effect of the a structured
learning method in handling high—dimensional data (Zou and
Hastie, 2005). Although many structured SCCA methods have been
proposed and could recover structure pattern practically. None of
them provides a theoretical bound for the grouping effect. In this
work, we have the following theorem which provides a qualitative
theoretical bound in grouping correlated features.

THEOREM 1 Given two datasets X and Y, and the pre—tuned par-
ameters (A, [1, y). Let it he the solution to our SCCA problem of Eqs.
(10) and (11). Without loss of generality, we consider the u,-th and
u,-th feature are only linked to each other on the graph, i.e. e,-ﬁ,- : 1.

9103 ‘01; JSanV 110 salaﬁuv soc] ‘BtHJOJtIBQ 30 AJtsraAtuf] 112 /310'S[BHJnOprOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

An improved Grathet method

1547

 

 

Algorithm 1. The AGN—SCCA Algorithm

 

Require:
T T
X:{X1:"'7Xn} 7Y:{Y17---7Yn}

Ensure:

Canonical loadings u and v.

1: Initialize u E RPX1,V E RqX1;L1: Du — Au and
L2 : D,, — A, only on the training set;

2: while not converged do

3: While not converged regarding u do
4: Solve u according to Eq. (10)

5: end While

6: While not converged regarding v do
7: Solve v according to Eq. (11)

8: end While

9: end While

10: Scale u so that  : 1, and v so that  : 1.

 

Let p,,- is the sample correlation between them, wiﬁi is their edge
weight. Then the estimated canonical loading u satisfies,

I721" — 72/13 —V1 + 211w“ 2(1 — 1711)) if 917 Z 07
1 (12)
Iﬁi+ﬁiI£V1+2—W\/2(1+pii)v ifPi/ < 07
and the estimated canonical loading U satisfies,
(11—13K; 2(1 —p’--) ifp’.. > o
I I _ (V2 + 2121017) I] I I] — I
1 (13)
I171 + 171) S 2(1 + 92,-). if Pl,- < 0-

(V2 + 212102,)

where wi-J- is the weight between the ith and ith feature of v, and p2,-
is their sample correlation coefficient.

The proof of this theorem can be found in Appendix A (See
Supplementary File). Theorem 1 not only provides an upper bound
for the difference between the canonical loading paths of the ith and
ith features when they are positively correlated, but also provides a
quantitative description when they are negatively correlated. If
pi,- Z 0, the higher correlation two features have, the smaller differ—
ence there is between their coefficients. While if pi,- < 0, a smaller
value will generate a closer—to zero value for the sum of their coeffi—
cients. This is desirable because AGN—SCCA can estimate the coeffi—
cients with equal amplitude except signs for two negatively
correlated features. This quantitative description for the grouping
effect demonstrates that our novel structured SCCA is suitable for
sparse structure learning.

2.4 The convergence analysis
We have the following theorems regarding the Algorithm 1.

THEOREM 2 The problem Eq. (8) is lower bounded by —1.

THEOREM 3 In each iteration, the AGN—SCCA algorithm monot—
onously decreases the objective value till it converges.

The proofs are provided in Appendix B and C (See
Supplementary File) due to space limitation. Since the objective
value keeps deceasing during the iteration, and the problem has the
lower bound, the proposed algorithm is guaranteed to converge to a
local optimum.

In our implementation, we set the stopping criterion of Algorithm 1
as max{l5ll5 E(ut+1— ut)} S r and max{l5ll5 E(vt+1—v,)}§ r,
where r is a predefined estimation error. In this paper, 17 : 10—5 is em—
pirically set based on experiments.

3 Experiments

3.1 Experimental setup

3.1. 1 Benchmarks

We chose three existing SCCA methods for comparison in this study,
one is the state—of—the—art method NS—SCCA (network—structured
CCA) (Chen et al., 2013), and the other two methods are the L1—
SCCA (CCA with lasso) and FL—SCCA (CCA with fused lasso). The
latter two can be found in package PMA (the PMA software package
implements both L1—SCCA and FL—SCCA, and they are widely used
as benchmark algorithms. See http://cran.r—project.org/web/packages/
PMA/for details), which is widely used for SCCA studies. We do not
compare our method with KG—SCCA (Yan et al., 2014) due to two
reasons: (i) KG—SCCA uses £2,1—norm on one canonical loading (simi—
lar to S2CCA), uses £2—norm of (u,- — sign(p,-,-)u,-) on the other (similar
to NS—SCCA), and requires predefined group and network structures.
(ii) NS—SCCA uses the £1—norm of (u,- —sign(p,-,-)u,-), which is sup—
posed to be more reasonable in sparse learning than KG—SCCA since
£1—norm is a sparse constraint but £2—norm is not. Therefore we in—
clude NS—SCCA instead of KG—SCCA as a competing method. We
also do not compare our method with GN—SCCA (Du et al., 2015) be—
cause it only focuses on the positively correlated features. In addition,
ssCCA (Chen et al., 2013), CCA—SG (CCA—sparse group) (Lin et al.,
2013) and S2CCA (Du et al., 2014) are opted out, since they are
knowledge—guided methods and applicable only when priori know—
ledge is available.

3. 1.2 Parameter tuning

According to Eqs. (10) and (11), we have six parameters to be tuned.
Obviously, this is very time consuming by blind grid search. Thus
we here employ some tricks to speed up the tuning procedure. The
major difference between the traditional CCA and SCCA is the pen—
alty terms. On one hand, SCCA and CCA will yield similar results if
the parameters are too small. On the other hand, SCCA will over—
penalize the result when the parameters are too large. Thus a neither
too large nor too small parameter is more reasonable. As a result,
we tune these parameters from [10—2,10_1, 10°, 101, 102]. All the
parameters are tuned through the nested 5—fold cross—validation
CV01, [1, y) : §Zi5:1 Corr(X,-u_,-,Y,-v_,-) where X,- and Y,- denote the
ith subset of the input data (testing set), and u_,- and v_,- mean the ca—
nonical loadings estimated from the training set. We choose the arg
max CV01, [1, y) as the tuned optimal parameters. For efficiency pur—
pose, these parameters are only tuned from the first run of the cross—
validation strategy. That is, the parameters are tuned when the first
four folds are used as the training set. Then we directly use the tuned
parameters for all the remaining experiments. Though this could
limit the performance for the rest of the experiments, we find that it
will not affect the performance significantly from the results which
will be shown later. All these methods utilize the same partition dur—
ing cross—validation to make a fair comparison.

3.2 Results on synthetic data

We simulate four different datasets with different properties in this
study, and we expect the diversity could make sure a thorough com—
parison. The true signals and the strengths of correlation coefficients
within these data are distinct. As a simulation of a large p small n

9103 ‘01; JSanV uo salaﬁuv soc] ‘BtHJOJtIBQ JO AJtsraAtuf] 112 /310'S[B111n0fp10}x0'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

1548

L.Du et aI.

 

D5151 D5152 D5153 D5154

SCCA<L1 Ground Tmih

Color Scale

ANS-SCCA NS~SCCA SCCA<FL

100 100 100 100 2
“II “II “II “III
a o o o _2
10° 5x163

“IE I I I I I

o _

100

“II I I I I I I

o -5

100 0.2

“II II II II I-

0 ~02

100 5x10'3

“II IE II II 1

O u v u v - u v u v ‘5
Fig. 1. Canonical loadings estimated on four synthetic datasets. The first col-
umn is for Dataset 1, and the second column is for Dataset2, and so forth. For
each dataset, the estimated weight of u is shown on the left figure, and v is
on the right. The first row is the ground truth, and each remaining one corres-
ponds to a method: (1) Ground Truth. (2) L1-SCCA. (3) FL-SCCA. (4) NS-SCCA.
(5) AGN-SCCA

problem, we here set the number of observations be smaller than the
number of features, i.e. n : 80, p : 100 and q : 120. The generation
procedure is similar to that in (Chen and Liu, 2012) except for the
last step: (i) We create u and v separately according the predefined
structure. (ii) We generate a latent variable 2 ~ N(0,I,,X,,). (iii) We
generate X with the entry: x,- N N (ziu, 2,), where
(2x )I-k : exp‘IW—WI, and Y with the entry: y,- N N(z,-v, 2y), where
(Z), )I-k : exp‘IW—WI. (iv) For the first group of nonzero coefficients
in u, we change the first half of their signs. At the same time, we also
change the signs of the corresponding data. As a result, we still have
X’u’ : Xu where XI and u’ are the data matrix and coefficients after
the sign swap. Note that these synthetic data are order—independent,
and thus this setup is equivalent to randomly change a portion of
signs for coefficients u (Yang et al., 2012). For the Y side, we do the
same. The details of the four datasets are as follow. (i) The first two
datasets have the same signal structure, i.e. the same group structure
regarding u and v. But their correlation coefficients are different.
The correlation coefficient of the first dataset is 0.52, while that of
the second dataset is 0.17. (ii) The third dataset is different from the
first two datasets in its group structure regarding u and v. Its correl—
ation coefficient is 0.58. (iii) The fourth dataset is different from all
the above three datasets in its group structure regarding u and v. Its
correlation coefficient is 0.51. The true signal of each dataset can be
observed from the first row in Figure 1.

In Table 1, we present the estimated correlation coefficients
from both training and testing data, and their differences from the
true correlation coefficients (i.e. the numbers in parentheses). We
use the boldface to highlight the highest value as well as those that
are not significantly smaller than the highest value. For the training
set, we observe that our method obtains the best correlation coeffi—
cients on Dataset 2 and Dataset 3, and it is only slightly smaller than
the best method on the rest of the two datasets. Though AGN—
SCCA does not obtain the highest for every dataset, it is not statistic—
ally different from the best method. If we consider the true correl—
ation coefficients, we observe that AGN—SCCA and L1—SCCA are
two methods which have smaller estimation errors. That is, both
AGN—SCCA and L1—SCCA identify more accurate correlation coeffi—
cients than FL—SCCA and NS—SCCA regarding the training results.
For the testing set, AGN—SCCA outperformed the competing meth—
ods on Dataset 3, and was not significantly different from the best
method on the remaining datasets. Besides, AGN—SCCA’s estimation
error is the smallest for Datasets 2—4, which means it performs bet—
ter than the competing methods regarding the prediction perform—
ance. Generally, this is more interesting since the testing

performance is more important than the training results. These re—
sults show that AGN—SCCA either outperforms or performs simi—
larly to those competing methods in terms of estimated correlation
coefficients.

We show the estimated canonical loadings of four SCCA meth—
ods in Figure 1. As we can see, none of these methods could generate
stable results for the small-n-large-p problem when using cross—
validation strategy. They still exhibit group structures for the esti—
mated canonical loadings. However, neither L1—SCCA nor
FL—SCCA can accurately recover the true signals. They cannot iden—
tify those coefficients with signs swapped. Thus they treat the
positively and negatively correlated features with no difference. NS—
SCCA and AGN—SCCA successfully recognize the coefficients whose
signs are changed. The reason is that AGN—SCCA uses the absolute
difference between the coefficients, and NS—SCCA takes advantage
of the sign of sample correlation. Note the sign of sample correlation
depends on the signal—to—noise ratio (SNR), and it is likely to be
wrong due to a high proportion of noise. Therefore, for the three
datasets with high correlations (Dataset 1, Dataset 3 and Dataset 4),
NS—SCCA could exhibit a similar pattern to AGN—SCCA with re—
spect to the canonical loadings. While for the second dataset whose
correlation is small, AGN—SCCA outperforms NS—SCCA in terms of
the structure pattern, especially for the canonical loading v. In order
to make this clear, we also calculate the AUC (area under ROC
curve) to present the performance regarding the canonical loadings
pattern in Table 2 with those best values marked in bold. We ob—
serve that both structured SCCA, i.e. AGN—SCCA and NS—SCCA,
perform consistently better than L1—SCCA and FL—SCCA. Our
AGN—SCCA obtains the best scores in most runs except on few
folds, especially for the canonical loadings v.

In summary, the AGN—SCCA not only estimates the most accur—
ate correlation coefficients in most cases, but also identifies the sig—
nal locations with the best accuracy in all the cases. These promising
results reveal that our method outperforms the competing methods,
showing that it can handle a range of synthetic datasets with distinct
structures and correlations.

3.3 Results on real neuroimaging genetics data

Apart from the synthetic data, it is essential to evaluate our method
on real neuroimaging genetics data. Real imaging genetics data used
in the preparation of this article were obtained from the Alzheimer’s
Disease Neuroimaging Initiative (ADNI) database (adni.loni.us—
c.edu). The ADNI was launched in 2003 as a public—private partner—
ship, led by Principal Investigator Michael W. Weiner, MD. The
primary goal of ADNI has been to test whether serial magnetic res—
onance imaging (MRI), positron emission tomography (PET), other
biological markers, and clinical and neuropsychological assessment
can be combined to measure the progression of mild cognitive im—
pairment (MCI) and early Alzheimer’s disease (AD). For up—to—date
information, see www.adni—info.org.

The genotyping and baseline amyloid imaging data (prepro—
cessed [11C] Florbetapir PET scans) of 283 non—Hispanic Caucasian
participants were downloaded from the ADNI website (adni.loni.us—
c.edu). The amyloid imaging data were preprocessed according to
the steps in (Yan et al., 2014), and then pre—adjusted by regressing
out the effects of the baseline age, gender, education and handed—
ness. Using the voxel—based imaging data, we extracted 191 ROI
level mean amyloid measurements, where the ROIs were defined
by MarsBaR AAL atlas. For the genotyping data, we included 58
SNP markers within the APOE gene, including the APOE e4
SNP rs429358 (i.e. the best—known AD genetic risk factor)

9103 ‘01; JSanV uo salaﬁuv 50'] ‘BtHJOJtIBQ JO AJtsraAtuf] 112 /310'S[BHJnOprOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

1549

An improved Grathet method

Uoéaommom H.003 an?kgogmomammowOno—Eocgawbmﬁ 0: €35.05? 0». 03:05:? New >=mo_om 0: 29:50.80 mo. mo;

duﬂEon 5 8508 20 20* :08 we 830.» 7252 089 BE. 850230 8: 05 UD< 2t 0:88 .202.

 

 

 

00.5 00.5 00.5 00.5 00.5 00.5 00.0 00.5 00.5 00.5 00.5 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.5 00.0 00.0 00.0 00.5 $8-ZO<
00.5 00.5 00.5 00.5 00.5 00.5 00.0 00.5 00.5 00.5 00.5 00.0 5.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 $8.0z
00.5 00.5 00.5 00.5 00.5 00.5 00.0 00.0 00.0 00.0 00.0 00.0 zmz zmz zmz zmz zmz zmz 00.0 00.0 00.0 00.0 00.0 000 $8.55
00.5 00.5 00.5 00.5 00.5 00.5 00.0 00.0 00.0 00.0 00.0 00.0 05.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 $8.5
> mﬁwmoq 1815an wmumﬁtmm

00.0 00.5 00.5 00.5 00.5 00.0 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.5 $8-ZO<
00.5 00.5 00.5 00.5 00.0 00.5 00.5 00.5 00.5 00.5 00.5 00.5 00.0 00.5 00.5 00.5 00.0 00.5 00.5 00.5 00.5 00.5 00.5 00.5 $8.0z
00.0 00.5 00.5 00.5 00.0 00.5 00.5 00.5 00.5 00.5 00.5 00.5 zmz zmz zmz zmz zmz zmz 00.5 00.5 00.5 00.5 00.5 00.5 $8.55
00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.0 00.5 00.5 00.5 00.0 00.5 00.5 00.5 00.5 00.5 00.5 00.5 $8.5
: mﬁwmoq 1815an wmumﬁtmm

7252 :00 n 80 $ $083 7252 200.0 M 80 0 $083 7252 50 n 80 0 $083 7252 200.0 M 80 5 333 80035

 

550:0 Em Z<m=>_ :9: new 22 $2229: some 00 03< oi “Sup 255E? co 09:me 702:0ch poymEzwo 00 33:0 mi 55:: 098 03< of. .N aim...

duﬂEon 5 8508 20 God :05 8:050 833A“ £55 080$ 889$ bucwumawa HO Z 20 005 805 58 830.» 08wa 2:. .83?» 20:8 E? 0 0:88 .00.? .me

+002 #85058 we :3 0 30:58 9 0:3 «5:85 0 0:88 .202. .5508 om? 020 58 800538 890830 85 58 “8:0 8:: 2t 58 886588 58:03:00 800538 2t :8389 885803 E 089555 888w€ BE.

 

 

$0.0 20.0.0000 $0 0.0 R0 000 00.0 200.080 00.0 00.0 $0 000 00.0 200.020 50.0 00.0 05.0 00.0 000 200.0500 00.0 $0 0.0 $0.0 00.0 $8.ZO<
5.0 5.0.00.0 $0 00.0 00.0 5.0 00 800.0000 00.0 $0 0.0 2.0 $0 300000 00.0 00.0 00.0 00.0 00 200.0000 00.0 00.0 05.0 2.0 $0 $8.0z
:0 200.0500 00.0 $0 $0 $0 00.0 65.0.0000 $0 0.0 00.0 00.0 00.0 ZNZ zmz ZNZ ZNZ zmz ZNZ 2000.050 05.0 00.0 00.0 05.0 000 $8.55
000 2000.00 5.0 00 E0 90 00.0 2.0.0.0000 $0 000 00.0 00.0 000 200.0050 00.0 00.0 05.0 90 00 200.050 50 K0 00.0 05.0 00.0 $8.5
£38m mqﬂmmxﬂ

00.0 200.0500 00.0 00.0 $0 $0.0 $0 2000.080 000 00.0 00.0 00.0 $0 20000000 00.0 5.0 00.0 05.0 000 200.500 $0 000 05.0 00.0 50.0 $8.ZO<
:0 30.0000 :0 00.0 $0 $0 00.0 50.0000 90 $0 0.0 00.0 00 200.0030 5.0 05.0 00.0 5.0 00.0 25.0.0000 $0 0.0 00.0 $0 00.0 $8.0z
00.0 20000000 $0.0 00.0 $0 $0.0 000 000.0000 00.0 00.0 00.0 00.0 5.0 ZNZ zmz ZNZ ZNZ zmz ZNZ 60.00000 00.0 $0 $0 000 000 $8.55
00.0 000.0000 5.0 $0 $0 $0 :0 300.0000 00.0 5.0 00.0 5.0 50 20.0050 00.0 05.0 00.0 5.0 00.0 20.0:000 00.0 $0 000 00.0 5.0 $8.5
0:385 mﬁﬁmﬁt

5cm .03. 7252 :00 n 80 $ $083 7252 200.0 M 80 0 $083 7252 50 n 80 0 $083 7252 200.0 M 80 5 $33 00955

 

550:0 Em Z<m=>_ :9: new 22 5222?; some 00 3:2 Eooo cozﬂotoo poymEzwo oi “Sup 255.56 so 338: cozmngawoco 22$ ._. aim...

1550

L.Du et aI.

 

Table 3. 5-fold cross-validation results on real data: the estimated correlation coefficients of each individual fold and their MEAN are shown

 

 

Methods Training results MEAN Testing results MEAN
Ll—SCCA 0.57 0.56 0.56 0.56 0.54 0.56 0.46 0.54 0.53 0.49 0.63 0.53
FL—SCCA 0.51 0.48 0.50 0.50 0.48 0.49 0.38 0.51 0.45 0.44 0.56 0.47
NS—SCCA 0.53 0.50 0.53 0.51 0.50 0.52 0.41 0.42 0.37 0.42 0.62 0.45
AGN—SCCA 0.61 0.59 0.59 0.59 0.58 0.59 0.48 0.59 0.57 0.52 0.65 0.56

 

The best value and those that are NOT signiﬁcantly worse (t—test with p-value smaller than 0.05) are shown in boldface.

(Ramanan et al., 2014). We aim to evaluate the associations between
the amyloid data and the APOE SNP data using the proposed method.

In Table 3, we present the correlation coefficients estimated by
the four different SCCA methods via 5—fold cross—validation strat—
egy. As we can see, AGN—SCCA can not only identify the strongest
correlation on the training set, but also outperform those competing
methods on the testing set. Although all methods yield acceptable
correlation coefficients, AGN—SCCA significantly and consistently
outperforms other SCCA methods, demonstrating its capability in
identifying strong imaging genetic associations.

We also show the canonical loadings estimated from the train—
ing set in Figure 2 using the heat maps. In Figure 2, each row
refers to a method. The estimated u, containing weights for genetic
markers, is shown on the left panel and the estimated v, containing
weights for the imaging markers, is shown on the right. For the ca—
nonical loading u, AGN—SCCA only identified the APOE e4 SNP
rs429358, i.e. the best—known AD genetic risk factor. L1—SCCA
and FL—SCCA also discovered the APOE e4 SNP, but reported
much more additional SNPs than AGN—SCCA. Thus their results
are not as sparse as AGN—SCCA. NS—SCCA also identified many
SNPs which is hard to interpret. For the v side, we can observe
that FL—SCCA fused the results of L1—SCCA because of its pairwise
smoothness penalty. However, their results consists of too many
signals, making them hard to interpret. NS—SCCA identified even
more signals than FL—SCCA and L1—SCCA due to its pairwise
smoothness imposed on the whole graph, which is suboptimal for
biomarker discovery.

As a result, we could see that AGN—SCCA exhibits a very clean
pattern and reports very few relevant imaging signals, including
frontal and caudate regions that are known to be related to AD (Jiji
et al., 2013). In short, the proposed AGN—SCCA algorithm success—
fully discovered a biologically meaningful associations between
APOE SNP rs429358 and the amyloid accumulations at the AD
related brain regions. This demonstrates that AGN—SCCA can not
only reveal strong imaging genetic associations, but also identify
meaningful and relevant genetic and imaging markers.

4 Conclusion

We have proposed a novel structured regularization term using the
pairwise difference between absolute values of two weights, and
incorporated it into a SCCA framework. This proposed structured
SCCA model, named as AGN—SCCA, aims to discover any group or
network structure laying behind the data. We have demonstrated
that AGN—SCCA has strong upper bound of grouping effect, and
have developed an iterative procedure with proven convergence.
The existing structured SCCA methods either use the group lasso
(Du et al., 2014; Lin et al., 2013; Yan et al., 2014) or the graph/net—
wrok—guided fused lasso (Chen et al., 2013; Chen and Liu, 2012;
Chen et al., 2012; Du et al., 2015; Yan et al., 2014) to model the
structure information. The first type of methods rely on prior

LisccA

: 0.25

S 0.2
2 0.1

‘ A 0.05

; (0.05

, V0.1

FL SCCA

NssccA

AGNsccA
.4: .4:
a 0

9103 ‘01; JSanV uo salaﬁuv 50’] 0211110311123 JO AJtsraAtuf] 112 /310'S[BHJnOprOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

$423958
Fronial
Caudaie

Fig. 2. Canonical loadings estimated on real imaging genetics dataset. Each
row corresponds to an SCCA method: (1) L1-SCCA. (2) FL-SCCA. (3) NS-
SCCA. (4) AGN-SCCA. For each method, the estimated weights of u are
shown on the left panel, and those ofv are on the right

knowledge to define the group structure, and the prior knowledge is
sometimes unavailable in real applications. The latter type of meth—
ods can perform well on any given priori knowledge. In case the
prior knowledge is not available, these methods can also work via
using the sample correlation to define the graph/network constraint.
However, they depend on the sign of sample correlation being
defined in advance, which could be wrongly estimated due to pos—
sible graph/network misspecification caused by noise (Yang et al.,
2012).

Our proposed SCCA is different from those previously published
ones in the following aspects: (i) AGN—SCCA employs a novel abso—
lute value based Grathet penalty, and it does not require to esti—
mate the sign of sample correlation. (ii) The AGN—SCCA could tune
positively correlated features as well as negatively correlated ones to
have similar weights despite the correlation signs. (iii) AGN—SCCA
has a strong theoretical upper bound for the grouping effect, and the
corresponding algorithm is guaranteed to converge fast.

We have compared AGN—SCCA with three competing SCCA
methods with different penalty functions, including L1—SCCA, FL—
SCCA and NS—SCCA, using both synthetic data and real imaging
genetics data. The experimental results demonstrate the following:
(i) For the estimated correlation coefficients, AGN—SCCA obtained
the best or comparable results on the synthetic data, and signifi—
cantly outperformed the competing methods on the real data. (ii)
For the estimated canonical loadings, AGN—SCCA yielded better ca—
nonical loading pattern on both synthetic data and real data, espe—
cially on the real data where it produced much cleaner patterns than
the competing methods. By discovering a strong association between
the APOE SNP data and the amyloid accumulation data in an AD
study, AGN—SCCA demonstrated itself as a promising structured
SCCA method. The theoretical convergence and upper bound of the
grouping effect further reveal that AGN—SCCA is of efficiency and
effectiveness in identifying meaningful bi—multivariate associations
in brain imaging genetics studies. In this work, we only tested AGN—

An improved Grathet method

1551

 

SCCA while using data—driven covariance structure as the graph/net—
work constraint. In the future, we will apply the AGN—SCCA model
to more general cases and test its performance when priori know—
ledge is available.

Acknowledgements

Data collection and sharing for this project was funded by the Alzheimer’s
Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant
U01 AG024904) and DOD ADNI (Department of Defense award number
W81XWH-12-2—0012). ADNI is funded by the National Institute on Aging,
the National Institute of Biomedical Imaging and Bioengineering, and
through generous contributions from the following: Abeie, Alzheimer’s
Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech;
BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.;
Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun;
F. Hoffmann-La Roche Ltd and its afﬁliated company Genentech, Inc.;
Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy
Research 8c Development, LLC.; Johnson 8c Johnson Pharmaceutical
Research 8c Development LLC.; Lumosity; Lundbeck; Merck 8c Co., Inc.;
Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack
Technologies; Novartis Pharmaceuticals Corporation; Pﬁzer Inc.; Piramal
Imaging; Servier; Takeda Pharmaceutical Company; and Transition
Therapeutics. The Canadian Institutes of Health Research is providing funds
to support ADNI clinical sites in Canada. Private sector contributions are
facilitated by the Foundation for the National Institutes of Health (www.
fnih.org). The grantee organization is the Northern California Institute for
Research and Education, and the study is coordinated by the Alzheimer’s
Disease Cooperative Study at the University of California, San Diego. ADNI
data are disseminated by the Laboratory for Neuro Imaging at the University
of Southern California.

Funding

At Indiana University, this work was supported by the National Institutes of
Health [R01 LM011360 to LS. and A.J.S., U01 AG024904 to M.W., RC2
AG036535 to M.W., R01 AG19771 to A.J.S., P30 AG10133 to A.J.S., UL1
TR001108 to A.S., R01 AG 042437 to P.C., and R01 AG046171 to R.K.],
the National Science Foundation [HS-1117335 to L.S.], the United States
Department of Defense [W81XWH-14—2—0151 to T.M., W81XWH—13-1-
0259 to M.W., and W81XWH-12-2—0012 to M.W.], and the National
Collegiate Athletic Association [Grant No. 14132004 to T.M.]. At University
of Texas at Arlington, this work was supported by the National Science
Foundation [CCF-0830780 to H.H., CCF-0917274 to H.H., DMS-O915228
to H.H., and 115-1117965 to H.H.]. At University of Pennsylvania, the work

was supported by the National Institutes of Health [R01 LM011360 to J.M.,
R01 LM009012 to J.M., and R01 LM010098 to J.M.].

Conﬂict of Interest: none declared.

References

Chen,J. et al. (2013) Structure-constrained sparse canonical correlation ana—
lysis with an application to microbiome data analysis. Biostatistics, 14,
244—25 8.

Chen,X. and Liu,H. (2012) An efﬁcient optimization algorithm for structured
sparse cca, with applications to eqtl mapping. Stat. Biosci., 4, 3—26.

Chen,X. et al. (2012). Structured sparse canonical correlation analysis. In:
AISTATS.

Du,L. et al. (2014). A Novel structure—Aware Sparse Learning Algorithm for
Brain Imaging Genetics. MICCAI, Boston, MA, pp. 329—336.

Du,L. et al. (2015 ). Gn-scca: Graphnet based sparse canonical correlation ana—
lysis for brain imaging genetics. In: Guo,Y. et al., (eds), Brain Informatics
and Health, Springer, London, UK, pp. 275—284.

Grosenick,L. et al. (2013) Interpretable whole—brain prediction analysis with
graphnet. NeuroImage, 72, 304—321.

Jiji,S. et al. (2013) Segmentation and volumetric analysis of the caudate nu—
cleus in alzheimer’s disease. Eur. ]. Radiol., 82, 1525—1530.

Lin,D. et al. (2013) Correspondence between fMRI and SNP data by group
sparse canonical correlation analysis. Med. Image Anal., 18, 891—902.

Parkhomenko,E. et al. (2009) Sparse canonical correlation analysis with appli—
cation to genomic data integration. Stat. Appl. Genet. Mol. Biol., 8, 1—34.

Ramanan,V.K. et al. (2014) APOE and BCHE as modulators of cerebral amyl—
oid deposition: a ﬂorbetapir pet genome—wide association study. Mol.
Psychiatry, 19, 351—35 7.

Vounou,M. et al. (2010) Discovering genetic associations with high—dimen-
sional neuroimaging phenotypes: a sparse reduced—rank regression ap-
proach. NeuroImage, 53, 1147—1159.

Witten,D.M. and Tibshirani,R.J. (2009) Extensions of sparse canonical correl-
ation analysis with applications to genomic data. Stat. Appl. Genet. Mol.
Biol., 8, 1—27.

Witten,D.M. et al. (2009) A penalized matrix decomposition, with applica-
tions to sparse principal components and canonical correlation analysis.
Biostatistics, 10, 515—534.

Yan,J. et al. (2014) Transcriptome-guided amyloid imaging genetic analysis via
a novel structured sparse learning algorithm. Bioinformatics, 30, i5 64—i5 71.

Yang,S. et al. (2012). Feature grouping and selection over an undirected graph.
In: Proceedings of the 18th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ACM, pp. 922—930.

Zou,H. and Hastie,T. (2005) Regularization and variable selection via the
elastic net.]. R. Stat. Soc.: Ser. B (Stat. Methodol.), 67, 301—320.

9103 ‘01; isanV uo salaﬁuv 50’] 0211110311123 JO AirsraAtuf] 112 /310'S[BHJnOprOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} papeolumoq

