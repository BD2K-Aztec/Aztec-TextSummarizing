ORIGINAL PAPER

doi:1 0. 1093/bioinfonnatics/btt760

 

Phylogenetics

Advance Access publication January 2, 2014

Optimization strategies for fast detection of positive selection

on phylogenetic trees

Mario Valle”, Hannes Schabauerz'e'l, Christoph Pacher4, Heinz Stockinger5,
Alexandros Stamatakis6'7, Marc Robinson-Rechaviz'3 and Nicolas Salamin2'3'*

1Swiss National Supercomputing Centre, Scientific Computing Group, Lugano, Switzerland, 2Department of Ecology
and Evolution, University of Lausanne, Lausanne, Switzerland, 3SIB Swiss Institute of Bioinformatics, Lausanne,
Switzerland, 4AIT Austrian Institute of Technology, Safety & Security Department, Vienna, Austria, 5Vital-IT Group, SIB
Swiss Institute of Bioinformatics, Lausanne, Switzerland, 6Heidelberg Institute for Theoretical Studies, Scientific
Computing Group and 7Karlsruhe Institute of Technology, Institute for Theoretical Informatics, Karlsruhe, Germany

Associate Editor: David Posada

 

ABSTRACT

Motivation: The detection of positive selection is widely used to study
gene and genome evolution, but its application remains limited by the
high computational cost of existing implementations. We present a
series of computational optimizations for more efficient estimation of
the likelihood function on large-scale phylogenetic problems. We illus-
trate our approach using the branch-site model of codon evolution.
Results: We introduce novel optimization techniques that substantially
outperform both CodeML from the PAML package and our previously
optimized sequential version SlimCodeML. These techniques can also
be applied to other likelihood-based phylogeny software. Our imple-
mentation scales well for large numbers of codons and/or species. It
can therefore analyse substantially larger datasets than CodeML. We
evaluated FastCodeML on different platforms and measured average
sequential speedups of FastCodeML (single-threaded) versus
CodeML of up to 5.8, average speedups of FastCodeML (multi-
threaded) versus CodeML on a single node (shared memory) of up
to 36.9 for 12 CPU cores, and average speedups of the distributed
FastCodeML versus CodeML of up to 170.9 on eight nodes (96 CPU
cores in total).

Availability and implementation: ftp://‘ftp.vital-it.ch/‘tools/FastCodeML/.
Contact: selectome@unil.ch or nicolas.salamin@unil.ch

Received on June 20, 2013; revised on December 19, 2013; accepted
on December 23, 2013

1 INTRODUCTION

The development of evolutionary models has a long tradition in
phylogenetics, and recent advances have enhanced our under-
standing of the molecular mechanisms involved. At the heart
of these advances is the democratization of the use of the likeli-
hood framework, which was made possible by algorithmic de-
velopments (Felsenstein, 1981) and the wide availability of

 

*To whom correspondence should be addressed.
7‘The authors wish it to be known that, in their opinion, the ﬁrst two
authors should be regarded as Joint First Authors.

powerful computing platforms. The surge of genomic data is,
however, pushing the limits of current implementations [e. g.
(Rannala and Yang, 2008)] and demands for the developments
of better and more efﬁcient ways to compute the phylogenetic
likelihood function (PLF).

The development of codon models is a good example to illus-
trate these current challenges and the beneﬁts that can be reached
by improving the efﬁciency of current likelihood calculations
(Gil et al., 2013). There are clear advantages to use codon
models in phylogenetics (Seo and Kishino, 2008), but these are
currently not widely used because of the large computational
burdens involved (Anisimova and Kosiol, 2009). Further, the
detection of positive selection has been facilitated by the devel-
opment of new codon models. However, their application to
genome-scale data comprising a large number of species, or in-
dividuals in the case of population genomic studies, remains
challenging. Thus, there exists an urgent need for improved im-
plementations and novel optimization techniques to analyse
emerging genomic datasets (Lemey et al., 2012; Murrell et al.,
2012; Schabauer et al., 2012).

The prevalent approach for detecting positive selection in pro-
tein-coding genes is to use Markov models of codon substitution
to estimate the ratio of non-synonymous to synonymous changes
along the branches of a phylogenetic tree (Yang, 2006). The
branch-site model (BSM) [Yang, 2006 (Section 8.4); Zhang
et al., 2005] allows to detect positive selection that affects a
subset of codon sites for a subset of branches in a phylogenetic
tree. This model is particularly useful to perform interspeciﬁc
comparisons and is probably the most widely used approach
for this speciﬁc purpose. The test compares a model that assumes
positive selection on one branch or on a set of a priori specified
branches (hypothesis H1) with a null model that does not incorp-
orate positive selection (hypothesis H0). If the test is signiﬁcant,
the Bayes Empirical Bayes (BEB) method is used to compute the
posterior probability of each particular codon to evolve under
positive selection along the speciﬁed branches Wang et al., 2005).
In CodeML, the test is usually applied iteratively and independ-
ently to each branch of a given phylogenetic tree (Anisimova and
Yang, 2007; Studer et al., 2008).

This approach is compute bound, and although alternatives
have recently been proposed, the limiting factor of such analyses

 

© The Author 2014. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non—Commercial License (http://creativecommons.org/licenses/by—
nc/3.0/), which permits non—commercial re—use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re—use,

please contact journals.permissions@oup.com

Vol. 30 no. 8 2014, pages 1129—1137

112 /310's112u1nofp10}x0"sotJBurJOJutotq/ﬁduq 11101} pQPBOIIIAAOG

9103 ‘Og anﬁnV uo ::

M. Valle et al.

 

still lies with the repeated calls to compute the PLF.
For example, the estimation of positive selection on a large gen-
omic vertebrate dataset (Proux et al., 2009) shows the enormous
computational requirements of such analyses [approx. 100 CPU
years for each release of the Selectome database (Kraut et al.,
2010)]. As a consequence, large gene trees, comprising more than
100 sequences, are usually excluded and faster implementations
of the BSM are urgently needed. This clearly illustrates the need
to further optimize current software and to develop more efﬁ-
cient computational approaches for maximum likelihood infer-
ence on phylogenetic trees.

Several recent studies introduced techniques for efﬁciently
computing positive selection on the branches of a phylogenetic
tree. One idea is to use stochastic mapping to count substitutions
along the branches of a tree and thereby derive dN/dS ratios
(Dutheil et al., 2012; Lemey et al., 2012). While this approach
is fast, it is computationally distinct. Alternatively, new models
have been proposed to avoid the likelihood ratio test (LRT) es-
timation of positive selection for all branches of the tree. Instead,
branch assignments are considered as a random effect within a
mixed effect framework (Murrell et al., 2012). Their model not-
ably differs from the BSM (Zhang et al., 2005) in that putative
positive selection is not optimized on a priori deﬁned branches,
but over a subset of branches which are determined by the soft-
ware. This technique reduces the computational cost of the test,
but the accuracy and robustness of this new model is not yet fully
characterized. Moreover, the authors introduced solutions for
parallelizing BSM computations, but the parallel approach is
not discussed in their article.

The bottleneck in efﬁciency of phylogenetic software is com-
monly the PLF, as the majority of runtime is spent here. In
(Stamatakis, 2011, p.2), the PLF is reported to consume >95%
of total execution time in maximum likelihood and Bayesian
tools for phylogenetic tree reconstruction. Although this was
estimated when searching for the best tree topology, which is a
key component of phylogenetic computations but not the focus
of this article, the PLF is still the core element in all phylogenetic
applications using maximum likelihood. All these areas would
therefore beneﬁt from an optimized PLF. Recent discussions
have proposed to use data augmentation strategies to speed up
the likelihood calculations by using heuristics to simplify the es-
timation of the conditional vectors at each node (Rodrigue and
Aris-Brosou, 2011). However, there are still opportunities for
improving the PLF with respect to sequential efﬁciency and par-
allelization techniques.

Our main objective is therefore to propose methodological and
algorithmic improvements and parallelization strategies to com-
pute the PLF without modifying the underlying evolutionary
model. Our optimizations and parallelizations yield substantial
speedups in the likelihood computations. Hence, we can apply
the BSM to large trees of several hundreds of sequences
and obtain results in feasible times. These computational opti-
mizations are thus of broad applicability to further likelihood-
based phylogenetic software, including but not limited to
nucleotide- and amino acid-based phylogenetic analyses in
both the maximum likelihood and Bayesian frameworks
(Nielsen, 2005).

1.1 Number of elementary tree operations

In the BSM framework, four site classes 0, 1, 2a and 2b are
applied to model combinations of purifying selection, neutral
evolution, and positive selection on foreground and background
branches. When computing hypotheses H0 and H1, each site class
has its distinct proportion according to its contribution to the
overall likelihood (of. the supplementary material for an intro-
duction to the BSM). These proportions only depend on the two
parameters [)0 and p1; each site class has a specific a) value for its
selective pressure in the foreground and in the background. we is
in the interval (0,1), (01 = 1 and either (1)2 > 1 (foreground for H1)
or (1)2 = 1 (foreground for H0). QMLZ} corresponds to comm},
respectively.

Computing the likelihood requires computing the transition
probabilities for a given branch length t by computing the
matrix exponential P, 2 eg’ 2 esn’, where Q is the instantaneous
substitution rate matrix, S is the symmetric codon substitution
matrix and 1'1 is the diagonal matrix of codon frequencies. The
resulting probability matrix P, is used to update the correspond-
ing conditional probability vector (CPV) w, that is, w/ = P,w.
Each CPV models the site-wise transition between 61 codon
states (universal genetic code) along each branch of the phylo-
genetic tree. This operation is applied to all sites of the multiple
sequence alignment (MSA) and to all nodes of the tree by means
of a post-order tree traversal.

The CPU-intensive computation of the CPV entails the fol-
lowing three computational kernels that operate on real dense
matrices (similar to SlimCodeML, see Section 2.1.2): (i) eigende-
composition of a symmetric matrix [see, e.g. (Bai et al., 2000)],
(ii) multiplication of a matrix by its transpose (resulting in a
symmetric matrix) and (iii) multiplication of a symmetric
matrix by a vector.

1.1.] How many decompositions? To compute eQ’ we need to
decompose Q for each distinct combination of parameters K
(transition to transversion rate), nj and a). The nj are constant
over site classes and parameter optimization steps; K may change
at each parameter optimization step (but is constant over site
classes); a) varies among optimization steps and site classes. For
each distinct value of a), Q is distinct and therefore needs to be
decomposed separately. There are three distinct a) values over all
site classes; hence, we need to decompose three Q matrices in the
first parameter optimization step. For subsequent steps, (01 = 1
remains constant, but Q1 may change because of a new K value.
The total number of Q decompositions does not depend on the
number of branches in the tree nor on the number of sites in the
MSA. In the general case, the number of Q matrices depends on
the number of unique substitution matrices in the model, which
can be large in mixture models [e. g. (Lartillot and Philippe, 2004;
Venditti et al., 2008)]. With respect to other evolutionary models,
similar optimizations may be applicable.

1.1.2 How many matriximatrix multiplications? P, has to be
computed for each combination of Q and t. For our case of
binary trees, the number of branches in the phylogeny equals
2n—2 where n is the number of extant taxa. For each distinct
Q, branches have to be computed separately. The BSM applies
Q0 and Q1 to each branch, but Q2 only to foreground branches.
In other words, P, has to be computed for all branches using Q0

 

1130

112 /310's112u1nofp10}x0"sotJBurJOJutotq/ﬁduq 11101} pQPBOIIIAAOG

9103 ‘Og isnﬁnV uo ::

FastCodeML

 

Site class

  

CPV1 CPV2 CW3 CW4 CPV5 CFVB CPV10 CPV11 CPV12 CPV13

CPV17 CPV18 CPV19 CPV20 CPV21 CFV25 CPVZS CPV27 CPVZS CPV29

 

2a

Foreground
branch

 

Site 1

Site 2

Fig. 1. Analysis on how many elementary subtree computations are ne-
cessary in the branch-site model; CPVm correspond to m distinct condi-
tional probability vectors, where matching m need to be computed only
once; Q{0,1,2} identify three distinct Q matrices for distinct comm)
values

and Q1 (site classes 0 and 1), and in addition on the foreground
branch(es) by using Q2 (site classes 2a and 2b). Therefore,
we need to compute P, 2m +l times for m branches in
the phylogeny and l foreground branches; this yields
(2 x (2n — 2)) + 1 = 4n — 3 branches when using a single fore-
ground branch. Overall, we need to compute 17 distinct P matri-
ces in our example 1. This matriximatrix multiplication is also
applied in further evolutionary models based on substitution
matrices.

1.1.3 How many matrixivector computations? In a straightfor-
ward approach, each CPV is computed along each branch for all
sites and all site classes. In our example this makes
8 x 4 x 2 = 64 CPV computations. If a CPV connected to a
leaf is computed on ‘clean’ data [no ambiguity symbols in
MSA (Comnish-Bowden, 1985)], the CPV at the leaf only con-
tains a single 1 (0 elsewhere). In this case, computing the resulting
CPV simpliﬁes to selecting the corresponding column of the P
matrix. In the general case, an upper limit of the number of
involved matrixivector multiplications per site class is the

number of branches in the phylogeny x the number of sites in
the MSA. Certainly, this number can be decreased depending on
similarities in the codons as discussed in Section 2.1.1 (‘subtrees
reuse’). Likewise, this step is important to all other evolutionary
models based on substitution matrices.

Further computational savings are possible. In this context, we
refer to a ‘subtree’ as a connected part of the phylogeny where at
least one node is a leaf. Whenever a particular branch of a single
site applies the same P and all other CPVs of its subtree match,
the particular CPV has a ‘twin’ in another site class and needs to
be computed only once. In Figure 1, such matching CPVs are
identiﬁed by matching indexes. For example, CPV23 appears in
site class 1 and in site class 2b, as also CPV20 and CPV21 have
twins, and they pairwise apply matching P matrices (here, all
based on Q1). These redundancies are caused by matching (00
values for site classes 0 and 2a and by matching (01 values for site
classes 1 and 2b. In our example, this means that only 40 out of
64 (62.5%) CPVs have distinct values and will hence have to be
computed. CPVs are computed recursively via a postorder tra-
versal propagating from the leaves towards the root (Felsenstein,
1981). Hence, for the BSM in general, the number of distinct
CPVs depends on the location of the foreground branch in the
tree (the closer to the root, the less CPV computations are
required).

2 IMPROVEMENTS

Here we discuss optimization techniques that we propose. Note
that we have not added any heuristics, and each of the following
improvements is supposed to be beneﬁcial independent of the
number of species and independent of the number of alignment
sites. Speciﬁc implementation issues are described along with
each optimization technique.

2.1 Sequential improvements

2.1.] Subtrees reuse The per-site likelihoods for a MSA are
independent of each other and can thus be computed in an ar-
bitrary order. If two or more sites of the MSA are identical, it
sufﬁces to only compute the logarithmic likelihood (lnL) on one
site and multiply it by the number of identical sites to obtain the
total lnL. This technique is used in most likelihood-based soft-
ware, but there are further redundant computations caused by
re-occurring patterns in the MSA.

In each subtree, there is a potential to economize CPV com-
putations for different sites of the MSA. If the same state appears
at two or more sites of a sequence, all occurrences yield identical
CPVs at the particular leaf. If the patterns of the sub-alignment
induced by a subtree match are identical for two or more sites,
the corresponding CPVs for the two sites are also identical.
However, identical patterns in the sub-alignments induced by a
subtree need to be identiﬁed first. The identiﬁcation of such
identical patterns in sub-alignments can be done, e. g. by search-
ing (i) sequentially or (ii) using a symbol table (Sedgewick and
Wayne, 2011, p.361). In the latter case, the key is the index of the
CPV within the tree, and the value associated with the key is its
CPV. In the straightforward approach (i), there are no costs on
storing values, but up to m i 1 lookups for a matching subpat-
tern, where m is the length of the MSA. For huge MSAs, it may

 

1131

112 /310's112u1nofp101x0"sotwuiJOJutotq/ﬁduq 111011 papBo1u/noq

9103 ‘0g isanV uo ::

M. Valle et al.

 

(a)
A
1

GC TTA GAT CCA AAC AGC TTA CTC ATA CAC
K J J K J K J K J
Y X 1 Y X

® ®

Z Z

R K

AGC 'I'I'A GAT CCA AAC CTC ATA CAC

Fig. 2. Subtrees reuse strategy depicted for two (not necessarily neighbor-
ing) sites in the MSA; in (a) subtree (1) contains identical codons for both
sites; consequently, in (b) the CPVs for both sites are identical and need to
be computed only once (dotted line)

be advantageous to implement the second approach, where the
additional cost for storing or linking site patterns is compensated
by a faster lookup. In FastCodeML, we identify reusable subtree
patterns in a preprocessing step and tag each node with the
codon sequence identiﬁed by the subtree rooted in this node.
Subsequently, a lookup of these tags for all sites with identical
subtrees is done. Once identified, the CPV that can be re-used is
linked via a pointer in the reusing tree, that is, this saves the costs
of computing this particular CPV. The unused subtree can be
freed to reduce memory consumption. In the example of
Figure 2, computing the two CPVs incident to two leaves in
box (D and the CPV at ® are redundant, because both codon
sites feature an identical subtree: all involved CPVs match. Thus
three CPV computations can be saved.

Related techniques for extending pattern detection and re-use
in the MSA to the subtree level have already been proposed
(Izquierdo-Carrasco et al., 2011; Stamatakis et al., 2002;
Sumner and Charleston, 2010). However, they focus on detecting
patterns and avoiding redundant likelihood computations on
trees whose topologies change in the course of ML tree search.
For dynamically changing trees, a trade-off between the pattern
detection and memory storage costs and the amount of saved
computations needs to be achieved. To reduce the cost of pattern
detection, the initial implementation of the Subtree Equality
Vector (SEV) technique (Stamatakis et al., 2002) only considered
subtree patterns that contained a single identical character. The
book keeping was subsequently further simpliﬁed to sites con-
sisting entirely of gaps (Izquierdo-Carrasco et al., 2011). In
Kosakovsky Pond and Muse (2004), the authors suggest to
sort nucleotide-based MSAs by site similarity to avoid redundant
computations. This approach minimizes memory consumption,
as only a subset of sites needs to be kept in memory. However,

this incurs additional costs for rearranging the sites in order
to maximize the number of lookups from neighboring sites.
The memory consumption for our application scenario
(Selectome database updates) does not represent a limiting
factor. Hence, all CPVs can be kept in memory, avoiding the
expensive reordering of sites. However, especially for memory-
intensive approaches, it may be more effective to keep only a
subset of all CPVs in memory and consider site sorting.

2.1.2 New matrix exponential and CPV computation In
Schabauer et al. (2012), we transformed the problem of comput-
ing the matrix exponential of non-symmetric Qt into a symmetric
problem as follows: we deﬁne the symmetric matrix A 2: Hi5 11%
and compute its eigendecomposition A = X AX T. By introducing
Y 2: XeA’/2, the matrix exponential of Qt becomes
eg’ 2 1T1 YYTrit.

An additional modiﬁcation transforms the final asymmetric
matrixivector multiplication eQ’w into a symmetric matrixi
vector product:

w/ = eQ’w = WWI-1w), (1)
whereIA’ = l'ITl/ZXeA’ﬂ. (2)

Note that PI” is by construction a symmetric matrix, whereas
11’”2 YYTI'II/2 is generally asymmetric. The advantage of this
modiﬁcation is that the symmetry reduces the number of neces-
sary matrix memory accesses by approx. 50% (Golub and Van
Loan, 2013, p.18). This technique has been implemented in
FastCodeML.

2.1.3 LRT optimization When optimizing parameter values for
H0 and H1 one after the other, one can save on parameter opti-
mization steps. Each step in the parameter optimization proced-
ure improves the associated lnL of the tree until convergence has
been reached. In this discussion, the optimizer may modify all
parameter values at each single step. One can either (i) optimize
H0 ﬁrst with high accuracy and iteratively improve H1 after-
wards: once 2(1nL(H1) — lnL(H0)) becomes larger than
(xf)Tl(1 — or), the parameter optimization for H1 can be stopped
because the LRT is already significant. This potentially saves
optimization steps for H1. Or we can (ii) optimize H1 ﬁrst,
then proceed analogously: the parameters of H0 are optimized
until 2(1nL(H1) — lnL(H0)) becomes smaller than (xf)Tl(1 — or).
In general, a signiﬁcant LRT (i.e. detecting positive selection) is
a relatively rare event (Kosiol et al., 2008; Studer et al., 2008).
Strategy (i) saves optimization steps if positive selection occurs;
strategy (ii) saves optimization steps if not. Consequently, with-
out prior knowledge of the frequency of occurrence of positive
selection in the MSA at hand, strategy (ii) (implemented in
FastCodeML) will yield larger savings. If the LRT is signiﬁcant,
a BEB is applied to identify the sites under positive selection.
Otherwise, FastCodeML does not execute the BEB, in contrast
to CodeML. In the general case, this optimization is applicable if
different models are compared, where each of them is optimized
iteratively.

 

1132

112 /310's112u1nofp101x0"sotwuiJOJutotq/ﬁduq 111011 papBo1u/noq

9103 ‘0g isanV uo ::

FastCodeML

 

2.2 Pal-allelization

While the parallelization of ML-based nucleotide- protein- and
codon models has already been addressed (Stamatakis, 2011)
(e.g. RAxML, IQPNNI, HyPhy), it has mostly been in the con-
text of tree topology optimization, and not for the likelihood
itself. The main challenge in parallelizing ML-based phylogeny
computations comes from the tree structure that leads to an ir-
regular domain decomposition (Tomko, 1995). An efﬁcient par-
allelization of the BSM is even more challenging due to its site
classes and dependencies in between.

Our implementation optimizes simultaneously all the param-
eters. The maximizer acts as an impenetrable boundary for par-
allelization, and we distinguish parallelization ‘above’ (coarse-
grain) and ‘within’ (ﬁne-grain) this boundary (of. supplementary
material, Fig. 1).

2.2.1 Coarse-grain parallelization: Gene-wise parallelization.
Because distinct genes typically have different evolutionary his-
tories with distinct branch lengths and evolutionary parameters,
phylogenies for genes are commonly estimated independently for
each gene. Consequently, single genes cannot be concatenated
into multi-gene alignments to attain high scalability by means
of a ﬁne-grain parallelization of the likelihood function [see, e. g.
(Stamatakis and Ott, 2009)]. Here we test for selection independ-
ently (gene-wise), these analyses can be carried out in an embar-
rassingly parallel way [see, e.g. (Foster, 1995, p.21)].

Foreground branch parallelization. A further BSM paralleliza-
tion option is the simultaneous analysis of distinct foreground
branches. This is possible because we want to test for positive
selection on each branch of a given phylogeny. Thus, the 2n — 3
tests for positive selection, where n is the number of taxa, can be
conducted in parallel by duplicating the tree data structure and
CPVs.

Under this parallelization strategy, a dedicated master process
broadcasts all model parameters, tree topologies and branch
lengths to all worker nodes. The workers then conduct the
tests independently of each other on different foreground
branches of the same tree. Afterwards, the worker nodes return
the estimated parameter values and the lnL scores to the master
process. We implemented this approach using MPI (Message
Passing Interface Forum, 1994). The foreground-branch based
parallelization can be combined with a site-wise ﬁne-grain par-
allelization of the per-tree likelihood computations (Section
2.2.2) into a hybrid parallelization scheme.

Hypotheses parallelization. Note that for each foreground
branch, hypotheses H0 and H1 can be computed independently
and simultaneously, thus increasing the degree of parallelism.
However, the simultaneous computation of H0 and H1 prevents
us from using the aforementioned LRT optimization (Section
2.1.3). Although the LRT and the subsequent BEB must be
computed after H0 and H1, they can be parallelized between
different foreground branch computations. This parallelization
strategy can be applied whenever two evolutionary models are
compared. It is implemented in FastCodeML via the same
master-worker scheme.

2.2.2 Fine-grain parallelization: Site-wise parallelization. A
common way to parallelize likelihood computations on shared
memory architectures is by parallelizing over the sites of the

WWW/W

Group 0 Group 1 Group 2 Group 3
. A A

Fig. 3. Load balancing strategy: the sites of the tree are grouped so that
each group depends only on groups at its left (continuous lines). A tree
can be moved to a group to its right (dashed line) only if it has no
dependencies from other trees in intermediate groups

MSA. This site-wise parallelization can be implemented using
OpenMP or POSIX Threads. MPI-based implementations exist
but focus on large MSAs that are outside the scope of this article.
However, while our subtree patterns re-use scheme (Section
2.1.1) reduces the number of computations along the branches,
it poses a load balance challenge: (i) a particular CPV for a site
can only be computed after the site whose results it reuses (i.e.
data dependency) has been computed and (ii) a site that reuses a
previously computed CPV exhibits a smaller workload which
leads to load imbalance.

The load balancing strategy we use in FastCodeML subdivides
the alignment sites into groups such that each group exclusively
reuses subtrees from the previous groups (Fig. 3). Each group is
assigned a rank value starting from zero. CPVs from groups with
lower rank values can potentially be reused. The ﬁrst group does
not reuse any subtree. All subtrees of a group can be parallelized,
because they are independent of each other. The groups are then
computed sequentially in order of rank. To balance the load for
each group, subtrees can be moved to higher ranked groups. To
increase parallelism, the trees of each group are replicated for
each site class that should be computed until no lower rank
group depends on it. The parallelization inside each group has
been implemented using OpenMP.

This site-wise parallelization strategy including load balancing
can likewise be applied to nucleotide- or protein-based MSAs.
The parallel performance may vary due to different computa-
tional load per site.

2.3 Implementation

FastCodeML has been implemented from scratch (except for the
BEB that was largely taken from the CodeML codebase) in ISO
C++ 2003 utilizing BLAS and LAPACK for linear algebra op-
erations, and Spirit (http://www.boost.org/doc/libs/release/libs/
spirit/) for tree parsing. We use the parameter optimization code-
base of CodeML.

3 EVALUATION

We measure median runtimes of 10 individual runs for each

evaluation (three on the large scale analysis in Section 3.5).

Speedup values are determined by S 2%, where T1 is the

 

1133

112 /310's112u1nofp101x0"sotwuiJOJutotq/ﬁduq 111011 papBo1u/noq

9103 ‘0g isanV uo ::

M. Valle et al.

 

runtime (elapsed time, wall-clock time) of the reference execution
and T2 the runtime of the execution to be evaluated on the same
dataset; for a relative speedup T1 and T2 denominate runtimes of
the same executable, while for the absolute speedup T1 is strictly
the original CodeML. Initial branch lengths were read from ﬁle,
while model parameters are initialized randomly. Memory con-
sumption of CodeML, SlimCodeML and FastCodeML for these
datasets is not a limiting factor and therefore not performance
critical. Although a single executable can be used for all subse-
quent evaluations, we built sequential, OpenMP parallelized,
MPI parallelized and hybrid executables separately. A summary
of the platforms used can be found in the supplementary
material.

3.1 Datasets

Table 1 contains the six datasets we used for evaluation. With
respect to the Selectome database, these empirical datasets are
representative for the cases: (D1) small number of species/
medium sequence length; (D2) small number of species/large se-
quence length; (D3) medium number of species/small sequence
length; (D4) large number of species/short sequence length; (D5)
a simulated dataset with positive selection based on dataset D1
(using PAML’s evolver choosing ‘evolverNSbranchsites’ for the
BSM with (1)2 = 5). Finally, we analyse in D6 a very large rbcL
dataset (Grass Phylogeny Working Group II, 2012) which
cannot be processed in a feasible time by CodeML.

3.2 Accuracy

In Table 2 we analyse the accuracy of FastCodeml with respect
to lan and LRT scores. We use SlimCodeML as a proxy for
good accuracy, as it gives very similar results as CodeML
(Schabauer et al., 2012), which is the established gold standard.
We note that the accuracy of computed lan is much higher than
typically required to discriminate between signiﬁcant and insig-
niﬁcant LRTs.

3.3 Sequential runtimes

Sequential speedups of FastCodeML (single-threaded) versus
CodeML and SlimCodeML for five datasets (H0 and H1, re-
spectively) on platform Macpro (of. supplementary material)
are depicted in Figure 4; here, FastCodeML includes the follow-
ing improvements: faster matrix exponentiation (Section 2.1.2)
and subtrees reuse (Section 2.1.1). LRT optimization (Section
2.1.3) is not considered, as either H0 or H1 is computed per
run. We observe speedups of FastCodeML versus CodeML
ranging from 2.6 to 5.8. The sequential FastCodeML is signiﬁ-
cantly faster than both CodeML and SlimCodeML on all five
datasets.

3.4 Parallel runtimes

3.4.1 Site-wise parallelization Figure 5 shows the scaling of
FastCodeML on a site-wise (OpenMP based) parallelization

Table 1. Test datasets of our analyses; remaining branches is the percentage of non-redundant branches for the given data over all sites of the alignment;
dataset D5 is generated based on ENSGT00390000016702.Primates.1 with an 2 5

 

 

Abbr. Full name No. of species No. of branches Remaining branches [%] Length (codons)
D1 ENSGT00390000016702.Primates. 1 7 12 37 .74 299
D2 ENSGT00530000063518.Primates.1 95 188 75.49 39
D3 ENSGT00550000073950.Euteleostomi.7 25 48 56.31 67
D4 ENSGT00580000081590.Primates. 1 6 10 20.92 5004
D5 Generated by evolver (PAML) 7 12 38.04 282
D6 Grass_rbcL 506 1242 19.54 414

 

Table 2. Accuracy of SlimCodeML and FastCodeml on Macpro; Aﬂowl) is the absolute difference of lan comparing either SlimCodeML or

FastCodeML with CodeML on H0 (H1), respectively

 

 

Dataset A110 AH, LRT pos. selection
SlimCode versus CodeML D1 1.5 > 10’5 3.5 > 10’6 5.4 > 10’5 no (./)

D2 3.5 >10T1 5.7 >10T2 7 >10T1 110 (/)

D3 7.8 >10T6 9.9 >10T3 2.2 >10T5 110 (/)

D4 9.] >10T8 9.6 >10T7 2.3 >10T6 110 (/)

D5 8.5 >10T10 6.8 >10T11 10.4 site 239 (/)
FastCodeML versus CodeML D1 1.1 > 10’2 4.5 > 10’6 —2.1 > 10’2 no (./)

D2 3.4 >10T1 2.8 >10T2 —5.l >10T1 110 (/)

D3 2.2 >10T2 2.1 >10T3 —3.9 >10T2 110 (/)

D4 1.5 >10T6 1.2 >10T6 —4.5 >10T9 110 (/)

D5 4.9 >10T10 1.6 >10T9 10.4 site 239 (/)

 

Note: ‘./’ indicates agreement of the computed result with CodeML.

 

1134

112 /310's112u1nofp101x0"sotwuiJOJutotq/ﬁduq 111011 papBo1u/noq

9103 ‘0g isanV uo ::

FastCodeML

 

 

Sequential overall speedups

 

 

 

9 I I
FastCodeML vs. CodeML H0 
8  FastCodeML VS. CodeML H1
FastCodeML vs. SlimCodeML H0
7  FastCodeML VS, SlimCodeML (H1) 

 

 

Speedup

 

 

 

 

 

 

1 I I I
D1 D2 D3 D4 D5
Dataset

 

Fig. 4. Sequential speedups of FastCodeML in comparison with
CodeML and SlimCodeML on Macpro for H0 and H1, respectively

Relative scaling site-wise parallelisation

 

 

 

 

 

 

 

 

  

 

 

    

 

 

 

 

12 . . . . . . . . . .
11
10
9
8
3
.D 7
8
a, 6
m
5
4
3 Linear (hypothetical) speedup  _
Relative speedup FastCodeML (no reuse -
2 _. Relative speedup FastCodeML (reuse
1 ’3' I I I I I I I I I I
1 2 3 4 5 6 7 8 9 10 11 12
Absolute scaling site-wise parallelisation
24
22
20
18
16
3
.D 14
8
a 12
m
10
8
6
4 Abs. sp. FastCodeML (no reuse vs. CodeML ------- I-
Abs. sp. FastCodeML (reuse vs. CodeML -I--
2

 

1 2 3 4 5 6 7 8 9 10 11 12
Cores

Fig. 5. Parallel site-wise relative (top) and absolute (bottom) speedups of
FastCodeML on Castor on dataset D2 for H1

strategy for dataset D2 on 1712 CPU cores (one thread per core);
we observe relative speedups comparing FastCodeML in 1 . . . n
versus 1 threads, reaching 11.1 for 12 cores without subtrees
reuse, and speedups up to 7.6 for 12 cores with subtrees reuse.
These relative speedups correspond to absolute speedups versus
CodeML of up to 23.4 without subtrees reuse, and speedups up
to 19.9 with subtrees reuse. While scaling of subtrees reuse is

Scaling foreground-branch based parallelisation

 

 

 

 

 

 

 

 

 

 

7 . . . . . 
a. 
:1
'8 4
o
a.
U]

3

2 _ Relative speedup FastCodeML —I.— ____________ ___

Linear (hypothetical) speedup --------- --
1 I I I I I
1 2 3 4 5 6 7

Worker nodes

Fig. 6. Parallel foreground branch (MP1 based) relative speedups of
FastCodeML for dataset D3 on Castor for H1; only a single CPU core
per node was used

slightly worse than without subtrees reuse, absolute runtimes
on this particular platform and dataset suggest to enable subtrees
reuse on 1711 cores but not on 12. The worse scaling of subtrees
reuse is presumably caused by load imbalance. Due to differences
in the sequential performance of subtrees reuse, we also expect
the performance of parallel subtrees reuse to vary with different
datasets. In general, the effectiveness of parallel subtrees reuse is
a trade-off between the number of redundant branches versus the
data dependencies introduced.

3.4.2 Foreground branch-based parallelization Figure 6 depicts
the relative scaling of FastCodeML on a foreground-branch
based parallelization strategy. The evaluation has been done
for dataset D3 on 177 worker nodes (single thread per node).
Due to the masteriworker scheme used, performance gains are
observed for two or more worker nodes. The analysis is done for
all possible 22 foreground branches, where the runtime for
CodeML is measured only on a single foreground branch but
multiplied by 22; running CodeML on all foreground branches is
expected to consume more than a day. We observe relative
speedups of up to 5.9 on 7 worker nodes, which corresponds
to absolute speedups from 3.3 to 19.4. In general, the relative
speedup for foreground branch-based parallelizations benefits
from a high ratio of foreground branches to available nodes,
as the workload can more easily be divided into balanced parts.

3.4.3 Hybrid parallelization Figure 7 depicts absolute scaling of
FastCodeML on a hybrid (foreground branch and site-wise) par-
allelization strategy implemented using OpenMP and MP1 on
177 worker nodes, where all 12 CPU cores are used.
Corresponding runtimes, relative and absolute speedup values
are summarized in Table 3. We observe relative speedups up to
6.3 on 7 worker nodes, which corresponds to absolute speedups
up to 170.9.

3.5 Large scale analysis

A large scale analysis has been conducted to prove the use of
FastCodeML beyond the capabilities of CodeML. In initial tests,
we verified that dataset D6 achieves its best runtime performance
on platform Castor (of. supplementary material) by using all 12

 

1135

:: 112 /310's1izumofp101xo"sotJBuiJOJutotq/ﬁduq 111011 papBo1u/noq

910z ‘09 Isnﬁnv 110

M. Valle et al.

 

Speedups hybrid parallelisation
192 I I I I I

 

168

/

 

 

120

96 //

72 /

48

24 Absolute speedup FastCodeML vs. CodeML —l+—

1(12) 2(24) 3(36) 4(48) 5(60) 6(72) 7(84)

Worker nodes (cores)

 

Speedup

 

 

 

 

 

 

Fig. 7. Parallel hybrid (OpenMP and MP1 based) scaling of
FastCodeML for dataset D3 on Castor for H1

Table 3. Overall parallel performance of FastCodeml versus CodeML on
Castor for dataset D3 on all possible foreground branches for H1;
CodeML runtime for absolute speedups is extrapolated from computing
a single foreground branch

 

 

Worker FastCodeML Rel. Abs.
nodes (cores) runtime [s] speedup speedup
1 (12) 429 1 27.6
2 (24) 218 2 54.2
3 (36) 151 2.9 78.4
4 (48) 114 3.8 103.7
5 (60) 93 4.7 126.5
6 (72) 81 5.4 145.3
7 (84) 69 6.3 170.9

 

available cores per node and by reusing subtrees (Section 2.1.1).
We analysed D6 for H0 and H1 running FastCodeML (multi-
threading) on 12 CPU cores and determined average runtimes of
three test runs. The average runtime of FastCodeML on dataset
D6 is 21.9 h for H0 and 31.9 h for H1. Due to time restrictions, we
evaluated only a single iteration of CodeML for D6 which took
2.2h on H0 (367 iteration steps) and 2.3h on H1 (426 iteration
steps) on the same platform. As we apply the same parameter
optimization codes, we use the average number of optimization
steps of FastCodeML on dataset D6 for the following speedup
metric: we extrapolate that CodeML would have finished execut-
ing in approximately 2.2 x 367 = 807.4 h (i.e. ca. 33.6 days) for
H0 and 2.3 x 426 = 979.8 h (i.e. ca. 40.8 days) for H1. The esti-
mated speedups comparing the single threaded CodeML with
FastCodeML running in 12 threads is thus 36.9 for H0 and
30.7 for H1. In this example, the LRT optimization saves 268
optimization steps for H1 (63%).

4 CONCLUSIONS

We introduced here three sequential code optimizations: an im-
proved matrix exponential, subtrees reuse and LRT optimization.

We observed signiﬁcant speedups versus both CodeML and our
previous version SlimCodeML, and the ﬁrst two optimizations
can be used in various likelihood computations in phylogenetics.
Moreover, we present a parallelization strategy that uses a fine-
grain and a coarse-grain approach. Overall, our improvements
allow for testing selection on phylogenetic trees which exceed
the possibilities of the original CodeML software; this is crucial
to tackle the genomic data avalanche. The discussed improve-
ments are motivated by the branch-site model but can, due to
the likelihood framework, be extended to nucleotide— and amino
acid-based MSAs as well as Bayesian approaches. We brieﬂy
identiﬁed such opportunities where applicable, but an extensive
discussion is subject to future work.

The optimization of the likelihood surface for phylogenetics
problems is complex and we have started experimenting with the
alternative parameter optimizers available in NLopt (http://ab-
initio.mit.edu/wiki/index.php/NLopt). It may be interesting to
compare different implementations of the BroydeniFletcheri
GoldfardiShanno (BFGS) optimization method, but a deeper
investigation of the global and derivative-free optimizers is
needed to better understand the potential solutions to find the
maximum likelihood estimator for complex evolutionary models.

In a future version the dependencies between nodes could be
modelled as a directed acyclic graph and the parallelism be based
on a dataﬂow model WarKhan et al., 2011) to study and poten-
tially further improve parallel performance. Moreover, the site
classes could be included into the dependency graph. This way a
more ﬁne-grained parallelism could be achieved. Increasing the
parallel performance becomes crucial with the trend of more
parallelism in future computer platforms (Dongarra, 2012).

ACKNOWLEDGEMENTS

We thank Sebastien Moretti and Walid Gharib for providing test
datasets and feedback on FastCodeML. The computations on
Castor were performed at the Swiss National Supercomputing
Centre (http://www.cscs.ch), and those on Vital-IT at the Vital-
IT (http://www.vital-it.ch) Center for high-performance comput-
ing of the SIB Swiss Institute of Bioinformatics.

Funding: This work is supported by the Swiss Platform for High-
Performance and High-Productivity Computing (HP2C), the
Swiss Federal Government through the Federal Office of
Education and Science, the Swiss National Science Foundation
(grant CR32I3_143768) and Etat de Vaud and the Vienna
Science and Technology Fund (WWTF) through project
ICT10-067 (HiPANQ).

Conflict of Interest: none declared.

REFERENCES

Anisimova,M. and Kosiol,C. (2009) Investigating protein—coding sequence evolu—
tion with probabilistic codon substitution models. Mol. Biol. Evol., 26, 25$271.

Anisimova,A. and Yang,Z. (2007) Multiple hypothesis testing to detect lineages
under positive selection that affects only a few sites. Mol. Biol. Evol, 24,
121971228.

Bai,Z. et al. (2000) Templates for the Solution of Algebraic Eigenvalue Problems: A
Practical Guide. SIAM, Philadelphia.

Comnish—Bowden,A. (1985) Nomenclature for incompletely speciﬁed bases in nu—
cleic acid sequences: recommendation 1984. Nucleic Acids Res., 13, 302173030.

 

1136

112 /310's1eu1n01p101x0"satieuiJOJutotq/ﬁduq 111011 papeo1umoq

9103 ‘0g1sn8nv uo ::

FastCodeML

 

Dongarra,J.J. (2012) High performance computing systems: status and outlook.
Acta Numerica, 21, 3797474.

Dutheil,J.Y. et al. (2012) Efﬁcient selection of branch—speciﬁc models of sequence
evolution. Mol Biol. Evol, 29, 186171874.

Felsenstein,J. (1981) Evolutionary trees from DNA sequences: a maximum likeli—
hood approach. J. Mol. Evol, 17, 3687376.

Foster,I.T. (1995) Designing and Building Parallel Programs. Addison—Wesley,
Reading, Mass.

Gil,M. et al. (2013) CodonPhyML: fast maximum likelihood phylogeny estimation
under codon substitution models. Mol Biol. Evol, 30, 127071280.

Golub,G.H. and Van Loan,C.F. (2013) Matrix Computations. 4th edn. Johns
Hopkins University Press, Baltimore, MD.

Grass Phylogeny Working Group II. (2012) New grass phylogeny resolves deep
evolutionary relationships and discovers C4 origins. New Phytol., 193, 3044312.

Izquierdo—Carrasco,F. et al. (2011) Algorithms, data structures, and numerics for
likelihood—based phylogenetic inference of huge trees. BMC Bioinformatics, 12,
1714.

Kosakovsky Pond,S.L. and Muse,S.V. (2004) Column sorting: rapid calculation of
the phylogenetic likelihood function. Syst. Biol, 53, 685$92.

Kosiol,C. et al. (2008) Patterns of positive selection in six mammalian genomes.
PLoS Genet., 4, e1000144.

Kraut,A. et al. (2010) Phylogenetic code in the cloud 7 can it meet the expectations?.
In: HealthGrid IOS Press, Birmingham, AL, pp. 55$3.

Lartillot,N. and Philippe,H. (2004) A bayesian mixture model for across—site hetero—
geneities in the amino—acid replacement process. Mol Biol. Evol, 21, 10931109.

Lemey,P. et al. (2012) A counting renaissance: combining stochastic mapping and
empirical Bayes to quickly detect amino acid sites under positive selection.
Bioinformatics, 28, 324873256.

Message Passing Interface Forum. (1994) MP1: a message—passing interface stand—
ard. Int. J. Supercomput. Appl. High Performance Comput., 8, 341.

Murrell,B. et al. (2012) Detecting individual sites subject to episodic diversifying
selection. PloS Genet., 8, e1002764.

Nielsen,R. (2005) Statistical Methods in Molecular Evolution. Springer, New York.

Proux,E. et al. (2009) Selectome: a database of positive selection. Nucleic Acids Res.,
37, 404—407.

Rannala,B. and Yang,Z. (2008) Phylogenetic inference using whole genomes. Annu.
Rev. Genomics Hum. Genet., 9, 2177231.

Rodrigue,N. and Aris—Brosou,S. (2011) Fast bayesian choice of phylogenetic
models: prospecting data augmentation—based thermodynamic integration.
Syst. Biol, 60, 8817887.

Schabauer,H. et al. (2012) SlimCodeML: an optimized version of CodeML for the
branch—site model. In: 11th International Workshop on High Performance
Computational Biology (HiCOMB). IEEE, New York, pp. 7007708.

Sedgewick,R. and Wayne,K. (2011) Algorithms. 4th edn. Addison—Wesley, Reading,
Mass.

Seo,T.K. and Kishino,H. (2008) Synonyrnous substitutions substantially improve
evolutionary inference from highly diverged proteins. Syst. Biol, 57, 3677377.

Stamatakis,A. (2011) Orchestrating the phylogenetic likelihood function on emer—
ging parallel architectures. In: Schmidt,B. (ed.) BioinformaticsiHigh
Performance Parallel Computer Architectures. CRC Press, Singapore,
pp. 857115.

Stamatakis,A. and Ott,M. (2009) Load balance in the phylogenetic likelihood
kernel. In: ICPP. IEEE, New York, pp. 3487355.

Stamatakis,A. et al. (2002) AxML: a fast program for sequential and parallel phylo—
genetic tree calculations based on the maximum likelihood method. In:
Bioinformatics Conference. IEEE, New York, pp. 21728.

Studer,R.A. et al. (2008) Pervasive positive selection on duplicated and nondupli—
cated vertebrate protein coding genes. Genome Res., 18, 139%1402.

Sumner,.1 . and Charleston,M. (2010) Phylogenetic estimation with partial likelihood
tensors. J. T heor. Biol, 262, 4137424.

Tomko,K.A. (1995) Domain Decomposition, Irregular Applications, and Parallel
Computers. Ph.D. thesis, University of Michigan, Michigan.

Venditti,C. et al. (2008) Phylogenetic mixture models can reduce node—density arti—
facts. Syst. Biol, 57, 2867293.

Yang,Z. (2006) Computational Molecular Evolution. Oxford University Press,
Oxford.

Yang,Z. et al. (2005) Bayes empirical bayes inference of amino acid sites under
positive selection. Mol Biol. Evol, 22, 110771118.

YarKhan,A. et al. (2011) QUARK Users' Guide: QUeueing and Runtime for Kernels.
Technical report. University of Tennessee Innovative Computing Laboratory.
ICL—UT—11—02, Tennessee.

Zhang,.1 . et al. (2005) Evaluation of an improved branch—site likelihood method for
detecting positive selection at the molecular level. Mol Biol. Evol, 22,
247272479.

 

1137

112 /310's1eu1nofp101xo"satieuiJOJutotq/ﬁduq won papeo1umoq

9103 ‘0g1sn8nv uo ::

