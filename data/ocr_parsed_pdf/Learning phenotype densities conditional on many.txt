BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Learning phenotype densities with the CTF

 

NPB methods, such as the Dirichlet Process, prompted tech—
niques like that in Muller et a]. (1996), which induced ﬂexible
conditional regression through joint modeling of the response
and predictors. Subsequent methods included the predictors in
71;, and/or 6;, via Dependent Dirichlet Process (DDP) mixtures.
De Iorio et a1. (2004) proposed an ANOVA DDP model with
ﬁxed weights {71h} that used a small number of categorical pre—
dictors to index random distributions for the response. Grifﬁn
and Steel (2006) developed an ordered DDP, where the predictor
vectors were mapped to speciﬁc permutations of the weights
{71h}, yielding different density estimates for different predictor
vectors. Reich and Fuentes (2007) and Dunson and Park (2008)
used the kernel stick—breaking process to allow predictors to in—
ﬂuence the weights. Chung and Dunson (2009) presented a fur—
ther alternative in the probit stick—breaking process, which uses a
probit transform of a real—valued function of the predictors to
incorporate them into the weights. Methods that use joint mod—
eling of response and predictors (Shahbaba and Neal, 2009;
Hannah et al., 2011; Dunson and Xing, 2009) are popular and
can work well under many circumstances, but estimation of the
marginal distribution of the predictors is a burden. Methods not
relying on discrete mixtures also exist; Tokdar et a]. (2010) de—
veloped a technique based on logistic Gaussian processes. Jara
and Hanson (2011) presented an approach using mixtures of
transformed Gaussian processes.

These and other methods of Bayesian density regression have
proven successful, but as datasets have grown in size and com—
plexity, these approaches encounter difficulties. This is even more
daunting when we consider interactions of discretely valued pre—
dictors because we must consider the factorial combinations of
those levels.

The associated challenges of variable selection and dimension—
ality reduction have been explored in Bayesian density regres—
sion. Dimensionality reduction has a goal similar to that of
variable selection, that of finding a minimal set of predictors
that account for variation in the response. The logistic
Gaussian process approach of Tokdar et a1. (2010) includes a
subspace projection method to reduce the dimension of the pre—
dictor space. Reich et a]. (2011) developed a technique for
Bayesian sufﬁcient dimensionality reduction based on a prior
for a central subspace. Although all of these approaches have
demonstrated their utility, they do not scale easily beyond p = 30
predictors.

There are also techniques like the random forest (Breiman,
2001) that aim to ﬁnd parsimonious models for density estima—
tion involving a large number of predictors. One disadvantage to
this type of ‘black box’ method is in interpreting the impact of
speciﬁc predictors on the response. Bayesian additive regression
trees (BART) (Chipman et al., 2006, 2010) focus on modeling the
conditional mean and assume a common residual distribution.
As previously noted, there are many questions that require learn—
ing about more than just the conditional mean of the response.
Another ﬂexible approach is the Bayes network (BN), which
considers the predictors and the response on equal footing to
develop a parsimonious network linking all variables (Pearl,
1988; Cowell et al., 1999; Lauritzen, 1992). The conditional dis—
tribution of the response given the predictors can be derived from
such a model, using developed BN techniques for mixed continu—
ous and discrete data (Lauritzen, 1992; Moral et al., 2001;

Langseth et al., 2012). A BN does estimate a joint density for
all of the predictors; the effort to estimate this very high—dimen—
sional nuisance parameter is unattractive, if the conditional dens—
ity is of primary interest.

We propose an approach based on a conditional tensor fac—
torization (CTF) for the mixing weights. As in the DDP and
certain of the kernel stick—breaking methods, the predictors in—
ﬂuence the mixing weights for this CTF model. The conditional
tensor factorization facilitates borrowing of information across
different proﬁles in a ﬂexible representation of the unknown
density. We focus our attention on situations involving continu—
ous responses and categorical predictors.

3 METHODS

We consider a univariate response y and a vector of p categorical pre-
dictors x 2 (X1, . . . , Xp), where the j”' predictor X) can take values
1, . . . ,  We would like a model that can ﬂexibly accommodate condi-
tional densities that change in complex ways with changes in the predictor
vector. In addition, we must consider situations where 17 >> n; there may
be no exemplars for certain predictor vectors. To address this, we propose
a Tucker-style factorization with the following general model for the
conditional density f(y|x):

A1 ‘7
MM = Z - - - Z ni......h,(x)w; 9/71.... .1.)

[71:1 17,21

P
where nhl_____hﬂ(x) = H nil/)(Xj). (2)
j:1

This form uses the maps 7101,1' = 1, . . . ,p to associate the predictor vector
x with a separate weight for each combination of the latent identiﬁers
h], ...,hp and thus with each of the In >< -- - >< kp kernels in the repre-
sentation. The XIV? row of 7101 is a vector of weights, one for each

hj = 1, ...,kj. These weights HY)(Xj), ...,JI,(‘,’?(XJ-) are all in [0,1] and
22:21 715309) 2 1. The number of latent predictors p, is the same as

the number of observed predictors, but the form of the 7157’? may mean

that different predictor vectors x result in the same sets of weights
n1.___.1(x), ...,nkl.___.kﬂ(x). This provides the mechanism for dimension
reduction that we will develop. An important distinction from the
HME is in the treatment of the weights m,le hﬂ(x) as a tensor factoriza-
tion and the use of kernels A(y;6hl.___.hﬂ), which do not depend on
the predictor vector x. This is similar in spirit to the classiﬁcation
approach proposed by Yang and Dunson, 2012, but we address
the problem of estimating an inﬁnite-dimensional conditional density
rather than the ﬁnite-dimensional problem of a categorical response dis-
tribution. In addition, we make distinct improvements in predictor selec-
tion to allow the approach to scale to larger numbers of candidate
predictors.

Tucker decompositions (Tucker, 1966) and other kinds of decompos-
itions have appeared in the machine learning literature before. Xu et a].
(2012) developed an ‘inﬁnite’ Tucker decomposition making use of latent
Gaussian processes rather than explicit treatment of tensors and matrices;
in comparison, the proposed method uses the Tucker decomposition to
characterize the mapping of predictors into weights. Other factorizations
have been used for similar problems; Hoff (2011) presented a reduced-
rank approach for table data, but this approach focused on the develop-
ment of estimates for the mean of a continuous response. Chu and
Ghahramani (2009) derive an approach for partially observed multi-
way data based on a Tucker decomposition; their objective is to learn

 

1 563

ﬁm'spzumofpmjxo'sopeuuopuorq/ﬁdnq

D. C.Kessler et al.

 

about the latent factors driving observations rather than the character-
ization of the response distribution or variable selection.

The collection across j = 1, ...,p forms a ‘soft’ clustering from the
d1 >< - - - >< dp possible realizations of the x vector to each of the
M 2 k1 ><  >< kp possible latent vectors. This means that a predictor
vector x is not exclusively associated with one of the M kernels, but has a
weight for each kernel determined by the product in (2). This allows each
observation to contribute some information about the inﬂuence of each
of the p sites, and thus allows borrowing of information across different
combinations of 111 , . . . , hp. In settings of extreme sparsity, where most of
the possible predictor vectors are not represented, this is an attractive
property. This uses many fewer parameters than a full factorial represen-
tation and is still ﬂexible enough to represent complex conditional distri-
butions. Finally, we assume normal kernels:

kl k1! [7
mm: Z---Z No»;111......1,,,r;¥___.,,) x HHS/11x1) (3)
i=1

17.:1 171,21

This resembles other mixture-based approaches to density estimation as
originally speciﬁed in (1), but the proposed model for the weights pro-
vides the desired support for sparsity and information borrowing as pre-
viously discussed. In addition, the kernel-speciﬁc means 114,“th and
precisions mm)” are not functions of the predictor vector. Figure 1
shows a conditional dependence graph for the model parameters and
the observed data.

3.1 Predictor selection

The ﬁrst task in learning the conditional distribution is to identify those
predictors that provide the most information about the response; the
second task is to learn the form of the conditional distribution given
this set of informative predictors. The k1, ...,kp parameters indicate
the number of latent levels for each predictor. Because each k) can take
on the values 1, . . . ,d)‘, the possible combinations of different values for
k1, ...,k,, can be immense, and including these as parameters in an
Markov chain Monte Carlo (MCMC) sampler is not an attractive option.

In the notation of (3), predictors exclusion is equivalent to identifying
those sitesj such that k,- = 1. Consequently, predictor vectors that differ
only at the 1” position will have the same conditional density, and the _i”'
predictor can be excluded from the model. To identify those j such that
k,- = 1, we use a predictor selection step based on a special form of the
710). For eachj = 1,  ,p and each x,- = 1,  ,4), we specify the 7103 so
that 71%)(xf) = 1 for exactly one 11/ and 7122(xf) = 0 for all In. 75 11,-. This
form for the 71m associates each predictor vector x with exactly one of the
M 2 k1 ><  >< kp kernels by giving that particular kernel a weight of
one. That is, if the set of maps 7103,1' = 1, ...,p is such that
n571|)(x,-1) = 1, ...,n);';)(x,-p) = 1 for values 111, ...,/1p, then only the
kernel indexed by 111, ...,/1p will have non-zero weight. For computa-
tional convenience, we use conjugate priors and make the simplifying
assumption that the prior precision of each kernel mean 114,“th is the
same as the kernel precision rm.”th for each 111, ...,/1p, so that

 

® ®  ﬁki

Fig. 1. Conditional dependence graph showing the relationship between
the model parameters and the observed data

 

 

 

i

Mm....I7,,|Th......h,, ~ N(0,r,j|1____.,,ﬂ) and rmth ~ Gamma(6,/2,y,/2).
Because the proposed form for 71(1), ...,rr(r”) maps each predictor
vector to exactly one of the M groups, we can collect the observations
that map to each of the M groups and compute a marginal likelihood for
each group. Given the prior structure, the simplifying assumptions and
the clusterings deﬁned by the 71‘“, ...,rr(r”), the log marginal likelihood
for the In” group is

1V"?
2

 

l
lager) — Elem. + 1)

+10g  — 10g  +% log(y,)

1 ( YnTJN”, )2
— EUVm + 5r) log:  Ym — W + Vt 9

where Y,,, is the vector of responses, N,,, is the number of observations in
group m and JNW is a N,77 >< 1 vector of 1’s. The sum of these M approxi-
mated log-marginal likelihoods gives a score for the particular levels of
k1, .. . , kp and the particular 71(1), . . . ,JIQ"). Using these scores for differ-
ent levels of k1, . . . ,kp and different hard-clustering forms of
71(1), ...,n(”), we can ﬁnd those predictors with inﬂuence on the condi-
tional density.

It is not generally feasible to evaluate every possible set of k1, . . . , kp,
even for moderately sized problems. Instead, we begin with the null
model, where k1 2 k2 =  = kp = 1 and propose random changes to
the different k) and the associated 71m. The randomly proposed changes
are of two types: ‘split’ and ‘merge’. A ‘split’ change at position j means
changing the 7:03 map so that the distinct x,,~ map to more levels. For

4/:

example, assume that the _] position has three observed levels (d,- = 3)
and the current form of 7103 is such that all three observed levels of x) are

. 1
mapped to the same level. In this case, k,- = 1 and ft") =  One pos-
1

sible ‘split’ move would propose 715? so that x,- = 2 maps to the second
. 1 0
latent level, so that k,- = 2 and 715:) = [0 1]. Conversely, a ‘merge’ move
1 0
will decrease the number of mapped levels by one; using the definitions
above, one such merge move would be to replace 715? with 7103. If site j
already has k,- = 4,, then only merge moves are considered. Likewise, if
site j already has k,- = 1, then only split moves are considered. We use a
Metropolis step to accept or reject the proposed change; the stochastic
search proceeds as:

(1) Set n,- = 0,19- : 1;_]'= 1, ...,p; set 71m =J,;/,j= 1, ...,p; com-
pute the marginal likelihood ML".

(ii) For j = 1, . . . ,p, draw from all possible split and merge moves
with equal probability. For a split, propose k; 219+ 1; for a
merge, propose k; = k,- — 1.

(iii) Compute ML* for the proposed conﬁguration; accept the move
with probability 1 /\  If the new conﬁguration is accepted, set
192k; and ML" 2 ML*; if 19> 1, set n,- = n,- + 1.

(iv) After T iterations of steps 2-3, compute inclusion probabilities
piziT/forjz l, ...,p.

(v) Retain those predictors such that ADJ->01; using at = 0.5 is equiva-
lent to choosing the median probability model.

 

This stochastic search is similar to George and McCulloch (1997). The
approach we propose here is simple and appealing, but in our initial
simulation studies we noticed a tendency for this search to choose
overly complex models. Model selection was sensitive to the order in
which the predictors were considered. When the important features
were considered after many unimportant factors, randomly induced as-
sociations in the data and stochastic variation in the search led to com-
plex models that were not improved by addition of the important
predictors.

 

1 564

ﬁm'spzumofpmjxo'sopeuuopnorq/ﬁdnq

Learning phenotype densities with the CTF

 

To combat this tendency, we introduced a preliminary predictor iden-
tiﬁcation step that considers each of the predictors in isolation. We can
represent the entire stochastic search on the j”' predictor with a  >< 
discrete-time Markov transition matrix derived from the acceptance and
move probabilities deﬁned above. We can then compute the long-run
proportion of time that the chain spends in states such that kj> 1. This
can be done in an embarrassingly parallel fashion, and the computation
of each 17/ proceeds quickly. For the simulation case, where  = 4 for all j,
computation of each 17/ took ~0.3 s. At the conclusion of this single-pre-
dictor search step, we arrange the predictors in descending order of these
17,, retaining only those predictors such that pj>oz, and proceed with the
full stochastic search to identify a ﬁnal predictor set.

3.2 Estimation after predictor selection

To estimate the parameters in the model using the selected predictors, we
introduce a prior precision to ~ Gamma(80 / 2, yo / 2) for each kernel mean
p,th J7], ~ N(0, to), a prior for each kernel precision
17“.  j ,7” ~ Gamma(8, / 2, y, / 2) and separate Dirichlet priors for each
weight vector nm(xj) ~ Diri(,}/ , . . . , a.

To facilitate computation, we augment the model proposed in (3) with
classiﬁcation vectors z,» that associates the I'm observation with exactly one
kernel and gives a complete-data likelihood that can be expressed as a
product:

k1 "N

N p 1[z,-:(/7....../7ﬂ)]
H  H {N011 Mhl.....hﬂ,T,:l1~_m,,ﬂ) >< Eng/W4 (4)
j:1

121/7121 17,21

The full conditional distributions are

(1) Mhl.....hﬂ| - - - ~ NULZIW ,7 ,(rZLuuhﬂYl), where:

ram), = r0 +r1......1,i.”:111z1=(h1,---,hp)1

ut......1,={r1......1.2£1y111z1=(h1,---,hp>1}/r;..-_..,
(2) nun/9| - - - ~ Gamma(8*/2, y*/2), where:

5* = 5, +211; l[z,» = (h,  ,h,,)]

V“ = y, + 2:111z1=(h1,...,hp)1(yi— 111......mz

(3) ml - -- ~ Gamma([50 + 441/1 [V0 + {22:21 - - -Z:Z:1 Milwﬁﬂﬂ/Z)

(4) (719%), ...,th?(5))|  ~ Diri(%+2§:1 1[x,»,- = z] 1[z,»,- = 1], 
#ng 1[x,,-=g] 1[zl»j=kj])forE= 1, ...,dJ-andjz 1, ...,p

(5) Prizi = zfm 5011, ---,hj71,mahj+1, ---,hp)l|“' (X $10G — ﬁzz")

HZZW] >< 7132(xlj) for m = 1, ...,kj within eachj: 1, ...,p; ¢(-)
indicates the standard normal density.

The updates for the MM” _[7,,, 1:th J7], and 71(1) can be done blockwise, and
the z,» can be updated blockwise at each position j. Using the ﬁnal pre-
dictor set and the full conditionals, we produce a posterior sample for the
model parameters. This posterior sample allows us to compute condi-
tional densities and credible intervals around those estimates for various
combinations of the predictors.

4 DISCUSSION
4.1 Simulation study

To assess the variable selection and prediction performance of
the CTF, we conducted a simulation study, varying the number
of training observations N e {300, 500,1000, 1500} and using a
consistent ground truth to produce simulated datasets with total
number of predictors p = 1000. In each case, the true model was
based on three predictors at positions 30, 201 and 801, each with
d, = 4 levels and including three—way interactions among these
predictors. The resulting marginal density is an equally weighted
mixture of 64 Gaussians with different means and the same

residual precision ‘17. In other words, an observation with
(XL 30,)Ci‘ 201, X," 801) = (3, 2,  IS drawn from N(/,L3‘2‘ 1, 1771), and
so forth for each of the 64 distinct predictor vectors.

For each of 20 training sets, we produced selected predictor
sets and posterior samples. We then made predictions for 20
validation sets drawn from the same underlying true distribution.
As competitor methods, we used random forests (RF) and
quantile regression random forests (QRF) (Meinshausen,
2006); these are implemented in the randomForest and
quantregForest packages in R. BART, as implemented in
the BayesTree package, was unable to run to completion on
any of the training sets, though we were able to use BART with
the real data example in Section 4.2. RF and QRF include pre—
dictor selection directly, and QRF directly addresses the idea of
coverage proportion. BART is another MCMC—based approach,
but it does not directly address variable selection, allowing us to
investigate the impact of the large predictor space. The implicit
cost in estimating the joint distribution of predictors and re—
sponse made Bayes networks unattractive.

We computed mean square prediction error (MSPE) as the
average squared difference between the response value predicted
by the model for a predictor vector from the validation set and
the actual response value for that observation. We deﬁned cover—
age proportion as the proportion of times that the 95% predic—
tion interval for an observation in the validation set included the
actual response value, averaged over the intervals for each pos—
terior sample. When comparing performance with that of the
competitors, we attempted to give those competitors whatever
advantages we could provide. In the case of RF, this meant that
we did two passes over the training data. The ﬁrst pass identiﬁed
important variables using the importance method in the
randomForest package. We used the ‘mean decrease in accur—
acy’ style of importance; this measurement is derived from the
impact of permuting out—of—bag data for each tree in the forest.
We then fed those variables identiﬁed as important as a pre—
selected set into a second run of RF. This generally improved
the MSPE performance of RF. An analogous method was not
available for QRF, so we could not treat that method in the same
manner. In each of the 20 cases for p = 1000 and training
N: 500, the CTF outperformed RF on mean square prediction
error and showed comparable 95% coverage proportions to
those derived from QRF; this is summarized in Figure 2. The
CTF and RF showed comparable accuracy in identifying import—
ant predictors, but RF tended to include many unimportant pre—
dictors. In contrast, the CTF produced no false—positive results,
identifying the correct subset of predictors in each case. This
performance is particularly attractive given the large number of
possible interactions in the original predictor set. Both RF and
QRF may have suffered because of the strong interactions pre—
sent in these simulated data.

4.2 Molecular epidemiology application

We also consider an application to an epidemiology dataset,
comparing CTF performance with that of the same competitor
methods (RF, QRF and BART). The dataset concerns DNA
damage to instances of different cell lines when exposed to en—
vironmental chemicals. The exposure types are hydrogen perox—
ide (H202) and methyl methane sulfonate (MMS), and the

 

1 565

ﬁm'spzumoipmjxo'sopeuuopnorq/ﬁdnq

D. C.Kessler et al.

 

 

 

+ CTF,1=0.5-O- FIF,’[=O.5
q;- CTF,I=1 -o- FIF,1=1

 

 

 

Mean Square Prediction Error

 

 

I I I | I I
400 600 800 1000 1200 1400

 

 

+ CTET=0.5+ QFIF,1=0.5
ns- CTF,I=1 --O- QRF,1=1

 

 

 

 

 

95% Interval Coverage

 

 

 

| I I | I I
400 600 800 1000 1200 1400
Training sample size
Fig. 2. Simulation study results, comparing CTF with random forests
and quantile regression random forests

remainder of the predictor set is genotype information on 49 428
single nucleotide polymorphisms (SNPs). Rodriguez et a]. (2009)
provide extensive details on the original experiments. One hun—
dred separate instances of each of 90 cell lines were exposed to
each chemical and examined at each of three time points (before
treatment, immediately after treatment and a longer time after
treatment). The nature of the measurement is destructive; at the
desired time interval, comet assay was performed on each cell
and the Olive tail moment (OTM) (Olive et al., 1991) was re—
corded; this assesses the amount of DNA damage in the cell, with
higher measurements indicating more damage. The cells from
each line are genetically identical, but the resulting distribution
of OTM has a different shape for each cell line. In addition, these
distributions are different at the separate time points; generally,
OTMs are smallest (least damage) before exposure to the chem—
ical, largest (most damage) immediately after exposure and
somewhere in—between after a longer recovery time.

We computed empirical quantiles of the OTM for each cell
line at each of the three time points and then derived a single—
number summary wil- to tie these three quantile vectors together
for cell line i and exposure j. The summary measure 111,-,- e (0, l) is
the value that minimizes

31
Z IWi/Qi/JVJi + (1 — Wi/)Qi/,L,h — Qi/,A,h| (5)
11:17
Here, Qif‘N‘h indicates the h/32’h quantile for the 1'” cell line’s
OTM distribution at the ‘No treatment’ time, with corresponding
quantities for the ‘Later’ time point and the ‘immediately After’
time point for the jth exposure. The use of only the higher quan—
tiles reﬂects our desire to learn more about the extremes of DNA
repair. We used a logit transform to derive our final response

in,

y”- : l0g(liw ); this is appropriate for the assumptions of the

 

model. Negative values of the response indicate that the OTM
distribution long after treatment is closer to the distribution right
after treatment; positive values indicate that the ‘long after’ dis—
tribution is closer to the distribution before treatment.

SNPs in genes thought to be associated with some aspect of
DNA repair were genotyped, leading to data on 49 428 individ—
ual SNPs. Given the small number of cell lines and the fact that

Table 1. Details for SNPS included in the ﬁnal CTF model for the mo-
lecular epidemiology data

 

 

Gene SNP Position

IGFBP5 RSll575170 217256085
TGFBR3 RSl7880594 92118885
CHClL RS9331997 47986441
XPA RS3176745 99478631

 

Table 2. Comparison of MSPE, 95% coverage proportion and mean
computation time for different methods applied to molecular epidemi-
ology data

 

 

Metric CTF RF QRF BART
MSPE 0.263 0.353 7 0.425
95% Coverage 0.961 7 0.928 0.817
Time (s) 3317 80 88 2343

 

many individuals have two copies of the major allele for these
SNPs, many of the SNP proﬁles were identical or had no indi—
viduals with two copies of the minor allele. We recoded the
genotypes so that one indicated at most one copy of the major
allele and two indicated two copies of the major allele. After
recoding, we reduced the predictor set to those SNPs with dis—
tinct proﬁles, leaving 23 210 SNPs for analysis.

We used leave—one—out cross—validation to assess the perform—
ance of CTF against the three competitors RF, QRF and BART.
We ran the variable selection chain for 5000 burn—in iterations
and computed inclusion probabilities from 10000 samples. We
ran the MCMC chain for 40 000 burn—in iterations and retained a
sample of 20 000 iterations. Autocorrelation diagnostics indi—
cated an effective sample size of 15 000. We used the same
burn—in and posterior sample sizes for BART. As in the simula—
tion study, we used the results from a first run of RF to seed a
final run of RF.

CTF showed consistent selection of the treatment (H202 or
MMS) as the most important predictor and selected a set of
four SNPs (IGFBP5, TGFBR3, CHClL and XPA) as predictors;
information about these SNPS is summarized in Table 1. In con—
trast, RF chose the treatment variable in only 56 of the 180 cross—
validation scenarios and did not consistently identify any other
predictors. Comparison with the competitor methods showed pat—
terns similar to the simulation study; Table 2 compares the results
from each method. The interactions between the treatment and
the various SNPs may be weak enough that they do not contrib—
ute to the same elevated MSPE that RF demonstrated in the
simulation study. Even though the MSPE for RF was close to
that for the CTF, the CTF was able to achieve lower MSPE while
not sacriﬁcing coverage performance. This improved performance
offsets the CTF’s higher computational time requirement.
Figure 3 shows estimated conditional densities with 95% credible
intervals from the full dataset given varying levels of the treatment
and of the IGFBP5 SNP while holding the other three SNPs at

 

1566

ﬁle'spzumoipmJXO'sopeuuowrorq/ﬁdnq

Learning phenotype densities with the CTF

 

H202, lGFBP5=Zero/One Copy

to. u:—

H202, IGFBP5=Two Copies

 

 

4 5
I I
4 5
I I

Estimated Density
3
I
a
I

0
I

l
21>

 

MMS, lGFBP5=Zero/One Copy

to. u:—

MMS, IGFBP5=Two Copies

 

 

5
I
5
I

e
e e- e-
in
CI
E m— m—
m
E N- N-
m
In

1
I

I
71 0 I 2 a 71 0 I 2 a
Loglt 0t minimizing weight Loglt 0t minimizing weight

0
I

 

 

 

 

 

 

Fig. 3. Selected conditional densities given different exposures and differ-
ent number of copies of the dominant allele at the IGBP5 SNP, holding
all other SNPS at the 0/1 level. Heavy black lines show the mean condi-
tional density and gray lines show the 95% credible interval

Table 3. Summary of conditional distribution characteristics

 

 

Proﬁle Mean Variance 90m %ILE
H202, IGFBP520/1 0.226 11.39 2.65
H202, IGFBP5=2 0.156 7.31 2.25
MMS, IGFBP5 = 0/1 0.023 9.76 2.07
MMS, IGFBP5=2 —0.023 6.11 1.88

 

the ‘Zero/One Copy’ level, and illustrates how the conditional
density changes in more than the conditional mean when the
predictor vector changes. In this case, the interaction between
MMS treatment and two copies of the major allele for this
IGFBP5 SNP tightens the density markedly, although it has a
more muted impact on the conditional mean. The change is less
dramatic under the exposure to H202. Here, the shift in the mean
response as treatment and genetic proﬁle change is less interesting
than the difference in conditional variance; under treatment with
H202, the mean response is slightly different than under treat—
ment with MMS, but the tail probabilities are noticeably differ—
ent. Table 3 summarizes these differences in conditional mean,
conditional variance and conditional 90" percentile for each scen—
ario. As suggested in Figure 3, the medians of the conditional
densities given the exposure (H202 or MMS) are close, but in
the tail of the distribution (the 90th percentile), the separation
between the estimated quantile curves is larger. This varying
shift in the 90th percentile reﬂects the interaction between the
exposure and the level of the IGFBP5 SNP.

5 CONCLUSON

We have presented a novel method for ﬂexible conditional dens—
ity regression in the common case of a continuous response and
categorical predictors. The simulation study and real data ex—
ample suggest that this conditional tensor factorization method
can have better performance than other modeling tools when

there is substantial interaction between the predictors of interest.
The CTF does have a higher computational time requirement
than the competitor methods, but the improvement in prediction
accuracy and coverage still make the CTF an attractive method.

A particularly appealing aspect of the CTF is predictor selec—
tion, which finds low—dimensional structure in the high—dimen—
sional predictor set. This reduction to more parsimonious
models yields a succinct description of the ways in which the
phenotype varies given exposure and SNPs. Finally, a distinct
advantage of the CTF is its ability to produce conditional density
estimates. This property of the CTF provides insight beyond a
simple conditional expectation and makes it possible to answer
more complex questions about the relationship between the re—
sponse and the predictors.

Funding: This research was supported in part by the Intramural
Research Program of the NIH, National Institute of
Environmental Health Sciences, 201 ES049032. David
Kessler’s work was partially supported by National Institute of
Environmental Health Sciences training grant T32ES007018.
David Dunson’s work was supported by Award Numbers
R01ES017240 and R01ES017436 from the National Institute
of Environmental Health Sciences.

Conﬂict of Interest: none declared.

REFERENCES

Bishop,C. and Svensen,M. (2003) Bayesian hierarchical mixtures of experts. In:
Proceedings of the Nineteenth Conference on Uncertainty in Artificial
Intelligence. pp. 57$4.

Breiman,L. (2001) Random forests. Mach. Learn., 45, 5732.

Chipman,H. et al. (2006) Bayesian ensemble learning. In: Advances in Neural
Information Processing Systems. MIT Press, Cambridge, Massachusetts, USA,
pp. 2657272.

Chipman,H. et al. (2010) BART: Bayesian additive regression trees. Ann. App].
Stat., 4, 26(r298.

Chu,W. and Ghahramani,Z. (2009) Probabilistic models for incomplete multi—
dimensional arrays. Proceedings of the 12th International Conference on
Artificial Intelligence and Statistics (AISTATS), Clearwater Beach, Florida,
USA.

Chung,Y. and Dunson,D. (2009) Nonparametric Bayes conditional distribution
modeling with variable selection. J. Am. Stat. Assoc., 104, 164(r1660.

Cowell,R. et al. (1999) Probabilistic Networks and Expert Systems. Springer, New
York, USA.

De Iorio,M. et al. (2004) An ANOVA model for dependent random measures.
J. Am. Stat. Assoc., 99, 2057215.

Dempster,A.P. et al. (1977) Maximum likelihood from incomplete data via the EM
algorithm. J. Roy. Stat. Society B (Methodological), 39, 1738.

Dunson,D. and Park,J. (2008) Kernel stick—breaking processes. Biometrika, 95,
3077323.

Dunson,D. and Xing,C. (2009) Nonparametric Bayes modeling of multivariate cat—
egorical data. J. Am. Stat. Assoc. ( Theory and Methods), 104, 104271051.
George,E. and McCulloch,R. (1997) Approaches for Bayesian variable selection.

Stat. Sin., 7, 332373.

Grifﬁn,J. and Steel,M. (2006) Order—based dependent Dirichlet processes. J. Am.
Stat. Assoc., 101, 1797194.

Hannah,L. et al. (2011) Dirichlet process mixtures of generalized linear models.
J. Mach. Learn. Res., 12, 192371953.

Hoff,P. (2011) Hierarchical multilinear models for multiway data. Comput. Stat.
Data Anal., 55, 5307543.

Jara,A. and Hanson,T. (2011) A class of mixtures of dependent tail—free processes.
Biometrika, 98, 5537566.

Jordan,M. and Jacobs,R. (1994) Hierarchical mixtures of experts and the EM
algorithm. Neural Comput, 6, 1817214.

 

1 567

ﬁle'spzumoipmJXO'sopauuoprrorq/ﬁdnq

D. C.Kessler et al.

 

Langseth,H. et al. (2012) Inference in hybrid Bayesian networks with mixtures of
truncated basis functions. In: Sixth European Workshop on Probabilistic
Graphical Models, pp. 1717178.

Lauritzen,S. (1992) Propagation of probabilities, means, and variances in mixed
graphical association models. J. Am. Stat. Assoc., 87, 109871108.

Meinshausen,N. (2006) Quantile regression forests. J. Mach. Learn. Res., 7,
9837999.

Moral,S. et al. (2001) Mixtures of truncated exponentials in hybrid Bayesian net—
works. In: Proceedings of the Sixth European Conference on Symbolic and
Quantitative Approaches to Reasoning Under Uncertainty (ECSQARU 2001).
New York, USA, pp. 15(r167.

Muller,P. et al. (1996) Bayesian curve ﬁtting using multivariate normal mixtures.
Biometrika, 83, 67779.

Olive,P. et al. (1991) DNA double—strand breaks measured in individual cells sub—
jected to gel electrophoresis. Cancer Res., 51, 467174676.

Pearl,J. (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, San Francisco, California, USA.

Reich,B. and Fuentes,M. (2007) A multivariate semiparametric Bayesian spatial
modeling framework for hurricane surface wind ﬁelds. Ann. Appl. Stat., 1,
2497264.

Reich,B. et al. (2011) Sufﬁcient dimension reduction via Bayesian mixture modeling.
Biometrics, 67, 88(r895.

Rodriguez,A. et al. (2009) Bayesian hierarchically weighted ﬁnite mixture models
for samples of distributions. Biostatistics, 10, 1557171.

Shahbaba,B. and Neal,R. (2009) Nonlinear models using Dirichlet process mixtures.
J. Mach. Learn. Res., 10, 182971850.

Tokdar,S. et al. (2010) Bayesian density regression with logistic Gaussian process
and subspace projection. Bayesian Anal., 5, 3197344.

Tucker,L. (1966) Some mathematical notes on 3—mode factor analysis.
Psyclwmetrika, 31, 279.

Waterhouse,S. et al. (1996) Bayesian methods for mixtures of experts. In: Advances
in Neural Information Processing Systems. MIT Press, Cambridge,
Massachusetts, USA, pp. 3517357.

Yang,Y. and Dunson,D.B. (2012) Bayesian Conditional Tensor Factorizations for
High—Dimensional Classiﬁcation. Working Paper. Duke University, Durham,
USA.

Xu,Z. et al. (2012) Inﬁnite Tucker decomposition: nonparametric Bayesian models
for multiway data analysis. In: Proceedings of the 29th International Conference
on Machine Learning, Princeton, New Jersey, USA.

 

1 568

/810'sleum0fp103x0'soIJBuiJOJuIOIq”:duq

