BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

BEETL-fastq

 

Having found some exact matches to our query within the
reads in this way, we continue the recursion to extend these
hits into the entire sequences of the reads that contain them.
Once the extension reaches the boundary of a read, a lookup
into an additional table allows the original positions of the reads
in the FASTQ ﬁle to be deduced. This last piece of information
enables the quality score and read ID strings to be extracted from
razip—compressed files that have been indexed using their order—
ing within the original FASTQ file as a key.

Speciﬁcally, given a query DNA sequence (1, our tool can pro—
vide, in increasing order of computational overhead:

0 the number of occurrences of q in the reads,

0 the full sequence of each of the reads that contain (1,

the quality score strings associated with the sequences that
contain (1,

o the read IDs of the reads whose sequences contain (1,

(For paired—read data) the read IDs, sequences and quality
scores of the reads that are paired with the sequences that
contain (1.

Three potential applications demonstrate the usefulness of this
fast k—mer search: an ‘in silico pull—down’ experiment in which
only the reads that cover a region of interest are selectively ex—
tracted for the purpose of variant calling or visualization, a de
novo assembly of reads from insertion breakpoints and the gen—
otyping of structural variants by means of tracking the k—mers
that overlap their breakpoints.

2 METHODS
2.1 Deﬁnitions

Given a string S comprising n symbols drawn from some alphabet a, we
mark its end by appending a unique symbol 3% that is lexicographically
smaller than any symbol in a. The BWT of S is deﬁned such that the i-th
symbol of BWT(S) is the character of S immediately preceding the sufﬁx
of S, which is i-th smallest in lexicographic order. The concept of the
BWT can be readily generalized to encompass a set of strings S1, . . . , Sm
if we imagine the strings are terminated with distinct end markers that
satisfy $1 <  <$m.

The deﬁnition of the BWT implies that any occurrence of a symbol
within BWT(S) has a one-to-one relationship with a sufﬁx of S that we
call its associated suffix. Given some query string Q that occurs at least
once in S, the characters in BWT(S) whose associated sufﬁxes start with
Q form a contiguous subsequence that we call the Q-interval of BWT(S).
If we have located the Q-interval in BWT(S), the backward search pro-
cedure allows the position and size of the pQ-interval to be deduced for
any symbol [7 by means of rank() computations, which count the number
of occurrences of 17 within intervals of BWT(S) (see, for example, Adjeroh
et al., 2008).

2.2 Index construction

FASTQ data are ﬁrst split into its three component streams: bases,
read IDs and quality scores. The read IDs and quality scores are each
dealt with in the same way: compressed with razip and augmented
with an index thatifor every 1024th readistores the offset in the
ﬁle at which the data associated with that read begin. The BWT of the
sequences is built using the algorithm described in Bauer et a]. (2011,
2013): we use our own BEETL library for this, but we also note the
several additional improvements in Heng Li’s implementation

(github. com/ lh3 /ropebwt) and promising recent work by Liu
et a]. (2014) that demonstrated accelerated BWT construction using
GPU technology.

During BWT construction, we also generate an ‘end-pos’ ﬁle contain-
ing an array that maps between the ordering of the read associated with
each 3% sign and its read number in the original FASTQ ﬁle. The BWT
itself is stored in a manner similar to that used by the sga assembler
(Simpson and Durbin, 2012), runs of characters being represented by
byte codes, the least signiﬁcant bits encoding the character and the re-
mainder of the byte denoting the length of the run. To speed the rank()
calculations needed for backward search, we create a simple index of the
BWT ﬁles by storing, once every 2048 byte codes, the number of times
each character has been encountered so far in the BWT.

The index construction ﬂow is shown in the top half of Figure 1, and
Table 1 summarizes the ﬁles generated by BEETL-fastq to store and
index the original FASTQ ﬁles.

2.3 Searching for a query sequence in the index
BEETL-fastq’s search mode consists of three main computation stages
shown in the lower half of Figure 1:
o BEETL-search performs the k-mer search and retrieves the Q-inter-
val matching each k-mer.

o BEETL-extend propagates each BWT position from these Q-inter-
vals to the end of each read, where we are able to identify the read
number. At the same time, BEETL-extend’s propagation of the k-
mers reconstructs each base of the reads.

0 From the read numbers, we extract read IDs and quality scores using
the razip ﬁles.

The read IDs, bases and quality scores are then interleaved to generate
the output FASTQ ﬁle.

3 RESULTS

3.1 The Platinum Genomes dataset

In the following sections, we refer to several datasets from
Illumina’s Platinum Genome (PG) project (Eberle et al., manu—
script in preparation), which aims to systematically identify all
variants in a large three—generation family (the CEPH/Utah

Readnames Bases Qualities

( Readnames. E BWT RLE + ( Q- E
razip end- as scoresJazip

'—
rch
extend

|
BEE
BEETL
V .
0
Bases m
. In
Read name Q-score razip
razi e: traction extraction
Resul read Result 0-
nar es sccres
—
. C

Fig. 1. BEETL-fastq ﬂow

 

 

 

 

 

 

 

 

 

2797

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

L.Janin et al.

 

pedigree 1463) by combining multiple variant calling approaches
and making use of the inheritance structure within the pedigree.
The 17 members of the pedigree were each sequenced to 50—fold
coverage, the total dataset comprising 27 billion reads of length
100, requiring 6.6TB of storage in FASTQ format (or 2.5 TB if
compressed with gzip). Both the raw sequences and variant calls
are publicly available (www.illumina.com/platinumgenomes/).

Our experiments with these genomes involved repeated
searches of k—mers. Our strategy was to prepare and search
each genome independently to get the beneﬁts of distributed pro—
cessing. One of them, designated NA12877, comprises 165.4 GB
of paired 100—mers, stored as gzipped FASTQ files totalling
151.71 GB, equivalent to 390.55 GB of uncompressed FASTQ
data. Compressing and indexing these data with BEETL—fastq
(i.e. conversion of gzipped FASTQ into BWT plus razip—com—
pressed quality scores and read IDs) took 7.5h on an Amazon
EC2 i2.2xl instance and reduced the data to 113 GB, a reduction
of 26%. From this, search query results were obtained at a rate
presented in Table 2, while the subsequent read extraction stage
consistently took around a further 20 ms per read.

An interesting option would be to store all 17 datasets as one
compressed entity. The redundant information present in these
genome reads would be grouped together by the BWT permuta—
tion, potentially leading to a better overall compression.
However, we would lose the 17—way parallelism that allows us
to achieve the search speed presented here. As our number of
datasets increase, we will group datasets together while still keep—
ing enough concurrency to match the required search time.
Keeping the datasets separated also allows us to target speciﬁc
members of the pedigree independently.

3.2 Comparison of read ordering strategies
Ordering the reads before indexing affects the time taken to build
the index and the time taken to build and query it. Here we
compare three different strategies:

0 Unordered: reads are left in the original FASTQ order,

0 Lexicographic order (L0): reads are sorted in lexicograph—
ical order of their bases,

Table 1. Files generated by the indexing process

 

File name Description

 

dataset-BO[0-6]
dataset-BO[0-6]. idX
dataset-end—pos
dataset.quals.rz

BWT ﬁles in run-length encoded format
BWT index ﬁles for faster random accesses
Read numbers of the ‘$’ signs in BWT
razip-compressed quality scores, able to start
decompressing at any given position

index ﬁle from compressed quality scores, to
map read numbers to character positions in
the razip ﬁle

razip-compressed read IDs

index ﬁle for compressed read IDs

dataset.quals.rz.idx

dataset.readIds.rz
dataset.readIds. rz. idX

 

Note: Together, these ﬁles comprise a lossless representation of the data in the
original FASTQ ﬁles.

0 Reverse lexicographic order (RLO): reads are sorted in lex—
icographical order of their reversed (not reverse—comple—
mented, although it would be equivalent) string.

Table 3 compares these strategies on the NA12877 reads.
Compression and search times were measured on a server with
16 2.5 GHz cores, although BEETL—fastq used at most six cores.
BWT construction is I/O—bound, and the underlying RAID disk
was averaging 100 MB/s.

The advantages of each strategy can be summarized as
follows:

0 Unordered: read IDs compress better, as they are usually
generated in an ordered way, which gets shufﬂed by the
other strategies. This is a small gain.

0 L0: the dataset—end—pos file is not needed, as the ‘$’ signs in
the BWT end up in the same order as the original reads. This
saves 7% of the total size.

0 RLO: the BWT files get longer runs of identical letters,
leading to a better compression rate (45% of BWT size,
7% of total size) and to faster search time (20% faster).

The RLO sort achieves a 7% reduction of the total size com—
pared with the unordered strategy. This comes at an extra cost in
initialization time, a stage where optimizations are still possible.
However, as our ﬂow needs a single initialization for an unlim—
ited number of searches, the main beneﬁts come at search time,
where the I/O—bound search needs to go through a BWT, which
is 45% smaller. Experiments confirm a 45% faster search.

In the case of a server accessing ﬁles remotely, as described in
the next section, another advantage lies in the fact that only
BWT index ﬁles are kept cached in RAM. Having a BWT
45% smaller means that a 45% smaller RAM footprint is
achieved with RLO.

It should be noted that the search—only time reported here
corresponds to the ﬁrst stage of reads extraction. The second
stage (extension of BWT positions) is also accelerated by 45%,
as it is based on the same BWT sufﬁx extension algorithm,
whereas the third stage (extraction of quality scores and read
Ids from razip files) is unaffected.

3.3 Cloud-based deployment

One interesting potential application is as a search service, an—
swering the queries of users on demand and saving them from
the need to download large FASTQ files in full.

A range of use cases was explored, from very fast search of a
few k—mers to longer batch search of millions of k—mers. Here we
report on the two extreme cases: large queries of millions of k—
mers under 1 h, and queries of a single k—mer expected to return
results under 1s, this last use case leading to a useful cloud

Table 2. BEETL-fastq search speed from NA12877 (time in seconds)

 

Number of 30-mers 1 10 100 1000 l 000 000

 

BEETL-search (30 cycles) 1 1.8 2.9 7.2 207
BEETL-extend (100 cycles) 1.4 6.1 11.2 37.5 360

 

 

2798

ﬁm'spzumofpmﬂo'sopeuuopnotq/ﬁdnq

BEETL-fastq

 

conﬁguration. Those timings are achieved on an Amazon EC2
instance querying indexes of the set of 17 human genome data—
sets described in Section 3.1, the indexes being held in Amazon
S3 storage.

Both use cases answer the following constraints in different
manners:

0 how to efﬁciently access the datasets stored on S3,
0 what level of parallelization is needed,

0 how to minimize the costs.

Keeping instances running helps achieve good speed, as the
data can be kept local, even in RAM if necessary. But this usu—
ally comes at a high cost. On the other hand, spawning Amazon
instances on demand takes a few minutes just to start the service.

3.3.] Large batch queries Large searches of millions of k—mers
are actually simple, as the time needed to spawn a large Amazon
instance and download the data are small enough compared with
the computation time.

Running a cluster of 17 Amazon instances (the number of
genomes in our dataset) with 120 GB of SSD and 30 GB of
RAM such as m3.2xlarge satisﬁes our constraints. The instances
achieve good overall download speed from S3, each being able to
keep one full genome ﬁles on disk and its BWT in RAM.
Keeping the genome searches on separate instances prevents
RAM contention.

3.3.2 Small queries with fast response Ideally, we would like a
search for one k—mer within our dataset to yield a response
within 1s. However, in practice this timing greatly depends on
the number of matching reads as the final razip extraction takes
around 20 ms per read. We decided to distinguish between the
three phases of a search: k—mer search in BWT, extension of
BWT positions to bases and read numbers and extraction of
qualities and read IDs. We focus here on the ﬁrst stage, the k—
mer search, which returns the number of matching reads. This is
useful information by itself, for example in genotyping
applications.

The first challenge was to decide where the tools can run: an
Amazon instance takes at least 1 min to start up, and downloading
one of the human genome datasets from S3 to an instance takes, in
the best case, 3 min for the sequences alone (around 16 GB,

Table 3. Effect of sorting strategies on size of the components of the
index [in GBytes (bits per base)], build time and search time for NA12877

 

 

Unsorted LO sort RLO sort
NA12877-30* 33 (1.71) 29.4 (1.53) 18 (0.93)
NA12877-end—pos 7.6 (0.39) 0 (0) 7.6 (0.39)
NA12877.qual.rz* 64.5 (3.35) 64 (3.32) 64.1 (3.33)
NA12877.readIds.rz* 7.9 (0.41) 14.9 (0.77) 14.9 (0.77)
Total: 113 (5.87) 108.3 (5.62) 104.6 (5.43)
Reduction versus fastq.gz (%) 726 729 732
Compression time (min) 590 602 647
Search-only time for 505 460 280

one 30-mer (ms)

 

assuming 100 MB/s). One solution is to prefetch the BWT ﬁles
local to an always—on Amazon instance. However, this is an ex—
pensive solution if the Amazon instance is large enough to process
17 human genome datasets in parallel.

A cheaper alternative is to leave the BWT files in Amazon S3
storage and access them remotely, transparently mounted as ﬁles
using httpfs (sourceforge .net/projects/httpfs). This
is a high—latency solution, which ends up working well. The index
ﬁles are pre—fetched and kept in RAM, and the quantity of RAM
needed is low enough that we can now pick a much cheaper
instance (such as m3 .medium).

The organization of our ﬁle system is such that the ﬁles
described in Table 1 appear in one working directory. Large
ﬁles accessed via httpfs are mounted in subdirectories and sym—
bolic links make them appear correctly in the working directory.
Small index files are directly stored in the working directory.
With the default BEETL—search tool, search queries of a single
30—mer were at this stage answered within 3 s.

An extra optimization was necessary to bring this down to 1 s;
instead of loading the index ﬁles from disk to RAM at the start
of each BEETL—search, a shared memory structure keeps the
index data permanently in RAM, with the ability to be re—used
across successive BEETL—search processes.

To make these results easily reproducible and for easy discovery
of our tools architecture, a public Amazon AMI image was cre—
ated, containing all the tools described here: BEETL tools, pre—
fetched index ﬁles and httpfs remote mounts of the BWT files for
the NA12877 human genome sample from S3. The image also
includes a web server able to launch k—mer queries on
NA12877. This image appears as ‘BEETL—fastq Bioinformatics’
in Amazon AMI (Amazon Machine Image) search.

4 APPLICATIONS

4.1 An ‘in silico pull-down’ experiment

We used BEETL—fastq to simulate a targeted sequencing experi—
ment in silico by extracting all read pairs from NA12878 (another
member of the PG pedigree) that overlap RBM15B, a single—
exon gene of length 6.6 kb. First, a non—overlapping set of 34—
mers covering the gene was extracted from the human reference
sequence. The use of non—overlapping sequences limits the redun—
dancy of our queries, but BEETL—fastq can deal equally well
with overlapping k—mers. We chose a length of 34 because pre—
vious studies (Li et al., 2010) suggest that 80% of the genome can
be uniquely targeted by a sequence of this size, but the only
restriction on query length is that imposed by the length of the
reads themselves.

Tuning the query k—mer length allows a trade—off between sen—
sitivity and speciﬁcity, and prior knowledge of the region being
targeted can guide this decision. Genomic regions of lower ex—
pected coverage or with many variants will require shorter and
possibly overlapping k—mers, whereas longer k—mers will better
target more repetitive regions. One could also imagine perform—
ing several queries over a range of k—mer lengths and then mer—
ging the retrieved read sets. This will obviously result in an
increased running time but will also increase sensitivity.

Next, we used BEETL—fastq to retrieve the FASTQ entries for
all read pairs in NA12878 for which at least one read of the pair

 

2799

ﬁm'spzumofpmJXO'sopauuopnotq/ﬁdnq

an?kgogmomammowoio~&o:3m7.omm\

3
1i
9
2
A
N

 

BEETL-fastq

 

k—mers in both parents. We found that the k—mer occurrences
match this expected pattern in 99.96% of all cases.

5 DISCUSSION

Our focus here has been to represent the input data in a lossless
fashion, but Table 3 highlights that the relative storage needs of
the sequences, quality scores and read IDs perhaps do not reﬂect
their relative importance, as the sequences and their indexes take
up only 17% of the total size, barely more than the 14% con—
sumed by the read IDs.

There remains some scope for further lossless compression: in
earlier work that focused exclusively on compression (Cox et al.,
2012), we achieved sub—0.5 bpb on the sequences using 7—zip
(www.7—zip.org, Igor Pavlov). Here, however, we sacriﬁce some
compression for the faster decompression, and thus faster search
that our simple byte code format gives us. The byte coding can
be further optimized, and it may also be advantageous to switch
between run—length encoding and naive 2—bits—per—base encoding
on a per—block basis, choosing whichever of the two strategies
best compresses each block.

However, to achieve signiﬁcant further compression, some
degree of lossy compression is likely to be necessary. Each of
the three data streams is potentially amenable to this, and our
methods can of course still be applied to the resulting data.

The free format of the read IDs limits our ability to comment
generally on the prospects of compressing such data further.
Nevertheless, it could be argued that, for paired data, the most
useful metadata to retain is which read is paired with which: this
could be simply encapsulated in an array containing one pointer
per read, consuming 0(logn) bits.

The space taken up by the sequences themselves would be
reduced by error correction, two possible strategies being the
trimming of low—quality read ends [as demonstrated by Cox
et al. (2012)] or a k—mer—based approach such as Musket (Liu
et al., 2013).

Lastly, the majority of the archive’s size is taken up by the
quality scores. More recent Illumina data default to a reduced—
representation quality scoring scheme that makes use of only 8 of
the 40 or so possible quality values, but the PG data we tested
still follow the full—resolution scoring scheme. The newer scheme
would likely reduce the size of the scores by about half. We have
also described a complementary approach (Janin et al., 2014)
that uses the k—mer context adjacent to a given base to decide

whether its quality score can be discarded without likely detri—
ment to variant calling accuracy.

Conﬂicts of Interest: All authors are employees of Illumina Inc., a
public company that develops and markets systems for genetic
analysis, and receive shares as part of their compensation.

REFERENCES

Adjeroh,D. et al. (2008) The Burrows—Wheeler Transform: Data Compression, Suﬂix
Arrays, and Pattern Matching. lst edn. Springer US.

Bauer,M.J. et al. (2011) Lightweight BWT construction for very large string collec—
tions. In: CPM (2011). LNCS. Vol. 6661, Springer Berlin Heidelberg, pp 2197
231.

Bauer,M.J. et al. (2013) Lightweight algorithms for constructing and inverting the
BWT of string collections. Theor. Comput. Sci., 483, 134448.

Burrows,M. and Wheeler,D.J. (1994) A block sorting data compression algorithm.
In: Technical report. DIGITAL System Research Center.

Cock,P.J.A. et al. (2010) The sanger FASTQ ﬁle format for sequences with quality
scores, and the Solexa/Illumina FASTQ variants. Ntwleic Acids Res., 38,
176771771.

Cox,A. et al. (2012) Large—scale compression of genomic sequence databases with
the Burrows—Wheeler transform. Bioim’ormatics, 28, 141571419.

Ferragina,P. and Manzini,G. (2000) Opportunistic data structures with applica—
tions. In: Proceedings of the 41st Annual Symposium on Foundations of
Computer Science. IEEE Computer Society, Washington, DC, pp. 3907398.

Janin,L. et al. (2014) Adaptive reference—free compression of sequence quality
scores. Bioinformatics, 30, 24730.

Langmead,B. et al. (2009) Ultrafast and memory—efﬁcient alignment of short DNA
sequences to the human genome. Genome Biol, 10, R25.

Li,H. and Durbin,R. (2009) Fast and accurate short read alignment with Burrows—
Wheeler transform. Bioinformatics, 25, 17544760.

Li,H. et al. (2009) The sequence alignment/map format and samtools.
Bioiry’ormatics, 25, 207872079.

Li,R. et al. (2010) De novo assembly of human genomes with massively parallel
short read sequencing. Genome Res., 20, 2657272.

Liu,C.M. et al. (2014) GPU—accelerated BWT construction for large collection of
short reads. arXiv: 1401.7457.

Liu,Y. et al. (2013) Musket: a multistage k—mer spectrum—based error corrector for
illumina sequence data. Bioinﬁ)rmatics, 29, 3087315.

Mills,R.E. et al. (2011) Mapping copy number variation by population—scale
genome sequencing. Nature, 470, 5%65.

Simpson,J.T. and Durbin,R. (2012) Efﬁcient de novo assembly of large genomes
using compressed data structures. Genome Res., 22, 5497556.

The 1000 Genomes Project Consortium. (2012) An integrated map of genetic vari—
ation from 1,092 human genomes. Nature, 491, 56—65.

Zerbino,D.R. and Birney,E. (2008) Velvet: algorithms for de novo short read assem—
bly using de bruijn graphs. Genome Res., 18, 8217829.

Zook,J.M. et al. (2014) Integrating human sequence data sets provides a resource of
benchmark SNP and indel genotype calls. Nat. Biotech., 32, 2467251.

 

2801

ﬁre'spzumofpmJXO'sopauuowrorq/pdnq

