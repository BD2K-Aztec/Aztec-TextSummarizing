Bioinformatics, 31119), 2015, 3172—3180

doi: 10.1093/bioinformatics/btv349

Advance Access Publication Date: 4 June 2015
Original Paper

 

Systems biology

CCLasso: correlation inference for
compositional data through Lasso

Huaying Fan91'2'3, Chengcheng Huang4, Hongyu Zhao5 and
Minghua Den91'3'5'*

1LMAN, School of Mathematical Sciences,2Beijing International Center for Mathematical Research, 3Center for
Quantitative Biology, Academy for Advanced Interdisciplinary Studies, Peking University, Beijing 100871, China,
4College of Global Change and Earth System Science, Beijing Normal University, Beijing 100875, China,
5Department of Biostatistics, Yale School of Public Health, New Haven, CT 06510, USA and 6Center for Statistical
Science, Peking University, Beijing 100871, China

*To whom correspondence should be addressed.
Associate Editor: Igor Jurisica

Received on January 13, 2015; revised on May 28, 2015; accepted on May 29, 2015

Abstract

Motivation: Direct analysis of microbial communities in the environment and human body has
become more convenient and reliable owing to the advancements of high—throughput sequencing
techniques for 16S rRNA gene profiling. Inferring the correlation relationship among members of
microbial communities is of fundamental importance for genomic survey study. Traditional
Pearson correlation analysis treating the observed data as absolute abundances of the microbes
may lead to spurious results because the data only represent relative abundances. Special care
and appropriate methods are required priorto correlation analysis for these compositional data.
Results: In this article, we first discuss the correlation definition of latent variables for compos—
itional data. We then propose a novel method called CCLasso based on least squares with (2’1 pen—
alty to infer the correlation network for latent variables of compositional data from metagenomic
data. An effective alternating direction algorithm from augmented Lagrangian method is used to
solve the optimization problem. The simulation results show that CCLasso outperforms existing
methods, e.g. SparCC, in edge recovery for compositional data. It also compares well with SparCC
in estimating correlation network of microbe species from the Human Microbiome Project.
Availability and implementation: CCLasso is open source and freely available from https://github.
com/huayingfang/CCLasso under GNU LGPL v3.

Contact: dengmh@pku.edu.cn

Supplementary information: Supplementary data are available at Bioinformatics online.

 

 

1 lntroductlon human life on our food, health and medicine (Gill et (11., 2006). The

Microbes play an important role in the environment and human life.
Bacteria and archaea have been found in extreme conditions such as
deep sea vents with high temperatures and rocks of boreholes
beneath the Earth’s surface (Pikuta et (11., 2007). The microorgan-
isms affect environments where they exist, and vice versa. It is esti-
mated that there are about 10 times microbe cells inhabiting our
human body than human cells (Savage, 1977). Microbes affect the

way in which microbes affect the human health remains largely
unknown. Analysis of the human niicrobionie may help us better
understand our own genome.

The increasing quality and reducing cost of sequencing
technologies provide great opportunity to analyze the microbe con]-
munities through sequencing. This represents a great improvement
over traditional microbe studies which are hindered by several

(63 The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3172

/3.IO‘SI€III[10[p.IO}X0‘SODEIIIJOJHIOIQ/[Zdnq

CCLasso

3173

 

limiting factors. First, only a small proportion of microbes can be
cultured under laboratory conditions. Second, only single microbe
can be studied in laboratories but it is well known that most
microbes need other microbes to survive. In contrast, sequencing
technologies allow researchers to collect information from the whole
genomes of all microbes in a community directly from their nat—
ural environment, facilitating mixed genomic surveys (Handelsman
et al., 1998).

When data are available across many communities, the depend—
encies among microbes, which can be measured by correlations,
may provide important clues on the interactions among microbes.
However, one unique feature of sequencing—based survey data is
that they only provide relative abundances of different microbes in a
community because the sequencing results are a function of sequenc—
ing depth and the biological sample size (Ni et al., 2013). Therefore,
metagenomic data collected from mixed genomic survey studies
belong to the so—called class of compositional data in statistics. It
was pointed out by Pearson (1897) more than one century ago that
correlation analysis method designed for absolute values could lead
to spurious correlations for compositional data. Great attention and
specialized methods are needed to appropriately analyze and inter—
pret compositional data. Filzmoser and Hron (2009) proposed a
procedure based on balances to measure correlations for compos—
itional data, but the groups defined by the balances cannot always
be clearly defined and separated from each other. Faust et al. (2012)
proposed CCREPE based on permutation and bootstrap to infer the
correlated significance but it’s difficult to explain the difference
between the permutation and bootstrap samples. Friedman and Aim
(2012) introduced correlation concepts of latent variables based on
log—ratio transformation of compositional data and proposed an
approximation method called SparCC to infer the correlation matrix
under sparse assumption. But SparCC does not consider the influ—
ence of errors in compositional data which may reduce the estima—
tion accuracy. In addition, there is no guarantee that the inferred
covariance matrix from SparCC is positive definite and even the cor—
relation coefficients may fall outside [—1, 1].

In this article, we propose a novel method based on least squares
with 21 penalty after log ratio transformation for raw compositional
data to infer the correlations among microbes through a latent vari—
able model, called Correlation inference for Compositional data
through Lasso (CCLasso). Similar to SparCC, CCLasso explicitly
considers the compositional nature of the metagenomic data in cor—
relation analysis, and it has the additional benefit that the estimated
correlation matrix of the latent variables for compositional data is
positive definite. We also propose an efficient alternating direction
algorithm of augmented Lagrangian method to solve the optimiza—
tion problem involved in our method. The tuning parameter that
balances the loss function and sparse assumption is chosen through
cross validation.

The performance of CCLasso is compared with SparCC through
simulation studies, using several correlation network structures and
sample sizes. The simulation results show that CCLasso gives more
accurate estimation for correlation matrix than SparCC as well as bet—
ter edge recovery. When CCLasso and SparCC are applied to estimate
the correlation networks of microbes from Human Microbiome
Project (HMP), we find that CCLasso is comparable with SparCC in
View of consistent accuracy and reproducibility. But for shuffled HMP
datasets there are supposed to be no correlations for any species,
SparCC always results some small correlations while CCLasso shrinks
these small values into 0. We believe that CCLasso can be applied to
study correlations of compositional data arising from metagenomic
data in natural environment and human body, and it is also broadly

applicable in many other contexts where there is interest to assess cor—
relations of variables from compositional data.

2 Methods

2.1 Correlation of latent variables for compositional

data

Suppose there are p microbe species and their absolute abundances
are random vector y : (311, . . . ,yp) which cannot be directly
observed in practice. Instead, only the compositional random vector

x : (x17  7x17),

xi: ” . (1)

 

can be observed from biological experiments. The absolute abun—
dances y are called latent variables since they cannot be directly
observed. The additive log normal distribution (Aitchison and Shen,
1980) is a special case for Equation (1) when 3! is from a multivariate
logarithm normal distribution. The relations among 3! are of more
relevance than x’s in both practice and theory. The interactions
among microbe species are described by 3! while there is a negative
correlation trend for compositional vector x from the constant sum
constraint even in the absence of any correlations among 3!,

17
Zack : 1 :> Z Cov(xi,xk) : —Var(x,).
12:1 #i

17
Let w : Z yk be the total absolute abundance for microbe species.
12:1
Covariances between the latent absolute abundance 3!, which cannot
be observed, and those of its compositional representation x, which
are observed, can be related through the base equation (1),

Cov(ln x,,ln xi) : Cov(ln 31,-, In 31,-) — Cov(ln 31,-, In w)
—Cov(ln w, In 31,-) + Var(ln w),

since In x,- : ln )1,- — ln w. Let 21“ x : Var(ln x), 21“ y : Var(ln y)
and a : Cov(ln y, In w) — Var(ln w)1p/2 where 1,, is a p X 1 vector
of 1’s, then the matrix form of connection between 21“ x and 21“ y
can be described as

21.. x 2 211., — alT — MT. (2)

We can focus on the correlation among log transforms of y and we
also call In 3! latent variables. When x is from additive logistic nor—
mal, the independences among In 3! are equivalent to 3!. Since there is
information loss from y to x through normalization procedure
(Equation 1), the problem of estimating 21“ y from the sample esti—
mation of 21,, x is undefined without any assumptions. This can be
easily seen from Equation (2) that there are {7(1) + 1) /2 equations
but p(p + 1) / 2 + 17 unknown parameters.

One way to get around this problem is to assume that 21“ y is
sparse which means that the interaction network among microbe spe—
cies has a small proportion of all possible edges present compared to
the fully connected network. Sparse structure is a very common as—
sumption for under—determinated problems such as linear regression
models (Tibshirani, 1996), Gaussian graphical models (Yuan and Lin,
2007) and compressed sensing (Candes and Tao, 2005) where the
number of unknown parameters is larger, sometimes much larger,
than the number of data points. For compositional data, there may
exist several sparse networks corresponding to the same 21“ x because

[310'sp2umofp105xo'sopeuHOJIItotq/ﬁdnq

3174

H.Fang et al.

 

y and its scaled form C(31))! (C(31) is any arbitrary positive random
variable which is a scaling factor) cannot be distinguished from the
base equation (1) if both 21,, y and 21,, (C(yjy) are sparse. The sparse
level of 21,, y is the key because there is at most one sparse network
21,, y whose edge density is no greater than % — [ﬁ corresponding to
the same 21,, x. And this sparse density condition cannot be relaxed
(See Supplementary Material). There are very few statistical methods
available to investigate the correlation among the latent variables In 31
with the exception of some recently introduced methods, e.g. SparCC
(Friedman and Alm, 2012).

To remove a in the right hand of Equation (2), we can choose a
matrix F with Rank(F) : p — 1 and F1,, : 0 and multiple F on both

sides in Equation (2),
1:21n xFT : 1:21n ,FT — FaigFT — FiapTFT : 1:21n ,FT. (3)

The left hand of Equation (3) is the variance of Fln x and the right
corresponding Fln 31. And their relationship can be seen as

Fln x : F(In y — lpln w) : Fln y.

The above relation can explain the two constraints for F. Rank(F)
: p — 1 ensures that there is a 1 — 1 correspondence between x and
Fln x since there is the constant sum constraint for x. So there is
no loss of information in statistical inference from Fln x instead of
x. Flp : 0 helps to cancel the common denominator to after log
transformation. There are many such transformation matrices sat—
isfying the two constraints, e.g. F : (Ep_1, —1p—1) is the linear trans—
formation for additive log ratio where the reference variables is xp
and F : El, — lplg/p for the centered log ratio for compositional
data where E, is a p X 17 identity matrix (Aitchison, 1982).

Let 2 Z 21“ y : [Jijjpxr
obtained after the fraction estimation from raw data such as metage—

The sample version S of 2111,, can be

nomic data through the Bayesian pseudo count method (Agresti and
Hitchcock, 2005 ). From Equation (3) and the sample estimation
S for 21,, x, we can get the following estimation equation,

FZFT : FSFT. (4)

Since Rank(F) : p — 1 and 2 is a 17 X1) positive definite matrix,
2 cannot be directly estimated through Equation (3). The additional
sparse assumption for 2 is reasonable in many application contexts,
such as metagenomic data, since most of variable pairs are not
expected to be correlated when the number of components is large.
Therefore, we can impose some sparsity constraints to help model
and infer 2 without other prior information.

2.2 SparCC and its limitations
Friedman and Alm (2012) proposed an iterative approximation ap—
proach called SparCC to solve the estimation equation (4) for a
number of special forms of transformation matrix F. In short,
SparCC first obtains a rough estimation for variance of latent vari—
able In y, and the corresponding correlation matrix. Then it uses a
threshold to remove the most correlated pair and repeatedly esti—
mates the variances and correlations until some terminating condi—
tions are met.

Under the above notation, SparCC’s algorithm can be summar—
ized as follows. First, SparCC obtains an estimation for the diagonal
of 2 from a rough approximation that

Ear/:0, Vi. (5)
#i

The rough approximation (Equation 5) supplies additional

17 equations for Equation (4). This assumption means that every com—
ponent has no correlations with others on average. Let F1 : (—1p_1,
Ep_1) and 212 : (221)T : C0v(ln y1,In 31-1), 222 : Var(In 31-1)
where In 31-1 : (In 312, . . . ,In yp)T, then Equation (4) can be written
as follows,

(-1p—17Ep—1)2(-1p—17Ep—1)T I FISFiT

:> 1,.15111;1 — 1,-1221— 2121;, + 222 : 131313,?

Computing the trace for both sides of above equation, we have

I7 I7
(p — 1)0’11+ :03} —  0'1,‘ 2 

i:2 i:2

[7 17
If E 01, : 0, then (17 — 1)011+ Z 0,, : tr(F1SFT). Let F,,z‘ : 2,
i:2 i:2
. . . ,p be the additive log ratio transformation matrix where the x, is
the reference variable, for example, Fl, : (E, —1p_1). Then similar to
F1, we have (17 — 1)0,-,- + Z0),- : tr(F,-SF,-T) for i: 2, ... ,p from
#i
the assumption (Equation 5). The corresponding solution is
Jii I

p
1% (tr(F,~SF,~T) _ ﬁgmsﬂ», (6)
—1

since El, +%1p1; : El, — $11,117,“). Then the basic coeffi—
cients can be obtained after substituting Equation (6 ) into Equation
(4). In fact, the above procedure is just a way to solve Equation (4)
and Equation (5). This is called basic SparCC in Friedman and Alm
(2012). A potential problem for SparCC is that 0,, in Equation (6)
can be negative, and so a minimal value Vmin is required to replace
negative 0,,. Second, SparCC employs an iterative refinement scheme
through excluding the strongest correlated pair if the corresponding
magnitude exceeds a given threshold or. The 0,, is updated through
removing the most significant correlation pair based on another
assumption like Equation (5),

Zn, 2 0. (7)

MC)

where C, denotes the set of indices of In 3!; identified to be strongly
correlated with In 31,. Finally, SparCC repeats the former two steps to
update the variance 0,, and the correlation matrix of In 31 through the
threshold or for a given iteration time or until no new strongly corre—
lated pair is identified or only three components left. And SparCC se—
lects a correlation threshold to give an interaction network.

As far as we are aware, SparCC is the first method to infer the cor—
relations among Iatent variables In 3! for compositional data. Its se—
cond step is an effective method to remove the strong assumption
(Equation 5) in the first approximation step and Equation (7) is
approximately right after removing the strongest pairs. Although it
represents a significant advance in analyzing compositional data,
SparCC has some limitations in the approximations. First, SparCC
directly solves Equation (4) with a series of approximate assumptions,
and the accuracy of Equation (4) is influenced by the errors resulting
from these approximations. Second, there is no consideration for the
overall property of the estimated correlation matrix. SparCC cannot
guarantee the inferred correlation matrix to be positive definite and
even the estimated correlations may fall out of [—1, 1].

2.3 CCLasso
We first note that the vectorization version of Equation (3)
together with sample variance S is e : (F®F)vec(2—S), where

/310‘spzumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

CCLasso

3175

 

8 satisfies  : 0 and Var(e) : (F (X) F)Var(vec(S))(FT (22) FT). Let
Vs : Var(vec(S)), then an inverse variance weighted loss function
can be given as follows,

LOSSl(2) 2 (vec(2 — S))T(FT ® FT)((F ® F)VS(FT ® FT»—1

1
2
(F (X) F)(vec(2 — S)),

(8)

where the inverse symbol M‘1 is the Moore—Penrose pseudo
inverse of M (Penrose and Todd, 1955). The solution to minimize
LOSS1(Z) in Equation (8) satisfies the estimation equation (4). One
important property of the loss function (Equation 8) is that it is
invariant for any choice of the linear transformation matrix F. This
property results from the fact that the information for 2 in the ori—
ginal data is kept after log ratio transformation.

The loss function LOSS1(Z) in Equation (8) is too complex to be
handle for the high—dimensional covariance matrix V5 (p2 X 172).
Inspired by Zhang and Zou (2012) for variance approximation of
sample variance, we can use the following loss function to substitute
Equation (8),

LOSSl’(2) 2 imam — S)FT)(FSFT)—1(F(>: — SW». (9)

The transformation matrix F in Equation (9) should be chosen rea—
sonably. Considering the symmetry of components, let
F0 : El, — % 11,1; be the transformation matrix of centered log ratio
with symmetric projection property F3 : F0, F2; : F0. It is sug—
gested that treating the weighting covariance matrix in loss func—
tions as diagonal performs well in some high—dimensional problems
(Chen et al., 2013). Let V : (diag(FoSFg;))_1. We may consider an—

other substitute for loss function,

mm 2 game: — s>F3>v<Fo<2 — on?»
(10)
2%))Fo<z—s)Forrzv.

The diagonal matrix V can be seen as a standardization matrix for
Fo(2 — S)Fo. The key idea of our method is that we use the loss func—
tion (Equation 10) because of its simplicity.

A reasonable approach to incorporating the sparse assumption
for 2 is to minimize loss function plus a suitable penalty. An ideal
penalty function is the number of non—zero elements in 2‘ which is
the off diagonal of 2. But it is computationally intractable where the
optimization involving )]2‘ ] )0 is a combinatorial optimization prob—
lem with an exponential complexity. A commonly used approach is
to replace Zo—norm by Zl—norm (Tibshirani, 1996; Yuan and Lin,
2007). We consider the following objective function combining loss
function and 21 penalty,

1
“2) = LOSSOZ) + PENQ) = 3 IIFo(2 - S)F0III/ + MIX—lb) (11)

where PEN(Z) : lnHZT  The tuning parameter 2,,20 in
Equation (11) is used to balance the fit of model (3) and the sparsity
assumption of 2. CCLasso aims to find a positive definite matrix 2)
so that
A . . 1 2 _
2 : arg min: arg min 2]]F0(2 — S)Fo]]V + MHZ H1, (12)
2>0 2>0

where 2 >— 0 means 2 should be positive definite. The corresponding
correlation matrix estimation can be derived from standardizing the
diagonal elements of 23. The optimization problem involved in
Equation (12) is convex since both the objective function f(2) and

the constraint region {2]2 >— 0} are convex. So the local minimiza—
tion of Equation (12) is global.

Compared with SparCC, CCLasso explicitly considers the error
terms behind the estimation equation (4) through the loss function
(Equation 10). The sparse assumption is directly handled through an
additional Z1—type penalty function in contrast to the additional
assumption (Equation 7) for SparCC. The estimated correlation
matrix from Equation (12) is positive definite and its elements are
located in [—1, 1] from the positive definite restriction.

2.4 Optimization algorithm and choice of 11,,

We develop an efficient algorithm based on the alternating direction
method to solve the constrained optimization problem in CCLasso
(Zhang and Zou, 2012). A relaxed version for Equation (12) can be
obtained after removing the positive definite constraint,

2 . 1 _
zzargmm2))F0(2—S)Fo))$+1,))2 ()1. (13)
222T

If the solution 2 in Equation (13) is positive definite, 2 : 2.
Otherwise the nearest positive definite matrix to E is used as 23.

To derive an alternating direction method for Equation (13),
we introduce a new matrix 21 and rewrite Equation (13) as follows,

- - , 1 _
(2.21) 2 argmm Erma—sworrvwnrrzl ()1.
222T.21:2

We consider the augmented Lagrangian function
1
L<2.21.A) 2 511F002 — Sm + 1.1112111
+tr(/\(2 - 21)) + (P/ZNI2 - 21111:,

where  -  is the matrix Frobenius norm. Let (2k, 2], Ak) be the so—
lution at step k, we update (2.21, A) according to

2k“ : arg min L(Z,Z]‘,Ak), (14]
222T
Elf“ : arg min L(2k+1,217Ak)7 (15]

21
and A’r+1 2 A’2 + p 2’?“ — 2k“ . Let 2k“ 2 s + M“ for (14), we
1

can write

. 1
Am 2 mg m EMPOAFOHZV + (more)?
A2AT
+ “(Aw + p(S — XI)»-

The above objective function on the right is quadratic for A and
AI2+1 is the solution of the following equation,

(1:0 VFOAFO + FOAFOVFO) + pA 2 —(A’2 + p(S — 2%)).

M2

Let F0 : UDoUT, (UTVU)11/2p + Ep-l/Z : UoDUg; (the subscript
11 means removing the last row and column) be the eigenvalue
decomposition for the corresponding matrix and
M : —UT(AIz +p(S — 2%))U/p, then the solution for the above
equation 15

U UTM U -CUT M
o{( o 11 o) } o 12>UT7 (16)

Ak+1 : 
M21 M22

where O is the Hadamard product of matrices and C,, : m.
To update 2]“, we define an operator C(A, /I) as follows,

/3.IO'S[EIIm0fp.IOJXO'SOIJEIIIJOJIIIOIq/ﬂduq

3176

H.Fang et al.

 

A, 1'21}
  i Aij—l  Aij>l,
’ ” 1),-+1 i213 A,<—1.
0. fee/7434;31-

From Equation (15), we write

2i“ = argzmin (Ir/201211112: - “(21Wz + PikH»)
1

then the solution of the above problem is 2]“ : G A7] + 2k+1,%2.
The following algorithm summarizes the details to carry out t e
above alternating direction method to solve the optimization prob—
lem (Equation 12) for CCLasso.

1. Initialization: k : 0, A0, 2(1) : Ep.
2. Repeat (a)—(d) until 2k and 2] converge:
1. 2k“ <— S + AI”1 where AI”1 is given in Equation (16);
2. 2’,“ <— G(A7‘ + Etta/"2);
3. Ak+1 (_Ak+p(2k+1_zli¢+1);
4. k <— k + 1.
3. Return the converged if as the solution for 2 deﬁned in
Equation (13).

The tuning parameter 2,,20 in Equation (11) has to be tuned
since it controls the balance between fitness of model (Equation 3)
and the sparsity assumption. A K—fold general cross validation of
loss function (Equation 10) is used to choose 2,, in this article. First,
all samples are divided into K disjoint subgroups as folds noted by 1,5
for k : 1,  ,K. These folds will be used as the training set and
testing set in turn. Second, for each k : 1, . .. ,K, compute Se and
S_k corresponding to the sample estimation of Var(In 17) through 1,,
and 11, ... ,Ik_1,1k+1, ... ,IK. The subscript —k means using all sam—
ples with the k—th fold left out. The weight matrix V for both train—
ing data and testing are based on all data. Thirdly, let S : S_k and
compute the estimation 2).), through Equation (12) for each
13kgK. Then compute the mean of K—fold cross validated errors for
the tuning parameter /I,,,

1 K 1 A
cv<1,)2 Ezirrma — SUFOIIiz-
12:1

Finally, we choose 2;:arg min gnCVUtn) as the final tuning
parameter.

3 Results

3.1 Simulation studies
Though a goal of a genomic survey study is to infer the correlations
among members of microbe communities from the abundance count
matrix, the estimation accuracy of correlation matrix using either
CCLasso or SparCC can be compared to assess their relative perform—
ance since they are both based on the same latent assumptions described
in Equation (1). The essential difference between these two methods is
the estimation procedure after obtaining the fraction estimation.

The compositional data are simulated from the additive logistic
normal distribution with a given mean and covariance matrix,

lnyNNmZ). 992$

The variation parameter of 14 controls the unbalance of components.

Every element of u is generated from a uniform distribution of
[—0.5, 0.5]. We focus on performance comparison between SparCC
and CCLasso on sparse correlation matrix with varying levels of
sparsity in our simulations. Five covariance structures are
considered:

1. Random Model: Every pair of components is connected with a
given probability 0.3 and the correlation strength is :0.15 with
equal probability 0.5.

2. Neighbor Model: Randomly select 17 points in the [0,1]2 plane.
Then connect the 10 nearest neighbors for each point with the
correlation strength 0.5.

3. AR(4) Model: Connect pair (1', j) if ]i — /]£4, and set the correl—
ation strength as 0.4, 0.2, 0.2 or 0.1 as the distance is 1, 2, 3 or
4, respectively.

4. Hub Model: Randomly select 3 points as hubs and the other 17 —
3 points as common points. Then connect each hub to others
with a probability 0.7 while creating edges with probability 0.2
among the common points and all edge strength is set to be 0.2.

5. Block Model: Divide 17 points into 5 blocks equally. Connect
each pair in the same block with probability 0.6 and correlation
strength 0.4 While connecting points in different blocks with
probability 0.2 and correlation strength 0.2.

To make the covariance matrix positive definite, the diagonal
elements of 2 are set large enough and then normalized all as 1. The
random model is a very common graph model in which every pos—
sible edge occurs independently with the same probability. Through
setting the strength as :0.15 with equal probability, the random
model roughly satisfies assumptions Equations (5) and (7) of
SparCC. The neighbor model is a 2—dimensional geography model in
which edges exist among nearest neighbors. The AR(4) model can
be considered as a model where points are ordered linearly along a
line where edges exist between those nodes whose distance is no
more than 4, and the correlation decreases as the distance increases.
The hub model describes a graph where some special nodes, called
hubs, are connected to others with a higher probability than the con—
nection probabilities among other nodes. The block model defines
network clustering where edge probabilities are higher within
groups than between groups. All models are sparse with different de—
grees of sparsity. The expect number of edges in the neighbor and
AR(4) model is proportional to 17 while 172 for the random, hub and
block model.

For all the models, we set 17 :50 and consider different sample
sizes 11 : 200, 300 and 500. For each model, and three combinations
of (p, n), we repeat simulations 100 times. The tuning parameter 2,,
is determined through 3—folds cross validation, and all data are used
to estimate 2 and the correlation matrix. We reimplemented
SparCC using R and the default tuning parameters or : 0.1,
kmx: 10 and V,,,,,, : 10—4 are used while the final correlation is
truncated by —1 and 1 as the lower and upper limit. SparCC is ro—
bust for its tuning parameters since only the strongest pair is
removed in each iteration (Supplementary Fig. 51).

To compare the performance between CCLasso and SparCC for
each combination of model setting and sample size, we define the

correlation inference accuracy by the mean absolute error d1(i), p)

:  ],?),-l- — pil] and the Frobenius norm distance d1:(i),p) :
K}

]]i) — pHF between the estimated correlation matrix ,0 and the true

one ,0. The area under the receiver operation characteristics curve

(AUC) is used to assess the performance of CCLasso and SparCC on

ﬁm'sreumol‘pquo'sopeuuowtotq/ﬁdnq

CCLasso

3177

 

recovering the non—zero entries in the sparse covariance matrix 2 to
avoid the threshold parameter selection.

Table 1 summarizes the performance of CCLasso and SparCC
for simulation studies in View of d1 and d}: distances and AUC. As
the sample size increases from 200 to 500, both d1 and 0’}: decrease
for both CCLasso and SparCC in each simulation setting. The esti—
mation errors of CCLasso are smaller than SparCC. And the corres—
ponding results suggest that CCLasso performs better than SparCC
in simulations. This may be due to the fact that CCLasso considers
random errors while SparCC does not. For edge recovery, CCLasso
also performs better than SparCC except for the random graph
model when sample size is 200 and 300. This can be explained by
the fact that the random model roughly satisfies assumptions
Equations (5) and (7) of SparCC. The accuracy and AUC are not
vey consistent such as d1 and d p for CCLasso is smaller than SparCC
but the AUC for SparCC is larger than CCLasso in the random mod—
el. This phenomenon’s reason is that the accuracy measures the con—
tinuous distance between the estimation and the true one while AUC
shows the discrimination between the non—zeros and zeros.

More detailed results for ROC are shown in Figure 1. As the sam—
ple size increases, the gap between CCLasso and SparCC increases.

Table 1. Performance comparisons of CCLasso and SparCC based
on simulation results

 

 

n Method L11 L11: 
Random Model
200 CCLasso 0.033(0.001) 2.954(0.049) 0.791(0.015)
SparCC 0.057(0.001) 3.528(0.080) 0.823(0.014)
300 CCLasso 0.028(0.001) 2.409(0.057) 0.885(0.012)
SparCC 0.047(0.001) 2.901(0.059) 0.891(0.011)
500 CCLasso 0.023(0.001) 1.994(0.053) 0.953(0.007)
SparCC 0.038(0.001) 2.332(0.056) 0.951(0.006)
Neighbor Model
200 CCLasso 0.039(0.003) 3.355(0.206) 0.948(0.015)
SparCC 0.076(0.001) 4.606(0.081) 0.888(0.014)
300 CCLasso 0.033(0.002) 2.675(0.151) 0.986(0.006)
SparCC 0.070(0.001) 4.176(0.060) 0.931(0.009)
500 CCLasso 0.026(0.002) 2.064(0.121) 0.999(0.001)
SparCC 0.065(0.001) 3.800(0.041) 0.967(0.006)
AR(4) Model
200 CCLasso 0.021(0.001) 2.444(0.134) 0.885(0.021)
SparCC 0.061(0.001) 3.766(0.087) 0.858(0.019)
300 CCLasso 0.018(0.001) 1.994(0.133) 0.922(0.017)
SparCC 0.052(0.001) 3.210(0.078) 0.890(0.017)
500 CCLasso 0.015(0.001) 1.549(0.087) 0.958(0.011)
SparCC 0.044(0.001) 2.693(0.059) 0.918(0.011)
Hub Model
200 CCLasso 0.037(0.001) 3.453(0.037) 0.749(0.021)
SparCC 0.067(0.001) 4.194(0.070) 0.690(0.014)
300 CCLasso 0.036(0.001) 3.133(0.047) 0.768(0.021)
SparCC 0.059(0.001) 3.686(0.049) 0.735(0.012)
500 CCLasso 0.032(0.001) 2.918(0.048) 0.828(0.018)
SparCC 0.051(0.001) 3.248(0.043) 0.788(0.010)
Block Model
200 CCLasso 0.039(0.001) 3.307(0.113) 0.782(0.014)
SparCC 0.070(0.001) 4.268(0.072) 0.734(0.010)
300 CCLasso 0.035(0.001) 2.773(0.079) 0.854(0.014)
SparCC 0.062(0.001) 3.788(0.052) 0.765(0.011)
500 CCLasso 0.029(0.001) 2.258(0.076) 0.924(0.011)
SparCC 0.057(0.001) 3.374(0.038) 0.796(0.012)

 

d1 and L1,. are the two distances between the estimated correlation matrix
and the true one deﬁned in the text. AUC is the area under the receiver oper—
ation characteristics curve. The results are the averages over 100 simulation
runs with standard deviations in brackets.

For the low false—positive rate such as 0.1, the true—positive rate for
CCLasso is larger than SparCC except the random graph model. An
interesting phenomenon is that both CCLasso and SparCC perform
poorly for the hub model, but as sample size increases the estimation
efficiency improves. One should use a much larger sample size for
some special graphical structures such as the hub model and the block
model than others for given precision. We also compare CCREPE
with CCLasso through ROC and find the performance of CCREPE is
similar to SparCC (Supplementary Fig. 52).

3.2 HMP data

Because of close relationships between ourselves and the microbes in
our body, the Human Microbiome Project Consortium (2012a,b)
aims to investigate the fundamental roles of the microbes in human
health and disease. The high—quality sequencing reads in 16S vari—
able regions 3—5 (V35) of HMP healthy individuals are used to
explore the correlation interactions among the microbes in 18 body
sites and the corresponding operational taxonomic units (OTUs) are
obtained from the HMPOC dataset, available at http://www.
hmpdacc.org/HMMCP/. We consider Phase I production study
(May1, 2010) and the first sample collected for multiple samples
from the sample body site of the same individual. The data are fur—
ther filtered by removing samples with less than 500 reads or more
than 60% Os are collected and by removing OTUs that are repre—
sented by less than 2 reads per sample on average or more than 60%
05. The transformation from counts to compositional data cannot
be directly normalized since both CCLasso and SparCC assume that
the Os in the OTU counts are not real 0 fractions. CCLasso adds all
counts by the maximum rounding error 0.5 and then normalizes the
counts to get compositional data. Friedman and Alm (2012) pro—
vided Bayesian framework to estimate the fractions from counts for
SparCC. The final estimation for SparCC is the median of estima—
tions in 20 replicated samples from the posterior distribution of
fractions.

Since there is no prior information for true correlation network
of taxon—taxon interaction in real data, we use consistent accuracy
and reproducibility to compare the performance of CCLasso and
SparCC. First, all data are used to construct a gold standard refer—
ence correlation matrix for CCLasso and SparCC. The estimated
correlation matrix in this step is treated as “known” since all data
are used. Second, we randomly select half samples to estimate the
correlation matrix through CCLasso and SparCC. The consistent ac—
curacy is measured by the Frobenius norm distance between the esti—
mated correlation matrices of the first and second step. The
consistent reproducibility is measured by the fraction of the same
edges shared for these two steps in the first gold reference network
which only the top 1/4 edges is used. This procedure is repeated 20
times for stable results.

The results are summarized in Table 2. CCLasso and SparCC
have similar performance in terms of consistent accuracy and repro—
ducibility. When the sample size is small, the reproducibility is low.
Even for large sample size such as left Antecubital fossa, the repro—
ducibility is only 0.64 for both CCLasso and SparCC. We can find
consistent accuracy and reproducibility are not good criteria from
the simulation data (Supplementary Table 51). Since there are sev—
eral optimization procedures for the cross validation of CCLasso,
SparCC is faster than CCLasso (Supplementary Table 52). The
reproducibility is robust for the top edges” choice (Supplementary
Table S3). We also compare the inferred correlation network from
CCLasso and SparCC using all samples for all body sites and find
their results are very similar (Supplementary Fig. S3 and Table S4).

ﬁm'sreumol‘piqxo'sopeuuowtotq/ﬁdnq

///
///.

///
///

Figure 2

Supplementary Fig. S4

al. (2014)

Paulson et al. (2013)

Biswas et al. (2014)

Biswas et

/3.IO'S[EIIm0fp.IOJXO'SOIJEIIIJOJIIIOIq/ﬂduq

3180

H.Fang et al.

 

(No. 2015CB910303). H.F.’s work was supported in part by a fel—
lowship from Graduate School of Peking University. H.Z.’s work
was supported in part by NIH GM59507.

Conflict of Interest: none declared.

References

Agresti,A. and Hitchcock,D.B. (2005). Bayesian inference for categorical data
analysis. Stat. MethodAppl, 14, 297—330.

Aitchison,]. (1982). The statistical analysis of compositional data. ]. R. Stat.
SOC. B, 44, 139—177.

Aitchison,]. and Shen,S.M. (1980). Logistic-normal distributions: Some prop-
erties and uses. Biometrilza, 67, 261—272.

Biswas,S. et al. (2014). Learning microbial interaction networks from metage-
nomic cout data. arXiv:1412.0207v1 [q-bio.QM].

Candes,E.]. and Tao,T. (2005). Decoding by linear programming. IEEE T
Inform. Theory, 51, 4203—4215.

Chen,]. et al. (2013). Structure-constrained sparse canonical correlation ana-
lysis with an application to microbiome data analysis. Biostatistics, 14,
244—25 8.

Faust,K. et al. (2012). Microbial co-occurrence relationships in the human
microbiome. PLOS Comput. Biol, 8, e1002606.

Filzmoser,P. and Hron,K. (2009). Correlation analysis for compositional data.
Math. Geosci., 41, 905—919.

Friedman,]. and Alm,E.]. (2012). Inferring correlation networks from genomic
survey data. PLoS Comput. Biol, 8, e1002687.

Gill,S.R. et al. (2006). Metagenomic analysis of the human distal gut micro-
biome. Science, 312, 1355—1359.

Handelsman,]. et al. (1998). Molecular biological access to the chemistry of
unknown soil microbes: a new frontier for natural products. Chem. Biol, 5,
R245—R249.

Human Microbiome Project Consortium. (2012a). A framework for human
microbiome research. Nature, 486, 215—221.

Human Microbiome Project Consortium. (2012b). Structure, function and
diversity of the healthy human microbiome. Nature, 486, 207—214.

Ni,]. et al. (2013). How much metagenomic sequencing is enough to achieve a
given goal?. Sci. Rep., 3, 1968, doi:10.1038/srep01968.

Paulson,].N. et al. (2013). Differential abundance analysis for microbial
marker-gene surveys. Nat. Methods, 10, 1200—1202.

Pearson,K. (1897). On a form of spurious correlation which may arise when
indices are used in the measurement of organs. Proc. R. Soc. Lond., 60,
489—502.

Penrose,R. and Todd,].A. (1955). A generalized inverse for matrices. Math.
Proc. Cambridge, 5 1.

Pikuta,E.V. et al. (2007). Microbial extremophiles at the limits of life. Crit.
Rev. Microbiol, 33, 183—209.

Savage,D.C. (1977). Microbial ecology of the gastrointestinal tract. Armu.
Rev. Microbiol, 31, 107—133.

Tibshirani,R. (1996). Regression shrinkage and selection Via the lasso.  R.
Stat. Soc. B, 58, 267—288.

Yuan,M. and Lin,Y. (2007). Model selection and estimation in the gaussian
graphical model. Biometrilza, 94, 19—35.

Zhang,T. and Zou,H. (2012). Sparse precision matrix estimation via lasso
penalized d-trace loss. Biometrilza, 99, 1—18.

ﬁm'sreumol‘piqxo'sopeuuowtotq/ﬁdnq

