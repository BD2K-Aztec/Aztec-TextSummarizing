BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Bayesian consensus clustering

 

functions and algorithms to perform consensus clustering have
been proposed [for a survey see Nguyen and Caruana (2007)].
Most of these methods do not inherently model uncertainty, and
statistical models assume that the separate clusterings are known
in advance (Wang et al., 2010, 2011). Consensus clustering is
most commonly used to combine multiple clustering algorithms,
or multiple realizations of the same clustering algorithm, on a
single dataset. Consensus clustering has also been used to inte—
grate multisource biomedical data (Cancer Genome Atlas
Network, 2012). Such an approach is attractive in that it
models source—speciﬁc features, yet still determines an overall
clustering, which is often of practical interest. However, the
two stage process of performing entirely separate clusterings fol—
lowed by post hoc integration limits the power to identify and
exploit shared structure (see Section 3.2 for an illustration of this
phenomenon).

Approach (2) effectively exploits shared structure, at the ex—
pense of failing to recognize features that are specific to each
data source. Within a model—based statistical framework, one
can ﬁnd the clustering that maximizes a joint likelihood.
Assuming that each source is conditionally independent given
the clustering, the joint likelihood is the product of the likelihood
functions for each data source. This approach is used by
Kormaksson et al. (2012) in the context of integrating gene ex—
pression and DNA methylation data. The iCluster method (Mo
et al., 2013; Shen et al., 2009) performs clustering by first fitting a
Gaussian latent factor model to the joint likelihood; clusters are
then determined by K—means clustering of the factor scores. Rey
and Roth (2012) propose a dependency—seeking model in which
the goal is to ﬁnd a clustering that accounts for associations
across the data sources.

More ﬂexible methods allow for separate but dependent
source clusterings. Dependent models have been used to simul—
taneously cluster gene expression and proteomic data (Rogers
et al., 2008), gene expression and transcription factor binding
data (Savage et al., 2010) and gene expression and copy
number data (Yuan et al., 2011). Kirk et al. (2012) describe a
more general dependence model for two or more data sources.
Their approach, called Multiple Dataset Integration (MDI), uses
a statistical framework to cluster each data source while simul—
taneously modeling the pairwise dependence between clusterings.
Savage et al. (2013) use MDI to integrate gene expression,
methylation, microRNA and copy number data for glioblastoma
tumor samples from TCGA. The pairwise dependence model
does not explicitly model adherence to an overall clustering,
which is often of practical interest.

2 METHODS

2.1 Finite Dirichlet mixture models

Here we brieﬂy describe the ﬁnite Dirichlet mixture model for clustering a
single dataset, with the purpose of laying the groundwork for the inte-
grative model given in Section 2.2. Given data Xn for N objects
(n = 1, . . . , N), the goal is to partition these objects into at most K clus-
ters. Typically X” is a multidimensional vector, but we present the model
in sufﬁcient generality to allow for more complex data structures. Let
ﬂane) deﬁne a probability model for X” given parameter(s) 6. For ex-
ample, f may be a Gaussian density deﬁned by the mean and variance
6 = (u, :72). Each X” is drawn independently from a mixture distribution

with K components, speciﬁed by the parameters 61, ..., 6K. Let
C,, e {1, ...,K} represent the component corresponding to X”, and in.
be the probability that an arbitrary object belongs to cluster k:

in. = P(C,, = k).

Then, the generative model is
X" ~ f(-|6,‘.) with probability 71k.

Under a Bayesian framework, one can put a prior distribution on
H =(rt1, ...,JTK) and the parameter set 6) = (61, ...,QK). It is natural
to use a Dirichlet prior distribution for H. Standard computational meth-
ods such as Gibbs sampling can then be used to approximate the poster-
ior distribution for H, G) and C = (C1, ..., CN). The Dirichlet prior is
characterized by a K-dimensional concentration parameter )3 of positive
reals. Low prior concentration (for example, [Bk 5 1) will allow some of
the estimated in. to be small, and therefore N objects may not represent
all K clusters. Letting K —> 00 gives a Dirichlet process.

2.2 Integrative model

We extend the Dirichlet mixture model to accommodate data from M
sources X1, . . . , XM. Each data source is available for a common set of N
objects, where an represents data m for object n. Each data source
requires a probability model fm(X,,|6m) parametrized by 6,". Under the
general framework presented here, each Xm may have disparate structure.
For example, X1" may give an image where f1 deﬁnes the spectral density
for a Gaussian random ﬁeld, while X2" may give a categorical vector
where f2 deﬁnes a multivariate probability mass function.

We assume there is a separate clustering of the objects for each data
source, but that these adhere loosely to an overall clustering. Formally,
each Xm" n = 1, ...,N is drawn independently from a K-component
mixture distribution speciﬁed by the parameters 6",], . . . ,QmK. Let
Lm" e {1, . . . ,K} represent the component corresponding to an.
Furthermore, let C,, e {1, ...,K} represent the overall mixture compo-
nent for object n. The source-speciﬁc clusterings |].,,, = (Lml , . . . , LmN) are
dependent on the overall clustering C = (C 1 , . . . , C N):

P(Lmn 2 MC") 2 VOL Cmam)

where Olm adjusts the dependence function v. The data Xm are independ-
ent of C conditional on the source-speciﬁc clustering |]_,,,. Hence, C serves
only to unify |]_1, . . . , ILM. The conditional model is

P(Lmn 2 lemm Cm emit) (X V03 Cm“m)fm(an|9mk)-
Throughout this article, we assume v has the simple form

v(Lm.,c.,am) = {EC 2 Lm" (1)

Kf’l" otherwise

where Olm 6 [11—0 1] controls the adherence of data source m to the overall
clustering. More simply aim is the probability that Lm" = C,,. So, if
Olm = 1, then |].,,, = C. The aim are estimated from the data together
with C and IL], ..., |]_,,,. In practice we estimate each Olm separately, or
assume that at] = . . . 2 (XM and hence each data source adheres equally to
the overall clustering. The latter is favored when M: 2 for identiﬁability
reasons. More complex models that permit dependence of the aims are
also potentially useful.

Let in. be the probability that an object belongs to the overall cluster k:

in. = P(C,, = k).

We assume a Dirichlet(ﬁ) prior distribution for H =(rt1, ...,JTK). The
probability that an object belongs to a given source-speciﬁc cluster fol-
lows directly:

1_

(X
P(Lmn =  = Hiram  _”k) K_ 

 

(2)

 

2611

ﬁre'spzumofpmﬂo'sopeuuopnoiq/ﬁdnq

E.F.Lock and D.B.Dunson

 

Table 1. Notation

 

N Number of objects

M Number of data sources

K Number of clusters

Xm Data source m

an Data for object n, source m

f,“ Probability model for source m
6.11. Parameters for f,,,, cluster k

pm Prior distribution for 6.11.

C,, Overall cluster for object n

:11. Probability that C,, = k

Lmn Cluster speciﬁc to an

v Dependence function for C,, and Lmn
Olm Probability that Lm" = C,,

 

Moreover, a simple application of Bayes rule gives the conditional distri-
bution of C:

M
P(C,, 2 MIL, 11,01) oc in. H v(Lm,,,k,oim),
m:1
where v is deﬁned as in (1).

The number of possible clusters K is the same for |]_1, . . . , ILM and C.
The link function v naturally aligns the cluster labels, as cases in which
the clusterings are not well aligned (a permutation of the labels would
give better agreement) will have low posterior probability. The number of
clusters that are actually represented may vary, and generally the source-
speciﬁc clusterings I]... will represent more clusters than C, rather than
vice versa. This follows from Equation (2) and is illustrated in Section 2
of the Supplementary Material. Intuitiver if object n is not allocated to
any overall cluster in data source m (i.e. LmngéC), then an does not
conform well to any overall pattern in the data.

Table 1 summarizes the mathematical notation used for the integrative
model.

2.3 Marginal forms

Integrating over the overall clustering C gives the joint marginal distri-
bution of IL], ..., ILM:

K A!
P({L... = I<..}:;;. 111a) cx Z 2:. 1‘1 v(k.., I<,a..). (3)
k2] m:1
Under the assumption that or] = . . . 2 (XM the model simpliﬁes:
K
P({L... = 19.}le 111a) cx 2 mm (4)

k2]

where [k is the number of clusters equal to k and U = % z 1. This
marginal form facilitates comparison with the MDI method for depend-
ent clustering. In the MDI model ¢ij>0 control the strength of associ-
ation between the clusterings IL,» and IL]:

M
P({L... =k..}:;;.|ﬁ,<1>)o< 1‘1 2%.... 1‘1 (1+¢.-) (5)

m:1 1i<j1k,-:k/1

where rim. 2 P(Lm,, = k). For K = 2 and ft], = 7%,, it is straightforward
to show that (4) and (5) are functionally equivalent under a parameter
substitution (see Section 3 of the Supplementary Material). There is no
such general equivalence between the models for K > 2 or M > 2, regard-
less of restrictions on H and (D. This is not surprising, as MDI gives a
general model of pairwise dependence between clusterings rather than a
model of adherence to an overall clustering.

2.4 Estimation

Here we present a general Bayesian framework for estimation of the
integrative clustering model. We use a Gibbs sampling procedure to ap-
proximate the posterior distribution for the parameters introduced in
Section 2.2. The algorithm is general in that we do not assume any spe-
ciﬁc form for the f,“ and the parameters 6.11.. We use conjugate prior
distributions for Olm, I‘[ and (if possible) 6.11..

o Olm ~ TBeta(am,bm, [17), the Beta(am, hm) distribution truncated
below by  By default we choose am 2 hm = 1, so that the prior
for aim is uniformly distributed between I]? and 1.

o I‘[ ~ Dirichlet(ﬁ0). By default we choose 130 = (1, 1, . . . , 1), so that the
prior for I‘[ is uniformly distributed on the standard (M — 1)-simplex.

o The 6.11. have prior distribution 17,“. In practice, one should choose pm
so that sampling from the conditional posterior pm(6mk|Xm, ll...) is
feasible.

Markov chain Monte Carlo (MCMC) proceeds by iteratively sampling
from the following conditional posterior distributions:

0 @mlxm, I]... ~pm(6mk|Xm, ll...) for k = 1, ...,K.

o |]_,,,|Xm, 8m,oim,C ~ P(k|Xm,,, C,,,6m,‘.,oim) for n = 1, ...,N, where

P(k|erla Cm  (X V03 Cm am)fm(an|9mk)-

amlC, I]... ~ TBeta(am + rm, hm + N — rm, 1%), where rm is the num-
ber of samples n satisfying Lm" = C,,.
CH]..., 11,0. ~ P(k|I‘I, {Lm,,,am};,‘f:1) for n = 1,  where

A!
Pam—L {meam}::1:1) (X 77k H VUQ Lmnaam)

m:1

o I‘IlC ~ Dirichlet(ﬁ0 + p), where p1. is the number of samples allo-
cated to cluster k in C.

This algorithm can be suitably modiﬁed under the assumption that
or] = . .. 2 (XM (see Section 1.2 of the Supplementary Material).

Each sampling iteration produces a different realization of the cluster-
ings C, |]_1, - - - , |]_,,,, and together these samples approximate the posterior
distribution for the overall and source-speciﬁc clusterings. However, a
point estimate may be desired for each of C, |]_1, - - - , I]... to facilitate in-
terpretation of the clusters. In this respect, methods that aggregate over
the MCMC iterations to produce a single clustering, such as that
described in Dahl (2006), can be used.

It is possible to derive a similar sampling procedure using only the
marginal form for the source-speciﬁc clusterings given in Equation (3).
However, the overall clustering C is also of interest in most applications.
Furthermore, incorporating C into the algorithm can actually improve
computational efﬁciency dramatically, especially if M is large. As pre-
sented, each MCMC iteration can be completed in 0(MNK) operations.
If the full joint marginal distribution of L1, ...,LM is used the compu-
tational burden increases exponentially with M (this presents a bottleneck
for the MDI method).

For each iteration, C,, is determined randomly from a distribution that
gives higher probability to clusters that are prevalent in {L1,,, . . . , Lmn}. In
this sense, C is determined by a random consensus clustering of the
source-speciﬁc clusterings. Hence, we refer to this approach as Bayesian
consensus clustering (BCC). BCC differs from traditional consensus clus-
tering in three key aspects.

(1) Both the source-speciﬁc clusterings and the consensus clustering
are modeled in a statistical way that allows for uncertainty in all
parameters.

(2) The source-speciﬁc clusterings and the consensus clustering are
estimated simultaneously, rather than in two stages. This permits

 

2612

ﬁre'spzumofpmﬂo'sopeuuopnoiq/ﬁdnq

Bayesian consensus clustering

 

borrowing of information across sources for more accurate cluster
assignments.

(3) The strength of association to the consensus clustering for each data
source is learned from the data and accounted for in the model.

We have developed software for the R environment for statistical
computing (R Development Core Team, 2012) to perform BCC on multi-
variate continuous data using a Normal-Gamma conjugate prior distri-
bution for cluster-speciﬁc means and variances. Full computational
details for this implementation are given in Section 1.1 of the
Supplementary Material. This software is open source and may be mod-
iﬁed for use with alternative likelihood models (e.g. for categorical or
functional data).

2.5 Choice of K

One can infer the number of clusters in the model by specifying a large
value for the maximum number of clusters K, for example K =N. The
number of clusters realized in C and the I]... may still be small. However,
we ﬁnd that this is not the case for high-dimensional structured data such
as that used for the genomics application in Section 3.3. The model tends
to select a large number of clusters even if the Dirichlet prior concentra-
tion parameters 130 are small. The number of clusters realized using a
Dirichlet process increases with the sample size; hence, if the number of
mixture component is indeed ﬁnite, the estimated number of clusters is
inconsistent as N —> 00 (Miller and Harrison, 2013). This is undesirable
for exploratory applications in which the goal is to identify a small
number of interpretable clusters.

Alternatively, we consider a heuristic approach that selects the value of
K that gives maximum adherence to an overall clustering. For each K, the
estimated adherence parameters Olm e [1% , 1] are mapped to the unit inter-
val by the linear transformation

Kotm — 1
“"7 ‘ K — 1

We then select the value of K that results in the highest mean adjusted
adherence

_ 1 M
(ﬁzﬁg 01:1.
m:1

This approach will generally select a small number of clusters that reveal
shared structure across the data sources.

3 RESULTS

3.1 Accuracy of a?

We find that with reasonable signal the am can generally be
estimated with accuracy and without substantial bias. To illus—
trate, we generate simulated datasets X1 : 1 x 200 and
X2 : 1 x 200 as follows:

(1) Let C deﬁne two clusters, where C" = 1 for
n e {1, ...,100} and C" :2 for n e {101, ...,200}.

(2) Draw at from a Uniform(0.5,1) distribution.

(3) Form =1,2andn = 1, ...,200,generateLm,, 6 {1,2} with
probabilities P(Lm,. = C") 2 at and P(L,,m 75 C.) = 1 — oz.

(4) For m = 1, 2, draw values X,,,,, from a Normal(1.5,1) dis—
tribution if L... = 1 and from a Normal(—1.5, 1) distribu—
tion if L... = 2.

We generate 100 realizations of the above simulation, and es—
timate the model via BCC for each realization. We assume

True Vs. Estimated (1

 

1.0

A
0.8
|

Estimate (x

0.7

0.6
I

 

 

 

0.5 0.6 0.7 0.8 0.9 1.0
True 0:
Fig. 1. Estimated 6i versus true at for 100 randomly generated simula-

tions. For each simulation, the mean value (it is shown with a 95% cred-
ible interval

Oil 2 Gig in our estimation and use a uniform prior; further com—
putational details are given in Section 4 of the Supplementary
Material. Figure 1 displays oi, the best estimate for both Oil and
a2, versus the true at for each realization. The point estimate
displayed is the mean over MCMC draws, and we also display
a 95% credible interval based on the 2.}975 percentiles of the
MCMC draws. The estimated a? are generally close to the true at,
and the credible interval contains the true value in 91 of 100
simulations. See Section 4 of the Supplementary Material for a
more detailed study, including a simulation illustrating the effect
of the prior distribution on oi.

3.2 Clustering accuracy

To illustrate the ﬂexibility and advantages of BCC in terms of
clustering accuracy, we generate simulated data sources X1 and
X2 as in Section 3.1 but with Normal(1,1) and Normal(—1,1) as
our mixture distributions. Hence, the signal distinguishing the
two clusters is weak enough so that there is substantial overlap
within each simulated data source. We generate 100 simulations
and compare the results for four model—based clustering
approaches:

(1) Separate clustering, in which a ﬁnite Dirichlet mixture
model is used to determine a clustering separately for X1
and X2.

(2) Joint clustering, in which a finite Dirichlet mixture model
is used to determine a single clustering for the concate—
nated data [X1 X21.

(3) Dependent clustering, in which we model the pairwise
dependence between each data source, in the spirit of
MDI.

(4) Bayesian consensus clustering.

The full implementation details for each method are given in
Section 5 of the Supplementary Material.

 

2613

ﬁre'spzumoipmﬂo'sopeuuopnoiq/ﬁdnq

E.F.Lock and D.B.Dunson

 

We consider the relative error for each model in terms of the
average number of incorrect cluster assignments:

.11 ,\ A
Z Z 1Jill/7m i Llllli}
Source error = ”’:1 ":1

li/[N

.\ A
211K). 75 Cu}
Overall error = ":1
N

where l] is the indicator function. For joint clustering. the source
clusters E”, are identical. For separate and dependent clustering.
we determine an overall clustering by maximizing the posterior
expected adjusted Rand index (Fritsch and Ickstadt. 2009) of the
source clusterings.

The relative error for each clustering method with M: 2 and
M: 3 sources is shown in Figure 2. Smooth curves are fit to the
results for each method using LOESS local regression
(Cleveland. 1979) and display the relative clustering error for
each method as a function ofor. Not surprisingly. joint clustering
performs well for or ~ 1 (perfect agreement) and separate cluster-
ing performs well when or ~ 0.5 (no relationship). BCC and de-
pendent clustering learn the level of cluster agreement. and hence
serve as a flexible bridge between these two extremes. Dependent
clustering does not perform as well with M :3 sources. as the

Source error M—2

 
 
     
 
 
  
   
  

l!)
“'2
O — Joint
o — Separate
<v>_ — Dependent
O BCC (estimated or)
m — BCC (true or)
N
o'
e 8
‘LE 0
5’
o'
.9
o'
L!)
C)
o'
0.5 0.6 0.7 0.8 0.9 1,0
Truea
Overall error M=2
L0
:5
— Joint
— Separate
— Dependent
;- BCC (estimated (x)
— BCC (true on)
0')
§ :5
m
N
o'
g . .
I

0.5 0.6 0.7 0.8 0.9 1.0

True a

Fig. 2. Source—specific and overall clustering error for 100 simulations with M: 2 and M : 3 data sources. shown forjoint clustering. separate clustering.

pairwise dependence model does not assume an overall cluster-
ing and therefore has less power to learn the underlying structure
for M >2.

3.3 Application to genomic data

We apply BCC to multisource genomic data on breast cancer
tumor samples from TCGA. For a common set of 348 tumor
samples. our full dataset includes

0 RNA gene expression (GE) data for 645 genes.

0 DNA methylation (ME) data for 574 probes.

o miRNA expression (miRNA) data for 423 miRNAs.

0 Reverse phase protein array (RPPA) data for 171 proteins.

These four data sources are measured on different platforms
and represent different biological components. However. they all
represent genomic data for the same sample set and it is reason-
able to expect some shared structure. These data are publicly
available from the TCGA Data Portal. See http:;";"people.duke.
edug"%7Ee1113fsoftware.html for R code to completely repro-
duce the following analysis. including instructions on how to
download and process these data from the TCGA Data Portal.

Breast cancer is a heterogeneous disease and is therefore a nat-
ural candidate for clustering. Previous studies have found

Source error M—3

Joint
Separate
Dependent
BCC (estimated a)
BCC (true or)

   
  

  
 
   
  

Error
0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40

0.5 0.6 0.7 0.8 0.9 1.0

True 0t

Overall error M=3

  

LO
CS I
— Joint
— Separate
V — Dependent
o’ BCC (estimated or)
BCC (true or)
9".
é o
m
N
o'
E

0.5 0.6 0.7 0.8 0.9 1.0

True (1

dependent clustering. BCC and BCC using the true or. A LOESS curve displays clustering error as a function of or for each method

 

2614

ﬁre'spzumoipropo'sopeuuopnorq/ﬁdnq

Bayesian consensus clustering

 

anywhere from 2 (Duan, 2013) to 10 (Curtis et al., 2012) distinct
clusters based on a variety of characteristics. In particular, 4 com—
prehensive sample subtypes were previously identified based on a
multisource consensus clustering of the TCGA data (Cancer
Genome Atlas Network, 2012). These correspond closely to the
well—known molecular subtypes Basal, Luminal A, Luminal B and
HER2. These subtypes were shown to be clinically relevant, as
they may be used for more targeted therapies and prognosis.

We use the heuristic described in Section 2.5 to select the
number of clusters for BCC, with intent to determine a clustering
that is well—represented across the four genomic data sources. We
select K :3 clusters, and posterior probability estimates were
converted to hard clusterings via Dahl (2006) to facilitate com—
parison and visualization. Table 2 shows a matching matrix com—
paring the overall clustering C with the comprehensive subtypes
defined by TCGA, as well as summary data for the BCC clusters.

The TCGA and BCC clusters show different structure but are
not independent (P—value <0.01; Fisher’s exact test). BCC clus—
ter 1 corresponds to the Basal subtype, which is characterized by
basal—like expression and a relatively poor clinical prognosis.
BCC cluster 2 is primarily a subset of the Luminal A samples,
which are genomically and clinically heterogeneous. DNA copy
number alterations, in particular, are a source of diversity for
Luminal A. On independent datasets Curtis et al. (2012) and
Jonsson et al. (2010) identify a subgroup of Luminal A that is
characterized by fewer copy number alterations and a more
favorable clinical prognosis (clusters IntClust 3 and Luminal—
simple, respectively). As a measure of copy number activity, we
compute the fraction of the genome altered (FGA) as described
in Cancer Genome Atlas Network (2012) Supplementary Section

Table 2. BCC cluster versus TCGA comprehensive subtype matching
matrix and summary data for BCC clusters

 

BCC cluster

 

 

 

1 2 3

TCGA subtype

1 (Her2) 13 6 20

2 (Basal) 66 2 4

3 (Lum A) 3 80 78

4 (Lum B) 0 3 73
5-year survival 0.67:i:O.20 0.94:1:0.08 0.81 :i:0.11
FGA 0.22:1:004 0.10:1:0.02 0.20:1:002
ER+ 13% 92% 94%
PR+ 7% 86% 75%
HER2+ 15% 12% 18%
8p11 ampliﬁcation 32% 19% 42%
8q24 ampliﬁcation 79% 39% 67%
5q13 deletion 61% 3% 14%
16q23 deletion 19% 66% 61%

 

Note: Summary data includes 5—year survival probabilities using the KaplaniMeier
estimator, with 95% conﬁdence interval; mean fraction of the genome altered
(FGA) using threshold T: 0.5, with 95% conﬁdence interval; receptor status for
estrogen (ER), progesteron (PR) and human epidermal growth factor 2 (HER2);
and copy number status for ampliﬁcation at sites 8p11 and 8q23 and deletion at sites
5q13 and 16q23.

VII (with threshold T=0.50) for each BCC cluster. Clusters 1
and 3 had an FGA above 0.2, while Cluster 2 had an FGA of
0.10 (Table 2). For comparison, those Luminal A samples that
were not included in Cluster 2 had a substantially higher average
FGA of 0.17 :i: 0.02. Cluster 3 primarily includes those samples
that are receptor (estrogen and/or progesterone) positive and
have higher FGA. These results suggest that copy number vari—
ation may contribute to breast tumor heterogeneity across sev—
eral genomic sources.

Figure 3 provides a point—cloud view of each dataset given by a
scatter plot of the ﬁrst two principal components. The overall
and source—speciﬁc cluster index is shown for each sample, as
well as a point estimate and ~95% credible interval for the ad—
herence parameter or. The GE data has by far the highest adher—
ence to the overall clustering (or = 0.91); this makes biological
sense, as RNA expression is thought to have a direct causal re—
lationship with each of the other three data sources. The four
data sources show different sample structure, and the source—
speciﬁc clusters are more well—distinguished than the overall
clusters in each plot. However, the overall clusters are clearly
represented to some degree in all four plots. Hence, the ﬂexible,
yet integrative, approach of BCC seems justiﬁed for these data.

Further details regarding the above analysis are given in
Section 6 of the Supplementary Material. These include the
prior speciﬁcations for the model, charts that illustrate mixing
over the MCMC draws, a comparison of the source—specific
clusterings Lmn to source—speciﬁc subtypes deﬁned by TCGA,
clustering heatmaps for each data source and short—term survival
curves for each overall cluster.

4 DISCUSSION

This work was motivated by the perceived need for a general,
ﬂexible and computationally scalable approach to clustering
multisource biomedical data. We propose BCC, which models
both an overall clustering and a clustering speciﬁc to each data
source. We view BCC as a form of consensus clustering, with
advantages over traditional methods in terms of modeling uncer—
tainty and the ability to borrow information across sources.

The BCC model assumes a simple and general dependence
between data sources. When an overall clustering is not
sought, or when such a clustering does not make sense as an
assumption, a more general model of cluster dependence (such
as MDI) may be more appropriate. Furthermore, a context—spe—
ciﬁc approach may be necessary when more is known about the
underlying dependence of the data. For example, Nguyen and
Gelfand (2011) exploit functional covariance models for time—
course data to determine overall and time—specific clusters.

Our implementation of BCC assumes the data are normally
distributed with cluster—speciﬁc mean and variance parameters. It
is straightforward to extend this approach to more complex clus—
tering models. In particular, models that assume clusters exist on
a sparse feature set (Tadesse et al., 2005) or allow for more gen—
eral covariance structure (Ghahramani and Beal, 1999) are grow—
ing in popularity.

While we focus on multisource biomedical data, the applica—
tions of BCC are potentially widespread. In addition to multi—
source data, BCC may be used to compare clusterings from
different statistical models for a single homogeneous dataset.

 

2615

ﬁre'spzumoiproyo'sopauuopnorq/ﬁdnq

E.F.Lock and D.B.Dunson

 

GE (oc=0.91i0.06)

‘IO 20 30

PC2

—‘IO 0

 

-30

PC 1
miRNA (oc=0.56i0.06)

10

PCZ

 

—15

PC1

ME (ot=0.69i0.06)

m 0
N * :5"
*ate at are at at '- "1.". -
v are t. *6 are at... o . f. °
8  *He * 1* _H+ f. )8"...
Ci. 0 ¢ ' o .
eral: % git ‘ . .
T ** * ﬁ 'I‘I‘. o
N +4411“ 11- °
I +
—6 —4 —2 o 2 4
PC1
RPPA (0i=0.7i0.06)
LO
8 o
D.

 

PC1

Fig. 3. PCA plots for each data source. Sample points are colored by overall cluster; cluster 1 is black. cluster 2 is red and cluster 3 is blue. Symbols
indicate source—specific cluster; cluster 1 is indicated by filled circles. cluster 2 is indicated by plus signs and cluster 3 is indicated by asterisks

Funding: National Institute of Environmental Health Sciences
(NIEHS) (R01-ES017436).

Conflict of'Inleresl: none declared.

REFERENCES

Cancer Genome Atlas Network. (2012) Comprel‘tensive molecular portraits of
human breast tumours. Nature. 490. 61770.

Cleveland.W.S. (1979) Robust locally weighted regression and smoothing scatter-
plots. J. Am. Stat. Assoc.. 74. 8297836.

Cartis.C. et al. (2012) The genomic and transcriptomic arcl‘titecture of2.000 breast
tumours reveals novel subgroups. Nature. 486. 3467352.

Dahl.D. (2006) .‘I/Iodel-Based Clusteringvfor Expr ‘sion Data via a Diric/ilet Process
.‘I/Iixture .‘I/Iodel. Cambridge University Press. Cambridge. UK.

Duan.Q. et al. (2013) Metasignatures identify two major subtypes ofbreast canoer.
CPl' P/iarnuicom. Sv‘st. P/iarmacol.. 3. e35.

Fritscl‘1.A. and Ickstadt.K. (2009) Improved criteria for clustering based on the
posterior similarity matrix. Bayesian Anal.. 4. 3677391.

Gl‘tal‘tramaniZ. and Beal.M.J. (1999) Variational inference for bayesian mixtures of
factor analys 1‘. In: Solla.S.A. et al. (ed.) Advances in Neural Information
Processing Srstems [2, [NIPS Conference, Denver, Colorado, USA, .‘VDI'L‘IH/M‘l’
297Decem/7er 4, [999]. The MIT Press. Cambridge. MA. USA. pp. 4497455.

Hubert.L. and Arabie.P. (1985) Comparing partitions. ./. Classif.. 2. 1937218.

Jonsson.G. et al. (2010) Genomic subtypes of breast cancer identified by array-
comparative genomic hybridization display distinct molecular and clinical char-
acteristics. Breast Cancer Res. 12. R42.

Kirk.P. et al. (2012) Bayesian correlated clustering to integrate multiple datasets.
Bioinfornuitics. 28. 329073297.

Kornmksson.M. et al. (2012) Integrative model-based clustering of microarray
methylation and expression data. Ann. Appl. Stat.. 6. 132771347.

Lock.E. et al. (2013) Joint and Individual Variation Explained (JIVE) for integrated
analysis of multiple data types. Ann. Appl. Stat.. 7. 5237542.

Lofstedt.T. and TryggJ. (2011) Onplsa novel multiblock method for the modelling
of predictive and ortl‘togonal variation. ./. C/iemom.. 25. 441455.

 

 

Miller.J.W. and Harrison.M.T. (2013) A simple example ofdirichlet prooess mixture
inconsistency for the number of components. arXir preprint arXiv:1301.2708.

Mo.Q. et al. (2013) Pattern discovery and cancer gene identiﬁcation in integrated
cancer genomic data. Proc. Natl Acad. Sci. USA. 110. 42454250.

Nguyen.N. and Caraana.R. (2007) Consensus clusterings. In: Proceedings of the 7d:
IEEE International Conference on Data .‘I/Iining (lCDM 2007/, October 28-31,
2007, Oma/ia, .‘V'e/traska, L'SA. pages 6077612. IEEE Computer Society.

Nguyen.X. and Gelfand.A.E. (2011) The Dirichlet labeling process for clustering
functional data. Stat. Sin.. 2]. 124971289.

R Development Core Team. (2012) R: A Language and EnvironmentVfor Statistical
Computing. R Foundation for Statistical Computing. Vienna. Austria. ISBN 3-
900051-07-0.

Ray.P. et al. (2012) Bayesian joint analysis of heterogeneous data. Preprint.

Rey.M. and Roth.V. (2012) Copula mixture model for dependency-seeking cluster-
ing. In: LanglordJ. and PineauJ. (eds) Proceedings of the 29th Interimtional
Conference on .‘I/Iac/iine Learning (ICML- [2). [CM L‘ 12. p. 9277934. New York.
NY. Omnipress.

Rogers.S. et al. (2008) Investigating the correspondence between transcriptomic and
proteomic expression profiles using coupled cluster models. Bioinfornuitics. 24.
289472900.

Savage.R.S. et al. (2010) Discovering transcriptional modules by bayesian data
integration. Bioinfornuitics. 26. “584167.

Savage.R.S. et al. (2013) Identifying cancer subtypes in glioblastoma by combining
genomic. transcriptomic and epigenomic data. arXir preprint arXiv:1304.3577.

Shen.R. et al. (2009) Integrative clustering of multiple genomic data types using a
joint latent variable model with application to breast and lung cancer subtype
analysis. Bioinfornuitics. 25. 290672912.

Tadesse.M.G. et al. (2005) Bayesian variable selection in clustering l‘tigh-dimen-
sional data. J. Am. Stat. .r/tssocu 100. 6024117.

Wang.H. etal. (2011) Bayesian cluster ensembles. Stat. Anal. Data .‘I/lining. 4. 54470.

Wang.P. et al. (2010) Nonparametric bayesian clustering ensembles. In: .‘I/Iac/iine
Learning and Knowledge Discorte in Data/mses. Springer. Berlin - Heidelberg.
pp. 435450.

Yuan.Y. et al. (201 l) Patient-specific data fusion defines prognostic cancer subtypes.
PLoS Comput. Biol. 7. el00222 .

Zhou.G. et al. (2012) Common and individual features analysis: beyond canonical
correlation analysis. Ar,\'ir preprint arXiv:1212.3913.

 

2616

/3.IO'S[EIIm0fp.IO_IXO'SOIJEIIIJOJIIIOIq/ﬂduq

