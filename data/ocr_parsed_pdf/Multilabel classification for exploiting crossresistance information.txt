BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Multilabel classification

 

machine learning methods (decision trees, artiﬁcial neural net—
works, support—vector machines, least—squares regression and
least angle regression) to predict drug resistance in HIV—1 for
16 drugs, including protease— and RT inhibitors. Kierczak
et al. (2009) developed a rough set—based model considering phy—
sico—chemical changes of mutated sequences compared with the
wild—type strain to predict RT inhibitor resistance in HIV—1.
Moreover, the same group developed the first systems biology
approaches to HIV—l drug resistance, showing networks of inter—
acting positions (Kierczak et al., 2010).

One important finding, however, has not been exploited in
these models so far, namely the occurrence of cross—resistance.
In the context of HIV—1, cross—resistance means that mutations
leading to a resistance against a speciﬁc drug, which is currently
part of the antiretroviral treatment of a speciﬁc patient, also
leads to resistance (In some cases, albeit less frequently, it
could also lead to a re—sensitization for other drugs.) against
other drugs (that may or may not be part of the same treatment).
In the current study, we analyzed cross—resistance in HIV—1 for a
dataset of more than 600 RT sequences and six nucleoside ana—
logues (NAs), namely Lamivudine (3TC), Abacavir (ABC),
Zidovudine (AZT), Stavudine (d4T), Didanosine (ddI) and
Tenofovir (TDF). Moreover, we developed a model that exploits
knowledge about cross—resistance to improve the overall predic—
tion accuracy for the whole repertoire of drugs used in this study.
To this end, we made use of novel methods for so—called multi—
label claSSlﬁcation (MLC), a generalization of conventional
(polychotomous) classiﬁcation that has recently gained increas—
ing attention in machine learning (Tsoumakas and Katakis,
2007).

To the best of our knowledge, this is the ﬁrst time information
about RT inhibitors cross—resistance has been explicitly inte—
grated in HIV—1 drug resistance prediction models. In contrast
to protease inhibitors (PIs) as well as non—nucleoside reverse
transcriptase inhibitors (NNRTIs), NAs have less side effects
(Sturmer et al., 2007). Moreover, the combination of different
drug classes during therapy may lead to unpredictable inter—
actions of PIs and NNRTIs with the cytochrome P450 system
and thus may complicate the therapy. Therefore, one option
might be the use of quadruple nucleoside therapy (Stiirmer
et al., 2007).

2 METHODS
2.1 Data

For our analyses, we used a publicly available dataset consisting of RT
sequences of HIV-1 with corresponding resistance factors (ICSO ratios) for
six NAs, namely 3TC, ABC, AZT, d4T, ddI and TDF (Rhee et al., 2006).
A summary of the dataset is shown in Table 1. For our method, we
needed RT sequences for which IC50 ratios for all mentioned drugs are
available, so we discarded the information about TDF (owing to the low
number of sequences) and merged the other sequences and IC50 ratios of
the remaining drugs. Strains lacking phenotypic results for any drug
analyzed in the current study were discarded before analysis.
Eventually, this yields 614 RT sequences with IC50 ratios for 3TC,
ABC, AZT, d4T and ddI. The class ratio (positive samples / negative
samples) for all drugs ranges between 0.83 and 2.39. All sequences origi-
nated from subtype B strains.

Table 1. Data overview

 

 

Drugs Number of IC50 ratio Class
sequences cutoff ratioa
3TC 629 z 3 2.18
ABC 624 z 2 2.39
AZT 626 z 3 0.91
ddI 628 z .5 1.03
d4T 626 > 1.5 0.83

 

"Ratio of resismnt versus susceptible sequences.

2.2 Predictive modeling

The goal of our study was to build models that can be used to predict
whether there are resistance mutations within an RT sequence of a spe-
ciﬁc virus for different NAs. Thus, we used drug-speciﬁc cutoffs for the
IC50 ratios to separate the sequences into the classes ‘resistant’ and ‘sus-
ceptible’ for the different drugs. For 3TC and AZT, the cutoff was set to
3, for d4T and ddI it was set to 1.5 and for ABC it was set to 2. This
means that sequences having a corresponding ICSO ratio above or equal to
the cutoff are deﬁned as ‘resistant’ (Table 1), whereas sequences having an
IC50 ratio below the cutoff are deﬁned as ‘susceptible’. For instance, an
RT sequence with a ratio of 10.3 for 3TC is deﬁned as ‘resistant’, whereas
another RT sequence with a ratio of 2.5 is deﬁned as ‘susceptible’.

In some studies, e.g. Rhee et a]. (2006), a third class was deﬁned as
‘intermediate resistant’. Nevertheless, since ‘intermediate resistant’ strains
are somehow resistant, too, we simply subsumed those sequences under
the ‘resistant’ category. The resulting classes are rather balanced: for each
drug (except for 3TC and ABC), ~50% of the RT sequences belong to the
class ‘resistant’ and 50% to the class ‘susceptible’. Concretely, the fraction
of RT sequences categorized as ‘resistant’ are, respectively, 50.81%,
47.72%, 45.44%, 68.57% and 70.52% for ddI, AZT, d4T, 3TC and ABC.

As already demonstrated in several protein classiﬁcation studies, e.g.
(Chowriappa et al., 2008; Dybowski et al., 2010; Heider et al., 2009,
2010), the use of physico-chemical properties and especially the use of
hydrophobicity characteristics (Kyte and Doolittle, 1982), lead to good
prediction results. Thus, we encoded the RT protein sequences into
hydrophobicity vectors and normalized them to length 240, which repre-
sents the average sequence length in the dataset, using Interpol (Heider
and Hoffmann, 2011). Thus, we eventually end up with a dataset consist-
ing of 614 instances (RT sequences), each of which is characterized in
terms of a normalized hydrophobicity vector of length 240. Moreover,
each instance is associated with five binary class labels, namely resistance
for ddI, AZT, d4T, 3TC and ABC. Our goal, now, is to train a model
that generalizes beyond these examples, i.e., that allows for accurately
predicting each of the ﬁve outputs on the basis of any normalized hydro-
phobicity vector given as input information.

2.3 Multilabel classiﬁcation

The above problem obviously falls in the realm of (binary) classification,
a well-established and thoroughly explored subﬁeld of statistics and ma-
chine learning. In fact, the arguably most simple way to solve it is to train
one binary classiﬁer for each of the ﬁve outputs, thereby splitting the
original multioutput problem into ﬁve single-output problems. Each of
these problems can then be solved individually, using the large repertoire
of existing methods for binary classiﬁcation.

This approach has an important disadvantage, however. It cannot take
any advantage of possible dependencies between the different outputs.
Modeling and exploiting such dependencies to improve prediction accur-
acy is one of the key goals of MLC. Intuitively, if the value of one output
may (statistically) depend on the value of others, then predicting all

 

1947

ﬁm'spzumoﬁuqxo'sopeuuopnorq/ﬁdnq

D.Heider et al.

 

outputs simultaneously should indeed be better than predicting them
separately. This is the main argument against simple decomposition tech-
niques like the one proposed above, called binary relevance (BR) learning
in the context of MLC.

More formally, let ﬂ = {A1, ...,Am} be a ﬁnite set of class labels (in
our case the resistance for the ﬁve drugs), and let X be an instance space
(in our case the 240-dimensional hydrophobicity vectors). An MLC task
assumes a training set S = {(x1,y1), ..., (x,,,y,,)}, generated independ-
ently and identically according to a probability distribution P(X, Y) on
X X 31. Here, 31 is the set of possible label combinations, i.e., the power
set of £. To ease notation, we deﬁne a label combination y as a binary
vector y = (yl, yz, ..., ym), in which yj = 1 indicates the presence (rele-
vance) and yj = 0 the absence (irrelevance) of M. Under this convention,
the output space is given by 31 = {0, l}'". The goal in MLC is to induce
from S a model h : X —> 31 that correctly predicts the subset of relevant
labels for unlabeled query instances x e X. Recall that, in our context,
‘relevance of a label’ stands for ‘resistance against a drug’; thus, a pre-
diction y = (l, 0, l, 0, 0) would suggest that the instance (RT sequence) at
hand is resistant against the ﬁrst and the third drug while being suscep-
tible to the others.

2.3 .1 Performance metrics The prediction of label subsets (vectors)
instead of single labels suggests different types of performance metrics for
MLC. Commonly used examples of such metrics, which also seem to be
meaningful in the context of our application, include the Hamming loss,
the subset 0/1 loss and the F-measure. Let X,m = {x1 , . . . , xN} be a set of
test instances. Moreover, let y,», = (ym, . . . , ym) e 31 be the labeling of
test instance x,», where y” is the value of label M for x,», and denote by
37,». 2 (fm, . . . , pm) the corresponding prediction produced by the clas-
siﬁer. The Hamming loss of the prediction 37,», is then deﬁned as the
fraction of labels whose relevance is incorrectly predicted (For a predicate
P, the expression |[P]| evaluates to 1 if P is true and to 0 if P is false.)

A 1 m A
Lﬂcvmyi.) = Q 2 [[m 7A yi.,-]] e [0, 1] (1)
1:1
The subset 0/1 loss simply checks whether the complete label subset is
predicted correctly or not:

1150;135:1713) 2 [ya 75323:” 5 {0,1} (2)

The F-measure essentially corresponds to the harmonic mean of the
precision and recall of the prediction; it is deﬁned as follows:

m A
2 Z yi. jyi. j

A ':1
Lptvivyi.) = ’—,,. e [0,1], (3)
yr,‘ + Zyij
1 1'21

5

1

where 0/0 = l by deﬁnition. The above metrics are used to evaluate the
prediction 37,». for an individual instance xi, i.e., they are computed in-
stance-wise. Correspondingly, in an experimental study, the average ac-
curacy would be reported as the average over all instances x,» in the test
data XW.

Apart from that, another option is of course to report accuracy in a
label-wise manner, namely to compute standard performance metrics
such as classiﬁcation rate (percentage of correct predictions) or AUC
(area under the receiverwperating characteristic curve) separately for
each label A,» 6 ﬂ. Note that some metrics can be computed instance-
wise as well as label-wise. For example, the label-wise version of the
F-measure is given by

N

2 EyiJJA/Lj
Livy-jg) = [VF—N 6 [0,11, (4)
gym + gym

where y.j = (le, . . . ,yNJ) is the vector of values for the label M and 37.]-
the corresponding vector of predictions.

2.4 Classiﬁer chains

Until now, several methods for MLC have been proposed in the litera-
ture. Here, we shall focus on a method called classifier chains (CCs; Read
et al., 2011), which, despite having been introduced only lately, already
enjoys great popularity. This is arguably due to the fact that it is based on
a simple and elegant yet effective idea for capturing label dependencies.
The CCs method learns m binary classiﬁers (each one dealing with the
BR problem associated with one label) linked along a chain, each time
extending the feature space by all previous labels in the chain. For in-
stance, if the chain follows the order A1 —> A2 —>  —> Am, then the
classiﬁer hf responsible for predicting the relevance of M is of the form

hf: X>< {0,1y"1 —> {0,1}. (5)

The training data for this classiﬁer consists of (expanded) instances
(xi,y,;1, . . . , yl; F1) labeled with y”, that is, original training instances x,»
supplemented by the relevance of the labels A], . . . ,Aj,1 preceding M in
the chain. Thus, the classiﬁer hf supposed to predict the label of class A]-
makes use of the preceding labels as additional input information, thereby
capturing possible dependencies between the labels. Theoretically, the CC
approach can be motivated by the product rule of probability
(Dembczynski et al., 2010):

P(ylx)=]_[P(nlx,y1, ...,yH) (6)
k2]

Note that, for training the classiﬁer (5), any standard method for
binary classiﬁcation can be used (logistic regression, decision trees, sup-
port vector machines, etc.).

At prediction time, when a new instance x needs to be labeled, a label
vector y 2 (J71, . . . , pm) is produced by successively querying each classi-
ﬁer hf. Note, however, that the inputs of these classiﬁers are not well-
deﬁned, because the supplementary attributes yl; 1 , . . . , yl; 1;] are not
available. These missing values are therefore replaced by their respective
predictions: y 1 used by hz as an additional input is replaced by y] = in (x),
yz used by h3 as an additional input is replaced by y; = h2(x,y1) and so
forth. Thus, the prediction y is of the form

.V = U11“), h2(x9hl(x))3 h3(x9hl(x)3h2(xahl(x)))3 ---)

The process of training a CC and using it for prediction is illustrated in
Figure l.

2.5 Ensembles 0f CCs

Realizing that the order of labels in the chain may inﬂuence the perform-
ance of the classiﬁer, and that an optimal order is hard to anticipate,
Read et al. (2011) propose the use of an ensemble of CC classiﬁers. This
approach combines the predictions of different random orders and,
moreover, uses a different sample of the training data to train each
member of the ensemble. Ensembles 0f classifier chains (ECC) have
been shown to increase prediction performance over CC by effectively
using a simple voting scheme to aggregate predicted relevance sets of the
individual chains. For each label A], relevance is predicted by thresholding
the proportion WJ- of classiﬁers predicting yj = l at a level I, i.e.,

)71' =  Z l]].

3 RESULTS AND DISCUSSION

The major goal of our experimental study was to provide empir—
ical evidence for the conjecture that capturing statistical depen—
dencies between HIV—1 drugs is instrumental in learning
classiﬁers for resistance prediction. Dependencies of that type

 

ﬁm'spzumol‘pmyo'sopeuuopnorq/ﬁdnq

 1' r:-Iwil'n'yr'J-’
 "Ill—"1W " l

I I I! 1.!I71::i.
.  I I IIjIIeliiJ
.  Jill-IIIIIIILJ
 .IIIIIIl-IIITI

I-i' mu “Ii-1|

ll’lijij

..r' I ' 1. IJII'" 
D.

 

/310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

D.Heider et al.

 

 

_ 3TC

15 20 25
|

10
I

 

0 50 100 150 200

 

ABC

25
I

20
I

10
I

 

0 50 100 150 200

 

AZT

15 20 25
I I

10

 

0 50 100 150 200

 

D4T

15 20 25
I I

10

 

0 50 100 150 200

 

DDI

15 20 25
I I

10

 

 

 

I I | | I
0 50 100 150 200

Fig. 2. Importance analyses from single classiﬁers. On the x-axis the sequence positions are shown, whereas the y-axis represents the sum of all decreases
in Gini impurity. Feature importance for ﬁve single random forests was assessed using the sum of all decreases in Gini impurity, which has been shown to
be more robust compared with the mean decrease in accuracy (Calle and Urrea, 2010)

 

1950

ﬁm'spzumol‘pmyo'sopeuuopnorq/ﬁdnq

Multilabel classification

 

Table 3. Average classification rate of logistic regression models trained
on different input information (original and supplemented)

Table 6. Performance of BR, CC and ECC in terms of instance-wise
metrics (mean :1: standard deviation), in brackets the rank

 

 

 

Input 3TC ABC AZT d4T ddl Types Hamming loss Subset 0/1 F-measure

x 0.821 0.764 0.689 0.702 0.667 BR 0.2159 :1: 0.0298 (3) 0.5775 :1: 0.0476 (3) 0.7455 :1: 0.0366 (3)
x + 3TC 7 0.766 0.696 0.701 0.689 CC 0.2129 :1: 0.0459 (2) 0.5098 :1: 0.0514 (2) 0.7631 :1: 0.0459 (2)
x + ABC 0.833 7 0.698 0.758 0.675 ECC 0.1947 :1: 0.0255 (1) 0.4961 :1: 0.0476 (1) 0.7787 :1: 0.0344 (1)
x + AZT 0.816 0.769 7 0.725 0.667

x + d4T 0.815 0.797 0.742 7 0.694 Note: Random forests (of size 16) were used as base learner.

x + ddl 0.852 0.776 0.711 0.735 7

 

Note: The numbers are determined through 10—fold cross—validation repeated ﬁve
times. The best result per label is highlighted in bold font.

Table 4. Performance of BR, CC and ECC in terms of instance-wise
metrics (mean :1: standard deviation), in brackets the rank

 

Types Hamming loss Subset 0/1 F -measure

 

BR 0.2695 :1: 0.0235 (2)
CC 0.2697 :1: 0.0266 (3)
ECC 0.2384 :1: 0.0538 (1)

0.6905 :1: 0.0519 (3)
0.6904 :1: 0.0528 (2)
0.6312 :1: 0.0538(1)

0.6741 :1: 0.0420 (3)
0.6788 :1: 0.0417 (2)
0.7166 :1: 0.0366(1)

 

Note: Logistic regression was used as base learner.

Table 5. Performance of BR (top), CC (middle) and ECC (bottom) in
terms of label-wise metrics (mean:1: standard deviation), in brackets the
rank

 

 

Dmgs Classiﬁcation rate AUC F-measure

3TC 0.8192 :1: 0.0512 (2) 0.8394 :1: 0.0638 (2) 0.8623 :1: 0.0441 (2)
ABC 0.7524 :1: 0.0543 (3) 0.7551 :1: 0.0613 (3) 0.8176 :1: 0.0478 (3)
AZT 0.6960 :1: 0.0590 (3) 0.6963 :1: 0.0631 (3) 0.6819 :1: 0.0534 (3)
d4T 0.7004 :1: 0.0483 (3) 0.7119 :1: 0.0621 (2) 0.6613 :1: 0.0748 (3)
ddl 0.6846 :1: 0.0657 (2) 0.6779 :1: 0.0888 (3) 0.6820 :1: 0.0769 (2)
3TC 0.8192 :1: 0.0512 (2) 0.8394 :1: 0.0638 (2) 0.8623 :1: 0.0441 (2)
ABC 0.7584 :1: 0.0538 (2) 0.7602 :1: 0.0592 (2) 0.8211 :1: 0.0469 (2)
AZT 0.7004 :1: 0.0578 (2) 0.7025 :1: 0.0612 (2) 0.6837 :1: 0.0574 (2)
d4T 0.7021 :1: 0.0616 (2) 0.7107 :1: 0.0650 (3) 0.6665 :1: 0.0790 (2)
ddl 0.6716 :1: 0.0450 (3) 0.6819 :1: 0.0620 (2) 0.6701 :1: 0.0525 (3)
3TC 0.8403 :1: 0.0548(1) 0.9119 :1: 0.0472(1) 0.8814 :1: 0.0425(1)
ABC 0.7980 :1: 0.0440 (1) 0.8566 :1: 0.0390(1) 0.8541 :1: 0.0375 (1)
AZT 0.7488 :1: 0.0565 (1) 0.8282 :1: 0.0543 (1) 0.7378 :1: 0.0549 (1)
d4T 0.7211 :1:0.0515 (l) 0.8115:1:0.0506 (1) 0.6874:1:0.0683 (1)
ddl 0.6999 :1: 0.0569 (1) 0.7761 :1: 0.0576(1) 0.7058 :1: 0.0514(1)

 

Note: Logistic regression was used as base learner.

3.1 The effect of chaining

To analyze the practical usefulness of classiﬁer chaining, we
compared the prediction accuracy of the following methods:

0 BR: A single binary classiﬁer is trained independently for

each of the five labels.

0 CCs: The five classiﬁers are trained according to the CC
approach outlined in Section 2.4. The chain was constructed

Table 7. Performance of BR (top), CC (middle) and ECC (bottom) in
terms of label-wise metrics (mean:1: standard deviation), in brackets the

 

 

rank

Drugs Classiﬁcation rate AUC F -measure

3TC 0.8289 :1: 0.0515 (2) 0.8910 :1: 0.0520 (2) 0.8815 :1: 0.0403 (2)
ABC 0.8235 :1: 0.0473 (3) 0.8575 :1: 0.0530 (3) 0.8828 :1: 0.0348 (3)
AZT 0.7852 :1: 0.0582 (2) 0.8800 :1: 0.0446 (3) 0.7827 :1: 0.0589 (3)
d4T 0.7655 :1: 0.0451 (3) 0.8603 :1: 0.0351 (3) 0.7417 :1: 0.0582 (3)
ddl 0.7177 :1: 0.0634 (2) 0.7962 :1: 0.0483 (3) 0.7292 :1: 0.0618 (2)
3TC 0.8289 :1: 0.0515 (2) 0.8910 :1: 0.0520 (2) 0.8815 :1: 0.0403 (2)
ABC 0.8295 :1: 0.0473 (2) 0.8705 :1: 0.0484 (2) 0.8861 :1: 0.0348 (2)
AZT 0.7965 :1: 0.0630 (2) 0.8726 :1: 0.0527 (3) 0.7928 :1: 0.0626 (2)
d4T 0.7721 :1: 0.0598 (2) 0.8668 :1: 0.0454 (2) 0.7603 :1: 0.0652 (2)
ddl 0.7085 :1: 0.0497 (3) 0.7922 :1: 0.0525 (3) 0.7373 :1: 0.0464 (2)
3TC 0.8392 :1: 0.0493 (1) 0.8942 :1: 0.0439 (1) 0.8905 :1: 0.0386 (1)
ABC 0.8404 :1: 0.0375 (1) 0.8801 :1: 0.0462 (1) 0.8943 :1: 0.0281 (1)
AZT 0.8144 :1: 0.0420(1) 0.8976 :1: 0.0361 (1) 0.8125 :1: 0.0447 (1)
d4T 0.7970 :1: 0.0503 (1) 0.8901 :1: 0.0341 (1) 0.7831 :1: 0.0590 (1)
ddl 0.7357 :1: 0.0392(1) 0.8215 :1: 0.0394 (1) 0.7573 :1: 0.0420 (1)

 

Note: Random forests (of size 16) were used as base learner.

by sorting the labels in decreasing order according to their
individual (BR) prediction accuracy (This is a commonly
used rule of thumb, which is motivated by the observation
that mistakes of a single classiﬁer tend to be propagated
along the rest of the chain (Senge et al., 2013); consequently,
strong classiﬁers should be placed at the top and poor ones
more toward the end of the chain.):

3TC 7 ABC 7 AZT 7 d4T 7 ddI

o ECCs: The ECC method described in Section 2.5 was im—
plemented with 10 chains, each time choosing the order of
labels at random. The threshold t was taken as 1/2.

All methods were implemented with standard logistic regres—
sion as a base learner. Prediction performance was measured in
terms of the Hamming loss (i), the subset 0/1 loss (ii) and the
F—measure (iii) as instance—wise metrics, and the classiﬁcation
rate, the AUC and the F—measure (iv) as label—wise metrics.
Each of these metrics was estimated by means of a 10—fold
cross—validation repeated ﬁve times, and results are reported in
terms of the mean values and the standard deviations. Moreover,
we also indicate the ranking of the three methods, with the best
performing method on rank 1 and the worst performing method
on rank 3.

 

1951

ﬁm'spzumol‘pmyo'sopeuuopnorq/ﬁdnq

D.Heider et al.

 

The results for the instance—wise metrics are summarized in
Table 4, those for the label—wise metrics in Table 5. Although
the differences are not always statistically signiﬁcant, as can be
seen from the standard deviations, the overall picture is clear and
obviously in favor of the chaining methods. In fact, chaining
achieves systematic (albeit sometimes small) gains in comparison
with standard BR learning. Among the two chaining methods,
ECC performs even stronger than CC and typically yields the
best results.

To make sure that the results are not too much inﬂuenced by
the underlying base learner used by all methods, we repeated the
same experiments with random forests (Breiman, 2001) instead
of logistic regression. These two learners exhibit different proper—
ties. In particular, while logistic regression fits a linear decision
boundary in the instance space, decision trees are much more
ﬂexible and able to model highly non—linear concepts; this ﬂexi—
bility is even increased by the aggregation of different trees in the
random forest approach. Thus, it comes at no surprise that the
performance of all methods is in general improved. Nevertheless,
in terms of relative comparison, the picture is more or less iden—
tical to the ﬁrst experiment with logistic regression. Both chain—
ing methods improve on BR, with ECC being even better than
CC (Tables 6 and 7).

4 CONCLUSON

We conclude with an affirmative answer to one of the main
questions of our study, namely whether or not cross—resistance
information can be used to improve overall accuracy in drug
resistance prediction. By using MLC methods, a relatively
recent development in machine learning, we were able to exploit
cross—resistance information for RT inhibitors. More concretely,
our results are based on a speciﬁc MLC method called CCs.

We consider these results as promising and, therefore, intend
to further explore this direction in future work. On the methodo—
logical side, we would like to try alternative MLC methods,
including the probabilistic variant of CCs proposed by
Dembczynski et al. (2010) but also approaches that are not
based on the idea of chaining. As an interesting property of
the former, let us mention that it does not only produce binary
predictions, but proper probability estimates of single labels or
label combinations. Predictions of that kind are interesting, not
only for the minimization of various loss functions, but also for
the purpose of representing uncertainty. Moreover, we want to
include multiclass and regression models to be able to predict
more classes, e. g. intermediate resistance, and even the resistance
factors.

On the application side, our study has focused on NAs so far,
although a typical clinical treatment includes drugs from several
classes. It might of course be interesting to test our approach for
other types of antiretroviral drugs, for example, non—nucleoside
RT inhibitors, and for other target proteins, such as HIV—1 pro—
tease and corresponding PIs. By now, our approach is limited to
specialized treatment cases and thus is currently not well applic—
able in clinical settings. However, in the future we plan to adapt
our approach for NNRTIs as well as for PIs. Moreover, all se—
quences used in the current study originated from subtype B

strains, thus the results of our model might be misleading if it
is applied to other subtypes.

Funding: This work was partially supported by the Center for
Synthetic Microbiology (SYNMIKRO), Marburg, Germany.

Conﬂict of Interest: none declared.

REFERENCES

Antinori,A. et al. (2006) Antiviral efﬁcacy and genotypic resistance patterns of
combination therapy with Stavudine/tenofovir in highly active antiretroviral
therapy experienced patients. Antivir. Ther., 11, 2337243.

Breiman,L. (2001) Random forests. Mach. Learn, 45, 5732.

Calle,M.L. and Urrea,V. (2010) Letter to the editor: stability of random forest
importance measures. Brief. Bioinform., 12, 86789.

Chowriappa,P. et al. (2008) Protein structure classiﬁcation based on
conserved hydrophobic residues. IEEE/ACM Trans. Comput. Biol. Bioinform.,
6, 639751.

Dembczyr'iski,K. et al. (2010) Bayes optimal multilabel classiﬁcation via probabil—
istic classiﬁer chains. In ICML, pages 2797286.

Dybowski,J.N. et al. (2010) Prediction of co—receptor usage of HIV—1 from geno—
type. PLoS Comput. Biol., 6, e1000743.

Garcia—Lerma,J.G. et al. (2003) A novel genetic pathway of human immunodeﬁ—
ciency virus type 1 resistance to stavudine mediated by the K65R mutation.
J. Virol., 77, 568575693.

Gouy,M. et al. (2010) SeaView version 4: A multiplatform graphical user interface
for sequence alignment and phylogenetic tree building. Mol. Biol. Evol, 27,
2217224.

Heider,D. and Hoffmann,D. (2011) Interpol: an R package for preprocessing of
protein sequences. BioData Min., 4, 16.

Heider,D. et al. (2009) A computational approach for the identiﬁcation of small
GTPases based on preprocessed amino acid sequences. Techno]. Cancer Res.
Treat, 8, 3337342.

Heider,D. et al. (2010) Predicting Bevirimat resistance of HIV—1 from genotype.
BMC Bioinformatics, ll, 37.

Johnson,V.A. et al. (2011) 2011 update of the drug resistance mutations in HIV—1.
Top. Antivir. Merl, l9, 15(7164.

Kierczak,M. et al. (2009) A rough set—based model of HIV—1 reverse transcriptase
resistome. Bioinform. Biol. Insights, 3, 1097127.

Kierczak,M. et al. (2010) Computational analysis of molecular interaction networks
underlying change of HIV—1 resistance to selected reverse transcriptase inhibi—
tors. Bioinform. Biol. Insights, 4, 1377146.

Kyte,J. and Doolittle,R. (1982) A simple method for displaying the hydropathic
character of a protein. J. Mol. Biol., 157, 1057132.

Lafeuillade,A. and Tardy,J.C. (2003) Stavudine in the face of cross—resistance be—
tween HIV—l nucleoside reverse transcriptase inhibitors: a review. AIDS Rev., 5,
8786.

Pennings,P.S. (2012) Smnding genetic variation and the evolution of drug resistance
in HIV. PLoS Comput. Biol., 8, e1002527.

Read,J. et al. (2011) Classiﬁer chains for multi—label classiﬁcation. Mach. Learn, 85,
3337359.

Rhee,S.Y. et al. (2006) Genotypic predictors of human immunodeﬁciency virus type
1 drug resistance. Proc. Natl. Acad. Sci. USA, 103, 17355717360.

Senge,R. et al. (2013) On the problem of error propagation in classier chains for
multi—label classiﬁcation. In: Schmidt—Thieme,L. and Spiliopoulou,M. (eds)
Data Analysis, Machine Learning and Knowledge Discovery. Proceedings“ of the
36th Annual Conference of the German Classification Society. Springer,
Hildesheim, Germany.

Sirivichayakul,S. et al. (2003) Nucleoside analogue mutations and Q151M in HIV—1
subtype A/E infection treated with nucleoside reverse transcriptase inhibitors.
AIDS, 17, 188971896.

Stiirmer,M. et al. (2007) Quadruple nucleoside therapy with Zidovudine, lamivudine,
abacavir and tenofovir in the treatment of HIV. Antivir. Ther., 12, 6957703.

Tripathi,K. et al. (2012) Stochastic simulations suggest that HIV—1 survives close to
its error threshold. PLoS Comput. Biol., 8, e1002684.

Tsoumakas,G. and Katakis,I. (2007) Multi label classiﬁcation: an overview. Int. J.
Data Warehouse M in., 3, 1713.

 

1952

ﬁm'spzumol‘pmyo'sopeuuoptrorq/ﬁdnq

