ORIGINAL PAPER

Vol. 27 no. 7 2011, pages 994-1000
doi:10. 1 093/bioinformatics/btr070

 

Data and text mining

Advance Access publication February 10, 2011

Network clustering: probing biological heterogeneity by sparse

graphical models
Sach Mukherjee1’2’* and Steven M. HiII1’2

1Department of Statistics and ZCentre for Complexity Science, University of Warwick, Coventry CV4 7AL, UK

Associate Editor: John Quackenbush

 

ABSTRACT

Motivation: Networks and pathways are important in describing the
collective biological function of molecular players such as genes or
proteins. In many areas of biology, for example in cancer studies,
available data may harbour undiscovered subtypes which differ in
terms of network phenotype. That is, samples may be heterogeneous
with respect to underlying molecular networks. This motivates a need
for unsupervised methods capable of discovering such subtypes and
elucidating the corresponding network structures.

Results: We exploit recent results in sparse graphical model learning
to put forward a ‘network clustering’ approach in which data are
partitioned into subsets that show evidence of underlying, subset-
level network structure. This allows us to simultaneously learn
subset-specific networks and corresponding subset membership
under challenging small-sample conditions. We illustrate this
approach on synthetic and proteomic data.

Availability: go.warwick.ac.uk/sachmukherjee/networkclustering
Contact: s.n.mukherjee@warwick.ac.uk

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on June 8, 2010 ; revised on February 2, 2011 ; accepted
on February 3, 2011

1 INTRODUCTION

Networks and pathways are important in understanding the
collective biological function of molecular components such as
genes or proteins. The importance of network-based thinking
in current biology has motivated a rich body of work in
bioinformatics on modelling approaches for biological networks,
including networks involved in gene regulation and protein
signalling (including Friedman, 2004; Husmeier, 2003; Mukherjee
and Speed, 2008; Sachs et (11., 2005; Schafer and Strimmer, 2005;
Segal et (11., 2003; Yip and Gerstein, 2009; Yu et (11., 2004; Zhu et (11.,
2007).

Most studies have focused on the case in which biochemical data
are treated as homogeneous with respect to an underlying biological
network structure. That is, the assumption is made that a single
network model is broadly appropriate for a given set of data. This is
a reasonable approach in many cases where it is Clear that biological
samples share overall phenotype, or where data can be ﬁltered into
roughly homogeneous groups using appropriate markers. However,
in many scenarios available data may contain subtypes which differ

 

*To whom correspondence should be addressed.

in terms of underlying network structure. A topical example comes
from cancer biology. Molecular studies have revealed a number
of subtypes of cancer (Perou et (11., 2000; S¢rlie et (11., 2001)
that have been found to differ markedly in terms of both biology
and response to treatment. The genomic aberrations harboured by
such subtypes may in turn be manifested in terms of differences in
gene regulatory or protein signalling networks. In settings where
subtypes have already been Characterized, this observation may
simply motivate stratiﬁcation by subtype in advance of network
modelling or supervised network-based approaches (e.g. Chuang
et (11., 2007). However, when such biological heterogeneity has
not been Characterized (1 priori this is not possible. Moreover, if
there are hitherto unknown subtypes which differ at the level of
network structure, discovering the subtypes and elucidating their
corresponding networks itself becomes an important biological goal.

This article develops an unsupervised analysis called network
clustering, which seeks to partition data that is heterogeneous in
the sense described above into subsets showing evidence of shared
underlying network structure. This is an approach which we believe
has broad utility. For example, in the study of human diseases it
is becoming Clear that traditional diagnostic Classiﬁcations may, in
many cases, understate underlying molecular heterogeneity, often
with therapeutic consequences. Indeed, in the cancer arena, the task
of discovering cancer subtypes and developing treatment regimes
that are subtype speciﬁc is emerging as one of crucial importance.
Of course, biological subtypes may differ dramatically at the level
of mean response, say in terms of gene or protein expression. In
such cases, existing Clustering methods are well suited to probing
heterogeneity (and have successfully done so in many studies) and
indeed a small number of key biomarkers may be sufﬁcient to
discover and discriminate subtypes. But when differences are truly
at the network or pathway level, conventional Clustering approaches
are likely to prove inadequate, for reasons we discuss below.

However, the network Clustering problem stated here is a
Challenging one. Network inference—from a single, homogeneous
dataset—itself remains a non-trivial task and the subject of
much ongoing work. The learning of network structure within an
unsupervised framework is doubly Challenging, because multiple
rounds of network inference are likely to be needed, often at small
sample sizes, in the same way that a standard Clustering method
typically involves multiple rounds of estimation on data subsets (or
‘soft’ equivalents thereof). We discuss below how we address these
concerns by using undirected graphical models and exploiting recent
work on 11-penalized inference.

The manner in which molecular players vary in concert, i.e. their
covariance, is central to the statistical description of the networks

 

994 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /3.Io's[Bruno[pJOJXO'sorwuiJOJurorqﬂ:duq 11101} papBOIH/lAOG

91oz ‘Og anﬁnv uo ::

Network clustering

 

they constitute. From a statistical perspective, a focus on networks
rather than individual genes/proteins corresponds to a shift from
a mean- to a covariance-centred point of view. Yet, widely used
Clustering methods either do not consider Cluster-speciﬁc covariance
(e.g. K—means, hierarchical Clustering) or do so via estimators
which are inapplicable or ill behaved under the conditions of
dimensionality and sample size typical of molecular applications
(conventional, ﬁll-covariance Gaussian mixture models). We
emphasize that this is not a general weakness of these powerﬁil and
widely used methods, but an important limitation in the network
context of interest here.

The remainder of this article is organized as follows. We ﬁrst
introduce notation and core ideas in the learning of undirected
graphical models and go on to describe the network Clustering
approach. We then present empirical results on ﬁilly synthetic data,
synthetic data combined with phospho-proteomic data pertaining to
cell signalling, and proteomic data obtained from cancer cell lines
that are part of the NCI60 panel. We Close with a discussion of some
of the ﬁner points and shortcomings of our work and point to some
directions for future research.

2 METHODS

2.1 Notation

Let X=[X1 ...XN], X, e R” represent a data matrix. comprising N data
vectors each containing measurements on [7 molecular players.

Clustering is a form of unsupervised learning in which observations are
partitioned into groups. called clusters. within which data points are related
in some sense. Letke {l,...,K}indexclusters. and C: {l,...,N}l—> {l,...,K}
be a cluster assignment function such that C(i) is the cluster to which the
i-th data point is assigned.

2.2 Gaussian Markov Random Fields

Graphical models (Jordan. 2004; Keller and Friedman. 2009; Lauritzen.
1996) are a class of statistical models in which a graph. comprising vertices
and linking edges. is used to describe probabilistic relationships between
random variables.

Markov Random Fields (MRFs) (Dempster. 1972; Rue and Held. 2005;
Speed and Kiiveri. 1986) are graphical models which use an undirected graph
G whose vertices are identiﬁed with variables under study. Each variable
in such a model is conditionally independent of all the others given its
neighbours in the graph.

In the case in which the data are jointly Gaussian (i.e. all variables
taken together have a multivariate Gaussian probability distribution). model
structure is closely related to the covariance matrix 2 of the Gaussian.
In particular. zero entries in the inverse covariance or precision matrix 1"
(22—1) correspond to missing edges in the graph G. That is. Fij=0 <=>
(i , j) ¢E(G). where E(G) denotes the edge set of the graph G.

2.3 Sparse Gaussian MRFs

The structure of a Gaussian MRF may be learned by estimating the pattern
of zeros in the inverse covariance matrix. The asymptotic properties of
standard covariance estimators assure recovery of such structure in the large
sample limit. However. at small-to-moderate sample sizes the estimation
problem is known be challenging. This is especially true in the sample
size/dimensionality regimes typical of multivariate data in molecular biology.

While small-sample estimation of covariance structure is hard. under an
assumption of sparsity. i.e. that there are relatively few edges in the graph
G. effective learning can still be possible. In the last few years. a number of
authors (including Banerjee et (11.. 2008; Friedman et (11.. 2008; Meinshausen
and Biihlmann. 2006; Ravikumar et (11.. 2010) have shown how 61-penalized

approaches can be used to learn sparse MRFs under challenging conditions.
Bayesian (Dobra et (11.. 2004; Jones et (11.. 2005) and shrinkage (Schafer and
Strimmer, 2005) approaches have also been proposed in the literature.

The computational efﬁciency and emphasis on sparsity of 61-penalized
approaches make them attractive in the present setting. Here. we follow in
particular the approach of Friedman et a1. (2008) and Banerjee et a1. (2008).
If 1" is the precision matrix specifying a Gaussian MRF. an 61 penalty is
employed to give the following penalized log-likelihood (as a function of
precision matrix 1"):

£0") = lOgIFI-Tr(SF)-p||F||1 (1)

where. S is the sample covariance matrix and p a parameter that controls the
overall sparsity of the solution.

The desired estimate is given by F=argmaxr £(F). Then. the learning
problem amounts to optimizing L over (positive deﬁnite) matrices 1". This is a
non-trivial optimization problem. Recently. Banerjee et a1. (2008) introduced
efﬁcient optimization procedures for this purpose. Brieﬂy. their approach
involves deriving a dual and then exploiting work due to Nesterov (2005)
on non-smooth optimization to develop an efﬁcient algorithm. We use this
algorithm here and refer the interested reader to the references for full
technical details.

The E 1 penalty in Equation (1) resembles a matrix analogue to the Lasso
which is widely used to learn sparse. high-dimensional regression models
(Tibshirani. 1996). Just as in the Lasso. the penalty term encourages sparse
solutions which have some regression coefﬁcients that are exactly zero; here
the penalty on the inverse covariance l" encourages solutions which are sparse
in the sense of having zero entries in l" and therefore relatively few edges
in the corresponding graphical model. In the ‘large [7. small n” regime that is
typical of molecular biology applications. sparsity can be statistically useful
in reducing variance. In many cases. sparsity may also be a biologically valid
assumption; for example. protein phosphorylation networks typically have
on the order ofp edges (see e.g. Tan et (11.. 2009).

2.4 11-penalized network clustering

We aim to (i) discover. in an unsupervised manner. subsets of the data which
share underlying network structure and (ii) characterize such subset-level
network structure. Our proposal is simple: we capture subset-level network
structure using 61-penalized Gaussian MRFs and carry out clustering or
subset identiﬁcation by an iterative scheme. Our algorithm is as follows:

1. Initialize: set 1:0. szsmax. Randomly partition data {X} into K
clusters subject to mink|{i:C(i)=k}|anin. where C(i)e{l...K}
denotes the cluster assignment for the i-th data point. nmin is a
parameter controlling smallest permitted cluster size and smax is a

positive constant giving an upper bound on model score (here. set to
1010).

2. Estimate: estimate cluster-speciﬁc parameters {ﬁk,f‘k} and store
current model log-likelihood in s:

A 1
M <— + X’
k |{l=C(l)=k}|i:C(Zi):k ’
n. e argmaxlOgIFl—Tr(SkF)—PHFH1
I‘
sold (— S

n
s <— Zlog (Normal(Xi | ﬁc(i),FEl(i)))

i:l

where Sk denotes the sample covariance matrix for cluster k.

3. Re-assign: re-assign data points to clusters using a temporary
cluster assignment function Clmp (assignment becomes permanent if

 

995

112 /3.Io's[Bumo[p.IOJxosorwuiJOJurorq”:duq 11101} papBoIII/noq

91oz ‘Og isnﬁnv uo ::

S.Mukherjee and S.M.HiII

 

algorithm is not stopped in 4). and store smallest cluster size in m:

ClmpU) <— argmaxNormal (Xi lﬁkT‘k—l)
ke(l...K)

m <— mkinllirCImpU)=/<}l
t <— t+1

4. Iterate or terminate: stop if reach maximum number of iterations T.
cluster size becomes too small or relative change in log-likelihood
model score is below a threshold 1':

If tST AND manin AND Is/Sold—IIZT,
C <— Clmp
Repeat 2, 3
Else
Stop
Output C, Fk, ﬁk, s

In all experiments below. we set T2100. rtmin=4 and r=IOT8. The
penalty parameter p is set at the start of the procedure following Banerjee
et a1. (2008) [see also Meinshausen and Biihlmann (2006) for further details].

A A tN—2(Ol)
p = (maturity-)—
W /N—2+z§,_2(a)

where Er, is the sample variance for variable t' and tN_2(Ot), ate [0, l] is
the quantile function of the Student’s t-distribution with N —2 degrees of
freedom. Lower 01 gives sparser solutions (and a more biased estimate); we
use a conservative choice of at: 0.1 in all experiments below. Optimization
is carried out using the Banerjee/Nesterov procedure. The worst-case time
complexity of our algorithm is (9(TKp4‘5 /e) where e > O is desired accuracy
for the Nesterov optimization procedure (Banerjee et (11.. 2008).

This overall approach can be thought of as analogous to a standard iterative
clustering. but with sparse graphical models used to deﬁne clusters. In all
our experiments. we used the best result (i.e. the highest overall model score
s) over 100 random initializations.

(2)

3 RESULTS

We show results on synthetic data, mixed data in which phospho-
proteomic data were combined with synthetic data and on proteomic

03 09

1 (a) T — 1(III)

I
r-lIl

---I

P
m
9
m

Rand Index
0
g,

4

Hand Index
9
\I

F_____

data obtained from cancer cell lines that are part of the NCI-60
panel. We are interested in examining the ability of 11-penalized
network Clustering to carry out two related tasks. First, to discover
data subsets deﬁned by network structure, i.e. to Cluster with respect
to underlying covariance structure. Second, to recover such subset-
speciﬁc network structure. We assess the performance on both tasks
below.

3.1 Synthetic data

Our simulation strategy was as follows. For each of two Clusters,
we generated data having known sparse inverse covariance
structure. We then withheld Cluster labels and analysed the data
in an unsupervised manner to generate the results shown here.
Sparse covariance structures were speciﬁed following a procedure
described in Banerjee et (11. (2008). A speciﬁed number of non-zero
entries were randomly, symmetrically placed to create a precision
matrix (corresponding to a graphical model with a speciﬁed number
of edges) and then data were generated from the corresponding
Gaussian model. The number of non-zero entries was set to equal
the number of variables 1). In the experiments below, we set 11:10
and used per-Cluster sample sizes of n=20, 30,40, 50. To mimic the
case in which biological subtypes differ subtly in terms of network
structure, the Cluster-speciﬁc network structures shared 6 out of 10
edges and each variable was set to have unit within-Cluster variance.
We are interested in the Challenging case in which Clusters differ at
the network level, but are similar at the level of mean response, we
therefore set the Cluster means to differ by a small amount (0.75 in
all experiments below).

3.1.] Clustering Figure 1 shows results for 100 simulated
datasets, comparing 11-penalized network Clustering with (i) K-
means; (ii) a recently introduced, message-passing-based algorithm
called afﬁnity propagation (Frey and Dueck, 2007); (iii) diagonal-
Covariance Gaussian mixture model (GMM) using expectation—
maximisation (EM); (iv) ﬁill-00variance GMM using EM; and (v)
network Clustering using an alternative network learning method
due to Schafer and Strimmer (2005) [this is rooted in shrinkage
estimation; algorithm was as above with shrinkage estimation
as described in Schafer and Strimmer (2005) used in place of

A
O
v

E I; 1(d) ’ F
i 0.9

9
m

P
m

Rand Index

Rand Index
0 O
\l on

 

 

I
I |
I T T i
‘l' "' —.—
I I I
as $1 I I . 06 Q ' oe é I as I
_ J_ I J. _ J. -- J. g -.- % J-
I
o.5 Q J. 0.5 :l 0.5 :l 05 g
E t  E 3' E t a a  3‘ E t  a  r; E t a a  
3 ﬁ 2 E, E a z E, E a z B E 5 z
a a a s a a s a a 2 a
0 0 0 0 0 0 w
7t:20 11:30 n:40 n:50

Fig. 1. Simulated data. clustering results. Boxplots over the Rand index with respect to true cluster membership (higher scores indicate better agreement with
the true clustering; a score of unity indicates perfect agreement) are shown for sample sizes of (a) rt: 20. (b) rt 2 30. (c) rt=40 and (d) rt :50 per cluster. Data
were generated for two clusters from known sparse network models (for details see text). with 100 iterations carried out at each sample size. Results shown
for K -means (KM). afﬁnity propagation (AP). diagonal-covariance Gaussian mixture model [GMM (diag)]. full-covariance GMM [GMM (full)]. network
clustering using shrinkage-based network learning (NCzshrink) and 61-penalized network clustering (NCle ). (For rt=20 the full-covariance GMM could not
be used as small-sample effects meant that it did not yield valid covariance matrices.)

 

996

112 /3.Io's[Bumo[p.IOJxosorwuiJOJurorq”:duq uroii papeolumoq

9103 ‘Og isnﬁnv uo ::

Network clustering

 

11-penalized estimation]. We used the ﬁinction kmeans within
the MATLAB statistics toolbox (with K :2 and 1000 random
initialisations). Afﬁnity propagation was used with default settings
(pair-wise similarities equal to negative Euclidean distance and
scalar self-similarity set to median similarity). EM was performed
using the gmmb_em ﬁinction within the MATLAB GMMBAYES toolbox
(Paalanen et (11., 2006) (with K :2, 100 random initializations and
maximum number of iterations set to T=1000; other stopping
criteria were the same as network Clustering, namely nmin=4 and
1': 10‘8). All computations were carried out in MATLAB R2009a.

We show boxplots over the Rand index with respect to the true
Cluster labels (the Rand index is a standard measure of similarity
between Clusterings; 1 indicates perfect agreement and 0 complete
disagreement). The proposed 11-penalized network Clustering shows
consistent gains relative to the other approaches. At all but the
smallest sample size, it is able to Closely approximate the correct
Clustering, with high Rand indices, even when other methods do
not perform well. The shrinkage-based network Clustering approach
also performs well, but is less effective than 11-penalization at all but
the largest sample size. In contrast, K -means, afﬁnity propagation
and diagonal-covariance GMMs do not perform well, even in the
largest sample case. This reﬂects the fact that these methods are
not able to describe the Cluster-level covariance structure that is
crucial in this setting. A full-covariance GMM provides gains over
the aforementioned three methods, but is both less effective and
more variable than network Clustering. This is due to the fact
that conventional ﬁill covariance estimators can be ill-behaved
at small sample sizes (indeed, in the n=20 case a conventional
ﬁill-00variance GMM did not yield valid covariance matrices and
therefore could not be employed).

3.1.2 Network reconstruction Figure 2 compares true, Cluster-
speciﬁc sparsity patterns (i.e. network structures) with those inferred
by network Clustering. We show results for network Clustering (both
11-penalized and shrinkage-based) and from a GMM. Following
the suggestion of a referee, we also show results obtained from
the application of 11-penalized network learning to data from the
Clusters obtained by a GMM and K—means. We are motivated by
settings in which underlying biological heterogeneity at the network
level has remained unrecognized. Then, the default approach is to
carry out network inference on the entire, heterogeneous dataset,
without Clustering. Results are shown also for this case; to allow a
fair comparison, we use the same 11-penalized network inference
method as is used for network Clustering.

We quantiﬁed the ability of network Clustering to recover Cluster-
level structure by computing the SHD between true and estimated
Cluster-speciﬁc graphs. The SHD is equal to the number of edges
that must be Changed (added or deleted) to recover the true graph;
lower scores indicate a Closer match to the true structure. Figure 2
shows SHD, as a ﬁinction of sample size, for the various approaches
described above. At all sample sizes, network Clustering provides
reductions in SHD relative to the other approaches. Shrinkage
network Clustering performs well in this regard at larger sample
sizes, while 11 network Clustering does better at the smallest
sample size. In contrast, inference without Clustering is unable
to recover network structure, and displays no improvements with
sample size, despite the use of state-of-the-art sparse 11-penalized
network inference. The GMM is outperformed by network Clustering
methods at all sample sizes, yet provides gains over network

  

 

 

 

 

 

 

16 -
14 -
12 -
10 -
E 8
co
6
_All data & L1 \
\
__ KM & L \\
4 1 \I_
._.-GMM (full) & L1 ~~~~~ -‘I
---- --GMM (full)
2 _Nc:L1
— — NC:shrink
0 20 30 40 50

Fig. 2. Simulated data. network reconstruction. Distance between true
and inferred networks. in terms of the number of edge differences
or ‘Structural Hamming Distance (SHD; smaller values indicate closer
approximation to true network) for simulated data at sample sizes of rt:
20, 30,40, 50 per cluster. Results shown for: 61-penalized network inference
applied to complete data. without clustering (‘All Data & L1”); K -means
clustering followed by 61-penalized network inference applied to the clusters
discovered (‘KM & L1”); clustering using a (full covariance) Gaussian
mixture model followed by 61-penalized network inference [‘GMM (full)
& LI’]; full covariance GMM [‘GMM (full)’]; network clustering using
61-penalized network inference (‘NCsz); and network clustering using
shrinkage-based network inference (‘NCzshrink’). Mean SHD over 100
iterations are shown. and error bars indicate SEM.

inference without Clustering at all but the smallest sample size. (In
all cases, we generated graphs from inverse covariance matrices by
thresholding to induce p edges; our main conclusions were robust
to the precise threshold used.)

3.2 Phospho-proteomic and synthetic data

We applied network Clustering to a proteomic dataset from Sachs
et (11. (2005) pertaining to cell signalling. We used the Sachs et (11.
dataset to generate network heterogeneity in the following way.
First, we subsampled n=40 data points (without replacement) from
the complete data (baseline data only, total of 853 samples, 1): 11
phospho-proteins). Then, we merged these subsampled data with
data generated using a known network structure (as described above;
as above within-Cluster marginal variances were unity). This gave
a single, heterogeneous dataset with a total sample size of N :80.
To Challenge the analysis and reﬂect the case in which biological
subtypes differ with respect to underlying signalling networks but
not in terms of mean phospho-protein abundance, we set the Cluster
means to differ by a small amount relative to the unit marginal
variance (0.75, as above).

3.2.] Clustering Figure 3 shows Clustering results obtained from
the Sachs et (11. phospho-proteomic data. As before, we compare
11-penalized network Clustering with K -means, afﬁnity propagation,
diagonal-covariance and full-covariance GMMs and shrinkage-
based network Clustering (settings as described above). We show
boxplots over the Rand index with respect to the true Cluster labels
over 100 datasets generated by subsampling as described above.

 

997

112 /3.Io's[Bumo[p.IOJxosorwuiJOJurorq”:duq moi; papeolumoq

9103 ‘Og isnﬁnv uo ::

S.Mukherjee and S.M.HiII

 

D1

 

J.
a 0.8 I
'U
5 I
g 0.7 T J—
0 6 l % l
J— I
0.5 J. J_
D. A A '-
5 < .8 a E 3-
P, 2’ ﬁ 2
E E S
(5 (5

Fig. 3. Phospho-proteomic and synthetic data. clustering results. Boxplots
over the Rand index with respect to true cluster membership (score of
unity indicates perfect agreement with the true clustering). Data with
a known. gold-standard cluster assignment were created using phospho-
proteomic data from Sachs et a1. (2005) as described in text. Boxplots
are over 100 subsampling iterations; per-cluster sample size was rt=40;
algorithms used were K -means (KM). afﬁnity propagation (AP). diagonal-
covariance Gaussian mixture model [GMM (diag)]. full-covariance GMM
[GMM (full)]. network clustering using shrinkage-based network inference
(NC:shrink) and 11-penalized network clustering (NC:L1).

3.2.2 Network reconstruction Figure 4a shows SHD (computed
as described above) between estimated and correct graphsl (these
were induced from corresponding inverse covariance matrices by
thresholding to return 1) edges; the result did not depend on
precise threshold) over 100 iterations of subsampling. Network
Clustering provides reductions in SHD (i.e. more accurate network
reconstruction) relative to the other methods used. Figure 4b—d
compares the correct inverse covariance for the proteomic data2
to estimates of it. Network Clustering is able to recover the
overall inverse covariance pattern or network structure. In contrast,
applying 11 -penalized network learning directly to the small-sample,
heterogeneous data, without Clustering, we ﬁnd that the correct
structure is obscured.

3.3 Cancer proteomic data

We applied our network Clustering method to proteomic data
obtained from cell lines belonging to the National Cancer Institute’s
NCI-60 panel (Shankavaram et (11., 2007). Speciﬁcally, we used
proteomic data from the 10 melanoma cell lines and 8 renal cell lines
within the panel. This gave a combined dataset with two subsets
corresponding to cancer type, and a total of N = 18 samples. We
used data for p = 147 proteins which did not show strong differential
expression between the cancer types (raw P-values under a Welch

 

1We note that since the models used here are continuous but undirected
they differ in formulation to the discrete. directed Bayesian network models
used in Sachs et a1. (2005). Moreover. we used only baseline proteomic
data without any perturbations. As a result. the network structure shown
here is similar but not identical to the directed graphs shown in the original
reference.

2Since the true covariance structure is not known for these proteomic data.
we obtained the structure labelled as ‘correct’ by applying network inference
to the full sample of 853 data points. On account of the large sample size.
this should closely approximate the true (population) covariance.

two-sample t-test all exceed 0.001; total number of proteins in
original dataset was 162; in addition to the P-value threshold, 7
proteins with erroneous zero values were discarded). These proteins
cover a broad range of signalling pathways (see Supplementary
Table S1). These data represent a high-dimensional setting in which
cancer type labels are known (but withheld from the algorithms),
thereby allowing objective assessment of Clustering accuracy with
respect to the labels.

Table 1 shows Clustering results obtained from these data.
Methods and settings are as described above. Despite the high
dimensionality and low sample size of the data, 11-penalized
network Clustering is able to make a good approximation to the
subtype labels. Indeed, over 100 iterations, the single highest model
score s gave the correct Clustering. Supplementary Figures S1 and S2
show the estimated inverse covariances for the two cancer types.
We note that while 11-penalized network Clustering provides gains
relative to the other methods, the shrinkage-based approach does not.
This is likely due to the small sample size, and echoes simulation
results above. We note that a conventional ﬁll-covariance GMM
approach was not applicable here as, due to the high dimensionality
with respect to sample size, it does not yield valid (symmetric and
positive deﬁnite) covariance matrices.

4 DISCUSSION AND FUTURE WORK

In this article, we addressed the question of probing molecular
heterogeneity in settings where (hitherto uncharacterized) biological
subtypes differ with respect to network structure. We did so by
introducing a network Clustering approach which takes advantage
of the recent developments in efﬁcient, 11-penalized learning of
undirected graphical models. The approach proposed permits both
the discovery of biological subtypes which differ at the network
level and inference about underlying network structures.

The ideas presented here are Closely related to a number of
Classical multivariate methods. We can think of network Clustering
as an unsupervised quadratic discriminant analysis with Cluster-
Conditional models having sparse inverse covariances. Unfavourable
conditions of dimensionality and sample size are exacerbated in
Clustering-type analyses because the limiting factor becomes not
just the overall sample size (which may already be small), but
rather the size of the smallest Cluster (or, from a mixture model
point of view, the smallest mixing coefﬁcient). For this reason,
some form of sparsity, regularization or shrinkage is crucial if full-
Covariance models are to be used for network Clustering. From this
point of view, the approach taken here, of emphasizing sparsity by
the use of 11-penalization, is just one of several possible Choices,
but in the biological setting, where sparsity is arguably a reasonable
assumption, an attractive one. An alternative approach to the ‘hard’
assignments used here would be a EM algorithm; this would yield in
effect a mixture model formulation with sparse covariance structure.
We also note that since the approach presented here builds on
a well-understood, probability model-based Clustering framework,
many existing diagnostic procedures (e. g. for statistical signiﬁcance,
determining number of Clusters, etc.) are readily applicable.

The network Clustering problem is Challenging, but one that
we believe will become increasingly important as emphasis shifts
from individual genes/proteins to networks, motivating covariance-
rather than mean-centred formulations. In this article, we sought
to formulate and address this problem in a tractable way and

 

998

112 /3.Io's[Bumo[p.IOJxosorwuiJOJurorq”:duq moi; papBOIIIAAOG

91oz ‘Og anﬁnv uo ::

Network clustering

 

(3) Structural Hamming Distance

praf

(b) Correct sparsity pattern

 

 

(c) NC: 11

(d) All data (no clustering) — 11

 

 

 

 

18 pmek
ploy
16 F'le
F|P3
D 14
I 1244/42
U)
12 pakts473
1o FKA
PKC I
3 Pas I
PINK
“4’ J _I' E 4’ J: *xmummm Om
.. .. as e o .2 t a a 5;: s s. t E a E
E E a 2 Z 5, n a
S 5‘ é 2 0' 3
= E (D 2
“ E
(5

pink

 

teaaaamges: asaaasnggs:
QEQEE?ED_D_D.Q “EBEE?§1ln-D-
EN 3'15
Q .

Fig. 4. Phospho-proteomic and synthetic data. network reconstruction. (a) SHD between correct and inferred networks. Results shown for 11-penalized
network inference applied to complete data. without clustering (‘All Data & L1 ’); K -means clustering followed by 11-penalized network inference applied
to the clusters discovered (‘KM & L1 ’); clustering using a (full covariance) Gaussian mixture model followed by 11-penalized network inference [GMM
(full) & L1’]; clustering and network inference using a GMM [‘GMM (full)’]; network clustering using 11-penalized network inference (‘NC:L1’); and
network clustering using shrinkage-based network inference (‘NCzshrink’). Mean SHD over 100 subsampling iterations were shown. and error bars indicate
SDs; (b) Correct sparsity pattern. Correct. large-sample sparsity pattern for proteomic data of Sachs et a1. (2005); (c) NC:11. Inverse covariance recovered
from small-sample. heterogenous data by 11-penalized network clustering and (d) All data (no clustering)-l1. Inverse covariance from 11-penalized network
inference applied directly to the complete. heterogenous data (see text for full details; per-cluster sample size rt=40; red and blue indicate negative and positive

values. respectively).

Table 1. Clustering results for proteomic data from cancer cell lines

 

KM AP GMM (diag) NC: sh NC:11

 

0.58:1:0 069* 0.62:I:O.11 0.55 :I:0.08 0.80:1:0.14

 

Mean Rand indices :I:SD for K -means (KM), afﬁnity propagation (AP), diagonal-
covariance Gaussian mixture model [GMM (diag)], shrinkage-based network clustering
(NC:shrink) and 11 -penalized network clustering (NC211 ); results shown over 100
iterations, each with 100 random initializations (*except for AP which is a deterministic
algorithm). Small sample proteomic data, obtained from melanoma and renal cell lines
from the NCI60 panel, were clustered as described in Main Text (p: 147 proteins, full
list in Supplementary Table S1; total sample size N = 18). Rand indices were calculated
with respect to labels indicating the known cancer type (melanoma or renal) of each
sample.

therefore focused on the simplest case of two clusters. However,
the approaches introduced here apply also to the general K -cluster
case. Figure 5 shows network clustering applied to simulated data
from K = 3,4 clusters [simulation regime as above, with all clusters
sharing 6 out of 10 edges and cluster-speciﬁc means differing by a
small amount (0.75, as above) from a zero mean cluster].

We based our approach on continuous, undirected graphical
models, exploiting recent results in computationally efﬁcient,
optimization-based learning for these models. Directed graphical
models, including Bayesian networks (BNs) and their variants,
have been popular in the study of biological networks (including
Husmeier, 2003; Mukherjee and Speed, 2008; Sachs et (11., 2005;
Segal et (11., 2003; Yu et (11., 2004). However, structural inference
for BNs remains challenging and for networks of even moderate
size, is typically computationally intensive. Since in the network
clustering setting, efﬁcient inference is essential to render the overall
analysis tractable, we chose to eschew BNs here. (For example, for
the cancer proteomic data, a single round of 11-penalized network
inference for p: 147 variables required 7 s on a standard desktop
computer; BN-based network inference for p = 147 variables would
require minutes to many hours depending on the precise approach
employed.) However, BNs are in many ways well suited to the study

 

J.

310.8

E

— -.-

§07%| E El 1

I I :

06 J- J.

0.5
a v a Z
., E v,
0 0
K:3

(I311

cm
11131

Rand Index

0 .0

\I on

r114

1*
r—ﬂj—4

 

0.6
0.5
l - - _
5 < a a E T!
13 t E g
a 2 U,
E E ‘2’
(D (5
K:4

Fig. 5. Simulated data. clustering results for K = 3,4 clusters. Boxplots over
the Rand index with respect to true cluster membership are shown for data
consisting of (a) K :3 clusters and (h) K :4 clusters. with per cluster
sample size of rt=50. Data were generated from known sparse network
models (for details see text). with 25 iterations carried out at each sample
size. Results shown for K -means (KM). afﬁnity propagation (AP). diagonal-
covariance Gaussian mixture model [GMM (diag)]. full-covariance GMM
[GMM (full)]. network clustering using shrinkage-based network inference
(NC:shrink) and 11 -penalized network clustering (NC:L1).

 

999

112 /3.Io's[Bumo[p.IOJxosorwuiJOJurorq”:duq moi; popcolumoq

91oz ‘Og isnﬁnv uo ::

S.Mukherjee and S.M.HiII

 

of gene regulatory and cell signalling networks and have been well
developed for these applications in recent years. For this reason,
we think that a promising line of research will be in developing
BN-based network clustering approaches which extend the work
presented here towards directed models.

ACKNOWLEDGEMENTS

SM would like to thank Paul Spellman, Peter Bijhlmann and Nicolai
Meinshausen for fruitful discussions.

Funding: The authors acknowledge the support of the UK
Engineering and Physical Sciences Research Council (EPSRC)
(EP/E501311/1) (SM. and SH.) and the US National Cancer
Institute (NCI) (U54 CA112970-07) (S.M.).

Conﬂict of Interest: none declared.

REFERENCES

Banerjee,O. et a1. (2008) Model selection through sparse maximum likelihood
estimation for multivariate Gaussian or binary data. J. Mach. Learn. Res, 9,
4857516.

Chuang,H.Y. et a1. (2007) Network-based classiﬁcation of breast cancer metastasis.
Mol Syst Biol, 3, Article 140.

Dempster,A.P. (1972) Covariance selection. Biometrics, 28, 1577175.

Dobra,A. et a1. (2004) Sparse graphical models for exploring gene expression data.
J. Multivar Anal, 90, 19&212.

Frey,B.J. and Dueck,D. (2007) Clustering by passing messages between data points.
Science, 315, 9727976.

Friedman,N. (2004) Inferring cellular networks using probabilistic graphical models.
Science, 303, 7997805.

Friedman,J. et a1. (2008) Sparse inverse covariance estimation with the graphical lasso.
Biostatistics, 9, 432441.

Husmeier,D. (2003) Reverse engineering of genetic networks with Bayesian networks.
Biochem. Soc. Trans, 31, 151G1518.

Jones,B. et a1. (2005) Experiments in stochastic computation for high-dimensional
graphical models. Stat. Sci., 20, 388400.

Jordan,M.I. (2004) Graphical models. Stat. Sci., 19, 14&155.

Koller,D. and Friedman,N. (2009) Probabilistic Graphical Models: Principles and
Techniques. The MIT Press, Cambridge, MA.

Lauritzen,S.L. (1996) Graphical Models. Oxford University Press, New York.

Meinshausen,N. and Biihlmann,P. (2006) High-dimensional graphs and variable
selection with the Lasso. Ann. Stat, 34, 143&1462.

Mukherjee,S. and Speed,T.P. (2008) Network inference using informative priors. Proc.
NatlAcad. Sci. USA, 105, 14313414318.

Nesterov,Y. (2005) Smooth minimization of non-smooth functions. Math. Prog., 103,
1274152.

Paalanen,P. et a1. (2006). Feature representation and discrimination based on Gaussian
mixture model probability densitiesepractices and algorithms. Pattern Recogn., 39,
134G858.

Perou,C. et a1. (2000) Molecular portraits of human breast tumours. Nature, 406,
7474752.

Ravikumar,P. et a1. (2010) High-dimensional Ising model selection using 11 -regularized
logistic regression. Ann. Stat, 38, 128771319.

Rue,l-I. and Held,L. (2005) Gaussian Markov Random Fields: Theory and Applications.
Chapman & Hall/CRC, Boca Raton, FL, USA.

Sachs,K. et a1. (2005) Causal protein-signaling networks derived from multiparameter
single-cell data. Science, 308, 5237529.

Schafer,J. and Strimmer,K. (2005) A shrinkage approach to large-scale covariance
matrix estimation and implications for functional genomics. Stat. Appl. Genet. Mol
Biol, 4, Article 32.

Segal,E. et a1. (2003) Module networks: identifying regulatory modules and
their condition-speciﬁc regulators from gene expression data. Nat Genet, 34,
1664176.

Shankavaram,U. et a1. (2007) Transcript and protein expression proﬁles of the NCI-
60 cancer cell panel: an integromic microarray study. Mol Cancer Ther., 6,
8207832.

Sorlie,T. et a1. (2001) Gene expression patterns of breast carcinomas distinguish tumor
subclasses with clinical implications. Proc. Natl Acad. Sci. USA, 98, 10869710874.

Speed,T.P. and Kiiveri,H.T. (1986) Gaussian Markov distributions over ﬁnite graphs.
Ann. Stat, 14, 1387150.

Tan,C. et a1. (2009) Comparative analysis reveals conserved protein phosphorylation
networks implicated in multiple diseases. Sci. Signal, 2, ra39.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. Roy. Stat. Soc.
Ser B, 58, 2674288.

Yip,K. and Gerstein,M. (2009) Training set expansion: an approach to improving the
reconstruction of biological networks from limited and uneven reliable interactions.
Bioinformatics, 25, 2437250.

Yu,J. et a1. (2004) Advances to Bayesian network inference for generating causal
networks from observational biological data. Bioinformatics, 20, 359443603.

Zhu,X. et a1. (2007) Getting connected: analysis and principles of biological networks.
Genes Dev., 21, 101071024.

 

1 000

112 /3.Io's[Bumo[pJOJXO'sorwuiJOJurorqﬂ:duq moi; pepeolumoq

91oz ‘Og isnﬁnv uo ::

