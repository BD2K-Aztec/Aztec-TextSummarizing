BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

M.Howison et al.

 

Thanks to the progress on computational efficiency of genome
assembly, it is now possible to tackle the difﬁcult goal of placing
de novo sequence assembly within an explicit statistical frame—
work. In such a framework, single assembly hypotheses selected
according to ad hoc optimality criteria are replaced by sets of
hypotheses accompanied by statistics that summarize conﬁdence
in each.

2 VARIANTS IN ASSEMBLIES

Alternate assembly hypotheses are called variants. There are
many types of variants, but they fall into two broad categories
that we refer to as hard and soft:

Hard variants correspond to real differences present in the
sample. Hard variation can include heterozygosity, somatic
polymorphism (as in the case of cancer), polymorphism across
multiple individuals when they are pooled for sequencing or vari—
ation across individuals when they are sequenced and assembled
separately but data are then combined across assemblies. Hard
variants reﬂect aspects of organism biology that may or may not
be of direct interest to the investigator.

Soft variants are uncertainties that are introduced during the
sequencing and assembly process, and include library prepar—
ation artifacts and sequencing errors. They persist when there
is not enough information to resolve conﬂicts and identify the
true assembly. Soft variants are nuisances that investigators seek
to reduce or work around.

Discerning between hard and soft variants presents difficult
statistical and computational challenges, and is a fundamental
difﬁculty for metagenome assembly in particular (Charuvaka
and Rangwala, 2011). Although hard and soft variants have dif—
ferent origins, they can both be described within a common
statistical framework, as they both result in multiple assembly
hypotheses. After this common framework is in place, the next
challenge will be to differentiate between hard and soft variants,
either by eliminating soft variation, or by learning to identify
each. However this is ultimately addressed, the very existence
of hard variation is a direct challenge to the expectation that
there is a single true assembly that accurately represents an
organism’s genome.

One of the best—studied types of hard variation is heterozygos—
ity in diploid individuals. Provided enough depth of coverage,
existing statistical methods can accurately identify alleles
(Nielsen et al., 2011). In the absence of enough coverage,
though, it becomes difficult to differentiate true alleles from
sequencing errors. The identiﬁcation of alleles from different
loci that are colocated on the same chromosome is called haplo—
type phasing. Phasing can be achieved computationally or experi—
mentally (Browning and Browning, 2011). Computational
phasing requires population level sampling, which is uncommon
in most studies of de novo genome assembly. Experimental phas—
ing relies on laboratory techniques that are applied during data
generation, such as developing fosmid libraries or separation of
chromosomes. This approach incurs higher costs, and it usually
involves additional computational phasing when phased haplo—
type fragments must be pieced together into larger haplotypes
(Browning and Browning, 2011). At present, phase information
from sequencing reads is not sufﬁcient to fully determine haplo—
type phase.

3 RECORDING VARIANTS

Some assemblers report variants in their output, though without
any accompanying statistical interpretation or distinction
between hard and soft variants. Although much of the focus
has been on tools for single nucleotide polymorphism (SNP)
detection, there is interest in larger—scale structural variants as
well. Preserving and reporting ambiguities in the assembly is an
important step toward assessing assembly uncertainty, especially
if future computational methods can incorporate alternative
assemblies. Assemblers that report variants include:

ALLPATHS-LG (Gnerre et al., 2011) has a custom intermedi—
ate output format for SNPs or homopolymers. For example,
the output sequence TC {A, T}GG represents an SNP, and
TT{ ,T,TT}AC represents a homopolymer. The authors note
that making use of this information in downstream analyses is
an important challenge for the field.

The String Graph Assembler (SGA) (Simpson and Durbin,
2012) retains variants that are not selected by the assembly algo—
rithm, but instead of storing them in a custom format, writes them
to a separate FASTA ﬁle that can be inspected after assembly.

ABySS (Simpson et al., 2009) similarly writes multiple variants
and organizes them into two FASTA f11es, one for SNPs and the
other for insertions41eletions.

Cortex (Iqbal et al., 2012) and fermi (Li, 2012) are both
designed to discover variants during assembly. Both show that
structural variant detection can be improved by discovering vari—
ants during assembly rather than through simply mapping the
assembly to a reference genome.

In addition to advances in the assemblers themselves, there
have also been improvements in data formats. The FASTG
(Jaffe et al., 2012) specif1cation addresses the problem of storing
complex polymorphisms and variants by using a graph represen—
tation for assembly output. Most assemblers’ ﬁnal output uses
a linear FASTA representation, with a record for each contig
or scaffold sequence. Although this format is compact, human—
readable and a suitable representation of a correct unambiguous
assembly, in practice most assemblies include ambiguities that
cannot be represented linearly. At the opposite extreme of the
linear FASTA representation is the intermediate output provided
by most assemblers that dumps out the complete unresolved
graph structure produced during assembly. For most down—
stream applications, this output is too verbose and too raw: it
might not even include the graph traversals chosen by the assem—
bler’s heuristics or algorithms as the final assembly.

FASTG attempts a balance between these two extremes. It is
an extension of the approach taken by ALLPATHS—LG, and
speciﬁes ‘constructs’ enclosed in brackets that can be inserted
into a typical FASTA sequence to represent local non—linear
features like gaps, alleles, tandem repeats or haplotypes. For
example, the sequence GANNNNN[5 :gap : size: (5 , 4. .6) ]
CAGGC [l : alt:allele—C,G] includes constructs for both a
gap of L6 bases and an SNP with a similar proportion of C and
G bases, which can therefore be interpreted as an allele.

Another example of a richer description for assembly output is
the ‘gene graph’, introduced by the GeneStitch (Wu et al., 2012)
method for reconciling and improving metagenomic assemblies.
Using alignments against a reference genome, GeneStitch iden—
tif1es clusters of gene fragments that are highly similar across the

 

2960

ﬁle'sreumofpmJXO'sopeuuowrorq/ﬁdnq

Statistically explicit sequence assembly

 

individual genomes within the metagenome. Instead of trying
to separate the individual genes, GeneStitch merges them into
a structure called a gene graph, which is a subgraph of the
assembly graph that connects all the similar gene fragments.
The gene graph is a condensed representation of the similar
genes, and individual genes can be reconstructed by traversing
paths through the gene graph.

Another middle ground between linear representation and full
assembly graph output is to simply enumerate the full set of
possible assemblies, the approach that the SGA assembler
takes when it writes alternative contigs to an auxiliary file.
This is analogous to the approach taken by phylogenetic infer—
ence tools that generate sets of phylogenetic trees. An investiga—
tor will typically construct and report a consensus tree, which is a
lossy summary of the full set of trees according to some statistical
justiﬁcation (Holder et al., 2008). Similarly, an assembler could
output the full set, but construct a consensus assembly for each
contig. The full set of assemblies is inherently redundant, and
could be compressed with generic text compression tools, like
gzip.

Although these representations are better suited to storing the
variation in assembly output than FASTA, they do not address
the statistical or computational problem of how to quantify the
uncertainty of a given assembly hypothesis. We discuss existing
approaches to these problems below.

4 MIS-ASSEMBLY APPROACHES

Earlier efforts to automate assembly validation successfully
applied statistical tests to identify ‘mis—assemblies,’ or regions
of an assembly hypothesis that violate speciﬂc statistical assump—
tions. For instance, the amosvalidate tool (Phillippy et al., 2008)
uses the compression—expansion statistic (Zimin et al., 2008) to
identify regions of an assembly where paired—end reads align
with insert sizes that deviate from an expected normal distribu—
tion. It also calculates statistics based on the overall read cover—
age, the distribution of k—mers and the presence of fragmented
read alignments.

More recently, the Recognition of Errors in Assemblies using
Paired Reads (REAPR) tool (Hunt et al., 2013) applied similar
metrics of fragment coverage and insert—size distribution to iden—
tify mis—assembled regions, and introduced the ability to call errors
at speciﬁc bases in an assembly hypothesis. Computationally, it
decides which individual bases are ‘error—free,’ meaning that the
base is supported by a speciﬁed number (by default 5) of perfectly
and uniquely aligned reads, and that the difference between the
theoretical and observed fragment coverage falls below a dynam—
ically inferred threshold. Regions with erroneous bases are
reported as mis—assemblies. The algorithm also distinguishes
between contig and scaffolding errors, and can produce a new
assembly where erroneous scaffolds are broken into separate
contigs.

5 LIKELIHOOD APPROACHES

In statistics, likelihood is the probability of the data if the data
were generated according to a speciﬁed hypothesis. In the con—
text of assembly, it is the probability of sequencing the observed

reads under a speciﬁed assembly hypothesis and model of read
generation.

Maximum likelihood estimation attempts to identify the
hypothesis that has the highest probability of producing the
observed data. A maximum likelihood assembly is the assembly
that has the highest likelihood. Maximum likelihood estimation
does not itself provide a conﬁdence interval on any particular
hypothesis; it simply provides a way to ﬁnd the hypothesis that
maximizes the probability of the data. The assembly with the
maximum likelihood may do a much better job than any other
assembly at explaining the data, or there may be millions of other
assemblies that are almost as likely. Even though a likelihood
approach does not directly quantify assembly uncertainty, it pro—
vides an explicit framework with a clear statistical interpretation
for optimizing and evaluating alternative assembly hypotheses.

The Computing Genome Assembly Likelihoods (CGAL) tool
(Rahman and Pachter, 2013) approximates the likelihood of an
assembly given the sequence reads and a generative model. To
reduce computational burden, read generation is considered only
in the region of the assembly where each read maps. The gen—
erative model incorporates separate terms for the length of a read
pair and its aligned site on the genome, and an error model for
SNPs, insertions and deletions. The generative model has to be
learned from the data. Because the distribution of insert sizes for
read pairs depends on both the sequencer and library prepar—
ation, CGAL uses the empirical distribution for the read pair
lengths. For the distribution of sites, it assumes uniform sam—
pling of read pairs across the genome. For the error model, it
assumes sequencing errors are independent events and learns the
substitution rates for each position and for each substitution
combination (because there are known biases for some sequen—
cing technologies), and the insertion and deletion rates for each
position in a read sequence. The aggregate CGAL score for an
assembly is the log of the product of the probabilities that each
individual read could have been generated from the assembly.

Although CGAL is not an assembler, it could be applied to
optimizing assembly by using the annotated likelihood score to
iteratively guide the selection of assemblies and parameter values.
In fact, a maximum likelihood genome assembler was already
proposed based on similar principles (Medvedev et al., 2009).
Like CGAL, it calculates likelihood based on the depth of read
coverage, but it does not incorporate paired—end information at
this stage. Instead, it takes the approach typical of many genome
assemblers of first assembling the conti gs, then resolving conﬂicts
by looking for contigs that agree with the orientation and insert
size of the paired reads. Also, it requires as a parameter the
accurate size of the target genome, which is not available in all
de novo assembly projects. A related design for maximum likeli—
hood assembly (Varma et al., 2011) uses a different formulation
that starts from an approximate size and estimates the actual size
during the optimization.

One of the limitations of the maximum likelihood approach is
that it relies on complex optimizations that are polynomial time
in the number of read sequences, compared with the linear time
algorithms used by most de Bruijn graph assemblers. Also,
unlike most assembly methods described in the literature, neither
the maximum likelihood methods by Medvedev et a]. nor Varma
et a]. provide an open—source reference implementation.

 

2961

ﬁle'sreumol‘pmJXO'sopeuuowrorq/ﬁdnq

M.Howison et al.

 

6 BAYESIAN APPROACHES

Instead of the probability that a given assembly hypothesis could
have generated the sequenced data (i.e. the likelihood), an inves—
tigator may be more interested in the conditional probability of
the assembly hypothesis after taking into account the sequenced
data. This is the posterior probability, P(H|D), of the assembly
hypothesis, and it is related to the likelihood, P(DIH), by the
Bayes’ theorem:

P(D|H)(P(H))

P(H|D) = P(D)

(1)
where P(D) and P(H) are the prior probabilities on the data and
the assembly hypothesis, respectively. The priors are the prob—
ability distributions that express the uncertainty before the data
are taken into account.

There is already at least one tool that considers posterior
probabilities on assemblies, the Assembly Likelihood
Evaluation (ALE) framework (Clark et al., 2013). ALE imple—
ments an expression for the probability that an assembly is cor—
rect, and also reveals the contribution of local regions of the
assembly to this score. This is an important advance toward
assessing the uncertainty of assemblies, especially because it is
made in the context of an explicit statistical framework rather
than ad hoc optimality criteria. ALE estimates the posterior
probability of an assembly (their P(SIR)) by estimating the
prior probabilities (their P(S)) directly from the data (i.e. an
empirical Bayes approach) in conjunction with an approximation
of the assembly likelihood (their P(RIS)) in a similar fashion to
CGAL. One of the most difﬁcult aspects of calculating a poster—
ior probability is deriving the prior probability of the read data,
P(R) (that they denote as Z). They address this challenge with a
rough but efﬂcient approximation of Z. They then refer to the
approximated posterior probability as the ALE score.

The ALE score is a comparative measure of assembly correct—
ness and should be compared among assemblies of the same
genome from the same sequenced data. The ALE score cannot
be calculated for different datasets because of the possible
inaccuracy in approximating the prior probability of the data,
which cancels out when computing a comparative score between
difference assembly hypotheses of the same data. In contrast,
CGAL could conceivably be used to compare the likelihood of
an assembly hypothesis against different datasets (for instance,
from different sequencing technologies) because it does not
calculate the prior probabilities of the data.

Markov chain Monte Carlo (MCMC) is an alternative
approach to approximating posterior probabilities. Rather than
approximate the posterior probability of a particular assembly as
ALE does, an MCMC approach would generate a set of alter—
native assembly hypotheses. This provides a natural way to deal
with assembly uncertainty. The frequency of a particular attri—
bute of the assembly in this set is an approximation of the pos—
terior probability of that attribute. In addition to deriving this
probability, the investigator can also examine the other alterna—
tive hypotheses. An investigator could ask, for example, ‘What
are the most probable hypotheses for gene order that together
account for 90% of the posterior probability?’

To overcome the challenges of estimating the prior probability
on the data, MCMC uses the ratios of posterior probabilities so

that the prior probability on the data cancels out and does need
to be calculated (for an introduction to MCMC, see Gilks et al.,
1996). MCMC methods have been applied to related problems,
such as assembling the haplotype of resequenced human gen—
omes (Bansal et al., 2008). However, we do not know of a de
novo assembly method that has used MCMC to generate a set of
assembly hypotheses. Like maximum likelihood assembly,
MCMC assembly will have signiﬁcant technical challenges with
computational cost and scalability because of the many samples
needed to construct a stable posterior distribution.

7 CONCLUSION

The pieces are now falling in place for assembly to move away
from point estimates that are selected according to ad hoc cri—
teria, toward a statistically explicit framework that provides not
only biologically relevant measures of certainty but also sets of
alternative hypotheses. This will greatly facilitate the evaluation
of assemblies, their application to speciﬁc biological questions,
improvements in assembly algorithms and integration with
downstream analyses that can then take assembly uncertainty
into account. Bioinformatics workﬂow frameworks, such as the
web—based framework Galaxy (Giardine et al., 2005) and the
lightweight command—line framework BioLite (Howison et al.,
2012), already provide biologists with functionality for establish—
ing provenance and reproducibility for computational analyses.
These workﬂow frameworks are the logical foundation for
implementing pipelines that propagate uncertainty through
complex multistage analyses.

ACKNOWLEDGEMENTS

The authors thank C. Titus Brown and an anonymous reviewer
for their helpful comments.

Conﬂict of Interest: none declared.

REFERENCES

Alkan,C. et al. (2011) Limitations of next—generation genome sequence assembly.
Nat. Methods, 8, 61765.

Bansal,V. et al. (2008) An MCMC algorithm for haplotype assembly from whole—
genome sequence data. Genome Res., 18, 133G1346.

Bradnam,K.R. et al. (2013) Assemblathon 2: evaluating de novo methods of genome
assembly in three vertebrate species. Gigascience, 2, 10.

Browning,S.R. and Browning,B.L. (2011) Haplotype phasing: existing methods and
new developments. Nat. Rev. Genet., 12, 7037714.

Chain,P.S. et al. (2009) Genomics. Genome project standards in a new era of
sequencing. Science, 326, 23G237.

Charuvaka,A. and Rangwala,H. (2011) Evaluation of short read metagenomic
assembly. BMC Genomics, 12, SS.

Clark,S.C. et al. (2013) ALE: a generic assembly likelihood evaluation framework
for assessing the accuracy of genome and metagenome assemblies.
Bioinformatics, 29, 4357443.

Earl,D. et al. (2011) Assemblathon 1: a competitive assessment of de novo short read
assembly methods. Genome Res., 21, 2224ﬁ2241.

Finotello,F. et al. (2012) Comparative analysis of algorithms for whole—genome
assembly of pyrosequencing data. Brief. Bioinform, 13, 2697280.

Giardine,B. et al. (2005) Galaxy: a platform for interactive large—scale genome ana—
lysis. Genome Res., 15, 145171455.

Gilks,W.R. et al. (1995) Markov Chain Monte Carlo in Practice. Chapman and
Hall/CRC, London.

 

2962

ﬁle'sreumol‘pmJXO'sopeuuowrorq/ﬁdnq

Statistically explicit sequence assembly

 

Gnerre,S. et al. (2011) High—quality draft assemblies of mammalian genomes
from massively parallel sequence data. Proc. Natl Acad. Sci. USA, 108,
151371518.

Holder,M.T. et al. (2008) A justification for reporting the majority—rule consensus
tree in Bayesian phylogenetics. Svst. Biol., 57, 81¢821.

Howison,M. et al. (2012) BioLite, a lightweight bioinformatics framework with
automated tracking of diagnostics and provenance. In: Proceedings of the 4th
USENIX Worksle on the Theory and Practice of Provenance ( TaPP'12 ).
Boston, MA, USA.

Hunt,M. et al. (2013) REAPR: a universal tool for genome assembly evaluation.
Genome Biol., 14, R47.

Iqbal,Z. et al. (2012) De novo assembly and genotyping of variants using colored de
Bruijn graphs. Nat. Genet., 44, 2267232.

Jaffe,D.B. et al. (2012) The FASTG Format Specification (v1.00). http://fastg.sour
ceforge.net/FASTG_Spec_v1.00.pdf. (27 February 2013, date last accessed).
Li,Y. et al. (2010) State of the art de novo assembly of human genomes from

massively parallel sequencing data. Hum. Genomics, 4, 2717277.

Li,H. (2012) Exploring single—sample SNP and INDEL calling with whole—genome
de novo assembly. Bioinformatics, 28, 183871844.

Mardis,E. et al. (2002) What is finished, and why does it matter. Genome Res., 12,
6697671.

Medvedev,P. and Brudno,M. (2009) Maximum likelihood genome assembly.
J. Compat. Biol., 16, 110171116.

Miller,J.R. et al. (2010) Assembly algorithms for next—generation sequencing data.
Genomics, 95, 3157327.

Nagarajan,N. and Pop,M. (2013) Sequence assembly deniystified. Nat. Rev. Genet.,
14, 1577167.

Nielsen,R. et al. (2011) Genotype and SNP calling from next—generation sequencing
data. Nat. Rev. Genet., 12, 443451.

Paszkiewicz,K. and Studholnie,D.J. (2010) De novo assembly of short sequence
reads. Brief. Bioinform., 11, 457472.

Phillippy,A. et al. (2008) Genome assembly forensics: finding the elusive niis—asseni—
bly. Genome Biol., 9, R55.

Rahman,A. and Pachter,L. (2013) CGAL: computing genome assembly likelihoods.
Genome Biol., 14, R8.

Ricker,N. et al. (2012) The limitations of draft assemblies for understanding
prokaryotic adapmtion and evolution. Genomics, 100, 1677175.

Salzberg,S.L. and Yorke,J.A. (2005) Beware of niis—assenibled genomes.
Bioinformatics, 21, 432G432].

Salzberg,S.L. et al. (2012) GAGE: a critical evaluation of genome assemblies and
assembly algorithms. Genome Res., 22, 5577567.

Schatz,M.C. et al. (2010) Assembly of large genomes using second—generation
sequencing. Genome Res., 20, 116571173.

Sinipson,J.T. and Durbin,R. (2012) Efficient de novo assembly of large genomes
using compressed data structures. Genome Res., 22, 5497556.

Sinipson,J.T. et al. (2009) ABySS: a parallel assembler for short read sequence data.
Genome Res., 19, 111771123.

Varma,A. et al. (2011) An improved maximum likelihood formulation for accurate
genome assembly. In: Proceedings of the 1st IEEE International Conference on
Computational Advances in Bio and Medical Sciences (ICCABS). Orlando, FL,
USA, pp. 1657170.

Wu,Y.—W. et al. (2012) Stitching gene fragments with a network matching algorithm
improves gene assembly for nietagenomics. Bioinﬁ)rmatics, 28, i3637i369.

Zimin,A.V. et al. (2008) Assembly reconciliation. Bioinformatics, 24, 4245.

 

2963

ﬁre'spzumol‘pmJXO'sopeuuowrorq/ﬁdnq

