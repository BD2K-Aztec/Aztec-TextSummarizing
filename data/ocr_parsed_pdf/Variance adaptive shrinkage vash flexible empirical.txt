Bioinformatics, 2016, 1—7

doi: 10.1093/bioinformatics/btw483

Advance Access Publication Date: 19 July 2016
Original Paper

 

Gene expression

Variance adaptive shrinkage (vash): flexible
empirical Bayes estimation of variances

Mengyin Lu1 and Matthew Stephen51'2'*

1Department of Statistics, University of Chicago, Chicago, 60637, USA and 2Department of Human Genetics,
University of Chicago, Chicago, 60637, USA

*To whom correspondence should be addressed.
Associate Editor: Oliver Stegle

Received on April 19, 2016; revised on June 21,2016; accepted on July 9, 2016

Abstract

Motivation: Genomic studies often involve estimation of variances ofthousands of genes (or other
genomic units) from just a few measurements on each. For example, variance estimation is an im—
portant step in gene expression analyses aimed at identifying differentially expressed genes. A
common approach to this problem is to use an Empirical Bayes (EB) method that assumes the vari—
ances among genes follow an inverse—gamma distribution. This distributional assumption is rela—
tively inflexible; for example, it may not capture ’outlying’ genes whose variances are considerably
bigger than usual. Here we describe a more flexible EB method, capable of capturing a much wider
range of distributions. Indeed, the main assumption is that the distribution of the variances is uni—
modal (or, as an alternative, that the distribution of the precisions is unimodal). We argue that the
unimodal assumption provides an attractive compromise between flexibility, computational tract—
ability and statistical efficiency.

Results: We show that this more flexible approach provides competitive performance with existing
methods when the variances truly come from an inverse—gamma distribution, and can outperform
them when the distribution of the variances is more complex. In analyses of several human gene
expression datasets from the Genotype Tissues Expression consortium, we find that our more flex—
ible model often fits the data appreciably better than the single inverse gamma distribution. At the
same time we find that in these data this improved model fit leads to only small improvements in
variance estimates and detection of differentially expressed genes.

Availability and Implementation: Our methods are implemented in an R package vashr available
from http://github.com/mengyin/vashr.

Contact: mstephens@uchicago.edu

Supplementary information: Supplementary data are available at Bioinformatics online.

 

 

1 Introduction

Genomic studies often involve estimation of variances of thousands of
genes (or other genomic units) from just a few measurements on each.
For example, variance estimation is an important step in gene expres-
sion analyses aimed at identifying differentially expressed genes. The
small number of measurements on each gene mean that simple esti-
mates of the variance at each gene leg. the sample variance) can be
quite unreliable. A common solution to this problem is the use of
Empirical Bayes (EB) methods, which combine information across all

genes to improve estimates at each gene. In particular they have the ef-
fect of ‘shrinking’ the variance estimates towards a common mean
value, which has a stabilizing effect, avoiding unusually large or small
outlying estimates that may have high error. A key question is, of
course, how much to shrink. While all EB methods aim to learn the
appropriate shrinkage from the data, existing EB approaches make
relatively inflexible modelling assumptions that could limit their ef-
fectiveness. Here we propose a new, more flexible, EB approach,
which can improve variance estimation accuracy in some settings.

(63 The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com l

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

M.Lu and M.Stephens

 

Perhaps the most commonly encountered example of the use of
EB methods is in gene expression analyses that aim to identify differ—
ences in gene expression among conditions. A typical pipeline for
identifying differentially expressed genes computes a P—value for
each gene using a t—test (two condition experiments) or F—test (mul—
tiple condition experiments), both of which require an estimate of
the variance in expression of each gene among samples. In the clas—
sical t—test or F—test, sample variances are used as plug—in estimates
of gene—specific variances. However, when the sample size is small,
sample variances can be inaccurate, resulting in loss of power
(Murie et 41., 2009). Hence, many methods have been proposed to
improve variance estimation. For example, several papers (Broberg
et al., 2003; Efron et al., 2001; Tusher et al., 2001) suggested adding
an offset standard deviation to stabilize small variance estimates. A
more sophisticated approach (Baldi and Long, 2001) used paramet—
ric hierarchical models to combine information across genes, using
an inverse gamma prior distribution for the variances, and a
Gamma likelihood to model the observed sample variances. This
idea was further developed by Lonnstedt and Speed (2002) and
Smyth (2004) into an Empirical Bayes (EB) approach that estimates
the parameters of the prior distribution from the data. This im—
proves performance by making the method more adaptive to the
data. Smyth (2004) also introduces the ‘moderated t—test’, which
modifies the classical t—test by replacing the gene—specific sample
variances with estimates based on their posterior distribution. This
pipeline, implemented in the software limma, is widely used in gen—
omics thanks to its adaptivity, computational efficiency and ease of
use.

While assuming an inverse—gamma distribution for the variances
yields simple procedures, the actual distribution of variances may be
more complex. Motivated by this, Phipson et al. (2016) (limma with
robust option, denoted by limmaR) modified the procedures from
Smyth (2004) to allow for some small proportion of ‘outlier’ genes
that have higher variability than expected under the inverse—gamma
assumption. Specifically, the limmaR procedure changes the moder—
ated t statistics from limma by decreasing their degrees of freedom
(df) in a way that varies for each gene, depending on whether the
gene looks like an outlier. Genes that look like an outlier have their
df reduced appreciably, making them less significant, whereas other
genes have their df unchanged or reduced very little. They showed
that, in the presence of such outliers, this procedure could improve
on the standard limma pipeline.

Here we consider a more formal EB approach to this problem,
which generalizes previous EB methods by replacing the usual in—
verse gamma prior distribution with a substantially more flexible
family of distributions. The main constraint we place on this prior is
that the distribution of the variances (or, alternatively, the preci—
sions) is unimodal. This unimodal assumption not only seems likely
to be plausible in many settings, but also provides an attractive com—
promise between flexibility, statistical stability and computational
convenience. Specifically it provides more flexibility and generality
than many parametric models while avoiding potential over—fitting
issues of fully non—parametric methods. (An alternative approach
would be to use some kind of regularization to prevent over—fitting;
see Efron (2016) for example.) We use a mixture of (possibly a large
number of) inverse—gamma distributions to flexibly model this uni—
modal distribution, and provide simple computational procedures to
fit this model by maximum likelihood of the mixture proportions.

Our procedure provides a posterior distribution on each variance
or precision, as well as point estimates (posterior mean). The meth—
ods are an analogue of the ‘adaptive shrinkage” methods for mean
parameters introduced in Stephens (2016), and are implemented in

the R package vashr (for ‘variance adaptive shrinkage in R’). We
compare our method with both limma and limmaR in various simu—
lation studies, and also assess its utility on real gene expression data.

2 Methods

2.1 Models

Suppose that we observe variance estimates 3%. . . . 312 that are esti—
mates of underlying ‘true’ variances 5%. . . . .512. Motivated by stand—

ard normal theory, we assume that
A2 - A2
5/-  ~ slag/d). 1e. 5/-  ~ Gamma(d;/2.d,~/(25/-2)). (1)

where the degrees of freedom d, depends on the sample size and we
assume it to be known.

Empirical Bayes (EB) approaches to estimating 51-2 (e.g. Smyth,
2004) are commonly used to improve accuracy, particularly when the
degrees of freedom  for each observation are modest. The EB ap—
proach typically assumes that the variances 51-2 are independent and

identically distributed from some underlying parametric distribution g:
5,2 ~g(-;0) (2)

where the parameters 0 are to be estimated from the data.

Equivalently, that the precisions (inverse variances), 5/72
from some la(-; 0). A standard approach (Smyth, 2004) assumes that

, are i.i.d.

g is an inverse—gamma distribution (i.e. la is a gamma distribution)
which simplifies inference because of conjugacy. Here we introduce
more flexible assumptions for g or 10: specifically that either g or 10 is
unimodal. By using a mixture of inverse gamma distributions for g
(i.e. a mixture of gamma distributions for 10), we can flexibly capture
a wide variety of unimodal distributions for g or 19, while preserving
many of the computational benefits of conjugacy.

2.2 A unimodal distribution for the variances

Let InvGamma(-;a. b) denote the density of an inverse—gamma dis—
tribution with shape a and rate b. This distribution is unimodal with
mode at c : b/(a + 1). To obtain a more flexible family of uni—
modal distributions with mode at c we consider a mixture of
inverse—gamma distributions, each with mode at c:

K
12:1
where
bk 2: (11k + 1)c. (4)

and 7th are mixture proportions. Each component in (3) has mode at c,
and the variance about this mode is controlled by (1,5, with large ah cor—
responding to small variance. By setting a to a large fixed dense grid of
values that range from ‘small’ to ‘large’, we obtain a ﬂexible family of
distributions, with hyperparameters TE, that are unimodal about 6.

We emphasize that the representation (3) is simply a computa—
tionally convenient way to achieve a flexible family of unimodal
distributions. Our goal is that K be sufficiently large, and the grid
of values a be sufficiently dense, that results would not change
much by making the grid larger and denser. In practice modest val—
ues of K (e.g. 10—16) are sufficient to give reasonable performance
(see below for specific details on choice of grid for 3). Using a dense
grid makes the hyperparameters TE non—identifiable, because differ—
ent values for TE can lead to similar values for g(-; 71.3.6), but this is
not a concern here because accurate EB inference requires only a

ﬁm'sreumol‘pquo'sopeuuopttotq/ﬁdnq

Variance adaptive shrinkage (vash)

 

good estimate for g and not Try. This approach is analogous to
Stephens (2016), which uses mixtures of normal or uniform distri—
butions, with a fixed grid of variances, to model unimodal distribu—
tions for mean parameters.

2.3 Estimating hyper—parameters
For K: 1 we estimate the hyperparameters (a, c) by maximizing the
likelihood

L(a.c;§§.....§}) :: p(§1.....§]la.c) (5)
2 Has-w) (6)
/:1
where
pew) 2Jz><§frs%>g<s}1a.c>ds} <7)
“(cl/Zr d- 2 b“
: (dyad/2%, (8)
r(dz/2)F(a)(d;5,- /2 + b)“ //
lb 2 (a + we]. (9)

We use the R command opt im to numerically maximize this likeli—
hood. The approach is similar to Smyth (2004), except that we use
maximum likelihood instead of moment matching.

For K > 1, as noted above, we use K ‘large’ (e.g. 10—16), fix the
values of 11k to a grid of values from ‘small’ to ‘large’, and estimate
the hyper—parameters c. n by maximizing the likelihood

L(n.c;a.§i.....3}) 2p(§1.....311n.a.c) (10)

:2.

Zntp(3;;ak.c) (11)
k

1

2..
II

where p(§;;ak.c) is given by (8). We center the grid of 11k values on
the point estimate 3 obtained for K: 1, to ensure that the grid val—
ues span a range consistent with the data (typically 11k lies between 0
and 100). Moreover, if the data are consistent with K: 1 then the
estimated TE will be concentrated on the component with 11k : 13, and
thus lead to similar results to limma.

To maximize the likelihood we use an iterative procedure that
alternates between updating c and TE, with each step increasing the
likelihood. Given 6, we update TE using a simple EM step
(Dempster et al., 1977). Given n we update 6 by optimizing (11)
numerically using optim. We use SQUAREM (Varadhan and
Roland, 2004) to accelerate convergence of the overall procedure.
See Appendix for details.

2.4 Posterior calculations

Using (3) as a prior distribution for 51-2, and combining with the like—
lihood (1) the posterior distribution of 51-2 is also a mixture of
inverse—gamma distributions:

17(5)?) 2 Z ﬁ,k1nvcamma(s};a,k. 5,1,). (12)
k

where

a”, 2: a, +d,-/2. (13)

51k 2: bk-I-dl'SI-Z/Z. 
«.122 mama/2) b?
71le, I—(dk) (bk+d/’;/Z/2)uk+d//2

 

(15)

ﬁjk :2

nk g‘d/‘Z I—(dk’ +d//2) [if]
E I . I
kl / Hap) (MIMI/f /2)uk,+u..2

Following Smyth (2004) we use the posterior mean of 5/72 as a

point estimate for the precision 5/72:

-_ _ - 5‘12
s. :E(s.zl§f):zn.k+. (16)
k bile

Note that each 13,-), / 5,), can be interpreted as a shrinkage—based esti—
mate of 5/72, since it lies between the observation 3/72 and the prior
mean of the kth mixture component 11;, /bk.

When estimating variances we use the inverse of the estimated pre—
cision (16). While it may seem more natural to use the posterior mean
of 51-2 as a point estimate for 51-2, we found that this can be very sensitive
to small changes in the estimated hyper—parameters a, and so can per—
form poorly. And while it may also be more natural to estimate vari—
ances on a log scale, for example using the posterior mean for log(s,~),
the absence of closed—form expressions makes this less convenient.

2.5 Unimodal prior assumption on variance or precision
The above formulation is based on assuming a unimodal prior distri—
bution for the variance 51-2, specifically by using a mixture of inverse—
gamma distributions all with the same mode. An alternative is to as—
sume a unimodal prior distribution for the precision 1/51-2, by using a
mixture of gamma distributions, all with the same mode. This is
equivalent to using a mixture of inverse—gamma distributions for the

variance 5-2 as in (3) above, but with

/
bk ::(ak—1)/c (17)

in place of (4), because the mode of a Gamma(a. b) distribution is at
c : (a — 1) / b. We present results for both approaches. In practice
one can assess which of the two models provides a better fit to the
data by comparing their (maximized) likelihoods (11). Note that in
many (but not all) cases the fitted prior distributions under either or
both approaches will end up being unimodal for both the variance
and the precision. However, even in these cases, the optimal likeli—
hood under each approach will typically differ because the family of
unimodal distributions being optimized over is different.

2.6 Testing effect size
In differential expression analysis, testing if [3/- : 0 is of primary inter—
est. Smyth (2004) suggested using the ‘moderated t—test’, which mod—
erated the sample variance and degree of freedom by the shrunk
variance estimates and its posterior degree of freedom. Here we derive
an analogue of this moderated t—test in our mixture prior setting.

The distribution of? given 3 is:

pres?) 2(p<3,1ﬁ,.s%>p<sﬂ§?>ds; (18>

: JN(BI-;ﬁj.sj2) - Z ﬁyklnvGamma(s,';zi,-k. giant's; (19)
12

Z ~jth<Eji 215k) ﬁjvgjk) (20)
k

where p,(-; U. u. 0) denotes the density of a generalized t—distribution
with degree of freedom U, location parameter p and scale parameter

ﬁm'sreumol‘piqxo'sopeuuopttotq/ﬁdnq

M.Lu and M.Stephens

 

Table 1. Parameters for the simulation scenarios with unimodal
prior on variance

Table 2. Parameters for the simulation scenarios with unimodal
prior on precision

 

 

 

 

Scenario Description Prior of 57-2 Scenario Description Prior of 1 /s]-2

A Single IG InvGamma( 10,1 1) E Single gamma Gamma(1 0,9)

B Single IG with outliers O.1InvGamma(3,4)+ F Single gamma with outliers O.1Gamma(2,1)+
O.9InvGamma(10,11) O.9Gamma(10,9)

C 1G mixture O.1InvGamma(3,4) + G Gamma mixture O.1Gamma(2,1) +
O.4InvGamma(5,6) + O.4Gamma(5,4) +
O.5InvGamma(20,21) 0.5Gamma(30,29)

D Long tail 0.7logN(0.0625,0.0625) + H Long tail 0.7logN(0.0625,0.0625) +

O.3logN(O.64,0.64)

log-normal mixture

O.3logN(O.64,0.64)

log-normal mixture

 

 

a (i.e. the density of u + 0T, where T, is a standard tdistribution on
U degrees of freedom).

Hence, under the null ([31- : 0),  follows a mixture of general—
ized t—distributions:

Mia-2r, 2 0) 2 Z ~,tpi(3,;2at.0.§,t). (21)
k

A p—value for testing I},- : 0 can therefore be computed as
p; 2Pr(1X;1 > 13,1). (22)

where X, follows the mixture of generalized t—distributions in
(21). In the special case where the mixture involves K: 1 compo—
nents this is equivalent to the P value from Smyth’s moderated t
test.

The P—value P, measure the significance of gene /. To select signifi—
cant differentially expressed genes and control the false discovery
rate, these P values can be subjected to the Benjamini—Hochberg pro—
cedure (Benjamini and Hochberg, 1995), or Storey’s procedure
(Storey, 2002, 2003), for example. Alternatively, the methods in
Stephens (2016) can be extended to incorporate the mixture likeli—
hood (20).

3 Results

3.1 Simulation studies

To compare and contrast our method with limma and limmaR we
simulate data from the model (1)—(3), with G : 10 000, and degrees
of freedom df: 3, 10, 50 (corresponding to sample sizes 4, 11 and
51 respectively) under various scenarios for the actual distribution
of variances (scenarios A—D) or precisions (scenarios E—H), as sum—
marized in Tables 1 and 2.

The simulation scenarios are designed to span the range from a
single inverse—gamma prior as assumed by limma, to more complex
distributions under which we might expect our method to outper—
form limma. Specifically we consider:

° Single 1G (or Single Gamma): single component inverse—gamma
prior on variance (or gamma prior on precision), which satisﬁes
the assumptions of limma.

° Single 1G (or Single Gamma) with outliers: two component
inverse—gamma prior on variance (or gamma prior on precision),
where one component models the majority of genes and the other
component, being more spread out, attempts to capture possible
outliers. The method limmaR is speciﬁcally designed to deal with
the case where large variance outliers exist.

° IG (Gamma) mixture: a more ﬂexible inverse—gamma mixture
prior on variance (or mixture gamma prior on precision) with
multiple components.

° Long tail log—normal mixture: log—normal mixture prior on vari—
ance or precision, which yields a longer tail than either the
inverse—gamma or the gamma distribution.

We also assume that 90% of the genes are not differentially ex—
pressed (Ifg : 0), while the rest of the genes are (Ifg ~ N(0.02)).
Here a is held fixed at 2.

For each simulation scenario we simulate 50 datasets and apply
limma, limmaR, and our proposed method (vash) to estimate 51-2 (or
1/51-2). We compare the relative root mean squared errors (RRMSEs)

of the shrinkage estimators, which we define by

]E(1/s/.2— 1/§/-2)2

RRMSEprec :: —M. (23)
(/]E(1/s/.2 — 1/51- )
lE(s/-2 — sf)2
RRMSEvar :: A2 2 (24)
 — 51-)

The RRMSE measures the improvement of a shrinkage estimator
. . . 4 . . 4 -

over Simply usmg the sample variance 5/. or precision 1/51. , w1th
RRMSE: 1 indicating no benefit of shrinkage. (We also show the
absolute RMSEs, i.e. the numerators of (23) and (24), in
Supplementary Materials; Tables S1, S2.)

Figure 1 and 2 show the RRMSEs of limma, limmaR and 1111510
for all scenarios. We summarize the main patterns as follows:

1. Across all scenarios, the mean RRMSE of 1111510 is consistently no
worse than either limma or limmaR, and is sometimes appre—
ciably better. In contrast, limmaR sometimes performs better
than limma and sometimes worse. In this sense Lids/a is the most
robust of the three methods.

2. In simulations under the simplest scenario (A and E) where the
assumptions of limma are met, all three methods perform simi—
larly. In particular, the additional ﬂexibility of 1111510 does not
come at a cost of a drop of performance in the simpler scenarios.

3. When sample sizes are small (df : 3) all methods perform simi—
larly under all scenarios. This highlights the fact that the beneﬁts
of more ﬂexible methods like Lids/a are small if samples sizes are
too small to exploit the additional ﬂexibility. Put another way,
for small sample sizes simple assumptions sufﬁce.

4. When sample sizes are large (df : 50) 1111510 can outperform the
other methods, particularly under the more complex scenarios
(C,D; G,H), which most strongly violate the assumptions of
limma. Indeed, in these cases both limma and limmaR can have
RRMSE > 1, indicating that they perform worse than the
unshrunken sample estimators. That is, when sample sizes avail—
able to estimate each variance are relatively large shrinkage

ﬁm'sreumol‘piqxo'sopeuuopttotq/ﬁdnq

 

Supplementary Fig. S1

Supplementary Figure S2

Lonsdale er [1]., 2013

Smyth, 2004

Law 61‘ (11., 2014

/310'S[BHm0prOJXO'SOIJBIIIJOJIIIOIq/ﬂdnq

53o”\Ewowsmoaﬁmowoxmoagoﬁsambwﬁ

Figure 3

Figure 3

Fig. 3 Figure 4

Variance adaptive shrinkage (vash)

 

References

Baldi,P. and Long,A.D. (2001) A Bayesian framework for the analysis of
microarray expression data: regularized t-test and statistical inferences of
gene changes. Bioinforrnatics, 17, 509—519.

Benjamini,Y. and Hochberg,Y. (1995) Controlling the false discovery rate: a
practical and powerful approach to multiple testing. ]. R. Stat. Soc. Ser. B
(Methodological), 57, 289—300.

Broberg,P. et al. (2003) Statistical methods for ranking differentially expressed
genes. Genome Biol., 4, R41.

Dempster,A.P. et al. (1977) Maximum likelihood from incomplete data via
the EM algorithm. ]. R. Stat. Soc. Ser. B (Methodological), 39, 1—38.

Efron,B. (2016) Empirical Bayes deconvolution estimates. Biometrilza, 103,
1—20.

Efron,B. et al. (2001) Empirical Bayes analysis of a microarray experiment.
I. Am. Stat. Assoc., 96,1151—1160.

Law,C.W. et al. (2014) Voom: precision weights unlock linear model analysis
tools for RNA-seq read counts. Genome Biol., 15, R29.

Lonnstedt,I. and Speed,T. (2002) Replicated microarray data. Stat. Sin., 12,
31—46.

Lonsdale,]. et al. (2013) The genotype-tissue expression (GTEx) project. Nat.
Genet, 45, 580—585.

Murie,C. et al. (2009) Comparison of small n statistical tests of differential ex-
pression applied to microarrays. BMC Bioinformatics, 10, 1—18.

Phipson,B. et al. (2016) Robust hyperparameter estimation protects against
hypervariable genes and improves power to detect differential expression.
Annals oprplied Statistics, 10, 946—963.

Smyth,G.K. (2004) Linear models and empirical Bayes methods for assessing
differential expression in microarray experiments. Statistical Applications in
Genetics and Molecular Biology, 3, Article 3.

Stephens,M. (2016) False Discovery Rates: A New Deal. hioinv, p. 038216.

Storey,].D. (2002) A direct approach to false discovery rates. ]. R. Stat. Soc.
Ser. B (Stat. Methodol.), 64, 479—498.

Storey,].D. (2003) The positive false discovery rate: a Bayesian interpretation
and the q-value. Ann. Stat., 31, 2013—2035.

Tusher,V.G. et al. (2001) Signiﬁcance analysis of microarrays applied to
the ionizing radiation response. Proc. Natl. Acad. Sci. U. S. A., 98,
5 1 16—5 121.

Varadhan,R. and Roland,C. (2004) Squared extrapolation methods
(SQUAREM): A new class of simple and efﬁcient numerical schemes for
accelerating the convergence of the em algorithm. johns Hopkins
University, Dept. ofBiostatistics Working Papers. Working Paper 63.

/310'S[BHmOprOJXO'SOIJBLUJOJIIIOIq/ﬂdnq

