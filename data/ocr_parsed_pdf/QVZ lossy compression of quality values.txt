Bioinformatics, 31(19), 2015, 3122—3129

doi: 10.1093/bioinformatics/btv330

Advance Access Publication Date: 28 May 2015
Original Paper

 

 

Sequence analysis

QVZ: lossy compression of quality values
Greg Malysa*, Mikel Hernaez*, ldoia Ochoa*, Milind Rao,

Karthik Ganesan and Tsachy Weissman

Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA

*To whom correspondence should be addressed.
Associate Editor: Inanc Birol

Received on November 19, 2014; revised on April 7, 2015; accepted on April 9, 2015

Abstract

Motivation: Recent advancements in sequencing technology have led to a drastic reduction in the
cost of sequencing a genome. This has generated an unprecedented amount of genomic data that
must be stored, processed and transmitted. To facilitate this effort, we propose a new lossy com—
pressor for the quality values presented in genomic data files (e.g. FASTO and SAM files), which
comprise roughly half of the storage space (in the uncompressed domain). Lossy compression
allows for compression of data beyond its lossless limit.

Results: The proposed algorithm OVZ exhibits better rate—distortion performance than the previ—
ously proposed algorithms, for several distortion metrics and for the lossless case. Moreover, it
allows the user to define any quasi—convex distortion function to be minimized, a feature not
supported by the previous algorithms. Finally, we show that OVZ—compressed data exhibit better
performance in the genotyping than data compressed with previously proposed algorithms, in the
sense that for a similar rate, a genotyping closer to that achieved with the original quality values is
obtained.

Availability and implementation: OVZ is written in C and can be downloaded from https://github.

com/mikelhernaez/qu.

Contact: mhernaez@stanford.edu or gmalysa@stanford.edu or iochoa@stanford.edu
Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

There has been a recent explosion of interest in genome sequencing,
driven by advancements in the sequencing technology. Although
early sequencing technologies required years to capture a 3 billion
nucleotide genome (Schatz and Langmead, 2013), genomes as large
as 22 billion nucleotides are now being sequenced within days
(Zimin et (11., 2014) using next-generation sequencing technologies
(Metzker, 2010). Further, the cost of sequencing a human-length
genome has dropped from billions of dollars to merely $4000
(http://systems.illumina.com/systems/hiseq-x-sequencing-system.ilmn)
within the past 15 years (Hayden, 2014). These developments in
efficiency and affordability have allowed many to envision whole-
genome sequencing as an invaluable tool to be used in both
personalized medical care and public health (Berg et (11., 2011). In
anticipation of the storage challenges that increasingly large and
ubiquitous genomic datasets could present, compression of the raw

data generated by sequencing machines has become an important
topic.

The output data of the sequencing machines is generally stored
in the widely accepted FASTQ format (Metzker, 2010). A FASTQ
file dedicates four lines to each fragment of a genome (a ‘read’)
analyzed by the sequencing machine. The first line contains a
header with some identifying information, the second lists the nu-
cleotides in the read, the third is similar to the first one and the
fourth lists a ‘quality value’ (also referred to as quality score) for
each nucleotide. The quality values are generally stored using the
Flared score, which corresponds to the particular number
Q : 7101og10P, where P is an estimate (calculated by the base
calling software running on the sequencing machine) of the prob-
ability that the corresponding nucleotide in the read is in error.
These scores are commonly represented in the FASTQ file with an
ASCII alphabet [33 : 73] or [64 : 104], where the value corresponds

(C7 The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3122

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

OVZ: lossy compression of quality values

3123

 

to Q + 33 or Q + 64, respectively. In addition, the information
contained in the FASTQ files is also found in the SAM files (Li et
al., 2009), which store the information pertaining to the alignment
of the reads to a reference.

Quality values, which comprise more than half of the com—
pressed data, have proven to be more difficult to compress than the
reads (Bonfield and Mahoney, 2013). Thus, generating better com—
pression tools for quality values is crucial for reducing the storage
required for large files. Unlike nucleotide information, the quality
values generated by sequencing machines tend to exhibit predictable
behavior within each read. Strong correlations exist between adja—
cent quality values as well as the trend that quality values degrade
drastically as a read progresses (Kozanitis et al., 2011). There is also
evidence that quality values are corrupted by some amount of noise
introduced during sequencing (Bonfield and Mahoney, 2013). These
features are well explained by imperfections in the base—calling
algorithms, which estimate the probability that the corresponding
nucleotide in the read is in error (Das and Vikalo, 2012). Further,
applications which operate on reads (referred to as ‘downstream ap—
plications”) often make use of the quality values in a heuristic man—
ner. This is particularly true for sequence alignment algorithms
(Langmead et al., 2009; Li and Durbin, 2009) and single—nucleotide
polymorphism (SNP) calling (DePristo et al., 2011; Li, 2011), the
latter having been shown to be resilient to changes in the quality val—
ues (in the sense that, in general, little is compromised in perform—
ance when quality values are modified (Ochoa et al., 2013;
Yu et al., 2014) (http://www.illumina.com/documents/products/
whitepapers/whitepaper_datacompression.pdf).

Based on these observations, lossy (as opposed to lossless) com—
pression of quality values emerges as a natural candidate for signifi—
cantly reducing storage requirements while maintaining adequate
performance of downstream applications. While rate—distortion the—
ory provides a framework to evaluate lossy compression algorithms,
the criterion under which the goodness of the reconstruction should
be assessed is a crucial question. It makes sense to pick a distortion
measure by examining how different distortion measures affect the
performance of downstream applications, but the abundance of ap—
plications and variations in how quality values are used makes this
choice too dependent on the specifics of the applications considered.

These trade—offs suggest that an ideal lossy compressor for qual—
ity values should not only provide the best possible compression and
accommodate downstream applications, but it should provide flexi—
bility to allow a user to pick a desired distortion measure and/or
rate.

In this work, we present such a scheme which we call QVZ
(‘Quality Values Zip”), which achieves significantly better rate—
distortion performance than any of the existing algorithms.
Specifically, the proposed algorithm obtains up to four times better
compression than previously proposed algorithms for the same aver—
age distortion. In addition, QVZ achieves lossless compression.
Moreover, we analyze the effect of QVZ on the genotyping and
show that better results are obtained than with the previously pro—
posed algorithms. Finally, we present some preliminary results that
suggest that lossy compression could potentially improve the geno—
typing with respect to the uncompressed data. This may be due to
the inherently noisy nature of the quality values, in ways that will be
thoroughly investigated in future work.

1.1 Survey of lossy compressors for quality values
Lossy compression for quality values has recently started to
be explored. Slimgene (Kozanitis et al., 2011) fits fixed—order

Markov encodings for the differences between adjacent quality
values and compresses the prediction using a Huffman code (ignor—
ing whether or not there are prediction errors). Q-Scores Archiver
(Wan et al., 2012) quantizes quality values via several steps of
transformations and then compresses the lossy data using an en—
tropy encoder.

Fastqz (Bonfield and Mahoney, 2013) uses a fixed—length code,
which represents quality values above 30 using a specific byte pat—
tern and quantizes all lower quality values to 2. Scalce (Hach et al.,
2012) first calculates the frequencies of different quality values in a
subset of the reads of a FASTQ file. Then the quality values which
achieve local maxima in frequency are determined. Anytime these
local maximum values appear in the FASTQ file, the neighboring
values are shifted to within a small offset of the local maximum,
thereby reducing the variance in quality values. The result is com—
pressed using an arithmetic encoder.

QualComp (Ochoa et al., 2013) applied rate—distortion theory
as a framework for designing a lossy compression algorithm when
mean—squared error (MSE) is the distortion measure. Quality value
data are first clustered using a k—means algorithm and then an opti—
mization problem is solved to minimize MSE of the compressed out—
put with respect to a rate constraint. BEETL (Janin et al., 2013)
first applies the Burrows—Wheeler Transform to reads and uses the
same transformation on the quality values. Then, the nucleotide suf—
fixes generated by the Burrows—Wheeler Transform are scanned.
Groups of suffixes which start with the same k bases while also
sharing a prefix of at least k bases are found. All the quality values
for the group are converted to a mean quality value, taken within
the group or across all the groups. RQS (Yu et al., 2014) first gener—
ates off—line a dictionary of commonly occurring k—mers throughout
a population—sized read dataset of the species under consideration.
It then computes the divergence of the k—mers within each read to
the dictionary and uses that information to decide whether to pre—
serve or discard the corresponding quality values. PBlock (Cénovas
et al., 2014) allows the user to determine a threshold for the max—
imum per—symbol distortion. The first quality value in the file is
chosen as the first ‘representative’. Quality values are then quan—
tized symbol—by—symbol to the representative if the resulting distor—
tion would fall within the threshold. If the threshold is exceeded,
the new quality value takes the place of the representative and the
process continues. The algorithm keeps track of the representatives
and run—lengths, which are compressed losslessly at the end. RBlock
(Cénovas et al., 2014) uses the same process, but the threshold in—
stead sets the maximum allowable ratio of any quality value to its
representative as well as the maximum value of the reciprocal of
this ratio. (Cénovas et al., 2014) also compared the performance of
existing lossy compression schemes for different distortion
measures.

Finally, Illumina proposed a new binning scheme for reducing
the size of the quality values. This binning scheme has been imple—
mented in the state—of—the—art compression tools CRAM (Fritz et al.,
2011) and DSRCZ (Roguski and Deorowicz, 2014).

To our knowledge, and based on the results of Cénovas et al.
(2014), RBlock, PBlock and QualComp provide the best rate—
distortion performance among existing lossy compression algo—
rithms for quality values that do not use any extra information. For
this reason, in Section 3 we use RBlock, PBlock and QualComp as a
representation of the existing state—of—the—art when comparing with
QVZ, together with CRAM and DSRCZ, which apply Illumina’s
binning scheme. For completeness, we also compare the lossless per—
formance of QVZ with that of CRAM, DSRCZ (in their lossless
mode) and gzip.

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

3124

G.Malysa et al.

 

2 Methods

As described previously, we seek to compress the quality scores pre—
sented in the genomic data. Let N be the number of quality score se—
quences to be compressed. The proposed algorithm assumes that all
the quality score sequences are of the same length L (for trimmed or
hard—clipped reads, please refer to the Supplementary Data). Each
sequence consists of ASCII characters representing the scores, be—
longing to an alphabet X, for example X : [33 : 73]. These quality
score sequences are extracted from the genomic file (e.g. FASTQ
and SAM files) prior to compression.

We model the quality score sequence X : [X1,X2, . . . ,XL] by a
Markov chain of order one: we assume the probability that X, takes
a particular value depends on previous values only through the value
of X,_1. We further assume that the quality score sequences are inde—
pendent and identically distributed (i.i.d.). We use a Markov model
based on the observation that quality scores are highly correlated
with their neighbors within a single sequence, and we refrain from
using a higher order Markov model to avoid the increased overhead
and complexity this would produce within our algorithm.

The Markov model is defined by its transition probabilities
P(X,)X,-_1), fori 6 1,2,. . . ,L, where P(XllXo) : P(Xl). QVZ finds
these probabilities empirically from the entire dataset to be com—
pressed and uses them to design a codebook. The codebook is a set
of quantizers indexed by position and previously quantized value
(the context). These quantizers are constructed using a variant of the
Lloyd—Max algorithm (Lloyd, 1982). After quantization, a lossless,
adaptive arithmetic encoder is applied to achieve entropy—rate
compression.

In summary, the steps taken by QVZ are as follows:

1. Compute the empirical transition probabilities of a Markov—1
Model from the data.

2. Construct a codebook (Section 2.2) using the Lloyd—Max algo—
rithm (Section 2.1).

3. Quantize the input using the codebook and run the arithmetic
encoder over the result (Section 2.3).

2.1 Lloyd—Max quantizer

Given a random variable X governed by the probability mass func—
tion  over the alphabet X of size K, let D E RKXK be a distortion
matrix where each entry Dx,y : d(x,y) is the penalty for recon—
structing symbol x as y. We further define )1 to be the alphabet of
the quantized values of size M 3K.

Thus, a Lloyd—Max quantizer, denoted hereafter as LM(-), is a
mapping X —> 31 that minimizes an expected distortion. Specifically,
the Lloyd—Max quantizer seeks to find a collection of boundary
points bk 6 X and reconstruction points 3!), E 31, where
k 6 {1,2, . . . ,M}, such that the quantized value of symbol x E X is
given by the reconstruction point of the region to which it belongs
(Fig. 1). For region k, any x E {bk_1, . . . ,bk — 1} is mapped to yk,
with b0 being the lowest score in the quality alphabet and bM the

Reconstruction Points
P(X )  . I

 

I |

I |

I |

I |

| I

I I

I |

‘ ’._.I I

‘ :‘_ : >
312 b' 213 I; X'

I
,I
I
I
'I
I
I
I
I
b 3

be .711

1 2

Fig. 1. Example of the boundary points and reconstruction points found by a
Lloyd—Max quantizer, for M: 3

highest score plus one. Thus, the Lloyd—Max quantizer aims to min—
imize the expected distortion by solving

M b,—1
{bk,yk}2/I:1 : argmin Z Z P(x)d(x,y,~). (1)
17,5045 1:1 x:b,,1

To approximately solve Equation (1), which is an integer pro—
gramming problem, we employ an algorithm which is initialized
with uniformly spaced boundary values and reconstruction points
taken at the midpoint of these bins. For an arbitrary D and P(-), this
problem requires an exhaustive search. We assume that the distor—
tion measure d(x, 3!) is quasi—convex over 31 with a minimum at
yzx, i.e. when x 33113312 or y; 3311 3x, d(x,y1)£d(x,y2). If the
distortion measure is quasi—convex, an exchange argument suffices
to show the optimality of contiguous quantization bins and a recon—
struction point within the bin. The following steps are iterated until
convergence:

1. Solving for yk: We ﬁrst minimize Equation (1) partially over the
reconstruction points given boundary values. The reconstruction
points are obtained as,

lat—1

yk : argmin Z P(x)d(x,y), Vk : 1,2,. . . ,M. (2)
y:{bk71~w~bk_1l 3pm,,

2. Solving for bk: This step minimizes Equation (1) partially over
the boundary values given the reconstruction points. bk could
range from {yrz + 1, . . . ,ka} and is chosen as the largest point
where the distortion measure to the previous reconstruction
value yk is lesser than the distortion measure to the next recon—
struction value 31,211, i.e.

bk 2 max {x e {3’12 + 1.....yt.1}=P<x>d<x.yt>:

P(x)d(x7yk+1)}Vk:1727---7M_1- 

Note that this algorithm, which is a variant of the Lloyd—Max
quantizer, converges in at most K steps.

Given a distortion matrix D, the defined Lloyd—Max quantizer
depends on the number of regions M and the input probability mass
function  Therefore, we denote the Lloyd—Max quantizer with
M regions as LM§4(-) and the quantized value of a symbol x E X as
LM§,(x).

An ideal lossless compressor applied to the quantized values can
achieve a rate equal to the entropy of LM§4(X), which we denote by
H(LM§4(X)). For a fixed probability mass function P(-), the only
varying parameter is the number of regions M. Since M needs to be
an integer, not all rates are achievable. Because we are interested in
achieving an arbitrary rate R, we define an extended version of the
LM quantizer, denoted as LME. The extended quantizer consists of
two LM quantizers with the numbers of regions given by p and
p + 1, each of them used with probability 1 — r and 1’, respectively
(Where 0 S r S 1). Specifically, p is given by the maximum number
of regions such that H(LMS(X)) < R (which implies
H(LM:+1(X)) > R). Then, the probability 1’ is chosen such that the
average entropy (and hence the rate) is equal to R, the desired rate.
More formally,

P
LMEﬁ (x) : {LMp (x), w.p. 1 r,

LMS+1(x), w.p. r,
p : max {x e {1, . . . ,K} ; H(LM§(X)):R} (4)
R — H(LM§ (X))

H<LMSII<X>> — H<LM5<X>> ‘

ﬁm'spzumol‘pmyo'sopcuuowtotq/ﬁdnq

OVZ: lossy compression of quality values

3125

 

2.2 Codebook generation

Because we assume the data follows a Markov—1 model, for a given
position i E {1, . . . ,L} we design as many quantizers Q; as
there were unique possible quantized values q in the previous con—
text i — 1. This collection of quantizers forms the codebook for
QVZ. For an unquantized quality score X,, we denote the quantized
version as Q,, so Q : [Q1,Q2, . . . ,QL] is the random vector repre—
senting a quantized sequence. The quantizers are defined as

1 , P(X )

Q _ LMEaHgm (5)
‘ P(XIIQI- 2 ) -,

Q; : LMEaH(X‘[Q:1:q),forz— 2, . . . ,L (6)

where or E [0, 1] is the desired compression factor. or : 0 corresponds
to 0 rate encoding, or: 1 to lossless compression and any value in
between scales the input file size by that amount. Note that the
entropies can be directly computed from the corresponding empir—
ical probabilities.

Next we show how the probabilities needed for the LMEs are
computed.

2.2.1 Computation of the probability P

To compute the quantizers defined above, we require P(X,+1[Q,),
which must be computed from the empirical statistics P(X,+1[X,)
found earlier. The first step is to calculate P(Q,[X,) recursively and
then to apply Bayes rule and the Markov Chain property to find the
desired probability:

P(QilXi) 2 QZP<Q,.Q,_1IX,>
2 grease--1); P<Q,_1.X,_1IX,>
—Q:P<Q,X,.Q,_I>X:P<Q,_IX,_1.X,>P<X,_1X.) (7)
2 Q: P(QianQi—1)ZP(Qi—1lXi—1)P(Xi—11Xi)

XI21

Equation (7) follows from the fact that Q,_1 <—> X,_1 <—> X,- form a
Markov chain. Additionally, P(Q,[X,~, Q,_1 : q) : P(Q;(X,) : Qi),
which is the probability that a specific quantizer produces Q, given
previous context q. This can be found directly from 7 [defined in
Eq. (4)] and the possible values for q. We now proceed to compute
the required conditional probability as

P(X-.119.) 2 ZP(XiIQ,->P(X,.I IX.» Q»
X.

 

(8)
: ZP(X,[Qi)P(Xi+1[Xi)
XI
: P(lg) ZP(Q,~[X,~)P(X,.X1'+1)I (9)
I X;

where Equation (8) follows from the same Markov chain as earlier.
Terms in Equation (9) are: (i) P(X,, X,+1): joint pmf computed em—
pirically from the data, (ii) P(Q,[X,): computed in Equation (7) and
(iii) P(Q,): normalizing constant given by

P(Qi : q) : ENQ, : qui)P(Xi)-
XI
The steps necessary to compute the codebook are summarized in

Algorithm 1. Note that support(X) denotes the support of the
random variable X or the set of values that X takes with non—zero

probability.

 

Algorithm 1 Generate codebook

 

Input: Transition probabilities P(X,[X,_1), compression factor or
Output: Codebook: collection of quantizers 
P <— P(Xl)
Compute and store Q1 based on P using Equation (5)
for all columns 1' :2 to L do
Compute P(Q,_1[X,_1 : x)Vx E support(X,_1)
Compute P(X,[Q,_1)Vq E support(Q,_1)
for all q E support(Q,_1) do
P ‘— P(XilQi—1 : q) .
Compute and store Q; based on P using Equation (6)
end for
end for

 

2.3 Encoding

The encoding process is summarized in Algorithm 2. First, we gener—
ate the codebook and quantizers. For each read, we quantize all
scores sequentially, with each value forming the left context for the
next value. As they are quantized, scores are passed to an adaptive
arithmetic encoder, which uses a separate model for each position
and context. For a detailed explanation of the arithmetic encoder,
we refer the reader to the Supplementary Data.

 

Algorithm 2. Encoding of quality scores

 

Input: Set of N reads {X,'}/.I:1
Output: Set of quantizers  (codebook) and compressed
representation of reads
Compute empirical statistics of input reads
Compute codebook  according to Algorithm 1
for all/:1 to N do

lX17H'7XLl PX].

Ql ‘— 910(1)

for all 1'22 to L do

Qi ‘— QIQ,,1(Xi)

end for

Pass [Q1, . . . ,QL] to arithmetic encoder
end for

 

2.4 Clustering

The performance of the compression algorithm depends on the con—
ditional entropy of each quality score given its predecessor. Earlier
we assumed that the data were all i.i.d., but it is more effective to
allow each read to be independently selected from one of several dis—
tributions. If we first cluster the reads into C clusters, then the vari—
ability within each cluster may be smaller. In turn, the conditional
entropy would decrease and fewer bits would be required to encode
X, at a given distortion level, assuming that an individual codebook
is available unique to each cluster.

Thus, QVZ has the option of clustering the data prior to compres—
sion. Specifically, it uses the K—means algorithm (MacQueen et al.,
1967), initialized using C quality value sequences chosen at random
from the data. It assigns each sequence to a cluster by means of
Euclidean distance. Then, the centroid of each cluster is computed as
the mean vector of the sequences assigned to it. Because of the lack of
convergence guarantees, we have incorporated a stop criterion that
avoids further iterations once the centroids of the clusters have moved

ﬁm'spzumol‘pmyo'sopeuuowtotq/ﬁdnq

(janmas 6/ al., 2014
al., 2013 Fritz (71 al., 2011
Roguski and Dcorowicz, 2014

DcPristo ct al., 2011 Zook ct al., 2014

Supplcrncntar) Data

Supplcrncntar) Data

Ochoa 6/

Figure 2

Figure 2

 

Supplementar) Data

PBlock
RBI: I('k

micmup

PBlock

/310'SIBanoprOJxo"SOIJBMJOJutotq//:dnq

Figure 2

Supplementar) Data

(janmas (71 Lil. (2014)

+
+
+
. r

and Durbin,

Table 1

Figure 3

\gﬂ/ I /

I‘IEIm‘k

QI c. mp
Illumina Biuuiug

Ill'IllI

InlanJ Iovzr

2009

Mu Inn

10 I l\11
lIIzIMB

 

Supplementar) Data

Supplementar) Data

Supplementar) Data

Li

/310'SIBanoprOJxo"SOIJBMJOJutotq//:dnq

mm:

I 1.15MB

TIIIIIII

«warm

Iv» Mr;

I

I‘ ~ 1“ NMME

\A'IIMB . mun


1

mun

mini
Ham

1,

Figure 4

 

51mm 1” r , 1 My, ltliﬁ'tl)‘\
III-HUT!

. ﬂ
Mmll's Immu

P131 .k
RBluI
. ,ZV

Illumina binning;

Supplementar) Data

/310'SIBanoprOJxo"SOIJBMJOJutotq//:dnq

OVZ: lossy compression of quality values

3129

 

Li,H. and Durbin,R. (2009) Fast and accurate short read alignment with
Burrows-Wheeler transform. Bioinformatics, 25, 1754—1760.

Li,H. et al. (2009) The Sequence Alignment/Map format and SAMtools.
Bioinformatics, 25, 2078—2079.

Lloyd,S. (1982) Least squares quantization in PCM. IEEE Trans. Inf. Theory,
28, 129—137.

MacQueen,]. et al. (1967) Some methods for classiﬁcation and analysis of
multivariate observations. In: Proceedings of the Fifth Berkeley Symposium
on Mathematical Statistics and Probability, Vol. 1, Oakland, CA, USA, pp.
281—297.

Metzker,M.L. (2010) Sequencing technologies the next generation. Nat. Rev.
Genet, 1 1, 31—46.

Ochoa,I. et al. (2013) Qualcomp: a new lossy compressor for quality scores
based on rate distortion theory. BMC Bioinformatics, 14, 187.

Roguski,1:. and Deorowicz,S. (2014) DSRC 2—industry-oriented compression
of FASTQ ﬁles. Bioinformatics, 30, 2213—2215.

Schatz,M.C. and Langmead,B. (2013) The DNA data deluge. IEEE Spectr,
50, 28—33.

Wan,R. et al. (2012) Transformations for the compression of FASTQ
quality scores of next-generation sequencing data. Bioinformatics, 28,
628—635.

Yu,Y.W. et al. (2014) Traversing the k-mer landscape of NGS read datasets
for quality score sparsiﬁcation. In: Research in Computational Molecular
Biology. Springer, pp. 385—399.

Zimin,A. et al. (2014) Sequencing and assembly of the 22—gb loblolly pine gen-
ome. Genetics, 196, 875—890.

Zook,].M. et al. (2014) Integrating human sequence data sets provides a re-
source of benchmark SNP and indel genotype calls. Nat. Biotechnol., 32,
246—25 1.

/810'S[BHmOprOJXO'SOIJBLUJOJIIIOIq/ﬂdnq

