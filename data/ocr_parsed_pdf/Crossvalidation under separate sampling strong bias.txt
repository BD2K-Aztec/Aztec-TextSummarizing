BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

an?kgowsmomammowoxmoa‘ocgawbmﬁ

 

Cross-validation under separate sampling

 

2.2 Classical cross-validation error estimation

For U C {1, .. . , 11}, let SW) denote the sample S with the points
indexed by U deleted, and deﬁne

W2”)(S,X>= Wnimomm, (7)
where |U| = m is the size of U. Now let k divide n and consider a

(random) partition {U,~; i=1, ...,k} of {1, . . . , n}. Then the clas—
sical k—fold cross—validation estimator is given by

. 1 1‘
ACVU») _
8" — 2 Z2(IMIL‘;I(S,Xq)EOIYq=0+IMIL‘;I(S,Xq)>OIYq=1). (8)
i=lqe ,-

If k = n, this reduces to the leave—one—out estimator

1 H

Al—

8" _ ; ZUMI‘(S.X,-)so [Yi =0 + IM‘(S.X.-)>0 IYI =1)’ (9)
i=1

where we have omitted the braces around the singleton index set {1'}.

Using the classical deﬁnition of cross—validation, (1)
does not hold with separate sampling, in general. To
demonstrate this, let N0 =2: 1 [15:0 be the (random) number
of points from population 1'10 in the sample S; the expected
cross—validation error rate under separate sampling is
E[§:V(k)|N0=n0]. For simplicity, we consider leave—one—out
cross—validation. From (9),

ElgilNo =n01= % P(VV111)(57X1) S 0|Y1=07N0 =n0)

+ % P(WE,I)(S,X1)>0|Y1=1,N0=n0)

= 71—” P(W,,0,1,,,.(s“l 51. x) : 01X 6 no) (10)
+ 3 P(W,,o,,,_1(30. Sam“). X>>0|Xe m)

= 77" 1518204,".1 + 3 Hanna].
On the other hand, it follows from (3) that
E18114 lNo =n01= 038271 IN) = no]
+(1—C)E[8171|N0=no] (11)
=CE[820,,1,,1]+(1— C)11l€,§0,m,1]-

2.3 Cross-validation for separate sampling

To adapt cross—validation to separate sampling, let U C {1, . . . , n0},
let V C {110 + 1, ...,n0 + m}, let SEU) and 5111/) denote the samples SO
and $1, with the points indexed by U and Vdeleted, respectively, and
deﬁne

WWWSO, sl, X)= W,0,m,,,,,,(sw>, s”, X). (12)

mm]
where |U] = m and IV | = I are the sizes of U and V, respectively.
Now let k0 divide no and k1 divide 111, and consider (random) partitions
{U,~;i=1, ...,k0} of {1,...,n0} and {V,~;i=1, ...,k1} of
{110 + 1, . .. , 110 + m}. Separate—sampling (k0, [(1)—fold cross—validation
estimators are deﬁned by
1 k0 k,

 a 
"05"] n0 k1 W "‘ /‘(So.sl.X,>:0’

i=lj=1reU,- "0""
(13)

k0 k]

Acv(k0,k]),l = 1 ‘ .
8mm] mko ZZZIWL"'V'(SO.SI,X,)>0‘

i=1j=lre V) "0""

These are estimators of the population—speciﬁc true errors 82M] and
1

8mm , respectively. One may use a convex combination of the previous
estimators to yield a separate—sampling cross—validation estimator of

the overall true error rate:

A cv(k0,k]) = Cgcv(k0,k1),0 +  _ C) A cv(k0,k]),l - 

"0am "0am SHOW]

If c is known (or known to a high degree of accuracy), then one can use
it in (14). If c is unknown, then there is no proper cross—validation
estimator of the overall error rate.

If k0 =n0 and k1=n1, then the (k0, [(1)—fold cross—validation
estimators defined previously reduce to separate—sampling leave—
one—out estimators:

"0 "l

Al,0 1
8n n = :21 W“ (S S X~><0’
0, 1 nonl i:l/:l "0.", 0, 1, 1_

(15)

"0 "l

A“ 1
8'10"] = ZZIWWI (S S X V)>0 .
s 110111 i=lj=l :10le 0a 1, "0+!

A convex combination of these yields a separate—sampling
leave—one—out estimator of the overall true error rate:
5’ =cél’0 +(1 —c)§[’l . (16)

Mom "om "om

Again, in the absence of knowledge of c, no proper estimator of
the overall error rate is possible.

We now show that a version of (1) holds for the separate—
sampling cross—validation estimator.

THEOREM. The cross—validation estimator in (14) satisﬁes

A  _
E1853"? '11—E18n07m/k01knﬂmi. (17>

PROOF. First notice that if U C {1, . . . , no} and
VC {n0+1,...,n0+n1}, with |U| = m and |V| = I, then

U A,
M0,"?(507517X1)
WnoimmF/(Sm $17  E 1-1076 U7
Us ~
M0,"?(507517X1)

Wn07m,n,,/(So, $1,  6 1-11, i E V.

Therefore, from (13),

A cv(k ,k ),0 _ 0
ELSHOJHO ] ]_E[8H0*"0/k0,"1*"1/k1]’

l
ACV(/C(),k]),l _ 1 ( 
ELSHOJH ]_E[8H0*"0/k0,"1*"1/k1] '

Finally,
A k,k _ A k,k,0 A k,k,l
E18252? '11—cEIezzfn? ‘1 Jul—015165;)? '1 1

_ 0 l
_ Cﬂsﬂoiﬂo/ko."1*m1/k1]+ (1 — C)E[8"0*"0/k05"1*"1/k1]
= Hsnrno/ko.nrrn1/k1]-

In the case of the separate—sampling leave—one—out estimator
deﬁned in (16), the preceding theorem reduces to

151%.] =E[snoil.n_11 . (19>

 

3351

ﬁm'spzumol‘pmyo'sopeuuopuoiq/ﬁdnq

U.M.Braga-Neto et al.

 

3 RESULTS AND DISCUSSION

3.1 Simulation study with synthetic and real data

We have performed a set of experiments using both synthetic
models and real data to examine the behavior of classical and
separate—sampling cross—validation under separate sampling.
Throughout we use 5—fold cross—validation. We consider four
well—known classiﬁcation rules: LDA, Quadratic Discriminant
Analysis (QDA), Linear Support Vector Machine (L—SVM)
and RBF—SVM (see the Supplementary Material for deﬁnitions
of these classiﬁcation rules).

To generate synthetic data, we use a model with class—condi—
tional 3—dimensional Gaussian distributions, N01,, 21), y = 0, 1,
where p0=[0,0, . . .,0, 0], y1=[0,0,...,0,6] and 2‘, has 02 on
the diagonal and ,0y off the diagonal. The pair (p0, p1) can take
on the values (0.8, 0.8) or (0.8, 0.4). We set 6 so that the
Mahalanobis distance between the classes for equal covariance
matrices and the Bhattacharyya distance between the classes for
unequal covariance matrices is 3. We consider n = 80 and
n = 1000, so that we can compare small—sample and large—
sample results.

We consider four public microarray real datasets: pediatric
acute lymphoblastic leukemia (ALL; Yeoh et al., 2002), acute
myeloid leukemia (AML; Valk et al., 2004), multiple myeloma
(Zhan et al., 2006) and breast cancer (Desmedt et al., 2007).
Table 1 provides a summary of these real datasets, including the
total number of features and sample size. For a detailed descrip—
tion of the data preparation, the readers are referred to the
Supplementary Materials. The experiments on real data are essen—
tially similar to those on synthetic data except that in real data
experiments we use t—test feature selection to reduce the dimen—
sionality to d = 3. In real data experiments, we consider only
11 = 80, which allows sufﬁcient data for holdout error estimation.

All experiments are performed for a range of
r= "71° 6 [015,085]. We ﬁx n and determine 110 according to
n0= {an. At each iteration, SO and 51 are randomly picked
from either a synthetic model or real data to train the classiﬁer
and compute the two cross—validation estimates. Finding the bias
requires knowing the true error, which is estimated on 5000 in—
dependent sample points from the synthetic distributions, or held
out points in the case of real data; however, owing to separate
sampling the ordinary holdout method cannot be applied, and
we use separate—sampling holdout as explained by Esfahani and
Dougherty (2014). We consider c = 0.001, 0.1, 0.3,
0.4, 0.5, 0.6, 0.7, 0.9, 0.999. For each classiﬁcation rule, we
repeat the process of obtaining the true error and its estimates
4000 times for each value of r and c to obtain a distribution of
estimates and true errors from which to compute the bias.

Table 1. Microarray studies used in this study

 

 

Dataset Description Features no/nl

(Desmedt et al., 2007) Breast cancer 22215 98/77
(Yeoh et al., 2002) Pediatric ALL 5077 149/99
(Valk et al., 2004) AML 22 215 116/157
(Zhan et al., 2006) Multiple myeloma 54613 156/78

 

In Figure 2, we provide the results for the synthetic data with
unequal covariance matrices [(po, p1)=(0.8,0.4)] for n = 80,
1000 and for two of the real datasets (Desmedt et al., 2007;
Valk et al., 2004). The complete set of results is given in the
Supplementary Material. In the ﬁgure, from left to right, the
columns correspond to LDA, QDA, L—SVM and RBF—SVM,
respectively. The top two rows of the figure correspond to the
real data from Desmedt et al. (2007) and Valk et al. (2004), and
the third and fourth rows correspond to the synthetic data with
n = 80 and n = 1000. The x—axis corresponds to the sampling
ratio r, the y—axis gives the bias, the solid lines are for the pro—
posed separate—sampling cross—validation, the dashed lines are
for classical cross—validation, and the colors code the value of c.

The trends are consistent across all experiments (including
those in the Supplementary Material): (i) for classical cross—
validation with c near 0.5, there is signiﬁcant optimistic bias for
large Ir 7 cl; (ii) for classical cross—validation with small or large c,
there is optimistic bias for large Ir 7 cl and pessimistic bias for
small lricl as long as |r — cl is not very close to 0; (iii) for separate—
sampling cross—validation, estimation is slightly optimistic and
almost unbiased across the range of Ir 7 cl. Combined with the
results of Esfahani and Dougherty (2014), the bias behavior of
classical cross—validation is especially harmful for large Ir 7 cl
because it masks the increase in classiﬁer error that occurs for
large Ir 7 cl, as shown in Figure 1. Furthermore, although the
deviation variance of classical cross—validation can be mitigated
by large samples, the bias issue generally remains just as bad for
large samples.

3.2 Two case studies

To further illustrate the effects of separate sampling on classical
cross—validation bias, we consider two published studies. The ﬁrst
(Ambroise and McLachlan, 2002) uses a colon microarray data—
set containing gene—expression measurements taken from 2000
genes for 62 tissue specimens, 40 tumorous tissues (class 0) and
22 normal tissues (class 1). Using the SVM—RFE classiﬁcation
rule (Guyon et al., 2002), the authors split the data into a training
and a test set, each including 31 specimens, by sampling without
replacement, such that the training data contain 20 tumorous
and 11 normal specimens. They compare the 10—fold cross—val—
idation error using (8) to the standard holdout estimate obtained
by counting the errors on the test set. But the standard holdout
estimate is unbiased under random sampling, not separate sam—
pling. For the latter, holdout estimation must take into account
the value of c to be unbiased (Esfahani and Dougherty, 2014).
Assuming the classiﬁer is applied to the US population, based on
the incidence rate of colorectal cancer among the US population,
which is 40/ 100 000 (Haggar and Boushey, 2009), c = 40 / 100 000.
The black solid and dotted curves in Figure 3 resemble the curves
plotted in Figure 1 of Ambroise and McLachlan (2002). The gray
solid and dashed curves are obtained by considering the
cross—validation scheme (14) and computing the true error
from (3). The error bars refer to the 95% confidence interval.
All curves show the averaged error and estimated error obtained
on 200 random splits of the data as mentioned above.
These curves show that regardless of the number of genes
considered in the classiﬁer, using the classical cross—validation
(8) induces ~13% optimistic bias with respect to the true

 

3352

ﬁm'spzumol‘pmyo'sopeuuopuoiq/ﬁdnq

 

duq

/3.10'spzuin0fpio;xo'soiiauiJOJuioiq”

U.M.Braga-Neto et al.

 

 

0.30 0.40 0.45
| I I

Error rate

0.25
|

 

0.20
|

 

 

 

log2(number of genes)

Fig. 3. The black solid and dotted curves resemble the expected true error
and cross-validation error rates reported in Figure l of Ambroise and
McLachlan (2002). The gray solid and dotted curves are the expected true
error and estimated error by using cross-validation scheme (14) when the
prior probability of colon cancer is set to be the incidence rate across the
USA. All curves show the averaged error and estimated error obtained on
200 random splits of the data

error, while the proposed cross—validation scheme is almost
unbiased.

In the second case study, we use the Parkinson’s dataset used
by Kaya et al. (2011). This dataset contains 22 biomedical voice
features and 195 measurements in which 48 belong to individ—
uals with Parkinson (class 0) and 147 measurements are taken
from healthy individuals (class 1), so that r =0.246. The authors
use this dataset to construct classiﬁers for diagnosis of
Parkinson’s disease based on distorted voice features. Four clas—
sifiers are constructed: naive Bayes (NB; Friedman et al., 1997),
C45 (Dietterich, 2000), kNN (k = 5) (Devroye et al., 1996) and
RBF—SVM. Although Kaya et al. (2011) have reported the
estimated classical cross—validation error on a single sample of
the data, we repeat the sampling procedure 200 times to get an
estimate of the expected cross—validation error using both the
classical (8) and the corrected cross—validation scheme (14). We
assume the prior probability c of Parkinson’s disease is deter—
mined by the incidence rate of Parkinson’s disease across the
USA, which is 13.4/100 000 (Van Den Eeden et al., 2003). In
Figure 4, the white bars are the expected classical cross—valid—
ation error rates; the shaded bars are the estimated error rates
using the separate—sampling cross—validation scheme. The bars
show the averaged estimated error obtained on 200 samplings
of the data. The behavior observed in Figure 2 makes it plaus—
ible that the error estimates for classical cross—validation will
exceed those of separate—sampling cross—validation, which is
nearly unbiased. This is true in all cases except for NB.
However, if we look carefully at Figure 2, we see that the
point at which the bias becomes optimistic (for increasing r)
can be well left of 0.5. This point is affected by the covariance
structure and the classiﬁcation rule. In this case, for NB, it is to
the left of 0.246.

Error rate

 

 

 

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35

9

Classiﬁer

NB C4.5 SVM

Fig. 4. The white bars are the expected classical cross-validation error
rates on the Parkinson’s dataset used by Kaya et al., (2011) for four
classiﬁers. The shaded bars are the estimated error rates by using cross-
validation scheme (14) when the prior probability of Parkinson’s disease
is set to be the incidence rate across the USA. The bars show the averaged
estimated error obtained on 200 samplings of the data

4 CONCLUDING REMARKS

We show in this article that classical cross—validation may display
substantial bias when it is applied in the separate sampling scen—
ario, which is common in biomedical studies. If one wishes to use
cross—validation with separate sampling, then one should use the
separate—sampling version of cross—validation, which is proposed
here, or else, signiﬁcant bias may result. This means that one
must know the prior probability c (at least a good approximation
of it). A similar requirement was made by Esfahani and
Dougherty (2014) to ensure proper performance of the classiﬁ—
cation rule. Using a sampling ratio significantly different from c
will result in poor classiﬁer design and, often, optimistic bias to
obscure the poor design. As concluded by Esfahani and
Dougherty (2014), given the ubiquity of separate sampling in
biomedicine, although it would incur some cost, it would be—
hoove the medical community to gather population statistics so
that accurate estimates of prior class probabilities would be
available. In the absence of such statistics, separate sampling
should not be used.

Funding: This work was supported by NSF award CCF—0845407
(Braga—Neto) and NIH grant 2R25CA090301 (Nutrition,
Biostatistics and Bioinformatics) from the National Cancer
Institute.

Conﬂict of interest: none declared.

REFERENCES

Ambroise,C. and McLachlan,G.J. (2002) Selection bias in gene extraction on the
basis of microarray gene—expression data. Proc. Natl Acuit. Sci. USA, 99,
6562$566

Anderson,T. (1951) Classiﬁcation by multivariate analysis. Rsyc/zometrika, 16,
31750.

 

3354

ﬁm'spzumol‘pmyo'sopnuuopuoiq/ﬁdnq

Cross-validation under separate sampling

 

Braga—Neto,U. and Dougherty,E. (2004) Is cross—validation valid for microarray
classiﬁcation? Bioinformatics, 20, 37¢380.

Desmedt,C. et al. (2007) Strong time dependence of the 76-gene prognostic signa—
ture for node—negative breast cancer patients in the transbig multicenter inde—
pendent validation series. Clin. Cancer Res, 13, 320773214.

Devroye,L. et al. (1996) A Probabilistic Theory of Pattern Recognition. Springer,
New York.

Dietterich,T.G. (2000) An experimental comparison of three methods for construct—
ing ensembles of decision trees: bagging, boosting, and randomization. Mac/i.
Learn, 40, 1397157.

Duda,R.O. et al. (2000) Pattern Cla.\'.\'i‘ﬁcation. Wiley, New York.

Esfahani,M.S. and Dougherty,E.R. (2014) Effect of separate sampling on classiﬁ—
cation accuracy. Bioinformatics, 30, 2427250.

Friedman,N. et al. (1997) Bayesian network classiﬁers. Mac/i. Learn, 29,
1317163.

Glick,N. (1973) Sample—based multinomial classiﬁcation. Biometrics, 29,
2417256.

Guyon,I. et al. (2002) Gene selection for cancer classiﬁcation using support vector
machines in machine learning. Mac/i. Learn, 46, 389422.

Haggar,F.A. and Boushey,R.P. (2009) Colorectal cancer epidemiology: incidence,
mortality, survival, and risk factors. Clin. Colon. Rectal. Sarg, 22, 1917197.
Kaya,E. et al. (2011) Effect of discretization method on the diagnosis of parkinsons

disease. Int. J. Innov. Compat. Inf., 7, 466941678.

Lachenbruch,P.A. and Mickey,M.R. (1968) Estimation of error rates in discrimin—
ant analysis. Teclinometrics, 10, 1711.

Valk,P.J. et al. (2004) Prognostically useful gene—expression proﬁles in acute myeloid
leukemi. N. Engl. J. Merl, 350, 161771628.

Van Den Eeden,S.K. et al. (2003) Incidence of parkinson’s disease: variation by age,
gender, and race/ethnicity. Am. J. Epidemiol, 157, 10157102.

Yeoh,E.J. et al. (2002) Classiﬁcation, subtype discovery, and prediction of outcome
in pediatric acute lymphoblastic leukemia by gene expression proﬁling. Cancer
Cell, 1, 1337143.

Zhan,F. et al. (2006) The molecular classiﬁcation of multiple myeloma. Blood, 108,
20232028.

 

3355

/310'S[BHmO[pJOJXO'SOIJBLUJOJIIIOICI”Idllq

