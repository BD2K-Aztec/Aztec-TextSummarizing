Bioinformatics, 31113), 2015, 2084—2090

doi: 10.1093/bioinformatics/btv086

Advance Access Publication Date: 16 February 2015
Original Paper

 

Sequence analysis

RAPTR-SV: a hybrid method for the detection
of structural variants

Derek M. Bickhart1'*, Jana L. HutchisonI, Lingyang Xuz,

Robert D. Schnabel3, Jeremy F. Taylor3, James M. Reecy4,
Steven SchroederI, Curt P. Van TasseIII, Tad S. Sonstegard1 and
George E. Liu1

1Animal Genomics and Improvement Laboratory, ARS, USDA, Beltsville, MD 20705, USA, 2Department of Animal
and Avian Sciences, University of Maryland, College Park, MD 20705, USA, 3Division of Animal Sciences,
University of Missouri, Columbia, MO 65211, USA and 4Department of Animal Science, Iowa State University,
Ames, IA 50011, USA

*To whom correspondence should be addressed.
Associate Editor: John Hancock

Received on September 23, 2014; revised on January 27,2015; accepted on February 9, 2015

Abstract

Motivation: Identification of structural variants (SVs) in sequence data results in a large number of
false positive calls using existing software, which overburdens subsequent validation.

Results: Simulations using RAPTR—SV and other, similar algorithms for SV detection revealed that
RAPTR—SV had superior sensitivity and precision, as it recovered 66.4% of simulated tandem
duplications with a precision of 99.2%. When compared with calls made by Delly and LUMPY on
available datasets from the 1000 genomes project, RAPTR—SV showed superior sensitivity for
tandem duplications, as it identified 2—fold more duplications than Delly, while making ~85% fewer
duplication predictions.

Availability and implementation: RAPTR—SV is written in Java and uses new features in the
collections framework in the latest release of the Java version 8 language specifications. A com—
piled version of the software, instructions for usage and test results files are available on the
GitHub repository page: https://github.com/njdbickhart/RAPTR—SV.

Contact: derek.bickhart@ars.usda.gov

 

 

1 Introduction

Among one of the larger classes of heritable genetic mutations,
structural variants (SVs) are difficult to detect within data derived
from current high throughput sequencing technologies. SVs have
been implicated as the causative agents of several phenotypes in ani-
mal species such as color-sidedness in cattle (Durkin et (11., 2012)
and peacomb in chickens (Wright et (11., 2009); however, their
reliable detection from next-generation sequencing data requires
cutting-edge computational algorithms and extensive molecular
validation. Much of the need for validation stems from the high false
discovery rates (FDRs) of several popular SV callers, several of

which have been shown to have a FDR of 90% (Mills et (11., 2011).
Additionally, the exact nucleotide breakpoints of SV events are
difficult to detect from sequence data using existing methods. Many
algorithms, such as read depth-based copy number variation (CNV)
detection (Alkan et (11., 2009), attempt to improve SV detection pre-
cision by lowering the resolution of detection; however, this pre-
vents reliable breakpoint estimation.

Higher resolution SV breakpoint detection has recently been the
subject of extensive research within the genomics community. Much
work has been done to utilize short-read library construction tech-
niques, such as paired-end read libraries, to infer the exact

Published by Oxford University Press 2015. This work is written by US Government employees and is in the public domain in the US. 2084

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

RAPTR—S V: a hybrid method for the detection of structural variants

2085

 

breakpoints of SVs in the genome as was implemented in the pro—
gram PEMER (Korbel et al., 2009). One such algorithm, imple—
mented in the program PINDEL, splits reads into smaller
constituents prior to realignment to the reference genome to find the
precise breakpoints of smaller events (Ye et al., 2009). These two
techniques were shown to contribute the highest quality SV predic—
tions in the recent human 1000 genomes project (Mills et al., 2011).
Still, these methods are highly prone to false positive SV call predic—
tions due to chimeric read fragments and repetitive sequence mis—
alignment. We expand on these methods by combining their
predictions to generate highly confident SV calls, which can be fil—
tered at runtime for improved accuracy. Such a strategy can be con—
sidered a ‘hybrid’ of the split read and paired—end algorithms, and
has previously been implemented in software such as Delly (Rausch
et al., 2012) and Lumpy—SV (Layer et al., 2014). We have also de—
signed our tool to be used on non—model organism reference assem—
blies by taking into account the uncertain nature of gap regions in
our runtime filters. We call our method RAPTR—SV which embodies
a combination of read pair (RP) and split—read (SR) methodologies.

2 System and methods

2.1 Simulated dataset

Test data were derived from simulated reads based on sequence
from cattle chromosome 29, extracted from the UMD3.1 reference
assembly (Zimin et al., 2009). In order to create simulation data, 50
replicates of chromosome 29 were generated with simulated, homo—
zygous, tandem duplications and deletions inserted into the chromo—
some at random. The maximum number of SVs per replicate was set
to 50 non—overlapping events. Fifty sets of simulated reads were gen—
erated using wgsim (https://github.com/lh3/wgsim) for each simu—
lated chromosome. Wgsim was run with the INDEL rate set to 0%
and all other settings at the default (500 bp insert size with a 50 bp
standard deviation in insert length). The equivalent of 10>< average
sequence coverage of the chromosome was generated using wgsim
within each simulation. The average sizes of tandem duplications
and deletions were ~2305 and 2281 bp, respectively. The largest
simulated deletion was 42 782 bp in size, whereas the largest tandem
duplicate was 33 201 bp. All programs and analyses were run on a
linux blade server with 24 threads and 100 GB of RAM.

2.2 Real datasets and benchmarking

Data were derived from two sources: (1) the NA12878 BAM files
released by the 1000 genomes project and (2) fastq files from an
Angus bull sequenced to 20>< coverage. The first dataset (1) repre—
sents an excellent ‘truth’ dataset with which to assess the accuracy
of the program on real, validated data provided by the research com—
munity. The second dataset (2) provides a contrast by demonstrating
the application of our method to a non—model organism’s raw data.
Binary alignment map (BAM) files for NA12878 were downloaded
from the NCBI mirror of the human 1000 genomes FTP site (ftp://
ftp—trace.ncbi.nih.gov/ 1 000genomes/ftp/technical/working/
20101201_cg_NA12878/NA12878.hiseq.wgs.bwa.raw.bam). This
alignment file contained reads originally used in the first Genome
Analysis Toolkit publication (DePristo et 41., 2011), so this dataset
is both highly validated and serves as an excellent test case of data
derived from an older Illumina chemistry and older read alignment
algorithms. Summary statistical tests run on the BAM index files
showed that the total fold coverage of the dataset was ~90>< distrib—
uted among eight separate read groups. True positive variant loca—
tions were downloaded from the ‘golden standard SV sets’ from the

human 1000 genomes SV paper (Mills et 41., 2011; Supplementary
Table S5). We note that the coordinates for these variants and the
BAM file are on the hg18 human reference genome, and to avoid
issues with coordinate liftover we did not convert the coordinates to
the hg19 assembly. In order to provide a non—model organism test
dataset, we used 20>< coverage sequence data derived from an
Angus bull. Reads that did not pass the chastity filter were removed
from the dataset, and the 3’ terminal ends of each read were
trimmed by up to two bases if the bases were flagged as poor quality
(ascii character: ‘1’; phred value: 0). Dataset comparisons were per—
formed by using custom Perl scripts and through the use of the
BedTools software suite (Quinlan and Hall, 2010). Simulation re—
sults, results from the Angus bull dataset and the custom scripts
used to process the data can be obtained from a subfolder of our
GitHub repository: https://github.com/njdbickhart/RAPTR—SV/tree/
master/test.

2.3 Read alignment and pre—processing

RAPTR—SV can be used to identify SVs within BWA—aligned,
SAMPE BAM files with or without read groups. Two features of the
BWA aligner that make it amenable for use with RAPTR—SV are
that it lists unmapped reads in the alignment file, and the fact that it
outputs soft—clipped alignments. Both of these types of data entries
are used to populate potential split read sites for the subsequent clus—
tering algorithm. The detection of SVs from paired—end and SR read
alignments benefits from the identification of all potential RP align—
ment locations and orientations. To identify these locations, we
wrap the MrsFAST short—read alignment tool version 2.0.5.4
(Hach et al., 2010) in our ‘preprocess’ pipeline. MrsFAST
identifies all read alignment positions in the reference genome in a
cache—oblivious fashion (Hach et al., 2010). This has the unintended
side—effect of increasing alignment time and alignment file size if re—
peats are not properly masked in the reference genome, so we
used RepeatMasker (http://www.repeatmasker.org/) on the
UMD3.1 cattle reference assembly (Zimin et al., 2009) and hg18 to
mask highly repetitive sequence. We downloaded a version of the
hg18, human reference genome from the UCSC genome browser
(http://hgdownload.soe.ucsc.edu/downloads.html). Average read
alignment lengths (Arp) and alignment length standard deviations (cap)
for each read group ID were estimated from the alignment of 10 000
sampled reads from that ID using the RAPTR—SV ‘preprocess’ mode.

3 Algorithm

3.1 Paired—end discordancy analysis

Our software uses an expanded algorithm for paired end discord—
ancy first used by Hormozdiari et al. in their publication of the soft—
ware, VariationHunter—CR (Hormozdiari et al., 2009). Let F1 and Fr
be the leftmost and rightmost map coordinates of the first read, re—
spectively, and let S1 and Sr be the map coordinates of the second
read. The orientation of the read is based on the 5’ to 3’ directional—
ity of the read compared with the reference genome, with a ‘+’ indi—
cating the same directionality and a ‘—’ indicating reverse
directionality. Now define the orientation of the sequential reads as
0, where O is comprised of the following set: {++, +—, —+, ——}.
A RP (P) would therefore include information from all five data
points: P:{(F1, Fr), (S1, Sr), 0}. The insert length (L) of RP (P), is
equivalent to the distance from the closest read coordinate of the
first read to the closest read coordinate of the second read based on
their orientation. Concordant reads are reads that do not deviate sig—
nificantly in insert length (L) or default read orientation (+—)

ﬁm'srcumol‘pquo'sopeuuowtotq/ﬁdnq

Balanced (B)

Unbalanced (U)

#—

Set of "H"

 

Figure 1

Figure 1

Yo (7/ LIL, 2009

Karakoc (7/ LIL, 2012

Ilach (7/ LIL, 2010

Rausch (7/ LIL, 2012

\r’azirani, 2001

/310'S[BHJDOFPJOJXO'SOTJBHIJOJHTth/ﬂdnq

RAPTR—S V: a hybrid method for the detection of structural variants

2087

 

MM : {n1, n2 . . . nk}, in a read, the PBP is determined from the fol—
lowing equation (Hormozdiari et 41., 2009):

 

1 MIMI") 1 phreiilﬁj)
PBP(MM) _ H<m+ 10 w — m X 10 w >

If a read alignment haS no mismatches, PBP : 1. The sum of PBP
estimates from uncovered discordant RPS, and the anchors of SR
pairs are used to select the first set in each iteration of the set weight
cover algorithm. Unbalanced Split read PBPS are currently not used
in the weight estimation as such alignments have a high probability
of being categorized as false positive calls without additional sup—
porting reads. After grouping the reads into their appropriate sets,
each set should constitute a unique variant call.

We incorporate four additional filters to reduce the likelihood of
false positive calls. First, we remove any sets from HG that contain
support only from unbalanced (U) Split reads. Such calls likely result
from chimeric read fragments or from alignments to repetitive re—
gions of the genome. Additionally, we allow the user to define a
threshold for the number of raw supporting reads that is necessary
to call a set. Calls that have only one read supporting them are indiS—
tinguishable from chimeric read fragments generated during
sequencing library creation, so the default setting of RAPTR—SV is
to filter sets that have only one supporting read. We also include a
filter that removes discordant RPS that span gaps in the assembly
during the program runtime. Assembly gaps often coincide with
highly repetitive regions of the genome that were difficult to resolve
during reference genome assembly. Although discordant reads that
span such gaps could denote actual variants, the uncertainty of refer—
ence genome structure in these regions confounds subsequent valid—
ation. This gap filtration utility is packaged as a separate application
programmer interface (API) in our BedUtilS library (httpS://github.—
com/njdbickhart/BedUtilS). Finally, we also allow the user to set a
threshold for the minimum summed PBP value for a retained set.
This filter serves to eliminate many spurious alignments that result
from repetitive region alignments as being reported as discordant
read sets. RAPTR—SV defaults to a minimum of one summed PBP
value for a set to be considered valid for further analysis; however,
the user is encouraged to increase this threshold depending on the
fold coverage of their input dataset.

4 Discussion

4.1 Performance and runtime statistics

To take advantage of multiple core systems, we have incorporated
into RAPTR—SV several functional programming tools included in
the latest jdk version 8 release. This enables uS to use both ‘divide
and conquer” and ‘map reduce” frameworks for some of the more
computationally intensive sections of the algorithm. We have
included a compatibility release of RAPTR—SV that will run on the
jdk version 7; however, it doeS not currently possess the same
threading potential as the main release. The average runtime for a
50 megabase (Mb) chromosome with 10>< coverage was ~15 min
on a Single thread, excluding prior BWA alignment time. This time
and resource estimate includes both the ‘preproceSS” and ‘cluSter”
modes of the program. Memory usage was ~1 GB, on average, per
thread. In order to profile resource usage for more common teSt case
usages in ‘cluSter” mode, we sampled reads from a 1000 genomes
reference dataset (NA12878; for further details see methods). Test
conditions included combinations of a large chromosome (human
chr1; 247Mb), a small chromosome (human chr22; ~50 Mb), 90><
coverage and 10>< coverage. We found that CPU usage scaled based

on the number of potential variant sets discovered during the ‘pre—
processing” step (see additional file: performance_statisticS.xlsx).
There was a linear trend when larger datasets were considered, sug—
gesting that the initial input stage of the ‘cluSter” algorithm took a
larger proportion of runtime for smaller datasets. Memory usage
was difficult to predict as we suspect that the Java virtual machine”S
(JVM”S) garbage collection algorithm acted responsiver to our
larger data tests and did not need to act to recover memory in our
smaller tests. Our largest trial (247 Mb chromosome; single thread;
90>< coverage) utilized 14 GB of RAM at peak usage. The smallest
trial dataset (5 0 Mb chromosome; single thread; 10>< coverage) used
5 GB at peak.

To limit the memory overhead incurred by analyzing supporting
reads, RAPTR—SV haS been designed to allow the user to specify
how many supporting RPS for each set are held in memory prior to
spilling to disk. Program runtime should theoretically scale well
with additional processor coreS given sufficient memory overhead
and suitable raid—Storage. We have included several command line
options to allow the user to scale the program”S resource consump—
tion against itS potential performance profile. We present an ex—
ample workflow in Figure 2 that shows the necessary prerequisite
fileS and the two modes of operation of the main program. The ‘pre—
proceSS” mode generates the necessary metadata fileS from input
BAM fileS, whereas the ‘cluSter” mode generates discordant read sets
and makes the final SV calls. There is the potential to use an alterna—
tive preprocessing step, such as Samblaster (httpS://github.com/
GregoryFaust/samblaster), to generate preliminary data to be used
in the RAPTR—SV ‘cluSter” mode; however, the RAPTR—SV ‘prepro—
ceSS” mode is not dependent on BWA—MEM alignment annotation
and can be used on a wide variety of ‘legacy” BWA alignment anno—
tations. In tests using the NA12878 dataset, we found that
Samblaster recovered 14—fold fewer discordant RPS than did the
RAPTR—SV ‘preproceSS” mode, likely due to the former”S reliance on
BWA—MEM annotations in the BAM fileS (teSt script:
aSSOciateDiscordantReadSSamDivet.pl). We would like to reiterate
that the NA12878 dataset waS aligned with the BWA ALN algo—
rithm, so such a discrepancy in detection between the two algo—
rithms may not apply to datasets aligned with BWA—MEM.

4.2 Simulations and comparison to existing tools

To finely tune the detection of SVS, we created Simulated SVS uSing
the current cattle reference assembly sequence from the smallest cat—
tle chromosome, chromosome 29. Simulated SVS were limited to de—
letions and tandem duplications to provide direct comparisons to
predictions made by the Delly/Duppy SV detection suite version
0.0.9 (Rausch et al., 2012) and Lumpy—SV version 0.2.6 (Layer
et al., 2014). Simulations were repeated 50 timeS, with an average of
42 SVS (21.23 deletions and 20.84 tandem duplications) per Simu—
lated dataset. Reads were aligned with BWA version 0.6.2—r126
prior to being used by all three detection programs. Delly and
Duppy were run with default settings, Lumpy—SV waS run with the
recommended settings (httpS://github.com/arq5x/lumpy—SV/blob/
master/README.rSt), and the RAPTR—SV preproceSS mode waS run
on 15 threads with an initial read sample Size limit of 100 000 readS.
Delly/Duppy can improve the accuracy of SV breakpoint detection
by utilizing a reference genome faSta file to align Split readS within
identified SV candidates. We provided the original reference genome
sequence for chromosome 29 to Delly/Duppy to improve their pre—
dictions. Similarly, Lumpy—SV can utilize SR mappings from other
software to improve prediction, so we used the Split_unmapped_to_—
faSta.pl Perl script included in the Lumpy—SV package to select

ﬁm'srcumol‘pquo'sopcuuowtotq/ﬁdnq

B\\".L\ aligned B \15
Reference Genome . Assembly Gap

Fiat:  If) “mm

DIHL‘UI'dtll'II rend pilirh‘

L'nnu'lpped
A ne hm‘ Reads and
Soft Clipped

Disem‘dant
Re ads

3. (Mr

Set Weigh
Cm'er

Deletions Insertions Tandem [)ups

 

Table 1

/310'SIBano[pJOJxo"SOIJBMJOJutotq//:duq

RAPTR—S V: a hybrid method for the detection of structural variants

2089

 

Although this method works very well at reducing the number of po—
tential discordant reads to be analyzed, it unfortunately discards
true positive discordant reads that may map to multiple locations in
the genome. MrsFAST, in contrast, outputs all discordant read loca—
tions, thereby allowing the RAPTR—SV algorithm the liberty of clus—
tering SRS into predicted SVS. The subsequent set weight cover
algorithm then selects for SV calls those that have the best support
and removes alternative mappings for reads that have already been
used to construct previous SV calls. Another key difference between
the algorithms is how SRS are prepared and used. Delly reduces the
complexity of SR analysis by searching for OEAS near SV intervals
predicted by discordant paired—end reads. Although this is an effi—
cient strategy for breakpoint detection, there is the potential for
Delly to miss SVS for which the only evidence is from SRS. RAPTR—
SV calculates SR estimates separately from discordant reads, thereby
allowing the program to identify SVS, which do not have evidence
from discordant RPS. Lumpy—SV does not include a split read align—
ment algorithm and instead relies on other software to create split
read mappings for SV detection. The method used to generate
Lumpy—SV split read alignments in this simulation was similar to the
general algorithm used by RATPR—SV; however, each split read was
assigned a single mapping location by BWA—SW and is therefore sub—
ject to the same limitations that are discussed earlier. We would like to
note that it is also possible to submit RATPR—SV calls to Lumpy—SV
for use in multiple—calling comparisons with other SV detection tools.

4.3 Identification of variants within a sequenced
individual from the 1000 genomes project

Sequence data derived from PCR—based library creation methods
used in Illumina technologies can often contain several contamin—
ants that confound the accurate detection of SVS. To test our soft—
ware against such a dataset, we downloaded the aligned reads from
one of the CEU trio individuals (NA12878) that served as a gold
standard for variant discovery. We used the hg18 reference assembly
for all subsequent alignments including the Delly/Duppy split read
prediction, and RAPTR—SV and Lumpy—SV split read alignment
stages. As with the simulation, Delly/Duppy and Lumpy—SV were
run with default/recommended settings. Specifically, Lumpy—SV was
run with an mw value of 4 and the split read alignments were gener—
ated by the Split_unmapped_to_fasta.pl script with the BWA—SW
pipeline. RAPTR—SV was run with a raw read filter of 5 and a Prob—
based Phred filter of 6.0 for deletions and 2.0 for tandem duplica—
tions and insertions. Delly/Duppy did not appear to distinguish be—
tween the 16 read groups of the BAM; however, RAPTR—SV
calculated separate average and standard deviation values for RP in—
sert lengths for each library. The BAM alignments were derived
from two separate libraries (Solexa—18483 and Solexa—18484),
which RAPTR—SV predicted had average read insert sizes of 380 and
407 hp with standard deviations of 40 and 43 bp, respectively.

Delly and Duppy predicted several large deletions and duplications
that extended over 10 Mb in size. The largest of these variants was
246 Mb in size, which accounted for nearly all of the bases on human
chromosome one. The largest events were predicted on the first human
chromosome by both programs, with nearly the entirety of the
chromosome predicted to be both duplicated and deleted by Duppy
and Delly, respectively. Interestingly, the largest duplication
(246 735 634 bp) and deletion (241 158 955 bp) predictions on
chromosome one had breakpoints that were not near any annotated re—
petitive elements or assembly gaps, which indicates that large chimeric
reads rather than misaligned RPS contributed to these false positive
calls. Similarly, Lumpy—SV predicted a large deletion (150 047 106 bp)

Table 2. NA12878 SV call benchmarks

 

 

Program Filtered Sensitivity Sensitivity > Sensitivity
callsi < mediant (%) mediant (%) total (%)

DELLY 1 545 981 59.5 62.3 60.90
DUPPY 155 638 2.0 13.3 7.64
LUMPY DELS 958 275 65.4 60.7 63.0
LUMPY DUPS 2132 7.3 14.0 10.6
RAPTR-SV DELS 477 316 46.1 66.7 55.0
RAPTR-SV TAND 22 546 2.6 26.0 14.2

 

‘All SV calls >2 Mb in length were ﬁltered from each respective dataset.
Bold highlighting indicates the best performance for the ‘Deletion’ and
‘Duplication’ categories. IThe median size of deletions and duplications in the
NA12878 truth set were determined to be 3441 and 5889 bp, respectively.
Sensitivity percentages indicate the number of truth set calls that had a 50%
reciprocal overlap with SV calls from the program.

and a large duplication (239 646 682 bp) on chromosome one that did
not intersect with known gap or repeat regions. RAPTR—SV did not de—
tect such large variants primarily due to the fact that such events often
cross assembly gaps and are filtered prior to SV calling. RAPTR—SV
also uses an event size filter to ensure that extremely large events can
be appropriately removed from final SV datasets.

After removing events larger than 2Mb from all three datasets,
we estimated the sensitivity rate of each program for known SVS in
NA12878 by requiring a reciprocal 50% overlap of the variant call
with the boundaries of the known variant site. Sensitivity was then
calculated as stated in the simulated dataset. Known variant sites
were obtained from the golden standard variant set for NA12878
provided by the Human 1000 genomes SV paper (Mills et al., 2011 ).
All three programs recovered the majority of the validated deletions
in NA12878 (Table 2). Delly and Lumpy—SV correctly predicted 44
and 58 more true positive, small deletions than RAPTR—SV, respect—
ively. A closer examination of the data revealed that RAPTR—SV had
identified 14 of these sites; however, the breakpoints of these events
could not be resolved due to noise in SR alignment in these regions.
If the stringency of the overlap conditions were reduced to a recipro—
cal 25% overlap between the SV call and the known site, RAPTR—
SV achieved a total sensitivity rate of 66.7% (data not shown),
matching Delly and Lumpy—SV. We noticed a discrepancy in the
sizes of known events and their sensitivity rate for each respective
program. Known deletion sites that were larger than the median of
the truth dataset (3441 bp) were predicted more frequently by
RAPTR—SV than by Delly and Lumpy—SV. All of the Delly and
Lumpy—SV exclusive deletion calls were below the median size,
which indicated that both programs have improved breakpoint pre—
cision at this lower size range than does RAPTR—SV. With respect to
duplications and insertions, RAPTR—SV correctly predicted twice
the number of true positive duplications than did Duppy, and
~30% more true positive duplications than Lumpy—SV. RAPTR—SV
was superior to Duppy and Lumpy—SV at predicting these events re—
gardless of SV size, though all three programs struggled to predict
smaller insertions and duplications. With sequencing datasets that
have high read insert variance and numerous potential chimeric
fragments, we therefore recommend using alignment—based INDEL
discovery methods for the identification of insertions of sequence
<50 bp in length. We have included a list of RAPTR—SV exclusive
SVS detected from this dataset that were previously validated as part
of the Mills et al. golden dataset. In all, 37 previously identified
events were identified exclusively by RAPTR—SV (see additional file:
RAPTR—SV.exclusive.NA12878.predictions.bed). We have also
included some initial comparisons from our use of RAPTR—SV and

ﬁm'srcumol‘piqxo'sopcuuowtotq/ﬁdnq

2090

D. M. Bickhart et al.

 

Delly on 20>< coverage sequence data derived from an Angus bull
(see additional files: angus_venn.pdf and angus_test_table.xlsx).
This initial comparison showed high overlap of RAPTR—SV calls
with Delly (46.1% bp overlap of deletions) and Duppy (73.4% bp
overlap of duplications); however, the size of the Delly/Duppy pre—
diction dataset eclipsed that of RAPTR—SV (128 Mb versus 5.56 Mb,
respectively). Without experimental validation, it is impossible to
predict how many of these discovered variants are true positives;
however, the Delly/Duppy dataset predicted that 5% of the cattle
genome was variable, which is at least twice the size of previously
determined estimates of 1.07—2.45% (Bickhart et al., 2012; Liu
et al., 2010; Zhang et al., 2014).

5 Conclusions

We present RAPTR—SV as a precise, tunable and scalable method
for the detection of SVS using paired—end sequence data. Groups
that are sequencing non—model organisms will likely find the most
benefit from our algorithm, though we hope that the entire research
community will adopt the tool. Future development goals will be to
reduce the memory overhead of the program and to incorporate an
algorithm to detect both chromosome translocations as well as read
depth signal.

Acknowledgements

We would like to thank Tabatha Cooper for suggesting the name for this
algorithm. Mention of trade names or commercial products in this article is
solely for the purpose of providing speciﬁc information and does not imply
recommendation or endorsement by the US Department of Agriculture. The
USDA is an equal opportunity provider and employer.

Funding

D.M.B., G.E.L., T.S.S., C.V.T. and 5.5. were supported by USDA CRIS pro—
ject numbers 1265-31000—096-00D and 1265-31000-104-00D. G.E.L. was
partially supported by AFRI grants no. 2011-67015-30183 from the USDA
NIFA. ].F.T. and R.D.S. were supported by the Agriculture and Food
Research Initiative competitive grant nos. 2011-68004-30214, 2011-68004-
30367 and 2013-68004-20364 from the USDA National Institute of Food
and Agriculture Animal Genome Program.

Conﬂict of Interest: none declared.

References

Alkan,C. et al. (2009) Personalized copy number and segmental duplication
maps using next-generation sequencing. Nat. Genet., 41, 1061—1067.

Bickhart,D.M. et al. (2012) Copy number variation of individual cattle gen-
omes using next-generation sequencing. Genome Res., 22, 778—790.

DePristo,M.A. et al. (2011) A framework for variation discovery and genotyp-
ing using next-generation DNA sequencing data. Nat. Genet., 43, 491—498.

Durkin,K. et al. (2012) Serial translocation by means of circular intermediates
underlies colour sidedness in cattle. Nature, 482, 81—84.

Hach,F. et al. (2010) mrsFAST: a cache-oblivious algorithm for short-read
mapping. Nat. Methods, 7, 5 76—5 77.

Hormozdiari,F. et al. (2009) Combinatorial algorithms for structural variation
detection in high-throughput sequenced genomes. Genome Res., 19,
1270—1278.

Karakoc,E. et al. (2012) Detection of structural variants and indels within
exome data. Nat. Methods, 9, 176—178.

Korbel,]. et al. (2009) PEMer: a computational framework with simulation-
based error models for inferring genomic structural variants from massive
paired-end sequencing data. Genome Biol., 10, R23.

Layer,R.M. et al. (2014) LUMPY: a probabilistic framework for structural
variant discovery. Genome Biol., 15, R84.

Liu,G.E. et al. (2010) Analysis of copy number variations among diverse cattle
breeds. Genome Res., 20, 693—703.

Mills,R.E. et al. (2011) Mapping copy number variation by population-scale
genome sequencing. Nature, 470, 59—65.

Quinlan,A.R. and Hall,I.M. (2010) BEDTools: a ﬂexible suite of utilities for
comparing genomic features. Bioinforma. Oxf. Engl., 26, 841—842.

Rausch,T. et al. (2012) DELLY: structural variant discovery by integrated
paired-end and split—read analysis. Bioinformatics, 28, i333—i339.

Vazirani,V. (2001) Approximation Algorithms. Springer-Verlag, Berlin,
Germany.

Wright,D. et al. (2009) Copy number variation in intron 1 of SOX5 causes the
pea-comb phenotype in chickens. PLoS Genet., 5, e1000512.

Ye,K. et al. (2009) PINDEL: a pattern growth approach to detect break points
of large deletions and medium sized insertions from paired-end short reads.
Bioinformatics, 25, 2865—2871.

Zhang,L. et al. (2014) Detection of copy number variations and their effects in
Chinese bulls. BMC Genomics, 15, 480.

Zimin,A.V. et al. (2009) A whole-genome assembly of the domestic cow, Bos
taurus. Genome Biol., 10, R42.

ﬁm'sreumol‘piqxo'sopeuuowtotq/ﬁdnq

