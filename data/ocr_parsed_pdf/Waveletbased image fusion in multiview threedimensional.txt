ORIGINAL PAPER

Vol. 28 no. 2 2012, pages 238-245
doi:10. 1 093/bioinformatics/btr609

 

Systems biology

Advance Access publication November 9, 2011

Wavelet-based image fusion in multi-view three-dimensional

microscopy

Jose L. Rubio—Guivernaulag‘k, Vasily Gurchenkovs, Miguel A. Luengo-Orozla2,
Louise Duloquin3, Paul Bourgine4, Andres Santos1’2, Nadine Peyrieras3’* and

Maria J. Ledesma-Carbay01’2a*

1Biomedical Image Technologies, ETSI Telecomunicaci n, Universidad Polit nica de Madrid, Madrid, 2Centre de
Investigaci n Biom dica en Red en Bioingenier a, Biomateriales y Nanomedicina (ClBER—BBN), Spain, 3Institut des
Syst mes complexes & NeD, Institut de Neurobiologie Alfred Fessard, CNRS, Gif—sur—Yvette and 4Institut des

Syst mes complexes & CREA, cole Polytechnique, CNRS, Paris, France

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: Multi-view microscopy techniques such as Light-Sheet
Fluorescence Microscopy (LSFM) are powerful tools for 3D + time
studies of live embryos in developmental biology. The sample is
imaged from several points of view, acquiring a set of 3D views that
are then combined or fused in order to overcome their individual
limitations. Views fusion is still an open problem despite recent
contributions in the field.

Results: We developed a wavelet-based multi-view fusion method
that, due to wavelet decomposition properties, is able to combine the
complementary directional information from all available views into
a single volume. Our method is demonstrated on LSFM acquisitions
from live sea urchin and zebrafish embryos. The fusion results show
improved overall contrast and details when compared with any of the
acquired volumes. The proposed method does not need knowledge
of the system’s point spread function (PSF) and performs better than
other existing PSF independent fusion methods.

Availability and Implementation: The described method was
implemented in Matlab (The Mathworks, Inc., USA) and a graphic
user interface was developed in Java.

The software, together with two sample datasets, is available at
http://www.die.upm.es/im/software/SPlMFusionGUl.zip

A public release, free of charge for non-commercial use, is planned
after the publication of this article.

Contact: jlrubio@die.upm.es; nadine.peyrieras@inaf.cnrs-gif.fr;
mledesma@die.upm.es

Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on June 9, 2011; revised on October 11, 2011; accepted
on October 29, 2011

1 INTRODUCTION

1.1 Multi-view 3D microscopy on live embryos

3D+time images of ﬂuorescently labeled cells in live model
organisms are essential to developmental biology (Dzyubachyk
et al., 2010; Muzzey and van Oudenaarden 2009; Truong and
Supatto 2011). When this kind of images is acquired with enough

 

*To whom correspondence should be addressed.

temporal and spatial resolution, tracking of every single cell and
reconstructing the cell lineage tree becomes possible (Olivier et al.,
2010; Swoger et al., 2010). Typical optical microscopy techniques
(like confocal or two-photon laser scanning microscopy) provide
images with resolution along the optical axis considerably worse
than lateral resolution. Moreover, image quality gets progressively
worse as light travels deeper inside the specimen, meaning that for
relatively large specimens (such as zebraﬁsh embryos or larvae) it
is impossible to get good images of the whole embryo.

To overcome this kind of limitations other microscopy techniques
have been developed. Spinning-disk microscopy (Graf et al., 2005),
for instance, provides much higher imaging speed compared to
scanned techniques. In theta-microscopy (Stelzer, 1994), separate
illumination and detection optical paths along orthogonal directions
are used in order to improve the axial resolution. In a more
recent approach called Light-Sheet-based Fluorescence Microscopy
(LSFM) (Huisken et al., 2004; Huisken and Stainier, 2009) wide-
ﬁeld detection is combined with a lateral light-sheet illumination
along the focal plane of the detection objective. LSFM is claimed to
offer several advantages over conventional confocal laser scanning
microscopy (CLSM) technique:

0 Unlike CLSM where a large part of the specimen is illuminated
when recording a single point or plane, LSFM illuminates only
the imaged plane, greatly reducing the overall photo-damage
of the specimen.

0 The system’s point spread function (PSF) is the combination
of the light-sheet shape and the detection objective’s PSF. For
this reason, when low numerical aperture detection lenses are
used, LSFM achieves signiﬁcantly better axial resolution than
other techniques such as confocal or two-photon ﬂuorescence
microscopy.

0 In LSFM wide-ﬁeld detection is used, meaning that a full
plane of the volume is acquired at once by an array of
detectors (typically a CCD camera). In contrast, laser scanning
microscopy (either confocal or two-photon) acquires one pixel
at a time. This means that, for identical frame rates, LSFM
has much longer per-pixel measurement time, which translates
in a higher number of collected photons per-pixel, higher
signal-to-noise ratio and dynamic range.

 

238 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /3.Io's[sumoIpJOJXO'soiwuiJOJuioiq”:duq wort pQPBOIII/lAOG

9103 ‘Og anﬁnv uo ::

Wavelet-based image fusion in multi-view 3D microscopy

 

 

Fig. 1. A rendering of the fusion of the three different views of a sea urchin
embryo. acquired at ~20h post-fertilization (hpf) is shown at the center.
Renders of the individual views are shown in a smaller scale around the
fusion.

Another important feature of typical LSFM implementations is their
ability to perform multi-view imaging, i.e. to obtain several volumes
of the sample with different orientations. This is usually achieved
by mounting the specimen under study on a rotating stage, and
it takes advantage of the large working distance of water dipping
lens objectives or of relatively low NA objectives, which makes
the implementation of the rotation stage easier. Each view is a 3D
volume formed as a stack of planes acquired sequentially. General
schemes of LSFM can be found in the literature (Huisken and
Stainier, 2009) and in Supplementary Figure S1. Such multi-view
acquisition capability is desirable because, despite the previously
mentioned advantages of LSFM over other ﬂuorescence microscopy
techniques, single views still show a series of problems (see
Supplementary Fig. S2), like an axial resolution still worse than
in plane resolution, slices increasingly dim and blurred with depth,
etc., which will be further described below.

1.2 Multi-view fusion techniques

The multi-view imaging capability of LSFM provides extra
information with respect to a single-view approach, as those regions
of the specimen that are acquired with lower quality in one view will
appear sharper and brighter in a different view.

To be fully useful, the information of the sample distributed
among several volumes by multi-view imaging should be combined
into a single volume (Fig. 1). Several techniques have been recently
proposed for this task (Krzic, 2009; Preibisch et (11., 2008 ; Swoger
et (11., 2007; Temerinac-Ott et (11., 2011).

One approach is to pose the problem as a multi-view
deconvolution, which can be solved using extensions of classic
iterative deconvolution algorithms like Richardson—Lucy (Krzic,
2009) or Maximum A Posteriori (Swoger et (11., 2007) to the
multi-view situation, by updating the estimate using one view at
a time.

This kind of algorithms performs ﬁne when the actual PSF is
well approximated and used in the deconvolution process. Its main
drawback is precisely the need for a good estimate of the PSF, which

is particularly difﬁcult to obtain when the PSF is spatially variant
as observed in the LSFM imaged volume due to the shape of light
sheet and thickness of specimens. In (Temerinac-Ott et (11., 2011)
a spatially variant Richardson—Lucy is proposed, where the PSF is
dynamically estimated based on ﬂuorescent beads embedded in the
sample. Finally, it is worth mentioning that PSF-based algorithms
require an extremely precise registration of the views.

A different approach called content-based fusion (Preibisch et (11.,
2008) was recently proposed in which the fused volume is the
weighted average of all the available views, using the local entropy
of each view as weights. This method is very fast and the results
provided show clear improvement with respect to each of the
acquired views.

We propose here an alternative method for multi-view fusion
on the Discrete Wavelet Transform (DWT) (Mallat, 2008) space.
An advantage of such approach when compared to multi-view
deconvolution is the fact that it does not require a PSF estimate.

Moreover, due to the use of multi-band transforms that decompose
each acquired view onto several scales and orientations, our method
is able to select the best view independently along each band and can
thus be considered as a multi-scale and multi-orientation content-
based fusion.

This enhanced orientation discrimination feature is very valuable
for LSFM data, as in a given region of the sample two or
more views often provide useﬁil information, each one along a
different orientation (an example of such situation is provided in
Supplementary Figure S3). In this kind of situation, the image-space
weighted average performed by the content-based fusion method
is unable to independently discriminate directional information
components on each view, while the proposed method can do so
by working in the wavelet-space.

2 METHODS

2.1 Problem description

The proposed method is designed to deal with the multi-view volume
fusion problem in LSFM. There are mainly two ways in which different 3D
ﬂuorescence microscopy acquisitions can be complementary in a multi-view
setting (Swoger et (11.. 2007):

0 Even though LSFM offers a relatively small axial PSF. due to the
use of orthogonal illumination and detection axes. axial resolution
remains usually worse than in-plane resolution. The in-plane and axial
resolution throughout the specimen’s volume change roles while it
rotates around an axis orthogonal to the detection axis. Two orthogonal
views offer the most complementary information in terms of PSF.

For highly diffusive samples (which applies to living tissues even when
relatively transparent) excitation and emission light far from the excitation
and detection objective respectively gets severely attenuated/absorbed as it
travels through the sample. which results in increasingly degraded image
quality deeper into the sample. For large specimens such as the zebraﬁsh
embryo or larva. each View will contain useful information for less than half
of the sample. The complementarity of views taken from different angles to
compensate for this effect is quite clear.

The goal of our method is to take a set of volumes. each one corresponding
to a different View of the same sample (e.g. live embryo or larva of chosen
animal models). and combine the complementary information available in
all of them to create a new fused volume of a better overall quality than any
original View alone.

 

239

112 /3.Io's[sumoIpJOJXO'soiwuiJOJuioiq”:duq 11101} papeolumoq

9103 ‘Og anﬁnv uo ::

J.L.Rubio-Guivemau et al.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

—_R

(a) (bl/Create Registered Volunms anti Masks \ (c)

K' 0“ f i‘ \"
v «'1‘ “ 6.999199 <
E F “4‘  . . _ . . . _ . _ . . . _ . - . ‘ gab w‘ﬂ‘ o_
a E «° I i «V 5
g . to
g. E _ I I T T T“ _

>0 total I  ""-" o
. l _ . _ . _ . _ . _ . _ . _ ._. =
3 = ............... _. <
3 — I l \ :—
_ a |"“’ - I ’
a m .—:o fl *' E
° n I . '
Gray-Level Registration 1 > “3" l . _ . _ . _ . _ . _ . _ . _._! . Bead-Based Registration .

\ / Ragistarad M k \n /

., + Padding as 8 .-
\_‘_ /'

Fig. 2. Overview of the pre-processing workﬂow. On the left (a) and on the right (b) we ﬁnd the two possible registration methods. while the ﬁnal step
is depicted in the middle (c). For gray-level registration (a) two additional steps are performed: automatic cropping and pre-alignment. while bead-based
registration (b) is directly performed on the acquired volumes. With either registration. the output of this step is a total transformation matrix for each of the

input volumes. which is then used (c) to create the registered volumes and masks that will be passed to the fusion method. Both the registered volumes and
the masks are padded as needed so that all of them have the same dimensions and all their information is preserved.

2.2 Pre-processing

Several pre-processing steps might be performed before applying the fusion
method to the data. e.g. cropping the volumes and. for very large datasets.
down-sampling the in-plane resolution to lower the computation time.
However. the only necessary step that must be performed before the fusion
is the alignment or registration of all the acquired views. because any
miss-registration will lead to artifacts in the ﬁnal fused volume.

The rotation angle of the sample mount is known from the acquisition
setup. But it only provides a coarse estimation of the actual transformation
between views. Several artifacts including the lack of precision and accuracy
in the rotation calibration and operation (Krzic, 2009) make a ﬁne registration
necessary.

Fine registration of LSFM images is quite challenging. as the depth-
dependent blurring means that the registration procedure is complicated
by information differences between the views. Furthermore. for large
specimens. some regions can be totally obscured because of light scattering
as described above. To face these problems. more views might be required
to ensure sufﬁcient overlap of usable information and external cues such
as ﬂuorescent beads in the mounting medium might be required (Preibisch
et al.. 2010). Two different approaches have been used during this work.
depending on the characteristics of the dataset:

0 A bead-based afﬁne registration algorithm (Preibisch et al.. 2010) was
used for datasets with ﬂuorescent beads embedded in the mounting
medium around the specimen.

0 A gray-level afﬁne registration algorithm (Thevenaz et al.. 1998) for
datasets with no ﬂuorescent beads. which uses just the information from
the image intensity values.

For small embryos such as sea urchin. bead-less sample preparation and
thus gray-level registration is preferred. However. 3D volumes acquired
from large embryos like zebraﬁsh show extensive blurring and eventually
little overlap between views. and we conﬁrm that the bead-based registration
proposed previously (Preibisch et al.. 2010) is a useful strategy. The detailed
pre-processing scheme used for each of the registration approaches is
depicted in Figure 2.

For the gray-level registration. each of the acquired volumes is ﬁrst
automatically cropped in order to reduce as much as possible the data size
and thus the time and memory consumption of the registration. Due to the
low noise level of LSFM images. their background is quite uniform and
such automatic cropping works well. Each cropped volume is then pre-
aligned according to the rotation step conﬁgured during the acquisition. and
the pre-aligned volumes are then passed to the gray-level afﬁne registration
algorithm. The transformation matrices corresponding to pre-alignment and

ﬁne registration are then composed to produce the total transformation matrix
of each input volume.

On the other hand. when bead-based registration is selected no initial
cropping is used. mainly because it would not make sense to remove the
beads needed for the registration algorithm to perform its task. No pre-
alignment step is needed either for this kind of registration. as it performs
well directly on the acquired volumes. Once the total transformation matrix
has been computed for each of the views (by composing pre-alignment and
ﬁne registration matrices in the gray-level case. or directly obtained from
the bead-based registration otherwise) they are applied to the original views
in order to obtain the registered volumes. which are padded as needed in
order to make all of them have the same dimensions. In parallel to this
transformation. binary masks are also computed that indicate which voxels
of each registered volume contain actual acquired data. These masks will
be later used to prevent border artifacts. as it will be further explained
later.

In both approaches. the ﬁrst View is arbitrarily chosen as a reference. and
either registration method is used to compute the afﬁne transform matching it
with the other views. When all the views have been brought to the reference
frame coordinate system. the fusion process can start.

2.3 3D wavelet fusion

There is a family of techniques named image fusion (Piella. 2003). typically
used on 2D images. which can effectively combine information from
different sources into a single composite image.

Among these techniques. wavelet-based image fusion (Li. 1995) is one of
the most widely used. and has already found some applications in biomedical
imaging (Rajpoot et al.. 2009).

The main idea behind this approach is to take advantage of the
properties of multi-band image decomposition schemes like the discrete
wavelet transform. In this kind of decompositions. an input image (or
volume) X0 is decomposed onto several bands (each one corresponding
to a speciﬁc scale and orientation. for instance) W (X 0) 2 {Y1, . . . , YK ,XK 
where Yk , k = 1...K are the different bands in which the image is decomposed
and X K is the residual low-pass approximation of X 0.

Salient features in the input image X0 become high-energy coefﬁcients
in at least one of the decomposition bands Yk. while smooth regions
become low- or zero-energy coefﬁcients. So. roughly speaking. by applying a
common multi-band decomposition to all the input images available. and then
choosing the highest energy coefﬁcients. a fused image combining salient
features from all input images is obtained.

The detailed process is outlined in Figure 3a and the different steps are
further described below. Before explaining the different steps. we recall the

 

240

112 /3.Io's[sumoIpJOJXO'soiwurJOJuroiq”:duq moi; paprolumoq

9103 ‘Og anﬁnv uo ::

Wavelet-based image fusion in multi-view 3D microscopy

 

 

 

 

 

 

 

 

 

 

 

         
  
   
 

mm

__ . _ . _ . _ . _ . _ .l Xivs /Y1
l . + '- -._. H
'X0 . .. | .1.) I" 
i 1  E14  ' .,_ Decision
. I : = “a Maps
E Vomms 1  Wavelet Activity \g _

. Decomposition 1 Measure 1 "

Mask 1  Multi-scaie
‘_ . _ . _ . _ . _ . _ .| Masks 2

 

 

 

 

 

 

 

 

 

 

! mum ' Activity
Measure |

Wavelet
E - Decomposition l

i l a

- I
g Mask! - /Multi-scaie ‘\
- . _ - — s - - - ‘ - -' Masks | K )1

(a)

 

 

 

 

 

 

I 1-1. 1,2
YF' Yr

Fusion
Decomposition

 

 

Volume

 

(b)

   

W(X")

 

Fig. 3. (3) Overview of the proposed fusion method. The inputs are a set of volumes containing the registered views X190 2 1...N) and a corresponding set

of masks K196: 1...N). Each of the registered views is then passed to the wavelet transform of choice, generating the wavelet decomposition bands Yiﬂ’m
(where n: 1...N is the scale index and m = 1 ...7 the orientation index) and the low-pass approximation residuals lev . In parallel the masks are also processed

to get the multi-level masks Kf(n=1...N) needed later. An activity measure a?” is computed for each of the wavelet coefﬁcients, and by looking at those
activity measures together with the multi-level masks the decision maps  are computed. Finally, the wavelet coefﬁcients are combined according to the

decision maps, giving place to the fusion decomposition {Yli’1,...,Y2/’7,Xfrv  and by applying the inverse transform we get the fused volume X); as output.

(b) Wavelet decomposition example of a 2D slice, using three scales and three orientations, resulting in nine bands (N = 3 and K :9). Although in this work
we use undecimated wavelet transform, for illustration purposes this ﬁgure represents the classical decimated wavelet transform.

inputs of the fusion method:

0 A set of I volumes  (i = 1 ...I), each of them containing a different
3D View of the specimen, which have already been registered, using
the ﬁrst volume X? as reference.

0 The corresponding set of binary masks K19, each of them indicating
which voxels on each of the volumes X19 contain relevant information.
These masks are used, for instance, to tell which voxels of a registered
volume come from the actual transformed volume, and which ones are
just padding, allowing us to avoid border artifacts due to incomplete
specimen coverage in some of the views.

Transform input volumes: we ﬁrst apply the wavelet transform of choice
to each of the input volumes:  = {Y}, ..., Yl-K,XiK}, where i=1...I is
the View index. Depending on the speciﬁc wavelet transform chosen, there
will be a different number and arrangement of the bands. If 3D DWT is
selected, we get 7 different orientations on each scale, so when N scales are
used during the decomposition, we end up with K = 7 .N bands. In this case
it is more practical to index the bands by scale (n: 1...N) and orientation
(m = 1 . . .7):
W(X§’) ={Y.1’1,...,Y5”m,YfV’7,XfV}

l l I

Figure 3b shows an example of the decomposition of a single slice of a LSFM
Volume, using a 2D DWT with three scales (N :3) and three orientations
on each scale, resulting in K23 ~N=9 bands.

The size of the bands will depend on the precise wavelet transform used.
For instance, when using decimated DWT, the bands’ size is halved on each
level, so that the total number of elements in the whole decomposition
 is always equal to that of X9. On the other hand, undecimated
implementations of DWT, keep the size of each Yf’m and lev equal to
X? ’5 size, so each additional level increases the total number of elements

in the decomposition. Undecimated implementations are usually preferred
for fusion applications due to its shift-invariance property (Amolins et al.,
2007; Gonzalez-Audicana et al., 2004; Redondo—Tejedor, 2007).

Generate multi-seale masks: parallel to the transformation of the input
volumes, the corresponding masks are processed in order to prepare adequate
multi-scale masks: Kﬂn: 1...N). These multi-scale masks are designed to
ensure that only valid voxels from the input volumes are taken into account
when generating the fused volume.

For this purpose we deﬁne the support of any given wavelet coefﬁcient
(Yin’m(-) or X1510), as the set of voxels in the corresponding input volume
which get involved in the computation of said coefﬁcient. The size of the
support will depend on the scale n and on the wavelet family used. When
using haar wavelets, for instance, the support will be a 2 x 2 x 2 sub-volume
for coefﬁcients in scale 1, a 4 x 4 x4 sub-volume for scale 2, and so on.

If we denote by S (Xf(-)) the support of the coefﬁcient X;1 (t ), the multi-
scale masks are built so that K151(-) takes the value 1 if the original mask K19
is equal to 1 at all the voxels v belonging to S (X?()), and otherwise it takes
the value 0, that is:

[(510)2{1 if K?(y)=1,VveS(Xll1(~))
0 otherw1se
Supplementary Figure S4 shows an example of how the use of these multi-
level masks prevents the appearance of artifacts when some of the acquired
views do not fully cover the specimen.

Measure activity: the next step is to compute an activity or saliency
measure at every location of each band from all the wavelet decompositions.
This will later be used to decide which of the wavelet coefﬁcients are related
to salient features in the input volumes, so that they should be included in the
fused volume. A common and simple choice, providing very good results,

 

241

112 /8.IO'S[BHmOprOJXO'SOIJBLUJOJIIIOlq”Idllq morj popeonimoq

9IOZ ‘OE ISUEHV Ho ::

J.L.Rubio-Guivemau et al.

 

is to use the energy or the module of wavelet coefﬁcients. In this context.
the activity measure for any given wavelet coefﬁcient Y1." "" (- ) is:
61:1,!"  )2 ‘Yinnn  

Computation of decision maps: this is the most important step in the process
depicted in Figure 3a. and it might be considered the core of the fusion
method. We mean by decision maps the weights that deﬁne how. for each
location in every band of the wavelet decompositions (including the residual
low-pass approximation). the coefﬁcients from all the input volumes are
combined to create the fused volume.

As already mentioned. these kind of methods aim at keeping the most
salient features. at different scales and orientations. among the input volumes.
Having the activity measure from the previous step. a straightforward way
to implement such behavior is to create decision maps My” that. for each
location. scale and orientation. select the maximum activity measure from
the different input volumes. The multi-level masks are taken into account in
order to avoid selecting coefﬁcients whose support includes non-valid voxels
from the input volumes. This can be achieved with the following decision
map equation:

1 M?” (- 1K,” (- ﬁgs, [a;‘”" (- 1K; (- >1

0 otherwise

Mlnmt  )2

However. this deﬁnition is not adequate for using on every decomposition
band. For instance. depending on the size of the objects present in the input
volumes. some scales of their wavelet decomposition might be dominated
by noise. so selecting the maximum coefﬁcients on those bands would lead
to increased noise in the fused volume. As an alternative for bands featuring
noisy components. selecting the minimum activity coefﬁcients instead of
the maximum leads to better results. Supplementary Figure S5 shows a
comparison demonstrating how using minimum activity selection on some
bands can improve the results.

In order to take full advantage of the orientation and scale discrimination
capability of the wavelet transform. it is necessary that the decision maps are
able to select. in a given region of the image space. information from different
views for different decomposition bands. In this sense. the local entropy
maps used in content-based fusion. as seen in ﬁgure 6 from (Preibisch et al..
2008). are too smooth. For this reason. in this work more local decision
maps like the ones we just deﬁned are preferred. In order to support this
statement. in Supplementary Figure S6 we show the result of combining
wavelet decomposition with local entropy maps. which gives results almost
identical to content-based fusion itself.

With respect to the low-pass approximations corresponding to each input
volume. they should contain similar information after extracting the relevant
features into the different bands. For this reason it is usually preferred to
average all the low-pass approximations. taking into account only those
coefﬁcients whose associate multi-level mask equals 1. which can be
implemented by deﬁning a different decision map equation. which is simply:
: K? (->

212.19%)

Other possible strategies are easy to incorporate into the framework by just
creating alternative deﬁnitions of the decision maps.

Combination of wavelet eoeﬁ‘ieients: once the decision maps are ready.
the next step is to combine the coefﬁcients from all the input volume
decompositions. generating the wavelet decomposition of the fused volume:

[ n,m n,m
 (_ )_ 2):. (M,- (- >- Y,- (- >>
F _ [ n,m
21':er (‘)
As for the residual low-pass approximation of the fused volume XN . it is
computed in the same way by averaging the low-pass approximations of all
the available volumes XlNJ: 1...I.

Inverse transform: ﬁnally. the wavelet decomposition generated in the
previous step is inverted to get the ﬁnal fused volume.

—1 1,1 , N,7 N 0
W ({Y ,...,Y£’”,YF ,XF})=XF

Mlnmt 

Table 1. Performance evaluation of both registrations and both fusion
methods

 

 

Step Method Time (min/view) Fixed Time (min)

Registration Gray-level 24.5 0
Bead-based 1.8 0

Fusion Wavelet 64.7 19.2
Content-based 1.5 0

 

All tests were performed on a Intel® Xeon® E5506@2.13 GHz with 48 GB RAM.

Table 2. Acquisition parameters for the two datasets used for this work

 

Animal Objective Volume size Voxel size (am) No. of No. of
[Dataset ID] (voxels) views time
[angle] steps

 

Sea urchin Zeiss
[09091668] 40></1.0NAW
Zebraﬂsh Zeiss
[100728215] 10></0.3NAW

972x972><82 0.185><0.185><1.85 3[120°] 406

600x600><111 1.48><1.48><5.55 5 [72°] 281

 

2.4 Algorithm implementation and performance

The whole algorithm has been implemented in MATLAB®. using a custom
3D extension of its undecimated discrete wavelet transform implementation
(swt and iswt functions).

Table 1 summarizes the time consumed by the two main parts of the overall
process: registration and fusion. Times are provided for the two registrations
used and for both the proposed fusion method and the content-based fusion
included in the ‘SPIM Registration” plug-in of Fiji (http://www.ﬁji.sc).

The results are averages based on several timesteps from each dataset.
and to allow comparison despite different volume sizes. times have been
normalized to a common volume size of 600 X 600 X 600. For wavelet-based
fusion. the ﬁnal inverse transform step is independent of the number of views.
so the time has been split in a per-view time plus a ﬁxed time.

The current implementation of the proposed method is quite more time-
consuming than content-based fusion. We expect a 5- to 10-fold reduction in
computation time when the method is recoded in C. Besides all the process
is based on ﬁlters implemented as convolutions with rather small kernels. so
the code should greatly beneﬁt from parallelization schemes.

2.5 Dataset description

Datasets used in this work were acquired using a Digital Scanned Light-Sheet
(DSLM) version of the LSFM microscope (Keller et al.. 2008) where the light
sheet is generated by scanning a light beam instead of using a cylindrical
lens as in the Selective Plane Illumination Microscope (SPIM) (Huisken
et al.. 2004). Our method was applied to various kinds of datasets. The
results presented here correspond to two datasets obtained from developing
sea urchin (Paraeentrotus lividus) and zebraﬁsh (Danio rerio) embryos.
Acquisition parameters are summarized in Table 2.

3 RESULTS

3.1 Live sea urchin embryo

A live sea urchin embryo was imaged for 20h, from 5h post
fertilization (hpf) until 25 hpf. The whole acquisition time was
divided in 180 s intervals, leading to over 400 time steps. At each
time step 3 volumes (i.e. views) were acquired, using a 120°
rotation of the sample between consecutive views. On all the
acquired volumes, signal intensity relates to the local concentration
of ﬂuorescent protein accumulated in the cell nucleus.

 

242

112 /3.Io's172an0prOJxosorwurJOJurorq”:duq 111011 papBo1umoq

9103 ‘0g15n8nv uo ::

Wavelet-based image fusion in multi-view 3D microscopy

 

 View 1 I View 2  ‘ View 3

 

Fusion

Fig. 4. Results of the proposed method on a sea urchin embryo. imaged at 20 h post-fertilization (hpf). Each column shows three orthogonal slices from a
single volume. Columns 13 contain slices from each of the three acquired views. while column 4 contains slices from the volume obtained by our fusion
method. Rows 173 show slices along XY. X2 and Y2 planes. respectively. At the bottom of the ﬁgure. three line proﬁles compare each of the individual
views with the fusion. On each proﬁle. the blue plot represents intensity values on the corresponding View. while the yellow line follows intensity on the fused
volume. Line segments have been overlaid to the X2 slices (middle row). in parallel to the group of ﬁve cells represented by the proﬁles.

The sample did not contain ﬂuorescent beads in this study,
so the image registration method used before the fusion is the
aforementioned gray-level afﬁne registration (Thevenaz et (11.,
1998).

After views registration, we applied the wavelet fusion method
as described in Section 2.3. A 3D undecimated DWT was used
as  , with haar wavelet ﬁlters and three scales (N :3).

Regarding the different possibilities for computation of decision
maps (as discussed in Section 2.3), the following conﬁguration
parameters were used for this dataset:

° Minimum activity selection for bands in scale 1 (the ﬁnest)
which is dominated by noise.

° Maximum activity selection for bands in scales 2 and 3, which
contain the most relevant detail information.

° Averaging of the low-pass approximations.

Figure 4 shows an example of the fusion performance, by comparing
slices and proﬁles of the fused volume and the acquired views
corresponding to a time step around 20 hpf.

Each of the three views provides an incomplete picture of the
embryo, with some regions showing sharp and bright nuclei, while

others exhibit heavy blurring and even some missing nucleus.
Nevertheless, slices from the fused volume present overall sharp
nuclei with good contrast. At the bottom of the ﬁgure line proﬁles
show that the fusion provides the best representation of cell
populations, because even though each View might show increased
brightness and contrast for some cells, they might completely miss
other ones that are indeed recovered in the fused volume.

The whole dataset (406 time steps) was processed similarly. A
complete View of the results for this dataset is showed in the video
displaying the evolution of the embryo during the 20 h of the imaging
procedure (Supplementary Movie S1 l)‘. Supplementary Movie S11
shows the improvement of individual cell identiﬁcation in the fused
volume that becomes even more obvious as the embryo grows and
the individual views are unable to properly capture the details of the
whole embryo.

3.2 Live zebraﬁsh embryo

The second example is a live zebraﬁsh embryo imaged for 12h,
starting at 5 hpf. The whole acquisition time was divided in

 

lDirect download available at http://www.die.upm.es/im/videos/SPIM/
Movie_Sll.avi (last accessed date November 27. 2011).

 

243

112 /§.IO'8112111110[p.IOJIXO'SOlIRu110‘1H1Olq/ﬂdnq 1110.11 popco1umoq

9103 ‘0g anSnV uo ::

J.L.Rubio-Guivemau et al.

 

155 s intervals, leading to over 280 time steps. At each time step
ﬁve volumes (i.e. views) were acquired, using a 72° rotation of
the sample between consecutive views. In all the acquired volumes,
signal intensity correlates with the ﬂuorescent protein concentration
in the cell nucleus.

Fluorescent beads were added to the mounting medium, so that
the bead-based registration method could be used. This means that
the ﬁve acquired views show little overlapping information as shown
in Supplementary Figure S7. For this reason, standard gray-based
registration would be very challenging for this dataset, and bead-
based registration is preferred.

The last column in Supplementary Figure S7 shows the result of
fusing the ﬁve available views (columns 1—5). The conﬁguration
parameters used for this dataset, including wavelet family, number
of scales and decision maps computation scheme, were identical to
the ones described in Section 3.1 for the sea urchin dataset.

The fusion represents the ﬁlll embryo better than any of the
individual volumes, and, while some regions of the embryo are
brighter in one speciﬁc view, the fusion captures all the relevant
information, keeping good contrast whenever such contrast exists in
any of the views (Supplementary Movie S12)2 It is worth mentioning
that, due to the size of this embryo, each view appears highly blurred
in some regions, but the fusion method is able to deal with this
situation and gives preference to non-blurred information from other
views.

Supplementary Figure S8 shows snapshots of the renders
available in the video, with the ﬁve original views around the
resulting fused volume. For both Supplementary Figure S8 and
Movie S12, a hard threshold was applied to the original and ﬁised
volumes in order to reduce the number of visible beads in the renders.

3.3 Comparison with existing methods

For both datasets, we have also compared our results with those
of the previously mentioned content-based weighted averaging
(Preibisch et (11., 2008), which is also PSF independent and freely
available as part of Fiji open-source image processing package.

The comparison between our method and Preibisch et (11 .’s method
is shown in Supplementary Figure S9 for the sea urchin embryo,
and in Figure 5 for the zebraﬁsh. The results of the wavelet-based
fusion method look less blurred, and more individual nuclei can be
distinguished. The latter is supported by the line proﬁle comparison
through a group of three nuclei in a blurred region (Fig. 5). For
the sea urchin, the difference is less striking, but the line proﬁles in
Supplementary Figure S9, reveals slightly better peak contrast with
our wavelet fusion method (in blue) compared to the content-based
fusion (in yellow).

Additionally, a quantitative comparison was carried out on two
different timesteps for both datasets, by measuring the contrast
of several line proﬁles, each one passing through two adjacent
nuclei. An overall 31% improvement was measured for the proposed
method in comparison to content-based fusion. The details are
provided in Supplementary Figure S10.

 

2Direct download available at http://www.die.upm.es/im/videos/SPIM/
Movie_S12.avi (last accessed date November 27. 2011).

Content-
Based
Fusion

Wavelet
Fusion

 

Fig. 5. Comparison of content-based fusion (left column) and our fusion
method (right column) for the zebraﬁsh embryo. same slices as in
Supplementary Figure S7. At the bottom. a line proﬁle comparison of both
(content-based fusion in yellow and our fusion in blue) across a group of
three cells in a blurred region. surrounded by insets showing more detail of
the areas where the line proﬁle was obtained.

4 DISCUSSION

We propose a novel methodology to ﬁise multi-view images, such
as those obtained from light-sheet-based ﬂuorescence microscopy,
but not limited to that particular technique. Almost any imaging
technique in which several volumes of the same object are acquired,
each of one providing incomplete and complementary information,
could beneﬁt from the proposed fusion scheme, including recent
light-sheet techniques like DSLM using structured illumination
(Keller et (11., 2010).

Our method is based on wavelet multi-scale and multi-orientation
decomposition of the input volumes, and does not rely on a priori
knowledge of the system PSF. While PSF-aware methods have more
potential for reducing PSF-related artifacts, the proposed method can
be used on those situations in which such knowledge is not available
as is the case with LSFM acquisition where the PSF varies over the
volume due to light sheet shape and to distorsion caused by the
specimen itself.

With respect to the other PSF-independent method mentioned
in this work, content-based weighted averaging, the new method

 

244

112 /§.IO'8112111110[p.IOJIXO'SOlHluLIO‘1H1Olq/ﬂdnq 111011 papBo1umoq

9103 ‘0g15n8nv uo ::

Wavelet-based image fusion in multi-view 3D microscopy

 

has improved orientation and scale discrimination as a direct
consequence of performing the fusion in the wavelet domain instead
of the intensity space. This improved discrimination is useful for
instance in regions of the sample where two views contribute sharp
details in different orientations.

This work focuses on the fusion method itself, and does not
consider the effect of other complementary methods like denoising
techniques which could be used in a pre-processing step. For
instance wavelet denoising (Chang et (11., 2000; Donoho and
Johnstone 1994) and bilateral ﬁltering (Paris et (11., 2009; Yang
et (11., 2009) seem good choices for future tests. Some variations
(cross/joint and dual bilateral ﬁltering) have been used for image
fusion in certain situations (Bennett et (11., 2007), but they are not
adequate in situations where different views do not cover the full
specimen, as uniform regions in the image being ﬁltered will stay
uniform regardless of the information provided by the other views.

A possible way to further improve the useﬁilness of the proposed
method would be to automatically adapt the wavelet fusion
parameters, e.g. number of scales and fusion scheme, to the content
of the input volumes.

Moreover, the general scheme can be accommodated to any kind
of wavelet transform (different base wavelet basis, non-separable
decompositions, etc.), making the proposed method highly ﬂexible
and thus adaptable to any kind of input volumes.

ACKNOWLEDGEMENTS

chN2P3, Pascal Calvat and Jean-Yves Nief for data management,
BioEmergences project and Uros Kzric for the SPIM/DSLM setup
installation in Gif sur Yvette, Miguel Carmona for the SPIM/DSLM
software development.

Funding: Ministry of Science and Innovation (TEC2008-06715-
C02-02), Spain; Agence Nationale de la Recherche, France; Agence
pour la Recherche sur le Cancer (to NE), France; Region Ile
de France, France; Centre National de la Recherche Scientiﬁque,
France; FP6 New Emerging Science and Technology EC program
(Embryomics and BioEmergences projects), European Union;
European Fund for Regional Development (FEDER), European
Union.

Conﬂict of Interest: none declared.

REFERENCES

Amolins,K. et al. (2007) Wavelet based image fusion techniques. ISPRS J Photogramm,
62, 2497263.

Bennett,E.P. et al. (2007) Multispectral bilateral video fusion. IEEE T. Image Process,
16, 118571194.

Chang,S.G. et al. (2000) Adaptive wavelet thresholding for image denoising and
compression. IEEE T. Image Process, 9, 153271546.

Donoho,D.L. and Johnstone,I.M. (1994) Threshold selection for wavelet shrinkage of
noisy data. In Proceedings of 16th Annual International Conference of the IEEE
EMBS. IEEE, Baltimore, MD, pp. A247A25.

Dzyubachyk,0. et al. (2010) Automated analysis of time-lapse ﬂuorescence microscopy
images: from live cell images to intracellular foci. Bioinformatics, 26, 242442430.

Gonzalez-Audicana,M. et al. (2004) Fusion of multispectral and panchromatic images
using improved IHS and PCA mergers based on wavelet decomposition. IEEE T.
Geosci. Remote, 42, 129171299.

Graf,R. et al. (2005) Live cell spinning disk microscopy. Adv. Biochem. Eng.
Biotechnol, 95, 57775.

Huisken,J. and Stainier,D.Y.R. (2009) Selective plane illumination microscopy
techniques in developmental biology. Development, 136, 196371975.

Huisken,J. et al. (2004) Optical sectioning deep inside live embryos by selective plane
illumination microscopy. Science, 305, 100771009.

Keller,P.J. et al. (2008) Reconstruction of zebraﬁsh early embryonic development by
scanned light sheet microscopy. Science, 322, 106571069.

Keller,P.J. et al. (2010) Fast, hi gh-contrast imaging of animal development with scanned
light sheet-based structured-illumination microscopy. Nat. Methods, 7, 637$42.

Krzic,U. (2009) Multiple-View Microscopy with Light-Sheet based Fluorescence
Microscope. PhD Thesis, University of Heidelberg, Heidelberg.

Li,H. (1995) Multisensor Image Fusion Using the Wavelet Transform. Graph. Model
Im. Proc., 57, 2357245.

Mallat,S. (2008) A Wavelet Tour of Signal Processing. 3rd edn. Academic Press,
Burlington, MA.

Muzzey,D. and Oudenaarden,A. van (2009) Quantitative time-lapse ﬂuorescence
microscopy in single cells. Annu. Rev. Cell Dev. Biol, 25, 3017327.

Olivier,N. et al. (2010) Cell lineage reconstruction of early zebraﬁsh embryos using
label-free nonlinear microscopy. Science, 329, 9677971.

Paris,S. et al. (2009) Bilateral Filtering: Theory and Applications. Foundations and
Trends® in Computer Graphics and Vision Vol. 4, No 1, pp. 1773.

Piella,G (2003) A general framework for multiresolution image fusion: from pixels to
regions. Inform. Fusion, 4, 2597280.

Preibisch,S. et al. (2008) Mosaicing of single plane illumination microscopy images
using groupwise registration and fast content-based image fusion. Proc. SPIE, 6194,
69140E 1%9140E 8.

Preibisch,S. et al. (2010) Software for bead-based registration of selective plane
illumination microscopy data. Nat. Methods, 7, 418419.

Rajpoot,K. et al. (2009) Multiview RT3D Echocardiography Image Fusion. Springer,
pp. 1347143.

Redondo-Tejedor,R. (2007) New Contributions on Image Fusion and Compression
Based on Space-Frequency Representations. PhD Thesis, Universidad Politecnica
de Madrid, Madrid.

Stelzer,E. (1994) Fundamental reduction of the observation volume in far-ﬁeld light
microscopy by detection orthogonal to the illumination axis: confocal theta
microscopy. Optics Commun., 111, 53G547.

Swoger,J. et al. (2007) Multi-view image fusion improves resolution in three-
dimensional microscopy. Optics Express, 15, 8029.

Swoger,J. et al. (2010) 4D retrospective lineage tracing using SPIM for zebraﬁsh
organogenesis studies. J. Biophotonics, 4, 1227134.

Temerinac-Ott,M. et al. (2011) Spatially-variant Lucy-Richardson deconvolution for
multiview fusion of microscopical 3D images. In IEEE International Symposium
on Biomedical Imaging 2011, IEEE, Chicago, IL, pp. 8997904.

Thevenaz,P. et al. (1998) Apyramid approach to subpixel registration based on intensity.
IEEE T. Image Process, 7, 2741.

Truong,T.V. and Supatto,W. (2011) Toward high-content/high-throughput imaging and
analysis of embryonic morphogenesis. Genesis, 49, 5557569.

Yang,Q. et al. (2009) Real-time 0(1) bilateral ﬁltering. In IEEE Computer Vision and
Pattern Recognition, 2009, IEEE, Miami, FL, pp. 5577564.

 

245

112 /3.Io's172an0prOJxosorwuiJOJurorqp:duq 111011 papeo1umoq

9103 ‘0g15n8nv uo ::

