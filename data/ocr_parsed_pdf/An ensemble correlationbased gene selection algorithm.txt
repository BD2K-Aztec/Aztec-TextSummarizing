ORIGINAL PAPER

Vol. 28 no. 24 2012, pages 3306—3315
doi: 10. 1 093/bioinformatics/bt5602

 

Data and text mining

Advance Access publication October 11, 2012

An ensemble correlation-based gene selection algorithm for
cancer classification with gene expression data

Yongjun Piao‘, Minghao Piao‘, Kiejung Park2 and Keun Ho Ryu”

1Department of Electrical and Computer Engineering, Chungbuk National University, Chungbuk, Korea and 2Division of
Bio-Medical informatics, Center for Genome Science, Korea National Institute of Health, Osong, South Korea

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: Gene selection for cancer classification is one of the most
important topics in the biomedical field. However, microarray data
pose a severe challenge for computational techniques. We need
dimension reduction techniques that identify a small set of genes to
achieve better learning performance. From the perspective of machine
learning, the selection of genes can be considered to be a feature
selection problem that aims to find a small subset of features that
has the most discriminative information for the target.

Results: In this article, we proposed an Ensemble Correlation-Based
Gene Selection algorithm based on symmetrical uncertainty and
Support Vector Machine. In our method, symmetrical uncertainty
was used to analyze the relevance of the genes, the different starting
points of the relevant subset were used to generate the gene subsets
and the Support Vector Machine was used as an evaluation criterion
of the wrapper. The efficiency and effectiveness of our method were
demonstrated through comparisons with other feature selection tech-
niques, and the results show that our method outperformed other
methods published in the literature.

Availability: By request from the author.

Contact: pyz@dblab.chungbuk.ac.kr; khryu@dblab.cbnu.ac.kr

Received on May 29, 2012; revised on September 18, 2012; accepted
on October 2, 2012

1 INTRODUCTION

Recently, there has been increasing interests in changing the em-
phasis of cancer classiﬁcation from morphologic to molecular
0(iong et al., 2001). Cancers are usually marked by a change
in the expression levels of certain genes; thus, the selection of
relevant genes for cancer classiﬁcation is an important task in
most cancer gene expression studies. These discriminative genes
are very useful in clinical applications, such as in recognizing
disease proﬁles Wang et al., 2006). However, microarray data
pose a severe computational challenge because of its high dimen-
sionality and small sample size (Saeys et al., 2007). From the
perspective of machine learning, the selection of genes is a feature
selection problem that aims to ﬁnd a small subset of features
with the most discriminative information for the target.
Feature selection is an important pre—processing step in elim-
inating irrelevant and redundant features for classiﬁcation. The
growing dimensionality of recorded data demands dimension
reduction techniques that identify small sets of features that

 

*To whom correspondence should be addressed.

lead to better learning performance. The objective of feature se-
lection is to provide faster and more effective models, and also to
avoid overﬁtting and the curse of dimensionality. Feature selec-
tion methods can be broadly categorized into three types: filter,
wrapper and hybrid (Pok et al., 2010; Talavera, 2005). The filter
methods use speciﬁc evaluation criteria that are independent of a
learning algorithm to identify a feature subset from the original
feature set. Filter techniques (Liu and Setiono, 1996; Liu et al.,
2002) are fast and scale easily to high-dimensional datasets, but
they ignore interaction with the classiﬁer. Wrapper methods
(Kim et al., 2000; Kohavi and John, 1997) use the classiﬁer to
evaluate the performance of each subset with a search algorithm.
Wrapper methods tend to ﬁnd the most suitable feature subset
for the learning algorithm, but they are very computationally
expensive. Hybrid methods (Kannan et al., 2010; Xie and
Wang, 2011) combine the advantages of ﬁlter and wrapper tech-
niques. These algorithms aim to achieve the best learning per-
formance with a predetermined learning algorithm and a similar
time complexity to filter algorithms H u and Liu, 2003).

A feature selection procedure can usually be divided into two
steps: subset generation and subset evaluation (Liu and Yu,
2005). The most important issue in generating a feature subset
is how to choose the search strategy and the starting point.
Complete search, sequential search and random search are the
typical search methods used for subset generation. Complete
search methods consider every feature subset to be a potential
candidate to guarantee finding the optimal result. However, the
computational time is intractable when the dimensionality is
high. Sequential search methods, such as Sequential Forward
Selection (SFS) and Sequential Backward Selection (SBS), sacri-
fice completeness by applying the greedy hill-climbing approach
(Han and Fu, 1996), which adds or removes features one at a
time. These algorithms are computationally simpler and faster
than a complete search strategy, but they can still lead to local
optima. Random search methods, such as random-start
hill-climbing and simulated annealing (Doak, 1992), start with
a randomly selected subset, and these algorithms help to escape
local optima in the search space.

During the past few years, the Support Vector Machine
(SVM) has become very popular because of its good perform-
ance on high-dimensional data. SVM was developed by Vapnik
(1995) to successfully solve the problems of handwritten digit
recognition (Adankon and Cheriet, 2009), object recognition
(Hanson and Halchenko, 2008), text classiﬁcation (Zaghloul
et al., 2009), cancer diagnosis (Akay, 2009) and bioinformatics
(Zhang et al., 2009).

 

3306 © The Author 2012. Published by Oxford University Press. All rights resen/ed. For Permissions, please e—mail: journals.permissions@oup.com

112 /310's112u1n0fp10}x0"sotJBurJOJutotq/ﬁduq 11101} popcolumoq

91oz ‘Og anﬁnv 110 ::

An ECBGS algorithm

 

In this article, we proposed a hybrid feature selection algo-
rithm named Ensemble Correlation-Based Gene Selection
(ECBGS) based on symmetrical uncertainty (SU) and SVM for
gene selection. Our proposed method combined a filter approach
and a wrapper method to remove the redundant features and to
ﬁnd the relevant features from the original feature set. For the
original feature set, SU was used as an evaluation criterion for
the filter, using the different starting points as a subset generation
strategy and SVM as the evaluation learning algorithm of a
wrapper. It was observed that the classiﬁer combined with our
proposed feature selection method obtained promising classiﬁca-
tion accuracy with a small gene subset on six gene expression
datasets.

2 METHODS

The hybrid model (Deisy et al., 2007) attempts to combine the advantages
of both ﬁlters and wrappers by exploiting their different evaluation cri-
teria in different search strategies. Hybrid models have the advantage of
including the interaction with a classiﬁcation algorithm, while at the same
time being far less computationally intensive than wrapper methods.
Many hybrid feature selection algorithms have been proposed in the
past few years. However, many search algorithms, such as SFS and
SBS, ignore feature redundancy during the feature subset generation pro-
cedure. Along with irrelevant features, redundant features also affect the
speed and accuracy of the classiﬁers (Y u and Liu, 2003). Furthermore,
many hybrid methods are still computationally expensive. Based on these
observations, the proposed feature selection method uses SU as the evalu-
ation measure in the ﬁlter step to select relevant genes. To use the SU
value, all the features need to be discretized. Then, different genes are
used as the starting point to generate multiple gene subsets, and the
generated subsets are evaluated by the SVM. Finally, we use the best
gene subset to train the SVM model. The outline of the classiﬁcation
procedure with ECBGS is shown in Figure 1.

2.1 Fast correlation-based ﬁlter

There are many ﬁlter approaches that have been developed, such as the
chi-squared test, mutual information, Pearson correlation coefﬁcients (Li
et al., 2011), Information gain, the Gain ratio and Relief (Gheyas and
Smith, 2010). These methods are fast but lack robustness against inter-
actions among features. Yu and Liu (2004) proposed a Fast

Original features

Select relevant features

     
 

Redundam‘yanalysis

  

 

Suhscl of Itulums

Fig. 1. Procedure of the classiﬁcation model with ECBGS

Correlation-Based Filter (FCBF) approach to remove the redundant
and irrelevant features. SU was used to measure the correlation:

I(3(XIY)=1'7'(X)-1'7’(XIY) (1)
SU(X, Y) = 2*IG(XIY)/(1'7'(X) + H(Y)) (2)

where IG(X| Y) is the information gain of X after observing variable Y.
H(X) and H(Y) are the entropy of variables X and Y. Using SU as the
correlation measure, the feature selection procedure can be done by con-
sidering the C-oorrelation and F-correlation.

Deﬁnition 1 (C-oorrelation): The correlation between any feature F, and
the class C is called C-correlation, denoted by SU,’C.

Deﬁnition 2 (F-correlation): The correlation between any pair of fea-
tures F, and F,- (i 75 j) is called F-correlation, denoted by SU,J.

FCBF removes irrelevant features by ranking C-oorrelation.
Redundant features could be deﬁned using predominant features and
the approximate Markov Blanket. A feature is predominant if it does
not have any approximate Markov Blanket in the current set. For two
relevant features, F, and F,- (175]), F,- forms an approximate Markov
Blanket for F, if

S015 Z SULC and SULJ' Z SULL‘ 

where S U, L. is the correlation between the feature and the class, and S U, j
is the correlation between feature 1' and featurej.

2.2 ECBGS

In general, FCBF identiﬁes a single feature subset for which the discrim-
inative capability is limited for classiﬁcation purposes (Liu et al., 2010).
To obtain multiple feature subsets, we use different starting points to
remove redundant features in the search procedure, which allow each
subset to have its own information and to avoid being trapped in local
optima. We call the algorithm ECBGS, and the pseudocode of our algo-
rithm is shown in Figure 2. As in Figure 2, given a dataset D that contains
N features (F1, F2, . . . , FN) and a class C, the algorithm seeks several
feature subsets, whereas each partial set does not include any redundant
features. In the ﬁrst part (lines 277), all of the features are sorted in
descending order according to the S U, ,. value, which is the correlation
between the i-th feature and the class. A feature with a higher SU value
indicates higher discrimination of this feature compared with other cate-
gories, and means that the feature contains useful information for classi-
ﬁcation. After calculating the SU values for all of the features, a threshold
for the results is established. If the SU value of a feature is higher than the
threshold, the feature is selected; if not, the feature is not selected. Dre, is
the selected relevant subset of the original features. In the second part
(lines $32), a number of feature subsets are derived by splitting the
redundant features in the relevant feature subset into several parts
[Subset(1), Subset(2), . . . , Subset(i)]. During the redundancy analysis, if
we remove the redundant features as FCBF does, the selected feature
subset cannot guarantee the best prediction for the classiﬁcation problem.
This is because the features that are highly correlated with the class are
also highly correlated with each other such that the removed feature
subset in FCBF can lead to a more accurate result. Therefore, in our
method, we choose the ﬁrst element of the Drem as a starting point for
redundancy analysis (line 13) and repeat the procedure until there are no
features in Dram. Drem is the subset of features that are removed in each
redundancy analysis step. The details of the redundancy analysis step
(lines 1%29) are presented in Yu and Liu (2004). At the ﬁrst iteration,
because there are no features in the Dram, we choose the most relevant
feature as the starting point (lines 1(F11). After generating a number of
feature subsets, each subset is evaluated by SVM, and the subset with the
best classiﬁcation accuracy will be selected for the ﬁnal input of the

 

3307

112 /310's112u1n0fp10}x0"sotJBurJOJutotq/ﬁduq 11101} papnolumoq

91oz ‘Og isnﬁnv 110 ::

Y.Piao et al.

 

 

Algorithm: Ensemble Correlation-Based Gene Selection (ECBGS)

input :D(F,, F2,  FM, 0) iiatratning data set
a it athreshold

output : bestSubset

 

1 begin

2 i: 0, Dam = NULL;

3 for i = 1 to N do

4 calculate SU,C for Fi;

5 if (3U,c >= 6)

6 insert Fi to D,,,,;

7 end;

8 do begin

9 Diemp = Dnel
10 if i 0",”, is NULL}
11 spoint = getFirstElemenﬂDmI)
12 else
13 spoint = getFirstElement(D,em)
14 remove spoint from Dram;
15 iiremove features whose ranking is higher
16 iﬂhan spoint
1? D“amp = removeFeaturesiDmmp)
18 ip = spoint;
19 do begin
20 fq = getNextElement(D,emp, in);
21 do begin
22 iq, = in;
23 if ( sum, >2 sum}
24 Insert fq into Dram;
25 remove iq from DMD;
26 fCl = getNextElementhew, I‘q,);
2? else 1,, = getNextEIemenﬂDmmp. tq):
28 end until (I,l == NULL);
29 in = getNextElement(D,emp, In)
29 end until (F'J == NULL):
30 subset[i] = Dump;
31 i++;

32 end until (Dram == NULL)
33 beslSubset = gelBestSubsetisubset);
34 end;

 

Fig. 2. ECBGS algorithm

classiﬁcation (line 33). If two subsets have identical classiﬁcation accur-
acy, the smaller subset will be the ﬁnal input.

Here, we illustrate the subset generation step of ECBGS using a heart
disease dataset that is available in the UCI (University of California at
Irvine) machine learning repository. The dataset consists of 13 features
denoted as F1, F2, . . . , F13 and 294 instances with ﬁve classes. First, the
algorithm sorts all of the features in descending order based on SU values
between the feature and class. Table 1 shows the SU value of each feature.
Then high-ranking features with SU values that are >0 are selected. Here,
0 is the predeﬁned threshold. As a result, F11, F9, F3, F10, F8 and F2 are
selected as the relevant subset. Next, we perform redundancy analysis for
this relevant subset. Starting with the ﬁrst element in the relevant subset,
F11, we calculate the SU value between F“ and the other features. If the
SU value between a feature and F1, is greater than the SU value between
this feature and class, it will be moved into the removed subset; if not, it
will remain in the current subset. As shown in Table 2, the SU value
between F“ and F9 is 0.427, which is larger than the SU value between F9
and the class. Thus, the feature F9 is considered to be redundant and
moved into the removed subset along with F10 and F8. Next, we choose
the next element of F1, in the relevant subset. The next element of F1, is
F3 because the feature F9 was already moved into the removed subset. We

Table 1. SU value between each feature and the class

 

F1 1 F9 F3 F10 F8 F2

 

SU value 0.225 0.211 0.184 0.184 0.07 0.05

 

The features that have 0 SU value (F4, F1, F12, F13, F5, F7 and F5) are not presented
in this table. All of the features are sorted in descending order by SU value.

Table 2. The SU value among the features in the ﬁrst iteration

 

1:11 F9 F3 1:10 F8 1:2

 

F1, 7 0.427 a 0.689 0.085 3
F3 7 7 7 7

 

“The SU value between two features is smaller than the SU value respect to the
class.

calculate the SU value between F3 and the other features. From Table 2,
we can easily see that there are no features that are redundant with F3
such that no features will be removed, and the same scenario occurs for
F2. Consequently, subset {F,,, F3, F2} is the subset generated by the ﬁrst
iteration of our algorithm, and subset {F9, F10, F8} is the removed subset
in the ﬁrst iteration.

In the second iteration, we use the ﬁrst feature in the removed subset,
F9, as the starting point to remove redundant features. In addition, we do
not need to analyze redundancy for the whole relevant subset because the
removed features are eliminated by the features that have rankings higher
than the starting point during the previous iteration. Thus, we only ana-
lyze the redundancy of the features that have rankings lower than the
starting point. In this example, subset {F9, F3, F10, F8, F2} is the analyzed
subset in the second iteration. As shown in Table 3, F3, F10 and F8 are
redundant with F9 such that the subset {F9, F2} will be selected, and
subset {F3, F10, F8} will be moved into the removed subset. We repeat
this procedure until there are no features in the removed subset. Thus, the
subsets generated from our method are {F,,, F3, F2}, {F9, F2}, {F3, F10,
F2} and {F8, F2}. Finally, these four subsets are evaluated by SVM, and
the subset that has the highest classiﬁcation accuracy is selected.

3 RESULTS
3.1 Datasets

To evaluate the effectiveness of our method, we used six publicly
available datasets. The datasets share common characteristics,
such as a very low sample/dimension ratio.

(1) The Breast_B dataset comprises 49 samples and 1213
genes. Primary breast tumors from the Duke Breast
Cancer SPORE frozen tissue bank were selected. Tumors
were either positive or negative for both estrogen and pro-
gesterone receptors.

(2) The Central Nervous System (CNS) dataset (Pomeroy
et al., 2002) is derived from patient samples of embryonal
tumors of the central nervous system. The dataset contains
60 samples, including 39 medulloblastoma survivors and
21 treatment failures, with expression profiles of 7129
genes.

 

3308

112 /310's112u1n0fp10}x0"sotJBMJOJutotq//:d11q 11101} popcorn/hog

9103 ‘Og isnﬁnv 110 ::

An ECBGS algorithm

 

Table 3. The SU value among features in the second iteration

 

F9 F3 F10 F8 F2

 

F9 7 0.218 0.368 0.10 a
F2 i i i i i

 

“The SU value between two features is smaller than the SU value respect to
the class.

(3) The Leukemia dataset (Golub et al., 1999) was produced
in a study that was aimed at building a model to discrim-
inate between acute myeloid leukemia and acute lymph-
oma leukemia tissues. Gene expression proﬁles have been
constructed from 72 people who have either acute lympho-
blastic leukemia or acute myeloid leukemia, and each
sample is composed of 7129 gene expression profiles.

(4) The Lymphoma dataset (Alizadeh et al., 2000) comes from
a study on diffuse large B-cell lymphoma. The dataset
consists of 62 samples and 4026 genes. There are three
types of samples in the dataset, with 42 samples of diffuse
large B-cell lymphoma, nine observations of follicular
lymphoma and 11 cases of chronic lymphocytic leukemia.

(5) The MLL_leukemia dataset (Armstrong et al., 2002) con-
tains three types of leukemia samples compared with the
binary-class leukemia dataset. This dataset contains a total
of 72 samples in three classes, acute lymphoblastic leuke-
mia, acute myeloid leukemia and mixed-lineage leukemia
gene (MLL), which have 24, 28 and 20 samples, respect-
ively. The number of genes is 12582.

(6) The prostate dataset was ﬁrst published by Singh et a].
(2002); it is a two-class classiﬁcation problem and contains
102 samples and 12600 genes. One of the tasks addressed
by the authors is to build a model that can discriminate
between normal prostate and tumorous prostate tissue.

3.2 Parameter settings for the SVM

Selecting the kernel and appropriate parameters plays an import-
ant role in SVM classiﬁcation performance. The radial basis
function (RBF) kernel is a commonly used kernel for three rea-
sons (Hsu et al., 2010). First, the RBF kernel can handle non-
linear relationships between class labels and attributes. Second, it
has fewer hyperparameters that inﬂuence the complexity of the
model selection than the Polynomial kernel. Third, the RBF
kernel has fewer numerical difficulties. The RBF kernel function
is here:

K06, x/) = eXp(—| Ix — x/l IZ/UZ) (4)

In our experiments, we chose the RBF kernel function, and the
parameters C and y must be optimized for the RBF kernel for
each dataset. To determine the best values of C and y, we con-
ducted a grid-search approach using 10-f01d cross validation. A
number of pairs of (C,y) values were attempted, and the pair
with the best accuracy was picked in the range of
C e {2’5,2’3, ...,215} and y e {2’15,2’13,...,23}.

3.3 Performance evaluation

To obtain a statistically reliable predictive measurement, we per-
formed 10-f01d cross validation on all the datasets. In 10-f01d
cross validation, each dataset was randomly partitioned into 10
parts. Nine parts were used as the training set, and the remaining
one was used as the testing dataset. In cancer classiﬁcation, it is
important to assess both false-positive and false-negative errors,
as these two types of errors usually have different consequences
(Ma and Huang, 2005). Therefore, we have used several meas-
ures to evaluate the effectiveness of our method:

0 Classiﬁcation accuracy 2 (TP + TN) / (TP + FP + FN + TN)
0 Precision 2 TP / (TP + FP)

TP rate 2 TP / (TP + FN)

FP rate = FP / (FP + TN)

0 Area Under Receiver Operating Characteristic Curve
(AUC) is a single-value measurement that ranges from 0
to 1.

where true positives (TP) denote the correct classiﬁcations of
positive examples, true negatives (TN) are the correct classiﬁca-
tion of negative examples, false positives (FP) denote the incor-
rect classiﬁcation of negative examples into the positive class and
false negatives (FN) represent the incorrect classiﬁcation of posi-
tive examples into the negative class.

In addition, some datasets such as Breast_B, Lymphoma and
MLL_Leukemia have the multi-class problem, which refers to
the input data being divided into more than two categories. To
solve this problem, we used the one-against—one approach that
constructs N(N-1)/2 binary classiﬁers for an N class dataset. The
posterior probabilities provided by individual binary classiﬁers
were combined using the pairwise coupling method (Hastie and
Tibshirani, 1998).

As mentioned previously, the FCBF method cannot always
identify the best feature subset for high-dimensional data,
which was demonstrated by comparing the prediction accuracy of
the FCBF feature selection algorithm with our method. Table 4
exhibits the classiﬁcation accuracy of the subsets that are selected
from each iteration of our method on six datasets with a 6&40%
training-test partition, and the best results in each row are shown
in bold letters. The subset selected from the second iteration of
our method is denoted as Set 2, the subset selected from the third
iteration is denoted as Set 3 and so on. Because our method uses
the same starting point as the FCBF initially, the subset selected
by the first iteration of our method will be the same as the subset
selected using FCBF. As shown in Table 4, although the FCBF
algorithm has the same accuracy as other subsets on the
MLL_Leukemia datasets, it has a lower accuracy than other
subsets in most cases, which proves that our method outperforms
the FCBF.

Tables $10 summarize the results of the classiﬁcation
achieved by ECBGS on different values of the relevance
threshold. The performance of the classiﬁer was evaluated by
precision, TP rate, FP rate and AUC. For each dataset, the
number of selected genes is listed as the ‘#Genes’ row. With
the increasing of threshold, there are no genes to be selected in
some datasets. In this case, we cannot measure the performance
of the classiﬁer, as there are no inputs for the classiﬁer.

 

3309

112 /310's112u1n0fp10}x0"sotJBMJOJutotq//:d11q 11101} popcorn/hog

9103 ‘Og isnﬁnv 110 ::

Y.Piao et al.

 

Table 4. The classiﬁcation accuracy (%) of FCBF and our method

 

 

Dataset FCBF Set 2 Set 3 Set 4 Set 5 Set 6 Set 7
1 90 95 85 70 85 85 7S

2 75 75 70.83 83.33 70.83 83.33 75

3 86.21 93.10 93.10 93.10 93.10 93.10 93.10
4 92.11 94.74 92.11 92.11 94.74 92.11 92.11
S 100 100 100 100 100 100 100
6 95.12 92.68 95.12 90.24 92.68 92.68 97.65

 

The training—test data partition is 60—40%, and each subset was evaluated using SVM. Set 2, Set 3,... , Set 7 are the feature subsets generated in the second, third, . . ., seventh
iteration of our algorithm, respectively. The bold values indicate the highest classiﬁcation accuracy.

Table 5. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the Breast_B dataset

 

 

Breast_B

Threshold 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8
#Genes 39 39 39 39 38 27 17 10 S 4 1 1 0 0 0 0 0
Precision 0.94 0.94 0.94 0.94 0.94 0.96 0.86 0.86 0.86 0.86 0.15 0.15 NA NA NA NA NA
TP rate 0.94 0.94 0.94 0.94 0.94 0.96 0.86 0.86 0.86 0.86 0.39 0.39 NA NA NA NA NA
FP rate 0.03 0.03 0.03 0.03 0.03 0.02 0.07 0.07 0.07 0.05 0.04 0.04 NA NA NA NA NA
AUC 0.98 0.98 0.98 0.98 0.98 0.98 0.94 0.93 0.93 0.93 0.58 0.58 NA NA NA NA NA

 

The third row shows the number of selected genes corresponding to the relevance threshold.

Table 6. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the CNS dataset

 

 

CNS

Threshold 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8
#Genes 36 36 36 36 23 7 4 1 0 0 0 0 0 0 0 0 0
Precision 0.90 0 .90 0 .90 0 .90 0.88 0.74 0.44 0.44 NA NA NA NA NA NA NA NA NA
TP rate 0.90 0.90 0.90 0.90 0.88 0.73 0.67 0.67 NA NA NA NA NA NA NA NA NA
FP rate 0.13 0.13 0.13 0.13 0.16 0.48 0.44 0.67 NA NA NA NA NA NA NA NA NA
AUC 0.88 0.88 0.88 0.88 0.86 0.63 0.50 0.50 NA NA NA NA NA NA NA NA NA

 

The third row shows the number of selected genes corresponding to the relevance threshold.

Table 7. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the Leukemia dataset

 

 

Leukemia

Threshold 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8
#Genes S9 S9 S9 S9 27 6 1 0 0 0 0 0 0 0 0 0 0
Precision 0.90 0 .90 0 .90 0 .90 0.90 0.85 0.76 NA NA NA NA NA NA NA NA NA NA
TP rate 0.90 0.90 0.90 0.90 0.90 0.85 0.76 NA NA NA NA NA NA NA NA NA NA
FP rate 0.11 0.11 0.11 0.11 0.15 0.23 0.29 NA NA NA NA NA NA NA NA NA NA
AUC 0.90 0.90 0.90 0.90 0.88 0.81 0.74 NA NA NA NA NA NA NA NA NA NA

 

The third row shows the number of selected genes corresponding to the relevance threshold.

This situation is denoted as ‘NA’ in the table. From Table 5, we
can know that the choice of the appropriate relevance threshold
is important, as it will directly affect the performance of the
classiﬁer. The classiﬁers show the best performance when the

threshold is in the range of 0.1}025 on Breast_B, CNS,
Leukemia and Prostate dataset. However, for the Lymphoma
and MLL_Leukemia dataset, the appropriate threshold is
much larger than other datasets (0.75 and 0.4 for Lymphoma

 

3310

112 /310's112u1n0[p101x0"sotwuiJOJHtotq/ﬁduq 111011 pap201um0q

9103 ‘0g1sn8nv 110 ::

An ECBGS algorithm

 

Table 8. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the lymphoma dataset

 

 

Lymphoma

Threshold 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
#Genes 89 89 89 89 89 86 86 79
Precision 1 1 1 1 1 1 1 1
TP rate 1 1 1 1 1 1 1 1
FP rate 0 0 0 0 0 0 0 0
AUC 1 1 1 1 1 1 1 1

0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8
75 69 61 54 37 31 23 9 4

1 0.99 1 0.99 0.99 1 1 1 0.96
1 0.98 1 0.98 0.98 1 1 1 0.95
0 0.01 0 0.01 0.01 0 0 0 0.01
1 0.99 1 0.99 0.99 1 1 1 0.98

 

The third row shows the number of selected genes corresponding to the relevance threshold.

Table 9. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the MLL_Leukemia dataset

 

MLL_Leukemia

 

Threshold 0 0.05 0.1 0.15 0.2 0.25 0.3 0
#Genes 117 117 117 117 113 105 89 5
Precision 1 1 1 1 1 1 1 0
TP rate 1 1 1 1 1 1 1 0
FP rate 0 0 0 0 0 0 0 0
AUC 1 1 1 1 1 1 1 0

.35
5

.99
.99
.01
.99

0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8
53 31 30 14 11 5 3 1 0

1 0.99 0.99 0.96 0.95 0.55 0.55 0.52 NA
1 0.99 0.99 0.96 0.94 0.69 0.68 0.56 NA
0 0.01 0.01 0.02 0.03 0.19 0.20 0.28 NA
1 0.99 0.99 0.99 0.98 0.78 0.78 0.66 NA

 

The third row shows the number of selected genes corresponding to the relevance threshold.

Table 10. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the Prostate dataset

 

Prostate

 

Threshold 0 0.05 0.1 0.15 0.2 0.25 0.3

#Genes 76 76 76 64 46 30 18

Precision 0.97 0.97 0.97 0.98 0.98 0.96 0.96
TP rate 0.97 0.97 0.97 0.98 0.98 0.96 0.96
FP rate 0.03 0.03 0.03 0.02 0.02 0.04 0.04
AUC 0.97 0.97 0.97 0.98 0.98 0.96 0.96

0.35
9

0.96
0.96
0.04
0.96

0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8
9 5 2 1 0 0 0 0 0

0.94 0.94 0.91 0.79 NA NA NA NA NA
0.94 0.94 0.91 0.70 NA NA NA NA NA
0.06 0.06 0.09 0.32 NA NA NA NA NA
0.94 0.94 0.91 0.69 NA NA NA NA NA

 

The third row shows the number of selected genes corresponding to the relevance threshold.

and MLL_Leukemia dataset, respectively). It is reasonable be-
cause the genes in the Lymphoma and MLL_Leukemia dataset
are more relevant than others. Moreover, the number of relevant
genes in the Lymphoma and MLL_Leukemia dataset is much
bigger than other datasets.

In addition, we compared the accuracies of SVM using the
features selected by the Gain Ratio (GR), Information Gain
(IG), ReliefF and our method. Furthermore, we used the SFS
and SBS search strategies in the subset generation step for these
three feature selection algorithms. These classiﬁcation accuracies
were obtained through 10-f01d cross validation. Table 11 shows
classiﬁcation accuracy of four feature selection algorithms on six
datasets. In the majority of the datasets, the accuracy is higher
than other methods, and the classiﬁer with our proposed feature
selection algorithm is found to result in the best prediction aver-
age accuracy, which was 95.71%. The other methods were found
to be 89.97, 91.89, 89.54, 93.89, 89.46 and 92.66%. To catch the

detailed characteristics of ECBGS, we also made the comparison
of accuracy obtained from nine different partitions of training
and test datasets. Figure 3 presents the classiﬁcation accuracy
on different sizes of training and test data. From Figure 3, one
can easily observe that the prediction performance of the classi-
ﬁcation model constructed from the feature subset that is gener-
ated using our method is better than other models in most cases.
Moreover, even when the training data set is small, our method
shows high prediction accuracy, whereas other methods primarily
make poor predictions. It is also obvious that, at least for one
training-test partition, the classiﬁcation accuracy of our method is
100% on all of the tested datasets. For example, the classiﬁcation
accuracy is 100% when the training-test partition is 7&30 or 8}
15 on the Breast_B dataset (Fig. 3a), as well as when the
training-test partition is 8&20 on the prostate dataset (Fig. 31).
Table 12 shows the running time for each feature selection
algorithm. For each method, the parameter of the ﬁlter part is

 

3311

112 /310's112u1n0[p1q1x0"sotwuiJOJHtotq/ﬁduq 111011 pap201um0q

9103 ‘0g1sn8nv 110 ::

Y.Piao et al.

 

Table 11. 10-fold cross validation classiﬁcation accuracy (%) of four feature selection methods

 

 

Dataset GR + SVM IG + SVM RelietF+SVM ECBGS
SFS SBS SFS SBS SFS SBS

1 87.76 93.88 87.76 97.96 93.88 91.84 95.92

2 75 73.33 73.33 90.00 65 83.33 90.00

3 86.11 87.5 90.28 90.28 90.28 87.50 90.28

4 100 100 98.31 95.16 100 100 100

5 95.83 98.61 94.44 95.83 94.44 97.22 100

6 95.10 98.04 93.14 94.12 93.14 96.08 98.04
Average 89.97 91.89 89.54 93.89 89.46 92.66 95.71

 

The last row is the total average accuracy of four methods on six datasets. The bold values indicate the highest classiﬁcation accuracy.

set to 0 throughout the experiments. From Table 12, it is clear
that ECBGS is significantly faster than the other three algo-
rithms. Moreover, the running time of these three algorithms is
extremely expensive because the number of selected features in
the ﬁlter part is >2000 on the lymphoma, MLL_Leukemia and
prostate datasets. This result veriﬁes that ECBGS is suitable for
high-dimensional microarray data analysis, saving a signiﬁcant
amount of time.

In addition to these feature selection algorithms, we can also
make a comparison with the results of other methods published
in the literature. Table 13 presents the best classiﬁcation accuracy
of other methods. From Table 13, we can see that the classiﬁca-
tion accuracy achieved by our method is higher than other meth-
ods and the number of selected genes is significantly smaller than
other methods except on the Breast_B dataset. Comparing the
results between the Breast_B and Prostate dataset, it is found
that greater amount of genes and samples can result in smaller
models than minor amount of genes and samples. This ﬁnding is
not surprising, as our method is based on the information theory
of entropy. Thus, for the dataset that has larger number of sam-
ples, the importance of the genes is easier to be captured.

Recently, much research has been performed on analyzing
gene expression data for cancer classiﬁcation using various
gene selection methods. The Lymphoma dataset has been cross
validated by many authors. Dettling and Biihlmann (2003)
modiﬁed the boosting classiﬁers and applied Wilcoxon’s two
sample tests to select discriminative genes; comparing the results
between our study and their proposed method, our best perform-
ing results are better than their results using about the same
number of genes. Liu et al. (2010) proposed an ensemble gene
selection method to analyze the gene expression data. The clas-
sification results for the Lymphoma dataset are identical to ours.
However, the performance on the CNS and Prostate dataset are
not better than ours (though they used leave one out cross val-
idation). Moreover, our method returns a much smaller set of
genes on these three datasets, and our method does not need to
predeﬁne the number of genes that will be selected. For the
Prostate dataset, Diaz—Uriarteb et al. (2006) reported 0.061
error rates with 18 genes; Yang et al. (2006) proposed two
gene selection methods which were not affected by the unba-
lanced sample class sizes. Their results with K-nearest—neighbor

and SVM for the MLL_Leukemia and Prostate dataset show
lower performance and they used larger set of genes (56 for the
MLL_Leukemia and 8 for the Prostate dataset). The Breast_B
dataset was ﬁrstly analyzed by West et al. (2001) using Bayesian
approach. Dettling and Biihlmann (2002) also used Breast_B
dataset and reported 4.08% of test error rates with 10 genes,
which was better than West et al. (2001). In comparison with
our method, although their method used a smaller subset of
genes, our method made prediction more accurately. Finally,
Hsu et al. (2011) introduced a hybrid feature selection method
based on Information Gain and F-score. They achieved 98.6%
of accuracy using SVM with 70 features on the Leukemia data-
set. However, this method seems to be difﬁcult to be used for
gene selection because they select too much genes. Therefore, our
proposed method is an ideal candidate for gene selection in
cancer classiﬁcation problem.

4 DISCUSSION

In this study, we used SVM classiﬁcation method to analyze the
gene expression data. A lot of research has been shown that
SVM is the most effective classiﬁer in performing accurate
cancer diagnosis from gene expression data (Statnikov et al.,
2005; Statnikov et al., 2008). SVM is interesting (Abeel et al.,
2010) because the number of parameters to be estimated essen-
tially depends on the number of samples rather than on the
number of features, which is particularly relevant with very
small sample-to-feature ratios. Moreover, SVM has many math-
ematical features that make them attractive for gene expression
analysis (George and Raj, 2011), including their ﬂexibility in
choosing a similarity function, sparseness of solution when deal-
ing with large datasets, the ability to handle large feature spaces
and the ability to identify outliers.

We have first examined the performance of our method using
SVM on six microarray datasets in terms of precision, TP rate,
FP rate and AUC. In ECBGS, there is a parameter, the relevance
threshold 0. Different settings of a will directly affect the number
of selected genes. The closer a is set to 1, the smaller the number
of selected genes is. From the experiments, we found that the
larger number of genes does not always lead to better perform-
ance. Therefore, it is very important to choose the appropriate

 

3312

112 /310'S[BIIJDOIPJOJXO'SOIJBLUJOJIIIOIq/ﬂduq 111011 pap201um0q

9103 ‘0g1sn8nv 110 ::

An ECBGS algorithm

 

Accuracy (96]
S
8

40.00

20.00

(61

120.010
I001”

E. 30.00

Accuracy {‘90)
5
3

60M

40.00

20.00

|20.00

1001!]

x
P
5

Accuracy (%J
3
8

40M

20.010

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

b c
I )IZDIIO I )I20.00
Breast_B CNS
i00.00 I00.00
§ 00.00 0. 00.00
8‘ B‘
E E
S S
(0.00 110.00
2 ' 3
-o--ei=i+va+5Fs ~-IG+s\rM+SF5 40m u-en+svu+ses ---ie+5\rM+ses ‘10-‘30 -m-on+sVM+SFs ~-Ie+s\rM+SF5
w HelefF+SUM+SFS +Pmooeecl method + FlellefF+SVM+SBS +Pmpoeed men-rod w ReliefF+StrM+SFS +Propoeoa method
20.04! 20.00
s °= a e s ‘= D s a °= s e
afﬁ~$$d§$g s§1§$$ee‘§§~ afﬁxﬁﬁdet$s
Diﬂerent size ol lreiniru end lee! " Different size of training and feet " Diﬂerent size oI Ireiriq end lest "
(Bi
120.00 I200“
CNS Leuhemie Leukemia
i00.00 i00.00
W; 'm/ “a?!
.7 5r“ " ates-A‘rﬂ .7
E. 00.00 “ y” E 00.00
8‘ 5‘
E E
a S
(0.00 $10.00
3 ’ 3
-o--Gn+svm+ses ~-IG+5VM¥SBS 4W) -«-Gn+sm+si=s --IG+S\I'M+5FS 4‘10” -vI-GR+SVM+SBS --IG+S\I'M+SBS
4 ReliefF+SVM+SBS +Propoeed moment + RellalF+SVM+SFS +Pmposad melhod 4 ReliefF+SVM+SBS +Proposed melhod
20.00 20.00
‘3 ‘3 D t? ‘1 (I t? ‘3) ‘9 Q {P
ﬁfﬁaﬁs‘e‘ﬁ‘y ffﬁﬁ,$a‘e‘$s~ effﬁepse‘di’dr
DiIlelerIl size oliieining and test “ Dillerent size OI training and lest " Different size ml Irain'ng and lea "
(h) (I)
i20.00 I20.00
Lymphoma Lymphoma MLL_Leukemia
.. E , 100.00 I - |01J_UO I—‘I—I—C—I—I’I—zj—I—ui
1 — - —.r ' I .e'
r a: , ..
ﬂ .0 “‘1‘ .. «x )5 Hue-“X
33.00.00 $00.00 w “I”. 
h :. “hf”
E E
I 3
§ 60.00 § 60.00
-«-GR+SVM+3FS —.—Ie+suM+3Fs 40°” -wGR+SVM+SBS —.—Ie+5\rM+ses 49”“ -m-GR+3VM+SFS —-—IG+3VM+SF5
«- HelialF+SVM+$FS *Flupoeed method + HeliefF+5VM+SBS i-Froposed method 1 Helie1F+5VM+SFS i-Froposed method
20.00 20.00
e e <o e o (o e e .0 o a e
0031345903033 00031691430311.0330 0959030116900er
Dilfelenleizeolirelnlngendieet " Mm elasofireinlngmheet " Dlﬂerenteizealtreiimg endleei "
(k) (l1I
iznm i20.nn
MLL_Leukemia Prostate Prostate
' _ _ ‘ x / ...___  I i00.00 010.00 .__.__.___'/,a—I—G—o—-—_.
“‘af—“V'W'lﬁ. ,‘ I‘eu—v—o’
\ Z " -. ‘lr n n r
V 33. 00.00 § 00.00
)1 5‘
E E
2 3
g 00.00 § 60.00
-w-GH+SVM+SBS -.-IG+e\rM+ses 4000 -u- GR+SVM+5F3 1A|G+SVM+5FS 4‘10“ '*-GR+SVM+SBS +|G+SVM+SBS
1 HeliefF+SVM+SBS +F’roposed method 1 FIeIIeIF+SVM+SFS +Pmpused melhod " HelielFtSVM+535 *Pivposed melhod
20.00 20.00

 

 

 

 

0““ at 015451" at a" as" $391“

Drlfelienl eize ofireinlng and [eel

 

$£ﬁ£

eine

assay

 

.0 e c
_ 035103019000901
ofireinlngmiieet Diflerentslzeoltremendlesl "

Fig. 3. Classiﬁcation accuracy of different sizes of training and test datasets. (a) and (b) indicate the results on the Breast_B dataset, (c) and ((1) indicate
the results on the CNS dataset, (e) and (1) indicate the result on Leukemia dataset, (g) and ([1) indicate the result on Lymphoma dataset, (i) and (j)
indicate the results on MLL_Leukemia dataset, (k) and (1) indicate the results on Prostate dataset. We performed two experiments for each dataset: one
applied the SBS strategy, and the other one applied the SFS strategy for three feature selection methods. As in the ﬁgure, when the training dataset is
small, the proposed method shows high prediction accuracy, and other methods primarily make poor predictions. Moreover, the proposed method
outperforms other methods in most cases

threshold for improving classiﬁcation accuracy. However, choos-
ing a proper threshold is difﬁcult because the distinct values for
each dataset are different. Through the experiments on six data-
sets, we found that the threshold was closely related with the
‘degree’ of the relevance of the genes. For example, in the

Lymphoma dataset, there are >300 genes with SU value that is
>0.5. In contrast, there are only 37 genes that satisfy the thresh-
old 0.5 in Breast_B dataset. Thus, the appropriate threshold for
the dataset that has higher ‘degree’ of the relevance has to be
larger than the one for the dataset that has lower ‘degree’.

 

3313

112 /810's1cumoip101x0"soncuiJOJHtotq/ﬁduq 1110.1} popco1umoq

9IOZ ‘OE ISUEHV 110 ::

Y.Piao et al.

 

Table 12. Running time (s) for each feature selection algorithm

 

 

 

Dataset ECBGS GR + SVM IG + SVM ReliefF + SVM

SFS SBS SFS SBS SFS SBS
1 7 2959 20 090 14885 18735 12471 >105
2 231 286 4129 234 1994 10 837 >105
3 243 1041 8823 608 12083 10 807 >105
4 90 >105 >105 >105 >105 >105 >105
5 809 >105 >105 >105 >105 >105 >105
6 863 >105 >105 >105 >105 >105 >105

 

The running time for GR—i— SVM, IG + SVM and ReliefF + SVM is much
>100000s on the No. 4, 5 and 6 datasets.

Table 13. The best accuracy (%) and the number of selected genes ob-
tained with our method and other approaches in the literature. The bold
letters indicate the results of our proposed method

 

Dataset Authors Accuracy (%) Number

 

of features
1 Dettling and B'L'thlmann 98.39 10
Proposed method 100 17
2 Kannan et al. 97.78 374
Liu et a]. 98.33 30
Tan et al. 88.33 74
Yeh et a]. 77.31 58
Proposed methods 100 20
3 Fujibuchi and Kate 97.8 170
Cho and Ryu 94.1 30
Cho and Won 97.1 50
Hsu et al. 98.6 70
Proposed method 100 5
4 Dettling and B'L'thlmann 98.39 10
Lee et al. 99.8 50
Liu et al. 100 30
Proposed method 100 9
S Yang et a]. 97.2 56
Yang et a]. 98.61 782
Kannan et al. 100 108
Proposed method 100 6
6 Yang et al. 95.1 8
Liu et a]. 96.08 30
Diaz-Uriarteb et al. 99.4 18
Yang et a]. 96.08 343
Proposed method 100 5

 

The bold values indicate the results of our proposed method.

Based on this observation, we can make some general recom-
mendations based on the experiments. (i) The threshold could be
selected as the mean of all the SU value of the genes respect to
the class or (ii) decide the threshold as the following equation:

a = (SUma,x — SUm,,,) * 0.7 (5)

where S Umx indicates the SU value of the most relevant gene
respect to the class, and S Um,n refers to the lowest one.

We have also compared ECBGS to the FCBF algorithm. The
result shows that the proposed ECBGS is able to generate the
most meaningful and discriminative genes in most cases.
However, if the number of features is <50, FCBF tends to be
more effective; it is because ECBGS produces the feature subsets
by removing top informative features. If the datasets have a
small number of features with a lack of useful information,
removing some informative features will result in less discrimina-
tive or meaningless subsets to train the classiﬁer. Furthermore,
we found that relevant and non-redundant features are selected
before repeating our method more than 10 times.

We have also made a comparison between the proposed
method and other feature selection methods in terms of classiﬁ-
cation accuracy and speed. One interesting observation is that
our method is still more powerful than other methods even
when small data are given. It indicates that ECBGS is more
appropriate than others for analyzing small datasets, such as
gene expression data. Moreover, our method is signiﬁcantly
faster than other feature selection methods. Additionally,
when the number of selected features is >2000, the computa-
tional costs of the other three feature selection methods are
very expensive.

The selection of discriminant genes is a common task for
cancer classiﬁcation. Research in Biology and Medicine may
beneﬁt from the examination of the top ranking genes to conﬁrm
recent discoveries in cancer research, or suggest new avenues to
be explored (Guyon et al., 2002). Recently, several gene selection
approaches (Jirapech-Umpai and Aitken, 2005; Li et al., 2004)
have been proposed to solve the cancer classiﬁcation problem. In
contrast to these methods, the prediction accuracy of our method
is competitive with a small subset of genes.

5 CONCLUSIONS

In this work, we proposed an ensemble gene selection algorithm
based on SU and SVM for cancer classiﬁcation. To select mul-
tiple gene subsets, we used different starting points during the
redundancy analysis step. In this way, we selected more inform-
ative genes than FCBF. We found that between two redundant
genes, the less relevant gene makes a poor prediction; however, a
combination of genes of this type can sometimes produce a com-
petitive result. During the experiments, we used six freely access-
ible benchmark datasets from the Internet to meet our objective,
which was to evaluate and investigate the performance of our
method using the classiﬁers trained from both 10-cross valid-
ation and different sizes of datasets. The results show that the
classiﬁcation model with our proposed gene selection algorithm
has higher prediction accuracy, and that our method can still
achieve high accuracy when the number of training instances is
small. Compared with other methods published in the literature,
our method yields good results. However, for different datasets,
the relevance threshold is different under the context of classiﬁ-
cation performance. Therefore, how to determine the relevance
threshold in a self-adaptive matter will be focused on our future
work. Moreover, we believe that our mechanism is also applic-
able to other feature selection problems and can be expanded to
other classiﬁcations of disease states.

 

3314

112 /8.10's11211.1n0[p.101x0'soticuiJOJHtotq/ﬁduq 111011 pap1201um0q

9103 ‘0g1sn8nv 110 ::

An ECBGS algorithm

 

ACKNOWLEDGEMENTS

The authors are grateful to anonymous referees for their valuable
comments.

Funding: The National Research Foundation of Korea (NRF)
grant funded by the Korea government (MEST) (No. 2012-
0000478) and the Korea Biobank Project (4851-307) of the
Korea Centers for Disease Control and Prevention.

Conﬂict of Interest: none declared.

REFERENCES

Abeel,T. et a]. (2010) Robust biomarker identiﬁcation for cancer diagnosis with
ensemble feature selection methods. Bioinformatics, 26, 3927398.

Adankon,M. and Cheriet,M. (2009) Model selection for the LS—SVM. Application
to handwriting recognition. Pattern Recognit., 42, 3264e3270.

Akay,M.F. (2009) Support vector machines combined with feature selection for
breast cancer diagnosis. Expert Syst. Appl., 36, 324(F3247.

A1izadeh,A. et a]. (2000) Distinct types of diffuse large B—cell lymphoma identiﬁed
by gene expression proﬁling. Nature, 403, 5037511.

Armstrong,S.A. et a]. (2002) MLL translocations specify a distinct gene expression
proﬁle that distinguishes a unique leukemia. Nat. Genet, 30, 41747.

Cho,S. and Ryu,J. (2002) Classifying gene expression data of cancer
using classiﬁer ensemble with mutually exclusive features. Proc. IEEE, 90,
174471753.

Cho,S. and Won,H. (2007) Cancer classiﬁcation using ensemble of neural networks
with multiple signiﬁcant gene subsets. App]. Intell., 26, 2437250.

Deisy,C. et a]. (2007) Efﬁcient dimensionality reduction approaches for
feature selection. In: International Conference on Computational
Intelligence and Multimedia Applications. Sivakasi, Tamil Nadu, pp. 1217127.

Diaz—Uriarteb,R. et a]. (2006) Gene selection and classiﬁcation of nricroarray data
using random forest. BMC Bioinformatics, 7, 3.

Dettling,M. and Biihlmann,P. (2003) Boosting for tumor classiﬁcation with gene
expression data. Bioinformatics, 19, 106171069.

Doak,J. (1992) An evaluation of feature selection methods and their application
to computer security. Technical report. Department of Computer Science,
University of California at Davis.

Fujibuchi,W. and Kato,T. (2007) Classiﬁcation of heterogeneous nricroarray data
by maximum entropy kernel. BMC Bioinformatics, 8, 2677277.

George,G. and Raj,V. (2011) Review on feature selection techniques and the impact
of SVM for cancer classiﬁcation using gene expression proﬁle. Int. J. Comput.
Sci. Eng. Surv., 2, 3.

Gheyas,I. and Smith,L. (2010) Feature subset selection in large dimensionality do—
mains. Pattern Recognit., 43, 5713.

Golub,T.R. et a]. (1999) Molecular classiﬁcation of cancer: class discovery and class
prediction by gene expression monitoring. Science, 286, 5317537.

Guyon,I. et a]. (2002) Gene selection for cancer classiﬁcation using support vector
machines. Machine. Learn., 46, 389422.

Han,J. and Fu,Y. (1996) Attribute-oriented induction in data mining.
In:Fayyad,U.M. et a]. (cd.) Advances in Knowledge Discovery sand Data
Mining. AAAI Press/The MIT Press, Cambridge, MA, pp. 339421.

Hanson,S. and Halchenko,Y. (2008) Brain reading using full brain support vector
machines for object recognition: there is no ‘face’ identiﬁcation area. Neural
Comput., 20, 48G503.

Hastie,T. and Tibshirani,R. (1998) Classiﬁcation by pairwise coupling. Ann. Statist.,
26, 451471.

Hsu,C.W. et a]. (2010) A Practical Guide to Support Vector Classmcation.
http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guidepdf (25 September 2012,
date last accessed).

Hsu,H.H. et a]. (2011) Hybrid feature selection by combining ﬁlters and wrappers.
Expert Syst. Appl., 38, 814448150.

Jirapech—Umpai,T. and Aitken,S. (2005) Feature selection and classiﬁcation for
nricroarray data analysis: evolutionary methods for identifying predictive
genes. BMC Bioinformatics, 6, 148.

Kannan,S. et a]. (2010) A novel hybrid feature selection via symmetrical uncer—
tainty ranking based local memetric search algorithm. Know]. Based Syst., 23,
58(k585.

Kim,Y. et a]. (2000) Feature selection for unsupervised learning via evolu—
tionary search. In: Proceedings of the Sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. New York, USA,
pp. 36$369.

Kohavi,R. and John,G.H. (1997) Wrappers for feature subset selection. Arty.
Intell., 97, 2737324.

Lee,J. et a]. (2002) An extensive comparison of recent classiﬁcation tools applied to
microarray data. Comput. Stat. Data Am], 48, 77787.

Li,P. et a]. (2011) QSE: a new 3—D solvent exposure measure for the analysis of
protein structure. Proteomics, ll, 379%3801.

Li,T. et a]. (2004) A comparative study of feature selection and multiclass classiﬁ—
cation methods for tissue classiﬁcation based on gene expression. Bioinformatics,
20, 24222437.

Liu,H. et a]. (2010) Ensemble gene selection for cancer classiﬁcation. Pattern
Recognit., 43, 276%2772.

Liu,H. et a]. (2002) Feature selection with selective sampling. In: Proceedings of the
Nineteenth International Conference on Machine Learning, 2002. Sydney,
Australia, pp. 395402.

Liu,H. and Setiono,R. (1996) A probabilistic approach to feature selectionia ﬁlter
solution. In: Proceedings of the Thirteenth International Conference on Machine
Learning. Bari, Italy, pp. 3197327.

Liu,H. and Yu,L. (2005) Toward integrating feature selection algorithms for clas—
siﬁcation and clustering. IEEE Trans. Know]. Data Eng., 17, 4917502.

Ma,S. and Huang,J. (2005) Regularized ROC method for disease classiﬁcation and
biomarker selection with microarray data. Bioinformatics, 21, 4356—4362.

Pomeroy,S.L. et a]. (2002) Prediction of central nervous system embryonal tumour
outcome based on gene expression. Nature, 415, 436442.

Pok,G. et a]. (2010) Effective feature selection framework for cluster analysis of
microarray data. Bioinformation, 4, 3857389.

Singh,D. et a]. (2002) Gene expression correlates of clinical prostate cancer behav—
ior. Cancer Cell, 2, 2037209.

Statnikov,A. et a]. (2005) A comprehensive evaluation of multicategory classiﬁca—
tion methods for microarray gene expression cancer diagnosis. Bioinformatics,
21, 631443.

Statnikov,A. et a]. (2008) A comprehensive comparison of random forests and
support vector machines for microarray—based cancer classiﬁcation. BMC
Bioinformatics, 9, 319.

Tan,A.C. and Gilbert,D. (2004) Ensemble machine learning on gene expression data
for cancer classiﬁcation. Bioinformatics, 20, 358373593.

Talavera,L. (2005) An evaluation of ﬁlter and wrapper methods for feature selection
in categorical clustering. In: Proceedings of 6th International Symposium on
Intelligent Data Analysis. Madrid, Spain, pp. 440—451.

Vapnik,V.N. (1995) The Nature of Statistical Learning Theory. Springer—Verlag,
New York, NY.

West,M. et a]. (2001) Predicting the clinical status of human breast cancer by using
gene expression proﬁles. Proc. Nat] Acad. Sci. USA, 98, 11462711467.

Xie,J. and Wang,C. (2011) Using support vector machines with a novel hybrid
feature selection method for diagnosis of erythemato—squamous diseases.
Expert Syst. Appl., 38, 580975815.

Xiong,M. et a]. (2001) Feature (Gene) selection in gene expression—based tumor
classiﬁcation. M0]. Genet. Metab., 73, 2397247.

Yang,K. et a]. (2006) A stable gene selection in microarray data analysis. BMC
Bioinformatics, 7, 228.

Yang,C.H. et a]. (2009) IG—GA: a hybrid ﬁlter/wrapper method for feature selection
of microarray data. J. Med. Biol. Eng., 30, 23728.

Yeh,J.Y. (2008) Applying data mining techniques for cancer classiﬁcation on gene
expression data. Cybern. Syst. Int. J., 39, 583402.

Yu,L. and Liu,H. (2003) Feature selection for high—dimensional data: a fast
correlation—based ﬁlter solution. In: Proceedings of the Twentieth International
Conference on Machine Learning (ICML—2003). pp. 8567863.

Yu,L. and Liu,H. (2004) Efﬁcient feature selection via analysis of relevance and
redundancy. J. Mach. Learn. Res., 5, 120571224.

Saeys,Y. et a]. (2007) A review of feature selection techniques in bioinformatics.
Bioinformatics, 23, 250772517.

Zaghloul,W. et a]. (2009) Text classiﬁcation: neural networks vs support vector
machines. Ind. Manag. Data Syst., 109, 7087717.

Zhang,L. et a]. (2009) A novel representation for apoptosis protein subcellular
localization prediction using support vector machine. J. T heor. Bio], 259,
3617365.

 

3315

112 /810's1au.1nofp.101xo"seriaurJOJHrorq/ﬁduq 111011 papaopunoq

9103 ‘0g1sn8nv uo ::

