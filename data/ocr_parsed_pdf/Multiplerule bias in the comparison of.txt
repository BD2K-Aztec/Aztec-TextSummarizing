ORIGINAL PAPER

Vol. 27 no. 12 2011, pages 1675-1683
doi:10. 1 093/bioinformatics/btr262

 

Data and text mining

Advance Access publication May 5, 2011

Multiple-rule bias in the comparison of classification rules

Mohammadmahdi R. Yousefil, Jianping Hua2 and Edward R. Dougherty1’2’3a*

1Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX 77843,
2Computational Biology Division, Translational Genomics Research Institute, Phoenix, AZ 85004 and 3Department of
Bioinformatics and Computational Biology, University of Texas M. D. Anderson Cancer Center, Houston, TX 77030,

USA
Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: There is growing discussion in the bioinformatics
community concerning overoptimism of reported results. Two
approaches contributing to overoptimism in classification are (i) the
reporting of results on datasets for which a proposed classification
rule performs well and (ii) the comparison of multiple classification
rules on a single dataset that purports to show the advantage of a
certain rule.

Results: This article provides a careful probabilistic analysis of the
second issue and the ‘multiple-rule bias’, resulting from choosing
a classification rule having minimum estimated error on the dataset.
lt quantifies this bias corresponding to estimating the expected true
error of the classification rule possessing minimum estimated error
and it characterizes the bias from estimating the true comparative
advantage of the chosen classification rule relative to the others by
the estimated comparative advantage on the dataset. The analysis
is applied to both synthetic and real data using a number of
classification rules and error estimators.

Availability: We have implemented in C code the synthetic
data distribution model, classification rules, feature selection
routines and error estimation methods. The code for multiple-rule
analysis is implemented in MATLAB. The source code is available
at http://gsp.tamu.edu/Publications/supplementary/yousefi11a/.
Supplementary simulation results are also included.

Contact: edward@ece.tamu.edu

Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on February 19, 2011; revised on April 9, 2011; accepted
on April 15,2011

1 INTRODUCTION

Three recent articles in Bioiformatics have lamented the difﬁculty
in establishing performance advantages for proposed classiﬁcation
rules (Boulesteix, 2010; Jelizarow et al., 2010; Rocke et al.,
2009). Two statistically grounded sources of overoptimism have
been highlighted. One considers applying a classiﬁcation rule to
numerous datasets and then reporting only the results on the dataset
for which the designed classiﬁer possesses the lowest estimated
error. The optimistic bias from this kind of dataset picking is
quantitatively analyzed in Youseﬁ et al. (2010), where it is termed

 

*To whom correspondence should be addressed.

‘reporting bias’ and where this bias is characterized as a ﬁinction of
the number of considered datasets. A second kind of overoptimism
concerns the comparison of a collection of classiﬁcation rules by
applying the classiﬁcation rules to a dataset and comparing them
according to the estimated errors of the designed classiﬁers. This
kind of bias, which we will call ‘multiple-rule bias’, has been
considered in Boulesteix and Strobl (2009) by applying a battery of
classiﬁcation rules to colon cancer and prostate cancer datasets and
then examining the effects of choosing classiﬁcation rules having
minimum cross-validation error estimates.

Whereas the thrust of Boulesteix and Strobl (2009) is to compare
the sources of multiple-rule bias in classiﬁcation rules, namely, gene
selection, parameter selection and classiﬁer function construction,
our interest is in studying multiple-rule bias as a function of the
number of rules being considered. In particular, we are interested
in the joint distribution, as a function of the number of compared
rules, between the minimum estimated error among a collection
of classiﬁcation rules and the true error for the classiﬁcation rule
having minimum estimated error, as well as certain moments
associated with this joint distribution. Although different with regard
to distributional speciﬁcs, this approach is analogous to the approach
taken in Youseﬁ et al. (2010), where the joint distribution involved
the minimum estimated error of the designed classiﬁer over a
collection of datasets and the true error of the designed classiﬁer on
the population corresponding to the dataset resulting in minimum
estimated error. This is a natural way to proceed because any bias
ultimately results from inaccuracy in error estimation, so that the
behavior of the joint distribution of interest and its moments are
consequent to the joint distribution of the error estimator and the true
error. Owing to the methodology in Boulesteix and Strobl (2009), it
would have been impossible for them to study this joint distribution
because they never concern themselves with true errors, only cross-
validation estimates. Hence, when they compare a minimal error to
a baseline error to arrive at a measure of optimistic bias, they are
comparing cross-validation estimates.

In characterizing multiple-rule bias, we begin with a more
general framework than the one just described; rather than simply
considering multiple classiﬁcation rules, we consider multiple
classiﬁer rule models, so that there are not only multiple
classiﬁcation rules, but also multiple error estimation rules being
employed. We deﬁne a classiﬁer rule model as a pair (W, E), where \I’
is a classiﬁcation rule, including feature selection if feature selection
is employed, and E is an error estimation rule (Dougherty and Braga—
Neto, 2006). The scenario in the preceding paragraph results where
there is only a single error estimation rule.

 

© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 1675

112 /3.Io's[Bumo[pJOJXO'sotwurJOJutotqﬂ:duq 11101} papeolumoq

9103 ‘Og anﬁnv uo ::

M.R.Yousefi et al.

 

2 SYSTEMS AND METHODS

We consider r classiﬁcation rules. \Ill,\112,...,\ll,. and s error
estimation rules. E1,  E5. on a feature-label distribution
F. These are combined to form m=rs classiﬁer rule models:
(WI, 300111, 32),  0111, 3.070112, 30,0112, 32), "-7014, 3.0- GiVen
a random sample 8,, of size n drawn from F . the classiﬁcation rules
yield r designed classiﬁers: tiri=\lli(8,,),i=1,2,...,r. The true error
of tlfi is given by EQUC=P(¢I,(X)7£Y). where (X,Y) is a feature-label
pair. For j=1,2,...,s. Ej provides an error estimate. 8:1,, for tiff. Since
the classiﬁcation rules are not identical. neither are the distributions of
aimeﬁtzme, . . . , aim nor are the distributions of 8:31,,  . . . ,  All true-error
and estimated-error distributions are functions of the random sample 8”.
Since all classiﬁcation rules operate on the same sample. the true errors
can be highly correlated. as will be the estimated errors. Without loss
of generality. we assume the classiﬁer models are enumerated so that
Es, [aim] 5 Es, [83m] 5 . .. 5 Es, [arm].
The minimum estimated error is

min_ ‘ 1,1 1,2 1,5 2,1 r,x
est —mm{8estigestt"'~8est~8est1"”8est ' (1)

e
Letting imin and jmin denote the classiﬁer number and error estimator
number. respectively. for which the error estimate is minimum. we have
min_ irnirnfrnin
Eest —5 't -

55

Suppose a researcher wishes to select the best performing of r
classiﬁcation rules on a feature-label distribution F and proceeds by taking
a random sample from F . designing a classiﬁer for each classiﬁcation rule.
and estimating the errors of the designed classiﬁers. If F is known. then the
true error of each designed classiﬁer can be evaluated. the classiﬁer with
minimum true error can be chosen. and the classiﬁcation rule leading to
that classiﬁer be declared ‘ best”. The truly best classiﬁcation rule has the
minimum expected error across all samples from F . so that what is happening
is that a single observation of aim is being used as an estimate for E5" [aims].
On the other hand. if F is unknown as is used in practice. then the errors of the
designed classiﬁers are estimated from sample data and the classiﬁcation rule
leading to the classiﬁer. (trim . with minimum estimated error is chosen as
‘best’. We are assuming that the researcher tries s error estimators and selects

the one with lowest error estimate. In this case. a single observation of egg“ is

being used to estimate Es" [833g ]. the basic performance measure for ‘11,!“in .
At issue is the goodness of this estimation. This involves the distribution of
the deviation A 2 egg“ — E5" [8 ]. which is marginal to the joint distribution
of (aginﬁg‘ﬁg ).

A key performance measure derived from the deviation distribution is the

bias of 23:21,“ as an estimator of E5" [8 ]. namely.

1min
true

1min
true

Bias(m,ri)=E$,1 [A] 2E3" [531"] —E5" [533;]. (2)

Estimation is optimistic if Bias(m, n) < 0. This can happen even if none of the
error estimation rules are optimistically biased. that is. even if E5" [8%] 3

Es" [aims] for i: 1,2, . . .,r andj=1,2,...,s. Indeed. even ifthis is so. owing
min 1

to estimation-rule variance. on any given sample it may be that 86,, <8Lme.
For instance. if among E 1, E2,  ES there is an error estimation rule. such

as leave-one-out cross-validation. that is slightly (pessimistically) biased and

possesses large variance. then we should expect that Es"  < Es" [aims].

In this case. Es"  <Esn [aim] 5E5" [egg] and Bias(m,n) <0. In the
way we have set up the general problem. not only can optimistic bias result
from considering multiple estimated errors among classiﬁers but also from
applying multiple error estimates for each classiﬁer.

From the generic arguments made thus far. we can state two properties
concerning the bias. First. as the number m of classiﬁer models grows.
the minimum of Equation (1) is taken over more estimated errors. thereby
increasing aims —8§;il“ and |Bias(m,n)| 2E5" [83%] —Esn [831"]. Second. as
the sample size n is increased. under the typical condition that the variance
of the error estimator decreases with increasing sample size. E5" [2331,“
increases and |Bias(m,n)| decreases. p

Bias is only one factor in estimating Es" [81min] by 8min. Another is the

LTUC CS1.
variance of egg“. In fact. one should consider the root-mean-square (RMS)

Distribution Sample

 

I

I i
Ly (I);  (I)?
I A

 

 

A I_H
\ f \
1 2 (’3)
1 1 1 2 1' s 1 1 1 2 r s 1 1 2 r s
Eesit 7Eest 7 - - ~ tsedt 59st igesl a ~ ~ ~ tees’t Eest ﬂeest 1' - - 15st
1 2 r 1 2 7‘ ' ' ' ' ' ' ' 1 2 r
strue tetrue 7 ' ' ' fetme Etrue tetrue 7 ' ' ‘ iettue gtme istrue 7 ' ‘ ‘ 7 Etrue
. . . . . . .
R
min imin 1 min imin 2 min imiu T)
E"est. 7 E"true E"est 7 E"true asst 7 E“true

Fig. 1. Multiple-rule testing procedure on a single sample.

‘ min ‘ ‘ imin ‘ ‘ ‘
error for em as an estimator of E5" [8mg]. which is given by

RMS(m, n) = , /E5n [A2] = , /Bias(m, m2 +Var$n [A]. (3)

Even should the bias be small. the estimation will not be accurate if the
deviation variance is large.

As discussed thus far. the classiﬁcation rules are ﬁxed; however. since our
interest is in the number of classiﬁcation rules (and error estimators). not
any particular rule or estimator. we assume there is a total of R classiﬁcation
rules from which we randomly choose r. This corresponds to a situation
where a researcher selects r from among a large collection (R) of potential
classiﬁcation rules and applies s error estimators to each selected rule. Given
r. there are  possible collections of classiﬁcation rules to be combined with
s error estimators to form  possible collections of classiﬁer rule models of
size m. Hence. the number m of classiﬁer rule models increments according
to s, 2s, . . . ,Rs. We denote the collection of classiﬁer models of size m by (1),".
To get the distribution of errors. one needs to generate independent samples
from the same feature-label distribution and apply the procedure shown in
Figure 1.

The previously discussed performance measures must be adjusted to take
model randomization into account. Given a sample 8”. for a realization of
<1)", we ﬁnd an expected deviation according to Equation (2). but now we
have a random process generating the realizations so we have to take the
expectation over that process to obtain the expected bias.

BiasAv(m.n)=Es, [Es, [Alena]. (4)
where the subscript indicates the average over (1),". A similar averaging arises

with the deviation variance to yield VarAV(m,n)=E¢m [Var$,1 [A|<1>,,,]]. The
RMS now takes the form

RMSAv(m,n)=Eq>,, [‘iEs,[A2|<I>ml]- (5)

1 1 » t t t l ’
Hav1ng discussed the performance measures relating to estimating Es" [egg
min

by as“ . we now consider the comparative advantage of the classiﬁcation rule

\Ilimin. Its true comparative advantage with respect to the other considered
classiﬁcation rules is
. g 1 .
l
Am=Es. [sag] — —r_, 2 Es. [stuel- <6)
i¢imin
Its estimated comparative advantage is given by
~ 1 » - ,
Asi=822‘,“— —r_1 2 star. (7)
i¢imin

where we use the error estimator associated with the pair (imimjmin) for
which the minimum estimated error is obtained (assuming that this would

 

1 676

112 /3.IO'S[1211,1110prOJXO'SOIJBLUJOJIIIOIq”K1111] wort papeolumoq

9103 ‘0g1sn8nv uo ::

Multiple-rule bias

 

be the error estimator chosen by a researcher for the sake of consistency).
The expectation, E$n[Aesl]. of this distribution gives the mean estimated
comparative advantage of ‘11,»min with respect to the collection. A key
bias to be considered is Cbias =Esn[AeS,] —ALme because it measures over
optimism in comparative performance. In the case of randomization. the
true comparative advantage becomes E4)”, [Auue|<1>m] and the mean estimated
comparative advantage becomes E¢m [Es,, [ACS,|<1>,,,]].

2.1 Simulation design

We use a general model based on multivariate Gaussian distributions with a
blocked covariance structure that conforms to various observations made
in microarray expression-based studies (Hua et al.. 2009). A battery of
distribution models can be constructed by changing model parameters to
generate different synthetic data samples. We also consider four real datasets.

2.1.1 Synthetic data In microarray studies. assuming a blocked
covariance matrix is a way of modeling groups of interacting genes where
there is negligible interaction between the groups. It has been used in genomic
classiﬁcation to model genes collected into distinct pathways, each pathway
being represented by a block (Dougherty et al.. 2007; Shmulevich and
Dougherty, 2007). As explained in Hua et al. (2009). although the model does
not embrace all details of the experimental procedures. it is general enough to
include major aspects and various complexity levels suitable for simulation
of real-world scenarios. Sample points are taken from two equally likely
classes, C0 and C1, having D features. Furthermore. by putting c equally
likely subclasses in C1, each having its own distribution, one can model
cases like different stages or subtypes of a cancer. Each sample point in C1
belongs to one and only one of these subclasses.

Features are categorized into two major groups. markers and non-markers.
Markers resemble genes causing disease or susceptibility to disease. The
groups have different class-conditional distributions for the two classes.
They can be further categorized into two different types: global and
heterogeneous markers. Global markers are homogeneously distributed
among the two classes with ng-dimensional Gaussian distributions and
parameters (05m,2%m) for class 0 and (prim,2%m) for class 1, where
ng is the total number of global markers. Heterogeneous markers are
divided into c subclasses within class 1. Each subclass is associated with
Dhm mutually exclusive heterogeneous markers having Dhm-dimensional
Gaussian distributions with parameters (111“, 211"“). The sample points not
belonging to this particular subclass are considered to have Dhm-dimensional
Gaussian class-conditional distributions with parameters (03m, 23‘“).

Assuming that global and heterogeneous markers possess identical
covariance structures. we use {20, 21} instead of {25321311} and
{23m,211‘m}. We assume that 202082 and 2120122, where 0'8 and 012
can be different. and that 2 has the following block structure:

2,00...0
02,0...0
>3: . ._ ...
00...0>:,,

where 2 p is a l><l matrix. with 1 on the diagonal and p off the diagonal.
In the block-based covariance structure. the markers are divided into equal-
size blocks of size I. Markers of different blocks are uncorrelated. while all
the markers in the same block are correlated to each other with correlation
coefﬁcient p.

A consequence of having unequal variances and subclasses in the class-
conditional distributions is to introduce non-linearities in the decision
boundaries for the model, where less global markers and larger difference
in the variances lead to a more non-linear decision boundary. Because the
global markers and the heterogeneous markers possess the same structure.
we can assume the same mean vectors {11.0, 11.1} for both groups. {ugﬂuﬂgm}
and {11.3m,idl‘m}. as we did for the covariance matrices. Furthermore. we
use the same structure for 11.0 and it] in the form of mo X (1,  1) and

Table 1. Distribution model parameters

 

 

Parameters Values/description

Mean m0 = 0.23,m1 = 0.8 (equal variance)
m0 = 0.11,m1 = 0.9 (unequal variance)

Variances 0'8 2 0.62 , 012 = 0.62 (equal variance)
0'8 = 062,012 2 1.22 (unequal variance)

Block size I: 5

Features D = 20000

Feature block correlation p = 0.8

Subclasses c = 2

Global markers ng = 20

Heterogeneous markers D1,,” = 50

High-variance non-markers D1,, = 2000

Low-variance non-markers DIV = 17 880

 

Table 2. A summary of the real datasets used in this study

 

Dataset Dataset type Feature | sample size

 

Yeoh et al. (2002) Pediatric ALL

Zhan et al. (2006) Multiple myeloma
Chen et al. (2004) HCC

Natsoulis et al. (2005) Drugs response on rats

5077| 149/99
54613|156/78
10237|75/82
8491|120/61

 

m1 X (1, 1, . . . , 1). respectively. where m0 and m1 are scalars (Hua et al.. 2009;
Youseﬁ et al.. 2010).

Similar to the global markers. there are two types of non-markers: high-
variance and low-variance non-markers. The th features belonging to
the former group are uncorrelated and their distributions are described by
pN(m0,o§)+(1—p)N(m1,olz), where m0, m1. 0'8 and 012 take values equal
to the means and variances of the markers. respectively. and p is a random
value uniformly distributed over [0,1]. The DIV remaining features are
uncorrelated low-variance non-markers, each having a Gaussian distribution
with parameters (mopg).

A typical microarray experiment usually contains tens of thousands of
probes (genes) but a small number of sample points, typically less than 200.
We choose the total number of features to be D=20000 and the number
of sample points to be 60 and 120. Two variance settings are considered:
equal variances {0% = 0.62, 012 = 0.62} and unequal variances {0% = 0.62, 012 =
1.22}. For the blocked covariance matrix. we choose block size [:5 and
correlation coefﬁcient p208. giving relatively tight correlation within a
block, which would be expected for a pathway. We do not choose model
parameters in accordance with the Bayes errors or the estimated errors;
rather. we choose them in accordance with achievable true errors seen in
real problems. Table 1 shows the parameters of the distribution models used
in this study. See Youseﬁ et al. (2010) for more details about the choice of
parameters.

2.1.2 Real data This study uses four real datasets from microarray
experiments consisting of more than 150 arrays: pediatric acute
lymphoblastic leukemia (ALL) (Yeoh et al.. 2002). multiple myeloma (Zhan
et al.. 2006), hepatocellular carcinoma (HCC) (Chen et al.. 2004) and drugs
and toxicants response on rats dataset (Natsoulis et al.. 2005). We use n: 60
sample points for training. The remaining sample points are held-out for
computing the true error. To the extent possible, we try to maintain the
original labeling and follow the data preparation directions used in the papers
reporting these datasets. Table 2 shows a summary of the four real datasets.
Full descriptions are presented in the Supplementary Materials.

 

1 677

112 /3.Io's[Bumo[p101x0's011eu1101u101qﬂ:duq 111011 popeo1umoq

9103 ‘0g1sn8nv uo ::

M.R.Yousefi et al.

 

Table 3. Classiﬁer rule models considered in this study

 

Classiﬁcation rule Feature selection Error estimation

 

3NN t-test 5-fold cross-validation
LDA t-test + SFS 10-fold cross-validation
DLDA LOO

NMC

L-SVM

RBF-SVM

 

2.2 Classiﬁcation schemes

Six classiﬁcation rules are considered: 3-nearest neighbors (3NN). linear
discriminant analysis (LDA). diagonal LDA (DLDA). nearest-mean classiﬁer
(NMC). linear support vector machine (L-SVM) and radial basis function
SVM (RBF-SVM). Two different feature selection methods are considered:
t-test and t-test followed by sequential forward search (t-test+SFS). One-
stage feature selection uses the t-test and ﬁve features are selected. For
two-stage feature selection. the number of features is reduced to 500 in
the ﬁrst stage (t-test) and then to 5 by SFS. Three cross-validation error
estimation methods are considered: 5-fold. 10-fold and leave-one-out (LOO).
Combining six classiﬁcation rules with two feature selection methods results
in R: 12 classiﬁcation rules from which we randomly choose r = 1,2, . . . ,R
and design the classiﬁers on one sample. Note that with three error estimators.
there is a maximum of 36 different classiﬁer rule models. Table 3 lists
the classiﬁcation rules. feature selection methods and error estimation
procedures utilized.

To illustrate the deﬁnitions. let us suppose we are only considering two
classiﬁcation rules, LDA and 3NN. without feature selection and two error
estimators. LOO and CV5 (5-fold cross-validation). LDA is based on the
discriminant for the optimal classiﬁer in a Gaussian model (Gaussian class-
conditional densities) with common covariance matrix by plugging the
sample means and pooled sample covariance matrix obtained from the data
into the discriminant. Assuming equally likely classes, LDA assigns x to
class 1 if and only if

(x—21>TE“(x—ic1>:(x—mTE—‘(x—ia). (8)

where it, is the sample mean for class a. 14:0, 1. and 2 is the pooled
sample covariance matrix. While derived under the Gaussian assumption
with common covariance matrix. LDA can provide good results when these
assumptions are mildly violated. For the 3NN rule, the designed classiﬁer is
deﬁned to be 0 or 1 at x according to which is the majority among the labels
of the 3 points closest to x.

In k-fold cross-validation, the given sample 8,, is randomly partitioned
into k folds (subsets) 80»). for i: 1,2, . . . , k. Each fold is left out of the design
process, a (surrogate) classiﬁer. 10,”, is designed on 8,, —8(,), the error of
tit,” is estimated as the counting error tit,” makes on 80»). and the cross-
validation estimate is the average error committed on all folds. If there is
feature selection. then it must be redone for every fold because it is part of the
classiﬁcation rule. In leave-one-out cross-validation, each fold consists of a
single point. Owing to computational requirements. k-fold cross-validation,
k <n. usually involves a random selection of partitions. In general. the bias
of cross-validation is typically slightly pessimistic. provided that the number
of folds is not too small. The problem with cross-validation is that it tends
to be inaccurate for small samples because, for these. it has large variance
(Braga-Neto and Dougherty, 2004) and is poorly correlated with the true
error (Hanczar et al.. 2007). For all error estimators. there is variation
resulting from the sampling process. For randomized cross-validation, a
second variance contribution. referred to as ‘internal variance”. arises from
the random selection of the partitions (see Hanczar and Dougherty, 2010).
The latter does not apply to LOO because only a single set of folds is possible.

Relative to the deﬁnitions in the Section 2. if we let \Ill be LDA. \112 be
3NN. E 1 be LOO and E; be CV5, then there are four classiﬁer rule models:

(LDA, LOO). (LDA, CV5), (3NN. LOO). (3NN. CV5). There are two true

LDA 3NN » . LDA,LOO LDA,CV5 3NN,LOO
errorsgbﬁlmévsand anus . four estimated errors. as“ . est . em
and 8 ’ . and 8mm is the minimum of the four estimated errors.

CS1. CS1.

2.3 Implementation

The raw output of the synthetic data simulation consists of the true
and estimated error pairs resulting from applying the 36 different
classiﬁcation schemes on 10000 independent random samples drawn from
the aforementioned four different distribution models. We approximate
the expected true error by taking the average of true errors of each
classiﬁcation rule over the samples. We generate all  possible collections
of classiﬁcation rules of size r. each associated with s error estimation
rules, resulting in  collections of classiﬁer rule models of size m.
For each collection, we ﬁnd the true and estimated error pairs from the
raw output data. Then. for each sample, we ﬁnd the classiﬁer model
(including the classiﬁcation and error estimation rules) in the collection
that gives the minimum estimated error. We record the estimated error, its
corresponding true error and the classiﬁcation and error estimation rules.
Given the collection, (1),", we compute A and A5,, for each sample. Then,
we apprOXimate EsnlAeSt1q>m19 VHS" [AICDML VESnlA21q>m19 ESnlA1q>m1
and Auue|<l>m. by taking the average over all the samples. Finally, we
approximate Es, [Es, [Alena]. Es, wars, [Alena]. Es, [ Es, [A2|<I>m]].
Ezpm [Esn [ACS,|<1>,,,]] and Ezpm [Auue|<1>m] by taking the average over the
generated collections.

Real data simulations differ somewhat from the synthetic data in the way
that the true and estimated errors are computed. For the synthetic data. we
generate 10000 pairs of samples (training set of size 60 or 120 and test set
of size 5000) from the assumed distribution model. But for a real dataset,
we randomly pick 60 sample points for training (classiﬁer design and error
estimation). The remaining held-out sample points are used to calculate the
true error. We repeat this process 10000 times.

3 RESULTS AND DISCUSSION

The full set of results appears in the Supplementary Material. In
the article, we provide representative examples for each issue.
We consider two cases regarding the error estimators: multiple
error estimators and a single error estimator. For multiple error
estimators, s = 3, we consider all three error estimators at once, and
m=3,6,...,36. For a single error estimator, s: 1, we have three
difference cases, depending on which error estimator is used, and
m = 1,2, ..., 12 for each error estimator. For s: 3, keep in mind that
the simulations are incremented in steps of three, 3,6, ..., 36, because
each classiﬁcation rule is evaluated with all three error estimators,
as would be the case in practice if an investigator were to consider
three error estimators. For a single error estimator, we show LOO
in the article and leave the others to the Supplementary Material.

3.1 Joint distributions

For the synthetic data, the joint distributions are estimated with a
bivariate Gaussian-kernel density estimation method. The ﬁrst set
of results (Supplementary Figures s1—s16) show joint distributions
between the minimum estimated errors and their corresponding true
errors, 82:1“ and 833%, for multiple and single error estimators,
different sample sizes and variances. Each plot includes the
regression line and a small circle showing the pair of sample
means. As m increases, the distributions tend to be more circular
(indicating less correlation) and also more compact (indicating
smaller variance). Furthermore, as m increases, the distributions

 

1 678

112 /3.Io's[Bumo[p101x0's011eu1101u101qﬂ:duq 111011 popeo1umoq

9103 ‘0g1sn8nv uo ::

Multiple-rule bias

 

m=2

 

imin
Etrue

  

est

Fig. 2. The joint distributions between egg“ and  with respect to the
collection size m. for all classiﬁer rule models for m =6, 21 , 36 (left column)
and for single LOO error estimation for m =2, 7, 12 (right column). The real
dataset is multiple myeloma by Zhan et al. (2006). The white line shows
the regression line and the circle indicates the sample mean of the joint

distribution.

move to the left, thereby demonstrating greater bias. Note the smaller
variation for sample size 120.

For the real data, the joint distributions are again estimated
with a bivariate Gaussian-kernel density estimation method.
Supplementary Figures s25—s40 show the joint distribution of
(8213“, 8%?) for multiple and single error estimators, and for different
real datasets. Figure 2 shows the distributions and regression lines
for the myeloma data: the left column is for multiple error estimators
and shows m=6,21,36; the right column is for the single error
estimator LOO and shows m = 2, 7, 12. Similar to the synthetic data,
as m increases, the distributions tend to be more circular, have
smaller variances and move to the left. What is most striking is the
absence of regression between 8221“ and 8131123. This lack of regression
is consistent with what has been observed in other settings when
cross-validation is used to estimate the true error (Hanczar et al.,
2007, 2010).

3.2 Moments and comparative performance

For the synthetic data and the multiple error estimator case, Figure 3
shows: (a) the expected bias, Bias AV; (b) the expected variance,

 

 

 

 

 

 

   

 

 

 

 

   

 

 

 

 

 

 

60 sample points, equal var. --------- -- 120 sample points, equal var.
- - - — — 60 sample points, unequal var. -------------- -- 120 sample points, unequal var.
(a) 0 (b) °~°1 \
\
’0'02 0.008 X
\
\
—0.04
> . 7 ~ ~ -
< 2 0006 _ _ _ _
g —0 06 5..
._. :6
m > 0004
—0.08
_m 0.002
—0.12
3 6 9121518212427303336 3 6 9121518212427303336
Collection size (In) Collection size (In)
(C) o_14 (d) —o.04
—0.05
0.12 _
E —o.os
> E
< °-‘ 2e —o.07
m :5
E 008 Q —0.08
E
“3‘ —o.09
0.06 m
70.1
0.04 —0.1

3 6 9121518212427303336
Collection size (m)

6 9 12151821 2427 303336
Collection size (in)

Fig. 3. (a) The expected bias, BiasAV; (b) the expected variance, VarAV; (c)
the expected RMS, RMSAV; and (d) the expected comparative performance
bias. Ezpm [Cbiasl<1>m]: resulted from the distributions of egg“ and  on the
synthetic data for all 36 classiﬁcation models. with respect to the collection

size m.

VarAV; (c) the expected RMS, RMSAV; and (d) the expected
comparative performance bias, Cbias. Note that Figure 3d does not
graph m=s because the comparative advantages are not deﬁned
when r: 1. The same applies for analogous subﬁgures in the rest
of the article. For increasing m, the bias and RMS get worse, but
even with m=3, the RMS is about 0.1 for sample size 60. For
this sample size and m: 36, the comparative-performance bias has
reached —0.1. Figure 4 shows corresponding results for a single
error estimator, LOO. These too are especially alarming for n=60.
Note that the RMS, actually, has a temporary small dip at m=2,
which is a result of steep decline in variance between m=1 and
m = 2.

For the real data, Figures 5 and 6 show corresponding results to
Figures 3 and 4, respectively (ignore for the moment the ‘average’
curves, which will soon be discussed). Note the widely different
behaviors between the different datasets. Since we are using the ﬁill
dataset as an empirical distribution to serve as an approximation
of the underlying feature-label distribution and are sampling from
the empirical distribution, the different biases result from different
behaviors of the error estimators on the different distributions. In
practice, given a single sample dataset, one would have no idea of
what kind of biases and RMS deviations to expect. This uncertainty
exempliﬁes the standard conundrum faced when one lacks prior
information regarding the feature-label distribution. In our case, the
problem is that error estimator performance is heavily dependent on
the underlying feature-label distribution.

 

1 679

112 £10s1tzuino[prontosoutzuiio1u1o1q//:dnq 111011 popeo1umoq

9103 ‘0g1sn8nv uo ::

M.R.Yousefi et al.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

60 sample points, equal var. --------- -- 120 sample points, equal var. Chen et al. (2004) --------- -- Yeoh et al. (2002) —e— Average
- — — — - 60 sample points, unequal var. -------------- -- 120 sample points, unequal var. - - - - - Natsoulis et al. (2005) -------------- -- Zhan et al. (2006)
73
10
(a) 0.02 (b) 0.02 (a) 0 (b) 3 X
o 2.5
0.015
> —o.02 2
< > >
 70.04 g 0.01 s; 1,5
m > :>
—0.06 1
0.005
'0-03 0.5
—0.1 -0.1
12345573910119 1 23435189101112 369121518212427303336 369121518212427303336
(3011309011 5123 (“11 couecn‘m 5125 (“1) Collection size (111) Collection size (m)
(d) —0.02 (C) 0.12 (d) —0.02
4'03 M
—0.04 0.1 _ .3
E E —0.04 '~ ...... w
e ---------------- ..
% —0.06 g 0.08 E ’0-05 4,
._ m ._
1:. .c: —0.06
D 2 g
as —o.oe of, 0-06 E —0.07
e e
—0.08
m 70.1 0.04 m
—0.09
0.04 —0.12 0.02 —0

123456789101112
Collection size (in)

23456789101112
Collection size (m)

Fig. 4. (a) The expected bias. BiasAV; (b) the expected variance, VarAV; (c)
the expected RMS, RMSAV; and (d) the expected comparative performance
bias, Ezpm [CbiaS|<1>m]: resulted from the distributions of egg“ and  on the
synthetic data for single LOO error estimation, with respect to the collection

size m.

3.3 Averaging over different populations

The problem of multiple-rule bias is exacerbated if one combines
it with applying the multiple rules across multiple datasets (Youseﬁ
et al., 2010) and then minimizes over both the classiﬁer models and
datasets; however, using multiple datasets can mitigate multiple-rule
bias if performances are averaged over the datasets. In this case,
each dataset is a sample from a feature-label distribution Fk, k:
1,2, . . .,K, and our concern is with average performance over the K
feature-label distributions. Assuming multiple error estimators, there
are m classiﬁcation rules being considered over the K feature-label
distributions. Our interest is now with

1 K 1 K

min - lslsk 1,2,/C

east (K)_m1n — 2 east ,—2 east ,...,
k=1 k=1

1 K 1 K 1 K
1,s,k 2,1,k r,s,k
Ezsest ’Ezsest "“7Ezsest (9)
k :1 k :1 k =1
' ‘ k . . .
where  is the estimated error of class1ﬁer Ill,- and the error
estimation rule E j on the dataset from feature-label distribution F k.

The bias takes the form

K
~ 1 ' . k
B(m,n,K)=E[s§;1“(K)] — E ZE [.9313 ] (10)
k=1
where 8:311"! is the true error of classiﬁer Illimm on the dataset from

feature-label distribution Fk.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

3 6 9121518212427303336
Collection size (In)

6 9 1215 1821 2427 303336
Collection size (in)

Fig. 5. (a) The expected bias, BiasAV; (b) the expected variance, VarAV; (c)
the expected RMS, RMSAV; and (d) the expected comparative performance
bias. Ezpm [Cbiagl<1>m]: resulted from the distributions of egg“ and  on the
real data for all 36 classiﬁcation models, with respect to the collection size m.

The ‘average’ curves in Figures 5 and 6 illustrate the effects of
averaging. They show less estimation bias, less variance, smaller
RMS and less comparative-performance bias when averaging is
employed, as opposed to using the datasets individually. The
situation is similar, albeit a bit more complicated, than the argument
in Youseﬁ et al. (2010) for averaging results over a large number of
datasets. Here, the averaging is done to mitigate multiple-rule bias.

It is possible to obtain theoretical results regarding the effect
of averaging on the bias of Equation (10). In particular, if all
error estimators are unbiased, then we prove in the Appendix A
that limK_,OO3(m,n,K)=0. If one looks closely at the proof, it
is clear that the unbiasedness assumption can be relaxed in each
of the lemmas. In the ﬁrst lemma, which shows that B(m,n,K) f
0, we need only assume that none of the error estimators is
pessimistically biased. In the second lemma, which shows that
limK_,003(m,n,K)30, unbiasedness can be relaxed to weaker
conditions regarding the expectations of the terms making up the
minimum in Equation (9); however, these conditions are rather
arcane and do not add much practical insight. Moreover, we are
interested in close-to-unbiased error estimators, speciﬁcally, the
cross-validation estimators, so that we can expect |B(m,n,K)| to
diminish with averaging, even if the limit of B(m,n,K) does not
actually converge to 0.

3.4 Concluding remarks

From a practical standpoint, the results obtained in the present paper
quantitatively demonstrate the large degree of overoptimism that
results from comparing classiﬁer rule models via their performances
on a small dataset owing to the inaccuracy of error estimation on

 

1680

112 /310's1eu1n0fp101x0'so1112111101u101q//:d1111 111011 pop1201um0q

9103 ‘0g1sn8nv 110 ::

Multiple-rule bias

 

 

 

Chen et al. (2004) --------- -- Yeoh et al. (2002) —e— Average

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

- - - - - Natsoulis et al. (2005) -------------- -- Zhan et al. (2006)
*3
x 10
(a) 0.02 (b) 6
o
> —0.02
i
in —0.04
70.06
—o.08
123456789101112 123456789101112
Collection size (111) Collection size (in)
(C) 0.1 —0.02
0-09 —o.03
0.08 4.04
> 0.07
<1 —o.05
‘0 0.06
E 70.06
0.05
0.04 7°07
0.03 —0.08
0.02 —o.09

123456789101112
Collection size (in)

23456789101112
Collection size (in)

Fig. 6. (a) The expected bias. BiasAV; (b) the expected variance, VarAV; (c)
the expected RMS, RMSAV; and (d) the expected comparative performance
bias, Ezpm [CbiaS|<1>m]: resulted from the distributions of egg“ and  on
the real data for single LOO error estimation, with respect to the collection

size m.

small samples. As the array of simulations show, optimistic bias
accrues rapidly with even a small number of models being compared.
We have observed from both simulations and theoretical analysis that
the problem can be mitigated by averaging performances across a
family of datasets; indeed, this is the recommendation that we put
forth. The downside is that averaging eliminates the possibility of
comparing classiﬁcation rule performances on a single population.
In fact, the latter possibility has been precluded at the outset by the
experimental design: too small of a sample to obtain accurate error
estimates. If there is only a single small sample, then the multiple-
rule bias precludes any conclusions whatsoever, whereas at least if a
collection of datasets are employed, then one may be able to make a
conclusion relative to the collection of populations (depending on the
accuracy of the error estimator). Even with averaging, we must offer
a word of caution. While the proposition we have proven regarding
the convergence of B(m,n,K) is promising, like most distribution-
free results it leaves open the rate of convergence, which in practice
determines the number of datasets one must utilize to reduce the bias
to some predetermined level. This leads us to some ﬁnal comments.

The concerns expressed regarding the difﬁculty of establishing
performance advantages for proposed classiﬁcation rules
(Boulesteix, 2010; Jelizarow et al., 2010; Rocke et al., 2009) reﬂect
ﬁindamental epistemological issues confronting bioinformatics
as it addresses the high-throughput environment with limited
sample sizes and limited statistical knowledge of how to deal with
this new world (Dougherty and Braga—Neto, 2006; Dougherty,
2008; Mehta et al., 2004). The problem addressed in this article
arises from the bias and variance, and therefore the RMS, of

error estimators. Very little is known about the performance of
common error estimators, in particular, cross-validation. To take
a salient example: LOO. Prior to 2009, all that was known about
LOO for LDA and Gaussian class-conditional distributions were
asymptotic expressions for the expectation and variance of the
estimator in one dimension (Davison and Hall, 1992). In 2009, the
distribution of the LOO estimator was discovered in this model for
an arbitrary dimension 111 without assuming a common variance
for m=1 and assuming a common covariance matrix with m > 1
(Zollanvari et al., 2009). Still, none of these results treated the
joint distribution of the estimated and true errors, nor, in particular,
the RMS. In 2010, the joint distribution was found exactly for
m = 1 without assuming a common variance and an approximation
was found for m>1 assuming a common covariance matrix
(Zollanvari et al., 2010). Besides the joint distribution via complete
enumeration (Xu et al., 2006) and the correlation (Braga-Neto
and Dougherty, 2010) for multinomial discrimination, there are
no other analytic results regarding the joint behavior of LOO
with the true error. This dearth of results is striking considering
that LOO was ﬁrst proposed in 1968 (Lachenbruch and Mickey,
1968), it has been used extensively, and the variance problems of
LOO have been known from at least 1978 (Glick, 1978). As for
more complicated cross-validation estimators that require random
resampling, essentially nothing is known. If lamentations regarding
the lack of performance characterization in bioinformatics are to
be abated, then much greater knowledge regarding error estimation
must be discovered.

ACKNOWLEDGEMENTS

We would like to thank the High-Performance Biocomputing Center
of TGen for providing the clustered computing resources used
in this study; this includes the Saguaro-2 cluster supercomputer,
a collaborative effort between TGen and the ASU Fulton High
Performance Computing Initiative.

Funding: National Institutes of Health grant (Saguaro-2 cluster,
lSlORR025056-01, in part) and the National Science Foundation
(CCF-0634794).

Conﬂict of Interest: none declared.

REFERENCES

Boulesteix,A.-L. (2010) Over-optimism in bioinformatics research. Bioinformatics, 26,
437439.

Boulesteix,A.-L. and Strob1,C. (2009) Optimal classiﬁer selection and negative bias
in error rate estimation: an empirical study on high-dimensional prediction. BM C
Med. Res. Met/1., 9, 85.

Braga-Neto,U.M. and Dougherty,E.R. (2004) Is cross-validation valid for small-sample
microarray classiﬁcation? Bioinformatics, 20, 3747380.

Braga-Neto,U.M. and Dougherty,E.R. (2006) Exact performance of error estimators for
discrete classiﬁers. Pattern Recognit, 38, 179971814.

Braga-Neto,U.M. and Dougherty,E.R. (2010) Exact correlation between actual and
estimated errors in discrete classiﬁcation. Pattern Recognit. Lett., 31, 407412.
Chen,X. et al. (2004) Novel endothelial cell markers in hepatocellular carcinoma.

Modern Pat/10L, 17, 119871210.

Davison,A.C. and Ha11,P. (1992) On the bias and variability of bootstrap and cross-
validation estimates of error rate in discrimination problems. Biometrika, 79,
2797284.

Dougherty,E.R. and Braga-Neto,U.M. (2006) Epistemology of computational biology:
mathematical models and experimental prediction as the basis of their validity.
J. Biol. Syst, 14, 65790.

 

1681

112 /310's1eu1n0fp101x0'so1112111101u101q//:duq 111011 pop1201um0q

9103 ‘0g1sn8nv 110 ::

M.R.Yousefi et al.

 

Dougherty,E.R. et al. (2007) Validation of computational methods in genomics. Curr
Genomics, 8, 1719.

Dougherty,E.R. (2008) On the epistemological crisis in genomics. Curr Genomics, 9,
69479.

Glick,N. (1978) Additive estimators for probabilities of correct classiﬁcation. Pattern
Recognit., 10, 2117222.

Hanczar,B. et al. (2007) Decorrelation of the true and estimated classiﬁer errors in
high-dimensional settings. EURASIP J. Bioinform. Syst. Biol, 2007, 38473.

Hanczar,B. and Dougherty,E.R. (2010) On the comparison of classiﬁers for microarray
data. Curr Bioinformatics, 5, 29739.

Hanczar,B. et al. (2010) Small-sample precision of ROC-related estimates.
Bioinformatics, 26, 8227830.

Hua,J. et al. (2009) Performance of feature selection methods in the classiﬁcation of
high-dimensional data. Pattern Recognit, 42, 409424.

Jelizarow,M. et al. (2010) Over-optimism in bioinformatics: an illustration.
Bioinformatics, 26, 199071998.

Lachenbruch,P.A. and Mickey,M.R. (1968) Estimation of error rates in discriminant
analysis. Technometrics, 10, 1711.

Mehta,T. et al. (2004) Towards sound epistemological foundations of statistical methods
for high-dimensional biology. Nat. Genet, 36, 9437947.

Natsoulis,G et al. (2005) Classiﬁcation of a large microarray data set: algorithm
comparison and analysis of drug signatures. Genome Res, 15, 7244736.

Rocke,D.M. et al. (2009) Papers on normalization, variable selection, classiﬁcation or
clustering of microarray data. Bioinformatics, 25, 7017702.

Shmulevich,I. and Dougherty,E.R. (2007) Genomic Signal Processing. Princeton
University Press, Princeton.

Xu,Q. et al. (2006) Conﬁdence intervals for the true classiﬁcation error conditioned on
the estimated error. Technol. Cancer Res. Treat, 5, 5797589.

Yeoh,E.J. et al. (2002) Classiﬁcation, subtype discovery, and prediction of outcome in
pediatric acute lymphoblastic leukemia by gene expression proﬁling. Cancer Cell,
1, 1337143.

Youseﬁ,M.R. et al. (2010) Reporting bias when using real data sets to analyze
classiﬁcation performance. Bioinformatics, 26, 68776.

Zhan,F. et al. (2006) The molecular classiﬁcation of multiple myeloma. Blood, 108,
202072028.

Zollanvari,A. et al. (2009) On the sampling distribution of resubstitution and leave-
one-out error estimators for linear classiﬁers. Pattern Recognit, 42, 270572723.

Zollanvari,A. et al. (2010) Joint sampling distribution between actual and estimated
classiﬁcation errors for linear discriminant analysis. IEEE Trans. Inform. Theory,
56, 7844504.

APPENDIX A

We prove that if all error estimators are unbiased, then
limK_,OO3(m,n,K)=0.

LEMMA A. 1. If all error estimators are unbiased, then B(m, n, K) g 0.

PROOF. Deﬁne the set Sn={8,1,83,...,8,{<}, where 85, k:
1,2,...,K is a random sample taken from the distribution Fk for
k: 1,2, ...,K. Also, we can rewrite Equation (9) as

K

. . 1 . . k
Sgltfl(l{)=ni11jn Ezsgg; , (A.1)
’ k=1

where i: 1,2, ...,r andj: 1,2, ...,s. Owing to the unbiasedness of
isj.
est

the error estimators, E51 [8 k] = E51. [Sal/:6]. Referring to Equations

(10) and (Al), we have

B(m,n,K)

K

~ 1 ' . k

=Es. [8221101 — E 2E8: [411: ]
k=1

1 K
iminsk
_ E 2E8; [Strue :l
k=1

1 K ' k l K ' k
- ls]: 1min:
5 mm E8" E Zsest _ E 2E5; [Strue :l
1” =1 k=1

1 K k 1 K
_ ' ids imimk
—  fig—1138" [80st :1 _ fig—1:135}: [Strue :l

1 K -~k 1 K -
=H.1i.n -ZEsk[Sls’§£] ——ZEsk[Si¥lTs’kj
1” Kk=1 " Kk=1 "

 

1 K .k 1 K . k
. l, 1min:
=0“ 2 ZESi 18ml ‘ 221581 18mm 1
E 0. (A2)

where the relations in the third and sixth lines result from Jensen’s
inequality and unbiasedness of the error estimators, respectively.
El

LEMMA A.2. If all error estimators are unbiased, then
limK_,OO3(m,n,K)30.

PROOF. Let

K

.. 1 -~k
l,]__§ : ls],
A _K 80st 7

K
. 1 .k
T1 = E E E5". [4%]. (A3)
k=1 k=1

Owing to the unbiasedness of the error estimators,
E5,1 [Ai’j] = Ti 3 1. Without loss of generality, we assume
T1 f T2 f  f T’ . To avoid cumbersome notation, we will
further assume that T1 < T2 (with some adaptation, the proof goes
through without this assumption). Let 26 = T2 — T1 and

S

B, = ﬂ<T1—6§A1’j 3T1+6)

ﬂ  [Ai’j ] > T1+6). (A.4)
i,j,k

Because leest | f 1, Varsi1 [AM] 3 l/K. hence, for 1'> 0, there exists
KSJ such that KngsI implies P(B,3(K)) > 1— 1'. Hence, referring
to Equation (10), for K EKSJ,

Es, [8221“ (10] = Es, [8221“(K) | Bs]P(Ba)
+E5n [$3th | Bg]P(Bg)

2 Es, [smi“(K) | Bs]P(Bs)

est
=E5n [minjAl’j]P(B,g)
3 (11 —a)(1 — 1'). (A5)

 

12 /310's12u1n0fp101x0'so112111101u101q//:d1111 111011 pop201um0q

9103 ‘0g1sn8nv 110 ::

Multiple-rule bias

 

Again referring to Equation (10) and recognizing that imin = l in 3,3,
for K 3 K5,“

K K

1 - , k 1 ' . k

E 2E5". [8336’ ]= E 2 (E51. [4111; |B,g]P(B,g)
k=1 k:

,_.

+Es, [4111" IB§]P<B§>)
1 K 1 k
5 E1205; [8,1, lBs]P(Bs)

+P<Bg>)

g T1 +1. (A.6)

Putting Equations (A5) and (A6) together and referring to Equation
(10) yields, for K EKSJ,

B(m,n,K)3(T1—6)(l—1')—T1—1'3—(21'+6) (A7)

Since 6 and 1' are arbitrary positive numbers, this implies
that for any 11>0, there exists Kn such that Kan implies
limK_,OOB(m,n,K)30, which is precisely what we want to
prove. D

Combining Lemmas Al and A2, we have proven that
limK_,OO3(m,n,K)=0 under the assumption that all the error
estimators are unbiased.

 

12 /310's12u1n0fp101x0'so112111101u101q//:d1111 111011 pop201um0q

9103 ‘0g1sn8nv 110 ::

