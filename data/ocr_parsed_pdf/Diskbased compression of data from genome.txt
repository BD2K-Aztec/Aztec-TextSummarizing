Bioinformatics, 31(9), 2015, 1389—1395

doi: 10.1093/bioinformatics/btu844

Advance Access Publication Date: 22 December 2014
Original Paper

 

Sequence analysis

Disk-based compression of data from genome
sequencing
Szymon GrabowskiI, Sebastian Deorowic22'* and tukasz Roguski3'4

1Institute of Applied Computer Science, Lodz University of Technology, Al. Politechniki 11, 90-924 Lodz, 2Institute of
Informatics, Silesian University of Technology, Akademicka 16, 44-100 Gliwice, 3Polish-Japanese Institute of
Information Technology, Koszykowa 86, 02-008 Warszawa, Poland and 4Centre Nacional de Analisis Genémico
(CNAG), 08-028 Barcelona, Spain

*To whom correspondence should be addressed.
Associate Editor: John Hancock

Received on September 19, 2014; revised on November 27, 2014; accepted on December 17, 2014

Abstract

Motivation: High—coverage sequencing data have significant, yet hard to exploit, redundancy. Most
FASTO compressors cannot efficiently compress the DNA stream of large datasets, since the re—
dundancy between overlapping reads cannot be easily captured in the (relatively small) main
memory. More interesting solutions for this problem are disk based, where the better of these two,
from Cox et al. (2012), is based on the Burrows—Wheeler transform (BWT) and achieves 0.518 bits
per base for a 134.0 Gbp human genome sequencing collection with almost 45—fold coverage.

Results: We propose overlapping reads compression with minimizers, a compression algorithm
dedicated to sequencing reads (DNA only). Our method makes use of a conceptually simple and
easily parallelizable idea of minimizers, to obtain 0.317 bits per base as the compression ratio,

 

allowing to fit the 134.0 Gbp dataset into only 5.31 GB of space.
Availability and implementation: http://sun.aei.polsl.pl/orcom under a free license.

Contact: sebastia n.deorowicz@polsl.pl

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

It is well known that the growth of the amount of genome sequenc-
ing data produced in the last years outpaces the famous Moore’s law
predicting the developments in computer hardware (Deorowicz and
Grabowski, 2013; Kahn, 2011). Confronted with this deluge of
data, we can only hope for better algorithms protecting us from
drowning. Speaking about big data management in general, there
are two main algorithmic concerns: faster processing of the data (at
preserved other aspects, like mapping quality in de novo or referen-
tial assemblers) and more succinct data representations (for com-
pressed storage or indexes). In this article, we focus on the latter
concern.

Raw sequencing data are usually kept in FASTQ format, with
two main streams: the DNA symbols and their corresponding qual-
ity scores. Older specialized FASTQ compressors were lossless,

squeezing the DNA stream down to about 1.5—1.8 bpb (bits per
base) and the quality stream to 3—4 bpb, but more recently it was
noticed that a reasonable solution for lossy compression of the qual-
ities has negligible impact on further analyzes, for example, referen-
tial mapping or variant calling performance (Cénovas et (11., 2014;
Illumina, 2012; Wan et (11., 2012). This scenario became thus imme-
diately practical, with scores lossily compressed to about 1 bpb
(Janin et (11., 2014) or less (Yu et (11., 2014). Note also that Illumina
software for their HiSeq 2500 equipment contains an option to re-
duce the number of quality scores (even to a few), since it was shown
that the fraction of discrepant single nucleotide polymorphisms
grows slowly with diminishing number of quality scores in
Illumina’s CASAVA package (http://support.illumina.com/sequenc-
ing/sequencingisoftware/casava.ilmn). It is easy to notice that now
the DNA stream becomes the main compression challenge. Even if

(63 The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1389

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

1390

S.Grabowski et al.

 

higher order modeling (Bonfield and Mahoney, 2013) or LZ77—style
compression (Deorowicz and Grabowski, 2011) can lead to some
improvement in DNA stream compression, we are aware of only
two much more promising approaches. Both solutions are disk
based. Yanovsky (2011) creates a similarity graph for the dataset,
defined as a weighted undirected graph with vertices corresponding
to the reads of the dataset. For any two reads s1 and 52 the edge
weight between them is related to the ‘profitability’ of storing s1 and
the edit script for transforming it into 52 versus storing both reads
explicitly. For this graph, its minimum spanning tree (MST) is
found. During the MST traversal, each node is encoded using the set
of maximum exact matches between the node’s read and the read of
its parent in the MST. As a backend compressor, the popular 7zip is
used. ReCoil compresses a dataset of 192M Illumina 36 bp reads
(http://www.ncbi.nlm.nih.gov/sra/SRX001540), with coverage
below 3—fold, to 1.34 bpb. This is an interesting result, but ReCoil is
hardly scalable; the test took about 14 h on a machine with 1.6 GHz
Intel Celeron CPU and four hard disks.

More recently, Cox et al. (2012) took a different approach,
based on the Burrows—Wheeler transform (BWT). Their result for
the same dataset was 1.21 bpb in less than 65 min, on a Xeon
X5450 (Quad—core) 3 GHz processor. The achievement is however
more spectacular if the dataset coverage grows. For 44.5—fold cover—
age of real human genome sequence data, the compressed size im—
proves to as little as 0.518 bpb (Actually in Cox et a1. (2012) the
authors report 0.484 bpb, but their dataset is seemingly no longer
available and in our experiments we use a slightly different one.)
allowing to represent the 134.0 Gbp of input data in 8.7 GB of
space.

Note that if the reference sequence is available, either explicit or
can be reconstructed (‘presumed’, in the terminology of Cénovas
and Moffat 2013), then compressing DNA reads is much easier and
high compression ratios are possible. Several FASTQ or SAM/BAM
compressors make use of a reference sequence, to name Quip (Jones
et al., 2012), Fastqz and qucomp (Bonfield and Mahoney, 2013) in
one of their modes, SlimGene (Kozanitis et al., 2011), CRAM (Fritz
et al., 2011), Goby (Campagne et al., 2013), DeeZ (Hach et al.,
2014) and FQZip (Zhang et al., 2015). Several techniques for com—
pressing SAM files, including mapping reads to a presumed refer—
ence sequence, were also explored in Cénovas and Moffat (2013).

In this article, we present a new reference—free compressor for
FASTQ data, Overlapping Reads COmpression with Minimizers
(ORCOM), achieving compression ratios surpassing the best
known solutions. For the two mentioned human datasets it obtains
the compression ratios of 1.005 and 0.317 bpb, respectively.
ORCOM is also fast, producing the archives in about 8 and 77 min,
respectively, using eight threads on an AMD Opteron 6136 2.4 GHz
machine.

2 Materials and methods

Lets : s]0]s]1] . . .s]n — 1] be a string oflength n over a finite alphabet
2 of size a. We use the following notation, assuming 03i£i<n: s]i]
denotes the (i + 1)th symbol of s, s]i . . . i] the substring s]i]s]i + 1] . . .
 (called a factor of s), and s o t the concatenation of strings s and t.
Our algorithm, ORCOM, follows the ancient paradigm of exter—
nal algorithms: distribute the data into disk bins and then process
(i.e. compress) each bin separately. Still, the major problem with
this approach in reads compression concerns the bin criterion:
how to detect similar (overlapping) reads, in order to pass
them into the same bin? Our solution makes use of the idea of min—
imizers (Roberts et al., 2004), a late bloomer in bioinformatics, cf.

(Chikhi et al., 2014; Deorowicz et al., 2014; Li et al., 2013;
Movahedi et al., 2012; Wood and Salzberg, 2014). Minimizers are a
simple yet ingenious notion. The minimizer for a read 5 of length r is
the lexicographically smallest of its all (r — p + 1) p—mers; usually it
is assumed that 17 < r. This smallest p—mer may be the identifier of
the bin into which the read is then dispatched. Two reads with a
large overlap are likely to share the same minimizer. In the next
paragraphs we present the details of our solution.

Assume the alphabet size a : 5 (ACGTN). A reasonable value of
p is about 10, but sending each bin to a file on disk would require
510 : 9.77M files, which is way too much. Reducing this number to
410 + 1 (all minimizers containing at least one symbol N, which are
rare, are mapped to a single bin, labeled N) is still not satisfactory.
We solved this problem in a radical way, using essentially one (In
fact, there is one extra file, with metadata, yet this one is of minor
overall importance and we skip further description.) temporary file
for all the bins, using large output buffers. To fetch the bin data in a
further stage, the file has to be opened to read and the required reads
extracted to memory from several locations of the file.

As DNA sequences can be read in two directions: forwards and
backwards (with complements of each nucleotide), we also process
each read twice, in its given and reverse—complemented form.
Additionally, we introduce a ‘skip zone’, that is, do not look for
minimizers in read suffixes of (default) length z: 12 symbols.
This allows for some improvement in read ordering in the next step.
The minimizers are thus sought over 2(r — z — p + 1) resulting
p—mers. We call them canonical minimizers.

However, a problem with strictly defined minimizers is uneven
bin distribution. This not only increases the peak memory use, but
also hampers parallel execution as the requirement for load balanc—
ing is harder to fulfill. To mitigate these problems we forbid some
canonical minimizers, namely those that contain any triple AAA,
CCC, GGG or TTT or at least one N (some of them, especially min—
imizers with runs of three or more As, are frequent). The allowed
canonical minimizers are further called signatures, a term that we
also used (with a slightly different definition) for the minimizers
used in KMC 2, a k—mer counting algorithm (Deorowicz et al.,
2014).

In the next step, when the disk bins are built, we reordered the
reads in bins to move overlapping reads possibly close to each other.
From a few simple sort criteria tried out, the one that worked best
was to sort the reads 5,, for all i, according to the lexicographical
order of the string s,[j. . .r — 1] o 5,]0. . ./ — 1], where j is the begin—
ning position of the signature for the read 5,. Such a reordering has a
major positive impact on further compression. The reason is that
overlaping reads are with high probability close to each other in the
reordered array. The size of the skip zone should be chosen care—
fully. When too small, some signatures will be found close to the
end of the read and the first factor of the sorting criterion,
s,[j. . .r — 1], will be too short to have a good chance of placing the
read among those that overlap it in the genome. On the other hand,
with the zone being too long many truly overlapping reads will be
forbidden.

The last phase is the backend compression on bin—by—bin basis.
We devised a specialized processing method, which produces several
(interleaving) streams of data, finally compressed with either a
well—known context—based compressor PPMd (Shkarin, 2002) or a
variant of arithmetic coder (Salomon and Motta, 2010) (We use a
popular and fast arithmetic coding variant by Schindler (http://
www.compressconsult.com/rangecoder/), also known as a RC.).
How we process the bins in detail, including careful mismatch han—
dling, is presented in the next paragraphs.

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

Disk—based compression of data from genome sequencing

1391

 

We maintain a buffer (sliding window) of m previous reads, stor—
ing also the position of the signature in each read. For each read, we
seek the read from the buffer which maximizes the overlap. The dis—
tance between a pair of considered reads depends on the number of
elementary operations transforming one into another. For example,
if the pair of reads is:

AACGTXXXXCGGCAT ,
CCTXXXXCGGCATCC ,

where XXXX denotes a signature, we match them after a (concep—
tual) alignment:

AACGTXXXXCGGCAT ,
CCTXXXXCGGCATCC ,

to find that they differ with one mismatch (G versus C) and 2 end
symbols of the second read have to be inserted, hence the distance is
cm X 1 + c, X 2, where cm and c; are the mismatch and the insert
cost, respectively. The default values for the parameters are: cm :2
and c1: 1, and they were chosen experimentally. In our example,
the final distance is thus 2 X 1 + 1 X 2 : 4. The read among the m
previous ones that minimizes such a distance, and is not greater than
max_dist, set by default to a half of read length, is considered a refer—
ence for the current read.
Next, the referential matching data are sent into a few streams.

Flags
Values from {fcopw fdiss, fex, fmisfmh}, with the following meaning:

° fcopy—the current read is identical to the previous one,

° fdiss—the read is not similar to any read from the buffer; more
precisely, the similarity distance exceeds a speciﬁed threshold
maxdista

° fax—the read overlaps with some read from the buffer without
mismatches (only its trailing symbols are to be encoded),

° fmiS—the read overlaps with some read from the buffer
with exactly one mismatch at the last position of the referenced
read,

° foth—the read overlaps with some read from the buffer, but not
in a way corresponding to ﬂags fax or fmis,

Lengths

Read lengths are stored here (1 byte per length in the current imple—
mentation, but a simple byte code can be used to handle the general
case).

The five streams: lettersN, lettersA, lettersC, lettersG, lettersT
(These are used only if ‘Flag’ is fex, fmis or fmh.)

‘LettersN’ stores (i) all mismatching symbols from the current
read where at the corresponding position of the referenced read
there is symbol N, and (ii) all trailing symbols from the current read
beyond the match (i.e. C and C in the example above).

‘LettersX’, for X E {A, C, G, T}, stores all mismatching symbols
from the current read where at the corresponding position of the ref—
erenced read there is symbol X (in our running example, the mis—
matching C would be encoded in the stream ‘lettersG’. Note that the
alphabet size for any ‘LettersX’ stream is 4, that is,
{A, C, G, T, N}\{X}.

Prev
(Used only if ‘Flag’ is fex, fmis or fmh.) Stores the location (id) of the
referenced read from the buffer.

Shift

(Used only if ‘Flag’ is fax, fmis or fmh) Stores the offsets of the current
reads against their referenced read. The offset may be negative. For
our running example, the offset is + 2.

Matches

(Used only if ‘Flag’ is fath) Stores information on mismatch posi—
tions. For our running example, the matching area has 13 symbols,
but 4 of them belong to the signature and can thus be omitted (as
the signature’s position in the current read is known from the corre—
sponding value in the stream ‘Shift’). A form of RLE (run—length
encoding) is used here. Namely, each run of matching positions (of
length at least 1) is encoded with its length on 1 byte, and if the byte
value is less than 255 and there are symbols left yet, we know that
there must be a mismatch at the next position, so it is skipped over.
‘Unpredicted’ mismatches are encoded with 0. For our example, we
obtain the sequence: 1, 7 (match of length 1; omitted mismatch;
match of length 11, which is 7 plus 4 for the covered signature’s
area).

HReads

(Used only if ‘Flag’ is fdiss.) Here the ‘hard’ reads (not similar enough
to any read from the buffer) are dispatched. They are stored almost
verbatim: the only change in the representation is to replace the sig—
nature with an extra symbol (.). This helps a little for the compres—
sion ratio.

Rev
Contains binary flags telling if each read is processed directly or first
reverse—complemented.

Some of the streams are compressed with a strong general—pur—
pose compressor, PPMd (http://compression.ru/ds/ppmdj1.rar),
using switches —04 —m16m (order—4 context model with memory use
up to 16MB), others with our range coder (RC), also of order—4.
Namely, the streams ‘Flags’ and ‘Rev’ are compressed with order—4
RC, the stream ‘LettersX’ with order—4 RC, where the context is
formed of the four previous symbols, and all the other streams are
compressed with PPMd.

The description presented above is somewhat simplified. We
took some effort to achieve high processing performance. In particu—
lar, the input data (read from FASTQ files, possibly gzipped) are
processed in 256MB blocks (block size configurable as an input
parameter) and added to a queue. Several worker threads find signa—
tures in them, perform the necessary processing and add to an out—
put queue, whose data are subsequently written to the temporary
file. Also further bin processing is parallelized, to maximize the
performance.

3 Results

We tested our algorithm versus several competitors on real and
simulated datasets, detailed in Table 1. The test machine was a
server equipped with four 8—core AMD Opteron 6136 2.4GHz
CPUs, 128 GB of RAM and a RAID—5 disk matrix containing 6
HDDs. We use decimal multiples for the units, that is, ‘M’ (mega) is
106, ‘G’ (giga) is 109, etc., for the file sizes and memory uses
reported in this section.

3.1 Real datasets
The experimental results with real read data are presented in the
upper parts of Tables 2 and 3. Apart from the proposed compressor

ﬁm'srcumol‘pquo'sopauuopttotq/ﬁdnq

1392

S.Grabowski et al.

 

ORCOM, we tested DSRC 2 (Roguski and Deorowicz, 2014), Quip
(Jones et al., 2012), FQZComp (Bonfield and Mahoney, 2013),
Scalce (Hach et al., 2012), SRcomp (Selva and Chen, 2013) and
BWT—SAP (Cox et al., 2012). All these competitors, with the excep—
tion of BWT—SAP and SRcomp, are FASTQ compressors, and all of
them present compression results of the separate streams. In Table 2
we also present the result of ReCoil (Yanovsky, 2011) on one data—
set, copied from Cox et al. (2012). ReCoil is too slow to be run on
all our data in reasonable time.

As we can see in Table 2, ORCOM wins on all datasets, in an
extreme case (Musa balbisiana) with almost twice better compres—
sion ratio than the second best compressor, BWT—SAP. Figure 1 con—
firms the intuition that the performance of our algorithm improves
with growing coverage. Table 3 presents the compression times and
RAM consumptions. Our compressor is also usually the fastest,
with rather moderate memory usage (up to 14 GB). We point out
that the first phase, distributing the data into bins, is not very costly,
for example, it takes less than 25% of the total time for the largest
dataset, Homo sapiens 2. In the memory use the most frugal is
BWT—SAP (which is another disk—based software), spending only
3 MB for each dataset. One should remember that compression
times are related to the number of used threads: Quip, FQZComp,
SRcomp and BWT—SAP are sequential (one thread), whereas DSRC
2, Scalce and ORCOM are parallel and use eight threads here.
Moreover, ORCOM, BWT—SAP and SRcomp compress the DNA
stream only, whereas the remaining compressors have full FASTQ
files on the input (with fake remaining streams in case of simulated
reads presented in Section 3.2), what hampers their performance in
compression speed and memory use (the compression ratios are
however given for the DNA stream only). For these reasons, the
results from Table 3 should not be taken too seriously; they are
given mostly to point out promising performance of our software.

ORCOM’s compression performance depends on two parame—
ters: the signature length and the skip zone length. How varying
these parameters affects the compression ratio is shown in Figures 2

Table 1. Characteristics of the datasets used in the experiments

and 3, respectively, on the example of two datasets. It seems that
choosing the signature length from {6, 7, 8} is almost irrelevant for
the compression ratio, but with longer signature the ratio starts to
deteriorate. The impact of the skip zone length depends somewhat
on the chosen signature length, yet from our results we can say that
any zone length between 8 and 16 is almost equally good.

We also show how replacing the straight minimizers with signa—
tures affects the compression ratio and memory consumption
(Table 4). The compression gain is slight, up to 2.3% on real data
(H.sapiens 2) and up to 6.9% on simulated data (H.sapiens 3).
Fortunately, the improvement is greater in memory reduction, some—
times exceeding factor 2. As we can see, in two cases using signa—
tures required more memory than with minimizers, but this is for
two relatively small datasets. Using signatures generally leads to
more even data distribution across bins. The bin size distribution is
still far from perfect though, as shown in Figure 4.

Finally, in Table 5 the sizes of individual streams after compres-
sion, for three datasets, are given. As one can see, the stream of hard
reads is hardest to compress, which we think justifies its name.

3.2 Simulated datasets

In the experiments for simulated datasets the reads were obtained by
randomly sampling H.sapiens reference genome (HG 37.3). The
number of non—N—symbols in the reference is approximately
2859 M. The H.sapiens 3 dataset contains 1.25 G reads of length
100 bp reads (to have the genome coverage like for H.sapiens 2).
Half of them were obtained directly from the reference and another
half were reverse complemented.

The reads in H.sapiens 4 dataset were obtained from H.sapiens 3
dataset by modifying each base with a probability 1 % (the probability
is independent from the base position). It is important to stress that
such a simulation of errors is far from what happens in real experi—
ments (e.g. in most sequencers the quality of bases depends on the
base position). Nevertheless, this simple error model allows us to com—
pute the theoretically possible compression ratio and compare the

 

 

Dataset (Organism) Genome length Number of Gbases Number of Mreads Average read length Accession number
G.gallus 1040 34.7 347 100 SRX043656
M.balbisiana 472 56.9 569 101 SRX339427
H.5apiens 1 3093 6.9 192 36 SRX001540
H.5apiens 2 3093 135.3 1340 101 ERA015743
H.5apiens 2-trim 3093 134.0 1340 100 ERA015743
H.5apiens 3 3093 125.0 1250 100 —
H.5apiens 4 3093 125.0 1250 100 —

 

Notes: Approximate genome lengths are in Mbases according to http://www.ncbi.nlm.nih.gov/genome/. Homo sapiens 3 and 4 datasets are artiﬁcial reads pro—

duced by sampling reference human genome. In H.5apiens 3 the sampled reads are exact and in H.5a17iens 4 bases are modiﬁed with probability 1%.

Table 2. Compression ratios for various datasets

 

 

Dataset DSRC 2 Quip FQZComp Scalce SRcomp ReCoil BWT-SAP ORCOM
G.gallus 1.820 1.715 1.419 0.824 1.581 — 0.630 0.433
M.balbisiana 1.838 1.196 0.754 0.342 0.522 — 0.208 0.110
H.5apiens 1 1.857 1.773 1.681 1.263 1.210 1.34 1.246 1.005
H.5apiens 2 1.821 1.665 1.460 1.117 NS — NS 0.327
H.5apiens 2-trim 1.839 1.682 1.474 0.781 Failed — 0.518 0.317
H.5apiens 3 1.832 1.710 1.487 0.720 Failed — 0.410 0.174
H.5apiens 4 1.902 1.754 1.568 1.022 Failed — 0.810 0.562

 

Notes: Compressed ratios, in bits per base. The results of our approach are presented in the rightmost column. The best results are in bold. ‘NS’ means that the

compressor was not examined as it does not support variable-length reads in a dataset. ReCoil was not examined in our experiments due to very long running

times [the only result comes from Cox et al. (2012) paper].

[310'sp2umofp105xo'sopeuHOJIItotq/ﬁdnq

 

 

 

 

 

 

|
+ G. gal/us
+ H. sap/ens 2-trim

 

 

 

 

 

 

  

 

 

 

 

 

 

Table 2

 

 

 

 

 

 

 

 

 

 

 

+ G. gal/us, [7 : (w —A— H. sap/ens 2,12 : (1
+ G. gal/us, [7 : 8 + H. sap/ens 2,17 : 8

+ G. gal/us, [7 : 10 + H. sap/ens 2,17 : 10

 

/3.10'spzumo[p10}xo"soiwuuoquioiqﬂ:duq

 

 

m

 

 

 

 

 

 

 

IIIL

rII|.rI_uI

_
_
_
n
_
_r

 

 

 

 

m

.

.
_::___ ___:____

=_:__ _

 

_=:___

 

5:: _

 

::

:39\Ewowsmoaﬁmowoxmoagoﬁsambaﬁ

Equation (1)

Disk—based compression of data from genome sequencing

1395

 

Acknowledgement

The authors thank the reviewers for helpful comments.

Funding

The Polish National Science Centre under the project DEC-2012/05/B/ST6/
03148 and also the European Union from the European Social Fund within
the INTERKADRA project UDAPOKL-04.01.01-00-014/10-00 (partially).
The infrastructure supported by POIG.02.03.01-24-099/13 grant: ‘GeCONiI-
Upper Silesian Center for Computational Science and Engineering’.

Conﬂict of Interest: Home declared.

References

Bonﬁeld,].K. and Mahoney,M.V. (2013) Compression of FASTQ and SAM
format sequencing data. PLoS One, 8, e59190.

Campagrle,F. et al. (2013) Compression of structured high-throughput
sequencing data. PLoS One, 8, e79871.

Canovas,R. and Moffat,A. (2013) Practical compression for multi-alignment
genomic ﬁles. In: B. Thomas (ed.) Proceeding ACSC’13 Proceedings of the
Thirty—Sixth Australasian Computer Science Conference. Darlinghurst,
Australia, Australian Computer Society, Inc., pp. 51—60.

Canovas,R. et al. (2014) Lossy compression of quality scores in genomic data.
Bioinformatics, 30, 2130—2136.

Chikhi,R. et al. (2014) On the representation of de Brui/n graphs. arXiv pre-
print arXiv:1401.5383.

Cox,A.]. et al. (2012) Large-scale compression of genomic sequence data-
bases with the Burrows—Wheeler transform. Bioinformatics, 28,
1415—1419.

Deorowicz,S. and Grabowski,S. (2011) Compression of DNA sequence reads
in FASTQ format. Bioinformatics, 27, 860—8 62.

Deorowicz,S. and Grabowski,S. (2013) Data compression for sequencing
data. Algorithms Mol. Biol., 8, 25.

Deorowicz,S. et al. (2014) KMC 2: Fast and resource-frugal la-mer counting.
arXiV preprint arXiv:1407.1507.

Fritz,M.-Y. et al. (2011) Efﬁcient storage of high throughput DNA sequencing
data using reference—based compression. Genome Res., 21, 734—740.

Hach,F. et al. (2012) SCALCE: boosting sequence compression algorithms
using locally consistent encoding. Bioinformatics, 28, 3051—3057.

Hach,F. et al. (2014) Deez: reference-based compression by local assembly.
Nat. Methods, 11, 1082—1084.

Illumina (2012) Reducing whole-genome data storage footprint. Technical re-
port, Illumina.

Janin,L. et al. (2014) Adaptive reference-free compression of sequence quality
scores. Bioinformatics, 30, 24—30.

Jones,D. et al. (2012) Compression of next-generation sequencing reads aided
by highly efﬁcient de novo assembly. Nucleic Acids Res., 40, e171.

Kahn,S.D. (2011) On the future of genomic data. Science(Washington), 331,
728—729.

Kozanitis,C. et al. (2011) Compressing genomic sequence fragments using
SlimGene.]. Comput. Biol., 18, 401—413.

Li,Y. et al. (2013) Memory efﬁcient minimum substring partitioning. In:
Proceedings of the 39th International Conference on Very Large Data
Bases. VLDB Endowment. pp. 169—180.

Movahedi,N.S. et al. (2012) De novo co-assembly of bacterial genomes from
multiple single cells. In: BIBM. IEEE Computer Society. pp. 1—5.

Roberts,M. et al. (2004) Reducing storage requirements for biological se-
quence comparison. Bioinformatics, 20, 3363—3369.

Roguski,l:. and Deorowicz,S. (2014) DSRC 2—industry-oriented compression
of FASTQ ﬁles. Bioinformatics, 30, 2213—2215.

Salomon,D. and Motta,G. (2010) Handbook of Data Compression. Springer,
London.

Selva,].]. and Chen,X. (2013) SRComp: Short read sequence compression
using burstsort and elias omega coding. PLoS One, 8, article no. e81414.

Shkarin,D. (2002) PPM: one step to practicality. In: Data Compression
Conference (DCC). Snowbird, UT, IEEE Computer Society Press. pp.
202—21 1.

Wan,R. et al. (2012) Transformations for the compression of FASTQ quality
scores of next-generation sequencing data. Bioinformatics, 28, 628—635.

Wood,D.E. and Salzberg,S.L. (2014) Kraken: ultrafast metagenomic sequence
classiﬁcation using exact alignments. Genome Biol., 15, R46.

Yanovsky,V. (2011) Recoil—an algorithm for compression of extremely large
datasets of DNA data. Algorithms Mol. Biol., 6, 23.

Yu,Y. et al. (2014) Traversing the la-mer landscape of NGS read datasets for
quality score sparsiﬁcation. In: R. Sharan (ed.) Research in Computational
Molecular Biology, Vol. 8394 Lecture Notes in Computer Science. Springer
International Publishing, Pittsburgh, PA, USA, pp. 385—399.

Zhang,Y. et al. (2015) FQZip: lossless reference-based compression of next
generation sequencing data in FASTQ format. In: 18th Asia Paciﬁc
Symposium on Intelligent and Evolutionary Systems, Vol. 2. Springer,
Singapore, pp. 127—135.

ﬁm'srcumol‘piqxo'sopauuowiotq/ﬁdnq

