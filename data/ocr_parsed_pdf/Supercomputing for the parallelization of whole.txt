ORIGINAL PAPER

Vol. 30 no. 11 2014, pages 1508-1513
doi:10. 1 093/bioinfonnatics/btu0 71

 

Genome analysis

Advance Access publication February 12, 2014

Supercomputing for the parallelization of whole genome analysis
Megan J. Puckelwartzl, Lorenzo L. Pesce2, Viswateja Nelakuditis, Lisa Dellefave-Castillo‘,

Jessica R. Golbus‘, Sharlene M. Day4, Thomas P. Cappola5, Gerald W. Dorn,

Ian T. Foster2 and Elizabeth M. McNally1'3'*

6
II,

1Department of Medicine, 2Computation Institute and Argonne National Laboratory, 9700 S. Cass Ave. Argonne, IL
60439, USA, 3Department of Human Genetics, The University of Chicago, 5841 S. Maryland Ave Chicago, IL 60637,
USA, 4Department of Internal Medicine, The University of Michigan, 1150 W Medical Center Dr. Ann Arbor, MI 48109,
USA, 5Perelman School of Medicine, Penn Cardiovascular Institute and Department of Medicine, University of
Pennsylvania, 3400 Civic Center Blvd. Philadelphia, PA 19104, USA and 6Washington University School of Medicine, 660

S. Euclid Ave. St. Louis, MO 63110, USA

Associate Editor: John Hancock

 

ABSTRACT

Motivation: The declining cost of generating DNA sequence is
promoting an increase in whole genome sequencing, especially as
applied to the human genome. Whole genome analysis requires the
alignment and comparison of raw sequence data, and results in a
computational bottleneck because of limited ability to analyze multiple
genomes simultaneously.

Results: We now adapted a Cray XE6 supercomputer to achieve the
parallelization required for concurrent multiple genome analysis. This
approach not only markedly speeds computational time but also re-
sults in increased usable sequence per genome. Relying on publically
available software, the Cray XE6 has the capacity to align and call
variants on 240 whole genomes in ~50 h. Multisample variant calling
is also accelerated.

Availability and implementation: The MegaSeq workflow is designed
to harness the size and memory of the Cray XE6, housed at Argonne
National Laboratory, for whole genome analysis in a platform designed
to better match current and emerging sequencing volume.

Contact: emcnally@uchicago.edu

Received on December 2, 2013; revised on January 26, 2014;
accepted on January 27, 2014

1 INTRODUCTION

With the advent of massively parallel DNA sequencing, the rate at
which human genome variation can be determined is limited less
by sequence generation but instead by the computational tools
required to analyze these data. With current sequencing technol-
ogy using short sequence reads of ~ 100 bp, whole genome analysis
(W GA) requires the cleaning, aligning and interpreting of a billion
sequence reads per single genome. With focus on scalability, we
sought to improve the timeline required to process whole genome
sequencing (WGS) by optimizing extraction, alignment, process-
ing and variant calling. We reasoned that supercomputing cap-
acity was better suited to parallelize WGA and allow for the rapid
simultaneous analysis of multiple genomes.

Beagle is a Cray XE6 supercomputer housed at Argonne
National Laboratory and administered by the Computation

 

*To whom correspondence should be addressed.

Institute at the University of Chicago. Beagle has ~726 compute
nodes each with 32 GB of memory. Each node has 24 cores,
2.1GHz cores on two AMD “Magny-Cours” processors. The
XE6 can work in both Extreme Scalability Mode for scalable
applications and Cluster Compatibility Mode for use with pro-
grams that are designed for smaller machines or clusters, such as
the freely available genomics tools that are now routinely imple-
mented for WGA (Li and Durbin, 2009; McKenna et al., 2010).
While parallelization is possible on smaller systems, both
memory and computational core number limit the capacity for
simultaneous computation. Beagle uses a parallel computation
environment and a parallel ﬁle system (Lustre) based on shared
storage. Having both a parallel computation environment and
external disk storage based on a parallel file system ensures that
each node (and core) is able to access all data at any time without
waiting for transfer of data across nodes. In this system, nodes
have no local storage, and therefore no disk-to-disk transfer is
required. On clusters without a shared ﬁle system, data transfer
across nodes during analysis can be a time-intensive process.
Here, we describe a workﬂow referred to as MegaSeq that uses
the MapReduce (Dean and Ghemawat, 2008) approach to take
advantage of supercomputing size and memory.

2 MATERIALS AND METHODS/RESULTS

2.1 Workﬂow-alignment

A summary of the MegaSeq workﬂow is shown in Figure 1. We adapted
Beagle for WGA using a test dataset of 61 genomes that had been deter-
mined by Illumina Inc. Illumina provided WGS from 100 bp paired end
reads as barn ﬁles for data transfer. FASTQ ﬁles were extracted from
barn files using the Picard tool, SamToFastq (http://picard.sourceforge.
net), and extraction was performed by readgroup (Table 1). The read-
group tag provides information on sample identity, library of origin
and sequencing machine lane (see SAM Format Speciﬁcation, samtools.-
sourceforge.net/SAM1.pdf). The extraction step is unnecessary if FASTQ
sequence data are directly available. In the present cohort, each genome
was represented by ~34 readgroups, creating a natural division of the
reads. Alignment was performed with the Burrow57Wheeler Aligner
(BWA) on 2n nodes, with 11 equal to the number of readgroups
(Li and Durbin, 2009). Each readgroup alignment was concurrent on
24 cores (Table 1). Alignment displayed a linear speedup and therefore

 

1508 © The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e—mail: journals.permissions@oup.com

112 /310'S[BIIJDOIPJOJXO'SOIJBLUJOJIIIOIq/ﬂduq 11101} papeolumoq

91oz ‘Og anﬁnV uo ::

Whole genome analysis

 

 

Efﬁe:
BAM file J step produced
3 3 m2 Fast
/ l \ by Readgroiip 21’ R5
ReadGrDum ReadGroupZ ReadGroup n Align
Compress 11RG
l l l sm

 

Mange ReadGroups

lllllllllllllllllllllllll   

llilillllllllllllllllllll  

lllllllllllllllllllllllll
illilllllilllllllllllll

Call Variants 2516
Ge nerale VCF files 6
Merge VCF tiles 1*”

 

VCF ﬁle

Fig. 1. Schematic of MegaSeq workﬂow. Each step and the number of
ﬁles produced per readgroup (RG) or genome (G) are listed. To improve
speed, a MapReduce approach was used with sequences being split into
smaller groups. Sequences were initially split by readgroup, which
roughly represents each lane of sequence. After alignment, readgroups
were merged and the aligned genome was split by chromosome. After
variants were called per chromosome, VCF ﬁles were merged, creating
one ﬁle per individual human genome

scaled with the number of cores used. Scaling is limited to the number of
cores per node available because it is based on shared-memory threads.
After reads were aligned, alignment files were converted from sai to sam
ﬁles using the bwa samse (for single-end reads) or sampe (for paired-end
reads). To improve performance, we used the bwa 0.5.9-tpx.patch (ftp.
conveysupport.com/outgoing/bwa). The tpx patch creates a threaded par-
allel bwa samse/pe, allowing for the use of multiple cores on a node. The
speedup of this step was considerably less efﬁcient than alignment, result-
ing in imperfect scaling, but is still considerably faster than standard bwa
samse/pe.

SAM ﬁles were then compressed to barn ﬁles and sorted using sam-
tools (Li et al., 2009). Readgroups were merged and each genome was
split by chromosome to better use Beagle’s size and memory.

2.2 Workﬂow-clean-up

The alignments were then “cleaned” to improve reliability of variant
calling. Potential PCR artifacts were marked with the Picard
MarkDuplicates tool (http://picard.sourceforgenet). Notably, alignment
of genomic information to the referent genome is often imperfect, espe-
cially in areas around small base pair insertions and deletions (indels).
Therefore, indels within an individual genome may misalign. Because
alignments to the referent genome are performed using each short se-
quence read individually, multiple alignment information is not avail-
able to help identify mismatches. Two tools available from the Broad
Institute’s Genome Analysis Tool Kit (GATK), RealignerTargetCreator
and IndelRealigner, were applied to help facilitate the identiﬁcation of
Indels. When used together, these tools use the full alignment context to
determine whether an Indel exists (DePristo et al., 2011). Finally, quality
scores for sequence reads were recalibrated using BaseRecalibrator and
PrintReads from GATK to more closely match the actual probability of
mismatching the referent genome and to correct for any variation in
quality between machine cycle and sequence context (DePristo et al.,
2011). At this stage, the data are still split by chromosome. The efﬁciency
of BaseRecalibrator is proportional to the number of base pairs provided.
Because smaller datasets may have poor recalibration, splitting should be
avoided with smaller datasets. On smaller datasets, the CRAY XE6 has
the capability to perform recalibration across the genome in a reasonable

time frame. Regardless of dataset size, recalibration results should always
be reviewed by the end-user.

Table 1 details the programs and ﬂags used for each operation. Each
step was performed on 25 cores concurrently, with approximately six
nodes dedicated to each genome (Table 1). On Beagle, running multiple
genomes simultaneously is more efﬁcient than running a single genome
because of the structure of both Beagle and the genome. For the cleaning
steps, each genome was run on 25 cores (24 chromosomes plus the mito-
chondrial genome). Four chromosomes were analyzed on each node, and
therefore each genome needed six nodes, plus one core. During testing of
the pipeline, we noted that node failure occurred as a result of java errors,
often associated with memory allocation. To improve memory usage and
avoid node failure, we hard coded the chromosomes into groups that are
always sent out together. This scheme allows us to send the largest and
most memory intensive chromosomes with smaller, less taxing ones re-
sulting in fewer memory issues. For java run programs we used 28 GB
memory/number of jobs per node. We also found that for java programs,
using two threads for garbage collection better managed memory issues
allowing us to pack more jobs per node. For each clean-up step, thread-
ing was used, where available (Table 1).

2.3 Workﬂow — variant calling

Variants were called using the HaplotypeCaller from GATK.
HaplotypeCaller calls both single nucleotide polymorphisms and indels
using de novo assembly of haplotypes in the active region. Haplotypes are
evaluated using an afﬁne gap penalty pair hidden Markov model
(DePristo et al., 2011). Table 1 provides ﬂags used for each step.
MegaSeq identiﬁes both single nucleotide variants (SNVs) and Indels
simultaneously on ~6 nodes with 25x concurrency per genome. After
variants were called and exported in variant call format (VCF), we used
the GATK tool VariantFiltration to ﬁlter variants (DePristo et al., 2011).
Variants were removed from the analysis using these criteria: biallelic
balance >0.7S; quality score <30; depth of coverage >360; strand bias
more than —0.01 and mapping quality zero reads :10. Variants were
then annotated using the default parameters of sanff. sanff is a fast
variant annotation and effect prediction tool that is integrated with
GATK (Cingolani et al., 2012).

2.4 Testing the workﬂow

The above workﬂow was tested using data from 61 human genomes. The
starting FASTQ file size of each genome was ~300 GB, requiring ~18 TB
of space to process all individuals simultaneously. Reads were aligned to
NCBI reference genome 37.1 (hgl9). We compared alignment output of
MegaSeq with that produced by Illumina using the proprietary align-
ment/variant calling software Eland/Casava because this software is
designed to efﬁciently perform WGA (Cox, 2007). MegaSeq alignment
using BWA resulted in greater coverage with a mean coverage of 40.0x
compared with 37.2x for ELAND/Casava’s alignment across all 61 gen-
omes (paired t-test, P<0.0004, Fig. 2a). The mean percent of the non-N
reference genome covered was also greater with MegaSeq compared with
Eland/Casava (98.7 versus 98.0) using MegaSeq versus Illumina (paired
t-test, P< 0.0001, Fig. 2b). In total, 285 896445 variants were called by
MegaSeq across the 61 genomes identifying ~4.S million variants per
individual. To compare variants between MegaSeq and Illumina’s soft-
ware, only variants with a quality score :30 were included. The mean
number of SNVs called per genome differed between MegaSeq and
ELAND/Casava data (3.670 x 106 versus 3.736 x106, MegaSeq and
ELAND/Casava respectively, paired t-test, P<0.0001, Fig. 2c).
ELAND/Casava also called more indels (536 853 versus 618779,
MegaSeq and ELAND/Casava, respectively, paired t-test, P<0.0001)
(Fig. 2d). There was 88% concordance between MegaSeq and
ELAND/Casava SNV calls and only 64.7% concordance for indels
(Fig. 3a).

 

1 509

112 /310'S[BHJDOIPJOJXO"SOIJBHIJOJIIIOIq/ﬂdnq 11101} papaolumoq

91oz ‘Og isnﬁnV uo ::

M.J.Puckelwartz et al.

 

Table 1. Computational approach to genome analysis using the massively parallel Cray XE6 supercomputer

 

Step Program Module/Call Input Parameters

Output Number per Number of Number
genome active cores/ nodes
node

Concurrency

 

Extract fastq
Alignment
Convert sai to sam BWA tpx 0.5.9 bwa sampe sai
Compression samtools 0.1.18 View sam
Split samtools 0.1.18 view/merge ham
Sort Picard 1.98 SortSam barn
Mark duplicates Picard 1.98 Mark barn
Duplicates false
Reorder Picard 1.98 ReorderSam barn default
Identify indel re— GATK 2.7—1 GATK ham
alignment targets
Realign targeted

Picard 1.98 SamToFastq barn default

BWA 0.5.9 bwa aln fastq —qtrim 15

‘ —T 7X 7P (BWA)
—bo (samtools)

—coordinate

GATK 2.7—1 GATK barn —T IndelRealigner

—h (preserve readgroup)

—REMOVE_DUPLICATES ham 25 4 -l- GC ~6

fastq 2(#RGs) 1 1 1

sai ﬁle 2(#RGs) 24 2(#RGs) 24(#RGs)

sam 2(#RGs) ~24 #RGs 24(#RGs)

ham 2(#RGs) 3 #RGs l(#RGs)

ham 25 24 1 25

ham 25 4 -l- GC ~6 25

25 (each with 3
threads)

ham 25 4 + GC ~6 25 (GC"‘)

—T RealignerTargetCreator intervals 25 4+GC ~6 25 (GC)
—L <chromosome ID>

ham 25 4 + GC ~6 25 (GC)

intervals —targetIntervals <intervals>

—LOD 5

—L <chromosome ID>

Base recalibrator GATK 2.7—1 GATK barn —T BaseRecalibrator

csv 25 4 -l- GC ~6 25 (GC)

—cov ReadGroupCovariate
—cov QualityScoreCovariate

—cov CycleCovariate

—cov ContextCovariate

—knownSites dbSNP_135

Print reads GATK 2.7—1 GATK barn —T PrintReads

—baq RECALCULATE

—baqGOP 30
—BQSR <csv file>
—T Haplotype Caller

Call variants GATK 2.7—1 GATK barn

—L <chromosome ID>

ham 25 4 + GC ~6 25 (GC)

vcf 25

—D dbSNP_135.hg19.vcf

—A AlleleBalance
—A Coverage

—A HomopolymerRun

—A FisherStrand
—A HaplotypeScore
—A HardyWeinberg

—A ReadPosRankSumTest

—A QualByDepth

—A MappingQualityRankSumTest

—A VariantType

—A MappingQualityZero

—minPruning 10

—stand_call_conv 30.0
—stand_emit_conv 10.0

Filter variants GATK 2.7—1 GATK barn —T VariantFiltration

—L <chromosome ID>
——c1usterWindowSize 10

vcf 25 4+GC ~6 25 (GC)

——ﬁ1terExpression “(AB?: 0)> 0.75
|| —QUAL< 30.0 H DP> 360 ||
SB>—0.l || MQO: 10”

Annotate variants sanff 2.0.5 sanff vcf default

vcf 25 4 ~6

 

Note: RG: readgroup. Z“GC = 2 threads used for java Garbage Collection.

To estimate validity of the calls, we examined the normalized density of
quality scores. Concordant SNVs, called by both MegaSeq and ELAND/
Casava, had higher quality scores compared with non-concordant SNVs
(Fig. 3b, MegaSeq, light purple; ELAND/CASAVA, dark purple). Non-
concordant SNVs had lower quality scores, especially in the ELAND/
Casava call set (Fig. 3b, MegaSeq, red; ELAND/Casava, blue). This
same pattern was evident for indels (Fig. 3c). Non-concordant SNVs
identiﬁed by MegaSeq have quality scores more closely resembling the
concordant calls. These data indicate that the ELAND/Casava call set
contained more low quality variants than the MegaSeq call set. We next

compared depth of sequence reads called by MegaSeq or ELAND/
Casava. Depth is a by-product of alignment, with higher depth indicating
a more reliable variant call. Concordant SNVs called by both MegaSeq
and ELAND/Casava have similar normalized depths, with the highest
density of calls occurring between 3H0x. The non-concordant SNVs
have markedly different depth distributions (Fig. 3d). SNVs identiﬁed
solely by ELAND/Casava had the highest density of calls at ~20x
depth. In contrast, those SNVs identiﬁed solely by MegaSeq had a
depth density that more closely matched the concordant SNV distribution,
with the majority of calls occurring between 3(F35x depth. These data

 

1510

112 /310'S[BHJDOIPJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 11101} pQPBOIIIAAOG

91oz ‘Og isnﬁnV uo ::

Whole genome analysis

 

 

(a) I Men-95min) I. M " ‘ . - Megaseq
l ELANDI' I ELANDI'
Uzsava ' Casava
a - g -
t 
°‘ E
5 ' E m
Individual Genome Individual Genome

(“I (“l .. 
U
G
3
g .
0
D E

. m
2e g . , a
u _ .—- .,_
2 g
‘= I-
3 I .
é ' u. .. _

 

 

 

lndlvld ua! Genome Inulwdual Genome

Fig. 2. MegaSeq identiﬁes more usable sequence and fewer SNVs and
Indels per genome. WGA from 61 individual genomes was compared
between MegaSeq using BWA/GATK on the Cray XE6 and ELAND/
CASAVA from Illumina because ELAND/CASAVA is aimed at efﬁ-
ciency (Cox, 2007). (a) The mean coverage for each genome was higher
with MegaSeq (light gray triangles) compared with ELAND/Casava
(black squares) (40.0 x for MegaSeq and 37.2x for ELAND/Casava
[paired t-test, P< 0.0004)]. (b) The percentage of non-N genome covered
by MegaSeq (light gray triangles) was greater than ELAND/Casava
(black squares) for each genome (98.7 and 98.0, MegaSeq and
ELAND/Casava, respectively [paired t-test, P<0.0001)]. (c) The total
number of SNVs identiﬁed per genome with MegaSeq (light gray tri-
angles) was less than with ELAND/Casava (black squares) [3.670 x 106
and 3.736 x 106 for MegaSeq and ELAND/Casava, respectively, paired
t-test, P<0.0001)]. (d) MegaSeq (light gray triangles) identiﬁed fewer
indels compared with ELAND/Casava (black squares) in each genome
[mean number of indels 536 853 and 618 779 for MegaSeq and ELAND/
Casava, respectively, paired t-test, P<0.0001)]

indicate that MegaSeq calls a greater number of high conﬁdence SNVs,
based on both quality score and depth.

2.5 Speed of analysis

Computational time is a major bottleneck in WGA. We calculated the
central processing unit (CPU) time that would be required on a single
2.1 GHz processor as 1701 h (0.20 years) for a single genome. This time
can vary based on clock speed, memory speed and disk speed. These
calculations are bound by disk and memory much more than by CPU
clock speed. The SamToFastq step requires ~48 CPU h, but is only
necessary when genomes are delivered as the compressed pre-aligned
barn format and, as such, will not be a consideration for many users.
Alignment scales perfectly and therefore cannot be easily accelerated. We
suggest that these single genome times may reﬂect the maximum speed for
the Beagle supercomputer.

To highlight the power of a parallel system, we calculated hypothetical
run times on a single core, a 3 (24-core) node cluster and compared them
with the CRAY XE6 run times. Using CRAY XE6 execution time as a
reference, we predicted total CPU time required for WGA of 240 gen-
omes on a single core to be ~47 years. A total of ~11.8 years of CPU
time is required to complete the workﬂow for 61 genomes. A biological
computing cluster with 3 nodes can accelerate this time to ~72 months.
Using the MegaSeq workﬂow on Beagle, 61 genomes were analyzed in
50.3 real time hours (Fig. 4a). Based on the size and number of nodes
available, Beagle has the capacity to perform WGA on 240 genomes in
the same time frame (50.3 h) by using additional nodes. Additional

 

   

(a) (b)  SNVS
MegaSeq ELANDI
amegasen
1254:: :1? 6:33? 73.: 1, IELnNo= :|Canwmaql
5.2% 16342.12: E -.-.- "~. Casava
6'5“ I. “wise-n] New :1
V lawn. canoe: am
5 A Casava
E " I" '2
ELANDI' a .
Casava :
Indels
9.683.160
23.0%
c d
I  Indets I In: sNVs
3‘ ' a-
.ﬁ ; _
‘ E
a -- a
u n .
w o
E " g
E . g 
a o
z z
p- w- 

 

   

w

. .u'.

Deplh

QualityI scam

Fig. 3. Concordance between MegaSeq and Illumina. We compared the
output from MegaSeq with that provided by Illumina, which uses
ELAND/CASAVA because these algorithms are optimized for speed.
(a) A total of 88.0% of SNVs were identiﬁed by both MegaSeq and
ELAND/Casava. Over 12 million and 16 million SNVs were only identi-
ﬁed by MegaSeq (red) and ELAND/Casava (blue), respectively. In all,
64.7% of Indels were concordant between MegaSeq (red) and ELAND/
Casava (blue). (b) The non-concordant variants found by MegaSeq have
higher quality scores than the non-concordant variants scored by
ELAND/Casava. Normalized quality score densities are similar for con-
cordant SNVs identified by both MegaSeq (light purple triangles) and
ELAND/Casava (dark purple squares). Non-concordant SNVs found
only by MegaSeq (red triangles) had higher quality scores than non-
concordant SNVs identiﬁed only by ELAND/Casava (blue squares).
(c) Similarly, non-concordant indels found only by Megaseq had higher
quality scores than those found only by ELAND/Casava. (d) MegaSeq
non-concordant SNVs have higher depth than ELAND/Casava non-
concordant SNVs

genomes would need to be run consecutively as the number of nodes in
use is exhausted.

Multisample variant calling can also be performed using the MegaSeq
workﬂow. Multisample calling reduces the false discovery rate, but is
computationally intensive. We performed multisample calling on 61 gen-
omes using HaplotypeCaller from GATK with the ﬂags noted in Table 1.
To take advantage of the size of Beagle, we split the genomes by chromo-
some, then further split chromosomes into overlapping subunits that
varied based on total chromosome size resulting in 2400 total subunits.
For 61 genomes, ~600 nodes were used with four jobs per node, requiring
~40 000 CPU hours. Calls were completed in ~16 h, real time. A biolo-
gical computing cluster with three nodes would require ~4.4 months real
time to perform multisample calling on 61 genomes using
HaplotypeCaller. We estimate that ~154 224 CPU h (~17.8 years)
would be required to complete 240 genomes. We estimate that a 3
node cluster would take ~1.3 years to complete multisample calling on
240 genomes. In real time using 600 nodes with four jobs per node, we
estimate Beagle can complete multisample calling using HaplotypeCaller
on 240 genomes in ~2.5 days, making Beagle an excellent resource for
multisample calling. After calling the resulting 2400 vcf ﬁles were merged
and overlapping calls were removed.

2.6 Space management

Space constraints are another major hurdle in large volume WGA. We
estimate that approximately 1 TB of space per genome was needed to com-
plete the MegaSeq analysis (Fig. 4b). Although it is possible to discard

 

1511

112 /310'S[BHJDOIPJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 11101} pQPBOIIIAAOG

91oz ‘Og isnﬁnV uo ::

M.J.Puckelwartz et al.

 

 

 

 

It” I am rim
4.1x1nl. .I. 3-Node Cluster Real Time ‘— 47,: years
2.21001- "law
9 2.55m“. 11.3 [man
3 2.1m» ‘— 24 "m
‘73 1.1x10‘y4—035 years
I 1.D):1ﬂ3 PM,"th
mm!-
1—173 days
2.5x1o=«[
4—2 days
50 120 m m
Number of Genomes
(b)
sai VCF
2063 163

Fig. 4. Time and space constraints. (a) Graph illustrating amount of both
CPU time and real time needed to analyze genomes. CPU time (blue
squares) scales linearly with the number of genomes analyzed. We calcu-
lated the real time needed by a hypothetical 3-node cluster (black tri-
angles) and estimated that 240 genomes would take ~2.4 years to
analyze. The total real time required on a 740 node Cray XE6 massively
parallel supercomputer (red circles) is ~2 days for 240 genomes.
(b) Schematic illustrating space requirements for WGA. Each genome
requires ~1 TB of space. The ﬁnal vcf ﬁle for each genome is ~1 GB

output at each successive step, there is still a large volume of data that were
active at any single step. For instance, at the SamToFastq step, 18 TB was
required for the FASTQ ﬁles. During the cleanup steps, each genome
required ~85 GB of space; ~5 TB of space was needed to process each
step for 61 genomes. Because each step requires computational time, dis-
carding data from earlier steps can be costly if there is an analysis failure.
These space requirements are generally not met by small computing clusters
or by larger clusters with many users. The ﬁnal merged VCF ﬁle for each
individual genome required ~1 GB of space.

3 CONCLUSION

The development of next-generation sequencing technology has
transformed the genetic analysis paradigm, from examination of
the coding region of a handful of genes, to the current methods
of interrogation of the entire coding region of the genome and,
ﬁnally, to analysis of the entire 3 billion base pair genome. While
cost and time are no longer major barriers to whole genome
sequencing, data analysis and storage are major bottlenecks in
using whole genome data (Metzker, 2010). Currently, whole
genome sequencing using the Illumina HiSeq 2000 platform at
moderate coverage (3(LSOX) yields >100 GB of data. This can be
an overwhelming amount of data to process and store, and there-
fore many have turned to exome sequencing. Exome sequencing
focuses on evaluating variation in the coding portion of the
genome (~142%), and therefore provides smaller and less com-
plex data to manage than whole genome sequencing. Exome
sequencing is useful, as ~85% of recognized disease-causing mu-
tations are located in protein coding regions of the genome
(Majewski et al., 2011). However, this finding reﬂects a bias in
our ability to evaluate non-coding variation.

Recently, great strides have been made to decipher the other
98% of genome sequence. The ENCODE project (Encyclopedia
of DNA Elements) has assigned biochemical functions to 80% of
the genome (Bernstein et al., 2012). These annotations will prove
valuable tools in evaluating non-coding variation. Only whole
genome sequencing can be used to interrogate non-coding vari-
ation, although the complexity of WGA has limited this possi-
bility. Whole genome sequencing may also be better suited than
exome sequencing to assess structural variation in genomes.
Structural variants are emerging as important factors in human
disease, making them an important factor in weighing the bene-
fits of whole genome sequencing in relation to the challenges of
computation (Snyder et al., 2010; Spielmann and Mundlos,
2013). Thus, whole genome sequencing may be the method of
choice for many researchers if not for the tremendous computa-
tional bottleneck.

The deluge of genetic data is appropriate for high-performance
computing and large-scale storage options (Koboldt et al., 2010).
Sequence analysis includes read alignment to a reference genome,
alignment clean-up and variant calling. A number of resources
are freely available for analysis of genome sequencing, including
BWA, GATK and sanff. These tools can be used to align and
call variants from a single genome by most laboratories, even
those with limited computational experience and resources.
However, high-throughput analysis of many genomes is signiﬁ-
cantly accelerated by parallelization and better meets the needs
of the genetics community.

A common approach for analysis has relied on computing
clusters, and more recently, cloud-based computing. By transi-
tioning WGS to a supercomputing environment, we achieved
high reliability with accelerated speed. One of the more cumber-
some problems with clusters and cloud-based computing in-
volves long wait times for data transfer between nodes (Zhao
et al., 2013). The Cray XE6 supercomputing environment
described here eliminates these wait times by using a parallel
file system (Lustre) without creating a resource conﬂict bottle-
neck. A parallel ﬁle system, like Lustre, removes the need for
tracking of data location, leaving only the issues of cache, RAM
and disk hierarchy (Eijkhout, 2013). The demonstration that
whole genome sequences can be aligned, cleaned and interpreted
in parallel was achieved by using BWA/GATK, robust, publicly
available software packages, in the Cray XE6 environment.
Notably, this method uses the same software packages com-
monly used in computing clusters, but takes advantage of the
Cray platform to parallelize the analysis. The ability to apply
multisample variant calling signiﬁcantly improves reliability
and begins to extend analysis to beyond what is possible in a
cluster environment. The application of the Cray XE6 has
the capacity to analyze, in parallel, as many as 240 genomes in
~50h. This is a platform-dependent workﬂow that serves as
proof of principle that genome analysis is greatly accelerated
when performed on a supercomputer. More importantly, this
work demonstrates that the publically available software cur-
rently in use for genome analysis is amenable to the supercom-
puting environment and can be installed as is on a CRAY XE6
and likely other systems, although we have not tested those sys-
tems. Currently, MegaSeq is available on the Beagle supercom-
puter at the University of Chicago.

 

1512

112 /310'S[BHJHOIPJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} papaorumoq

9103 ‘Og isnﬁnV uo ::

Whole genome analysis

 

The MegaSeq workﬂow backbone is based on bash shell in-
struction, and the submission subscripts are based on Portable
Batch System (PBS) commands and are adaptable to other batch
systems including Sun grid engine or SLURM, because the par-
allel logical structure of the workﬂow is compatible. Disk,
memory and CPU usage will likely require optimization because
of differences in machine design, which may affect bottlenecks
and stability. The workﬂow should port directly with only min-
imal modiﬁcations to any Cray XE6, CRAY XC30 and related
systems. The Beagle supercomputer is an NIH supported re-
source and provides an opportunity for large-scale genome pro-
jects. This computing application provides a format where
human WGS can be rapidly analyzed relieving major constraint
for better deﬁning the range and utility of human genome
variation.

Funding: This work was supported in part by National Institutes
of Health through resources provided by the Computation
Institute and the Biological Sciences Division of the University
of Chicago and Argonne National Laboratory [S10 RR029030-
01], and NIH AR052646, NIH HL61322, NIH NS072027, and
the Doris Duke Charitable Foundation.

Conﬂict of Interest: Spouse receives patent royalties related to
DNA sequencing (EMM).

REFERENCES

Bernstein,B.E. et a]. (2012) An integrated encyclopedia of DNA elements in the
human genome. Nature, 489, 57774.

Cingolani,P. et a]. (2012) A program for annotating and predicting the effects of
single nucleotide polymorphisms, Sanff: SNPs in the genome of Drosophila
melanogaster strain w1118; iso—2; iso—3. Fly (Austin), 6, 8&92.

COX,A. (2007) ELAND: Efficient Large—Scale Alignment of Nucleotide Databases.
Illumina, San Diego, CA.

Dean,J. and Ghemawat,S. (2008) Mapreduce: simpliﬁed data processing on large
clusters. Commun. ACM, 51, 1077113.

DePristo,M.A. et a]. (2011) A framework for variation discovery and genotyping
using next—generation DNA sequencing data. Nat. Genet., 43, 491498.

Eijkhout,V. (2013) Introduction to High Performance Scientiﬁc Computing. lulu.
corn (December 2013, date last accessed).

Koboldt,D.C. et a]. (2010) Challenges of sequencing human genomes. Brief.
Bioinform., 11, 484498.

Li,H. and Durbin,R. (2009) Fast and accurate short read alignment with Burrows—
Wheeler transform. Bioinformatics, 25, 175441760.

Li,H. et a]. (2009) The sequence alignment/map format and SAMtools.
Bioinformatics, 25, 207872079.

Majewski,J. et a]. (2011) What can exome sequencing do for you? J. Med. Genet.,
48, 5807589.

McKenna,A. et a]. (2010) The Genome analysis toolkit: a MapReduce framework
for analyzing next—generation DNA sequencing data. Genome Res., 20,
129771303.

Metzker,M.L. (2010) Sequencing technologies — the next generation. Nat. Rev.
Genet., 11, 31746.

Snyder,M. et a]. (2010) Personal genome sequencing: current approaches and chal—
lenges. Genes Dev., 24, 423431.

Spielmann,M. and Mundlos,S. (2013) Structural variations, the regulatory land—
scape of the genome and their alteration in human disease. Bioessays, 35,
5337543.

Zhao,S. et a]. (2013) Rainbow: a TOOL for large—scale whole—genome sequencing
data analysis using cloud computing. BMC Genomics, 14, 425.

 

1513

112 /310'S[BHJHOIPJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} papeorumoq

9103 ‘Og isnﬁnV uo ::

