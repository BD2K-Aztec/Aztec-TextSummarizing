Bioinformatics, 31 (18), 2015, 3008—301 5

doi: 10.1093/bioinformatics/btv315

Advance Access Publication Date: 20 May 2015
Original Paper

 

 

Systems biology

Trans-species learning of cellular signaling
systems with bimodal deep belief networks
Lujia Chen, Chunhui Cai, Vicky Chen and Xinghua Lu*

Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA 15237, USA

*To whom correspondence should be addressed.
Associate Editor: Alfonso Valencia

Received on September 14, 2014; revised on April 21, 2015; accepted on May 17, 2015

Abstract

Motivation: Model organisms play critical roles in biomedical research of human diseases and
drug development. An imperative task is to translate information/knowledge acquired from model
organisms to humans. In this study, we address a trans—species learning problem: predicting
human cell responses to diverse stimuli, based on the responses of rat cells treated with the same
stimuli.

Results: We hypothesized that rat and human cells share a common signal—encoding mechanism
but employ different proteins to transmit signals, and we developed a bimodal deep belief network
and a semi—restricted bimodal deep belief network to represent the common encoding mechanism
and perform trans—species learning. These 'deep learning’ models include hierarchically organized
latent variables capable of capturing the statistical structures in the observed proteomic data in a
distributed fashion. The results show that the models significantly outperform two current state—
of—the—art classification algorithms. Our study demonstrated the potential of using deep hierarch—
ical models to simulate cellular signaling systems.

Availability and implementation: The software is available at the following URL: http://pubreview.
dbmi.pitt.edu/TransSpeciesDeepLearningl. The data are available through SBV IMPROVER web—
site, https://www.sbvimprover.com/challenge—2/overview, upon publication of the report by the
organizers.

Contact: xinghua@pitt.edu

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 lntroductlon responses, when compared with corresponding human cells. Thus,

Due to ethical issues, modal organisms such as rat and mouse have
been widely used as disease models in studying disease mechanisms
and drug actions (Brown, 2011; McGonigle and Ruggeri, 2014). For
example, mouse models have been used to study the disease mechan—
isms and treatment of type—2 diabetes (Omar et (11., 2013). Since sig—
nificant differences exist between species in terms of genome,
cellular systems and physiology, the success of using model organ—
isms in biomedical research is hinged on the capability to translate/
transfer the knowledge learned from model organisms to humans.
For example, when using a rat disease model to screen drugs and in—
vestigate the action of drugs, rat cells inevitably exhibit different
molecular phenotypes, such as proteomic or transcriptomic

in order to investigate how the drugs act in human cells, it is critical
to translate the molecular phenotypes observed in rat cells into cor—
responding human responses.

Recent species—translation challenges organized by the Systems
Biology Verification combined with Industrial Methodology for
Process Verification in Research (SBV IMPROVER, 2013) provided
an opportunity for the research community to assess the methods
for trans—species learning in systems biology settings (thissorrakrai
et (11., 2015). One challenge task was to predict human cells’ prote—
omic responses to distinct stimuli based on the observed proteomic
response to the same stimuli in rat cells. More specifically, during
the training phase, participants were provided with data that

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3008

9103 ‘Og JSanV 110 seleﬁuv soc] ‘BtHJOJtIBQ 30 AJtsreAtuf] 112 /310'S[BIIJDO[pJOJXO"SOTJBHIJOJulolq/ﬂdnq 11101} popcolumoq

Trans—species learning of cellular signaling systems

3009

 

measured the phosphorylation states of a common set of signaling
proteins in primary cultured bronchial cells collected from rats and
humans treated with distinct stimuli (Poussin, 2014). In the testing
phase, the proteomic data of rat cells treated with unknown stimuli
were provided, and the task is to predict the proteomic responses of
human cells treated with the same stimuli (Fig. 1).

To address the trans—species learning task, a simplistic approach
is to train regression/classification models that use the phosphoryl—
ation data from rat cells as input features and treat the phosphoryl—
ation status of an individual protein from human cells (treated with
the same stimulus) as a target class. In this way, predicting the prote—
omic profile of human cells can be addressed as a series of
independent classification tasks or within a multi—label classification
framework (Jin et (11., 2008; Tsoumakas and Katakis, 2007).
However, most contemporary multi—label classification methods
treat the target classes as independent or are incapable of learning
the covariance structure of classes, which apparently does not reﬂect
biological reality. In cellular signaling systems, signaling proteins
often form pathways in which the phosphorylation of one protein
will affect the phosphorylation state of others in a signaling cascade,
and cross—talk between pathways can also lead to coordinated phos—
phorylation of proteins in distinct pathways (Alberts et (11., 2008).
Another shortcoming of formulating trans—species learning as a con—
ventional classification problem is that contemporary classifiers,
such as the support vector machine (Bishop, 2006) or regularized re—
gression/classification (Friedman et (11., 2010), concentrate on deriv—
ing mathematical representations that separate the cases, whereas
the real goal of trans—species learning is to capture the common sig—
naling mechanisms employed by cells from both model organisms
and humans in response to a common stimuli. Indeed, the corner—
stone hypothesis underpinning trans—species learning is that there is
a common encoding mechanism shared by cells from different
species, but distinct signaling molecules are employed by different
species to transmit the signals responding to the same environmental
stimuli. Therefore, it is important to explore models that are com—
patible to the above hypothesis.

 

 

RAT HUMAN

 

 

 

 

 

 

 

 

 

 

6%

 

 

 

 

 

training test training test

Fig. 1. Trans-species learning task specification. The objective of the SBV
challenge was to predict the phosphorylation states of a set of proteins in
human cells treated with different stimuli, based on the observed phosphoryl-
ation states of the same set of proteins in rat cells treated with the same
stimuli. The blocks labled as "training" are matrices representing the
observed phosphorylation states of proteins under different treatment condi-
tions in human and rat cells. In test phase, the phosphorylation states of the
proteins in rat cells treated with a set of unknown stimuli are provided, and
the task is to predict the phosphorylation states of the human cells treated
with the same stimuli

Recent advances in deep hierarchical models, commonly referred
to as ‘deep learning’ models (Bengio et (11., 2012; Hinton et (11.,
2006; Hinton and Salakhutdinov, 2006), provide an intriguing
opportunity to model the common encoding mechanism of cellular
signaling systems. These models represent the signals embedded in
observed data using multiple layers of hierarchically organized hid—
den variables, which can be used to simulate a cellular signaling sys—
tem because the latter is also organized as a hierarchical network
such that signaling proteins at different levels compositionally
encode signals with different degrees of complexity. For example,
activation of the epidermal growth factor receptor (EGFR) will lead
to a broad change of cellular functions including the activation of
multiple signaling molecules such as Ras and MAP kinases (Alberts
et (11., 2008), which in turn will activate different transcription fac—
tors, e.g. Erie—1 and c—Jun/c—Fos complex, with each responsible for
the transcription of a subset of genes responding to EGFR treatment.
The signals encoded by signaling molecules become increasingly
more specific, and they share compositional relationships.
Therefore, deep hierarchical models, e.g. the deep belief network
(DBN) (Hinton et (11., 2006), are particularly suitable for modeling
cellular signaling systems.

In this paper, we present novel deep hierarchical models based
on the DBN model to represent a common encoding system that en—
codes the cellular response to different stimuli, which was developed
after the competition in order to overcome the shortcomings of the
conventional classification approaches we employed during compe—
tition. We applied the model to the data provided by the SBV
IMPROVER challenge and systematically investigated the perform—
ance. Our results indicate that, by learning better representations of
cellular signaling systems, deep hierarchical models perform signifi—
cantly better on the task of trans—species learning. More import—
antly, this study leads to a new direction of using deep networks to
model large ‘omics’ data to gain in depth knowledge of cellular sig—
naling systems under physiological and pathological conditions,
such as cancer.

2 Methods

In this study, we investigated using the DBN model (Hinton et (11.,
2006) to represent the common encoding system of the signal trans—
duction systems of human and rat bronchial cells. A DBN contains
one visible layer and multiple hidden layers (Fig. 2A). An efficient
training algorithm was introduced by (Hinton et (11., 2006; Hinton
and Salakhutdinov, 2006), which treats a DBN as a series of re—
stricted Boltzmann machines (RBM; Fig. 2B) stacked on top of each
other. For example, the visible layer 11 and the first hidden layer,
17(1), can be treated as a REM, and the first and second hidden layers,
lo (1) and lo (2), form another RBM with lo (1) as the ‘visible’ layer. The
inference of the hidden node states and learning of model param—
eters are first performed by learning the RBM stacks bottom—up,
which is followed by a global optimization of generative parameters
using the back—propagation algorithm. In certain cases, edges be—
tween visible variables can be added in a RBM to capture the rela—
tionship of the visible variables, which leads to a semi—restricted
RBM (Fig. 2C). In the following sub—sections, we will first introduce
the models and their inference algorithms.

2.1 Restricted Boltzmann Machines (RBMs)

A RBM is an undirected probabilistic graphical model consisting of
a layer of stochastic visible binary variables (represented as nodes in
the graph) v E {0, 1}D and a layer of stochastic hidden binary

91oz ‘Og JSanV uo soleﬁuv soc] ‘BtHJOJtIBQ JO AJtsroAtuf] 112 ﬁle'smumofqutxo"sotJBurJOJutotq/ﬁduq 11101} popcolumoq

3010

L.Chen et al.

 

variables h E {0, 1}F. A RBM is a bipartite graph in which each vis—
ible node is connected to every hidden node (Fig. 2B) and vice versa.
The statistical structure embedded in the visible variables can be
captured by the hidden variables. The RBM model defines the joint
distribution of hidden and visible variables using a Boltzmann distri—
bution as follows:

Pr(v,h; 0) 2 ﬂaw—E“, h; 0)) (1)

The energy function E of the state {v, h} of the RBM is defined as
follows:

E(v, h; 0) : —aTv — bTh — VTWh

D F D F
I —Za,-v,- — Zbl'l’ll' — ZZUihiu/i,‘
i:1 [:1

1'21 721

(2)

where v,- is the binary state of visible variable i; la,- is the binary state
of hidden variable i; 0 : {a,b,W} are the model parameters. a,- rep—
resents the bias for visible variable i and b,- represents the bias for
hidden variable j. W), represents the weight between visible variable i
and hidden variable i.

The ‘partition function’, Z, is derived by summing over all pos—
sible states of visible and hidden variables:

 : quhexp(_E(vv 11;  

The marginal distribution of visible variables is

Pr(v; 0) 2 thv, h; 0) 2 ﬁzhexp(—E(V, h; 0)) (4)

2.2 Learning parameters of the RBM model

Learning parameters of a RBM model can be achieved by updating
the weight matrix and biases using a gradient descend algorithm
(delta methods; Hinton and Salakhutdinov, 2006).

wH1 : w’ + Aw (5)

BlogPr(1/)

AWil' : E 

I 6(< Uihi>data_ < Uihi>model) (6)
where e is the learning rate; < vihi>data is the expected product of
the observed data and inferred hidden variables conditioning on
observed variables; < vihi>mode1 is the expected product of the
model—predicted v and h. One approach to derive < Uihi>mode1 is to
obtain samples of v and h from a model—defined distribution using
Markov chain Monte Carlo (MCMC) methods and then average the
product of the samples, which may take a long time to converge.
Representing the < vihi>model derived MCMC chain after conver—
gence as < wig->00, one updates the model parameter L0,; as follows:

AW); 2 E(< Uihi>data— < Uihi>oc) (7)

To calculate < wig->00, one can alternatively sample the states of
hidden variables given visible variables and then sample the states of
visible variables given hidden variables (Salakhutdinov et al., 2007)
based on the following equations.

Pr(lo,- 2 1lv) 2 a(b,- + 2:;1wiiui) (8)

My, 2 1lh) 2 a(a,- + ELM-17,) (9)

where a(x) is the logistic function 1/(1 + exp(—x)).

The convergence of a MCMC chain may take a long time. Thus,
to make RBM learning more efficient, we adopted a learning algo—
rithm called contrastive divergence (CD) (Welling and Hinton,
2002). Instead of running a MCMC chain for a very large number
of steps, CD learning just runs the chain for a small number n of
steps and minimizes the divergence between Kullback—Leibler diver—
gence KL(pO)) pee) and KL(pn))poc) to approximate < vihi>model
(Carreira—Perpinan and Hinton, 2005).

Therefore, the updating algorithm for a parameter of a RBM can
be rewritten as follows:

AWil : E(< Uihi>data_ < Uihi>model)

(10)

: e(< Uihi>Pr(l7)1/;w)_ < Vihi>n)
Aa, : e(< vi>dm— < 14>”) (11)
Ab) : E(< hi>data_ < hl>”) (12)

The pseudocode for training a RBM is as follows:
Repeat for t iterations:

1) Infer state of hidden units 17,-o given visible units
110 Pr(lo,-o)vo)

PI’U’II'O I 1)!)0) I (7(bl't + 21:1 Wil'tl/io) I < [7/0 >

2) Gibbs Sampling < 17,-o >—> binary matrix 17,-o
3) Infer state of visible units 11,1 given hidden units
loo Pr(v,-1)l’)o)

Prom 2 woo) 2 «(at + ELM-t < 17,-0 >> 2 < vn >

4) Infer state of hidden units 17,-1 given visible units
111 Pr(lo,-1)v1)

Pl’U’II-l : Ill/1) : a'(b,'t +  Wil't < 11,1 >) :< [7/1 >

5) Update parameters (weight between visible 1' and hid—
den /, bias of visible and bias of hidden)

Wil'Hl 2 W171 + E(< WOT/7,1) > — < VAT/7,1 >)
: Wiit + e(< vio>T < 17,-o > — < vi1>T < 17,-1>)
a,-H1 : at + e(< 12,0 > — < 12,1 >)

b,"”’ I bit + E(< bio > — < [7/1 >)

2.3 Learning a Deep Belief Network

Unlike a RBM, which captures the statistical structure of data using a
single layer of hidden nodes, a DBN strives to capture the statistical
structure using multiple layers in a distributed manner, such that each
layer captures the structure of different degrees of abstraction.
Training a DBN involves learning two sets of parameters: (i) a set of
recognition weight parameters for the upward propagation of infor—
mation from the visible layer to the hidden layers, and (ii) a set of gen-
erative weight parameters that can be used to generate data
corresponding to the visible layer. The learning of recognition weights
can be achieved by treating a DBN as a stack of RBMs and progres—
sively performing training in a bottom—up fashion (Hinton et al.,
2006; Hinton and Salakhutdinov, 2006). For example, one can treat
the visible layer 11 and the first hidden layer 17(1) as a RBM, and then
we can treat hidden layer 17(1) as a visible layer and form a RBM with
the hidden layer 17(2). Following the stack—wise learning of RBMs
weight parameters and instantiation of hidden variables in the top

9103 ‘Og isnﬁnv uo seleﬁuv soc] ‘BtHJOJtIBQ JO AJtSJQAtuf] 112 ﬁle'smumofquixo"soticuiJOJutotq/ﬁduq 11101} popcolumoq

Trans—species learning of cellular signaling systems

3011

 

layer, learning the generative weights across all layers can be achieved
by a backpropagation algorithm as in training standard neural net—
works. The pseudo—code for training a 4—layered DBN is as follows:
Input: Binary data matrix
Output: recognition and generative weights

Randomly initialize parameters
Train RBM for layer 1
Train RBM for layer 2
Train RBM for layer 3
Train RBM for layer 4

95"???‘1‘

Backpropagat ion

2.4 Bimodal DBN (bDBN)

A traditional DBN assumes that data are from one common distri—
bution, and the task is to use distributed hidden layers to capture the
structure of this distribution. However, our task of transferring
the knowledge learned from rat cells to human cells deviates from
the traditional assumption in that humans and rats may use different
pathways and signaling molecules to encode the response to a com—
mon stimulus. Thus our task is to learn a common encoding system
that governs two distributions, which may each have its own mode,
hence a bimodal problem. Inspired by the bimodal deep Boltzmann
machine model and multimodal deep learning (Liang, 2015; Ngiam,
2011; Srivastava and Salakhutdinov, 2012), which uses a multi—lay—
ered deep network to model the joint distribution of images and
associated text, we designed a modified variant of bimodal DBN
(bDBN) to capture the joint distribution of rat and human prote—
omic data. Our hypothesis is that rat and human cells share a com—
mon encoding system that respond to a common stimulus, but
utilize different proteins to carry out the response to the stimulus.
Thus, we can use the hidden layers to represent the common encod—
ing system, which regulates distinct human protein phosphorylation
and rat phosphorylation responses.

2.4.1 Training

Traditional bimodal models dealing with significantly different in—
put modalities such as audio and video (Fig. 3A) (Ngiam, 2011;
Srivastava and Salakhutdinov, 2012) usually require one or more

 

Fig. 2. Graph representation of the Deep Belief Network and related models.
(A) The graph representation of a 4-Iayered deep belief network. The double
circles represent visible variables, and the single circles represent hidden
variables. (B) The graph representation of a restricted Boltzmann machine.
(C) The graph representation of a semi-restricted Boltzmann machine in
which visible variables are connected

separate hidden layers to first capture the statistical structure of
each type of data and then model their joint distribution with com—
mon high level hidden layers. However, in our setting, although rat
and human proteomic data have their own modalities, they are not
drastically different. Therefore, instead of using two separate hidden
layers, we devised a modified bimodal DBN, in which a rat training
case and a human training case treated with a common stimulus are
merged into a joint input vector for the bDBN and connected to a
common hidden layer 17(1) (Fig. 3B). In this model, the training pro—
cedure is the same as training a conventional DBN using the algo—
rithm described in Section 2.3, but the prediction is carried out in a
bimodal manner. Under this setting, the hidden layers are forced to
encode the information that can be used to generate both rat and
human data, i.e. the hidden layers behave as a common encoder.

2.4.2 Prediction

When using a trained bDBN to predict human cell response to a spe—
cific stimulus based on the observed rat cell response to the same
stimulus, we only used the rat data to update the states of nodes in
the first hidden layer, Pr lamb/mt , with doubled edge weights
(2 X W‘DRat) from rat variables to hidden variables (red edges in Fig.
4). Then the upper hidden layers were updated using the same
method as in a conventional DBN using the recognition weights.
When the top hidden layer 17(4) was updated using rat data, the
bDBN propagated the information derived from rat data down—
wards to 17‘” using generative weights as in a feed forward neural
network to predict the human data (Fig. 4A). We finally predicted
the human cell response Pr (vhumn (17(1)) with weights only from hid—
den variables in 17(1) to human visible variables.

2.5 Semi—restricted bimodal deep belief network
(szBN)

Since signaling proteins in a phosphorylation cascade have regula—
tory relationships among themselves, we further modified the
bottom Boltzmann machine, consisting of 17(1) and 11, into a semi—re—
stricted Boltzmann (Taylor and Hinton, 2009), in which edges
between proteins from a common species are allowed (Fig. 3C). In
this model, the hidden variables in 17(1) capture the statistical struc—
ture of the ‘activated regulatory edges’ between signaling proteins,
instead of ‘activated protein nodes’. In this model, each human

hm

If“ hm

shared ﬁemsentallon

hm

hm hm

   

‘31in

Audio Inpul Video Input combined lunar! a Ballnpm

Fig. 3. Training DBN models. (A) A diagram of a conventional bimodal DBN.
The green and orange nodes represent different input modalities, e.g. audio
and video inputs, and each type is first modeled with a separate hidden layer,
and the joint distribution is modeled with a common higher layer hidden
nodes. (B) A 4-Iayered bimodal DBN for modeling rat and human proteomic
data. The blue and red nodes represent human and rat phosphoproteins re-
spectively. The bottom layer consists of observed variables. Upward arrows
represent recognition weights and downward arrows represent generative
weights. C) A szBN. Additional edges between proteins from the same spe-
cies are added

9103 ‘Og isnﬁnv uo sojoﬁuv soc] ‘BtHJOJtIBQ JO AJtSJQAtuf] 112 ﬁle'smumofquixo"soticuiJOJutotq/ﬁduq 11101} pQPBOIII/IAOG

3012

L.Chen et al.

 

   

A B
hm hm
his] htal
hm hm
hm hm
H.1mnnW'“ .   Ratw“: Hummw“

 

Fig. 4. Prediction with bDBN and szBN models. (A) Prediction with bDBN.
(B) Prediction with szBN. When predicting human phosphoprotein states,
information derived from rat phosphoprotein states is propagated upward
using weights represented by red arrows and then propagated downwards
using the weights represented by blue arrows to predict human phosphopro-
tein states

protein was connected to other human proteins, and the same rule
was applied to each rat protein. However, we didn’t allow inter—
actions between human proteins and rat proteins. The interaction
between proteins, which is represented as I, was added into the
negative phase shown below:

Pm, 2 llh) 2 a(a,- + Z:1W,-,-lo,- + 1,) (13)

1,- : zzﬁukmﬂa (14)

where I is the inﬂuence of the phosphorylation states of other pro—
teins on that of the ith protein.

Anik : e(< vivk>data— < Vij>model) where [2 51E 1' (15)

2.6 Performance evaluation

We adopted the evaluation metrics that were used to evaluate and
compare the performance of submitted models in the SBV
IMPROVER challenge, which include AUROC (area under receiver
operator characteristic; Bradley, 1997), AUPRC (area under the pre—
cision—recall curve; Davis and Goadrich, 2006; Goadrich et al.,
2004), Jaccard Similarity (Dombek et al., 2000), Matthews correl—
ation coefficient (Petersen et al., 2011), Spearman correlation (Brott
et al., 1989) and Pearson correlation (Adler and Parmryd, 2010), to
measure the accuracy of the prediction. In all metrics except for
Jaccard Similarity, the higher the score, the more accurate the model
is. We performed a series of cross—validation experiments, in which
we held out the three repeated experiments corresponding to one
stimulus of both rat and human cells, performed model training,
and test the performance using the held—out samples. All results dis—
cussed in the paper were derived from these cross—validation
experiments.

2.7 Model selection

When training a deep hierarchical model, often the first task is to de—
termine the structure of the model, i.e. the number of layers and the
number of hidden nodes per layer. However, currently there is no
well—established method for model selection when training deep
learning models. Therefore, we performed a series of cross—valid—
ation experiments to search for an ‘optimal’ structure for bimodal
and semi—restricted bimodal DBNs. We set the initial structure of

both bDBN and szBN to the following ranges: 17(1): 30—50; 17(2):
25—40; 17(3): 20—30; and 17(4): 20—25. We iteratively modified the
structure of the model by changing the number of hidden nodes
within a layer using a step size of 5 and explored all combinations in
the range stated above. In this case, the total number of models
tested is 120 (5"4"’3"’2) for both bDBN and szBN. Under each
particular setting, we performed a leave—one—out experiment to as—
sess the performance of a model. In such an experiment, we held out
both human and rat data treated by a common stimulus as the test
case, trained models with data treated by the rest of stimuli, and
then we predicted the states of human phosphoproteins using the
held—out rat data as illustrated in Figure 4. By doing this, we pre—
dicted human data treated by all stimuli, and we evaluated and com—
pared the performance using the AUROC of different models and
retained the model structure that led to the best performance. Note,
during leave—one—out training of a model with a given structure, the
parameters associated with each model can be different, and there—
fore the results reﬂect the fitness of the model with a particular
structure after averaging out the impact of individual parameters, an
approach closely related to Bayesian model selection (Bishop, 2006 ).

2.8 Baseline predictive models

As a comparison to bDBN and szBN, we formulated the task of
predicting human cell response based on rat cell response to a com—
mon stimulus as a classification problem, and we employed two cur—
rent state—of—the—art classification models, a support vector machine
(SVM) (Bishop, 2006) with a Gaussian kernel (Karatzoglou et al.,
2004) and an elastic—net regularized generalized linear model
(GLMNET) (Friedman et al., 2010) to predict human cell responses.
In this setting, we trained a classification model (SVM or
GLMNET) for one human protein using a vector of rat proteomic
data collected under a specific condition as input features (independ—
ent variables) and the human protein response under the same con—
dition as a binary class variable (dependent variables). We trained
one such classifier for each human protein class. We performed
leave—one—out cross—validation using SVM and GLMNET models re—
spectively. The results predicted by SVM and GLMNET were then
compared with the results predicted by DBN and szBN using the
metrics discussed in Section 2.6.

3 Results
3.1 The data

The protein phosphorylation response data in this study was pro—
vided by SBV IMPROVER (SBV IMPROVER, 2013). The data con—
tains the phosphorylation status of 16 proteins collected after
exposing rat and human cells to 26 different stimuli (Table 1). Each
stimulus was repeated 3 times. The SBV IMPROVER organizers
preprocessed the proteomic data into binary values to represent if a
protein was phosphorylated under a specific condition. We directly
utilized the binary input for our DBN models.

3.2 Model selection results

In order to identify the ‘optimal’ model structure that perform well,
we examined the performance of each model with a specific struc—
ture configuration stated in Section 2.7. For a given model, we per—
formed a leave—one—out cross validation experiment and calculated
the AUROC for the model. The average of the AUROCs for 120
bDBN models was 0.80, and the highest one is 0.86. The bDBN
structure yielding the best AUROC consisted of four hidden layers
with the following numbers of nodes 35, 30, 30 and 20, from 17(1) to

9103 ‘01; isnﬁnv uo sojoﬁuv soc] ‘BtHJOJtIBQ JO £11512»);qu 112 ﬁle'smumofquixo"soticuiJOJutotq/ﬁduq 11101} pQPBOIII/IAOG

Trans—species learning of cellular signaling systems

3013

 

17(4) respectively. For szBN, the mean of the AUROCs for 120 can—
didate models is 0.86 and the highest one was 0.93. The number of
nodes for the four layers for the best szBN model was 30, 30, 30
and 20, from 17(1) to 17(4) respectively. A tentative explanation for the
different numbers in 17(1) between bDBN and szBN is that the
edges between the visible variables in the szBN partially captured
the statistical structures of the visible variables, which reduced the
need for additional nodes in the layer 17(1). In the following sections,
we report the results derived from bDBN and szBN with these
two specific structures with the highest AUROCs.

3.2.1 Hyper parameters used for model training

The weights were updated using a learning rate of 0.1, momentum
of 0.9 and a weight decay of 0.0002. The weights were initialized
with random values sampled from a standard normal distribution
multiplied by 0.1. Contrastive divergence learning was started with
n : 1 and increased in small steps during training.

3.3 Comparison among different models
Table 2 shows the comparisons between different predictive models
in terms of 6 evaluation metrics. We highlighted the best value for
each metric using bold face letters. When comparing bDBN with
SVN and GLMNET, the results show that bDBN performs better in
terms of AUROC and Spearman’s correlation, but underperformed
in terms of AUPRC, Jaccard similarity, and Pearson correlation.
This is potentially due to the fact that we performed model selection
mainly using AUROC as the criteria. Strikingly, with the addition of
protein—protein edges in the visible layer, the 4—layered szBN per—
forms much better than all other models measured in all metrics.
Based on the AUROC value, the performance of the 4—layered
szBN > 4—layered bDBN > SVM > GLMNET. However, ranking
varies depending on the scoring method. It is known that models
pursuing optimal area under the ROC curve is not guaranteed to op—
timize the area under the Precision—Recall curve (Davis and
Goadrich, 2006). Indeed, we noted that the AUROC for the 4—lay—
ered DBN is better than the one for GLMNET. However, the
AUPRC for the 4—layered DBN is worse than the one for GLMNET
(Table 2; Fig. 5).

Table 1. Proteins and stimuli involved in this study

 

Stimuli 5AZA, AMPHIREGULIN, BETAHISTINE, BISACODYL,
CHOLESTEROL, CLENBUTEROL, EGF, EGF8,
FLAST, FORSKOLIN, HIGHGLU, IFNG, IGFII, 1L4,
MEPYRAMINE, NORETHINDRONE, ODN2006,
PDGFB, PMA, PROKINECITINZ, PROMETHAZINE,
SEROTONIN, SHH, TGFA, TNFA, WISP3, DME

Proteins AKTl, CREBl, FAK1, GSK3B, HSPBl, IKBA, KS6A1,
K5631, MK03, MK09, MK14K11, MP2K1, MP2K6,
PTN11, TF65, WNK1

 

Table 2. Leave-one-out accuracy scores of models

3.4 Biological interpretation of learned edges between
proteins in szBN

The best predictive power of the szBN reflects the importance of
capturing the correlation between signaling proteins. We then inves—
tigated whether the learned correlations between signaling proteins
are biologically sensible, although it should be noted that
Boltzmann machine models cannot infer causal relationships. For
each protein, we picked the top 3 strongest interaction edges for rat
and human respectively, and we organized the results as shown in
Figure 6. In this figure, if the interaction between a pair of proteins
exists in both rat and human data, the edge is colored green. If the
interaction is rat only, there is a blue line between the two proteins.
If the interaction is human only, there is a red line between the two
proteins. The results indicate that, while some common correlations
are shared between rat and human cells, different covariance struc—
ture exists in different proteomic data.

Due to the fact that signal transduction in live cells are dynamic
events, it is difficult to thoroughly evaluate the accuracy of inferred
interactions even through further experimentations. Conventional
evaluation metrics such as sensitivity and specificity are difficult to
assess in this study. Since it is possible that the signal transduction
between a pair of proteins known to have a regulatory relationship
may not be present under the experimental conditions of this study,
accurately assessing sensitivity is challenging; similarly, since there
are seldom reports or databases stating that signal transduction
never occurred between a pair of proteins, it is challenging to assess
if the lack of an edge between a pair of proteins in our model really
represents a true negative outcome. As such, conventional metrics
such as AUROC cannot be applied in our evaluation. However, we
noted that we were able to assess with reasonable confidence the
positive predictive value (PPV) of the model, i.e. the percentage of
the predicted signal transduction interactions that is known in litera—
ture. We performed a comprehensive literature review and cited the
references supporting the predicted regulatory relationship and
known protein—protein interactions in Supplementary Tables. The
results indicate that most of the predicted regulatory relationships

 

 

      

 

 

 

 

 

 

A ___ B D.
_- ‘— - shDBM—ﬂllyasiILZZ
- ﬂnN-dlawrs:0.d17
no _  swmsa
2 D GLMNET:0.444
E! E ‘
3 .9 no. _
E g o
03 {J
8 E
a: 1 V. _
a I shDﬂN-dlmrsﬂllﬁ D
l— N 3? --- DBN-dluyers:ﬂ.859
c: “- -- HEM-om N .
E swam D- —'
D etmuermoe _
9' : I I : | l I I
0.0 02 0.4 06 GB 1.0 02 0.4 06 00 10
False positive rate Recall

Fig. 5. ROC and RFC curves of different models. (A) Performance results of
four models in terms of AUROC. (B) Performance results of four models in
terms of AUPRC

 

 

AUPRC AUROC Jaccard. Matthews. Speaman. Pearson.

Similarity Correlation. Coefficient Correlation Correlation
4—layered bDBN 0.417 0.859 0.750 0.373 0.323 0.235
4—layered szBN 0.632 0.936 0.531 0.616 0.391 0.460
SVM 0.493 0.724 0.692 0.411 0.231 0.392
GLMNET 0.444 0.709 0.717 0.374 0.194 0.282

 

9103 ‘01; isnﬁnv uo sopﬁuv 50’] 0211110111123 JO £11512»);qu 112 /310's1cu1n0[p1q1x0"soticuiJOJutotq/ﬁduq 111011 popco1umoq

3014

L.Chen et al.

 

 

 

 

Fig. 6. Protein correlation network learned from the 4-Iayered szBN. Ovals
represent the proteins. A green line represents a common edge shared be-
tween human proteins and rat proteins; a red line represents an edge be-
tween human proteins; a blue line represents an edge between rat proteins

are supported by the literature or have evidence of physical inter—
actions between the proteins. Thus, the results support the notion
that the szBN correctly captured the correlation (thereby signal
transduction or cross talks) between phosphoproteins.

4 Discussion

In this study, we investigated the utility of novel deep hierarchical
models in a trans—species learning setting. To our knowledge, this is
the first report using deep hierarchical models to address this type of
problem. Our results indicate that, by learning to represent a com—
mon encoding system for both rat and human cells, the deep learn—
ing models outperform contemporary state—of—the—art classifiers in
this trans—species learning task.

The empirical success of deep hierarchical models may be attrib—
uted to the following advantages. First, the DBN is capable of learn—
ing novel representations of the data that are salient to the task at
hand. The DBN models are more compatible to the biological sys—
tems that generate the observed data. The hidden variables at the
different layers of the DBN models can capture information with
different degrees of abstraction, thus allowing the models to capture
a more complex covariance structure of the observed variables. It is
possible that hidden nodes at lower layers, e.g. 17(1), directly capture
the covariance of the observed protein phosphorylation states,
whereas the higher layers can capture the crosstalk between signal—
ing pathways that only occur in response to specific stimuli. Thus,
shallow models that only concentrate on the covariance at the level
of observed variables, such as SVM and elastic network, would have
difficulties capturing such a high—level covariance structure of the
data. It is now well appreciated that feature—learning methods, such
as DBN, tend to outperform feature selection methods in complex
domains, such as image classification and speech recognition
(Bengio et al., 2012; Hinton et al., 2006; Hinton and Salakhutdinov,
2006). Second, DBN strives to learn the common encoding system
for both human and rat data, and it naturally performs multi—label

classification by taking into account the covariance of the class vari—
ables. However, a conventional classifier, such as a SVM, can only
predict one human protein as the class variable in an independent
manner, thus failing to capture the covariance of class variables and
yielding inferior performance.

The szBN model developed in this study provides a novel ap—
proach capable of simultaneously learning interactions and predict—
ing the state of phosphoproteins. Interestingly, the model assigns
differential weights to the edges between phosphoproteins when
comparing those from rat and human cells, which potentially indi—
cates that different parts of signaling pathways are preferentially uti—
lized in a species—specific manner. However, this hypothesis still
needs to be experimentally tested in a relatively larger dataset.

Deep hierarchical models are particularly suited for modeling
cellular signaling systems, because signaling molecules in cells are
organized as a hierarchical network and information in the system is
compositionally encoded. Our results indicate that DBNs were cap—
able of capturing the complex information embedded in proteomic
data. Interestingly, in contrast to the training of deep learning mod—
els in a machine learning setting such as object recognition in image
analysis where usually a large number of training cases is required,
our results show that the DBN models performed very well given a
moderate size of training cases. This indicates that biological data
tend to have strong signals that can be captured by DBNs with rela—
tive ease. Our study demonstrates the feasibility of using deep hier—
archical models to simulate cellular signaling systems in general, and
we foresee that deep hierarchical models will be widely used in sys—
tems biology. For example, one can use deep hierarchical models to
study how cells encode the signals regulating gene expression, to de—
tect which signaling pathway is perturbed in a specific pathological
condition, e.g. cancer. Finally, models like our bDBN and szBN
provide a novel approach to simultaneously model multiple types of
‘omics’ data in an ‘integromics’ fashion.

Funding

This research was supported by the National Library Of Medicine of the
National Institutes of Health under Award Number ROlLM 010144,
ROlLM012011 and U54HG008540.

Conﬂict of Interest: none declared.

References

Adler,J. and Parmryd,I. (2010) Quantifying colocalization by correlation: the
Pearson correlation coefﬁcient is superior to the Mander’s overlap coefﬁ-
cient. Cytom Part A, 77A, 733—742.

Alberts,B. et al. (2008) Molecular Biology of the Cell. New York, NY:
Garland Science, Taylor 86 Francis Group, LLC.

Bengio,Y. et al. (2012) Representation learning: a review and new perspec-
tives. arXiv.org.

Bishop,C.M. (2006) Pattern Recognition and Machine Learning. Springer,
New York.

Bradley,A.P. (1997) The use of the area under the roc curve in the evaluation
of machine learning algorithms. Pattern Recogn., 30, 1145—1 15 9.

Brott,T. et al. (1989) Measurements of acute cerebral infarction—a clinical
examination scale. Stroke, 20, 864—870.

Brown,S.D. (2011) Disease model discovery and translation. Introduction.
Mammalian Genome Ofﬁ ]. Int. Mammalian Genome Soc., 22, 361.

Carreira-Perpinan,M.A. and Hinton,G.E. (2005) On Contrastive Divergence
Learning, In: Artiﬁcial Intelligence and Statistics, pp. 33—40.

Davis,J. and Goadrich,M. (2006) The relationship between Precision—Recall
and ROC curves. Proceedings of the 23rd International Conference on
Machine Learning, pp. 233—240.

9103 ‘01; isanV uo sopﬁuv 50’] 0211110111123 JO AJtSJQAtuf] 112 /310's1cu1nofp1q1xo"soticuiJOJutotq/ﬁduq 111011 popco1umoq

Trans—species learning of cellular signaling systems

3015

 

Dombek,P.E. et al. (2000) Use of repetitive DNA sequences and the PCR to
differentiate Escherichia coli isolates from human and animal sources. Appl.
Environ. Microh., 66, 25 72—25 77.

Friedman,J. et al. (2010) Regularization paths for generalized linear models
via coordinate descent]. Stat. Softw., 33, 1—22.

Goadrich,M. et al. (2004) Learning ensembles of ﬁrst—order clauses for recall-
precision curves: a case study in biomedical information extraction. Lect.
Notes Artif. Int., 3194, 98—115.

Hinton,G.E. et al. (2006) A fast learning algorithm for deep belief nets. Neural
Comput., 18,1527—1554.

Hinton,G.E. and Salakhutdinov,R.R. (2006) Reducing the dimensionality of
data with neural networks. Science, 313, 504—507.

Jin,B. et al. (2008) Multi—label literature classiﬁcation based on the Gene
Ontology graph. BMC Bioinformatics, 9, 525.

Karatzoglou,A. et al. (2004) kernlab—an S4 package for kernel methods in R.
]. Stat. Softm, 11, 1.

Liang,M. et al. (2015) Integrative data analysis of multi—platform cancer data
with a multimodal deep learning approach. IEEE Trans. Comput. Biol.
Bioinf, 99, 1.

McGonigle,P. and Ruggeri,B. (2014) Animal models of human disease: chal—
lenges in enabling translation. Biochem. Pharmacol., 87, 162—171.

Ngiam,J. et al. (2011) Multimodal deep learning. Proceedings of the 28th
International Conference on Machine Learning (ICML-11).

Omar,B.A. et al. (2013) Enhanced beta cell function and anti—inﬂammatory ef-
fect after chronic treatment with the dipeptidyl peptidase-4 inhibitor

vildagliptin in an advanced—aged diet-induced obesity mouse model.
Diabetologia, 56, 1752—1760.

Petersen,T.N. et al. (2011) SignalP 4.0: discriminating signal peptides from
transmembrane regions. Nat. Methods, 8, 785—786.

Poussin,C. et al. (2014) The species translation challenge—a systems
biology perspective on human and rat bronchial epithelial cells. Scientiﬁc
Data, 1, 1—14.

thissorrakrai,K. et al. (2015) Understanding the limits of animal models as
predictors of human biology: lessons learned from the sbv IMPROVER
Species Translation Challenge. Bioinformatics, 31, 471—483.

Salakhutdinov,R. et al. (2007) Restricted Boltzmann Machines for
Collaborative Filtering. Proceedings of the 24th International Conference
on Machine Learning, pp. 791—798.

SBV IMPROVER. (2013) SBV IMPROVER: Species Translation Challenge
Overview.

Srivastava,N. and Salakhutdinov,R. (2012) Multimodal learning with deep
Boltzmann machines. In: NIPS, pp. 2231—2239.

Taylor,G.W. and Hinton,G.E. (2009) Factored conditional
restricted Boltzmann Machines for modeling motion style. In: Proceedings
of the 26th Annual International Conference on Machine Learning,
pp. 1025—1032.

Tsoumakas,G. and Katakis,I. (2007) Multi-label classiﬁcation: an overview.
Data Warehousing Mining, 3, 1—13.

Welling,M. and Hinton,G.E. (2002) A new learning algorithm for Mean Field
Boltzmann Machines. Lect. Notes Comput. Sci., 2415, 35 1—35 7.

9103 ‘01; JSanV 110 sopﬁuv 50'] ‘1211110111123 10 1015191111111 112 /310's112u1n0[p101x0'so1112111101u101q//:d1111 111011 pop1201um0q

