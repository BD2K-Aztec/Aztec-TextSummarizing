ORIGINAL PAPER

Vol. 27 no. 18 2011, pages 2554-2562
doi:10. 1 093/bioinformatics/btr444

 

Gene expression

Advance Access publication July 29, 2011

Bayesian prediction of tissue-regulated splicing using RNA

sequence and cellular context

Hui Yuan Xionglal, Yoseph Barash1’25r and Brendan J. FreylsZa*

1Department of Electrical and Computer Engineering, University of Toronto, Toronto, M583G4 and 2Banting and
Best Department of Medical Research, Centre of Cellular and Biomoleoular Research, University of Toronto, Toronto,

M583E1 , Canada

Associate Editor: Ivo Hofacker

 

ABSTRACT

Motivation: Alternative splicing is a major contributor to cellular
diversity in mammalian tissues and relates to many human diseases.
An important goal in understanding this phenomenon is to infer a
‘splicing code’ that predicts how splicing is regulated in different cell
types by features derived from RNA, DNA and epigenetic modifiers.
Methods: We formulate the assembly of a splicing code as a
problem of statistical inference and introduce a Bayesian method that
uses an adaptively selected number of hidden variables to combine
subgroups of features into a network, allows different tissues to share
feature subgroups and uses a Gibbs sampler to hedge predictions
and ascertain the statistical significance of identified features.
Results: Using data for 3665 cassette exons, 1014 RNA features
and 4 tissue types derived from 27 mouse tissues (http://genes
.toronto.edu/wasp), we benchmarked several methods. Our method
outperforms all others, and achieves relative improvements of 52% in
splicing code quality and up to 22% in classification error, compared
with the state of the art. Novel combinations of regulatory features
and novel combinations of tissues that share feature subgroups were
identified using our method.

Contact: frey@psi.toronto.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on May 3, 2011; revised on July 7, 2011; accepted on July
23, 2011

1 INTRODUCTION

Alternative splicing enables individual genes to generate different
transcripts, by selectively including or excluding RNA sequences.
High-throughput sequencing shows that over 90% of human genes
are alternatively spliced, mostly in a tissue-dependent manner (Fan
et (11., 2008; Wang et (11., 2008). The importance of alternative
splicing is evidenced by numerous examples of genes whose
functions are switched depending on which alternative transcript
(isoform) is expressed, plus analyses showing that a large fraction
of human disease mutations affect splice site selection (Wang
and Cooper, 2007). These results underscore the importance of
accounting for splicing regulation when modeling gene expression.

For over two decades, researchers have sought to deﬁne splicing
regulatory models in the form of a mapping from genomic features

 

*To whom correspondence should be addressed.
lThe authors wish it to be known that. in their opinion. the ﬁrst two authors
should be regarded as joint First Authors.

600 - —I—

500 -
400 -

300 -

Code Quality

200 -

100-

 

 

 

 

O
a}
J

9° “L '9“ x 6 Q d“ x0
“96 €33 05‘98 do“? \0 or {$6 6&0
“NW $0» 9. 96 \o e
g\ ‘99 0°: 0°
‘5“0 eg x?)
so“

Fig. 1. A comparison of methods for predicting tissue-regulated splicing in
mouse. using the metric of ‘code quality’ measured in bits (see main text).

and cellular conditions to predicted abundances of alternative
transcripts (Blencowe, 2006; Chan and Black, 1997; Hartmann and
Valcarcel, 2009; Lim and Sharp, 1998). In the words of Wang and
Burge (2008), ‘An important long-term goal in the community is
to determine a ‘splicing code’: A set of rules that can predict the
splicing pattern of any primary transcript sequence’.

Recently, we described the assembly of a mouse splicing code
that can be used to predict the regulatory properties of previously
uncharacterized exons, predict regions in the unspliced transcript
that when mutated led to changes in splicing patterns, and reveal
novel regulatory mechanisms (Barash et al., 2010a). Our purpose
here is to (i) describe a dataset and evaluation method that
researchers can use to improve and extend splicing codes; (ii)
introduce a Bayesian technique that uses hidden variables to
model relationships between features and splicing changes within
a network; and (iii) benchmark several machine learning methods.

Figure 1 compares our Bayesian technique to several other
methods in terms of ‘code quality’, which is the amount of genome-
wide splicing variability accounted for by RNA sequence features
(see below). The result labeled ‘tissue only’ indicates how much
splicing variability is accounted for by tissue type, i.e. that different
tissues have different overall levels of splicing. Most of the methods

 

2554 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /3.Io's[Bruno[pJOJXO'soiwurJOJurorqﬂ:duq 11101} pQPBOIII/lAOG

9103 ‘Og anﬁnv uo ::

Bayesian prediction of splicing

 

 

 

 

 

(b) RNA Features

 

 

 

 

 

 

 

 

 

 

 

Constitutive Alternatively Constitutive
Exon 1 Spliced Exon Exon 2
Intron 1 Intron 2
RNA Feature
Extraction

 

Feature Set

 

 

Bayesian
Splicing Code

i

Predictions

 

 

 

    
 

Hidden
Variables

0
ill

CNS Muscle Embryo Digestive

Fig. 2. (a) A ‘splicing code’ uses features extracted from primary RNA to predict splicing patterns. (b) A Bayesian neural network uses RNA features (top)
to determine hidden variables (middle) that are used to predict tissue-dependent splicing (bottom). Inference involves adding and deleting features, hidden

variables and connections (solid and dashed lines).

we examined were able to account for signiﬁcantly more variability
by relating splicing levels with features in the unspliced RNA, such
as potential binding sites of factors that regulate splicing. The result
labeled ‘boosting’ was obtained in our original work on the splicing
code (Barash et al., 2010a). A simple nearest neighbours method,
which uses previously proﬁled exons with similar sequence features
to make predictions, accounts for very little additional variability,
if any. This result suggests that the features governing splicing
operate in a combinatorial fashion. Methods based on principal
components analysis (PCA), multinomial regression and regularized
neural networks perform similarly to the original method. The SVM
outperforms the original method. The Bayesian technique described
below performs signiﬁcantly better than all other techniques.

Next, we describe our publicly available dataset, review a log-
likelihood-based measure of splicing code quality and explain how
inference of the splicing code can be formulated as statistical
inference. We describe our Bayesian technique and several other
approaches, before providing details about how they compare in
terms of code quality and classiﬁcation accuracy. We investigate
properties of the Bayesian method such as how it beneﬁts by sharing
hidden variables when making predictions for different tissues, and
how the number of selected hidden variables differs between tissues.
We examine novel combinations of regulatory features that are
elucidated by our method and conclude by discussing promising
directions for further research in the area.

2 LEARNING REGULATORY MODELS

We are not really interested in exactly how only a single transcript
is spliced at a particular point in time within a particular cell.
This knowledge would neither provide an understanding of how
splicing works nor enable us to predict what would happen under
different conditions. A more compelling scientiﬁc goal is to infer a
network model that summarizes related splicing patterns, accounts
for variability in cellular conditions and how they inﬂuence splicing,

and predicts how splicing will occur for novel RNA sequences.
We expect such a model to provide predictions in the form of
probabilities, because we can measure cellular conditions in only an
approximate manner and also because the underlying biochemical
processes are stochastic. By peering inside the inferred network,
researchers can predict regulatory mechanisms.

Taking a predictive modeling approach, we seek a network model
that can be applied to a comprehensive set of RNA sequences
to make accurate probabilistic predictions for how splicing will
occur under different cellular conditions, using features derived
from RNA, DNA, epigenetic modiﬁers, etc. Here, we use only RNA
features (Fig. 2a). This can be viewed as a problem of statistical
inference, where we assume that the set of RNA sequences and
cellular conditions is an unbiased sample from a distribution of
interest, such as all alternatively spliced exons in a group of mouse
tissues. Given a set of corresponding RNA sequences, cellular
conditions and splicing patterns, statistical inference is used to infer
a predictive model. Previously, we used this approach to produce a
model that can make predictions for previously unseen transcripts
and veriﬁed several novel predicted regulatory mechanisms by
mutating sequences in a minigene reporter (Barash et (11., 2010a).

Data used to infer the code: Using data from Fagnani et a1. (2007)
and a preprocessing method described in Barash et al. (2010b), we
generated a new, publicly available evaluation protocol that can
be used to infer predictive splicing models and compare different
inference methods. RNA feature vectors and splicing patterns for
different tissues are provided for several training sets and test sets
at http://genes.toronto.edu/wasp.

The set of RNA sequences was obtained by mining EST and
CDNA libraries and identifying 3665 cassette-type mouse exons, i.e.
exons that are sometimes included and sometimes excluded in the
spliced transcripts extracted from tissues and cell lines. The fraction
of isoforms including each cassette exon was proﬁled across 27
mouse tissues. Predicting fractions of isoforms is difﬁcult because
many unobserved variables contribute to exon-speciﬁc regulation.

 

2555

112 /3.Io's[Bruno[pJOJXO'soiwurJOJurorqﬂ:duq 11101} papBOIII/lAOG

9103 ‘Og anﬁnv uo ::

H. KXiong et al.

 

We found it was easier to predict, for each exon and tissue type, the
direction of change of the fraction of isoforms including the exon,
relative to other tissues. In Barash et al. (2010b), we introduced
a preprocessing method that estimates such relative changes while
taking into account different noise sources. Splicing changes were
estimated for four cellular conditions, approximately corresponding
to cells from central nerve system (CNS) tissues, muscle tissues,
whole embryos plus embryonic cell lines and digestive tissues. For
each exon and tissue type, we generated three real-valued, positive
prediction targets qinc, qe"C and qr1C corresponding to probabilities
that the exon is more likely to be included in the given tissue relative
to other tissues, more likely to be excluded or more likely to exhibit
no change relative to other tissues (Supplementary Fig. S1). These
targets need not add up to one; their sum relates to the conﬁdence
in the observed splicing pattern and can be estimated using a
probability model (Barash et al., 2010b). However, for simplicity, we
normalized them in the current dataset s.t. qin°+qex°+qn° = 1. In
each tissue, ~ 10% of exons exhibit increased inclusion or exclusion
(qinC/exc >09) and the entropy of the mean target distribution is
0.63 bits per tissue.

For each exon in the dataset, 1014 features were extracted
from the exon and its two ﬂanking exons, windows of 300 nt of
intronic sequence adjacent to those exons, and other regions of the
unspliced transcript (Barash et al., 2010a). Introns with <300 nt were
padded with blanks. Examples of features include region-speciﬁc
counts of short sequence motifs, scores for potential RNA-binding
protein binding sites, exon and intron lengths, secondary structure
probabilities and whether or not exon inclusion or exclusion
introduces a premature termination codon. Feature values may be
binary, integer, discrete but non-ordinal, or real-valued, and real-
valued features may be sparse or densely distributed (Supplementary
Fig. S2). Many groups of features are highly correlated, such as
those derived using slightly different literature-curated deﬁnitions
of binding sites.

The feature vectors form a 3665 X 1014 matrix, with rows
corresponding to exons and columns corresponding to features, and
the tissue-dependent targets form a 3665 X 12 matrix, with each row

containing qinc, qe"C and 6]“ for each of the four tissue types.

Training sets and test sets: The number of features is large relative
to the number of examples, so methods that are not regularized
will likely overﬁt the data. The situation is made worse by the fact
that the targets are sparse, having only ~2400 bits of information
per tissue. Care needs to be taken to avoid overﬁtting the model
in such a way that generalization is not possible. For example,
when gradient descent is used to train a multinomial regression
model, the training log-likelihood continues to increase while the
test log-likelihood quickly peaks and then rapidly drops below the
log-likelihood achieved by a naive guessing scheme (Supplementary
Fig. S3). To ensure that reported results are unbiased, we obtain them
using held out test data. Also, care was taken to remove redundancies
between training cases and test cases by checking for exon sequence
similarity (Barash et al., 2010a). We use ﬁve-fold cross-validation
and to estimate conﬁdence intervals we repeat the procedure six
times using different randomly generated data partitions. For each
partition, ﬁve models are constructed and the corresponding ﬁve
test set performances are summed together to obtain an unbiased
estimate of performance.

A complete set of feature vectors and targets (q’s) are provided
for training sets and test sets at http://genes.toronto.edu/wasp. In
these datasets, for each partition, every exon is used once for testing
and four times for training. For each fold and each partition, the
preprocessing stage used to convert microarray measurements to
targets (Barash et al., 2010b) is constructed independently using only
training exons. Then, measurements of test exons are preprocessed
to produce the test targets for that partition and fold. This procedure
is designed to avoid reporting performance estimates that are biased
by using test data to develop the preprocessing stage. Note that the
targets for the same exon may differ in different folds.

Measuring code quality using relative log-likelihood: The feature
vectors and targets (q’s) described above are used to train a model.
For each exon and tissue type, the model outputs predictions in the
form of three probabilities pinc, pexc and pm, that are meant to be
similar to the training targets qinc, qe"C and q“. The code quality
for tissue t is measured thus (Barash et al., 2010a):

p5
at: Z Z q§,elog(%). (1)

eeExons se{inc,exc,nc}

where p?! e and qgse are the model prediction and the target for exon
e, splicing change s and tissue t. 21" is the average of qgse across all
tissues and exons, 21" =Ztﬁquge/ZS/Ztﬁeque, and corresponds to
the prediction made by a naive guesser that ignores the RNA feature
vector and tissue type. Note that if ZS q?! 6 > ZS q‘t’sei, then exon e
counts more than exon e’ toward the code quality, 7-1; .

Code quality can be viewed as a difference of two Kullback—
Leibler (KL) divergences: H. =:9quiiq>—DKL(qt,eiipt,e>.
These two KL divergences measure, in bits of information, how
much the predictions from the model and the naive guesser inform
us about the splicing patterns. The difference between them is the
amount of additional information provided by the model, beyond
naive guessing. When the predictions perfectly match the targets,
the highest possible code quality is obtained. It equals the entropy
of the targets minus the cross-entropy between the targets and the
naive guesser: ~ 2.5 bits/exon. A negative code quality implies that
the prediction is worse than naive guessing; this can occur if the
predictions are overly conﬁdent and sometimes wrong. The net code
quality is obtained by summing over tissues: H = Eta-158116520.

Code quality can be alternatively interpreted as the improvement
in log-likelihood, calculated using partial data counts:

H=£_£Naive7 (2)
£=ZZqu,elogp;,ev £Naive=ZZZqzelogZﬂ
t e s t e s

Inferring a model inﬂuences only the ﬁrst term, [1, so the code quality
H can be optimized using likelihood-based methods.

3 ALGORITHMS

3.1 Bayesian Neural Network

To account for combinatorial interactions between RNA features, we
consider models with hidden variables that are non-linear ﬁmctions
of combinations of subsets of features (Rumelhart et al., 1986).
Hidden variables are used to predict tissue-dependent splicing
changes, as shown in Figure 2b. Since it is not known beforehand

 

2556

112 /3.Io's[BrunoIpJOJXO'sonBurJOJurorq”:duq 111011 pepBo1umoq

9103 ‘0g15n8nv uo ::

Bayesian prediction of splicing

 

which features should be connected to each hidden variable, an
exact search over all possible models is not feasible. Also, as
explained above, overﬁtting is a concern because the data are limited,
sparse and noisy. To address these issues, we take a Bayesian
approach (Bishop, 2006; MacKay, 1992; Neal, 1996) where the
computational task is to sample from a posterior distribution over
models. Predictions for novel test cases are made by averaging the
predictions from the sample of models, and important features can
be identiﬁed by checking to see if they are used more frequently
than expected at random under the prior distribution of models.

Model architecture: We use a two layer network (Fig. 2b) that
receives as input F RNA features x1,...,x1: and uses them to
determine the values of up to N hidden variables h1,...,hN,
which are used to determine the prediction probabilities p1“, I)?”
and p?” for each tissue t. Parameters are used to account for
how input features inﬂuence hidden variables and how hidden
variables inﬂuence prediction probabilities. A parameter value of
zero indicates an absent connection. If all parameters connecting a
hidden variable to the outputs are zero, then the hidden variable is
effectively absent from the network. The algorithm described below
is used to search over network structures and parameter values.

The inﬂuence of feature rf on hidden variable h,- is accounted for
by the real-valued parameter wfsi. The hidden variables process the
sum of weighted features using a non-linear sigmoid function:

hi=1/(1+e_2;slwfvixf). (3)

The outputs of the hidden variables are used to compute the
prediction probability p; for each splicing pattern s (inclusion,
exclusion or no change) and each tissue t as follows:

I); = eZi‘L1VI,ihi/( 2 (32:1 Vii/thi)_ 
s/e{inc,exc,nc}
Here, the inﬂuence of hidden variable h,- on the prediction for
splicing change s in tissue t is accounted for by the parameter vii.
Depending on the connectivity (non-zero weights), each feature
may be used by more than one hidden variable and each hidden
variable may be used to predict splicing in more than one tissue.
These properties enable the model to account for combinations of
features that are tissue speciﬁc or shared across different tissues. To
test whether allowing the model to share hidden regulatory variables
across tissues is important, we also tried a model without sharing of
hidden variables, i.e. where one model was trained for each tissue.

Prior distribution over models: We use a prior distribution that
allows ﬂexibility in the number of hidden variables and the
connectivity in the network. The prior distribution over each
parameter has a ‘spike and slab’ form (Ishwaran and Rao, 2005),
which enables connections to be shut off. The input features
are connected to hidden variables independently with Bernoulli
probability l—ot and the parameters for connected variables have
standard normal distributions. The number of hidden variables n;
used to predict splicing in each tissue is Poisson distributed with
an expected number of hidden variables A. Non-zero hidden-to-
output parameters have multivariate standard normal distributions.
Under the Bernoulli and Poisson priors, the hidden variables are
exchangeable and their connections to different input features are
independent, as are their connections to different tissues. We truncate
the Poisson distribution using a maximum number of allowed hidden
variables, N, which controls the sharing of hidden variables between

tissues. If N is large compared with A, sharing occurs infrequently,
whereas if N is small, different tissues are likely to share hidden
variables.

Based on initial experiments using validation data, we set a = 0.1
to encourage sparse use of RNA features, and we set A: 10 and
N :30 to encourage moderate sharing of hidden variables and
so that on the order of 10 hidden variables are used to predict
each tissue. Finally, to facilitate traversing the space of possible
models using Gibbs sampling as described below, we discretize
the parameters, so that WfJ-e{—5.0,—4.9,...,4.9,5.0} and vise
{—5.0,—4.8,...,4.8,5.0}. We found that the performance of our
method is robust to the above choices (Section 2 in Supplementary
Material and Fig. 4).

Markov Chain Monte Carlo sampling: We use Gibbs sampling
to sample from the posterior distribution over models. In each
iteration, we ﬁrst sample the Wf’fs in sequence from their posterior
distributions while ﬁxing all other parameters:

WfJ ~P(W.f,i)l—Il—Il—[Pi,e(wiVIII"?- (5)
e t S

P(w.fsl-) is the spike-and-slab prior distribution over WfJ, and the
likelihood p?! e(w,v) is computed using Equations (3) and (4). Next,
for each hidden variable i and tissue t, we jointly sample viii:
(vinc vexc v23) from its posterior distribution:

t,i ’ t,i ’
m~P(v.,i>]'[1'[1'[p:,e(w.v>qie. (6)
e t S

where P(vt3i) is the spike-and-slab prior on vii,- = (vii-CNSEC,  .
Initially, all parameters are set to zero and in each iteration of
Gibbs sampling, the features and hidden variables are processed
in random order. The parameters (w’s and v’s) are recorded after
each iteration up to a maximum number of 2000 iterations. The
initial 150 samples are not used when making predictions, because
we found that at least that many iterations were needed for mixing
(Supplementary Fig. S5). Using the feature vector for a test exon e’ ,
predictions are made using each model in this ensemble of models,

and the predictions are averaged together to make a ﬁnal prediction.

3.2 Other methods included in the benchmark

We examined several popular methods, including the boosting
method used in Barash et al. (2010a). For methods that use validation
data to set a regularization parameter, we set aside 1/4 of the
training data for validation using code quality. Using the selected
regularization parameter, the model was re-trained using all training
data.

The simplest method was k nearest neighbors (Bishop, 2006): For
a test exon e’, the q-values corresponding to the k training exons
e68 whose feature vectors were closest in L2 to the test feature
vector were averaged to make a prediction: 1); = $266551; 6. k
was chosen using validation data, as described above.

We examined three regularized multinomial regression methods
(Bishop, 2006). The ﬁrst method was early stopping, where the
parameters were initialized to small random values and then batch
gradient descent with a learning rate of 0.1 was used to adjust the
parameters using training data, until the code quality of validation
data reached its maximum. The second method used principal
component analysis (PCA) to reduce the feature vector dimension
from F to k, where k was chosen using validation data. The

 

2557

112 /3.Io's[BrunoIpJOJXO'sonBurJOJurorq”:duq urorj popBo1umoq

9103 ‘0g15n8nv uo ::

H. KXiong et al.

 

third method, named PCA pursuit, constructed a feature vector by
recursively selecting PCA features that gave the largest increase in
validation code quality, until the validation code quality reached its
maximum. For each of the above methods, 10 training runs were
applied and the validation data was used to select the best model.

We also tried several variations of the support vector machine
(SVM) (Scholkopf and Smola, 2002). For each tissue type, three
one-versus-all SVMs were trained using the L2 kernel for each of the
labels inc, exc and no, which were obtained by thresholding the q’s at
0.5. The resulting triplets of real-valued discriminants were used as
inputs for multinomial regression, to predict the three probabilities
11151:, pixec, p22,. Results were poor when we trained SVMs using all
1014 features, so PCA was used to project them onto a subspace
with 40 dimensions, which was selected using validation data.

To test whether a non-Bayesian version of the model described
in the previous section could give good results, we trained a
fully connected network with 10 hidden variables (which equals
the expected number of hidden variables A) using early stopping
(Bishop, 2006; Rumelhart et al., 1986). The parameters were
initialized to small random values and batch gradient descent with a
learning rate of 0.1 was used to train the network until the validation
code quality reached a maximum. The best of 10 trained models was
selected using validation data.

We also examined naive Bayes, where the features are assumed
to be independent given the splicing class (inc, exc, no) and tissue
type. For each tissue, every non-binary feature was binarized by
searching for a threshold that maximized its mutual information
with the q-distributions. Then, for each splicing class, the Bernoulli
probability of every binary feature was independently estimated
using the training data. Given a test feature vector, Bayes’ rule was
used to compute the posterior probability of each class.

It is often a good idea to average the predictions from quite
different methods, since they may err in different ways. Based on
initial experiments, we combined the predictions from the best non-
Bayesian methods, including the SVM and multinomial regression
using high-variance PCA features as inputs, plus the neural network
and multinomial regression trained using early stopping.

4 RESULTS

Comparisons of test code quality: All methods were evaluated using
held out test data as described above and the test code quality for
several methods is plotted in Figure 1 (see below for ﬁirther details).
The Bayesian method achieved a relative improvement of 52% over
the previously published result obtained using boosting (Barash
et al., 2010a), and signiﬁcantly outperforms the other methods.
While most other methods performed reasonably well, a notable
exception is the nearest neighbor method. This may be due to a
large fraction of irrelevant features and subgroups of features that
operate combinatorially, which simple nearest neighbor methods are
not well-suited to dealing with.

Table 1 gives a breakdown of code quality according to tissue
type.1 For each tissue and direction of regulation, bold font is used
to indicate the highest code quality, regular font is used to indicate
values that are signiﬁcantly lower than the highest value (P < 0.021,

 

lNaive Bayes is not shown, because it achieves a negative code quality due
to extreme posterior probabilities induced by making a highly inaccurate
feature independence assumption.

Table 1. Comparison of splicing test accuracy for different methods,
measured using code quality (bits)

 

 

 

Method Tissue type
CNS Muscle Embryo Digest

Boosting (Nature 2010) 198 :l: 7 71:1: 1 78 :l: 2 64 :l: 3
Nearest neighbors 19 :I: 15 21 :I: 2 20 :I: 4 24 :I: 6
Regression, early stop" 202 :I: 23 103 :I: 10 79 :I: 7 73 :I: 7
Regression, PCA" 210 :I: 18 98 :I: 5 90 :I: 3 65 :I: 4
Regression, PCA pursuit 192 :I: 30 95 :I: 7 92 :I: 3 57 :I: 8
SVM, PCA" 223 :l: 12 95 :l: 7 97 :l: 7 72 :l:4

Neural net, early stop* 196 :I: 12 100 :I: 10 77 :I: 9 67 :I: 8
Avg predictions from>i< 232 :I: 14 114 :I: 5 102 :I: 4 82 :I: 4
Bayesian method 263 :I: 13 129 :I: 4 126 :I: 3 105 :I: 8

Without sharing 240 :I: 16 112 :I: 3 104 :I: 3 76 :I: 7

 

:l: indicates 1 SD; top performances are shown in bold; * denotes methods that were
bagged together (predictions averaged).

t-test), and bold font is used for the other values to indicate that
they might be comparable to the highest value. Using the best result
published to date as a baseline (Barash et al., 2010a), the Bayesian
technique achieves improvements ranging from 30% in CNS tissues
to almost 80% in muscle tissues. Interestingly, when the Bayesian
method was applied to each tissue separately so that hidden variables
were not shared across tissues, lower code qualities were obtained.
Later, we explore how hidden variables are successfully shared
across tissues.

Among all tissues, most methods achieved their highest code
quality for CNS tissues. One reason for this is that there is a higher
amount of tissue-speciﬁc splicing variability in CNS tissues, so the
splicing information capacity is higher. Out of those exons exhibiting
tissue-variable splicing (qi“°30.99 or qex°3099 in at least one
tissue), 51% exhibit changes in CNS tissues, compared with 23,
32 and 25% in muscle, embryonic and digestive tissues, which is
consistent with human RNA-Seq analysis (Pan et al., 2008; Wang
et al., 2008). Another reason is that many of the RNA features were
derived from previous work, which concentrated on the regulation
in brain and muscle tissues. So, the inference problem is more
straightforward for those tissues. A third reason is that splicing
in certain tissues may be controlled by relatively easily inferred
mechanisms, e.g. well-studied regulators such as Fox contribute to
the regulation of splicing in both muscle and CNS tissues.

Averaging the predictions from the best non-Bayesian methods led
to an increase in code quality over the individual predictors, but the
Bayesian method performed signiﬁcantly better. We wondered if the
other methods had anything complementary to offer to the Bayesian
method, so we included the Bayesian method in the average
predictor. Interestingly, there was no signiﬁcant improvement
beyond the stand-alone Bayesian method’s performance, even when
we adjusted the relative weighting of the methods using test data
(the Bayesian method weight was 0.94).

Comparisons of classiﬁcation accuracy: The log-likelihood based
measure of code quality described above takes into account how
accurately each method can assess its conﬁdence in its predictions.
A different, but related task is to apply a threshold to each method’s
prediction probabilities and measure classiﬁcation accuracy. We
deﬁned two binary classiﬁcation tasks for each tissue type,

 

2558

112 /3.Io's[BrunoIpJOJXO'sonBuiJOJuioiq”:duq 111011 popBo1umoq

9103 ‘0g isnﬁnv uo ::

Bayesian prediction of splicing

 

Table 2. Comparison of splicing test accuracy, measured using area under
the ROC curve

 

Method Tissue type

 

CNS Muscle

 

Inclusion Exclusion Inclusion Exclusion

 

Boosting (Nature 2010) 76.1 :I: 0.6 60.3 :I: 1.3 70.7 :I: 0.5 60.7 :I: 0.5
Nearest neighbors 71.7 :I: 1.0 53.8 :I: 1.2 60.0 :I: 1.8 53.1 21:13
Regression, early stop* 75.7 :I: 0.8 61.3 :I: 0.8 74.8 :I: 1.0 62.6 :I: 1.0
Regression, PCA" 77.1 :I: 0.6 61.3 :I: 0.8 73.4 :I: 0.4 63.5 :I: 0.5
Regression, PCA pursuit 76.6 :I: 1.2 61.1 :I: 1.7 73.5 :I: 1.2 62.8 :I: 1.0
SVM, PCA* 77.0 :I:1.1 61.1:I: 0.9 72.5 :I: 0.6 61.8 :I: 1.3
Naive Bayes 75.4 :I: 0.6 62.1 :I: 1.1 73.3 :I: 0.5 62.9 :I: 0.8
Neural net, early stop" 75.6 :I: 0.7 60.9 :I: 1.0 74.2 :I: 1.1 62.6 :I: 1.6
Avg predictions from >i< 77.3 :I: 0.7 62.1 :I: 0.7 74.7 :I: 0.6 63.4 :I: 0.7
Bayesian method 79.1 :I: 0.5 63.1 :I: 0.6 77.0 :I: 0.5 63.2 :I: 0.8

Without sharing 77.5 :I: 0.7 62.9 :I: 0.8 75.6 :I: 0.5 63.6 :I: 0.8

 

Method Tissue type

 

Embryo Digestive

 

Inclusion Exclusion Inclusion Exclusion

 

Boosting (Nature 2010) 53.9 :I: 0.9 69.6 :I: 0.4 63.9 21:12 63.8 :I:1.1
Nearest neighbors 52.7 :I: 2.1 59.0 :I: 0.9 55.3 21:10 56.4 21:10
Regression, early stop* 54.8 :I: 1.6 68.8 :I: 0.7 64.9 :I: 0.6 64.5 :I: 1.3
Regression, PCA" 55.0 :I: 1.4 70.3 :I: 1.1 64.0 :I: 0.9 64.5 :I: 0.9
Regression, PCA pursuit 55.5 :I: 2.8 70.3 :I:1.1 63.3 :I: 0.6 63.9 :I: 0.9
SVM, PCA" 56.5 :I: 1.5 69.8 :I:1.0 63.6 :I:1.0 64.1 :I:1.1
Naive Bayes 53.4 :I:1.0 68.6 :I: 0.9 64.7 :I:1.1 65.0 :I: 0.6
Neural net, early stop" 54.3 :I: 1.1 69.0 :I: 1.7 64.3 :I: 0.5 63.6 :I: 1.6
Avg predictions from>i< 55.7 :I: 1.2 70.7 :I: 1.2 65.6 :I: 0.6 65.2 :I: 1.1
Bayesian method 56.8 :I: 0.6 73.0 :I: 1.1 68.5 :I: 0.5 68.2 :I: 0.7

Without sharing 55.3 :I: 1.0 71.3 :I: 1.0 66.8 :I: 0.6 66.0 :I: 0.8

 

:l: indicates 1 SD; top performances are shown in bold; * denotes methods that were
bagged together (predictions averaged).

corresponding to identifying exons exhibiting increased inclusion or
exclusion. For each tissue type, ambiguous exons with 0.1 < q < 0.9
were removed from the analysis. This screening retained on average
91, 81, 81 and 72% of the exons for classiﬁcation in CNS, muscle,
embryonic and digestive tissues. Then, for each tissue type, aim
and as"C were thresholded at 0.5 to deﬁne positive and negative
examples. For the Bayesian method, we found that predictions
for increased inclusion in CNS and muscle tissues and increased
exclusion in embryonic tissues are the most accurate, whereas
predictions for the reversed effect in those tissues are signiﬁcantly
less accurate (ROC curves are plotted in Supplementary Fig. S6).

Table 2 summarizes the classiﬁcation accuracies for all methods in
terms of the area under the ROC curve. The Bayesian method is the
only consistent top performer. As before, when hidden variables are
not shared across tissues, performance drops. Classiﬁcation results
are mostly consistent with the code quality results in Table 1; a
notable exception is Naive Bayes, whose extreme probabilities give
poor code quality but reasonable classiﬁcation results.

The relative improvement in classiﬁcation error of the Bayesian
method over the original boosting method (Barash et al., 2010a)

ranges from 22% for exon inclusion in muscle tissues to 7% for
exon exclusion in CNS tissues. These improvements correspond to
correctly classifying an additional 187 and 92 exons. Interestingly,
these are larger sets of exons than examined in most studies of
splicing regulation, cf. (Zhang et al., 2010).

Differences in performance for different tissues and regulatory
effects may suggest different types of regulatory mechanisms.
Increased inclusion was easiest to predict in CNS and muscle tissues,
whereas increased exclusion was easiest to predict in embryonic
tissues. All methods performed comparably well on predicting
increased exclusion in muscle tissues.

Analysis of hidden variable connectivity: The signiﬁcant
improvement of the Bayesian neural network over other methods
along with the fact that it explicitly selects features and hidden
variables, serves as a strong incentive to probe into the inferred
model structure and how various features are used to predict splicing
regulation. An advantage of the Bayesian approach is that it does
not place all bets on one model, but instead provides a distribution
over models so that hypotheses about selected features and model
structures can be tested statistically. In the analyses reported below,
the distribution over models was approximated using an ensemble
of 60 000 models obtained from 2000 samples taken from each
Gibbs sampling run for each of the 30 different training sets (5-fold
cross-validation using six different random data partitions).

First, we examined how frequently each hidden variable was used
to make predictions for each tissue type. Figure 3a indicates whether
or not each hidden variable (row) was connected to the predictor
for each tissue (color coded) after each iteration of Gibbs sampling
(column), for one of the training sets. During the ﬁrst ~ 50 iterations,
hidden variables are infrequently used for predictions, because
beneﬁcial parameter values and feature combinations have not yet
been learnt. Later, hidden variables are more frequently used and
their connectivity becomes more stable. However, the plot supports
the arguments made above that the Bayesian method beneﬁts from
broad exploration of connectivity and parameter settings. Often,
connections are made brieﬂy before being discarded. In other cases,
the connection between a hidden variable and tissue is more stable
and lasts for hundreds of Markov chain Monte Carlo (MCMC)
iterations. In some of those cases, other tissues attempt to beneﬁt
by using the stable hidden variable. For example, from samples 260
to 500, hidden variable 4 is primarily used to predict splicing in CNS
tissues, but is sometimes also used to predict splicing in muscle and
digestive tissues, and to a lesser degree in embryonic tissues.

Figure 3b plots the distribution of the inferred number of hidden
variables for each tissue, along with the prior distribution. All tissue
types use fewer hidden variables than expected under the prior, with
the exception of CNS, which uses more. Two possible explanations
are that the regulation of splicing in CNS tissues is more complex
than in other tissues, and that the dataset (features and/or splicing
patterns) is biased toward having more information about splicing
regulation in CNS tissues.

We next asked how frequently hidden variables were connected
to the output variables for multiple tissues. Figure 3c plots the
distribution of the inferred number of hidden variables that are used
to make predictions for different numbers of tissues, along with the
prior distribution. The number of hidden variables connected to a
single tissue matches the prior quite closely. The number of hidden
variables connected to two or three tissues is lower than expected

 

2559

112 /3.Io's[BrunoIpJOJXO'soiwuiJOJuioiq”:duq 111011 popeo1umoq

9103 ‘0g isnﬁnv uo ::

H. KXiong et al.

 

(a)

Hidden variables

sﬁsaasaasas:awmsmmswme

125

 

(b) 1
8
TE:- CNS Muscle Embryo
a 4
2 1o 10 10
OJ
8 I | h
2 -I _ I._ I.,
0o 10 20 0o 10 20 0o 10 20

Number of hidden variables used

  
 
  

500

-1 tissue

- 2 tissues
- 3 tissues
- 4 tissues

0
o 10 20 0 5 10 15 20
Number of hidden variables used

Fig. 3. (3) Use of different hidden variables (rows) by different tissues (colors), after each iteration of Gibbs sampling (columns). Blue, central nervous

system (CNS) tissues; red, muscle tissues; green: embryonic tissues; black: digestive tissues. (b) Distribution of the number of hidden variables used to make
predictions for different tissue types, a priori (solid curves) and a posteriori (bars). (c) Distribution of the number of hidden variables used to make predictions

for different numbers of tissues, a priori (solid curves) and a posteriori (bars).

under the prior, but substantial nonetheless. For example, in over ~
87% of the models, at least three hidden variables were connected to
two tissues. Interestingly, the number of hidden variables connected
to four tissues is substantially higher than expected under the prior.

Analysis of selected features and their connectivity: It was shown
above that the number of hidden variables connected to all tissue
types is signiﬁcantly higher than expected under the prior. Upon
examination, we found that those hidden variables tend to be
connected to features measuring conservation levels in the up-
and downstream introns, the presence of secondary structures, the
strength of splice site junctions, exon length and the introduction of
premature termination codons upon exon inclusion or exclusion.
We explored features that were selected by the Bayesian method
and compared them to previously published results. The 10 most
commonly used features were the same as those reported in Barash
et al. (2010a), but the 50 most commonly used features had a
lower overlap of ~70%. These features include measurements of
junction score, exon length and conservation, along with potential
binding sites of regulators such as Fox, (n)PTB and Cugbp. The
Bayesian method selected an overlapping but somewhat different
set of motifs from the collection derived using conservation analysis
(Yeo et al., 2007). Slightly fewer previously deﬁned motifs were
used, especially those for the neural-speciﬁc regulators Nova1/2

(Licatalosi et al., 2008). In contrast to previous work (Barash et al.,
2010a), the Bayesian method was able to achieve signiﬁcantly
higher prediction accuracy by using alternative deﬁnitions of motifs
and detecting a larger number of combinations of simpler and less
speciﬁc motifs through the use of hidden variables.

To demonstrate that the Bayesian method beneﬁts from less
frequently selected features, we identiﬁed the 40 most frequently
selected features and discarded the remaining features. We re-
applied the Bayesian learning procedure to one of the six training
sets using the reduced feature set and found that the test code quality
decreased by 16%.

We next asked whether we could identify relationships between
features, which may correspond to ﬁinctional modules in the
regulation of alternative splicing. Hidden variables are not
identiﬁable and indeed the MCMC procedure often deactivates
previously useﬁil hidden variables and activates new ones (Fig. 3a),
so methods for analyzing static model structures are inappropriate.
Instead, we examined the frequency with which pairs of most
frequently used features were co-wired to the same hidden unit,
the identity of which could vary across the 600 000 models in
the ensemble. To avoid the problem of feature degeneracy, we
labeled each feature using manually deﬁned feature categories, such
as ‘Fox motif in the upstream intron’, which includes alternative
deﬁnitions of Fox motifs located anywhere in the upstream intron.

 

2560

112 /3.IO'S[BumOIpJOJXO'SOIIBLUJOJIIIOICV/Idllq 111011 popeo1umoq

9103 ‘0g isnﬁnv uo ::

Bayesian prediction of splicing

 

   
    
 
 

uucnUU n,
FAICIJGIH
ni2|

I1!

I
(HIAJWH 2:

in

   
 
  
 
       
 
    

In
.nUUAG. I Ij
UGUI::U.I2_
Il'
.I2I
II-

II
I2l

nuuunii n|

Jumth '

Fig. 4. Correlation of usage for pairs of frequently selected feature
categories. White, insigniﬁcant correlation; dark grey, high-signiﬁcance
correlation (p < 10““), Fisher exact test).

We then computed the statistical signiﬁcance of the overlap between
pairs of feature categories. Figure 4 shows the resulting symmetric
matrix of —log10P-values, with rows and columns rearranged using
clustering. The highly related features in the lower right part of the
matrix primarily correspond to features that are used by frequently
connected hidden units (e.g. hidden variables 1 and 10 in Fig. 3a).
These include transcript structure features such as exon length, plus
motifs corresponding to commonly active regulators, such as Fox,
(n)PTB and Cugbp. In the upper right part of the matrix, there are
less commonly used features, which are sometimes co-wired via
frequently connected hidden variables, but which also sometimes
form separate modules. The presence of an exonic splicing enhancer
(ESE) in the alternative exon is co-wired via frequently connected
hidden variables, but is also co-wired jointly with Quaking-like
(le) binding sites in the downstream intron and Nova binding
sites in the alternative exon. Other novel relations include the
conservation-based motif cluster UUUAAC and the strength of the
junction between the alternative exon and the ﬂanking intron, and
GU-rich motifs in the upstream intron and le motifs.

Scope for improvements from larger datasets: To direct future
research, it is useful to predict whether or not additional gains in
performance can be achieved through the use of larger datasets.
While it is generally true that increasing the amount of training
data can only improve test performance, returns will diminish as
performance closes in on the maximum achievable level. To explore
how sensitive the achieved test code quality is to the amount of
training data, we trained the Bayesian method using seven differently
sized training sets that were obtained by subsarnpling the original
training data. In Figure 5, we plot test code quality against the
training set size (log-scale). For all tissues, there is no evidence that

 

 

 

 

300
‘- ‘ --- CNS
------ -- Muscle
250 .
— Embryo I ’
- - - Digestive 
200 ,r
3? 3”
a ,I
g 150 _,
a) ‘l
“g .
0 100

    

 

 

100 200 400 800 1600 3200
Number of training exons

Fig. 5. The effect of the number of training cases on test code quality.

the method is near to a maximum code quality, suggesting that larger
datasets can be used to achieve signiﬁcantly higher code qualities.

5 CONCLUSIONS

Deriving a splicing code that uses combinations of RNA features to
predict how splicing will occur under different cellular contexts is
critical to understanding gene regulation. We introduced a novel
Bayesian method that uses hidden variables within a network
architecture to model non-linear relationships between putative
regulatory features and splicing changes. We compiled a freely
available benchmarking dataset along with a methodology for
evaluating different techniques.

Our method achieved relative improvements of 52% in test code
quality and up to 22% in test classiﬁcation accuracy, compared
with the state of the art (Barash et al., 2010a). It correctly
classiﬁed hundreds of additional tissue-dependent splicing changes,
and outperformed all other methods that we tested. Even when
predictions from four of the best other methods were combined, the
Bayesian technique performed signiﬁcantly better. Using a sample of
models from the posterior distribution, the importance of individual
RNA features and their pairwise combinations were assessed. These
feature combinations were mostly consistent with the previous
results, and also included novel predictions.

A promising ﬁiture direction is to proﬁle more cassette exons,
since we found that this will likely lead to signiﬁcant improvements.
It would also be useful to account for other kinds of alternative
splicing such as alternative splice sites and mutually exclusive
exons. Ultimately, we would like to be able to predict the relative
abundances of entire transcripts.

Our methodology and the Bayesian technique can be applied to
datasets proﬁling larger numbers of tissues, different species, and
different types of alternative splicing. When we applied the Bayesian
technique to splicing patterns derived from RNASeq data for 16
human tissues, along with the RNA feature deﬁnitions that we used
for mouse, classiﬁcation rates ranging from 67% in thyroid gland to
83% in whole brain were obtained.

A multispecies splicing code with shared and species-speciﬁc
regulatory subprograms can be inferred using matched tissue data

 

2561

112 /8.IO'S[BHmO[pJOJXO'SOIJBuIJOJHIOIq//Idllq urori popeo1umoq

9IOZ ‘OE ISUEHV Ho ::

H. KXiong et al.

 

and by feeding the feature vectors and target splicing patterns for
all species into the learning algorithm.

It was found that using large numbers of features improved code
quality, pointing to the importance of further exploring new feature
types, such as those derived using in vivo (Licatalosi et al., 2008)
or in vitro (Ray et al., 2009) RNA binding data, DNA, chromatin
structure and histone modiﬁcations (Luco et al., 2011).

We believe that the method developed in this study, along with
the accompanying benchmark dataset, will help push the envelope of
our ability to predict splicing outcomes, with possible applications
ranging from analyzing transcripts of genes with low expression to
disease-speciﬁc mutation analysis.

ACKNOWLEDGEMENT

We thank Ben Blencowe, Geoffrey Hinton, Yann LeCun and Radford
Neal for discussions.

Funding: Canadian Institutes for Health Research Operating Grant
MOP-106690 (to B.J.F.); Genome Canada and Ontario Genomics
Institute Grants (to B.J.F.); Natural Sciences and Engineering
Research Council (NSERC) Grant SMFSU 379968-09 (to B.J.F.);
Canadian Foundation for Innovation and Ontario Research Fund
Grant 203788 (to B.J.F.). B.J.F. is a Fellow of the Canadian Institute
for Advanced Research and an NSERC E.W.R. Steacie Fellow.

Conﬂict of interest: None declared.

REFERENCES

Barash,Y. et al. (2010a) Deciphering the splicing code. Nature, 465, 53759.
Barash,Y. et al. (2010b) Model-based detection of alternative splicing signals.
Bioinformatics, 26, i325.

Bishop,C.M. (2006) Pattern Recognition and Machine Learning. Springer, NY.

Blencowe,B. (2006) Alternative splicing: new insights from global analyses. Cell, 126,
3747.

Chan,R. and Black,D. (1997) The polypyrimidine tract binding protein binds upstream
of neural cell-speciﬁc c-src exon n1 to repress the splicing of the intron downstream.
Mol. Cell. Biol., 17, 4667.

Fagnani,M. et al. (2007) Functional coordination of alternative splicing in the
mammalian central nervous system. Genome Biol., 8, R108.

Hartmann,B. and Valcarce1,J. (2009) Decrypting the genome’s alternative messages.
Curr Opin. Cell Biol., 21, 3777386.

Ishwaran,H. and Rao,]. (2005) Spike and slab gene selection for multigroup microarray
data. J. Am. Stat. Assoc, 100, 7644780.

Licatalosi,D. et al. (2008) Hits-clip yields genome-wide insights into brain alternative
RNA processing. Nature, 456, 464469.

Lim,L.P. and Sharp,P.A. (1998) Alternative splicing of the ﬁbronectin EIIIB exon
depends on speciﬁc TGCATG repeats. Mol. Cell. Biol., 18, 390073906.

MacKay,D. (1992) A practical Bayesian framework for backpropagation networks.
Neural Comput, 4, 448472.

Neal,R. (1996) Bayesian Learning for Neural Networks, Vol. 118. Springer, NY.

Pan,Q. et al. (2008) Deep surveying of alternative splicing complexity in the human
transcriptome by high-throughput sequencing. Nat Genet, 40, 141371415.

Ray,D. et al. (2009) Rapid and systematic analysis of the RNA recognition speciﬁcities
of RNA-binding proteins. Nat Biotechnol., 27, 6677670.

Rumelhart,D. et al. (1986) Learning representations by back-propagating errors. Nature,
323, 5337536.

Scholkopf,B. and Smola,A. (2002) Learning With Kernels: Support Vector Machines,
Regularization, Optimization and Beyond. MIT Press, MA.

Wang,Z. and Burge,C. (2008) Splicing regulation: From a parts list of regulatory
elements to an integrated splicing code. RNA, 14, 802.

Wang,G. and Cooper,T. (2007) Splicing in disease: disruption of the splicing code and
the decoding machinery. Nat Rev. Genet, 8, 7497761.

Wang,E. et al. (2008) Alternative isoform regulation in human tissue transcriptomes.
Nature, 456, 470476.

Yeo,G et al. (2007) Discovery and analysis of evolutionarily conserved intronic splicing
regulatory elements. PLoS Genet, 3, e85.

Zhang,C. et al. (2010) Integrative modeling deﬁnes the nova splicing-regulatory
network and its combinatorial controls. Science, 329, 439.

Luco,R.F. et al. (2011) Epigenetics in alternative pre-mRNA splicing. Cell, 144, 16.

 

2562

112 /3.Io's[BrunoprOJXO'soneuiJOJuioiqp:duq 111011 popeo1umoq

9103 ‘0g isnﬁnv uo ::

