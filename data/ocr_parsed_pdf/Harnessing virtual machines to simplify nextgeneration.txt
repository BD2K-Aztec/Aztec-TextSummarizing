REVIEW

Vol. 29 no. 17 2013, pages 2075-2083
doi:1 0. 1093/bioinfonnatics/btt352

 

Sequence analysis

Advance Access publication June 20, 2013

Harnessing virtual machines to simplify next-generation DNA

sequencing analysis

Julie Nocql'z'l, Magalie Celtonl'z'e'l, Patrick Gendron‘, Sebastien Lemieuxl'4 and

Brian T. Wilhelm1'2'*

1Institute for Research in Immunology and Cancer, University of Montreal, 2Laboratory for High-Throughput Genomics,
Department of Medicine, University of Montreal, QC H3T 1J4, Canada, 3INRA, UMR1083, Sciences pour I'Oenologie,
Montpellier, France and 4Laboratory for Functional and Structural Bioinformatics, Computer Sciences and Operation

Research, University of Montreal, QC H3T 1J4, Canada
Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: The growth of next-generation sequencing (NGS) has not
only dramatically accelerated the pace of research in the field of gen-
omics, but it has also opened the door to personalized medicine and
diagnostics. The resulting flood of data has led to the rapid develop-
ment of large numbers of bioinformatic tools for data analysis, creating
a challenging situation for researchers when choosing and conﬁguring
a variety of software for their analysis, and for other researchers trying
to replicate their analysis. As NGS technology continues to expand
from the research environment into clinical laboratories, the challenges
associated with data analysis have the potential to slow the adoption
of this technology.

Results: Here we discuss the potential of virtual machines (VMs) to be
used as a method for sharing entire installations of NGS software
(bioinformatic ‘pipelines’). VMs are created by programs designed to
allow multiple operating systems to co-exist on a single physical
machine, and they can be made following the object-oriented para-
digm of encapsulating data and methods together. This allows NGS
data to be distributed within a VM, along with the pre-configured soft-
ware for its analysis. Although VMs have historically suffered from poor
performance relative to native operating systems, we present bench-
marking results demonstrating that this reduced performance can now
be minimized. We further discuss the many potential benefits of VMs
as a solution for NGS analysis and describe several published
examples. Lastly, we consider the benefits of VMs in facilitating the
introduction of NGS technology into the clinical environment.
Contact: brian.wilhelm@umontreal.ca

Received on March 13, 2013; revised on June 4, 2013; accepted on
June 14, 2013

1 INTRODUCTION

1.1 Next-generation sequencing

In recent years, novel approaches in the development of
massively parallel DNA sequencing chemistry technologies
have resulted in a dramatic increase in the amount of DNA se-
quence data that is being produced (Mardis, 2011). As a result,

 

*To whom correspondence should be addressed.
7‘The authors wish it to be known that, in their opinion, the ﬁrst two
authors should be regarded as joint First Authors.

these novel sequencing machines, often called second-generation
or next-generation sequencing (NGS) technologies, have simul-
taneously reduced the cost of sequencing a human genome by a
factor of >1 million, while reducing the time required from years
to a single day. Moreover, this process of improvement is con-
tinual: the amount of sequencing data obtained from an Illumina
DNA sequencer, for instance, has quintupled in 2009 alone
(Langmead et al., 2009), and to date, this trend shows no signs
of reaching a plateau. In addition, the cost of sequencing hard-
ware has also decreased, resulting in an extremely rapid adoption
of this (Stein, 2010).

Although the scientiﬁc value of this immense and ongoing ﬂood
of data has been profound, it has created dramatic new bioinfor-
matic challenges in managing and analyzing the resulting data.
The ‘mapping’ of short DNA sequences (‘sequence reads’) to a
reference genome, a frequent ﬁrst step of analysis, typiﬁes the
problem. Although traditional DNA mapping software, such as
BLAST (Altschul et al., 1990) and BLAT (Kent, 2002), are highly
accurate, they operate at speeds that are orders of magnitude
slower than the rate at which NGS sequence data can be generated
(Li et al., 2008a, b). This large (and continually growing) asym-
metry between data generation and analysis capacity has forced a
rapid evolution in the algorithms for read mapping. A recent ex-
ample is provided by the Spliced Transcripts Alignment to a
Reference (Dobin et al., 2012) short read-mapper, which can pro-
cess >550 million 76 bp reads/h, on a 12 core server. The require-
ment to rapidly adapt or develop bioinformatic tools suitable for
the volume and nature of NGS data being generated by large-scale
genomics projects worldwide is not limited to read mapping, but
also implicates every step of the downstream analysis. An enor-
mous collection of tools (both commercial and open-source) for
NGS analysis are currently available, a small subset of which are
shown in Table 1, grouped by analysis task.

Even as the bioinformatics community has clearly responded
to the need to develop new tools for NGS analysis, the diversity
of available tools has also paradoxically created another level of
complexity, hindering the potential impact of this technology. In
a typical NGS analysis, for instance, 578 separate bioinformatic
tools might be required to map reads, call sequence variants,
analyze splicing patterns, assess differential expression and per-
form gene set enrichment analysis [e. g. DAVID (Da Wei Huang
and Lempicki, 2008)]. Ideally, these tools would be installed and

 

© The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e—mail: journals.permissions@oup.com 2075

112 /310'sreu1nofp103x0"sorJBurJOJurorq/ﬁduq 11101} papeolumoq

9103 ‘Og isnﬁnV uo ::

J.Nocq et al.

 

Table 1. Selection of various NGS programs and analysis packages

 

 

Tools Reference
Read mapping BFAST (Homer et al., 2009)
MAQ (Li et al., 2008a, b)
BWA (Li and Durbin, 2009)
Bowtie (Langmead et al., 2009)
SHRiMP (Rumble et al., 2009)
ELAND2 (Cret et al., 2009)
SOAP2 (Li et al., 2009)
SSAHA2 (Ning et al., 2001)
Read assembly Velvet (Zerbino and Birney, 2008)
SSAKE (Warren et al., 2007)
ABySS (Simpson et al., 2009)
Trans-ABySS (Robertson et al., 2010)
AllPaths (Butler et al., 2008)
Variant calling SamTools (Li, 2011)
VarScan (Koboldt et al., 2009)
SNVMix (Goya et al., 2010)
VarSifter (Teer et al., 2012)
SomaticSniper (Larson et al., 2012)
Copy number CNV-seq (Xie and Tammi, 2009)
analysis SegSeq (Chiang et al., 2008)
CNVnator (Abyzov et al., 2011)
SeqCBS (Shen and Zhang, 2012)
ExomeCNV (Sathirapongsasuti et al., 2011)
Differential DEseq (Anders and Huber, 2010)
gene expression DEGseq (Wang et al., 2010)
Cufﬂinks (Trapnell et al., 2012)
edger (Robinson et al., 2010)
baySeq (Hardcastle and Kelly, 2010)
NOISeq (Tarazona et al., 2011)
Alternative TopHat (Trapnell et al., 2009)
splicing AltAnalyze (Salomonis et al., 2009)
MATS (Shen and Zhang, 2012)
ChIP peak ﬁnding PeakSeq (Rozowsky et al., 2009)
MACS (Zhang et al., 2008)
FindPeaks (Fejes et al., 2008)
Visualization tools SAMSCOPE (Popendorf and Sakakibara, 2012)
IGV (Robinson et al., 2011)
UCSC Browser (Karolchik et al., 2009)
Artemis (Rutherford et al., 2000)
Genoview (Laczik et al., 2012)

 

configured in such a way that the raw sequence data could be
pushed through each of the tools sequentially (in a bioinformatic
‘pipeline’) in a largely automated way, to generate biologically
meaningful results. Creating such a pipeline ﬁrst requires end
users to download a number of individual programs, in addition
to dependencies, such as gene annotation files, while ensuring
that the format of the annotations is compatible with all of the
tools. Using multiple NGS analysis programs also presents other
challenges: despite many tools having a standard form of input,
in many cases the resulting output from one program cannot be
directly used in a subsequent tool without additional (often
minor) reformatting to ensure compatibility. Furthermore,
most NGS programs have a wide variety of speciﬁc parameters
that need to be set, all of which may produce subtle or gross

differences in the output. Although default values are generally
included, end users may not often understand the precise impact
of changing these values for their analysis without investing sub-
stantial time to perform systematic testing. A final complication
in this scenario arises from the constant and asynchronous de-
velopment and release of individual software tools, which can
lead to losses of compatibility and breakdowns in established
analysis pipelines when software is updated. The end result of
this situation is that the process of establishing and maintaining a
bioinformatic pipeline is neither trivial nor does it foster repro-
ducibility of analysis results, given the wide variety of tools and
parameters involved (Nekrutenko and Taylor, 2012).

The impact of the complexity of NGS bioinformatic analysis is
not limited to difficulties in extracting biological meaning from
experiments, but it may also directly impede the adoption of
NGS technology (e. g. within clinical settings where it has already
been demonstrated to have enormous utility). Indeed, novel
DNA sequencing technologies have empowered the concept of
‘personalized medicine’, whereby medical treatment is tailored to
differences in individual patient genomes (Schweiger et al., 2011).
Despite the success of NGS-based studies of cancer (Pfeifer and
Hainaut, 2011) and rare diseases (Ng et al., 2010), the use of
sequencing-based diagnostics is still lagging far behind its use
in research. Part of the reason for this is certainly the validation
of such techniques, but another signiﬁcant factor is likely to be
the limited bioinformatics resources available within hospitals to
perform sequence analysis. Although general IT resources are
likely to be available in major hospitals and clinics, the specia-
lized knowledge to install and optimize NGS bioinformatics
software, and to conﬁgure these for a variety of clinical uses
(e.g. cancer genomics, bacterial meta-genomics), is unlikely to
exist. As a result of this lack of resources, the potential beneﬁts
of NGS technology will undoubtedly take longer to arrive as a
standard of practice for medical diagnostics.

In light of the problems highlighted above, there are several
basic requirements that all bioinformatics tools developed for
NGS analysis should ideally fulﬁll to offer the greatest beneﬁt
to the biomedical community:

(1) Compatibility: Within the domain of NGS analysis, the
data generated are principally images and DNA sequences
derived from these images, although in practice the pri-
mary images are no longer retained. The past $6 years
have seen the emergence of defacto standards for sequence
ﬁles (FASTQ) (Li et al., 2008), sequence alignments
[sequence alignment/map (SAM) or binary alignment/
map (BAM) ﬁles] (Li et al., 2009) and sequence variants
(variant call formats). Despite this ‘common currency’ for
information exchange, not all programs accept every one
of these formats, and some trivial differences [such as
slight variations in quality scores in FASTQ files, lack of
standard file extension (.fq versus .fastq), output format,
etc] can impact compatibility between programs.

(2) Facility to reproduce analysis: At present, a majority of
NGS tools was designed to be used on a command line
in a Unix environment. The effort involved in exactly
recreating a speciﬁc configuration for a server with iden-
tical software and annotation used for a published analysis

 

2076

112 /310'sleu1nofp103x0"sorJBurJOJurorq/ﬁduq 11101} papeolumoq

9103 ‘Og isnﬁnV uo ::

Virtual machines for NGS analysis

 

Table 2. Selection of integrated packages for NGS analysis

 

 

Tools Reference URL Open-source? Clinically Read Cost model
certified? mapping?
Avadis NGS Inc, San Francisco, www.avadis-ngs.com No No Yes Annual licence
CA, USA
Genespring Agilent Technologies genespring-support.com No No No Annual licence
Goldenhelix Inc. Bozeman, MT, USA www.goldenhelix.com No No No Annual licence
Integrated Galaxy (Goecks et al., 2010) https://main.g2.bx.psu.edu/root Yes No Yes Free; storage limit
packages CLC Genomics Aarhus, Danemark www.clcbio.com No No Yes Annual licence
Genomatix Genomatix Software www.genomatix.de/ No No Separate Annual licence
GmbH
NextGENe SoftGenetics, State www.softgenetics.com/ No No Yes Annual licence
College, PA NextGENe.html

 

represents a signiﬁcant barrier for direct comparisons be-
tween experimental datasets.

(3) Open-source tools: There is a broad understanding that
publicly funded academic software should be freely avail-
able to the entire research community, and the distribution
of the source code obviously has numerous advantages.
The distribution of open-source software not only allows
other researchers the opportunity to critically evaluate the
underlying algorithm, but examination of the method-
ology used can also serve as a mechanism to stimulate
research into novel approaches to improve performance.

1.2 Centralized resources and cloud computing

To address these concerns, a practical method should be de-
veloped to distribute NGS tools that ensure that all tools work
compatibly and allow a comprehensive analysis. An excellent ex-
ample of such a package is Genome Analysis Tool Kit (GATK)
(McKenna et al., 2010) developed by the Broad Institute that,
excluding initial read mapping, allows users to perform an inte-
grated analysis using a single application. Individual tools within
GATK (called ‘walkers’) must still be run individually at the com-
mand line, however, using all appropriate parameters. Other com-
mercial software packages that allow for an integrated analysis are
also available (Tables 1 and 2); however, the license fees required
and proprietary/closed source code can be disincentives for their
use by some laboratories. Simultaneously, the development of
centralized resources, in contrast to tools meant primarily to be
downloaded by end users, have also been developed. One popular
example of such a resource is Galaxy, an open-source web-based
platform developed by Penn State and Emory University for data
analysis and manipulation (Goecks et al., 2010). Galaxy was de-
veloped in 2005, and the published description of the system 5
years later notes that the public web server processes ~5000
jobs/day, clearly demonstrating the demand for tools for large-
scale integrative analysis. Within Galaxy, multiple steps in an
analysis method can be saved and shared as a ‘workﬁow’, enabling
others to precisely replicate the same process used by another
group. Despite such useful features, Galaxy still requires users
to either upload their data to the public Web site (presenting prac-
tical limitations when working with large amounts of raw

sequence data) or to download the Galaxy software and conﬁg-
ure it to run locally. Because the Galaxy software is currently not
distributed containing a suite of bioinformatic software, a local
installation presents similar issues for configuration of all of the
individual software tools as discussed above. Although the
Galaxy interface can be customized to include whatever tools
are required, it is not clear that the system was designed to meet
Clinical Laboratory Improvement Amendments/Health
Insurance Portability and Accountability Act certiﬁcation re-
quirements for use within clinical settings (although it is possible
that it could be modiﬁed in such a way as to do this). Therefore,
despite its utility and popularity, Galaxy does not at the moment
represent a simple, hands-off and pre-configured solution for
NGS analysis for all potential users.

Because of the signiﬁcant computational resources required for
NGS analysis, there has also been growing interest in the use of
cloud computing to address current bioinformatic bottlenecks.
The dramatic growth in remote computing services by such com-
panies as Amazon (Amazon EC2), Microsoft (Windows Azure),
Rackspace (Rackspace cloud) and Google (Google Cloud plat-
form) now means that researchers have the option of simply
paying for their computing requirements on an ad hoc basis,
rather than building and maintaining their own physical comput-
ing infrastructure. For many small laboratories, this represents a
highly cost-effective solution, especially when their demands for
such hardware are highly periodic. These same cloud resources
can be used for large-scale data storage and sharing, which is also
an attractive option for small to mid-sized laboratories that do not
wish to invest in computer hardware. The costs associated with the
use of cloud computing for storage and analysis are low enough
that it would not represent a barrier to their use, although the
research community will likely still prefer ‘permanent’ repositories
for data (e. g. Gene Expression Omnibus (GEO)/SRA at NCBI,
Arrayexpress at EBI) for long-term data storage. Although cloud
computing does offer many advantages for NGS analysis, there
have been sporadic large-scale failures of cloud resources (e.g.
Amazon EC2) in the past that have left popular hosted web
services unavailable. In the context of highly critical, rapid turn-
around diagnostic sequencing, the potential risk of such out-
ages for cloud-based clinical NGS analysis might be considered
too high.

 

2077

112 /310's112u1n0fp101x0"sorJBurJOJHrorq/ﬁduq 111011 pap1201um0q

9103 ‘0g isnﬁnV uo ::

J.Nocq et al.

 

 

Host OS (e.g. Windows)

Program Program

Program Program

Program Program

     

 

 

Fig. 1. Schematic View of a VM. A typical system VM and the guest
operating system (guest OS) will exist in the form of a number of ﬁles on
the hard drive of the host operating system (host OS). The guest OS is
accessed through software that runs through the hypervisor or virtual
machine manager (VMM), which interprets instructions from the guest
OS to execute these on the underlying hardware. Data and programs of
the host and guest OS generally exist in strong isolation even on the same
hardware, allowing large numbers of operating systems to co-exist
independently

Hardware layer

2 VIRTUAL MACHINES

Another potential solution to the problems with NGS data
analysis has arisen through an unrelated advance in computer
software, namely, the development of Virtual machines (VMs).
The basic principle of a VM is the use of software to emulate the
architecture, behavior or functionality of another computer
system. As discussed below, because VMs can be created and
distributed already containing the software that will run within
the VM, they can be used to make collections of NGS tools
available as a single downloadable unit.

2.1 Development of VMs

The implementation of the concept of a VM has evolved from
pioneering work carried out by International Business Machine
(IBM) in the 19605 and 705 (Creasy, 1981), with their develop-
ment of VM operating systems for mainframe computers, to the
enormous range of systems currently available. Although the
term ‘Virtual machine’ is used to describe a wide variety of sys-
tems, they can be broadly classiﬁed into two categories, based on
the level for which they provide abstraction (Smith and Nair,
2005). For the purpose of encapsulating analysis pipelines, we
will focus on system VM where, using a Virtual machine manager
(VMM), an entire hardware platform is emulated. The VM thus
encompasses software tools, libraries and operating systems and,
when active, reserves both physical disk space and RAM on the
underlying hardware (Fig. 1).

Such VMs enable what are termed ‘guest operating systems’
(guest OS) to simultaneously operate on the same hardware as a
native OS, but in strong isolation. Because an active VM is seen
as a simple process to the host OS, it can readily be interrupted
and its exact state saved to a ﬁle. This ‘snapshot’ can then be
archived and reactivated at any time, including after being trans-
ferred to a different hardware platform. A number of different
VMMs have been developed over the past decade (Table 3).
Because of their capacity to abstract and share underlying

hardware, the development of VM has also largely underpinned
the growth of cloud computing services offered by companies
such as Amazon, Google, Microsoft and others. The growth of
VMs has also been aided through efforts of commodity proces-
sor manufacturers [e. g. Integrated Electronics (Intel), Advanced
Micro Devices (AMD)] to include hardware support for Virtual-
ization, in the form of new processor extensions to the x86 archi-
tecture. This has led to substantial improvements in both
performance and ﬂexibility of VMs. This growth in support,
along with the utility of VMs within a variety of commercial
domains, has created what is now a highly competitive market
for VMMs. Interestingly, there has been little adoption of this
software within the setting of academic and biomedical research,
despite its potential to address the problems discussed above.

2.2 VMs as a solution for NGS software distribution

The application of VMs for the distribution of tools for NGS
analysis has a number of potentially compelling advantages,
largely because it offers the possibility of following an object-
orientated paradigm of encapsulating data and methods to-
gether. Because VMs can recreate all aspects of an entire OS, it
is possible for such Virtual systems to be created already contain-
ing a variety of preconﬁgured bioinformatic software. The dis-
tribution of such a VM would then allow end users to circumvent
issues of ensuring compatibility between various versions of soft-
ware and compiling and conﬁguring such tools. Given that many
research centers with large NGS projects will likely have already
invested substantial resources in testing and optimizing software
choices and parameters for their analysis, a VM built to reﬂect
this testing would allow many other centers to beneﬁt from this
effort and avoid ‘reinventing the wheel’.

Indeed, this idea of a central repository of validated ﬁle ver-
sions is similar to the approach taken for the distribution of
various ﬂavors of Linux. At the same time, the distribution of
VMs containing bioinformatic pipelines, rather than individual
tools, would also enable researchers to more easily replicate pre-
cise analysis steps performed by other groups, helping to ensure
that conclusions drawn from such comparisons are not artifacts
of software parameters used for analysis. This is facilitated by the
fact that NGS-VMs exist as a collection of ﬁles, making it simple
to save a ‘snapshot’ of the NGSiVM used for a publication,
which can then be directly shared with other laboratories. An
illustration of the differences in current and potential VM-based
NGS software distribution is shown in Figure 2. For archiving,
repositories such as GEO and ArrayExpress, which already store
NGS data (which represents 99% of the space required for an
NG$VM), it should be straightforward to store the data within
a VM, allowing end users to download and rerun exactly the
same analysis as the original authors. Such transfers could also
be made directly to cloud compute resources to facilitate analysis
or reuse of the published methods with new data.

Although such ‘NG&VMs’ would not, by themselves, remove
issues of different NGS software choices between groups, it is
entirely likely that there would be coalescence around a few spe-
ciﬁc VMs developed by large institutes (e. g. genome centers), in
the same way as has occurred for individual NGS tools [e.g.
BWA (Li and Durbin, 2009) or Bowtie (Langmead et al.,
2009) for read mapping].

 

2078

112 /310'S[BHJDOIPJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 111011 pap1201um0q

9103 ‘0g isnﬁnV uo ::

Virtual machines for NGS analysis

 

Table 3. List of virtual machine software

 

System requirements Guest OS supported

 

         

 

 

 

‘ Windows 98

 

 

 

Ubuntu 5.10 /6.06 Desktop /7.04 - 12.04
Mac OS X Server (Leopard, Snow Leopard)

Mandriva 2009.0/2009.1

openSUSE 11.0-11.3

Mandrake 10.1

Fedora Core 1/4/5/6

RHEL6, Oracle Linux 6
A Red Hat Linux 9

Solaris 10 5.08 and later

Xandros 3/4

        

     

     

     

         

      

 

I Oracle Sola ris

 

 

 

 

X
A III
B a
"E ‘1
w a
E .5 E‘
E E A A 4? a
a m- a a a 4.:
E =1 § 5 4 4 a a
g .5 a 1: 3 3 Q h
E a '5 i: i: 13 8 a
2 ~— I E w w a o o. i—
ll 2 v 00 l\ > N X Z
3 < E .. m m m m m m
d: a: = In 3 3 3 3 3 3
o E O o o o o o o
In “5 ._ u 'o 'o '5 'o 'o 'o
E m .E 3 .E .E .E .E .E .E
> 0 E :I: 3 3 3 3 3 3
VirtualBox 1 NS W,L,O
Vmware NS* >1 GB W,L
Parallels Desktop 2, 4 15 GB 0
Windows Virtuach 2,>2 15GB W
Oracle VM Server 1,2 6 GB Not required
KVM NS* >1 GB L

 

.supported

Dnot supported

 

 

 

 

ﬁreported to be supported

 

Software downloads

cw All DEG
darernon my"; dulutliun

Read KNP Pe-ik
Mapper (allcr tinder

 

 

 

 

 

  
    

 

 

  

E and nan i

: mlhinVM :


 

Fig. 2. Comparison of NGS software distribution methods. Traditional
methodologies require individual laboratories to independently download
(black arrows) identical versions of software and then conﬁgure them
using equivalent parameters. Raw sequencing data are generally trans-
ferred from central repositories (e.g. GEO at NCBI), wheras annotation
data and parameters are obtained from the laboratory or publication.
Alternatively, an NG&VM used by the first laboratory (below black
dotted line) could be redistributed as a single unit, either directly or
through storage intermediates, that could include the annotation, precise
parameters and the data used

Even with this possibility, however, it would remain straight-
forward for end users to download such a VM and adjust any of
the parameters for the bioinformatic tools within the VM exactly
as they would for the tool in a native system. Given the relatively

small size of most NGS software, a slightly more sophisticated
approach for NG$VMs could be to include a wide variety of
software (e.g. several mappers and several variant callers) such
that multiple pipelines could be available to end users. Indeed,
given the fairly universal availability of high-speed Internet con-
nections, even the distribution of NG&VMs containing pre-
installed genomes and sequence databases could be a practical
solution. The development of scientiﬁc peer-to-peer networking
tools, such as BioTorrents (Langille and Eisen, 2010), could also
simplify the distribution of large NGSiVMs. With the addition
of a simple graphical user interface to direct the underlying bio-
informatic tools and access parameters for the programs, NG$
VMs could dramatically simplify the process of performing NGS
data analysis.

2.3 Published examples of bioinformatics VMs

The potential application of NG$VMs in research has already
started to be recognized with the development of several ex-
amples of systems to facilitate speciﬁc types of analyses. For
example, a recent publication described Cloud BioLinux, a pub-
licly accessible VM composed of >135 bioinformatics packages,
along with documentation, desktop interface and graphical soft-
ware applications (Krampis et al., 2012). This resource is publicly
available as a web service, hosted on the Amazon EC2 cloud,
providing high-performance infrastructure for bioinformatics
computing that can be configured according to user require-
ments. This tool presents a useful and scalable solution that
allows researchers to specify the required environment for their
data, in addition to the time required for analysis. Although
designed to run on compute farms, such as EC2, this is not a
requirement of the system and local private versions can also be
installed. Although this VM contains an extensive array of

 

2079

112 /310's112u1n0fp101x0'sopBHJJOJHrorq/ﬁduq 111011 pap1201um0q

9103 ‘0g isnﬁnV uo ::

J.Nocq et al.

 

bioinformatic tools, it is not speciﬁcally tailored to NGS ana-
lysis, but rather represents a sort of comprehensive bioinfor-
matics tool box. In contrast, other published VMs such as
CloVR (Angiuoli et al., 2011) use similar methodologies to
Cloud BioLinux, but in this case the VM has been specialized
to deal with a speciﬁc type of NGS analysis. Using either a
cloud-hosted VM or a local instantiation, the CloVR VM uses
a variety of software targeted toward metagenomics, microbial
genomics and small genome phylogenetics. The CloVR VM fea-
tures a dashboard function with color-coded pre-conﬁgured ana-
lysis pipelines that allow users to alter any parameters for the
analysis though graphical user interface widgets (e. g. drop-down
menus and radio buttons). Aside from downloading the required
free VM software (e. g. VirtualBox or VMware), users only need
to download the appropriate VM image file (.Vdi or .dek, re-
spectively) to get started. Another targeted NGSiVM program
suite for distributed de novo genome construction and motif ﬁnd-
ing has also recently been developed (Corwin et al.), and Corwin
and colleagues also demonstrate the applicability of their VM in
running either locally or on a cluster. The development of such
NG$VMs is highly encouraging; however, at the moment they
remain highly targeted for specific analysis types. Additionally,
although there are more examples of software developed and
distributed as VMs, such as XperimentR (Tomlinson et al.,
2013) and NG&SNP (Grant et al., 2011), a simple and compre-
hensive NG$VM pipeline, complete with scripts to automate
underlying tools that could address all of the issues we have
raised, has, to our knowledge, yet to be published.

3 VM PERFORMANCE

Although the use of a comprehensive NG&VM would remove a
number of critical bottlenecks in NGS analysis, VMs have

Table 4. Benchmarking of virtual systemsa

historically suffered from the inherent inefﬁciency caused by
the requirement of trapping and emulating privileged instruc-
tions within the host OS. This step is essential to allow com-
mands from the guest OS in the VM to be converted and
executed on the underlying hardware. Although signiﬁcant im-
provements have been made in hardware support for Virtualiza-
tion in the past few years, any VM will still be limited by the
underlying memory, central processing unit (CPU), input/output
(I/O) and disk space requirements, all of which must also be
shared with the host OS. Despite these constraints, the continu-
ing trend for rapidly decreasing costs for memory and disk space,
along with the modest requirements for most VM software,
means that this does not represent a real limitation.
Regardless, the systematic evaluation of the extent to which cur-
rent VMs perform relative to native hardware has not been well
explored, and so we examine this issue.

3.1 Performance of VMs as a barrier to practical use

To directly assess the impact of some of these constraints in a
setting relevant for NGS analysis, we performed a benchmarking
comparison using ﬁve different conﬁgurations on identical hard-
ware (Table 4). We first examined VM performance through
mapping and CPU intensive analysis of 8 million 100 bp RNA
sequence reads using TopHat (Trapnell et al., 2009), Casava and
SAMtools (Durbin, 2009) software. All software was run on
either on a native Linux OS or a guest Linux OS running on
either a Linux or Windows host OS using VMware or
VirtualBox. As expected, most VM conﬁgurations suffered a
performance penalty of ~25% on average, relative to the same
task run on the native Linux conﬁguration. Although such a
modest decrease in performance would likely be acceptable in
most academic settings, one of the conﬁgurations (V irtualBox
4.2.0 running Scientiﬁc Linux 6.3 on Windows 7 Enterprise

 

Linux VMWare/Linux VirtualBox/Linux VMWare/Windows VirtualBox/Windows Average Runtime Amazon Runtime

native

increase (%) EC2 m1.xlarge increase (%)

 

TopHat mapping 175 229 229 208
(2 threads) 266 313 355 282
Casava alignment 71 93 106 88
(2 threads) 142 175 205 164
Casava build (sort,bam) 18 23 28 21
samtools mpileup chrl 7.22 9.97 9.97 8.75

185 212.8 22 336 92
257 301.8 13 428 61
79 91.5 29 129 82
150 173.5 22 258 82
20 23.0 28 29 61
6.75 8.9 23 13.33 85

 

Synthetic I/O tests, identical hardwareb

 

Linux native VMWare/Linux VirtualBox/Linux VMWare/Windows VirtualBox/Windows Average Standard Speed

deviation decrease (%)

 

Bonnic++ 108 669 98 791 87 404 78 785
write buff 204 891 129 239 146 835 190 057
Readc 94 627 49 018 65 586 84 936
read buff. 229 532 93 002 115 556 236 038
seek/s 548 645.4 327 354

89 513 88 623.3 8214.9 —18.4
200 329 1666150 34031.2 —18.7
76 994 69133.5 15 585.5 —26.9
166686 152 820.5 63 467.3 —33.4
442 442.1 144.2 —19.3

 

Note: Hardware: 6 core Intel Xeon X5675@3.07GHz,12 MB cache,24 GB RAM ECC DDR3 1333 MHz,2x500 GB HD (6Gbps NL SATA), RAID 0. Software: Scientiﬁc
Linux 6.3, Windows 7 Enterprise Edition, VMware player 5.0.0, VirtualBox 4.2.0. Amazon EC2 m1.xlarge instance: 4 cores Intel Xeon E5—2650@2.00 GHz, 15 GB; $0.48/hr.

“runtime results in minutes.
b(write/read, in KB/s).

 

2080

112 /310'S[BIIJDOIPJOJXO'SOIJBLUJOJIIIOIq/ﬂduq 111011 pap1201um0q

9103 ‘0g1sn8nV uo ::

Virtual machines for NGS analysis

 

Edition) was in fact remarkably efﬁcient. Although this particu-
lar conﬁguration averaged only an ~7% decrease in speed com-
pared with the native Linux conﬁguration, in two of the
benchmark tests (variant calling and multithreaded read map-
ping) it was actually faster than the native conﬁguration. This
VM/OS combination is of particular interest, given that many
end users work in a Windows environment, whereas most NGS
software is developed to run speciﬁcally on Linux machines.

Given the large ﬁle sizes associated with NGS data, we also
wanted to test the hard drive and file system performance of
these conﬁgurations using the Bonnie++ utility, a ﬁle system
benchmarking tool that measures specific parameters such as
ﬁle creation, write, read and removal times (Coker, 2001). As
with the CPU tests, we found that the performance penalty of
a VM was relatively modest, on average (~23%); however, there
were again speciﬁc tests where VM configurations remarkably
outperformed the native Linux conﬁguration. Although our VM
benchmarking was not exhaustive (especially given the number
of VMs and bioinformatic tools available), the results neverthe-
less suggest that, perhaps contrary to common belief, VMs do
not necessarily have signiﬁcantly poorer performance compared
with a native OS. In agreement with our results, other bench-
marking tests have also concluded the computational ‘cost’ of
VMs is relatively low compared with native execution on hard-
ware (Corwin et al.). In addition, because VMs physically exist as
several ﬁles on the hard drive of the host machine, the entire
system, and all the data it contains, can simply be transferred
to newer hardware on a regular basis to further improve
performance when required. Despite the caveat that the ﬁnal
performance will ultimately be highly dependent on the hardware
setup, software tool conﬁguration and the speciﬁc VM used, our
results indicate that a suitable NG$VM could be conﬁgured in
such a way that reduces its impact on performance.

Given the interest in cloud computing, we also performed
benchmarking of our VM on Amazon EC2 in an m1.xlarge in-
stance. Although the results indicate a longer runtime on EC2, it
is important to note that differences in the hardware used (e.g.
processor differences, RAM available; Table 4), and the poten-
tially way in which instances are managed on EC2, likely inﬂu-
enced performance results. More importantly, the Amazon cloud
allows job scaling over orders of magnitude, such that the abso-
lute time required for an analysis could be arbitrarily reduced by
simply initiating additional instances and dividing the data
among the instances. Although we cannot directly compare
our local versus cloud-based results, the ease with which we
could transfer and run our VM highlights the potential of
these remote resources to enable small laboratories with no com-
putation infrastructure to download a VM, populate it with their
data, and then transfer this to a cloud compute center for rapid
and on—demand analysis.

3.2 Potential for N GS—VM growth in biomedical ﬁeld

The concept of bundling software within a VM is not novel;
indeed one of the largest VM developers, VMware, currently mar-
kets this conﬁguration of tools within a VM as a ‘Virtual
Appliance’. The possibility for NG$VM usage to be broadly
adopted by the research community will be dependent on a sig-
niﬁcant change in philosophy, although there is already a growing

appreciation that there are problems with the current methods of
software use and distribution (Nekrutenko and Taylor, 2012).
Beyond the research environment, a more compelling push for
the development of NG&VMs will likely come from the growing
application of NGS to clinical diagnostics. According to the
American Hospital Association, there were almost 37 million hos-
pital admissions in the USA alone in 2012 (http://www.aha.org/
research/rc/stat-studies/fast—facts.shtml). If only a fraction of
these admissions would require diagnostic sequencing (e. g. meta-
genomic analysis of bacterial infections prior to antibiotic treat-
ment), the informatics requirements at thousands of hospitals
would be enormous. Therefore, as great as the current difﬁculties
are in research centers (where some bioinformatic resources are
often available), the problem for hospitals and medical clinics may
soon be orders of magnitude worse. The possibility of developing
a standardized, and clinically certiﬁed, NGS-VM could dramat-
ically reduce the complexity of integrating NGS technologies into
diagnostic settings, allowing its broader adoption and use. With
respect to clinical certiﬁcation for VMs (e. g. Clinical Laboratory
Improvement Amendments /Health Insurance Portability and
Accountability Act compliance), issues around information secur-
ity, audit trails, access controls, etc. could conceivably be ad-
dressed within the initial design of the VM itself, similar to any
other hospital information system. For example, if one hospital
(in conjunction with a laboratory or research center) developed an
NG$VM that could be clinically validated for a speciﬁc purpose
(e. g. identifying mutations in a panel of cancer genes), this same
NG$VM should be able to be downloaded and used with little or
no modiﬁcation by another hospital for the same purpose.
Because there are a ﬁnite number of diagnostic procedures
likely to be adapted to NGS technology, a repository of such
clinical NG$VMs could be created, along with any details of
their implementation. This approach would not only remove the
incredible redundancy of every hospital and clinic independently
downloading and configuring multiple tools for analysis, but it
would also facilitate meta-analysis of diagnostic results between
centers.

4 CONCLUSIONS

The advent of extremely rapid, inexpensive high-throughput
DNA sequencing will likely be considered to be one of the
most significant developments in the history of biomedical re-
search. At the same time, the power of this approach has become
limited by the very factors that made it revolutionary to begin
with, namely, the ability to rapidly and cheaply produce massive
amounts of sequence data. Although new bioinformatic tools
have successfully dealt with speciﬁc challenges associated with
analyzing NGS data, the creation of a large and disparate col-
lection of tools has made it difﬁcult to ensure reproducibility of
computational analysis and is hindering the adoption of NGS
technology. VMs present a simple and realistic option for
addressing both of these concerns, and their potential has,
until now, been only partially exploited. Despite the requirement
for a signiﬁcant shift in the paradigm for distribution of
bioinformatic tools, the future alternative of an even great
collection of individual OS-speciﬁc NGS tools will carry far
fewer beneﬁts for end users of this technology. By exploiting
the object-oriented model of distributing data and methods as

 

2081

112 /310'S[BIIJDOIPJOJXO'SOIJBLUJOJIIIOIq/ﬂduq 111011 pap1201um0q

9103 ‘0g1sn8nV uo ::

J.Nocq et al.

 

a single entity, VMs cannot only simplify NGS analysis, but will
also facilitate the growth and integration of this technology into
additional ﬁelds.

ACKNOWLEDGEMENTS

We would like to thank Samuel Marguerat and colleagues
within IRIC for critical review of the manuscript and helpful
comments.

Funding: Cole Foundation, Le Fonds de recherche du Quebec 7
Sante grant number #25341 (to B.T.W), Genome Quebec (to
B.T.W. and S.L.) and Institut National de la Recherche
Agronomique (to MC.)

Conflict of Interest: none declared.

REFERENCES

Abyzov,A. et al. (2011) CNVnator: an approach to discover, genotype, and char—
acterize typical and atypical CNVs from family and population genome sequen—
cing. Genome Res., 21, 9747984.

Altschul,S.F. et al. (1990) Basic local alignment search tool. J. Mol. Biol., 215,
403410.

Anders,S. and Huber,W. (2010) Differential expression analysis for sequence count
data. Genome Biol., 11, R106.

Angiuoli,S.V. et al. (2011) CloVR: a virtual machine for automated and portable
sequence analysis from the desktop using cloud computing. BMC
Bioinformatics, 12, 356.

Butler,J. et al. (2008) ALLPATHS: de novo assembly of whole—genome shotgun
microreads. Genome Res., 18, 8107820.

Chiang,D.Y. et al. (2008) High—resolution mapping of copy—number alterations with
massively parallel sequencing. Nat. Methods, 6, 997103.

Coker,R. (2001) Bonnie++. http://www. coker. com. au/bonnie+, (03 July 2013,
date last accessed) 53.

Corwin,J. et al. A virtual machine program—suite for distributed de novo gen—
ome construction and motif ﬁnding. http://www.blueideas.de/ecs234_s10_
cloudoomputingpdf (03 July 2013, date last accessed).

Creasy,R.J. (1981) The origin of the VM/370 time—sharing system. IBM J. Res.
Devel., 25, 483490.

Cret,O. et al. (2009) A hardware algorithm for the exact subsequence matching
problem in DNA strings. Rom. J. Inf. Sci. Technol., 12, 51767.

Da Wei Huang,B.T.S. and Lempicki,R.A. (2008) Systematic and integrative ana—
lysis of large gene lists using DAVID bioinformatics resources. Nat. Protoc., 4,
44457.

Dobin,A. et al. (2012) STAR: ultrafast universal RNA—seq aligner. Bioinformatics,
2‘), 15721.

Durbin,R. (2009) The sequence alignment/map format and SAMtools.
Bioinformatics, 25, 207872079.

Fejes,A.P. et al. (2008) FindPeaks 3.1: a tool for identifying areas of enrichment
from massively parallel short—read sequencing technology. Bioinformatics, 24,
172971730.

Goecks,J. et al. (2010) Galaxy: a comprehensive approach for supporting accessible,
reproducible, and transparent computational research in the life sciences.
Genome Biol., 11, R86.

Goya,R. et al. (2010) SNVMix: predicting single nucleotide variants from next—
generation sequencing of tumors. Bioinformatics, 26, 73(P736.

Grant,J.R. et al. (2011) In—depth annotation of SNPs arising from resequencing
projects using NGS—SNP. Bioinformatics, 27, 230(k2301.

Hardcastle,T.J. and Kelly,K.A. (2010) baySeq: empirical Bayesian methods for
identifying differential expression in sequence count data. BMC
Bioinformatics, 11, 422.

Homer,N. et al. (2009) BFAST: an alignment tool for large scale genome resequen—
cing. PLoS One, 4, e7767.

Karolchik,D. et al. (2009) The UCSC genome browser. Curr. Protoc.
Bioinformatics, 1.4. l—l.4. 26.

Kent,W.J. (2002) BLATithe BLAST—like alignment tool. Genome Res., 12,
656—664.

Koboldt,D.C. et al. (2009) VarScan: variant detection in massively parallel sequen—
cing of individual and pooled samples. Bioinformatics, 25, 228372285.

Krampis,K. et al. (2012) Cloud BioLinux: pre—conﬁgured and on—demand bioinfor—
matics computing for the genomics community. BMC Bioinformatics, 13, 42.

Laczik,M. et al. (2012) Geno viewer, a SAM/BAM viewer tool. Bioinformation, 8,
107.

Langille,M.G. and Eisen,].A. (2010) BioTorrents: a ﬁle sharing service for scientiﬁc
data. PLoS One, 5, c1007].

Langmead,B. et al. (2009) Ultrafast and memory—efficient alignment of short DNA
sequences to the human genome. Genome Biol., 10, R25.

Larson,D.E. et al. (2012) SomaticSniper: identiﬁcation of somatic point mutations
in whole genome sequencing data. Bioinformatics, 28, 3117317.

Li,H. (2011) A statistical framework for SNP calling, mutation discovery, associ—
ation mapping and population genetical parameter estimation from sequencing
data. Bioinformatics, 27, 298772993.

Li,H. and Durbin,R. (2009) Fast and accurate short read alignment with Burrowsi
Wheeler transform. Bioinformatics, 25, 175441760.

Li,H., Ruan,J. and Durbin,R. (2008) Mapping short DNA sequencing reads
and calling variants using mapping quality scores. Genome Research, 18,
185171858.

Li,H. et al. (2008a) Maq: mapping and assembly with qualities, Version 0.6, 3.
Genome research, 18, 185171858.

Li,H. et al. (2009) The sequence alignment/map format and SAMtools.
Bioinformatics, 25, 207872079.

Li,R. et al. (2008b) SOAP: short oligonucleotide alignment program. Bioinformatics,
24, 7137714.

Li,R. et al. (2009) SOAP2: an improved ultrafast tool for short read alignment.
Bioinformatics, 25, 196671967.

Mardis,E.R. (2011) A decade/’s perspective on DNA sequencing technology.
Nature, 470, 1987203.

McKenna,A. et al. (2010) The Genome Analysis Toolkit: a MapReduce framework
for analyzing next—generation DNA sequencing data. Genome Res., 20,
129771303.

Nekrutenko,A. and Taylor,J. (2012) Next—generation sequencing data interpret—
ation: enhancing reproducibility and accessibility. Nat. Rev. Genet., 13, 6677672.

Ng,S.B. et al. (2010) Massively parallel sequencing and rare disease. Hum. Mol.
Genet., 19, R1197R124.

Ning,Z. et al. (2001) SSAHA: a fast search method for large DNA databases.
Genome Res., 11, 172571729.

Pfeifer,G.P. and Hainaut,P. (2011) Next—generation sequencing: emerging lessons
on the origins of human cancer. Curr. Opin. Oncol., 23, 62.

Popendorf,K. and Sakakibara,Y. (2012) SAMSCOPE: an OpenGL—based real—time
interactive scale—free SAM viewer. Bioinformatics, 28, 12731277.

Robertson,G. et al. (2010) De novo assembly and analysis of RNA—seq data. Nat.
Methods, 7, 9097912.

Robinson,J.T. et al. (2011) Integrative genomics viewer. Nat. Biotechnol., 29, 24426.

Robinson,M.D. et al. (2010) edgeR: a Bioconductor package for differential expres—
sion analysis of digital gene expression data. Bioinformatics, 26, 1397140.

Rozowsky,.l. et al. (2009) PeakSeq enables systematic scoring of ChIP—seq experi—
ments relative to controls. Nat. Biotechnol., 27, 6375.

Rumble,S.M. et al. (2009) SHRiMP: accurate mapping of short color—space reads.
PLoS Comput. Biol., 5, e1000386.

Rutherford,K. et al. (2000) Artemis: sequence visualization and annotation.
Bioinformatics, 16, 944w945.

Salomonis,N. et al. (2009) Alternative splicing in the differentiation of human em—
bryonic stem cells into cardiac precursors. PLoS Comput. Biol., 5, e1000553.

SathirapongasutiJF. et al. (2011) Exome sequencing—based copy—number vari—
ation and loss of heterozygosity detection: ExomeCNV. Bioinformatics, 27,
264872654.

Schweiger,M.R. et al. (2011) The power of NGS technologies to delineate the
genome organization in cancer: from mutations to structural variations and
epigenetic alterations. Cancer Metastasis Rev., 30, 1997210.

Shen,J.J. and Zhang,N.R. (2012) Change—point model on nonhomogeneous Poisson
processes with application in copy number proﬁling by next—generation DNA
sequencing. Ann. App]. Stat., 6, 476496.

Simpson,J.T. et al. (2009) ABySS: a parallel assembler for short read sequence data.
Genome Res., 19, 111771123.

Smith,J.E. and Nair,R. (2005) The architecture of virtual machines. Computer, 38,
32738.

Stein,L.D. (2010) The case for cloud computing in genome informatics. Genome
Biol., 11, 207.

 

2082

112 /310's112u1n0[p1q1x0"soiwurJOJHioiq/ﬁduq 111011 papao1um0q

9103 ‘0g1sn8nV uo ::

Virtual machines for NGS analysis

 

Tarazona,S. et al. (2011) Differential expression in RNA—seq: a matter of depth.
Genome Res., 21, 221%2223.

Teer,J.K. et al. (2012) VarSifter: visualizing and analyzing exome—scale sequence
variation data on a desktop computer. Bioinformatics, 28, 599400.

Tomlinson,C.D. et al. (2013) XperimentR: painless annotation of a biological ex—
periment for the laboratory scientist. BMC Bioinformatics, l4, 8.

Trapnell,C. et al. (2009) TopHat: discovering splice junctions with RNA—Seq.
Bioinformatics, 25, 1 10571111.

Trapnell,C. et al. (2012) Differential gene and transcript expression analysis of
RNA—seq experiments with TopHat and Cufﬂinks. Nat. Protoc., 7, 5627578.

Wang,L. et al. (2010) DEGseq: an R package for identifying differentially expressed
genes from RNA—seq data. Bioinformatics, 26, 1367138.

Warren,R.L. et al. (2007) Assembling millions of short DNA sequences using
SSAKE. Bioinformatics, 23, 50(F501.

Xie,C. and Tammi,M.T. (2009) CNV—seq, a new method to detect copy number vari—
ation using high—throughput sequencing. BM C Bioinformatics, 10, 80.

Zerbino,D.R. and Birney,E. (2008) Velvet: algorithms for de novo short read
assembly using dc Bruijn graphs. Genome Res., 18, 8217829.

Zhang,Y. et al. (2008) Model—based analysis of ChIP—Seq (MACS). Genome Biol., 9,
R137.

 

2083

112 /310's112u1n0[p1q1x0"soiwurJOJHioiq/ﬁduq 111011 papao1um0q

9103 ‘0g1sn8nV uo ::

