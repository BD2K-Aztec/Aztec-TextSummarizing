Bioinformatics, 31 (22), 2015, 3600—3607

doi: 10.1093/bioinformatics/btv371

Advance Access Publication Date: 23 July 2015
Original Paper

 

 

Structural bioinformatics

High-order neural networks and kernel methods
for peptide-MHC binding prediction

Pavel P- Kuksa1’2’3'f, Martin Renqiang Min3'*'*, Rishabh Dugar3"r and
Mark Gerstein4'5'6'*

1Institute for Biomedical Informatics, 2Department of Pathology and Laboratory Medicine, University of
Pennsylvania School of Medicine, Philadelphia, PA 19104, USA, 3Department of Machine Learning, NEC
Laboratories America, Princeton, NJ 08540, USA, 4Program of Computational Biology and Bioinformatics and
5Department of Molecular Biophysics and Biochemistry and 6Department of Computer Science, Yale University,
New Haven, CT 06511, USA

*To whom correspondence should be addressed.
TThe authors wish it to be known that, in their opinion, the first three authors should be regarded as Joint First Authors.
Associate Editor: Anna Tramontano

Received on December 20, 2014; revised on June 8, 2015; accepted on June 11, 2015

Abstract

Motivation: Effective computational methods for peptide—protein binding prediction can greatly
help clinical peptide vaccine search and design. However, previous computational methods fail to
capture key nonlinear high—order dependencies between different amino acid positions. As a result,
they often produce low—quality rankings of strong binding peptides. To solve this problem, we pro—
pose nonlinear high—order machine learning methods including high—order neural networks
(HONNs) with possible deep extensions and high—order kernel support vector machines to predict
major histocompatibility complex—peptide binding.

Results: The proposed high—order methods improve quality of binding predictions over other pre—
diction methods. With the proposed methods, a significant gain of up to 25—40% is observed on the
benchmark and reference peptide datasets and tasks. In addition, for the first time, our experiments
show that pre—training with high—order semi—restricted Boltzmann machines significantly improves
the performance of feed—forward HONNs. Moreover, our experiments show that the proposed
shallow HONN outperform the popular pre—trained deep neural network on most tasks, which dem—
onstrates the effectiveness of modelling high—order feature interactions for predicting major histo—
compatibility complex—peptide binding.

Availability and implementation: There is no associated distributable software.

Contact: renqiang@nec—Iabs.com or mark.gerstein@yale.edu

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 IntrOducuon responses, etc. Moreover, peptide—mediated interactions play im—

Complex biological functions in living cells are often performed portant roles in the development of several human diseases including
through different types of protein—protein interactions. An import— cancer and viral infections. Because of the high medical value of pep—
ant class of protein—protein interactions are peptide (i.e. short chains tide—protein interactions, a lot of research has been done to identify
of amino acids) —mediated interactions, and they regulate import— ideal peptides for therapeutic and cosmetic purposes, which renders
ant biological processes such as protein localization, endocytosis, in silico peptide—protein binding prediction by computational meth—
post—translational modifications, signalling pathways and immune ods an important problem in immunomics and bioinformatics

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3500

9103 ‘Og JSanV uo salaﬁuv soc] ‘BtHJOJtIBQ JO AJtsraAtuf] 112 /3.IO'S[BIIJI’IOLPJOJXO'SOpBLUJOJIItth/ﬂduq 11101} popcolumoq

High—order neural networks and kernel methods

3601

 

(Brusic et (11., 2002; Hoof et (11., 2009; Lundegaard et (11., 2011;
Nielsen et (11., 2003).

In this article, we propose novel machine learning methods to
study a specific type of peptide—protein interaction, i.e. the inter—
action betvveen peptides and major histocompatibility complex class
I (MHC I) proteins, although our methods can be readily applicable
to other types of peptide—protein interactions. Peptide—MHC
I protein interactions are essential in cell—mediated immunity, regu—
lation of immune responses, transplant rejection and vaccine design.
Therefore, effective computational methods for peptide—MHC
I binding prediction will significantly reduce cost and time in clinical
peptide vaccine search and design.

Previous computational approaches to predicting peptide—MHC
interactions are mainly based on linear or bi—linear models, and they
fail to capture key non—linear high—order dependencies between dif—
ferent amino acid positions. Although previous kernel support vec—
tor machine (SVM) and Neural Network (NetMHC) (Giguere et (11.,
2013; Hoof et (11., 2009; Lundegaard et (11., 2011) approaches can
capture nonlinear interactions between input features, they fail to
model the direct strong high—order interactions between features. As
a result, the quality of the peptide rankings produced by previous
methods is not good. Producing high—quality rankings of peptide
vaccine candidates is essential to the successful deployment of com—
putational methods for vaccine design. For this purpose, we need to
effectively model direct non—linear high—order feature interactions to
directly capture interactions between primary (anchor) and second—
ary amino acid residues involved in the formation of peptide—MHC
complexes.

Deep learning models such as deep neural networks (DNNs) pre—
trained with restricted Boltzmann machine (RBM) have been suc—
cessfully applied to handwritten digit classification, embedding,
image recognition and many other applications (Hinton, 2010; Min
et (11., 2010; Ranzato et (11., 2013). But they have never been success—
fully applied to peptide—protein interaction problems.

In this article, we propose using high—order semi—RBMs to pre—
train a feed—forward high-order neural network (HONN) and propose
high—order kernel SVM for peptide—MHC binding prediction, includ—
ing identification of MHC—binding, naturally processed and presented
(NPP) and immunogenic peptides (T—cell epitopes). Our proposed
models achieved a significant gain of up to 25—40% over the state—of—
the—art approach on benchmark and reference peptide datasets and
tasks. Furthermore, our shallow HONNs even outperformed popular
powerful pre—trained DNNs that was applied to model peptide—MHC
binding prediction for the first time by this work.

2 Related work

Position-specific scoring matrix (PSSM) and matrix-based methods:
In Nielsen et al. (2004) Reche and Reinherz (2007) and Reche et al.
(2002), PSSMs were derived from a set of known binding peptides
and PSSM matching score was used as an indicator of the binding
potential of a query peptide. In Peters and Sette (2005), the peptide
binding task was solved as a matrix—vector regression problem.
Neural network-based methods: In Zhang et al. (2005) and Brusic et
al. (2002), neural networks were built to predict peptide binding po—
tentials by encoding peptides and contact residues on the MHC mol—
ecules as a fixed—dimensional vector of amino acid and contact
residues. Similarly, in Nielsen et al. (2003), Buus et (11., (2003) and
Lundegaard et al. (2011), neural networks and committees of net—
works with peptide representations combining sparse, BLOSUM
and profile HMM encodings of the peptides were used. In Hoof

et al. (2009), both the peptide sequence and MHC protein sequence
were used as input to neural networks to enhance predictive ability
for MHC alleles with limited peptide binding data. Kernel-based
methods: The work in Salomon and Flower (2006) used the local
alignment kernel method for predicting MHC—II—peptide binding. In
Tung et al. (2011 ), weighted—degree kernels were adopted to identify
immunogenic peptides. The work in Liu et al. (2007) employed sup—
port vector regression (with RBF, polynomial, etc. kernels) using
sparse encoding of a peptide sequence and 11—dim physicochemical
amino—acid descriptors. Recent work (Giguere et (11., 2013) used ker—
nel logistic regression for MHC—II—peptide binding prediction using
both peptide and MHC sequences. In Gigure et al. (2013), an SVM
with kernel from (Giguere et (11., 2013) was used for NPP (‘eluted’)
peptide prediction.

3 Methods

For the peptides to bind to a particular MHC allele (i.e. its peptide—
binding groove), the sequences of the binding peptides should be ap—
proximately superimposable: contain amino acids or strings of
amino acids (k—mers) with similar physicochemical properties at ap—
proximately the same positions along the peptide chain.

It is then natural to model peptide sequences X : x1,x2, . . . ,xn,
x,- E 2 (i.e. sequences of amino acid residues) as a sequences of de-
scriptor vectors d1,  ,dn, encoding relevant properties of amino
acids observed along the peptide chain and/or MHC—peptide inter—
action terms.

3.1 Descriptor sequence peptide representations
Although the descriptor vectors d,- in general may be of unequal
length, in the matrix form (equal—sized vectors (1,- 6 RR) of this rep—
resentation (‘feature—spatial—position matrix’), the rows are indexed
by features (e.g. individual amino acids, strings of amino acids,
k—mers, physicochemical properties and peptide—MHC interaction
features), while the columns correspond to their spatial positions
(coordinates). Figure 1 illustrates descriptor sequence representation
of a nonamer.

In this descriptor sequence representation, each position in the
peptide is described by a feature vector, with features derived from the
amino acid occupying this position or from a set of amino acids (e.g. a
k—mer starting at this position or a window of amino acids centred at
this position) and/or amino acids present in the MHC protein mol—
ecule and interacting with the amino acids in the peptide.

positions

 

 

 

-0.267 -0.296 -0.353 0.199 0.008 -0.132 0.255 0.303 0.173

 

0.018 -O.186 0.071 0.238 0.134 0.174 0.038 -0.057 0.286

 

-0.265 0.389 -0.088 -0.015 -0.475 0.07 0.117 -0.014 0.407

 

-0.274 0.083 -0.195 -0.068 -0.039 0.565 0.118 0.225 -0.215

 

descriptor values
(features)

0.206 0.297 -0.107 -0.196 0.181 -0.374 -0.055 0.156 0.384

 

 

 

 

 

 

 

 

 

 

 

Fig. 1. Peptide descriptor sequence representation of a nonamer
‘MVLSAFDER' using 5-dim amino acid descriptors

91oz ‘Og JSanV uo salaﬁuv soc] ‘BtHJOJtIBQ JO AJtSJQAtuf] 112 /310'S[BIIJHOLPJOJXO'SOIJ’BLUJOJIIIOICI”K1111] 11101} papeolumoq

3602

P. P. Kuksa et aI.

 

The purpose of a descriptor is to capture relevant information
(e.g. physicochemical properties) that can be used by our HONNs
and kernel functions to differentiate peptides into binding, non—
binding, immunogenic, etc.

A real-valued descriptor of an amino acid is a quantitative de—
scriptor encoding (i) relevant properties of amino acids such as their
physicochemical properties and substitution probabilities by other
amino acids and/or (ii) interaction features (such as binding energy)
between the amino acids in the peptide and those in the MHC
molecule. An example of the real—valued descriptor sequence repre—
sentation of a peptide using 5—dim physicochemical amino acid de—
scriptors is given in Figure 1.

3.2 DNN and HONN

Given the matrix—form descriptor representation of each peptide based
on BLOSUM substitution matrix as illustrated above, we concatenate
all the columns of the matrix into a long vector as input feature vector
to our neural networks. In this representation, a 9—mer peptide is rep—
resented by a 180—dimensional continuous vector, with each amino
acid represented by its corresponding 20—dimensional substitution
probabilities. Instead of using an ensemble of traditional neural net—
works to predict MHC class—peptide bindings as in the state—of—the—art
approach NetMHC (Buus et al., 2003; Lundegaard et al., 2011;
Nielsen et al., 2003), we propose to use HONNs pre—trained with a
special type of high—order semi—RBMs called mean—covariance RBMs
(mcRBMs) (Ranzato et al., 2013), capable of capturing strong high—
order interactions of feature descriptors of input peptides, to produce
high—quality rankings of binding peptides (T—cell epitopes). The pre—
training strategy has been widely adopted for training a popular
powerful model called DNN (Bengio, 2009; Hinton, 2006).

DNN has attracted world—wide attention in the machine learning
community recently. In Hinton (2006), it has been shown that DNN
is more powerful than shallow neural networks and performs much
better than shallow ones on a benchmark dataset widely used in ma—
chine learning. In this article, for the first time, we apply DNN to
predict peptide—MHC binding, and we compare its performance to
our proposed HONN. DNN is shown on the left panel of Figure 2.
We use Gaussian RBM to pre—train the network weights of its first
layer, and we use binary RBM to pre—train the connection weights
of upper layers in a greedy layer—wise fashion (see Hinton, 2006 for
detailed descriptions about pre—training). Our proposed HONN is
shown on the right panel of Figure 2. We use mcRBM to pre—train

      
 

output unit

optional

mean ovariance
hidden hidden
units units

U U U
visible units visible units

DNN HONN

Fig. 2. The structure of DNN (left) and HONN (right)

the network weights of it first layer, and we optionally add upper
layers, and we use binary RBM to pre—train the connection weights
in possibly available upper layers. In both DNN and HONN, we
use a logistic unit as our final output layer, and then we use back—
propagation to fine—tune the final network weights by minimizing
the cross entropy between predicted binding probabilities P" and
target binding probabilities tn as follows,

N
—thn10gpn+(1 _tn)10g(1 _Pﬂ)l7 (1)

11:1

where N is the total number of training peptides.

The pre—training module mcRBM of HONN extends traditional
Gaussian RBM to model both mean and explicit pairwise inter—
actions of input feature values, and it has two sets of hidden units,
mean hidden units hm modelling the mean of input features and co—
variance hidden units hg gating pairwise interactions between input
features. If the gating hidden units are binary, they act as binary
switches controlling the pairwise interactions between input fea—
tures. The energy function of mcRBM with factorized weights for
reducing computational complexity is defined as follows,

E(V7 hg7 hm) :   UiCif)2  hkPkf) — 261111,"
f i k i
—Z bkhkg — Zvihimwii — Z Ckl’lkm
1: ii I:

(2)

where i indexes visible units such as peptide sequence features,
/ indexes hidden units and f indexes the factors. Using this energy
function, we can derive the conditional probabilities of hidden units
given visible units, as well the respective gradients for training the
network. The structure of this factorized mcRBM is shown on the
bottom of the right panel of Figure 2, the hidden units on the left
model the mean of input features and those on the right model the
input covariance. During pre—training, we used Contrastive
Divergence (Hinton, 2002) to learn the factorized weights in
mcRBM as in Gaussian REM, and we used Hybrid Monte Carlo
sampling to generate the negative samples as in Ranzato et al.
(2013) with 20 leap—frog steps. The structures and parameters of
both DNN and HONN are decided based on performance on valid—
ation sets. In fact, for our HONN, only the learning rates, batch size
and the number of hidden units need to be carefully tuned, and the
final performance is not sensitive to other hyper—parameters. During
the training phase, our algorithm randomly selects 10% of the ori—
ginal training data as validation set for early stopping. When the al—
gorithm monitors that the validation error increases up to 10 times
even if the training error is still decreasing, we end the training pro—
cess for early stopping. Although HONN can be easily extended to
have many upper layers to form a deep architecture, HONN with—
out deep extensions works best in all our experiments, which is
probably due to the limited training data we have.

3.3 High—order kernel models

The sequence of the descriptors corresponding to the peptide X :
x1,x2,  ,xiXi, x,- E 2 (as in, e.g. Fig. 1) can be modelled as an
attrihuted set of descriptors corresponding to different positions
(or groups of positions) in the peptide and amino acids or strings of
amino acids occupying these positions:

XA :{(piidi)}:'t:1

where p,- is the coordinate (position) or a set (vector) of coordinates
and d,- is the descriptor vector associated with the p,, with 71 indicating

91oz ‘Og JSanV uo salaﬁuv soc] ‘BtHJOJtIBQ JO AJtsraAtuf] 112 /3.IO'S[BIIJHOLPJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} papeolumoq

High—order neural networks and kernel methods

3603

 

the cardinality of the attributed set description XA of peptide X. The
cardinality of the description X A corresponds to the length of the
peptide (i.e. the number of positions) or to in general to the number of
unique descriptors in the descriptor sequence representation. A unified
descriptor sequence representation of the peptides as a sequence of
descriptor vectors is used to derive attributed set descriptions X A.

3.4 High—order kernel functions on peptide descriptor
sequence representations

In the following, we define kernel functions for peptides based on
peptide descriptor sequence representations (such as in Fig. 1). The
proposed kernel functions for peptide sequences X and Y have the
following general form:

: Zka(pf§.p,-’:)kd(d§vdi:) (3)

ix iv

where  is a descriptor sequence (e.g. spatial feature matrix) rep—
resentation of a peptide, XA(YA) is an attributed set corresponding
to M(X) (M(Y)), kd(-, -), kp(-, -), are kernel functions on descriptors
and context/positions, respectively, and iX, iy index elements of the
attributed sets XA, YA. While led measures similarity between de—
scriptors, the context/position kernel lap measures similarity of the of
the descriptor context (e.g. position and spatial distribution of
amino acids). A number of kernel functions for descriptor sequence
(e.g. matrix) forms  is described below.

Using real—valued descriptors (e.g. vectors of physicochemical at—
tributes), with RBF or polynomial kernel function on descriptors,
the kd(da, dB) is defined as

eXP (-Ydllda — drill)
where yd is an appropriately chosen weight parameter, or
(<dm dB) + 6)!)

where p is the degree (interaction order) parameter and c is a param—
eter controlling contribution of lower order terms.

Kernel functions kp(-,  on position sets p,- and p,- are defined as
a set kernel

kp(pi7p/’) : Z Zk(iil’lai 

iEPi lEP,’

where

L” [3 =exz>(—odog(li—il)) + i3

is a kernel function on pairs of position coordinates (i, j).

The position set kernel function above assigns weights to inter—
actions between positions (i, /) according to k(i, flat, [3).

The descriptor kernel function (e.g. RBF or polynomial) be—
tween two descriptors d,- : (ding,  ,d;) and d,- : (d’j,d§7  ,
d‘R) induces high—order (i.e. products—of—features) interaction fea—
tures (such as d,1 diz . . . dip for polynomial of degree p) between pos—
itions/attributes.

The proposed kernel function (Eq. 3) captures high—order inter—
actions between amino acids/positions by considering essentially
all possible products of features encoded in descriptors d of two or
more positions. The feature map corresponding to this kernel is
composed of individual feature maps capturing interactions
between particular combinations of the positions. The interaction

maps between different positions pa and pb are weighted by the pos—
ition/context kernel function kp(pa, pb).

4 Data

To assess the performance of our high—order methods, we tested our
methods on three prediction tasks:

1. MHC-I hinding prediction. The datasets used for MHC—I bind—
ing prediction task are listed in Table 1.

2. Naturally processed (NP) (‘eluted’) peptide prediction. We use
recently compiled benchmark data from the 2nd Machine
Learning in Immunology competition (MLI—II). Table 2 provides
details of this dataset.

3. T—cell epitope prediction. We use data of known T—cell epitopes
to test ability of the methods in predicting promising candidates
for clinical development.

For all the tasks, we focused on the 9—mer peptides. For MHC—I
binding prediction, we threshold at a standard value IC50 : 500
to separate binding peptides (I C50 < 500) and non—binding
(IC50 > 500) peptides and focus on three alleles, HLA—A"'0201,
HLA—A"'0206 and HLA—A’i2402. The choice of these alleles is moti—
vated by the target population group (Japanese) in our research lab.
The application of our method to other alleles or peptide lengths
would be straightforward.

4.1 Training and testing protocol

For MHC—I binding prediction, we train our models for each allele
on the publicly available data from the Immune Epitope Database
and Analysis Resource (IEDB) (Vita et al., 2010). The datasets
(http://www.iedb.org) are labelled with IEDB suffix in Table 1.

For testing, we use the experimental data from our lab for
each allele. These datasets are denoted with ‘Japanese’ suffix in
Table 1. For ‘Japanese’ data, the experimentally determined bind—
ing strength is measured as log (Kd), where Kd is a dissociation coef—
ficient, i.e. higher negative values of log (Kd) suggest stronger
binders.

The training ‘IEDB’ datasets and the test ‘Japanese’ datasets
are completely disjoint. The average sequence identity between any
peptide in the ‘Japanese’ datasets and the most similar peptide
from IEDB data is about 46—55% (Supplementary Table S10).

4.2 Evaluation metrics
To assess performance, we use two sets of metrics, classical binary
metrics and non—binary relevance metrics.

Binary performance metrics. We used (i) area under ROC curve
(AUC) and (ii) area under ROC curve up to first n false positives
(ROC—n).

Non-binary relevance/quality metrics. While classical binary per—

a

formance metrics use binary relevance (i.e. ‘1 : relevant, ‘0’ : non—

relevant), to take into account more ‘precise’ relevance measure, i.e. the

Table 1. Peptide-MHC binary datasets (binding/non-binding)

 

 

Dataset No. peptides No. binders No. non—binders
A0201—IEDB 8471 3939 4532
A0201—Japanese 281 106 175
A0206—IEDB 1820 951 869
A0206—Japanese 278 97 181
A2402—IEDB 2011 890 1121
A2402—Japanese 405 1 76 229

 

9103 ‘Og JSanV uo salaﬁuv soc] ‘BtHJOJtIBQ JO AJtsraAtuf] 112 /3.IO'S[BIIJHOLPJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} papeolumoq

3604

P. P. Kuksa et al.

 

Table 2. NP peptide datasets

Table 3. Comparison of AUC test scores on A0201-Japa nese data

 

 

 

 

 

Dataset No. peptides No. eluted No. non—eluted Method AUC ROG—10 ROG—20 ROG—30 ROG—50
A0201 —MLI—II 8225 971 7254 hkSVM 79.60 32.71 50.59 63 .67 77.56
A0201—MLI—II—EvalSet 492 63 429 DNN 77.23 30.34 47.03 60.1 1 74.95
HONN 77.26 33.39 48.14 60.11 74.98
hkSVM+HONN 79.11 35.59 50.51 62.99 77.02
binding strength of the peptides, we use normalized discounted cumula— NetMHC 7690 2651 4502 5 837 74~47

tit/e gain (nDCG), a classical non-binary (graded) relevance metric.

Given a list of peptides P1. . . . .PN ordered by the output scores
of the predictor f (P1).  .f(PN), the discounted cumulative gain
(DCGN) is defined as a sum of individual peptide relevance scores
(experimentally determined binding strength) (11.112. . .. .qn dis—
counted by the log of their position i in the list:

N .
2% — 1
DCGN : +
1;: log (1 + 1)

The normalized DCGN is defined as a ratio between DCG of the
method and an ideal DCG iDCGN (i.e. DCG of an ideal ordering of
peptides from the highest degree of binding affinity to the lowest
binding affinity):

DCGN
iDCGN

 

nDCGN I

The normalized DCGN value is then ranges between 0 and 1,
with nDCGN: 1 corresponding to the ideal value (i.e. normalized
DCG: 1 when the predictor orders peptides according to their ac—
tual binding strength).

We find this measure (nDCG) to be more indicative of the predic—
tion performance of the MHC—I binding prediction method as it dir—
ectly assesses whether the predictor ranks stronger binders higher than
weaker binders [as opposed to binary measures (e.g. area under ROC
curve) that measure whether ‘binders’ are ranked higher than ‘non—
binders’ irrespectiver of the actual peptide binding strength]. This
measure is popular for assessing performance of the document re—
trieval systems (e.g. Web search engines) as it is maximized if the most
relevant documents appear at the top of search results, but it has not
been used to differentiate performance of the MHC binding pre—
dictors. In the case of the peptide—MHC prediction, the nDCG is
maximized if peptides are placed (according to the predictor output)
in the ideal order: from the strongest binders to the weakest/non—bind—
ers. We emphasize that the two methods with the same AUC scores
may differ significantly with respect to their nDCG scores: even with
the equally good separation between ‘binders’ and ‘non—binders’ for
the two methods, the method that correctly ranks stronger binders
higher than weaker binder will have a higher nDCG score.

5 Results

We first present results for MHC—I binding prediction on benchmark
datasets and experimental data from our lab (section 5.1). We show
next results on predicting peptides NP by the MHC pathway (sec—
tion 5.2). Finally, we show results for predicting promising T—cell
epitopes for clinical development (section 5.3). The following AUC
and nDCG scores are shown in percentage.

5.1 MHC—l binding prediction

We train a DNN, a high—order semi—RBM (HONN) and a high—
order kernel SVM (hkSVM) on IEDB data. In our experiments, we
use BLOSUM substitution matrix as continuous descriptors of input
peptide sequences.

 

The bold values highlight the best comparable performance achieved by
different methods.

Table 4. Comparison of AUC test scores on A0206-Japa nese data

 

 

Method AUC ROG—10 ROG—20 ROG—30
hkSVM 86.23 54.84 72.58 78.68
DNN 80.24 52.42 64.02 71.31
HONN 84.41 49.7 69.7 77.78
hkSVM + HONN 86.24 54.24 73.33 80.2
NetMHC 83.93 50.91 67.42 76.77

 

The bold values highlight the best comparable performance achieved by
different methods.

Table 5. Comparison of AUC test scores on A2402-Japa nese data

 

 

Method AUC ROG—5 ROG—10 ROG—30
hkSVM 90.59 68.8 75.92 86.93
DNN 89.1 63.52 70.96 84.75
HONN 86.29 54.88 65.04 81.17
hkSVM+ HONN 91.07 72.16 77.76 87.55
NetMHC 88.88 53.76 66.88 84.48

 

The bold values highlight the best comparable performance achieved by
different methods.

We compare with the popular NetMHC method that has been
shown to yield state—of—the—art accuracy for MHC—I binding predic—
tion with respect to other best published methods (see e.g. Gigure
et al., 2013; Lundegaard et al., 2011; Zhang et al., 2009).

We first use ‘Japanese’ datasets to test our methods. Results
are shown in Tables 3—5 for target alleles on Japanese test data—
sets. Corresponding ROC curves are shown in Figure 3 (top row).
We also plot nDCG@n curves in Fig. 3 (bottom row), where nDCG
@n is nD CG up to nth peptide in the sorted output (i.e. nDCG of the
top—n predicted peptides).

As evident from the AUC and ROC—n results in the tables and
ROC plots, our method achieves significant improvements in sepa—
rating ‘binders’ versus ‘non—binders’. For example, for A2402 allele,
ROC—n: 10 score increases from 66.88 for NetMHC to 77.76 for
HONN and hkSVM. Similar improvements are observed on A0201
allele data where ROC—n : 10 score improves from 26.61 for
NetMHC to 35.59 with HONN and hkSVM.

Observed improvements in the AUC and ROC—n scores across
all alleles are significant (paired signed rank test, P value 1.22e—4).

To further validate our methods, we used recent benchmark
MHC—I binding data proposed in Kim et al. (2014) consisting of the
training data (BD2009) and independent (BLIND) test data
(Supplementary Table S8). We report performance on the independ—
ent test data (BLIND) in Supplementary Table S9. As can be seen
from the results in the table, while the area under ROC curve (AUC)
scores are very similar for both our method and the NetMHC

9103 ‘Og isanV uo salaﬁuv soc} ‘BtHJOJtIBQ JO AirsraAtuf] 112 /310'S[BIIJHOLPJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} papeolumoq

High—order neural networks and kernel methods

3605

 

 

 

 

    

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

0.7 . 1
0.6 _ _ -
  _ . . . . . . . . . .
5 °-5 5 -  - a
e e a
    _ . . . . . . . . . . . . . . . . . . . _ 
O o O
D. . . . . . . . . . . . . . . . . . . . Q. Q.
a, 0'3 ------- -- a, 0.4- g
a a u
i— 02 . . . . . . . . . . . . . .. . _ i— '—
5 0-2' f NetMHC 5 ‘ 0'2 - - - - NetMHC
0.1 .:. . . . . . . . . . . . . . . . .. _ _ 5 _Our method 5 01 I our methOd
0 ; . . 0 ; . ; . ; o . . .
0 0.05 0.1 0.15 0.2 0 0.05 0.1 0.15 0.2 0.25 0.3 o 0.05 0.1 0.15 0.2
False positive rate False positive rate False positive rate
(a) ROC curves on test A0201 allele. (b) ROC curves on test A0206 allele. (0) ROC curves on test A2402 allele.
1 0.95
0.9- — 03.
: 0_3_ : 0.85 c
0) © ©
8 o 0.8- - o
o 0.7- g o
'5 u g
S a, 0 75 . m
a 0.6- -  I. ‘ , a
E E 0.7-i ~ E
2 0.5- r I ° ’ °
I: : : z 0551’ . . . . . . . . _ Z
' —Our method I —Our method 0 5
_ i . .   _ -
0.55 ' ' ' ' 0.45 ' ' ' '
0 20 40 60 30 0 20 40 60 80 10 20 30 40 50
n n n

((1) Normalized DCG curves on test

A0201 allele. A0206 allele.

(e) Normalized DCG curves on test

(f) Normalized DCG curves on test
A2402 allele.

Fig. 3. ROC curves (top row) and normalized discounted cumulative gain (nDCG) curves (bottom row)

method, for the very highest ranked peptides [low false—positive (FP)
rates], both hkSVM and HONN+hkSVM perform better on
average compared with NetMHC as measured by ROC—n scores
[e.g. ROC—1 scores of hkSVM or HONN are higher in about 67%
(31/46) of the tested alleles]. Observed improvements in ROC—n
scores (low FP rates) are significant (paired signed rank test P
values: 7e—3 and 1.38e—2 for hkSVM and HONN+hkSVM,
respectively).

At the same time, the results in terms of nDCG quality scores
suggest significant increase in ranking quality (Tables 6—8). Our
method ranks peptides by their actual binding strength significantly
better than other methods. We observe that strong binders are
placed much higher in the classification results compared with the
state—of—the—art NetMHC method. For instance, for the A0201 al—
lele, nDCG@n scores improve from 60.98, 63.50 achieved by
NetMHC to 65.94, 70.61 using our HONN method for n : 20 and
n : 30 respectively.

We note that for both HONN and DNN, the pre—training is crit—
ical to achieve good performance. The performance comparisons of
DNN and HONN with and without pre—training are in the supple—
mentary material (Supplementary Tables S2—S7). All the results of
DNN and HONN reported in the main article are based on pre—
training and fine—tuning.

Using a combination of network and kernel models further im—
proves peptide—MHC recognition as evident by the increase in both
area under ROC curve scores (improved ‘binder’ versus ‘non—binder’
separation) and nDCG metric quality scores (improved ranking of
peptides by binding strength).

We note that unlike the previous approaches that utilized quanti—
tative binding information during training, no quantitative

Table 6. A0201-Japanese data

 

 

Method nDCG@ 10 nDCG@ 20 nDCG@ 30 nDCG@ 50 nDCG
hkSVM 60.69 61.75 66.78 74.11 85.01
DNN 63.89 65.59 70.12 74.57 86.33
HONN 63.93 65.94 70.61 75.55 86.46
hkSVM + HONN 65.69 65.12 71.49 76.46 86.98
NetMHC 59.48 60.98 63.50 72.68 83.94

 

Relevance/ranking quality (nDCG).
The bold values highlight the best comparable performance achieved by
different methods.

Table 7. A0206-Japanese data

 

 

Method nDCG@ 10 nDCG@ 20 nDCG@ 30 nDCG
hkSVM 76.52 74.64 82.49 91.43
DNN 77.50 82.21 81.72 91.74
HONN 75.39 78.06 79.92 90.80
hkSVM + HONN 80.2 76.98 83.75 91.75
NetMHC 70.97 73.60 82.57 89.88

 

Relevance/ranking assessment (nDCG).
The bold values highlight the best comparable performance achieved by
different methods.

information regarding actual binding strength was used to train our
models. However, even with only hinary training data [i.e. only with
binding (B) versus non—binding (NB) information], our models cor—
rectly order peptides according to their binding strength. This can be
attributed to explicit high—order interaction modelling by our method

9103 05 isanV uo so1a§uv 50’] 0211110311123 JO [(1151910qu 112 /3.IO'S[BIIJHOLPJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} papeo1umoq

 

 

 

 

 

3606 P.P.Kuksa et al.
Table 8. A2402-Japanese data Table 9. NP peptide prediction (MLl-ll competition)

Method nDCG@ 10 nDCG@ 30 nDCG Method AUC ROC-10 ROC-20 ROC-30 ROC-50
hKSVM 53.77 64.33 86.68 hkSVM 94.75 53.65 65.71 71.48 77.46
DNN 51.07 56.88 84.36 HONN 93.17 49.21 58.20 64.13 72.73
HONN 57.36 60.82 85.20 DNN 91.80 30.48 41.11 51.32 62.92
hkSVM + HONN 60.41 69.59 87.35 hkSVM + HONN 94.96 53.65 68.25 74.39 79.59
NetMHC 55.98 68.76 87.57 NetMHC 92.26 10.63 28.33 40.21 54.32

 

Relevance/ranking assessment (nDCG)
The bold values highlight the best comparable performance achieved by
different methods.

weight vector Cf connecting features and covariance factor

 

0.1
0.05
0
—0.05
—0.1
—0.1 5
20 40 60 80 100 120 140 160 180
feature index
connection weights between features and mean hidden units

a:

.5 0.02

.E

.E 0.01

3

é 0

.‘Q

.C

c —0.01

8

E —0.02

 

20 40 60 80 100 120 140 160 180
feature index

Fig. 4. The learned weights of HONN with largest absolute values

that allows to capture intrinsic binding strength information.
Nevertheless, our models can easily use quantitative training data (e.g.
1C5 0) to further improve our results.

To visualize the learned weights of HONN, we used 8 mean hid—
den units, 1 covariance hidden unit and 1 factor unit to train HONN
on the training data of A2402. We obtained AUC score 86.02 and
nDCG score 85.01 that are slightly worse than the ones in Tables 5
and 8. In Figure 4, the factorized rank—1 interaction weight vector
with absolute values greater than 0.1 is shown in the top, and the
weight matrix connecting input features and mean hidden units with
absolute values greater than 0.02 is shown at the bottom. This figure
clearly shows that positions 2, 8, 9 and the interaction between middle
position and position 9 are very important for predicting 9—mer pep—
tide binding, which has experimental support from the crystal struc—
ture of the interaction complex (Cole et al., 2006).

5.2 NP peptide prediction
We test ability of our methods on a difficult task that aims at pre—
dicting whether a peptide is NP by the MHC pathway (‘eluted’).
This is a very important task as only a fraction of binding peptides
(see ‘MHC—I binding task’ in Section 5 .1) constitute a set of peptides
that are processed to the surface of a cell and may serve as epitopes.
Eluted peptide prediction thus aims at verifying whether a peptide
not only binds to a given MHC molecule, but that it is also NP by
MHC pathway in vivo.

To train our models, we used the data provided by 2012
Machine Learning in Immunology competition (MLI-II) http://bio.
dfci.harvard.edu/DFRMLI/ HTML / natural . php.

MHC-NPa 88 .06 — — — —

 

Comparison of test AUC scores.

3Quoted from Gigure et al. (2013).

The bold values highlight the best comparable performance achieved by
different methods.

Table 10. Prediction of WT1-derived epitopes

 

NetMHC-rank hkSVM + HONN-rank

 

A0201 allele

WT-TEST-PEPTIDEI 2 1
WT-TEST-PEPTID E2 20

A0206 allele

WT-TEST-PEPTIDEI 2 1
WT-TEST-PEPTIDE2 8 3
A2402 allele

WT-TEST-PEPTIDEI 41 2
WT-TEST-PEPTID E2 7 4

 

We directly train our models to recognize NPP peptides,
using ‘eluted’ peptides as a positive set, and all other peptides
(non—binders+non—eluted binders) as a negative set. We then test
our models on the data composed of non—eluted binding peptides,
non—binding peptides and NP (‘eluted’) peptides. We used the same
training and test split as specified in the competition. We compare
our approach with the popular NetMHC method, which was used
as a benchmark in the competition, as well as the recently intro—
duced MHC—NP (Gigure et al., 2013) method that yielded state—of—
the—art accuracy for NP peptide prediction.

Table 9 lists results of NP peptide prediction (9—mers) on the
test set in terms of AUC, ROC—n and F1 scores. Our approach signifi—
cantly outperforms both NetMHC method and the MHC—NP (Gigure
et al., 2013) method. Supplementary table S11 shows the performance
of hkSVM for the other test alleles with similar improvements on test
peptides with all varying lengths (8—mers to 1 1—mers).

5.3 Epitope prediction

We demonstrate ability of the method to predict promising peptides
for clinical development using as an example WT1—derived strong
binding peptides WT—TEST-PEPTIDEl and WT-TEST-PEPTIDEZ
discovered by NEC—Kochi Univ. We compare the performance of
our method and NetMHC by ‘predicting’ in a retrospective way
these T—cell epitopes from WT1 antigen. Peptides (441 9—mers) that
are part of WT1 antigen are ranked by the output scores of
NetMHC and our method (HONN and hkSVM). The order of the
WT—TEST—PEPTIDEl and WT—TEST—PEPTIDEZ peptides in the
output (out of the 441 peptides) of the two prediction methods is
given in Table 10. As evident from the table, our method ranks these
peptides higher than NetMHC method.

9103 ‘09 isanV no so1a§uv s01 02111100111123 JO [fume/nu 11 112 /810's112um0[p10}x0'sopcurJOJutotq/ﬁduq 11101} papao1umoq

High—order neural networks and kernel methods

3607

 

6 Discussion and future work

In this article, we propose using nonlinear high—order machine learn—
ing methods including HONN and hkSVM for peptide—MHC I pro—
tein binding prediction. Experimental results on both public and
private evaluation datasets according to both binary and non—binary
performance metrics (AUC and nDCG) clearly demonstrate the
advantages of our methods over the state—of—the—art approach
NetMHC, which suggests the importance of directly modelling non—
linear high—order feature interactions across different amino acid
positions of peptides. Our results are even more encouraging con—
sidering that our models were only trained on a subset of the binary
binding datasets used by NetMHC and NetMHC was also trained
on private quantitative binding datasets.

In the future, we will use available quantitative binding datasets
to refine our HONN model with possible deep extensions, and we
will incorporate the descriptors of structural contacting amino acids
on MHC proteins into current feature descriptors. The addition of
peptide binding strength and structural information will potentially
further improve the performance of our current models.

Acknowledgements

We thank Dr Keiko Udaka for providing valuable experimental datasets and
validations and Dr Hans Peter Graf for helpful discussions.

Funding

This work was mainly supported by the funding from NEC Laboratories
America.

Conﬂict of Interest: none declared.

References

Bengio,Y. (2009) Learning deep architectures for ai. Found. Trends Mach.
Learn., 2, 1—127.

Brusic,V. et al. (2002) Prediction of promiscuous peptides that bind HLA class
I molecules. Immunol. Cell Biol., 80, 280—285.

Buus,S. et al. (2003) Sensitive quantitative predictions of peptide-MHC bind-
ing by a ‘Query by Committee’ artiﬁcial neural network approach. Tissue
Antigens, 62, 378—384.

Cole,D.K. et al. (2006) Crystal structure of HLA—A‘l2402 complexed with a
telomerase peptide. Eur. ]. Immunol, 36, 170—179.

Giguere,S. et al. (2013) Learning a peptide-protein binding afﬁnity predictor
with kernel ridge regression. BMC Bioinformatics, 14, 82.

Gigure,S. et al. (2013) MHC—NP: Predicting peptides naturally processed by
the MHC. ]. Immunol. Methods, 400401, 30—36.

Hinton,G. (2002) Training products of experts by minimizing contrastive di—
vergence. Neural Comput., 14, 1771—800.

Hinton,G. (2006) A fast learning algorithm for deep belief nets. Neural
Comput., 18,1527—1554.

Hinton,G.E. (2010) Learning to represent visual input. Philos. Trans. R. Soc.
B Biol. Sci., 365, 177—184.

Hoof,I. et al. (2009) NetMHCpan, a method for MHC class I binding predic-
tion beyond humans. Immunogenetics, 61, 1—13.

Kim,Y. et al. (2014) Dataset size and composition impact the reliability of
performance benchmarks for peptide—MHC binding predictions. BMC
Bioinformatics, 15, 241.

Liu,W. et al. (2007) In silico prediction of peptide-MHC binding afﬁnity using
SVRMHC. In: Flower,D.R. (ed.) Immunoinformatics, Volume 409 of
Methods in Molecular Biology, Humana Press, Totowa, NJ, USA, pp.283—
291.

Lundegaard,C. et al. (2011) Prediction of epitopes using neural network based
methods. ]. Immunol. Methods, 374, 26—34.

Min,M.R. et al. (2010) Deep supervised t—distributed embedding. In:
Proceedings of the 27th International Conference on Machine Learning
(ICML-I 0), Haifa, Israel, june 21—24, 2010, Omnipress, Norristown, PA,
USA, pp. 791—798.

Nielsen,M. et al. (2003) Reliable prediction of T-cell epitopes using
neural networks with novel sequence representations. Protein Sci., 12,
1007—1017.

Nielsen,M. et al. (2004) Improved prediction of MHC class I and class II
epitopes using a novel Gibbs sampling approach. Bioinformatics, 20, 1388—
1397.

Peters,B. and Sette,A. (2005 ) Generating quantitative models describing the se—
quence speciﬁcity of biological processes with the stabilized matrix method.
BMC Bioinformatics, 6, 132.

Ranzato,M. et al. (2013) Modeling natural images using gated MRFs. IEEE
Trans. Pattern Anal. Mach. Intell., 35, 2206—2222.

Reche,P.A. and Reinherz,E.L. (2007) Prediction of peptide-MHC binding
using proﬁles. In: Flower,D.R. (ed.), Immunoinformatics, Volume 409
of Methods in Molecular Biology, Humana Press, Totowa, NJ, USA, pp.
185—200.

Reche,P.A. et al. (2002) Prediction of MHC class I binding peptides using pro—
ﬁle motifs. Hum. Immunol, 63, 701—709.

Salomon,J. and Flower,D. (2006) Predicting class II MHC-peptide binding:
a kernel based approach using similarity scores. BMC Bioinformatics, 7,
501.

Tung,C.-W. et al. (2011) POPISK: T-cell reactivity prediction using support
vector machines and string kernels. BMC Bioinformatics, 12, 446.

Vita,R. et al. (2010) The immune epitope database 2.0. Nucleic Acids Res.,
38, D854—D862.

Zhang,G. et al. (2005) MULTIPRED: a computational system for prediction
of promiscuous HLA binding peptides. Nucleic Acids Res., 33(Web Server
Issue), 172—179.

Zhang,H. et al. (2009) Pan-speciﬁc MHC class I predictors: a benchmark of
HLA class Ipan—speciﬁc prediction methods. Bioinformatics, 25, 83—89.

9103 05 isnﬁnv uo sa1af§uv 50’] 0211110311123 JO [(1151910qu 112 /3.IO'S[BIIJHOLPJOJXO'SOIJ’BLUJOJIIIOICI”Zduq 11101} papeo1umoq

