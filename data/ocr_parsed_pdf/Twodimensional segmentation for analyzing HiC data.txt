Vol. 30 ECCB 2014, pages i386—i392
doi:10. 1093/bioinfonnatics/btu443

 

Two-dimensional segmentation for analyzing Hi-C data

Celine Lévy-Leduc”, M. Delattre‘, T. Mary-Huard1'2 and 8. Robin1
1AgroParisTech/INRA MIA 518, 75005 Paris and 2UMR de Génétique Végétale, INRA/Univ. Paris-Sud/CNRS, 91190

Gif-sur-Yvette, France

 

ABSTRACT

Motivation: The spatial conformation of the chromosome has a deep
influence on gene regulation and expression. Hi-C technology allows
the evaluation of the spatial proximity between any pair of loci along
the genome. It results in a data matrix where blocks corresponding to
(self-)interacting regions appear. The delimitation of such blocks is
critical to better understand the spatial organization of the chromatin.
From a computational point of view, it results in a 2D segmentation
problem.

Results: We focus on the detection of cis-interacting regions, which
appear to be prominent in observed data. We define a block-wise
segmentation model for the detection of such regions. We prove
that the maximization of the likelihood with respect to the block
boundaries can be rephrased in terms of a 1D segmentation problem,
for which the standard dynamic programming applies. The perform-
ance of the proposed methods is assessed by a simulation study
on both synthetic and resampled data. A comparative study on
public data shows good concordance with biologically confirmed
regions.

Availability and implementation: The HiCseg R package is available
from the Comprehensive R Archive Network and from the Web page
of the corresponding author.

Contact: celine.levy-leduc@agroparistech.fr

1 INTRODUCTION

Many key steps of the cell development and cycle, such as DNA
replication and gene expression are inﬂuenced by the 3D struc-
ture of the chromatin (Dixon et al., 2012). The folding of the
chromosome in the space deﬁnes chromosomal territories, the
function of which has been studied for few years now
(Lieberman-Aiden et al., 2009). Typically, topologically associat-
ing domains contain clusters of genes that are co-regulated (Nora
et al., 2012). Thus, the detection of chromosomal regions having
close spatial location in the nucleus will provide insights for a
better understanding of the inﬂuence of the chromosomal con-
formation on the cells functioning.

Several chromosome conformation capture technologies
have been developed in the past decade, among which Hi-C is
the most recent. This technology is based on a deep sequencing
approach and provides read pairs corresponding to pairs of
genomic loci that physically interact in the nucleus (Lieberman-
Aiden et al., 2009). The raw measurement provided by Hi-C
is therefore a list of pairs of locations along the chromosome,
at the nucleotide resolution. These measurement are often
summarized as a square matrix Y, where YU- stands for
the total number of read pairs matching in position i and
position j, respectively. Positions refer here to a sequence of

 

*To whom correspondence should be addressed.

non-overlapping windows of equal sizes covering the genome.
The number n of windows may vary from one study to another:
Lieberman-Aiden et a]. (2009) considered an Mb resolution,
whereas Dixon et a]. (2012) went deeper and used windows of
100 kb.

Blocks of higher intensity arise among this matrix, revealing
both cis— and trans-interacting regions (Fraser et al., 2009).
Although both types of interaction are likely to exist, cis—inter—
acting regions seem to be prominent in the data (see Dixon et al.,
2012, and Figs 7 and 8, for instance), and some have been
confirmed to host co-regulated genes (Nora et al., 2012). Such
regions result in block of higher signal along the diagonal of the
data matrix. The purpose of the statistical analysis is then to
provide a fully automated and efﬁcient strategy to determine
these regions. A first attempt was presented in Dixon et a].
(2012), where the author strategy is ﬁrst to summarize the 2D
data into a 1D index, called the directionality index, then to
apply a regular hidden Markov model to the summary data to
retrieve the segmentation.

In this article, we show that such a two-step strategy can be
avoided, and that summarizing the data is not required to solve
the segmentation problem. Detecting diagonal blocks can be seen
as a particular 2D segmentation issue. The 2D segmentation has
been widely investigated for the detection of contour with arbi-
trary shape in images (see, for example, Darbon and Sigelle,
2006a, b; Hochbaum, 2001). From a computational point of
view, image segmentation is an open problem because no prede-
fined ordering exists that could be used to provide exact and
efﬁcient algorithms. Compared with contour detection, it is
worth noticing that Hi-C data segmentation displays a speciﬁc
pattern that did not receive any special attention from the image
processing community. One of our contributions is to prove
that this 2D segmentation problem boils down to a 1D segmen-
tation problem for which efﬁcient dynamic programming algo-
rithms apply (Bellman, 1961; Lavielle, 2005; Picard et al., 2005).
Our formulation of the problem also allows us to solve some non-
block diagonal segmentation problems (see the end of Section
2.2).

The article is organized as follows. In Section 2, we deﬁne a
general statistical model for Hi-C data, which can deal with
both raw and normalized data. We prove that the maximum
likelihood estimates of the block boundaries can be efﬁciently
retrieved. In Section 3, we ﬁrst present an extensive simulation
study to assess the performance of our approach on both
simulated and resampled data. We then apply the proposed
methodology to the data studied by Dixon et a]. (2012),
which are publicly available, and compare our results with
their regions. The package implementing the proposed method
is presented in Section 4 where some open problems are also
discussed.

 

© The Author 2014. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non—Commercial License (http://creativecommons.org/licenses/by—nc/3.0/), which permits
non—commercial re—use, distribution, and reproduction in any medium, provided the original walk is properly cited. For commercial re—use, please contact joumals.permissions@oup.oom

112 /310'S[BIIJHO[pJOJXO"SOIJBLUJOJIIIOIq/ﬂduq moi; papnolumoq

91oz ‘Og isanV uo ::

Two-dimensional segmentation for analyzing Hi-C data

 

2 STATISTICAL FRAMEWORK

2.1 Statistical modeling

We ﬁrst define our statistical model. Because the Hi-C data
matrix is symmetric, we only consider its upper triangular part
denoted by Y, in which YU (1 5 i 5 j 5 n) stands for the inten-
sity of the interaction between positions i and j. We suppose
that all intensities are independent random variables with
distribution

Mi,j=IE(Yi.j) (I)

where the matrix of means (aw-)lfl-S-Sn is an upper triangular
block diagonal matrix. An example of such a matrix is displayed
in Figure 1 (left). Namely, we define the (half) diagonal blocks
D;(k=1,...K*) as

D;={(z21) : tier : isj 5 r; — 1} (2)

where 1=t3<tf<  <t;<.=n+1 stand for the true
block boundaries and K* for the true number of blocks. We
further deﬁne ES as the set of positions lying outside these
blocks:

Yi,j~p('; My),

ESZIUUJ): 1 :ifjsnmeL). (3)

where X denotes the complement of the set A. The parameters
(aw) are then supposed to be block-wise constant:

Mil-=14 if(i.1)eD,:.k=1,...,K*,
=M3 if(i.;)eE;.

As for the distribution p(-; a“) deﬁned in (1), we will consider
Gaussian, Poisson or negative binomial distributions:

(G) 3 Yi,j NAM/hm 02),
(P) 1 Yu N Will-,1) (5)
(B) 2 Yr; ~ NIX/my ¢)-

(4)

The Gaussian modeling (G) will be typically used for
dealing with normalized Hi-C data and the others [(P)
and (B)] to deal with raw Hi-C data, which are count
data. In Models (G) and (B), note that the parameters
a and q) are assumed to be constant and depend neither on
i nor on j.

 

   

I I1 ta, t, t, t t} t? r, t4

D1: R25  Dla R25 
(m), (M) ("1): in.) R 
1 Rs i 3 i
(“0) D2 (IL 3) 

2 R. 2 E R4

1 (in) i W

D, D,

MA) I“)

 

 

 

 

 

Fig. 1. Examples of block diagonal and extended block diagonal matrices
(aid-)lsiﬂ-Sn. Left: Model (4), right: Model (9)

2.2 Inference

We now consider the estimation of the block boundaries
(t;)05kSK* in the case where the number of blocks K* is known.
Model selection issues will be discussed in Section 2.3. We con-
sider a maximum likelihood approach. For an arbitrary set of
blocks Dk, with boundaries (tk)0<k<K and parameters (ak)0<k<K,
the log-likelihood of the data satisfying (1) and (4) writes _ _

e(Y)= Z logpmjwij)

l5i5j5n
K

=2 2 10gp(Yi-.j;uk)+ Z 10gp(Yi.j;/‘L0)v
k= l(i,])e Dk (i,j)e E0

where Dk and E0 are defined as in (2) and (3), respectively, except
that the t;s are replaced by the tks.

Parameter estimation For given boundaries to, . . . , tK, the esti-
mation of the block parameters ak is straightforward for each of
the distribution considered in (5). Denoting KAY”) and €0(Y,-,j)
the contribution of each data point to the log-likelihood (up to
some constants), in Dk and E0, respectively, we get, for known
parameters ([5 and a0,

rim-JP —(Y,-,- — Ewan-JP —(Y,-,j — Ho)?
Kim,» = Yu 10% (7k) — 7k, 30130111) = Yu 10% (Mo) — #07
em» = —¢ log n+7,» + Yr,- log (Tn/n+7»).
rim/F —¢ log (¢+M0)+ Im- log (loo/own».
where Yk=ZU§DeDk YU/lel, for k in {1, . . . , K}, |A| denoting
the cardinality of the set A.
Dynamic programming algorithm Let us now consider the esti-

mation of the boundaries to, . . . tK. The objective function can be
rewritten as follows:

K
€(Y)=Z Z €k(Yi,j)+ Z 30(Yu)

k= 10,1) 6 D, (L1) e E0
K
= Z em) + 2 tom.»
k=l (Lj) e D, (11/) e Rk

where Rk corresponds to the rectangle above Dk (see Fig. 1),

namely, Rk={(i,J):tk,15j 5 tk —1,15i5tk,1—1}. (Note

that R1 is empty.) Note that the rectangles Rk do not overlap

and that E0 =URk, so the last equality holds. The important

point here is t at the objective function is now additive with

respect to the successive intervals {tk,1, . . . tk — 1}, 1 5 k 5 K.
Defining the gain function

C(tk,l.tk—1)= Z amt-H Z roan-,1» (6)

(L!) E Dk (ll!) 6 Rk

we have to maximize w.r.t. 1=t0<t1< ... <tK=n+l

K
2 C(lkeli tk — 1),
k=l

 

112 /3.10'spzu.mo[p10}xo"sorJBMJOJurorq//:d11q moi; papnolumoq

91oz ‘Og isanV uo ::

C.Lévy-Leduc et al.

 

which can be done using the standard dynamic programming
recursion (Bellman, 1961). For any 1 5 L 5 K and l<‘L' 5 n,
we deﬁne

IL(T)= max
l=to<tl<m<tL=r+l

L
2 C(lkeli lk — 1)

k=l

the value of the objective function for the optimal segmentation
of the submatrix made of the ﬁrst 1' rows and columns of Y into
L blocks. Clearly, we have 11(1) = C(l, 1'),

12(1'): max  l1 — I)+C(l1, T)
l<tl<r+l

= max+lll(t1—1)+C(t1, T)

l<tl<r
and, for 3 5 L 5 K,

IL(T)= 1 max llLelULel — 1)+C(IL717 T)- (7)

<tL,1 <r+

Hence, the optimal segmentation can be recovered with complex-
ity 0(Kn2), once the C(-, -) have been computed.

Common parameters The optimization procedure described
above applies when both [to and q) are known. Estimates of
these parameters can be obtained in the following way. The es-
timate [20 of no can be computed as the empirical mean of the
observations lying in the right upper corner of the matrix Y, for
instance,

To ={(i,j): 1 5 i 5 n/4, (3n/4-l- 1) 5j 5 n}. (8)

As for the overdispersion parameter of the negative binomial
distribution ([5, we computed (i) as follows: (25:03/(83 —/10),
where 8% corresponds to the empirical variance of the observa-
tions lying in the same right upper corner of the matrix Y as
for [20.

Non-block diagonal segmentation problem Observe that a simi-
lar procedure could be used for dealing with a more general
matrix (aw-)lfl-E/Sn defined by

“111:”; if(i.J) e D;,k=1,...,K*,

* ~ . k k 
=ptjc lf(l,j)€Rk,k=2,...,K,

where the diagonal blocks D; and the rectangles R; are deﬁned
as above (see Fig. 1, right). In this case, no prior estimation of
any mean parameter is required, as each a}; is speciﬁc to one
single rectangle.

2.3 Model selection issue

In the case where the value of K* in the model deﬁned by (1) and
(4) is known a priori (tAk)l<k< K can be obtained from the recur-
sion (7), which actually gives the values of (tk)l<k<K for all
1 5 K 5 Kmax, where Kmax is a given upper bound for the
number of blocks. If K* is unknown, it can be estimated by K
deﬁned as follows:

K=Argmax15KSK IK(n). (10)

mm

This strategy is illustrated in the next section.

3 RESULTS

Dixon et al. (2012) studied intrachromosomal interaction matri-
ces for various chromosomes in both the human genome and the
mouse genome at different resolutions (20 and 40 kb) and iden-
tiﬁed topological domains for each analyzed chromosome. Both
the data and the topological domains found by Dixon et al.
(2012) are available from the following Web page http://
chromosome.sdsc.edu/mouse/hi-c/downloadhtml. We worked
on the same data, at a resolution 40kb, to study the performance
of our approach described above.

3.1 Application to synthetic data

We conducted several Monte Carlo simulations ﬁrst on synthetic
data and then on resampled real data to assess the sensibility of
our method to block size and signal-to-noise ratio. The synthetic
data are generated by using the domains found by Dixon et al.
(2012) for Chromosome 19 of the cortex mouse. As for the
resampled data, they are generated by using the Hi-C data of
the chromosomes of the human embryonic stem cells (hESCs)
provided by Dixon et al. (2012). The different simulation strate-
gies are further described hereafter.

3.1.] Fixed block design To evaluate the performance of our
methodology in the negative binomial framework, we generated
block diagonal matrices according to Model (5) (B) where (an)
is deﬁned by (4). More precisely, we generated 50 block diagonal
interaction matrices of size n = 300 with a structure inspired
by the one found by Dixon et al. (2012) for the inter-
action matrix of Chromosome 19 of the mouse cortex. The
different parameters a}; as and q) are estimated from this
matrix. This resulted in matrices including ﬁve diagon-
al blocks such that a: =2.87, a; =4.85, a; =7.92, a: =4.33,

a;=ll.99, a3=0.09 and ¢=0.67. Then, for each simulated
dataset, new matrices were derived by multiplying the a;s by
a constant c e {0.1, 0.2, 0.3, . . . , 1} to reduce the signal-to-noise
ratio. For each simulated dataset and each constant, we com-
puted K and the corresponding fks using the procedure
described in Section 2.

The upper part of Figure 2 displays the histograms of the
estimated change-points for c = 0.1, c = 0.2 and c = 0.5. The
black dots correspond to the true change-points, and the bars
indicate the frequency of each estimated change-point. One can
observe that both the change-points and the number of change-
points are well estimated even in low signal-to-noise ratio frame-
works (except for c = 0.1). The bottom part of Figure 2 displays
the log-likelihood curves (up to some constants) with respect to
K for the same values of c, obtained on a given simulated matrix.
The dotted line indicates the location of the estimated number of
change-points. Even when the signal-to-noise ratio is small, the
estimated number of change-points K corresponds to the true
number of change-points K*. When the signal-to-noise ratio is
too small, i.e. for c = 0.1 here, some model selection issues arise.
Figure 2 shows that for such signal-to-noise ratio, the method
provides some spurious change-points within the blocks having
the lowest mean. When c = 0.1, the value of the mean in the ﬁrst
diagonal block is very low (0.28) and very close to #0.
Nevertheless, when taking the true number of blocks, the true
change-points are recovered. We also assessed the performance

 

i388

112 /310'S[BHJHOIPJOJXO'SOIJ’BLUJOJIIIOICI”Zdllq moi; papBOIH/noq

91oz ‘Og isnﬁnv uo ::

Two-dimensional segmentation for analyzing Hi-C data

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

0.. 0.. 0..
w w w
o" o" o"
LO LO (O
o" o" o"
‘1, ‘1, ‘1,
O O O
N._ N._ N._
O O O
0.. m 0.. 0..
O I I I l I I l O I l I I I l I O I I I l I I I
0 50 100 150 200 250 300 0 50 100 150 200 250 300 0 50 100 150 200 250 300
O z
8 O
Q 8 8_
I o_ O
w l!)
‘0 ‘0‘? ‘0'
O - O O
O O O
a a — go
2% £0 28.
I 0’ I8 lg
cm" c"o- 9|
3' 3T 4
T _ O
O
o o 8-
e.  r
‘T’ I 5 10 15 20 5 10 15 20
K K

 

Fig. 2. First line: Histograms of the estimated change-points in a ﬁxed block design for different signal-to-noise ratios in the negative binomial
framework (from left to right: c = 0.1, c = 0.2, c = 0.5). The dots correspond to the true change-points, and the bars indicate the frequency of each
estimated change-points. Second line: plots of the log-likelihood as a function of the number of change-points for one simulated dataset in the negative
binomial framework for different signal-to-noise ratios (from left to right: c = 0.1, c = 0.2, c = 0.5). The dotted and solid lines give the value of the

log-likelihood (up to some constants) for K and K, respectively

of our methodology in the Poisson framework, and we obtained
similar results, which are not reported here.

3.1.2 Resampling of the data In this second analysis, we first get
the boundaries found by Dixon et al. (2012) in all the chromo-
somes of the hESCs. We shall call the corresponding blocks the
Ren domains. From these domains, we generate a set of diagonal
blocks (D1, ..., DK), such that (i) the size of each block is drawn
in the empirical distribution of Ren domain lengths and (ii) the
cumulated number of positions is not >300. Once the block sizes
are drawn, we choose at random a human chromosome, and for
each diagonal block Dk, a Ren domain in this chromosome is
randomly selected, and observations in block Dk are resampled
from the Ren domain data. Accordingly, the data outside the
diagonal blocks are simulated by resampling from the data of the
E0 Ren domain in the selected chromosome. This strategy is
repeated 100 times to obtain 100 interaction matrices.
Compared with the previous simulation design, one can observe
that the change-point positions now change from one dataset
to the other, and that the data are not anymore simulated
according to a negative binomial distribution. While the statis-
tical analysis of datasets generated from this second simulation
setting is more difﬁcult, it allows one to visit more realistic data
conﬁgurations closely similar to real data. We report here the
results obtained when the simulated data are analyzed with
Model (5) (B), the results obtained with Model (5) (P) being
similar.

Figure 3 (left and center) displays two log-likelihood curves
(up to some constants) as a function of the number of change-
points. The solid and dotted lines indicate locations of the true
and estimated number of change-points, respectively. One can

observe that while the maximum is not always achieved at the
true number of change-points K*, the estimated value K corres-
ponding to the maximum likelihood is still fairly close to K*. The
true and estimated numbers of change-points are identical for 91
of the 100 simulations, and the absolute difference |K — K*| is
never >2 except for one example.

To further assess the quality of the estimated segmentation
compared with the true one, we computed the Hausdorff dis-
tance between these two segmentations deﬁned in the segmenta-
tion framework as follows, see Boysen et al. (2009) and
Harchaoui and Lévy-Leduc (2010):

d(t*,t)=max (d1(t*,f),d2(t*.2)). (11)
where t*=(t:, . . . , t2) 2:61. mile) and
d1(a,b)= sup inf la — bl, (12)
bebaea
d2(a, b) =d1(b, a). (13)

A small value of d2 (distance from true to estimate) means that
an estimated change-point is likely to be close to a true change-
point. A small value of d1 (distance from estimate to true) means
that a true change-point is likely to be close to each estimated
change-point. A perfect segmentation results in both null d1 and
d2. Oversegmentation results in a small d2 and a large d1.
Undersegmentation results in a large d2 and a small d1, provided
that the estimated change-points are correctly located. The two
parts d1 and d2 of the Hausdorff distance were computed in
the right part of Figure 3. Both distances d2 (‘true to estimate’)
and d1 (‘estimate to true’) were not >1 for 96 of the 100
simulations.

 

i389

112 /310'S[BIIJUOIPJOJXO"SOIJBLUJOJIIIOIq/ﬂduq moi; papBOIH/noq

91oz ‘Og isnﬁnv uo ::

C.Lévy-Leduc et al.

 

 

 

 

 

 

 

 

, \ o
O o
o o
m o
F ,
‘0' ‘c:3
o 0'
f’: f’:
2g 2
= :0
IF lo
a"? 8’°°'
4 4‘?
O _
8
o
N o
I N.
T. . . i
0 10 20 30 40

 

o o

 

 

 

 

 

 

 

I I
True to Estimate Estimate to True

Fig. 3. Left, center: Two examples of a log-likelihood curve (up to some constants) as a function of the number of change-points. Solid and dotted lines
indicate the true and estimated number of change-points, respectively. Right: Two parts of the Hausdorff distances computed by taking the true

(respectively the estimated) segmentation as reference

 

8. ale
a" * alt-
; *
S
i. * is
078- 0 *
CF
2
o 0
c.
O 0
n
o_ o are
2‘“ ° A o A Q
A
A A A A
0

 

 

 

14 1'6 1'8 20 22

Chromosome
Fig. 4. Number of change-points for the Chromosomes 1372 found by
the Bing Ren approach (‘*’), by HiCseg with Model (5) (P) (‘0’) and (5)
(B) (“A’)

3.2 Application to real data

In this section, we applied our methodology to the raw inter-
action matrices of Chromosomes 13722 of the hESCs at reso-
lution 40kb, and we compared the estimated number of blocks
and the estimated change-points found with our approach to
those obtained by Dixon et al. (2012) on the same data, as no
ground truth is available for those datasets.

From Figure 4, we can first see that the approach of Dixon
et al. (2012) tends to produce, in general, more change-points
than our strategy except for Chromosome 22. This can also be
seen in Figure 5, which displays the log-likelihood curves (up to
some constants) with respect to K as well as the number of
change-points proposed by Dixon et al. (2012) (dotted line)
and our approach (solid line).

We also compared both methodologies by computing the two
parts of the Hausdorff distance deﬁned in (12) and (13) for
Chromosomes 13722. More precisely, Figure 6 displays the box-
plots of the d1 and d2 parts of the Hausdorff distance without
taking the supremum. We can observe from this ﬁgure that some
differences exist between the segmentations produced by the two
approaches, but that the boundaries of the blocks are close.

To further illustrate the differences that exist between both
approaches, we display in Figures 7 and 8 the segmentations
provided by both approaches in the case of Chromosomes 17
and 19, respectively. In the case of Chromosome 17, we can
only provide the segmentation obtained with Model (5) (P)
because the overdispersion parameter r?) is infinite (the mean

 

   

Log-likelihood

-7e+05
Log-likelihood

 

 

 

 

 

 

-9e+05

 

02
o
o
N
o
o
w
o
o
4:.
o
o
01
O.
o
02
o
o
N
o
o
w
o
o
4:.
o
0

Fig. 5. Left: Log-likelihood (up to some constants) as a function of K for
the analysis of Chromosome 15 using Model (5) (P). The dotted vertical
lines is the number of blocks chosen by the Dixon et al. (2012) approach,
and the solid one correspond to the one of our approach. Right: The
same for Chromosome 19 using Model (5) (B)

 

40

30

o s o . a

i
Mi m a m

20

o

e

, I i  — g i . 
‘ 22 1‘3 ‘

1‘8 20 ‘ ‘

15 1‘8 20 22
Chromosome Chromosome

10
a M can
jam one

 

 

 

 

 

 

 

20 25

15

o

10

 

 

 

 

o, égéaéggzﬂ

13 15 1‘8 20 22
Chromosome

 

 

Chromosome

Fig. 6. Boxplots for the inﬁmum parts of the Hausdorff distances
all (left part) and d2 (right part) between the change-points
found by Dixon et al. (2012) and our approach for Chromoso-
mes 13722 for Model (5) (P) [(a) and (b)] and for Model (5) (B) [(c)
and (d)]

 

i390

112 /310'S[BHJHOIPJOJXO"SOIJBLUJOJIIIOIq/ﬂdnq moi; papeorumoq

9103 ‘Og isnﬁnv uo ::

Two-dimensional segmentation for analyzing Hi-C data

 

 

Fig. 7. Topological domains detected by Dixon et al. (2012) (lower tri-
angular part of the matrix) and by our method (upper triangular part of
the matrix) from the interaction matrix of Chromosome 17 of the hESCs
using Model (5) (P)

 

Fig. 8. Topological domains detected by Dixon et al. (2012) (lower tri-
angular part of the matrix) and by our method (upper triangular part of
the matrix) from the interaction matrix of Chromosome 19 of the hESCs
using Model (5) (P)

and the variance outside the diagonal blocks are of the same
order). In the other case where Models (5) (P) and (B)
can be applied, we used the following test procedure for
overdispersion under the Poisson model to decide between
both segmentations. Considering the data lying in T 0 as
defined in (8), we first estimate the mean within this region by
[2:20.206 To Y,~/N0 where N0 stands for the number of data
points within T 0. We then consider the test statistic
Q, = 2W) 6 To Yf/No. Reminding that, if Y has a Poisson distri-
bution with mean a, we have [E(Y2)=;i+;i2 and
M Y2) = 4m + 6n2 + a, it follows that

A A2
WWW/(0, 1)

,/4,23+6,22+,2

under the hypothesis that all observations from T 0 arise from the
same Poisson distribution.

Following this rule, we chose Model (5) (B) only for
Chromosomes l and 2. We can see from this ﬁgure that with
the naked eye, the diagonal blocks found with our strategy pre-
sent a lot of similarities with those found by Dixon et al. (2012).
We did not report the segmentations that we obtained for the
Chromosomes 1722, but they are available from the Web page of
the corresponding author http://www.agroparistechfr/mmip/
maths/essaimia/_media/equipes:membres:page:supplementary_
eccb.pdf.

4 CONCLUSION
4.1 HiCseg R package

In this article, we propose a new method for detecting cis—inter-
acting regions in Hi-C data and compare it with a methodology
proposed by Dixon et al. (2012). Our approach described in
Section 2 is implemented in the R package HiCseg, which is
available from the Web page of the corresponding author
http://www.agroparistech.fr/mmip/maths/essaimia/_media/equip
es:membres:page:hicseg_l.l.tar.gz and from the Comprehensive
R Archive Network. In the course of this study, we have shown
that HiCseg is an efficient technique for achieving such a
segmentation based on a maximum likelihood approach. More
precisely, HiCseg package has two main features, which make
it attractive. Firstly, it gives access to the exact solution of
the maximum likelihood approach. Secondly, as we can see
from Figure 9 and Table l, which give the computational
times on synthetic data following Models (5) (G), (P) or (B),
HiCseg is computationally efﬁcient, which makes its use
possible on real data coming from Hi-C experiments. Note
that the computational times of Figure 9 were obtained with a
computer having the following conﬁguration: RAM 3.8 GB,
CPU 1.6 GHz and those of Table l with a computer
having the following conﬁguration: RAM 33 GB, CPU
8 x 2.3 GHz.

4.2 Open questions

Our methodology could be extended, both to improve the
algorithmic efﬁciency of our method and the modeling of the
data.

On the one hand, all available approaches work with data
binned at the resolution of several kb. However, the original
data are collected at the nucleotide resolution. One of the main
challenges would be to alleviate the computational burden of the
algorithm to fully take advantage of the Hi-C technology high
resolution. Recent advances in segmentation algorithms for 1D
data, such as those proposed by Killick et al. (2012) or Rigaill
(2010), seem promising for dealing with this issue.

On the other hand, the modeling could be improved in two
directions. First, as observed by Phillips-Cremins et al. (2013),
Hi-C interaction matrices display a hierarchical structure corres-
ponding to regions interacting at different scales. The proposed
segmentation model does not account for such a structure but
could be improved in such a direction. Second, a more reﬁned
modeling of the dispersion could be considered. While assuming
a common dispersion parameter for non-diagonal blocks is

 

i391

112 £10sinuino[pio‘ixo'sounuuo‘iquIq/ﬁdnu moi; papeommoq

9103 ‘Og isanV uo ::

C.Lévy-Leduc et al.

 

 

Time (in seconds)
2
|

 

 

 

I I I I I I
200 400 600 800 1000 1200
n

Fig. 9. Computational times for Model (5) (G) (‘0’), (P) (‘A’) and (B)
(‘0’)

Table 1. Computational times (in seconds) for Model (5) (G), (P) and (B)

 

n 1000 2000 3000 4000 5000 6000 7000

 

(G) 1.96 17.01 60.56 143.68 280.53 513.87 834.01
(P) 1.92 16.47 57.22 134.91 264.15 453.99 755.21
(B) 1.95 16.60 58.07 135.52 264.62 457.15 783.05

 

sensible because the signal is very low (and therefore, there is
little room for large changes in dispersion), the strategy that we
propose could incorporate non-homogeneous dispersion param-
eters for the diagonal blocks. This could be achieved, for in-
stance, by estimating a dispersion parameter per diagonal
block. Note that these two extensions could be implemented in
the same efﬁcient algorithmic framework as the one proposed in
the article. These extensions will be the subject of a future work.

ACKNOWLEDGEMENTS

The authors would like to thank the French National Research
Agency ANR, which partly supported this research through the
ABS4NGS project.

Funding: Part of this work was supported by the ABS4NGS
ANR project (ANR-1 l-BINF-0001-06).

Conﬂicts of interest: none declared.

REFERENCES

Bellman,R. (1961) On the approximation of curves by line segments using dynamic
programming. Commun. ACM, 4, 284.

Boysen,L. et al. (2009) Consistencies and rates of convergence of jump penalized
least squares estimators. Ann. Stats., 37, 1577183.

Darbon,J. and Sigelle,M. (2006a) Image restoration with discrete constrained total
variationipart I: Fast and exact optimization. J. Math. Imaging Vision, 26,
2617276.

Darbon,J. and Sigelle,M. (2006b) Image restoration with discrete constrained total
variationipart II: Levelable functions, convex priors and non—convex case.
J. Math. Imaging Vision, 26, 2777291.

Dixon,J.R. et al. (2012) Topological domains in mammalian genomes identiﬁed by
analysis of chromatin interactions. Nature, 485, 37(r380.

Fraser,J. et al. (2009) Chromatin conformation signatures of cellular differentiation.
Genome Biol, 10, R37.

Harchaoui,Z. and Lévy—Leduc,C. (2010) Multiple change—point estimation with a
total variation penalty. J. Am. Statis. Assoc., 105, 148(kl493.

Hochbaum,D.S. (2001) An efﬁcient algorithm for image segmentation, markov
random ﬁelds and related problems. J. ACM, 48, 6867701.

Killick,R. et al. (2012) Optimal detection of changepoints with a linear computa—
tional cost. J. Am. Statis. Assoc., 107, 159(%1598.

Lavielle,M. (2005) Using penalized contrasts for the change-point problem. Signal
Proc., 85, 150171510.

Lieberman—Aiden,E. et al. (2009) Comprehensive mapping of long—range inter—
actions reveals folding principles of the human genome. Science, 326, 28%293.

Nora,E.P. et al. (2012) Spatial partitioning of the regulatory landscape of the x—
inactivation centre. Nature, 485, 3817385.

Phillips—Cremins,J.E. et al. (2013) Architectural protein subclasses shape 3D organ—
ization of genomes during lineage commitment. Cell, 153, 128171295.

Picard,F. et al. (2005) A statistical approach for array CGH data analysis. BMC
Bioinformatics, 6, 27. www.biomedcentral.com/l471—2105/6/27.

Rigaill,G. (2010) Pruned dynamic programming for optimal multiple change—point
detection. ArX iv, 1004.0887.

 

i392

112 /310'S[BHJHOIPJOJXO'SOIIBLUJOJIIIOICI”Idllq uioii papeorumoq

9103 ‘Og isanV uo ::

