Bioinformatics, 32(11), 2016, 1716—1723
doi: 10.1093/bioinformatics/btw029
Advance Access Publication Date: 30 January 2016

 

Original Paper

 

Gene expression

FastLSU: a more practical approach for the
Benjamini—Hochberg FDR controlling procedure
for huge-scale testing problems

Vered Madar1 and Sandra Batista2'*

1Statistical and Applied Mathematical Sciences Institute, Research Triangle Park, NC 27709, USA and 2Department
of Computer Science, Princeton University, Princeton, NJ 08540, USA

*To whom correspondence should be addressed.
Associate Editor: Ziv Bar-Joseph

Received on 24 July 2015; revised on 21 December 2015; accepted on 15 January 2016

Abstract

Motivation: We address a common problem in large—scale data analysis, and especially the field of
genetics, the huge—scale testing problem, where millions to billions of hypotheses are tested to—
gether creating a computational challenge to control the inflation of the false discovery rate. As a
solution we propose an alternative algorithm for the famous Linear Step Up procedure of

Benjamini and Hochberg.

Results: Our algorithm requires linear time and does not require any P—value ordering. It permits
separating huge—scale testing problems arbitrarily into computationally feasible sets or chunks.
Results from the chunks are combined by our algorithm to produce the same results as the control—
ling procedure on the entire set of tests, thus controlling the global false discovery rate even when
P—values are arbitrarily divided. The practical memory usage may also be determined arbitrarily by

the size of available memory.

Availability and implementation: R code is provided in the supplementary material.

Contact: sbatista@cs.pri nceton.ed u

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

In many fields the substantially increased scale of data available has
resulted in a significant increase in the size of multiple hypotheses
testing problems. In genetics, in particular, typical GWAS studies
consist of 105 7 106 SNPs (Hindorff et (11., 2015) while eQTL stud-
ies (Wright et (11., 2014; Xia et (11., 2012), newly advanced methyla-
tion studies (Smith et (11., 2014), and imaging studies (Stein et (11.,
2010) usually start with 109 tests. These testing problems are huge-
scale as opposed to large-scale used by Efron (2004) to describe
studies consisting of hundreds to thousands of hypotheses. It is pref-
erable to control the false discovery proportion rather than the num-
ber of false positives for a huge-scale testing problem. Therefore the
PDR or the pPDR approaches are favored and both tend to offer
larger, more powerful sets of results than those yielded by the con-
servative PWER control. These huge-scale multiple hypotheses

testing problems create numerous computational challenges when
many tests, say of the order 106, are performed with all of the P-val-
ues of more or less equal importance. As a result some simpler test-
ing procedures such as rigid P-value thresholds may be used that
sacrifice power and correctness.

Alternatively tests may be separated or chunked into smaller sets
or chunks that are more computationally feasible. Efron (2008)
notes that the problem of separating hypotheses tests has not
received great attention and warns of some pitfalls in chunking
P-values, but focuses on grouping tests that share a biological prop-
erty rather than arbitrary, computationally feasible chunks. Cai and
Sun (2009) and later (Benjamini and Bogomolov, 2014) propose al-
ternative solutions to Efron’s grouping problem but do not address
the problem of arbitrary, computationally feasible chunking. We
confront the computationally feasible chunking problem for the

(C7 The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1716

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

FastLS U

1717

 

Benjamini—Hochberg false discovery rate (Benjamini and Hochberg,
1995). We show on data from Stranger’s HapMap study (Stranger
et 41., 2007; Xia et al., 2012) that if results from separate tests are
not combined correctly, there is considerable inflation of type I
error, offer an explication for this occurrence, and propose our algo—
rithm as a solution.

Consider a huge—scale testing problem of size m where our goal
is to select exactly R 2 0 significant tests. Of the R significant dis—
coveries, exactly V 2 0 tests will be false discoveries (i.e. truly non—
significant tests that are declared significant). A common approach
in multiple hypotheses testing problems is to control the family—wise
error rate, FWER : Pr(V 2 1), the probability of selecting at
least one false discovery. For huge—scale testing a more favorable
alternative is to control the false discovery proportion,
FDP : V/max(R,1), the proportion of truly false tests among the
significant R. Some will prefer to control the positive FDR,
pFDR : E(FDPlR > 0), the expectation of the FDP when significant
tests are selected, while others will opt to control the false discovery
rate, FDR, the expectation of the FDP, E(FDP). The FDR is always
of a potentially smaller magnitude than the FWER and of the pFDR
(FDR:pFDR-Pr(R > 0)). Yet, in reality for huge—scale testing,
FWER >> FDR and sometimes, pFDR z FDR. Therefore both FDR
and pFDR control approaches tend to offer larger, more powerful
sets of results than those that might be offered by the conservative
FWER control. For a further discussion about FWER, FDR and
pFDR refer to Farcomeni (2004).

In huge—scale testing when the m P—values are partitioned into
Chunks, it is challenging to control the FWER, pFDR or FDR over
the entire collection of m P—values. Controlling these error rates on a
per chunk basis, if not done correctly, may interfere with the overall
results by introducing more false discoveries. Although the same dif—
ficulty arises for the FDR and pFDR control (Benjamini and
Bogomolov, 2014; Efron, 2008), it is easier to illustrate this for the
FWER. Consider, for instance, an example of FWER control using
the Bonferroni approach by collecting all P—values less than 
Assuming that the number m of P—values happen to be very large so
that m should be divided into k chunks, each of of size m, so that
m :  mi. Applying Bonferroni in chunks of size m, will tend to
select more significant results than applying it over the entire set of
m : 2m,- P—values since oc/m is less than oc/mi. In the case of
F WER control, using a fixed bound of oc/m for all the chunks is the—
oretically preferred but often yields no significant results. A stricter
constant cut—off on all sets of tests as suggested by Dudbridge and
Gusnanto (2008) for GWAS was developed based on results from
simulated GWAS. However such an ad—hoc approach eliminates
from the entire multiple hypotheses testing problem any knowledge
of the actual significance level or used.

1.1 The Benjamini—Hochberg linear step up procedure
for controlling the false discovery rate
The Benjamini—Hochberg Linear Step Up (LSU) procedure is de—
signed to control the FDR at desired signficance level or (Benjamini
and Hochberg, 1995 ). As a result for huge—scale multiple hypotheses
tests of equal importance, controlling a proportion of false discov—
eries, especially on the average, has increased power over procedures
that control the FWER such as Bonferroni or a rigid cut—off bound
such as 5 X 10—8 suggested for GWAS (Dudbridge and Gusnanto,
2008). The larger the multiple hypotheses testing problem is the
more powerful the LSU is over procedures that control the FWER.
While the LSU procedure is still one of the most cited proced—
ures, its application had required sorting all P—values in decreasing

order to look for the largest P—value that satisfies a simple condition.
In face of a huge—scale testing problem rather than apply LSU, some
researchers had preferred to use harsh P—value cut—offs as mentioned
above or to divide their huge—scale set of tests into computationally
feasible smaller chunks and apply a multiple hypotheses testing pro—
cedure on each chunk selecting as the final significant results the
union of the results in each of the chunks. Efron (2008) warns of the
danger in such aggregation from the perspective of pFDR, pointing
out that some chunks might have a larger proportion of significant
results than others, and aggregating the significant results can yield
misleading estimates. Moreover different chunking of tests might
yield different sets of significant results. Analysis done by different
groups of populations or chromosomes may not give the same num—
ber of significant tests as analysis that is applied on equal sized sub—
sets; for example, it is well known that chromosome 6 (Mungall
et 41., 2003) has a higher proportion of significant HLA SNPs
than other chromosomes. We shall show in Section 2 that sorting
the P—values, arbitrary thresholds, and arbitrary aggregation of re—
sults are not necessary and do not improve computational time effi—
ciency compared to our algorithm.

1.2 A faster algorithm for LSU

Our alternative algorithm to the Benjamini—Hochberg LSU, FastLSU,
performs linear scans instead of sorting P—values, but takes into ac—
count the overall size of the testing problem. FastLSU tiles the LSU
procedure to give one global set of results that does not differ from
applying LSU to the entire set of tests. Our algorithm addresses the
same objective function as the original LSU. Our approach is provably
faster than the conventional approach that relies on sorting P—values.
It may also be used on arbitrary chunks of arbitrary size with an arbi—
trary space constraint in order to return the same set of significant re—
sults as those from applying LSU to the entire set of tests.

In the following section we address the difference between
grouping and chunking tests and the difficulty in arbitrarily chunk—
ing tests by giving examples of inflation of type I error. In Section 3
we present FastLSU on a single set of tests and prove its equivalence
to LSU, time efficiency and space efficiency. We present FastLSU on
arbitrary chunks and also show its correctness and efficiency in
Section 4. We offer suggestions for finalizing the report of significant
tests in Section 5 and conclude with discussion in Section 6. R Code
for an implementation of the algorithm is given in the supplemen
tary materials.

2 An exercise of three HapMap groups and their
chunking for stranger's cis-eOTL study

FastLSU controls the global FDR, not a family—based bound or a
group—based bound. Our final results are not affected by the proced—
ure of separating tests. This is an important distinction because
chunking is arbitrary and based primarily on computational effi—
ciency without any consideration for relationships between the
hypotheses being tested. This is in contrast to group—testing proced—
ures where for example, hypotheses may be grouped based on ex—
perimental knowledge such as all the tests from the same
chromosome, the back and front parts of the brain (Efron, 2008), or
different population groups in HapMap (Stranger et al., 2007). In
Section 3.2 we will show how the FastLSU algorithm can even im—
prove the efficiency of a group—based controlling procedure.
FastLSU is particularly suited to the current applications in genetics
because we typically seek the significant set of SNPs or genes, and
the family structure is usually of less importance than managing the

[310'sp2umofp105xo'sopeuHOJIItotq/ﬁdnq

1718

V. Madar and S. Batista

 

computational burden. Genetic family structures typically do not re—
quire any correction for family selection because each genetic family
tends to contribute significant results of its own. (This is the case for
the following example of 3 families of HapMap). For this reason in
the remainder of this section, we will give motivating examples of
applying FastLSU on the three groups of approximately 14 X 106
cis—eQTLs of Stranger’s HapMap study (Stranger et al., 2007; Xia
et al., 2012) in order to demonstrate the problems with arbitrary
chunking without combining the results as FastLSU does.

Stranger et al. (2007) presents an eQTL study over 4 HapMap
population samples: 30 Central Europeans(CEU), 45 Chinese
(CHB), 45 Japanese(]PT) and 30 trios from Nigerians(YRI). To in—
crease power each group is analyzed separately. We follow the rec—
ommendations of the SeeQTL website (Xia et al., 2012) and
consider the CHB and JPT together. We define cis—eQTLs as within
1 Mb upstream or downstream of a gene. Each population has a set
of approximately 14 million P—values that were split into Chunks of
the following sizes: 1M, 900K, 800K, 700K, 600K, 500K, 250K,
100K, 50K, 25K, 10K and 5K. We contrast the differences in the
number of significant results when the results are combined by tak—
ing the union of the results of LSU on each chunk versus those se—
lected by FastLSU Algorithm 2 in Figure 1.

Applying the FastLSU yields 9228 significant results for CEU,
6497 for YRI and 33 507 for the CHB & JPT group. Most notable is
that the results for FastLSU Algorithm 2 do not change across the
chunk sizes whereas the alternative of taking the union of the results
on each chunk increases not only the number of significant tests se—
lected, but also the maximal P—value reported as the chunk size de—
creases. For example, applying FastLSU at level 10% for the complete
chunk of approximately 14M P—values yields 9228 discoveries for the
CEU. However, applying LSU on 1370 chunks of size 10K yields 16
925 significances which is equivalent to an overall FDR control of
23.75%. When 10K sized chunks are used for the CHB—JPT group,
41 587 P—values are selected as significant and this would require
overall FDR control of or : 15.58% For the YRI group when 10K
sized chunks, there are 10 330 significant P—values and an or : 21.1%
would be required for this overall FDR control. This implies that per—
forming the analysis using 1300—1400 chunks of size 10K rather than
a single chunk inflates the type I error by 50%. While this is compel—
ling experimental evidence, a more compelling explanation is that the

Significance 10% LSU testing of Stranger’s HapMap cis—eQTLs

 

o CEU(chunkmg) A
A cHBJPrrcmnkmg)
+ YRl(chunkmg)
r CEu(Fas|LSLl)
- - - - cHBJPrrFalesu)
- VR|(Falesu)

40000
|

AAAAA A

 

30000
|

# significant p—values in total

20000
|

 

10000
l
o
+

 

 

 

 

_16 —14 —12 —10
—log(chunk size in Kilobyte)
Fig. 1. Varying chunk size over Stranger's HapMap populations. Dashed lines

show the consistent result of applying the FastLSU as in Algorithm 2. The tri-
angle, dot, and plus shapes represent results of applying the LSU under 10%

union of partial orderings on subsets of a set is in general not equal to
the partial ordering on the entire set.

Moreover when the size of the chunks decreases, the significance
interval is being divided into larger sub—intervals with each more
likely containing more P—values spanning a greater range of values,
so that the set of selected significant P—values will more likely con—
tain a larger cut—off of P—values, and therefore a considerably larger
value for the maximal significant P—value. The maximal value of sig—
nificant P—values we obtain for the case of a single chunk is 6.7><
10—5 for CEU, 4.4 X 10—5 for YRI and 2.5 X 10—4 for CHE—JPT.
For chunks of the size 100K (about 13 to 14 chunks), the max
P—values are slightly higher especially for the CHB—JPT group: 1.7><
10‘4 for CEU, 3.5 X 10‘4 for YRI and 1.2 X 10‘3 for CHE—JPT.
For chunks of 10K size (about 130—140 chunks) the maximal
P—values are considerably higher still: 0.0063 for CEU, 0.0034 for
YRI and 0.011 for CHB—JPT. As we mentioned, there is no need to
apply any correction for group or family selection as required by
Benjamini and Bogomolov (2014) since all three groups in the
HapMap example give significant results.

3 The FastLSU algorithm

The usual way to apply the LSU (Benjamini and Hochberg, 1995) at
level 0 S or S 1 is to sort the P—values in descending order,
p(,,,) 2 p(m_1) 2 - -- 2 pm. Starting from the largest P—value to the
smallest, we need to look for the kth largest P—value that satisfies
pug) <koc/m. One may view the LSU algorithm (Benjamini and
Yekutieli, 2001) as a search for optimal index r

r : argmax{i : pm <  (1)

This observation motivates the following Fast Linear Step Up
(FastLSU) algorithm that controls the FDR at level or:

 

Algorithm 1. FastLSU for a single batch — without sorting the
P—values

 

Step 1. Start with ro : m and count all P-ualues < root/m; let
r1 be their count.

Step 2. Do for k : 2. . . . .m Count all P—ualues < rk_1oc/m; let
rk be their count.

Step 3. Repeat step k for k : k + 1 until rk : rkH or k :m.

 

Example 1. We demonstrate Algorithm 1 on the example appear-
ing in the original 1995 LSU paper (Benjamini and Hochberg,
1995) with or : 0.05. Consider the 15 P-ualues:

0.6528. 0.7590. 0.0298. 0.4262. 0.0459. 0.0278. 0.0001. 0.0019.
0.0004. 0.0201. 1.0000. 0.5719. 0.3240. 0.0095. 0.0344

At the first step, we find 9 P—ualues< 0.05. That is, r1 : 9. At
the second step, r2 : 7, 7 P—ualues are < 9 - 0.05/15 : 0.03. At the
third step, r3 : 5 P—ualues are < 7- 0.05/15 : 0.2333. We stop
with r4 : 4 P—ualues that satisfy < 4 - 0.05/15 : 0.01333. The se-
lected P—ualues are 0.0001.0.0019.0.0004.0.0095, and the reader
can check that these 4 P—ualues are exactly the ones selected using
the original LSU P—ualue sorting algorithm.

3.1 The equivalence to LSU and computational
efficiency

THEOREM 1. For a significance level or, the Algorithm 1 maximizes
the same objective function (1) that is used by the LSU. Therefore,

ﬁm'srcumol‘pquo'sopcuuowtotq/ﬁdnq

FastLS U

1719

 

the FastLSU controls the FDR at level or and gives the same selected
set of significant results as would be obtained by applying the
Benjamini—Hochberg LSU FDR controlling procedure (Benjamini
and Hochberg, 1995 ).

PROOF. Consider a batch of P—values which we will denote as C.
Let 0 S t S 1 and define 8(t : C) : {p < t : p is P—value E C} to be
the number of P—values from C smaller than t. The set 8  consists
of all P—values from C that are smaller than  Hence, for the case of
a single batch, it is possible to verify that the search for

r : argmax{i 2 i:  (2)

is equivalent to looking for the largest rth P—value satisfying
pm 3 r—ﬂ’f, as requested by (1).

THEOREM 2. The FastLSU Algorithm 1 requires O(m) time and
O(m) space where m is the number of P—values to be considered.

The proof to Theorem 2 is given in the Appendix A with ac—
companying pseudocode for procedural language implementa—
tions. The main observation behind the linear time algorithm is
that it is a constant time check where, in terms of what proportion
of units of size oc/m, a P—value is relative to at. It is then only a single
scan to count the number of P—values that fall within each range
relative to or and to find the range of P—values that satisfy the LSU
condition. This is not commonly how statisticians consider LSU
because we are not comparing the P—values directly to each other;
we are comparing them by examining the range in which they fall
relative to or and only implicitly to each other. We demonstrate the
linear scans used in the linear algorithm in the following example.

Example 2. To demonstrate this equivalence we apply FastLSU
on the example appearing in the original 1995 FDR paper
(Benjamini and Hochberg, 1995) with or : 0.05. Instead of sorting
all 15 P-values we apply Algorithm 1 using 3 linear scans and find 4
significant P—values. Consider the 15 P—values from the example
given in the original LSU paper (Benjamini and Hochberg, 1995)
from Example 1. In the first linear scan, we label each P-value with
what interval k, k :  such that the P—value is at most koc/m re-

spectively as follows:
+7+797+7 147 97 17 17 1777+7+7 +7 37 

For P—values greater than or, a lahel+is used. During the same
linear scan, we can also maintain counts for the number of P—values
labeled with k from 1 to m : 15 as follows:

3.0.1.0.0.0.1.0.2.0.1.0.0.1.0

In the second linear scan, we check for k from m : 15 to 1 if the
the number of significant P—values remaining, 9 in this case, minus
the number of P—values in ranges examined so far equals to k as
follows:

9.8.8.8.7.7.5.5.4.4.4.4

This occurs when k : 4. On the third and final scan, we return
as significant those P—values with interval, r S 4. The selected
P—values are 0.0001. 0.0019. 0.0004. 0.0095 as in Example 1.

3.2 Applying FastLSU over families or groups of
P—values of equal relevance

Efron (2008) distinguishes between the groups of imaging P—values
that arise from the front brain and the back brain, and develops an
empirical Bayes set—up to combine the significant P—values from
the two families. Benjamini and Bogomolov (2014) extended
Efron’s idea of groups into the voxel families of MRI where each
set of P—values from a specific voxel of a certain location are

treated as a separate, homogeneous group of relevance. For the
first step each voxel group is analyzed by applying the LSU at level
or. For the second step the number of significant groups is collected
and LSU is performed again with another significance level 51* to
correct for the selection of groups. If, for example, S groups out of
G are shown to have at least one significant P—value for LSU of
level or, 01* is set to Soc/G for each of the groups (Benjamini and
Bogomolov, 2014).

Given the equivalence between LSU and FastLSU, FastLSU may
be used for each step in the approach of Benjamini and Bogomolov.
The second step needs only be done for groups that have at least one
significant result. Since only significant P—values for each group need
to be considered, FastLSU may be applied to each group by
using or” : $53—53
P—values of which rg(oc) were declared significant of level or. Since
FastLSU achieves efficiency by the simple recalling and correction of

or where each group g (g : 1.2.. . . . G) contains mg

the constants m (or mg) for the number of tests, a similar approach
also works for Storey—Efron’s positive FDR approach (Efron, 2004;
Storey, 2002). To be more precise, the complete grid of /l values
should be collected in each step and then the q—values should be cor—
rected accordingly.

4 Applying LSU on arbitrary chunks of P-values

Algorithm 1 may be extended for when P—values are divided into an
arbitrary number of chunks of arbitrary size. The size of the chunks
may be determined by practical memory constraints or any other cri—
teria. The iterative steps are applied on each chunk. The significant
results from each chunk are combined using another iterative step to
form the final set of significant results.

 

Algorithm 2. FastLSU for the case of two or more chunks of
P—values

 

Given a set of m tests and P—values divided into n separate chunks

of sizes In, P—values for i : 1. ....n such that m :  mi:

0 Step 1. On each of the chunks, count the number of
P-values less than or. Denote this count as rill).

0 Step k +1. On each of the chunks (starting from k :1)
count the number of P—values less than  A“) or /m,
and let A”) be the count for the chunk at this step.

0 Repeat Step k + 1 until k + 1 : m or  r512“) :

221:1 rgk). Mark these P—values as signiﬁcant under LS U.

 

An example of Algorithm 2 applied to the example appearing in
the original 1995 FDR paper (Benjamini and Hochberg, 1995) is
given in the Appendix A.

There are many ways one can alter Algorithm 2. For example,
P—values that do not satisfy the condition in the iterative step that
preserves the LSU may be either flagged or dropped. Alternatively
Algorithm 1 may be applied to each chunk independently starting
with the initial value for re : m — m,- + r51) and still tiling by the
total number of tests, m. The advantage of this is that this may be
done in parallel if desired. The results of the chunks may then be
combined into a single set, if space permits, and Algorithm 1 applied
to this set to return the final set of significant results. If space does
not permit the results of the chunks to be combined, they may be
combined up to the space limit. Then the iterative step of Algorithm
2 above may be applied to the resultant chunks starting by checking
for P—values less than m’ oc/m where m’ is the size of the union of the
results.

ﬁm'srcumol‘pquo'sopcuuowtotq/ﬁdnq

1720

V. Madar and S. Batista

 

THEOREM 3. For a significance level or, and a collection of c
chunks the Algorithm 2 gives the same selected set of significant re—
sults as would be obtained by applying the Benjamini—Hochberg
(Benjamini and Hochberg, 1995) LSU FDR controlling procedure
over the set of all P—values from the c chunks.

PROOF. [Proof for Algorithm 2] Suppose we have exactly c disjoint
chunks of P—values C1.C2. . . . .CC, with sizes m, (i: 1.2.... .c) such
thatC : Uf:1Ci and (C) : m : 2.21 mi P—values. Let rc be the number
of selected significant P—values that satisfy the LSU objective (1):

rczargmax{r:r:#5(g 15)) (3)

Algorithm 2 can be applied to each c chunks and the last step of
Algorithm 2 is to finalize the selection. Accordingly for chunk C, we
search for the largest rf that satisfy the objective:

r: : argmax{r : r : #8 <—(7 + m _ min 2C.)  (4)

m
Weclaimthat
rcSrf+m—m,~. foranyi:1.....c (5)

This implies that each of the P—values selected significant from
the search for (3) within chunk Ci must be selected while searching
for argmax r: as in (4) since p 3 re - oc/m implies p g
(r: + m — 

It is possible to verify that paw) the kth P—value in chunk C,- can—
not be larger than, p<k+m_m,), the (k +m — mi) th P—value in the

union C:
Page.) S Poem—m.) for all]? I 17 - - - 7mi- (6)

In particular, by the condition (4), let pm) be the largest P—value
in C, that satisfies ppm) < (rf + m —  When m,- > rf, it fol—

*

lowsthat,fork : 17m7mi _,,i’
(r: + k + m — mJa/m < my,“ 3 pvﬂkﬂwmm (7)

(When m,- : rf (5) holds trivially since at most the entire set of tests
may be selected as significant). To prove (5 ) by contradiction, let us
compare between pm.) to p<rt+k+m_m,). If we assume that

re > rf + m — m,, then we can define a positive integer k : rc — (r,-

+m — m,-) for which following the inequality in (7) holds,
rc - oc/m :(rf +k +m — 
< meme.)

3 p(rf+k+m—mi) : p(rc)‘

However, this is in contradiction to the condition (3) that provides
pm.) < 70 ' 

In conclusion we show that applying FastLSU over the global
union of P—values give the exact same selection of significant
P—values. We search for ré over the union of the results on each
chunk of size 2.21 rf

* TE ' 9‘ i l
rc : #8<728(r12C1)U---U8(rC:CC)>.

Since the inequality (5) ensures that P—values that were not se—
lected under the condition (4) would have not been selected under
the condition (3), we conclude that ré : rc. (II

THEOREM 4. The FastLSU Algorithm 2 requires O(m) time and
O(m) space where m is the number of P—values to be considered.
The practical memory usage may be restricted to an arbitrary limit
for the largest chunk size, m".

Proof for the running time and space limitations are shown in
the Appendix A.

5 Useful tips for finalizing the report of
significant P-values

If we assume that the R tests were declared significant by applying
either the LSU or FastLSU, we offer tips for finalizing the set of sig—
nificant P—values. In the remainder of this section we explain how to
protect the FDR control of FastLSU against dependency structures
and when such correction is actually needed. We will also explain
how to compute q—values (adjusted P—values) for the final results
without keeping the entire set of P—values and show how to add a
set of simultaneous confidence intervals for the significant test statis—
tics while accounting for the selection effect.

5.1 Correcting against general case of dependence

The LSU procedure is conservative under the general type of positive
regression dependence on subsets (PRDS) (Benjamini and Yekutieli,
2001), so applying the LSU at significance level of or always ensures
FDR g or for PRDS P—values. The PRDS class contains a larger set of
structures, among them, the independent case and any positively
associated P—values such as P—values obtained from a two—sided t
test. Since the LSU and FastLSU are equivalent, applying the
FastLSU at level or on PRDS P—values will control the FDR at level—or,
as well.

For other types of non PRDS dependency, such as in the case of
pairwise comparisons, it is recommended to use the Benjamini—
Yekutieli procedure (Benjamini and Yekutieli, 2001) that applies
LSU using 01* : oc/c instead of or where c : 221:1 1/k z
In m + 0.5772. By the same argument the FastLSU under non PRDS
will have FDR g or when applied under 01* : m. As a matter
of practice, we suggest to first perform FastLSU using the signifi—
cance level or. Then, if the P—values are non PRDS, correct the R se—
lected P—values by applying FastLSU again over the single batch
consisting of R P—values using or” : %. This approach provides
FDR g or since 01* < or.

5.2 How to compute q—values to a selected subset of
significant tests

When it is preferable to report q—values or adjusted P—values, we
suggest how this may be done more efficiently. If we assume that R
tests were selected as significant, let the P—value pm) be the largest
P—value satisfying p(R)m/R < or. All P—values larger than pm)
were not selected as significant since p(,-)m/i > or for i > R and have
q—values > or. Therefore it is sufficient to consider only the set of se—
lected R P—values. The LSU q—value, am, for the P—value, pg) has the
form (Yekutieli and Benjamini 1999)

qm : j:R.R1rl11nui+1{p(,-)m/i.qm}. fori < R 

q(R) : P(R)m/Ra for i : R.

From this we can see that the algorithms presented by (Yekutieli
et al., 1999) and (Storey, 2002) are O(RlogR). One needs only sort
the R selected P—values in descending order and then beginning from
largest P—value assign the corresponding q—value in a final linear
scan recording the minimum q—value assigned thus far. The q—values
will also be descending assigned in this way and there is no need to
compare previous values except the minimum q—value thus far. The
value for the q—value for qm will only change when it is less than the
minimum seen thus far and the minimum q—values will span a range

ﬁm'srcumol‘pquo'sopcuuowtotq/ﬁdnq

FastLS U

1721

 

of P—values until either a sufficient number of P—values have been
covered or a sufficient range in P—values have been covered.
Alternatively, one can use existing procedures (such as the function
p.adjust in R software or PROC MULTTEST in SAS) to compute
adjusted P—values for the set of R selected P—values and multiply the
results by R/m to correct them.

5.3 Confidence intervals for selected subset of
significant results

A less common approach in genetic studies is to report the signifi—
cant test results by constructing a set of confidence intervals for the
test statistics. While P—value is merely a measure of the magnitude of
the test statistic, a confidence interval may offer the additional infor—
mation about the dispersion of that magnitude. The selection ad—
justed confidence intervals (Benjamini and Yekutieli, 2005) offer an
appropriate construction that corrects against the false coverage ef—
fect of selection. For a useful example see (Jung et al., 2011) for how
this method is used for the significant log—fold changes of RNA
Microarrays.

6 Discussion

We presented an efficient algorithm to apply correctly the Benjamini—
Hochberg Linear Step Up FDR controlling procedure in a huge—scale
testing problem. Since we have shown that our algorithm requires
only linear time, our algorithm is provably not any more computa—
tionally burdensome than even using a rigid Bonferroni cut—off for
control. However, unlike the rigid Bonferroni cut—off, our approach
ensures the FDR control at level or and this is a more powerful alterna—
tive to controlling the FWER, especially when the multiple testing
problem is of huge—scale. Our approach is also scalable to any subset—
ting or chunking of the overall subset of P—values.

In addition we offered tips for performing the LSU over a huge—
scale multiple hypotheses testing problem, such as showing how to
correct for dependency or how to compute q—values directly from
the subset of LSU significant results. We hope that these algorithms
and tips offer better insight into the huge—scale testing problem ra—
ther than just black—box solutions. We encourage altering the steps
in performing Algorithm 2, for instance, either by applying it se—
quentially or in parallel or a mixture of both.

The amount of inflated type I error we observed during the exer—
cise on different chunk sizes of the HapMap populations strongly
suggests the need for greater diligence in correctly separating
hypotheses tests, whether the objective is to control the FWER, the
FDR, or pFDR. This observation was also reported by Efron (2008),
but a full investigation on real data with decreasing chunk sizes was
not performed. Our suggested approach solves this for the case of
the Benjamini—Hochberg FDR. As we have alluded, a similar ap—
proach can be adopted to control correctly the Efron—Storey’s pFDR
(Storey, 2002) and this remains open for further research. In add—
ition the severe phenomena of inflated type I error we observed sug—
gests more care may be required in reporting and interpreting results
in genetics literature especially in the case of GWAS and eQTL stud—
ies. Namely to properly understand the significance results there is a
need for consistent consideration of the algorithms or software used
for controlling and separating the hypotheses tests and for recording
the chunk sizes used for a study.

Acknowledgements

The authors would like to thank to the referees and wish to thank their COI-
leagues for their many helpful comments. Special thanks are reserved for Dr.
Kai Xia for his great help with the HapMap data.

Funding

This material was based upon work partially supported by the National
Science Foundation under Grant DMS-1127914 to the Statistical and Applied
Mathematical Sciences Institute. Any opinions, ﬁndings and conclusions or
recommendations expressed in this material are those of the authors and do
not necessarily reﬂect the Views of the National Science Foundation.

Conﬂict of Interest: none declared.

References

Benjamini,Y. and Bogomolov,M. (2014) Selective inference on multiple fami-
lies of hypotheses]. R. Stat. Soc. Ser. B, 76, 297—318.

Benjamini,Y. and Hochberg,Y. (1995) Controlling the false discovery rate: a
practical and powerful approach to multiple testing. ]. R. Stat. Soc. Ser. B,
57, 289—300.

Benjamini,Y. and Yekutieli,D. (2001) The control of the false discovery rate in
multiple testing under dependency. Ann. Stat, 29, 1165—1188.

Benjamini,Y. and Yekutieli,D. (2005) False discovery rate—adjusted multiple
conﬁdence intervals for selected parameters. ]. Am. Stat. Assoc, 100, 71—93.

Cai,T.T. and Sun,W. (2009) Simultaneous testing of grouped hypotheses: ﬁnd-
ing needles in multiple haystacks.  Am. Stat. Assoc, 104, 1467—1481.

Dudbridge,F. and Gusnanto,A. (2008) P-Value less than say 5 X 10’8 can be
regarded as convincingly signiﬁcant. Genet. Epidemiol, 32, 227—234.

Efron,B. (2004) Large-scale simultaneous hypothesis testing: the choice of a
null hypothesis.  Am. Stat. Assoc, 99, 96—104.

Efron,B. (2008) Simultaneous inference: when should hypothesis testing prob-
lems be combined? Ann. Appl. Stat, 2, 197—223.

Farcomeni,A. (2004) Multiple Testing Procedures Under Dependence, With
Applications, Ph.D. Thesis, Dipartimento di Statistica, Probabilita’ e statis-
tiche applicate, Universita’ di Roma “La Sapienza”.

Hindorff,L.A. et al. (2015) A Catalog of Published Genome-Wide Association
Studies. Available at: www.genome.goV/gwastudies. (11 January 2015, date last
accessed).

Jung,K. et al. (2011) Reporting FDR analogous conﬁdence intervals for the log
fold Change of differentially expressed genes. BMC Bioinf., 12, 288.

Mungall,A.]. et al. (2003) The DNA sequence and analysis of human chromo-
some 6. Nature., 425, 805—811.

Smith,A.K. et al. (2014) Methylation quantitative trait loci (meQTLs) are con-
sistently detected across ancestry, developmental stage, and tissue type.
BMC Genomics, 15, 148.

Stein,].L. et al. (2010) Voxelwise genome-wide association study (VGWAS).
Neuroimage, 53, 1160—1174.

Storey,]. (2002) A direct approach to false discovery rates. ]. R. Stat. Soc. Ser.
B, 64, 479—498.

Stranger,B.E. et al. (2007) Population genomics of human gene expression.
Nat. Genet., 39, 1217—1224.

Wright,F.A. et al. (2014) Heritability and genomics of gene expression in per-
ipheral blood. Nat. Genet., 46, 430—437.

Xia,K. et al. (2012) seeQTL: a searchable database for human eQTLs.
Bioinformatics, 28, 451—452.

Yekutieli,D and Benjamini,Y. (1999) Resampling—based false discovery rate control—
ling multiple test procedures for correlated test statistics. ]. Stat. Plan. Inference,
82, 171—196.

ﬁm'srcumol‘piqxo'sopcuuowtotq/ﬁdnq

1722

V. Madar and S. Batista

 

A1. PROOFS AND CODES

A1.1 Proof of theorem 2

PROOF. [Proof of Theorem 2] We present the algorithm here in three
steps with running time for each step and accompanying
pseudocode.

Bin. Classify all P—values into the bins of the interval, [0. or] each of
size 1/m and keep a total of all P—values with value less than or, m".

Label each P—value with a value k such that k :  or k : k + 1

ﬁ
if equals  where or is the signiﬁcance level and p" is the given

P—value being labeled. If p" is labeled with bin k less than or equal to
m, increment the count for bin k and increment current P—value
count, m". If a p" has label k greater than m, it may be ﬁltered, so no
counts need to be incremented for such P—values (although they can
be labeled with arbitrarily large values for k). Running time and
memory: Labeling each P—value and incrementing the labeled bin
count requires only constant time and a single pass through the
P—values. In addition to storing the P—values, the P—value labels and
bin counts also need to be stored, also requiring O(m) space each,
and a variable for the current P—value count, m" .

Accumulate. In this step we ﬁnd the r" the signiﬁcant bin to return
all P—values in bins less than or equal to this bin as signiﬁcant. To
do so, starting from the highest labeled bin’s count, i.e. for m, keep
a partial sum of the total number of P—values in the bins thus far. If
the current bin’s has a non—zero bin count and its value is equal to
m" minus then the current partial sum, then return the current bin
as r" the signiﬁcant bin. Running time and memory: This step can
be done in a single scan of the bin counts and only requires add—
itional variables for the signiﬁcant bin, r" and the partial sum.

Return. Return as signiﬁcant all the P—values that were labeled
with a k less than or equal to r". Running time and memory: This
requires only a single scan of the P—value labels and no additional
space.

A1.2 Pseudocode for proof of theorem 2

Let p_vals [m] be an array of P—values ,
p_val_labels [m] be array of bin labels for P—values ,
count_bins [min] be an array of bin counts ,
m the total number of P—values , and alpha the
significance level.
Initialize p_val_labels and count_bins to O.
sig_pvals : 0; #number of significant P—vals
for i : l to m {
p* :p_vals [i];
k : ceil (mp*/alpha) ; #Bin
if (ceil (mp*/alpha) equals mp*/alpha) increment k;
p_val_labels [i] :k;
if (k less than or equal to m) {
increment count_bins [k] ;
increment sig_pvals;

}
}
r* : 0; #significant bin
total_p_vals : O; # partial sum of p_values
for i :m to l {
add count_bins [i] to total_p_vals] ; #Accumulate
if (sig_pvals — total_p_vals equals i) {
r* : i and break;

}
}

for i : l to m {
if (p_val_labels [i] less than or equal to r*)
#Return
return p_vals [i] as signficant;

A1.3 Example for Algorithm 2

Example 3. To demonstrate the Algorithm 2 consider the 15
P—values appearing in Example 1 and also in the original LSU paper
(Benjamini and Hochberg, 1995). Further assume that for some rea—
son the 15 P—values are divided into two chunks. The ﬁrst chunk,
say C1, consists of the ﬁrst 8 P—values:

C1:{0.6528.0.7590.0.0298.0.4262.0.0459.0.0278.0.0001.0.0019}.

and the second chunk, C2
C2 : {0.0004.0.0201.1.0000.0.5719.0.3240.0.0095.0.0344}.

Algorithm 2 applied for or : 0.05 has three steps: In Step 1, we scan
the 8 P—values in C1 and seek for the largest P—value, p(,-;C1) satisfying
< (i + 15 — 8)oc/ 15. This can he done in a similar manner to applying
Algorithm 1. First scan gives 5 P—values < (8 + 15 — 8)0.05/
15 : 0.05. Second scan gives 4 P—values < (5 + 15 — 8)0.05/15
: 0.04 and stops with the 4 P—values < (4+ 15 — 8)0.05/15 :
0. 0367. The selected P—values for the ﬁrst step are

8(0.0367 : C1) : {0.0298.0.0278.0.0001. 0.0019}.

In Step 2, we scan the 7 P—values in C2 and seek for the largest
pWZ) < (i + 15 — 7)0.05/15. First scan gives 4 P—values < (7+ 15
—7)0.05/15 : 0.05 and immediately stops with these 4 P—values
< (4 + 15 — 7)0.05/15 : 0.04. The selected P—values are

8(0.04 : C2) : {0.0004. 0.0201.0.0095. 0.0344}.

Next, we follow the last step of Algorithm 2 which is a comhin-
ation step and applies on the collection of 8 P—values selected at the
former steps:

{0.0298. 0.0278. 0.0001. 0.0019. 0.0004. 0.0201. 0.0095. 0.0344}.

All 8 selected P—values are clearly<0.05. Second, 5 P—values
< 8 - 0.05/15 : 0.0267, and third scan ﬁnds out 4 P—values < 5
-0.05/15 20.0167 that also satisfy <4-0.05/15. The selected
P—values are, again, 0.0001. 0.0019. 0.0004. 0.0095.

A1.4 Proof of theorem 4

Proof. [Proof of Theorem 4] For m P—values arbitrarily divided into
n chunks of size mg for c : 1. ....n such that the maximum chunk
size is m*, we show that the algorithm is still linear in m and never
uses more than m* space.

Bin. This step is as for Algorithm 1. It is important to note that
the binning is done relative to m and not the size of the chunk.
The only important difference is that bin sums are not maintained
because of the m" space limitation. Labels need not be stored ei—
ther. This step also counts the sum, m’, of all P—values across all
groups that are less than or. Running time and memory: This re—
quires a linear scan. A count variable can be kept for each group
in order to get m’.

Accumulate. For each group, ﬁnd the bin sums for the largest m"
partition of bins not covered yet, i.e. ﬁnd bin sums for bins m’ — /'

ﬁm'srcumol‘piqxo'sopcuuowtotq/ﬁdnq

FastLS U

1723

 

m" + 1 to m —  — 1)m* for/ : 1. ....n and we do both of the fol—
lowing before incrementing /. Accumulate bin sums across the
chunks. This can be done in a linear scan of the chunks and a sin—
gle array accumulating bin sums across chunks. Finding r* is as in
Algorithm 1 Step 2. However, now only m" bins may be checked
at a pass before needing to increment /. The subtotal of P—values

counted thus far is maintained after i is incremented. Running
time and memory: This step must be repeated at most n times and
requires a linear scan of the data. At any point at most m" space
plus several count variables are used.

Return. This step is as in Algorithm 1.

/310'S[BHmOprOJXO'SOIJBLUJOJIIIOlq/ﬂduq

