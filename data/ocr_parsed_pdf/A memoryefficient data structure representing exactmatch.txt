ORIGINAL PAPER

Vol. 27 no. 14 2011, pages 1901-1907
doi:10. 1 093/bioinformatics/btr321

 

Genome analysis

Advance Access publication June 2, 2011

A memory-efficient data structure representing exact-match
overlap graphs with application for next-generation DNA

assembly
Hieu Dinh* and Sanguthevar Rajasekaran*

Department of Computer Science and Engineering, University of Connecticut, Storrs, CT 06269, USA

Associate Editor: Alex Bateman

 

ABSTRACT

Motivation: Exact-match overlap graphs have been broadly used in
the context of DNA assembly and the shortest super string problem
where the number of strings n ranges from thousands to billions. The
length 6 of the strings is from 25 to 1000, depending on the DNA
sequencing technologies. However, many DNA assemblers using
overlap graphs suffer from the need for too much time and space
in constructing the graphs. It is nearly impossible for these DNA
assemblers to handle the huge amount of data produced by the next-
generation sequencing technologies where the number n of strings
could be several billions. If the overlap graph is explicitly stored, it
would require 9(n2) memory, which could be prohibitive in practice
when n is greater than a hundred million. In this article, we propose a
novel data structure using which the overlap graph can be compactly
stored. This data structure requires only linear time to construct and
and linear memory to store.

Results: For a given set of input strings (also called reads), we can
informally define an exact-match overlap graph as follows. Each
read is represented as a node in the graph and there is an edge
between two nodes if the corresponding reads overlap sufficiently.
A formal description follows. The maximal exact-match overlap of
two strings x and y, denoted by ovmax(x,y), is the longest string
which is a suffix of x and a prefix of y. The exact-match overlap
graph of n given strings of length 6 is an edge-weighted graph
in which each vertex is associated with a string and there is an
edge (x, y) of weight w=E—|ovmax(x,y)| if and only if mg)», where
lovmax(x,y)| is the length of ovmax(x,y) and A is a given threshold.
In this article, we show that the exact-match overlap graphs can
be represented by a compact data structure that can be stored
using at most (2A—1)(2llogni +[logM)n bits with a guarantee that
the basic operation of accessing an edge takes 0(logA) time. We
also propose two algorithms for constructing the data structure for
the exact-match overlap graph. The first algorithm runs in 0(Mnlogn)
worse-case time and requires 0(A) extra memory. The second one
runs in 0(Mn) time and requires O(n) extra memory. Our experimental
results on a huge amount of simulated data from sequence assembly
show that the data structure can be constructed efficiently in time and
memory.

Availability: Our DNA sequence assembler that incorporates
the data structure is freely available on the web at
http://www.engr.uconn.edu/~htd06001/assembler/leap.zip
Contact: hdinh@engr.uconn.edu; rajasek@engr.uconn.edu

 

*To whom correspondence should be addressed.

Received on January 24, 2011; revised on May 20, 2011; accepted
on May 25,2011

1 INTRODUCTION

An exact-match overlap graph of n given strings of length K each is
an edge-weighted graph deﬁned as follows. Each vertex is associated
with a string and there is an edge (x,y) ofweight a) = K — lovmax (x,y)|
if and only if a) f A, where A is a given threshold and |0vm3x(x,y)| is
the length of the maximal exact-match overlap of two strings x and y.
1' :5 —A f |0vm3x(x,y)| is called the overlap threshold. The formal
deﬁnition of the exact-match overlap graph is given in Section 2.

Storing the exact-match overlap graphs efﬁciently in term of
memory becomes essential when the number of strings is very large.
In the literature, there are two common data structures to store a
general graph G: (V,E). The ﬁrst data structure uses a 2D array of
size |V| >< |V|. We refer to this as an array-based data structure. One
of its advantages is that the time for accessing a given edge is 0(1).
However, it requires £2(|V|2) memory. The second data structure
stores the set of edges E. We refer to this as an edge-based data
structure. It requires Q(|V| + IE I) memory and the time for accessing
a given edge is 0(logA), where A is the degree of the graph. Both
these data structures require S2(|E|) memory. If the exact-match
overlap graphs are stored using these two data structures, we will
need £2(|E |) memory. Even this much of memory may not be feasible
in cases when the number of strings is over a hundred million. In
this article, we focus on data structures for the exact-match overlap
graphs that will call for much less memory than |E|.

1.1 Our contributions

We show that there is a compact data structure representing the
exact-match overlap graph that needs much less memory than |E|
with a guarantee that the basic operation of accessing an edge takes
0(logA) time, which is almost a constant in the context of DNA
assembly. The data structure can be constructed efﬁciently in time
and memory as well. In particular, we show that

0 The data structure takes no more than (2A—l)(2|’logn]+
|'logM)n bits.

0 The data structure can be constructed in 00in) time.

As a result, any algorithm that uses overlap graphs and runs in time
T can be simulated using our compact data structure. In this case, the
memory needed is no more than (2)» — l)(2 |'logn] + [logM )n bits
(for storing the overlap graph) and the time needed is 0(TlogA).

 

© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 1901

112 /3.ro's[Bruno[pJOJXO'sorwurJOJurorqﬂ:duq urorr papBOIII/lAOG

910E ‘OE JSHBHV uo ::

H.Dinh and S.Rajasekaran

 

If A is a constant or much smaller than n, our data structure will
be a perfect solution for any application that does not have enough
memory for storing the overlap graph in a traditional way.

Our claim may sound contradictory because in some exact-match
overlap graphs the number of edges can be 9(n2) and it seems like it
will require 9(n2) time and memory to construct them. Fortunately,
because of some special properties of the exact-match overlap
graphs, we can construct and store them efﬁciently. In Section 3.1,
we will describe these special properties in detail.

Brieﬂy, the idea of storing the overlap graph compactly is from
the following simple observation. If the strings are sorted in the
lexicographic order, then for any string x the lexicographic orders
of the strings that contain x as a preﬁx are in a certain integer
range or integer interval [a,b]. Therefore, the information about
out-neighborhood of a vertex can be described using at most A
intervals. Such intervals have a nice property that they are either
disjoint or contain each other. This property allows us to describe
the out-neighborhood of a vertex by at most 2A — 1 disjoint intervals.
Each interval costs 2 |'logn] + |'logA] bits, where 2 |'logn] bits are for
storing its two bounds and |'logA] bits are for storing the weight.
We have n vertices so the amount of memory required by our data
structure is no more than (2A — 1)(2|'logn] + |'logA] )n bits. Note that
this is just an upper bound. In practice, the amount of memory may
be much less than that.

1.2 Application: DNA assembly

The main motivation for the exact-match overlap graphs comes
from their use in implementing fast approximation algorithms for
the shortest super string problem which is the very ﬁrst problem
formulation for DNA assembly. The exact-match overlap graphs can
be used for other problem formulations for DNA assembly as well.
Exact-match overlap graphs have been broadly used in the context
of DNA assembly and the shortest super string problem where
the number n of strings ranges from thousands to billions. The
length K of the strings is from 25 to 1000, depending on the DNA
sequencing technology used. However, many DNA assemblers using
overlap graphs are time and memory intensive. If an overlap graph
is explicitly stored, it would require 9(n2) memory which could be
prohibitive in practice. In this article, we present a data structure for
representing overlap graphs that requires only linear time and linear
memory. Experimental results have shown that our preliminary DNA
sequence assembler that uses this data structure can handle a large
number of strings. In particular, it takes about 2.4 days and 62.4 GB
memory to process a set of 660 million DNA strings of length 100
each. The set of DNA strings is drawn uniformly at random from a
reference human genome of size of 3.3 billion base pairs.

1.3 Related work

Gusﬁeld et al. (1992) and Gusﬁeld (1997) consider the all-pairs
suﬁix-preﬁx problem which is actually a special case of computing
the exact-match overlap graphs when A = K. They devised an 0(Kn+
n2) time algorithm for solving the all-pairs sufﬁx-preﬁx problem. In
this case, the exact-match overlap graph is a complete graph. So the
run time of the algorithm is optimal if the exact-match overlap graph
is stored in a traditional way.

Although the run time of the algorithm by Gusﬁeld et al. is
theoretically optimal in that setting, it uses the generalized sufﬁx
tree which has two disadvantages in practice. The ﬁrst disadvantage

is that the space consumption of the sufﬁx tree is quite large (Kurtz,
1999). The second disadvantage is that the sufﬁx tree usually suffers
from a poor locality of memory references (Ohlebusch and Gog,
2010). Fortunately, Abouelhoda et al. (2004) have proposed a sufﬁx
tree simulation framework that allows any algorithm using the sufﬁx
tree to be simulated by enhanced sufﬁx arrays. Ohlebusch and Gog
(2010) have made use of properties of the enhanced sufﬁx arrays to
devise an algorithm for solving the all-pairs sufﬁx-preﬁx problem
directly without using the sufﬁx tree simulation framework. The run
time of the algorithm by Ohlebusch and Gog is also 0(Kn+n2).
Please note that our data structure and algorithm can be used to
solve the sufﬁx-preﬁx problem in 0(AKn) time. In the context of
DNA assembly, A is typically much smaller than n and hence our
algorithm will be faster than the algorithms of Gusﬁeld (1997) and
Ohlebusch and Gog (2010).

Exact-match overlap graphs should be distinguished from
approximate-match overlap graphs which is considered in Myers
(2005), Medvedev et al. (2007) and Pop (2009). In the approximate-
match overlap graph, there is an edge between two strings x and y
if and only if there is a preﬁx of x, say x’, and there is a sufﬁx of y,
say y’, such that the edit distance between x’ and y’ is no more than
a certain threshold.

2 BACKGROUND

Let 2 be the alphabet. The size of 2 is a constant. In the context
of DNA assembly, 2 = {A,C, G,T}. The length of a string x on 2,
denoted by |x|, is the number of symbols in x. Let x[i] be the i-th
symbol of string x, and x[i,j] be the substring of x between the i-th
and the j positions. A preﬁx of string x is the substring x[1,i] for
some i. A sufﬁx of string x is the substring x[i, |x|] for some i.

Given two strings x and y on 2, an exact-match overlap between
x and y, denoted by ov(x,y), is a string which is a sufﬁx ofx and a
preﬁx of y (notice that this deﬁnition is not symmetric). The maximal
exact-match overlap between x and y, denoted by ovmax (x, y), is the
longest exact-match overlap between x and y.

Exact-match overlap graphs: informally, an exact-match overlap
graph is nothing but a graph where there is a node for each read and
two nodes are connected by an edge if there is a sufﬁcient overlap
between the corresponding reads.

To be more precise, given n strings s1,s2, ...,sn and a threshold A,
the exact-match overlap graph is an edge-weighted directed graph
G=(V,E) in which there is a vertex vieV associated with the
string si, for lfifn. There is an edge (vi,vj)eE if and only if
|si| — |ovmax(si,sj)| 3A. The weight of the edge (vi,vj), denoted by
w(vi,vj), is Isl-l — |ovmax(si,sj)|. See Figure 1.

If all the n input strings have the same length K, then ‘L'=K —A is
called the overlap threshold. If there is an edge (vi, vj) in the graph,
it implies that the overlap between sl- and sj is at least 1'.

The set of out-neighbors of a vertex v is denoted by OutNeigh(v).
The size of the set of out-neighbors of v, |0utNeigh(v)|, is called
the out-degree of v. We denote the out-degree of v as degom(v)=
|0utNeigh(v) |.

For simplicity, we assume that all the strings s1,s2,...,sn have the
same length K. Otherwise, let K be the length of the longest string
and all else works.

The operation of accessing an edge given its two endpoints: given
any two vertices v,- and vj, the operation of accessing the edge (v,- , vj)

 

1 902

112 /3.ro's[Bumo[pJOJXO'sorwurJOJurorqﬂ:duq urorr popeolumoq

9103 ‘0g1sn8nv uo ::

Memory-efficient overlap graph

 

Si

 

31‘

.r--'
tr-'-‘
VL_. __.

v

w(s,-, s_,-) S A |0vmax(si; sj)|

Fig. 1. An example of an overlap edge.

is the task of returning w(vi,vj) if (vi,vj) is actually an edge of the
graph, and returning NULL if (vi,vj) is not.

3 METHODS

3.1 A memory-efﬁcient data structure representing an
exact-match overlap graph

In this section. we present a memory-efﬁcient data structure to store an exact-
match overlap graph. It only requires at most (2A —1)(2[logn'| + [logA'l )n
bits. It guarantees that the time for accessing an edge. given two end points
of the edge. is 0(logA). This may sound like a contradictory claim because
in some exact-match overlap graphs the number of edges can be 9(n2) and
it seems like it should require at least 9(n2) time and space to construct
them. Fortunately. because of some special properties of the exact-match
overlap graphs. we can construct and store them efﬁciently. In the following
paragraphs, we will describe these special properties.

The graph representation we envision is one where we store the
neighbors of each node. The difference between the standard adjacency lists
representation of a graph and ours lies in the fact that we only use 0(A)
memory to store the neighbors of any node. We are able to do this since
we sort the input strings in lexicographic order and employ a data structure
called PREFIX deﬁned below.

Another crucial idea we employ is the following: let x be any string and
let the input reads be sorted in lexicographic order. If we are interested in
identifying all the input strings in which x is a preﬁx. then these strings will
be contiguous in the sorted list. If we use the sorted position of each read
as its identiﬁer. then the neighbors of any node can be speciﬁed with 0(A)
intervals (as we show next).

Without loss of generality. we assume that the n input strings s1,s2, . . . , s”
are sorted in lexicographic order. We can assume this because if they are
not sorted. we can sort them by using the radix sort algorithm which runs in
0(Kn/w) time. where w is the word length of the machine used. assuming
that the size of the alphabet is a constant. If the alphabet is not of size 0(1).
the sorting time will be 0(nKlog(|E|)/w).

We associate an identiﬁcation number with each string s, and its
corresponding vertex v, in the exact-match overlap graph. This identiﬁcation
number is nothing but the strings lexicographic order i. We will access
an input string using its identiﬁcation number. Therefore. the identiﬁcation
number and the vertex of an input string are used interchangeably. Also. it
is not hard to see that we need [logn'l bits to store an identiﬁcation number.
We have the following properties.

Given an arbitrary string x. let PREFIX (x) be the set of identiﬁcation
numbers of all the input strings for which x is a preﬁx. Formally.
PREFIX(x)= {ilx is a preﬁx of si}.

PREFIX enables us to specify the neighbors list of any node in the
graph compactly. In PROPERTY 3.1. PROPERTY 3.2. PROPERTY 3.3.
and Lemma 3.1. we prove certain properties of PREFIX and ﬁnally show
that we can represent the neighbors list of any node as 2A — 1 intervals.

PROPERTY 3.1. If PREFIX (10750, then PREFIX (x): [a,b], where [a,b] is
some integer interval containing integers a,a+1, . . .,b— 1,b.

PROOF. Let a = miniepREp[X(X)i and b = maxiepREplxm l. Clearly.
PREFIX (x) E [a, b]. On the other hand. we will show that [a, b] E PREFIX (x).

Let i be any identiﬁcation number in the interval [a,b]. Since the input
strings are in lexicographically sorted order. s,,[1, lxl] 5s,»[1,|x|] 5 s),[1, lxl].
Since aePREFIX(x) and bePREFIX(x). s,,[1,|x|]=s;,[1,|x|]. Thus.
s,,[1,|x|]=s,»[1,|x|]=s;,[1,|x|]. Therefore. x is a preﬁx of si. Hence.
i e PREFIX (x).

For example. let

s1 = AAACCGGGGTI'T

a

2 = ACCCGAATI'TGT

a

3 = ACCCTGTGGTAT
s4 = ACCGGCTI'TCCA
s5 = ACTAAGGAATI’T

a

5 = TGGCCGAAGAAG

If x=AC. then PREFIX(x)=[2,5]. Similarly. if x=ACCC. then
PREFIX(x)=[2,3].

Property 3.1 tells us that PREFIX(x) can be expressed by an interval
which is determined by its lower bound and its upper bound. So we only
need 2llogn'l bits to store PREFIX (x). In the rest of this article. we will
refer to PREFIX(x) as an interval. Also. given an identiﬁcation number
i. checking whether i is in PREFIX(x) can be done in 0(1) time. In the
Subsection 3.2.1. we will discuss two algorithms for computing PREFIX (x).
for a given string x. The run times of these algorithms are O(|x|logn) and
0(lxl). respectively.

Property 3.1 leads to the following property.

PROPERTY 3.2. OutNeigh(v,») = U15mg PREFIX(si [w +1, |s,»|]) for each
vertex V). In the other words, OutNeigh(v,») is the union ofat most A non-empty
intervals.

PROOF. Let Vj be a vertex in OutNeigh(v,»). By the deﬁnition of the exact-
match overlap graph. 15 |s,»| — lovmax(si, sj)| =w(v,», vi) 5 A. Let w(s,», sf): a).
Clearly. ovmax(_si,_sj)=_si[w+l, |s,»|] =sj[1, lovmax(si,sj)|]. This implies that
vjePREFIX(.si[w+1, |s,»|]).

On the other hand. let Vj be any vertex in PREFIX (s, [w+ 1, |s,»|]).
it is easy to check that Vj e OutNeigh(v,»). Hence. OutNeigh(v,») =
UISmskPREFIX(si[w+1, |s,»|]).

From Property 3.2. it follows that we can represent OutNeigh(v,») by

at most A non-empty intervals. which need at most 2Allogn'l bits to
store. Therefore. it takes at most 2nA [logn'l bits to store the exact-match
overlap graph. However. given two vertices vi and vi. it takes 0(A) time
to retrieve w(vi,v]-) because we have to sequentially check if Vj is in
PREFIX(s,» [2, |s,»|]).PREFIX(s,» [3, |.s,|]).. . . .
PREFIX(si [A + 1, ls, |]). But if OutNeigh(v,») can be represented by k disjoint
intervals then the task of retrieving w(vi, vi) can be done in 0(logk) time by
using binary search. In Lemma 3.1. we show that OutNeigh(v,») is the union
of at most 2A — 1 disjoint intervals.

PROPERTY 3.3. For any two strings x ana1 y with |x| < lyl, either one of the
two following statements is true:

- PREFIX(y)gPREFIX(x)
- PREFIX(y)ﬂPREFIX(x)=0

PROOF. There are only two possible cases that can happen to x and y.

Case 1: x is a preﬁx of y. For this case, it is not hard to infer that
PREFIX (y) E PREFIX (x).

Case 2: x is not a preﬁx of y. For this case, it is not hard to infer that
PREFIX (y) ﬂPREF IX (x) = (A.

 

1 903

112 /3.ro's[Bumo[pJOJXO'sorwurJOJurorqp:duq urorr popeolumoq

9103 ‘0g1sn8nv uo ::

H.Dinh and S.Rajasekaran

 

[1, 93] [100, 130]

/\

[1, 20] [25, 50] [110, 120]

\/\ /\

[3, 5] [7, 8] [12, 15] [28, 36] [40, 47] [112, 114][116, 120]

[30, 33]
Fig. 2. A forest illustration in the proof of Lemma 3.1.

LEMMA 3.1. Given A intervals [a1,bl],[a2,b2]...[a;,,b;,] satisfying Property
3.3, the union of them is the union of at most 2A—1 disjoint intervals.
Formally, there exist p52A—1 disjoint intervals [a’l ,b’l],[a’2,b’2]...[a]’,,b]’,]
such that Ulsisk[a,,b,] = Ulsislﬂalﬂbg].

PROOF. We say interval [a,,b,] is a parent of interval [a,,b,] if [a,,b,]
is the smallest interval containing [a,,b,]. We also say interval [a,,b,] is a
child of interval [a, , b,]. Since the intervals [a, , b,] are either pairwise disjoint
or contain each other. each interval has at most one parent. Therefore. the
set of the intervals [a,,b,] form a forest in which each vertex is associated
with an interval. See Figure 2. For each interval [a,,b,]. let I, be the set of
maximal intervals that are contained in the interval [a,,b,] but disjoint from
all of its children. For example. if [a,,b,]:[1,20] and its child intervals
are [3,5],[7,8] and [12,15]. then I,={[1,2],[6,6],[9,11],[16,20]}. If the
interval [a,,b,] is a leaf interval (i.e. an interval having no children). I, is
simply the set containing only the interval [a,, b,]. Let A = U 1 (K A1,. We will
show thatA is the set of the disjoint intervals [a;., b;] satisfying the condition
of the lemma.

First. we show that UlslSA [a,, b,] = Umégbneﬂag, b;]. By the construction
of I,. it is trivial to see that Umégbﬂeﬂagbg] §U15i5A[a,,b,]. Conversely.
it is enough to show that [611,171]EU[ﬂ{,b{]eA[a;,b;] for any lfifA. This
can be proved by induction on verticesl in each tree of the forest. For the
base case, obviously each leaf interval [a,,b,] is in A. Therefore. [a,,b,] E
Umégbneﬂag, b;] for any leaf interval [a,, b,]. For any internal interval [a,, b,].
assume that all of its child intervals are subsets of U[ﬂ{,b{]eA[a;»,b;»]. By the
construction of I,. [a,,b,] is a union of all of the intervalslin I, and all of its
child intervals. Therefore. [a,,b,] E Umjbﬂeﬂagbg].

Secondly. we show that the intervals in A are pairwise disjoint. It is
sufﬁcient to show that any interval in I, is disjoint with every interval in
I, for i7éj. Obviously. the statement is true if [a,,b,]ﬂ[a,,b,]=0. Let us
consider the case where one contains the other. Without loss of generality.
we assume that [a,,b,] C [a,,b,]. Consider two cases:

Case 1: [a,, b,] is the parent of [a,,b,]. By the construction of 1,. any interval
in I, is disjoint from [a,,b,]. By the construction of any interval in  is
contained in [a,,b,]. Therefore. they are disjoint.

Case 2: [a,,b,] is not the parent of [a,,b,]. Let [a,,b,] = [a,0,b,0] C
[a,. ,b,l.]... C [a,h ,b,h] =[a,,b,]. where [a,,,b,,] is the parent of [a,r_1,b,r_l].
From the result in Case 1. any interval in I,r is disjoint from [a,r_l ,b,r_l] for
1 5 t 5 h. So any interval in I, is disjoint from [a,,b,]. We already know that
any interval in  is contained in [a,,b,]. Thus. they are disjoint.

Finally. we show that the number of intervals in A is no more than 2A — 1.
Clearly. |A|=Z;‘:1|I,|. It is easy to see that the number of intervals in I,
is no more than the number of children of [a,,b,] plus one. which is equal
to the degree of the vertex associated with [a,,b,] if the vertex is not a

root of a tree in the forest. and equal to the degree of the vertex plus one
if the vertex is a root. Let (1 be the number of trees in the forest. Then.
|A| 2 EL] |I,| 5 2?“:1d,+q=2|E| +p. where d, is the degree of the vertex
associated with [a,,b,] and E is the set of the edges of the forest. We know
that in a tree the number of edges is equal to the number of vertices minus
one. Thus. |E| =A — (1. Therefore. |A| 5 2A — q 5 2A — 1. This completes our
proof.

The above proof yields an algorithm for computing the disjoint intervals
starting from the forest of intervals. Once the forest is built. outputting the
disjoint intervals can be done easily at each vertex. However. designing a fast
algorithm for constructing the forest is not trivial. In the Subsection 3.2.2. we
will discuss an O(AlogA)-time algorithm for constructing the forest. Thereby.
there is an O(AlogA)-time algorithm for computing the disjoint intervals
[a;.,b;.] in Lemma 3.1. given A intervals satisfying Property 3.3. Also. from
Property 3.3 and Lemma 3.1. it is not hard to prove the following theorem.

THEOREM 3.1. OutNeigh(v,) is the union of at most 2A —1 disjoint
intervals. Formally, OutNeigh(v,) = UlSmSp[a,,,,b,,,] where p 5 2A — 1,
[a,,,,b,,,] ﬂ[a,,,/ , b,,,/] = (Afar 1 5 m #m’ 517. Furthermore, w(v,,v,) =w(v,, vk)
for any 15m 517 andfor any v,,vk e [a,,,,b,,,].

Theorem 3.1 suggests a way of storing OutNeigh(v,) by at most (2A — 1)
disjoint intervals. Each interval takes 2[logn] bits to store its lower bound
and its upper bound. and [logA] bits to store the weight. Thus. we
need 2[logn] + [logA] to store each interval. Therefore. it takes at most
(2A — 1)(2[logn] + [logA]) bits to store each OutNeigh(v,). Overall. we need
(2A — 1)(2[logn] + [logA] )n bits to store the exact-match overlap graph. Of
course, the disjoint intervals of each OutNeigh(v,) are stored in sorted order
of their lower bounds. Therefore. the operation of accessing an edge (v,,v,)
can be easily done in O(logA) time by using binary search.

3.2 Algorithms for constructing the compact data
structure

In this section. we describe two algorithms for constructing the data structure
representing the exact-match overlap graph. The run time of the ﬁrst
algorithm is 0(AKnlogn) and it only uses 0(A) extra memory. besides
Kn[log|2|] bits used to store the n input strings. The second algorithm runs
in O(AKn) time and requires O(n) extra memory. As shown in Section 3.1.
the algorithms need two routines. The ﬁrst routine computes PREFIX (x) and
the second one computes the disjoint intervals described in Lemma 3.1.

3.2.1 Computing interval PREFIX (x) In this subsection. we consider the
problem of computing the interval PREFIX (x). given a string x and n input
strings s1 ,s2, . . . , s” of the same length K in lexicographical order. We describe
two algorithms for this problem. The ﬁrst algorithm takes O(lxllogn) time
and 0(1) extra memory. The second algorithm runs in O(lxl) time and
requires O(n) extra memory.

The ﬁrst algorithm runs in phases. In each phase. it considers one of the
symbols in x. In the ﬁrst phase. it considers x[1] and obtains a list of input
strings for which the ﬁrst symbol is x[1]. Since the input strings are in sorted
order. this list can be represented as an interval [a1,b1]. Followed by this.
in the second phase the algorithm considers x[2]. From out of the strings in
the interval [a1,b1]. it identiﬁes strings whose second symbol is [x2]. These
strings will form an interval [a2,b2]; and so on. The interval that results at
the end of the k-th phase (where k 2 |x|) is PREFIX (x). In each phase. binary
search is used to ﬁgure out the right interval.

In the second algorithm. a trie is built for all the input strings. Each node
in the trie corresponds to a string u and the node will store the interval for
this string (i.e. the node will store a list of input strings for which u is a
preﬁx). For any given x. we walk through the trie tracing the symbols in x.
The last node Visited in this traversal will have PREFIX (x) (if indeed x is a
string that can be traced in the trie).

 

1 904

112 /3.ro's[Bumo[pJOJXO'sorwurJOJHrorqp:duq urorj popeo1umoq

9103 ‘0g1sn8nv uo ::

Memory-efficient overlap graph

 

A binary search basedalgorithm: let [a,, b,] =PREFIX(x[1, i]) for 1 5 i 5 |x|.
It is easy to see that PREFIX(x)=[a1X1,b1X1]E [a1x1_1,b1x1_1] E ...E [a1,b1].
Consider the following input strings, for example.

s1 = AAACCGGGGTIT
s2 = ACCAGAATITGT

s3 = ACCATGTGGTAT
s4 = ACGGGCTI'TCCA
s5 = ACTAAGGAATTI'

s5 = TGGCCGAAGAAG
x = ACCA

Then, [a1 ,b1]= [1,5], [a2,b2] = [2,5], [a3,b3]= [2,3] and PREFIX(x)=
[614,174] =[213l-

We will ﬁnd [a,,b,] from [a,_1,b,_1] for i from 1 to lxl, where [a0,b0] =
[1,n] initially. Thereby, PREFIX (x) is computed. Let Col, be the string that
consists of all the symbols at position i of the input strings. In the above
example, Col3 =ACCGTG. Observe that the symbols in string Col, [a,,b,]
are in lexicographical order for 1 5 i 5 |x|. Another observation is that
[a,,b,] is the interval where the symbol x[i] appears consecutively in string
Col, [a,_1 , b,_ 1 ]. Therefore, [a, , b,] is determined by searching for the symbol
x[i] in the string Col,[a,_1,b,_1]. This can be done easily by ﬁrst using the
binary search to ﬁnd a position in the string Col, [a,_ 1 ,b,_ 1] where the symbol
x[i] appears. If the symbol x[i] is not found, we return the empty interval
and stop. If the symbol x[i] is found at position c,, then a, (respectively b,)
can be determined by using the double search routine in string Col, [a,_ 1 , c,]
(respectively, string Col,[c,,b,_1]) as follows. We consider the symbols in
the string Col,[a,_1,c,] at positions c,—20,c,—21,...,c,—2k,a,_1, where
k = |_log(c, — a,_1)]. We ﬁndj such that the symbol Col, [c, — 21] is the symbol
x[i] but the symbol Col, [c, —2j+1] is not. Finally, a, is determined by using
binary search in string Col,[c,—2j,c,—2j+l]. Similarly, b, is determined.
The pseudo-code is given as follows.

1. Initialize [a0,b0]= [Ln].

2. for i=1 to |x| do

3. Find the symbol x[i] in the string Col, [a,_1 , b,_1] using binary search.
4. if the symbol x[i] appears in the string Col,[a,_1,b,_1] then

5 Let c, be the position of the symbol x[i] returned by the binary

search.

6. Find a, by double search and then binary search in the string
C01i [ai—l 7 Ci]-

7. Find b, by double search and then binary search in the string
COZiICivbi—ll-

8. else

9. Return the empty interval (4.

10. end if

11. end for

12. Return the interval [a1x1,b1x1].

Analysis: as we discussed above, it is easy to see the correctness of the
algorithm. Let us analyze the memory and time complexity of the algorithm.
Since the algorithm only uses binary search and double search, it needs 0(1)
extra memory. For time complexity, it is easy to see that computing the
interval [a,,b,] at step i takes 0(log(b,_1—a,_1))=0(logn) time because
both binary search and double search take 0(log(b,_ 1 —a,_ 1)) time. Overall,
the algorithm takes O(lxllogn) time because there are at most |x| steps.

A trie-based algorithm: as we have seen in Subsection 3.2.1, to compute the
interval [a,,b,] for symbol x[i], we use binary search to ﬁnd the symbol x[i]
in the interval [a,_1,b,_1]. The binary search takes 0(log(b,_1—a,_1))=
0(logn) time. We can reduce the 0(logn) factor to 0(1) in computing the
interval [a,,b,] by pre-computing all the intervals for each symbol in the
alphabet E and storing them in a trie. Given the symbol x[i], to ﬁnd the
interval [a,,b,] we just retrieve it from the trie, which takes 0(1) time. The
trie is deﬁned as follows (Fig. 3). At each node in the trie, we store a symbol

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

A G
[1, 1] [2, 2]
10 c 7
 [2, 3]
[1, 5] 1 T
0 [3, 3]
............ .. C G 7
[2, 5] [4, 4]
....9. ..... .. 9
T
[6, 6]  .... __
11 [5, 5]
9

 

 

 

Fig. 3. An illustration of a trie for the example input strings in
Subsection 3.2.1.

and its interval. Observe that we do not have to store the nodes that have only
one child. These nodes form chains in the trie. We will remove such chains
and store their lengths in each remaining node. As a result, each internal node
in the trie has at least two children. Because each internal node has at least
two children, the number of nodes in the trie is no more than twice the number
of leaves, which is equal to 2n. Therefore, we need O(n) memory to store
the trie. Also, it is well known that the trie can be constructed recursively in
O(Kn) time.

It is easy to see that once the trie is constructed, the task of ﬁnding the
interval [a,,b,] for each symbol x[i] takes 0(1) time. Therefore, computing
PREFIX (x) will take O(lxl) time.

3.2.2 Computing the disjoint intervals In this subsection, we consider
the problem of computing the maximal disjoint intervals, given k intervals
[a1 , b1], [a2, b2], ..., [ak , bk] which either are pairwise disjoint or contain each
other. As discussed in Section 3.1, it is sufﬁcient to build the forest of the
k input intervals. Once the forest is built, outputting the maximal disjoint
intervals can be done easily at each vertex of the forest.

The algorithm works as follows. First, we sort the input intervals in
non-decreasing order of their lower bounds a,. Among those intervals
whose lower bounds are equal, we sort them in decreasing order of their
upper bounds b,. So after this step, we have (i) a1 5a; 5... Sak and (ii)
if a,=a, then b, >b, for 15i<j5k. Since the input intervals are either
pairwise disjoint or contain each other, for any two intervals [a,,b,] and
[a,+1,b,+1] (for 15i<k) the following statement holds. Either [a,,b,]
contains [a,_1_ 1,b,_1_ 1] or they are disjoint. Observe that if [a,,b,] contains
[a,+1,b,+1], then [a,,b,] is actually the parent of [a,+1,b,_1_1]. If they are
disjoint, then the parent of [a,_1_ 1 , b,_1. 1] is the smallest ancestor of [a,, b,] that
contains [a,_1_1 ,b,_1_1]. If such an ancestor does not exist, then [a,_1_1 ,b,_1_ 1] does
not have a parent. Let A,={[a,l ,b,1],...,[a,m,b,m]} be the set of ancestors
of [a,,b,], where i1 <  < i,,,. It is easy to see that [a,l ,b,1] C... C [a,m ,b,m].
Therefore, the smallest ancestor of [a,,b,] that contains [a,_1_ 1,b,_1_ 1] can be
found by binary search, which takes at most 0(logk) time. Furthermore,
assume that [a,f,b,f] is the smallest ancestor, then the set of ancestors of
[a,+1,b,+1] is A,_1.1={[a,l,b,1],...,[a,f,b,f]}. Based on these observations,
the algorithm can be described by the following pseudo-code.

1. Sort the input intervals [a,,b,] as described above.
2. Initialize A :0. /* A is the set of ancestors of current interval [a,, b,] */
3. for i=1 tok—l do

4. if [a,,b,] contains [a,_1_1 ,b,_1_1] then

5. Output [a,,b,] is the parent of [a,_1_1 ,b,_1_1].

6.  [611.14, [71.1.1] into A.

7. else

8. Assume thatA={[a,l ,b,1],...,[a,m,b,m]}.

9. Find the smallest interval in A that contains [a,_1_ 1 ,b,_1_ 1].
10. if the smallest interval is found then

 

1 905

112 /3.ro's[Bumo[pJOJXO'sorwurJOJHrorqp:duq urorj popeo1umoq

9103 ‘0g1sn8nv uo ::

H.Dinh and S.Rajasekaran

 

11. Assume that the smallest interval is [a,f,b,f].
12. Output [a,]. ,b,f] as the parent of [a,_1_1 ,b,_1_1].
13- SBtA=llai1 1bi1 l,---,[ai,,bi,l, [aH—r 1724—11}-
14. else

15- SetA=llai+1.bi+1l}-

16. end if

17. end if

18. end for

Analysis: as we argued above, the algorithm is correct. Let us analyze
the run time of the algorithm. Sorting the input intervals takes O(klogk)
time by using any optimal comparison sorting algorithm. It is easy to see
that ﬁnding the smallest interval from the set A dominates the running
time at each step of the loop, which takes 0(logk) time. Obviously, there
are k steps so the run time of the algorithm is O(klogk) overall. Notice
that the sorted list of the intervals is actually a pre-order traversal of the
forest. So the time complexity of the algorithm after sorting the intervals
can be improved to O(k). However, the improvement does not change the
overall time complexity of the algorithm since sorting the intervals takes
O(klogk) time.

3.2.3 Algorithms for constructing the compact data structure In this
subsection, we describe two complete algorithms for constructing the data
structure. The algorithms will use the routines in Subsections 3.2.1 and
3.2.2. The only difference between these two algorithms is the way of
computing PREFIX. The ﬁrst algorithm uses the routine based on binary
search to compute PREFIX and the second one uses the trie-based routine.
The following pseudo code describes the ﬁrst algorithm.

1. fori=1tondo

2. forj=2toA+1do

3. Compute PREFIX (s,[j, |s,|]) by the routine based on binary search
given in Subsection 3.2.1.

4. end for

5. Output the disjoint intervals from the input intervals

PREFIX(s,[2, |s, |]), . . . ,PREFIX(s, [A +1, |s,|]) by using the routine in
Subsection 3.2.2.
6. end for

An analysis of the time and memory complexity of the ﬁrst algorithm
follows. Each computation of PREFIX in line 3 takes O(Klogn) time and
0(1) extra memory. So the loop of line 2 takes 0(AKlogn) time and 0(A)
extra memory. Computing the disjoint intervals in line 5 takes O(AlogA) time
and 0(A) extra memory. Since A 5 K, the run time of loop 2 dominates the
run time of each step of loop 1. Therefore, the algorithm takes 0(AKnlogn)
time and 0(A) extra memory in total.

The second algorithm is described by the same pseudo code above
except for line 3 where the routine of Subsection 3.2.1 for computing
PREFIX (s,[j, |s,|]) is replaced by the trie-based routine of Subsection 3.2.1.
Let us analyze the second algorithm. Computing PREFIX in line 3 takes O(K)
time instead of O(Klogn) as in the ﬁrst algorithm. With a similar analysis
to that of the ﬁrst algorithm, the loop of line 2 takes O(AK) time and 0(A)
extra memory. Constructing the trie in line 1 takes O(Kn) time. Therefore,
the algorithm runs in O(AKn) time. We also need O(n) extra memory to store
the trie. In many cases, n is much larger than A. So the algorithm takes O(n)
extra memory.

It is possible to develop a third algorithm by revising the step in line 2 to
line 4 in the pseudo code that computes the set of intervals of the sufﬁxes
of each input string s,. The revision is based on sufﬁx trees and the binary
search-based algorithm given in Subsection 3.2.1. The idea is to build a
sufﬁx tree for every input string s,. Note that every leaf in the sufﬁx tree
corresponds to a sufﬁx of the input string s,. After building the sufﬁx tree,
we populate every leaf in the sufﬁx tree with the corresponding interval
by traversing the sufﬁx tree and computing the interval for each node in the
sufﬁx tree given the interval for its parent. Finally, we output the intervals for
those sufﬁxes whose lengths are no less than the overlap threshold 1' =K — A.
It is easy to see that the time needed to determine the interval for any node

in the sufﬁx tree is 0(logn) given the interval for its parent. Because the
sufﬁx tree has O(K) nodes, it takes O(Klogn) time to compute intervals. In
addition, it takes O(K) time to build the sufﬁx tree and O(AlogA) time to ﬁnd
the disjoint intervals. Since A 5 K 5 n, we will spend O(Klogn) time for each
input string s,. As a result, the entire algorithm will run in time O(Knlogn).
Also, the entire algorithm will take O(K) extra memory because each of the
sufﬁx trees takes O(K) memory. In practice, logn is smaller than A and hence
this algorithm could also be of interest.

4 RESULTS

We have implemented a DNA sequence assembler named Large-
scale Efﬁcient DNA Assembly Program (LEAP) that incorporates
our data structure for the overlap graphs. The assembler has three
stages: preprocess input DNA sequences, construct overlap graph
and assemble. In the context of DNA sequence assembly, the input
DNA sequences are called reads. In the ﬁrst stage, we add the reverse
complement strings of the reads. Then we sort them and remove
contained reads. The second stage is the main focus of our article,
constructing the data structure of the overlap graph. The last stage
basically analyzes the overlap graph, then retrieves unambiguous
paths and outputs the contigs accordingly.

We tested our assembler on simulated data as follows. First, we
simulated a genome G. Then each read of length K is drawn from a
random location in either G or the reverse complement of G. Reads
drawn from the genome are error-free reads. The number n of the
drawn reads is determined by the coverage depth, c, by the equation
n=c%. We considered three datasets with the same read length
K = 100, the same coverage depth c = 20 and different genome sizes:
238 Mb, 1 GB and 3.3 GB. The number of reads in the datasets is
47.6 million, 200 million and 660 million, respectively. The size of
the ﬁrst genome is approximately the size of human chromosome
2. The size of the third genome is approximately the whole human
genome size. For the ﬁrst and the second dataset, we have run our
assembler with varying values of the overlap threshold 1': 30, 40,
50, 60 or 70. We only tried the overlap threshold 1:30 for the
last dataset because the run time was quite long, about 2.4 days. To
assess the quality of the contigs, we aligned them to the reference
genome and found that all the contigs appeared in the reference
genome. We have run our assembler on a Ubuntu Linux machine of
2.4 GHz CPU and 130 GB RAM. To save memory usage, we choose
the binary search-based algorithm to construct the overlap graph in
the second stage. The details are provided in Tables 1 and 2.

The DNA sequence assembler developed by Simpson and Durbin
(2010) also employs the overlap graph approach. Their assembler,
named String Graph Assembler (SGA), uses the sufﬁx array and
FM-index for the entire read set to construct the overlap graph. This
article reported that the bottleneck in terms of time and memory
usage was in constructing the sufﬁx array and FM-index that
required 8.5 h and about 55 GB of memory on the ﬁrst dataset. The
total processing time was 15.2 h. On the third dataset, they estimated
by extrapolation that the step of constructing the sufﬁx array and
FM-index would require about 4.5 days and 700 GB of memory. The
total processing time on the third dataset would be more than that.
However, SGA has been improved in terms of memory efﬁciency
since its ﬁrst version was released. Unfortunately, while the second
version of SGA improves memory usage, its run time increases. We
were able to run the latest version of SGA on the same machine on the
datasets. The source code of the latest version of SGA can be found

 

1 906

112 /3.ro's[Bumo[pJOJXO'sorwurJOJHrorqp:duq urorj popeo1umoq

9103 ‘0g1sn8nv uo ::

Memory-efficient overlap graph

 

Table 1. The detail results for the ﬁrst dataset

 

 

Genome size (Mb) 238 238 238 238 238
Number of read (M) 47.6 47.6 47.6 47.6 47.6
Overlap threshold (bp) 30 40 50 60 70
Overlap time (h) 0.68 0.57 0.46 0.36 0.26
Intervals  551 472 392 312 232
Edges  1298 904 668 471 286
Overlap memory (GB) 5.4 4.5 3.7 3 2.2
Assembly N50 (Kb) 19376 3171 429 50 5
Longest contig (Kb) 49 835 8326 1847 262 43
Total time (h) 0.84 0.71 0.61 0.5 0.4

 

Table 2. The detail results for the second and the third datasets

 

Genome size (Gb) 1 1 1 1 1 3.3
Number of read (M) 200 200 200 200 200 660
Overlap threshold (bp) 30 40 50 60 70 30

 

Overlap time (h) 5.1 4.2 3.5 2.7 1.9 48.5
Intervals (B) 2.91 2.48 2.06 1.64 1.22 6.58
Edges (B) 5.43 4.35 3.12 2.18 1.45 14.7
Overlap memory (GB) 28.1 23.5 20 15.7 11.8 62.4
Assembly N50 (Kb) 2713 646 168 32 5 771
Longest contig (Mb) 103 82 35 14 6 381
Total time (h) 6.1 5.3 4.5 3.7 2.9 57.3

 

Table 3. Time and memory comparison between the assemblers

 

 

 

Dataset LEAP Latest SGA Old SGA
Time Memory Time Memory Time Memory
(GB) (days) (GB) (GB)
238Mb 0.8h 5.4 1.1 7.9 15.2h 55
1GB 6.1h 28.1 5.6 38.5 a a
3.3 GB 2.4 d 62.4 7 34.5 d 700

 

at https://github.com/jts/sga. Table 3 provides the time and memory
comparison between the assemblers. For all of the datasets, we have
run the two assemblers with the same overlap threshold 1' = 30. The
contigs output by the two assemblers were almost the same.

5 CONCLUSION

We have described a memory-efﬁcient data structure that represents
the exact-match overlap graph. We have shown that this data

structure needs at most (2A — 1)(2[logn] + [logA])n bits, which is a
surprising result because the number of edges in the graph can be
O(nz). Also, it takes 0(logA) time to access an edge through the
data structure. We have proposed two fast algorithms to construct
the data structure. The ﬁrst algorithm is based on binary search and
runs in 0(AKnlogn) time and takes 0(A) extra memory. The second
algorithm, based on the trie, runs in 0(AKn) time, which is slightly
faster than the ﬁrst algorithm, but it takes O(n) extra memory to store
the trie. The nice thing about the ﬁrst algorithm is that the memory
it uses is mostly for the input strings. This feature is very crucial for
building an efﬁcient DNA assembler.

We are also developing our assembler LEAP that incorporates the
data structure for the overlap graph. The experimental results show
that our assembler can efﬁciently handle datasets of size equal to
that of the whole human genome. Currently, our assembler works for
error-free reads. In reality, reads usually have errors. If the error rate
is high, our assembler may not work well. However, with improving
accuracy in sequencing technology, the error rate has been reduced.
If the error rate is low enough, we will have many error-free reads,
which means that our assembler will still work in this case. Also,
an alternative way to use our assembler is to ﬁrst correct the reads
before feeding them to our assembler. In future, we would like to
adapt our assembler to handle reads with errors as well.

ACKNOWLEDGEMENTS

The authors would like to thank Varnsi Kundeti for many fruitful
discussions.

Funding: This work has been supported in part by the
following grants: National Science Foundation (0326155), National
Science Foundation (0829916), National Institutes of Health
(1R01LM010101-01A1).

Conﬂict of Interest: none declared.

REFERENCES

Abouelhoda,M. et al. (2004) Replace sufﬁx trees with enhanced sufﬁx arrays.
J. Dis. Algorithm, 2, 53786.

Gusﬁeld,D. (1997) Algorithms on Strings, Trees, and Sequences. Cambridge University
Press, New York.

Gusﬁeld,D. et al. (1992) An efﬁcient algorithm for the all pairs sufﬁx-preﬁx problem.
Inf Process. Lett., 41, 1817185.

Kurtz,S. (1999) Reducing the space requirement of sufﬁx trees. Softw. Pract. Exp, 29,
114971171.

Medvedev,P. et al. (2007) Computability of models for sequence assembly. In
Proceedings of Workshop on Algorithms for Bioinformatics, pp. 2897301.

Myers,E.W. (2005) The fragment assembly string graph. Bioinformatics, 21, 79785.

Ohlebusch,E. and Gog,S. (2010) Efﬁcient algorithms for the all-pairs sufﬁx-preﬁx
problem and the all-pairs substring-preﬁx problem. Inf Process. Lett, 110, 1237128.

Pop,M. (2009) Genome assembly reborn: recent computational challenges. Brief
Bioinformatics, 10, 3547366.

Simpson,J.T. and Durbin,R. (2010) Efﬁcient construction of an assembly string graph
using the fm-index. Bioinformatics, 26, i3677i373.

 

1 907

112 /3.ro's[Bumo[pJOJXO'sorieurJOJHrorqﬂ:duq urorj popeo1umoq

9103 ‘0g1sn8nv uo ::

