Biainfarmatics, 31, 2015, i365—i374
doi: 10.1093/bioinformatics/btv264
ISMB/ECCB 2015

 

Protein (multi-)location prediction: utilizing
interdependencies via a generative model

Ramanuja Simha1, Sebastian Briesemeisterz, Oliver Kohlbacher2 and
Hagit Shatkay1'3'4'*

1Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA, 2Applied
Bioinformatics, Center for Bioinformatics, University of Tuebingen, Germany, 3Center for Bioinformatics and
Computational Biology, University of Delaware, Newark, DE, USA and 4School of Computing, Queen's University,
Kingston, ON, Canada

*To whom correspondence should be addressed.

Abstract

Motivation: Proteins are responsible for a multitude of vital tasks in all living organisms. Given that
a protein’s function and role are strongly related to its subcellular location, protein location predic—
tion is an important research area. While proteins move from one location to another and can local—
ize to multiple locations, most existing location prediction systems assign only a single location
per protein. A few recent systems attempt to predict multiple locations for proteins, however, their
performance leaves much room for improvement. Moreover, such systems do not capture depend—
encies among locations and usually consider locations as independent. We hypothesize that a
multi—location predictor that captures location inter—dependencies can improve location predictions
for proteins.

Results: We introduce a probabilistic generative model for protein localization, and develop a sys—
tem based on it—which we call MDLoc—that utilizes inter—dependencies among locations to pre—
dict multiple locations for proteins. The model captures location inter—dependencies using
Bayesian networks and represents dependency between features and locations using a mixture
model. We use iterative processes for learning model parameters and for estimating protein loca—
tions. We evaluate our classifier MDLoc, on a dataset of single— and multi—localized proteins derived
from the DBMLoc dataset, which is the most comprehensive protein multi—localization dataset cur—
rently available. Our results, obtained by using MDLoc, significantly improve upon results obtained
by an initial simpler classifier, as well as on results reported by other top systems.

Availability and implementation: MDLoc is available at: http://www.eecis.udel.edu/~compbio/mdloc.
Contact: shatkay@udel.edu.

 

1 Introduction

 

Proteins are responsible for a multitude of diverse vital tasks in all
living organisms (Rost at £11., 2003). Given that a protein’s function
and role are strongly related to its subcellular location, protein loca—
tion prediction is an important research area (Alberts at £11., 2002;
Nair and Rost, 2008). Furthermore, the location of a protein helps
understand the protein’s prospective utility as a drug target (Bakheet
and Doig, 2009). Methods for determining protein locations include
experimental as well as high—throughput computational ones. The
experimental methods accurately determine protein locations, but
are typically time consuming and are typically not cost effective for
finding locations for a large number of proteins. Such methods

©The Author 2015. Published by Oxford University Press.

include mass spectrometry (Dreger, 2003) and green fluorescence
detection (Simpson at £11., 2000). On the other hand, the computa—
tional methods are fast, and can potentially predict locations for
proteins Whose actual locations have not yet been experimentally
determined. Most of the prediction systems represent proteins using
sequence—derived features and utilize machine learning methods (e.g.
Blum at £11., 2009; Emanuelsson at £11., 2000; Nakai and Kanehisa,
1991; Shatkay at £11., 2007).

Proteins move from one location to another and localize to multiple
subcellular compartments (Murphy, 2010; Pohlschroder at £11., 2005).
For instance, the enzyme TREXI, which assists in DNA repair, is

i365

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.U/),
which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact

journa|s.permissions@oup.com

112 /310'S[BIIJHO[pJOJXO'SOlJBLUJOJIIlOlq”K1111] 11101} popcorn/hog

9103 ‘Og anﬁnv uo ::

i366

R.Simha et aI.

 

primarily present in the cytoplasm but is also transported to the nu—
cleus in response to DNA damage (Tomicic et al., 2013). Thus, predict—
ing multiple locations for proteins is important, as protein movement
across locations enables the protein to serve multiple distinct functions.
Nevertheless, all prediction systems mentioned earlier and most current
systems assign only a single location per protein. Since proteins localize
systematically, and translocation occurs only among specific locations
for the purpose of a particular subcellular function, our hypothesis is
that modeling inter—depedencies among locations can assist in predict—
ing locations of proteins more accurately.

Posing the problem using computational, machine—learning
terms, assigning multiple locations to proteins is a mnlti-label clas-
sification task. Traditional single—label classification assigns a sin—
gle label (location) to each instance (protein), and is addressed by
methods such as Support Vector Machines (Scholkopf and Smola,
2002), naive Bayes or neural networks (Russell and Norvig, 2010).
Multi—label classification, on the other hand, aims to associate
each instance with possibly multiple classes. Some of the simplest
and commonly used approaches transform the multi—label classifi—
cation task into one or more single—label classification task(s)
(Tsoumakas et al., 2010); such approaches do not capture label in—
ter—dependencies. More sophisticated multi—label classification
approaches attempt to capture label inter—dependencies and in—
corporate them into the classification process. Such multi—label
classification methods have not yet been employed in the context
of protein location prediction.

In this article, we present a new, dependency—based probabilistic
generative model for eukaryotic protein localization, and develop a
multi—location prediction system—which we call MDLoc, to predict
locations of multiply localized proteins. As was done before
(Briesemeister et al., 2010a; King and Guda, 2007; Li et al., 2012),
we use sequence—derived features and Gene Ontology (GO) terms to
represent proteins. Here we introduce a new model using Bayesian
networks to directly address and capture inter—dependencies among
locations. Furthermore, we present the concept of location depend-
ency sets and use a mixture model to represent feature dependency
on location-combinations. The new system uses a generative model
and an iterative procedure for estimating its parameters, and effect—
ively improves the estimation process of multi—locations. Our
method is based on iteratively learning parameters of the location—
Bayesian—network and the mixture model, while re—inferring the lo—
cation estimates of the proteins in each iteration. This improves on
our preliminary system, which comprised a collection of Bayesian
network classifiers, where location inter—dependencies were not
learnt as part of the model but rather captured based on simple esti—
mates of location values (Simha and Shatkay, 2014).

We evaluate MDLoc on a dataset derived from the DBMLoc
dataset (Zhang et al., 2008), which is the most comprehensive pro—
tein multi—localization dataset currently available, using multiple
runs of 5—fold cross—validation. We show that the performance of
MDLoc on multi—localized proteins improves over earlier results for
a top performing system, YLoc+ (Briesemeister et al., 2010a). The
improved results obtained by MDLoc demonstrate the advantage of
utilizing location inter—dependencies and feature dependencies on 10—
cations in the prediction process.

The rest of the article proceeds as follows: Section 2 surveys
methods for protein multi—location prediction. In Section 3, we
introduce the concept of location dependency sets and provide rele—
vant notations; we also present our new probabilistic generative
model for protein localization, which captures dependencies be—
tween protein—features and locations. In Section 4, we discuss the
model parameters, the learning procedure used for finding them,

and the inference technique used for predicting multiple protein 10—
cations. Experiments and results are presented in Section 5, followed
by conclusions and future directions.

2 Related work

A number of recent location prediction systems attempt to
predict multiple locations for proteins, however their performance
leaves much room for improvement. While most use sequence—
derived features (e.g. amino acid composition) and GO terms to
represent proteins and to predict protein locations, a few are
based solely on sequence—based similarity. The former class of
methods incorporate one or more of the following classifiers: k—
nearest neighbors (k—NN, Chou et al., 2011), Support Vector
Machines (Li et al., 2012), naive Bayes (Briesemeister et al., 2010a)
and neural networks (Emanuelsson et al., 2000). KnowPredsire (Lin
et al., 2009) is an example of the latter, similarity based, class of
methods.

Systems that use k—NN adaptations to predict multiple locations
for proteins include WoLF PSORT (Horton et al., 2007), Euk—
mPLoc (Chou and Shen, 2007), iLoc—Euk (Chou et al., 2011) and an
ensemble system (Li et al., 2012). WOLF PSORT outputs for a query
protein the location—combination that is most frequent among the
protein’s k—NN in the training set; the predictions are thus restricted
to location—combinations already present in the set. Both iLoc—Euk
and Euk—mPLoc compute a score for each candidate location, based
on the query protein; iLoc—Euk outputs locations having the highest
scores; the number of locations is the same as that associated with
the query protein’s nearest neighbor in the dataset; Euk—mPLoc as—
signs the protein to locations whose score lies within a certain devi—
ation from the highest score.

All the methods described thus far treat locations as independent
from one another and do not utilize possible inter—dependencies
among locations in the prediction process. A few systems, however,
have tried to make use of location inter—dependencies to predict mul—
tiple locations for proteins. For example, the classifier by He et al.
(2012) attempts to use pairwise location— correlation in the predic—
tion process, but does not use more complex inter—dependencies.
YLoc+ (Briesemeister et al., 2010a) introduces a new class for each
location—combination represented in the training dataset and uses a
naive Bayes classifier to predict a probability distribution over these
new classes. Thus, each classifier prediction is restricted to location—
combinations in the training set. YLoc+’s performance was
evaluated using the most comprehensive protein multi—localization
dataset and is the highest among current multi—location prediction
systems. In our earlier preliminary work (Simha and Shatkay, 2014),
we used a collection of Bayesian network classifiers to predict mul—
tiple locations of proteins. The simplified model used did not incorp—
orate location—interdependencies into the iterative learning process,
but rather utilized one—time estimates of location values to establish
interdependencies. The performance of that classifier was compar—
able to that of YLoc+ when using the same dataset, but did not im—
prove on it.

In the next section, we present a new probabilistic generative
model for protein localization that directly incorporates the learning
of location inter—dependencies into the iterative learning process.
Additionally, we introduce the concept of location dependency sets,
which enables us to capture feature dependencies on location—
combinations in a mixture model setting. The resulting system
MDLoc, shows significant improvement, according to all evaluation
metrics, compared with previously reported performance for protein
multi—location prediction.

112 /310'S[BIIJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”K1111] 11101} popcorn/hog

9103 ‘Og anﬁnv uo ::

Protein (multi—)Iocation prediction

i367

 

3 A probabilistic generative model for protein
localization

As we and others have done before (Briesemeister et al., 2010a;
Garg and Raghava, 2008; Simha and Shatka , 2014), we represent
each protein P as a weighted feature vector, f P :  , . . . , ff) where
d is the number of features. Let S : {$1, . . . ,s,,} be the set of q sub—
cellular components in the cell. Each protein P localizes to at least
one—and possibly more than one—location. The locations of each
protein P are represented by a location indicator vector,
lP : (1%),“ 015;) of 0/1 values, where 15’ : 1 if P localizes to s,-, and
If : 0 otherwise. We view each location indicator 1,? as a value
taken by a random variable L,- and each feature fI-P as a value taken
by a random variable Ff. Given a protein P, represented as a vector
f P , the multi—localization task amounts to assigning a (correct) 0/1
value to each of the entries 1?.

3.1 Modeling location inter—dependency

We use Bayesian networks to model inter—dependencies among sub—
cellular locations. A Bayesian network consists of a directed acyclic
graph G : (L, E) whose set of nodes L corresponds to random vari—
ables and set of edges E indicates dependencies among the variables.
In our case, nodes represent location variables denoted
L : {L1, . . . ,Lq}. Each variable L, corresponds to a location 5,-
within the cell and takes on a 0/1 value. Figure 1 shows an example
Bayesian network we learn over location variables. A directed edge,
for instance, from membrane to cytoplasm represents the assertion
that knowing that a protein localizes to the membrane inﬂuences the
level of belief about the protein localizing to the cytoplasm.
According to the conditional independence relationship encoded in
the Bayesian network, each variable L,- is conditionally independent
of its non—descendants given its parents Pa(L,-) (for additional details
see Russell and Norvig, 2010). The joint distribution of the location
variables can thus be calculated as:

q
Pr(L1, . . . ,Lq) : HPr(L,-lPa(L,-)). (1)
i:1

3.2 Capturing location—feature dependency

The value of each feature represents a certain characteristic of a
protein, such as the relative abundance of each amino acid in the
protein’s amino—acid composition (King and Guda, 2007). In our ex—
periments, we use the exact same features used by Briesemeister
et al. (2010a), as explained in Section 5.1. For the purpose of pre—
dicting locations for a protein, we view a protein as though it was
generated through a stochastic process, in which each of its feature
values was determined. The value of each feature variable F,-
(1 S j S d) is assigned based on the values taken by one or more
location random variables; that is, each feature value may depend
on multiple locations and not just on one. For instance, consider a

Plasma Extracellular Nucleus Cytoplasm Golgi Endoplasmic
Membrane Space Apparatus Reticulum

Fig. 1. An example location-Bayesian-network that we learn. Directed edges
represent dependencies between the connected nodes. The location associ-
ated with each variable is shown below the corresponding node

feature capturing the abundance of tryptopban (Trp) residues in the
amino acid composition of a protein; we denote the random variable
associated with this feature by FTrp. The value of this feature varies
greatly between proteins known to localize to the membrane vs.
those that are known to localize to both the membrane and the cyto-
plasm. Specifically, the probability of a membrane protein to have
more than three Trp residues (formally denoted as the conditional
probability: Pr(F—1-,p > 3lLMem : 1)), is 0.36 [this high probability
agrees with the well—established importance of Trp’s role in
membrane proteins (Schiffer et al., 1992)]. In contrast, the probabil—
ity of proteins known to be multi—localized to both the membrane
and the cytoplasm to have more than three Trp residues
(Pr(F—1-,p > 3lLMem : 1, LCyt : 1)), is only 0.15 (the probability val—
ues are calculated based on the dataset described in Section 5.1).
Thus, the feature value depends on more than a single location
value. To accurately capture the dependency between protein
features and location—combinations, we view each feature value as
depending on a set of location indicator values.

Recall that we view a protein as represented by (i.e. comprised
of) a set of features. As such, we view each possible location of a
protein P as depending on a set of locations to which proteins with
similar feature values (including P itself) are likely to be localized.
We thus introduce the concept of location dependency sets. For a
location 5,, its dependency set comprises the minimal set of locations
{5,1, . . . ,sim} such that the likelihood of a protein to localize to 5,
depends on (i.e is correlated or anti—correlated with) its likelihood of
to localize to each of {s,,,...,s,-m}. Using the Bayesian network
framework, we note that a dependency as described above between
the locations 5,; and s,- can be represented as a directed edge from the
graph node L,, to L,-. Given a Bayesian network that represents
the dependencies among locations in this way, we can thus denote
the location dependency set for each location variable L,- as the par—
ents of L,- in the Bayesian network. As such, we define q location de—
pendency sets, one set per location,

L81:{L1}U Pa(L1), . . . ,LSq : {Lq} U Pa(Lq), (2)

where Pa(L,-) (1 S i S q) denotes the parents of location variable L,-
in a Bayesian network.

Given a Bayesian network G, the steps involved in protein gener—
ation are discussed in the rest of this section. We use a coin-toss
model to set location indicator values, and two die-roll processes to
set feature values. For each feature, one die roll is used to select a lo—
cation dependency set, and another to assign the actual feature
value. We next describe each of the steps in detail.

3.3 Setting location values

As part of the generative process for a protein P, we view the value
of a location indicator If (1 S i S q) as set by tossing a coin C,; if the
coin comes up Heads, the location indicator If is set to 1; otherwise
If : 0. The probability of C,- to come up Heads is:_Pr(L,- : 1 )Pa(L,-)).
Values comprising the location indicator vector 1 P are thus set by
tossing the location—specific coins in a sequence one after the other.
We assume that there is a specific order in which the coins are
tossed. To establish the order, we use a topological ordering of loca—
tion variables in the Bayesian network G denoted as Lt1,. . . ,Ltq,
where each parent in the network appears before its descendant; an
example of such an ordering of nodes based on the network in
Figure 1 is L2,L1,L3,L4,Lq,L5. Consequently, coin Cd is tossed
first, and based on its outcome, the location indicator value If, is
set, then Cd is tossed and 15 is set, and so on, until th is tossed and
If; is set.

112 /310'S[BIIJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”Idllq 11101} popco1umoq

9103 ‘0g15n8nv uo ::

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

i368 R.Simha et al.
Determining the Location Vector Selecting a Set of Locations
Toss Toss Toss Selected
- 1 -) L  .
location —) location a —) location egal‘gﬁzmb j 51 l L sk Location
coin 0,, coin Ctz ' ' p' _ coin C 7 Dependency
Heads 91122 _ 1 l q -) LS,1 I set
Heads 91:: = [Tails 91,2
_ p _> tq . .
[Tans-)1“ =0 l P _ II, [P lp [Tails 9113 = O . AssrgnmgaFeature-Value
‘ I’m-"w; ’q 19fP—1
_ _ Roll felature j ' P _ Feature For each
Location Indicator Vector die D j P f j — V j Value feature 
k .../n,-)f,- =n,-
Fig. 2. The generative process for a protein P. First, location coins, Q1,...,Qq, are tossed (top left); based on the outcomes, location indicator values, [5, . . . , lg,

are chosen (bottom left). Collectively, these values make up the location indicator vector IP. For each feature F], the die  is then tossed to select a location de-
pendency set (top right); based on the selected set LSk, the feature die D”? is tossed to pick the feature-value fJ-P (bottom right)

3.4 Setting feature values
We further view each feature value fI-P (1 S j S d) as selected from
among nl- possible distinct values by adhering to the following steps:

1. A dependency set is selected: A location dependency set is chosen
based on a probability distribution over q such sets [see
Equation (2) for the sets deﬁnitions]. For each feature Ff, let 73””
be a random variable that takes on the values 1, . . . ,q, where a
value i (1 S i S q) indicates that the ith location dependency set
is selected. We denote the event of select_ing the ith set, LS,-, by
if” : i. Given a location indicator vector 1 (selected in the previ—
ous step), to select a location dependency set, a die DE with q
faces is rolled. If the die D5 lands with‘the ith face up, the set
L_S,- is selected. The probability of DTPI to come up as i is:
95 (13,-) : Pr(7tFl : 1'17).

2. A feature-value is assigned: Based on the values taken by vari—
ables in a selected location set, the feature value is chosen. Given
that the set LS1z was selected, we assume that a die D? with n,-
faces is rolled to pick a value for feature Ff. If the die D? lands
with the vith face up, the feature value is set to v,-. The probabil—
ity of D? to come up as v,- is: Pr(F,- : villi, Pa(Lk)), where F,- is
the random variable associated with the jth feature.

Based on this model, each feature value fI-P is set independently of
oth_er features to construct the complete feature vector of the protein
Pafpz<f1P7HWf5l‘

The generative process for a protein P is summarized as shown
in Figure 2: First, the location coins are tossed in the order
Cg, Cg, . . . , th—as shown on the left side of the figure. If the coin
Cg (1 S i S q) comes up Heads, the location indicator 15 is set to 1;
otherwise If; : 0; Collectively, this results in choosing the location
indicator vector l‘P. Next, for each feature F,- (1 S j S d), the loca-
tion vector die DEEP is rolled; if the die lands with the lath face up,
the set LS1z is selected—as shown on the top—right side of the figure.
Based on the selected set LSk, the feature die D? is rolled; if the die
lands with the v,-th face up, the feature value fI-P is set to v,—as
shown on the bottom—right side of the figure.

We note that our generative model makes the following two in-
dependence assumptions:

1. The feature values flp, . . . ,f; of a protein P, are conditionally in-
dependent of each other given the protein’s location indicator
vector 1 P, formally:

—6 —6 d —6
Pr(f”ll”) :HPros-WP). <3>
[:1
While this assumption may oversimplify the underlying biological

mechanisms, it works well in practice and has proven useful before

 

 

 

 

 

 

9k(Fj)

 

 

 

 

Fig. 3. The probabilistic graphical model for the generation of protein fea-
tures. Directed edges represent dependencies between nodes. Locations and
features are shown as circles and location sets as squares. Shaded nodes rep-
resent observed variables and unshaded nodes represent latent variables.
The variable #7 takes on a value k, indicating the selection of the set LSk, with
a probability 6k(FJ-). The rectangular plate notation is used to represent repli-
cation of features and location sets with the same dependencies

(Briesemeister et al., 2010a). Moreover, our model carefully accounts
for inter—dependencies among locations, as well as among locations and
features, thus indirectly capturing interdependencies among features.

2. Given the values taken by a location variable Lk and its parents
Pa(Lk) in a selected location dependency set LSk, the feature
value for a protein, fI-P, is conditionally independent of all other
location values, formally:

Prof/Pu : k,1{’,.. .,1§) : Pr(fI-PllP,Pa(Lk)). (4)

Figure 3 shows the protein generation process using the standard no—
tation of a probabilistic graphical model. Nodes represent random
variables and directed edges represent dependencies among vari—
ables. The values of location and feature random variables are gov—
erned by a probability distribution and as such are denoted using
circles. In contrast, the value of each location dependency set vari—
able LS1z is assigned deterministically based on the values of the loca—
tion variable Lk and its parents Pa(Lk), and is denoted as a square.
The variables representing locations, features, and location depend—
ency sets are observed and hence are shown as shaded; the rest of
the variables are latent and are shown unshaded. The latent variable
73”” takes on a value [2, indicating the selection of the location set
LSk, with a probability 0;,(Fi). As was shown in Figure 1, edges
among location variables capture inter—dependencies among loca—
tions. The rectangular plate notation is used to represent replication
of feature and location set variables with the same dependencies.
The lack of feature—feature edges captures the conditional indepen—
dencies among features given location sets.

112 /310's1cu1n0[p10}x0'sopcuiJOJutoiq/ﬁduq 11101} popco1umoq

9103 ‘0g15n8nv uo ::

Protein (multi—llocation prediction

i369

 

Under the independence assumptions and the structure of our
model described earlier, the joint probability of the location indica—
tor vector 1 P and the feature vector f P is expressed as:

PM?) 2 Pr<7P>Pr<z7PFP> 2
q d q _.
2 M2 lilPa(Li))XH29£P(Fj)Pr(Fj 2 film. 21P,Pa(Li>>,
r21 j:1le:1

(5)

where each term corresponds to a parameter of the generative
model as described below:

a. H221 Pr(L, : If ]Pa(L,-)) is the factorization of the joint probabil—
ity Pr( 1”) : Pr(L1 : If, . . . ,Lq : lg), over the individual q loca—
tion indicator values;

b. Pr(F,- : [fl-P]Lk,Pa(Lk)) denotes the conditional probability of a
feature value ff (1 S j S d, where d is the total number of fea—
tures), given the values taken by a location variable La and its
parents Pa(L,-) comprising the location dependency set LS1z
(Ender the current model G);

c. 0]:  denotes the probability that the location dependency set
LS1z wag selected for a given feature Fl- and a location indicator
vector 1 P.

4 Model learning and protein multi-location
prediction

In this section, we introduce the procedure used for learning the struc—
ture and the parameters of our generative model and for predicting
multiple locations for proteins. We present an expectation maximiza—
tion (EM) algorithm to estimate the hidden parameters and explain
the inference technique used for multi—location prediction.

As our goal is to predict multiple locations for proteins, we use
the probabilistic generative model presented in Section 3 to predict a
0/1 value for each location variable L,-. To obtain the model, we use
an iterative process (see Fig. 4) in which the structure of a Bayesian
network and the parameters of the generative model [shown in
Equation (5)] are learned. Each iteration consists of first learning a
network structure and estimating its parameters, and following the
learning by performance assessment of the resulting model by using
it to infer the locations of proteins in the training dataset. This pro—
cess is continued until a stopping criterion is met, namely, until the
prediction performance of the learned model on the proteins from
the training—set does not improve between two successive iterations.
Typically the process does not require more than ten iterations to
complete. To measure prediction performance in each iteration, we
use the F1—score metric, which is formally defined later in Section
5.2. We next discuss the procedures used for learning the structure
and the parameters of our model.

4.1 Model learning

In each iteration of the learning process, we obtain a Bayesian net—
work structure of locations using the software package BANJO
(Smith et al., 2006) and estimate the model parameters shown in the
previous section in Equation (5). The initial Bayesian network struc—
ture is learned from protein locations in the training set, and itera—
tively updated to reﬂect the most—recently estimated locations.

To estimate the model parameters described in components (a)
and (b) of Equation (5), we calculate the maximum likelihood esti—
mates from frequency counts in the training dataset. As for compo—
nent (c) there, thg location set probability 9],  for a given location
indicator vector 1 and a feature F,- cannot be directly computed from

 

PREDICT locations

INITIALIZE
! for training proteins

model structure
and parameters

 
 
 
 
  
 
     

Imml

 

 

 

 

 

   
  
    

Predlction
performance
improved?

  
 

Learned
M odel

 

 

UPDATE model structure and
Parameters Wales

 

 

 

Fig. 4. A summary of our model-learning process. The rectangular boxes rep-
resent steps in the learning process, the diamond indicates checking for a
stopping criterion, and the oval represents the output, which in our case is
the learned model. Directed edges indicate the order among steps

the dataset. We thus use an EM glgorithm (Dempster et al., 1977) to
estimate the hidden parameter 0! (Pi), as described next.

In the E-step, for each protein P and each of its feature values fI-P
in the training set, we compute the probability of a location set LS1z
to be used to determine the protein’s feature value as:

eiiFi>Pr<zePllKPa<Li>>

Pr(?» 2 lay/.117"): (6)

a _. ‘

I P
2 9!: (Fi)Pr(f,-Pllpi P8020)
k:1
In Equation (6), for each location indicator vector IP and feature

. . . vﬂl’ . . . . . . .
F,, the distribution 01  over q location sets is initialized as uni—

form; thus initially 0?(F,-) : 1/q for all k, (1 g k 3 q). The condi—
tional probability, Pr(fI-P UP ,Pa(Lk)), of a feature value fI-P given the
location set LS1z (where LS;a : L;a U Pa(Lk)) is initialized to the max—
imum likelihood estimate computed using the training dataset.

In the M—step, we re—estimate all the model parameters. For each
location indicator vector 1 and feature F,, the probability of a loca—
tion dependency set LS1z is re—estimated as:

22

2  {PlTP2T.rP2v,»}

elm-)2 q
E Z

—6

H “1 {I’ll P2T,r,P2v,»}

mil“? 2 W, 71’) Pr(}‘I-P]TP)

 

7 (7)
mil“? 2 W30 Pr(ffﬁl’)

where v,- is a feature value of F,- and [2 denotes the selection of the de—
pendency set LSk. That is, in the numerator, for each feature, E, we
go over all feature values v,- that F,- takes, and all proteins in the set
that have this feature value; we sum the probability of having used the
dependency set LS1z to generate feature value v,—weighted by the
probability of observing that feature value. The denominator is a nor—
malization factor ensuring that probabilities sum to 1. The prob_ability
of a set LS1z to be selected for determining fI-P,Pr(7tF” ikW’, 1”), is
calculated in the E-step [see Equation (6)]. Note that 95,  is com—
puted separately for each feature since feature—values are determined
independently of each other during protein generation.

To re—estimate the _cpnditional probability Pr(F,- : vi]LSk), we
introduce the not_ation 1 {sh to denote the restriction of the location
indicator vector 1 P to only those locations that are members in the
location dependency set LSk. The conditional probability is then cal—
culated as:

“(E I U/lLii P3020) I Pr(Fi : VilLsk) 2
mil“? 2 l2]fI-P,TP)Pr(fI-P]TP)
{PW}, :TupﬂP:Vi}

Z Z

, LP 2
v’ {Pl’uk:’ukrl§P:Vi}

 

Pr(iFr 2 aginnofﬁl’)

112 /310'S[BIIJHO[pJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 11101} popco1umoq

9103 ‘0g isanV uo ::

i370

R.Simha et aI.

 

This re—estimation formula is similar to the one shown in Equation
(7), but taking into account only those proteins in the training
set that are localized to the locations included in the dependency
set LSk.

The process of alternating between the E—step and the M—step is
carried out until convergence is reached, i.e. until changes to the hid—
den parameter values between iterations are no greater than 0.05.
Throughout the estimation process, we use Laplace smoothing to
avoid overfitting, by adding fractional pseudocounts to observed
counts of events (Russell and Norvig, 2010). The smoothing param—
eter (or) is set to 0.5, which is close to the count of rare events and al—
most insignificant compared with counts of frequent ones. We next
present the inference procedure that we use for predicting protein
locations.

4.2 Multiple location prediction _‘

Given a protein P, represented as a feature vector f P , our task is to
predict its location indicator vector 1 P, i.e. we need to assign a 0/1
value to each of its location indicators 15’ (1 S i S q). Under the
Bayesian network model, this task translates to inferring the value
of the random variable L,-, which in turn depends on the values of its
parent nodes Pa(L,-). We thus infer the values of the location de—
pendency set {L,-} U Pa(L,-). The inference procedure aims to assign
values to L, 3nd to Pa(L,-) such that the conditional probability,
Pr(L,-, Pa(L,-)]fP) is maximized.

To infer these values, we follow ari iterative process. We start by
initializing all location indicators in l P to 0. For any value—assign—
ment, 1,- to L,-, we denote by 1330,) the values assigned to all parents
of L,-. We also denote by L,- the current value assignment for all loca—
tion random variables in the network other than L,- and Pa(L,-). In
each iteration, we consider in turn each of the random variables L,-.
For all possible value assignments, l,-, 1330,), to L,- and Pa(L,-), re—
spectively, we calcul_ate the conditional probability,
Pr(L, : l,-,Pa(L,-) : P_a(l,-)]fP,L,-). The value assignment to L,- that
produces the highest probability is the one used as the current esti—
mate for L,-. As noted earlier, the process typically requires about ten
iterations to reach convergence.

We next describe our experiments and the results obtained using
the protein generation model.

5 Experiments and results

We implemented our algorithms for learning parameters of the genera—
tive model and for inferring locations using Python. We have applied
our system MDLoc to the largest available dataset of multi—localized
proteins, previously used for training YLocT (Briesemeister et al.,
2010a). Next, we describe the dataset and the evaluation methods we
use, followed by experiments and results obtained using MDLoc. We
also provide several specific examples demonstrating the utility of
incorporating location inter—dependencies into the prediction process.

5.1 Data

In our experiments, we use a dataset first constructed for an exten—
sive comparison of multi—location prediction systems as part of the
evaluation of YLoc+ (Briesemeister et al., 2010a). It contains 5447
single—localized proteins, originally published by Hoglund et al.
(2006), and 3056 multi—localized proteins, originally published as
part of the DBMLoc dataset (Zhang et al., 2008). As in a true
prediction scenario it is not known a priori whether a protein may
localize to a single or to multiple locations, we train our system on
the combined set of proteins, thus enabling it to handle the actual

prediction task. The dataset is already homology—reduced, i.e. pro—
teins sharing >80% sequence identity with another protein in the
dataset were removed. We compare the performance of our system
to that of others using only multi—localized proteins (3056 proteins)
because the only results publicly available for the other systems were
obtained on this dataset (Briesemeister et al., 2010a). The single—
localized proteins are from the following locations (abbreviations
and number of proteins per location are given in parentheses): cyto—
plasm (cyt, 1411 proteins); endoplasmic reticulum (ER, 198); extra
cellular space (ex, 843); golgi apparatus (gol, 150); lysosome (lys,
103); mitochondrion (mi, 510); nucleus (nuc, 837); membrane
(mem, 1238); peroxisome (per, 157). The multi—localized proteins
are from the following pairs of locations: cyt_nuc: 1882 proteins;
ex_mem: 334; cyt_mem: 252; cyt_mi: 240; nuc_mi: 120; ER_ex:
115; ex_nuc: 113. Note that all the multi—location subsets used have
over 100 representative proteins. We use the exact same representa—
tion of a 30—dimensional feature vector as used for evaluating
YLocT (for further details see Briesemeister et al., 2010b): (i) thir—
teen features derived directly from the protein sequence data; (ii)
nine features constructed using pseudo—amino acid composition
(Chou, 2001); (iii) two annotation-based features constructed using
two distinct groups of PROSITE patterns; (iv) six annotation-based
features based on GO—annotations.

5.2 Experimental setting and performance measures

We compare the performance of MDLoc to that of our prelimin—
ary system (Simha and Shatkay, 2014) and to other systems, spe—
cifically, YLocT (Briesemeister et al., 2010a), Euk—mPLoc (Chou
and Sheri, 2007), WoLF PSORT (Horton et al., 2007)
and KnowPredsire (Lin et al., 2009), whose results on the multi—
localized proteins are described in a previously published compre—
hensive study by Briesemeister et al. (2010a). The comparison uses
the exact same dataset from that study, and employs multiple runs
of stratified 5—fold cross—validation. That is, we ran 5—fold—cross—
validation five complete times (25 runs in total), using a different
five—way split each time. The use of multiple runs with multiple
splits helps validate the stability and the significance of the results.
The total training time for our system for the 25 training
experiments is about 8 hours (wall—clock), when running on a
standard Dell Poweredge machine with 32 AMD Opteron 6276
processors.

To formally define the evaluation measures we use, let D be a
dataset containing proteins. For a given protein P, let
MP : {5,- ] If : 1, where 1 S i S q} be the set of locations to which
protein P localizes according to the dataset, and let

A P
MP : {5,- ] l- : 1, where 1 S i S q} be the set of locations that a

1

classifier predicts for P, where l is the 0/1 prediction obtained for
location 5,. We use adapted measures of multi—label precision and re—
call denoted Presi and Recsi and defined as follows (Briesemeister
et al., 2010a):

1 1MPnMP]

resi z—APX A—P;
HP 6 DlSi E M }l Pemsiéw W l

1 \MPﬂMP]

Recsi z—Px —P
]{PeD]s,eM }] ]M]

PED]erMP

We also use the adapted measure of accuracy proposed by
Tsoumakas et al. (2010) for evaluating multi—label classification.
Some of these measures have also been previously used for multi—
location evaluation (Briesemeister et al., 2010a; He et al., 2012).

112 /310'S[BIIJHO[pJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 11101} popco1umoq

9103 ‘0g isanV uo ::

Protein (multi—llocation prediction

i371

 

The multi—label accuracy and the F1—label score used for the evalu—
ation of YLocT (Briesemeister et al., 2010a) are computed as:
1 (MP 0 MP

—) and F1—label :

Ace : _ A 1 22 X Presi >< Recsi.
)DlpeDlMP UMP) as

my Presi + Recsi

Finally, to evaluate the correctness of predictions made for each loca—
tion 5,, we use the standard precision and recall measures, denoted
by Pre—Stdsi and Rec—Stdsi and defined as: Pre—Stdsi : TP/(TP + FP)
and Rec—Stdsi : TP/(TP + FN), where TP (true positives) denotes
the number of proteins that localize to s,- and are predicted to local—
ize to 5,, PP (false positives) denotes the number of proteins that do
not localize to s,- but are predicted to localize to s,-, and EN (false
negatives) denotes the number of proteins that localize to s,- but
are not predicted to localize to s,-. The F1—score for location 5,- is
defined as:

2 X Pre—Stdsi >< Rec—Stdsi

F — , : —
1 scores“ (Pre—Stdsi+ Rec—Stdsi)

5.3 Classification results

In this section, we compare the performance of our system with that
of existing location prediction systems over the commonly used set
of multi—localized proteins. We also report experiments using the
combined set of single and multi—localized proteins as mentioned in
Section 5.1. Our analysis includes an examination of the per—
location break—up of the results. Additionally, we focus on several
specific examples demonstrating the benefit of incorporating loca—
tion interdependency into our prediction system.

Table 1A shows the F1—label score and the accuracy obtained by
our current system MDLoc compared with those obtained by other
multi—location predictors [YLoc+, Euk—mPLoc, WoLF PSORT and
KnowPredsire as reported by Briesemeister et al. (2010a) in Table 3]
and by our preliminary system (Bayesian network classifiers, denoted
BNCs, Simha and Shatkay, 2014), using the same set of multi—local—
ized proteins and evaluation measures. The table shows that MDLoc
performs better than the existing top—systems, including YLocT which
has the best performance reported so far and whose predictions are

based only on location—combinations in the training set. In contrast,
MDLoc is not limited to the location—combinations in the training set,
as it represents dependency of features on location—combinations in a
generalizable manner, and directly captures inter—dependencies among
locations. The only other system that attempts to capture such
dependencies is our preliminary system BNCs.

To illustrate the use of interdependency, consider the protein
Securin which is included in our dataset and localizes to both the
cytoplasm (cyt) and the nucleus (nuc). Securin, initially present in
the cytoplasm, translocates to the nucleus in response to DNA
damage (Kim et al., 2007). While MDLoc assigns it to both the cyt
and the nuc, YLocT assigns it to the nuc only. Our system utilizes
the dependency between nuc and cyt (represented by a directed
edge between the two locations, see Fig. 1) to make an accurate
multi—location prediction. Location dependencies reﬂect intrinsic
relationships that locations share with each other, and in this case,
it is well—known that proteins shuttle continuously between the nu—
cleus and the cytoplasm to control a variety of functions such as
cell cycle progression (Gama-Carvalho and Carmo-Fonseca,
2001). MDLoc’s benefit from capturing the interdependency be—
tween cyt and nuc is also reﬂected in its significantly higher
Multilabel—Precision and Multilabel—Recall (Presi and Recsi, re—
spectively) for the cyt and the nuc as shown in Table 1B. As an—
other example, consider Protransforming growth factor alpha
(TGF-alpba), a protein that assists in cell growth (See NCBI’s
Gene database, http://www.ncbi.nlm.nih.gov/gene/7039), localizes
to both the extracellular space (ex) and the plasma membrane
(mem), and is correctly assigned by MDLoc to both. Here MDLoc
employs the well—known dependency between the extracellular
space and the plasma membrane, as reﬂected for instance in the
exocytic trafficking pathway (Tokarev et al., 2000), and in the
transition of proteins such as bsp 90-alpba (initiated by TGF-
alpba) from the extracellular space to the plasma membrane in re—
sponse to stress (Cheng et al., 2008). Again, the value of utilizing
interdependencies is demonstrated in MDLoc’s significantly im—
proved precision in terms of Multilabel—Precision (Presi) on the ex
and mem proteins (while still retaining a similar level of recall,
Recsi, to that of YLoc+).

Table 1. Multi-location prediction results, averaged over 25 runs of 5-fold cross-validation, for multi-localized proteins only

 

 

 

 

 

 

(A)
MDLoc BNCS YLocJr Euk—mPLoc WOLF PSORT KnowPredsim
FI—label 0.71 (i 0.02) 0.66 (:r 0.02) 0.68 0.44 0.53 0.66
Am: 0.68 (i 0.01) 0.63 (:r 0.01) 0.64 0.41 0.43 0.63
(B)
cyt (2374) p—value nuc (2115) p—value mem (586) p—value ex (562) p—value mi (360) p—value
Recs; MDLoc 0.750 (10.012) <0.001 0.776 (10.014) <0.001 0.527 (:0.022) 0.01 0.547 (:0.035) 0.01 0.519 (:0.026) 0.04
YLOC+ 0.712 (:0.009) 0.728 (:0.011) 0.543 (10.018) 0.573 (10.026) 0.536 (10.031)
Pres; MDLoc 0.911 (10.008) <0.001 0.929 (10.008) 0.03 0.807 (10.036) <0.001 0.833 (10.044) <0.001 0.832 (10.042) <<0.001
YLOC+ 0.893 (:0.010) 0.924 (:0.008) 0.764 (:0.029) 0.740 (:0.053) 0.765 (:0.033)
Rec-Stdsi MDLoc 0.817 (10.021) <0.001 0.746 (10.028) <0.001 0.588 (:0.042) 0.04 0.385 (:0.058) 0.3 0.388 (:0.062) 0.03
YLOC+ 0.786 (:0.020) 0.684 (:0.015) 0.614 (10.042) 0.401 (10.037) 0.429 (10.060)
Prec-Stdsi MDLoc 0.942 (10.009) 0.01 0.904 (:0.014) 0.02 0.794 (10.039) <0.001 0.830 (10.046) <0.001 0.784 (10.057) <<0.001
YLOC+ 0.935 (:0.009) 0.914 (10.014) 0.730 (:0.047) 0.771 (:0.055) 0.670 (:0.055)

 

Standard deviations are shown in parentheses (if available). The highest values are shown in boldface. (A) Overall Fl—label scores and overall accuracy (Acc) ob—
tained using our current system MDLoc, our preliminary system (denoted BNCs, Simha and Shatkay, 2014), YLoc+ (Briesemeister et al., 2010a), Euk-mPLoc
(Chou and Shen, 2007), WOLF PSORT (Horton et al., 2007) and KnowPredsi,e (Lin et al., 2009). The four rightmost columns are taken directly from Table 3 in
the article by Briesemeister et al. (2010a). (B) Per location scores: Multilabel—Precision (Pre,,) and Recall (Recsl), as well as standard precision (Pre—Stdsl) and recall
(Rec-Stdsl), for each location 5,, for MDLoc and YLocT. Results for YLoc+ were reproduced using our ﬁve-way splits. The p—values indicate the statistical signiﬁ—
cance of the differences between the values obtained from MDLoc and from YLocT.

112 /310'S[BIIJHO[pJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 11101} popco1umoq

9103 ‘0g isanV uo ::

i372 R.Simha et al.

 

Table 2. Multi-location prediction results, per location, averaged over 25 runs of 5-fold cross-validation, for the combined set of single- and
multi-localized proteins

 

 

cyt (3785) p—value nuc (2952) p—value ex (1405) p—value mem (1824) p—value mi (870) p—value

Rec5i MDLoc 0.825 (10.009) <0.001 0.830 (10.010) <0.001 0.780 (10.020) <0.001 0.822 (10.012) <0.001 0.773 (10.013) <<0.001
BNCs 0.795 (:0.011) 0.784 (:0.017) 0.737 (:0.022) 0.780 (:0.014) 0.730 (:0.025)

Presi MDLoc 0.819 (10.013) 0.03 0.822 (:0.014) 0.02 0.864 (:0.020) <0.001 0.872 (:0.014) <0.001 0.861 (:0.024) 0.001
BNCs 0.809 (:0.018) 0.832 (10.013) 0.912 (10.019) 0.900 (10.012) 0.885 (10.023)

Rec—Std5i MDLoc 0.867 (10.015) 0.1 0.808 (10.021) <0.001 0.715 (10.030) <0.001 0.842 (10.017) <0.001 0.719 (10.028) <<0.001
BNCs 0.861 (:0.014) 0.736 (:0.031) 0.652 (:0.024) 0.805 (:0.017) 0.664 (:0.034)

Prec—Std5i MDLoc 0.854 (10.014) 0.001 0.783 (:0.020) 0.6 0.839 (:0.028) <0.001 0.882 (:0.014) <0.001 0.843 (:0.026) 0.001
BNCs 0.840 (:0.011) 0.786 (10.026) 0.906 (10.022) 0.900 (10.015) 0.873 (10.034)

 

The table shows the same measures used in Table 1B obtained over the combined dataset using our current system MDLoc, and using our preliminary system
(denoted BNCs) (Simha and Shatkay, 2014). The highest values are shown in boldface. The p—values indicate the statistical signiﬁcance of the differences between
the values obtained from MDLoc and those obtained from BNCs. Standard deviations are shown in parentheses.

Table 3. Multi-location prediction results, per location-combination, obtained using one run of 5-fold cross-validation, for multi-localized
proteins only

 

cyt_nuc(1882) ex_mem (334) cyt_mem (252) cyt_mi (240) nuc_mi(120) ER_eX(115) ex_nuc(113)

 

Both locations correct MDLoc 1253 (66.6%) 34 (10.2%) 31 (12.3%) 36 (15%) 15 (12.5%) 35 (30.4%) 51 (45.1%)
BNCs 976 (51.9%) 16 (4.8%) 15 (6%) 25 (10.4%) 11 (9.2%) 16 (13.9%) 54 (47.8%)
First location correct MDLoc 1603 (85.2%) 87 (26%) 186 (73.8%) 164 (68.3%) 43 (35.8%) 66 (57.4%) 73 (64.6%)
BNCs 1578 (83.8%) 60 (18%) 174 (69%) 165 (68.8%) 37 (30.8%) 66 (57.4%) 68 (60.2%)
Second location correct MDLoc 1481 (78.7%) 258 (77.2%) 82 (32.5%) 99 (41.3%) 67 (55.8%) 51 (44.3%) 72 (63.7%)
BNCs 1240 (65.9%) 246 (73.7%) 68 (27%) 85 (35.4%) 64 (53.3%) 27 (23.5%) 68 (60.2%)

 

For each combination, the table shows the number of proteins with correct predictions for both locations, for the ﬁrst of the two locations, and for the second
of the two locations, using MDLoc and using our preliminary system (BNCs, Simha and Shatkay, 2014). The highest values are shown in boldface.

As an example for MDLoc’s ability to handle proteins whose lo- does not simply assign a protein to each location whose probability
cation-combination is not included in the training set, consider is higher, but rather, it simultaneously considers a set of locations
Transmembrane emp24 domain-containing protein 7 (emp24). It 10— and assigns each protein to the set whose overall probability is high,
calizes to the ER and transports secretory proteins to the golgi com— leading to a higher precision.
plex (gol) (Belden and Barlowe, 1996). (The tables shown do not Table 2 shows the per—location prediction results on the com-
include ER and gol proteins, as the number of proteins from either bined dataset of both single- and multi—localized proteins obtained
of these locations in the dataset is very small.) MDLoc assigns by MDLoc, in comparision to those obtained by BNCs (Simha and

emp24 to both the ER and the gol, whereas YLoc+ assigns it to the Shatkay, 2014). While MDLoc’s precision values are somewhat
ER only. As indicated before, MDLoc makes use of the dependency lower than those of BNCs, MDLoc’s recall is typically higher.
which captures the relationship between the ER and the gol, both of MDLoc simultaneously infers the probability of a set of locations; in
which act as components in the exocytic trafficking pathway contrast, BNCs uses an independent Bayesian network structure to
(Tokarev et al., 2000). We thus see that MDLoc is not restricted to infer the probability of each location separately. As such, the likeli—

predicting only pre—defined location—combinations. hood of BNCs to correctly assign the combination of several loca—

Table 1B shows the per—location prediction results for multi— tions to a protein is much lower than its probability to correctly
localized proteins obtained by MDLoc compared with those assign a single location, which directly translates into a relatively
obtained by YLoc+ (Briesemeister et al., 2010a). Per—location predic— low recall measure. When using MDLoc, the increase in recall values
tions for the other systems are not shown here as they are not for almost all cases is higher than the decrease in the precision val—
publicly available. Results are shown for the five locations with the ues, except in the case of the extracellular space (ex). Notably, pro—
largest number of associated proteins. For each location s,-, we show teins in the extracellular space all originate from or are bound
Multilabel—Precision (Presi) and Multilabel—Recall (Recsi) as well as toward another location within the cell and as such predicting them
standard precision (Pre—Stdsi) and recall (Rec—Stdsi). For the cyto— as extracellular is challenging for most prediction systems.
plasm and the nucleus, which have a large number of proteins, the Moreover, MDLoc assigns some proteins hitherto known to lo—
precision and recall values obtained using MDLoc are significantly calize only to a single location into multiple locations. It is likely
higher in most cases than those obtained using YLoc+. For locations that at least some of these additional predicted locations are indeed
with much fewer proteins, while the recall values when using correct and can be the subject of an experimental validation. For in—
MDLoc are marginally lower than when using YLoc+, MDLoc’s stance, Calreticulin (Cal) is currently annotated by SwissProt as

precision values are typically significantly higher than those of localized to the ER only. However, MDLoc assigns it to both the
YLoc+. We note that YLoc+ assigns each protein to all the locations ER and the ex, and work by Gold et al. (2010) suggests that it in—
whose probability exceeds a pre—defined threshold; as such, the deed relocates from the ER to the ex.

number of locations it assigns exceeds that to which the protein ac— We also examine the statistically significant differences in the

tually localizes resulting in a lower precision. In contrast, MDLoc Multilabel—Recall for the location with the highest number of

112 /310'S[BIIJHO[pJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 11101} popco1umoq

9103 ‘0g isanV uo ::

Protein (multi—llocation prediction

i373

 

multi—localized proteins (cytoplasm, 2374 proteins) and the location
with the lowest number (endoplasmic reticulum, 115 proteins). The
Multilabel—Recall for cytoplasm (Reccyt) increases from 0.80 when
classifying using BNCs, to 0.83 when using MDLoc. Similarly, the
Multilabel—Recall for endoplasmic reticulum (RecER, not shown in
Table 2) increases from 0.64 to 0.69. This analysis demonstrates the
advantage of using MDLoc for predicting protein locations, not just
for locations that have a large number of associated proteins but
also for locations that are associated with relatively few proteins.

Table 3 shows the prediction results obtained using MDLoc in
contrast to those obtained using BNCs (Simha and Shatkay, 2014)
for all location—combinations, using multi—localized proteins only.
For each location combination in the dataset, we show the number
of proteins with correct predictions for both locations, as well as for
the first of the two locations, and for the second, separately. For al—
most all combinations, the number of proteins whose location is cor—
rectly predicted by MDLoc is significantly higher than the
corresponding number when using BNCs. We examine the predic—
tions for the location—combination with the highest number of pro—
teins (cytoplasm and nucleus—1882 proteins) and its constituent
locations (cytoplasm—1411 and nucleus—837 proteins). As can be
seen from the table, the number Of multi—localized proteins whose
combined—location is correctly predicted increases significantly from
976 when classifying using BNCs, to 1253 when using MDLoc. The
increase shows that location inter—dependencies learnt using MDLoc
help to improve predictions for multi—localized proteins.

6 Conclusion and future work

We presented a new probabilistic generative model for protein local—
ization based on Bayesian networks and a mixture model, and de—
veloped a system MDLoc, to predict multiple locations for proteins.
MDLoc takes advantage of the location inter—dependencies and lo—
cation—feature dependency to provide a generalizable method for
predicting multiple locations for proteins. Our results demonstrate
the utility of using location inter—dependencies in the prediction pro—
cess, and show that the performance of MDLoc improves over cur—
rent state—of—the—art reported results.

MDLoc significantly improves over our own preliminary method
which used a relatively simple collection of Bayesian network classi—
fiers (Simha and Shatkay, 2014) whose performance was on par
with that of YLoc+ (Briesemeister et al., 2010a). In our previous
method, location inter—dependencies were not learnt as part of the
model but rather captured based on simple estimates of location
values. In contrast, MDLoc uses a generative model
comprising Bayesian networks to directly address and capture inter—
dependencies among locations, and a mixture model to represent
feature dependency on location-combinations. We iteratively learn a
Bayesian network over location variables while estimating the loca—
tions using expectation maximization.

Our future work includes exploring alternative ways to learn the
mixture model parameters, to evaluate the model learned in each it—
eration of our current process, and to perform multi—location infer—
ence. We will also conduct experiments testing our system’s
performance on more complex location—combinations. Having a
larger set of multi—localized proteins from plant—, fungi— and animal—
specific organelles will also enable us to explore the possibility of
building a model for each taxonomic group.

As another direction, we will also experiment with features other
than the ones previously used by YLOC+, utilizing multiple data—
sources, which is likely to be more appropriate for representing pro—
teins in the context of multi—location prediction.

Acknowledgements

We are grateful to I. Simmons, C. Shannon and H. Wei for assisting us with
the development of a website to enable web access for MDLoc.

Conﬂict of Interest: none declared.

References

Alberts,B. et al. (2002) Molecular Biology of the Cell. Vol. 4. Garland Science,
New York.

Bakheet,T. and Doig,A. (2009) Properties and identiﬁcation of human protein
drug targets. Bioinformatics, 25, 45 1—45 7.

Belden,W. and Barlowe,C. (1996) Erv25p, a component of copii-coated ves—
icles, forms a complex with Emp24p that is required for efﬁcient endoplas-
mic reticulum to golgi transport. I. Biol. Chem, 271, 26939—26946.

B1um,T. et al. (2009) MultiLoc2: integrating phylogeny and Gene Ontology
terms improves subcellular protein localization prediction. BMC
Bioinformatics, 10, 274.

Briesemeister,S. et al. (2010a) Going from where to why — interpretable predic—
tion of protein subcellular localization. Bioinformatics, 26, 1232—1238.

Briesemeister,S. et al. (2010b). YLoc—an interpretable web server for predict—
ing subcellular localization. Nucleic Acids Res., 38(Web Server issue),
W497—W502.

Cheng,C. et al. (2008) Transforming growth factor alpha (TGFalpha)-
stimulated secretion of HSP90alpha: using the receptor LRP-l/CD91 to pro-
mote human skin cell migration against a TGFbeta-rich environment during
wound healing. Mol. Cell. Biol., 28, 3344—3358.

Chou,K. (2001) Prediction of protein cellular attributes using pseudo—amino
acid composition. Cell Mol. Life Sci., 43, 246—255.

Chou,K. et al. (2011) iLoc-Euk: a multi-label classiﬁer for predicting the sub-
cellular localization of singleplex and multiplex eukaryotic proteins. PLoS
One, 6, e18258.

Chou,K. and Shen,H. (2007) Euk—mPLoc: a fusion classiﬁer for large-scale eu—
karyotic protein subcellular location prediction by incorporating multiple
sites. I. Proteome Res., 6, 1728—1734.

Dempster,A. et al. (1977) Maximum likelihood from incomplete data via the
EM algorithm. I. R. Stat. Soc. Series B, 39, 1—38.

Dreger,M. (2003) Proteome analysis at the level of subcellular structures. Eur.
I. Biochem., 270, 589—599.

Emanuelsson,O. et al. (2000) Predicting subcellular localization of proteins
based on their N-terminal amino acid sequence. I. Mol. Biol., 300, 1005—
1016.

Gama-Carvalho,M. and Carmo-Fonseca,M. (2001) The rules and roles of
nucleocytoplasmic shuttling proteins. FEBS Lett., 498, 15 7—163.

Garg,A. and Raghava,G. (2008) ESLpred2: Improved method for predicting
subcellular localization of eukaryotic proteins. BMC Bioinformatics, 9, 503.
Gold,L. et al. (2010) Calreticulin: non-endoplasmic reticulum functions in

physiology and disease. FASEB I., 24, 665—683.

He,I. et al. (2012) Imbalanced multi—modal multi—label learning for subcellular
localization prediction of human proteins with both single and multiple
sites. PLoS One, 7, e37155.

HOglund,A. et al. (2006) MultiLoc: prediction of protein subcellular localiza—
tion using N-terminal targeting sequences, sequence motifs, and amino acid
composition. Bioinformatics, 22, 1158—1165.

Horton,P. et al. (2007) WOLF PSORT: protein localization predictor. Nucleic
Acids Res., 35 (Web Server issue), W585—W587.

Kim,D. et al. (2007). Securin induces genetic instability in colorectal cancer
by inhibiting double-stranded DNA repair activity. Carcinogenesis, 28,
749—75 9.

King,B. and Guda,C. (2007) ngLOC: an n-gram—based Bayesian method for
estimating the subcellular proteomes of eukaryotes. Genome Biol., 8, R68.
Li,L. et al. (2012) Prediction of protein subcellular multi—localization based on
the general form of Chou’s pseudo amino acid composition. Protein Pept.

Lett., 19, 375—387.

Lin,H. et al. (2009) Protein subcellular localization prediction of eukaryotes
using a knowledge-based approach. BMC Bioinformatics, 10 (Suppl. 15), 8.

Murphy,R. (2010) Communicating subcellular distributions. Cytometry A.,
77, 686—692.

112 /310'S[BIIJHO[pJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 11101} popco1umoq

9103 ‘0g15n8nv uo ::

i374

R.Simha et al.

 

Nair,R. and Rost,B. (2008) Protein subcellular localization prediction using
artiﬁcial intelligence technology. Funct. Proteomics, 484, 435—463.

Nakai,K. and Kanehisa,M. (1991) Expert system for predicting protein local-
ization sites in gram-negative bacteria. Proteins, 11, 95—110.

Pohlschroder,M. et al. (2005) Diversity and evolution of protein translocation.
Annu. Rev. Microbiol., 59, 91—111.

Rost,B. et al. (2003) Automatic prediction of protein function. Cell Mol. Life
Sci., 60, 2637—2650.

Russell,S. and Norvig,P. (2010) Artiﬁcial Intelligence—A Modern Approach,
3rd edn. Pearson Education, New Jersey, USA, 3rd edition.

Schiffer,M. et al. (1992) The function of tryptophan residues in membrane
proteins. Protein Eng, 5, 213—214.

Scholkopf,B. and Smola,A. (2002) Learning with Kernels: Support Vector
Machines, Regularization, Optimization, and Beyond. MIT Press, MA, USA.

Shatkay,H. et al. (2007) SherLoc: high-accuracy prediction of protein subcel—
lular localization by integrating text and protein sequence data.
Bioinformatics, 23, 1410—1417.

Simha,R. and Shatkay,H. (2014) Protein (multi—)location prediction: using lo—
cation inter-dependencies in a probabilistic framework. Algorithm Mol.
Biol, 9, 8.

Simpson,I. et al. (2000) Systematic subcellular localization of novel proteins
identiﬁed by large—scale cDNA sequencing. EMBO Rep., 1, 287—292.

Smith,A. et al. (2006) Computational inference of neural information ﬂow net-
works. PLoS Comput. Biol., 2, e161.

Tokarev,A. et al. (2000) Overview of Intracellular Compartments and
Trafﬁcking Pathways. Landes Bioscience, Texas, USA.

Tomicic,M. et al. (2013) Human three prime exonuclease TREXl is induced
by genotoxic stress and involved in protection of glioma and melanoma cells
to anticancer drugs. Biochim. Biophys. Acta 1833, 1832—1843.

Tsoumakas,G. et al. (2010). Mining multi-label data. In: Maimon,O. and
Rokach,L. (eds). Data Mining and Knowledge Discovery Handbook, 2nd
edn. Springer, Heidelberg, 667—685.

Zhang,S. et al. (2008) DBMLoc: a database of proteins with multiple subcellu-
lar localizations. BMC Bioinformatics, 9, 127.

112 /310'S[BIIJHO[pJOJXO"SOIJBHIJOJIIIOlq/ﬂdnq 11101} popco1umoq

9103 ‘0g15n8nv uo ::

