BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Givennsampl
Bsimulated aa:

Bu): A: Underlylng Irue
best error rate

m inl

Bax B: Eslirnatcdlrue
ﬁrrnr ratafrnm sim.

l' — “Inn ._

_ IEI:mm..

 

/3.10'S[Bum0[p10}x0'SOpBLUJOJuyqu”:duq

best aSSI Icatlon err rrate

sample size {N

probabl My Chosen as P

I-CI'IH K='1
knn k=3
Finn k=5
dlda

qda

par‘n
svr‘n-liﬁear
svm-radial
rf

nnet

 

Bias correction for selecting the minimal-error classifier

 

inﬂates the bias to the extent that may not be compensated by the de-
crease of the true best classiﬁcation error rate. In other words, examining
too many classiﬁers and choosing the best is not a good practice, espe-
cially if the added classiﬁers are likely not the top performers. Therefore,
caution is called in the small sample size regime when reporting the min-
imal error rate from multiple classiﬁers in practice, and it is advantageous
if the optimal classiﬁers can be applied as early as possible without adding
more low-performing classiﬁers. The theorems in the next subsection
show that the increasing magnitude of bias for small sample sizes or
large numbers of examined classiﬁers are common statistical properties
in data analysis.

2.3 Properties and asymptotic theorems of MEC bias
The proofs of the following theorems are included in the Appendix:

THEOREM 1. Given a smaller set of classiﬁers, adding more classifiers
will decrease the true best classiﬁcation error rate (i.e. Pj‘Ml 5 Pj‘Ma if
M 1 >M2).

THEOREM 2. For a given observed dataset DW from A", E(F’:‘M)<P:‘M,
where 152M = minlSmSM PM, and 15"," is the cross-validation error rate of
DW using classiﬁer m. In other words, the bias of MEC error rate bn‘M
= E0521”) — Pj‘M is strictly <0.

THEOREM 3. For observed datasets DW from A" of varying n and a
ﬁxed number of classiﬁers M 2 2, it holds that lim ,Hoo 15”M =
limnkoo,H00 Pj‘M. In other words, bn‘M —> 0as n —> 00 for ﬁxed M.

Theorem 1 shows that when we have two sets of classiﬁers and the
smaller set is a subset of the larger set, the larger set classiﬁers will yield
a smaller true best classiﬁcation error rate. Theorem 2 shows that the
expected MEC error rate E 152M always underestimates the true min-
imal error rate Pj‘M and the negative bias always strictly holds. This is
consistent with the result in Figure 3. In Theorem 3, when the number of
classiﬁers M is ﬁxed, the bias diminishes to zero as the sample size n
increases to inﬁnity. This is also consistent with the 10-classiﬁer result
in Figure 3b where the bias diminishes to around zero when n is beyond
320. According to Figure 3d, we observe that although the true best
classiﬁcation error rate does not decrease after the QDA is applied, the
MEC bias estimate continued to decrease as more classiﬁers are included.
This theoretical result brings clear caution to use MEC without bias
correction. In other words, if a researcher runs n = 2(P30 samples of
pilot study and examines M = 300 classifiers via conventional cross-
validation to choose the best, the minimal error rate from the 300
classiﬁers will likely generate low (or almost zero) error rate, while the
underlying true error rate may stay high. The researcher may be misled to
expand the study to a larger cohort or a prospective clinical trial, and
eventually ﬁnd it difﬁcult to validate the model and cannot translate into
a clinically useful diagnostic tool.

2.4 Three existing bias correction methods

In the literature, several methods have been developed to correct the
downward bias of the MEC error rate, and most have focused on cor-
recting the bias of parameter estimation via cross-validation for a given
machine learning method. Below, we introduce four bias correction meth-
ods that we will compare in this article (Bernau et al., 2013; Tibshirani
and Tibshirani, 2009; Varma and Simon, 2006). Bernau et a]. (2013) as-
sessed the condition with multiple machine learning methods, while the
others focused on correcting the bias of parameter tuning via cross-val-
idation (e.g. estimate K for KNN) for a given machine learning method.
In practice, if one considers many machine learning models along with
feature selection and parameter tuning, the number of classiﬁers (M)
examined can easily reach several hundreds. All three methods considered
here can be generalized to this situation.

Nested cross validation (nestedCV): Instead of using a single loop
cross-validation to ﬁnd the minimal error estimate for a particular clas-
siﬁer, nestedCV uses two CV loops (shown in Supplementary Fig. S1).
The dataset is initially divided into training and testing sets. Then
LOOCV is applied on the training set using all the classiﬁers, and the
classiﬁer with the smallest error rate is selected and used to build the
model based on the training set and then evaluate the error rate on the
testing set in the end. Therefore, the testing set is independent of the
model selection stage, including the selection of MEC. Finally, the pro-
cess is repeated until each sample acts as the testing set once; thus it is a
double LOOCV with two CV loops. The computation therefore scales
with the square of the sample size. Instead of LOOCV, it is possible to use
5-fold or 10-fold cross-validation to accelerate the computing when the
sample size is large.

Weighted mean correction (WMC/WMCS): The method (Bernau
et al., 2013) is proposed to be a smooth analytical alternative to
nestedCV, which is a weighted mean of the resampling error rates, ob-
tained using the different machine learning models/parameter values.
Instead of using cross-validation, it is based on repeated subsampling.
Then it estimates the unconditional error rate as a weighted sum of the
error rate of every classiﬁer on all the subsamples. The weights are esti-
mated with two variants, WMC and WMCS. Compared with nestedCV
in the original paper, the method is more stable and has a much lower
computational demand. We apply the R package CMA to implement this
method, and the subsampling fraction is chosen to be 0.8 in this study.

Tibshirani’s procedure (TT): E applies the idea of estimating the bias
and adding back to the minimal error rate estimate to correct for the bias
in the setting of K-fold cross-validation. It estimates the true best classi-
ﬁcation error rate as 215,,M — %Zf:113,,MJ{ where ELM is the biased
MEC error rate when sample size is n with M classiﬁers and PM“. is
the minimal error rate in the kth-fold among all classiﬁers (Tibshirani and
Tibshirani, 2009). It does not require a signiﬁcant amount of additional
computation as in nestedCV and scales linearly with the number of cross-
validation folds. Owing to the calculation of 15th Tibshiranis’ method
is not suitable for LOOCV or when the size of the left-out test set is too
small, and this estimate was shown to over-estimate the bias in some
settings (Bernau et al., 2011; http://epub.ub.uni-muenchen.de/12231/).
The Tibshiranis’ approach targets correcting the optimization bias of
the conditional minimal error rate, which heavily depends on the single
observed dataset, and the results are more variable. On the contrary, the
IPL approach proposed below considers correcting the optimization of
the unconditional minimal error rate by using a resampling technique to
take into account the sampling variation and, therefore, the results are
more reliable and stable.

 

 

2.5 The resampling-based IPL method

In this section, we propose a new resampling-based IPL method to cor-
rect the MEC error rate bias and estimate the true optimal classiﬁcation
error rate Pj‘M. By constructing learning curves for each individual clas-
siﬁer from repeated resampling of the original dataset at different sub-
sample sizes (Mukherjee et al., 2003), we could estimate the error rate of
each classiﬁer by ﬁtting a learning curve. Supplement Figure S1 shows the
concept of IPL for learning curve ﬁtting using 2D simulated data from
Section 2.2. Five simulations in each of the various sample sizes (n) are
performed, and the LOOCV error rates (P) from QDA are demonstrated.
The trend of decreasing error rates with increasing sample sizes is clear.
By ﬁtting an IPL function (P = a - n’“ + b; a, b, at > 0), the learning curve
can be well estimated.

Our proposed method applies the IPL concept using repeated subsam-
pling as follows. Consider sample sizes 15n1<n2<---<nL<n. For a
given machine learning method m, assume that the true error rate
equals Pm," and these true error rates follow an IPL function:
Pm," = amnl’a’" + hm. Normally, we assume am, bm, am > 0, as theoretically
larger sample size contains more information to produce a lower

 

3155

ﬁm'spzumol‘pmﬂo'sopeuuopuotq/ﬁdnq

lme opumal ervov vale
mmcv M=2

nestedCV we: a

1m: uylimnl :vmr rm

iPL M:Z
iPL M=m

Ime opumal error ram
ch5 M=2

chs M:Iﬂ

 

x lme opumal ervov vale
o 11 pm
A 11 Man

x 1m: uylimnl :vmr rm

a mac M:2
A ch M=m

 

 

duq

/310's112u1n0fp10}x0'sopeuiJOJuioiq”

an?kgogmomammowoio~&o:3m7.omm\

 

Y.Ding et al.

 

4 CONCLUSION AND DISCUSSION

With the advances of high—throughput genomic and proteomic
techniques, data are generated in an unprecedentedly increasing
pace. Machine learning methods have become a powerful tool in
almost all biomedical research of complex diseases to seek new
diagnostic or treatment selection tools. In most studies, small
sample sizes are encountered (n = 3&60), and researchers are
tempted to test many classiﬁers and select the best to report
(i.e. applying the MEC). In this article, we illustrated the down—
ward bias of MEC error rate when selecting from many machine
learning models in biomedical classiﬁcation problems. In the
application of high—throughput genomic data, this problem is es—
pecially magnified because the addition of feature selection easily
increases the number of classiﬁcation models to several hundreds.
We ﬁrst demonstrated the problem using a 2D toy example where
QDA is known to be the best classiﬁer. The simulation results and
asymptotic theoretical results both illustrated the need of bias
correction for MEC, especially when sample size (n) is limited
and the number of classiﬁers examined (M) is large. We discussed
three existing methods (nestedCV, WMC/WMCS and TT) and
developed a new IPL method from the concept of learning curve
ﬁtting. Application of all four methods to the 2D toy example,
ﬁve selected GEO datasets and two large breast cancer datasets
concluded that nestedCV and TT overestimated the error rate,
whereas WMC/WMCS produces a ﬂuctuating estimate around
the true estimates. IPL provided a stable and accurate solution.
The method has an additional advantage to extrapolate and pre—
dict the optimal error rate for larger sample sizes, a useful feature to
help decide whether it is worthwhile to expand the study to recruit
more samples. We note that nestedCV and WMC/WMCS target
the error rate of the wrapper algorithm, which is slightly different
from the MEC error rate we target in this article. This is consistent
with the result that bias corrections by nestedCV and WMC/
WMCS are less accurate and more ﬂuctuating. Our proposed
IPL method has the advantage of directly targeting the MEC
error rate and completely avoid the wrapper algorithm issue.

Our article provides a careful framework and theoretical in—
vestigation of the problem, and our result shows that severe bias
can be generated for MEC with small sample size (e. g. n = 3(P
60) and a large number of classiﬁers (e.g. M = 300). Without
bias correction, one runs the risk of obtaining an overly optimis—
tic error estimate of the classiﬁcation model, excitedly expanding
the investigation to larger independent cohorts and, eventually,
failing to validate and translate into a useful clinical tool. The
IPL method we proposed in this article not only generates more
accurate bias correction but also provides extrapolation esti—
mates to determine whether larger cohorts might warrant im—
proved accuracy. In the era of pursuing translational research
and personalized (or precision) medicine, rigorous evaluation
and interpretation of the machine learning results are essential
to evaluate the clinical potential of a research finding.

There are a few limitations or considerations for our study.
First, the IPL method has the modeling assumption that learning
curves of each classiﬁer could be fitted well by IPL. Although
there is no theoretical proof that this always holds, our simula—
tion and real data showed good ﬁt to the assumption. Second,
the curve ﬁtting of the IPL method relies on subsampling at
smaller sample sizes. Therefore, if the original sample size is

small, IPL will yield an unstable estimate. Third, the IPL meth—
ods are more costly compared with WMC/WMCS because of its
need to subsample at different sample sizes. However, because all
the classiﬁers are fitted independently, it is easy to parallelize the
computation. Last, we sum up all different feature selections,
machine learning methods and their associated parameter setting
into M classiﬁers in the investigation. Theoretically different
sources of classiﬁers have different correlated performance.
Understanding their correlations may elucidate the contribution
of bias from different sources and develop a better solution. In
addition, we demonstrated the idea that one should include high—
performing machine learning methods in the selection and avoid
adding low—performing methods. In practice, one may determine
high— and low—performance methods from empirical studies (e. g.
comparison of performance in similar studies in large databases,
such as GEO). How to systematically integrate the information
to decide the set of classiﬁers for investigation is still an
open question. All code and source ﬁles are available at http://
tsenglab.biostat.pitt.edu/publication.htm to reproduce the results
in the article. An R package ‘MLbias’ is also available.

ACKNOWLEDGEMENT

The authors would like to thank suggestions from the reviewers
that have signiﬁcantly improved this article.

Funding: (NIH R21MH094862).

Conﬂict of interest: none declared.

REFERENCES

Allison,D.B. et a]. (2006) Microarray data analysis: from disarray to consolidation
and consensus. Nat. Rev. Genet, 7, 55—65.

Bernau,C. et a]. (2011) Correcting the optimally selected resampling—based error
rate: a smooth analytical alternative to nested cross—validation. In: Technical
report. Department of Statistics, University of Munich.

Bernau,C. et a]. (2013) Correcting the optimal resampling—based error rate by esti—
mating the error rate of wrapper algorithms. Biometrics, 69, 6937702.

Berrar,D. et a]. (2006) Avoiding model selection bias in small—sample genomic
datasets. Bioinformatics, 22, 124571250.

Boulesteix,A.—L. and Strobl,C. (2009) Optimal classiﬁer selection and negative bias
in error rate estimation: an empirical study on high—dimensional prediction.
BMC Med. Res. Met/1040]., 9, 85.

Curtis,C. et a]. (2012) The genomic and transcriptomic architecture of 2,000 breast
tumours reveals novel subgroups. Nature, 486, 3467352.

Dupuy,A. and Simon,R.M. (2007) Critical review of published microarray studies
for cancer outcome and guidelines on statistical analysis and reporting. J. Natl
Cancer Inst., 99, 1477157.

Efron,B. (2009) Empirical Bayes estimates for large—scale prediction problems.
J. Am. Stat. Assoc., 104, 101571028.

Fu,W.J. et a]. (2005) Estimating misclassiﬁcation error with small samples via boot—
strap cross—validation. Bioinﬁ)rmatics, 21, 197971986.

Mukherjee,S. et a]. (2003) Estimating dataset size requirements for classifying DNA
microarray data. J. Compat. Biol., 10, 1197142.

Slawski,M. et a]. (2008) CMA: a comprehensive bioconductor package for super—
vised classiﬁcation with high dimensional data. BMC Bioinformatics, 9, 439.
Tibshirani,R.J. and Tibshirani,R. (2009) A bias correction for the minimum error

rate in cross—validation. Ann. App]. Stat., 3, 8224529.

Varma,S. and Simon,R. (2006) Bias in error estimation when using cross—validation
for model selection. BMC Bioiiy’ormatics, 7, 91.

Wood,I.A. et a]. (2007) Classiﬁcation based upon gene expression data: bias and
precision of error rates. Bioiiy’ormatics, 23, l36¥1370.

Youseﬁ,M.R. et a]. (2010) Reporting bias when using real data sets to analyze
classiﬁcation performance. Bioinformatics, 26, 68776.

 

3158

ﬁm'spzumol‘pmﬂo'sopeuuowtotq/ﬁdnq

