ORIGINAL PAPER

Vol. 30 no. 9 2014, pages 1273-1279
doi:1 0. 1093/bioinfonnatics/btt773

 

Systems biology

Advance Access publication January 8, 2014

Characterizing cancer subtypes as attractors of

Hopfield networks

Stefan R. Maetschke1 and Mark A. Ragan1‘2'*

1The University of Queensland, Institute for Molecular Bioscience, Brisbane, QLD 4072, Australia and 2Australian
Research Council Centre of Excellence in Bioinformatics, Australia

Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: Cancer is a heterogeneous progressive disease caused
by perturbations of the underlying gene regulatory network that can be
described by dynamic models. These dynamics are commonly mod-
eled as Boolean networks or as ordinary differential equations. Their
inference from data is computationally challenging, and at least partial
knowledge of the regulatory network and its kinetic parameters is
usually required to construct predictive models.

Results: Here, we construct Hopfield networks from static gene-
expression data and demonstrate that cancer subtypes can be char-
acterized by different attractors of the Hopfield network. We evaluate
the clustering performance of the network and find that it is compar-
able with traditional methods but offers additional advantages includ-
ing a dynamic model of the energy landscape and a unification of
clustering, feature selection and network inference. We visualize the
Hopfield attractor landscape and propose a pruning method to gen-
erate sparse networks for feature selection and improved understand-
ing of feature relationships.

Availability: Software and datasets are available at http://acb.qfab.
org/acb/hclust/

Contact: m.ragan@uq.edu.au

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on September 20, 2013; revised on December 4, 2013;
accepted on December 30, 2013

1 INTRODUCTION

Cellular function and development are controlled by networks
that regulate the expression of genes. The importance of precise
regulation of gene expression through development and cellular
differentiation is well known and perturbations of gene regula-
tion are associated with many complex diseases including cancer
(del Sol et al., 2010; Pe’er and Hacohen, 2011).

At any point in time, the current state of a cell can be
described by its expression proﬁle (transcripts and their concen-
trations), and the set of all possible states deﬁnes the state space
of the cell (Huang et al., 2005). Most states are unstable, and the
system will converge to a stable state of low energyian attractor
stateiwhen perturbed (Huang, 2009). Waddjngton’s epigenetic
landscape of cell development (Waddington, 1940, 1957) and
Kauffman’s attractor networks (Kauffman, 1969a,b) are ex-
amples of state space models of gene regulatory networks in

 

*To whom correspondence should be addressed.

which attractors represent stable low-energy states that emerge
from the dynamics of the underlying network. In such models,
attractor states correspond to normal developmental stages of
the cell (Huang et al., 2005) or disease states such as cancer
(Huang et al., 2009b); trajectories in state space describe devel-
opmental processes or cancer progression (Huang, 2009; Huang
and Ingber, 2006), and the energy (elevation of the landscape) is
related to the transition rates or transition probabilities between
states (Bhattacharya et al., 2011; Wang et al., 2011).

The existence of attractor states and of state-space trajectories
of cell differentiation has been experimentally veriﬁed (Chang
et al., 2008; Huang et al., 2005; Huang et al., 2009a) and different
methods (Layek et al., 2011b) have been used to model such
attractor systems, with Boolean networks (BNs) and ordinary
differential equations (ODEs) being the most common ones
(Cheng et al., 2012). Current methods are unable to describe
the full complexity of a complete gene regulatory network, and
applications are, therefore, focused on speciﬁc aspects of subsys-
tems. For instance, recent cancer-related applications of BNs
include cancer networks described by logic circuit stuck-at fault
models (Layek et al., 2011a; Lin and Khatri, 2012), models of
signaling and DNA repair pathways (Esfahani et al., 2011;
Fumia and Martins, 2013; Saez-Rodriguez et al., 2011;
Rodriguez et al., 2012), models of cell adhesion for tumorigenic
cells (Guebel et al., 2012) and cancer networks modeling the G1 /
S transition Wang et al., 2013). ODEs have been used to model
cancer cell populations (Gyori et al., 1988; Kansal et al., 2000),
immune response (de Pillis et al., 2005; KoleV, 2003; Lucia and
Maino, 2002) and therapy (d’Onofrio et al., 2009; Isaeva and
Osipov, 2009). Cheng et a]. (2012) provide a comprehensive
overview.

BN— and ODE-based models are typically small (a few genes),
manually crafted and require intricate explicit knowledge of the
regulatory subsystem and its molecular kinetics. There have been
attempts to construct such networks from data, but this is difﬁ-
cult due to the complexity of the networks and the lack of suit-
able time-series data (Esfahani et al., 2011). Here, we propose
Hopﬁeld networks as a novel approach to efﬁciently construct
large attractor networks from static gene-expression datai
neither pathway data nor kinetic information is required. It
has been argued that cancer is an intrinsic attractor state of the
molecular-cellular network (Ao et al., 2008; Huang, 2011), and
here we demonstrate that attractors of Hopﬁeld networks
can correspond to cancer subtypes. Speciﬁcally, we construct
Hopﬁeld networks for numerous cancer datasets, cluster the
samples according to their attractor association and measure

 

© The Author 2014. Published by Oxford University Press. All rights resen/ed. For Permissions, please e—mail: journals.permissions@oup.com 1273

112 /310's112u1n0fp10}x0"sotwuiJOJutotq/ﬁduq mot; popcolumoq

91oz ‘Og anﬁnV uo ::

S.R.Maetschke and M.A.Ragan

 

how accurately the clusters reﬂect the true subtypes of the data-
set. In addition, we use Hopﬁeld networks to perform feature
selection, network pruning and network inference and present a
novel method to visualize the attractor landscape of the network.

It is important to appreciate that two levels of description are
in play: the real physical network of transcription factors and
other biomolecules that, by regulating gene expression, generates
and maintains the cell state; and the Hopﬁeld network, the struc-
ture of which we infer from observed data (here, sets of gene-
expression patterns) indicative of this cell state. Each level uses
concepts of states, trajectories and attractors. Here, we explore
mappings between these concepts, e.g. between cancer subtypes
as attractors in a Waddington landscape and attractor states in a
Hopﬁeld model.

2 METHODS AND RESULTS

In the following sections, we ﬁrst describe the data used, then introduce
Hopﬁeld networks and describe their application to the clustering of
cancer subtype data. We then present novel approaches to prune net-
works and to visualize their high-dimensional energy functions. Finally,
we perform an extensive comparison of Hopﬁeld networks with other
common methods for the clustering of cancer data.

2.1 Data

We used two data suites to study Hopﬁeld network and their perform-
ance. First, we examine a suite containing 12 cancer microarray datasets
with clinically defined subtypes created by Wang et a]. (2012) with the
purpose of comparing different feature selection methods.

The second is a suite containing 35 cancer gene-expression datasets
generated by de Souto et a]. (2008) for the evaluation of clustering meth-
ods. The datasets in this suite have been preprocessed: 10% of the largest
and smallest values are discarded, and only genes showing a sufﬁciently
high fold-change have been selected [see de Souto et a]. (2008) for details].
Missing data were detected in dataset alizadeh—ZOOO—v3, and after
consultation with the authors the two rows concerned were replaced by
the corresponding rows from dataset alizadeh—ZOOO—VZ.

For all datasets, we performed sample-wise Z-score normalization, and
in the case of single-channel arrays a gene-wise log2 transformation was
applied before normalization.

The example data used in the Sections 2.2725 are a reduced version of
the yeoh-ZOOZ—v] dataset from de Souto’s suite. Originally created by Yeoh
et a]. (2002), it contains gene-expression data of acute lymphoblastic leu-
kemia (ALL), with 43 samples labeled as subtype T-ALL and 205 samples
labeled as subtype B-ALL. We took the 43 samples of class T-ALL and the
ﬁrst 43 samples from class B-ALL in the order they appeared in the original
dataset, and then extracted the 50 most-variable genes (largest variance) to
construct a smaller balanced dataset with 50 genes and 86 samples. The
comparison of clustering performance in Section 2.6 was performed on the
Wang and the de Souto suites described earlier in the text.

2.2 Hopﬁeld network

A Hopﬁeld network is a graph composed of nodes [6 1, ...,n and
weighted but undirected edges w,-,~ between nodes i and j. In the original
Hopﬁeld network, nodes have binary states s,- e {0, + 1}. Here, we use a
slight modiﬁcation with ternary states s,- 6 {—1,0, + 1}, with the only
purpose to simplify the pruning of the network described later. The
dynamic of the network is deﬁned by the following recursive formula

n
SEHrl) = sgn  was?) , (l)
1'

where s?) is the state of the i-th node at time step t and sgn(>) is the

signum function

+1 for x> 0
sgn(x) = 0 for x = 0 (2)
—1 for x< 0.

Given any initial state 50 = (s‘l), . . . ,.s?,)T in state space (5, for t —> co
the network states 5“) will converge to an attractor state with monoton-
ically decreasing energy

1 " 1
E(s) = —§Zs,-w,-jsj = —§sTWs. (3)
131'

Attractors are local minima of the energy function E(>), and the system
can be interpreted as relaxing from a higher energy state to a minimum
energy state. States or patterns pk e (5, k e {1, . . . ,m} can be established
(stored) as attractors of the network using Hebbian learning

1 m
W=—ZPkPkT—Ia (4)
m k

where network weights are computed as the sum over the outer products
of the pattern vectors. The resulting weight matrix W is a normalized
covariance matrix of the patterns pk but with a zero diagonal. Initializing
the network state 50 to some input pattern and relaxing the network using
Equation (1) allows the recall of stored patterns even when the input
patterns are distorted or noisy.

2.3 Clustering

Here, we use Hopﬁeld networks to characterize different cancer subtypes
as network attractors. Let P 2 (p1, . . . , pm) be a matrix of patterns, con-
structed by discretizing the data D of a gene-expression matrix
P = sgn(D), where columns pk contain samples from different cancer
subtypes and rows are genes.

We ﬁrstly construct a network W from P using Hebbian learning [see
(4)], and then recall each sample until it reaches its attractor, using
Equation (1). In matrix notation a recall step (relaxation) for all patterns
in P is simply the product of the pattern matrix and the weight matrix W
followed by discretization

S(t+1) = Sg"(S(t)W), (5)

with 8(0) 2 P. Each column of the state matrix Sm describes one of the pk
patterns after the t-th recall step.

We do not label patterns by cancer subtype class but instead leave it to
the learning algorithm to construct attractors that capture the similarities
between patterns. Speciﬁcally, we hope that the algorithm creates attrac-
tors that group samples according to subtype, i.e. essentially functions as
a clustering method. During recall, input patterns would then converge to
one of the attractors, where the state vector of the attractor represents the
label and molecular signature of a specific subtype.

Figure 1(a) displays the expression matrix D for ALL data and state
matrices 8(0) . . .S(7) over eight iterations of recall (matrices are transposed
to preserve space). DT has 50 columns (genes) and 86 samples (rows),
with the top half containing samples of subtype B-ALL and the lower
half of subtype T-ALL. In the last iteration, the state matrix is stable,
indicating that all samples have converged to their respective attractors.
Inspection of 8(7) reveals only two different states (attractors) and only
one sample that has converged to the wrong attractor (marked by a
triangle).

Figure 2(a) shows the corresponding weight matrix of the network.
Large weights (dark blue or red color) tend to occur in the upper left part
of the weight matrix. This is because the genes in the ALL dataset were
selected and ordered according to their variance (see Section 2.1).

To summarize, we constructed a Hopﬁeld network from static gene-
expression data and let samples converge to the network attractors.

 

1 274

112 /310'S[BIIJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”Idllq moi; papBoIH/noq

91oz ‘Og isnﬁnV uo ::

Characterizing cancer subtypes

 

(3)

expression data

network states

      

nodes

 

T T T
8(0) 3(1 ) S(2)

   

attractor states

 

T T T T T
3(3) 3(4) 3(5) 3(6) 3(7)

Fig. 1. Relaxation of all samples over eight iterations to their attractor states for the (a) unpruned and (b) the pruned weight matrix. Triangles in the state
matrices of the last iteration mark samples that converged to the wrong attractor (misclassiﬁed)

(a) _ .,,,.-. _ ..-__. (b). ..,,.., . .
 lrl'.I-#;nll.4"'~:l_a: I 1' Lg. m.- I x l l' -

'11.- " -| '- 11-4 rai- - "I +' ' ' M
ﬂag-,3 'E‘gﬁ-ﬁ' "31-3 3,, :. : .  ' 7 ' 
E BEES-'1' *iﬁ‘ﬁh'hz'l'” I. I - I. ."I‘ =1 .-' ' I '

“ “.93; a?“ "3'  ' I :-: i '. ' . 0‘2
émil 'I‘  if. g? .5111"; . . u'. ' 'I 0.0
=' wrig- 'i“”‘- :-:-..-i -'. " .. .

3,319.“ _ r -.-.‘,,§'_'="-'_"- . : ' ." .. M

H. ‘35- .‘a_l.":."l:i..'..-.;_. ':‘ -o.4

l - - y t t. .._ .

I "  (11"‘2", - :f'ﬂq _ 06

Fig. 2. Weight matrix of the Hopﬁeld network for the ALL data
(a) before and (b) after pruning. Blue color indicates positive weights
(coexpression of genes), read color indicates negative weights (anti-
coexpression) and yellow color indicates zero weights (no coexpression)

We use the network as a clustering algorithm in which clusters are iden-
tiﬁed by attractor states that represent molecular signatures and class
labels of subtypes. We do not use labeled samples or specify the
number of clusters during the learning phase.

2.4 Pruning

The weight matrices created by Hebbian learning are typically dense [see
Fig. 2(a)], in contrast to gene regulatory networks that tend to be sparse
(Huang et (11., 2005). Although the learning algorithm does not aim to
provide an accurate reconstruction of the regulatory network underlying
the expression data, a sparse network is nevertheless preferable, as
memory consumption is lower, recall is more efﬁcient, and most import-
antly, relationships between genes are more easily understood.

In the following, we introduce a method to prune the network without
destroying its capability to classify samples. To this purpose, we set all
weights wij with an absolute value smaller than a threshold 1: to zero

wil- for lWijl Z ‘L'

W“) = { 0 for |w,-j|<t, (6)

and determine the largest threshold rm,“ that does not reduce the esti-
mated clustering accuracy ER](t) of the network by more than a given
factor a

‘L'. (7)

‘L' = max
3.1. ERI(1)> 1—0.

Let Cm“ be the true sample labels (e.g. subtypes), C0 the sample labels
predicted by the network for the unpruned matrix W(0) and C, the labels
predicted for the pruned matrix W(‘L'). The true clustering accuracy
TRI(r) of the network for a speciﬁc threshold 1: can be determined by
comparing the true labels Cm“ with the predicted labels C,. Following de
Souto et a]. (2008), we also use the Adjusted Rand Index ARI(-, -) by
Hubert and Arabie (1985) (see Supplementary Material) to measure the
clustering accuracy.

TRI(r) = ARI(Cm, C.) (8)

The ARI is 1 for a perfect prediction and negative close to zero for
predictions that are not better than chance. The computation of this
True Rand Index TR](t) requires, however, the true labels CW3, which
are unknown in practice. Instead we compute an Estimated Rand Index

EH03) = ARI(C0, C.) (9)

by taking the accuracy of the unpruned network at r = 0 as a baseline
and assuming C0 to be the true labeling. As a consequence ER](0) is
always 1 and the ER] will follow the TR] closely, provided TRI(0) is
close to 1.

Figure 3 plots the TR], the Estimated Rand Index ER] and the density
of the weight matrix for different pruning thresholds ‘L'. The plots for TR]
and ER] are almost identical, and for an allowed 20% decrease (at = 0.2)
of the ER] the maximum pruning threshold rm becomes 0.34. For this
threshold, the density of the weight matrix is reduced from 92% to 5%,
where density is the percentage of non-zero entries in the weight matrix.
Figure 2 displays the weight matrix (a) before and (b) after pruning.

This pruning method ensures that the overall clustering accuracy of the
network is largely maintained. However, what effect does pruning have
on the attractors of the network and on the labeling of samples? Figure 1
shows recall and the attractors for the (a) unpruned and (b) pruned net-
work. The expression matrix D and the state matrix Sm), which is simply
Sm) = sgn(D), are identical in both cases. Recall and attractors, however,
are different. Because the pruned network contains nodes (genes) that

 

1 275

112 /310'S[BIIJHO[pJOJXO'SOIJ’BLUJOJIIIOICI”Idllq mm; papBOIH/noq

9IOZ ‘OE ISUEHV Ho ::

S.R.Maetschke and M.A.Ragan

 

 

 

    

 

 

 

1-0 -- TRI ‘
+—+ ERI
-—- Density
0.8- .
I _ _ _ _ _ _ _
a
g 0.6»
a)
D
2‘
Lu
E: 0.4»
,_
0.2»
0.0- _
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8

 

 

 

Threshold

Fig. 3. True Rand Index (TRI), Estimated Rand Index (ERI) and density
of the weight matrix for different pruning thresholds 1:. Best threshold
rm,“ marked by dashed line

have lost all edges (interactions), their states become zero (yellow) and
remain zero [see Equation (1)]. Consequently, the attractor states in 8(7)
differ only for genes that interact (via their gene products) with other
genes, and the weight matrix [Fig. 2(b)] describes the strengths and signs
of these interaction. Only one sample is misclassiﬁed (Fig. 1, marked by a
triangle) for the pruned and the unpruned networks.

To summarize, we prune the network by removing small weights as
long as the estimated classiﬁcation accuracy decreases by no more than a
given percentage (e.g. or = 0.2). Pruning not only results in a sparse net-
work but also functions as a feature selection method. Genes without
interactions correspond to nodes ﬁxed to zero state that do not affect
attractors, and the molecular signatures that attractor states represent can
be compressed by removing those genes. The pruned network further-
more provides insight into the relationships of the genes that make up the
molecular signatures. Thus, Hopﬁeld networks provide a framework to
integrate clustering, feature selection and network inference.

2.5 Visualization

The energy function [see Equation (3)] of an attractor network is typically
high-dimensional and cannot be visualized directly. Current pictures of
energy landscapes are, therefore, either artistic renderings not based on
actual data or are limited to systems with only two genes. Here, we pro-
pose a simple method to visualize the energy landscape of networks with
arbitrary numbers of genes.

Let anm be a matrix with expression data, containing n samples in
rows and m genes in columns. First, we reduce the dimensionality of the
expression matrix to two dimensions using principal component analysis

Dn><2 = DnXmeXZa 

where the transformation matrix T is composed of the eigenvectors of
Dfmenxm, and mez contains the two eigenvectors of T with the largest
eigenvalues. After transformation, anz contains the ﬁrst and second
principal components of anm.

Next, we construct a regular 2D grid kaz with k 2D points in
the same range as the points in anz and perform an inverse princi-
pal component analysis to map the grid points to the high-dimensional

space
kam = GkXZTTIa 

where T’1 is the inversion of the transformation matrix T computed in
the ﬁrst step.

m —20
33‘ —40
3 —50

—8

    

S 12‘. pc'
10
Fig. 4. Energy landscape of the leukemia network. The x- and y-axes
show the ﬁrst and second principal components of dimensionality
reduced data. Red points indicate samples of the T-ALL subtype, blue
points indicate B-ALL samples and attractor states are in green

In the third step, we compute the energies Eg = E(kam) for the grid
data and E4 = E(anm) for the sample data in the high-dimensional
space, with

E(S) = —%STWS. (12)

By interpolating between the energy values Eg over the grid data kaz,
we can render the surface of the energy landscape in three dimensions.
Similarly, the expression data anz and their energies E; can be plotted.
Figure 4 depicts the energy surface (grid) of the Hopﬁeld network, the
samples of the leukemia data (red and blue dots) and the two attractor
states (green) of the network.

Several properties of the landscape are of interest. First, the attractors
are minima of the energy function, and their basins of attraction are
clearly visible. Second, in contrast to clustering methods such as
k-means, attractors are not the centers of the point clouds deﬁned by
the two subtypes. Instead, samples are divided by a ridge separating the
subtypes. Third, the landscape is shaped like a staircase due to the
discrete state function sgn(>) of the network nodes.

The Supplementary Material contains additional renderings of the
energy surface including a plot displaying the trajectories of samples
converging to their attractors. The visualization method is not limited
to Hopﬁeld networks but can be applied whenever there is an energy
function and a dataset with points of interest.

2.6 Clustering comparison

In Section 2, we applied Hopﬁeld networks to clustering, feature selection
and network inference on a small example dataset. Here, we focus on the
clustering aspect and study the performance of Hopﬁeld networks in
comparison with a selection of other clustering algorithms on a larger
suite of datasets.

Hopﬁeld networks classify samples without specifying the number of
clusters or any other parameter. Therefore, we limit our comparison with
clustering algorithms that also detect the number of clusters automatic-
ally such as DBSCAN (Ester et al., 1996), affinity propagation (Frey and
Dueck, 2007), mean shift (Barash and Comaniciu, 2004), k-means
(MacQueen, 1967) and Ward’s hierarchical clustering (Ward, 1963).
For the latter two, the silhouette coefficient (Rousseeuw, 1987) is

 

1276

112 /3.10's112u.1n0fp.10}x0"sotwurJOJutotq/ﬁduq 11101} papBOIH/noq

91oz ‘Og isnﬁnV uo ::

Characterizing cancer subtypes

 

computed to determine the number of clusters. All methods were
used with their default settings as implemented in the scikit-leam
Python library, with the exception of DBSCAN, for which the default
setting 6 = 0.0 resulted in a poor performance, and we chose 6 = 1.0
instead.

To avoid bias due to a speciﬁc feature selection method, we ﬁrstly
measure the clustering accuracy (ARI) of the various algorithms for
increasing numbers of genes (features), where genes are sorted according
to their variance (most-variable ﬁrst). Figure 5 plots the ARI averaged
over all datasets in Wang’s suite for 1(F2000 genes. Most clustering al-
gorithms reach their peak accuracy between 200 and 1000 features and
stabilize thereafter. Adding further features does not improve or decrease
the ARI substantially, with the notable exception of the HOPFIELD
method, which shows a strong loss in accuracy with increasing numbers
of genes (black curve). Apparently, the Hopfield learning algorithm is
sensitive to the growing noise and diminishing sample differences for
sample vectors of increasing dimensionality.

The poor performance of the Hopﬁeld clustering for larger number
of features can be avoided by using a similarity matrix instead of the
sample matrix S as input. Let D = [ai(s,-,sj)],-j be a matrix containing
the Euclidean distances d(, >) between sample vectors 5,- and 51-, and
M = 1 — D/max(D) the corresponding similarity matrix. Describing
samples not by their feature vectors (gene-expression values) but by the
similarities between feature vectors is a common transformation of input
data. For instance, afﬁnity propagation, mean shift and DBSCAN all
require the similarity matrix as input. We mark all methods using the
similarity matrix with a star (*).

Figure 5 shows that HOPFIELD* performs consistently better than
HOPFIELD and does not suffer from the decrease in accuracy for grow-
ing numbers of features. The accuracy of HOPFIELD* is comparable
with the best-performing methods KMEANS+SIL, WARD+SIL and
DBSCAN*, whereas AFFPROP* and MEANSHIF* achieve consider-
ably lower accuracies over the entire range of feature numbers.

In Section 2.4, we introduced pruning for Hopﬁeld networks; here we
are interested in its impact on the clustering accuracy. In contrast to the
previous evaluation of accuracy for different numbers of features, in
the following we select the number of features that explains most of the
variance in the data. Speciﬁcally, we rank genes in Wang’s datasets ac-
cording to variance, plot variance versus the number of genes and ﬁnd the
elbow of the plot (point closest to origin). This common method to select
features does not require labeled samples (in contrast to fold-change, for
instance). Feature selection for de Souto’s datasets was performed by
de Souto et a]. (2008) (see Section 2.1), and we use the original data.

Table 1 lists the prediction accuracies (ARI) averaged over Wang’s and
de Souto’s data suites for the different clustering algorithms including
Hopﬁeld clustering with (HOPFIELD+P*, HOPFIELD+P) and without
pruning (HOPFIELD*, HOPFIELD). As the data show, pruning usually
improves prediction accuracy of Hopﬁeld networks slightly and leads
to sparser networks (95 —> 35% density for or = 0.2) but has the disad-
vantage of increasing the computation time.

The feature selection method described earlier in the text tends to result
in rather large feature sets (> 2000 genes) for Wang’s datasets, and the
accuracy of the HOPFIELD method is therefore poor (see Fig. 5). Using
the similarity matrix as input greatly improves Hopﬁeld clustering (com-
pare HOPFIELD with HOPFIELD*) but is not beneﬁcial to k-means
and Ward’s hierarchical clustering in most cases. The best-performing
methods are WARD+SIL and KMEANS+SIL. DBSCAN* is the fastest
method and highly accurate on Wang’s data but shows low accuracy on
de Souto’s data. HOPFIELD+P* is the fourth and fifth ranking method
on Wang’s and de Souto’s datasets, respectively. Although Hopﬁeld clus-
tering is not the best-performing method, it is the simplest, with average-
to-good accuracy and speed.

Overall prediction accuracies are fairly low, but it has to be taken into
account that most datasets are composed of many subtypes and that

 

0:40— i  M
Ml . It, “ 'I‘l
0.35-  Wiuﬂw

 

 

 

 

 

 

0.25- .
E i
< l
0.20 -
0.15 ‘
— KMEANS+SIL
— WARD+SIL
0-10' — DBSCAN“ '
— AFFPROP*
005. — MEANSHIFPf _
— HOPFIELD*
— HOPFIELD
0.000 500 1000 1500 2000

 

#genes

Fig. 5. Adjusted Rand Index (ARI) of different clustering algorithms
averaged over Wang’s datasets for increasing numbers of genes (features).
Genes were ranked and selected by variance. Methods marked with as-
terisk operate on the similarity matrix instead of the sample matrix

Table 1. Comparison of clustering methods

 

 

Method Wang Time de Souto Time
ARI :l: std ARI :l: std
KMEANS+SIL 0.373 :I: 0.29 6.65 0.305 :I: 0.29 20.40
WARD+SIL 0.417:I:0.31 3.52 0.254:I:0.24 5.96
KMEANS+SIL* 0.340:I:0.25 5.50 0.300:l:0.29 11.12
WARD+SIL* 0.336:l:0.24 2.31 0.271 :l:0.29 4.14
DBSCAN* 0.374 :I: 0.32 0.33 0.142 :I: 0.20 0.39
AFFPROP* 0.283:I:0.19 0.51 0.165:l:0.14 1.66
MEANSHIFT* 0.260 :I: 0.28 1.49 0.075 :I: 0.21 6.00
HOPFIELD* 0.356 :I: 0.24 1.41 0.242 :I: 0.22 3.36
HOPFIELD+P* 0.366 :I: 0.25 5.65 0.249 :I: 0.22 13.75
HOPFIELD 0.069:l:0.19 7.65 0.129:I:0.18 17.96
HOPFIELD+P 0.073 :l:0.19 7.79 0.128:I:0.17 18.54

 

Note: Adjusted Rand Index (ARI) with standard deviation (std) and computation
time (wall time) in seconds for different clustering algorithms averaged over datasets
in Ward’s and de Souto’s suite. Methods marked with star (*) use the similarity
matrix instead of the sample matrix as input.

accuracy varies greatly between datasets and clustering algorithms (see
Supplementary Material). Which algorithm performs well depends
strongly on the dataset. We recommend, therefore, that a variety of
methods should be evaluated for any given dataset.

3 DISCUSSION

3.1 Hopﬁeld networks as cancer attractor model

Following the hypotheses that cancers are attractors within gene
regulatory networks (Ao et al., 2008; Huang, 2011), we con-
structed Hopfield networks from cancer expression data and
demonstrate that network attractors correspond to cancer sub-
types. Hopﬁeld networks are simple models, and because they
are inferred from static data, they cannot be expected to model

 

1 277

112 /3.10's112u.1n0fp.10}x0"soiwurJOJuioiq/ﬁduq 11101} papBOIH/noq

91oz ‘Og isnﬁnV uo ::

S.R.Maetschke and M.A.Ragan

 

the topology or the dynamics of the real regulatory network with
great accuracy. Inference of networks from data is ill-posed in
general, and different networks can generate the same dynamics
(Hickman and Hodgman, 2009). Despite these limitations,
Hopﬁeld networks can capture important properties of real
physical networks. As we have shown, they can address cancer
subtype characterization in the framework of state-space attrac-
tors. As coexpression networks themselves, they can provide
valuable information about relationships among genes and
enable the selection of important genes.

For other attractor network models, the energy function is
related to the transcription rates or probabilities between states
(Bhattacharya et al., 2011; Wang et al., 2011). The Hopﬁeld model
is derived from the Ising model (Ising, 1925) in which energy is
correlated with the probability of a state. However, Ising models
are not constructed by Hebbian learning, nor are standard
Hopﬁeld networks probabilistic. Therefore, it is not straightfor-
ward to link Hopﬁeld energies with cell-state probabilities.

Finally, the inner working of Hopﬁeld models and of real
regulatory models is fundamentally different, and thus their
overall dynamics should be expected to differ. Exploration of
this potential mapping is not made easier by the paucity of
data about network states underlying real cell-state attractors.
Nevertheless, as shown here, Hopﬁeld networks can be used to
derive molecular signatures for cancer subtypes from their
Hopﬁeld attractors.

3.2 Alternative learning algorithms

We used Hebbian learning to construct the weight matrix of the
Hopﬁeld network. For networks with sufﬁciently many edges,
and for a broad range of topologies, Hebbian learning creates
stable attractors (Bar-Yam and Epstein, 2004). For randomly
chosen patterns, the maximum asymptotic value mc(n) to recover
all m patterns exactly is mc(n) = n/(4logn) (McEliece et al.,
1987), although for correlated patterns the storage capacity of
the network is much lower (Storkey and Valabregue, 1999).
Although for datasets with several hundred genes the capacity
of the network is typically sufﬁcient to store the molecular sig-
natures of a few subtypes, other learning algorithms potentially
are more-accurate classiﬁers.

We evaluated high-capacity learning rules by Kanter and
Sompolinsky (1987) and Storkey and Valabregue (1999) that
allow storage of n linearly independent patterns. However, they
have 0(n4) complexity, resulting in prohibitively long learning
times, whereas Hebbian learning is of quadratic complexity.
Furthermore, the clustering accuracy was considerably lower
than that of Hebbian learning (data not shown) because attrac-
tors for individual samples were generated instead of grouping
similar patterns by assigning a single attractor.

We use Hopﬁeld networks in a novel way by not storing a
single pattern (e. g. a molecular signature) for each cancer sub-
type, but instead leave it to the learning algorithm to discover
subtypes and establish network attractors from the complete set
of unlabeled samples.

3.3 Network pruning

The networks generated by Hebbian learning are usually dense,
with >95% of the weights non-zero. Gene regulatory networks,

on the other hand, tend to be sparse and scale-free (Huang et al.,
2005). Apart from the ER], we explored other measures to
determine good pruning thresholds such as the silhouette
coefﬁcient (Rousseeuw, 1987) or maintaining a ﬁxed number
of clusters but found the ER] to work best (results not shown).
We have shown that the accuracy of Hopﬁeld clustering can
be improved if the similarity matrix is used as input instead of the
sample matrix. For the common case of gene-expression datasets
with a large number of genes and a small number of samples, the
similarity matrix is much smaller than the sample matrix, result-
ing in lower memory consumption and computation time.
However, pruning of a network constructed from similarity
matrices does not filter unimportant genes but samples.

4 CONCLUSION

We introduced Hopﬁeld networks as a state-space model in
which attractors characterize cell states, and used the model to
identify cancer subtypes in gene-expression data. Compared with
other clustering algorithms, the Hopﬁeld network is not neces-
sarily the most accurate. However, it has other important advan-
tages, including the uniﬁcation of clustering, feature selection
and network inference, and as a modeling framework for epigen-
etic landscapes. Furthermore, Hopﬁeld networks are simple to
implement and do not require any meta-parameters when used
for clustering.

We presented methods to visualize the high-dimensional
energy landscape of the model and to prune the network to derive
sparse networks that are more open to a biological interpretation
than traditional clustering or feature-selection methods.
Furthermore, Hopfield networks can be efﬁciently simulated
on quantum computers; recent improvements (www.dwavesys.
com) now enable the implementation of larger networks
(512 qubit) with potentially dramatic speedups.

Many interesting questions remain, including the power of
network pruning as a feature-selection method, the accuracy of
inferred regulatory networks, whether and how known regula-
tory interactions can be incorporated, whether the energy func-
tion can reﬂect the probability or frequency of transitions among
actual cell states, and whether developmental trajectories can be
mapped to trajectories in this landscape (Waddington, 1940,
1957). The standard Hopfield network is likely too simple a
model for many applications; directed stochastic Hopﬁeld-like
networks with more advanced learning algorithms will likely be
required.

Attractor models of regulatory networks offer great potential.
Genes are not treated independently but in combination, and
trajectories of normal cell development or tumor progression
can be predicted. If an accurate model can be constructed,
time points, amounts and combinations of treatments can be
identiﬁed that most effectively inﬂuence cell fates with minimum
intervention.

ACKNOWLEDGEMENT

The authors thank Josha Inglis, Melissa Davis and Sriganesh
Srihari for helpful discussions and suggestions.

 

1 278

112 /3.10's112u.1n0fp.10}x0"soiwurJOJuioiq/ﬁduq 11101} papBOIH/noq

91oz ‘Og isnﬁnV uo ::

Characterizing cancer subtypes

 

Funding: Australian Research Council (DP110103384 and
CE034822).

Conﬂict of Interest: none declared.

REFERENCES

Ao,P. et al. (2008) Cancer as robust intrinsic state of endogenous molecular—cellular
network shaped by evolution. Med. Hypotheses, 70, 678484.

Bar—Yam,Y. and Epstein,I. (2004) Response of complex networks to stimuli. Proc.
Natl Acad. Sci. USA, 101, 43414345.

Barash,D. and Comaniciu,D. (2004) A common framework for nonlinear diffusion,
adaptive smoothing, bilateral ﬁltering and mean shift. Image Vis. C0mput., 22,
7&8].

Bhattacharya,S. et al. (2011) A deterministic map of Waddington’s epigenetic land—
scape for cell fate speciﬁcation. BMC Syst. Biol., 5, 85.

Chang,H. et al. (2008) Transcriptome—wide noise controls lineage choice in mam—
malian progenitor cells. Nature, 453, 5447547.

Cheng,T. et al. (2012) Understanding cancer mechanisms through network
dynamics. Brief. Funct. Genom’ws, 11, 5437560.

de Pillis,L. et al. (2005) A validated mathematical model of cell—mediated immune
response to tumor growth. Cancer Res., 65, 795(k7958.

de Souto,M. et al. (2008) Clustering cancer gene expression data: a comparative
study. BMC Bioinformatics, 9, 497.

del Sol,A. et al. (2010) Diseases as network perturbations. Curr. Opin. Biotechnol.,
21, 56(r571.

d’Onofrio,A. et al. (2009) On optimal delivery of combination therapy for tumors.
Math. Biosci., 222, 13726.

Esfahani,M. et al. (2011) Probabilistic reconstruction of the tumor progression pro—
cess in gene regulatory networks in the presence of uncertainty. BMC
Bioinformatics, l2 (SuppL 10), S9.

Ester,M.I. (1996) A density—based algorithm for discovering clusters in large spatial
databases with noise. In: Second International Conference on Knowledge
Discovery and Data Mining. Vol. 96, AAAI Press, Menlo Park, California,
pp. 22(r231.

Frey,B. and Dueck,D. (2007) Clustering by passing messages between data points.
Science, 315, 9727976.

Fumia,H. and Martins,M. (2013) Boolean network model for cancer pathways:
predicting carcinogenesis and targeted therapy outcomes. PLoS One, 8, e69008.

Guebel,D. et al. (2012) Analysis of cell adhesion during early stages of colon cancer
based on an extended multi—valued logic approach. Mol. Biosyst., 8, 12331242.

Gyori,I. et al. (1988) Time—dependent subpopulation induction in heterogeneous
tumors. Bull. Math. Biol., 50, 681496.

Hickman,G. and Hodgman,T. (2009) Inference of gene regulatory networks using
Boolean—network inference methods. J. Bioinform. Comput. Biol., 7, 101%1029.

Huang,S. (2009) Reprogramming cell fates: reconciling rarity with robustness.
BioEssays, 31, 54(r560.

Huang,S. (2011) On the intrinsic inevitability of cancer: from foetal to fatal attrac—
tion. Semin. Cancer Biol., 21, 1837199.

Huang,S. and Ingber,D. (2006) A non—genetic basis for cancer progression and
metastasis: self—organizing attractors in cell regulatory networks. Breast Dis.,
26, 27754.

Huang,S. et al. (2005) Cell fates as high—dimensional attractor states of a complex
gene regulatory network. Phys. Rev. Lett., 94, 128701.

Huang,A. et al. (2009a) Using cell fate attractors to uncover transcriptional regu—
lation of HL60 neutrophil differentiation. BMC Syst. Biol., 3, 20.

Huang,S. et al. (2009b) Cancer attractors: a systems View of tumors from a gene
network dynamics and developmental perspective. Semin. Cell Dev. Biol., 20,
8697876.

Hubert,L. and Arabie,P. (1985) Comparing partitions. J. Classyf, 2, 19%218.

Isaeva,O. and Osipov,V. (2009) Different strategies for cancer treatment: mathem—
atical modelling. Comput. Math. Methods Med., 10, 2537272.

Ising,E. (1925) Beitrag zur Theorie des Ferromagnetismus. Zeitschrmfu'r Physik,
31, 2537258.

Kansal,A. et al. (2000) Emergence of a subpopulation in a computational model of
tumor growth. J. Theor. Biol., 207, 431441.

Kanter,I. and Sompolinsky,H. (1987) Associative recall of memory without errors.
Phys. Rev. A, 35, 380.

Kauffman,S. (1969a) Homeostasis and differentiation in random genetic control
networks. Nature, 224, 1777178.

Kauffman,S. (1969b) Metabolic stability and epigenesis in randomly constructed
genetic nets. J. T heor. Biol., 22, 4377467.

Kolev,M. (2003) Mathematical modelling of the competition between tumors and
immune system considering the role of the antibodies. Math. Comput. Model,
37, 114%1152.

Layek,R. et al. (2011a) Cancer therapy design based on pathway logic.
Bioinformatics, 27, 5487555.

Layek,R. et al. (2011b) From biological pathways to regulatory networks. Mol.
Biosyst., 7, 8437851.

Lin,P.—C.K. and Khatri,S. (2012) Application of Max—SAT—based ATPG to optimal
cancer therapy design. BMC Genomics, 13 (SuppL 6), SS.

Lucia,U. and Maino,G. (2002) Thermodynamical analysis of the dynamics of tumor
interaction with the host immune system. Physica A, 313, 5697577.

MacQueen,J. (1967) Some methods for classiﬁcation and analysis of multivariate
observations. In: Cam,L.M.L and Neyman,J. (eds) Proceeding of the Fifth
Berkeley Symposium on Mathematical Statistics and Probability. Vol. 1,
University of California Press, Berkeley, CA, pp. 2817297.

McEliece,R. et al. (1987) The capacity of the hopﬁeld associative memory. IEEE
Trans. Inf. Theory, 33, 461482.

Pe’er,D. and Hacohen,N. (2011) Principles and strategies for developing network
models in cancer. Cell, 144, 8644873.

Rodriguez,A. et al. (2012) A Boolean network model of the FA/BRCA pathway.
Bioinformatics, 28, 8587866.

Rousseeuw,P. (1987) Silhouettes: a graphical aid to the interpretation and validation
of cluster analysis. J. Comput. Appl. Math., 20, 5345.

Saez—Rodriguez,J. et al. (2011) Comparing signaling networks between normal and
transformed hepatocytes using discrete logical models. Cancer Res., 71, 54031 1.

Storkey,A. and Valabregue,R. (1999) The basins of attraction of a new Hopﬁeld
learning rule. Neural Netw., 12, 8697876.

Waddington,C. (1940) Organisers and genes. Cambridge University Press,
Cambridge, United Kingdom.

Waddington,C. (1957) The strategy of the genes. George Allan & Unwin, London.

Wang,C. et al. (2012) mCOPA: analysis of heterogeneous features in cancer expres—
sion data. J. Clin. Bioinforma, 2, 22.

Wang,J. et al. (2011) Quantifying the Waddington landscape and biological paths
for development and differentiation. Proc. Natl Acad. Sci. USA, 108,
825778262.

Ward,J. (1963) Hierarchical grouping to optimize an objective function. J. Am. Stat.
Assoc., 58, 2367244.

Yang,L. et al. (2013) Robustness and backbone motif of a cancer network regulated
by mir—17—92 cluster during the Gl/S transition. PLoS One, 8, e57009.

Yeoh,E. et al. (2002) Classiﬁcation, subtype discovery, and prediction of outcome in
pediatric acute lymphoblastic leukemia by gene expression proﬁling. Cancer
Cell, 1, 1337143.

 

1 279

112 /8.10's112u.1n0[p.10}x0"soiwurJOJuioiq/ﬁduq uteri papBOIH/noq

9103 ‘Og isnﬁnV uo ::

