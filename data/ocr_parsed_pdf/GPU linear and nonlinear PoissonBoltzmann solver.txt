BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

J.Colmenares et al.

 

3 GPU IMPLEMENTATION

The GPU implementation borrows from the algorithm originally
given by Nicholls and Honig (1991). All the calculations related
to the stencil are done on the GPU card (or equivalent device).
Interestingly, the stencil in Equation (3) shows a checkerboard
(‘odd and even’) structure, implying that updating a point
requires only the knowledge about its nearest neighbors, which
are of opposite parity. Therefore, the execution can be divided in
two segments, alternating the update of odd and even points.
Within each segment, the calculations are independent, so any
further parallelization is trivial. Therefore, the physical grid is
partitioned in two logical subgrids, odd and even. Our GPU
implementation further exploits this structure and loads alter—
nately odd and even points to the ‘texture’ memory of the
device to optimize the memory access. Dedicated data structures
separately address charged grid points, and those that are at the
boundary between regions with different dielectric. A thread
starts from every grid point of the bottom xy slice of each sub—
grid and then proceeds along the z direction. A single step along
2 in a subgrid corresponds to an increment of two in the physical
grid. Nearby threads in a xy slice of a subgrid are gathered in
blocks and are given access to a common shared memory.
Basically, a thread does a loading step, followed by an updating
step and finally it moves to the upper slice in the same subgrid.
For example, before updating an odd point a thread loads to the
shared memory the even point having its same index. Because all
the threads of a block act in parallel, and thanks to a purposely
designed indexing scheme, an odd thread block in one step loads
simultaneously all the even grid points needed for the updating
task with the exception of the neighboring points that pertain to
the adjacent blocks and of those that are in the z direction. We
cope with the ﬁrst problem by adding a suitable overlap between
blocks borders. The ‘z — 1’ and the ‘z+ 1’ points of the physical
grid are not present in the shared memory and therefore need
further accesses to the ‘texture’ memory. The number of these
accesses is halved by saving each ‘z+ 1’ point in a temporary
variable, which plays the role of the ‘z — 1’ point once the
thread has moved to the upper slice. In the Supplementary
Material, a graphical description of the algorithm is provided,
whereas a more detailed explanation in the case of the linear PBE
is given in Colmenares et al. (2013). Similarly to the DelPhi ap—
proach, the non—linearity is treated as an additive charge—like
term, which affects only the grid points that are located in solu—
tion and which is gradually inserted during the solution. Whether
to update the non—linear term is decided at the CPU level, based
on a heuristic approach as in the DelPhi code [Rocchia et al.
(2001)].

4 RESULTS

The results in Table 1 show the speedup between the serial code
executed on a CPU with an AMD Opteron (1.4 GHz) chip and a

Table 1. Fatty acid amide hydrolase protein78325 atomsi297 X 297 X
297 grid points

 

Non-linear solver
GPU (CPU, speedup)

Computing Linear solver
step GPU (CPU, speedup)

 

Boltzmann 9.060 s (l min 31s, X 10.05)
Iteration 0.015 s (0.18 s, X 10.60)
Total 10.250 s (l min 38s, ><9.61)

8.83 s (1min 28s, ><9.96)
0.0355 (0.44s, ><12.57)
10.14s (1min 37s, ><9.55)

 

Note: The execution time is reported per computing step: ‘Boltzmann’ includes the
overall time spent for Laplace and Boltzmann updates. ‘Iteration’ is the time spent
in a single iterative step. ‘Total’ includes all the iterations and the initialization of the
GPU card.

Tesla Kepler K20m. The solver was run on the fatty acid amide
hydrolase protein with 8325 atoms. A monovalent salt concen—
tration of 0. 15 M was used. The relative dielectric constant of the
molecule was taken as 2.0 and that of the solution as 80.0. The
Debye length is roughly 8 A. The speedup of the non—linear
algorithm is comparable with that of the linear one. In fact,
the former beneﬁts from a larger number of ﬂoating point oper—
ations but it suffers from a larger number of data transfers.

ACKNOWLEDGEMENT

The authors acknowledge the IIT platform Computation and
Dr Decherchi for help in linking the module to the DelPhi code.

Funding: National Institutes of Health (1R01GM093937—01).

Conﬂict of Interest: none declared.

REFERENCES

Colmenares,J. et al. (2013) Solving the linearized Poisson—Boltzmann equation on
GPUs using CUDA. In: 21st Euromicro International Conference on Parallel,
Distributed anal Network—Based Processing (PDP). pp. 42(F426.

Debye,P. and Huckel,E. (1923) Zur theorie der elektrolyte. Physik. Zeits., 24,
1857206.

Grochowski,P. and Trylska,J. (2007) Continuum molecular electrostatics, salt
effects, and counterion binding — Review of the Poisson—Boltzmann theory
and its modiﬁcations. Biopolymers, 89, 937113.

Nicholls,A. and Honig,B. (1991) A rapid ﬁnite difference algorithm, utilizing
successive over—relaxation to solve the Poisson—Boltzmann equation.
J. Comput. Chem., 12, 4354145.

Rocchia,W. (2005) Poisson—Boltzmann equation boundary conditions for biological
applications. Math. Compat. Modell, 41, 110971118.

Rocchia,W. et al. (2001) Extending the applicability of the nonlinear poisson—
boltzmann equation: multiple dielectric constants and multivalent ions.
J. Phys. Chem. B, 105, 6507$514.

Stoer,J. and Bulirsch,R. (2002) Numerical Mathematics. Springer, Berlin Heidelberg
New York.

Warwicker,J. and Watson,H. (1982) Calculation of the electric potential in the
active site cleft due to —helix dipoles. J. Mol. Biol, 157, 671$79.

 

570

ﬁm'spzumofpmﬂo'sopeuuopuorq/ﬁdnq

