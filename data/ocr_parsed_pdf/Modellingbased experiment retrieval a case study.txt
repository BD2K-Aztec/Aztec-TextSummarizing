Bioinformatics, 32(9), 2016, 1388—1394

doi: 10.1093/bioinformatics/btv762

Advance Access Publication Date: 6 January 2016
Original Paper

 

 

Data and text mining

Modelling-based experiment retrieval: a case
study with gene expression clustering

Paul Blomstedt1'*"’, Ritabrata Dutta”, Sohan Seth”, Alvis Brazma2
and Samuel Kaski1'*

1Helsinki Institute for Information Technology HIIT, Department of Computer Science, Aalto University, Espoo,
Finland and 2European Molecular Biology Laboratory, European Bioinformatics Institute, EMBL-EBI, Wellcome
Trust Genome Campus, Hinxton, UK

*To whom correspondence should be addressed.

TThe authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors
1Present address: Institute for Adaptive and Neural Computation, School of Informatics, University of Edinburgh, UK
Associate Editor: Jonathan Wren

Received on 15 May 2015; revised on 10 December 2015; accepted on 28 December 2015

Abstract

Motivation: Public and private repositories of experimental data are growing to sizes that require
dedicated methods for finding relevant data. To improve on the state of the art of keyword searches
from annotations, methods for content—based retrieval have been proposed. In the context of gene
expression experiments, most methods retrieve gene expression profiles, requiring each experiment
to be expressed as a single profile, typically of case versus control. A more general, recently sug—
gested alternative is to retrieve experiments whose models are good for modelling the query dataset.
However, for very noisy and high—dimensional query data, this retrieval criterion turns out to be very
noisy as well.

Results: We propose doing retrieval using a denoised model of the query dataset, instead of the ori—
ginal noisy dataset itself. To this end, we introduce a general probabilistic framework, where each ex—
periment is modelled separately and the retrieval is done by finding related models. For retrieval
of gene expression experiments, we use a probabilistic model called product partition model, which
induces a clustering of genes that show similar expression patterns across a number of samples.
The suggested metric for retrieval using clusterings is the normalized information distance.
Empirical results finally suggest that inference for the full probabilistic model can be approximated
with good performance using computationally faster heuristic clustering approaches (e.g. k—means).
The method is highly scalable and straightforward to apply to construct a general—purpose gene ex—
pression experiment retrieval method.

Availability and implementation: The method can be implemented using standard clustering algo—
rithms and normalized information distance, available in many statistical software packages.

Contact: paul.b|omstedt@aa|to.fi or samuel.kaski@aalto.fi

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 IntrOducuon often of interest for researchers to retrieve experimental datasets with

As the use of high—throughput molecular measurement technologies relevance to a given experiment, in order to increase the power of stat—
continues to spread, an ever increasing amount of data from biological istical analyses and to be able to make novel findings not obtainable
experiments is being stored in publicly available repositories. It is then from one experiment alone. The current standard practice relies on

©The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1388

9103 ‘Og JSanV 110 salaﬁuv soc] ‘BtHJOJtIBQ 30 AJtsraAtuf] 112 /310'S[BIIJHO[pJOJXO'SOTJBLUJOJIITOTCI”Zduq 11101} popcolumoq

Modelling—based experiment retrieval

1389

 

searching for relevant experiments by keyword annotations (e.g. Zhu
et al., 2008). However, despite efforts to maintain compliance with
standard formats of documenting experiments, e.g. the MIAME stand—
ard (Brazma, 2001), information about experiments may often be miss—
ing, insufficient or suffer from variations in terminology (e.g.
Baumgartner et al., 2007; Schmidberger et al., 2011). In view of the
challenges associated with keyword—based retrieval, the complementary
task of querying a database of experiments using measurement data, in—
stead of keywords, has recently received increased attention in the
literature.

Most earlier content—driven methods used for retrieval of gene
expression data represent each experiment in terms of a profile over
genes, or alternatively, over known gene sets or gene modules pre—
dicted from other data sources, see Hunter et al. (2001), Fujibuchi
et al. (2007), Caldas et al. (2009), Engreitz et al. (2010) and Georgii
et al. (2012) and references therein. A representative example is to
compute differential expression profiles of case versus control, use
the correlation between activity profiles as the measure of relevance,
and retrieve the experiments with the highest correlations (e.g.
Engreitz et al., 2010). This requires auxiliary information about the
experiments, namely case and control labels of experiment samples,
and possibly additional a priori defined sets of important genes. In
the context of gene expression time series, representative examples
of retrieving gene expression profiles include Smith et al. (2008) and
Hafemeister et al. (2011 ).

Recently, two feasibility studies have gone beyond reducing ex—
periments into single profiles by using probabilistic modelling of the
experiments in the database being queried. Faisal et al. (2014),
assumed that the query dataset can be explained as a mixture of the
learnt models, each model learnt from one dataset, such that the
measure of relevance is given by the inferred mixture weights. In a
slightly different approach (Seth et al., 2014), experiments were
retrieved by evaluating the posterior marginal likelihoods, given the
query data, of individual models stored for the experiments in the
database.

In this paper, we introduce a method for retrieving full datasets,
i.e. experiments consisting of multiple samples, which is also based
on probabilistic modelling. However, instead of using the query
dataset itself as a query, we use a model learnt from it. The measure
of relevance is therefore not a likelihood, but instead a suitably
defined metric between the models. The argument is that for noisy
and complex datasets, it is beneficial to extract relevant characteris—
tics of the query dataset in the same way as was done with the data—
sets that are being queried. We also make explicit the importance of
marginalizing out nuisance parameters which are not directly rele—
vant for the retrieval task. For example, in a gene expression study,
one is often more interested in how sets of genes are co—regulated,
rather than their exact expression values which are additionally af—
fected by numerous other inﬂuences. We tackle the specific problem
of retrieving gene expression experiments by using a product parti—
tion model (Jordan et al., 2007) to cluster together genes that show
similar expression patterns across a number of samples. By integrat—
ing out expression levels of the gene sets (i.e. cluster—specific infor—
mation), only the co—expression patterns revealed by the clustering
structure are retained. The clustering induced by the query dataset is
then finally compared with the clusterings associated with the ex—
periments in the database using the normalized information distance
(Vinh et al., 2010). Notice that this approach does not involve any
‘training stage’, compared to that of Seth et al. (2014), and the re—
trieval step does not involve solving an optimization problem, com—
pared to Faisal et al. (2014).

While gene clustering has a long history in characterizing gene ex—
pression datasets (Eisen et al., 1999; D’haeseleer, 2005), it appears
not to have been used in the context of experiment retrieval before.
The use of gene clustering provides a straightforward way of charac—
terizing each experiment with minimal preprocessing of the data
while capturing central co—expression patterns. Essentially all previ—
ous approaches for retrieving gene expression data have converted
the data to differential expression (or gene set enrichments) requiring
fixed and known case—control distinctions. In contrast, we have only
applied standard quality control and RMA normalization steps car—
ried out in—house at the European Bioinformatics Institute (EBI) for
datasets in the Expression Atlas database (see Petryszak et al., 2014).
Our experimental evaluation further suggests that, for the current ap—
plication, inference of the full probabilistic model can be approxi—
mated by some computationally faster heuristic clustering algorithm,
such as k—means (see Supplement Section A of the online material).
The computational simplicity makes the method highly scalable and
easy to apply in a black—box manner, as a general—purpose retrieval
scheme.

2 Approach

Let D, denote a data matrix from some experiment of interest, and let
{D,,,}£Ind:1 be a database of M datasets from previously conducted ex—
periments. The aim is to retrieve datasets from among the {Dm}x:1
with similar characteristics as the query dataset Dq. Due to the complex
nature of the data, there is no single sensible or obvious way of com—
paring datasets (matrices of possibly different sizes). We propose using
a model to characterize each dataset, with the aim of reducing noise
and making relevant aspects of the data more tangible, while making
the experiments comparable. The retrieval task then consists in ranking
the models {Mm}£In/l:1, inferred from {Dm}x:1, with respect to their
similarity with the query model M,, inferred from Dq. Note that in a
broad sense, the commonly used differential expression can be con—
sidered as one model type, and clustering as another.

To elaborate on the above idea further, we will now assume that
the data generating mechanism of each dataset can be represented in
terms of a probabilistic model with density f in some family
 E (9}. Often, the parameter 0 can be decomposed as
0 : (A, 1/1), where 1/1 is the parameter or characteristic of interest (e.g.
gene clusters) and A is a nuisance parameter (e.g. average expression
level of the gene cluster). Marginalizing out (integrating the density
over) A then yields a model family completely determined by

1/1 E ‘P. Making this operation explicit, the key quantity used in
inferring a representative model for a dataset D is the marginal

likelihood,
any) : (Anny, rm v (2le (1)

where TQM-(ill) is a prior density on A. Ideally, we would then pro—
ceed with a fully Bayesian approach to infer a posterior density (or
distribution) TEW('ID) over ‘I’, and use M :: TEW('ID) to characterize
D. However, for computational reasons we will here choose only a
single element of ‘I’ to represent D. Under zero—one loss, the optimal
choice is then the maximum a posteriori (MAP) solution

1/1 = argrggﬂﬂDlI/IWMI/O}, (2)

where my is a prior over ‘I’. Accordingly, we now define the repre—
sentative model for D as M 2: 1/7 .

91oz ‘Og JSanV 110 salaﬁuv soc] ‘BtHJOJtIBQ 30 AJtSJQAtuf] 112 /310'sleu1n0[p103x0"sotwurJOJutotq/ﬁduq 11101} papeolumoq

1390

P. Blomstedt et al.

 

If a suitable function d : M X M —> IR can be defined for the
pairwise relations between the elements of the model space M, a
natural ranking among M1, . . . ,M M E M will be induced by evalu—
ating d (M,,, Mm) for all m. For coherence of the ranking scheme,
we will make a further assumption that d is a metric. That is, for all
M, M’, M" E M, we require that

(M1) d(M,M’) 2 0

(M2) d(M,M’) : 0 if and only if M : M’
(M3) d(M,M’) : d(M’,M)

(M4) d(M,M”) g d(M,M’) + d(M’,M”).

With the above conditions satisfied, the function d conforms to
the intuition of a distance, and furthermore, provides a solid founda—
tion for the design of data structures and algorithms, as the model
space M forms a metric space. We finally note that metrics are also
available for probability distributions, making the described frame—
work applicable in cases where computational resources allow for
representing the elements of M as full posterior distributions.

3 Methods

3.1 Probabilistic model for gene clustering
The first task in constructing a retrieval scheme is to choose an ap—
propriate model for the experiments. While several different
approaches, with varying aims and assumptions, exist for modelling
gene expression data, a particularly simple and frequently used ap—
proach is that of gene clustering (e.g. D’haeseleer, 2005), which
seeks to cluster together genes that show similar expression patterns
across a number of samples. Here, we use a probabilistic clustering
approach which simultaneously infers both the number of clusters
as well as the optimal clustering structure.

Consider first a gene expression data matrix D of dimension
n X p, where n is the number of genes and p is the number of sam—
ples. A clustering S : {$1, . . . ,sk} is a partition of the set N : {1, . . .
,n} into [2 E {1, . . . ,n} non—empty and non—overlapping subsets, or
clusters, such that Ufﬂsc : N and sc ﬂ sci : 0, for c 9E c’. We focus
here on a probabilistic formulation of clustering, which makes expli—
cit use of partition structures, namely the product partition model
(PPM). Technically, PPM assumes that items in the same cluster are
exchangeable and items in different clusters are independent (see
Jordan et al., 2007). Using the terminology of Section 2, the param—
eter of interest for this model is the partition structure S, while the
nuisance parameter is a vector of cluster—specific model parameters,
A : (21, . . . ,Ak). This leads to a marginal likelihood of the form (see
Equation (1))

17(DIS) : (AnDi/Iswlswoda
la la
: IAlIf(D(S‘>unsurpassed/I = Hummus), (3)
c:1 c:1
where D(S‘) denotes the subset of D which is indexed by 55. Note
that the assumption of independence between clusters entails con—
structing the marginal likelihood as a product of cluster—specific

components.
The prior distribution for S will likewise be constructed as a
product,
la
IP(S) : KH h(sc), for all k E {1, . . . ,n}, (4)

c:1

where K ensures normalization to 1 over the model space S and
h(sc) Z 0 for all subsets 55. Note that (4) actually specifies the joint
distribution for S and [2, but since the latter is implied by the former,
we omit [2 from the notation. It can be shown that a PPM with K
and h(sc) chosen such that

la
1151—1055) -1)!
:1—7 (5)

110+i—1
":1

112(5) 2

3n

where (5,) is the number of observations in cluster 55 and 110 > 0 con—
trols the tendency to form new clusters, can be obtained by integrat—
ing out the model parameters in a Dirichlet process mixture model
(Dahl, 2009).

The cluster—specific marginal likelihoods p(D(S‘)IsC) in Equation
(3) can in principle take any suitable form. Here, we assume that for
D(S‘) : [xv-Li E sC,/' : 1,. . . ,p, the observations in each sample/ are
independently generated from N014, 1767-1) with a conjugate
NormalGammaQio, po, (10, [30) prior on the unknown model param—
eters. Furthermore, we make the simplistic assumption that the sam—
ples themselves are independent, conditional on a cluster assignment
(see Hand and Yu, 2001, for a discussion about the implications of
this assumption in a classification context). The resulting cluster—
specific marginal likelihoods may then be written as

 

P 010
D(Sc) C : 2 —% @%r(“i)iﬁ 6
p< ls)  n) (pl) mo) ﬁlo, < >

where

s _ 1
pi : Po + (SCI, “7' I 0to +%, M :mZW’i)
C

1'65:

1 _2 ISIP(7'—M)2
rizro+§Z(xii—xi> +w.
1'65:

I

Blomstedt et al. (2015) introduced a PPM for clustering mixed
discrete and continuous data, where the continuous component was
of form (6). Following their implementation, we normalize each col—
umn of the data matrix D : UE:1D(S‘) to have zero mean and unit
variance, and set the hyperparameter values to no : 0 and
p0 : no : [lo : 1. Furthermore, the model is equipped with a prior
of the form (5), with 110 : 1. Finally, combining Equations (3)—(6),
an optimal clustering S w.r.t. a dataset D is given by the MAP solu—
tion (see Equation (2))

S = argmaX{P(DlS)1P’(S)}- (7)
$65

3.1.1 Inference

To find the optimal clustering S E S as defined in Equation (7), we
use a stochastic greedy search algorithm, which moves in the model
space by successive application of move, split and merge operators;
for further details, see Blomstedt et al. (2015). While being more ef—
ficient for the optimization task than standard Markov chain Monte
Carlo methods, for large amounts of data the algorithm still requires
a considerable amount of computation time. To that end, some com—
putational simplifications based on heuristic clustering procedures
are discussed in Supplement Section A of the online material. Note,
however, that all results presented in Section 4 of the main text are
based on the stochastic greedy search algorithm described above, in—
stead of these simplifications.

91oz ‘Og JSanV 110 salaﬁuv soc] ‘BtHJOJtIBQ 30 AJtsraAtuf] 112 /310'sleu1n0[p103x0"sotwurJOJutotq/ﬁduq 11101} papeolumoq

Modelling—based experiment retrieval

1391

 

3.2 Distance metric for clusterings

Assuming now that each of the experiments in a database has been
represented with a clustering S E S, the remaining task is to find a
function d which can be defined on S and satisfies conditions (M1)—
(M4) above. In recent years, a new generation of information—theor—
etic distance measures has emerged (see e.g. Meila, 2007; Vinh
et al., 2010), which possess many desirable properties, such as the
metric property, and which have been employed because of their
strong mathematical foundation and ability to detect non—linear
similarities.

Vinh et al. (2010) conducted a systematic comparison of informa—
tion—theoretic distance measures, concluding that the preferred ‘gen—
eral—purpose’ measure for comparing clusterings is the normalized
information distance, later denoted dNID. To give a definition of this
measure, we first introduce some notation. Brieﬂy, for two clusterings
S and S’, the number of items co—occurring in clusters sc 6 S and sci

. . . la la’
6 S’ 1s g1ven by n“: : Isa ﬂ s’cll, w1th 26:1 Edd n“: : n. The mar—

ginal sums are denoted by n5. : 25:1 n“: and ncl : 2:1 n55]. A
key realization in the derivation of information—theoretic distance
measures is that each clustering induces an empirical probability distri—
bution over the set {1, . . . ,k}, such that the probability of a randomly
chosen item i E N being in cluster 55 is given by P(i E 55) : n5.
Similarly, the joint probability of the pair (i,/') E N X N co—occurring
in clusters 55 and s’c, is given by P((i,/') E sc >< s’d) : n55:  The en-
tropy of a clustering S, describing the uncertainty associated with as—
signing items into the clusters of S, is then formulated as

la
H(S) : —Z]P’(i e sc)logP(i e 5,).
c:1
The mutual information of clusterings S and S’, which measures
how much having knowledge of S’ reduces H (S) (or vice versa), is
further defined as

k I

,7 k .. , 1P((i,/)e.<,x$;,)
I(S,S) _ ;;P((z,/) e s, X 5,)1ogP—(i 6 56M 6 5,6).

It can also be interpreted as a measure of dependence in the sense
that if S and S’ are independent, then I (S, S’ ) : 0. Finally, from the
above quantities we obtain dNID as

I(S,S’)

dN1D<S,S’>= 1 —W'

(8)

4 Results

4.1 Data and experimental setup

To evaluate the modelling—based retrieval scheme developed in
Sections 2 and 3, we used as a starting point all differential expression
experiments conducted on the A—AFFY—44 affymetrix genechip avail—
able in Expression Atlas (EA; http://www.ebi.ac.uk/gxa, see Petryszak
et al., 2014) as of June 4, 2014. Only experiments with both measure—
ment data and analytics data available were considered. Furthermore,
experiments with a very small number of genes were discarded. Since
most experiments had expression measurements for more than
54 670 genes, this number was set as the lower limit. Based on the
above selection process we obtained an initial set of 447 experiments.
In a second stage, we selected a subset of these experiments based on
the availability of experimental factor ontologies (EFO; http://www.
ebi.ac.uk/efo/, see Malone et al., 2010), which were used as ground
truth in the evaluation. More specifically, we retained those

experiments which had at least one of the EFO types ‘cell type’, ‘dis—
ease’ or ‘organism part’ present. Moreover, experiments having mul—
tiple values for a given EFO type were excluded, and finally only
experiments with the same EFO value present in at least two experi—
ments were included in this study, resulting in a final set of 251 ex—
periments (for a list of accession numbers, see Supplement Section D).
The number of samples per experiment varied between 6 and 353,
the median number of samples being 22.

Out of the final set of 251 experiments, three partly overlapping
subsets corresponding to each of the EFO types were formed. These
consisted of 103 experiments with values recorded for ‘cell type’, 76
with values for ‘disease’ and 174 with values for ‘organism part’. The
number of different EFO values in these sets of experiments were 23,
19 and 32, respectively. In evaluating retrieval performance with re—
spect to a given EFO type, experiments having the same value were
considered mutually relevant, and other experiments irrelevant. Note
that the above EFO types were not the main conditions of interest on
which differential gene expression had been studied in the experi—
ments, but were chosen to give a more general description of the ex—
periments. A more complete ground truth was not readily available as
most other EFO types were only present in small subsets of the experi—
ments. Retrival performance was measured using precision and recall,
taken as an average of successively using each of the experiments as a
query to retrieve among the remaining experiments.

In order to reduce the number of genes for clustering, we initially
selected for each of the 251 experiments the top 5 genes resulting
from a ‘non—specific’ search in EA, in which genes with the highest
absolute values of t—statistics in any available contrast come first, ir—
respective of whether they are reported with high t—statistics in the
remaining contrasts (for further details about listing genes in EA, see
Petryszak et al., 2014). Finally, by taking the union of these genes
over all experiments, we arrived at 1125 genes per experiment. The
selection process per se is not an essential part of our approach but
done for computational convenience only. In a preliminary stage of
our analyses, we experimented with different numbers of genes but
found that this only had a minor impact on the results, see
Supplement Section B for further details. The datasets used in the
analyses, along with code for downloading, selecting experiments
and genes, processing and analyzing the data are also available in
the online supplementary material of this paper.

4.2 Comparison of retrieval schemes

We will now proceed to evaluating the performance of the retrieval
approach proposed in Section 2. For gene expression data, we learn
for each experiment a Gaussian product partition model (PPM)
which implies a clustering over genes, see Section 3. The clustering
Sq learned from the query data is then related to the clusterings S1,
...,SM by evaluating the distances dN1D(Sq,Sm),m : 1, . . . ,M, see
Equation (8). This approach will be contrasted with two alternative
approaches for content—based retrieval previously suggested in the
literature. The first one of these is closely related to the proposed ap—
proach in that it learns a PPM for each experiment in the database.
However, instead of evaluating distances, it evaluates the marginal
likelihoods p(DqlSm) of the learnt models, given the query dataset.
A higher likelihood is then an indication of a higher relevance to the
query dataset. A similar approach, albeit for a different model fam—
ily, was recently suggested in Seth et al. (2014). The term ‘model—
ling—based retrieval’ has previously been used by Faisal et al. (2014)
to describe an approach based on probabilistic modelling but using
a likelihood as the measure of relevance. To make a distinction be—
tween the approach proposed here and approaches based on

9103 ‘Og JSanV 110 sajaﬁuv soc] ‘BtHJOJtIBQ 30 AJtsraAtuf] 112 /310'S[BIIJHO[pJOJXO'SOIJ'BLUJOJIIIOIq/ﬂduq 11101} papeolumoq

1392

P. Blomstedt et al.

 

evaluating likelihoods, we will in this comparison refer to the former
as model-distance-hased retrieval and the latter as likelihood-based
retrieval. See Section 5 for a further discussion about the differences
between the two approaches.

The second alternative approach, differential expression hased
retrieval, assumes that a statistical test to detect differentially ex—
pressed genes has been conducted beforehand. The method is then
based on correlating the gene—specific differential expression p—val—
ues of the query experiment with those of the database experiments.
An approach similar to this was suggested by Engreitz et al. (2010).
If targeted at differential expression profiles obtained under specific
conditions known to be important, this scheme has much potential
to achieve good retrieval performance. On the other hand, it as—
sumes more background knowledge and preprocessing of the data
than the suggested retrieval schemes based on gene clustering. Here,
we do not assume a specific condition of interest but choose in each
experiment for the selected 1125 genes the smallest p—values under
any of the conditions tested and reported in Expression Atlas. We
also experimented with a much larger set of 40569 genes, constitut—
ing the maximal common set of genes tested in all experiments, but
this resulted in slightly inferior performance. The correlation meas—
ure used was Pearson’s correlation. We finally note that differential
expression based retrieval schemes can also be formulated under the
general framework of Section 2 using some appropriate probabilistic
model for differential expression, as formulated in e.g. D0 et al.
(2006).

The results of the comparison between the retrieval schemes are
shown in Figure 1. Here, the model—distance—based retrieval scheme
clearly outperforms the two other schemes. A notable feature of the
results is the surprisingly poor performance of the likelihood—based
approach. This may be due to the well—known fact that gene expres—
sion measurements tend to be extremely noisy. In essence, the mar—
ginal likelihood p(Dq (Sm) measures how well the query dataset Dq is
predicted by a model Sm, learnt from dataset Dm. Even if experi—
ments q and m are in some way related, the idealized model Sm may
still not provide a good prediction for data Dq. Therefore, instead of
using the complex and possibly very noisy dataset Dq as query input,
retaining only the characteristics relevant for retrieval in both Dq
and Dm may help to improve performance, as illustrated in the
results.

4.3 Biological information in gene clustering

Any single EFO type will necessarily capture only one aspect of an
experiment, whereas a meaningful retrieval task usually involves an
evaluation of relevance between experiments in terms of a combin—
ation of aspects. It is therefore of interest to study the effect of

composing the ground truth as a combination of multiple EFO
types. In the current experimental setup, the ground truth for each
of the EFO types ‘cell type’, ‘disease’ and ‘organism part’, can be
represented as a symmetric binary matrix G of dimension M X M,
such that entry g,,,- : 1 iff experiments i and j are mutually relevant.
A ground truth which requires a match in t EFO types can then be
formed by summing the three matrices and requiring g,,,- : t.

In Figure 2, the model—distance—based retrieval scheme is eval—
uated against ground truth relevances requiring (a) any EFO type to
match (t Z 1) (b) two or more matches (t Z 2) and (c) all EFO types
to match (t : 3). The number of experiments satisfying these condi—
tions are 251, 54 and 6, respectively. Intuitively, the ground truth
can be considered increasingly informative as the number of match—
ing EFO types required to declare relevance increases. A retrieval
scheme capturing biologically relevant information should then be
in better agreement with a more informative ground truth. Although
the curves of Figures 2a and b are not directly comparable due to the
differing number of experiments used, the shape of the latter gives
an indication of a better agreement. In Figure 2c, owing to the small
number of available experiments, the ground truth is compared with
the single most relevant experiment (out of five possible ones)
retrieved for each query. Here, the retrieval result matches the
ground truth in four of the six queries.

4.4 Annotations and gene clustering combined
As noted previously in Section 1, information about experiments
may often be missing, insufficient or suffer from variations in ter—
minology (Baumgartner et al., 2007; Schmidberger et al., 2011) des—
pite a formal declaration of compliance with MIAME criteria
(Brazma, 2001). Hence, even in cases where keyword—based retrieval
is of primary interest, it may be advantageous to complement a
query with information provided by gene clustering. A straightfor—
ward way of combining these two types of information is the follow—
ing. Assume that a database of M experiments is being queried and
that L S M experiments are found to match the keyword query.
More formally, the result can be encoded as a binary vector of
length M with L elements having value 1. A model—distance—based
retrieval scheme, on the other hand will return a vector of length M
with each element representing the distance of the corresponding ex—
periment—specific model to the query model. Element—wise multipli—
cation of these vectors then effectively induces a ranking of the
experiments retrieved in the keyword—based query. The underlying
idea is that this ranking will reﬂect some information which is not
present in the queried keyword(s) alone.

To test the combined method, we considered all experiments
matching in both ‘cell type’ and ‘organism part’, resulting in a total

 

 

 

 
 
 
  

 
   
 

 

 

 

   

 

 

 

 

 

 

 
  
   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(a) - . . . (b) 0 7 - - . . _ (c) . . . .
- ---- -- Random ' ------- Random ‘ - ---- -- Random
—Mode|—distance-based ' 0 6 —Mode|—distance-based _ 0_4 —Mode|—distance-based -
- - - - Likelihood-based ' - - - Likelihood-based - - - Likelihood-based
-------------DE-based .  ""DE-based -------------DE-based
: c 0'5 - c _
.Q .Q .Q
'8 - -8 o 4 - '8
9 9 9
Q Q Q '
. 0.3 ‘
0.2 ‘ ‘ .
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
Recall Recall Recall

Fig. 1. Precision-recall curves comparing model-distance-based, likelihood-based, and differential expression (DE) based retrieval using the following EFO types

as ground truth: (a) cell type, (b) disease, (0) organism part

9103 ‘Og JSanV uo sajaﬁuv 50'] ‘BtHJOJtIBQ JO AJtsraAtuf] 112 /310'sleumofp1q1xo"sotwurJOJutotq/ﬁduq 11101} papeolumoq

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Modelling—based experiment retrieval 1393
(a) (b) (c)
06 06 H E-GEOD-10718 |
5 E-GEOD-5372
05 05 g E-GEOD-11761
= a j
04 C 04 x _ _
g g a, E GEOD 13899
8 o 3 8 o 3 EE-GEOD-mn
n. “- 8
o 2 o 2 E-GEOD-8023 |
Q; s o
01 01 .04“ 6“} ,5") N099 6° ,0”:5
0° 0° 0° 0° 0° 0°
0 o 00 00 00 00 00 00
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Q; Q; Q; Q; Q; Q;
Recall Recall

Retrieved experiment

Fig. 2. Evaluation of model-distance-based retrieval scheme with respect to a ground truth requiring (a) at least one, (b) at least two, (0) exactly three matching
EFO types. The rightmost subfigure compares the ground truth matrix (rectangles with black borders) with the single most relevant retrieved experiment per
query (grey rectangles) for the six experiments having a simultaneous match in all three EFO types. Accession numbers for the experiments are provided as a

reference

of 43 experiments (all other combinations of two EFO types resulted
in significantly less experiments). A match in both of these EFO
types was used as ground truth. The idea was then to retrieve experi—
ments assuming only one of the EFO types to be known, comple—
menting keyword—based retrieval with rankings from model—
distance—based retrieval. Retrieving experiments assuming only ‘cell
type’ to be known resulted in an average precision of 0.55 for key—
word—based retrieval and 0.61 (mean average precision) for com—
bined retrieval, the corresponding numbers being 0.81 and 0.84,
respectively, when only ‘organism part’ was assumed to be known.
In both cases we were able to see a slight improvement in perform—
ance for the combined approach, suggesting that keyword—based re—
trieval may benefit from being complemented with auxiliary
information, such as gene clustering.

5 Discussion

In this paper, we have introduced a general probabilistic framework
for content—driven retrieval of experimental datasets. Compared to
earlier works which also employ probabilistic modelling (e.g. Caldas
et al., 2009, 2012; Faisal et al., 2014; Seth et al., 2014), we do not
use the likelihood of the query data as a measure of relevance, but
instead learn a model of the query data and compare models. We
argue that this reduces noise in the query input. With nuisance par—
ameters further marginalized out, only characteristics relevant for
the retrieval task are retained. A special instance of the general
framework introduced in this paper has been previously used as a
comparative method in a simulation study (Seth et al., 2014) with
performance slightly inferior to a likelihood—based approach. The
simulation setting in that earlier study was, however, very simplis—
tic compared to datasets encountered in many real—life scenarios,
such as that of Section 4, where the model—distance—based ap—
proach was now seen to clearly outperform its likelihood—based
counterpart.

Contrary to likelihood—based approaches, the model—distance—
based approach requires all models under consideration to belong to
the same family. Although this may seem somewhat restrictive, in
particular for the potential future scenario in which individual re—
searchers independently store models in a repository along with
their datasets (e.g. Faisal et al., 2014), there are also scenarios
where the assumption is feasible. Datasets which arise as a result
of some specific type of experiment are often in practice modelled
using a fairly standardized set of approaches. In particular, if the

models are constructed automatically, or by a curator of a data re—
pository, the assumption of the models belonging to the same fam—
ily is feasible.

As a specific application of the general framework, in Sections 3
and 4 we proposed a retrieval scheme for gene expression experi—
ments based on gene clustering. It turned out, with our current data,
that clustering was even a surprisingly good model for this purpose;
with minimal preprocessing and prior knowledge about the experi—
ments, it was able to yield reasonable retrieval performance (Section
4.2) and to capture biologically relevant characteristics about the ex—
periments (Section 4.3). Finally, we showed that it is straightforward
to combine model—distance—based (or any modelling—based) retrieval
with retrieval using available keywords (Section 4.4).

Acknowledgement

The authors would like to thank Ugis Sarkans for providing useful informa-
tion about Expression Atlas.

Funding

This work was financially supported by the Academy of Finland (Finnish
Centre of Excellence in Computational Inference Research COIN, Grant no.
251170).

Conﬂict of Interest: none declared.

References

Baumgartner,W.A., Jr. et al. (2007) Manual curation is not sufﬁcient for anno—
tation of genomic databases. Bioinformatics, 23, i41—i48.

Blomstedt,P. et al. (2015) A Bayesian predictive model for clustering data of
mixed discrete and continuous type. IEEE Trans. Pattern Anal. Mach.
Intell., 37, 489—498.

Brazma,A. (2001) Minimum information about a microarray experiment
(MIAME) — towards standards for microarray data. Nat. Genet., 29,
365—371.

Caldas,J. et al. (2009) Probabilistic retrieval and visualization of biologically
relevant microarray experiments. Bioinformatics, 25, i145—i153.

Caldas,J. et al. (2012) Data-driven information retrieval in heterogeneous col—
lections of transcriptomics data links SIM2s to malignant pleural mesotheli—
oma. Bioinformatics, 28, 246—253.

Dah1,D.B. (2009) Modal clustering in a class of product partition models.
Bayesian Anal., 4, 243—264.

D’haeseleer,P. (2005) How does gene expression clustering work? Nat.
Biotechnol., 23, 1499—1501.

9103 05 isnﬁnv uo sopﬁuv soc] ‘BIIIJOJIIBD 10 AJtSJQAtuf] 112 /310'S[BHJnOprOJXO'SOIJBLUJOJIIIOICI”Idllq 111011 popco1umoq

1394

P. Blomstedt et al.

 

Do,K.-A., Muller,P., and Vannucci,M., eds (2006). Bayesian Inference for Gene
Expression and Proteomics. Cambridge University Press, Cambridge, UK.

Eisen,M.B. et al. (1999) Cluster analysis and display of genome—wide expres-
sion patterns. PNAS, 95, 14863—14868.

Engreitz,J.M. et al. (2010) Content-based microarray search using differential
expression proﬁles. BMC Bioinformatics, 11, 603.

Faisal,A. et al. (2014) Toward computational cumulative biology by combin-
ing models of biological datasets. PLoS ONE, 9, e113053.

Fujibuchi,W. et al. (2007) Cellmontage: similar expression proﬁle search ser-
ver. Bioinformatics, 23, 3103—3104.

Georgii,E. et al. (2012) Targeted retrieval of gene expression measurements
using regulatory models. Bioinformatics, 28, 2349—235 6.

Hafemeister,C. et al. (2011) Classifying short gene expression time—courses
with Bayesian estimation of piecewise constant functions. Bioinformatics,
27, 946—952.

Hand,D.J. and Yu,K. (2001) Idiot’s Bayes — not so stupid after all? Int. Stat.
Rev., 69, 385—398.

Hunter,L. et al. (2001) GEST: a gene expression search tool based on a novel
Bayesian similarity metric. Bioinformatics, 17, 5115—5122.

Jordan,C. et al. (2007) Statistical modelling using product partition models.
Stat. Modell., 7, 275—295.

Malone,J. et al. (2010) Modeling sample variables with an experimental factor
ontology. Bioinformatics, 26, 1112—1118.

Meila,M. (2007) Comparing clusterings — an information based distance. ].
Multivar. Anal., 98, 873—895.

Petryszak,R. et al. (2014) Expression Atlas update — a database of gene and tran-
script expression from microarray- and sequencing—based functional genomics
experiments. Nucleic Acids Res., 42, D926—D932. (Database issue),

Schmidberger,M. et al. (2011) Conceptual aspects of large meta—analyses with
publicly available microarray data: a case study in oncology. Bioinf Biol.
Insights, 5, 13—39.

Seth,S. et al. (2014). Retrieval of experiments by efﬁcient comparison of mar-
ginal likelihoods. In: Loo, C. et al. (eds.) Neural Information Processing,
Volume 8835 of Lecture Notes in Computer Science. Springer International
Publishing, pp. 135—142.

Smith,A.A. et al. (2008) Similarity queries for temporal toxicogenomic expres-
sion proﬁles. PLoS Comput. Biol., 4, e1000116.

Vinh,N.X. et al. (2010) Information theoretic measures for clusterings com—
parison: Variants, properties, normalization and correction for chance. ].
Mach. Learn. Res., 11, 2837—2854.

Zhu,Y. et al., (2008) GEOmetadb: powerful alternative search engine for the
Gene Expression Omnibus. Bioinformatics, 24, 2798—2800.

9103 05 isnﬁnv uo sopﬁuv soc] ‘BIIIJOJIIBD 10 AJtSJQAtuf] 112 /310'S[BHJnOprOJXO'SOIJ’BLUJOJIIIOICI”Idllq 111011 popco1umoq

