Bioinformatics, 2016, 1—8

doi: 10.1093/bioinformatics/btw506

Advance Access Publication Date: 2 August 2016
Original Paper

 

 

Data and text mining

An efficient method to estimate the optimum
regularization parameter in RLDA

Daniyar Bakir, Alex Pappachen James and Amin Zollanvari*

Department of Electrical and Electronics Engineering, Nazarbayev University, Astana, 010000, Kazakhstan

*To whom correspondence should be addressed
Associate Editor: Jonathan Wren

Received on April 26, 2016; revised on July 4, 2016; accepted on July 21, 2016

Abstract

Motivation: The biomarker discovery process in high—throughput genomic profiles has presented
the statistical learning community with a challenging problem, namely learning when the number
of variables is comparable or exceeding the sample size. In these settings, many classical tech—
niques including linear discriminant analysis (LDA) falter. Poor performance of LDA is attributed to
the ill—conditioned nature of sample covariance matrix when the dimension and sample size are
comparable. To alleviate this problem, regularized LDA (RLDA) has been classically proposed in
which the sample covariance matrix is replaced by its ridge estimate. However, the performance of
RLDA depends heavily on the regularization parameter used in the ridge estimate of sample covari—
ance matrix.

Results: We propose a range—search technique for efficient estimation of the optimum regulariza—
tion parameter. Using an extensive set of simulations based on synthetic and gene expression
microarray data, we demonstrate the robustness of the proposed technique to Gaussianity, an as—
sumption used in developing the core estimator. We compare the performance of the technique in
terms of accuracy and efficiency with classical techniques for estimating the regularization param—
eter. In terms of accuracy, the results indicate that the proposed method vastly improves on simi—
lar techniques that use classical plug—in estimator. In that respect, it is better or comparable to
cross—validation—based search strategies while, depending on the sample size and dimensionality,
being tens to hundreds of times faster to compute.

Availability and Implementation: The source code is available at https://github.com/danik0411/0pti—
mum—rlda

Contact: amin.zollanvari@nu.edu.kz

Supplementary information: Supplementary materials are available at Bioinformatics online.

 

1 IntrOdUCtion p matrix, [I : [/i1./il. . . . ./i/,]I is a p-dimensional parameter vector

Ridge estimation is a type Of Shrinkage and traces back to the piom to be estimated, and a is the Iz-dimensional error vector with mean 0

eel.ng work of Hoerl and Kennard (Hoerl’ 1962; Hoerl and and covariance matrix (Ill/7. If we assume X is a full (column) rank

Kennard’ 1970351)) on estimating regression parameters. The), com matrix I]? <11), the ordinary least-square solution to this familiar lin-

sidered the standard linear model 631‘ model 18 glven by

y : X13 + s. (l) I}: (XIX) ‘X"‘y. (2)

where y is the Iz-dimensional observation vector, X is a known n X Howevels When [7 >71: the SOIUUOH I2) does 11‘“ 6X1“ because X

X becomes degenerate. Even the solution obtained by generalized

(C7 The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com l

/310‘sleumo[p10}xo‘soneuHOJIItotq/ﬁdnq

D. Bakir et al.

 

inverse form of matrix XTX is not working well. Hoerl and
Kennard (Hoerl, 1962; Hoerl and Kennard, 1970a,b) then formu—
lated a problem in which the residual sum of squares is replaced
by its Zz—penalized form given by

L2(ﬁ)élly-Xﬂllz +k11ﬁ112. (3)

where k >0 denotes a penalty factor controlling the length of If.
Minimizing L203) results in the so—called ridge regression given by

ﬁ: (xTX + klp)_1XTy. (4)

In this way, the inverse of possibly ill—conditioned XTX is stabi—
lized by adding the scalar matrix klp. This idea was then used by
Di Pillo (1976) to replace the estimate of the sample covariance
matrix used in linear discriminant analysis (LDA) by its ridge esti—
mate resulting in the so—called regularized LDA (RLDA). The goal is
to improve the performance of LDA in situations where dimension—
ality of observations, 17, is larger or comparable to the number of
measurements, 11. Di Pillo (1979) attempts to determine the opti—
mum value of the optimum regularization parameter in RLDA. On
this Di Pillo’s study, Peck and Ness (1982) comment that ‘He found
the analytical solution to this problem intractable, and so used a
simulation study to choose an optimum value for k [the regulariza—
tion parameter]. He concluded that if an algorithm can be found
which leads to a value of k near the optimum value, then consider—
able improvement in the PCC [probability of correct classification]
should occur’.

Friedman (1989) suggested the use of cross—validation in finding
the optimum value of regularization parameter. In this procedure,
cross—validation is used to estimate the true error of RLDA for each
value of the regularization parameter selected from a pre—specified
set of size 25—5 0. The estimate of the optimum regularization par—
ameter is then the one that results in the minimum cross—validation
estimate of true error. Despite the computational complexity of
cross—validation in such a search algorithm [e.g. see comments in
Friedman (1989), Sharma et al. (2014) and Tasjudin and Landgrebe
(1998)], this approach has remained the most popular method in
estimating the optimum value of regularization parameter in
RLDA—for instance, see Guo et al. (2007), Bandos et al. (2009), Ye
et al. (2006), Huang et al. (2009) and Ye and Xiong (2006) to cite
just a few articles.

Recently, we constructed a generalized consistent estimator of
true error of RLDA. In this regard, we proposed an estimator that
converges to true error in a double asymptotic sense. In this setting,
the estimator converges to the actual parameter in an asymptotic
scenario in which dimension and sample size increase in a propor—
tional manner (11 —> 00. p —> 00 and 17/11 —>] > 0) (Zollanvari and
Dougherty, 2015). In developing this estimator, we assumed that the
true distributions governing the data follow multivariate Gaussian
model. However, the underlying mechanism to develop the estima—
tor was based on double asymptotics and random matrix theory,
both of which suggest applicability of the estimator in non—Gaussian
settings as well [see p. xii in Girko (1995), p. 335 in Bai and
Silverstein (2010) and Zollanvari (2015)]. In this work, we employ
this estimator of true error in a one—dimensional search to estimate
the optimum regularization parameter of RLDA. As such, we em—
ploy data taken from seven gene expression microarray studies as
well as synthetically generated Gaussian and non—Gaussian data. We
compare the performance (in terms of accuracy and efficiency) of
the search technique that uses this estimator with similar search
schemes that use cross—validation or plug—in estimators. Using an ex—
tensive set of simulations, we observe that the proposed technique is

an efficient method that can outperform cross—validation and plug—
in estimate—based schemes in estimating the optimum regularization
parameter of RLDA.

Throughout this work, we use boldface lower case letters to de—
note a column vector. A boldface upper case letter denotes a matrix
and tr[.] is the trace operator. The identity matrix of p dimension is
denoted by IP.

2 Systems and methods
2.1 RLDA classifier

Assume a separate sampling scheme is employed: n : no + 111 sam—
ple points are collected to constitute the sample S in R”, where n, no
and 111 are non—random and pre—determined and where So : {x1,xz,
. . . ,xm} and S1 : {xm,+1,xm,+2, . . . ,xn} are randomly selected from
populations Ho and H1, respectively. In this two—class problem, a
classifier is a function 1p” : R” —> {0,1}. If p" is given by 1,0,,(X) : 0
if x E R0 and [pn(x) : 1 if x E R1, where R0 and R1 are measurable
sets partitioning the sample space, then the true error of 1p”, denoted
by 8, is defined to be the probability of misclassification,

e : ozolef(x)0)dx + u1fRof(x)1)dx : 51080 + {1181, (5)

where or, is the prior probability for class i, 8, is the error contributed
by class i, and f(x)0) and f(x)1) are the class—conditional densities
governing 1'10 and 111, respectively. Separate sampling is very com—
mon in biomedical applications, where data from two classes are
collected without reference to the other class, for instance, when dis—
criminating two types of tumors or when distinguishing a normal
from a pathological phenotype. With separate sampling, the prior
probabilities or, cannot be estimated from the sample, an issue with a
long history in the study of LDA (Anderson, 1951). Both classifica—
tion rules (Esfahani and Dougherty, 2014) and error estimation rules
(Braga—Neto et al., 2014) need to be adjusted for separate sampling
rather than use their usual random—sampling definitions; otherwise,
they suffer performance degradation. The adjustment requires that
510 and 511 be known, as assumption made in this study. In our case,
the adjustment is straightforward because it simply means that we
directly use 010 and 011 rather than their random—sampling estimates
% and  In practice, the salient point is that given 11, no and 111 are
chosen so that Q is as close to 010 as possible (Esfahani and
Dougherty, 2014).

Assuming I'I, follows a multivariate Gaussian distribution
N(pi,2), for i: 0, 1, where 2 is the common non—singular covari—
ance matrix of both class, replacing the unknown mean and the co—
variance matrix of classes in Bayes rule (optimum classifier) results
in LDA, which is characterized by Anderson’s statistics,

_ _ T
WLDA<XO.X1.C.X> : (x —  C4020 — xi). (6)

— 7 1 — 7 1
where x0 — n—szleso x, and x1 — ZZXIEsl

for classes 0 and 1, respectively, and C is the pooled sample covari—

x, are the sample means

ance matrix,

W

 

: 7
C 110+,“ _2 7 ( )
where
1 _ _ T
Ci In._1Z (XI-Xi)(XI-Xi) - (8)
’ xxesi

/310‘spzumo[p10}xo‘sopeuHOJIItotq/ﬁdnq

Optimum regularization parameter: gene expression data

 

In this work, we consider a form of RLDA classifier that is
obtained by using ridge estimators of the inverse covariance ma—
trix in WLDA; that is, by using (I + yC)_1 and y > 0. in (6), which
yields

 

_ _ T
W(xo,x1,C,x) : <x—X0:X1> H(xo —x1), (9)
where
H z (1, + yC)_1. (10)

The designed RLDA classifier is then given by

1, ifW(xo,x1,C.x) S c

RLDAX : 7 (11)
I” I) {0, ifW(xo,x1,C,x)>c

1—rxo
do '

 

where c : log

2.2 RLDA true error, optimum regularization and their
estimates
The true error of WELDA is given by (5). Given sample S", for i: 0, 1,

s,- :P<<—1>‘W<20.xl.c.x> : <—1>‘clxe Emma (12>
Under the multivariate Gaussian model, we have

i+1 _ _ i
 _¢(<—1> C(ﬂi7X07X17H)+(—1)6>7 (13)
D(xo,x1,H,2)

where  denotes the cumulative distribution function of a stand—
ard normal random variable and

_ _ T
_ _ X ‘I'X _ _
C(ﬂi7X07X17H): <”i_ 0 2 1) H(X0_X1)7

 

(14)
D(io.iiiH72) : (io - ii)TH2H(io - i1)-

Given training data, the optimal choice of y is the value of y,
which minimizes the overall true error 8 as defined by (5) and (13);
to Wit, Vol” : argminy 8. However, true error depends on unknown
population parameters [1,- and 2, which must be estimated from
training data. As such, the optimum regularization parameter de—
pends on unknown distributional parameters and must be estimated
from data as well. Even with the assumption of knowing the true

“I” is the solution of a non—linear equa—

distributional parameters, y
tion that needs to be solved numerically. To see the latter statement
and for simplicity of presentation, let on : 1 and 011-, : 0, i: 0, 1,
which means Vol” : argminy e : argminy 8i. By taking the derivative
of 8,- defined in (13) with respect to y, setting the derivative to zero,
and after some tedious but straightforward algebraic manipulations

opt

we observe that y is the unique positive solution of the following

equation:
C(ﬂivi07i17HCH) C(ﬂi7i07i17H)

: 1
D(io,i1.H.ch> D(io.i1.H.2> ’ ( 5)

 

where dependency of equation on y is via H defined in (10).
The non—linearity of the equation makes a closed form expression of
yopt hopeless. As such, a range search strategy is a feasible path
forward.

The objective in the range search is to determine the y that
minimizes the estimate of true error of RLDA. In this regard, a clas—
sical estimate of true error is obtained by replacing the un—
known parameters by their sample estimate, resulting in

standard plug—in estimator of true error, which is given by
(McLachlan, 2004)

1P 7 ¢<(—1)i+1G(i,,io,i1,H)+(—1)ic>.

8- — (16)
D(xo,x1,H,C)

1

It is straightforward to see that for; fixed 17, as n,- —> 00, we have
x,- —> p,- and C —> 2, and therefore, are EFLDA
vergence in probability.

In Zollanvari and Dougherty (2015), we proposed the following

where _1’> denotes con—

estimator for true error of RLDA:

 

,- —2 3 ,-
(—1)+1G(ii.io.i1.H)+W+(—1)c
a? : (I) A ' ,
(1 + V6)2D(i07i17H7 
(17)
where
L_JEI_

6 "0+n1—2 "0+n1—2 

 

 

: . p trlHl ‘
/(1 — no+m—2 + nc+n1-2>

Using random matrix theory and under double asymptotic con—
ditions, the estimator (17) converges (almost surely) to true error.
The double asymptotic conditions are mainly characterized by
no —> oo,n1 —> 00,17 —> 00, with the assumption that the following
limits exist: “in—Jo > 0.5—1—>]1 > 0, and ﬁe] < 00.
Nevertheless, the readers are referred to Zollanvari and Dougherty
(2015) for the complete list of conditions used in developing (17).

We use the following protocol to estimate yo” using a set of
benchmark gene expression datasets and, at the same time, compare
the performance of the proposed search strategy based on various
estimators of error. The estimators that we use are 5—fold cross—
validation with five repetitions (CV5F—5R), leave—one—out (loo),
plug—in (5P) available from (16) and our proposed double—

asymptotic estimator 5-D available from (17). The experiments on

1
real data and synthetic data are essentially similar except that in
real—data experiments we employ t—test feature selection to reduce

the dimensionality to P : 50 and P : 150.
Protocol (Real Data):

° Step I: Let r denote the ratio of the total number of sample points
in class 0 to the total amount in class 1 in the full dataset. Let
npun denote the sample size in the full dataset. Fix a value n
< 71le and let it be the number of training sample points that
are randomly taken out of the whole dataset such that n : no
+n1 with n, being the number of training sample points in class i.
We choose no : (rn1j , where  is the ﬂoor function. This prac—
tice resembles a random sampling scheme in which oco z % and
011 z  Therefore, we use these values of or, to ﬁnd the overall
error rate from (5) and the held—out samples. In order to set aside
enough sample points for testing (i.e. the "Full — n held—out sam—
ple), we restrict the training sample size to n E [30. 100] (for syn—
thetic data, we consider n E [30. 300]).

° Step H: For a prescribed value of regularization parameter y in a
prescribed range, design the RLDA classiﬁer by (9). We discretize
the range with the exponential function (10000) for i : {—10,
—9. —8. . . . . 10} that covers values from 0.001 to 1000. The
above exponential function has been chosen to improve the efﬁ—
ciency of the search. This choice seems to be a reasonable one be—
cause a small perturbation in large values of y is a smaller
relative change with respect to a similar perturbation in small

ﬁm'sreumol‘pquo'sopeuuoptrotq/ﬁdnq

D. Bakir et al.

 

values of y. This implies that the effect of the former perturbation
in changing the true error of the classiﬁer may not be as large as
the latter perturbation (although in terms of magnitude both per—
turbations are the same). In other words, for large values of y
having a ﬁne discretization is not as critical as small values.

° Step HI: For each value of y in the prescribed set of points, esti—
mate the error of the designed classiﬁer using as estimator of
error (CV5F—5R, 100, EP and ED). Obtain the holdout estimate of
the true error (taken as the true error) from the test data.

° Step IV: The estimate of the optimum y is the V which results in
the smallest error estimate on the prescribed range of v. For the
estimated optimum y record the value of true error (available
from Step III).

° Step V: Repeat Steps I—IV, 500 times for each n and determine
the average expected error of RLDA.

3 Results and discussion

Based on the protocols described in Section 2, we have performed a
set of experiments employing both synthetic models and gene ex—
pression microarray data to examine the performance of the search
scheme based on various estimators. First, we consider seven pub—
licly available datasets on breast cancer (van de Vijver et al., 2002),
pediatric acute lymphoblastic leukemia (Yeoh et al., 2002), hepato—
cellular carcinoma (Chen et al., 2004), toxicants response on rats
(Natsoulis et al., 2005), diffuse large B—cell lymphoma (Rosenwald
et al., 2002), node—negative breast cancer (Desmedt et al., 2007) and
acute myeloid leukemia (Valk et al., 2004). Table 1 provides a sum—
mary of these datasets, including the total number of genes and sam—
ple size. For a description of the data preparation, the readers are
referred to Supplementary Section 1. shows the expected true and es—
timate of error for RLDA classifier as a function of regularization
parameter y for different number of sample points ranging from 30
to 100 chosen from datasets listed in Table 1 with p : 50 and
p : 150, respectively. This leaves us with 8 (sample sizes) X 7 (data—
sets) >< 2 (dimensionalities):112 experiments on real data. As seen
in the far right column of these figures, for each sample size, the true
error of classifier decreases as a function of y and then increases
for increasing y with the optimal y corresponding to the minimum
true error at the bottom of the valley. In this regard, in all experi—
ments such a ‘peaking phenomenon” occurs in the pre—specified
range of y E [0001,1000] with 75% of times (84 out of 112) hap—
pening in the range [0.1, 100]. Notice that this peaking phenom—
enon is also observed in curves of estimated errors (columns 1—3
in Fig. 1 and Supplementary Fig. S1) except for the plug—in estimator,
suggesting that plug—in is not a good estimator of the optimum y.
Figure 2(a—n) shows the expected true error of RLDA classifier
designed using the estimate of the optimum y (the y that results in
the minimum estimated error in Fig. 1 and Supplementary Fig. S1)
obtained from various estimators as a function of sample size on
each dataset. We observe that an RLDA classifier designed by dou—
ble asymptotic estimator ED has a better or comparable performance
to RLDA classifiers constructed using plug—in, CV5F—5R and 100 es—
timators. At the same time, we have to note that to compute ED, we
only need to evaluate the closed—form expression presented in (17).
Consequently, ED is tens to hundreds of times faster to compute
than cross—validation estimators. To illustrate this point, we have
plotted the ratio of average time it takes to compute CV5F—5R and
leave—one—out estimators to the time it takes to compute EP and ED
estimators in experiments related to Chen et al. (2004) (see Fig. 3).

Table 1. Microarray studies used in this work

 

 

Dataset Features no /n1
Chen et al. (2004) 10 237 75/82
Desmedt et al. (2007) 22 215 98/77
Natsoulis et al. (2005) 8491 120/61
Rosenwald et al. (2002) 5013 114/89
Valk etal. (2004) 22 215 116/157
van de Vijver etal. (2002) 10 237 180/115
Yeoh et al. (2002) 5077 149/99

 

The actual average compute time is presented in the Supplementary
Section S6.

Note that the pre—specified range of y is important to obtain a
realistic view of the performance of estimators. For example, if we
limit the search range of y to [0.1, 100], then in the Natsoulis’ experi—
ment, the classical plug—in estimator 5P, which is not expected to
have a good performance in small—sample situations, outperforms
all other estimators (see Supplementary Fig. S2). This behavior is be—
cause in this dataset for all examined sample sizes the optimum
regularization parameter is larger than or close to the upper limit of
the range of y E [0.1, 100]. This can be seen from the figure on the
third row, fifth column in Figure 1. At the same time in all datasets,

5P points to the upper bound of the range as the estimate of the opti—

mum regularization parameter, which in the Natsoulis’ experiment
happens to be closer to the actual optimum regularization parameter
(see the plot in the third row, fourth column of Fig. 1).

We also used synthetic data to compare the performance of esti—
mators in estimating optimum y. Figure 2(o—t) shows the results
for a wide range of Bayes (optimum) error and 17:20 for data
taken from Gaussian and skew—normal distributions. For the com—
plete set of results along with the protocol used for synthetic experi—
ments, see Supplementary Sections S4 and S5. In most of
experiments on synthetic data, ED uniformly outperforms other esti—
mators of y.

The efficiency of the proposed procedure is a direct consequence
of having a closed form for the core estimator that we use in the
search. The good performance is due to convergence of the core esti—
mator to true error in a double asymptotic regime. Classically, the
notion of statistical consistency guarantees the performance of an es—
timator in situations where the number of measurements unbound—
edly increases for a fixed dimensionality (n —> 00, 17 fixed). In a finite
sample operating regime, this implies that in order to expect an
acceptable performance from an estimator, we need to have many
more sample points than variables. However, in a double asymptotic
regime the magnitude of p and n are kept comparable (p/ n —> ] > 0
with ] being an arbitrary number) and, as a result, we generally ex—
pect an acceptable performance of developed estimators in a wide
range of dimension and sample size. We note that both cross—
validation and plug—in estimators are statistically consistent in a clas—
sical sense whereas the core estimator that we use in the search is a
consistent estimator in a double asymptotic sense.

4 Concluding remarks

A recently proposed estimator of true error of RLDA based on dou—
ble asymptotics is used in a one—dimensional search to optimize the
performance of the classifier in terms of regularization parameter.
While in developing the core estimator used in the search we have

/310‘spzumo[p10}xo‘soneuHOJIItotq/ﬁdnq

 

 

 

 

 

 

 

 

et al. (2004) van de Vijver et al. (2002)

 

 

 

 

 

 

 

 

Yeoh et al. (2002)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Chen et al. (2004) Desmedt et al. (2007) Natsoulis et al. (2005) Rosenwald et al. (2002) Valk

/310'S[EUm0prOJXO'SOIJEIIIJOJIIIOIq/ﬂduq

 

 

 

 

 

 

 

 

 

 

Valk et al. (2004)

 

 

 

Chen et al. (2004)
van de Vijver et al. (2002)

 

 

 

 

 

 

 

 

 

 

 

Desmedt et al. (2007) Natsoulis et al. (2005)

Yeoh et al. (2002)

Supplementary Section 84

 

 

 

 

 

 

 

 

 

 

 

 

 

Rosenwald et al. (2002)

/310'S[EUm0prOJXO'SOIJEIIIJOJIIIOIq/ﬂduq

Optimum regularization parameter: gene expression data

 

 

— CVSF —5R/dasym —est

 

- - loo/dasym —est

- ° - CVSF —5R/p|ug—in - - loo/plug—in

 

 

(a)

 

 

 

 

60 8O 1 00

4O

 

 

 

 

2O

 

 

 

Sample Size

Fig. 3. The ratio of average compute time of CVSF-SR and leave-one-out (loo) estimators to average compute time of EP (plug-in) and E

30 4O 5O 60 7O 8O 90 100
Sample Size

D (dasym-est) estimators

versus sample size: (a) p: 50; (b) p: 150. See Supplementary Section 86 forthe actual compute time in terms of seconds on a personal computer

assumed the Gaussianity of the data, the underlying mechanism to
develop the estimator is based on random matrix theory. The uni-
versality principle of random matrix theory though suggests applic-
ability of developed estimators in non-Gaussian settings as well [see
p. xii in Girko (1995), p. 335 in Bai and Silverstein (2010) and
Zollanvari (2015)]. In this work, we conducted an extensive set of
simulations using both synthetic and gene expression microarray
data to compare the performance of our technique in terms of ex-
pected error of the constructed RLDA and the compute time to simi-
lar search schemes that use classical error estimators (5-fold cross-
validation with five repetitions, leave-one-out and plug-in estima-
tor). We observe that the proposed technique is tens to hundreds of
times faster than cross-validation to compute, while at the same
time results in a comparable or better classification accuracy of
the constructed RLDA. The good accuracy of the proposed tech-
nique on non-Gaussian real data and synthetic data used in this
study confirms robustness of the estimator to non-Gaussianity of
data. The next natural step in this line of work is to estimate the
RLDA regularization parameter that minimizes the area under the
ROC curve.

Funding: This work was partially supported by the Nazarbayev
University Social Policy Grant (to AZ)

Conflict ofInterest: none declared.

References

Anderson,T. (1951) Classiﬁcation by multivariate analysis. Psychometri/ea,
16, 31—50.

Bai,Z.D. and Silverstein,j.W. (2010). Spectral Analysis of Large Dimensional
Random Matrices. Springer-Verlag, New York.

Bandos,T.V. et al. (2009) Classiﬁcation of hyperspectral images with regular-
ized linear discriminant analysis. IEEE Trans. Ceosci. Remote Sens., 47,
862—873.

Braga-Neto,U. et al. (2014) Cross-validation under separate sampling: strong
bias and how to correct it. Bioinformatics, 30, 3349—3355.

Chen,X. et al. (2004) Novel endothelial cell markers in hepatocellular carcin-
oma. Mod. Pat/001., 17, 1198—1210.

Desmedt, C. et al. (2007) Strong time dependence of the 76-gene
prognostic signature for node-negative breast cancer patients in the transbig
multicenter independent validation series. Clin. Cancer Res., 13,
3207—32 14.

Di I’illo,I’.j. (1976) The application of bias to discriminant analysis. Commnn.
Stat—Theor. M., 5, 843—854.

Di I’illo,I’.j. (1979) Biased discriminant analysis: Evaluation of the opti-
mum probability of misclassiﬁcation. Commnn. Stat — Theor. M., 8,
1447—1457.

Esfahani,M.S. and Dougherty,E.R. (2014) Effect of separate sampling on clas-
siﬁcation accuracy. Bioinformatics, 30, 242—250.

Friedma11,j. (1989) Regularized discriminant analysis. Amer. Stat. Assoc.,
84,165—175.

Girko,V.L. (1995). Statistical Analysis of Observations of Increasing
Dimension. Kluwer Academic Publishers, Dordrecht.

Guo,Y. et al. (2007) Regularized discriminant analysis and its application in
microarrays. Biostat, 8, 86—100.

Hoerl,A.E. (1962) Application of ridge analysis to regression problems. Chem.
Eng. Frog, 58, 54—59.

Hoerl,A.E. and Kennard,R.W. (1970a) Ridge regression: Applications to non-
orthogonal problems. Technometrics, 12, 69—82.

Hoerl,A.E. and Kennard,R.W. (1970b) Ridge regression: Biased estimation
for nonorthogonal problems. Technometrics, 12, 5 5—59.

Huang,D. et al. (2009) Comparison of linear discriminant analysis methods
for the classiﬁcation of cancer based on gene expression data.  Exp. Clin.
Cancer Res., 28, 1—8

McLachlan,G. (2004). Discriminant Analysis and Statistical Pattern
Recognition. Wiley, New York.

Natsoulis,G. et al. (2005) Classiﬁcation of a large microarray data set: algo-
rithm comparison and analysis of drug signatures. Genome Res., 1,
724—736.

I’eck,R. and Ness,j.V. (1982) The use of shrinkage estimators in lin-
ear discriminant analysis. IEEE Trans. Pattern Anal. Mac/0. Intell., 4,
409—424.

Rosenwald,A. et al. (2002) The use of molecular proﬁling to predict survival
after chemotherapy for diffuse large-b-cell lymphoma. N. Eng. Med., 346,
1937—1947.

Sharma,A. et al. (2014) A feature selection method using improved regulariza-
tion discriminant analysis. Mac/0. Vision Appl., 25, 775—786.

Tasjudin,S. and Landgrebe,D.A. (1998) Covariance estimation for limited
training samples. In: Proceedings of the IEEE Geoscience and Remote
Sensing Symposium (IGARSS), Seattle, WA, pp. 2688—2690.

Valk,I’.j. et al. (2004) I’rognostically useful gene-expression proﬁles in acute
myeloid leukemia. N. Eng. Med., 350, 1617—1628.

van de Vijver,M. et al. (2002) A gene-expression signature as a predictor of
survival in breast cancer. N. Engl.  Med., 347, 1999—2009.

Ye,j. and Xiong,T. (2006) Computational and theoretical analysis of null
space and orthogonal linear discriminant analysis. Mac/0. Learn. Res., 7,
1 183—1204.

/310'S[EUm0prOJXO'SOIJEIIIJOJIIIOIq/ﬂduq

D. Bakir et al.

 

Ye,]. et al. (2006) Efﬁcient model selection for regularized linear discriminant
analysis. In: Proceedings of the 15th ACM International Conference on
Information and Knowledge Management, Arlington, Virginia, pp. 5 32—539.

Yeoh,E.]. et al. (2002) Classiﬁcation, subtype discovery, and prediction of out—
come in pediatric acute lymphoblastic leukemia by gene expression proﬁl-
ing. Cancer Cell, 1, 133—143.

Zollanvari,A. (2015) High-dimensional statistical learning: Roots, justiﬁca-
tions, and potential machineries. Cancer Inform., 5, 109—121.

Zollanvari,A. and D0ugherty,E.R. (2015) Generalized consistent error
estimator of linear discriminant analysis. IEEE Trans. Sig. Proc., 63,
2804—2814.

/810'sleumofp103x0"soueuuogutotqﬂ:duq

