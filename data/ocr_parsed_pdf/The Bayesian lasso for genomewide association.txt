ORIGINAL PAPER

Vol. 27 no. 4 2011, pages 516—523
doi: 1 0. 1093/bioinformatics/btq688

 

Genetics and population analysis

Advance Access publication December 14, 2010

The Bayesian lasso for genome-wide association studies

Jiahan Li1’2, Kiranmoy Dasla2, Guifang Fulaz, Runze U142 and Rongling Wu1’2’*

1Department of Statistics, Pennsylvania State University, State College, PA 16802 and ZCenter for Statistical
Genetics, Pennsylvania State University, Hershey, PA 17033, USA

Associate Editor: Jeffrey Barrett

 

ABSTRACT

Motivation: Despite their success in identifying genes that affect
complex disease or traits, current genome-wide association studies
(GWASs) based on a single SNP analysis are too simple to elucidate
a comprehensive picture of the genetic architecture of phenotypes.
A simultaneous analysis of a large number of SNPs, although
statistically challenging, especially with a small number of samples,
is crucial for genetic modeling.

Method: We propose a two-stage procedure for multi-SNP modeling
and analysis in GWASs, by first producing a ‘preconditioned’
response variable using a supervised principle component analysis
and then formulating Bayesian lasso to select a subset of significant
SNPs. The Bayesian lasso is implemented with a hierarchical model,
in which scale mixtures of normal are used as prior distributions for
the genetic effects and exponential priors are considered for their
variances, and then solved by using the Markov chain Monte Carlo
(MCMC) algorithm. Our approach obviates the choice of the lasso
parameter by imposing a diffuse hyperprior on it and estimating it
along with other parameters and is particularly powerful for selecting
the most relevant SNPs for GWASs, where the number of predictors
exceeds the number of observations.

Results: The new approach was examined through a simulation
study. By using the approach to analyze a real dataset from the
Framingham Heart Study, we detected several significant genes that
are associated with body mass index (BMI). Our findings support the
previous results about BMl-related SNPs and, meanwhile, gain new
insights into the genetic control of this trait.

Availability: The computer code for the approach developed is
available at Penn State Center for Statistical Genetics web site,
http://statgen.psu.edu.

Contact: rwu@hes.hmc.psu.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on August 16, 2010; revised on December 3, 2010;
accepted on December 9, 2010

1 INTRODUCTION

Recent genotyping technologies allow the fast and accurate
collection of genotype data throughout the entire genome for
many subjects. By genome-wide association studies (GWASs), the
genetic variants associated with a complex disease or trait, their
Chromosomal distribution and individual effects, can be identiﬁed.
GWASs are based on either case—control cohorts to test the

 

*To whom correspondence should be addressed.

associations between SNPs and diseases or population cohorts to
estimate genetic effects of SNPs on traits. In both cases, there are
hundreds of thousands of SNPs genotyped on samples involving
thousands of subjects. This typical problem, having the number
of predictors far exceeding the number of observations, makes
it impossible to analyze the data using traditional multivariate
regression. In current GWASs, simple univariate linear regression
that analyzes one SNP at a time is usually used and, by adjusting for
multiple comparisons, the signiﬁcance levels of the detected genes
are then calculated (McCarthy et al., 2008).

These single SNP-based GWASs have been instrumental for
reproducibly detecting signiﬁcants genes for various complex
diseases or traits (Donnelly, 2008). However, such strategies have
three major disadvantages, limiting the future applications of
GWAS. First, because most complex traits are polygenic, a single
SNP analysis can only detect a very small portion of genetic variation
and, also, may not be powerful for identifying weaker associations
(Hoggart et al., 2008). Second, different genes may interact with
each other to form a complex network of genetic interactions,
which cannot be Characterized from a single SNP analysis. Third,
many GWASs analyze genetic associations separately for different
environments, such as males and females, and then make an across-
environment comparison in genetic effects. This analysis is neither
powerful nor precise for the identiﬁcation of gene—environment
interactions. Because of these limitations, many authors have
developed various approaches for simultaneously analyzing multiple
SNPs for GWASs (Logsdon et (11., 2010; Wu et al., 2009; Yang et al.,
2010), although most approaches focus on case—control cohorts.

There is a daunting need on the development of a variable
selection model to identify SNPs with signiﬁcant effects on
quantitative traits in population cohorts and estimate all selected
predictors simultaneously. Traditionally, a subset of predictors in
a regression model is obtained by forward selection, backward
elimination and stepwise selection, but these approaches are
computationally expensive and unstable even when the number of
predictors is not large. Recently, alternative approaches have been
developed, including ridge regression, bridge regression (Frank and
Friedman, 1993), least absolute shrinkage and selection operator
(LASSO) (Tibshirani, 1996), elastic net (Zou and Hastie, 2005) and
the smoothly Clipped absolute deviation (SCAD) penalty (Fan and
Li, 2001). For the number of variables much larger than that of
subjects, as commonly seen in GWASs, Fan and Lv (2008) proposed
a two-stage procedure for variable selection by ﬁrst suppressing
the high dimensionality of response into its low-dimensional
representation and then ﬁnding a subset of predictors that can predict
the suppressed response. A similar two-stage approach was also
developed by Paul et al. (2008).

 

516 © The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 ﬁlo's[Bumo[pJOJXO'sotwurJOJutotqﬂ:duq 11101} papBOIIIAAOG

9103 ‘Og anﬁnv uo ::

Lasso for GWAS

 

In this article, we for the ﬁrst time integrate Paul et al.’s
preconditioning procedure into LASSO to develop a two-stage
strategy for identifying important SNPs in GWASs. In step one, we
ﬁnd a linear combination of predictors that are strongly correlated
with the response by a supervised principle component analysis
and get a consistent ‘preconditioned’ estimate of response variable.
In step two, we implement the Bayesian lasso (Park and Casella,
2008) for variable selection based on the ‘preconditioned’ response
that mitigates the observational noise. The Markov Chain Monte
Carlo (MCMC) algorithm is used to estimate all the parameters.
The Bayesian hierarchical model is implemented to control an issue
of over-ﬁtting that arises when too many associations are included.
Our model shows a great ﬂexibility to ﬁt many SNPs and many
covariates at the same time. The statistical properties of the model
were tested through simulation studies. We used a real GWAS dataset
from the Framingham Heart Study (FHS) to validate the useﬁilness
and utilization of the new model.

2 BAYESIAN GWAS MODEL

2.1 Preconditioning

When the number of predictors far exceeds the number of
observations, preconditioning via a supervised principal component
analysis is recommended to reduce the effect of observational noise
on model selection (Paul et al., 2008). In a GWAS of 11 subjects, we
express a response variable y (assumed to be normally distributed)
as a function of p SNPs genotyped throughout the entire genome
using a linear model

y = Wb + e, (1)
where W=(w1,...,wn)T is a (n Xp) design matrix, b=(b1,...,bp)T
is the vector of regression coefﬁcients and e~Nn(0,021n) is the
residual error.

The design matrix is reduced to one that consists of only
those predictors whose estimated regression coefﬁcients exceed a
threshold 6 in the absolute value. Thus, the reduced design matrix
Wreduced consists of the j’-th column of W, where 1" EU: lﬁjl >6}.
The principal components of Wreduced, called supervised principal
components, are computed. The ﬁrst m supervised principal
components can serve as independent variables in a linear regression
model, from which a consistent predictor y of the true response
is obtained. In practice, we select 6 by S-fold cross-validation.
Since only the ﬁrst few components are useful for prediction, in the
following examples we consider the ﬁrst three principal components.
Next, a standard variable selection procedure will be conducted for
the preconditioned response variable y.

2.2 Lasso penalized regression

Given phenotypical measurements and genotype information, we
could obtain the preconditioned response y based on the generic form
of linear regression (1). However in GWASs, a number of covariates,
which are either discrete or continuous, may be measured for each
subject. In order to estimate genetic effects precisely by adjusting for
these covariates, a GWAS model that takes into account the effects
of important covariates would be more appropriate. Therefore, we
describe the preconditioned value 9,- of a quantitative trait for subject
1 as

y,=M+X,Ta+ziTﬁ+giTa+;,Td+e,-, i=1,...,n, (2)

where [t is the overall mean, X,- is the dl-dimensional vector
of discrete covariates for subject 1', oc=(a1,...,ad1)T is the
vector of regression coefﬁcients for discrete covariates, Zl- is
the dZ-dimensional vector of continuous covariates for subject
1', ﬁ=(ﬂl,...,ﬂd2)T is the vector of regression coefﬁcients for
continuous covariates, a = (a1, ...,ap)T and (1: (d1, ...,dp)T are the
p-dimensional vectors of the additive and dominant effects of SNPs,
respectively, E,- and {l- are the indicator vectors of the additive and
dominant effects of SNPs for subject 1', and e,- is the residual error
assumed to follow a N (0,02) distribution. The j-th elements of E,-
and {l- are deﬁned as

1, if the genotype of SNP j is AA
Eij = 0, if the genotype of SNPj is Aa
— 1, if the genotype of SNPj is aa,

” _ 1, if the genotype of SNPj is Aa
{I} _ 0, if the genotype of SNPjis AA or aa.

Despite 1) >> 11 in the GWAS, most of the regression coefﬁcients in
(2) are expected to have no or only weak effects on the phenotype.
To identify a few SNPs that may have notable effects and enhance
prediction performance, we put L1 lasso penalties on the sizes
of additive effects and the dominant effects and encourage sparse
solutions using

I7 [7
Zlajlgt, Zldj|§t*, for130,t*30, (3)
j=1 j=1

where t and 1* are a certain value Chosen to penalize the additive
and dominant effects, respectively. Thus, parameters in Equation (2)
are estimated by the penalized least squares

[7 [7
élli—M—Xd—Zﬁ—Ea—§d||2+AZ|ajl+A*Zldj|. (4)
j=1 j=1
where y=(y1,...,y,,)T, [L=(/L,...,/L)T, X=(X1,...,X,,)T, z:
(21,...,z,,)T, g=(§1,...,gn)T, ;=(;1,...,;,,)T, and A and x* are
tuning parameters or lasso parameters that control the degrees of
shrinkage in the estimate of the genetic effects.

2.3 Bayesian hierarchical model and prior distributions
Noting the form of the Ll-penalty term in (4), Tibshirani (1996)
suggested that lasso estimates can be interpreted as posterior mode
estimates when the regression parameters have independent and
identical Laplace (i.e. double-exponential) priors. Therefore, when
lasso penalties are imposed on the additive and dominant effects of
SNPs, the conditional prior for aj is a Laplace distribution with the
scale parameter 0/)»:

I7
A '2
2 _ _ —Alavl/x/;
77(alt7 )— e l 7 (5)
1.11 2V 02
Similarly, the conditional Laplace prior for dj is
2 p 1* Md l/F
tr(d|a )= —e_ f ‘7‘. (6)
1.11 2V 02

Since the Laplace distribution can be represented as a scale
mixture of a normal distribution with an exponential distribution

 

517

112 ﬁlo's[BumoIpJOJXO'sotwurJOJutotq”:duq 11101} papeolumoq

9103 ‘Og anﬁnv uo ::

J.Li et al.

 

(Andrews and Mallows, 1974), we have the following hierarchical
representation of the penalized regression model:

Muhanﬂz~Nnm+Xa+zﬁ+sa+zdﬂZInt
a~Nd,(0. 2a).

ﬁNNdzmyzﬁ),

2 2 2 2 - 2 2
ale ,Tl,...,Tp~Np(0,0‘ d1ag(‘l,’1,...,‘lfp)),

2 2 p *2
11,...,rp|).~1_{exp(3).
J:

2 *2 *2 2- *2 *2
dla ,‘L'1 ,...,‘l,'p ~Np(0,o d1ag(‘L'1 ,...,‘l,'p )),

2 2 [7 A*2
If ,...,‘L';; |A*~ HCXIIKT),
i=1

02 ~7T(0‘2),

2 2 2 *2 *2
0,11,...,p,1'1 ,...,‘l,'p >0.

After integrating out 112,...,‘L'2 and 1T2,...,T , the conditional

priors on a and d have the desired forms (5) and (6), respectively.
We assign conjugate normal priors to ac and 13 because they are low
dimensional and not the parameters of interest. Finally, since the data
are usually sufﬁcient to estimate It and a, we can use an independent,
ﬂat prior 71(12): 1 for It and a non-informative scale-invariant prior
71(02) = 1/02 for 02.

The tuning parameters of the ordinary lasso can be prespeciﬁed
by cross-validation, generalized cross-validation or the idea based
on Stein’s unbiased risk estimate. However, in the Bayesian lasso,
A and 2* can be estimated along with other parameters by assigning
appropriate hyperpriors to them. This procedure avoids the Choice of
lasso parameters and allows us to determine the amount of shrinkage
from the data. In particular, we consider the conjugate gamma priors

on 22/2 and 2*2/2,
A2
71' 3 ~Gamma(a,b),

*2

A>I<2
7T 7 ~Gamma(a*,b*).

where a, b, a* and [9* are small values so that the priors
are essentially non-informative. With this speciﬁcation, lasso
parameters can be treated similar to the other parameters and
estimated by the Gibbs sampler.

3 POSTERIOR COMPUTATION AND
INTERPRETATION

3.1 MCMC algorithm

We estimate the parameters by sampling from their conditional
posterior distributions through the MCMC algorithm. The joint

posterior distribution can be expressed as:

IT(/L,0£,ﬁ,a,‘t%,...,T[%,)v,d,TT2,...,T;2,)v*,02|§’)

0< nil=17T(§’i|‘)”(M)JT(02)JT(oc)Jt(ﬁ)

f2,no)Irfmr}mummyIrfzmefzuonm)

Two-level hierarchical modeling allows us to easily
derive the conditional posterior distributions of parameters
and hyperparameters, from which the Gibbs sampler
draws posterior samples. Conditional on the parameters
(a,d,‘t%,...,‘L'2,‘tf2,...,t*2), the model is the standard linear
regression and, thus, the conditional posterior distributions of
(0543,02) are

n .~._ _ .T _ .T _ .T
al.~Nd1(2,<z,-=1X.ol u 2,3 s,a 441072,),

02

—1
’.'_ X-xT
with 2’=<#+231 ,

 

U2

 

U2

—1
’.' z-z.T

with 2”=(Zl=1%+2;1) .
U

3|.~Nd5 (21/(Zil=lzi6’i_M_X;“_§;a_§;d))72”),

1 " ,
02|.~1nv—x2(n.EZm—M—de—Zfﬁ—Efa-€fd)2).

i=1

Conditional on the parameters (17%,...,T[%,TT2,...,T;2,(¥,}3), the

model becomes the weighted linear regression, and thus the
conditional posterior distributions of (a, d) are

a|.~N(Aglgg,—h—X{a—zfﬁ—;fd),azA;1),

—1
with A;1 = <E§T+diag(112,...,t[%)_1) ,

di-~N(A;1;oi—u—X$a—Z$ﬁ—§$a>.aZA;1).

_ . — —1
with Ad 1 = <§§T+d1ag(‘tf2, ...,‘l,';;2) 1) .

Moreover, the full conditional for 112,...,T[%,TT2,...,T;;2 are
conditionally independent, with
I , A202 2 .
—2|-~Inverse-Gauss1an —2,A , J:l,...,p,
Ti 1
and
. A*20-2
—|...~Inverse-Gauss1an —,A*2 , j=1,...,p.
*2 2
Ti 51‘

Finally, with the conjugate priors Gamma(a,b) and
Gamma(a*,b*), the conditional posterior distributions of the

 

518

112 ﬁlo's[BumoIpJOJXO'sotwuiJOJutotq”:duq mot} p9p201umoq

9103 ‘0g15n8nv uo ::

Lasso for GWAS

 

hyperparameters are gammas

17 1.2
22|-~Gamma p+a, é+b ,
i=1
and
17 [*2
A*Z-~Gamma +a*, J—+b*
| p  2
1:

An efﬁcient Gibbs sampler based on these full conditionals
proceeds to draw posterior samples from each ﬁill conditional
posterior distribution, given the current values of all other
parameters and the observed data. This process continues until all
Chains converge. We use the potential scale reduction factor R to
access the convergence (Gelman and Rubin, 1992). Once R < 1.1 for
all scalar estimands of interest, we continue to draw 15 000 iterations
to obtain samples from the joint posterior distribution.

3.2 Posterior interpretation

The proposed MCMC algorithm for our Bayesian lasso model
can provide posterior median estimates of the additive effects and
dominant effects of individual SNPs, while adjusting for the effects
of all other SNPs and covariates. Furthermore, using the posterior
samples of a, d, and the observed genotypes, we can calculate the
proportion of the phenotypic variance explained by a particular SNP,
i.e. heritability, by

h; = 213113003]. + (131 —I30)dj)2 +4I3%13%dj2

1 varo>

 

, j=1,...,p,

where 131 is the estimated allele frequency for A, and 130 is the
estimated allele frequency for a, Ezj is the median estimate of

the additive effect for SNP j and glj is the median estimate of
the dominant effect for SNP j. Since heritability estimates are
unitless, they could guide variable selection and identify SNPs that
have relatively large effects on the phenotype.

4 RESULTS

4.1 Worked example

We used the newly developed model to analyze a real GWAS
dataset from the FHS, a cardiovascular study based in Framingham,
Massachusetts, supported by the National Heart, Lung and Blood
Institute, in collaboration with Boston University (Dawber et al.,
1951). Recently, 550000 SNPs have been genotyped for the entire
Framingham cohort (Jaquish, 2007), from which 418 males and
559 females were Chosen for our data analysis. These subjects
were measured for body mass index (BMI) at different ages from
29 to 61 years. As is standard practice, SNPs with minor allele
frequency < 10% were excluded from data analysis. The numbers
and percentages of non-rare allele SNPs vary among different
Chromosomes and ranges from 4417 to 28 771 and from 64% to
72%, respectively.

In principle, our approach can handle an extremely large number
of SNPs at the same time. To save our computing time, however,
we use those SNPs that cannot be neglected according to a
simple single SNP analysis. We Chose the phenotypic data of
BMI in a middle measure age of each subject for a single SNP

 

 

350 250

3|]I]
lell

25l]

M
=
=

Frequency
Frequency

5|]

 

 

 

 

 

n
22 24 Eli 28 313

BMI Precondltloned BMI

 

Fig. 1. The histograms of original and preconditioned BMI.

analysis, separately for males and females. Supplementary Figure S1
gives —log10 P-values for each SNP in the two sexes from
which 1837 SNPs with a —log10 P-value of > 3.5 in at least
one sex was selected for Bayesian lassso analysis. Before this
analysis, we imputed missing genotypes for a small proportion of
SNPs (5.16%) according to the distribution of genotypes in the
population. A preconditional analysis with m: 3 and 6:0.426 was
used to mitigate observational noise, leading to the preconditioned
phenotypes. Like original measures, the preconditioned BMI also
displays a normal distribution (Fig. 1), which meets the normality
assumption required by the new approach.

By treating the sex as a discrete covariate and age as a continuous
covariate, we imposed lasso penalties on the additive effects
a1,...,ap and dominant effects d1,...,dp to identify those SNPs
with notable effects on BMI. We employ the proposed MCMC
algorithms to estimate all parameters and implement variable
selection, where 20, = 1, 2,3 = 1 and all parameters in the conjugate
gamma hyperpriors are 0.1. In unreported tests, we ﬁnd that the
posteriors are not sensitive to these prior speciﬁcations, as long as
a and b are small values so that the priors are relatively ﬂat (Park
and Casella, 2008; Yi and Xu, 2008). Figure 2 plots the estimated
additive and dominant effects of each SNP after adjusting for the
effects of other SNPs and covariates. The heritability explained
by each SNP is shown in Figure 3. The Bayesian hierarchical
model automatically shrinks small coefﬁcients to zero, and hence the
posterior estimates of a, d and  can guide variable selection. We
Claim that a genetic effect is signiﬁcant if its 95% posterior credible
interval does not contain zero. Alternatively, Hoti and Sillanpaa
(2006) suggested to preset a threshold value, c, such that one SNP is
included into the ﬁnal model if the heritability explained by this SNP
is greater than c. We usually report the SNPs with high heritabilities
and, thus, this threshold can be Chosen on more subjective grounds.

Table 1 tabulates the names and positions of SNPs with the
heritability (hf?) greater than 0.5, as well as the estimated additive
effects and heritabilities. We do not report the estimated dominant
effects since they are relatively low in this example. The Bayesian
lasso tends to shrink small effects of genes into zero. Assuming that
a=d = 0.4 for a SNP with allele frequencies of 0.5 in a population,
the additive and dominant variances explained by this SNP is
%a2 = 0.08 and édz = 0.04, respectively. Thus, there is a possibility

 

519

112 ﬁlo's[BumoIpJOJXO'sotwuiJOJutotq”:duq uIOJJ p9p201umoq

9103 ‘0g15n8nv uo ::

J.Li et al.

 

lb

 

0.5 I I

0.3 e
0.2 e

0.1 l '

 

Estimated additive eﬂ’ect

 

 

0 500 1000 1500
SNP

 

0.03 I - - I
0.025 -
0.02 -
0.015 -
0.01 -
0.005 -

Estimated dominant effect

~0.005 -

 

 

 

‘um a son 1000 1500

SNP

Fig. 2. Estimated additive (A) and dominant effects (B) of 1837 SNPs from
the Framingham heart study.

that the dominant effects are shrunk to a greater extent than the
additive effects if they are of similar size. This may partly explain
why the dominant effects estimated for signiﬁcant SNPs are much
smaller than the additive effects.

The amount of shrinkage in the estimates of additive and dominant
effects are quantiﬁed by two hyperparameters A and A* determined
from the data. The posterior medians for A and A* are 54.474 and
54.523, respectively, with the 95% posterior intervals being [53.325,
55.626] and [53.359, 55.678], respectively. These suggest that the
tuning parameters for the additive and dominant effects can be
estimated precisely.

Since ﬁve signiﬁcant SNPs are selected from Chromosome 1, and
four from Chromosome 10, we will ﬁirther examine the correlations
among the signiﬁcant SNPs from the same Chromosome, as
suggested by a referee. The correlation matrix of ﬁve signiﬁcant
SNPs from Chromosome 1 is given by

1.00 0.85* 0.86* —0.01 0.01
0.85* 1.00 0.78* —0.01 0.02
0.86* 0.78* 1.00 —0.04 0.02 ,
—0.01 —0.01 —0.04 1.00 —0.84*
0.01 0.02 0.02 —0.84* 1.00

where star denotes signiﬁcant correlations at the signiﬁcance level
1%. Clearly, these SNPs can be Classiﬁed into two groups, and within
each group, SNPs are highly correlated. The correlation matrix of

 

b

0.6 - - - - - I—
0.4

0.2 -

0';  I  ILJ 

0 200 400 600 300 1000
SNP

Eslim ated additive effect

 

 

 

 

0.1

0 .00

0.06

0.04

0 .02

Estimated dominant effect

 

 

 

43,02 0 200 400 600 000 1000

SNP

 

Fig. 3. Estimated additive (A) and dominant effects (B) based on 50
simulations.

four signiﬁcant SNPs from Chromosome 10

1.00 0.53* 0.48* 0.30*
0.53* 1.00 0.52* 0.53*
0.48* 0.52* 1.00 0.45*
0.30* 0.53* 0.45* 1.00

also suggested that these SNPs are Closely linked to each other.

4.2 Computer simulation

The new approach is investigated through simulation studies. We
generate data according to the model (2) with [2:0, 02: 10 and
n = 500. For ease of simulation, El-j is derived from ul-j, where each uij
has a standard normal distribution marginally, and p = c0v(u,-j , uik) =
0.1. Then, to mimic a SNP with equal allele frequencies, we set

1, ul-j>c
Sly: 0. —CEuijEC
—1,u,-j<—c,

where —c is the ﬁrst quartile of a standard normal distribution.
Finally, {if is derived from Eij. We assume that there are 1000 SNPs
from which 20 are signiﬁcant for a phenotypic trait. The positions
and additive and dominant effects of individuals are given in Table 2.
It is assumed that the trait is measured at a subject-speciﬁc age,
following the data structure of the FHS.

 

520

112 ﬁlo's[BumoIpJOJXO'sotwuiJOJutotq”:duq uIOJJ p9p201umoq

9103 ‘0g anﬁnv uo ::

Lasso for GWAS

 

Table 1. Information about signiﬁcant SNP5

 

Chromosome Name Position Additive Heritability (%)

 

1 5566185476 8445140 0.15 0.74
1 5566374301 8451728 0.19 1.36
1 5566295856 8578082 0.20 1.35
1 5566516012 198313489 —0.13 0.89
1 5566364251 198321700 0.12 0.66
10 5566311679 32719838 0.33 4.65
10 5566293192 32903593 0.27 2.08
10 5566303064 32995111 0.36 5.93
10 5566128868 33407810 0.28 2.75
20 5566171460 22580931 —0.13 0.78
22 5566055592 23420006 0.33 5.13
22 5566164329 23420370 0.37 6.70

 

Table 2. Genetic effects of 20 assumed SNP5 for data simulation

 

 

Position Additive Position Dominant
100 1.2 50 1.2
200 1.2 150 1.2
300 1.2 250 1.2
400 0.8 350 0.8
500 0.8 450 0.8
600 0.8 550 0.8
700 0.4 650 0.4
800 0.4 750 0.4
850 1.2 850 1.2
900 0.8 900 1.2
950 1.2 950 0.8
1000 0.8 1000 0.8

 

Figure 3 gives the estimated additive and dominant genetic effects
of different SNP5 over 50 simulations, and Figure 4 plots the
heritability explained by each SNP. It is Clear that lasso penalties
shrink small genetic effects to zeros, resulting in sparse solutions
of the regression coefﬁcients. In general, the 20 assumed SNP5
can be well identiﬁed and their additive and dominant effects well
estimated. Also, two hyperparameters A and A* whose inﬂuence
the degree of shrinkage can be well estimated. In Supplementary
Figure 2, the histograms of these two hyperparameters are shown.

Then, we carry out another simulation study to compare the
performance of preconditioned Bayesian lasso, Bayesian lasso
without preconditioning and the traditional single SNP analysis.
Without loss of generality, only the additive model is considered.
Speciﬁcally, we generate data on n=200 and p: 500 or 1000
according to the model (2), with [2:0, 02 = 10, p=0. 1, aj :1 for
1§j§20and aj=0forj>20.

We apply three methods to the 100 simulated datasets: single SNP
analysis (SSA), standard Bayesian lasso (B-lasso) and the Bayesian
lasso applied to the preconditioned response from supervised
principal components (PB-lasso). In single SNP analysis, we reject
the null hypothesis that the genetic effect of an individual SNP equals
to zero at the signiﬁcance level of 5% with the FDR adjustment. For
the Bayesian lasso and preconditioned Bayesian lasso, we reject
the null hypothesis based on 95% Bayesian credible intervals. To

2.5 .

 

Heritability
3

d
i

9
m

 

 

 

ﬂ.l I i iIilii i i.l I

0 200 400 600 000 1000
SNP

Fig. 4. Estimated heritability explained by each SNP based on 50
simulations.

ameliorate the bias of the parameter estimates introduced by lasso
penalties, we always reﬁt the linear regression model without the
penalty term using only those SNP5 selected by the model selection
procedure.

For each estimated genetic effect obtained from each method, we
calculate the average bias and empirical standard error over 100
simulations. Since the ﬁrst 20 genetic effects are non-zeros with the
same true value, in Table 3 we report the average values over the
ﬁrst 20 SNP5 and over the rest of the SNP5 separately. The standard
error of each average is in parentheses. In the column labeled ‘Aver.
Nonzeros’, we present the average number of non-zero coefﬁcients
correctly identiﬁed to be non-zero, or the average number of zero
coefﬁcients incorrectly estimated to be non-zero in 100 replications.
In the column ‘Proportion of Correct-ﬁt’, we present the proportion
of replications that the exact true model was identiﬁed.

As can be seen from Table 3, the single SNP analysis tend to
overestimate the genetic effect, since when we test a SNP for the
association with the phenotype, we assume the genetic variation
is solely due to this particular SNP, and ignore the effects from
all other SNP5. Therefore, in terms of parameter estimates, model
selection methods that simultaneously estimate the genetic effects
associated with all SNP5 outperform the traditional single SNP
analysis. In terms of variable selection, although preconditioned
Bayesian lasso has a slightly higher false positive rate due to
the preconditioning step, it greatly improves the probability of
correctly identifying regression coefﬁcients with non-zero effects.
Moreover, as the number of SNP5 gets larger, single SNP analysis
detected fewer important SNP5, since this method subjects to
severe multiple comparison adjustment. However, preconditioned
Bayesian lasso is still able to identify non-zero coefﬁcients and zero
coefﬁcients correctly in almost every simulation. Supplementary
Table 1 displays the simulation results when p=0.5, which are
consistent with our ﬁndings.

Since our method is from the Bayesian perspective and is based
on the Gibbs sampler, the computational time is relatively high.
For example, for each replicate in this simulation study, averagely
it takes 439 s when p=1000 and 109 seconds when p=500 on a
2.0 GHz PC.

 

521

112 /3.IO'S[1211anprOJXO'SOIJBLUJOJIIIOIq”Idllq uIOJJ p9p201umoq

910E ‘OE JSHBHV uo ::

J.Li et al.

 

Table 3. Simulation results for three methods based on 100 simulations

 

Method Bias Empirical Aver. Proportion of

Standard Error non-zeros correct-ﬁt

 

n=200,p=500./31* 1320

SSA 4.17 1.99 16.62 0.08
(0.21) (0.19) (1.51)
B-lasso 0.07 0.34 18.28 0.18
(0.04) (0.02) (1.36)
PB-lasso 0.00 0.35 19.68 0.63
(0.03) (0.03) (0.65)
n: 200~l7=500» 1321 f 5500
SSA 0.44 0.43 0.78 0.46
(0.05) (0.07) (1.09)
B-lasso 0.00 0.04 0.95 0.42
(0.01) (0.01) (0.94)
PB-lasso 0.00 0.03 1.25 0.30
(0.01) (0.04) (0.73)
n=200,p=1000. [317 1320
SSA 4.13 1.96 15.71 0.04
(0.18) (0.17) (2.73)
B-lasso 0.36 0.38 17.11 0.08
(0.06) (0.07) (2.69)
PB-lasso 0.00 0.36 19.24 0.51
(0.04) (0.03) (1.81)
n=200,p=1000. ,3217 ,31000
SSA 0.43 0.43 0.42 0.69
(0.04) (0.06) (0.84)
B-lasso 0.00 0.02 0.33 0.76
(0.01) (0.01) (0.18)
PB-lasso 0.00 0.02 1.17 0.56
(0.00) (0.01) (1.38)

 

5 DISCUSSION

When the number of predictors p is much larger than the number of
observations 11, highly regularized approaches, such as penalized
regression models, are needed to identify non-zero coefﬁcients,
enhance model predictability and avoid over-ﬁtting (Hastie et al.,
2009). The L1 penalized regression or lasso is such one of the
most popular techniques. In this article, we presented a Bayesian
hierarchical model with lasso penalties to simultaneously ﬁt and
estimate all possible genetic effects associated with all SNPs in
a GWAS, adjusting for both discrete and continuous covariates.
Lasso penalties are imposed on the additive and dominant effects,
and implemented by assigning double-exponential priors to their
regression coefﬁcients. It shrinks small effects toward zero and
produces sparse solutions. In this framework, SNPs with signiﬁcant
genetic effects can be identiﬁed more accurately.

We ﬁt the model in a fully Bayesian approach, employing
the MCMC algorithm to generate posterior samples from the
joint posterior distribution, which can be used to make various
posterior inferences. Although computationally intensive, it is easy
to implement and provides not only point estimates but also
interval estimates of all parameters. The Bayesian lasso treats
tuning parameters as unknown hyperparameters and generates their
posterior samples when estimating other parameters. This technique

avoids the Choice of tuning parameters, and automatically accounts
for the uncertainty in its selection that affects the estimation of
the ﬁnal model. In contrast, standard lasso algorithms usually
select tuning parameters by K -fold cross-validation, which involves
partitioning the whole dataset and reﬁtting the model many times.
This process may result in unstable tuning parameter estimates.

In order to improve the performance of lasso when p is greater
than n, preconditioning is considered before variable selection.
Preconditioning encourages the principal components of a reduced
design matrix to be highly correlated with the response, and thus in
most cases only the ﬁrst or ﬁrst few components tend to be useful for
prediction. It denoises the response variable so that variable selection
becomes more efﬁcient. Our simulation demonstrated that when p
greatly exceeds n, preconditioned Bayesian lasso could successfully
identify almost all the SNPs with true genetic effects. By analyzing
real data, our approach is shown to produce biologically relevant
results. For example, the approach detected a signiﬁcant SNP
ss66171460 at position 225 80931 of Chromosome 20 associated with
BMI. It is interesting to note that this SNP is within 500 kb of the
FOXA2 (Forkhead Box A2) gene, an important genetic variant that
regulates obesity (Wolfrum et al., 2003).

One simulation example of Paul et al. (2008) implies that, in the
context of GWASs, SNPs that are marginally independent of the
phenotype could be screened out by preconditioning, but can be
identiﬁed by standard variable selection techniques such as lasso or
Bayesian lasso. In theory, if SNPs are correlated with the phenotype
through marginal correlations, we believe the preconditioning step
is worthwhile to identify more important SNP5. However, in reality,
since different SNPs may display interactions, this approach may
not work perfectly. In any case, this two-step variable selection
procedure should always be advantageous over a single SNP
analysis, because we are always testing the marginal correlation
between the predictor and response when one SNP is analyzed at a
time.

Motivated by Tibshirani (1996), Park and Casella (2008)
developed the Bayesian lasso and demonstrated the diabetes data
(Efron et al., 2004) with p = 10 and n = 442. We applied the Bayesian
lasso to the high-dimensional regression problem, and improved it
by preconditioning. We have concentrated on the preconditioned
Bayesian lasso method for continuous trait in GWASs. The proposed
preconditioning procedure and MCMC algorithm can be readily
extended to survival data analysis and lasso penalized logistic
regression in case—control disease gene mapping. Also, we may look
for gene—gene interaction effects after identifying main effects, as
suggested by Wu et al. (2009). The model with a capacity to identify
epistatic interactions will enables geneticists to decipher a detailed
picture of the genetic architecture of a complex trait.

Funding: NSF/NIH Mathematical Biology grant (No. 0540745);
NIDA; NIH grants (R21 DA024260 and R21 DA024266). The
content is solely the responsibility of the authors and does not
necessarily represent the ofﬁcial views of the NIDA or the NIH.

Conﬂict oflnterest: none declared.

REFERENCES

Andrews,D.F. et al. (1974) Scale mixture of normal distributions. J. R. Stat. Soc. Sen
B, 36, 997102.

 

522

112 /3.IO'S[1211anprOJXO'SOIJBLUJOJIIIOIq”Idllq uIOJJ p9p201umoq

9103 ‘0g15n8nv uo ::

Lasso for GWAS

 

Dawber,T. et al. (1951) Epidemiological approaches to heart disease: the Framingham
study. Ame. J. Public Health, 41, 2797286.

Donnelly,P. (2008) Progress and challenges in genome-wide association studies in
humans. Nature, 465, 7287731.

Efron,B. et al. (2004) Least angle regression (with discussion). Annn. Stat. , 32, 407499.

Fan,J. and Li,R. (2001) Variable selection via nonconcave penalized likelihood and its
oracle properties. J. Am. Stat. Assoc, 96, 134871360.

Fan,J. and Lv,J. (2008) Sure independence screening for ultrahigh dimensional feature
space (with discussion). J. R. Stat. Soc. Ser B, 70, 8497911.

Frank,I.E. and Friedman,J.H. (1993) Astatistical view of some chemometrics regression
tools. Technometrics, 35, 1097148.

Gelman,A. and Rubin,D.B. (1992) Inference from iterative simulation using multiple
sequences. Stat. Sci., 7, 4577511.

Hastie,T. et al. (2009) High-dimensional problems: p >N. The Elements of Statistical
Learning, 2nd edn. Springer, New York.

Hoggart,C.et al. (2008) Simultaneous analysis of all SNPs in genome-wide and re-
sequencing association studies. PLaS Genet, 4, e1000130.

Hoti,F. and Sillanpaa,M.J. (2006) Bayesian mapping of genotype >< expression
interactions in quantitative and qualitative traits. Heredity, 97, 4718.

Jaquish,C. (2007) The Framingham heart study, on its way to becoming the gold standard
for cardiovascular genetic epidemiology? BMC Med. Genet, 8, 63.

Logsdon,B.A. et al. (2010) A variational Bayes algorithm for fast and accurate multiple
locus genome-wide association analysis. BMC Biainfarmatics, 27, 11758.

McCarthy,M. et al. (2008) Genome-wide association studies for complex traits:
consensus, uncertainty and challenges. Nat. Rev. Genet, 9, 35G369.

Park,T. and Casella,G. (2008) The Bayesian lasso. J. Am. Stat. Assoc, 103, 681$86.

Paul,D. et al. (2008) Preconditioning for feature selection and regression in high-
dimensional problems. Annn. Stat, 36, 159571618.

Tibshirani,R. (1996) Regression shrinkage and selction via the lasso. J. R. Stat. Sac.
Ser B, 58, 2677288.

Wolfrum,C. et al. (2003) Role of Foxa-Z in adipocyte metabolism and differentiation.
J. Clin. Invest, 112, 3457356.

Wu,T.T. et al. (2009) Genome-wide association analysis by lasso penalized logistic
regression. Biainfarmatics, 25, 71L721.

Yang]. et al. (2010) Common SNPs explain a large proportion of the heritability for
human height. Nat. Rev. Genet, 42, 5657569.

Yi,N. and Xu,S. (2008) Bayesian lasso for quantitative trait loci mapping. Genetics,
179, 104571055.

Zou,H. and Hastie,T. (2005) Regularization and variable selection via the elastic net.
J. R. Stat. Soc. Ser B, 67, 3017320.

 

523

112 /3.IO'S[1211anprOJXO'SOIJBLUJOJIIIOlq”K1111] mot} papeo1umoq

9103 ‘0g15n8nv uo ::

