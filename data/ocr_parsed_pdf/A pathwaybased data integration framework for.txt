BIOINFORMA TICS

  

Gene Express ion
Dala

    
 
     
 
 

Copy Number
Varialion Dala

   

Decision
Funclion

 
   
 

ER-slalus

  
 
  

Olher Clinical
lnformalion

  

/3.10'S[Bum0[p10}x0'SOpBLUJOJuyqu”:duq

A pathway-based data integration framework

 

these methods to the METABRIC data for breast cancer (Curtis
et al., 2012) with an application to prediction of mortality risk.
Of course, there have been a number of other studies predicting
breast cancer outcome using clinical data (e.g. Wishart et al.,
2010) or EXP data (e.g. Buyse et al., 2006) alone but few that
combine and weight the signiﬁcance of these different prognostic
indicators.

There are alternative methods for supervised learning using
multiple types of data, and we will pursue a comparison of our
method against these alternatives in Section 3.2.3. We consider
methods proposed by Chen and Zhang (2013), Chen et al. (2009),
Le Cao et al. (2010) and Witten and Tibshirani (2009).
Furthermore, in a recent competition by Sage Bionetworks (the
DREAM Breast Cancer Prognosis challenge), EXP, CNV and
clinical data from the METABRIC dataset were made available
for evaluating the performance of different approaches for
predicting breast cancer survival. The model that gave the best
results (Chen et al., 2013) was an ensemble of different methods,
including Cox regression based on the Akaike Information
Criterion, a Generalized Boosting Model and k—nearest
neighbors. This model included prior knowledge based on the
selection of groups of genes. In Bilal et al. (2013) the authors
analyzed several models submitted to this competition, including
Random Forest, Lasso—based regression models, Elastic Nets and
boosting and ensemble models, and thus we compare with these.

1.1 Multiple kernel learning

Kernel—based learning machines (Scholkopf and Smola, 2002;
Shawe—Taylor and Cristianini, 2004), such as Support Vector
machines (SVMs), are a well—studied class of methods for classi—
ﬁcation problems. For binary classiﬁcation with two well—
separated classes of data (Fig. 2), the learning task amounts to
ﬁnding a directed hyperplane, that is, an oriented hyperplane such
that datapoints on one side will be labeled y,~ = +1 and those on
the other side as y,~ = —1. The directed hyperplane found by an
SVM is intuitive. It is that hyperplane, which is maximally dis—
tant from the two classes of labeled points located on each side.
The closest such points on both sides have most inﬂuence on
the position of this separating hyperplane and are the support
vectors. The distance between these support vectors and the
separating hyperplane is the margin. The separating hyperplane
is given as w - x + b = 0 where b is the bias and w, the weights
(- denotes the scalar product). With datapoints x,~ (i = 1, . . . , m)

 

Fig. 2. The argument inside the decision function of a classiﬁer is
w - x + b. The separating hyperplane corresponding to w - x + b = 0 is
shown as a line in this 2D plot. This hyperplane separates the two classes
of data with points on one side labeled y,» = +1 (w - x + b 2 0) and points
on the other side labeled y,» = —1 (w - x + b<0)

having corresponding labels y,~ = :|:1, the decision function is
therefore f(x,~) = sign(w - x,~ + b). Therefore, datapoints are cor—
rectly classiﬁed if y,~(w - x,~ + b)>0 Vi. The decision function f(x)
is invariant under a positive rescaling of the argument inside the
sign—function. This leads to an ambiguity in deﬁning the margin.
Hence we implicitly fix a scale for (w, b) by setting w - x + b = 1
for the closest points on one side and w - x+b = —1 for the
closest on the other side. Let x1 and x2 be two support vectors
on both sides (Fig. 2). Ifw - x1 +b 2 1 and w - X2 +b 2 —1, we
deduce that w-(xl —xz) = 2. For the separating hyperplane
w - x+ b = 0, the normal vector is w/||w||2 (where ||w||2 is
the square root of w - w). Thus the margin is half the projection
of the vector x1 — x2 onto the normal vector w/ ||w||2 that
gives (x1 —x2)-w/||w||2 = 2/||w||2. Therefore, the margin is
y = 1/||w||2. Maximizing the margin is therefore equivalent to
minimizing:
1
5 llwlli (1)
subject to the constraints:
y,~(w - x,~ + b) 3 1 Vi (2)

As a constrained optimization problem, the above formulation
can be reduced to minimization of a Lagrange function, consist—
ing of the sum of the objective function and the m constraints
multiplied by their respective Lagrange multipliers, on (that satisfy
on 3 0). This is the primal formulation of an SVM:

L(w, b) = %(w - w) — fang/{w - x,~ + b) — l) (3)
i=1

At the minimum, we can take the derivatives of L(w, b) with
respect to b and w and set these to zero. This gives the conditions
2:1 any, 2 0 and w =  ot,y,~x,~. Substituting w back into
L(w, b) we get the dual formulation:

W(ot) = 2m —%
i=1

which must be maximized with respect to the on subject to the
constraints:

m
“WWW/(Xi ‘ X1) (4)
i,j=l

Otf 2 0 Zaiyi = 0 
[:1

The advantage of the learning task in (4, 5) is that it is con—
strained quadratic programming from optimization theory, and
hence it is a concave problem with a unique solution.

Having found those of, which optimize (4), the predicted label
for a new datapoint z is given by the sign of

an = Zaryrxi ~ z) + b (6)
i=1

where b is the bias:

1 m
b=__ fli.l
2 [{ilﬁlgxn  “by/(X x,))
+ {H.321}  wry/(Xi - x,))]

Many datasets are not linearly separable. An appealing prop—
erty of kernel—based methods, such as SVMs, is that we can map

(7)

 

839

ﬁm'spzumofpmjxo'sopnuuopuorq/ﬁdnq

J.A.Seoane et al.

 

input data into a higher dimensional space, called feature space,
where the datapoints are linearly separable. With a mapping
x,~ —> <I>(x,~) to feature space, from (4) we see that datapoints
are represented by a mapped dot product in this higher dimen—
sional space i.e. by Ki, 2 <I>(x,~) - <I>(xj). Ki,- is called the kernel
matrix, and we can construct kernels for discrete and continu—
ously valued data and other data objects such as graphs and text
strings. A particular choice of kernel amounts to an implicit
choice of mapping function though, in practice, we do not
need to know the form of this mapping function.

We can therefore construct classiﬁers with a decision function
dependent on a variety of different types of input data. With
different types of data encoded in different kernels, this approach
is called MKL (reviewed in Campbell and Ying, 2011; Gonen
and Alpaydin, 2011). A common approach to MKL is to con—
struct a composite kernel as a linear combination of base kernels:

[7
Ki; = 2 AM?) (8)
(:1

where  is the base kernel derived from each type of data Z and
there are assumed p such types of data. The kernel coeﬂicients,
A“), are subject to the constraints:

P
W 3 0 EN) = 1 (9)
1::1

and so the objective function to optimize in on and A“) is given by

m 1 m [7
Wm“) = 2m — 5 aim/yiy;[ZWK§f1] (10>
i=1 i,j=l «:1
which we optimize via
min max W(Ot, AW) (11)
w 06

subject to the constraints (5) and (9). This is a linear program—
ming problem in A“) and a quadratic programming problem in on
and could thus be approached as a quadratically constrained
linear programming problem (Bach et al., 2004), for example.

The kernel coefficients A“) weight the signiﬁcance of particular
kernels and are therefore a measure of the relative importance of
different types of data in the ﬁnal decision function. The different
types of data that are input to this decision function will likely
have different intrinsic scales. Thus, to account for this variabil—
ity across datasets, all base kernels are normalized to unit trace
norm in the experiments discussed later in the text.

2 METHODS

After MKL is complete, the kernel coefﬁcients, A“), indicate the relevance
of different types of data. Thus, if A“) is zero then data type (3 is not
relevant or the information it contains may be implicit in another type of
data. Thus MKL can indicate that acquisition of certain types of data
may not be necessary. For a dataset such as METABRIC (Curtis et al.,
2012), the component types of data can have a large number of features
and the large majority of these features are likely to be irrelevant to
prediction of mortality risk. If irrelevant data substantially outweighs
relevant data then we must consider feature selection strategies. In the
context of multiple types of data, this feature selection would need to be
performed differently per type of data.

2.1 Feature selection

In this article, we start by considering feature selection in the context of
MKL. We will use a large number of kernels, with variable numbers of
features per kernel. Thus the algorithm ﬁnds which kernels, and hence
which features per data type, are most relevant for the given classiﬁcation
problem. The feature set per kernel can be chosen through statistical scor-
ing (e.g. by ranking those features most statistically aligned with the class
labels) or by biological insight (e. g. by selection of a set of genes known to
belong to a speciﬁc pathway). To implement this approach we would need
to select an MKL method that typically gives a sparse combination of
kernel coefﬁcients A“). We have selected the SimpleMKL method of
Rakotomamonjy et al. (2008) because of its observed sparse solution in
our previous studies (Damoulas et al., 2008; Ying et al., 2009a, b) and has
proven efﬁciency when the number of kernels is high (Kloft et al., 2011).

SimpleMKL performs an optimization over both the parameters of the
SVM (01,») and the kernel coefﬁents (Am) via an iterative gradient descent
method. This approach is efﬁcient for high dimensional datasets, as
memory consumption remains stable during minimization, in contrast
to other implementations based on quadratically constrained quadratic
programming (Bach et al., 2004) or semi-inﬁnite linear programming
(Lanckriet et al., 2004). Importantly, this particular MKL implementa-
tion uses a two-norm regularization leading to a sparse solution in the
kernel coefﬁcients.

During construction of the base kernels, Kg), features were grouped
into sets. The features in a speciﬁc set can be grouped by statistical sig-
nificance. We used the MATLAB bioinformatics toolbox ran/(feature
function for this purpose. When grouping features by statistical score,
we found best results could be achieved using the t-test measure, using the
class labels of the training set. Once the features in each set are ranked by
statistical signiﬁcance, an individual base kernel was constructed for each
set of the ﬁrst 2, 3 and up to N features.

To give further ﬂexibility in terms of the kernel function, for each
individual set of features, we used several different types of kernel
matrix. We used a linear kernel, as some of the data types had many
features (e.g. the EXP and copy number variability data) and so we are
considering a sparse set of datapoints in a high dimensional space. Thus it
is reasonable to assume datapoints from each class belong to linearly
separable sets and therefore a linear kernel is sufficient. We further
used polynomial base kernels with 2 and 3 degrees of freedom and
non-linear Gaussian kernels. Given our remarks about the separability
of the data, we found the method gave a value of zero for the kernel
coefﬁcients for the Gaussian kernels. Thus the decision functions were
only dependent on linear and polynomial kernels in our experiments in
Section 3.2.

As a means for incorporating further biological information, we
derived additional base kernels each with a feature set based solely on
genes known to be members of a speciﬁc pathway. The pathway infor-
mation was derived from KEGG (Kyoto Encyclopedia of Genes and
Genomes. http://www.genome.jp/kegg/). We discuss these in more
detail in Section 3.3.

The algorithm therefore has signiﬁcant ﬂexibility over the set of base
kernels used in constructing the most appropriate decision function. Once
the algorithm reached an optimum for the objective function, the large
majority of the kernel coefﬁcients A“) had a value of 0.0 in subsequent
experiments and thus the corresponding kernels do not contribute to the
decision function. Non-zero coefﬁcients indicate the informative kernels.
In the experimental Section 3.2, we consider the performance of the clas-
siﬁer based solely on single data types and multiple data types in addition
to performance with and without feature selection.

2.2 Introduction of a conﬁdence measure

For many medical prediction problems, it would be useful to have a
conﬁdence measure associated with a predicted label. For classiﬁcation

 

840

ﬁre'spzumofpmjxo'sopnuuopuorq/ﬁdnq

an?kgogmomammowoxmommsocgawbmﬁ

 

an?kgogmomammowoxmommsocgawbmﬁ

llm
égne Exp + ch

Gene exp . ch
+ EH

Gene exp e ch

Input aaia

g
m
m
m
y
a
m
m
p

Baseline

 

an?kgogmomammowoxmommsocgawbmﬁ

 

J.A.Seoane et al.

 

includes other pathways in addition to KEGG pathways. For
this reason, a direct comparison is not possible, but we can con—
sider if pathways present in our analysis are also covered in the
pathway analysis of Curtis et al. Using all the clinical data,
the EXP and CNV data types, and with survival as outcome,
our MKL method used 81 pathway—based kernels, of which 27
are enriched in the Int2 cluster and 21 in the Int5 cluster. If we
restrict to EXP, CNV and ER—status only for input data, our
MKL method used 83 pathways, of which 28 are present in the
Int2 cluster and 23 in the Int5. In the Supplementary Material,
we give a complete list of the pathways used, associated scores
and scores of Curtis et al. (these scores are not comparable).

With our MKL analysis, one of the highest signiﬁcance path—
ways is RNA transport, which has been previously reported as a
key pathway in the recurrence of nonismall—cell lung cancer (Lu
et al., 2012). Other significant pathways were cell adhesion mol—
ecules, endocytosis, the insulin signaling pathway and the mTOR
signaling pathway. The arachidonic acid metabolism pathway
(Iwamoto et al., 2011; Nassar et al., 2007) and N—glycan biosyn—
thesis (Dennis et al., 1999) both feature and have reported asso—
ciations with breast cancer development, as does SOCS
[Suppression of cytokine signalling pathway (Sasi et al., 2010)],
which is a negative regulator of the JAK—STAT signalling path—
way and it is associated with improved clinical outcome in breast
cancer. As also reported by Curtis et al., 2012, the Systemic
Lupus Erythematosus pathway featured and, with an association
to ER—status, also has an association with survival—status.

4 DISCUSSION

We now discuss some broad conclusions that can be drawn from
this study, various ways in which classiﬁer test accuracy can be
further improved and other contexts in which we could apply the
method outlined.

Two key conclusions coming from our study are the import—
ance of incorporating prior knowledge and performing feature
selection. Prior knowledge was represented by pathway informa—
tion. Where appropriate, data were grouped into clusters repre—
senting their particular pathway membership. Using feature
selection within each such cluster, we use the most representative
features within each pathway. Because of the sparse nature of
this particular MKL implementation, we can select a set of path—
ways that are most relevant to survival prediction.

As expected, classiﬁers that can use all the available data are
more powerful that those that use only one type of data. MKL
methods also have the advantage that they weight the contribu—
tion of individual data types, and thus indicate their relative
signiﬁcance. Our study highlighted the importance of using all
available clinical data alongside expression array and CNV data.
In addition, expression and CNV data were best incorporated
into the classiﬁer by using pathway—based kernels. Further im—
provements came from using a cautious classifier that only makes
predictions on a restricted class of high conﬁdence cases and by
removal of ambiguously labeled samples from the training data.
These last improvements highlight the importance of using a
confidence measure associated with the label assignment and
motivate further work on devising robust probabilistic classiﬁers
for use with MKL.

Using these various measures, predictive performance moved
from ~64% for prediction with EXP array data alone, to almost
80%, with the qualiﬁcation that prediction is made on 69.2% of
individuals. However, it is reasonable to expect that this test
accuracy can be improved beyond 80% through the use of add—
itional types of data and further refinement of the method.
MicroRNA array data, methylation data and condensed infor—
mation from images of tumor biopsies are complimentary types
of data, which could provide additional base kernels, in addition
to string kernels (Campbell and Ying, 2011; Shawe—Taylor and
Cristianini, 2004), incorporating sequence data. Furthermore,
expression by certain individual genes [e. g. p27 (Alkarain et al.,
2004)] or small sets of genes [e. g. associated with T P53, (Jamshidi
et al., 2013)] has documented correlation with survival status,
and these genes could be given extra weight by assigning them
individual base kernels.

For the methodology, there are some further directions that
could be considered. Rather than using KEGG pathway data,
we could investigate other approaches to feature selection, such
as ﬁltering features based on Gene Ontology labels. The kernel
coefﬁcients would then indicate which Gene Ontology labels
are most relevant to predicting survival outcome. If one of
the two classes is viewed as more clinically important than the
other, then we could use an asymmetric soft margin (Veropoulos
et al., 1999) during SVM training: this improves test accuracy
on one class, at the expense of accuracy on the other.
The SimpleMKL method used in this article has associated
publically available software (http://asi.insa—rouen.fr/enseig
nants/arakoto/code/mklindex.html); it is an effective and repre—
sentative MKL algorithm and it gives a sparse representation
over the set of base kernels. However, a large number of MKL
methods have been proposed (Gonen and Alpaydin, 2011) and
some methods with a less sparse solution may give higher accur—
acy (Kloft et al., 2011). In short, additional data, further refine—
ment of the method and the use of a cautious classiﬁer could lead
to a test performance nearer 90%. This performance, though,
would be achieved at the cost of a wide range of genomic and
clinical measurements and does not result in prediction with all
patients.

Nomograms and simple clinical measures such as ER—status
are reliable indicators of disease progression for breast cancer.
A predictive method, such as that described, would need to be
competitive against these. However, it is in other contexts that
similar studies could be effective. Thus, for prostate cancer, there
is a well—recognized problem distinguishing aggressive from low—
risk cancer. In the US, ~20% of men will be diagnosed with
prostate cancer, whereas only 3% would die from the disease
(Altekruse et al., 2010). With limited ability to predict risk,
many tumors are unnecessarily labeled as high risk and treated
aggressively. It would be interesting to see if the test accuracies
stated in this article can be achieved with prostate cancer and
other cancers. This would require similar large datasets with a
broad range of genomic and clinical measurements. To get
good predictive performance, the dataset would need to contain
a sufﬁcient number of aggressive disease examples and not
just represent the spectrum of disease observed in the general
populationiwhich is numerically weighted toward low—risk
disease.

 

ﬁre'spzumofpmjxo'sopnuuopuorq/ﬁdnq

A pathway-based data integration framework

 

ACKNOWLEDGEMENTS

This work was carried out using the computational facilities
of the Advanced Computing Research Centre, University of
Bristol — http://www.bris.ac.uk/acrc/.

Funding: UK Medical Research Council (G1000427). This study
made use of data generated by the Molecular Taxonomy of
Breast Cancer International Consortium. Cancer Research UK
and the British Columbia Cancer Agency Branch.

Conflict of Interest: none declared.

REFERENCES

Agius,P. et al. (2009) Bayesian unsupervised learning with multiple data types. Stat.
Appl. Genet. Mol Biol, 8, Article 27.

Alkarain,A. et al. (2004) p27 deregulation in breast cancer: prognostic signiﬁcance
and implications for therapy. J. Mammary Gland Biol. Neoplasia, 9, 67780.
Altekruse,S.F. et al. (2010) SEER Cancer Statistics Review, 1975—2007. National

Cancer Insitute, Bethesda, MD.

Archambeau,C. and Bach,F. (2011) Multiple gaussian process models. In: NIPS 23
workshop on New Directions in Multiple Kernel Learning, arXiV: 110.5238.
Bach,F. et al. (2004) Multiple kernel learning, conic duality and the SMO algorithm.
In: Proceedings of the Twenty—first International Conference on Machine Learning

(ICML 2004). ACM, New York, NY, USA, p. 6.

Bilal,E. et al. (2013) Improving breast cancer survival analysis through competition—
based multidimensional modeling. PLoS Comput. Biol, 9, e1003047.

Breiman,L. (2001) Random forests. Mach. Learn., 45, 5732.

Buyse,M. et al. (2006) Validation and clinical utility of a 70—gene prognostic signa—
ture for women with node—negative breast cancer. J. Natl Cancer Inst., 98,
118371192.

Campbell,C. and Ying,Y. (2011) Learning with Support Vector Machines. Morgan
and Claypool, San Rafael, Californa, USA.

Caruana,R. et al. (2004) Ensemble selection from libraries of models. In:
Proceedings of the 21st International Conference on Machine Learning. ACM,
New York, NY, USA, p. 18.

Chen,B.J. et al. (2009) Harnessing gene expression to identify the genetic basis of
drug resistance. Mol. Syst. Biol, 5.

Chen,Z. and Zhang,W. (2013) Integrative analysis using module—guided random
forests reveals correlated genetic factors related to mouse weight. PLoS
Comput. Biol, 9, e1002956.

Cheng,W.Y. et al. (2013) Development of a prognostic model for breast cancer
survival in an open challenge environment. Sci. Trans]. Med., 5,
181ra5(%181ra50.

Curtis,C. et al. (2012) The genomic and transcriptomic architecture of 2,000 breast
tumours reveals novel subgroups. Nature, 486, 34(r352.

Damoulas,T. et al. (2008) Inferring sparse kernel combinations and relevance vec—
tors: an application to subcellular localization of proteins. In: Proceedings of the
Seventh International Conference on Machine Learning and Applications
(ICML'08). IEEE Computer Society, San Diego, CA.

Dennis,J.W. et al. (1999) Glycoprotein glycosylation and cancer progression.
Biochim. Biophys. Acta, 1473, 21734.

Friedman,J.H. (2001) Greedy function approximation: a gradient boosting
machine. Ann. Stat., 29, 118971232.

Gonen,M. (2012) Bayesian efﬁcient multiple kernel learning. In: Proceedings of
the 29th International Conference on Machine Learning (ICML 2012 ).
Edinburgh, Scotland.

Gonen,M. and Alpaydin,E. (2011) Multiple kernel learning algorithms. J. Mach.
Learn. Res., 12, 221172268.

Huopaniemi,I. et al. (2010) Multivariate multi—way analysis of multi—source data.
Bioinformatics, 26, i3917i398.

Iwamoto,T. et al. (2011) Gene pathways associated with prognosis and chemother—
apy sensitivity in molecular subtypes of breast cancer. J. Natl Cancer Inst., 103,
26¢272.

Jamshidi,M. et al. (2013) Germline variation in TP53 regulatory network genes
associates with breast cancer survival and treatment outcome. Int. J. Cancer,
132, 204472055.

Kloft,M. et al. (2011) Lp—norm multiple kernel learning. J. Mach. Learn. Res., 12,
9537997.

Lanckriet,G.R.G. et al. (2004) A statistical framework for genomic data fusion.
J. Mach. Learn. Res., 5, 27772.

Le Cao,K.—A. et al. (2010) Integrative mixture of experts to combine clinical factors
and gene markers. Bioinformatics, 26, 119271198.

Lu,Y. et al. (2012) Gene—expression signature predicts postoperative recurrence in
smge I non—small cell lung cancer patients. PLoS One, 7, e30880.

Nassar,A. et al. (2007) COX—2 expression in invasive breast cancer: correlation with
prognostic parameters and outcome. Appl. Immunohistochem. Mol. Morphol,
15, 2557259.

Platt,J. (1999) Probabilistic outputs for support vector machines and comparison to
regularised likelihood methods. In: Advances in Large Margin Classifiers. MIT
Press, pp. 61774.

Rakotomamonjy,A. et al. (2008) SimpleMKL. J. Mach. Learn. Res., 9, 249172521.

Rogers,S. et al. (2010) Inﬁnite factorization of multiple non—parametric views.
Mach. Learn., 79, 2017226.

Sasi,W. et al. (2010) Higher expression levels of SOCS 1,3,4,7 are associated with
earlier tumour stage and better clinical outcome in human breast cancer. BMC
Cancer, 10, 178.

Savage,R. et al. (2010) Discovering transcriptional modules by Bayesian data inte—
gration. Bioinformatics, 26, i1587i167.

Scholkopf,B. and Smola,A.J. (2002) Learning with Kernels. MIT Press.

Shawe—Taylor,J. and Cristianini,N. (2004) Kernel Methods for Pattern Analysis.
Cambridge University Press, Cambridge, UK.

Veropoulos,K. et al. (1999) Controlling the sensitivity of support vector machines.
In: Proceedings of the International Joint Conference on Artificial Intelligence,
IJCA199. Stockholm, Sweden, pp. 5&60.

Wishart,G.C. et al. (2010) PREDICT: a new UK prognostic model that predicts
survival following surgery for invasive breast cancer. Breast Cancer Res., 12, R1.

Witten,D.M. and Tibshirani,R.J. (2009) Extensions of sparse canonical correlation
analysis with applications to genomic data. Stat. Appl. Genet. Mol. Biol, 8,
1727.

Ying,Y. et al. (2009a) Enhanced protein fold recognition through a novel data
integration approach. BMC Bioinformatics, 10, 267.

Ying,Y. et al. (2009b) Class prediction from disparate biological data sources using
an iterative multi—kernel algorithm. Lect. Notes Bioinform., 5780, 4274138.
Yuan,Y. et al. (2011) Patient—speciﬁc data fusion deﬁnes prognostic cancer subtypes.

PLoS Comput. Biol, 7, e1002227.

 

ﬁre'spzumofpmﬂo'sopnuuopuorq/pdnq

