Bioinformatics, 3216), 2016, 918—925

doi: 10.1093/bioinformatics/btv644

Advance Access Publication Date: 10 November 2015
Original Paper

 

Databases and ontologies

Flexible data integration and curation using a
graph-based approach

Samuel Croset*, Joachim Rupp and Martin Romacker

Roche Innovation Center Basel, F. Hoffmann-La Roche AG, [EH-4070 Basel, Switzerland

*To whom correspondence should be addressed.
Associate Editor: John Hancock

Received on June 5, 2015; revised on October 20, 2015; accepted on October 21, 2015

Abstract

Motivation: The increasing diversity of data available to the biomedical scientist holds promise for
better understanding of diseases and discovery of new treatments for patients. In order to provide a
complete picture of a biomedical question, data from many different origins needs to be combined
into a unified representation. During this data integration process, inevitable errors and ambiguities
present in the initial sources compromise the quality ofthe resulting data warehouse, and greatly di—
minish the scientific value of the content. Expensive and time—consuming manual curation is then
required to improve the quality ofthe information. However, it becomes increasingly difficult to dedi—
cate and optimize the resources for data integration projects as available repositories are growing
both in size and in number everyday.

Results: We present a new generic methodology to identify problematic records, causing what we
describe as ’data hairball’ structures. The approach is graph—based and relies on two metrics trad—
itionally used in social sciences: the graph density and the betweenness centrality. We evaluate
and discuss these measures and show their relevance for flexible, optimized and automated
data curation and linkage. The methodology focuses on information coherence and correctness to
improve the scientific meaningfulness of data integration endeavors, such as knowledge bases
and large data warehouses.

Contact: samuel.croset@roche.com

Supplementary information: Supplementary data are available at Bioinformatics online.

 

 

1 Introduction

With the current quantity and diversity of data available in the bio-
medical domain (Lipinski et (11., 2014; Marx, 2013), it becomes in-
creasingly necessary to combine the information coming from
multiple sources, in a variety of formats, into a unified representation.
This practice goes by the name of data integration or record linkage
(Winkler, 1995). Different reasons motivate the exercise: it can be for
instance the appealing possibility to query and analyze information
about complementary thematics (e.g. gene—disease relationship), con-
solidation of some knowledge existing about one topic, or the absorp-
tion of a source into another for maintenance needs (e.g. dataset
coming from company acquisitions). Recently, data integration ef-
forts have been particularly active in the drug discovery domain, with

platforms such as transMART (Szalma et (11., 2010) or Open
PHACTS (Williams et (11., 2012a), focused on building a pharmaco-
logical space from public repositories. Other examples include
ChemSpider (Pence and Williams, 2010), a resource providing a cen-
tral hub related to chemical names and structures from various
sources or Identifiers.org (Juty et (11., 2012), a cross-reference platform
for biomedical identifiers and data connections.

Fundamentally, data integration can be seen as a problem
of creating the correct links between equivalent, yet disconnected,
database records. This process is sometimes called ‘stitching’, or ‘rec-
onciliation’ (Bollacker et (11., 2008; Dong et (11., 2014). Once records
are rightfully associated, it becomes possible to query across or
merge them if necessary. As links are created between entries, it is

(63 The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 918

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

Flexible data integration and curation

919

 

intuitive to rely on an abstract graph structure to represent the prob-
lem faced. Vertices or nodes are records or entities of interest; edges
represent the associations between them. An illustration of the graph
abstraction is the Semantic Web: this series of standards relies on the
Resource Description Framework (RDF) and a graph structure to fa-
cilitate the interoperability and integration of independent pieces of
information on the World Wide Web (Berners—Lee et 41., 2001). The
Semantic Web also provides means to establish equivalence between
segregated records (sameAs or exactMatch relations), with the use
of rules or reasoners for instance. Moreover, the biomedical domain
is rich in unique identifiers (e.g. chemical structure identifiers like
InChIKeys) and cross-references which can be used to automatically
assess equivalence between database entries: if one record references
another, it might be possible to deduce that the two entities are the
same, for instance.

All these strategies are automatable and reliable in theory, how-
ever complications can arise quickly if cross-references are absent,
errors present in the original sources or if the data is fuzzy and am-
biguous by nature, such as with drug names and chemical structures
for instance (Tiikkainen et al., 2013; Williams et al., 2012b). The ef-
fect of erroneous information is dramatic for data integration: records
can get incorrectly associated with other entries, themselves recur-
sively linked to other records. This cascade of events leads to the cre-
ation of unwanted ‘hairballs’, which we define as groups of records
not equivalents and not supposed to he linked, but yet connected he-
cause of the deficient state of the data. In such a scenario, manual
intervention from expert curators is required in order to improve the
quality and scientific validity of the dataset. Unfortunately, with the
increasing size of biomedical databases and repositories, it becomes
more likely that such errors will arise by chance, and more expensive
and time consuming to correct them by hand.

In this document, we use an in-house data integration project
related to the creation of a drug product terminology in order to il-
lustrate a generic and flexible approach to identify and handle prob-
lematic and erroneous records during the integration process. The
drug terminology is built from millions of database entries present
in eight heterogeneous sources, including four from third-party ven-
dors (Integrity, Cortellis, PharmaProject and AdisInsight), one de-
veloped internally and three public drug databases (DrugBank, part
of ChEMBL and ChEBI). Our motivation is to build and maintain
an integrated database (also called data warehouse) from these vari-
ous sources, each of which contains a part of the desired information
(see Fig. 1). The graph-based method presented allows researchers
to isolate and prioritize problematic entries and flexibly adjust the

A Diazepam . Lipitor ‘
. Valium dlazepam .
b diazepam
Ro-5-2807
B Diazepam C

. Valium

Ro-5-2807 diazepam

Fig. 1. The data integration challenge. (A) Records coming from multiple
sources, represented by different colours. All these records refer to the same
drug product, in this case diazepam, but contain only partial information
about it, the synonyms. (B) The entries have to be integrated and merged into
consolidated records. (C) In the process, erroneous entries have to be
excluded, to improve the quality and consistency of the resulting data
warehouse

curation work required over an automatic integration to maximize
the quality and scientific usefulness of the data.

1.1 Limitations of previous work

The methodology presented in this manuscript differentiates itself
from previous record linkage approaches by its flexibility to handle
many different sources and exclude erroneous records, as well as
with its capability to deal with redundant identifier types. We briefly
summarize in this section the state of the art and the current
limitations.

Record linkage depends on the presence of one or more common
identifiers in the available datasets in order to assert equality between
database entries (Hernandez and Stolfo, 1998, Winkler, 2014). It can
for instance be the name of a person, or in our case one of the various
synonyms used to describe drug products (e.g. regulatory name, brand
name). Previously published techniques can then be divided in two cat-
egories: deterministic and probabilistic approaches (Roos and Wajda,
1991). Deterministic methodologies rely on a series of custom rules to
create the equivalence between the records, using techniques close to
text-mining. Records are merged based on exact or similarity-based
matching of the common identifiers. Such an approach is straightfor-
ward and can deal with data coming from many sources, yet it is very
vulnerable to erroneous information (Wajda et 41., 1991).
Deterministic approaches do not provide means to detect and remove
mistakes in the original data and are therefore not suitable for our
needs: the presence of an incorrect drug product synonym would for in-
stance result in the mixing of two or more drug products in the same
category, situation to be avoided at all costs. Probabilistic-based
approaches try to link pairs of records by assigning weights to a range
of identifiers and by determining a likelihood for a match to be correct
(Fellegi and Sunter, 1969). These techniques have the clear limitation
of handling only pairs of records, whereas we needed n data sources to
be considered at once for the integration process. Moreover, these
methodologies usually assumed that identifiers are independents
(Wilson, 2011), which is not the case with drug names: a brand name
appears for instance late in the life cycle of a product, and is always
preceded by a regulatory name. Finally, none of the previous
approaches are designed to exclude erroneous records, they limit them-
selves to determine whether there is a match or non-match between re-
cords. This feature is critical for our use-case and for data quality.

To conclude, the approach we introduce is graph-based, i.e. re-
cords are abstracted as vertices, further linked by edges, and is there-
fore compatible with any graph representation such as RDF or
graph database for implementation. The identification of erroneous
records relies on two indicators from graph theory and traditionally
used in social science analysis: the graph density and the between-
ness centrality. We demonstrate and evaluate their relevance in
the data integration and cleaning tasks. The work presented finds its
application for the automatic combination of large amounts of ex-
pensive data coming from different sources, where curation needs to
be prioritized and optimized in order to generate high quality scien-
tific content. Future use cases also include the management of data-
sets resulting from mergers and acquisitions, in order to integrate
the new valuable information with existing internal databases and
reduce the amount of maintenance work necessary.

2 Methodology

A total of eight different sources were used to build the data ware-
house (i.e. the drug product terminology). The methodology for the
integration of the data is decomposed in three steps, respectively

[310'sp2umofp105xo'sopeuHOJIItotq/ﬁdnq

920

S.Croset et al.

 

called Link, Evaluate and Clean. Depending on the desired outcome,
it is also possible to add an extra final step called Merge in which re—
cords are merged together. Briefly, in the Link step the graph struc—
tures are formed by flexibly creating edges between the initial
entries from the sources. The presence of particular label types was
used to connect the records; for instance, if two records have a
brand name in common, an edge between the two entries was cre—
ated. This step results in the creation of many graphs, reflecting the
associations between records. In the Evaluate step, metrics are com—
puted over the graphs (i.e. betweenness centrality and density).
From these measures, it is possible to isolate the problematic graphs
and entries, which can be cleaned manually or automatically given a
certain threshold (Clean step). Each of these steps is described in de—
tail in this section.

2.1 Data sources and implementation

The creation of a comprehensive terminology about drug products
requires the integration of multiple starting data sources, each pro—
viding a subset of the wanted information (see Fig. 1). A total of
373 823 drug records are to be integrated, related to 1 881 760
labels representing the possible synonyms a drug can have: labora—
tory code, generic name, brand name, identifier, cross—reference, etc.
(see Supplementary Fig. 1 for full scope). Three of these sources are
public databases: DrugBank (11 584 drug records), ChEBI (53 185
drug records) and ChEMBL (84 901 drug records, entries with syno—
nyms only). Alongside these, we also used four private feeds from
various vendors: Thomson Reuters Integrity (87 561 drug records,
including only entries with a generic or brand name), Thomson
Reuters Cortellis (57410 drug records), PharmaProjects (64 206
drug records) and AdisInsight (32 633 drug records). The last data
source is an internal repository containing the drugs developed
within the company alongside their synonyms and laboratory codes
(5 312 drug entries). This starting set of sources illustrates one of the
data integration challenges for pharmaceutical companies: informa—
tion comes with different level of privacy and confidentiality and
often requires a custom in—house solution. All sources were updated
the 17th of September 2015 in different formats (XML, RDF, SQL)
and loaded in a Java web application built using the Play! frame—
work 1.2.7 using MongoDB as back—end store.

For our use—case, each drug entry is abstracted as a vertex (also
called node) and has a list of one or more labels related to it. These
labels will in turn be used to create the links between the records. As
a result, a graph structure is created, over which it is possible to de—
rive various measures.

2.2 Graph measures

Connected data can be abstracted as graph structures, and a data in—
tegration problem can be seen as correctly creating undirected links
or edges between pairs of records or vertices. The links are created
following a certain condition, for instance given the presence of a
cross—reference, or using labels shared by records. One of the main
benefits of the graph abstraction is the possibility to re—use the math—
ematical tools and descriptors of graph theory. We wanted to iden—
tify problematic records coming from S complementary yet different
sources. To perform this task, we used the graph density as a theor—
etical measure of deviation against the perfect case (i.e. a complete
graph, discussed later). Once problematic graphs, or ‘hairballs’, are
identified, it is possible to isolate and clean ambiguous records using
the betweenness centrality, or measure of the relative influence of a
vertex within the graph based on its connectivity. This section

introduces these various graph descriptors and serves as a theoretical
reference.

2.2.1 Complete graphs and data sources

A complete graph is an undirected graph in which every pair of dis—
tinct vertices are connected by an edge (Wikipedia, 2014a). Given a
number V of vertices, the associated completed graph is denoted K N.
Its number Eh of edges can be calculated as follows:

V>< V—1
Bk: <2 >

(1)

Complete graphs are interesting for data integration purposes;
they represent a perfect agreement between records coming from dif—
ferent sources. For example, consider the integration of three data—
bases, each containing some complementary and partially
overlapping information, such as three resources describing drugs;
one expects the entries present in one of these databases to be also
equivalent to some of the records in the other resources. For in—
stance, the entry related to the diazepam in ChEBI (CHEBI:49575)
is equivalent to the entry describing the diazepam in DrugBank
(DB00829) and so forth, based on the synonyms they have in com—
mon. If links were drawn between equivalent records, one would
therefore expect to find triangle—shaped graphs where the equivalent
records from each three databases were connected to each other.
The presence of the triangle—shaped graphs gives confidence in the
integration: one node from each resource is reciprocally linked to a
corresponding node from each other resource, forming a densely
connected network, representing an ideal data integration situation.
In the case of three databases being integrated, the triangle shape
corresponds to the complete graph K 3 of three vertices (see Fig. 2A).

From this specific example, it is possible to generalize the ap—
proach considering the data integration of S sources; for such cases
it is expected to find complete graphs K5, where one record from
each of the S original sources is linked to every other equivalent re—
cord from the other sources. In summary, complete graphs represent
the perfect cases for data integration purposes, where records repre—
sented as vertices are tightly connected together by the edges repre—
senting an equivalence relationship, with a size depending on the
number of starting sources S.

2.2.2 Density

In practice, linked records often do not form complete graph struc—
tures, but have a geometry drifting away from it, due to the only
partial overlap of information (e.g. non—existing cross—references or
shared labels). It is possible to quantify this deviation using a metric
called density. The density of a graph is a number ranging from 0 to
1 and reflecting how connected a graph is in regards to its maximum
potential connectivity. Given an undirected graph with E edges and
V vertices, the density d is defined as:

2><E

diary—1)

(2)

The density can also be formulated as the ratio between actual
connections, i.e. the number of existing edges in the graph (E), and
the number of maximum potential connections between vertices,

called PC:

E

dzﬁ

(3)

PC is the same as the number of edges Eh in the complete graph,
as defined in Equation 1 and leading to the equality:

ﬁm'srcumol‘pquo'sopcuuopttotq/ﬁdnq

Figure 2

 

Figure 3

1

Brandes, 2001

/310‘srcumo[p10}xo‘sopcuHOJIItotq/ﬁdnq

922

S.Croset et al.

 

4'\.

2'15
00

4's. of?»

Fig. 3. Data integration process. (A) The records from the source databases are downloaded locally. (B) The graph structure is formed on top of the source re-
cords, and equivalence edges created. (C) The graph metrics are computed (density and betweenness centrality). (D) Hairballs are identified (graphs with low
density) and the problematic records are excluded from the build (vertices with high betweenness centrality). (E) Finally, the entries present in the same graph
are merged, in order to create consolidated records containing the originally segregated information (optional step)

purpose, yet it could be easily adapted for different use-cases or
datasets.

Links between records are undirected in order to capture as sim-
ply as possible the fact that a pair of records are semantically
identical. This relation could also be represented by a bi-directed
edge, or using two directed edges starting from each vertex of the
pair. However, such a representation would add unnecessary com-
plexity to the problem as the edge directionality is not used during
the computation of the graph density and BC. We outlook at the
end of this document the potential use and impact of other graph
metrics and representation, such as weighted edges.

2.3.2 Evaluate

The graphs created in the Link step vary widely in size and shape
(see Section 3). Because of the fuzziness and ambiguity of the ori-
ginal data, some records aggregate in ‘hairball-like’ structures,
which must be cleaned and disambiguated. The betweenness central-
ity and density are both helpful metrics in this regard, and they are
computed in this step. We used the GraphStream library (Dutot et
al., 2007) to calculate the betweenness centrality (Equation (5)), the
computation of the density was implemented directly in the pro-
gram. We also created a series of methods to export and visualize
the metrics, as illustrated in the Section 3. This step is generic and
can be applied on the top of any type of graph structure.

2.3.3 Clean

Problematic and ambiguous graphs can be identified from the met-
rics computed in the Evaluate step. Depending on the time, available
resources and dataset size, a threshold can be applied to flexibly iso-
late and classify the graphs to be further inspected. The density and
the betweenness centrality are however not flawless measures: for
instance, sometimes graphs with low density can be correct (false
positives) or erroneous nodes can have a low betweenness centrality
(false negatives). In order to find the optimal threshold values yield-
ing the best results, we performed a receiver operating characteristic
(ROC) analysis. For this purpose, both graph measures were con-
sidered as independent binary classifiers, and we manually inspected
a series of cases to generate the curves. The optimal threshold values
were determined using Matthews correlation coefficient (MCC). In
order to find the optimal density value separating the problematic
graphs from the correct ones, experts looked at 200 randomly se-
lected graphs created in the Link step (see Supplementary Fig. 2).
Based on this training set, the density threshold of 0.59 gave the best

results at separating graphs to be inspected versus correct ones
(MCC:0.651). The optimal betweenness centrality threshold was
derived from inspecting 100 random vertices coming from true
problematic graphs. The betweenness centrality threshold of 0.33
gave the best results and was used to separate vertices to be excluded
from vertices to be kept (MCC: 0.837, see Supplementary Fig. 3).
This approach was used as we wanted most of the process to be
automated; note that it is also possible to define the thresholds based
on the number of cases to inspect manually or any other external
parameters. The Clean and Evaluate steps can be repeated at will
until a satisfactory state of the data is reached. We repeated them
four times before merging the records (see Supplementary Fig. 4 for
effect of cleaning steps on the dataset).

2.3.4 Merge

The final step is optional and relevant only when the records need to
be merged or consolidated. The graph structure is no longer required
for such scenarios, as only merged individual entries are necessary in
the final data warehouse. As our work required the output to be de-
livered in this fashion, we merged the cleaned graphs and simplified
the duplicate and redundant labels.

2.3.5 Warehouse content evaluation and availability

The suitability of the data warehouse content for scientific tasks was
evaluated against a list of largest selling pharmaceutical products
Q4 2013, obtained from Wikipedia the 10th of December 2014
(Wikipedia, 2014b). Given a brand name on the list, the task was to
check if an entry was present in the data warehouse, and if this entry
was correct or not: only one drug product should be described in the
entry, and all the information about the product should be located on
this entry and not spread across multiple records. We chose this list of
best selling products, as corresponding entries in the warehouse are more
likely to contain ambiguities and errors due to an often larger synonym
set for the given products. These drug products are also more likely to be
searched and referred to by the users of the warehouse, given their im-
portance on the market, and are therefore solid evaluation points. The
Wikipedia list contains a few errors and imprecisions on its own, high-
lighted on the Supplementary Tables 1 and 2 alongside the results. More
evaluation work was performed internally, in particular a successful
demonstration of superiority against existing solution (not presented).
A subset of the final database containing the public sources is openly
available at https://github.com/loopasam/ﬂexibledata-integration. The

[310'sp2umofp105xo'sopeuHOJIItotq/ﬁdnq

Williams et al., 2012b

    

/ \.
was

~33”
‘ V".

  

(it

Figure 4

Figure 5

 

 

Figure 4

Figure 6

/3,IO'S[EIIm0fp.IOJXO'SOIJEIIIJOJIIIOIq”Zduq

 

Supplementary Material

 

 

Kidd, 2006

Batchelor et al., 2014

dnq

[310‘SIEIIIHOIPJOJXO‘SODEIIIJOJIItotqﬁ

Flexible data integration and curation

925

 

assertion of equivalence between records can be implemented as a
series of steps including for instance, data cleaning and transform—
ation, in order to best match the desired outcome (e.g. chemical
structures, cross—references, free—text label). Secondly, the approach
can handle 11 numbers of data sources, and is not limited to handle
only pairs of records. Thirdly, the density measure helps to extract
numbers regarding how confident one can be in regards to the qual—
ity of the integrated information and to determine how much cur—
ation is required on the top of the data. Finally, the betweenness
centrality identifies the erroneous records, and can be used to resolve
issues either automatically or manually. Thresholds for graph meas—
ures can be flexibly adjusted too, in order to optimise the work
based on the time and resources available.

The abstraction of the data integration problem as a graph repre—
sentation has been already introduced by previous work (Berners-
Lee et al., 2001; Bollacker et al., 2008; Singhal, 2012), yet the use of
graph measures on the top of the data to assess the quality of the in—
tegration and to detect anomalies is novel as far as our knowledge
goes. From a usability perspective, we believe that the quality of the
integrated data is more important than the technology or standards
used; in this regard, the methodology can be implemented in a var—
iety of frameworks, from RDF graphs to traditional relational data—
bases, as needed by the user. Other graph indicators could be used
in the future for similar tasks and to quantify different problem
types coming from data integration: As an example, we decided not
to assign weights to the edges of the graphs. This choice was moti—
vated by the sparsity of the data; we estimated that records sharing
one label were just as likely to be equivalent to records sharing mul—
tiple labels. Considering weights could result in the computation of
different graph metrics, useful to characterize a particular type of re—
cords, assuming a rationale could be defined for the alternative indi—
cators the same way it was done here with the density and BC.

A particularly challenging task resulting from data integration
initiatives is the maintenance of the created resource, the update of
records and the incorporation of new information. This can be im—
plemented alongside the methodology proposed, in different ways:
First it is possible to create incremental versions of the warehouse.
With this approach, the new data sources are downloaded following
a certain time period, and the integrated dataset is rebuilt from
scratch each time, following the same graph—based methodology.
New and corrected records are tested again, and excluded if neces—
sary. The main drawback of this option comes from the difficulty of
maintaining unique identifiers, as equivalences can vary from one re—
lease to the other. The second possibility for data update considers
the current data warehouse just as any standard starting source, and
tries to match and disambiguate new records following the same
methodology as presented in this article. Identifiers are easier to
maintain as the new version derives from the old one, but it might
become challenging to keep track of the origin of the data. Removed
information from starting sources, such as synonyms, can also be
trickier to detect and correct.

In conclusion, the goal of data integration is to provide a complete
picture of a scientific problem; we introduced here a methodology
emphasizing data coherence and correctness to fulfill this greater
task, where ambiguities, redundancies and errors are identified and
removed. In summary, the methodology presented addresses practical
concerns faced by large scale data integration exercises, prevalent
nowadays in the drug discovery domain. In an ideal world, database
entries and cross—references would be perfectly maintained and errors
non—existent, which would enable the straightforward creation of a
semantic network between entities. In practice, a costly and tedious
curation step is often necessary in order to control the quality of the

resource and the scientific value of its content. The presented method—
ology focuses on this aspect, in order to best prepare the integrated
data for the derivation of scientific knowledge.

Fu ndi ng
F. Hoffmann-La Roche AG.

Conﬂict of Interest: none declared.

References

Batchelor,C. et al. (2014) Scientiﬁc lenses to support multiple views over
linked chemistry data. In: The Semantic Weh—ISWC 2014. Springer.
pp. 98—113.

Berners-Lee,T. et al. (2001 ) The semantic web. Scientiﬁc American, 284, 28—3 7.

Bollacker,K. et al. (2008) Freebase: a collaboratively created graph database
for structuring human knowledge. In: Proceedings of the 2008 ACM
SI GMOD international conference on Management of data. ACM,
pp. 1247—1250.

Brandes,U. (2001) A faster algorithm for betweenness centrality’“. Journal of
Mathematical Sociology, 25, 163—177.

Dong,X. et al. (2014) Knowledge vault: A web-scale approach to probabilistic
knowledge fusion. In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, pp. 601—610.

Dutot,A. et al. (2007) Graphstream: A tool for bridging the gap between com-
plex systems and dynamic graphs. In Emergent Properties in Natural and
Artiﬁcial Complex Systems. Satellite Conference within the 4th European
Conference on Complex Systems (ECCS’2007).

Fellegi,I.P. et al. (1969) A theory for record linkage. ]. Am. Stat. Assoc., 64,
1183—1210.

Hernandez,M.A. et al. (1998) Real-world data is dirty: Data cleansing and the
merge/purge problem. Data Mining Knowled. Discov., 2, 9—37.

Juty,N.et al. (2012) Identiﬁers. org and miriam registry: community resources
to provide persistent identiﬁcation. Nucleic Acids Res., 40, D580—D586.

Kidd,]. (2006) Life after statin patent expiries. Nature Reviews Drug
Discovery, 5, 813—814.

Lipinski,C.A. et al. (2014) Parallel worlds of public and commercial bioactive
chemistry data. ]. Med. Chem.

MarX,V. (2013) Biology: The big challenges of big data. Nature, 498, 255—260.

Pence,H.E. and Williams,A. (2010) Chemspider: an online chemical informa-
tion resource]. Chem. Educ., 87, 1123—1124.

Roos,L. and Wajda,A. (1991) Record linkage strategies. part i: Estimating in-
formation and evaluating approaches. Methods Inform. Med., 30, 117—123.

Singhal,A. (2012) Introducing the knowledge graph: things, not strings.
Ofﬁcial Google Blog.

Szalma,S. et al. (2010) Effective knowledge management in translational medi-
cine.]. Trans. Med., 8, 68.

Tiikkainen,P. et al. (2013) Estimating error rates in bioactivity databases. ].
Chem. Inform. Model., 53, 2499—2505.

Wajda,A. et al. (1991) Record linkage strategies: Part ii. portable software and
deterministic matching. Methods Inform. Med., 30, 210—214.

Wikipedia (2014a) Complete graph.

Wikipedia (2014b) List of largest selling pharmaceutical products.

Williams,A.]. et al. (2012a) Open phacts: semantic interoperability for drug
discovery. Drug Discov. Today, 17, 1188—1198.

Williams,A.]. et al. (2012b) Towards a gold standard: regarding quality in
public domain chemistry databases and approaches to improving the situ-
ation. Drug Discov. Today, 17, 685—701.

Wilson,D.R. (2011) Beyond probabilistic record linkage: Using neural net-
works and complex features to improve genealogical record linkage. In
Neural Networks (I ]CNN), The 2011 International joint Conference on,
pages 944. IEEE.

Winkler,W.E. (1995) Matching and record linkage. Business Survey Methods,
1, 355—384.

Winkler,W.E. (2014) Matching and record linkage. Wiley Interdisciplinary
Reviews: Computational Statistics, 6, 313—325 .

ﬁm'srcumol‘piqxo'sopcuuopttotq/ﬁdnq

