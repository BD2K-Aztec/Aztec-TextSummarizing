BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

D.Quang and X.Xie

 

to the online methodology. Furthermore, we show that
EXTREME can achieve similar results to MEME in a fraction
of the execution time. We also show that using the entire dataset
is necessary to discover infrequent motifs, which is not always
practical to do with MEME. To the best of our knowledge, this
is the first application of the online EM algorithm to motif
discovery.

2 MATERIALS AND METHODS

2.1 MEME

The original MEME algorithm applies the batch EM algorithm to infer
PFMs. Here, we provide a brief overview of MEME’s model and how
MEME applies the batch EM algorithm to infer parameters.

2.1.] MEME’S model Let Y: (Y1, Y2,  YN) be the dataset of
sequences, where N is the number of sequences in the dataset. Each se-
quence is over the alphabet A = (A,C, G, T). MEME uses a mixture
model that breaks up the dataset into all n (overlapping) subsequences
of length W, which it contains. We will refer to this new dataset as
X 2 (X1, X2, . . . ,X"). The mixture model is a two-component
model that assumes each subsequence is either an instance of the motif
or background. Other variants of MEME place additional constraints.
The one occurrence per sequence variant assumes that each sequence
contains one instance of the motif. The zero or one occurrence per
sequence variant assumes each sequence can have zero or only one oc-
currence of the motif. These two variants make slight modiﬁcations to
MEME’s probabilistic model. We will only consider the two-component
model.

The background component is characterized as a zero-order Markov
model parameterized by the vector 6,,g = (f0~ A,f0~ C,f0.G,f0~ T) where fM is
the background frequency of letter k. The motif model is characterized by
the PFM 6m 2 (f1,f2, ...,fW). Eachﬁ=(f,~,A,ﬁ,C,ﬁyg,f/VT) is a parameter
of an independent random variable describing a multinomial trial repre-
senting the distribution of letters at position j in the motif. Am param-
eterizes the probability that any W-mer is generated by the motif model
while Abg = l — Am is the probability that any W-mer is generated by the
background model. 6 = (6m, 6,,g) and A 2 (Am, Abg) are unknown param-
eters that are inferred from the known data X. Therefore, the MEME
model is

p(Zi=l|6,A)=Am,l§i§n (1)

P<Xi|Zz> 9) = p(XiI6m)Z:p(2mebg)l’Z: (2)

where Z,- is a binary latent variable that has a value of 1 if X,- is drawn
from the motif model or 0 if X ,- is drawn from the background model. Z ,-’s
true value is unknown, but its conditional expected value, deﬁned here as
ZEO), for a given set of parameters can be calculated as follows:
X l» 9 A
21.0) = E[Z,»|X, 9, A] = M (3)
P(Xi|9m)}‘m +I7(Xi|9bg)}‘bg

To calculate ZEO), we need to know the form of p(X,»|6m) and the form of
p(X,»|6,,g). MEME assumes the distributions of the motif class and back-
ground class are

W
17(Xilem) = H Hﬂfxm (4)

j:1 keA

W
P<ng>=nnxs€w (5)

j:1 keA

where X ,3) is the letter in the jth positon of subsequence Xi, and I(k,a) is
an indicator function

1  a = k
0 otherwise

M, a) = { (6)

2.1.2 Batch EM A and 6 are iteratively improved in the batch EM
algorithm. In the E-step, the expected counts of all nucleotides at each
position are calculated based on the current guess of the parameters. In
the M-step, the parameters are updated based on the values calculated in
the E-step. MEME repeats the E and M steps until the change in em
(Euclidean distance) falls below a threshold (default: 1076). The E and M
steps are as follows:

n
CjJ‘» = ZE[Z§0)I(I(,
[:1
n W 0
E—step: 
[211:1
forkeAandj: 1,2, ..., W
fit. = —“/-:+ﬂ'< forj = 0, 1,  W
M _ Step: k;(/.I1+ﬁlz)

n (0
Z.
Am _— 2 ,2

1:]

To discover multiple motifs, MEME associates an ‘erasing factor’ E,-
for each position in the data. The erasing factors vary between 0 and l
and are set to I initially to indicate no erasing has taken place. Each time
a motif is discovered, the erasing factors are reduced by a factor repre-
senting the probability that the position overlaps an occurrence of
that motif. More details concerning how MEME erases are in Bailey
and Elkan (1995a). MEME also implements pseudo counts
)3 = (ﬁA,ﬁC,ﬁ0,ﬁT) in the M-step to prevent any letter frequency 
from becoming 0. This is because if any letter frequency becomes 0,
its value cannot change.

EM performs maximum likelihood estimation to maximize an object-
ive function. The new estimates in the M-step are always guaranteed to
increase the value of the objective function. As the E and M steps are
repeated, EM algorithms converge to a maximum. For MEME, the ob-
jective function is the expected value of the log likelihood of the model
parameters 6 and A given the joint distribution of the data X and missing
data Z:

EﬂOg “9’ “X: 2)] =  ZED) 10g(p(Xilem)/\m)
I: " (7)
+  (1 — ZEW)10g(p(Xilebg)Abg)

2.1.3 Seeding The EM algorithm is sensitive to initial conditions and
prone to converging to local maxima. To mitigate this problem, MEME
tests many seeds and runs the EM algorithm to convergence from the
‘best’ seed. The exact details of how MEME performs seeding can be
found in Bailey and Elkan (1995b).

2.1.4 Scoring the motlfs Motif instances are determined according to
Bayesian decision theory. After a motif is discovered, a subsequence X,- is
classiﬁed as being an occurrence of the motif only if

17(Xilem) *3)
log —(Xil6bg)) > log<Am (8)

For each motif discovered, MEME calculates its E—value. This E—value
is the number of motifs, with the same width and number of occurrences,
that can generate an equal or higher log likelihood ratio if the dataset had
been generated according to background model. The log likelihood ratio
[Ir 2 log(p(sites|motif)/ log(sites|background)) is a measure of how differ-
ent the sites are from the background model. Calculating the E—value

 

1 668

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

EXTREME

 

exactly can be time consuming, so it is not computed directly. It is instead
heuristically calculated as a function of the total information content and
the number of occurrences (Bailey et al., 2010).

2.1.5 Time complexity For each iteration of the batch EM algo-
rithm, the number of operations performed is approximately propor-
tional to W. Each batch EM iteration has a time complexity of 0(n W).
Although the number of iterations can vary, it is typically proportional to
n. Therefore, the algorithm scales quadratically with the size of the data-
set and has a time complexity of 0(n2W). The seed searching also scales
quadratically with the size of the dataset (Bailey and Elkan, 1995b).

2.2 EXTREME

EXTREME shares many similarities with MEME, especially in the im-
plementation. At the center of the EXTREME algorithm is the online
EM algorithm. We provide an overview of the online EM algorithm and
how EXTREME implements the online EM algorithm to discover
motifs.

2.2.1 Online EM Like the batch EM algorithm, the online EM algo-
rithm also repeatedly iterates between E and M steps, which update the
parameters. In contrast to the batch EM algorithm, each iteration of the
online EM algorithm operates on only one observation, X,-, instead of the
whole dataset X.

Following the instructions in Cappé and Moulines (2009), the E and M
steps, as derived from (7), are as follows:

0
Smj =Sm.z>1 + 16(2) ) —Sm.z>1)

Ell/(.11: Cj.k.i71 + JG<ZEO)I(/C X211”) — CLAIM)
E — step: CM.) = 60.11.24

W
+14<Z<1 — 210))IUCXLJ') — 50.11.14
1'21
fork e A,j= 1,2, ...,W,and >i=1,2, ...,n
M—step: ﬁkzi‘k“ forj=0,1, ...,W

£711.;

 

lzeA
Am 2 Smj

The step size is y,» 2 mi”. oz and yo are set to 0.6 and 0.05, respectively.
These are by no means the most optimized set of parameters, but they are
adequate for accurate motif discovery. As shown in Cappé and Moulines
(2009), the online EM algorithm converges to a local maximum of the
likelihood function (7) for at e (05, 1].

The E and M steps are repeated until a convergence threshold (default:
1076) in terms of the symmetrized KullbackiLeibler divergence between
the PFM estimates at a user-deﬁned number of intervals (default: 100) of
W-mers at the end of a complete pass through the dataset is satisﬁed. The
KullbackiLeibler divergence between two PFMs A and B is calculated as
follows:

_ 1 . ﬂ . ﬂ
KLD(A,B) —  (AM. log(3jpk) +3“. Iog(Ajpk)) (9)
If convergence is not reached at the end of a pass, the exponent at is
updated to the midpoint between ot’s current value and one and
EXTREME performs another pass through the dataset. EXTREME
repeats these steps until the convergence threshold is met.
To accommodate pseudo counts, we modify the indicator function
from (6):

l + [Bk a = k
[Bk otherwise

I(k, a) = { (10)

By default, EXTREME sets )3). to 0.0001 times the frequency of letter k in
the entire dataset.

To accommodate reverse complements, we also modify the calculation
of ZEO) from (3) so that for each X,-, the reverse complement is also
evaluated and ZEO) takes the higher of the two values. MEME, in con-
trast, handles reverse complements by adding a reverse-complemented
copy of the data, essentially doubling the size of the data.

2.2.2 Seeding Before running the online EM algorithm, the order of
the W-mers X ,- is randomized. The online EM algorithm is therefore a
stochastic algorithm. This means that different runs of the online EM
algorithm can yield different results, even if ran multiple times from the
same initial conditions. This can present a problem for seeding because
even using the best seed from MEME’s heuristic is not guaranteed to
generate the optimal or even consistent solutions, causing EXTREME to
converge to local maxima. On the other hand, this also means that seeds
that would yield non-optimal solutions in MEME can yield optimal so-
lutions in EXTREME. In fact, local maxima may actually correspond to
biologically relevant motifs, especially in datasets that are rich in motifs
such as DNase-Seq data. Furthermore, an efﬁcient online EM implemen-
tation of MEME offers little beneﬁt if runtimes are dominated by the
inefﬁcient seed search.

EXTREME’s seeding strategy applies a search-based motif discovery
algorithm to ﬁnd motifs to initialize the online EM algorithm. Similar to
DREME (Bailey, 2011), the seeding algorithm ﬁnds words that are en-
riched in a sequence dataset relative to a negative sequence dataset. We
use the same dinucleotide shufﬂe algorithm used in DREME to generate
a dinucleotide-shufﬂed version of the input sequence set as the negative
sequence set. The seeding algorithm counts the number of occurrences of
words in the positive sequence set and the negative sequence set and
associates a ‘z-score’ with each word. The z-score is given by

 

5+ — .L
z _ J: (11)
where 3+ and L are the number of occurrences of the word in the positive
sequence and negative sequence sets, respectively. If L is zero for a word,
it is changed to one to prevent division by zero. Unlike DREME, our
seeding algorithm searches for words that are not exact. Each word con-
tains g universal wildcard letters surrounded by ﬂanking sites of l unam-
biguous letters. For example, TCAGNNGGAC is a word with a gap
length, g, of 2 and a half length, l, of 4. The gap length, g, varies between
the user-deﬁned parameters gm,” and gm“. Z-scores for each value of g
are normalized by dividing by the standard deviation of all z-scores for
each respective value of g. Words that have a normalized z-score that
exceed a user-deﬁned threshold, 2mm)” and have at least a user-deﬁned
number of occurrences, Sm)”, in the positive sequence set are aligned and
grouped together using a hierarchical clustering algorithm we adapted
from Xie et a]. (2005). Word clusters are converted to frequency count
matrices by counting the number of occurrences of each letter at each
position along the alignment. The counts are weighted by the normalized
z-score of each word in a cluster so that more signiﬁcant words will
contribute more to the count matrix than less signiﬁcant words. A
count matrix is converted to a PFM, 6,", by dividing each matrix element
by its respective row sum. The initial expected counts, 6, is initially set to
the initial em as well. 6,,g and the expected background counts co are set to
the nucleotide frequency in the dataset. Am and SW) are initialized to the
predicted number of motif occurrences divided by n, the total number of
W-mers. We predict the number of motif occurrences for a given PFM
seed as the number of W-mers that have a goodness-of-ﬁt score >0.7 [see
Pan and Phan (2009) for details].
We also alter the form of p(X 49",) from (4):

W
17(Xi19m) = 1er H 11:12:11..) (12)
j:1 keA

The bias factor 11! has a value between 0 and 1. A bias factor closer to 0
biases the motif discovery toward subsequences that more closely match

 

1 669

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

D.Quang and X.Xie

 

the current motif guess, decreasing the number of discovered motif oc-
currences. A bias factor closer to 1 makes the motif discovery less select-
ive, increasing the number of motif occurrences. After convergence, motif
occurrences are identiﬁed using (8). 11! is initially set to 1, and its value is
varied in a binary search fashion until the number of discovered motif
occurrences is between .sites,,,,-,, (default: 10) and sitesnm. (default: 5 times
the number of predicted motif sites). Up to 15 different values of 11! are
tried before EXTREME stops. Because each initial PFM guess can be
tested independently, this seeding strategy can be parallelized to allow
multiple motifs to be discovered simultaneously. Hierarchical clustering
of the discovered motifs can then identify individual motif classes.

2.2.3 Time complexity Each pass through the dataset with the online
EM algorithm has a time complexity of 0(n W). Typically, the online EM
algorithm reaches convergence after one to ﬁve passes through the data,
so the overall time complexity is proportional to the width of the motif
and the size of the dataset. The seeding algorithm’s word search also
scales linearly with the dataset size, while the hierarchical clustering is
inefﬁcient and can scale cubically with the number of words to cluster. In
practice, EXTREME as a whole scales linearly in time complexity with
the dataset size.

2.2.4 Implementation EXTREME is written in Python and is avail-
able on Github. To calculate E-values, EXTREME uses Cython bindings
to the original MEME C source code to call the appropriate functions.
EXTREME requires ~8 GB of memory for a 10 Mb dataset. Most of the
memory is devoted to MEME’s E-value calculation, which involves a
preprocessing step that does not scale well to large numbers of motif sites.

3 RESULTS

MEME is a popular motif discovery algorithm. It has been a
valuable tool in the ongoing challenge of identifying regulatory
elements. However, its performance scales poorly with large
datasets. Experiments such as ChIP—Seq and DNase—Seq gener—
ate data that are too large for MEME to process in a practical
amount of time without discarding most of the data. To over—
come this challenge, we have developed EXTREME, a motif
discovery algorithm that can process ChIP—Seq and DNase—Seq
data efﬁciently without discarding any data. We ﬁrst show, using
simulated datasets, that MEME’s running time scales much
faster than EXTREME’s running time with respect to dataset
size. Using a ChIP—Seq dataset and a DNase—Seq dataset, we
demonstrate that using the entire dataset of sequences is neces—
sary to discover infrequent motifs. We also show that the motifs
discovered by EXTREME are similar in quality to the motifs
discovered by MEME.

3.1 Comparison of MEME and EXTREME performance

We compare MEME and EXTREME using several simulated
datasets. Simulated datasets are generated with the RSAT suite
of tools (Thomas—Chollier et al., 2011). We generate four se—
quence datasets, each containing 1000 random masked th9 gen—
omic sequences of a single length (100, 200, 300 or 400 bp), using
the RSAT random—genome—ﬂagments tool. This masked reference
genome was preprocessed with RepeatMasker (Smit et al., 199&
2010) and Tandem Repeats Finder (Benson, 1999) so that re—
peats (with period of g 12) are masked by capital Ns. For each of
the four sequence datasets, we implant 50, 100, 500 or 1000 in—
stances of the JASPAR (Sandelin et al., 2004) VDR/RXRA het—
erodimer motif (Supplementary Fig. Sl) using the RSAT

random—motifs and implant—sites tools, yielding 16 simulated
datasets, each containing 1000 sequences of varying lengths
and number of motif sites.

For the seeding step of each EXTREME run, we search for
words with a half—length l: 6, a gap length g between gmin = 0
and gmax22, a normalized z—score greater than the threshold
zthrﬁyh=5 and at least s,,,,-,,=5 occurrences in the positive se—
quence set. The words are clustered and we select the cluster
containing the most words to convert to a PFM seed from
which to initialize the online EM algorithm. Because the online
EM algorithm is a stochastic algorithm, we repeat the online EM
portion of the run 30 times for each dataset with different
random seeds to initialize the pseudorandom number generator
to get a good estimate of performance. We also run MEME on
each of the 16 simulated datasets to ﬁnd a single motif of a width
between 12 and 17 under the two—component model to approxi—
mate the same parameters for the EXTREME run. Figure 1
shows that MEME’s running time scales much faster than
EXTREME’s running time with respect to the input size for
all ‘noise’ levels. Extrapolating from these data, MEME can
take weeks to discover a motif in a 10 Mb dataset. EXTREME
can complete this same task in hours. With the exception of one
of the datasets, MEME is marginally more accurate than
EXTREME in each case (Supplementary Fig. S3). In the one
exception, MEME fails to converge to the correct motif because
there are not enough true motif occurrences relative to the data—
set size for MEME’s seeding algorithm to pick a good seed. As
the number of motif occurrences increases, both MEME and
EXTREME better approximate the true PFM and the relative
difference between their results diminish. Although
EXTREME’s running time and accuracy vary more as the
noise level increases (Supplementary Figs. 82 and S3),
EXTREME still consistently generates results comparable with
those of MEME in a fraction of MEME’s running time.

 

 

 

 

 

 

O
O
8 _
N I 505ites
0 1005ites
A 500$ites
8 o 1000sites
g - MEME
A N --- EXTREME
*3
8 §_
:9; a
a:
.E
'— 8
c» _
 E
C
:3
1!
O
O _
O
In
C, _

 

 

 

 

Input Size (kilobase-pairs)

Fig. 1. Comparison of MEME and EXTREME performance on simu-
lated datasets of varying sequence length and motif sites. The x-axis is the
total number of base pairs in the simulated dataset. The y-axis is the total
running time it takes for MEME or EXTREME to complete seeding and
reach convergence

 

1670

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

 

c
A
1* 2 m.” m ..g Gc
WW All 4.. 6.9, 62.,
Am  . Cw . my.
M mt  a  2 Ma 1.“.
I CTDa f c CT. CT AI! 0,
A _ A A A
._  m , m_ w .2
m m A U” m Aw MHW Aw hnI-W WW
I T2 T2 ll TTw II II. II II
I cg ,4
c C“.
CT ll
G_
ll

 

SOIJEIIIJOJIIIOIq/ﬂduq

/BJO'S[Eumo_fpm}X0'

EXTREME

 

(a) : 10186, E : 1.8 X 10—8878 (b) : 11072, E : 5.0 X 111—7021



(e) = 464, E = 2.4 X 10*897



(d) 115,-... = 923, E = 4.3 X 10*1130

 

(c1nsites : 931, E : 4.5 X 10—353

149111118492.

(f) = 1771, E = 6.0 X 10*184

Fig. 4. Six examples of motifs discovered by EXTREME in the K562 dataset. Number of non—overlapping motif sites in non—repetitive regions and E—
values shown below each motif. The E—values show how significant the motifs are, calculated according to MEME’s heuristic

   

(a) NRSF, E = 3.1 X 10*25 (b) CTCF, E = 1.2 X 10*21

   

(c) STAT1,E : 5.6 x 10*6 (d)NFYA, E : 3.1 X 10*10

Fig. 5. TOMTOM comparisons of motifs discovered by EXTREME
with motifs in databases. Each panel shows the logo of the motif dis—
covered by EXTREME (lower logo) aligned with the best matching motif
in the databases (upper logo), along with the name of the best matching
motif and significance value of the match

by EXTREME closely match motifs discovered by MEME,
the results can be used to reliably associate FPs with well-studied
TFs. This also means that any discovered novel motifs can con-
ﬁdently be associated with TFs. This is especially useful for the
study ofTFs that lack a suitable antibody for ChIP experiments.

While EXTREME is effective in motif discovery, there is
still much room for improvement. EXTREME’s performance
can be vastly improved if it were reimplemented in C. Future
implementations of EXTREME can also incorporate
more MEME elements such as the one occurrence per sequence
and zero or one occurrence per sequence models. To encourage
further investigation, we have made EXTREME publicly avail-
able at the Github repository httpz//github.com/uci-cbcl/
EXTREME.

Funding: National Institute of Biomedical Imaging and
Bioengineering, National Research Service Award (EB009418)
from the University of California, Irvine, Center for Complex
Biological Systems.

Conﬂict of Interest: none declared.

REFERENCES

Bailey,T.L. (2011) DREME: motif discovery in transcription factor ChIP-seq data.
Bl(}/l/f(}l‘lilllf/l’.\‘, 27, 165371659.

Bailey,T.L. and Elkan,C. (1994) Fitting a mixture model by expectation maximiza-
tion to discover motifs in bipolymers. Proe. 1111. Con]. In1e/l. S111. Mol. Bio/., 2,
28736.

Bailey,T.L. and Elkan,C. (1995a) Unsupervised learning of multiple motifs in bio-
polymers using expectation maximization. M11171. Le11rn., 21, 51780.

Bailey,T.L. and Elkan,C. (1995b) The value of prior knowledge in discovering
motifs with MEME. Proe. 1111. Con]. In1e/l. $11.11. Mol. Bio/., 3, 21729.

Bailey,T.L. e1 11/. (2010) The value of position-speciﬁc priors in motif discovery
using MEME. BMC Bl(}/l_lf(}l‘lilllf/l’.\‘, 11, 179.

Benson,G. (1999) Tandem repeats ﬁnder: a program to analyze DNA sequences.
N111'Iei1' Aeidr Re.\:., 27, 5737580.

Birney,E. e1 11/. (2007) Identification and analysis of functional elements in 1% of
the human genome by the ENCODE pilot project. N11111re, 447, 7997816.

Cappe,O. and Moulines,E. (2009) On-line expectationimaximization algorithm for
latent data models. J. R. S1111. So1'. Seriex B S1111. Me1/1o1/ol., 71, 5937613.

Cooper,G.M. e1 11/. (2005) Distribution and intensity of constraint in mammalian
genomic sequence. Genome Rein, 15, 9017913.

Dempster,A.P. e111/. (1977) Maximum likelihood from incomplete data via the EM
algorithm. J. R. S1111. So1'. Series B Me1/1o1/o/., 39(1), 1738.

Gupta,S. e1 11/. (2007) Quantifying similarity between motifs. Genome Bio/., 8, R24.

Hesselberth,J.R. e1 11/. (2009) Global mapping of protein-DNA interactions in 1'i1'o
by digital genomic footprinting. N111. Me1/1o1/x, 6, 2837289.

Johnson,D.S. e1 11/. (2007) Genome-wide mapping of in 1'i1'o protein-DNA inter-
actions. S1'ien1'e, 316, 149771502.

Machanick,P. and Bailey,T.L. (2011) MEME-ChIP: motifanalysis of large DNA
datasets. Bl(}/l/f(}l‘lilllf/l’.\‘, 27, 169fr1697.

Neph,S. e1 11/. (2012) An expansive human regulatory lexicon encoded in transcrip-
tion factor footprints. N11111re, 489, 83790.

Pan,Y. and Phan,S. (2009) Threshold for positional weight matrix. Eng. Le11., 16,
4987504.

Reid,J.E. and Wernisch,L. (2011) STEME: efﬁcient EM to ﬁnd motifs in large data
sets. N111'lei1' A1'i1/.\: Re.\:., 39, e126.

Sandelin,A. e1 11/. (2004) JASPAR: an open-access database for eukaryotic tran-
scription factor binding proﬁles. N111'lei1' A1'i1/x‘ Rein, 32, D917D94.

Sandve,G.K. e1 11/. (2006) Accelerating motif discovery: motif matching on parallel
hardware. In: A/gori1/11nx in Bioinfor1n111i1'x. Springer, Berlin Heidelberg,
pp. 1977206.

Smit,A. e1 11/. (199fr2010) RepeatMasker Open-3.0.

Thomas-Chollier,M. e1 11/. (2011) RSAT 2011: regulatory sequence analysis tools.
N111'Iei1' Aeidr Re.\:., 39, W867W9l.

Xie,X. e1 11/. (2005) Systematic discovery of regulatory motifs in human promoters
and 3‘ UTRs by comparison of several mammals. N11111re, 434, 3387345.

 

1 673

[310'smumofp105xo'sopeuuopnorq/ﬁdnq

