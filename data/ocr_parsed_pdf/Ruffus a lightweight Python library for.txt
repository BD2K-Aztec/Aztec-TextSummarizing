Vol. 26 no. 21 2010, pages 2778-2779
APP N doi:10.1093/bioinformatics/btq524

 

Genome analysis

Advance Access publication September 16, 2010

Ruffus: a lightweight Python library for computational pipelines

Leo Goodstadt

Medical Research Council Functional Genomics Unit, Department of Physiology, Anatomy and Genetics, University of

Oxford, Oxford OX1 SQX, UK
Associate Editor: Martin Bishop

 

ABSTRACT

Summary: Computational pipelines are common place in scientific
research. However, most of the resources for constructing pipelines
are heavyweight systems with graphical user interfaces. Ruffus is a
library for the creation of computational pipelines. Its lightweight and
unobtrusive design recommends it for use even for the most trivial
of analyses. At the same time, it is powerful enough to have been
used for complex workflows involving more than 50 interdependent
stages.

Availability and implementation: Ruffus is written in python. Source
code, a short tutorial, examples and a comprehensive user manual
are freely available at http://www.ruffus.org.uk. The example program
is available at http://www.ruffus.org.uk/examples/bioinformatics
Contact: ruffus@llew.org.uk

Received on August 4, 2010; revised on August 31, 2010; accepted
on September 9, 2010

1 INTRODUCTION

Large-scale computational analyses are now integral to many
biological studies. ‘Workﬂow’ management systems have accord-
ingly proliferated, including Taverna (Oinn et al., 2004), Biopipe
(Hoon et al., 2003) and Pegasys (Shah et al., 2004). These are highly
featured, designed for automated and robust operation even by non-
expert users, managed using graphics user interfaces and speciﬁed
in XML or proprietary domain-speciﬁc languages.

However, these workﬂow systems can be too cumbersome
for explorative and empirical studies with novel datasets. The
appropriate scientiﬁc approach cannot always be determined a priori.
On the other hand, the advantages of computational pipelines over
ad hoc scripts, even for simple tasks, are all more apparent with
increasingly complex datasets and the use of parallel processing.

The standard Unix build (software construction) system ‘make’
has been widely used to keep track of dependencies in scientiﬁc
pipelines. ‘Makeﬁles’ specify the ﬁles names of data for the input and
output of each stage of a pipeline as well as the ‘rules’ (commands)
for generating each type of output from its corresponding input. The
entire pipeline is represented by a statically inferred dependency
(directed acyclic) graph for the succession of data ﬁles. The same
‘rule’ can be applied to multiple data ﬁles at the same time, for
example, to run BLAST searches on many sequence ﬁles in parallel.
Automatic data tracking in pipelines allows only the out-of-date parts
of the analyses to be rescheduled and recalculated, with minimal
redundancy. This is necessary when parts of the pipeline are subject
to rapid cycles of development or where the underlying data is being
generated continually.

Unfortunately, ‘make’ is not a good ﬁt for the design of scientiﬁc
pipelines. ‘Make’ speciﬁcations are written in an obscure and limited
language. (This is mitigated in ‘make’ replacements such as ‘scons’
or Ruby ‘rake’). Pipeline dependencies are not speciﬁed directly
but inferred by the ‘make’ program by linking together ‘rules’ in the
right order. This means that scientiﬁc pipelines can be difﬁcult to
develop, understand and debug.

So-called ‘embarrassingly parallel’ problems are particularly
common in bioinformatics; examples include BLAST and HMMer
searches of sequence databases, or region-by-region genome
annotation. The number of parallel operations needed varies at ‘run-
time’ with the presented data: a larger sequence ﬁle might be split
up into smaller fragments to be processed in parallel. However,
‘make’ systems and their kin require all operations in a pipeline
to be determined when the build script is analysed, because of the
reliance on static, pre-calculated dependency graphs. They cannot
easily deal with, for example, the splitting up of large problems into
smaller fragments to be computed in parallel, if the number of such
fragments depends on the input data and runtime conditions, and
can only be determined in the middle of running the pipeline.

In this article, we present a new lightweight library for
computational pipelines that explicitly supports these programming
tasks. Some of its main advantages of Rufﬁis are:

0 Rufﬁis conﬁguration ﬁles are normal Python scripts. Python
is a modern dynamically typed programming language known
for its elegance, simplicity, and that is already widely used in
the bioinformatics community (Cook et (11., 2009). Standard
Python tools can be used to develop and debug Rufﬁis scripts.

0 Like ‘makeﬁles’, Ruffus scripts can run only the out-of-date
parts of the pipeline, using parallel processing if appropriate.

0 Pipeline dependencies are speciﬁed explicitly for maximal
Clarity and ease of documentation.

0 A ﬂowchart of the pipeline can be printed out in a variety
of graphical formats. Detailed trace output is available,
documenting which operations are up-to-date or will be run
(Fig. 1).

2 DESIGN

Rufﬁis is a module for the python language that adds lightweight
support for computational pipelines. Each stage of the pipeline is a
separately written (normal) python function. By convention, strings
contained in the ﬁrst two arguments of pipelined ﬁinctions are
assumed to be names of input and output ﬁles for this stage. The
modiﬁcation times of the underlying ﬁles are used to determine if
this part of the pipeline is up-to-date or not, and should be re-run.

 

2778 © The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org

112 /3.IO'S[1211,1110prOJXO'SOuBLUJOJIIlOlq”Idllq 11101} popeolumoq

9103 ‘{g anﬁnv 110::

Ruffus

 

 

Pipeline:
Job : [originalfa 7> ".segment] completed
Completed Task : splitFasta
,,,,,,,,,, ,1 Job : [1.segment e> 1.blastResult] completed
|
Job : [2.segment e> 2.blastReSult] completed
IrunBIastl

Job : [3.segment e> 3.blastResu1t] completed
Job = [4.segment —> 4.blastResu1t] completed
Completed Task : runBlast

    
 

Job = [[l.blastResult, 2.blastResult,
3.blastResult, 4.blastResult]
7> Final.blast_results] completed
Completed Task : CombineBlastResults

combincBlastRcsults

 

 

 

Fig. 1. Trace output and ﬂowchart for a simple Ruffus pipeline.

Table 1. Examples of Ruffus ‘decorator’ keywords

 

Ruffus Keyword Function of annotated pipeline function

 

Split Splits up input ﬁle into a number of output ﬁles
(a one-to-many operation)

Transform Transforms each input into a corresponding output
Merge Merges multiple input into a single output

(a many-to-many operation)
Collate Group together subsets of input. summarizing each

as a separate output.

 

Ruffus ensures that these pipeline functions are called in the right
order with appropriate arguments. For example, when the pipeline
speciﬁes BLAST (Altschul et al., 1990) searches on four sequence
ﬁles, three separate calls to the appropriate python function will be
made, in parallel if necessary.

To register pipeline stages, Ruffus provides some simple
keywords (Table 1) using standard python syntax. These python
‘decorators’ placed before each function indicates how the stages
of the pipeline are linked together, the type of operation and what
arguments to supply to each stage of the pipeline.

3 FUNCTION AND EXAMPLES

A standard bioinformatics task for running a blast search efﬁciently
in parallel might involve splitting the initial large sequence ﬁle into
smaller pieces, calling the BLAST executable for each, and then
combining the separate high scoring segment pairs (HSPs) into the
ﬁnal list of matches. These three operations would be represented by
three python functions ‘decorated’ by the ‘split’, ‘transform’
and ‘merge’ Rufﬁis keywords. The syntax (in outline) would be as
follows:

from ruffus import *

@split ( "original . fasta" , " * . segment")
def splitFasta(sequle, segments):
# code to split sequence file into
# as many fragments as appropriate
# depending on the size of "original.fasta"

@transform(splitFasta, suffix ( " .segment") ,
" .blastResults")

def runBlast(sequle, blastResultFile):
# code to run blast here

@merge (runBlast, " final .blast_results ")
def combineBlastResults (blastResultFile,
combinedBlastResultFile) :
# code to combine results here

pipeline_run( [combineBlastResults] , verbose = 3,
multiprocess = 5)

This will run the three-stage pipeline using up to ﬁve
processors in parallel, ﬁrstly splitting up the starting
sequence ﬁle ‘original . fasta’ into multiple ﬁles with
the sufﬁx ‘ . segment’, then running the BLAST program
(Altschul et al., 1990) to produce corresponding ﬁles with the
‘ .blastResult’ sufﬁx, and ﬁnally combining all these into the
ﬁle ‘final.blast_results’. The trace ﬁle for this simple
pipeline, as well as its ﬂowchart produced by Ruffus, is shown in
Figure 1.

More challenging examples using, for example, the ﬁill power of
regular expressions to manage pipeline data ﬁles, can be found in
the Ruffus documentation.

4 CONCLUSION

Rufﬁis is a python library for programming computational pipelines
with lightweight, unobtrusive syntax. It provides all the power of
traditional build systems such as automatic data tracking, but in
a modern package suited to the needs of bioinformatics. Sample
ﬂowcharts of Rufﬁis pipelines, a tutorial, a detailed manual as well
as source code are freely available from http://www.ruffus.org.uk
and http://code.google.com/p/ruffus.

ACKNOWLEDGEMENTS

Many thanks to Andreas Heger for advice on the design of Ruffus;
Chris Nellaker and T. Grant Belgard for their many suggestions; and
Chris Ponting for his support throughout this project.

Funding: Medical Research Council.

Conﬂict oflnterest: none declared.

REFERENCES

A1tschu1,S.F. et at. (1990) Basic local alignment search tool. J. Mol. Biol., 215, 403410.

Cock,P.J. et at. (2009) Biopython: freely available Python tools for computational
molecular biology and bioinformatics. Bioinformatics, 25, 142271423.

Hoon,S. et at. (2003) Biopipe: a ﬂexible framework for protocol-based bioinformatics
analysis. Genome Res., 13, 190L1915.

Oinn,T. et al. (2004) Taverna: a tool for the composition and enactment of bioinformatics
workﬂows. Bioinformatics, 20, 304573054.

Shah,S.P. et al. (2004) Pegasys: software for executing and integrating analyses of
biological sequences. BMC Bioinformatics, 5, 40.

 

2779

112 /3.io's[Bumo[p.IOJxosorwurJOJurorq”:duq uroii popcolumoq

9103 ‘{g isnﬁnv 110::

