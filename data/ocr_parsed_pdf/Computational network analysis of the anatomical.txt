ORIGINAL PAPER

Vol. 27 no. 23 2011, pages 3293-3299
doi:10. 1 093/bioinformatics/btr558

 

Gene expression

Advance Access publication October 7, 2011

Computational network analysis of the anatomical and genetic

organizations in the mouse brain

Shuiwang Ji

Department of Computer Science, Old Dominion University, Norfolk, VA 23529, USA

Associate Editor: John Quackenbush

 

ABSTRACT

Motivation: The mammalian central nervous system (CNS)
generates high-level behavior and cognitive functions. Elucidating
the anatomical and genetic organizations in the CNS is a key
step toward understanding the functional brain circuitry. The CNS
contains an enormous number of cell types, each with unique gene
expression patterns. Therefore, it is of central importance to capture
the spatial expression patterns in the brain. Currently, genome-wide
atlas of spatial expression patterns in the mouse brain has been
made available, and the data are in the form of aligned 3D data arrays.
The sheer volume and complexity of these data pose significant
challenges for efficient computational analysis.

Results: We employ data reduction and network modeling
techniques to explore the anatomical and genetic organizations in
the mouse brain. First, to reduce the volume of data, we propose to
apply tensor factorization techniques to reduce the data volumes.
This tensor formulation treats the stack of 3D volumes as a 4D
data array, thereby preserving the mouse brain geometry. We then
model the anatomical and genetic organizations as graphical models.
To improve the robustness and efficiency of network modeling,
we employ stable model selection and efficient sparsity-regularized
formulation. Results on network modeling show that our efforts
recover known interactions and predicts novel putative correlations.
Availability: The complete results are available at the project
website: http://compbio.cs.odu.edu/mouse/

Contact: sji@cs.odu.edu

Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on May 2, 2011; revised on October 1, 2011; accepted on
October 3, 2011

1 INTRODUCTION

The mammalian central nervous system (CNS) generates high-level
control functions, and knowledge on the anatomical and genetic
organizations in this system can elucidate the functional brain
Circuitry. The enormous complexity of this system is reﬂected in
the large number of cell types, each with unique gene expression
patterns. Therefore, it is of central importance to capture the
anatomical localization of gene expressions in the brain. Recent
advances in bioimaging technologies, such as the high-throughput
in situ hybridization (ISH) technique, have made it possible to
capture the spatial expression patterns in the adult mouse brain (Lein
et (11., 2007). Consequently, genomic-scale expression atlases in the
form of digital images have been produced at increasing speed and

resolution. The marriage of image processing tools and advanced
computational methods opens the door for unraveling the functional
brain Circuitry and the generation of high-level cognitive functions
on top of it.

The Allen Brain Atlas (ABA) (Lein et (11., 2007) contains 3D
atlas of gene expression in the adult mouse brain and is one of
the most comprehensive datasets for spatial expression patterns in
the mammalian CNS. It provides cellular resolution 3D expression
patterns in the male, 56-day-old C57BL mouse brain. In this atlas,
genome-wide coverage is available in sagitally oriented sections. In
addition, coronal sections at a more reﬁned scale are available for
a set of about 4000 genes showing restricted expression patterns.
The image data are generated by in situ hybridization using gene-
speciﬁc probes, followed by slide scanning, 3D image registration
to the Allen Reference Atlas (ARA) (Dong, 2009) and expression
segmentation (Lein et (11., 2007; Ng et (11., 2007). This results in
a set of spatially aligned 3D volumes of size 67 x41 X 58, one for
each gene, that document the spatial expression patterns of genes
in the mouse brain. Efﬁcient and effective analysis of these high-
throughput data can shed light on the global function of mammalian
CNS (Jones et (11., 2009). On the other hand, the sheer volume and
complexity of these data pose signiﬁcant Challenges for efﬁcient
computational analysis. Hence, computational understanding of
these data is limited to unsupervised techniques, which Cluster the
brain regions into co-expressed groups (Bohland et (11., 2010).

In this article, we employ advanced computational techniques to
model the anatomical and genetic organizations in the mouse brain
as networks. First, to reduce the size of data and accelerate efﬁcient
analysis and storage, we propose to apply tensor factorization
techniques to reduce the data volumes (Kolda and Bader, 2009;
Wrede, 1972). This tensor formulation treats the stack of 3D volumes
as a 4D data array, thereby preserving the mouse brain geometry.
Based on the reduced data, we model the anatomical and genetic
organizations as graphical models in which each vertex represents
a spatial location or a gene, and the edges between vertices encode
the correlations between locations and genes (Dempster, 1972;
Edwards, 2000). To improve the efﬁciency of network modeling,
we employ an approximate formulation for Gaussian graphical
modeling, which involves a series of sparsity regularized regression
problems (Meinshausen and Buhlmann, 2006). The efﬁciency of this
approximate formulation enables us to employ a robust estimation
technique known as stability selection (Meinshausen and Buhlmann,
2010), which estimate and combine multiple models based on
resampling.

We apply the data reduction and network modeling techniques
to learn the anatomical and genetic networks underlying the mouse

 

© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 3293

112 /3.Io's[BrunoIpJOJXO'sonBurJOJurorq”:duq 11101} papBOIII/lAOG

9103 ‘Og anﬁnv uo ::

S.Ji

 

brain using the ABA expression volume data. Results show that the
expression patterns of spatially adjacent voxels tend to correlate. We
also observe that the expression patterns of certain brain structures
are correlated to the patterns of a large number of other regions,
some of which are spatially distant. In-depth analysis reveals that
such correlation patterns recover existing knowledge on the brain
ﬁinctionality. Our efforts on genetic network modeling identify
functionally related genes that act in a concerted manner in the
mouse brain.

2 HIGH-ORDER FEATURE EXTRACTION VIA
TENSOR FACTORIZATION

In ABA. the ISH image series of each gene are aligned to the ARA. To
faithfully capture the mouse brain geometry. a 3D grid is employed to divide
the 3D ARA space into quadrats. and expression information within each
quadrat is summarized. Speciﬁcally. an expression segmentation algorithm
is employed to identify expressed cells. and then an expression energy value
is computed from each voxel as a function of the intensity and density of
expression within that voxel. These image processing steps convert each
expression pattern into a 3D volume. To enable the application of matrix
computation techniques such as the singular value decomposition (SVD).
these volumes are usually converted to vectors and stacked into a data
matrix (Bohland at (11.. 2010). However. such conversion fails to retain the
spatial locality and other high-order information in the expression volumes.
To overcome this limitation. we propose to treat the 3D volumes as 3D
tensors and stack them together to form a 4D tensor. We then employ tensor
factorization techniques to reduce the dimensionality of this 4D tensor along
each mode. resulting in signiﬁcant data compression.

A key advantage of this tensor representation is that the associated
tensor computation techniques. such as high-order SVD and low-rank tensor
approximation. can be employed to compress the data without ﬂattening the
internal structure of the high-order data array. These techniques approximate
the original tensor by a core tensor multiplied by a basis matrix along each
mode. Hence. the core tensor and the set of basis matrices give a compact
representation of the original tensor. and the core tensor captures the major
information in the original tensor.

2.1 Background on tensors

Tensors. also known as multidimensional matrices (Kolda and Bader, 2009;
Wrede, 1972). are higher order generalizations of vectors (ﬁrst-order tensors)
and matrices (second-order tensors). The order of a tensor is the number
of indices. also known as modes or ways. In this article. tensors are
denoted by boldface Euler script letters. e.g. %ERJ1XJ2X“‘XJN. and its
elements are denoted as leJZWWJ-N. where lfjann for n=1,...,N. As a
generalization of matrix multiplication. the n-mode tensor-matrix product
deﬁnes the multiplication of a tensor by a matrix in mode n (Lathauwer
at (11.. 2000a). The n-mode product of a tensor %ER11X12X“‘XJN with a
matrix A e R1 X!" = (ail-W) is denoted by 51’? ><,,A. The result is a tensor of size
J1 ><  ><J,,_1 ><I><Jn+l ><  ><JN deﬁned elementwise as

In

(%X”A)jl~jn—lijn+l~jN = E x]1~Jn—1]'nfn+1~jNaijn-
jnzl

Let “J e R’1 “2 X“‘XJN be another tensor of the same size as 53/”. The scalar
product of these two tensors is deﬁned as:

ll .12 JN

<%’Q>ZZZ”'Z)9]afZawafNyflafZawafN' (1)

11211221 fN=l

Based on the scalar product. the Frobenius norm of a tensor 51’? can be

deﬁned as
||Qf||=v<ﬁﬂﬁf> (2)

 

 

 

 

 

Fig. 1. Illustration of tensor factorization. The three-way tensor on the left
is factorized into the products of a core tensor and three basis matrices on
the right.

The mode-n vectors of 51’? are the Jn-dimensional vectors obtained from
51’? by varying index j” while keeping all other indices ﬁxed. Tensors can
be converted into matrices via a process known as unfolding (Kolda and
Bader, 2009). Speciﬁcally. the mode-n unfolding of 51’? yields a matrix X(,,) E
R1" X(11J2“‘J"-11"+1“JN) whose columns consist of the mode-n vectors of 53/”.
The mode-n rank of 51’? . denoted as ranknwr’f). is deﬁned as the rank of
the matrix obtained from mode-n unfolding of 51’? : rankn(.%”)=rank(X(,,)).
Tensors have been used in a wide range of domains including microarray
data analysis (Omberg at (11.. 2007) and natural image modeling (Vasilescu
and Terzopoulos. 2004; Wang et (11.. 2005).

2.2 Tensor factorization

High-order singular value decomposition (HOSVD) (Lathauwer at (11..
2000a) is a generalization of the SVD for matrices. Given a tensor 51’? e
R’1 “2 X“‘XJN. its HOSVD can be expressed as

Qf=yx1U<1>X2U<2>xme U”, (3)

where .96 R11X12X"'XJN . and U“) E R!" X!" . for n: 1, . . . ,N. are orthogonal
matrices. In HOSVD. the basis matrices {U00};l are computed as the left
singular matrices of the mode-n unfolding of 51’? . and the core tensor can
then be computed as

y=%x1(U<‘>)Tx...xN(U<N>)T. (4)

Given a tensor %ERJIX12X"'XJN, a rank-(R1,...,RN) factorization of
51’? (Lathauwer at (11.. 2000b) is formulated as ﬁnding a tensor  with
rankn(5i’f)=R,,5rankn(Q/”) for lfnfN such that the following cost
function is minimized:

Zi’fzargminllﬁf—ﬁfll. (5)
SK

It follows from this deﬁnition that  can be expressed as
5%”:‘6X1VWX2V®><---><NV(N), (6)

where (KERRI XRZX'"XRN is called the core tensor and V“) ER!" XR"
(1 in: N) has orthonormal columns. When the basis matrices {V(")}2/:l
are given. the core tensor ‘6 can be readily computed as Lathauwer et al.
(2000a)

<€=ﬁfmvm>i MW)? x  WV”)? (7)

Hence. the key to the low-rank tensor factorization problem is to compute
the basis matrices. The factorization of a 3D tensor is illustrated in Figure 1.

One of the commonly used algorithms to compute the basis matrices is
the alternating least squares (ALS) method (Lathauwer at (11.. 2000b). In
each iteration of this method. one of the basis matrices is optimized while all
others are ﬁxed. Speciﬁcally. when V“), . . . , V(”_l), VWH), . . . , V(N) are ﬁxed.
we ﬁrst compute 513,): <gf//I><1(V(l))T ><  ><,,_1(V(”_1))T ><,,_).1(V(”+1))T ><
 ><N(V(N))T. Then the columns of V“) can be obtained as the ﬁrst Rn
columns of the left singular matrix of (515/00,). which is the mode-n unfolding
of 513,). In ALS. the basis matrices are usually initialized as the truncated basis

 

3294

112 /3.Io's[Bruno[p.IOJxosorwurJOJurorq”:duq uron papBOIIIAAOG

9103 ‘Og anﬁnv uo ::

Computational network analysis

 

matrices from HOSVD (Lathauwer at (11.. 2000b). That is. V“) is initialized
as the ﬁrst Rn columns of U“). for n: 1, . . . ,N. When the size of the tensor
is very large and cannot ﬁt into memory. an out-of—core algorithm can be
applied by partitioning the tensor into blocks (Wang et (11.. 2005).

The advantages of tensor-based methods in comparison to matrix-based
approaches have been addressed in the literature (Omberg at (11.. 2007;
Vasilescu and Terzopoulos. 2004; Wang et (11.. 2005). In summary. tensor-
based methods have the following two major advantages: (i) tensor-based
methods can be applied to large datasets for which matrix-based methods are
too expensive to apply. For example. the size of the data array for genetic
network modeling in this article is 3012 X 67 X41 X58. While the tensor-
based method requires the SVD of three matrices of sizes 67 X 67. 41 X 41.
and 58 X 58. respectively. the matrix-based method requires the SVD of a
matrix of size 3012 X 159, 326. (ii) Although matrix-based methods give the
lowest reconstruction error due to the best low-rank approximation property
of matrix SVD. tensor-based methods preserve the geometry of the high-
order data array. In the literature. tensor-based and matrix-based methods
have been compared in classiﬁcation tasks (Ye. 2005). Speciﬁcally. it has
been shown that. though tensor-based methods give larger reconstruction
error. they usually yield higher classiﬁcation accuracy.

3 NETWORK CONSTRUCTION VIA SPARSE
MODELING

The 4D tensor of gene expression obtained from the ABA is factorized as
described above. The core tensor retains most of the information in the
original tensor while its size is signiﬁcantly reduced. This data reduction step
is critical for the subsequent efﬁcient analysis. Based on the reduced data. we
employ sparse graphical modeling approaches to construct the anatomical
and genetic networks underlying the mouse brain.

3.1 A sparsity regularization formulation

Gaussian graphical models are a class of methods for modeling the
relationships among a set of variables (Edwards. 2000; Whittaker. 1990). In
this formulation. the d-dimensional variable x: [x1,x2,...,xd]T is assume
to follow a multivariate Gaussian distribution x~N ()1, E). where ueRd
and X 61W” are the mean and covariance. respectively. The conditional
dependency between pairs of variables can be encoded into a graphical
model in which vertices represent variables and edges characterize the
conditional dependency between variables. In particular. there is an edge
between nodes corresponding to X) and xj if and only if these two variables
are conditionally dependent given all other variables. This is equivalent to
the saying that there exists an edge between nodes corresponding to X) and x]-
if and only if the (i,j)-th entry of the inverse covariance matrix (also known
as concentration matrix) Q: 2‘1 is non-zero (Dempster, 1972; Edwards.
2000). This correspondence is illustrated in Figure 2.

Given a set of n observations y1,y2, . . . ,yn. the concentration matrix can be
estimated by maximizing the penalized log likelihood as follows (Banerjee
at (11.. 2008; Friedman at (11.. 2008; Yuan and Lin. 2007):

Ozarg logdetQ—trace(S§2)—A||Q||1, (8)

>

where detfl is the determinant of f2. Q>O represents that Q is positive
deﬁnite. S denotes the empirical covariance matrix computed from data.
and “Q”; is the 1-norm of 52. which is the sum of the absolute values of the
entries of $2. The ﬁrst two terms in Equation (8) are the log likelihood. and the
last term is used to enforce that many entries of Q are set to zero. yielding
a sparsely connected graph. This formulation has been used to model the
gene networks in Arabidopsis thaliana (Wille at (11.. 2004). The optimization
problem in Equation (8) is convex and can be solved by several algorithms
such as the interior point method (Banerjee at (11.. 2008) and the graphical
lasso algorithm (Friedman at (11.. 2008). However. all these algorithms are
computationally expensive and can only be applied to small-scale problems.
For the modeling of mouse brain networks. we have thousands of genes and
tens of thousands of voxels; hence. this formulation is not applicable.

 

 

 

 

 

 

 

 

 

Fig. 2. Illustration of the concentration matrix (A) and the corresponding
graphical model (B). The zero entries in the concentration matrix are unﬁlled
while the non-zero entries are ﬁlled with green. In this example. x1 and X5
are conditionally independent given all other variables.

In Meinshausen and Buhlmann (2006). an approximate formulation is
proposed to learn Gaussian graphical models by solving a series of sparse
regression problems. Speciﬁcally. the conditional dependencies between X)
and all other variables are learned by solving the following 1-norm penalized
regression problem known as lasso (Tibshirani. 1996):

6v=arg min llyi—Y_iW||2+A||W||1. (9)
weRd‘l

where YTi = [y1, . . . ,yi_1,yi+1, . . . .yn] E RdXV‘Tl) is the data matrix obtained
by removing the i-th data item. The conditional dependencies between X)
and all other variables are obtained from the corresponding components in
the weight vector w. Note that the regression of xi onto Xi and that of x]-
onto X) may not give the same result. Hence. two simple schemes. based
on logic operations or and and. are proposed to interpret the results. In the
ﬁrst scheme. two variables are considered to be conditionally dependent
if either of them yields non-zero weight (Meinshausen and Biihlmann.
2006). In the second scheme. they are considered as conditionally dependent
if both of them give non-zero weights. The ﬁrst scheme is employed in
this work (Meinshausen and Biihlmann. 2006). The pairwise relationships
between all pairs of variables can be obtained by running the sparse
regression problem in Equation (9) for each variable. A critical observation
that leads to the efﬁciency of the formulation in Equation (9) is that it
involves solving (1 independent lasso problems. one for each variable. The
lasso problem can be solved very efﬁciently by many algorithms such as the
accelerated gradient method (Liu et (11.. 2009). It has been shown that this
sparse regression formulation of Gaussian graphical modeling maximizes
the pseudo likelihood (Friedman at (11.. 2010) and is an approximation to the
maximum likelihood scheme in Equation (8) (Banerjee 61611., 2008; Friedman
at (11.. 2008). In particular. the exact maximization of log likelihood involves
solving the lasso problems iteratively as in the graphical lasso algorithm
(Friedman 61611., 2008). and the formulation in Equation (9) can be considered
as a one-step approximation to the maximum likelihood scheme. We employ
this approximate formulation to learn the mouse brain networks due to its
efﬁciency.

3.2 Robust estimation via stability selection

The regularization parameter A in Equation (9) controls the trade-off between
the sparsity of solution and data ﬁt. Speciﬁcally. when A is set to a very
large value. most of the entries of w are set to zero. Hence. a challenge in
practice is how to select the value for A. Stability selection (Meinshausen and
Buhlmann. 2010) addresses this problem by ideas similar to the ensemble
learning methods widely used in machine learning (Buhlmann. 2004). In
stability selection. we choose a set of A values denoted by A. instead of a
single A value. For each A e A. we compute the selection probability for each
variable. which is deﬁned as the probability of each variable been selected
when randomly resampling from the data. Formally. let I be a random
subsample of y1,y2,...,yn of size Ln/2j drawn without replacement. The
selection probability for variable X) is deﬁned as

11:, =P{xi 90(1)}. (10)

 

3295

112 /3.Io's[Burno[p.IOJxosorwurJOJurorq”:duq uror} papaolumoq

9103 ‘Og anﬁnv uo ::

S.Ji

 

where AMI) denotes the set of variables that have been selected when I
is used as the sample and the regularization parameter is set to A. Note
that this deﬁnition of A)‘(I) is independent of the speciﬁc method used
for variable selection. The probability in Equation (10) is with respect to
both the random sampling and other sources of randomness such as that
induced by the algorithm as we discuss below. For every variable X), the
stability path is given by the selection probabilities 121;,A e A. It has been
shown in Meinshausen and Buhlmann (2010) that 100 random resampling
is sufﬁcient to obtain accurate estimates.

Based on the selection probabilities, stable variables can be deﬁned. For
a cutoff nlhr with 0 < nlhr <1 and a set of parameters A, the set of stable
variables are deﬁned as

Sslable 2 {xi : max(  ) Z nlhr}. (11)
AeA ’
By choosing the set of stable variables under the control of the cutoff nlhr, we
keep variables with a high selection probability and discard those with low
selection probabilities. It has been show that the results of stability selection
vary little for sensible choices of the cutoff nlhr and the parameter set A.

It has also been shown that performance can be further improved if
additional randomness is introduced into the lasso problem in Equation (9). In
particular, we can randomize the amount of regularization for each variable
by solving the following problem:

A . _» IWkl
= ~—Y ’ 2 1 —, 12
w argwgﬁglny. wn + Z, Ck ( >
keDg’
where Dgi={1,...,i—1,i+1,...,a’}, c,» are IID random variables in [01,1]

and 01 e (0, 1] is a user-speciﬁed weakness factor.

4 RESULTS AND DISCUSSION

4.1 Experimental setup

In this article, we use a set of expression volumes for 3012 genes
documented in the coronal sections as in Bohland et (11. (2010). This
set of genes exhibit restricted expression patterns and thus are of high
neurobiological interest. For anatomical network modeling, we only
use the left hemisphere voxels, since only this part of the brain is
annotated in ARA. This gives rise to a 4D tensor of size 3012 X
67 X 41 X 33 in which the ﬁrst index corresponds to genes, and the
other three indices represent the rostral—caudal, dorsal—ventral and
left—right spatial directions, respectively. In tensor factorization, we
keep the dimensionality of the last three modes while reduce the
dimensionality of the ﬁrst mode, since we are interested in modeling
the relationships among brain voxels. For genetic network modeling,
we use the full volumes, and the size of our 4D tensor is 3012 X 67 X
41 X 58. In this case, we keep the dimensionality of the ﬁrst mode
while reducing the dimensionality of the other three modes.

The computational experiments were performed on a Cluster
consisting of 256 cores and 512 GB RAM. The lasso formulation was
solved using the SLEP package (Liu et (11., 2009). We can determine
the A value that enforces w to be a zero vector in Equation (9) (Liu
et (11., 2009), and this A value is denoted as Amax. Then we
set A ={0.1Amax,0.2Amax, ...,0.9Amax}. The selection probabilities
were estimated on 100 random resampling, and the weakness
factor a was set to 0.8. The sizes of reduced data were set to
retain 90 and 80% of the original information for anatomical and
genetic network modeling, respectively, based on the computational
resource requirements. Speciﬁcally, the size of the reduced tensor is
179 X 67 x41 X 33 in anatomical network modeling and is 3012 X
22 X 13 X 19 in genetic network modeling.

 

Fig. 3. Sample correlation patterns from the coronal View when the cutoff
nlhr=0.3. The vertices are color-coded according to the ARA annotations.
Each vertex is labeled with the ARA informatics ID of the brain structure,
and the corresponding structure name is given in Supplementary Table S2.

4.2 Results on anatomical network modeling

Computational modeling of the anatomical organization in the
mouse brain yields a graph in 3D space in which the vertices
represent brain regions, and the edges Characterize the expression
correlations between regions. The correlation patterns can be
visualized by showing slices of the 3D brain network on 2D planes.
Figure 3 shows one slice of the brain network along the coronal
section. We can observe that most of the edges connect adjacent
regions, showing that spatially adjacent regions tend to exhibit
correlated expression patterns. Note that these correlation patterns
are learned without knowing the spatial locations of voxels.
Although most of the edges connect spatially adjacent regions,
there are apparent exceptions. A slice-by-slice examination of the
entire anatomical networks at multiple cutoffs reveal that the voxels
annotated as dentate gyrus (DG) in the ARA are highly correlated to
many voxels in distant regions as shown in Figure 4. According to
Classical neuroanatomy, the DG plays an important role in learning
and memory by processing and representing spatial information,
and it has always been a topic of intense interest (Scharfman, 2007).
It has been shown that the DG receives multiple sensory inputs
including vestibular, olfactory, visual, auditory and somatosensory
from its upstream perirhinal cortex and entorhinal cortex. It plays
the role of a gate or ﬁlter, blocking or ﬁltering excitatory activity
from the inputs and controlling the amount of excitation that is
propagated to the downstream hippocarnpus (Scharfman, 2007). A
Close examination of Figure 4 shows that the correlation patterns are
largely consistent with those Classical results. A more quantitative
analysis of the results show that the correlation patterns obtained
solely based on gene expressions match well with the known
functions of DG. In particular, the expression patterns of the
DG is highly correlated to those of the cerebral cortex and the
main olfactory bulb, which provide sensory inputs to DG. In
addition, DG is highly correlated to the hippocampal region and
the retrohippocampal region, propagating the ﬁltered signals to its
downstream regions. We also observe that the intra-DG correlations
dominate, demonstrating again that most of the edges connect
spatially adjacent regions. Besides the correlations with known

 

3296

112 /3.Io's[Burno[p.IOJxosorwurJOJurorq”:duq uror} papaolumoq

9103 ‘Og isanV uo ::

Computational network analysis

 

 

 

/
I
M!

 

 

 

 

 

 

 

 

 

 

 

 

see
ones

0 c

o.

.

eeoeo
eeoeoe
oeoeoo

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Fig. 4. Slices of the correlation patterns in the coronal (left), sagittal (middle) and horizontal (right) views when nlhr = 0.4 (top) and 0.5 (bottom). Each vertex
is labeled with the ARA informatics ID of the brain structure, and the corresponding structure name is given in Supplementary Table S2.
The region with the largest number of connections corresponds to the brain structure dentate gyrus.

functions, our modeling of the anatomical networks also identiﬁes
many new relationships with DG that are not known from Classical
anatomical studies.

Based on the obtained networks in 3D space, a variety of network
analysis and visualization techniques can be employed to analyze
the anatomical organization in the mouse CNS. In Bohland et (11.
(2010), the K -means algorithm is used to Cluster the brain voxels
into groups based on dimensionality reduced expression data, and
a metric known as the S index was employed to quantitatively
Characterize the correspondence of the Clustering results with the
Classical anatomy reﬂected in the ARA annotations. Speciﬁcally,
let R: {r1 , ...,rN} be a partition of the set of brain voxels in
which each rl- comprises the set of indices of the voxels that map
to that Cluster (or anatomical label). The spatial overlap between
a region from the ARA and the Clustering result is deﬁned as:
Pij= lriﬂrjl/lrjl. From the Pl-j values that are computed over all
pairs of ARA regions and Cluster result, we can then derive a global
scalar index of similarity between the two partitions. Since Pl-j #Pji,
Xij is deﬁned as Xi~=max{Pl-J~,Pji} along with Wl-j=Ul-j/ZUl-j,
where Ul-j=min{|rl-|,|rj|} if Xl-j>0 and 0 otherwise. Finally, the
S index is deﬁned as S: 1 —4Zij Wl-le-j(1 —Xl-j).

To compare our network modeling method with the K—means
Clustering, we apply the leading eigenvector community detection
algorithm proposed by Newman (2006) and treat each detected
community as a Cluster. Since different cutoff values mm in the
stability selection yield different graphs, we vary mhr from 0.5 to
0.85 and detect communities from each of the resulting graphs. We
then run K -means with K equal to the number of communities so that
the results are comparable. Since the results of K -means depend on
the initialization, we run this algorithm 10 times and Choose the one
with the best result. We compute the S index for each case and report

the results in Figure 5. We can observe that the community detection
results consistently give higher S index values, indicating that the
structures of our anatomical networks are in higher accordance
with the Classical anatomy. We also plot the number of detected
communities as the cutoff Changes in Figure 5. We can see that the
number of communities lies approximately between 100 and 250,
which is largely in correspondence with the number of structures in
Classical anatomy. Detailed results on community identiﬁcation are
provided in the Supplementary Material.

The Classical anatomy was created mainly based on brain
functions. Since functions are mainly determined by gene
expression, the expression patterns within anatomical structures
should be more correlated than those across structures. To validate
this hypothesis, we show the distribution of the edges within and
across the anatomical structures when nlhr=0.5 in Figure 6. We
also show the number of edges within and across structures when
the cutoff varies from 0.2 to 0.9. We can observe that the edges
within structures dominate in all cases, indicating that the expression
patterns within Classical anatomy are highly correlated. We can
also observe from Figure 6 that the proportion of edges within
anatomical structures increases as the cutoff increases. This indicates
that most of the cross-structure edges have relatively small selection
probabilities, and they are removed as the cutoff increases. The
ranked lists of regions in terms of the number of connections are
provided in the Supplementary Material.

4.3 Results on genetic network modeling

Modeling of the gene interactions using the techniques described in
Section 3 yields a network consisting of 3012 vertices in which
vertices represent genes, and edges Characterize the correlations

 

3297

112 /3.Io's[Bruno[p.IOJxosorwurJOJurorq”:duq uror} pepBo1umoq

9103 ‘0g1sn8nv uo ::

S.Ji

 

   
 
   

 

 

-9- Community
—6— K—means

 

 

 

 

 

 

Cutoff

'0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85

 

m
o.) o.)
o 01
o o

N
01
O

200 -

150-

Number of communities

100

 

 

 

50 ' ' ' ' ' '
0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85

Cutoff

 

Fig. 5. Comparison of the communities detected in the anatomical networks and the K -means clustering results. (A) Shows the S index comparison between
the anatomical structures in ARA and the results of community detection and K -means. (B) Shows the number of communities as the cutoff changes.

A (Scaled) number of edges within and
between regions

 

  

Mouse brain regions

 

 

 

Mouse brain regions

Number of edges

5

 

 

 

 

 

25 X 10
2 - Between region
-Within regions
1.5
1
0.5

 

 

 
                 

0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Cutoff

Fig. 6. Visualization of the edge distribution within and between different brain regions. (A) Shows the distribution of the edges within and across the
anatomical structures when nlhr=0.5. The rows and columns correspond to the structures annotated in the ARA. This matrix is normalized to the interval
[0, 1] row by row, and hence it is not symmetric. Each row indicates the proportion of a particular structure’s edges that connect to other structures. In particular,
each entry (i, j) in the matrix represents the proportion of structure i ’s edges that connect to structure j. (B) Shows the number of edges within and between

anatomical structures in ARA as the cutoff changes.

between genes. Since genes involved in the same pathway usually
exhibit similar expression patterns, correlated expression patterns
may imply similar biological functions. We hence use Gene
Ontology (GO) (Ashburner et (11., 2000) to evaluate the functional
relationships among tightly connected genes in the network. In
particular, we consider a gene and its direct neighbors as a group
(Gustafsson et (11., 2005) and evaluate the functional enrichment
of each group using the hypergeometric distribution (Boyle et (11.,
2004). We apply Bonferroni correction for multiple hypothesis
testing and consider GO terms with corrected P < 0.05 as statistically
signiﬁcant (Boyle et (11., 2004). We vary the cutoff mm and observe
that most of the groups are annotated with at least one statistically
signiﬁcant GO term. In particular, when mhI=0.5, there are 2702
groups annotated with at least one statistically signiﬁcant GO term,
and the average number of terms per group is 15. This indicates that
most of the groups are associated with multiple enriched terms.

It has been previously observed that the degrees of many
biological networks follows a power-law distribution (Barabasi

and Oltvai, 2004). This indicates that there exists a small number
of highly connected genes known as hubs. We vary the cutoff
and observe that the set of highly connected genes are largely
consistent (details provided in the Supplementary Material). We
report the top 10 genes with the largest number of connections
in Table 1 when nlhr=0.8 and show slices of their expression
patterns in the Supplementary Material. We can observe that all
these groups are highly enriched with the biological function
binding or protein binding, implicating that they are likely to
encode transcription factors. Among these 10 genes, the APP
encodes an integral membrane protein expressed in many tissues and
concentrated in the synapses of neurons. Homologous proteins have
been identiﬁed in other organisms such as Drosophila, C.e1eg(1ns
and all marnmals. APP is best known for its association with
the Alzheimer’s disease, and mutations in critical regions of APP
cause familial susceptibility to Alzheimer’s disease. It would be
interesting to investigate how the ‘hubness’ of APP is related to
CNS disease.

 

3298

112 /3.Io's[Bruno[p.IOJxosorwurJOJurorq”:duq uror} pepBo1umoq

9103 ‘0g1sn8nv uo ::

Computational network analysis

 

Table 1. Top 10 genes with the largest number of connections when
nthI=O8

 

Gene No. of neighbors GO molecular function Corrected P-value

 

App 274 Binding 8.00e-31
st 231 Binding 2.50e-21
Acsl5 219 Binding 6.00e-20
Nrgn 155 Protein binding 5.66e-24
Acadvl 95 Binding 1.68e-05
Sytl 94 Protein binding 4.36e-14
Chn2 82 Protein binding 1.15e-07
Btgl 69 Binding 5.74e-08
Eef1a1 59 Binding 1.79e-06
Apoe 54 Binding 6.76e-06

 

The molecular function and corrected P-values are also shown.

5 CONCLUSONS

We model the anatomical and genetic organizations in the
mouse brain as networks. To enable robust and efﬁcient network
construction, we employ tensor factorization techniques to reduce
the data volumes. The resulting networks recover known relations
and predict novel correlations not known from the literature. The
employed network modeling formulation is an approximate scheme.
It would be interesting to compare this approximate formulation
with the exact one on small datasets, where exact optimization can
be applied. The proposed methods can be applied to model other
biological systems, such as the Drosophila transcriptional networks.
We will explore the network modeling of other biological systems
in the future.

Funding: This work was supported by Old Dominion University.

Conﬂict of Interest: none declared.

REFERENCES

Ashburner,M. et al. (2000) Gene Ontology: tool for the uniﬁcation of biology. Nat
Genet, 25, 25729.

Banerjee,O. et al. (2008) Model selection through sparse maximum likelihood
estimation for multivariate Gaussian or binary data. J. Mach. Learn. Res., 9,
4857516.

Barabasi,A.-L. and Oltvai,Z.N. (2004) Network biology: understanding the cell’s
functional organization. Nat Rev Genet, 5, 1017113.

Bohland,J.W. et al. (2010) Clustering of spatial gene expression patterns in the mouse
brain and comparison with classical neuroanatomy. Methods, 50, 1057112.

Boyle,E.I. et al. (2004) GO::TermFinder4)pen source software for accessing Gene
Ontology information and ﬁnding signiﬁcantly enriched Gene Ontology terms
associated with a list of genes. Bioinformatics, 20, 371e3715.

Biihlmann,P. (2004) Bagging, boosting and ensemble methods. In Gentle,J. et al. (eds)
Handbook of Computational Statistics: Concepts and Methods. Springer, Berlin,
Heidelberg, Germany, pp. 8777907.

Dempster,A.P. (1972) Covariance selection. Biometrics, 28, 1577175.

Dong,l-I.W. (2009) The Allen Reference Atlas: A Digital Color Brain Atlas of the
C5 7BL/6J Male Mouse. John Wiley & Sons, Inc., Hoboken, New Jersey.

Edwards,D. (2000) Introduction to Graphical Modelling, 2nd edn. Springer, New York,
Inc.

Friedman,J. et al. (2008) Sparse inverse covariance estimation with the graphical lasso.
Biostatistics, 9, 432441.

Friedman,J. et al. (2010) Applications of the lasso and grouped lasso to the estimation
of sparse graphical models. Technical Report. Department of Statistics, Stanford
University, Stanford, CA.

Gustafsson,M. et al. (2005) Constructing and analyzing a large-scale gene-to-
gene regulatory network-lasso-constrained inference and biological validation.
IEEE/ACM Trans. Comput. Biol. Bioinformatics, 2, 25L261.

Jones,A.R. et al. (2009) The Allen Brain Atlas: 5 years and beyond. Nat Rev Neurosci,
10, 8217828.

Kolda,T.G. and Bader,B.W. (2009) Tensor decompositions and applications. SIAM Rev,
51, 4557500.

Lathauwer,L.D. et al. (2000a) A multilinear singular value decomposition. SIAM J.
Matrix Anal. Appl., 21, 125371278.

Lathauwer,L.D. et al. (2000b) On the best rank-1 and rank-(R1,R2,~~~,RN)
approximation of higher-order tensors. SIAM J. Matrix Anal. Appl., 21, 132L1342.

Lein,E.S. (2007) Genome-wide atlas of gene expression in the adult mouse brain.
Nature, 445, 1687176.

Liu,J. et al. (2009) SLEP: Sparse Learning with Eﬁicient Projections. Arizona State
University, Tempe, Arizona, USA.

Meinshausen,N. and Buhlmann,P. (2006) High-dimensional graphs and variable
selection with the lasso. Ann. Stat, 34, 143G1462.

Meinshausen,N. and Buhlmann,P. (2010) Stability selection. J. R. Stat. Soc. Ser B Stat.
Methodol, 72, 417473.

Newman,M.E.J. (2006) Finding community structure in networks using the eigenvectors
of matrices. Phys. Rev E, 74, 036104.

Ng,L. et al. (2007) Neuroinforrnatics for genome-wide 3-D gene expression mapping
in the mouse brain. IEEE/ACM Trans. Comput. Biol. Bioinformatics, 4, 3827393.

Omberg,L. et al. (2007) A tensor higher-order singular value decomposition for
integrative analysis of DNA microarray data from different studies. Proc. Natl Acad.
Sci. USA, 104, 18371718376.

Scharfman,H.E. (2007) The Dentate Gyrus: A Comprehensive Guide to Structure,
Function, and Clinical Implications. Elsevier Science, Amsterdam, The
Netherlands.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. R. Stat. Soc.
Ser B, 58, 2677288.

Vasilescu,M.A.O. and Terzopoulos,D. (2004) TensorTextures: multilinear image-based
rendering. ACM Trans. Graph, 23, 3367342.

Wang,H. et al. (2005) Out-of—core tensor approximation of multi-dimensional matrices
of visual data. ACM Trans. Graph., 24, 5277535.

Whittaker,J. (1990) Graphical Models in Applied Multivariate Statistics. Wiley,
New York, NY.

Wille,A. et al. (2004) Sparse graphical Gaussian modeling of the isoprenoid gene
network in Arabidopsis thaliana. Genome Biol., 5, R92.

Wrede,R.C. (1972) Introduction to Vector and Tensor Analysis. Dover Publications,
Mineola, NY.

Ye,J. (2005) Generalized low rank approximations of matrices. Mach. Learn, 61,
1677191.

Yuan,M. and Lin,Y. (2007) Model selection and estimation in the Gaussian graphical
model. Biometrika, 94, 19735.

 

3299

112 /3.IO'S[1211an[pJOJXO'SOIJBLUJOJIIIOIqﬂIdllq uror} pepeo1umoq

9103 ‘0g1sn8nv uo ::

