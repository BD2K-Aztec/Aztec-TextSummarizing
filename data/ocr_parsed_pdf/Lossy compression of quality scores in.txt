BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Lossy compression of quality scores

 

Table 1. Example quality scores using Phred + 33

 

 

 

Letter Quality value Estimated error probability (%)
( 40 20
7 55 0.6
F 70 0.02
U 85 0.0006
61 100 0.00002
SEQ

 

IAATGCCTAAGCGTTAGI

 

QUAL

 

IFFEGGGGGFHHFFFDEI

 

Value :

 

I70706971717171717072727070706869I

 

Fig. 1. Example of a sequence and quality components. The value of each
quality score is also shown

If lossy compression is being considered, the maximum degree
of information loss that can be tolerated needs to be speciﬁed as
an input to the compressor. That requirement presupposes that a
measure of loss has been defined, taking as its inputs two data
sequences of the type being represented and providing (using
some meaningful unit) a numeric score that summarizes the
degree to which they differ. Table 2 shows some of the measures
that can be applied to pairs of Z—element strictly positive se—
quences (Cha, 2007). In all of them, if the two sequences are
identical, the score is low (0, or, in the case of Maszin
Distance, 1); when the two sequences are different, the measure
is higher. For example, for the two [=6—element sequences
X = (70, 69, 70, 71, 69, 73) and Y = (70, 70, 70, 70, 70, 73), the
Mean Manhattan Distance is 0.5. The measures shown in
Table 2 will be used in Sections 3 and 4 to evaluate, and in
some cases control, the accuracy of lossy compression regimens
for quality score sequences.

1.3 Bit encodings

Once the set of symbols to be coded has been determined, the
compressor encodes each symbol into a bit sequence. A fixed—
length binary code or variable—length prefix—free code is used to
represent the symbols contained in the data, with the choice
dependent on the probability distribution that arises over the
set of symbols.

The simplest variable—length code is Unary, which represents a
non—negative number x as x l—bits, followed by a 0—bit. Unary has
the advantage of being able to represent arbitrarily large values,
but requires as many bits as the value being coded. The Elias
Gamma code is more complex, but also more effective, with a
value x represented as the concatenation of the length of the
binary representation of x in Unary, and the binary
representation of x+ 1, omitting its most signiﬁcant bit

Table 2. Examples of fidelity measures between strictly positive numeric
vectors X and Y of length (3 [adapted from Cha (2007)]

 

 

 

Measure Function
l
Mean Manhattan Distance(X, Y) % lXi — Yil
[:1
Max : Min Distance(X, Y)  
l
Mean Squared Error(X , Y) i (X i — Yi)2
i=1
ChebyshevDistance(X, Y) 1ma); |Xi — Y,»|
525
l
. . iXt" Yii
‘ ID. t X —Z‘:'
Soeige IS‘ ance( , Y) Z}; max“) Y”

z
Lorentzian Distance(X, Y) Z log2(l + |X,» — Yil)
i=1

 

Note: Scores of zero indicate that X and Y are identical except for the Maszin
measure, where one is the minimum value.

(Elias, 1975). The Golomb code also consists of a Unary/
Binary combination, but makes use of a parameter b, with the
unary component storing the quotient q = Lx/bJ , and a minimal—
length binary component storing the remainder r = x — b-q. The
parameter b is chosen according to the characteristics of the set of
x values to be stored. Table 3 shows examples of these codes,
covering the first six integers. Note that the 3—bit Binary code is
limited to inputs 0 g x<8 and assumes that all of the symbols
have equal probability; the Unary, Gamma and Golomb codes
assume that the symbol probabilities are non—increasing and
place no upper limit on the values x that can be represented. If
the symbol probabilities are not non—increasing as the distance
from the origin increases, a permutation table can be used so that
monotonicity can be maintained. Moffat and Turpin (2002) de—
scribe all of these mechanisms in detail.

1.4 Subsequent applications

One use of sequence data is to identify locations of variation
from a specified reference sequence. These variations can assist
with the determination of the genotype for each individual at
each site, allowing, for example, the study of diseases (Nielsen
et al., 2011). The process of finding variations is known as single
nucleotide polymorphism (SNP) and insert/deletion (indel) detec—
tion. The normal output of this process is a set of SNPs and
indels, stored in a standard format such as variant call format
(VCF) (Danecek et al., 2011). Each line in a VCF file represents a
different variation, storing its position and allele in the reference
genome, plus the bases related to the variation, a computed qual—
ity score and extra information about the variation. We use com—
parisons between generated VCF files as an applied evaluation of
the effect that lossy compression of quality scores has on se—
quence data use.

2 PREVIOUS APPROACHES

The SLIMGENE tool presented by Kozanitis et al. (2010) was
one of the first methods for independent compression of quality

 

2131

ﬁm'spzumoﬁuqxo'sopeuuopuoiq/ﬁdnq

R.Ca'novas et al.

 

Table 3. Examples of Binary, Unary, Gamma and Golomb codes for the
integers (P6, assuming that values X z 0 are to be coded

 

 

Integer Binary Unary Gamma Golomb
0 000 0 0 0 0

1 001 10 10 0 010

2 010 110 101 011

3 011 1110 110 00 100

4 100 11110 110 01 1010

5 101 111110 11010 1011

6 110 1111110 11011 1100

 

Note: The Binary code shown here uses 3 bits per integer (and hence has an upper
limit of x = 7); the Golomb code is constructed using I) = 3.

scores. Kozanitis et al. give two compression methods, both
based on the observation that neighboring quality values are,
in general, close to each other. Their ﬁrst method encodes the
differences between consecutive quality values using a Huffman
code, while the second approach inserts a Markov model before
the Huffman code. Deorowicz and Grabowski (2011) observed
that the quality sequences were quasi—random with mild depend—
ence on score position, that some sequences ﬁnish with several ‘#’
characters and also that there can be strong local correlations
within individual records. In their proposal, a bit ﬂag is stored
per sequence to indicate whether the sequence is preﬁxed by a
string of hashes. The hash preﬁx is removed and the rest of the
sequence is stored using a Huffman code. They also proposed
storing a modiﬁed version of the quality sequence, where equal
consecutive values are stored as runs. For example, the sequence
‘FFEGGGGGFHHFFFDE’ shown in Figure 1 would be trans—
formed to ‘FEGFHFDE’, plus a list 1,0,4,0,1,2,0,0 of run—lengths.
The new sequence and the vector of run—lengths are then stored
using Huffman codes.

Wan et al. (2012) examine several compression methods for
compressing the quality ﬁeld, and propose a new mechanism in
which quality scores are transformed into a bin number, with
each bin representing an interval. They present three lossy trans—
formations: UniBinning, Truncating and LogBinning. The
UniBinning approach creates uniform splits across the error
probability distribution, that is, if the parameter is four (and
two—bit Binary codes will be used), the ﬁrst bin spans all quality
values corresponding to error probabilities between 0 and 25%.
The Truncating transformation separates all the quality values
independently, creating a special bin for the 1 largest values, with
I being a user—defined parameter. The LogBinning transform—
ation is similar to UniBinning, but with the bins formed accord—
ing to the logarithms of the probabilities, by constructing
equal—sized segments according to quality values.

After applying a binning transformation, Wan et al. form
groups of k mapped quality values into a block, and preﬁx the
block with a header containing some local information, depend—
ing on the lossless method used. Their MinShifting transform—
ation stores the minimum quality value (1mm of each block, and
the k values (1 of the block are each replaced by q — (1min before
being coded. The Frerrdering transformation permutes all the
quality values in each block according to their frequency, so that

the most commonly occurring values are assigned the shortest
codes when Gamma or Golomb are used. The third option con—
sidered by Wan et al. is GapTranslating, in which all the values
of a block are changed to the difference—values presented by
Kozanitis et al. Finally, after a lossy transformation and then a
lossless mapping, Wan et al. use one of the coding regimens
summarized in Section 1.3.

Ochoa et al. (2013) describe the QualComp tool, which com—
presses quality scores based on the assumption that they are
drawn from a Gaussian distribution (Lapidoth, 1997) parameter—
ized by the position of the quality value in the quality read,
assuming that there is a mean and standard deviation for each
position i e [1, Z] in the read. The mean and covariance matrix of
the quality scores are extracted from the entire input, and singu—
lar value decomposition is applied to the covariance matrix to
recover standard deviations to be used as parameters of the
Gaussian. Once those parameters are decided, they are stored
for use by the decoder, and used to derive the number of bits
used to encode each quality value, by minimizing the mean
square error of a rate distortion problem. This optimization
problem is parameterized by a user input, which is the total
number of output bits the quality scores can consume, expressed
as a rate r. The solution to the problem speciﬁes, for position i in
a read, the number of bits, ,o,-, that should be used to encode the
quality value in position i. For each quality value, if it is to be
coded in ,0,- bits, the Gaussian for that position is split into 2"i
regions of equal probability, and a bit pattern selected to indicate
the region in which the quality value lies. The decoder reads in
the Gaussian parameters and r, performs the optimization to
recover the ,0,- values and then emits a representative correspond—
ing to the region speciﬁed by each code word.

Other approaches compress the quality scores depending on
information of the read sequences. Tembe et al. (2010) describe a
simple lossless compression technique over the sequence and the
quality ﬁelds, combining all distinct pairs of genomic base and
respective quality value, and then using a Huffman code. That is,
the DNA sequence and quality scores of an alignment are rep—
resented by a joint sequence of code words, one per each
(base, quality) pair. Janin et al. (2013) present a lossy compres—
sion scheme based on the premise that, if a base in a read can be
predicted by the context of the read, then the base and the re—
spective quality need not be stored. To compute predictions, they
use the BurrowsiWheeler transform (Burrows and Wheeler,
1994) and a longest common prefix array, together with a user—
deﬁned minimum context length.

We also consider CRAM format files (Fritz et al., 2011). These
files are generated using CRAMTOOLS, a suite of Java programs
and interfaces that compress DNA sequence and quality data
stored in SAM/BAM format, making use of an external refer—
ence genome. Sequences are stored as a difference relative to the
external reference, and hence the external reference genome must
be provided each time compression or decompression is under—
taken. Several lossy options for storing quality scores are pro—
vided in the CRAMTOOLS suite. We explored three combinations:
Preserve, Bin—Preserve and Match—Bin—Preserve. The Preserve
mode only stores quality scores for reads whose mapping quality
scores are higher than a user—deﬁned threshold. The other two
approaches are variations of Preserve, in which the prefix Bin
means that the quality scores are stored using eight bins like Wan

 

2132

ﬁm'spzumoﬁuqxo'sopeuuopuoiq/ﬁdnq

Lossy compression of quality scores

 

et al.’s LogBinning; prefix Match indicates that only quality
scores of bases that match with the reference are retained.

3 ALGORITHM

We now present a lossy compression approach based on loca—
lized properties of the quality scores, following the lead estab—
lished by Kozanitis et al. (2010). The new algorithms represent
quality scores by separating them into blocks of variable size,
where all the values contained in each block comply with a
chosen parameter p according to some measure criterion. For
each block, we store its length and a representative value. The
representative value also depends on which measure is used. For
example, if Mean Manhattan Distance (Table 2) is used as the
ﬁdelity measure, blocks must be formed such that none of the
values in each block are more than p from the corresponding
representative value. That is, one possible conﬁguration allows
blocks to span a range of up to 2p+ 1 different quality scores,
with the midpoint of each block’s range taken as the representa—
tive value. Generalizing from this starting point, the use of other
measures, or other values of p, results in different blocks being
formed and hence different amounts of space being required for
the set of quality scores, with different degrees of approximation
to the original values.

 

Algorithm 1: Inputs quality[0 . . .13 — l] and a threshold p.

 

l: Representatives <— []

: RunLengths <— []

: block <— []

: bloc/(Jen <— 0

'fori=0—>IZ—ldo

if FulfillCriteria(block G9 quality[1], p) then
block <— block 69 quality[1]
bloc/(Jen <— blockJen + l

9: else

10: rep <— CalculateRepresentative(block)

l l: Representatives <— Representatives G9 rep

l2: RunLengths <— RunLengths G9 bloc/(Jen

13: black <— quality[1]

l4: bloc/(Jen <— l

15: end if

16: end for

17: Representatives <— Representatives G9 rep

18: RunLengths <— RunLengths G9 bloc/(Jen

19: return Representatives and RunLengths

90$???“me

 

Algorithm 1 describes the proposed process, with ‘|]’ used to
represent the empty list, and with the ‘69’ operator used to
append a single value to a list of the same type of object. To
determine the block boundaries, a left—to—right greedy search is
undertaken, seeking to make each block as long as possible,
consistent with the constraints imposed by the chosen fidelity
criterion and the chosen fidelity limit p. Method FulfillCriteria
determines whether it is possible to include the next quality value
in the current block, or whether it is necessary to start a new
block; method CalculateRepresentative computes the best repre—
sentative value for a given block, given that at least one valid
representative exists. For example, if the measurement metric

QUAL

 

IFFEGGGGGFHHFFFDEI

 

Value :

 

I 70 7o 69 71 71 71 71 71 7o|72 72 70 7o 70|68 69|

 

Representatives:
70 71 68
Run-Lengths:
9 5 2

Fig. 2. Quality values being formed into blocks consistent with a Mean
Manhattan Distance guarantee (the approach used in the P-Block trans-
formation) when p: 1

being used is Mean Manhattan Distance, then
FulfillCriteria(qvals, p) is implemented by computing

(max qvals[i]> — (min qvals[i]> 3 2p
where the range of i is 0 g i < |qvals|. Within a set that meets this

criterion, the value

_ (max,~ qvals[i]) + (min,~ qvals[i])
_ 2
guarantees that the Mean Manhattan Distance for the whole

sequence will be not greater than p. For any given block, there
may also be other integer values rep/ for which

rep

max |qvals[i] — rep/l S P
I

and in that case, method CalculateRepresentative is free to use a
secondary criterion, such as minimizing the average distance be—
tween the selected representative and the block’s values.

We call this optioniin which Mean Manhattan Distance is
used in both FulfillCriteria and CalculateRepresentativeithe
P—Block mechanism and control it with the parameter p.
Figure 2 illustrates the application of P—Block to the example
sequence.

As a second example of how Algorithm 1 might be instan—
tiated, if a given upper bound r must be guaranteed using the
Max:Min Distance metric, then FulfillCriteria (qvals, r) needs to
check if an integral rep exists such that (max, qvals[i]) / rep < r and
rep/(miniqvals[i]) <r. This R—Block approach, parameterized by
a maximum ratio r, recognizes that it may be preferable to be
more precise in representing low quality scores (when the base is
assessed as being more likely to be in error) than high ones.

After either of these transforms has been applied to an
alignment, the k elements in the Representatives array are
stored as a minimum and maximum, followed by k values
relative to that range, using a Binary code of
[log2(maxk Representatives[k] — mink Representatives[k] + 1)‘|
bits per value. The k values in the RunLengths array are stored
using the Gamma code. When p or r is sufﬁciently large, only one
value is stored for each read, covering every quality value in that
alignment.

In related work, we have examined compression techniques for
the other components of SAM/BAM—format ﬁles (Canovas and
Moffat, 2013), including the bases, and added a small index that

 

2133

ﬁm'spzumol‘pioyo'sopeuuopnoiq/pdnq

R.Ca'novas et al.

 

Table 4. Test sequences: quality scores were extracted from two SAM
ﬁles, NA12878 .chr20 .qual, and HG01477 .chrll .qual

 

 

File Size (MB) Reads Read length H0
NA12878.chr20.qual 5017.98 51585658 101 3.384
HG01477.chrll.qual 617.22 6407925 100 3.937

 

Note: The columns show the size of the sequence of extracted quality scores as
ASCII bytes, the number of reads, the length of each read and the entropy (H0,
in bits per quality score) of the quality sequence.

allows decoding to commence at every 500th read. The space
used to store the index information is a small fraction of the
total space required, and the functionality that it offers is not
part of the experimentation that is reported here.

4 IMPLEMENTATION

We compare the quality scores of the original ﬁle against the
quality scores in a reconstructed file using different approaches
and a range of fidelity measures (Table 2). The ﬁdelity of a
mechanism is computed by averaging all of the per—read scores
generated for that measure. The quality scores associated with a
version of the NA12878 genome are the primary test ﬁle used. In
particular, we work with reads that were associated with
chromosome 20 (That is, we extracted the quality score fields
from the file NA12878.Hi.WGS.bwa.cleaned.recal.hgl9.20.bam
fetched from https://usegalaxy.org/library_common/browse_li—
brary?show_deleted=False&cntrller=library&use_panels=False
&id=f9ba60baa2e6ba6d.). To further validate our results, we
also take genome HG01477 from the 1000 Genomes Project
(ftp://ftp.1000genomes.ebi.ac.uk/voll/ftp/), using the reads that
were aligned with chromosome 11; those results are presented in
the Supplementary Material. Table 4 gives details of two ﬁles of
sequence quality values. As part of the processing pipeline used in
the experimentation, we also made use of reference chromosomes
chrll and chr20 (Accessed from ftp://ftp.ncbi.nlm.nih.gov/sra/
reports/Assembly/GRCh37—HGl9_Broad_variant/).

The implementations explored include Wan et al.’s
LogBinning and UniBinning, the QualComp mechanisms and
our P—Block and R—Block approaches. We also include the
CRAMTOOLs lossy models, in recognition of their extensive use
in data repositories such as the 1000 Genomes Project. To estab—
lish a benchmark for lossless compression, results for GZip 79
are also included. Figure 3a shows the various trade—offs mea—
sured when ﬁdelity is assessed using Mean Manhattan Distance;
Figure 3b shows the same trade—off, but with the vertical scale
changed to Max:Min Distance; and Figure 3c likewise, but with
ﬁdelity measured using Mean Squared Error.

Each of the implementations provides a trade—off ‘knob’
that adjusts the balance between ﬁdelity and compression rate.
The graphs are plotted using QualComp rates r={0,0.5,1,2};
P—Block parameters p: {1,2,4,8,16,32}; R—Block ratios
r: 1 + {0.05,0.1,0.2,0.4,0.8,1.6,3.2,6.4}; UniBinning thresholds
17: {80,100}; and LogBinning thresholds b: {5,10,20,30,40,60}.
For CRAM, lower bound mapping qualities of 40, 50 and 60

Preserve O
Bin-Preserve E
12 Mgch-Bin-Preserve
UniBinning <>
LogBinning +
10 QualComp +
8 P-Block +
g R-Block +
175
5 8i
C
g
«1 6
.C
C
E
4 £
2 o
0 ﬂ
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Space in MB
Average Mean Manhattan Distance
b 6
() Preserve O
55' Bin-Preserve
Match-Bin-Preserve E
5 UniBinning <>
LogBinning +
El QualComp +
4-5 P-Block +

R-Block +

Max:Min Distance
00
U1

 

3
2.5
2
1.5
1 v ~ 5
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Space in MB
Average Max:Min Distance
0 200
() <> Preserve O
180 Bin-Preserve V
Match-Bin-Preserve El
160 O UniBinning <>
LogBinning +
140 QualComp +
P-Block +
Fl-Block +

Mean Square Error

 

n
v 7 7 v - v u
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Space in MB
Average Mean Squared Error

Fig. 3. Fidelity versus space trade-offs for the quality sequence
NA12878.chr20.qual. In the case of P-Block and R-Block, low
values of the parameter give points in the top left; as the parameter is
increased, the distance measure decreases and the space increases.
(a) Average Mean Manhattan Distance. (b) Average Max:Min
Distance. (c) Average Mean Squared Error

were used with the Preserve, Bin—Preserve and Match—
Bin—Preserve modes.

The three components of Figure 3 show that the QualComp,
P—Block and R—Block approaches outperform the other methods.
The new approaches are particularly well suited to the Max:Min

 

2134

ﬁle'sreumol‘pmJXO'sopeuuowrorq/pdnq

Lossy compression of quality scores

 

Distance, as was the intention, while Ochoa et al.’s QualComp
offers good performance when fidelity is quantiﬁed using Mean
Squared Error. Worth noting is that QualComp offers a further
modality, allowing the quality ﬁle to be separated into c clusters
and QualComp applied to each cluster separately. The results
obtained via this approach did not result in any appreciable dif—
ference in performance.

We also analyze the effect of lossy compression of quality
scores on a downstream application. For each lossy file gener—
ated, we compute its VCF (see Section 1.4) and compare it with
the VCF generated from the original ﬁle. To compare two VCF
ﬁles, we use the methodology of Ochoa et al. (2013) and deﬁne a
true positive (T P) as a variation that is found in both VCF ﬁles, a
true negative as a location at which neither ﬁle records a vari—
ation, a false negative as a variation that is only found in the
VCF of the lossless ﬁle and a false positive (FP) as a variation
that is found only in the VCF of the lossy ﬁle. The TP values are
further separated into half and equal TP, where equality is regis—
tered if the same variation is found in both VCF ﬁles, and half is
registered if a variation is found in the same position in both files,
but the type of variation is not exactly the same. In all our ex—
periments the amount of ‘half TP’ was no higher than 0.004% of
the total TP value, and they were included in the FP count with—
out affecting the results.

From these values the precision and recall of the processed se—
quence can be computed, given by Precision = T P/(T P + FF)
and Recall: T P/(T P+ FN), respectively; they in turn can be
combined into a single statistic known as an F—Score by combin—
ing them as F—Score = 2-Precision-Recall/ (Precision + Recall).
Table 5 provides precision and recall scores for a selected
range of methods applied to NA1287 8 . chr20 .qual, together
with the F—Scores computed from them; Figure 4 plots F—Score
as a function of compressed size, clearly illustrating the super—
iority of the new approaches. Similar results were obtained using
the HG01477 . chrll . qual ﬁle, included in the Supplementary
Material.

As Figure 4 and Table 5 illustrate, our two approaches out—
perform other techniques, that is, for a given level of precision or
recall, the P—Block and R—Block methods allow more compact
storage of quality values than do other mechanisms. For ex—
ample, if 99.0% in both precision and recall is regarded as
being a minimum threshold for having high—conﬁdence in the
outcomes, then of the methods compared, P—Block with p=4
generates the most compact representation. The 563 MB
required for the quality values in that conﬁguration is less than
one—third of the space required by the lossless GZip approach.
Figure 4 and Table 5 also include an additional point, labeled
OneQual, showing what would happen if no quality scores at all
were stored, and every quality value was assumed to be the aver—
age quality score from the input sequence.

Note that the different methods all have comparable encoding
and decoding speeds, and we do not compare them on that basis.

5 DISCUSSION

We have described and measured the performance of a range of
lossy compression techniques for quality scores. The QualComp
approach, and our two new methods, offer superior trade—off
options when ﬁdelity is assessed according to the measures

Table 5. Recall and precision percentages for VCF outputs generated
after different lossy methods are applied to the quality scores in the ﬁle
NA12878 . chr20 . qual

 

 

Method Size Variations Precision Recall F-Score
(MB) found
OneQual, q = ‘ ?’ = 63 0.0 83 859 97.7 95.2 96.4
R-Block, 924.2 138.7 86 971 98.3 99.3 98.8
P-Block,p= 16 141.5 87172 98.3 99.6 99.0
R-Block, 6 21.4 256.6 87 038 98.7 99.9 99.3
QualComp, r = 0.5 310.5 86 757 98.8 99.6 99.2
P-Block, p: 4 562.8 86 495 99.4 99.9 99.6
QualComp, r=1 621.1 86 543 99.1 99.7 99.4
R-Block, 6 = 1.10 784.6 86 353 99.6 99.9 99.8
CRAM Bin-Preserve, 50 821.4 86 593 98.9 99.5 99.2
LogBinning, b: 10 829.5 86 952 98.5 99.6 99.1
CRAM 867.6 88 008 97.2 99.4 98.2
Match-Bin-Preserve, 50
P-Block, p=2 970.4 86195 99.8 99.9 99.8
LogBinning, b: 20 1055.3 86 506 99.2 99.7 99.5
UniBinning, b = 100 1213.3 86 830 98.2 99.0 98.6
QualComp, r = 2 1242.2 86 355 99.5 99.8 99.7
CRAM, Preserve 50 1704.8 86 636 99.0 99.7 99.3
LogBinning, b: 60 1774.4 86 211 99.8 99.9 99.9
GZip 79 (lossless) 1901.0 86062 100.0 100.0 100.0

 

Note: The rows are ordered by increasing compressed size; scores >99% are
highlighted.

in
a)
0’.
o
in
8
a, .
5 o
o
‘1’
L
10
[x
0’.
O -.- LogBinning
'I- QualComp
“' P—Block
LD -0- R—Block
co
o, -v- OneQual
0' V
0 500 1000 1500

Size (Mb)

Fig. 4. Effect of lossy compression techniques on the quality of VCF
computation for NA12878 .chr20 .qual. Each line represents a differ-
ent lossy compression method; higher lines represent more desirable
methods

in Table 2. Our new approaches outperformed QualComp in
the Max:Min Distance criterionia measure that is well—suited,
we believe, to bioinformatics applications.

Our experiments also quantified the extent to which use of
lossy compression affected the performance of a typical down—
stream application of genetic data, and demonstrate that vari—
ation detection can still be reliably carried out, even with
relatively compact storage of the quality scores.

 

2135

ﬁle'sreumol‘pmJXO'sopeuuowrorq/pdnq

R.Ca'novas et al.

 

The approaches described here make use of sequentially
greedy generation of blocks. One interesting question that we
have not yet examined is whether mechanisms for globally opti—
mal block construction will make a measurable difference in
overall outcomes. Another area that we have not yet fully
explored is the coding mechanisms used for the block represen—
tatives and for the block lengths; it may be that tailored codes
(rather than Binary and Gamma) can offer further space savings
once the particular characteristics of the data streams are taken
into account. We are also interested in quantifying the effect
that lossy compression has on other downstream applications
of genetic sequencing data.

ACKNOWLEDGMENTS

The authors thank Idoia Ochoa—Alvarez for her assistance with
QualComp; Vadim Zalunin for helping with the CRAMTOOLs
usage; and Wei Shi and Jan Schroder for sharing their knowledge
of the area. Finally, we thank the anonymous referees for their
careful input.

Funding: NICTA is funded by the Australian Government as rep—
resented by the Department of Broadband, Communications
and the Digital Economy and the Australian Research Council
through the ICT Center of Excellence program. This work was
also funded by the Australian Research Council under the
Future Fellowship scheme.

Conflict of Interest: none declared.

REFERENCES

Ansorge,W. (2009) Next—generation DNA sequencing techniques. N. Biotechnol,
25, 1957203.

Burrows,M. and Wheeler,D. (1994) A block—sorting lossless data compression
algorithm. In: Technical report SRC—RR 124. Digital Equipment Corporation
Systems Research Center.

Canovas,R. and Moffat,A. (2013) Practical compression for multi—alignment gen—
omic ﬁles. In: Proceedings of 36th Australasian Computer Science Conference.
Adelaide, Australia, pp. 5140.

Cha,S. (2007) Comprehensive survey on distance/similarity measures between prob—
ability density functions. Int. J. Math. Models Methods Appl. Sci., 1, 30(P307.

Church,G.M. (2006) Genomes for all. Sci. Am., 294, 4&54.

Cock,P.A. et al. (2010) The Sanger FASTQ ﬁle format for sequences with quality
scores, and the Solexa/Illumina FASTQ variants. Nucleic Acids Res., 38,
176771771.

Danecek,P. et al. (2011) The variant call format and VCFtools. Bioinformatics, 27,
215672158.

Deorowicz,S. and Grabowski,S. (2011) Compression of DNA sequence reads in
FASTQ format. Bioinformatics, 27, 8W862.

E1ias,P. (1975) Universal codeword sets and representations of the integers. IEEE
Traits. Inf. Theory, 21, l94~203.

Ewing,B. and Green,P. (1998) Base—calling of automated sequencer traces using
Phred.II. Error probabilities. Genome Res., 8, 18(r194.

Fritz,M.H. et al. (2011) Efﬁcient storage of high throughput DNA sequencing data
using reference—based compression. Genome Res., 21, 73¢740.

Giancarlo,R. et al. (2009) Textual data compression in computational biology: a
synopsis. Bioinformatics, 25, 157571586.

Janin,L. et al. (2014) Adaptive reference—free compression of sequence quality
scores. Bioinformatics, 30, 24430.

Kozanitis,C. et al. (2010) Compressing genomic sequence fragments using
SLIMGENE. In: Proceedings of the 14th Annual International Conference on
Research in Computational Molecular Biology, RECOMB'IO. pp. 31(F324.

Lapidoth,A. (1997) On the role of mismatch in rate distortion theory. IEEE Tran.
Inf. Theory, 43, 3847.

Li,H. et al. (2009) The sequence alignment/map format and SAMtools.
Bioinformatics, 25, 207872079.

Mardis,E.R. (2008) Next—generation DNA sequencing methods. Ann. Rev.
Genomics Hum. Genet, 9, 387402.

Moffat,A. and Turpin,A. (2002) Compression and Coding Algorithms. Kluwer
Academic Publishers, Norwell, MA.

Myllykangas,S. et al. (2012) Overview of sequencing technology platforms.
In: Rodriguez—Ezpeleta,N. et al. (eds) Bioiiy’ormatics for High Throughput
Sequencing. Springer, New York, NY, pp. 11725.

Nielsen,R. et al. (2011) Genotype and SNP calling from next—generation sequencing
data. Nat. Rev. Genet, 12, 443451.

Ochoa,I. et al. (2013) QualComp: a new lossy compressor for quality scores based
on rate distortion theory. BMC Bioinformatics, 14, 187.

Tembe,W. et al. (2010) G—SQZ: compact encoding of genomic sequence and quality
data. Bioinformatics, 26, 219272194.

Wan,R. et al. (2012) Transformations for the compression of FASTQ quality scores
of next—generation sequencing data. Bioinformatics, 28, 628435.

 

2136

ﬁle'sreumol‘pmJXO'sopeuuowrorq/pdnq

