BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

A LP model for protein inference

 

Protein Peptide Spectra
Experimental process
I II In

llll ..
“lid

ll H l.
_ _
Protein inference Peptide identiﬁcation

Fig. 1. Protein identiﬁcation using mass spectrometry in shotgun prote-
omics. In the experimental process (from left to right), proteins are di-
gested into peptides, which are then subjected to mass spectrometry to
produce MS/MS spectra. In the data analysis process (from right to left),
there are two major computational problems: peptide identiﬁcation and
protein inference. This article focuses on developing effective algorithms
for protein inference

 

 

 

 

Research
problem

 

 

 

 

probability of one peptide being present given a protein.
ProteinProphet (Nesvizhskii et al., 2003), one of the most
widely used protein inference methods, learns ‘degenerate pep—
tide weight’ using an EM—like algorithm. In fact, such degenerate
peptide weight corresponds to the probability of one protein
being present conditional on the presence of a given peptide.
Alternatively, MSBayesPro (Li et al., 2009b) utilizes the concept
of peptide detectability, which is defined as the probability of
detecting a peptide in a standard sample by a standard prote—
omics routine if its parent protein is expressed. Fido (Serang
et al., 2010) models the probability with which a sample peptide
is generated from a protein containing it with a constant prob—
ability. HSM (Shen et al., 2008) considers ﬁve types of mechan—
isms that a peptide can be generated by a protein, i.e. the
conditional probability that one peptide is present has five pos—
sible values.

The attempts of treating the peptide degeneracy problem
rigorously in the second category have obtained promising re—
sults; however, they still have some limitations.

First, ProteinProphet employs an EM—like iterative procedure
to estimate protein probabilities. This method is described pro—
cedurally rather than derived from a well—deﬁned optimization
model. In contrast, MSBayesPro, HSM and Fido derive their
models from clear, explicitly stated statistical assumptions.
However, they formulate the protein inference problem as a com—
binatorial optimization problem. This means that they may gen—
erate different inference results from the same dataset when
obtaining the optimal solution is too time—consuming.

Second, Fido and HSM use a very small set of parameters to
approximate all possible values that the conditional probability
can take. Such a simpliﬁcation makes it possible to create efﬁ—
cient accompanying algorithms, but it may also limit the capabil—
ity of achieving better inference performance. In contrast, there
are no such limitations in ProteinProphet and MSBayesPro.
Unfortunately, the conditional probabilities in ProteinProphet

are calculated using a formula that has not been rigorously jus—
tiﬁed. The peptide detectability values in MSBayesPro are pre—
dicted using a complex model trained on other datasets.

Finally, existing methods involve many parameters that are
not easy to specify. For example, Fido needs to have a grid
search in order to find good values for its three parameters.

The aforementioned observations motivate our research. In
this article, we take a step further toward completely solving
the protein inference problem with particular emphasis on pep—
tide degeneracy. To that end, we present a linear programming
(LP) model for protein inference, which is built on two simple
probability equations.

We ﬁrst introduce the joint probability that both a protein and
its constituent peptide are present in the sample. To obtain a
linear model, we use a mathematical transformation of this
joint probability as the variable. The marginal probability of a
peptide being present can be expressed as a formula in terms of
the linear combination of these variables. If we assume that the
marginal probability of each identified peptide being present is
known, the protein inference problem could be formulated as the
following optimization problem: ‘minimize the number of pro—
teins of non—zero probabilities while the calculated peptide prob—
ability should be as close to its known value as possible’. We
show that this optimization problem actually can be written as a
LP problem, which has only one parameter that is easy to specify
and has a clear interpretation. This new protein inference algo—
rithm is named as ProteinLP. Experimental results on six data—
sets show that our ProteinLP algorithm is a competitive and
complementary approach for protein inference.

The main contributions of the work described in this article
can be summarized as follows:

0 To our knowledge, our work is the ﬁrst LP formulation for
the protein inference problem. Our method guarantees to
ﬁnd the optimal solution.

0 Instead of using conditional probability, our model is the
ﬁrst attempt of addressing the peptide degeneracy problem
with the joint probability. It greatly simpliﬁes the model
without sacriﬁcing the discrimination power.

The rest of this article is organized as follows. In Section 2, we
describe our method in detail. Section 3 presents the experimen—
tal results and Section 4 concludes the article.

2 METHODS

Given m candidate proteins and n identiﬁed peptides, the protein infer-
ence problem can be formulated as an optimization problem: select a
possibly small subset of candidate proteins that best ‘explains’ these pep-
tides. Such an optimization problem can be formulated in quite different
ways. In this section, we present a LP model for protein inference, which
can be solved very quickly with standard LP solver.

We use a vector of indicator variables (x1, . . . , x), .. . , Xm) to denote
the set of m candidate proteins and another indicator vector
(yl , . . . ,y,-, . . . ,yn) to denote the set of n identiﬁed peptides. In addition,
we assume that we know the probability that each peptide is present in
the sample, which is provided by peptide identiﬁcation algorithms such as
Mascot (Perkins et 01., 1999) or post-processing tools such as
PeptideProphet (Keller et 01., 2002). The peptide probability vector is

 

2957

ﬁm'spzumol‘pmjxo'sopeuuqurorq/ﬁdnq

T.Huang and Z.He

 

denoted by (21, . . . , 2,», . . . , z,,). Notation and deﬁnitions used in this art-
icle are summarized in Table l.

2.1 Model

Let Pr(y,» = 1) denote the probability that peptide 1' is present and
Pr(Xj = 1) denote the probability that protein j is present in the sample.
A peptide is present if at least one of its parent proteins is present:

In
Pro/l»:1)=l—H[l—Pr(yi=l,Xj=1)], (1)
j:l
where Pr(y,» = l,Xj = 1) denotes the joint probability that peptide 1' and
protein j are both present in the sample.
Similarly, for each protein j, we have

’1
Pr(Xj=l)=l—l—[[l—Pr(yi=l,Xj=l)]. (2)

[:1
Through the logarithmic transformation, we convert the product relation
in Equations (1) and (2) into the sum relation so as to build a LP model:

ln[l —Pr(y,»= 1)] = ln[l —Pr(y,» = 1,x,-= 1)] (3)

m
1:1
and

I!
ln[l —P1‘(Xj=l)]=ZlIl[l —Pr(y,»= 1,x,-= 1)]. (4)
[:1
Since the peptide probability and protein probability are not linear with
respect to the joint probability, we use pi]- : ln[l — Pr(y,» = l,Xj = 1)]
instead of Pr(y,» = l,Xj = l) as the variable of our model. Then, both
the peptide probability and protein probability can be expressed as a
function of the linear combination of these variables. In other words,
we can use the sum of pi]- to calculate both peptide probability and protein
probability.

From aforementioned analysis, we can see that the joint probability of
peptide and protein can serve as the bridge between peptide probability
and protein probability. On the one hand, we can use the joint probability
to explain the known peptide probability. On the other hand, we can
calculate the unknown protein probability and tackle peptide degeneracy
issue through joint probability. Therefore, the protein inference problem
is equivalent to ﬁnding an optimal joint probability matrix, calculated
from the matrix P 2 (ply).

Based on aforementioned observations, we present a LP formulation
for the protein inference problem:

In
mlax  z, (5)
1:1
Vj : z, 5 0 (6)
VU 517g 2 [j (7)

Table 1. Notations and deﬁnitions

m

w : ln(l — z.» — e) 5 Zn;- (8)
1'21
Vi: ln(l — z,» + e) 2 Zn;- (9)
1'21
. . < 0 ’ 'e N '
V1,] 1m N l; 0  8(1)’ (10)

where Ne(1) is the set of all proteins that can generate peptide 1'.

In Figure 2, we provide a vivid illustration on the main idea of this LP
model. Some further remarks on the model and constrains are listed
below.

0 The constraints (8) and (9) control the difference between the
observed and calculated peptide probabilities. Here, we regard 2,»
as the observed peptide probability and Pr(y,» = l) as the calculated
value where 1 pi]- : ln[l — Pr(y,» = 1)]. In constraints (8) and (9),
e e [0, l] is the only parameter of our model, which is the difference
between the observed and calculated peptide probability. This par-
ameter reﬂects our conﬁdence on peptide identiﬁcations. For
instance, 6 = 0 means that we believe the input peptide probability
is perfectly accurate so that we have to adjust the variable 17,]- to make
the equation hold. Hence, this parameter has a clear interpretation
and it can be speciﬁed with ease. In our implementation, we use
6 = 0 as the default setting.

0 The constraint (7) is to ﬁnd the minimum value in p]- (the jth col-
umn of matrix P). Since only a subset of candidate proteins are
truly present in the sample, some protein probability values
should be zero. In order to achieve this goal, we control the max-
imum joint probability assigned to each protein. Since ln(l — X)
is a monotonic decreasing function, the maximum joint prob-
ability Pr(y,» = l, Xj = 1) corresponds to the minimum value
ln[l — Pr(y,» = l, Xj = 1)] in 171-. Then we maximize it in the objective
function (5) so as to shrink some protein probabilities to 0.

o The observed peptide probability 2,» can be equal to one. This will
cause a problem in our implementation since ln(l — X) is minus in-
ﬁnity when X: 1. To address this problem, we reset the observed
peptide probability to 0.99999 when 2,» = l.

o pi]- : 0 and If is the minimum value in 17]- so that If should be not
more than zero, as speciﬁed in constraint (6).

o For notation convenience, we use n X m variables in the LP formu-
lation described earlier in the text. In fact, the actual number of
variables is less than n X m since the peptideprotein bipartite
graph is very sparse. As shown in constraint (10), we set all pi]- : 0
if peptide 1' is not contained in protein j and consider only the re-
maining joint probabilities as variables. This greatly improves the
running efﬁciency of our method. Constraint (10) also ensures that
Pr(y,» = l,Xj = 1) falls into [0,1] since it is a probability value.

 

 

Notations Deﬁnitions

(1, . . . , i, . . . , n) All n peptides identiﬁed by peptide identiﬁcation algorithms

(1, . . . , j, . . . ,m) All m proteins that might have generated these n peptides

(yl, . . . , y), . . . , y") Peptide vector: indicator variables of peptides’ presences if peptide 1' is present, y,» = 1; otherwise y,» = 0
(X1, ...,Xj, ..., Xm) Protein vector: indicator variables of proteins’ presences

(21, . . . , 2,», . . . , z,,) The probabilities of peptides’ presences estimated by peptide identiﬁcation algorithms or PeptideProphet

 

 

2958

ﬁm'spzumol‘pmjxo'sopeuuqurorq/ﬁdnq

A LP model for protein inference

 

Column constraints :> V], 1': pl). 2 I].

r’(

pm 1012 .. . pm
4021 P22 ‘ ‘ ‘ [72,”
: I  : 111[1-Pr(y,. =1,xj =1)]
@1 1””2 ' ' ‘ PM

\ 1n(1-z.-s) s in,

Row constraints 2 Vi : f :1
ln(l—2, +5) 2 21:17,},
F

P:  =

 

 

 

 

Fig. 2. P 2 (pi!) is a n X m matrix, where 17,]- is equal to
ln[l — Pr(y,» = l,Xj = 1)], and Pr(y,» = l,Xj = l) is the joint probability
that peptide 1' and protein j are both present in the sample. In the model,
the linear program has two kinds of constraints: column constraints and
row constraints. The row constraints require that for each peptide 1', the
difference between the observed peptide probability and the calculated
peptide probability should be no greater than a threshold 6. The column
constraints can shrink some protein probabilities to 0

o In the model, we group the proteins with the same set of identiﬁed
peptides together and regard each group as a single entity.

0 Our LP model is quite ﬂexible and can be extended easily. For in-
stance, we currently assign a global deviation threshold 6 to all pep-
tides. In fact, we can also use an individual deviation threshold 6,» for
each peptide 1'. This will provide us the possibility of assigning larger
deviation thresholds to certain peptides that are suspected to be
error-prone.

After obtaining the solution matrix P, the protein probability is calcu-
lated as:

Pr(XJ-= 1): 1—1116’1. (11)
[:1

3 EXPERIMENTS
3.1 Data

We use six datasets in our experiments. All the datasets are pub—
licly available. Among these six datasets, l8 mixtures (Klimek
et al., 2008), Sigma49 and yeast (Ramakrishnan et al., 2009a)
have a corresponding protein reference set as the set of
ground—truth proteins. An identiﬁed protein is labeled as a true
identification if it is present in the protein reference set. Another
three datasets are DME (Brunner et al., 2007), HumanMD
(Ramakrishnan et al., 2009b) and HumanEKC (Ramakrishnan
et al., 2009a), which have no reference sets and we use the
target—decoy strategy for performance evaluation. In this
target—decoy strategy, the MS/MS spectra are searched against
a mixed protein database containing all target protein sequences
and an equal number of decoy sequences. Then, we consider an
identified protein as a true identiﬁcation if it comes from the
target protein database. The detailed information about the six
datasets can be found in the supplementary Tables S1 and S2.

3.2 Database search

The search engine used in our experiment is XlTandem
(V2010.10.01.l) (David and Cottrell, 2004). For 18 mixtures,
Sigma49 and yeast datasets, all MS/ MS data are searched against

their own protein sequence databases. For DME, HumanMD
and HumanEKC, the spectra need to search against both target
and decoy protein databases.

During the database search, we use default search parameters
wherever possible, assuming that parameters have already been
optimized. Some important parameter values are listed in the
following: fragment monoisotopic mass error=0.4Da; parent
monoisotopic mass error: 100 ppm; minimum peaks :15 and
minimum fragment m/z: 150.

Peptide probabilities are computed using PeptideProphet
included in Trans—Proteomic Pipeline (TPP) v4.5. Any peptide
identiﬁcations with probability <0.05 are excluded in the input.
For any peptide sequence that is matched by multiple spectra
with different scores, we choose the highest identification score.

3.3 Protein inference

We compare our method with ProteinProphet (Nesvizhskii et al.,
2003), MSBayesPro (Li et al., 2009b) and Fido (Serang et al.,
2010). All these three algorithms treat the peptide degeneracy
issue explicitly in terms of conditional probability, and their soft—
ware packages are publicly available. For the proteins that
cannot be distinguished with respect to identiﬁed peptides,
ProteinProphet, Fido and ProteinLP put all of them into the
same group. Whenever we refer to the number of TPs or false
positives (FPs) identiﬁed at a threshold or use these values in a
calculation, all proteins in the group are reported and the group
probability is used as their protein probabilities. Alternatively,
we can select one representative from each protein group in the
performance comparison for these three algorithms (please check
the supplementary Section 1 for details).

3.3.] ProteinProphet We run ProteinProphet included in the
TPP (v4.5) software with the default parameter values.

3.3.2 MSBayesPro We first obtain the predicted peptide
detectabilities from http://darwin.informatics.indiana.edu/appli—
cations/PeptideDetectabilityPredictor/. This website currently
only predicts scores of tryptic peptides. For those non—tryptic
peptide identiﬁcations, we assign detectability scores to them
by ourselves. The principle is peptide detectability = median (pre—
dicted detectability scores from the same parent protein) / 3. Then,
we run MSBayesPro for the ﬁrst time with the peptide probabil—
ity ﬁle and peptide detectability ﬁle as input to estimate the pro—
tein priors. Finally, we run MSBayesPro for the second time to
obtain the protein probabilities using priors from the ﬁrst run as
additional input. The probability of each protein is reported ac—
cording to the value of Positive_Probability_by_memorizing no
matter what MAP_state_by_Memorizing value is.

3.3.3 Fido We run Fido with its default parameter setting.

3.3.4 ProteinLP We use Glpk for Java (v4.47) as the LP
solver and set 6 = 0 in the experiment for ProteinLP.

3.4 Results

We evaluate the performance of different methods by creating a
curve, which plots the number of TPs as a function of q—value. An
identiﬁed protein is labeled as a TP if it is present in the corres—
ponding protein reference set or target protein sequence database

 

2959

ﬁm'spzumol‘pmjxo'sopeuuqurorq/ﬁdnq

 

 

 

 

 

 

/310'SIBan0prOJx0'sopeuiJOJurorq”:duq

A LP model for protein inference

 

ProteinLP is approximately (or tied with other algorithms) the
best inference algorithm on four datasets (yeast, DME,
HumanMD and HumanEKC) and the second best on 18 mix—
ture data. Locally, it beats ProteinProphet ﬁve times, outper—
forms both MSBayesPro and Fido four times.

Second, ProteinLP has the largest number of TPs among the
highest ranking proteins when q—value=0 (i.e. 0 FPs) on three
datasets: DME, HumanMD and HumanEKC. Other three algo—
rithms can also achieve such a property on some datasets. The
number of these datasets is l, 0 and 2 for ProteinProphet,
MSBayesPro and Fido, respectively.

Finally, the experimental results from simple l8 mixture to
complex human data show the trend that ProteinLP is more
powerful on processing the MS data generated from real samples.

To compare the capability of different methods in tackling the
peptide degeneracy issue, we present the identification results of
four methods when inferring proteins containing a high—scoring
degenerate peptide in Table 2. For each dataset, we count the
number of TPs and FPs identified by ProteinProphet,
MSBayesPro, Fido and ProteinLP with the same number of re—
ported proteins. Then, we divide the identiﬁed proteins into two
classes: ‘degenerate proteins’, which contain a high—scoring de—
generate peptide and ‘simple proteins’ which do not contain any
such degenerate peptide. From Table 2, we have the following
observations.

First, ProteinProphet and ProteinLP can identify more degen—
erate proteins than MSBayesPro and Fido in most cases. This is
because both ProteinProphet and ProteinLP tend to assign a
degenerate peptide to the parent protein with more identiﬁed
peptides. As a result, some degenerate proteins will have much
higher probability than other proteins. In contrast, MSBayesPro
and Fido do not have such a tendency. When we let different
methods report the same number of proteins, ProteinProphet
and ProteinLP will return more degenerate proteins since these
proteins are ranked more front by these two methods.

Second, MSBayesPro can always report the least number of
FP degenerate proteins on 18 mixtures, Sigma49 and yeast at the

Table 2. Accuracy on proteins containing degenerate peptides

cost of identifying less TP degenerate proteins. All four methods
report zero FP degenerate proteins on HumanMD and
HumanEKC datasets.

Third, ProteinLP is able to identify more TP degenerate pro—
teins than the other three methods on DME, HumanMD and
HumanEKC datasets. Our method never reports the most FP
degenerate proteins. Moreover, ProteinLP identiﬁes the least
number of FP degenerate proteins on DME dataset.

Overall, MSBayesPro is more powerful in controlling the false
discovery rate with respect to degenerate proteins, whereas our
method offers a reasonable trade—off between TP and FP rates.

Using the same set of identified proteins in Table 2, we also
plot two groups of Venn diagrams to further check the overlap
and difference among (degenerate) proteins identiﬁed by differ—
ent inference algorithms in supplementary Figures S3 and S4.
These figures show that the set of proteins identified from the
same dataset by different methods can vary significantly.
Moreover, ProteinLP can always report some additional (degen—
erate) proteins that have never been identified by the other three
methods on all datasets except Sigma49. This fact further con—
ﬁrms that ProteinLP can serve as a strong and complementary
approach for protein inference.

ProteinLP requires only one parameter: 6. We choose 6 = 0 as
the default setting. To test the effect of this parameter, we run
ProteinLP over a rough grid of e that ranges from 0 to 0.9. We
omit parameter value of 1.0 since all the protein probabilities are
zero under this parameter setting. We use the number of TPs at
certain q—value threshold as the performance metric to assess the
effect of different parameters. We choose 0.3 as the q—value
threshold for 18 mixtures and Sigma49 and 0.01 for all the
other four datasets, respectively. As shown in Figure 4, the per—
formance of our method is sensitive to different parameter spe—
ciﬁcations, and e = 0 is not the best choice. To address the
parameter selection problem, we develop an entropy—based ap—
proach for setting a proper value automatically (see supplemen—
tary Section 2 for details).

 

 

 

 

PP MSB Fido PLP PP MSB Fido PLP
TP FP TP FP TP FP TP FP TP FP TP FP TP FP TP FP
18 miXtureS Sigma49
Simple proteins 17 8 17 11 17 9 17 9 27 1 32 6 30 5 30 1
Degenerate proteins 1 5 0 3 1 4 0 5 5 10 4 1 5 3 5 7
Yeast DME
Simple proteins 366 4 469 7 373 2 398 4 38 0 145 18 129 11 33 0
Degenerate proteins 164 4 62 0 106 57 132 4 136 1 11 1 34 1 142 0
HumanMD HumanEKC
Simple proteins 70 0 119 1 111 6 64 0 147 0 184 0 180 0 143 0
Degenerate proteins 54 0 4 0 7 0 60 0 49 0 12 0 16 0 53 0

 

For the six datasets, we count the number of true positives and false positives identiﬁed by ProteinProphet (PP), MSBayesPro (MSB) and Fido and ProteinLP (PLP) among
their top— k ranked proteins, where k is 31, 43, 538, 175, 124 and 196 for 18 mixtures, Sigma49, yeast, DME, HumanMD and HumanEKC datasets, respectively. The value of
k is determined according to the number of proteins with probability of 1.0 reported by ProteinProphet. We divide the identiﬁed proteins into two classes: ‘degenerate proteins”
are proteins that share a high—scoring (3 0.90) peptide with another protein and ‘simple proteins” do not share such a peptide with any other protein.

 

2961

ﬁm'spzumol‘piqxo'sopeuuquioiq/ﬁdnq

 

Sigma49
Yeast
—0— DM E

HumanMD
+HumanEKC

+——+——«——¢——+——4——+—

 

 

 

/310'SIBan0prOJx0'soiieuiJOJuioiq”:duq

