BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

Inten/ention in GRNs

 

turned to dynamic programming—based finite—horizon (Datta
et al., 2003) and inﬁnite—horizon external control in which the
steady—state distribution (SSD) is altered (Pal et al., 2006). These
works were followed by several articles developing approaches to
external intervention that take into account practical issues that
arise from therapeutic constraints (Ching et al., 2009; Youseﬁ
et al., 2012), biological complexity (Pal et al., 2008) and compu—
tational limitations (Faryabi et al., 2007; Ivanov et al., 2010).
Structural intervention, on the other hand, involves a one—time
change in the network regulatory structure (wiring) so that the
long—run behavior of the network is altered in a desired manner.
Given a collection of potential structural changes, the problem is
to ﬁnd an optimal structural intervention resulting in a max—
imum alteration of the SSD toward the direction of desirable
states and away from undesirable states (Qian and Dougherty,
2008; Shmulevich et al., 2002c; Xiao and Dougherty, 2007).

In the main, optimal inﬁnite—horizon intervention for GRNs
has involved the speciﬁcation of a cost function based on the
current state of the system and the desirability of potential tran—
sitions. Thus, the long—run effect of the optimal policy on the
network dynamics becomes a by—product of this cost function—
based optimization problem. As phenotype is associated with
steady—state behavior, as in the case of attractor cycles in BNs
and PBNs (Kauffman, 1993), from a practical perspective, it
would be better to determine optimality directly in terms of
long—run behavior. Direct optimization has been addressed to
some extent in Qian et al. (2009), where the authors propose
three classes of greedy stationary intervention policies that
bypass the need for a user—deﬁned cost function and directly use
long—run behavior as the optimization criterion to reduce the mass
of the SSD corresponding to undesirable states and increase the
mass corresponding to desirable states (Qian et al., 2009).

In this article, we rigorously formulate this intervention prob—
lem and provide an optimal intervention policy with a computa—
tional complexity equivalent to solving a linear program (LP)
optimization problem in Section 2.4 for a general cost function
and in Section 2.5 for maximal phenotype alteration. We also
consider a variant of this optimization problem where one might
constrain the steady—state probability mass of some ‘ambiguous’
states in the network while ﬁnding the optimal intervention policy.
This is especially important in therapeutic methods because it is
prudent to avoid introducing new probability mass to states asso—
ciated with unknown phenotypes (Qian and Dougherty, 2012).
We demonstrate the performance of optimal policies using syn—
thetically generated networks and two real networks derived from
the metastatic melanoma and mammalian cell cycle.

2 SYSTEMS AND METHODS
2.1 PBNs

PBNs address uncertainty in the structure of BNs or the dynamics of state
transitions (Shmulevich et al., 2002a). We restrict ourselves to binary
PBNs, it being assumed that the value of each gene is quantized to two
levels 0 or 1, where 0 corresponds to an unexpressed (OFF) gene and 1
corresponds to an expressed (ON) gene. The mathematical theory extends
directly to PBNs with discrete-valued nodes.

DEFINITION 1. A Boolean network BN( V, F) is fully characterized by a

set ofn nodes, V = {v1, v2, . . . , v"}, (representing genes {g1,g2, ...,g"} or

their products) and a list of Boolean functions F = {/1 ,fz, . . . ,f"} describ-
ing the functional relationships between the nodes. The Boolean function
f : {0, l}j"r—>{0, 1} determines the value of node i at time k+ 1, given the
value of its predictors at time k by

vi.“ =f(v2,vf,...,v’l‘),

where {vi1 , viz, . . . , vii-1'} corresponds to values of {gi1,g’7, . . . ,g’j‘} asj,- pre-
dictors of gene g’.

In a BN, all genes are assumed to update synchronously in accordance
with the Boolean functions assigned to them. A BN( V, F) evolves accord-
ing to a genome-wide GAP, VA. 2 (vi, vi, . . . , vﬁ) at time k, forming the
state space of size 2". There is a bijection between v and its decimal
representation X e 5‘ = {0, 1,. . . ,2" — 1} by X = 2le 2"’ivi. In this con-
text, genes that are reﬂective of an undesirable state are called target
genes. 3 is partitioned into subsets of desirable and undesirable states,
denoted by D and U, respectively. Given an initial state, a BN will even-
tually enter a set of states, called an attractor cycle. The set of states
leading to that attractor cycle is known as the basin of attraction
(BOA) (Dougherty et al., 2010).

DEFINITION 2. A PBN( V, F, P, q,p) is characterized by a set of n nodes,
V ={v1, v2, . . . , v"}, and a set ofm constituent BNs, F = {F1,F2, . . . , Fm},
called contexts, a selection probability vector P = {171,172, . . . ,pm} over F,
a network switching probability q and a random gene perturbation prob-
ability p. Random switching and gene perturbation are also assumed to
be mutually exclusive.

At any time point, with probability q, the network dynamics may
switch from the current governing constituent BN to another according
to the selection probability vector P. It is assumed that the probability of
switching to a new constituent network is independent of the current
network. We also allow that the current network may switch to itself
when a switch is called for (Dougherty et al., 2010). In addition, with
probability p, the current state of each gene in the network can be ran-
domly ﬂipped. The PBN is said to be conteXt-sensitive if q< 1, the inter-
pretation being that there are latent variables outside the network whose
changes cause the model network to behave stochastically (Brun et al.,
2005). If q: 1, the PBN is called instantaneously random, the interpret-
ation being that the uncertainty in the PBN arises from uncertainty in
model inference (Shmulevich et al., 2002a). Averaging over the various
contexts reduces the transition probability matrix (TPM) of a context-
sensitive PBN to the instantaneously random PBN with identical param-
eters (Faryabi et al., 2009). By deﬁnition, a PBN inherits the attractor
structures from its context BNs without perturbation. With sufﬁciently
small perturbation probability p, the long-run behavior of a PBN will
reﬂect the attractor structures within the context BNs (Brun et al., 2005).
A Boolean network with perturbation, BNp, is a PBN in which m = 1.

Transition rules of any PBN can be modeled by a homogeneous
Markov chain, whose states of the TPM are the GAPs of the underlying
regulatory network (Shmulevich et al., 2002a). Let 8 = {(X, y) : X e 5‘,
y 6 {1,2, . . . ,m}} denote the state space of the PBN. The sets
1) and U corresponding to the set of desirable, and undesirable
states can be deﬁned as D = {(X,y) : X 6 1—3,}; 6 {1,2, . . . ,m}}
and U = {(X,y) : X e 22,}; 6 {1,2, . . . ,m}}. We denote by
{Zk e 8,k = 0,1,...} the stochastic process of the state of the PBN
that has both the information about the current constituent BN and
GAP of the underlying network. Originating from state ie 8, the
successor state j e 8 is selected randomly according to the transition
probability 7?, with its ijth element deﬁned by pij é P(Z,‘.+1 = j|Zk = i)
for all k = 0,1,.. .. The transition probabilities of this Markov chain
can be calculated as explained in Dougherty et al. (2010). Owing to
the random gene perturbation, the equivalent Markov chain is erg-
odic and has a unique invariant measure, 71, equal to its limiting
distribution.

 

1 759

ﬁlO'SIlZIImOprOJXO'SOplZILLIOJLIIOIQ/[idnq

M.R. Yousefi and ER. Daugherty

 

We assume that the PBN admits an external control input A from a set
of actions, A, specifying the type of intervention on a set of control genes.
For instance, A = 0 may indicate no-intervention, and A = 1 may indicate
that the expression level of a single gene, g", c e {1, 2, . . . , n}, is ﬂipped. In
this intervention scenario, the control action A = 1 at state (X, y) replaces
the row corresponding to the state (X, y) in the original TPM of the
underlying Markov chain by the row corresponding to the state (X, y),
where the binary representation of X is the same as X except in bit v",
where it is ﬂipped. Let {AA 6 A, k = 0, 1, . . .} denote the stochastic pro-
cess of actions taken. The law of motion for the controlled network is
represented by a matrix 73(a) with its ijth element defined as

Pij(a) = P(Zk+1 = jIZk = i, AA = a), (1)

this being the probability of going to j at time k+ 1 starting from statei
and taking action a at time k. As the original Markov chain is ergodic, the
controlled chain will also be ergodic having a unique invariant measure.
The action process is stochastic in two ways: (i) the state process is sto-
chastic, and the action process is a mapping from the entire history of
states and actions to the action space so that the action process is also
stochastic and (ii) we allow the actions to be random depending on the
history and current state of the system. Indeed, the pair (Zk,A,‘.) is a
stochastic process.

2.2 Constrained MDPs

Discrete time MDPs constitute a class of sequential decision making
problems where the system of interest evolves stochastically at discrete
time units. One can observe system states at times k = 0, 1, . . . , N, where
N, called the ‘horizon’ may be finite or inﬁnite. At each k, a decision
maker can alter the costs or dynamics of the system by choosing some
parameters, called ‘actions’. The dynamical movement of the system from
one state to another is completely characterized, given the current state
and action taken at this state via a probability vector over all possible
next states. The goal is to optimize an objective function (e. g. the inﬁnite-
horizon expected average cost) and ﬁnd an optimal control policy indu-
cing the optimal cost. More than one objective cost may exist, and the
decision maker minimizes one of the objectives subject to constraints on
the others. This class of MDPs are referred to as constrained MDPs
(Altman, 1999) and mathematically deﬁned as follows.

DEFINITION 3. A tuple {8, A, 73(a), g, r} deﬁnes a constrained MDP,
where 8 denotes the state space of a ﬁnite-state Markov chain, A is a
ﬁnite set of actions, A(i) E A denoting the set of actions available at state
i (we also denote by [C = {(i, a) : i e 8, a e A(i)} the set of state-action
pairs), 73(a) defined in Equation (1) is the TPM of the controlled
Markov chain, g: [C —> [R is an immediate cost, and r : [C —> [RD is a
D-dimensional vector of immediate costs defining the D constraints.

Denote by {z,‘.,k = 0,1,...} and {al.,k = 0,1,...} the sequences of
observed states and actions. A policy is a prescription for taking actions
at each k. Actions may be taken in accordance with a random mechan-
ism, possibly a function of the entire history of the system up to time k.
To make this precise, let Hk be the random vector of previous states and
actions occurring up to time k and hk be the observed history, i.e.
h» = (z0,a0,zl,a1, . . . ,Zk,a/‘»). A policy p, = (#0411, . . . ,MN) iS a SC-
quence prescribed by the decision maker that steers the dynamics of
the underlying system. If the history hk,1 is observed up to time k,
then the decision maker chooses an action a e A(z,‘.) with probability
Mk(a|hk71,lk)a Where

2 “Hulk/{71,1021-

aeA(zk)

0 S “Hulk/{71,195 1,

The class of all control policies It is denoted by M. The initial statei of
the Markov chain and any given policy I), determine a unique probability
measure  over the space of all trajectories of states and actions, which

correspondingly deﬁnes the stochastic processes ZA. and Ak of the states
and actions for the controlled system. Eé‘ denotes the corresponding ex-
pectation relative to which cost criteria are deﬁned.

DEFINITION 4. The inﬁnite-horizon expected average costs are
N
1

Eyg(Zk,Ak),

 

G(i, It) 2 lim sup
N~>oo :

1
N+l

 

N
Rd(i, ,t) = lim sup 2 Eff/[(Zk, AA), for d = 1,2, . . . ,D.
N~>oo [(20

Given real numbers V1, V2,..., V”, we can formulate the optimization
problem in its most general form: for an initial state i,

minuem G(i, M),

OP] 2 subject to R“ (i, It) 5 V“ for all d: 1,2, . . . , D.

The set of all policies that satisfy the constraints is called the feasible
region. Let G*(i) denote the optimal value achieved by the optimization
procedure. A feasible policy If is optimal if G*(i) = G(i, pf“), for i e 8.
The search space M is inﬁnite (possibly uncountable); however, one may
identify classes within M that possess beneﬁcial properties. We consider
three classes: Markov policies, stationary policies and stationary deter-
minist policies, denoted by MM, MS and MD, respectively. MM in-
cludes policies for which MA. is only a function of zk for any k. MS is a
subset of MM and includes policies for which MA. is time invariant (not
dependent on k). This means that the probabilities that the control policy
It assigns to different actions only depend on the state and not the time.
For example, for a PBN with transition probabilities deﬁned in Equation
(1) and any policy u 6 M5, the underlying stochastic process becomes a
stationary Markov chain with the set, Q(p,), of transition probabilities
deﬁned by

[tn-(u) = Z pn-(awli), (2)

aeA(i)

for all i, j e 8. The third class, M D, is the set of all stationary determin-
istic policies such that p,(a|i) is either 0 or 1 for every i e 8. In this case,
p, : 8 —> A is a single-valued transformation from the states to the
actions.

Usually in the context of MDP problems, minimizing an objective
function (the inﬁnite-horizon expected total discounted cost or the ex-
pected average cost) with no constraints, i.e. D20, is carried out by
formulating a set of dynamic programming functional equations,
known as the Bellman optimality equations. Using these equations, it
can be shown that the optimal control policies belong to the class M D
of policies. Hence, instead of searching the entire space of all history-
dependent randomized control policies for the optimal solution, one can
focus only on a set of coupled minimization problems over a (much
smaller) set of actions.

2.3 Suboptimal phenotype alteration

Our goal is to ﬁnd an intervention policy to maximally shift the long-run
probability mass of undesirable states to desirable ones. Several algo-
rithms have been proposed for intervention in Markovian GRNs that
avoid using a user-deﬁned cost function and work directly with the tran-
sition probabilities of the Markov chain associated with the network to
approximate this goal. These algorithms are motivated by heuristics and
obtain suboptimal policies. Owing to space limitations, here we only list
the algorithms, with detailed descriptions being given in the
Supplementary Material: (i) mean-ﬁrst-passage-time (MFPT) control
policy (Vahedi et al., 2008), (ii) BOA control policy (Qian et al., 2009),
(iii) SSD control policy (Qian et al., 2009) (iv) and conservative SSD
(CSSD) control policy (Qian et al., 2009).

 

1760

ﬁm'spzumofpmjxo'sopeuuqquIq/ﬁdnq

Inten/ention in GRNs

 

Under the MFPT, BOA, SSD and CSSD control policies, as well as
for user-deﬁned cost functions, there might be an introduction of signiﬁ-
cant mass at states representing complications and harmful side effects to
healthy cells, even though these are classiﬁed in the set of desirable states
because they are not undesirable relative to the particular pathology of
interest.

In Section 2.8.2, we will discuss a lO-gene network involving the gene
WNTSA and explain why an intervention that downregulates WNTSA
may have the beneﬁcial effect of suppressing metastatic phenotypes. For
this reason, networks involving WNTSA have been used in a number of
control studies (Datta et al., 2003; Faryabi et al., 2009; Pal et al., 2006). In
particular, in Qian and Dougherty (2008), structural intervention consist-
ing of a one-time change to the governing regulatory logic was applied to
a seven-gene network containing WNTSA with the aim downregulating it
in the long-run. Among the genes in the network, the minimum steady-
state probability mass for upregulated WNTSA was obtained by perturb-
ing the function determining the status of WNTSA. Unfortunately, this
intervention had the effect of placing a large mass (>0.4) at a state that
had virtually no mass in the original SSD and in which STC2 is upregu-
lated. Several studies have shown that STC2 plays a role in carcinogen-
esis. For instance, STC2 is upregulated in breast and ovarian cancer cells,
following exposure to hypoxia (Law and Wong, 2010a), and high levels of
STC2 expression are associated with increased invasiveness and metasta-
sis in ovarian cancer cells (Law and Wong, 2010b). Hence, the uncon-
strained optimal structural intervention should be avoided. A better
alternative would be to perturb the machinery governing RETl, where
the results in Qian and Dougherty (2008) show that an appropriate per-
turbation of its regulatory logic reduces the steady-state mass of the
upregulated WNTSA states almost as much as direct WNTSA interven-
tion, and without the side effect of signiﬁcantly upregulating STC2.

From a general perspective, it may be prudent to restrict the newly
introduced mass to states associated with known healthy phenotypes. To
achieve this end, the set D of desirable states can be further partitioned
into two categories, 1),, representing known healthy states (those known
to be associated with a healthy phenotype) and 1),, representing ambigu-
ous states that may (or certainly will) lead to phenotypes that, although
not undesirable relative to the pathology of immediate interest, are them-
selves known to be undesirable or of which nothing is known. The goal is
to ﬁnd a control policy that shifts the long-run probability mass of un-
desirable states as much as possible and at the same time keeps the long-
run probability mass of each ambiguous state below some level. Being
that phenotypes are associated with attractors, a conservative approach is
to deﬁne the ambiguous states as those states in D that belong to the
nonattractor states of the original network so that the control will not
introduce new attractors (phenotypes). This approach is not only conser-
vative but also does not require prior knowledge of which states in D are,
or are not, associated with pathological phenotypes. In the WNTSA ex-
ample just cited, the intervention leading to the high-mass state in which
STC2 was upregulated would not have been allowed under this criterion.
The constrained SSD (conSSD) and constrained CSSD (conCSSD) con-
trol policies aim to ﬁnd such (suboptimal) interventions (Qian and
Dougherty, 2012).

None of the six long-run-based heuristic algorithms use a rigorous
optimization to ﬁnd optimal intervention policies with respect to the ob-
jectives and constraints.

2.4 Occupation measures and the primal LP

To proceed optimally in the case of constrained problems, we note that it
has been shown that the objective functions for different cost criteria are a
linear function of ‘occupation measures’ for stationary control policies. A
salient consequence is that the original problem of ﬁnding the optimal
cost and control policy can be transformed into an LP, referred to as the
primal LP, where the optimization variables are the occupation measures
(Derman, 1970; Kallenberg, 1983). The optimal solutions to this primal

LP determine, as a one-to-one relationship, the optimal stationary control
policies (Altman, 1999). Let us ﬁrst state an assumption that we will make
throughout the rest of this article.

ASSUMPTION 1. The MDP is assumed to be unichain, meaning that for
any p, e MD, the corresponding Markov chain described by the transi-
tion probabilities Q(p,), deﬁned in Equation (2), has at most one ergodic
class and a (perhaps empty) set of transient states.

This assumption holds for the case of controlled PBNs, as the Markov
chain corresponding to a given PBN is ergodic due to random gene per-
turbation. We now deﬁne the occupation measures.

DEFINITION 5. For any given initial statei and policy It, and any state-
action pair j and a e A(j), the occupation measure is

1

N - _
Vju(l,l1«) — N+ 1

N
ZPﬂzk =j,AA. = a).
[:0

In other words, under any policy It and given Z0 2 i, vjNu(i, It) is the
expected frequency, up to time N, of entrances into state j when action a
is taken. Let I)” (i, It) denote the matrix of vjNu(i, It) over all j and a. Let
V(i, It) be the nonempty set of all limit points of the sequence
{vN(i,p,),N = 0,1,.. .}. Then, for any i e 8, u e M and v e V(i,p,),

Z Z wagon) : G(LM),

jeS ueAG)
with equality holding for some v e V(i, miin particular, it holds when
u 6 MS (Altman, 1999). Moreover, the union of all V(i,p,) over all
u E M, where the V(i,p,) are singletons, is equal to the union of all
V(i,p,) over all p, 6 MS (Derman, 1970). If p, 6 MS, then the V(i,p,)
are all singletons and independent of the initial state i. Furthermore,
for vju(i,p,) e V(i,p,), the Markov chain with transition probabilities
Q(p,), deﬁned in Equation (2), has a unique invariant probability measure
vector 7101,), which satisﬁes

mm) = Z Ill-“(L [1,), for all i, j e s, (3)
uEAG)
arm) = N(M)Q(M), (4)
anm) = 1, mm) 3 0, for all j e s. (5)
jeS

Therefore, one only needs to search the space of M5 for the optimal
solution by solving the following LP problem (Altman, 1999):

mum Z Z ViugUaah

jeS ueA(j)
LP2 I
Z Z vjlrdo‘m): Vd,d=1,2,...,D,
jeSueAG)
Z Vju=z Z Viapij(a)aj E 5,
subject to 116/40) ieSueA(i)
Z Z Vjuz 1,

jeS aeAG)
vi“ 3 0, for all j e 8,0 6 AU)-

Let v* be a minimizing argument of the LP2 problem. Under
Assumption 1, one can recover an optimal randomized stationary
policy If by

3‘
. ju
Wall): 2 V3,“, (6)

uEAG)

 

whenever Ede/(G) v13; >0, and if Ede/(G) v}; = 0, then p,*(a|j) = 1 for some
a e A(j). As the Markov chain for a controlled PBN is ergodic, there are
no transient states. Hence, Ede/(G) v13; 75 0.

 

1761

ﬁm'spzumofpmjxo'sopeuuqquIq/ﬁdnq

M.R. Yousefi and ER. Dougherty

 

THEOREM 1. (Altman, 1999) The optimization problems CPI and LP2
are equivalent, and there is a one-to-one correspondence between their
feasible (and optimal) solutions.

THEOREM 2. (Ross, 1989) If LP2 is feasible and p," is an optimal policy
constructed by the aforementioned procedure, then there exists a list of at
most |8| + D actions for If so that p," requires randomization in at most
D states.

If there are no constraints on the expected average cost criteria,
i.e. D20, then there is no randomization and pf“ e MD. The main
result of this section can be summarized as follows: Under
Assumption 1, for every policy in M, there exists a stationary policy
in MS that achieves the same limit point for the occupation measures;
hence, we only need to consider the set MS for any optimization prob-
lem involving occupation measures of state and action in its objective
and constraints.

2.5 Maximal phenotype alteration

We desire an intervention policy that maximally shifts the long-run prob-
ability mass of undesirable states to desirable states. This objective func-
tion essentially concerns the long-run behavior of the occupation
measures marginalized over the actions. Thus, based on the discussion
at the end of Section 2.4, we can limit the policy space to MS without loss
of generality.

Let A = A(j) = {0, l} for allj e 8. If policy a 6 MS, then the amount
of shift in the aggregated probability of undesirable states for a PBN
controlled under a is deﬁned as

AmAM) = 2m — 2mm), (7)
jell jell
where 7'! and 7101,) are the unique vectors of the invariant probability
measure for the Markov chains governed under the TPMs 73 and
Q(p,), respectively, which also satisfy Equations (3) to (5). In general,
—1 5 AJTMUJ.) 5 1 and our goal is to maximize it.

LEMMA 1. A?!“ is maximized by solving the LP2 problem with the
immediate cost function g being 1 for undesirable states and 0 otherwise.

PROOF. It can be easily seen from Equation (7) that maximizing
Anum) is equivalent to minimizing 21.6“ JTj([.L), as 7'! is ﬁxed. Also, if
we let the immediate cost function g(Z,‘., AA.) take the form:

. 1, if j e U,
gU’a) _ {0, otherwise,

we have

2 Z vita, logo, a) = Z Z vita, u) = 2 mm),

jeS aeA jell aeA jell

for all i and any a 6 MS, where we used Equation (3) for the second
equality. Therefore, it can be verified that we need to again solve LP2,
with the suggested choice of g, to ﬁnd the optimal v, which at the same
time maximizes the shift Amy.

2.5 .1 Unconstrained optimal intervention If there are no constraints
on the cost criteria (D = 0) for the shift maximization problem, then we
can rewrite LP2 as

min 2 2 vi“,
” jeMueA
Z Vjuzz 2 Via pij(a)a.l E 8;
LP3 I aeA I65 uEA
subject to Z 2 vi“: 1,
jeS aeA

vi“ : 0,for allj eS,a e A.

The unconstrained (UC) optimal intervention policy aﬁc can be
constructed using Equation (6) when v* yields the minimum in LP3. As
there are no constraints, pfch EMD so that pfch is stationary and
deterministic.

2.5.2 Phenotypically constrained optimal intervention The LP3
problem aims to locate an optimal control policy inducing maximal
shift from the undesirable mass to eradicate phenotypes associated with
the pathology of interest. However, when there are ambiguous states in
the network, the optimization problem can be stated as locating a control
policy that maximizes the shift of undesirable mass while satisfying an
upper bound constraint for newly introduced steady-state mass into am-
biguous states in 1)“. Similar to the choice of g in the proof of Lemma 1,
one can deﬁne r" for d = 1,2, . . . , |Du|, each corresponding to a state
j“ e D“. The phenotypically constrained (PC) optimization problem can
be formulated as

min 2 z 
” jeMueA
2}.le : V“,d=1,2,...,lvll,r e D...
“E
LP“ 2 . 2 via: 2 2 Via pij(a)aj E 5,

subject to aeA ieS aeA
Z 2 via: 1,
jeSaeA

vi“ : 0,fora11je 8,a e A.

One can assume that V“ = A for all d. The PC optimal intervention
policy a; can be constructed using Equation (6) when v* yields the
minimum in LP4. As there are |Du| additional constraints on the opti-
mization problem, a; 6 MS and it requires randomization in at most
|Du| states.

2.6 Computational complexity

In any optimization problem, computational complexity is a concern. In
particular, how well does the algorithm scale to larger networks? In our
case, there are ISI >< |A| decision variables and |8| equality constraints in
both LP3 and LP4 problems. LP4 has |Du| additional inequality con-
straints. We use IBM ILOG CPLEX Optimizer using a dual simplex
method to solve both LP3 and LP4 problems. Although, the worst-case
computational complexity of the simplex method has been shown to be
exponential in the number of decision variables and constraints, prob-
abilistic analyses of the simplex method have indicated that the average
complexity of this method is polynomial. Moreover, it has been proven
that solving an LP, in general, requires a number of operations polyno-
mial in the number of decision variables and constraints. Speciﬁcally,
Karmarkar’s algorithm, which belongs to the general class of interior
point methods, solves LPs in polynomial time with reasonable efﬁciency
(Megiddo, 1987). The computational complexities of all the suboptimal
policies mentioned in the article are also reported to be either linear or
polynomial.

Besides the guaranteed optimality of our method over the greedy al-
gorithms, the linear programming approach is particularly useful when
optimization problems involving nonlinear functions or side constraints
are considered (Derman, 1970). In such cases, using a dynamic program-
ming approach makes the analysis much more complicated and compu-
tationally prohibitive. Based on our estimates and the reported size of the
linear programming problems that have been solved using IBM ILOG
CPLEX Optimizer, it is possible with the current technology to ﬁnd the
optimal UC and PC policies for networks with 20 genes. Hence, it avoids
the kinds of model reduction methodologies that have been developed in
the context of GRN control (Ghaffari et al., 2010; Ivanov et al., 2007,
2010; Qian et al., 2010), thereby avoiding the loss of optimality engen-
dered by reductions.

 

1762

ﬁm'spzumofpmjxo'sopeuuqquIq/ﬁdnq

Inten/ention in GRNs

 

2.7 Robustness

As with any optimization procedure, the algorithm is optimal so long as
the assumed model is correct. In the case of GRNs, model ﬁdelity is
affected by several factors: data extraction, discretization, gene selection
and network generation. Hence, robustness becomes an issue. Robustness
refers to an algorithm’s performance on models that are close to, but not
equal to, the model on which it has been derived. The robustness of
stationary control for PBNs has been considered in Pal et al. (2008),
and a robust intervention strategy has been obtained by minimizing the
worst-case cost over an uncertainty class of networks. Such a minimax
control approach is typically conservative because it gives equal import-
ance to extreme cases. Thus, a Bayesian approach was formulated in Pal
et al. (2009). A Bayesian approach requires a prior distribution on the
uncertainty class of networks and therefore is dependent on signiﬁcant
prior knowledge, which may not be available. Here, we describe the per-
turbation bounds discussed in Pal et al. (2008), where uncertainty is
studied in the context of the transition probabilities. The intervention
strategy is derived on the estimated TPM, 73, and is applied to the
actual network TPM,  The basic question is that, for a given control
policy, how does the mismatch between 73 and 7N3 affect the SSD of the
controlled network? Although the discussion in Pal et al. (2008) applies to
stationary deterministic control and the equivalent and our general set-
ting is of wider scope, both the UC and PC algorithms have stationary
control so that the analysis of Pal et al. (2008) is applicable.

Given that the class of allowed interventions consists of ﬂipping the
value of a gene, application of a stationary policy a derived from the
uncontrolled estimated TPM 73 converts 73 to a controlled TPM Q(p,),
where Q(p,) = Tm)? and Na) represents a stochastic matrix, which has
at most |A| nonzero entries adding up to 1 in each row (these nonzero
entries are found from the control policy a). Let 7'! and 71m) denote the
SSDs corresponding ~to 73 and Q(p,), respectively. With 73 being the actual
uncontrolled TPM, Q(p,) = T(p,)7~3 is the controlled TPM resulting from
applying T(p,) to  Let ft and 7701,) denote the SSDs of 7N3 and Om),
respectively. Robustness concerns the difference rim) — 7101,) based on
the estimation error E g 73 — 

Assuming that the actual and estimated networks possess the same
state space, which allows for different topologies and different regulatory
functions, the SSD difference can be bounded by lit—J?qu K||E||oo
where q: 1 or 00 and K >0 are some constants. K is called a condition
number. Some condition numbers will yield tighter bounds than the
others (Cho and Meyer, 2001). Pal et al. (2008) considered the ergodicity
coefficient

11(7)) = sup ‘XTP

|XT| l:1
\‘Tl:0

 

19

where 1 denotes a column vector of appropriate length having all entries
equal to 1 (Seneta, 1988). If 11(7)) 75 1, then (Pal et al., 2008)

c 1
{N(M)-N(M)115WIIEHW

Hence if, the error is small, then the SSD of the controlled actual
process is close to the SSD of controlled estimated process from which
the control policy has been derived, the bound depending on the ergodi-
city coefﬁcient. Other perturbation bounds using different condition
numbers were examined via simulations in Pal et al. (2008).

Having discussed the general case of Markov chains, Pal et al. (2008)
went on to relate the general results to PBNs by showing how different
classes of possible uncertainties for a PBN would translate into uncer-
tainties in the TPM for the corresponding Markov chain.

2.8 Simulation setup

We design simulation studies on synthetically generated and two real
networks. We ﬁnd optimal and suboptimal control policies for both

UC and constrained problems and calculate their performance under
different parameter choices. To reduce the complexity of graphs and
tables, we do not consider the MFPT and BOA policies. From the per-
spective of demonstrating the optimality of the UC and PC algorithms,
there is no loss in this omission, as it has been amply demonstrated that
the SSD and CSSD policies generally outperform the MFPT and BOA
policies relative to the criterion of shifting the SSD (Qian et al., 2009). For
constrained optimization problems, the set of ambiguous states, 1)“, and
an upper-bound A must be deﬁned. In practice, these should be deﬁned
based on biomedical knowledge related to the GRN and treatment ob-
jectives. Here, we take an approach similar to Qian and Dougherty
(2012). For PBNs, we deﬁne D“ = {i e D : m 5 r}, where 7'! is the invari-
ant measure of the TPM of the uncontrolled network and r is a user-
deﬁned threshold. We let I = 1/2" and set A = maxiEDU m. For BNps,
ambiguous states are the states in D that belong to the nonattractor states
of the original BNs so that the control will not introduce new attractors.

2.8.1 Synthetic networks We generate 2500 random PBNs and 2500
random BNps with different parameters and report the average perform-
ance across all networks as well as some statistics on the performance of
each intervention policy. The performance of optimal policies might not
substantially exceed that of suboptimal policies when averaged across
randomly generated networks for a couple of reasons. First, randomly
generated networks may have certain structures making them unrespon-
sive to the intervention policies. Second, many networks might possess
structure for which the suboptimal and optimal policies are almost iden-
tical. Be that as it may, the key point is that there are networks for which
the optimal policy signiﬁcantly outperforms the suboptimal ones. Here,
we use the difference in the amounts of shifts made by optimal and sub-
optimal policies to quantify the gain made by using the optimal policy:

Am) = Maui“) — mm) = anm) — 2mm"),

jell jell

where 71(p,*) and 71m) denote the invariant probability measures for the
network under the optimal and suboptimal policies, respectively. Clearly,
0 5 A(a) 5 1. For each network A(a) is calculated for all previously
discussed suboptimal policies, for the UC and constrained problems sep-
arately. As the networks are randomly generated, A(a) is a random
variable. To show the effectiveness of optimal policies, we graph the
tail probabilities, i.e. the empirical complementary cumulative distribu-
tion function (CCDF), of A(a).

To keep the computational time tractable, we consider instantaneously
random PBNs, the states representing the GAPs at any given time, and
n = 6,7 or 8 genes. The state space is 8 = {0,1,2, ...,2" — 1} and is
generated from four equally likely constituent BNs with the maximum
number of predictors (1} 5 K for all I) for each Boolean function set to two
or three. The bias of each PBN is the probability that each Boolean
regulatory function takes on the value 1. We assume that it is taken
randomly from a beta distribution with mean 6 e {0.25,0.5,0.75} and
the standard deviation 0.01, thereby affecting the dynamics of a randomly
generated BN. The gene perturbation probability is either 0.001 or 0.01.
A similar setting is used for generating BNps, except that there is only one
constituent BN. For any given BNp and PBN, the TPMs of the corres-
ponding Markov chains are computed as explained in Section 2.1. We
choose the control and target genes to be the least and most signiﬁcant
bits in the binary representation of states, respectively, and assume that
downregulation of the target gene is undesirable.

The actual construction is done in the following manner. For each
constituent BN in a random PBN, we ﬁrst randomly select the predictors
of each gene with all genes having the same probability of being selected.
The value of each gene given the values of its predictors is now deter-
mined by a Bernoulli random variable whose probability of being 1
equals the bias (the bias is also randomly generated from a beta distri-
bution with a mean 6 and a given standard deviation). This process will
construct a random truth table corresponding to a BN. Having all the

 

1 763

ﬁm'spzumofpmjxo'sopeuuqquIq/ﬁdnq

M.R. Yousefi and ER. Dougherty

 

constituent BNs and gene perturbation probability deﬁned, we directly
calculate the TPM of an instantaneously random PBN without actually
constructing it.

2.8.2 Metastatic melanoma network A metastatic melanoma net-
work is derived from gene expression data collected in a study of meta-
static melanoma (Bittner et al., 2000; Dissanayake et al., 2008;
Weeraratna et al., 2002). By manipulating the concentration level of
WNT5A protein secreted by a melanoma cell line, one can directly
affect the metastatic status of the cell as measured by the standard
in vitro assays for metastasis. For intervention purposes, antibodies can
be designed to bind with WNT5A and block it from activating its recep-
tor. This will signiﬁcantly weaken its ability to induce a metastatic pheno-
type, which suggests that reducing the WNT5A gene’s action could
reduce the chance of a melanoma metastasizing. Therefore, it is desirable
to downregulate WNT5A as much as possible through appropriate inter-
vention mechanisms (Datta et al., 2003; Qian and Dougherty, 2008). We
use a network composed of 10 genes from a set of 587 genes (Qian and
Dougherty, 2012). The genes, listed from the most signiﬁcant bit to the
least signiﬁcant bit, are WNT5A, pirin, S100P, RETl, MMP3, PHOC,
MARTl, HADHB, synuclein and STC2. This ordering of genes is only
for demonstration purposes and does not affect our analysis. The regu-
latory relationships between these genes are presented in Table 1. We
construct a BNp with p=0.001 and assume that the upregulation of
WNT5A is undesirable. For the control genes, we choose each gene
in the network as the control gene and find the optimal and subopti-
mal intervention policies in addition to their effects on the steady-state
shift.

2.8.3 Mammalian cellcycle network To characterize the dynamical
behavior of normal mammalian cells during the cell cycle, Faure et al.
(2006) proposed a Boolean-logic regulatory network containing three
key genes: Cyclin D (Cch), retinoblastoma (Rb) and p27. For a
normal mammalian organism, cell division is coordinated with overall
growth via extracellular signals controlling the activation of Cch.
These signals indicate whether a cell should undergo cell division or
remain in a resting state. Rb is a tumor-suppressor gene and is ex-
pressed in the absence of the cyclins that inhibit Rb by phosphorylation.
Gene p27 is also active in the absence of the cyclins. An active p27
blocks the action of Cch or CycA and, hence, Rb can also be ex-
pressed, even in the presence of Cch or CycA, resulting in a stop in the
cell cycle. In the wild-type cell-cycle network, when p27 is active, the cell
cycle can be stopped. Following a proposed mutation for this network,
we assume that p27 can never be activated (always OFF), thereby
creating a situation where both Cch and Rb might be inactive
(Faure et al., 2006). Under these conditions, the cell can cycle in the
absence of any growth factor, thereby causing undesirable proliferation.
The mutated network has nine genes, Cch, Rb, E2F, Cch, CycA,
Cdc20, th1, chH10 and CycB, ordered from the most signiﬁcant bit
to the least signiﬁcant bit in the binary representation. Similar to the
metastatic melanoma network, this ordering is only for the sake of
presentation. Table 2 lists the regulatory functions of the mutated
cell-cycle network following Boolean logic.

We construct an instantaneously random PBN for this mutated net-
work, where depending on the state of the extracellular signal that deter-
mines the state of Cch as being ON or OFF, there are two constituent
BNs, which we assume to be equally likely. We set p=0.001. As cell
growth in the absence of growth factors is undesirable, the undesirable
states are those for which Cch and Rb are both downregulated (OFF).
We consider all genes except Cch and Rb as the possible control gene
and ﬁnd the optimal and suboptimal intervention policies in addition to
their effects on the steady-state shift.

Table 1. Regulatory functions of a metastatic melanoma network

 

 

Gene Node Predictor functions

WNT5A v1 (v; A vs A v?) \/ (v_5 A v6)

pirin v2 (W A E A vs) V (v1 A E A V_5)
SlOOP v3 v7

RETl v4 (W A v; A v4) \/ (v7 A v4)

MMP3 v5 (v4 A V9) \/ (W)

 V6  /\  V (V4 /\ V7 /\ V10)

 V7 V7

HADHB v8 (v1 A v5)\/(v_5AW)\/(v1 Av_5A vg)
synuclein v9 (W A W A v—m) \/ (v4 A v_7 A v10) \/ v7
STC2 v10 v_3

 

Table 2. Regulatory functions of a mutated mammalian cell-cycle
network

 

 

Gene Node Predictor functions

Cch v1 Extracellular signal

Rb V2 (V—l A W A V_5 A W)

EZF V3 (V_2 A V_5 A V?)

CYCE V4 (V3 A V7)

CycA V5 (V3 A v7 A v? A (MD
We Av—zAv—sAW»

Cdc20 v6 v9

th1 v7 (v_5 A V—g) \/ v6

chHlO v8 WV(v7 A vs A(v6\/ v5 \/ vg))

CYCB V9 (V? A W)

 

3 RESULTS AND DISCUSSION

The entire set of simulation results can be found in the
Supplementary Material. Here, we provide some results that rep—
resent the general trends observed in the simulations.

3.1 Synthetic networks

We present results for the synthetic networks with seven genes.
Corresponding results for six and eight genes are given in the
Supplementary Material, where it is evident that the same trends
are observed. Figure 1 shows the shift in the probability mass of
undesirable states induced by the optimal and suboptimal poli—
cies, Equation (7), averaged over all random networks with vari—
ous structural properties and for different values of 6. These
graphs clearly show the optimality of uﬁc and a; compared
with their suboptimal counterparts. In general, the suboptimal
policy ucssd yields the best results, close to optimal, among the
suboptimal policies. However, uconcssd does not induce as much
shift as [2:0, which is due to the fact that the search space of
conCSSD policy is M D, whereas it is MS in the LP4 problem.
The amount of average shift induced by a; is signiﬁcantly less
than uﬁc owing to the additional constraints imposed on the
optimization LP4 problem.

 

1 764

ﬁm'spzumofpmjxo'sopeuuqquIq/ﬁdnq

an?kgogmomammowoio~&o:3m7.omm\

 

M.R. Yousefi and ER. Dougherty

 

Table 3. Shift in the steady-state mass of undesirable states of the metastatic melanoma network for different control genes and different control policies

 

 

Control WNT5A pirin S100P RETl MMP3 PHOC MARTl HADHB synuclein STC2
SSD 0.0632 0.1630 0.1679 0.1646 0.1701 0.1701 0.1684 0.0000 0.0029 0.1674
CSSD 0.0675 0.1632 0.1679 0.1681 0.1701 0.1701 0.1684 0.0000 0.0029 0.1674
UC 0.0677 0.1632 0.1681 0.1681 0.1701 0.1701 0.1684 0.0000 0.0029 0.1674
conSSD 0.0000 0.0003 0.1548 0.0000 0.0054 0.0041 0.0165 0.0000 0.0013 0.1146
conCSSD 0.0675 0.0043 0.0080 0.0005 0.0059 0.0057 0.0016 0.0000 0.0029 0.0040
PC 0.0677 0.0548 0.0139 0.1473 0.0076 0.0076 0.1682 0.0000 0.0029 0.0070

 

Table 4. Shift in the steady-state mass of undesirable states of the mutated mammalian cell-cycle network for different control genes and different control

 

 

policies

Control E2F Cch CycA Cdc20 thl chH 1 0 CycB
SSD 0.1972 0.0573 0.0334 0.0565 0.0835 0.0122 0.0598
CSSD 0.1972 0.0573 0.0335 0.0836 0.0837 0.0138 0.0843
UC 0.1972 0.0573 0.0335 0.0836 0.0837 0.0138 0.0843
conSSD 0.0374 0.0207 0.0329 0.0119 0.0327 0.0001 0.0244
conCSSD 0.0571 0.0573 0.0332 0.0332 0.0506 0.0118 0.0254
PC 0.1857 0.0573 0.0332 0.0565 0.0509 0.0120 0.0481

 

willing to take. The seemingly intuitive choice of directly perturb—
ing WNT5A to control WNT5A (ﬁrst column) only produces
the best result with the conSSD policy. This is because of feed—
back in the network. Not only can a direct approach absent
constraint produce unwanted phenotypes, a direct approach
may not even achieve the desired reduction in long—run probabil—
ity mass of the target gene on account of complicated feedback
loops.

The aggregated mass of undesirable states for the uncontrolled
mammalian cell cycle network is 21.6“ m = 0.2012. Table 4 gives
the amount of shift made by all control policies for different
control gene candidates. The table shows that choosing E2F as
the control gene induces the maximum shift under different con—
trol policies. Thus, E2F can be considered as a potential control
gene for intervention. It is also evident that the difference be—
tween the performances of PC optimal policies and conCSSD is
signiﬁcant.

4 CONCLUDING REMARKS

Heretofore, two external control approaches have been taken to
shift the steady—state mass of a GRN: (i) use a subjectively
deﬁned cost function for which desirable shift of the steady—
state mass is a by—product and (ii) use heuristics to design a
greedy algorithm. The present article uses a linear programming
approach to optimally shift the steady—state mass and therefore
outperforms both of the preceding approaches. Moreover, it
does with less computational overhead and therefore can be
applied to larger networks. Owing to its generality, the same
basic LP structure is used for both unconstrained and con—
strained optimization problems. Thus, at least in the case of

Markovian GRNs, in particular, PBNs, previously proposed
methods can be abandoned whenever the goal is to optimally
shift the steady—state mass.

Conﬂict of Interest: none declared.

REFERENCES

Altman,E. (1999) Constrained Markov Decision Processes. Chapman & Hall/CRC,
Boca Raton, FL, USA.

Bittner,M. et al. (2000) Molecular classiﬁcation of cutaneous malignant melanoma
by gene expression proﬁling. Nature, 406, 5367540.

Brun,M. et al. (2005) Steady—state probabilities for attractors in probabilistic
Boolean networks. Signal. Process, 85, 199372013.

Ching,W.K. et al. (2009) Optimal control policy for probabilistic boolean networks
with hard constraints. [ET Svst. Biol, 3, 90799.

Cho,G.E. and Meyer,C.D. (2001) Comparison of perturbation bounds for the sta—
tionary distribution of a Markov chain. Linear Algebra Appl, 335, 1377150.

Datta,A. et al. (2003) External control in Markovian genetic regulatory networks.
Mach. Learn., 52, 1697191.

Datta,A. and Dougherty,E.R. (2006) Introduction to Genomic Signal Processing with
Control. CRC Press.

Derman,C. (1970) Finite State M arkovian Decision Processes. Academic Press, New
York.

Dissanayake,S.K. et al. (2008) Wnt5a regulates expression of tumor—associated anti—
gens in melanoma via changes in signal transducers and activators of transcrip—
tion 3 phosphorylation. Cancer Res., 68, 10205710214.

Dougherty,E.R. and Datta,A. (2005) Genomic signal processing: diagnosis and
therapy. IEEE Signal Process. Mag., 22, 1077112.

Dougherty,E.R. et al. (2010) Stationary and structural control in gene regulatory
networks: basic concepts. Int. J. Syst. Sci., 41, 5716.

Faryabi,B. et al. (2007) On approximate stochastic control in genetic regulatory
networks. [ET Syst. Biol, 1, 3617368.

Faryabi,B. et al. (2009) Intervention in context—sensitive probabilistic
Boolean networks revisited. EURASIP J. Bioinform. Syst. Biol, 2009, Aritcle
ID 360864.

 

1766

ﬁm'spzumofpmjxo'sopeuuqquIq/ﬁdnq

Inten/ention in GRNs

 

Faure,A. et al. (2006) Dynamical analysis of a generic Boolean model for the control
of the mammalian cell cycle. Bioinformatics, 22, e124—e131.

Ghaffari,N. et al. (2010) A COD—based reduction algorithm for des—
igning stationary control policies on Boolean networks. Bioinformatics, 26,
155671563.

Ivanov,I. et al. (2007) Dynamics preserving size reduction mappings for probabil—
istic Boolean networks. IEEE Trans. Signal Process, 55, 231(k2322.

Ivanov,I. et al. (2010) Selection policy—induced reduction mappings for Boolean
networks. IEEE Trans. Signal Process, 58, 48714882.

Kallenberg,L.C.M. (1983) Linear Programming and Finite Markovian Control
Problems. Mathematisch Centrum, Amsterdam.

Kauffman,S.A. (1993) The Origins of Order: Self—Organization and Selection in
Evolution. Oxford University Press, New York, NY, USA.

Law,A. and Wong,C. (2010a) Stanniocalcin—2 is a Hif—l target gene that promotes
cell proliferation in hypoxia. Exp. Cell Res, 316, 466—476.

Law,A. and Wong,C. (2010b) Stanniocalcin—2 promotes epithelialmesenchymal
transition and invasiveness in hypoxic human ovarian cancer cells. Exp. Cell
Res, 316, 342573434.

Megiddo,N. (1987) On the complexity of linear programming. In: Advances in
Economic Theory: Fifth World Congress. Cambridge University Press, New
York, NY, USA, pp. 2257268.

Pal,R. et al. (2006) Optimal inﬁnite—horizon control for probabilistic Boolean net—
works. IEEE Trans. Signal Process, 54, 237572387.

Pal,R. et al. (2008) Robust intervention in probabilistic Boolean networks. IEEE
Trans. Signal Process, 56, 128(Fl294.

Pal,R. et al. (2009) Bayesian robustness in the control of gene regulatory networks.
IEEE Trans. Signal Process, 57, 366773678.

Qian,X. and Dougherty,E.R. (2008) Effect of function perturbation on the steady—
state distribution of genetic regulatory networks: optimal structural interven—
tion. IEEE Trans. Signal Process, 56, 4966—4976.

Qian,X. and Dougherty,E.R. (2012) Intervention in gene regulatory networks via
phenotypically constrained control policies based on long—run behavior. IEEE
ACM Trans. Compat. Biol, 9, 1237136.

Qian,X. et al. (2009) Intervention in gene regulatory networks via greedy control
policies based on long—run behavior. BMC Syst. Biol, 3, 61.

Qian,X. et al. (2010) Smte reduction for network intervention with probabilistic
Boolean networks. Bioinformatics, 26, 309873194.

Ross,K.W. (1989) Randomized and past—dependent policies for Markov decision
processes with multiple constraints. 0per. Res, 37, 474477.

Seneta,E. (1988) Perturbation of the stationary distribution measured by ergodicity
coefﬁcients. Adv. Appl. Probab., 20, 2287230.

Shmulevich,I. et al. (2002a) Probabilistic Boolean networks: a rule—based uncer—
tainty model for gene regulatory networks. Bioinformatics, 18, 2617274.

Shmulevich,I. et al. (2002b) Gene perturbation and intervention in probabilistic
Boolean networks. Bioinformatics, 18, 131971331.

Shmulevich,I. et al. (2002c) Control of stationary behavior in probabilistic Boolean
networks by means of structural intervention. J. Biol. Syst., 10, 431445.

Vahedi,G. et al. (2008) Intervention in gene regulatory networks via a stationary
mean—ﬁrst—passage—time control policy. IEEE Trans. Biomed. Eng, 55,
231972331.

Weeraratna,A.T. et al. (2002) Wnt5a signaling directly affects cell motility and in—
vasion of metastatic melanoma. Cancer Cell, 1, 2797288.

Xiao,Y. and Dougherty,E.R. (2007) The impact of function perturbations in
Boolean networks. Bioinformatics, 23, 126571273.

Youseﬁ,M.R. et al. (2012) Optimal intervention strategies for therapeutic methods
with ﬁxed—length duration of drug effectiveness. IEEE Trans. Signal Process,
60, 493(k4944.

 

1 767

ﬁm'spzumofpmjxo'sopeuuqquIq/pdnq

