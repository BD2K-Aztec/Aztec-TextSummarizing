BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

D.Zhou et al.

 

Table 1. The sentences in the MLEE Corpus in which ‘hydrolysis’ was
annotated as an event trigger

 

Sentences

 

l Angiostatin inhibits both ATP synthesis and ATP hydrolysis (Moser
et al., 2001) and interferes with intracellular pH regulation (Wahl and
Grant, 2002; Wahl et al., 2002).

2 Our data suggest that VEGFR2-mediated regulation of endothelial
function is dependent on different, but speciﬁc, Rab-mediated GTP
hydrolysis activity required for endosomal trafﬁcking.

 

Table 2. Examples of the similar contexts where the two words
‘proteolysis’ and ‘hydrolysis’ occur in Medline

 

Hydrolysis Proteolysis

 

Use of indices of proteolysis of
caseins such as the proteasei
peptone, m-casein and PI

An increase of the products of
casein hydrolysis, the proteasei
peptone (pip) fraction and
minor (m) caseins

With the ease of resistance to
proteolysis, the development of
sequence-speciﬁc AApeptides. . .

AApeptides are resistant to
enzymatic hydrolysis

 

However, such approaches rely on abundant annotated train—
ing data and may not work well when certain event instances are
rare in the training data. For example, the word ‘proteolysis’
does not occur as an event trigger for catabolism type in the
training data of the multi—level event extraction (MLEE)
corpus (Pyysalo et al., 2012). Therefore, it is difﬁcult to recognize
it as an event trigger in the sentence ‘The effects of IGF—1 are
mediated principally through the IGF—1R but are modulated by
complex interactions with multiple IGF binding proteins that
themselves are regulated by phosphorylation, proteolysis, poly—
merization, and cell or matrix association’, which appears in the
test set. Nevertheless, we notice that another word ‘hydrolysis’
was annotated as an event trigger in the training data as shown in
Table 1. If we search through Medline (http://www.ncbi.nlm.nih.
gov/entrez/query/static/overviewhtml), we can find that the two
words ‘proteolysis’ and ‘hydrolysis’ occur in similar context thus
tend to have similar meanings following the distributional hy—
pothesis (Harris, 1970). Examples of the similar context where
‘proteolysis’ and ‘hydrolysis’ occur in Medline are presented in
Table 2. If we can learn such domain knowledge and incorporate
it into trigger word identiﬁcation, then ‘proteolysis’ might be
correctly identiﬁed as an event trigger even if it did not appear
in the training data at all.

In this article, we argue that biomedical domain knowledge,
such as words, tends to occur in similar context, is highly related
and this can be incorporated into the learning process of the
event trigger classiﬁer to improve the performance of trigger
word identiﬁcation. In speciﬁc, we propose a novel framework
to learn biomedical knowledge from a large text corpus built
from Medline and embed it into word features using neural lan—
guage modeling. The embedded features are further combined

with the well—designed syntactic and semantic context features
using the multiple kernel learning (MKL) method for classiﬁer
training. We conducted extensive experiments on the MLEE
corpus (Pyysalo et al., 2012), and the results show that >2.5%
improvement on F—score is achieved using the proposed frame—
work when compared with the state—of—the—art approach, demon—
strating the effectiveness of the proposed framework.

The rest of the article is organized as follows. Section 2 pre—
sents the proposed framework, which consists of three steps,
domain knowledge embedding, local context features extraction
and MKL. Experimental setup and results are discussed in
Section 3. Finally, Section 4 concludes the article.

2 OUR APPROACH

Our proposed framework for event trigger identiﬁcation works
as follows, which is illustrated in Figure 1. First, scientiﬁc pub—
lications from Medline are crawled to form a corpus where
domain knowledge can be obtained. Then a neural language
model is built from such a corpus using unsupervised learning.
The distributional representation for each word is induced as the
feature of the word (word embedding). Then, for sentences in the
training and testing datasets, protein name identiﬁcation, syntac—
tic parsing and dependency parsing are performed and local
context features are extracted from the parsing results. After
that, features induced by neural language model and features
extracted from syntactic and dependency parsing results are com—
bined through MKL. Finally, training and testing are conducted
on the combined feature set.

In what follows, we first describe how to formulate the task of
event trigger identiﬁcation as a classiﬁcation problem, in which
two sets of features, domain knowledge embedding and local
context features, are used. Then, we present how to learn the
parameters of our proposed uniﬁed classiﬁcation framework
using MKL. Finally, we discuss how the two feature sets can
be constructed.

2.1 Problem deﬁnition

Event trigger identiﬁcation in the biomedical domain can be seen
as the task of assigning labels to words. Existing approaches for
event trigger identiﬁcation typically rely on annotated training
data where those event trigger words are labeled with their cor—
responding event types. A rich set of manually designed features
are then extracted from annotated sentences and fed into a clas—
siﬁcation algorithm such as support vector machines (SVMs) for
training. In our approach here, we adopt a similar procedure
of training a classiﬁer from annotated data for trigger word
identiﬁcation. However, apart from the annotated training
data, we additionally crawled articles from Medline to form a
corpus where domain knowledge can be extracted.

Given sentences 8 = {s,~ : w,~1w,~2..w,~m,i= l...L}, their corres—
ponding trigger annotations T = {t,~ : ailaiz...am,-a i = l...L}
and an additional unannotated corpus where domain knowledge
can be extracted, 8“ = {s,~ : w,~1w,~2...w,~m, i = (L + l), (L + 2)...
(L+ UL)} where L and UL are the numbers of sentences in
the training data and the domain corpus, respectively, the object—
ive is to estimate a hypothesis f : Sv—>T minimizing the prediction
error on unseen data. For traditional machine learning

 

1588

ﬁm'spzumoﬁuoyo'sopeuuopuorq/ﬁdnq

Event trigger identification using domain knowledge

 

 

 

 

 

 

 

Medline _ Unsupervised Neural
Corpus Learning LM
Corpus Pre'
processing

 

 

    

 

 

 

 

 

 

 

 

Feature
Set 1

  
   

Local
Context Feature
Features Set 2
Etraction

Training
D ata

    
  

   
  
   
     

 

 

 

V
SVM
Classifier

Kernel
Combination

 

 

 

 

Testing
Data

Fig. 1. The system architecture of our proposed framework for event trigger identiﬁcation

approaches, f is determined by minimizing the loss between the
prediction f(<I>(w)) for the training instance w and its actual label
aw based on some loss function Loss. Here <I>(w) is the feature set
related to w. As will be shown in Section 2.4, local context
around w is used for constructing <I>(w). Moreover, to make
sure that words occurring in similar contexts share the same
class label, the loss between the prediction f2(\IJ(w’)) of w’ and
aw is also minimized, where w’ is the word that is found to have
highest contextual similarity with w from in the domain corpus
and \Il(w’) is another type of feature set related to w’. As will be
shown in Section 2.3, contextually similar words can be modeled
using neural language modeling. Because w’ is the word with the
highest contextual similarity with w, \Il(w’) can be approximated
as \IJ(w). Our final objective function is

f: argmin Z(LOSS(f1(<I>(W)),aw)+rLOSS(ﬁ(‘IJ(W)),aw)) (1)
./'=(./i‘./'2)E(‘) w

where r is a parameter controlling the trade—off between two

losses. When r20, Equation (1) reduces to the object function

for classiﬁcation based on local context features only.

2.2 Parameter learning

In our framework, we use the local context features derived from
the annotated training data for CD and use word embeddings
induced from the domain corpus using neural language modeling
for \D. We use SVM for bothfl andfz andfl 2 (W1, <I>(w)) + b1
and f2 2 (W2, \Il(w)) + b2. Therefore, the above problem can then
be solved by optimizing the parameters of w], wz,b1,b2, r.
However, these parameters cannot be optimized directly using
the general learning approach for SVM. By considering param—
eters optimization as learning the optimal weights of different
types of features from the data automatically, the problem is
converted into to feature combination. Under kernel learning,
feature combination is translated into kernel combination by
defining two kernels K1,K2 based on <I>(w),\ll(w). There are
many possible ways for kernel combination. A simplest one is
to average several kernels by setting r: 1.

In our work here, we use MKL (Bach et al., 2004), which has
been shown to produce good results in object classiﬁcation in
computer vision (Gehler and Nowozin, 2009). The aim of MKL
is to learn a kernel combination during the training phase of the
algorithm by optimizing jointly over a linear combination of
kernels  ﬁ,K,~(w, w’) and the parameters of an SVM, where

m is the number of kernels to be combined. Under MKL, the
object function described in Equation (1) is changed to

1 2 N 2
mph}; 5 2 when; + C2 L(a,.,, b + 2 5,1900%) (2)
0“ ‘ i=1 j=l i=1

where N is the number of training instances, C is a predeﬁned
positive trade—off parameter between model simplicity and clas—
sification error, typically used in SVMs, or = (on, ...,OtN)T is the
vector of dual variables corresponding to each separation con—
straint, K1(w) = ((<I>(wl), <I>(w)), ..., (<I>(WN), <I>(w)))T, K2(w) 2
(014m), W»,  M»th w(»v>>>T, K1 = (<<I>(w.~>,<1>(wj>>>NxN,
K2 2 ((W(w,~), \IJ(w,«)))NxN and L(awj, t) = max(0, l — awj t) is the
hinge loss. For efﬁciency and interpretability, the objection func—
tion subjects to

51+52=1,5120,5220 (3)

Here, ﬂl and ﬁg are the weighting of two features set. The prob—
lem can be solved using the SimpleMKL (Rakotomamonjy et al.,
2008) Toolbox (http://asi.insa—rouen.fr/enseignants/~arakotom/
code/mklindex.html). The decision function is of the following
form,

2
sigmz t3.~<1<,~(x>Ta+b>> (4)
i=1

2.3 Word embeddings learned by neural
language modeling

We use neural language modeling (Huang et al., 2012) to learn
word representations by discriminating the next word given its
local context and global context. Given a word sequence
s,~ = (w,~1,w,~2,...,w,~n) and a document  =(W/1,W/2,...,W/m),
which contain 5,, the goal of the model is to discriminate the
win (the correct one) from a random word w. Thus, the object
function of the model is to minimize the ranking loss for each
(Si) 

Z: Z max(0, l —f(s,~,d;) +f(s}",a’,«), (5)

i j we V\w,~,,

where 5}" = w,~1, M2,...,W,"n,1,W is the sequence by changing the
last word win into w. The dataset for learning the language model
can be constructed by considering all the word sequences in the
Medline corpus. Positive examples are the word sequences from

 

1589

ﬁm'spzumol‘pmjxo'sopeuuopuorq/ﬁdnq

D.Zhou et al.

 

Medline, whereas negative examples are the same word sequence
with the last word replaced by a random one.

Instead of using only local context for language model learn—
ing, document context (or global context) is also considered.
Thus, the score function f(s,~, (l1) is replaced by two functions,
score’(s,~, (if) and scor6g(Sr, (1;), Which are defined to capture
local context and global context, respectively.

The score function of local context score’(s,~, (l1) is calculated by
a neural network with one hidden layer:

a1 =g(WiX’ +bi> (6)

score[ = leall + bl2 (7)

where X[ = [.x1,x2,...,xn] is the concatenation of the n word
embeddings representing sequence s,-, g is an element—wise acti—
vation function such as tanh, all is the activation of the hidden
layer with h' hidden nodes, W’l and W’2 are the ﬁrst and second
layer weights of the neural network, respectively, and b1, b’2 are
the biases of each layer.

The score function of global context scoreg(s,~, (l1) is calculated
by a two—layer neural network:

ai=h(Wth+bi> (8)
scoreg = Wﬁaf + bi (9)
where
Z ¢(W./z)Xz
Xg = [’i‘n—m
:1 $017!)

which is the weighted average of all word vectors in the docu—
ment (ll, (j) is a weighting function describing the importance of
word w_,-, in the document (ll, h is an element—wise activation func—
tion such as tanh, af 6 Rhg“ is the activation of the hidden layer
with hg hidden nodes, W and W are the first and second layer
weights of the neural network, respectively, and bf, bi are the
biases of each layer. The local score preserves word order and
syntactic information, whereas the global score uses a weighted
average that is similar to bag—of—words features, capturing more
of the semantics and topics of the document.

The gradient of the objective is sampled by randomly choosing
a word from the vocabulary as a corrupted example for each
sequencedocument pair (s,~, The derivative of the ranking
loss is taken with respect to the parameters and these weights
are updated via backpropagation.

2.4 Local contexts features

The syntactic and semantic features used in the framework are
generated from the outputs of GDep (a dependency parser)
(Sagae and Tsujii, 2007) and Enju parser (a syntactic parser)
(Miyao and Tsujii, 2008).

All the features used in the framework are extracted based on
(Pyysalo et al., 2012), described as follows:

0 Lexical and syntactic features of the word itself. The features
such as whether the word has a capital letter, whether it is at
the beginning of the sentences, whether it has a number,

whether it has a symbol, whether it is in a trigger word
dictionary, whether it is in a protein base form, its POS
tag and n—grams of characters (n = 2,3,4) are extracted.
For features like whether it has certain property, boolean
value is used for the feature value. In addition, to check
whether a word is in the trigger word dictionary, we con—
structed a dictionary by collecting all the trigger words from
the training set. Triggers that contain more than one word
are ﬁltered. Also, hyphenated compound words are added
into the dictionary if one of its words already appears in the
trigger word dictionary.

Local context features. For the sequence of three words
before or after the candidate word, n—grams (n = l, 2, 3, 4)
are used. For example, for the word ‘retarget’ in the sentence
‘The binding of I kappa B/MAD—3 to NF—kappa B p65 is
sufﬁcient to retarget NF—kappa B p65 from the nucleus to
the cytoplasm’, the word sequence ‘is sufﬁcient to retarget
protein from the’ is used to generate the relevant n—grams.
Also, each word is represented by its base form, the POS tag
and the relative position (before or after) to the target word.

Local dependency features. The two—depth path started from
the candidate word in the dependency tree generated from
the GDep parser is identiﬁed first. Features are then ex—
tracted from the path such as n—grams (n: 2) of dependen—
cies, n—grams (n = 2, 3) of words represented by their base
forms and the POS tags and n—grams (n = 2, 3, 4) of depen—
dencies and words. For word tokens not having two—depth
paths, such as the root node or the direct children of the root
node, these types of features are ignored. N—grams (n = 2) of
dependencies are represented as dependencyldependency2.
Similarly, n—grams (n = 2, 3) of words or n—grams
(n = 2,3,4) of dependencies and words are represented as
wordliword2iword3 or wordlidependencyliword2 and so
on. For example, for the word ‘retarget’ in the sentence ‘the
binding of I kappa B/MAD—3 to NF—kappa B p65 is sufﬁ—
cient to retarget NF—kappa B p65 from the nucleus to the
cytoplasm.’, its two—depth path ‘retarget —> AMOD —> suf—
ficient —> PRD —> is’ can be retrieved from the GDep par—
sing results. Its n—grams (n = 2) of dependencies are given as
‘AMOD PRD’.

Shortest path features. The shortest path, a directed path
between the candidate and the closest protein, is also
retrieved from the dependency parse generated from GDep
parser. The vertex walks, edge walks, n—grams (n = 2, 3, 4) of
dependencies, n—grams (n = 2, 3,4) of words represented as
base forms plus POS tags and the length of path are ex—
tracted as the path features. For example, for the word
‘retarget’ in the sentence ‘The binding of I kappa B/MAD—
3 to NF—kappa B p65 is sufﬁcient to retarget NF—kappa B
p65 from the nucleus to the cytoplasm.’, its shortest path is
‘retarget<— OBJ <—protein’. The length of path that is l,
edge walks as retarget<— OBJ <—protein, vertex walks as
OBJ can be extracted. The reason of using shortest path is
that a candidate and its closest proteins are much more
likely to be involved in a biomedical event. Thus, features
extracted from the shortest path should be useful for detect—
ing triggers in biomedical event extraction.

 

1590

ﬁm'spzumol‘pmjxo'sopeuuopuorq/ﬁdnq

Event trigger identification using domain knowledge

 

3 EXPERIMENTS

In this section, we present our experiments to evaluate the effect—
iveness of the proposed framework. We will first discuss results
obtained on event trigger identiﬁcation in comparison with the
best performance obtained so far. We will then present perform—
ance comparison results with or without using the MKL method
for comparison, followed by the results of using neural language
models trained under different corpora. Finally, we compare our
results with those obtained using the latent Dirichlet allocation
(LDA) model as another distributional semantics approach
instead of neural language modeling.

3.1 Experimental setup

We used MLEE corpus for our experiments on trigger words
identification. Instead of focusing exclusively on molecular—
level entities and process, MLEE corpus is extended to encom—
pass all levels of biological organization from the molecular to
the whole organism. The corpus is generated from 262 PubMed
abstracts on angiogenesis, which involves a tissue/organ—level
process closely associated with cancer and other organism—level
pathologies. Texts in that domain represent a good test case for
event extraction across multiple levels of biological organization.
The annotation follows the guideline formalized in the BioNLP
2009 Shared Task on event extraction. In this guideline, events
are n—ary associations of participants (entities or other events)
with speciﬁc role such as theme and cause. Each event is assigned
a type from a ﬁxed set defined for the task (e.g. binding and
phosphorylation) and is associated with a speciﬁc span of text
stating the event, termed the event trigger. The events are cate—
gorized as four groups such as ‘ANATOMICAL’,
‘MOLECULAR’, ‘GENERAL’ and ‘PLANNED’, which are
further classiﬁed into 19 classes. These 19 classes are the target
classes of our trigger word classiﬁer. It is worth noting that we
used a combination of training and development datasets of the
MLEE corpus for training, and the test set for testing.

To train a neural language model, we additionally built a
corpus from Medline because of its wide coverage of topics in
the biomedical domain. Abstracts of biomedical literature pub—
lished in 2011 and 2012 were retrieved to build the corpus.

All the sentences in the Medline corpus were preprocessed
such as lowercasing and stemming. We chose the most frequent
words in the corpus to construct vocabularies with different size
D = {15, 000, 30,000, 60,000,90,000}. Words starting with
a digital number are mapped to the ‘NUMBER’ token. Words
starting with a special character are mapped to the ‘UNUSUAL’
token. Other rare words not in the dictionary are replaced with
the ‘UNKNOWN’ token. For neural language model training,
we used 50 dimensional embeddings and set the number of
hidden units to 100.

3.2 Experimental results

This section presents the evaluation results in details. In our
framework, the one—versus—rest SVMs are used for trigger word
classiﬁcation. To alleviate the unbalanced classiﬁcation problem,
we boosted the positive examples by placing more weights on
them during training.

Table 3. Comparison of the performance of event trigger identification

 

 

Method Recall (%) Precision (%) F-score (%)
Baseline 81.69 70. 79 75. 84
Proposed 81.29 75.56 78.32

 

3.2.1 Event trigger identification results We implemented a
baseline following the approach proposed in (Pyysalo et al.,
2012), which achieved the state—of—the—art performance on trigger
word identiﬁcation using the features extracted from the syntac—
tic and semantic parsing results as described in Section 2.4. We
conducted experiments on the MLEE corpus and compared our
framework with the baseline approach. Table 3 lists the recall,
precision and F—score obtained on the test set of the MLEE
corpus. In the results reported here, we trained a neural language
model on the Medline corpus with the vocabulary size of 30 000.
Using the features induced from the learned neural language
model, the performance of event trigger identiﬁcation is
improved significantly with ~5% on precision. The overall im—
provement on F—score is ~2.5%. To further investigate how the
improvement is achieved, we analyzed the experimental results of
the baseline approach and the proposed framework. We found
that positive instances identiﬁed correctly by the baseline ap—
proach are still identiﬁed correctly by the proposed framework
in 97.8% of cases. Out of the false—negative instances identiﬁed
by the baseline, 7.8% were correctly identiﬁed as positive in—
stances by our framework.

To further study the difference of our proposed framework
against the existing state—of—the—art approach in different event
categories, we list the detailed results in each event category in
Table 4. It can be observed from the table that of 19 event types,
our proposed framework outperforms the baseline approach on
13 event types and gives almost identical results on another 5
event types. To investigate the performance improvement under
different event types, we analyze the relationship between per—
formance improvement and the size of the training data in each
event category. The results are illustrated in Figure 2. It can be
observed that the performance improvement decreases when the
size of the training data increases. The largest improvement
(100%) is achieved in the ‘dephosphorylation’ event type when
there are only five training instances. Our approach successfully
identiﬁed the ‘dephosphorylation’ event triggers in all three in—
stances in the test set, while the baseline approach failed to iden—
tify any of them. When the training data are relatively abundant,
our approach appears to have less improvement compared with
the baseline.

From the above observations, we can speculate that our
proposed framework with domain knowledge incorporated is
particularly effective when facing with scarce training data.
The only exception is the ‘transcription’ event type with 30 train—
ing instances for which the baseline identiﬁed one event trigger
correctly from the test set, while our approach failed to recognize
any. It is shown as negative performance improvement in
Figure 2. One possible reason is that words contextually similar
to ‘transcription’ are not annotated in the training set either.

 

1591

ﬁm'spzumol‘pmjxo'sopeuuopuorq/ﬁdnq

an?kgogmomammowoxmoagocgawbmﬁ

 

Event trigger identification using domain knowledge

 

Table 7. Event trigger identiﬁcation performance with neural language
model with different vocabularies

Table 9. Event trigger identiﬁcation performance using neural language
modeling versus LDA

 

 

Size of vocabulary Recall (%) Precision (%) F-score (%) Method Recall (%) Precision (%) F-score (%)
15 000 82.03 73.90 77.75 Baseline 81.69 70.79 75.84

30000 81.29 75.56 78.32 LDA 81.12 71.64 76.08

60 000 81.29 75 78.02 NLM 81.29 75.56 78.32

90 000 80.60 74.68 77.53

 

Table 8. Event trigger identiﬁcation performance with neural language
model trained from difference sources

 

 

Method Recall (%) Precision (%) F-score (%)
Wikipedia 82.60 70.50 76.07
Medline 81.29 75.56 78.32

 

here, they cover at least 95% of word occurrences in the whole
corpus.

Table 7 lists the results obtained on the test set of the MLEE
corpus with different vocabulary size. It can be observed that the
ﬁnal performance of the proposed framework outperforms the
baseline approach regardless which vocabulary was used. The
relative improvement on F—score ranges between 1.7 and 2.5%.
We also observe that increasing the vocabulary size improves the
performance with the peak reached at 30 000. Based on the above
observation, we can conclude that the choice of the vocabulary
can be made by considering its coverage of the words in the
corpus.

3.2.4 Learning neural language model from dif erence source To
explore the effectiveness of embedding domain knowledge into
language model, we compare the event trigger identiﬁcation re—
sults with neural language model trained on Wikipedia
(Collobert et al., 2011). The Wikipedia corpus contains a wide
range of topics in general domains. The results are shown in
Table 8. Compared with the baseline approach, using the
Wikipedia corpus did not appear to improve the performance
of event trigger identiﬁcation. Nevertheless, learning the neural
language model from the Medline corpus gives superior perform—
ance on event trigger identiﬁcation than the baseline. Only
domain—speciﬁc knowledge can be used to improve the perform—
ance of event trigger identiﬁcation.

3.2.5 Neural language model versus topic model To further in—
vestigate the effectiveness of neural language model, we compare
the event trigger identiﬁcation results with word classes induced
by the LDA model, which is a generative graphical model ori—
ginally proposed for topic discovery (Blei et al., 2003). Assuming
that each document is represented as an unordered collection of
words and characterized by a particular set of topics, disregard—
ing grammar and word order, the LDA model can be used for
grouping the words in similar topics in an unsupervised way.
Each word in the LDA model is represented as probability

 

distribution over topics, and then combined with the features
described in Section 3.2 for training SVM classiﬁers for event
trigger identiﬁcation. In our experiments, the LDA model by
varying the number of topics {50, 100, 150, 200, 250} using the
Stanford topic modeling toolbox (http://nlp.stanford.edu/down—
loads/tmt/tmt—0.4/). The optimal topic number is chosen using
the perplexity measure on the 10% held—out set from our
Medline corpus. The ﬁnal event trigger identification results
using LDA are reported in Table 9 by setting the topic number
to 200. It can be observed that LDA only gives an almost neg—
ligible improvement of 0.24% in F—score compared with the
baseline and it performs worse than our proposed framework
using neural language modeling. Two possible reasons are (i)
LDA ignores word ordering in documents, which is important
when comparing words occurring in similar semantic context and
(ii) it is difﬁcult to choose the proper number of topics (or word
classes) that group words into well—separated semantic clusters.
On the contrary, our proposed framework is based on neural
language modeling, which learns the distributional representa—
tion of words without the need of specifying the number of
induced word classes.

4 CONCLUSIONS AND FUTURE WORK

In this article, we have proposed a novel framework to construct
a feature set for learning classiﬁers for event trigger identiﬁca—
tion. In particular, biomedical domain knowledge is learned
from a large text corpus built from Medline and embedded
into word features using neural language modeling. The
embedded features are combined with the well—designed syntactic
and semantic context features, which is further used for event
trigger classiﬁer learning. Experimental results on the MLEE
corpus show that >2.5% improvement on F—score is achieved
by the proposed framework when compared with the state—of—
the—art feature—based approach, demonstrating the effectiveness
of our proposed framework. In future work, we will further in—
vestigate the feasibility of our proposed framework on other
corpora. Another possible future direction is to incorporate
domain—speciﬁc prior knowledge into neural language model
learning using semi—supervised learning to further improve the
performance of event trigger identiﬁcation.

ACKNOWLEDGEMENTS

The authors thank the anonymous reviewers for their insightful
comments. They also thank Dr Makoto Miwa for his suggestions
on constructing the baseline system.

 

1 593

ﬁm'spzumol‘pmjxo'sopeuuopuorq/ﬁdnq

D.Zhou et al.

 

Funding: This work was funded by the National Natural Science
Foundation of China (61103077), Ph.D. Programs Foundation
of Ministry of Education of China for Young Faculties
(20100092120031), the Scientiﬁc Research Foundation for the
Returned Overseas Chinese Scholars, State Education
Ministry, the Cultivation Program for Young Faculties of
Southeast University and the Shenzhen International
Cooperation Research Funding (GJHZ20120613110641217).

Conﬂict of Interest: none declared.

REFERENCES

Bach,F.R. et al. (2004) Multiple kernel learning, conic duality, and the SMO algo—
rithm. In: Proceedings of the 21st International Conference on Machine Learning.
New York.

Blei,D. et al. (2003) Latent Dirichlet allocation. J. Mach. Learn. Res., 3, 99371022.

Collobett,R. et al. (2011) Natural language processing (almost) from scratch. J.
Mach. Learn. Res., 12, 249372537.

Gehler,P. and Nowozin,S. (2009) On feature combination for multiclass object clas—
siﬁcation. In: IEEE 121/1 International Conference on Computer Vision 2009,
Vol.1. pp. 217228.

Harris,Z. (1970) Distributional structure. In: Papers in Structural and
Transformational Linguistics. D. Reidel Publishing Company, Dordrecht,
Holland, pp. 7757794.

Huang,E.H. et al. (2012) Improving word representations via global context and
multiple word prototypes. In: Annual Meeting of the Association for
Computational Linguistics 2012, Vol. 1. pp. 8737882.

Kim,J.—D. et al. (2009) Overview of bionlp’09 shared task on event extraction. In:
Proceedings of the Workshop on BioNLP. NJ, pp. 179.

Kim,J.D. et al. (2012) The genia event and protein coreference tasks of the bionlp
shared task 2011. BMC Bioiry’ormatics, 13, SI.

Miyao,Y. and Tsujii,]. (2008) Feature forest models for probabilistic hpsg parsing.
Comput. Linguist, 34, 35780.

Nedellec,C. et al. (2013) Overview of bionlp shared task 2013. In: Proceedings of the
BioNLP Shared Task 2013 Workshop. Bulgaria, Soﬁa, pp. 177.

Pyysalo,S. et al. (2012) Event extraction across multiple levels of biological organ—
ization. Bioiry’ormatics, 28, i5757i581.

Rakotomamonjy,A. et al. (2008) SimpleMLK. J. Mach. Learn. Res., 9, 249172521.

Sagae,K. and Tsujii,]. (2007) Dependency parsing and domain adaptation
with LR models and parser ensembles. In: EMNLP—CoNLL'2007, Vol. 1,
pp. 10444050.

Zhou,D. and He,Y. (2011) Biomedical events extraction using the hidden vector
state model. In: Artificial Intelligence in Medicine, Vol. 53, pp. 2057213.

 

1 594

ﬁmspzumol‘ptoyo'sopeuuowtotq/pdnq

