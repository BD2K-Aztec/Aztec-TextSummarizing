ORIGINAL PAPER

Vol. 27 no. 7 2011, pages 903-911
doi:10. 1 093/bioinformatics/btr040

 

Genome analysis

Advance Access publication January 28, 2011

Conveyor: a workflow engine for bioinformatic analyses

Burkhard Linke1’*, Robert Giiegerichi2 and Alexander Gioesmann1
1Bioinformatics Resource Faciliy, Center for Biotechnology and 2Faculty of Technology, Bielefeld University, 33615

Bielefeld, Germany
Associate Editor: Alex Bateman

 

ABSTRACT

Motivation: The rapidly increasing amounts of data available from
new high-throughput methods have made data processing without
automated pipelines infeasible. As was pointed out in several
publications, integration of data and analytic resources into workflow
systems provides a solution to this problem, simplifying the task
of data analysis. Various applications for defining and running
workflows in the field of bioinformatics have been proposed and
published, e.g. Galaxy, Mobyle, Taverna, Pegasus or Kepler. One
of the main aims of such workflow systems is to enable scientists
to focus on analysing their datasets instead of taking care for
data management, job management or monitoring the execution
of computational tasks. The currently available workflow systems
achieve this goal, but fundamentally differ in their way of executing
workflows.

Results: We have developed the Conveyor software library, a
multitiered generic workflow engine for composition, execution and
monitoring of complex workflows. It features an open, extensible
system architecture and concurrent program execution to exploit
resources available on modern multicore CPU hardware. It offers the
ability to build complex workflows with branches, loops and other
control structures. Two example use cases illustrate the application of
the versatile Conveyor engine to common bioinformatics problems.
Availability: The Conveyor application including client and server are
available at http://conveyor.cebitec.uni-bielefeld.de.

Contact: conveyor@CeBiTec.Uni-Bielefeld.DE; blinke@ceBiTec.Uni-
Bielefeld.De.

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on October 26, 2010; revised on January 7, 2011 ; accepted
on January 19,2011

1 INTRODUCTION

Workﬂows have become an important aspect in the ﬁeld of
bioinformatics during the last years (e.g. Romano, 2007 and
Smedley et (11., 2008). Applications like Galaxy (Goecks et (11.,
2010), Taverna (Hull et (11., 2006), Pegasus (Deelman et (11., 2005)
and Kepler (Altintas et (11., 2004) offer an easy way to access
local and remote resources and perform automatic analyses to test
hypotheses or process data. In many cases, they have become
a reasonable alternative to write simple software tools like Perl
scripts, especially for users without an in-depth computer science
background. Libraries like Ruffus (Goodstadt, 2010) add workﬂow

 

*To whom correspondence should be addressed.

functionality to programming languages, providing methods to
deﬁne and build workﬂow within own applications.

A workﬂow is built from several linked steps that consume inputs,
process and convert data and produce results. The most simple
workﬂows are linear Chains of steps to convert input data to the
required output. More complex setups may also include loops,
branches, parallel and conditional processing, reading from various
sources and writing different outputs in various formats.

For creating reusable workﬂow components, a processing step
may be composed of nested processing steps, allowing the user to
build complex pipelines for higher level analysis.

Many workﬂow engines act as a wrapper using existing command
line utilities or enact and orchestrate existing web services as
basic modules for their processing steps. As a result, adding new
processing steps by wrapping existing applications often does only
require little or no programming effort.

Of course, this comes at a price. Passing data between processing
steps depends on a common data format, especially in the case
of distributed processing nodes. Most analysis tools available as
command line applications or web services are consuming simple
text formats, e.g. the FASTA format for DNA and amino acid
sequences. These formats are in turn used by the workﬂow engines
to exchange data between processing nodes. Integrating other
processing nodes or input sources requires explicit data conversion
prior to processing. This often leads to the loss of information;
e.g. gene features annotated in EMBL or GenBank entries cannot
retain all qualiﬁers after converting them to FASTA format. An
analysis done by Wassink et a1. (2009) shows that most tasks
used in publicly available Taverna workﬂows are dedicated to
data conversion. To some extent, this problem is solved by meta
information provided with types that allow the deﬁnition of type
hierarchies and interfaces. The BioMoby (Wilkinson et (11., 2008)
data type management is an example for an ontology-based approach
to data type handling; nonetheless, it requires extra efforts by the
developer and/0r maintainer. Other attempts to deﬁne common data
types like BioXSD (Kalas et (11., 2010) or Seibel et al. (2006)
were made, but none of them has been successfully adopted by
the community yet. The situation is even worse if legacy data from
applications are to be integrated into a workﬂow. Accessing data, e. g.
stored in a relational database or available by a local application
only, requires special processing steps; passing the data between
distributed processing nodes may not be possible at all.

Another problem arises from the nature of web services used in
processing steps. Although they offer an elegant and easy way to
provide and consume useful services, users have to be aware of
the pitfalls of web services if they rely on them for an analytical
workﬂow. A service may become unavailable without prior notice

 

© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 903

112 ﬁlo's[Bruno[pJOJXO'sorwurJOJurorqﬂ:duq 11101} papBOIII/lAOG

91oz ‘Og anﬁnv uo ::

B.Linke et al.

 

of the provider. For plain web services, only the syntax of input and
output data types is deﬁned, for example by using a WSDL ﬁle. They
completely lack information about semantics and whether a data type
is compatible to another type used in a different service. Passing
large amounts of data to or from web services adds a noticeable
processing overhead. Last but not least, external web services should
not be used with conﬁdential data, since the exact service provider
and the means of transferring data (unsecure/secured by https) are
unknown in many workﬂow systems. In a scenario working with
conﬁdential data, all services used in a workﬂow system should be
deployed in the internal network only.

Workﬂow systems also attempt to offer an alternative way to
run more complex analyses and thus replace small software tools,
e.g. Perl scripts. Controlling the ﬂow of data in workﬂows is
essential for these applications. This includes branching the ﬂow
and using alternative processing based on intermediate results, and
enabling/disabling complete processing branches for reusability of
workﬂows.

With high-throughput methods producing more and more data,
using web service or grid-based solutions introduces additional
bottlenecks, and service providers will likely get into problems
providing the necessary resources. A possible solution could be
the integration of local compute infrastructures into the workﬂow,
accessing external resources for special tasks only.

A thorough analysis of the mentioned systems shows that all offer
some of these requested features. Taverna provides a considerable
amount of processing nodes based on various web service registries,
but support for a local execution or control ﬂow is rather limited.
Mobyle (Neron et (11., 2009) offers a web-based workﬂow system,
but lacks the necessary ﬂow control elements to create complex
workﬂows. Pegasus is designed for large grids and includes the
necessary interfaces for handling distributed data and various batch
systems, but operates on ﬁle-based data only. Galaxy is also able
to utilize compute Clusters and grids, and also uses plain formats
only. Kepler ﬁnally implements almost all of the requested features,
but its type system only differentiates between primitive types and
opaque objects for data tokens, without including features like type
hierarchies or interfaces.

In contrast to the other workﬂow tools considered so far, Rufﬁis
is a library providing a framework for deﬁning workﬂow steps
and executing pipelines within Python applications. Although the
indiviual steps are implemented as Python functions, it does not
utilize Python data types, but relies on ﬁles.

2 APPROACH AND DESIGN

To overcome these limitations, we have developed Conveyor
as a novel software library offering its ﬁmctionality to other
applications. A Client-server setup based on the library is used to
separate the design of a workﬂow from its execution. Additionally,
a command line tool allows to execute workﬂows in a batch
manner. As a software library, Conveyor may also be included into
other applications, providing an easy to update and maintain data
processing layer.

Workﬂows in Conveyor are represented by directed graphs,
composed of nodes for processing and input/output steps, and edges
moving data between nodes. This design allows simple pipelines
built from concatenated processing steps, and also complex graphs
with branches, loops and parallel ﬂow of data. Data are passed one

by one between nodes, creating a stream of input data. This model
enables the parallel processing of nodes, using multiple CPU cores
if available. For a ﬁrst impression, the reader may look ahead at the
genome annotation workﬂow presented in Section 4 (Fig. 6).

Node and data types in Conveyor are constructed with a strongly
typed, interface driven, object-oriented design. It supports an
easy wrapping of existing Classes for ﬁmctionality and thus the
integration of existing applications into Conveyor. The interface-
driven design makes processing steps usable with any appropriate
data type. Relations between data types can be easily expressed by
object-oriented concepts like inheritance and do not require extra
efforts.

2.1 Data types

The deﬁnition of a data type in Conveyor only requires marking
a Class with a special empty interface. No ﬁirther restrictions are
applied to data types. Relations between types are expressed in an
object-oriented way by inheritance or by implementing additional
interfaces.

2.2 Node types

Node types are derived from certain abstract Classes that provide a
default implementation of the node life cycle and execution control.
The developer does not have to take care of the life cycle at all;
nonetheless, the base Classes are designed to allow developers to
override the default behaviour if necessary.

Important information about node types is expressed by meta data
(attributes in .NET). This includes a human-readable name for the
node type, a description and a Classiﬁcation of the type based on
tags. This information is also available in the design GUI to help
selecting the right node type for a speciﬁc task.

A node Class may contain a number of elements that are examined
by reﬂection and it deﬁnes the way a user can work with a node:

Endpoints: an endpoint is a data exchange point of a node, either
consuming data or producing data. A Conveyor instance is build by
creating nodes and connecting their endpoints, thus modelling the
data ﬂow in the graph. An endpoint itself is deﬁned as generic data
type, with the type parameter being the type of data that is exchanged
via the endpoint. As a result, the node Class implementation does not
need to use type casts to work with data elements exchanged at an
endpoint. The number of endpoints is not limited by the design;
the currently implemented node Classes have between one and up
to eight endpoints. Nonetheless, at least one endpoint is required to
make a node Class usable at all. Similar to the node Class itself, an
endpoint may be decorated with a description via attributes that are
visible to the user.

Conﬁguration ﬁelds: these ﬁelds deﬁne conﬁguration values that
may inﬂuence the processing within a node Class. Table 1 shows
the available conﬁguration types. Conﬁguration ﬁelds are visible to
the user and for mandatory ones, the user is required to enter valid
information prior to running the graph. A node Class may have any
number of conﬁguration ﬁelds and it is the responsibility of the Class
itself to Check that all ﬁelds contain reasonable values. Fields may
be decorated with attributes, providing a user visible description, a
default value and a ﬂag to mark them as optional.

Settings: in contrast to conﬁguration ﬁelds mentioned before,
settings are static objects that refer to a node Class instead of a

 

904

112 ﬁlo's[BumoIpJOJXO'sorwurJOJurorq”:duq uror} papBOIII/lAOG

91oz ‘Og anﬁnv uo ::

Conveyor

 

Table 1. Conﬁguration ﬁeld types available in Conveyor nodes

 

 

Type Description

Integer 32 bit signed integer

Long 64 bit signed integer

Boolean boolean value

Double IEEE double precision value

String UTF-8 string

File A stream read as ﬁle

Enumeration<T> One of the values of the enumeration type T

Selection<T,U<T> > One of the values of type T, generated by type U

 

The enumeration type uses the values from the enumeration type given as type
parameter, eliminating the need for comparing string values in the node code. The
selection type provides a more generic way to deﬁne a selection of values. The ﬁrst
parameter is the item type, and the second parameter a class generating these items.
Selection types are used for example to provide a list of BLAST databases by scanning
certain directories.

Table 2. Setting types available in Conveyor

 

Type Description

 

A boolean value

A directory in the ﬁle system, with optional check for
existence of the directory and entries in the directory.
Double A ﬂoating point value

Boolean
Directory

Executable A binary in the ﬁle system, with optional scanning of the
default search path for the binary.

Integer A 32 bit integer

String A UTF-8 string

StringList A list of UTF-8 strings

Type A class implementing a given interface

TypeList A list of classes implementing an interface

 

Static ﬁelds using these types allow for an easy conﬁguration of plugins and the core
system by external conﬁguration ﬁles.

single node instance. They can be used to deﬁne setup and policy-
dependent values like the paths to executables or default values.
Table 2 shows the various types available as settings in node classes.
The value of a setting cannot be manipulated by the user. Instead,
the system may either deduce the value itself if possible (e.g. in the
case of an executable, Conveyor uses the application search path to
locate executables) or the administrator may use a simple graphical
user interface after installing Conveyor for adjusting the settings.
To avoid an unnecessary growth of the number of node and data
types and to reduce redundancy, Conveyor supports generic types for
both data and node elements. Generics add another abstraction layer
to types, allowng developers to implement ﬁlnctionality shared
between similar, but not related types. The best known example are
the list types in Java or C#. The generic type contains all functionality
necessary for the list, the concrete type only determines the element
type of the list. This feature simpliﬁes the implementation of generic
node types, and allows their type safe use. Certain functionality
only needs to be implemented once as a generic node type. It
also allows the creation of higher order functions as known from
ﬁlnctional programming languages. For example, the core library
already contains nodes acting similar to the well-known map and
fold operations for list types. An example node class implementation

1 using System;
2 using System.ComponentModel;

3 using Conveyor.Core;

4 using log4net;

5

6 namespace Conveyor.Dictionary

7 l

8 [Name("CreateDictionary"ll

9 [Descriptionﬂcreates a dictionary using keyrvalue pairs")]
10 public sealed class Createnictionary (T, U) : NodeProcessing
11 where T : Data where u; Data

12 l

13 [Description ("key of a pair"l]

14 private InputLinkEndpoint <T> key;

15

15 [Description ("value or a pair")l

17 private InputLinkEndpoint <U> value;

18

19 [Description("tne dictionary after reading all available key-value pair")l
20 private OutputLinkEndpoint <Dictionary<T, u» dictionary;
21

22 private Dictionary<T, u) current;

23

24 public override void Verify (l l

25 base.Verify (i;

26 current : new Dictionary <T, U>(),-

27 )

28

29 public override void Finish () (

30 dictionary .pushlcurrent );

31 base.Finish (i.-

32 )

33

34 public override void Process (1 (

35 Tk : key.Pull();

36 U v : value.Pulll);

37 if (current.ContainsKeylk))

38 // print a warning about duplicate keys

39 LogManager .GetLogger (GetType l) .GetGenericTypeDefinition(J).
4o WarnFcrmat ("key (0) already exists in dictionary in node I1)",k,this);
41 else

42 current.Addlk,v);

43 )

Fig. 1. Example of a node class for creating a dictionary from key-value
pairs. The code shows the complete class, including documentation, all
methods and ﬁelds. The dictionary is built by collecting key-value pairs
read from the inputs. The node terminates if either input is depleted, and the
dictionary is sent to the output. Lines 10+ 11 deﬁne the node class, using
two generic type parameters for the key and value types. Line 8 contains a
user-Visible name for the class, and line 9 a description of the function. Lines
13720 declare the ﬁelds for the two input endpoints and the output endpoint,
including a description for each endpoint. The remaining lines overwrite the
default life cycle methods to initialize the internal dictionary for collecting the
key-value pairs, and sending it to the output upon termination. The Process()
method in lines 3442 also checks whether a key is already stored in the
dictionary and prints a warning to a conﬁgurable logging facility.

is shown in Figure 1. It deﬁnes a generic node type for building a
dictionary from key—value pairs.

The Conveyor client-server system does not solely address the
needs of an end user; by allowing complex control structures like
branches and loops, it also focuses on experienced users with a
computer science or data analysis background, and it is intended
as a replacement for analyses written in Perl scripts or other
programming languages.

2.3 Life cycle

Figure 2 depicts the life cycle of a node in a Conveyor instance. The
methods used to change from state to state are as follows:

Verify(): after reading the deﬁnition of a node instance in a Conveyor
instance from a XML formatted ﬁle, the processing engine creates
an object of the node type given in the deﬁnition, sets conﬁguration
values and creates connections between endpoints of nodes as
deﬁned in the XML ﬁle. It then invokes this method to check

 

905

112 ﬁlo's[BumoIpJOJXO'sorwurJOJurorq”:duq wort papeolumoq

91oz ‘Og lsnﬁnv uo ::

B.Linke et al.

 

 

 

Verify() VERIFIED
CREATED Heady”

Process()

 

 

 

 

Fig. 2. Life cycle of Conveyor nodes: after creation and conﬁguring the
node (e.g. setting conﬁguration values and links to other nodes), the Verify()
method validates the conﬁguration. Upon successful validation, the node
enters the VERIFIED state and is able to process data using the Ready() and
Process() methods. The life cycle is terminated either by calling Finish()
to indicate normal termination and changing to the FINISHED state, or by
calling Error() in case of an exception during processing.

that (i) all endpoints of the node are connected, (ii) the data types
deﬁned for the endpoints are compatible and (iii) all mandatory
conﬁguration ﬁelds are set. Derived node types may overwrite
and extend this method, e.g. to check their conﬁguration ﬁelds for
reasonable values, open ﬁles, connect to a database or to perform
any operation necessary to setup a node.

Ready(): nodes within a Conveyor instance are executed indepen-
dently of other nodes in a thread of their own; the Ready() method is
used by the thread to determine whether the node is ready to process
data. The default implementation signals the ability to process data if
all endpoints deﬁned as input to the node can provide at least one data
element. It also checks the state of connected nodes and terminates
processing if one of the connected nodes has terminated. Most
derived classes should work ﬁne with the default implementation;
nonetheless, the method may be overwritten to implement a different
logic.

Finish()/Error(): being the last steps in the life cycle, these methods
are used to release resources claimed by a node instance. Derived
classes should overwrite this method and release resources allocated
during the Verify() method at this stage.

Process(): ﬁnally, this method is intended for derived classes to
implement whatever functionality the node class provides.

The base classes already provide default implementations for all
of these methods except Process(). The core library also contains a
number of generic class deﬁnitions for special purposes like simple
unary operations, binary operations, generation of data elements and
more. A complete overview of these classes is found in the manual
available at the Conveyor web site.

2.4 Graph execution

The number of cores available in modern CPUs has been growing
over the recent years from single core CPUs to modern multi-
core CPUs with 8 or more cores available on a single chip. Thus,
parallelization of processing is a design goal of Conveyor and every
processing step is running independent of other steps. The processing
step itself may implement its own logic, use an external command
line application or invoke web services. Conveyor does not restrict
the ﬁlnctionality in any way, but expects a node type to respect
the order and cardinality of the data ﬂow. Nodes that rely on these
constraints may be marked with annotations that result in an extra
check during runtime. The map, for example, transforms a list of data
objects into a list of processed objects, maintaining their order and
cardinality. The transformation loop may not contain any node type
violating these rules. If a node type has to deviate from the default
behaviour, it has to be marked with a corresponding annotation.

 

 

User scope Server scope

 

 

 

 

 

Fig. 3. Architecture of the Conveyor system, showing the three main
components and component’s scopes.

An example is the PassFilter node, that ﬁlters elements based on an
external condition. If elements are dropped, it violates the cardinality
rule, and may not be used in most loops. All annotations are available
in the meta data at runtime, allowing the processing engine to
validate the rules and their exceptions.

3 IMPLEMENTATION

To reconcile the design deﬁned in the previous paragraph, the
implementation of Conveyor is done using a 2-fold strategy:
Java for the GUI design client and the .NET framework for the
processing engine. Being deployed as a web start application, the
GUI design client is available on every platform supported by
Java. Updates to the client are automatically installed if an intemet
connection is available. Information about Conveyor services and
their corresponding processing steps are cached by the client to
allow users to work ofﬂine; submitting a Conveyor instance and
monitoring it naturally requires access to the central service.

The .NET framework was chosen for the implementation of the
core library and the processing engine. It offers a variety of different
programming languages for implementation. Among designated
languages like C# or F#, using libraries from many other languages
including C, Java and Python is supported. The core library and the
processing engine are implemented in C#, without any dependency
on external libraries that are not part of the .NET framework. This
ensures that Conveyor itself is portable across platforms supported
by the .NET framework or by Mono,l the free implementation of
.NET available for Unix-like systems and Mac OS X.

The overall architecture is split into three components (see Fig. 3).

3.1 Core library and processing engine

The core library contains the basic building blocks of Conveyor. This
includes main classes for data import, data export and processing
steps and the interface to mark a class as a data type usable by
Conveyor. It also contains wrappers for primitive data types like
numbers and strings, and deﬁnitions of control ﬂow processing steps.

The processing engine manages the plugins and controls the
execution of a Conveyor instance. It provides a ﬂexible conﬁguration
mechanism for processing steps and includes a command line utility
for executing a Conveyor instance, and a web service-based interface
for the design GUI. The complete core library is independent of the

 

lCross platform, open-source .NET development framework,
http://www.mono-project.com

 

906

112 ﬁlo's[BumoIpJOJXO'sorwurJOJurorq”:duq moi; paprolumoq

91oz ‘Og lsnﬁnv uo ::

Conveyor

 

application domain; every ﬁlnctionality and all data types speciﬁed
for a certain ﬁeld can be bundled in plugins. Similar to Ruffus,
the Conveyor core and processing libraries may be included in
applications, enhancing them with workﬂow capabilities.

3.2 Plugins

Conveyor is bundled with a number of default plugins for
ﬁlnctionality used in many ﬁelds of application. It includes handling
of tables, lists and parallelization of command line tools on
compute clusters. These are independent of the ﬁeld of application.
To simplify developing a plugin, all information about available
processing steps and data types is extracted automatically from the
plugin by reﬂection.

Most plugins available for Conveyor handle bioinformatics tasks.
Two simple rules were used while developing the bioinformatics
node types in Conveyor. These rules ensure that node types are
usable independent of the context.

Data interfaces instead of classes: all important data types are
declared as interfaces, with a default implementation available for
constructing them in Conveyor instances or other node types.

Processing nodes depend on interfaces only: node types never refer
to a concrete class. The only exception of this rule are processing
steps building data types from primitives, e.g. creating a DNA
sequence object from a name and the sequence itself. This rule
ensures that the node types work independently of the concrete data
type and avoids the need for semantically duplicated node types.
As an example, the BLAST wrapping node types only rely on the
Sequence interface, allowing to use them with any data type that
implements this interface. The node type does not have to take
care whether the data are read from a FASTA ﬁle, is deﬁned as a
feature in an EMBL ﬁle or provided as an opaque object by a linked
application. Node and data types for handling various sequence
formats and invoking tools like BLAST (Altschul et al., 1997),
ClustalW (Thompson et al., 1994), Muscle (Edgar, 2004), etc. are
already implemented. A ﬁlll list is available at the Conveyor web
server.

3.3 Design GUI

Being the main interface to the Conveyor system, the graphical
user interface client is designed with a focus on usability and
simplicity. It allows users to create Conveyor instances, monitor
graphs running on Conveyor servers, and it can be used to retrieve
and view the results of computations. The client is written in the Java
programming language, using a Webstart setup for easy installation
and upgrading of the client. Given a web start environment on a
computer (which comes with the Java runtime), clicking a simple
button on a web page is all that is needed to install the client;
ﬁlrthermore, the client is able to check for updates at every start,
keeping itself in sync without any effort on behalf of the user. A
list of servers is stored by the client, together with a cache of nodes
and data types available on the servers. The cache allows the user to
work ofﬂine with the system, without the need of an active internet
connection. A new Conveyor pipeline is designed by drag’n’drop.
Figure 4 shows the main design screen, with the list of available
node types on the left and the design area on the right. The list
of node types may be searched for terms in node type names and
descriptions. Dragging a node type from the list on the left side to

 

 

 

 

 

Fig. 4. Screenshot of the Conveyor graph designerithe screen is split into
three parts: the list of available node types on the left side, an outline of
the current graph in the lower left corner and the main working area for
manipulating the graph instance. New nodes are added by dragging them
from the node type list and dropping them on the work area. Connections
between nodes are also created by dragging a line from one node to another.
Conﬁguration items are accessible by a double click on a node in the work
canvas. Node colours in the work space indicate whether the node is an input
node (blue), a processing node (green) or an output node (red).

the design area on the right side will create an instance of the node.
Connections are simply created by clicking on the source and the
target node, conﬁguration values are set by a double click on the
node. One of the key features built into the design interface is the
ability to create composite node types. These are node types build
from other nodes, allowing the user to create complex ﬁlnctionality
by combining lower level functions. Composite node types may
also be annotated and send to the server, allowing them to be shared
with other users. After designing the graph, it may be sent to the
associated Conveyor server for execution. The designer GUI keeps
track of running graphs and stores information about them on the
local computer. The user thus may design a graph, submit it to the
server, shut down his computer and return the next day and retrieve
the results. A permanent connection to the server is not needed for
Conveyor. During execution, the designer GUI displays information
about the progress, including the status of each node and the number
of data elements passed along each connection in the graph. Figure 5
shows a screenshot of the Conveyor design GUI while monitoring
a Conveyor instance.

4 USE CASES

Conveyor offers a new platform for implementing analytical
workﬂows. Two example use cases demonstrate the unique features
like the type system and control ﬂow nodes.

4.1 Workﬂow 1: annotation by reference genome

The workﬂow presented in this use case study implements a pipeline
for annotating a genome using an annotated reference genome.
The new ultrafast sequencing technologies introduced in the recent
years result among other beneﬁts in a ﬂood of new draft genomes.
Instead of concentrating on a single organism or a single strain, a
complete species including several subspecies can be sequenced in
a single run. Due to the amount of data generated in these cases,

 

907

112 ﬁlo's[BumoIpJOJXO'sorwurJOJurorq”:duq moi; paprolumoq

91oz ‘Og lsnﬁnv uo ::

B.Linke et al.

 

 

mm

 

mum —

 

 

 

Fig. 5. Execution of a workﬂow in the Conveyor graph designer: the
integrated execution monitor displays a graph currently being executed on
a Conveyor service. Finished nodes are shown in gray, and nodes currently
executing or waiting for data are coloured green. The monitor also provides
access to results of output nodes after execution has ﬁnished.

   

 

Fig. 6. Example workﬂow for annotating a genome using a reference
genome. (a) The preparation of the reference genome. (b) Gene prediction
for the novel genomes. (c) Search for homologues in the reference genome.
(d) Copy of annotation ﬁelds from matching CDS. (e) Search for homologues
in a public database. (f) Copy of annotation ﬁelds from matching database
entry. (g) Mark still unassigned CDS. (h) Export of genome.

manually ﬁnishing or annotating these genomes is not feasible
with respect to ﬁnancial or man power constraints. The research
objectives thus focus on the identiﬁcation of differences among the
draft genomes by mapping them onto a reference and comparing the
results for the different strains. In many cases, manual annotation
is only done for the genes of interest, if done at all. The overall
workﬂow is built from three different parts: gene prediction on plain
DNA genomic sequences, homology search for the predicted genes
versus an annotated reference genome and handling of predicted
genes without homologues. The result of the workﬂow is exported
as EMBL-formatted ﬁle containing the predicted genes and their
annotations. The workﬂow in this use case uses a BLAST search
to map genes to the reference genome. It also uses another BLAST
search for genes without mapped reference genes. The workﬂow as
shown in Figure 6 consists of several parts: (a) the preparation of the
reference genome. A BLAST database is built from all CDS features
of the given EMBL or GenBank ﬁle; (b) gene prediction for the novel
genomes. Putative coding regions are predicted using the Glimmer3

(Delcher et al., 1999) prokaryotic gene prediction software with
default parameters, and CDS regions are created; (c) search for
homologues in the reference genome. Using BLAST, all CDS of the
novel genome are compared with the CDS of the reference genome.
The results are ﬁltered based on e-value and per cent coverage
and put into a list; (d) copy of annotation ﬁelds from matching
CDS. If a homologous CDS was found during the BLAST search,
its annotation ﬁelds like gene product, gene name and function
are transferred to the novel CDS; (e) search for homologues in a
public database. For novel CDS without known similar genes in the
reference genome, a public database like the SwissProt database as
part of the UniProt database [Consortium (2010) or the GenBank
database, Benson et al. (2010)] is used with BLAST to search for
homologues; (f) copy of annotation ﬁelds from matching database
hits. Similar to (a) the content of annotation ﬁelds of the best hits
against the public database are transferred to the novel CDS; (g)
mark still unassigned CDS. All CDS without assigned annotation
during steps d) and f) are marked as being putative; (h) Export
of genome. The complete genome including the predicted CDS is
exported as an EMBL or GenBank ﬁle.

The main beneﬁts of implementing the above workﬂow in the
Conveyor system are ﬂexibility and extensibility. The use case
only shows an exemplary design of the workﬂow. Changing the
reference organism or the gene prediction tool is only a matter of
changing a node in the workﬂow. Thresholds e.g. for the e-value
of BLAST results or settings like the database to use in step (e)
are conﬁguration ﬁelds directly accessible in the designer GUI. The
complete workﬂow is implemented without writing a single line of
code or invoking an application except Conveyor and the designer
GUI. Moreover, the command line execution utility that comes with
Conveyor allows executing the workﬂow completely without user
interaction. It also offers the ability to change the conﬁguration
for individual runs. The BLAST steps used in this use case are
good examples for generic types. The BLAST result data type is
a generic type, using a type parameter for the BLAST query and
another type parameter for the database sequence. Retrieving the
query or the subject from a BLAST hit thus results in the original
feature from the EMBL or GenBank ﬁle, with ﬁlll access to all
feature keys as annotation ﬁelds. This also allows transferring keys
usually not exported to BLAST databases like notes or EC numbers
in a consistent way.

4.2 Workﬂow 2: core genome calculation

With several genomes of a selected species available, dividing
them into groups according to their phenotype and determining the
speciﬁc gene content of these groups is the next important step in
comparative genomics. Conveyor provides all necessary ﬁlnctions
for this kind of analysis. A ﬁgure showing the complete workﬂow is
available in the Supplementary Material. The core genome consists
of sets of all genes present in all organisms, taking paralogs into
account. Based on annotated genomes, the homologues of all genes
of one organism are computed. If the list of homologues of a
single gene only contains one homologue in each organisms, it is
considered a member of the core genome.

The workﬂow thus consists of several parts: (i) import of genomes
including their annotation from EMBL or GenBank formatted ﬁles.
Sequences shorter than a given length are ﬁltered out, e.g. plasmids.
(ii) A list of all genome IDs is created. (iii) A blastable database

 

908

112 ﬁlo's[BumoIpJOJXO'sorwurJOJurorq”:duq moi; paprolumoq

91oz ‘Og lsnﬁnv uo ::

Conveyor

 

is created from the CDS of all organisms. (iv) The genome with
the lowest number of coding sequences is selected. (v) A second
blastable database containing all coding sequences of the smallest
genome is created. (vi) Homologous sequences for all coding
sequences of the smallest genomes are determined using BLAST.
(vii) The list of BLAST hits is reduced to signiﬁcant hits. Their bit
scores have to exceed a certain ratio if compared to the bit score
of the most signiﬁcant hit (ﬁrst hit in BLAST result). (viii) The
set of sequences referred to by the remaining BLAST hits has to
contain a single gene of each organism to be considered to be a
part of the core genome. (ix) Each sequence detected as a possible
homologue is compared against all coding sequences of the smallest
genome using BLAST. The step ﬁlters out possible paralogs in the
genome and ensures that all genes in all candidate sets are indeed
homologues. (x) Alignments are created from the sequences of all
candidate core gene sets using Muscle and written as result using
the Phylip alignment format.

The workﬂow is only an exemplary implementation of a core
genome calculator. Other restrictions may also apply, e.g. ﬁltering
out genes related to mobile elements. The number of genomes
processed by the workﬂow is not restricted. Many parameters like
thresholds for BLAST results, the required score ratio in step
(vii) or the required length of the genome sequence used in step
(i) can easily be adapted to the input sequences. The workﬂow
may also be extended by additional processing steps based on the
alignment of the core gene sets. Nodes for alignment processing
and phylogenetic analyses are already available for Conveyor.
This workﬂow demonstrates the list processing and branching ﬂow
features of Conveyor.

5 RESULTS

The use cases shown in the previous section demonstrate the
usefullness of the Conveyor system to common problems. Both
use cases were processed with the set of available Escherichia
coli genomes2 (Accession numbers NC_011602, NC_011603,
NC_008253, NC_011748, NC_008563, NC_009837, NC_009838,
NC_012947, NC_012759, NC_012967, NC_004431, NC_010468,
NC_009786, NC_009787, NC_009788, NC_009789, NC_009790,
NC_009791, NC_009801, NC_011745, NC_009800, NC_011741,
NC_011750, NC_010473, NC_000913, AC_000091, NC_002127,
NC_002128, NC_002695, NC_002655, NC_007414, NC_011350,
NC_011351, NC_011353, NC_013008, NC_013010, NC_011742,
NC_011747, NC_011407, NC_011408, NC_011411, NC_011413,
NC_011415, NC_011416, NC_011419, NC_010485, NC_010486,
NC_010487, NC_010488, NC_010498, NC_011739, NC_011749,
NC_011751, NC_007941, NC_007946, NC_011740, NC_011743)
by concatenating their GenBank formatted ﬁles available at the
NCBI web site. The workﬂows in both use cases were executed on
a single host with four Intel Xeon E7540 CPUs running at 2 GHz.
Each CPU provides six physical cores and six additional virtual
cores by hyper threading, summing up to 48 cores managed by
the operation system. The system is equipped with 256GB system
RAM and 200 GB swap. Conveyor was conﬁgured to use a thread-
based processing model with one thread per workﬂow node, and
a process-based processing model for external applications. The
number of parallel running processes were set to one, two, four,

 

2Available at end of April 2010.

Annotation by reference genome runtime

 

 

 

 

 

3 0, 000
20, 000
1 0, 000

80,000

70,000
"2 60,000
E 50,000 I Real
g I User
.5 40,000 I Sys.
§
><
LU

 

Parallel processes

Fig. 7. Runtime benchmark for the reference annotation use case. The plots
shows the average runtime of the workﬂow, splitted into the user, system
and overall time (measured by the UNIX time command).

Relative runtime for reference genome annotation
2x I I I I I

I Real
I User
I Sys.
I Rel. Real

 

 

 

 

 

Relative execution

 

Parallel processes

Fig. 8. Relative runtime of the reference annotation use case. Values of the
single process run are used as base. The relative real time is the ratio of
the real runtime times the number of processes used by the real runtime
in the single process case.

eight and sixteen. Each setup was run ﬁve times. Execution time
was measured using the time command, reporting the overall time
(real), the accumulated CPU time of all processes (user) and the time
spend in the kernel (sys). The complete benchmark data is available
as Supplementary Material.

5.1 Reference annotation benchmark

The ﬁrst use case was executed with the plain genome sequences of
all 58 Escherichia coli replicons as multiple FASTA input ﬁle and the
ofﬁcial GenBank annotation of E.coli K—IZ DH] OB (NC_010473)
as reference genome. Figure 7 shows the average runtime of the
workﬂow for different numbers of parallel running processes. The
relative runtime is presented in Figure 8. While the user time scales
well with the number of processes, the system overhead signiﬁcantly
increases, up to 80% in the 16 process benchmark. This is due to the
increasing number of ﬁle handles and threads used to communicate
with the processes. Nonetheless, the relative runtime of the workﬂow
only increases by 3—4%. Conveyor thus scales well with the number
of external processes.

5.2 Core genome benchmark

For the second use case, the ofﬁcial GenBank annotations of the
mentioned E.coli strains were used as input. The genome size ﬁlter

 

909

112 ﬁlo's[BumoIpJOJXO'sorwurJOJurorq”:duq moi; paprolumoq

9103 ‘Og lsnﬁnv uo ::

B.Linke et al.

 

Table 3. Comparison of the features provided by the different workﬂow
engines

 

Mobyle Taverna Pegasus Kepler Conveyor Galaxy Ruffus

 

(2.x)
User interface
Web based + — — _ _ + _
Ofﬂine client — + + + + _ _
Processing
Local execution — — + + + — +
Batch support — + + + + + _
Grid/cloud support — — + + + + _
Threading support — + + + + _ _
Workﬂow features
File based + — + _ _ + +
Type based — .I.a _ +b +c _ _
Composed steps — + + + + — +
Flow control — — — + + _ _

 

Since AnaBench only wraps command line applications, it does not support any of the
advanced features. Taverna 2.x is based on a new processing model that executes nodes
in separate threads, with a beneﬁt for locally executed services. Most services of the
default installation rely on web services nonetheless. The grid support of Conveyor
currently only supports certain nodes executing command line applications; a future
extension will also distribute nodes on a compute grid. Galaxy may use a Torque or
DRMAA compute cluster for distributing processing steps.

aWSDL/ontology based.

bScalar types and opaque objects without inheritance.

CGeneric object-oriented type system.

was set to 1 Mb, resulting in 26 full-sized entries to be processed. A
BLAST score ratio cutoff of 0.8 was used to ﬁlter the BLAST hits
in step (viii). Figures showing the absolute and relative runtimes are
available in the Supplementary Material. Due to the different nature
of the external tools used in the workﬂow, e.g. different database
sizes in case of the blast tools, running parallel processes adds a
noticeable overhead in the case of few processes, but scales well
with many instances in parallel. Compared to the former benchmark,
the system overhead increases in a less dramatic way, and the overall
runtime also scales well.

6 DISCUSSION

A comparison of Conveyor and the workﬂow systems mentioned in
Section 1 is shown in Table 3. The Conveyor system offers many of
the features found in workﬂow systems. This includes encapsulating
ﬁlnctionality in processing steps, composite steps built from other
steps and an engine to run a workﬂow. Similar to Taverna and Kepler,
Conveyor uses a data-oriented view, modelling data ﬂow from input
to output. In contrast to Taverna, the Conveyor plugin system does
not depend on external providers for most of its ﬁlnctionality. The
current set of plugins implements locally executed processing steps,
with an option to use a [DRMAA, distributed resource management
application API Troeger et al. (2007)] compliant scheduling system
for executing external applications like BLAST. New plugins for
integrating web services are planned nonetheless. Similar to the
concept of directors in Kepler, Conveyor uses a conﬁgurable system
for executing workﬂows. Depending on the machine running the
workﬂow, it may be executed with multiple threads, single-threaded
or distributed on a compute cluster. The unique advantage of the
Conveyor system with respect to the other workﬂow engines is
the object-oriented type system. Features like inheritance, interfaces

and generic data and node types greatly reduce the complexity of
workﬂows and the effort for developing new plugins. Especially the
generic types reduce redundancy and allow preserving the original
data type in almost all processing nodes. In contrast, Taverna data
types are based on textual representation of data, using WSDL and
a hand-curated ontology to describe the relations between types and
their structure. Kepler allows to restrict data types to primitive types
or opaque objects only; object-oriented concepts like inheritance
or interfaces are not supported by either system. Pegasus in turn
is the most advanced of the compared systems, designed for large
workﬂows with huge amounts of data, and large compute grids with
distributed ﬁle systems. With respect to its intended use cases, it
is clearly superior to Conveyor. Nonetheless, it neither provides a
strong type system nor does it offer an easy way for local execution
of workﬂows. As shown in the use cases studies, Conveyor offers
complete control over the data ﬂow, allowing loops and conditional
branches. With experienced users in mind, the Conveyor system
offers a viable alternative to writing scripts in Perl and other
languages. The ability to extend the system by plugins and the
generic type system allows developers to provide node types and
data types based on third party applications, making their data and
functionality available to Conveyor. Finally, the results of the use
cases shows that Conveyor performs well on off-the-shelf hardware
with a reasonable price.

6.1 Future work

While the current implementation of Conveyor has been focussed
on the core library and processing engine, the further development
will centre around the ﬁlnctions provided and the user interface.
A number of additional plugins are planned and are under
development. This includes taxonomy information handling,
HMMER 3 support,3 deep sequencing analysis including support
for SAM and BAM formats (Li et al., 2009). The interface
driven object-oriented approach will also simplify integration of
additional sequence formats like the mentioned BioXSD deﬁnition
into Conveyor. The user interface and the Conveyor web site will be
extended to support the exchange of workﬂows and subworkﬂows
between users, including a cookbook of proven approaches to
common analyses. New components will transform a workﬂow
into a standalone application and web service, allowing the rapid
development and deployment of workﬂows.

7 CONCLUSION

As presented in the preceding sections, the Conveyor system offers
a comprehensive and versatile system for data analysis. Although
it is designed to work in any ﬁeld of application, the use cases
presented in the former sections clearly prove its ﬁdelity to the
ﬁeld of bioinformatics. The unique design, especially the powerful
object model for both data and nodes, allows the system to ﬁll
the gap between web service-based approaches and writing custom
software.

ACKNOWLEDGEMENTS

The authors ﬁlrther thank the BRF system administrators for
technical support.

 

3No publication available for HMMER 3 yet.

 

910

112 ﬁlo's[BumoprOJXO'sorwurJOJurorq”:duq moi; papeolumoq

9103 ‘Og lsnﬁnv uo ::

mmwwm

 

Funding: Bundesministerium fiir Bildung und Forschung (BMBF)
grants (0315599B and 0315599A) ‘GenoMik-Transfer’ to BL.

Conﬂict of Interest: none declared.

REFERENCES

Altintas,I. et al. (2004) Kepler: an extensible system for design and execution
of scientiﬁc workﬂows. In Proceedings of Scientiﬁc and Statistical Database
Management, 21723 June 2004, Santorini Island, Greece. IEEE Computer Society,
pp. 423424.

A1tschu1,S. et al. (1997) Gapped blast and psi-blast: a new generation of protein database
search programs. Nucleic Acids Res, 25, 338973402.

Benson,D.A. et al. (2010) GenBank. Nucleic Acids Res, 38, D467D51.

Consortium,T.U. (2010) The Universal Protein Resource (UniProt) in 2010. Nucleic
Acids Res, 38, D1427D148.

Deelman,E. et al. (2005) Pegasus: a framework for mapping complex scientiﬁc
workﬂows onto distributed systems. Scientiﬁc Program. J., 13, 2197237.

Delcher,A. et al. (1999) Improved microbial gene identiﬁcation with glimmer. Nucleic
Acids Res, 27, 4636.

Edgar,R.C. (2004) Muscle: multiple sequence alignment with high accuracy and high
throughput. Nucleic Acids Res, 32, 179271797.

Goecks,J. et al. (2010) Galaxy: a comprehensive approach for supporting accessible,
reproducible, and transparent computational research in the life sciences. Genome
Biol, 11, R86.

Goodstadt,L. (2010) Ruffus: a lightweight Python library for computational pipelines.
Bioinformatics, 26, 277872779.

Hull,D. et al. (2006) Taverna: a tool for building and running workﬂows of services.
Nucleic Acids Res, 34, 7297732.

Kalas,M. et al. (2010) BioXSD: the common data-exchange format for everyday
bioinformatics web services. Bioinformatics, 26, i540.

Li,l-I. et al. (2009) The sequence alignment/map format and SAMtools. Bioinformatics,
25, 2078.

Neron,B. et al. (2009) Mobyle: a new full web bioinformatics framework.
Bioinformatics, 25, 3005.

Romano,P. (2007) Automation of in-silico data analysis processes through workﬂow
management systems. Brief Bioinformatics, 9, 57$8.

Seibel,P. et al. (2006) XML schemas for common bioinformatic data types and their
application in workﬂow systems. BMC Bioinformatics, 7, 490.

Smedley,D. et al. (2008) Solutions for data integration in functional genomics: a critical
assessment and case study. Brief Bioinformatics, 9, 5327544.

ThompsonJ. et al. (1994) CLUSTAL W: improving the sensitivity of progressive
multiple sequence alignment through sequence weighting, position-speciﬁc gap
penalties and weight matrix choice. Nucleic Acids Res, 22, 4673.

Troeger,P. et al. (2007) Standardization of an api for distributed resource management
systems. In Proceedings of the IEEE International Symposium on Cluster
Computing and the Grid (CCGrid 2007), 14717 May 2007, Rio de Janeiro, IEEE
Computer Society, Brazil, pp. 619$26.

Wassink,I. et al. (2009) Analysing scientiﬁc workﬂows: why workﬂows not only connect
web services. In 2009 Congress on Services-I, July 6710, 2009, Los Angeles,
California, USA, IEEE Computer Society, pp. 3144321.

Wilkinson,M. et al. (2008) Interoperability with Moby 1.0 it’s better than sharing your
toothbrush. Brief Bioinformatics, 9, 2207231.

 

911

112 ﬁlo's[BrunoprOJXO'sorwurJOJHrorqp:duq uror} popeo1umoq

9103 ‘0g lsnﬁnv uo ::

