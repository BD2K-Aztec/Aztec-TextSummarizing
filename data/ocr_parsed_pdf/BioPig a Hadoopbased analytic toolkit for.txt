BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

 

Programming Model
Data Size
Programming
Complexity
Developmental Time
Computational Speed

 

    

Serial
Small
Easy

Short
Slow

Parallel {MPO
Big
Dillicu lt

Long
F.

  

Parallel lBioPigl

  

/810'SIBum0[pJOJXO"sorJBuIJOJurorqﬂ:duq

H.Nordberg et al.

 

 

Code Listing 2

—— a more elaborate pig script example to ﬁnd all duplicates of a
—— set of sequences within speciﬁed Hamming distance

—— reads = the target set of read sequences (hdfs ﬁle path)

—— distance = the Hamming distance (int)

—— output = the location for the output (hdfs ﬁle path)

,_

register /.../biopig—core—l.0.0—job—pig.jar
%default distance 0
%default p 100

MM

4 deﬁne PAIRMERGE gov.jgi.meta.pig.eval.SequencePairMerge();

5 deﬁne CONSENSUS gov.jgi.meta.pig.eval.GenerateConsensus();

6 deﬁne IDENTITYHASH gov.jgi.meta.pig.eval.IdentityHash();

7 deﬁne UNPACK gov.jgi.meta.pig.eval.UnpackSequence();

8 deﬁne HAMMINGDISTANCE gov.jgi.meta.pig.eval.HammingDistance();

—— load the target sequences
9 READS = load '$reads' using gov.jgi.meta.pig.storage.FastaStorage as (id:
chararray, d: int, seq: bytearray, header: chararray);

—— group the read pairs together by id and ﬁlter out any reads that
—— do not have a matching pair.
—— then combine the mate pairs into a single sequence

10 GROUPEDREADS = group READS by id parallel $p;
ll MERGEDREADS = foreach GROUPEDREADS generate ﬂat—
ten(PAIRMERGE(READS)) as (id: chararray, d: int, seq: bytearray);

—— generate the hash

12 HASH = foreach MERGEDREADS generate
IDENTITYHASH(UNPACK(Seq)) £18 hash, UNPACK(seq) as seq;
[3 HASHNEIGHBORS = foreach HASH generate ﬂat—
ten(HAMMINGDISTANCE($0, $distance)) as hash, '0', $1 as seq;

—— now merge all similar reads together
15 E = group HASHNEIGHBORS by $0 parallel $p;
16 F = foreach E generate $0, count($l), CONSENSUS($1);

-- return output
17 store E into '$output';

 

 

 

Most of the work in the script is done in the various functions deﬁned in
the BioPig library. PAIRMERGE() takes a set of sequences and merges
them together into a single sequence; IDENTITYHASHO takes a
sequence and returns the hash from the ﬁrst 16 bases and the last 16
bases; and CONSENSUS() takes a set of sequences and calculates the
majority base at each position and returns a new sequence. The sequence

Table 1. BioPig functions and programs callable from Pig

data loader used in line 2 also is deﬁned in the BioPig library and sup-
ports reading and parsing FASTA-formatted sequence ﬁles.

3 RESULTS

3.1 The BioPig framework and its design principles

The ideal analysis solution should address the three main chal—
lenges in data—intensive sequence analysis: (i) scalability: it should
scale with data size; (ii) programmability: it should leverage a
high—level data ﬂow language that provides abstraction of paral—
lelism details, which enables bioinformatics analysts to focus on
analysis over large data sizes without concerns as to paralleliza—
tion, synchronization or low—level message passing; and (iii) port—
ability: it should be portable without extensive modiﬁcation to
various IT infrastructure. With these design principles in mind,
we developed the BioPig data analytic toolkit on top of Apache
Hadoop and Pig (Fig. l).

The resulting BioPig toolkit has both the scalability and ro—
bustness offered by Apache Hadoop, which uses the data—parallel
methodology of MapReduce to parallelize analysis over many
computing cores without significant loss in computing perform—
ance, and the programmability and parallel data ﬂow control
offered by Pig. In addition, as both Hadoop and Pig are imple—
mented in Java, BioPig inherits their portability.

The BioPigmodular implementation consists of a set of libraries,
which add an abstraction layer for processing sequence data.
Within BioPig’s open extensible framework, functions and libraries
can be updated and added easily. The ﬁrst release of BioPig con—
tains three core functional modules. The BioPigIO module reads
and writes sequence ﬁles in FASTA or FASTQ format. It enables
Hadoop to split sequence files automatically and distribute them
across many compute nodes. The BioPigAggregation module is a
wrapper for common bioinformatics programs, such as BLAST,
CAP3, kmerMatch and Velvet. Finally, there is a set of utilities that
make working with nucleotide sequences more efﬁcient by using
compression and encoding. Table 1 lists the functions provided by
the current version of BioPig.

3.2 A simple BioPig application demonstrates scalability,
programmability and portability

To systematically evaluate the scalability, programmability and

portability of the BioPig framework, we implemented a kmer

 

Name Function

 

BLAST/BLAT
Cap3, Minimus, Newbler, Velvet Assembler wrappers
FASTA/FASTQ I/O

Wrappers for BLAST/BLAT

FASTA/FASTQ format readers and writers compatible with Hadoop’s block I/O

KmerGenerator Generate Kmers from a sequence

N50 Calculate N50 value for a set of sequences

PackSequence Store bases in a compact, but still printable, form. Usede to save space and also for debugging
HammingDistance Implementation of Hamming distance

SequencePairMerge Merge pairs with the same sequence ID and marked as 0 and l

SubSequence Get part of a sequence

GenerateConsensus Calculate base consensus

 

 

3016

ﬁm'spzumol‘pmyo'sopeuuopnotq/ﬁdnq

BioPig

 

counting application (Section 2) to test its performance and com—
pare it with other commonly used serial and parallel methods.

We tested our implementation and report results on two
different IT infrastructures:

(l) Magellan system at National Energy Research Scientiﬁc
Computing Center (NERSC), a 78—node IBM iDataPlex
cluster. Each of its nodes has two quad—core Intel Xeon
X5550 2.67GHz processors (8 cores/node) and 24 GB of
DDR3 1333 MHz memory.

(2) Amazon Elastic Compute Cloud, a cluster of 15 nodes
(1 head node and 14 compute nodes), each node is a
cc2.8xlarge instance, which has 60.5 GB memory and 88
Elastic Compute Units (2 x Intel Xeon E5—2670, eight—core).

The size of the dataset we used ranges from the 100 Mb to
500Gb from the cow rumen metagenomic data (Hess et al.,
2011). Kmer counting is an essential component of many bio—
informatics methods, such as genome and transcriptome assem—
bly. Even though it is simple to compute, serial programs using
kmer counting quickly run out of memory as the size of sequence
data increases (Fig. 2). For simplicity, we ﬁxed the size of kmer
to 20 (K = 20) and compared the scalability of the BioPig kmer
counting program, KmerGenerator, to a serial version, Tallymer
(Stefan et al.) and an open—source MPI—version (https://github.
com/JGI—Bioinformatics/Kmernator).

As shown in Figure 2A, when run on the Magellan Hadoop
cluster (NERSC) using 1000 mappers and reducers, respectively,
BioPig KmerGenerator scales well from 1 to 500Gb of input
sequences. In contrast, Tallymer ran out of memory on a
single node with 48 GB RAM with le dataset, and our
straightforward implementation of the MPI—version ran out of

 

 

 

 

 

 

 

 

 

 

 

 

 

 

A K—mer Counting on NERSC K-mer Counting on Amazon EC2
105 16000
A A 14000
1‘3 104 3
g § 12000
g 103 /_/I 8 10000
v V 0
w ‘1’ 8000
E .E
i: 102 '— 6000
3 E
'3- r . er 4000
BioPi 2000
100 0
10*1 10° 101 102 103 o 20 40 so so 100
Data Size (Gb) Data Size (Gb)
2 5X 104 ContigExtension on NERSC D GenerateContig on NERSC
' 3500
r ’ a
z
7,? 2 p, A 3000 l/
'u I '0 r
g r ‘g 2500 a z
o / 8 ’
a) 1.5 x , r
m r g 2000 ,
I; ’ ’ V I /
a:
E 1 0’ E 1500 I r
/ ._ z
a r 3 1000 ,’
0 0.5 , ’ EL °
, I 0 500
p
0
0 20 40 60 80 100 0 20 4o 60 so 100
Data Size (Gb) Data Size (Gb)

Fig. 2. Scalability of BioPig over increasing size of input sequencing data.
(A) A comparison of the scalability of kmer counting program imple-
mented with BioPig, serial programming or MP1 programming models.
All three programs were run on the same machines. (B) Kmer counting
using Amazon EC2. (C and D) The scalability of ContigExtension
(C) and GenerateContig on NERSC (D), the dotted lines are generated
by linear regression

memory at 50 GB on the same machines tested. We noticed that
when the size of the datasets is small (1, 5 and 10Gb), the
KmerGenerator takes about the same time to finish, possibly
due to the overhead of the MapReduce framework that BioPig
based on. As data sizes reach 10Gb and higher, however,
KmerGenerator scales well with data.

To investigate whether or not other applications based on
BioPig also scale with data, we tested ContigExtention and
GenerateContig on increasing data sizes (10, 50 and 100Gb).
ContigExtention script (Code Listing 3) iteratively searches for
short reads that map to the ends of a long sequence (contig)
based on kmer matching and assembles the matching reads
with the long sequence to extend it. The algorithm is linear,
and we observed near linear performance with data (Fig. 2C).
GenerateContig script consists of two steps: it starts by running
BLAST to ﬁnd short reads matching a long sequence, and
subsequently assembles them into contigs by calling Velvet.
The first step is linear in relation to input size, whereas the
second assembly step is not. As a result, the overall performance
of the script over data displays a nonlinear scaling pattern
(Fig. 2D).

 

Code Listing 3

1 #l/usr/bin/python
2 from org.apache.pig.scripting import *
3 indexFile = 'index.data'
4 contigFile = 'contigs.data'
5 p = 100
6 Pig.fs("rmr output")
7 PP = Pig.compile("""
8 register /. . ./biopig-core-l.0.0-job.jar;
-- load the target sequences
9 readindex = load '$indexFile' using PigStorage as (seq: bytearray, kmer:
bytearray);
-- load the contigs, create the index
10 contigs = load '$contigFile' using PigStorage as (geneid: chararray, seq:
chararray);
11 contigindex = foreach contigs generate geneid,
FLATTEN(gov.jgi.meta.pig.eval.KmerGenerator(seq, 20)) as
(kmer:bytearray);
-- join reads with the contigs database
12 j = join readindex by kmer, contigindex by kmer PARALLEL $p;
13 k = foreach j generate contigindex::geneid as contigid,
gov.jgi.meta.pig.eval.UnpackSequence(readindex::seq) as readseq;
14 kk = distinct k PARALLEL $p;
15 l = group kk by contigid PARALLEL $p;
16 m = foreachl {
17 a = $1.$1;
18 generate $0, a;
19 }
-- join the contigid back with the contigs
20 n = join contigs by geneid, m by $0 PARALLEL $p;
21 contigs = foreach n generate $0 as geneid,
gov.jgi.meta.pig.aggregate.ExtendContigWithCap3($l, $3) as res:(seq:chararray,
val:int);
22 split contigs into notextended if res.val==0, contigs if res.val==1;
23 contigs = foreach contigs generate $0 as geneid, res.seq as seq;
24 store contigs into 'output/step-$i';
25 store notextended into 'output/data-$i';
26 
27 foriin range(50):
28 stats = PP.bind().runSingle()
29 if not stats.isSuccessful():
30 break;
31 if( stats.getNumberRecords('output/step-'+str(i)) <= 0):
32 break;
33 else:
34 contigFile = 'output/step-'+str(i)

 

 

 

 

3017

ﬁm'spzumol‘pmyo'sopeuuopnotq/ﬁdnq

H.Nordberg et al.

 

KmerGenerator does kmer counting with just eight lines of
code (Code Listing 1). To achieve the same task, Tallymer and
the MPI—version use thousands of lines of code. This suggests
BioPig has excellent programmability and can greatly reduce the
amount of software development time. This programmability
feature is particularly attractive to next—generation sequencing
data analysis, as the sequencing technology rapidly changes
and software constantly needs to be updated.

To test the portability of BioPig, we evaluated KmerGenerator
function on Amazon Elastic Compute Cloud (Amazon EC2).
Porting BioPig to Amazon was simply a matter of uploading
the BioPig core JAR (Java ARchive) ﬁle, along with the bioinfor—
matics tools called from BioPig code. Without any change to the
underlying code, KmerGenerator was successfully run on both
platforms, demonstrating the portability of the BioPig frame—
work. Furthermore, we observed scalability on EC2 similarly
as on the Magellan system with various sizes of sequence datasets
(Fig. 2B).

3.3 Embedding BioPig into other programming languages

BioPig has the ﬂexibility to be embedded into other languages,
such as Python or JavaScript, to achieve the types of control
ﬂows such as loops and branches that are not currently available
in the Pig language. This ﬂexibility greatly extends the usefulness
of the BioPig toolkit.

Code Listing 3 illustrates a Python script with BioPig
embedded. This contig extension algorithm greedily extends by
iteratively searching for reads that can extend the contig followed
by extending the contigs via assembling the reads at each end. In
each iteration, the Cap3 assembler is used to extend the contigs,
of which only those contigs being extended go into the next
iteration. The loop will stop when there are no contigs left to
be extended or when it reaches a predeﬁned number of steps.

3.4 BioPig and related MapReduce-based frameworks

Several computational frameworks have been recently developed
for bioinformatics. Despite that they all aim to scale analysis to
big data by implementing existing algorithms onto MapReduce/
Hadoop, they differ signiﬁcantly on the speciﬁc algorithms im—
plemented (Table 2). Therefore, it is not straightforward to com—
pare the performance of BioPig with these frameworks. The only
exception is SeqPig (http://seqpig.sourceforge.net/). SeqPig and
BioPig are two independent projects, and they share some simi—
larities. First, they both extend Pig, and therefore have the same
programming syntax. Second, there are a few similar functions
(such as sequence import and export). Because they are based on

Table 2. BioPig compared with other related frameworks

the same framework (Hadoop and Pig), when run in the same
hardware environment, they are expected to have similar run
time performance. The difference between the two projects lies
in their user—deﬁned functions. BioPig includes several kmer—
based applications that are not available in SeqPig (Table 2). It
also provides wrappers to run many frequently used bioinfor—
matics applications such as BLAST, Velvet and CAP3. In con—
trast, SeqPig mainly implements functions of Picard (http://
picard.sourceforge.net) and SamTools (Li et al., 2009). SeqPig
and BioPig can be installed side—by—side on the same Hadoop and
Pig cluster. From the same Pig script, it is straightforward to call
both BioPig and SeqPig functions.

Table 2 provides a comparison of BioPig with these related
frameworks.

4 DISCUSSION

In this work, we present a solution for improved processing of
large sequence datasets in many bioinformatics applications.
Using only a few core modules, we believe we have demonstrated
the usefulness of this toolkit, while its modular design should
enable many similar applications that ﬁt in the MapReduce
framework. BioPig has several advantages over alternative par—
allel programming paradigm: it is easy to program; it is scalable
to process large datasets (with 500Gb being the largest one
tested); and it is generically portable to several examples of
Hadoop infrastructure, including Amazon EC2, without
modiﬁcation.

We also noticed several limitations of BioPig, most of which
likely derived from Hadoop itself. For example, it is slower than
handcrafted MPI solutions. This is due to both the latency of
Hadoop’s initialization and the fact that generic MapReduce al—
gorithms are not optimized for speciﬁc problems. For big
datasets this limitation may not be a problem, as the time
spent on data analysis far exceeds the cost for the start—up la—
tency. Recently, the Hadoop community has started to address
this problem. Certain commercial implementations of
MapReduce, such as IBM’s Symphony product, have been de—
veloped to reduce Hadoop’s start—up latency. Another promising
solution is SPARK, which can speed up Hadoop applications
100 times by using a low latency, in memory cluster computing
(Zaharia et al., 2012).

Another issue is computing resource demand. When dealing
with huge datasets, BioPig shifts the need from expensive re—
sources such as large memory (leB RAM) machines and/or
parallel programming expertise, to large disk space on commod—
ity hardware. For example, a kmer/read index in BioPig needs

 

 

Framework Programming models Bioinformatics applications

BioPig Pig/Java/Hadoop FASTA I/O; several kmer-based algorithms; wrapper for BLAST and short read assemblers

Biodoop Python/Hadoop FASTA I /0; sequence alignment and alignment manipulation; wrapper for BLAST (Leo et 0]., 2009)

GATK Java/MapReduce Short read alignment, variant calling

Hadoop-BAM Java/Hadoop BAM (Binary Alignment/Map) I/O; alignment manipulation functions based on Picard API
(Niemenmaa et al., 2012)

SeqPig Pig/Java/Hadoop FASTA, FASTQ, BAM I/O; alignment-related functions; sequence statistics

 

 

3018

ﬁm'spzumol‘pmjxo'sopeuuopnotq/ﬁdnq

BioPig

 

lots of disk space, with the largest index taking up 15 TB of disk
space for the 500 Gb dataset. This trade—off is usually a favored
option, as RAM is more expensive than hard disk. Therefore,
processing large datasets with BioPig requires a stable Hadoop
environment with fast interconnects and plentiful disk space.
One might reduce the requirement by keeping fewer copies
of the ﬁles in the HDFS system, which is a trade—off for
redundancy.

BioPig is built on MapReduce and Hadoop, so algorithms
that do not run well on MapReduce will not run well with
BioPig either. For example, large short read assembly algorithms
involve large graph processing. Currently, they require message—
passing and have not yet been implemented on MapReduce.

ACKNOWLEDGEMENTS

The authors would like to thank Shane Cannon for providing
technical support for Hadoop clusters, Rob Egan for providing
MPI—based kmer counting program and Nicole Johnson for
helpful edits and suggestions.

Funding: Department of Energy Joint Genome Institute was
supported in part by the Ofﬁce of Science of the US.
Department of Energy under (Contract No. DE—AC02—
05CH112 and DE—AC02—05CH11231) (cow rumen metage—
nomics data analysis and informatics).

Conﬂicts of Interest: none declared.

REFERENCES

Dean,J. and Ghemawat,S. (2008) MapReduce: simpliﬁed data processing on large
clusters. Comman. ACM, 51, 1077113.

1000 Genomes Project Consortium. et al. (2010) A map of human genome variation
from population—scale sequencing. Nature, 467, 106171073.

Hamming,R.W. (1950) Error detecting and error correcting codes. AT&T Tech. J.,
29, 1477160.

Hess,M. et al. (2011) Metagenomic discovery of biomass—degrading genes and
genomes from cow rumen. Science, 331, 4634167.

J0urdren,L. et al. (2012) Eoulsan: a cloud computing—based framework facilitating
high throughput sequencing analyses. Bioinformatics, 28, 154271543.

Kolker,N. et al. (2011) Classifying proteins into functional groups based on
all—versus—all BLAST of 10 million proteins. 0mics, 15, 5137521.

Langmead,B. et al. (2009) Searching for SNPs with cloud computing. Genome Biol,
10, R134.

Langmead,B. et al. (2010) Cloud—scale RNA—sequencing differential expression
analysis with Myrna. Genome Biol, 11, R83.

Leo,S. et al. (2009) Biodoop: bioinformatics on hadoop. In: Parallel Processing
Workshops, 2009. ICPPW'OQ. International Conference on IEEE. Vienna,
Austria, pp. 415422.

Li,H. et al. (2009) The sequence alignment/map format and SAMtools.
Bioiip’ormatics, 25, 207872079.

McKenna,A. et al. (2010) The Genome Analysis Toolkit: a MapReduce frame—
work for analyzing next—generation DNA sequencing data. Genome Res., 20,
129771303.

Metzker,M.L. (2010) Sequencing technologies — the next generation. Nat. Rev.
Genet., 11, 31746.

Mullis,K.B. and Faloona,F.A. (1987) Speciﬁc synthesis of DNA in vitro via a
polymerase—catalyzed chain—reaction. Met/toil Enzymol., 155, 3357350.

Nguyen,T. et al. (2011) CloudAligner: a fast and full—featured MapReduce based
tool for sequence mapping. BMC Res. Notes, 4, 171.

Niemenmaa,M. et al. (2012) Hadoop—BAM: directly manipulating next generation
sequencing data in the cloud. Bioinformatics, 28, 87(r877.

P0pe,P.B. et al. (2010) Adaptation to herbivory by the Tammar wallaby includes
bacterial and glycoside hydrolase proﬁles different from other herbivores.
Proc. Natl Acad. Sci. USA, 107, 14793714798.

Schatz,M.C. (2009) CloudBurst: highly sensitive read mapping with MapReduce.
Bioiip’ormatics, 25, 136371369.

Stefan,K. et al. (2008) A new method to compute K—mer frequencies and its appli—
cation to annotate large repetitive plant genomes. BMC Genomics, 9,
147172164.

Taylor,R.C. (2010) An overview of the Hadoop/MapReduce/HBase framework
and its current applications in bioinformatics. BMC Bioinﬁ)rmatics, 11
(Suppl. 12), SI.

Warnecke,F. et al. (2007) Metagenomic and functional analysis of hindgut
microbiota of a wood—feeding higher termite. Nature, 450, 5607565.

Zaharia,M. et al. (2012) Resilient distributed datasets: a fault—tolerant abstrac—
tion for in—memory cluster computing. In: Proceedings of the 9th USENIX
Conference on Networked Systems Design and Implementation. USENIX
Association, San Jose, CA, p. 2.

Zerbino,D.R. and Birney,E. (2008) Velvet: algorithms for ale novo short read
assembly using de Bruijn graphs. Genome Res., 18, 8217829.

 

3019

ﬁm'spzumol‘pmjxo'soptzuuopnotq/ﬁdnq

