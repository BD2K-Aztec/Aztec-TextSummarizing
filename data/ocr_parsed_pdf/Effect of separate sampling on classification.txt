BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

an?kgowsmomammowoxmoa‘ocgawbmﬁ

AW

 

M.S.Esfahani and E.R.Dougherty

 

where

2 2 2
0‘, Pray Pray

ByJ: pig? 0% p.03 , (3)

v
2 2
.V

pray pp? 0
in which :73 is the variance of each variable and the p,»,i e {1, 2, 3, 4}, are
the correlation coefﬁcients inside blocks. We consider both identical and
unequal covariance matrices. We assume common correlation coefﬁcient,
pi = p,i 6 {1,2, 3,4}.

A typical microarray or next-gen RNA sequencing (Mortazavi et al.,
2008; Wang et al., 2009) experiment yields expressions for thousands of
genes, but a small number of sample points, typically <200. Therefore,
data-based feature selection is typically employed; however, since our sole
aim is to study the effect of the ratio r on the expected true error, we do
not consider feature selection and assume a model containing a reason-
able number, D, of features (which is equivalent to assuming that a set of
D genes has been chosen by the researcher based on prior biological
knowledge). We let D: 15. Two covariance matrix settings are con-
sidered: identical covariance matrices, 0% = 012 = 0.4, and unequal co-
variance matrices, :73 = 0.4, 012 = 1.6, with block size I: 5 and
correlation coefﬁcient p = 0.8 corresponding to tight correlation within
a block. The parameter settings are summarized in Table 1.

Seven classiﬁcation rules are considered: 3-nearest neighbor (3NN), 5-
nearest neighbor (5NN), linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), linear support vector machine (L-SVM),
radial basis function SVM (RBF-SVM) and decision tree (DT). The
SVM classiﬁers are trained from the package LibSVM written in
MATLAB (Chang and Lin, 2011). A decision tree classiﬁer is trained
using the MATLAB classregtree function.

2.2 Effect of sampling ratio—real data

Four microarray real datasets are used: pediatric acute lymphoblastic
leukemia (ALL) (Yeoh et al., 2002), acute myeloid leukemia (AML)
(V alk et al., 2004), multiple myeloma (Zhan et al., 2006) and breast can-
cer (Desmedt et al., 2007). We follow the data preparation instructions
reported in the cited articles. The properties of these datasets are sum-
marized in Table 2. The right-most column in Table 2 contains the initial
feature size, number of sample points in classes 0 and 1, respectively, from
left to right. The Supplementary Material provides detailed descriptions
of these datasets. The same classification rules as those used for the syn-
thetic data are applied to the real data. T—test feature selection is used to
reduce the original set of genes down to D = 15.

2.3 Holdout error estimation

Because we are going to use real data, we wish to use holdout error
estimation; however, the standard holdout procedure, which is unbiased
with random sampling, become biased, perhaps severely so, with separate
sampling. Therefore, we redeﬁne holdout for separate sampling.

Table 1. Distribution model parameters

 

Parameters Value/description

 

Mean Mo = 0.310, M] = 0.810
Covariance matrix 0% = 0.4, 012 = 0.4 (identical covariance)
0% = 04,012 2 1.6 (unequal covariance)

Block size I = 5
Feature size D = 15
Feature block correlation p = 0.8

The true error of a designed classiﬁer 1b,, is given by
s" = Pr(1//,,(X>¢ Y)
= 61341141007é 0| Y: 0)
+(1— C)Pr(1//n(X) 7E 1|Y= 1)
= 682 + (1 — ()8).

(4)

Relative to a random sample, S”, the expected true error is
Es,,[8n] = cEs.[82] + (1 — c>Es.[s;]. (5)

For standard holdout error estimation, the sample is split into I points
(the training set) to train the classiﬁer and m points (the test set) to
estimate the error, where in this scenario the notation indicates that the
total sample size is n = t + m. Let S,, Sm, Sm”, and Sml denote the set of
training data, the full set of test data, the class-0 test points, and the class-
1 test points, respectively. The holdout estimator is

A l
8(Illn)=— Z 117,,(X;>¢Y.~

(th Y;)€Sm

mg l
= Wm— : 1w,,(X.-)¢Y.-
0 (Xi. Yoesn0 (6)

m1 1
+—— Z 1w,,(X.->¢Y.-
m m1(X.~.Y.-)es,,,,

ml A1

8 (111"),

_m0 A0

— —8 (W) + —
m m
where 50 and 51 denote the holdout estimators of £2 and 8,1,. Taking
expectations in (6) yields

Es,,[§] = €139,150] + (1 — C)Es,, [511- (7)

Because the test data are independent from the training data, the hold-
out estimator is unbiased given the training data, which means that
ES”[5|S,] :8". Taking the expectation relative to the training data
yields E5115] 2 ES/[ES”[5|S,]] = ES”[8,,]. Similar expressions apply to 50
and 5], namely, ES”[50] 2 E5182] and E51151] = ES”[8,1']. Thus,
5, 50, and 51 are unbiased estimators of a", £2, and 8,1,, respectively, and
Expression (7) corresponds term by term to (5).

With separate sampling, taking expectations in (6) yields
A m0 A ml A
55,,[8] = — 53,180] + —Es,, [81]
m m

m0 0 m1 1
: WESII[8"] + WESII[8"]’

(8)

because the ratio % is ﬁxed. Hence, 5 is not unbiased. The bias depends
on the difference between c and  If c is known, then the holdout
estimator can be redeﬁned as

5. = c5“ + (1 — (951, (9)

for both random and separate sampling. In both cases it is unbiased:
taking expectations on the right-hand side of (9) yields the right-hand

Table 2. Real datasets used in this article

 

Dataset Dataset type Feature|Sample size

 

Yeoh et al., 2002 Pediatric ALL
Valk et al., 2004 AML

Zhan et al., 2006 Multiple myeloma
Desmedt et al., 2007 Breast cancer

5077| 149/99
22215|116/157
54613 | 1 56/78
22215|98/77

 

 

244

ﬁm'spzumol‘pmﬂo'sopeuuopuorq/ﬁdnq

531111111115; “1111 sampling with
size of IL and . Ae' ul‘ n 7 m and
' - (11‘ Given ran: ‘

the given [HIM] r ‘ h ” ’

{FAMITF \rlet»
tiuu Hill-51)

dllllllg

'ﬁm- mining

true er 1'
commutation
lmlulnnr ermr

mmmmu

 

/810'spzum0fp10}x0'sopBLuJOJurorq”:duq

éékkg

D‘
n
'9
\
i
S.
a,
9,
B
p:
a.
0
.w
o
it,
o
'1
£1
0
E
5
¥ a
.w
o
'1
cm
\

Effect of separate sampling on classification accuracy

 

Analogously, for sufﬁcient large r,
E[8n(cz)|r] — E[8n(61)|r] <0- (12)

We shall assume that whatever classiﬁcation rule and feature—
label distribution we are considering, (11) and (12) hold for suf—
ﬁciently small and sufﬁciently large r, respectively.

The next lemma, whose proof is in the Supplementary
Material, states a fundamental property of the error curves.

LEMMA 3.2.1 If a classification rule is strictly class—wise smart
relative to the family {ﬂ(x,y)}, then, for c2>c1, E[8,,(cz)|r]—
E[8n(c1)|r] is a strictly decreasing function of r.

If we only assume class—wise smart, then E[s,,(c2)|r]—
E[sn(c1)|r] would only be sure to be a decreasing function of r.

The next lemma, whose proof is in the Supplementary
Material, shows that constraining c results in a corresponding
constraining of the expected error.

LEMMA 3.2.2 Suppose c1<c2<c3. IfE[8n(C3)|r] 3 E[8,,(c1)|r],
the" E[8n(63)|r] Z E[8n(62)|r] Z E[8n(c‘1)|r]- If E[8n(cs)lr] E
E[8n(c‘1)|r], the" E[8n(63)lr] E 1:18n(62)|r] E 1:18n(61)|r]-

To ease notation, we will say that E[sn(c2)|r] is between
E[8n(cl)|r] and E[8n(c3)|r]  either E[8n(c3)|r] Z E[8n(62)|r] Z
E[8n(cl)|r] or E[8n(c3)|r] E E[8n(62)|r] E E[8n(cl)|r]-

The salient proposition concerning the error curves involves a
strictly decreasing function g(r) of r that is positive for sufﬁ—
ciently small values of r and negative for sufﬁciently large
values of r. Since r is a discrete variable in (0,1), we have a
sequence of values 0<r1<  <r",1 <1. Ifg were continuous,
then there would exist a unique value r* such that g(r*) 2 0,
g(r)>0 for r<r*, and g(r)<0 for r>r*. But since g is discrete,
this basic proposition is slightly altered. Rather, there are two
possibilities: (i) there exists a unique value r* = r; for some value
j such that g(r*) 2 0, g(r) >0 for r<r*, and g(r)<0 for r>r* or
(ii) there is a unique value r,- such that g(r)>0 for r g r; and
g(r)<0 for r 3 r)“. In the second case, we select a
point r* e (rim/+1), say, the mid—point, and then we have
g(r)>0 for r<r* and g(r)<0 for r>r*, as in the ﬁrst case.
In the next theorem, whose proof is in the Supplementary
Material, we will be interested in a ‘unique’ point r* 6 (0,1).
For the second case, we interpret this to mean that there is a
unique interval (r1, r/H) and r* is the selected point in that
interval.

Given the preceding discrete interpretation, we shall say that a
function p(r) ‘crosses’ function q(r) at r* if p(r) 3 q(r) for r<r*
and ifp(r) g q(r) for r>r*.

THEOREM 3.2.3 If a classification rule is strictly class—wise smart
relative to {fC(X, y)}, then there exists a unique point r* such that
for any C2 >c1, E[8,,(Cz)|r] crosses E[8n(c1)|r] at r*.

This is precisely the theorem we want because it means that all
error curves cross at r*.

In the error curves of Figure 3, we observe that r* provides a
minimax value; that is, rmm = r* yields the minimum value of
E[s,,(c)|r] when taking the maximum error over all values of
E[8,,(c)|r] for r e(0,1) :

mm

r : argminmaxE[sn(c)|r], (13)

where we must keep in mind that r e R = {r1, ...,r",1} is a
discrete variable. The next theorem, proven in the
Supplementary Material, formalizes this observation.

THEOREM 3.2.4 Consider a classification rule that is strictly
class—wise smart relative to {fc(x, y)} and let rmm be the minimax
value deﬁned by (13). If Theorem 3.2.3 yields a unique point
r* 2r), then rmm = r); otherwise,  Theorem 3.2.3 yields an
interval (r/,r/+1), then either r,- or r,“ is the minimax ratio, deter—
mined by

if Elsgllf/l E EIShIK/H]

. l4
ifE[82|r;]>E[8,11|K/+1] ( )

r,
rmm : { ./
0+1

3.3 Practical implications of the error curves

Recall the practical implications we drew regarding Figure 1 in
the Section 1: (i) if c is known, then do separate sampling with
n0 2 cn; where the equal sign means ‘as close to cn as possible’;
(ii) if c B c’, then for small n do separate sampling with n0 2 c’n;
(iii) if one has no idea regarding the value of c, then sampling
must be random. Looking at the curves in Figure 3 (and similar
ﬁgures in the Supplementary Material), we see that the curve for
c has its minimum value at r: c or r = c’ B c and, in the latter
case, LIE"(c)|r] B E[s,,(c’)|r]. Hence, the first two recommenda—
tions hold for the other classiﬁcation rules examined.

Going beyond the case where c is known or approximately
known, consider the third implication, where one has no good
idea concerning the value of c. Then the minimax rmm is an
option. Its suitability depends upon the classiﬁcation rule and
feature—label distribution. As we can see from Figure 3, except
for extreme values of c, E[s,,(c)|rmm] tends not to be too much
greater than Hsn(c)|c]. Of course, there is a practical problem:
while we may well know the classiﬁcation rule, we will not know
the feature—label distribution.

3.4 Algorithm to approximate rmm

Algorithm 1 provides an iterative algorithm for approximating
rmm when the feature—label distribution is unknown. The proced—
ure is an empirical illustration of Theorem 3.2.4, which requires
E[s,,(c)|r], which now needs to be approximated to approximate
rmm. Algorithm 1 uses holdout error estimation. The expectation
of this error estimate is taken by iterative random sampling from
the dataset. Here we give a brief overview of the algorithm.
The inputs to the algorithm are: dataset denoted by S N, clas—
sification rule, number of points to be held out for error estima—
tion from classes 0 and l, denoted, respectively, by n96“, and n‘les‘
and number of iterations, MaxIters, for computing the expected
holdout error estimate. The maximum number of points after
holding out test sample points is Nnew = N — (nag +n‘les‘),
denoted class—wise as Ngew and Néew. The algorithm searches
over possible values for r, from 0 to 1, until a stopping criterion
is met. Suppose we fix the total sample size n. Then, considering
the ﬁrst extreme case, r = 0, we need to have at least n points in
class 1 to draw sample points from, randomly, i.e. N l 3 n. On

new

the other hand, when r: l, we similarly should have N0 3 n.

Hence, we should have n g min{N0 Nl }, whereby we set

. neW’ new
n=m1n{N0 Nl }.

HEW ’ HEW

 

247

ﬁm'spzumol‘pmJXO'sopeuuopuorq/ﬁdnq

M.S.Esfahani and E.R.Dougherty

 

The algorithm’s search criterion is based on Theorem 3.2.4: in
a ‘while loop’ over an increasing sequence of the ratios r, the
algorithm computes the estimated slope of the expected error (as
a function of c), this being slopenew = E[§2|r] — Héjlr] (line 22 of
the algorithm), obtained by plugging the error estimates (lines
7721 of the algorithm) into the unknown slope formula
E[82|r] — E[s,11|r]. Because the classiﬁcation rule is strictly class—
wise smart, for sufﬁciently small r, the slope is positive, and it
becomes negative for sufﬁciently large r (refer to Supplementary
Material file for further explanation). Once a point is reached at
which the sign of the slope becomes non—positive, the ‘while loop’
stops increasing r. Thereafter, the three different possibilities
given by Theorem 3.2.4 are checked, in lines 2332, and finally
a single rmm is returned. Although the returned minimax ratio is
only computed for sample size n deﬁned above, the class—sizes
can still be conservatively adjusted per rmm in the dataset S N
because, for a ratio given by the algorithm, if one increases the
sample size, then in the worst case the error is as large as the
minimax value returned by the algorithm.

 

Algorithm 1 Iterative algorithm to approximate r‘n‘n (an implementation
of Theorem 3.2.4 using an estimate of the expected error estimate)

1: Input: Dataset SN, Classiﬁcation rule \11, nil“, n30“, MaxIters

2: Output: rm‘n
- . _ 0 l _ l l
3' Deﬁne' Niicw _ N0 _ ntcst’ Nncw _ N _ "test

n =min{N0 1v1 }

new ’ new

 

 

4: Initialize: j = 0, r = 0, slopenew = 1, sign 2 1

5: while sign > 0 do

6: Set: a <— E[ég|r], slopeold <— slopencw, r <—f;

7: Reset: 13mm = 0, 12153;] = 0

8: for i = 1 to MaxIters do

9:  <— n?cs‘ randomly drawn points from SR,
10:  <— n‘lcs‘ randomly drawn points from S}v
“3 5:53.: ‘— 51?in U 5753‘]

123 Sivm, ‘- Sh\5‘n°::;0a Slim. ‘- SlAS‘nfii‘l

13: SNW <— sgm U Slaw

14: SO" <— rn randomly drawn points from S?V“CW
15: S}, <— (1 — r)n randomly drawn points from S}Vncw
16: s" <— 53 U S},

17: on <— \II(S,,)

18: Compute 52, 5,1, of (in, using 

19: Add 52, and 5,1, to E[§2|r] and E[é,1,|r], respectively
20: end for

21: EIéBIrImfiiilzleélétlri615131121.

22: slopenew <— E[52|r] — E[é,1,|r]

23: sign <— slopencwslopeold

24: j <— j + 1

25: end while

26: if sign 2 0 then

27: rmm (— r

28: else if a < 13153;] then

29 rmm (— I — l

30 else

31: r‘n‘n <— r
32: end if

33: return r‘n‘n

 

3.5 Adjusting sample sizes

Consider the common situation in which no and n1 have been
determined beforehand, but suppose one knows c. The curves of
Figure 3 still apply but we are not free to choose no and n1, so
that we cannot choose n0: cn. Nevertheless, we desire the train—
ing data to be apportioned according to c and we want to use as
much data as is possible. These conditions mean that for training
we want class sample sizes m0 and m1 such that m = mg + m1 is
maximized given the constraints m0 2 cm, m1 = (1 — c)m,
m0 3 no, and m1 3 n1. The solution is to let m = [min{"7°, 
To see the effect of adjusting sample sizes, we consider the
difference, A(r, c) = LIE"(c)|r] — E[sm(c)|c], between the ex—
pected true errors of two cases, n being the original sample size
and m the adjusted sample size. When the sampling ratio is r and
the true prior probability is c, A(r, c) can be interpreted as the
penalty incurred. Figure 4 shows A(r, c) for L—SVM and RBF—
SVM for the equal covariance model described in Table 1. The
result for the case with unequal covariance matrices can be found
in the Supplementary Material. The two parameters r and c take
values from 0.06 to 0.94 with the step size of 0.04. As expected,
as |r — c| increases, A(r, c) signiﬁcantly increases. When
r B c, A(r, c) B 0, which is always the minimum. The figure
shows that except when r is very close to c, A(r, c) >0, meaning
that, even though m<n, a correct sampling ratio more than
compensates for the loss of data due to subsampling.

3.6 Population-based minimax theory

The minimax value in the error curves of Figure 3 depends on the
sampling distribution and results from the fact that E[s,,(c)|r] is
minimized over c for a single value r*. In Anderson (1951), a
population—based minimax approach was taken to arrive at a
‘best’ choice for c in (1) in the Gaussian model with common
covariance matrix under separate sampling. Here we extend the
population—based mimimax approach to arrive at much more
general solution than that given by Anderson. It is based upon
the fact that the Bayes classiﬁer can be determined via a discrim—
inant involving the class—conditional densities. Anderson also
utilized the Bayes classiﬁer in his analysis but he restricted it to
the Gaussian model with common covariance matrix, in which
case the Bayes classiﬁer is given by LDA using the actual par—
ameters rather than their estimates as in (1).

Given the class—conditional distributions and prior probabil—
ities, the Bayes classiﬁer, I/IBayeS, is determined by the
discriminant

f(x|0) l — c

—l , 15
ﬂxl 1) 02; C ( )
where I/IBayes(X) = 1 if DBayes(x) g 0 and I/IBayes(X) = 0 if
DBayes(x)>0. The regions assigned to the two classes are
R1 = {x : DBayes(x) g 0} and R0 = {x : DBayes(x)>0}. If c is un—
known and replaced by a, then the discriminant becomes

 

 

DBayes(x) = log

 

248

ﬁm'spzumol‘pmjxo'sopeuuqurorq/ﬁdnq

   

/310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

J

— Anderson (:7

' ' ‘LDA with (
‘- - LDA with ( 7 (‘ (known r)

 

/310'S[BIIJHO[pJOJXO'SOIJBLUJOJIIIOIq/ﬂduq

