BIOINFORMA TICS

0'so;112u110}u101q//2duq

SIBumoprOJX

£10"

C.Bemau et al.

 

across different populations and laboratories employing compar—
able but not identical methods. Generalist algorithms may be
preferable in important settings, for instance when a researcher
develops a model using samples from a highly controlled envir—
onment, but hopes the model to be applicable to other hospitals,
labs, or more heterogeneous populations.

In this article we systematically use independent validations
for the comparison of learning algorithms, in the context of
microarray data for disease—free survival of estrogen receptor—
positive breast cancer patients. Although concern has been
often expressed about the lack of independent validation of gen—
omic prediction models (Micheel et al., 2012; Subramanian and
Simon, 2010), independent validation has not been systematically
adopted in the comparison of learning algorithms. This deﬁ—
ciency cannot be addressed for prediction contexts where related,
independent datasets are unavailable. For many cancer types,
however, several micro—array studies have been performed to
develop prognostic models. These datasets pave the way for a
systematic approach based on independent validations. For in—
stance, a recent meta—analysis of prognostic models for late—stage
ovarian cancer provides a comparison of publicly available
microarray datasets (Waldron et al., 2014). Furthermore,
Riester et al. (2014) showed that combining training datasets
can increase the accuracy of late—stage ovarian cancer risk
models. Thus situations exist in genomic data analysis where
comparable, independent datasets are available, and these pre—
sent an opportunity to use independent validation as an explicit
basis for assessing learning algorithms.

We propose what we term ‘leave—one—dataset—in’ cross—study
validation (CSV) to formalize the use of independent validations
in the evaluation of learning algorithms. Through data—driven
simulations, and an example involving eight publicly available
estrogen receptor—positive breast cancer microarray datasets, we
assess established survival prediction algorithms using our ‘leave—
one—dataset—in’ scheme and compare it to conventional cross—
validation.

2 METHODS
2.1 Notation and setting

>We consider multiple datasets i = 1, . . . , I with sample sizes N1, . . . , N1.
Each observation s appears only in one dataset i, i.e. datasets do not over-
lap, and the corresponding record includes a primary outcome Y‘l‘.‘ and a
vector of predictor variables X); throughout this article X; will be gene-
expression measurements. Our goal is to compare the performance of dif-
ferent learning algorithms k = 1, . . . , K that generates prediction models
for the primary outcome using the vector of predictors. Throughout this
article, the primary outcome is a possibly censored survival time.

We are interested in evaluating and ranking competing prediction
methods k = 1,. . . , K. Since the ranking may depend on the application,
the ﬁrst step is to deﬁne the prediction task of interest. We focus on the
prediction of metastasis-free survival time in breast cancer patients based
on high-throughput gene-expression measurements. Our approach and
the concept of CSV, however, can be applied to different types of re-
sponse variables and any other prediction task.

2.2 Algorithms

We assess six learning algorithms (k = 1,...,6) appropriate for high-
dimensional continuous predictors and possibly censored time-to-event

outcomes: Lasso and Ridge regression (Goeman, 2010), CoxBoost (Binder
and Schumacher, 2008), SuperPC (Blair and Tibshirani, 2004), Unicox
(Tibshirani, 2009) and Plusminus (Zhao et al., 2013). All parameters were
tuned either by default methods included in their implementation (Ridge
and Lasso regression: R-package glmnet) or by testing a range of param-
eters in internal cross-validation. Our focus is not to provide a compre-
hensive array of algorithms, but simply to use a few popular,
representative algorithms to investigate CSV.

2.3 CSV matrices

We refer in this article to m-fold cross-validation and related resampling
methods collectively as cross-validation (CV). Our ranking procedure for
learning algorithms is based on a square matrix Z“ of scores
(k = 1,. . . ,K). The (i, j) element in the matrix measures how well the
model produced by algorithm k trained on dataset i performs when
validated on dataset j. Since we consider K methods we end up with K
method-speciﬁc square matrices Z1, . . . , ZK. We set the diagonal entries
of the matrices equal to performance estimates obtained with 4-fold CV
in each dataset. We will call Z“ the CS V matrix.

Possible deﬁnitions for the  . scores include the concordance index in
survival analysis (Harrell et al., 1996), the area under the operating char-
acteristic curve in binary classiﬁcation problems, or the mean squared
distance between predicted and observed values in regression problems.

We use survival models and focus on a concordance index, the C-
index, which is a correlation measure (Gnen and Heller, 2005) between
survival times and the risk scores, such as linear combinations of the
predictors, provided by a prediction model. The heatmap in Figure 1A
displays the CS V matrix of C-statistics obtained through validation of
eight models trained on the studies in Table 1 with Ridge regression.

2.4 Summarization of a CSV matrix

In order to rank learning algorithms k = 1, . . . , K, we summarize each
matrix Z“ by a single score. We consider following two candidate
approaches.

1 The Sim le Avera e of all non-dia onal elements of the Z“ matrix:
( ) p g g

k
2?..-
CSV = L
[(1 — 1)

(2) The Median or more generally a quantile of the non-diagonal
entries of Z“. Quantiles offer robustness to outlier values, and
the possibility to reduce the inﬂuence of those studies that are
consistently associated with poor validation scores, both when
used for training and validation, and independently of the learning
algorithm.

2.5 True global ranking

Throughout our analyses the score j is a random variable. First, studies
i and j can be seen as randomly drawn from a population of studies.
Second, observations within each study can be considered as randomly
drawn from the unknown and possibly different distributions F,- and F)
underlying studies i and j. With this view of j as random variable, we
consider the theoretical counterparts of the empirical aggregating scores
(simple average and quantiles) described in Section 2.4 to summarize Z“.
The theoretical counterparts are the expected value or quantiles of each
j score, i 751', obtained by integrating the two levels of randomness that
we described. The true global ranking of the learning algorithms
k = 1, . . . , K is then deﬁned by these expected values (or quantiles), one
for each algorithm. We will call the ranking global because it depends on
the super-population (Hartley and Sielken, 1975) and not which popula-
tions were sampled by the available datasets.

 

i106

ﬁm'spzumofpmﬂo'sopeuuopnorq/ﬁdnq

raumtgoimondwmowoﬁo~&o§w5.o~m\

 

 

C.Bemau et al.

 

maximum mean (Miller et al., 2011). The 50% of genes with lowest
variance were removed. Subsequently, gene-expression values were
scaled by linear scaling of the 2.5 and 97.5% quantiles as described by
Haibe-Kains et al. (2012).

2.7 Simulation design

We simulate heterogeneous datasets with gene-expression proﬁles and
time to event outcomes from a joint probability model. We deﬁne the
model through a resampling procedure that we apply to the eight breast
cancer datasets in Table 1. The resampling scheme is a combination of
parametric and nonparametric bootstrap (Efron and Tibshirani, 1993).
The goal of our simulation study is to compare CV and CSV when used
for ranking and evaluation of competing learning algorithms. Here we
use resampling methods to iteratively simulate realistic ensembles of
breast cancer datasets from a hierarchical probability model that we
deﬁne using the actual datasets in Table 1. CV and CSV are then assessed
with respect to their ability to recover the true global ranking, which we
compute through Monte-Carlo integration.

We will quantify the ability to recover the ranking by using the
Kendall correlation between the true global ranking and the estimates
obtained with CV or CSV.

For I; = 1, . . . ,B = 1000 iterations, we generate a collection of I = 8
datasets as follows. First, we sample eight study labels with replacement
from the list of breast cancer studies in Table 1. This step only involves
simulations from a multinomial Mult(8,[l/8,..., 1/8]) distribution. We
resample the collection of study labels to capture variability in study
availability, and heterogeneity of study characteristics. Second, for each
of the eight randomly drawn labels, we sample N = 150 patients from the
corresponding original dataset, with replacement. If a study is randomly
assigned to the j—th label, then each of the N = 150 vectors of predictive
variables is directly sampped from the empirical distribution of the j—th
study in Table 1. Finally, we simulate the corresponding times to event
using a proportional hazards model (parametric bootstrap) ﬁtted to the
j—th dataset:

 : M(tlx)=A{;(t) x exp(xTﬁj)~ (1)

where A/(tlx) is the individual hazard function when the vector of pre-
dictors is equal to X and [31- denotes a vector of regression coefﬁcients. We
combine the truncated inversion method in Bender et al. (2005) and the
NelsoniAalen estimator for cumulative hazard functions to simulate sur-
vival times that reﬂect survival distributions and follow-up of the real
studies. We set the vector [31- to be the coefﬁcients ﬁtted in study j = 1, . . . ,
I using the CoxBoost method (Binder and Schumacher, 2008). A different
regression method could have been used at this stage. The collections of
simulated datasets are then used both (i) to compute by Monte-Carlo
method the true global ranking deﬁned in Section 2.5, and (ii) to compute
ranking estimates through CV and CSV. Figure 1A displays, for each pair
of studies (1,1) in Table 1, the C-index obtained when training a model by
Ridge regression on dataset i (rows), and validating that model on dataset
j (columns). We computed the diagonal elements (i =1) by 4-fold CV.
Figure 1B displays mean C-indices for each (i, j) combination across
simulations, when the training and validation studies are generated
resampling the i-th and j—th study. Here diagonal elements are computed
by averaging C-indices with the training and validation datasets inde-
pendently generated by resampling from the same study.

The strong similarity between the two panels is reassuring, in particular
with regard to the clear separation of the eight studies into two groups.
The ﬁrst group includes studies MNZ, STl, ST2, TRP, UNT and VDX,
and produces more accurate prediction models than the remaining stu-
dies. The datasets in this group are also associated with higher values of
the concordance index when used for validation. This difference between
the two groups is also illustrated in Figure 1C. It displays the non-diag-
onal entries of the matrices represented in the left and middle panels, that
is the average C-indices from simulated datasets, against the C-indices

from real data. This scatterplot shows a clear two-cluster structure: the
yellow dots display the 30 training and validation combinations within
studies MNZ, STl, ST2, TRP, UNT and VDX. We will return to this
cluster structure in the discussion.

2.8 Evaluation criteria for simulations

In simulation studies we can assess and rank algorithms based on their
ability to recover the true underlying models Mime, i = 1, . . . , I. In this
subsection, we introduce a criterion that reﬂects the degree of similar-
ity between the true regression coefﬁcients )3,» that, were used to
simulate the i-th in silico dataset and the coefﬁcients :5]. ﬁtted through
algorithm k on the j—th simulated dataset. We consider the i = j and i 751'
cases separately. Similarity between vectors is usually quantiﬁed by com-
puting the Euclidean distance between them. However, since our focus is
on prediction, w, use

@(Xiﬁi, X55]. ), the correlation between true and estimated patient-
speciﬁc prognostic scores, to measure the similarity between the true )3,»
and estimated regression coefﬁcients )3. . Here X,- is the matrix of pre-
dictors of dataset i and cor denotes Pearson’s correlation. The average

S’s‘ur : (1/1) '  C/KOF (Xi/31w X13?» (2)

over the I studies, provides a measure of the ability of learning algorithm
k to recover the model that has generated the training dataset, hence the
index self.

Another criterion of interest is the ability of a learning algorithm k to
recover the vector of regression coefﬁcients )3,» when it is trained on a
separate dateset j 75 i and the unknown models underlying datasets i and j
might differ from each other.

This can be quantiﬁed with

Sigma/(1(1— 1)» - 2 264m», XE“). (3)
i #t
where the index across emphasizes the focus on cross-study similarity, i.e.
on the ability of algorithm k to recover the coefﬁcients )3,» when ﬁtted on
dataset j, with j 75 i.

In alternative to averaging across studies, or pairs of datasets, as in
Equations (23) one can also use different summaries, e.g. quantiles, as
we do in Section 2.4. Both Sfclf and Sﬁcmss are summary statistics to assess
and compare learning algorithms. We denote the ranking obtained by
ordering the algorithms according to SselﬁSacmss) by Rsen(Racmss). Both
5ch and Sﬁcmss vary across simulations of the datasets ensembles, al-
though the hierarchical simulation model remains ﬁxed and their com-
putations involve the vectors [3,», i = 1, . . . , I. We will therefore call the
rankings Rself and Raeross local because they are speciﬁc to the collection
of datasets at hand.

3 RESULTS
3.1 Simulated data

Our focus in the simulation study is on differences between the
rankings and performance estimates obtained by using CV and
CSV. We will use W and W to denote the means of the
diagonal and non—diagonal elements of a CSV matrix, respect—
ively. Recall that we compute the diagonal elements through CV.

Figure 2A shows, for K = 6 algorithms, the distributions of
W and W, and Figure 2B shows the distribution of the rank—
ings estimates, across 1000 simulated collections of eight data—
sets. Table 2 compares the medians of the distributions in
Figure 2B with the true global rankings that we obtained using
the criteria in Section 2.4. The rank of method k is 1 if it

 

i108

ﬁm'spzumofpmﬂo'sopnuuopnorq/ﬁdnq

 

55,2kgogmoddmmowoxwoa‘oﬁsambmﬁ

vs. I.
E .
o .

55,2kgogmoddmmowoxwoa‘oﬁsambmﬁ

CSV for the assessment of prediction algorithms

 

Z—matrix column— and row—wise. In the column—wise case correl—
ations, between CSV and CV summaries, vary across algorithms
~0.5, while in the row—wise case all the correlations are negative.
Overall, we can consider cross— and within—study prediction as
two related but distinct problems.

We also noted that CV is less suitable for detection of outlier
studies than CSV; in particular CV can estimate encouraging
prediction performances even on studies associated, under each
training algorithm, with poor CSV summaries i. For instance,
with the SuperPC algorithm all but one C—index estimates ob—
tained with CV are above 0.6.

3.4 Specialist and generalist algorithms

Our analyses lead to the question of whether some algorithms
can be considered as generalist or specialist procedures according
to our definitions. Our examples are not exhaustive and add—
itional comparisons, within the development of new prognostic
models, are necessary in order to determine ‘specialist’ or ‘gen—
eralist’ tendencies of these algorithms. However, the fact that
Ridge regression, Lasso regression and C oxBoost are ranked dis—
tinctly better accordingly to CV than CSV, in most iterations of
our simulation study, suggests that these algorithms might be
specialist procedures and adapt to the speciﬁc properties of the
individual dataset. The status of generalist versus specialist, for
each algorithm, can be discussed using the local performance
criteria Sself and Sums, which are conceived to measure
within—single—studies and generalizable prediction performances.
We note that CoxBoost and Ridge regression tend to achieve
better ranks in Rsehv than in Racmss. In particular CoxBoost im—
proves its position by 1 or 2 ranks in most simulations, which is
similar to what we observed comparing CoxBoost’s CSV and CV
rankings. In summary, in our study, these two algorithms seem
to haveiaccordingly to all the criteria that we consideredia
tendency to specialize to the dataset at hand. We mention that,
as one can expect, for all the algorithms Sself is consistently
higher than 53mm. We also compared CV to independent
within—study validation using our simulation model. For the inde—
pendent within—study validation, we iteratively pair two datasets
generated using identical regression coefﬁcients and gene expres—
sion distributions. Subsequently, we train a model on the ﬁrst
dataset and evaluate it on the second one. As can be seen in
Supplementary Figure S5, CV values, as expected, are slightly
smaller than for the independent within—study validations.

4 DISCUSSION AND CONCLUSION

In applying genomics to clinical problems, it is rarely safe to
assume that the studies in a research environment faithfully rep—
resent what will be encountered in clinical application, across a
variety of populations and medical environments. From this
standpoint, study heterogeneity can be a strength, as it allows
to quantify the degree of generalizability of results, and to inves—
tigate the sources of the heterogeneity. This aspect has long been
recognized in meta—analysis of clinical trials (Moher and Olkin,
1995). Therefore, we expect that an increased focus on quantify—
ing cross—study performance of prediction algorithms will con—
tribute to the successful implementation of the personalized
medicine paradigm.

In this article we provide a conceptual framework, statistical
approaches and software tools for this quantiﬁcation. The con—
ceptual framework is based on the long—standing idea that ﬁnite
populations of interest can be viewed as samples from an inﬁnite
‘super—population’ (Hartley and Sielken, 1975). This concept is
especially relevant for heterogeneous clinical studies originating
from hospitals that sample local populations, but where re—
searchers hope to make generalizations to other populations.

As an illustrating example, we demonstrate CSV on eight in—
dependent microarray studies of ER—positive breast cancer, with
metastasis—free survival as the endpoint of interest. We also de—
velop a simulation procedure involving two levels of non—
parametric bootstrap (sampling of studies and sampling of ob—
servations within studies) in combination with parametric boot—
strap, to simulate a compendium of independent datasets with
characteristics of predictor variables, censoring, baseline hazards,
prediction accuracy and between—dataset heterogeneity realistic—
ally based on available experimental datasets.

Cross—validation is the dominant paradigm for assessment of
prediction performance and comparison of prediction algorithms.
The perils of inﬂated prediction—accuracy estimations by incor—
rectly or incompletely performed cross—validation are well
known (Molinaro et al., 2005; Subramanian and Simon, 2010;
Simon et al., 2011; Varma and Simon, 2006). However, we
show that even strictly performed cross—validation can provide
optimistic estimates relative to CSV performance. All algorithms,
in simulation and example, showed distinctly decreased perform—
ance in CSV compared to cross—validation. Although it would be
possible to further reduce between—study heterogeneity, for ex—
ample by stricter ﬁltering on clinical prognostic factors, we believe
this degree of heterogeneity reﬂects the reality of clinical genomic
studies and likely other applications. Some sources of biological
heterogeneity are unknown, and it is impossible to ensure consist—
ent application of new technologies in laboratory settings.
Prediction models are used in presence of unknown sources of
variation. Formal CSV provides a means to assess the impact of
unknown or unobserved confounders that vary across studies.

In simulations, the ranking of algorithms by CSV was closer to
the true rankings defined by cross—study prediction, both when
we considered Racross and the global true ranking. Surprisingly,
CSV was also competitive with CV for recovering true rankings
based on within—study prediction, such as Rself. Although the
performance differences we observed between algorithms were
smaller than the difference between CV and CSV, Lasso consist—
ently compared poorly with most of the competing algorithms,
both under CV and CSV evaluations. Lasso, and other algo—
rithms that ensure sparsity have been shown to guarantee poor
prediction performances in previous comparative studies
(Bovelstad et al., 2007; Waldron et al., 2011).

Systematic CSV provides a means to identify relevant sources
of heterogeneity within the context of the prediction problem of
interest. By simple inspection of the CSV matrix we identiﬁed
two outlier studies that yielded prediction models no better than
random guessing in new studies. This may be related to known
differences in these studies: smaller numbers of observations,
higher proportions of node positive patients, different treatments
and larger tumors (Supplementary Figs S&S9). Conversely,
other known between—study differences do not seem to have
created outlier studies or clusters of studies as seen in the Z

 

i111

ﬁm'spzumofpmﬂo'sopnuuopnorq/ﬁdnq

C.Bemau et al.

 

matrix, such as between studies where all or no patients received
hormonal treatment. We note that incorporation of clinical prog—
nostic factors into genomic prognostic models could likely pro—
duce gains in CSV accuracy, and that such multi—factor
prognostic models could also be assessed by the proposed
matrix of CSV statistics.

In practice it is neither possible nor desirable to eliminate all
sources of heterogeneity between studies and between patient
populations. The adoption of ‘leave—one—in’ CSV, in settings
where at least two comparable independent datasets are available,
can provide more realistic expectations of future prediction model
performance, identify outlying studies or clusters of studies, and
help to develop ‘generalist’ prediction algorithms which will hope—
fully be less prone to ﬁt to dataset—speciﬁc characteristics. Further
work is needed to formalize the identiﬁcation of clusters of com—
parable studies, to develop databases for large—scale cross—study
assessment of prediction algorithms, and to develop better ‘gen—
eralist’ prediction algorithms. Appropriate curated genomic data
resources are available in Bioconductor (Gentleman et al., 2004)
through the curatedCRCData, curatedBladderData and
curatedOvarianData (Ganzfried et al., 2013) packages, and in
other common cancer types through InSilicoDB (Taminau
et al., 2011). In realms where such curated resources are available,
CSV is in practice no more difﬁcult or CPU—consuming than
cross—validation, and should become an equally standard tool
for assessment of prediction models and algorithms.

ACKNOWLEDGEMENT

We wish to thank Benjamin Haibe—Kains for making the curated
breast cancer datasets used in this study publicly available.

Funding: German Science Foundation [BO3139/2—2 to A.L.B.].
National Science Foundation [grant number CAREER DBI—
1053486 to CH. and DMS—1042785 to G.P.]; National Cancer
Institute [grant 5P30 CA006516—46 to GP. and 1RC4
CA156551-01 to L.W. GP. and LT].

Conﬂict of interest: none declared.

REFERENCES

Baek,S. et al. (2009) Development of biomarker classiﬁers from high—dimensional
data. Brief. Bioinform., 10, 5377546.

Baggerly,K.A. et al. (2008) Run batch effects potentially compromise the usefulness
of genomic signatures for ovarian cancer. J. Clin. Oncol, 26, 118(r1187.

Bender,R. et al. (2005) Generating survival times to simulate Cox proportional
hazards models. Stat. Med, 24, 171371723.

Binder,H. and Schumacher,M. (2008) Allowing for mandatory covariates in
boosting estimation of sparse high—dimensional survival models. BMC
Bioinform., 9, l4.

Blair,E. and Tibshirani,R. (2004) Semi—supervised methods to predict patient sur—
vival from gene expression data. PLoS Biol, 2, 5117522.

Boulesteix,A.L. (2013) On representative and illustrative comparisons with real data
in bioinformatics: response to the letter to the editor by smith et al.
Bioiiﬁ’ormatics, 29, 266$2666.

Bovelstad,H.M. et al. (2007) Predicting survival from microarray dataia compara—
tive study. Bioinﬁ)rmatics, 23, 208(k2087.

Castaldi,P.J. et al. (2011) An empirical assessment of validation practices for mo—
lecular classiﬁers. Brief. Bioinform., 12, 1897202.

Chin,K. et al. (2006) Genomic and transcriptional aberrations linked to breast
cancer pathophysiologies. Cancer Cell, 10, 5297541.

DeméarJ. (2006) Statistical comparisons of classiﬁers over multiple data sets.
J. Mach. Learn. Res., 7, 1730.

Desmedt,C. et al. (2007) Strong time dependence of the 76-gene prognostic signa—
ture for node—negative breast cancer patients in the transbig multicenter inde—
pendent validation series. Clin. Cancer Res., 13, 320773214.

Efron,B. and Tibshirani,R.J. (1993) An Introduction to the Bootstrap. Chapman and
Hall, New York.

Foekens,J.A. et al. (2006) Multicenter validation of a gene ExpressionBased prog—
nostic signature in lymph NodeNegative primary breast cancer. J. Clin. Oncol,
24, 166571671.

Ganzfried,B.F. et al. (2013) curatedOvarianData: clinically annotated data for the
ovarian cancer transcriptome. Database, [Epup ahead of print, doi: 10.1093/
database/bat013, April 2, 2013].

Gentleman,R.C. et al. (2004) Bioconductor: open software development for com—
putational biology and bioinformatics. Genome Biol, 5, R80.

Goeman,J. (2010) ll penalized estimation in the cox proportional hazards model.
Biometr. J., 52, 7&84.

Gnen,M. and Heller,G. (2005) Concordance probability and discriminatory power
in proportional hazards regression. Biometrika, 92, 9657970.

Haibe—Kains,B. et al. (2012) A three—gene model to robustly identify breast cancer
molecular subtypes. J. Natl Cancer Inst., 104, 3117325.

Harrell,F.E. et al. (1996) Multivariate prognostic models: issues in developing
models, evaluating assumptions and adequacy, and measuring and reducing
errors. Stati. Med, 15, 3617387.

Hartley,H.O. and Sielken,R.L. Jr (1975) A ‘Super—Population viewpoint” for ﬁnite
population sampling. Biometrics, 31, 4117422.

Leek,J.T. et al. (2010) Tackling the widespread and critical impact of batch effects in
high—throughput data. Nat. Rev. Genet., 11, 7337739.

Micheel,C. et al. (2012) Evolution of Translational Omics: Lessons Learned and the
Path Forward National Academies Press, Wahington, D.C.

Miller,J.A. et al. (2011) Strategies for aggregating gene expression data: the collap—
serows R function. BMC Bioinform., 12, 322.

Minn,A.J. et al. (2005) Genes that mediate breast cancer metastasis to lung. Nature,
436, 5187524.

Minn,A.J. et al. (2007) Lung metastasis genes couple breast tumor size and meta—
static spread. Proc. Natl Acad Sci. USA, 104, 6740—6745.

Moher,D. and Olkin,I. (1995) Meta—analysis of randomized controlled trials: A
concern for standards. JAMA, 274, 196271964.

Molinaro,A.M. et al. (2005) Prediction error estimation: a comparison of resam—
pling methods. Bioinﬁ)rmatics, 21, 330173307.

Riester,M. et al. (2014) Risk prediction for Late—Stage ovarian cancer by meta—ana—
lysis of 1525 patient samples. JNCI J Natl Cancer Inst., [Epup ahead of print,
doi:10.1093/jnci/dju048, April 3, 2014].

Schemper,M. and Smith,T.L. (1996) A note on quantifying follow—up in studies of
failure time. Clinical Trials, 17, 3437346.

Schmidt,M. et al. (2008) The humoral immune system has a key prognostic impact
in node—negative breast cancer. Cancer Res., 68, 540575413.

Simon,R.M. et al. (2009) Use of archived specimens in evaluation of prognostic and
predictive biomarkers. J. Natl Cancer Inst., 101, 144671452.

Simon,R.M. et al. (2011) Using cross—validation to evaluate predictive accuracy of
survival risk classiﬁers based on high—dimensional data. Brief. Bioinform., 12,
2037217.

Sotiriou,C. et al. (2006) Gene expression proﬁling in breast cancer: understanding
the molecular basis of histologic grade to improve prognosis. J. Natl Cancer
Inst., 98, 2627272.

Subramanian,J. and Simon,R. (2010) Gene expression—based prognostic signatures
in lung cancer: ready for clinical use? J. Natl Cancer Inst., 102, 464—474.

Symmans,W.F. et al. (2010) Genomic index of sensitivity to endocrine therapy for
breast cancer. J. Clin. Oncol, 28, 411141119.

Taminau,J. et al. (2011)inSilicon: an R/Bioconductor package for accessing human
affymetrix expert—curated datasets from GEO. Bioiiy’ormatics, 27, 320$3205.
Tibshirani,R. (2009) uniCox: Univarate shrinkage prediction in the Cox model.

R package version 1.0.

Varma,S. and Simon,R. (2006) Bias in error estimation when using cross—validation
for model selection. BMC Bioinformatics, 7, 91.

Waldron,L. et al. (2011) Optimized application of penalized regression methods to
diverse genomic data. Bioinformatics, 27, 339973406.

Waldron,L. et al. (2014) Comparative meta—analysis of prognostic gene signatures
for Late—Stage ovarian cancer. JNCI J Natl Cancer Inst., [Epub ahead of print,
doi:10.1093/jnci/dju049, April 3, 2014].

Zhao,S. et al. (2013) Mas—o—menos: a simple sign averaging method for discrimin—
ation in genomic data analysis. http://biostats.bepress.com/harvardbiostat/
paper158/ (24 October 2014, date last accessed).

 

i112

ﬁm'smumofprqxo'sopnuuowrorq/ﬁdnq

