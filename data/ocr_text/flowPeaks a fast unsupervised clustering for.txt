ORIGINAL PAPER

Vol. 28 no. 15 2012, pages 2052-2058
doi: 1 0. 1 093/bioinformatics/bts300

 

Data and text mining

Advance Access publication May 17, 2012

flowPeaks: a fast unsupervised clustering for flow cytometry data
via K -means and density peak finding

Yongchao Ge* and Stuart C. Sealfon

Department of Neurology and Center of Translational System Biology, Mount Sinai School of Medicine, New York, NY

10029, USA

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: For flow cytometry data, there are two common
approaches to the unsupervised clustering problem: one is based on
the finite mixture model and the other on spatial exploration of the
histograms. The former is computationally slow and has difficulty to
identify clusters of irregular shapes. The latter approach cannot be
applied directly to high-dimensional data as the computational time
and memory become unmanageable and the estimated histogram is
unreliable. An algorithm without these two problems would be very
useful.

Results: In this article, we combine ideas from the finite mixture
model and histogram spatial exploration. This new algorithm, which
we call flowPeaks, can be applied directly to high-dimensional data
and identify irregular shape clusters. The algorithm first uses K-
means algorithm with a large K to partition the cell population into
many small clusters. These partitioned data allow the generation
of a smoothed density function using the finite mixture model.
All local peaks are exhaustively searched by exploring the density
function and the cells are clustered by the associated local peak.
The algorithm flowPeaks is automatic, fast and reliable and robust
to cluster shape and outliers. This algorithm has been applied
to flow cytometry data and it has been compared with state of
the art algorithms, including Misty Mountain, FLOCK, flowMeans,
flowMerge and FLAME.

Availability: The R package flowPeaks is available at https://
github.com/yongchao/flowPeaks.

Contact: yongchao.ge@mssm.edu

Supplementary information: Supplementary data are available at
Bioinformatics online

Received on March 19, 2012; revised on April 27, 2012; accepted on
May 14, 2012

1 INTRODUCTION

In analyzing ﬂow cytometry data, one fundamental question is
how to divide the cells into distinct subsets with the phenotypes
deﬁned by the ﬂuorescent intensity of the cell surface or intracellular
markers. The unsupervised clustering for ﬂow cytometry data is
traditionally done by manual gating, where cells are sequentially
clustered (gated) in one—dimension (1D) or 2D with the aid of 2D
contour plots and 1D histograms. Manual gating has two problems:
it is (i) highly subjective, depending on the users’ expertise and
the sequences of the markers to draw the gates and where to
draw the gates and, (ii) tedious, for data consisting of n channels,

 

*To whom correspondence should be addressed.

the user needs to check and draw the gates on possibly 
pairs of 2D contour plots. The automatic gating of the cells, in
machine learning called unsupervised clustering, has become an
active research area for the past several years. There are currently
two common approaches to address the unsupervised clustering
problem, one is based on the ﬁnite mixture model (Aghaeepour
et al., 2011; Chan et al., 2008; Finak et al., 2009; Lo et al., 2008;
Pyne et al., 2009) and the other is based on spatial exploration of
the histograms (Naumann et al., 2010; Qian et al., 2010; Sugar
and Sealfon, 2010). Both approaches have their own weaknesses.
The ﬁnite mixture model assumes that the data are generated by a
mixture of Gaussian distributions, Student’s t—distribution or skewed
t—distributions. Some of these methods require data transformation
to reduce the data asymmetry. There are two issues faced by the ﬁnite
mixture model: (i) how many components are needed and (ii) the
cluster shape is not necessarily the same as what the model assumed.
Most authors resort to the Bayesian information criterion (BIC) or
some variants to determine the optimum number of components
(Finak et al., 2009; Lo et al., 2008; Pyne et al., 2009), which still
leaves ambiguity as there are competing ﬁnite mixtures that give
similar BIC with completely different partitions of the data. The BIC
approach is also computationally very burdensome since it needs to
compute the clustering for all possible K and then determine the
best K. If the cluster shape is not convex or very asymmetrical,
these algorithms are likely to split a single cluster into several
small ones. The new— generation algorithms such as Misty Mountain
(Sugar and Sealfon, 2010) and FLOCK (Qian et al., 2010) try to
ﬁnd the irregular shape and not to rely on K. They are fast and they
ﬁnd the data—dependent cluster shape. However, the new— generation
algorithms cannot be applied directly to high—dimensional data.
Thus, Misty Mountain needs to ﬁrst apply principal component
analysis to reduce the dimension and FLOCK needs to search a 3D
subspace that is optimal for a particular cluster. These dimension
reduction techniques may result in information loss. In this article,
our goal is to combine these two approaches, allowing us to quickly
detect the data—dependent cluster shapes so that the algorithm can
be applied directly to high—dimensional data.

2 METHODS
2.1 What is a cluster

As said in Jain (2010), there is inherent vagueness in the deﬁnition of a
cluster. We want to illustrate what a cluster is with a toy example. Figure 1
shows a density function of two Gaussian distributions when varying the
mean of the ﬁrst distribution. In Figure 2, the means are ﬁxed, and the
proportion for the ﬁrst Gaussian distribution is varied. Most ﬁgures show
two distinct peaks. However, we can see that the data should be considered

 

2052 © The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /§.IO'SIBUJHOprOJXO'SOIlBIHJOJUIOICI/ﬁdnq U101} pepBOIUAAOG

9IOZ ‘091sn8nv uo ::

flowPeaks: a unsupervised clustering for flow data

 

 

 

 

 

 

 

 

 

 

 

 

 

A l~l1=—3 B I~l1-—1 C I~l1-1

O O O

N — N — N —

d o d

3 3 3

o _ d _ o _

3 3 3

o d 0

LO LO LO

0 — O — O —

o d o

O O O

o _ O_ _ o _

O I I I I I O I I I I I O I I I I
—10 0 5 10 —10 —5 0 5 10 —5 0 5 10

Fig. 1. The overall density with two Gaussian mixture components with
different choices of mean for the ﬁrst component. The y—axis for the
black curve is the probability density f (x) at x given by the x—axis. The
overall density function f(x)=w1¢(x; m,012)+(1 —w1)¢(x; u2,022), where
w1=0.7,u2=4,01=2,02=1.5, ¢(x;u,02) is the density function of the
Gaussian distribution with mean it and variance 02, and 11.1 takes the values
of —3, —1, and 1, respectively. The two components wk¢(x; uk, 0,?) (k = 1, 2)
are respectively given by the red and green curves (A color version of this
ﬁgure is available as Supplementary Material)

 

 

 

 

 

 

 

 

 

 

 

 

A w1=0.7 B w1=0.95 C w1=0.99

O O O

N— N— N—

O O 0

LO LO LO

0 c5 0

O O O

0 c5 0

LO LO LO

0— O— O—

0 c5 0

O O O

o- Q— o-

o I I I I I o I I I I I o I I I I I
—10 0 5 10 —10 0 5 10 —10 0 5 10

Fig. 2. The overall density with two Gaussian mixture components
for different choice of M. We set u1=—3,u2=4,01=2,02=1.5. The
presentation of this ﬁgure is the same as in Figure 1 except that m is ﬁxed
at —3 and w1 takes the values of 0.7, 0.95 and 0.99 for the three panels (A
color version of this ﬁgure is available as Supplementary Material)

as one cluster in Figures 1C and 2C, because there is only a single peak.
An ideal cluster would be such that the corresponding probability density
function has a unique peak (mode) and every point can move to the peak
following a monotonically nondecreasing path. In this article, we use K —
means as a building block to estimate the probability density function (see
Sections 2.2 and 2.3), which is then used to partition the clusters based on
the above consideration (see Section 2.4).

2.2 K-means

The K —means algorithm has traditionally been used in unsupervised
clustering, and was applied to ﬂow cytometry data as early as in Murphy
(1985), and as recently as in Aghaeepour et al. (2011). In fact, K —means is
a special case of a Gaussian ﬁnite mixture model where the variance matrix
of each cluster is restricted to be the identity matrix. Our use of K —means
is not for the ﬁnal clustering, but for a ﬁrst partition of the cells, for which
we can compute the smoothed density function. In the literature, the most
popular K —means implementation is based on Lloyd’s algorithm (Lloyd,
1982). Since there are many local minima, the ﬁnal clustering depends
critically on the initial seeds. We used the seeds generation algorithm from the
K—means+ + algorithm (Arthur and Vassilvitskii, 2007). Let x,- =(x11, ...,xfl)
be a d—dimensional vector for the measurements of cell i and C}, be the seed

vector for cluster h. Initially, a random cell is picked and assigned to cl.
To sequentially determine the seed for cluster k (k =2, ...,K), we ﬁrst
compute the minimum Euclidean distance for all cells to the previous k — 1
seeds by
d3={hZEirik_l}||x,—ch||2,i= 1,... ,n.

A cell x,- is selected to be the seed ck of the k—th cluster according to the
probability d3 /{ZJ’7:1dj2}. After the seeds for all K clusters are assigned,
Lloyd’s algorithm (Lloyd, 1982) will iterate with the following two steps:
assign each data point with a cluster label according to the smallest distance
to the K seeds (cluster membership assignment step) and then recompute the
center vector of all data points that are assigned with the same cluster label
(center update step). The updated center vectors become the seed vectors for
the cluster membership assignment step in the next iteration. We use a k-d tree
representation of cells (Kanungo et al., 2002) for improved computing
speed for the implementation of Lloyd’s algorithm. After Lloyd’s algorithm
converged, we further applied the Hartigan and Wong’s (1979) algorithm
to recompute the cluster centers and cluster membership to decrease the
objective function 217:1 ||x,- — CL, “2, where L,- e 1, ...,K is the cluster label of
x,- and ck is the center vector for cluster k e 1, ...,K. We could have applied
the Hartigan and Wong’s algorithm directly to the seeds, but the computation
is too slow.

In general clustering, it is important to specify a good K in the K —
means algorithm. For our purpose, a very accurate speciﬁcation of K is not
necessary. However, it is still important that the K can give a smooth density
in which the peaks can reveal the clustering structure. This speciﬁcation of K
is similar to the determination of the number of bins in drawing histograms.
We adopted the formula of Freedman and Diaconis (1981)

192%) —x£1))/{2-IQR(xj)-n_1/3} forj: 1,...,d, (1)

where xiwxén) are, respectively, the minimum and maximum of the j—th

dimension of the data xj =  ,xé, . . . ,x{,) and IQR(') is the interquartile range
of the data, deﬁned as the difference between the 75th percentile and 25th
percentile. Then our K is deﬁned as the median of Kj’s, i.e.

K = Imedian(K1, . . . , Km, (2)

where H is the ceiling function that maps a real number to the smallest
following integer.

2.3 Gaussian ﬁnite mixture to model the
density function

After K —means, we may approximate the density function f (x) by the
Gaussian ﬁnite mixture models,

K
f(x)=Zwr.-¢(x;itk, 2k),

k=1
where the proportion wk of the k—th component satisﬁes ngk :1 and
ZElekzl and ¢(x;uk,2k) is the probability density function of the
multivariate normal distribution with mean uk and variance matrix 2k. After
applying the K —means algorithm of Section 2.2, we have already partitioned
the data into K clusters, and for the k—th cluster, we can compute the sample
proportion wk, sample mean uk and sample variance matrix 23k (a rigorous
writing would require the hat notation, which is ignored for the sake of
simplicity). However, the estimate 2k may be too noisy, and we want to
smooth the variance matrix by

§k=lk'h2k+(1—lk)'h020,

where h and ho are customized parameters tuned to make the density function
smoother or rougher. The default setting in the software is h = 1.5 and kg = 1.
Here, Akznwk/(k+nwk) so that a greater wk results in a Ak closer to 1;
20 is the variance matrix assuming the data are uniformly distributedin
the whole data range and is a diagonal matrix with its (j, j) element 26’] =

{( ('n) —x§1))/k1/d}2 forj=1,...,d.

 

2053

112 /§.IO'SIBUJHOprOJXO'SOIlBIHJOJUIOICI/ﬁdnq 11101; pepBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

KGe and S. C.Sealfon

 

2.4 Peak search and merging

According to our deﬁnition, a cluster is deﬁned by the local peak. For all
cells, we can use the greatest gradient search (hill climbing) to ﬁnd which
local peak a given cell can reach. This rules out any global optimization
strategy such as the conjugate gradient algorithm. It is computationally very
time consuming to search all the local maximums of the density function
for all cells. Since the cells are pre—grouped by the K —means, we only need
to search the local peaks for the centers of the K —means clusters. The hill
climbing method searches along the greatest gradient of the density function.
If we take the negative of the density function as the optimization function,
the hill climbing of peak search can be achieved by the deepest descent
algorithm, which is implemented by the GSL library at http://www.gnu.org/
software/gs1/. We also need to restrict the step size in case it steps too far
away and jumps to another local peak. When the data move from one K —
means cluster into another K—means cluster, we can speed it up by moving
directly to the center of the other cluster. When two peaks are relatively
close, they should be joined together and considered as a single peak. We
search the two peaks with the closest Euclidean distance and check if the two
clusters may not be too different from a single cluster. The details on the local
peak search and peak merging are described in the Appendix. Algorithm 1
gives the summary of the steps to use in K —means and density peak ﬁnding
in order to cluster the ﬂow cytometry data as implemented in the software
ﬂowPeaks. In the end, we will obtain IC (5 K) of merged clusters, each of
which consists of one or many K —means clusters.

 

Algorithm 1 Summary of the ﬂowPeaks algorithm

 

1. Apply the Freedman—Diaconis formula in each dimension of
the data to obtain the number K of clusters for K —means [see
Equations (1) and (2)].

2. Use the K-means++ algorithm to generate the initial seeds of
the K clusters.

3. Use the k—d tree data representations to apply Lloyd’s K—
means algorithm until it converges.

4. Further apply the Hartigan and Wong’s K —means algorithm to
improve the compactness of the clusters.

5. Compute wk, ,uk, Elk for k = 1, - -- ,K using the partitions of the
K —means.

6. Based on the density function generated by Gaussian ﬁnite
mixture model, compute the local peak starting from the
centers ,uk, k: 1, - -- ,K (see Algorithm A1 in the Appendix).

7. Apply Algorithm A2 in the Appendix to merge peaks
hierarchically.

8. The K clusters of the ﬁnal K —means algorithm are regrouped
according to the merged peaks.

 

2.5 Cluster tightening

The default setting in the ﬂowPeaks algorithm is to not identify the outliers.
Some data points may lie far from the center or cannot be unambiguously
classiﬁed into a speciﬁc cluster. We determine whether a data point is an
outlier using the following strategy. Let £(x) be the ﬁnal merged cluster label
of data point x. Let a),- and f,(x) (respectively) be the proportion and the
probability density function of the i—th ﬁnal merged cluster. The proportion
a),- is the sum of wk’s of the K —means clusters that form the i—th ﬁnal merged
cluster. The density function f,(x) itself is a Gaussian ﬁnite mixture based

on the K—means clusters that are merged into the i—th ﬁnal cluster, while

the overall density function f(x) is based on all K—means clusters (see

Section 2.3) and f (x) = 1 CUL‘f,‘ (x). A point x is an outlier if
f(x)/man{f(y)I€(y)=€(x)} £0.01.

01'

1C
wt(x)ft(x)(x)/ Zeal-ﬁx) 5 0.8.
i=1
The numbers 0.01 and 0.8 can be adjusted in the software settings.

3 RESULTS
3.1 Datasets

Barcode data: The data were generated for a barcoding experiment
(Krutzik and Nolan, 2006) with varying concentrations of
ﬂurophores (APC and Paciﬁc Blue). The ﬂow cytometry data have
180912 cells and three channels with an additional channel for
Alexa. The manual gates for the 20 clusters to be used for assessing
cluster algorithm performance were created from ﬂowJo (www.
ﬂowjo.com).

Simulated concave data: The data were simulated with two
distinctive concave shapes based on the idea from the supplemental
material of Pyne et al. (2009). It has 2729 rows and 2 columns.
Both barcode data and simulated concave data along with their gold
standard cluster labels are available in the ﬂowPeaks package.

GvHD dataset: Graft versus host disease dataset and the manual
gates are obtained from Aghaeepour et al. (2011). This dataset
contains 12 samples, and the cells are stained with four markers,
CD4, CD8b, CD3 and CD8. In addition, two channels FS and SS
are also measured. These data are mostly analyzed based on the four
markers unless speciﬁed otherwise. The numbers of cells of the 12
samples range from 12 000 to 32 000.

Rituximab data: The ﬂow cytometry data that are obtained from
the ﬂowClust package (Lo et al., 2009). They have 1545 cells and
two channels of interest. The data were originally produced by
Gasparetto et al. (2004). The barcode data, simulated data and GvHD
datasets have gold standard cluster labels (either by simulation or
manual gating) to assess performance. The rituximab data are used
for the purpose of exploration. Figure 3 displays all four datasets.

3.2 Different metrics to assess the cluster algorithm
performance

The most widely used metric to assess how a candidate clustering
algorithm compares with the gold standard, for which the correct
cluster membership is known, is the adjusted Rand index (Hubert
and Arabie, 1983; Rand, 1971). The Rand index (Rand, 1971) is
based on the percentage of the agreement between the two clustering
methods. Let us assume that n data points are labeled differently
with two different clustering methods, say Method A and Method B
with KA and KB clusters. LetAi,i=1,...,n and Bi,i= 1,...,nbe the
cluster labels for the two methods. The Rand index is deﬁned as

n
in ex  ( l J an , J)/(2),
131<J§n

where I (~) is the indicator function. The adjusted Rand corrects for
chance, and the general form is

Index — Expected Index

 

Max Index — Expected Index '

 

2054

112 /§.IO'SIBUJHOprOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; pepBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

flowPeaks: a unsupervised clustering for flow data

 

 

 

 

 

 

 

 

 

 

I:
'2
Cl
(“'1
1:!
a-
D
u at
El.
q;
I:
D—
1:!
D
D—
Ln
C’— I I I I I I I I I I I I I
5'30 1500 E 50".) 3501} -2 -'I 0 1 E" 3 4
Paeiiieblue a:
C D D D
D_ ‘ Gd "' __
D D .I_l
...-.r
G .
ﬂ—
III
C
D—
I. W I
DJ 0':-
_l _'
LL LL

4GB
|

EIIIIIIII

 

 

 

 

   

 

 

I I I I I I I I I I
GI EEIIII 400 EDD 5-00 1000 El 201] 400 EDD EDD I'D-Ell!
FL1 .H FL1.H

Fig. 3. Four example datasets: (A) barcode data with the 20 clusters.
(B) Concave data with two clusters. (C) One of the 12 samples in the GvHD
dataset. The plot only shows the scatter plot of the ﬁrst two of the four
channels, where different colors indicate different manual gated clusters and
black points are outliers. (D) Rituximab data (A color version of this ﬁgure
is available as Supplementary Material)

In order to compute the adjusted Rand index, we ﬁrst deﬁne the
contingency tables

n
Ital, 2:1(Ai =61 and Bi =19),

i=1

for a=1,...,KA, 19:1,  ,KB. The marginal sums on the
contingency tables are then deﬁned as

KB KA
na.+ = Znala and n+1; = Enab-
bzl 61:1

Note that Inga  111+, b. The adjusted Rand index
can be quickly computed using the following formula (Hubert and
Arabie, 1983)

Zal) (nail?) _ 2a  2b (nab)
(2.. ("3’0 + Zr, ("3%) /2 — 2.. ("3: 222C130

The F —measure (Fung et al., 2003) is based on a greedy
strategy to match the two clustering. It has been used in 2010s
ﬂowCAPI (http://ﬂowcap.ﬂowsite.org/summit2010.html) and in the
ﬂowMeans algorithm paper (Aghaeepour et al., 2011) to assess the

 

performance of different algorithms. The F —measure is deﬁned as

 

n ,+
F2: an mlaxF(a,b),
a

_ 2R(a,b)P(a,b) _ na, _ na,
where F(a,b)_ Wﬂam— na : ,P(a,b)— 

Rosenberg and Hirschberg (2007) proposed the V—measure to
evaluate the clustering algorithm. This measure uses entropy to
assess how much a second clustering provides extra information
for the ﬁrst clustering. For the clustering Method A, the entropy is

 

KA

H(A) : _ Z nar;+ 10g "2+

 

 

a=1

and the conditional entropy

 

 

 

KB "+12 KA n b n b

, 61, a,

H(AlB)_—E n E n logn
b=1 61:1 +,b +,b

 

 

K K

B A na,b na,b
=—E E n logn .

b=1a=l +’b

The conditional entropy H (A |B) is always no greater than the entropy
H (A). The extra information provided by Method B for Method A
is the reduced entropy H (A) —H (A|B). After normalization, we can
deﬁne

h:1—H(A|B)/H(A)-I(H(A);é0).

In the above equation, by deﬁnition it = 1 if H (A) :0. If we reverse
the positions of A and B, we can deﬁne

c =1—H(BIA)/H(B)-I(H(B)7é0)

If Method B is the candidate clustering to be compared with the gold
standard clustering A, h evaluates the homogeneity of clustering for
Method B, while 6 evaluates the completeness. The homogeneity
ensures that the gold standard labels (A labels) for all data points
of a candidate cluster B are unique. Completeness ensures that for
each gold standard cluster (A cluster), data points are all assigned
to a single candidate cluster (B cluster). Details can be found in
Rosenberg and Hirschberg (2007). The V—measure is a weighed
harmonic mean of h and C, VB =(1+,8)hc/(,8h—I—c). In this artice,
we will ﬁx ,8 to be 1.

3.3 Application

Table 1 displays the running time of all algorithms that are applied
to the concave and barcode datasets described in Section 3.1. The
algorithms ﬂowPeaks, Misty Mountain (Sugar and Sealfon, 2010),
FLOCK (Qian et al., 2010) and ﬂowMeans (Aghaeepour et al.,
2011) are falling into a category where the computational time is
under several minutes so that they can compete with manual gating,
while FLAME (Pyne et al., 2009) and ﬂowMerge (Finak et al.,
2009) take too much computational time to be practically useful.
Among the ﬁrst four algorithms, a good seeding strategy and k—d tree
implementation make ﬂowPeaks a little bit faster than the other
algorithms.

When we applied the three metrics in Section 3.2 to assess
different algorithms, we removed the outliers according to the gold
standard. Tables 2 and 3 give the performance of different algorithms
to be compared with the gold standard. We see that ﬂowPeaks does
quite well for the barcode data and the concave data. Due to the slow

 

2055

112 /B.IO'SIBUJHOprOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; pepBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

KGe and S. C.Sealfon

 

 

 

       

 

 

 

 

 

 

A B
3_ B_
D D
[‘1' IF}
:2  ._ 1* r:
a_ ' - D_
D a I 'I D a
El. - ' EL
'5': _ I. _._ .- a: _
3_ :5' 3_
E“ '  E
C} IL Ci
3“ a s“
".
D— I I' ‘1 D—
I d I I: I I I I I I I 1 I I
5011 1500 25011 35011 500 15111} 2500 35011
Pacifietnlue Paciliebiue

Fig. 4. Application of ﬂowPeaks to the barcode data. (A) the bold boundary
displays the clusters output by ﬂowPeaks with their centers (69), the dotted
lines are the boundary for the underlying K —means clusters with their
centers (0). The local peaks are indicated by A. (B) The same as in (A)
except the outliers have been identiﬁed as black points and other secondary
information was not displayed, and the clusters are labeled according to their
proportions (wk) (A color version of this ﬁgure is available as Supplementary
Material)

Table 1. Comparison of the running time of different ﬂow cytometry
clustering algorithms

 

ﬂowPeaks Misty FLOCK ﬂowMeans FLAME ﬂowMerge

 

mountain
Concave 0.13 0.59 6 6.2 1434 3202
Barcode 2.3 24.7 14 82.3 80 952 132 446

 

The running time is shown in wall-clock seconds on the same desktop computer
except that FLOCK and FLAME were run, respectively, at immport (http://immport.
niaid.nih. gov) and gene pattern websites (http:www.broadinstitute.org/cancer/software/
genepattern)

Table 2. The performance of different algorithms on the barcode data

 

ﬂowPeaks Misty FLOCK ﬂowMeans FLAME ﬂowMerge

 

mountain
Adj-Rand 0.998 0.971 0.258 0.998 0.859 0.801
F-measure 0.993 0.984 0.341 0.993 0.868 0.887
V-measure 0.996 0.967 0.567 0.995 0.946 0.952

 

Adj-Rand is for the adjusted Rand index.

speed of ﬂowMerge and FLAME and the difﬁculty to batch running
FLOCK and FLAME, which are only available from a web interface,
for performance comparison on the 12 samples in the GvHD dataset,
we only selected ﬂowPeaks, Misty Mountain and ﬂowMeans, which
are the three best algorithms according to Tables 2 and 3. Table 4
shows that ﬂowPeaks is better than the other two algorithms for
the GvHD dataset. We have displayed the ﬂowPeaks results for the
four datasets in Figures 4A, 5A—C. Since rituximab does not have a
gold standard, the visual display shows that ﬂowPeaks does a good
job revealing the cluster structure of the data. Figure 5D displays
the application of ﬂowPeaks in the GvHD data when FSC and SSC
channels are included. The clustering on 6D highly agrees with 4D
with only 0.59% of points classiﬁed differently between 6D and 4D.

Table 3. Performance of different algorithms on the concave data

 

ﬂowPeaks Misty FLOCK ﬂowMeans FLAME ﬂowMerge

 

mountain
Adj-Rand 1.000 1.000 0.501 0.723 0.232 0.952
F-measure 1.000 1.000 0.683 0.884 0.438 0.987
V-measure 1.000 1.000 0.667 0.713 0.480 0.932

 

Adj-Rand is for the adjusted Rand index.

Table 4. The comparison of the performance on the 12 samples of the GvHD
dataset

 

ﬂowPeaks Misty mountain ﬂowMeans

 

Adj—Rand 0.807 (0.175) 0.675 (0.287) 0.573 (0.292)
F—measure 0.924 (0.075) 0.859 (0.146) 0.848 (0.120)
V—measure 0.816 (0.135) 0.664 (0.205) 0.639 (0.199)

 

Adj-Rand is for the adjusted Rand index. Each entry lists the mean and standard
deviation.

4 SOFTWARE

We have implemented the algorithm in C++ wrapped into an R
package named ‘ﬂowPeaks’. The following example illustrates how
to use the basic functions of this R package

library(flowPeaks)
data(barcode)
fp<—flowPeaks(barcode[,c(l,3)])
plot(fp,drawlocalpeaks=TRUE)

The above R script will display Figure 4A. In order to identify
the outliers to obtain Figure 4B, we can proceed further with the
following script

fpc<—assign.flowPeaks(fp,fp$x)
plot(fp,classlab=fpc,drawboundary=FALSE,
drawvor=FALSE,drawkmeans=FALSE,drawlab=TRUE)

For further use of the software ﬂowPeaks, one can consult the
package’s vignette pdf ﬁle and help documents.

5 DISCUSSION AND FUTURE WORK

In this article, we described the algorithm ﬂowPeaks that combines
the K —means and density function peak ﬁnding to partition the
ﬂow cytometry data into distinct clusters. We have compared our
algorithm with other state of the art algorithms for real and simulated
datasets. Our algorithm is fast and able to detect the non—convex
shapes. We should point out that ﬂowPeaks’s goal is to ﬁnd the
overall density shape and search for global structure. It will not
be able to uncover overlapping clusters as shown in Figure 1C or
the rare cluster as shown in Figure 2C. The ﬂowPeaks algorithm
is based on the geometrical shape of the density function. Prior to
apply ﬂowPeaks, data transformation may be necessary to reveal the
structure, and irrelevant channels need to be ﬁrst discarded to avoid
the curse of dimensionality. Due to the curse of dimensionality, if the
data dimension is too high and the number of cells is too low where

 

2056

112 /810's12urnofp101x0'sor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

flowPeaks: a unsupervised clustering for flow data

 

 

 

 

 

 

 

 

 

 

I I I I I
-2 -1 El 1 2 3 4 D 211-0 401} EDD BUD 1000
x FL1 .H

 

 

“ fp4ﬂ.lab - '
prDlab -
GSJab- I —I

FL1 .H I

FL3.H
SSC—H
FSCH

FLE.H

I I ~. I FL4.HI
I1] 2m] 401} ED!) Bun Iii:

FLI.H 2m: .1 n ' I] IL'L'ﬂ

 
       

 

 

 

Fig. 5. The application of ﬂowPeaks to different datasets. See Figure 4A for
explanations of legends. (A) Concave data. (B) One sample of the GvHD
dataset that is shown in Figure 3C. The boundaries between ﬂowPeaks
clusters and between K —means clusters are not drawn as two non—overlapping
clusters generated in 4D may overlap in a 2D projection. (C) Rituximab
dataset. (D) The same sample in (B) with FSC and SSC channels included.
Due to the long running time required for the heatmap, 4000 data points
were randomly selected to generate the cluster—tree and the heatmap. The
three rows fp4D.1ab, fp6D.lab and GS.lab, respectively, display the class
labels of the ﬂowPeaks on 4D, ﬂowPeaks on 6D, and the Gold Standard,
where different colors indicate different clusters. The signal intensities of
all six channels are displayed in the heatmap with the key displayed on the
bottom (A color version of this ﬁgure is available as Supplementary Material)

the density function cannot be reliably estimated by ﬂowPeaks, users
should alternatively use the heatmap to visualize the data.

As commented in Jain (2010), there is not a single clustering
algorithm suitable for all datasets. This is probably true for ﬂow
cytometry clustering. There is not a good collection of ﬂow
cytometry data with gold standard gates, which makes algorithm
comparison very challenging. The comparison in Section 3.3
should not be taken literally. We tend to agree with Naumann
et al. (2010) that ‘it is too early for extensive comparisons of
automated gating procedure’. The current approach of using the
manual gating as a gold standard to compare the automatic gating
algorithm is very subjective. We participated with ﬂowPeaks and
support vector machine algorithm in 2011’s ﬂowCAP II (http://
ﬂowcapﬁowsite.org/summit2011.html). Our algorithm gave 100%
prediction accuracy for the clinical ﬂow cytometry data, establishing
us as one of the best algorithms. We have released our datasets in
our ﬂowPeaks package with the gold standard gates so that one
can test one’s favorite algorithm with our datasets. The source

code and windows binary built of the R package ﬂowPeaks is
available at https://github.com/yongchao/ﬂowPeaks. The package
is in the progress of being permanently hosted at the Bioconductor
(Gentleman et al., 2004; Ihaka and Gentleman, 1996) with open
source code for algorithm developers and batching processing.

APPENDIX

Mathematical notation

For the sake of clarity, we will use the following notation. Assume
the data consist of n points in d dimension.

Let the underlying clusters, obtained by K —means, be labeled
as 1,...,K. The density function generated by the ﬁnite mixture
model is

K
ﬁx):ZWk/IikIl/ZCXP{—(x—Mk)t§;:1(X—Mk)},
k=1

where wk, ,uk, 22k are the weights, means and the smoothed variance
matrix of cluster k, respectively, for k: 1, ...,K. The derivative of
the density function at x is deﬁned as

f’(x)=3f(x)/BX-

According to the K —means algorithm, the cluster label of x can be
deﬁned as

L(x>=argnnn§=1IIx—ttku2.

Searching the local peak starting from a point x

As we do not want to jump over the local peak, when the data fall
into a cluster k, we deﬁne the maximum step size
max - ~i,i
ﬂk Digit! 2" '
The detailed computations for the local peak search are described in
Algorithm A1. We initially set a small step size ,8 (Step 0), and try
to ﬁnd a step size such that the density function f improves (Step
2 and Step 3). If the same step size improves twice in a row (Nsuc
denote the number of continuous improvements), then we double
the step size; otherwise we half the step size. If the point is falling
into a new cluster, we want to ﬁnd out if we can jump to the new
center directly (Step 6). The details are described in Algorithm A1.

The algorithm on merging local peaks

When two peaks are close and the density function between the two
peaks is relatively ﬂat, the two peaks should be combined into one.
For each underlying K —means cluster, we deﬁne the nearest neighbor
cluster distance by

Sk=min{||,u.k—,u.i|| :i€{1,...,K} and i7ék}.

For an arbitrary position x, we can similarly deﬁne the function
S (x) :5 L06). Let x and y be two points, we deﬁne the tolerance that
describes how the density function of the line segment that connects
x and y can be approximated by a straight line

. (Flux) +nL(y))/2
n / K ’

tea—fat)
12(0)

 

tol = max
te[0,1]

 

 

 

2057

112 /810's12urnofp101x0'sor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

KGe and S. C.Sealfon

 

 

Algorithm A1 searching the local peak starting from a point x

 

0. Set x0 2:x, k0 :L(x), ,8 2:,8gaX/10, Nsuc :0 and n 2:0
1. If ,8 is small or |[f’(xn)|| is small, stop. If n is too large, stop.

2. Let y2:xn

3. If f(y)>f(xn), then Nsuc2:NsuC—I—1 and go to Step 4;
otherwise ,8 2: ,8 /2, Nsuc 2:0 and go to Step 1

4. If Nsuc Z 2, then ,8 2:min(2,8, 8,313“).

5. Let xn+1 2:y and kn+1 2:L(y). If kn+1 :kn, then go to Step
6; otherwise, go to Step 7

6. Update ,8 by setting ,8 to be 833/ 10. Check if we can jump
directly to the center of the new cluster: if f ( ,u kn + l) > f (xn+1 ),
then set xn+1 2: ,uk

n+1 '

7. Update n2:n—I—1, go to Step 1.

 

where z, :x + t(y —x) and 1%,) :f(x) —I—t(f(y) —f(x)). The function
f(zt) is the ﬁtted density function at the position z; by using a straight
line to connect the two points (x, f (x) and (y, f (y)). The second term
in deﬁning tol corrects for cluster sample sizes.

Many K —means centers may reach the same local peak. A local
peak can then be represented by a subset P, of {1, ...,K} and its
location is denoted by vj, where j : 1, . . . ,Np and Np is the number
of distinct local peaks. In other words, for each k in P-, ,uk will
move to the same vj by using our local peak algorithm. Initially,
set Gg :{g},g:1,...,Np, i.e. each peak set just contains a single
peak (Step 0). Two peak sets can be merged only if the two peaks are
relative close and the density function between the peaks is relatively
ﬂat (Step 1). Gg are merged hierarchically (Step 2). The details are
given in Algorithm A2. After the algorithm completes, NG is the
number of IC (see Section 2.4) ﬁnal clusters.

 

Algorithm A2 peak merging algorithm

 

0. Let Gg:{g},g:1,...,Ng, where NG is initially Np.

1. Set (g, h) : argmin(g,h){d(Gg, Gt) 2 Gg and G}, can be merged,
g<h}, If (g,h) do not exist, stop; otherwise go to Step 2.
Gg and G}, can be merged only if there exists a p E Gg and a
p’ E G}, such that t0l(vp, 1);) 5 tolo and “up — 1);, || 5 2(S(vp)—I—
S(vp/)).

2. Merge Gg and G}, as in the following:
a. Update Gg 2: Gg UGh
b. Set Gil Z=Gh+1, ..., GNG_1=GNG
c. Update NG 2:NG —1 

3. If NG :1 stop; otherwise go to Step 1.

 

ACKNOWLEDGEMENTS

We thank Fernand Hayot, Istvan Sugar and German Nudleman
for valuable comments and discussions. We thank Ryan Brinkrnan
and Nima Aghaeepour for providing the GvHD dataset and the
associated manual gates. We appreciate the reviewers’ insightful
comments, resulting in a much improved article.

F unding: National Institute of Allergy and Infectious Diseases
[contract HHSN 266200500021C].

Conﬂict of Interest: none declared.

REFERENCES

Aghaeepour,N. et al. (2011) Rapid cell population identiﬁcation in ﬂow cytometry data.
Cytometry A, 79, 6—13.

Arthur,D. and Vassilvitskii,S. (2007) k-means++: the advantages of careful seeding.
In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, New Orleans, SIAM, pp. 1027—1035.

Chan,C. et al. (2008) Statistical mixture modeling for cell subtype identiﬁcation in ﬂow
cytometry. Cytometry A, 73, 693—70 1.

Finak,G. et al. (2009) Merging mixture components for cell population identiﬁcation in
ﬂow cytometry. Adv. Bioinformatics, 2009, 247646.

Freedman,D. and Diaconis,P. ( 1981) On the histogram as a density estimator: L2 theory.
Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete, 57, 453—476.
Fung,B.C.M. et al. (2003) Hierarchical document clustering using frequent itemsets. In
Proceedings of the Third SIAM International Conference on Data Mining (SDM),

San Francisco, CA, SIAM, pp. 59—70.

Gasparetto,M. et al. (2004) Identiﬁcation of compounds that enhance the anti-lymphoma
activity of rituximab using ﬂow cytometric high-content screening. J. Immunol.
Methods, 292, 59—71.

Gentleman,R. et al. (2004) Bioconductor: open software development for computational
biology and bioinformatics. Genome Biol, 5, R80.

Hartigan,J.A. and Wong,M.A. ( 1979) A K-means clustering algorithm. Appl. Stat, 28,
100—108.

Hubert,L. and Arabie,P. (1983) Comparing partitions. J. Classif, 2, 193—218.

Ihaka,R. and Gentleman,R. (1996) R: a language for data analysis and graphics.
J. Comput. Graph. Stat, 5, 299—314.

Jain,A.K. (2010) Data clustering: 50 years beyond K-means. Pattern Recogn. Lett., 31,
651—666.

Kanungo,T. et al. (2002) An efﬁcient k-means clustering algorithm: analysis and
implementation. IEEE Trans. Pattern Anal, 24, 881—892.

Krutzik,P. and Nolan,G. (2006) Fluorescent cell barcoding in ﬂow cytometry allows
high-throughput drug screening and signaling proﬁling. Nat. Methods, 3, 361—368.

Lloyd,S.P. (1982) Least squares quantization in PCM. IEEE Trans. Inform. Theory,
IT-28, 129—139.

Lo,K. et al. (2008) Automated gating of ﬂow cytometry data via robust model-based
clustering. Cytometry A, 73, 321—32.

Lo,K. et al. (2009) ﬂowClust: a Bioconductor package for automated gating of ﬂow
cytometry data. BMC Bioinformatics, 14, 145.

Murphy,R.F. (1985) Automated identiﬁcation of subpopulations in ﬂow cytometric list
mode data using cluster analysis. Cytometry, 6, 302—309.

Naumann,U. et al. (2010) The curvHDR method for gating ﬂow cytometry samples.
BMC Bioinformatics, 11, 44.

Pyne,S. et al. (2009) Automated high-dimensional ﬂow cytometric data analysis. Proc.
Natl. Acad. Sci. USA, 106, 8519—8524.

Qian,Y. et al. (2010) Elucidation of seventeen human peripheral blood B-cell subsets
and quantiﬁcation of the tetanus response using a density-based method for the
automated identiﬁcation of cell populations in multidimensional ﬂow cytometry
data. Cytometry B, 78, S69—S82.

Rand,W.M. (1971) Objective criteria for the evaluation of clustering methods. J. Am.
Stat. Assoc, 66, 846—850.

Rosenberg,A. and Hirschberg,J. (2007) V—Mmeasure: a conditional entropy-based
external cluster evaluation measure. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, Prague, Association for Computational Lingusistics,
pp. 410—420.

Sugar,I.P. and Sealfon,S.C. (2010) Misty Mountain clustering: application to fast
unsupervised ﬂow cytometry gating. BMC Bioinformatics, 11, 502.

 

2058

112 /§.IO'SIBUJHOprOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

