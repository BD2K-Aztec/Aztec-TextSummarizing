Vol. 30 ISMB 2014, pages i139—i148
doi: 10. 1 093/bioinformatics/btu293

 

Graph-regularized dual Lasso for robust eQTL mapping

Wei Chengl, Xiang Zhang2, Zhishan Guol, Yu Shi3 and Wei Wang“

1Department of Computer Science, UNC at Chapel Hill, Chapel Hill, NC 27599, 2Department of EECS, Case Western
Reserve University, OH 44106, USA 8Department of Mathematics, University of Science and Technology of China, Hefei
23002, China and 4 Department of Computer Science, University of California, Los Angeles, CA 90095, USA

 

ABSTRACT

Motivation: As a promising tool for dissecting the genetic basis of
complex traits, expression quantitative trait loci (eQTL) mapping has
attracted increasing research interest. An important issue in eQTL map-
ping is how to effectively integrate networks representing interactions
among genetic markers and genes. Recently, several Lasso-based
methods have been proposed to leverage such network information.
Despite their success, existing methods have three common limita-
tions: (i) a preprocessing step is usually needed to cluster the networks;
(ii) the incompleteness of the networks and the noise in them are not
considered; (iii) other available information, such as location of genetic
markers and pathway information are not integrated.

Results: To address the limitations of the existing methods, we
propose Graph-regularized Dual Lasso (GDL), a robust approach for
eQTL mapping. GDL integrates the correlation structures among
genetic markers and traits simultaneously. It also takes into account
the incompleteness of the networks and is robust to the noise. GDL
utilizes graph-based regularizers to model the prior networks and does
not require an explicit clustering step. Moreover, it enables further
refinement of the partial and noisy networks. We further generalize
GDL to incorporate the location of genetic makers and gene-pathway
information. We perform extensive experimental evaluations using
both simulated and real datasets. Experimental results demonstrate
that the proposed methods can effectively integrate various available
priori knowledge and significantly outperform the state-of-the-art
eQTL mapping methods.

Availability: Software for both C+ + version and Matlab version is
available at http://www.cs.unc.edu/~weicheng/.

Contact: weiwang@cs.ucla.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

1 INTRODUCTION

Expression quantitative trait loci (eQTL) mapping aims at iden-
tifying single nucleotide polymorphisms (SNPs) that inﬂuence
the expression level of genes. It has been widely applied to dissect
genetic basis of complex traits (Bochner, 2003; Michaelson et al.,
2009). Several important issues need to be considered in eQTL
mapping. First, the number of SNPs is usually much larger than
the number of samples (Tibshirani, 1996). Second, the existence
of confounding factors, such as expression heterogeneity, may
result in spurious associations (Listgarten et al., 2010). Third,
SNPs (and genes) usually work together to cause variation in
complex traits (Michaelson et al., 2009). The interplay among
SNPs and the interplay among genes can be represented as net-
works and used as prior knowledge (Musani et al., 2007; Pujana

 

*To whom correspondence should be addressed.

et al., 2007). However, such prior knowledge is far from being
complete and may contain a lot of noises. Developing effective
models to address these issues in eQTL studies has recently at-
tracted increasing research interests (Biganzoli et al., 2006; Kim
and Xing, 2012; Lee and Xing, 2012; Lee et al., 2010).

In eQTL studies, two types of networks can be utilized. One is
the genetic interaction network (Charles Boone and Andrews,
2007). Modeling genetic interaction (e. g. epistatic effect between
SNPs) is essential to understanding the genetic basis of common
diseases, since many diseases are complex traits (Lander, 2011).
Another type of network is the network among traits, such as the
protein—protein interaction (PPI) network or the gene co-expres-
sion network. Interacting proteins or genes in a PPI network are
likely to be functionally related, i.e. part of a protein complex or
in the same biological pathway (von Mering et al., 2002).
Effectively utilizing such prior network information can signifi-
cantly improve the performance of eQTL mapping (Lee and
Xing, 2012; Lee et al., 2010).

Figure 1 shows an example of eQTL mapping with prior net-
work knowledge. The interactions among SNPs and genes are
represented by matrices S and G, respectively. The goal of eQTL
mapping is to infer associations between SNPs and genes repre-
sented by the coefﬁcient matrix W. Suppose that SNP ® is
strongly associated with gene (9. Using the network prior, the
moderate association between SNP G) and gene @may be iden-
tified since 0) and C2), @and ©have interactions.

To leverage the network prior knowledge, several methods
based on Lasso have been proposed (Biganzoli et al., 2006;
Kim and Xing, 2012; Lee and Xing, 2012; Lee et al., 2010). In
Biganzoli et a]. (2006), the group-Lasso penalty is applied to
model the genetic interaction network. In (Kim and Xing,
2012) and (Lee et al., 2010), the authors consider groupings of
genes and apply a multi-task Lasso penalty. In (Lee and Xing,
2012), the authors further extend the model to consider grouping
information of both SNPs and genes. These methods apply a
‘hard’ clustering of SNPs (genes) so that a SNP (gene) cannot
belong to multiple groups. However, a SNP may affect multiple
genes and a gene may function in multiple pathways. To address
this limitation, in (Jenatton et al., 2011), the authors develop a
model allowing overlap between different groups.

Despite their success, there are three common limitations of
these group penalty based approaches. First, a clustering step is
usually needed to obtain the grouping information. To address
this limitation, (Kim and Xing, 2009; Li and Li, 2008) introduce
a network-based fusion penalty on the genes. However, this
method does not consider the genetic-interaction network.
A two-graph—guided multi-task Lasso approach is developed in
(Chen et al., 2012) to make use of gene co-expression network
and SNP-correlation network. However, this method does not

 

© The Author 2014. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by—nc/3.0/), which
permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact

journals.permissions@oup.com

112 /310's113umo [p.IOJXO'SSUBUHOJUIOIQ/ﬁdllq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

W.Cheng et aI.

 

consider the network prior knowledge. The second limitation of
the existing methods is that they do not take into consideration
the incompleteness of the networks and the noise in them (von
Mering et al., 2002). For example, PPI networks may
contain false interactions and miss true interactions (von
Mering et al., 2002). Directly using the grouping penalty inferred
from the noisy and partial prior networks may introduce new
bias and thus impair the performance. Third, in addition to
the network information, other prior knowledge, such as loca-
tion of genetic markers and gene-pathway information are also
available. The existing methods cannot incorporate such
information.

To address the limitations of the existing methods, we propose
a novel approach, Graph-regularized Dual Lasso (GDL), which
simultaneously learns the association between SNPs and genes
and reﬁnes the prior networks. To support ‘soft’ clustering
(allowing genes and SNPs to be members of multiple clusters),
we adopt the graph regularizer to encode structured penalties
from the prior networks. The penalties encourage the connected
nodes (SNPs/ genes) to have similar coefﬁcients. This enables us
to ﬁnd multiple-correlated genetic markers with pleiotropic ef-
fects that affect multiple-correlated genes jointly. To tackle the
problem of noisy and incomplete prior networks, we exploit the
duality between learning the associations and reﬁning the prior
networks to achieve smoother regularization. That is, learning
regression coefﬁcients can help to reﬁne the prior networks, and
vice versa. For example, in Figure 1, if SNPs C3) and G) have
strong associations with the same group of genes, they are
likely to have interaction, which is not captured in the prior
network. An ideal model should allow to update the prior net-
work according to the learned regression coefﬁcients. GDL can
also incorporate other available prior knowledge such as the
physical location of SNPs and biology pathways to which the
genes belong. The resultant optimization problem is convex and
can be efﬁciently solved by using an alternating minimization
procedure. We perform extensive empirical evaluation of the
proposed method using both simulated and real eQTL datasets.
The results demonstrate that GDL is robust to the incomplete
and noisy prior knowledge and can signiﬁcantly improve the
accuracy of eQTL mapping compared to the state-of—the-art
methods.

Genotypes
(genetic

interaction
network) 1/\\2 / \I4 @
i \ 3 ,’/
I: @ Duality
: 
' D
' C\ B/ \
(PIT.its X<\/\ / \ / @
,gene
co-expression //
network,...) \

Fig. 1. Examples of prior knowledge on genetic-interaction network S
and gene—gene interactions represented by PPI network or gene co-
expression network G. W is the regression coefﬁcients to be learned

2 BACKGROUND: LINEAR REGRESSION WITH
GRAPH REGULARIZER

Throughout the article, we assume that, for each sample, the SNPs
and genes are represented by column vectors. Important notations
are listed in Table 1. Let x=[x1, x2, . . . , xK]T represent the K
SNPs in the study, where x,- e {0, 1, 2} is a random variable cor-
responding to the i-th SNP (e.g. 0, l, 2 may encode the homozy-
gous major allele, heterozygous allele and homozygous minor
allele, respectively). Let z = [21, 22, . . . , ZN]T represent expression
levels of the N genes in the study, where zj is a continuous random
variable corresponding to the j-th gene. The traditional linear re-
gression model for association mapping between x and z is

z=Wx+u+e, (l)

where z is a linear function of x with coefﬁcient matrix W and ,u is
an N x 1 translation factor vector. And 6 is the additive noise of
Gaussian distribution with zero-mean and variance yI, where y is
a scalar. That is, e~N(0, yI).

The question now is how to deﬁne an appropriate objective
function over W that (i) can effectively incorporate the prior
network knowledge, and (ii) is robust to the noise and incom-
pleteness in the prior knowledge. Next, we first brieﬂy review
Lasso and its variations and then introduce the proposed GDL
method.

2.1 Lasso and LORS

Lasso (Tibshirani, 1996) is a method for estimating the regression
coefﬁcients W using 61 penalty for sparsity. It has been widely
used for association mapping problems. Let X = {xdll g d g D}
e [RKXD be the SNP matrix and Z={zd|1 g d g D} e [RNXD be
the gene-expression matrix. Each column of X and Z stands for
one sample. The objective function of Lasso is

. 1
%n§||Z—WX—M1II%+77IIWII1 (2)

Table 1. Summary of notations

 

 

Symbols Description

K Number of SNPs

N Number of genes

D Number of samples

X e [RKXD The SNP matrix data

Z G [RNXD The gene matrix data

L e IRNXD A low-rank matrix

So 6 [RKXK The input afﬁnity matrices of the genetic-interaction
network

G0 6 [RNXN The input afﬁnity matrices of the network of traits

S e [RKXK The reﬁned afﬁnity matrices of the genetic-interaction
network

G e [RNXN The reﬁned afﬁnity matrices of the network of traits

W e [RNXK The coefﬁcient matrix to be inferred

72(5) The graph regularizer from the genetic-interaction
network

Rm) The graph regularizer from the PPI network

D(-, -) A non-negative distance measure

 

 

i140

112 /310's113umo [p.IOJXO'SSUBUHOJUIOIQ/ﬁdllq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

GDL for robust eQTL mapping

 

where H - ||F denotes the Frobenius norm, || - H1 is the El-norm, 1
is an 1 x D vector of all 1’s, )7 is the empirical parameter for the
51 penalty and W is the parameter (also called weight) ma-
trix parameterizing the space of linear functions mapping from
X to Z.

Confounding factors, such as unobserved covariates, experimen-
tal artifacts and unknown environmental perturbations, may
mask real signals and lead to spurious ﬁndings. LORS (Yang
et al., 2013) uses a low-rank matrix L e [RNXD to account for the
variations caused by hidden factors. The objective function of
LORS is

. 1
mm —||Z—WX—ul—LII%+nIIWII1+MILII* (3)
W,M,L2

where l | - ||,k is the nuclear norm, )7 is the empirical parameter for the
51 penalty to control the sparsity of W and A is the regularization
parameter to control the rank of L. L is a low-rank matrix assuming
that there are only a small number of hidden factors inﬂuencing the
gene-expression levels.

2.2 Graph-regularized Lasso

To incorporate the network prior knowledge, group sparse Lasso
(Biganzoli et al., 2006), multi-task Lasso (Obozinski and Taskar,
2006) and SIOL(Lee and Xing, 2012) have been proposed. Group
sparse Lasso makes use of grouping information of SNPs; multi-
task Lasso makes use of grouping information of genes, while
SIOL uses information from both networks. A common draw-
back of these methods is that the number of groups (SNP and gene
clusters) has to be predetermined. To overcome this drawback, we
propose to use two graph regularizers to encode the prior network
information. Compared with the previous group penalty-based
methods, our method does not need to pre-cluster the networks
and thus may obtain smoother regularization. Moreover, these
methods do not consider confounding factors that may mask
real signals and lead to spurious ﬁndings. In this article, we further
incorporate the idea in LORS (Yang et al., 2013) to tackle the
confounding factors simultaneously.

Let So 6 [RKXK and G0 6 [RNXN be the afﬁnity matrices of the
genetic interaction network (e.g. epistatic effect between SNPs)
and network of traits (e.g. PPI network or gene co-expression
network), and D50 and DG0 be their degree matrices. Given
the two networks, we can employ a pairwise comparison between
W*,- and W*j (l g i< j f K) : if SNPs i and j are closely related,
||W*,- — W*j| IE is small. The pairwise comparison can be naturally
encoded in the weighted fusion penalty 21,. “mi — W*j||%(So),-,j.
This penalty will enforce ||W*,- —W*j||%=0]for closely related
SNP pairs (with large (80),” value). Then, the graph regularizer
from the genetic-interaction network takes the following form

1
= E E ||W*,‘ — W*jllg(SO)i,j
U

R“) (4)

= tr(W(DSO — SO)WT).

Similarly, the graph regularizer for the network of traits is

73(6) = tr(WT(DG0 — G0)W). (5)

These two regularizers encourage the connected nodes in a graph
to have similar coefﬁcients. A heavy penalty occurs if the
learned-regression coefﬁcients for neighboring SNPs (genes) are
disparate. (D50 — So) and (DG0 — G0) are known as the combina-
torial graph Laplacian, which are positive semi-deﬁnite (Chung,
1997). Graph-regularized Lasso (G—Lasso) solves the following
optimization problem

1
' — Z—WX— l—L 2
$132M u IIF (6)
+27||W||1+).||L||*+aR(S)+,BR(G).

where a, ,6 > 0 are regularization parameters.

3 GDL

In eQTL studies, the prior knowledge is usually incomplete and
contains noise. It is desirable to reﬁne the prior networks accord-
ing to the learned regression coefﬁcients. There is a duality be-
tween the prior networks and the regression coefﬁcients: learning
coefﬁcients can help to reﬁne the prior networks, and vice versa.
This leads to mutual reinforcement when learning the two parts
simultaneously.

Next, we introduce the GDL. We further relax the constraints
from the prior networks (two graph regularizers) introduced in
Section 2.2, and integrate the G-Lasso and the dual reﬁnement of
graphs into a uniﬁed objective function

1
' — Z— X— l—L 2+ +y L
wiﬁélguéoz“ W M HF nilWIh H H.

+atr(W(DS — S)WT> + ,Btr(WT(DG — G)W>
+i/IIS — SoiI2F+piiG — Golliv
(7)

where y, p>0 are positive parameters controlling the extent to
which the reﬁned networks should be consistent with the original
prior networks. DS and DG are the degree matrices of S and G.
Note that the objective function considers the non-negativity of S
and G. As an extension, the model can be easily extended to
incorporate prior knowledge from multiple sources. We only
need to revise the last two terms in Equation (7) to
ny21||s — S,||%+pi:1||G — Gillfv, Where f and e are the
number of sources for genetic interaction networks and gene
trait networks, respectively.

3.1 Optimization: an alternating minimization approach

In this section, we present an alternating scheme to optimize the
objective function in Equation (7) based on block coordinate
techniques. We divide the variables into three sets: {L}, {S, G}
and {W, ,u}. We iteratively update one set of variables while
ﬁxing the other two sets. This procedure continues until conver-
gence. Since the objective function is convex, the algorithm will
converge to a global optima. The optimization process is as fol-
lows. The detailed algorithm is included in the Supplementary
Material (Algorithm 1).

(1) While ﬁxing {W, ,u}, {S, G}, optimize {L} using singular
value decomposition (SVD).

 

i141

112 /310's113u1no [p.IOJXO'SOllBIIHOJUIOIQ/ﬁdllq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

W.Cheng et aI.

 

LEMMA 3.1. (Mazumder et al., 2010) Suppose that matrix A has
rank r. The solution to the optimization problem

. 1 2
— —— -+
mﬁn 211A BIIF MIBH. (8)
is given by is =H,(A), where H,(A) =UD,VT with
Di =diag[(d1 — A)+,  (d, — A)+], UDVTis the Singular Value
Decomposition (SVD) of A, D=diag[d1,  dr], and
(di — 90+ =maX ((di — A), 0), (1 E if r)-

Thus, for ﬁxed W, ,u, S, G, the formula for updating L is
L <— H)(Z — WX — ,al). (9)

(2) While ﬁxing {W,,u}, {L}, optimize {S, G} using semi-
non-negative matrix factorization (semi-NMF) multiplicative
updating on S and G iteratively (Ding et al., 2010). For the op-
timization with non-negative constraints, our updating rule is
based on the following two theorems. The proofs of the theorems
are given in Section 3.2.

THEOREM 3.2. For ﬁxed L, ,u, W and G, updating S according to
Equation (10) monotonically decreases the value of the objective
function in Equation (7) until convergence.

a(WTW) + + 2ys0

s s _
<— 0 2yS + a(WTW) + adiag(WTW)JK

 

(10)

where J K is a K x K matrix of all 1’s. 0,  are element-wise
operators. Since WTW may take mixed signs, we denote
WTW=(WTW)+ — (WTW)_, where (WTW),+]. =(|(WTW),.J|+
(WTW).,,~)/2 and (WW); = (1(WTW),,-1 — (WTW),,)/2.

THEOREM 3.3.. For ﬁxed L, ,u, W and S, updating G according
to Equation (11) monotonically decreases the value of the ob-
jective function in Equation (7) until convergence.

,B(WWT) + + 2pG0

G G _
<— 0 2pc + ,B(WWT) + ,Bdiag(WWT)JN

(11)

 

where JN is an N X N matrix of all 1’s.

The above two theorems are derived from the Karush—Kuhn—
Tucker (KKT) complementarity condition (Boyd and
Vandenberghe, 2004). We show the updating rule for S below.
The analysis for G is similar and omitted. We ﬁrst formulate the
Lagrange function of S for optimization

L(S) =atr(W(DS — S)WT) + yIIS — 50113,. (12)

The partial derivative of the Lagrange function with respect to
S is
VSL = — aWTW — 2yS0 + 2yS + adiag(WTW)JK. (13)

Using the KKT complementarity condition for the non-
negative constraint on S, we have

VsLo s=0. (14)

The above formula leads to the updating rule for S
in Equation (10). It has been shown that the multiplicative
updating algorithm has ﬁrst order convergence rate (Ding
et al., 2010).

(3) While ﬁxing {L}, {S, G}, optimize {W, ,u} using the coord-
inate descent algorithm.

Because we use the 51 penalty on W, we can use the coordinate
descent algorithm for the optimization of W, which gives the
following updating formula:

F(m(i,j), 77)

(xxT)j,j + 205(DS — S)”- -l- 2ﬂ(DG _ G)”. (15)

 

WM 2
where F(m(i,j), n) =Sign(m(i,j))maX (lm(i,j)| — 17,0), and

K
m(i, j) = (sz),J — E :w,,k(xxT)k,j
k=1

k¢j

K N

— 205: w,-,,,(DS — 5),“. — 25: (DG — G),,ka,j.
k=1 k=1
kyéj kaéj

(16)

The solution of updating ,u can be derived by setting
VML(,u)=0, which gives

2%

D (17)

3.2 Convergence analysis

In the following, we investigate the convergence of the
algorithm. First, we study the convergence for the second step.
We use the auxiliary-function approach (Lee and Seung, 2000)
to analyze the convergence of the multiplicative updating for-
mulas. Here we ﬁrst introduce the deﬁnition of auxiliary
function.

DEFINITION 3.4. ~Given a function L(h) of any parameter k,
a function Z(h, h) is an auxiliary function for L(h) if the condi-
tions

Z(h, 15) Z L(h) and Z(h, h) =L(h), (18)
are satisﬁed for any given h, ll (Lee and Seung, 2000).

LEMMA 3.5. If Z is an auxiliary function for function L(h),
then L(h) is non-increasing under the update (Lee and Seung,
2000).

[10+ 1) = argmin Z(h, km). (19)
h

THEOREM 3.6. Let L(S) denote the Lagrange function of S for
optimization. The following function

 

i142

112 /§IO's113umo [p.IOJXO'SOIlBIlIJOJUIOIQ/ﬂdllq 1110131 popeOIHAAOQ

9IOZ ‘09 lsnﬁnv uo ::

GDL for robust eQTL mapping

 

~ 2 ~2
~ S?+S. _S?+S.
Z(S, S) = 0,: W12], M + a: (WMWM) M

— a: (WiJWLk) + Sjjc +10g§j’k> ‘I‘ y: 
jk

ijk SJ k

~ S-
_ 2y: (So)j,ij,k (I + log Sﬁ") + y: (50);,16.

Jk '

 

 

],k
(20)

is an auxiliary function for L(S). Furthermore, it is a convex
function in S and its global minimum is

a(WTW) + + 23/50

s = s - _ .
O 2yS + a(WTW) + adiag(WTW)JK

(21)

 

THEOREM 3.6. can be proved using a similar idea to that in (Ding
et al., 2006) by validating (i) L(S) g Z(S, S), (ii) L(S) =Z(S, S)
(iii) Z(S, S) is convex with respect to S. The formal proof is
provided in the Supplementary Material.

THEOREM 3.7. Updating S using Equation (10) will monotonic-
ally decrease the value of the objective in Equation (7), the ob-
jective is invariant if and only if S is at a stationary point.

PROOF. By Lemma 3.5 and Theorem 3.6, for each subsequent
iteration of updating s, we have L((S)°) = Z((S)0, (S)°) Z Z((S)
1, (5)0) Z Z((S)1, (5)1) =L((S)1) Z  Z L((S)Iter)- Thus L(S)
monotonically decreases. Since the objective function Equation
(7) is obviously bounded below, the correctness of Theorem 3.2 is
proved. Theorem 3.3 can be proved similarly. D

In addition to Theorem 3.7, since the computation of L in the
ﬁrst step decreases the value of the objective in Equation (7), and
the coordinate descent algorithm for updating W in the third step
also monotonically decreases the value of the objective, the al-
gorithm is guaranteed to converge.

4 GENERALIZED GDL

In this section, we extend our model to incorporate additional
prior knowledge such as SNP locations and biological pathways.
If the physical locations of two SNPs are close or two genes belong
to the same pathway, they are likely to have interactions.
Such information can be integrated to help reﬁne the prior
networks.

Continue with our example in Figure 1. If SNPs C3) and G)
affect the same set of genes (@and ®), and at the same time, they
are close to each other, then it is likely there exists interaction
between G) and GD.

Formally, we would like to solve the following optimization
problem

1

' — x—z— 1—L2+ W +iL
w,n,nn§1§),G202HW M “F n11 111 1111.
+ a: D(W*i’ W*J')Si,j + :8 E D(Wi*, Wj*)Gi,j.

i,j i,j

(22)

Here D(-, -) is a non-negative distance measure. Note that the
Euclidean distance is used in previous sections. S and G are ini-
tially given by inputs So and G0. We refer to this generalized
model as the generalized GDL (GGDL). GGDL executes the
following two steps iteratively until the termination condition
is met: (i) update W while ﬁxing S and G and (ii) update
S and G according to W, while guarantee that both
Z”, D(W*,, W*j)S,,j and Z”, D(W,*, Wj*)G,-,j decrease.

These two steps are based on the aforementioned duality be-
tween learning W and reﬁning S and G. The detailed algorithm is
provided in the Supplementary Material. Next, we illustrate the
updating process assuming that S and G are unweighted graphs.
It can be easily extended to weighted graphs.

Step 1 can be done by using the coordinate decent algorithm.
In Step 2, to guarantee that both Z”, D(W*i, W*j)Sl‘,j and Z”, D
(Wi*, Wj*)Gl‘,j decrease, we can maintain a ﬁxed number of 1’s in S
and G. Taking G as an example, once GM- is selected to change
from 0 to 1, another element GM with D(W,*, Wj*) <D(W,v*, Wj/*)
should be changed from 1 to 0.

The selection of (i, j) and (i’,j’) is based on the ranking of
D(W,‘*, Wj*) (l g i< j g N). Speciﬁcally, we examine 1c pairs (the
choice of 1c depends on the user’s belief in the quality of the prior
network. For example, it can be 5% of all (i, j) pairs) with the
smallest distances. Among them, we pick those having no edges
in G. Let 730 be this set of pairs. Accordingly, we examine 1c pairs
with the largest distances. Among these pairs, we pick up only
those having an edge in G. Let 731 be this set of pairs. The
elements of G corresponding to pairs in 730 are candidates for
updating from 0 to 1, since these pairs of genes are associated
with similar SNPs. Similarly, elements of G corresponding to
pairs in 731 are candidates for updating from 1 to 0.

In this process, the prior knowledge of gene pathways can be
easily incorporated to better reﬁne G. For instance, we can fur-
ther require that only the gene pairs in 730 belonging to the same
pathway are eligible for updating, and only the gene pairs in 731
belonging to different pathways are eligible for updating. We
denote the set of gene pairs eligible for updating by 730’ and
731’, respectively. Then, we choose min (IPO’I, I731’l) pairs in set
730’ with smallest D(W,'*, Wj*) ((i, j) e 730’) and update Gw- from 0
to 1. Similarly, we choose min (IPO’I, I731’l) pairs in set 731’ with
largest D(W,v*, W17...) ((i’,j’) e 731’) and update GM from 1 to 0.

Obviously, all D(Wi*,Wj*),S are smaller than D(w,v*,wjt*) if
Ic< w. Thus,  ,D(W,-*, Wj*)Gl‘,j is guaranteed to decrease.
The updating process, for S is similar except that we compare
columns rather than rows of W and use SNP locations rather
than pathway information for evaluating the eligibility for updat-
ing. The updating process ends when no such pairs can be found
so that switching their values will result in a decrease of the
objective function.

The convergence of GGDL can be observed as follows. The
decrease of the objective function value in the ﬁrst step is straight-
forward since we minimize it using coordinate decent. In the
second step, the change of the objective function value is given by

_aD(W*iS , W*jS) + OZD(W*;‘S’ , W>1<js’)
—ﬂD(Wie*, Wm) f WWW Witt)

which is always negative. Thus, in each iteration, the objective

(23)

 

i143

112 /§IO's113umo IpJOJXO'SOllBIIIJOJUIOIQ/ﬁdllq 1110131 popeOIHAAOQ

9IOZ ‘09 lsnﬁnv uo ::

W.Cheng et aI.

 

W(true) GDL-Lasso G—Lasso SIOL MtlassoZG Multi—task Sparse group LORS Lasso

 

 

 

 

 

10 —_ 10 -- J 10 u”. 10 - 10 10 10 - .10 - - 1o -
20— —20=.- '20.. -20.- 20 - 20— 20-- —20-- “2o-— - 0.8
30 —— 30"- - 30 -- 30 - 30 30 30' 30' -' 30 -‘
40 4o 40 - 40 40 4o 40 40 4o - 0.6
50 50 50 50 50'- 50 50 50 50 -
60 60— ' 60— 60 60 '—“'_ 60 60 _ T- 60— '60 0.4
70— 70 _ 70 _ 70 70 -' -- I 70 7o ‘ 70. ‘J 70. -— I
80 80 - so - so so so 80 - 80 - 80_ _ - 0.2
90 - 90 - - 90 -r 90 — 90 a-_ 90 90 -:- 90 :- 9o 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

1oo-‘—-—-——-1ooZ-l~—-—-100" ' 1ooLL~e—-1ooL'————-100— 100 100 ' 100' "
246810 246810 246810 246810 246810 246810 246810 246810 246810

Fig. 2. Ground truth of matrix W and that estimated by different meth-

ods. The x-axis represents traits and y-axis represents SNPs. Normalized

absolute values of regression coefﬁcients are used. Darker color implies
stronger association

function value decreases. Since the objective function is non-nega-
tive, the process eventually converges.

THEOREM 4.1. GGDL converges to the global optimum if both
ZijD(Wi*, Wj*) and ZijD(W*i, W*j) are convex to W.

PROOF: The last two terms in Equation (22) are linear with re-
spect to S and G, and convex to W according to the conditions
listed. Thus the objective function is convex over all variables. A
convergent result to the global optimum can be guaranteed. D

5 EXPERIMENTS

In this section, we perform extensive experiments to evaluate the
performance of the proposed methods. We use both simulated
datasets and real yeast eQTL dataset (Brem et al., 2005). For
comparison, we select several state-of—the—art methods, including
SIOL (Lee and Xing, 2012), two graph guided multi-task lasso
(mtlasso2G) (Chen et al., 2012), sparse group Lasso (Biganzoli
et al., 2006), sparse multi-task Lasso (Biganzoli et al., 2006),
LORS (Yang et al., 2013) and Lasso (Tibshirani, 1996). For all
the methods, the tuning parameters were learned using cross
validation.

5.1 Simulation study

We ﬁrst evaluate the performance of the selected methods using
simulation study. Note that GGDL requires additional prior
knowledge and will be evaluated using real dataset.

We adopt the same setup for the simulation study as that in
(Lee and Xing, 2012; Yang et al., 2013) and generate synthetic
datasets as follows. 100 SNPs are randomly selected from the
yeast eQTL dataset (Brem et al., 2005) (112 samples). Ten gene-
expression proﬁles are generated by Z11 =Wj,,X+ E], +Ej,k
(1 g j g 10), where Ej*~/\/'(0,021) (a: 1) denotes Gaussian
noise. E]... is used to model non-genetic effects, which is drawn
from N(0, t2), where ‘E=0.l. 2 is generated by MMT, where
M e [RDXC and MUN/W0, l). C is the number of hidden factors
and is set to 10 by default. The association matrix W is generated
as follows. Three sets of randomly selected four SNPs are asso-
ciated with three gene clusters (1—3), (4—6), (7—10), respectively.
In addition, one SNP is associated with two gene clusters (1—3)
and (4—6), and one SNP is associated with all genes. The associ-
ation strength is set to l for all selected SNPs. The clustering
structures among SNPs and genes serve as the ground truth of the

 

 

 

 

 

 

 

 

 

 

 

 

S ground truth So prior Reﬁned S
.x I I. 1 _ _ _ 1 _ _ _ 1
20' "x. . 20  _ _. 20 .i‘t. .Q .
440 . .‘x m 40 ‘ ‘3: 40 
a Z 
gay _ RR 0.5% 60 .   (15,0600 (NO 0.5
80. . . x, 80 _ _ _ ' 800. I x
0'-'- “‘0 0'--'= '0 0'--'-= '0
20 40 60 80100 20 40 60 80100 20 40 60 80100
SNP SNP SNP
G ground truth G0 prior Reﬁned G
1 1 1
2 2 2
t: 4 I: 4 .1: 4
g 6 0.5 g 6 0.5g 6 0.5
8 8 8
10 0 10 0 10 10
2 4 6. 8 10 2 4 6 8 10 2 4 6 _ 8 10
tralt trait trait

Fig. 3. The ground truth networks, prior partial networks and the reﬁned
networks

prior network knowledge. Only two of the three SNP (gene)
clusters are used in W to simulate incomplete prior knowledge.

Figure 2 shows the estimated W matrix by various
methods. The x-axis represents traits (1—10) and y-axis represents
SNPs (1—100). From the ﬁgure, we can see that GDL is more
effective than G-Lasso. This is because the dual reﬁnement en-
ables more robust model. G-Lasso outperforms SIOL and
mtlasso2G, indicating that the graph regularizer provides a
smoother regularization than the hard clustering based penalty.
In addition, SIOL and mtlasso2G do not consider confounding
factors. SIOL and mtlasso2G outperform multi-task Lasso and
sparse group Lasso since it uses both SNP and gene grouping
information, while multi-task Lasso and sparse group Lasso only
use one of them. We also observe that all methods utilizing prior
grouping knowledge outperform LORS and Lasso which cannot
incorporate prior knowledge. LORS outperforms Lasso since it
considers the confounding factors.

The ground-truth networks, prior networks and GDL-reﬁned
networks are shown in Figure 3. Note that only a portion of the
ground-truth networks are used as prior networks. In particular,
the information related to gene cluster (7—10) is missing in the
prior networks. We observe that the reﬁned matrix G well cap-
tures the missing grouping information of gene cluster (7—10).
Similarly, many missing pairwise relationships in S are recovered
in the reﬁned matrix (points in red ellipses).

Using 50 simulated datasets with different Gaussian noise
(02 = l and 02 = 5), we compare the proposed methods with al-
ternative state-of—the—art approaches. For each setting, we use 30
samples for test and 82 samples for training. We report the aver-
aged result from 50 realizations. Figure 4 shows the ROC curves
of TPR-FPR for performance comparison, together with the
areas under the precision-recall curve (AUCs) (Chen et al.,
2012). The association strengths between SNPs and genes are
set to be 0.1, l and 3, respectively. It is clear that GDL outper-
forms all alternative methods by effectively using and reﬁning the
prior network knowledge. We also computed test errors. On
average, GDL achieved the best test error rate of 0.9122, and
the order of the other methods in terms of the test errors is:
G-Lasso (0.9276), SIOL (0.9485), Mtlasso2G (0.9521), Multi-
task Lasso (0.9723), Sparse group Lasso (0.9814), LORS
(1.0429) and Lasso (1.2153).

 

i144

112 /810's112u1no [p.IOJXO'SOIlBIIIJOJUIOIQ/ﬂdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

 

(a)

 

I — GDL —Lasso — G—Lasso — SIOL MtlassoZG

Multi—task H 1 H Sparse group  LORS 11111 1' Lasso I

 

strength=0.1 strength=l strength=3

   

   
   

 

 

“0 0.05 0.1 0.15 0.2 "o 0.05 0.1 0.15 0.2 "o 0.05 0.1 0.15 0.2
FPR FPR FPR

(a) variance of errors (a2 = 1)

(C)

 

I — GDL’Lasso — G’Lasso — SIOL Mtlass02G Multiitask 1 H H Sparse group  LORS 11111 '1 Lasso I

 

strength: 1

strength=3

strength=0. l

  
   

   

 

"0 0.05 0.1 0.15 0.2 “0 0.05 0.1 0.15 0.2 "0 0.05 0.1 0.15 0.2
FPR FPR FPR

(c) variance of errors (a2 = 5)

§

 

 

- strength=3

GDL for robust eQTL mapping
I:Istrength=1
- strength=0. l

Lasso LORS Sparse groupMulti-task MtlassoZG SIOL G-Lasso GDL-Lasso

.0
u:

 

.0 .o .o .0
p—A N Ln -I>

AUC of precision—recall curve

0

(b) AUC of precision-recall curve (a2 = 1)

g

 

 

   
 

.0
U]

 

- strength=0. 1

.o .o .o .9
h—I N U) &

AUC of precision-recall curve

0

Lasso LORS Sparse groupMulti-task MtlassoZG SIOL G—Lasso GDL-Lasso

(d) AUC of precision-recall curve (a2 = 5)

Fig. 4. Power curves for synthetic data. The left plots show the ROC curve, where our model GDL achieved maximum power. The black solid line
denotes what random guessing would have achieved. The right plots illustrate the areas under the precision-recall curve (AUCs) of different methods

To evaluate the effectiveness of dual reﬁnement, we compare
GDL and G-Lasso since the only difference between these two
methods is whether the prior networks are reﬁned during the
optimization process. We add noises to the prior networks by
randomly shufﬂing the elements in them. Furthermore, we use

the signal-to-noise ratio deﬁned as SNR= 

 

+XE (Yang et al.,

2013) to measure the noise ratio in the eQTL datasets. Here,
we ﬁx C: 10, ‘E=0.l, and use different 0’s to control SNR.

Figure 5 shows the results for different SNRs. For a ﬁxed
SNR, we vary the percentage of noises in the prior networks
and compare the performance of selected methods. From the
results, we can see that G-Lasso is more sensitive to noises in
the prior networks than GDL is. Moreover, when the SNR is
low, the advantage of GDL is more prominent. These results
indicate using dual reﬁnement can dramatically improve the ac-
curacy of the identiﬁed associations.

5.2 Yeast eQTL study

We apply the proposed methods to a yeast (Saccharomyces cer-
evisiae) eQTL dataset of 112 yeast segregants generated from a
cross of two inbred strains (Brem et al., 2005). The dataset ori-
ginally includes expression proﬁles of 6229 gene-expression traits
and genotype proﬁles of 2956 SNPs. After removing SNPs with
>10% missing values and merging consecutive SNPs high link-
age disequilibrium, we get 1017 SNPs with unique genotypes
(Huang et al., 2009). After removing the ones with missing
values, 4474 expression proﬁles are selected. The genetic
interaction network is generated as in (Lee and Xing, 2012).
We use the PPI network downloaded from BioGRID (http://
thebiogrid.org/) to represent the prior network among genes. It
takes ~l day for GGDL, and ~th for GDL to run into
completion.

 

 

- a - GDL—Lasso- 9 - G—Lasso- ~ - LORS - x - Lasso I

 

 

 

 

 

 

 

 

 

 

 

1 : . . : 1 ' : : : : : : :
D “UL-E I I : : : :  :
9 A ' : ' .
E ‘QIRL ' E it“. 2 ; ; . ' .
o 095 ......“I...°;......P.~n..ﬁ.,§.£.i 0.95  0.951;:tkén  .. :.. ..  ..
r r ' : : : E- .~ :
c4 ' °~° ' 5 1.x r r 2 °.. “we-
a '  : v.9 :  a I!" a
I .\°'9~e ‘s-qo‘f : n : T
M 09 ' ‘ ‘ ‘ ' ‘  ' ‘ ‘ ‘ ' ‘ ‘  ‘ ‘ ‘ ' ‘ ‘ ‘  ‘ ‘ ' ‘ ‘ ‘  ‘ ' ‘ ‘ ‘ ' “ 09 " 'QEG' 09 ‘ ‘ ' ' ‘ ‘ "; ‘ ‘ ' ' ‘ ‘ "‘0; ' ‘ ‘ ' ' ‘ ‘ ' ' ‘ ‘ ' ' ‘ ‘ ' ' ‘ ‘ ' ' “
(3.1 "°'?"'f"'f'".'"" ; 5 5 2 ~, . 3°»
- - - ' : : : ' : 0
Col—:4 )‘I-X-g-x-T-x-8-‘x-g-x-a‘ : E E 3 : E :‘9 .
o i . . Z "‘-9-'-‘§-’-§-“-§-‘-‘ . E . 9‘
B 085 . . . . . .  . . . . . .  . . . . . . .  . . . . . .  . . . . . . ..  0.85 .. ..: ..  ..  ..  8‘”
<0 ' ' ' ...-.f-..;-.-;..-f.-.-.
: O . E 3 E n)!- -X-)K-)(--X-X-X--X-X-D(-!(
0.3 0.0 0.0
0 0.4 0.6 0.8 l 0 0.2 0.4 0.6 0.8 l 0 0.2 0.4 0.6 0.8 l

0.2
random shufﬂe rate random shufﬂe rate random shufﬂe rate

Fig. 5. The areas under the TPR—FPR curve (AUCs) of Lasso, LORS,
G—Lasso and GDL. In each panel, we vary the percentage of noises in the
prior networks So and G0

5.2.1 cis- and trans-enrichment analysis

We follow the standard cis—enrichment analysis (Listgarten et al.,
2010) to compare the performance of two competing models.
The intuition behind cis—enrichment analysis is that more cis-
acting SNPs are expected than trans-acting SNPs. A two-step
procedure is used in the cis—enrichment analysis (Listgarten
et al., 2010): (i) for each model, we apply a one-tailed Mann—
Whitney test on each SNP to test the null hypothesis that the
model ranks its cis hypotheses no better than its trans hypoth-
eses, (ii) for each pair of models compared, we perform a two-
tailed paired Wilcoxon sign-rank test on the P—values obtained
from the previous step. The null hypothesis is that the median
difference of the P—values in the Mann—Whitney test for each
SNP is zero. The trans-enrichment is implemented using similar
strategy (Brem et al., 2003), in which genes regulated by tran-
scription factors (obtained from http://www.yeastract.com/
download.php) are used as trans-acting signals.

In addition to the methods evaluated in the simulation study, GGDL
is also evaluated here (with/c = 100000, 17 = 5, A = 8, a = 15, ,6 = 1) (for
GDL, n=5,).=8,a=15,,8= l, y=15,p=l). The Euclidean

 

i145

112 /810's112u1noprOIXO'soI1euIIOIUIOIq//:d11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

W.Cheng et aI.

 

Table 2. Pairwise comparison of different models using ciS-enrichment and trans-enrichment analysis

 

 

 

 

 

<
I

 

GDL G—Lasso SIOL Mtlasso2G Multi-task Sparse group LORS Lasso
CiS-enrichment
GGDL 0.0003 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001
GDL — 0.0009 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001
G-Lasso — — <0.0001 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001
SIOL — — — 0.1213 0.0331 0.0173 <0.0001 <0.0001
Mtlasso2G — — — — 0.0487 0.0132 <0.0001 <0.0001
Multi—task — — — — — 0.4563 0.4132 <0.0001
Sparse group — — — — — — 0.4375 <0.0001
LORS — — — — — — — <0.0001
T ram-enrichment

GGDL 0.0881 0.0119 0.0102 0.0063 0.0006 0.0003 <0.0001 <0.0001
GDL — 0.0481 0.0253 0.0211 0.0176 0.0004 <0.0001 <0.0001
G-Lasso — — 0.0312 0.0253 0.0183 0.0007 <0.0001 <0.0001
SIOL — — — 0.1976 0.1053 0.0044 0.0005 <0.0001
Mtlasso2G — — — — 0.1785 0.0061 0.0009 <0.0001
Multi—task — — — — — 0.0235 0.0042 0.001 1
Sparse group — — — — — — 0.0075 0.0041
LORS — — — — — — — 0.2059

(3) (b) (C)

3." ... ..  '8 .o f ,3“. .3... 7 u .r ' 2:. '" . Z . f .v f-

        .   

  ..    :.: ;_:.. : - -  . ;   .-:'.:

O V.. .. o .. . “I .1 .. 5 VI .

 

 

 

“a
i

‘4"- ..

 

,....:

 

 

 

I.
I
:EE<
O

Q
r

‘4':

 

a. ‘ e a ‘ an e u
I II III IV VVI VII VIIIX X. In XII XIII XIV XV XVI
Genomic pos1t10n(SNP)

GGDL-Lasso

; a . e s a
I II III IV VVIVII VIInx x XI XII XIII XIV xv XVI
enomIc p051t10n(SNP)

GDL-Lasso

 

I a. e a a e e, - .2: a p"
I II III IV VVI VII VIIIx x x1 XII XIII XIV xv XVI
Genomic p051t10n(SNP)

GL-Lasso

Fig. 6. The top-1000 signiﬁcant associations identiﬁed by different methods. In each plot, the x-axis represents SNPs and y-axis represents genes. Both

SNPs and genes are arranged by their locations in the genome

distance is used as the distance metric. We rank pairs of SNPs and genen
according to the learned W. S is reﬁned if the locations of the two SNPs
are <500 bp. G is reﬁned if the two genen are in the same pathway. The
pathway information is downloaded from Saccharomyces Genome
Database [SGD (http://www.yeastgenome.org/)].

The results of pairwise comparison of selected models are
shown in Table 2. In this table, a P—value shows how signiﬁcant
a method on the left column outperforms a method in the top
row in terms of cis and trans enrichments. We observe that the
proposed GGDL and GDL have signiﬁcantly better enrichment
scores than the other models. By incorporating genomic location
and pathway information, GGDL performs better than GDL
with P—value<0.0001. The effectiveness of the dual reﬁnement
on prior graphs is demonstrated by GDL’s better performance
over G-Lasso. Note that the performance ranking of these
models is consistent with that in the simulation study.

The top-1000 signiﬁcant associations given by GGDL, GDL
and G-Lasso are shown in Figure 6. We can see that GGDL and
GDL have stronger cis—regulatory signals than G-Lasso does. In
total, these methods each detected ~6000 associations according

to non-zero W values. We estimate FDR using 50 permutations
as proposed in (Yang et al., 2013). With FDR g 0.01, GGDL
obtains ~4500 signiﬁcant associations. The plots of all identiﬁed
signiﬁcant associations for different methods are given in the
Supplementary Material.

5.2.2 Reﬁnement of the prior networks

To investigate to what extend GGDL is able to reﬁne the prior
networks and study the effect of different parameter settings on
K, we intentionally change 75% elements in the original prior PPI
network and genetic-interaction network to random noises. We
feed the new networks to GGDL and evaluate the reﬁned net-
works. The results are shown in Figure 7. We can see that for
both PPI and genetic-interaction networks, many elements are
recovered by GGDL. This demonstrates the effectiveness of
GGDL. Moreover, when the number of SNP (gene) pairs (1c)
examined for updating reaches 100 000, both PPI and genetic-
iteration networks are well reﬁned.

 

i146

112 /810's112u1noprOIXO'soI1em101uIOIq//:d11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

GDL for robust eQTL mapping

 

5.2.3 Hotspots analysis

In this section, we study whether GGDL can help detect more
biologically relevant associations than the alternatives.
Speciﬁcally, we examine the hotspots which affect >10 gene
traits (Lee and Xing, 2012). The top-15 hotspots detected by
GGDL are listed in Table 3. The top-15 hotspots detected by
other methods are included in the Supplementary Material.
From Table 3, we observe that for all hotspots, the associated
genes are enriched with at least one GO category. Note that
GGDL and GDL detect one hotspot (12), which cannot be de-
tected by G-Lasso. They also detect one hotspot (6), which can
not be detected by SIOL. The number of hotspots that are sig-
niﬁcant enriched is listed in Table 4. From the table, we can see
that GGDL slightly outperforms GDL since it incorporates the
location of SNPs and gene-pathway information.

6 DISCUSSION

As a promising tool for dissecting the genetic basis of common
diseases, eQTL study has attracted increasing research interest.

.0

   

.0

ratio of correct interaction

100,000

200,000 300,000 400,000
K

Fig. 7. Ratio of correct interactions reﬁned when varying K. The initial
input networks only contain 25% correct interactions

Table 3. Summary of the top-15 hotspots detected by GGDL

The traditional eQTL methods focus on testing the associations
between individual SNPs and gene expression traits. A major
drawback of this approach is that it cannot model the joint
effect of a set of SNPs on a set of genes, which may correspond
to biological pathways.

Recent advancement in high-throughput biology has made a
variety of biological interaction networks available. Effectively
integrating such prior knowledge is essential for accurate and
robust eQTL mapping. However, the prior networks are often
noisy and incomplete. In this article, we propose novel graph-
regularized-regression models to take into account the prior net-
works of SNPs and genes simultaneously. Exploiting the duality
between the learned coefﬁcients and incomplete prior networks
enables more robust model. We also generalize our model to
integrate other types of information, such as SNP locations
and gene pathways. The experimental results on both simulated

Table 4. Hotspots detected by different methods

 

GGDL GDL G-Lasso SIOL LORS

 

Number of hotspots 15 l4 13 10 9
signiﬁcantly enriched
(top 15 hotposts)

Number of total 65 82 96 89 64
reported hotspots (size> 10)

Number of hotspots 45 56 61 53 41
signiﬁcantly enriched

Ratio of signiﬁcantly 70 68 64 60 56

enriched hotspots (%)

 

 

 

ID Sizea Locib GO° Hitsd GDL (an)? GDL (hits)f G—Lasso(all)g G—Lasso(hits)h SIOL(a11)i SIOL(hitsy' LORS(a11)k LORS(hits)l
1 31 XII:1056097 (1)*** 7 31 7 32 7 8 6 31 7
2 28 1118183292391 (2)** 5 29 5 28 5 58 5 22 4
3 28 XII:1056103 (l)*** 7 29 6 28 6 1 1 2 O
4 27 111:79091 (2)*** 6 29 6 28 6 28 7 10 2
5 27 III:175799..177850 (3)* 3 26 3 23 3 9 2 18 4
6 27 XII-10599251059930 (I)*** 7 27 7 27 7 0 0 5 1
7 25 111:105042 (2)*** 6 23 6 25 6 5 3 19 4
8 23 III:201166..201167 (3)*** 3 23 3 22 3 13 2 23 3
9 22 XII:1054278..1054302(1)*** 7 26 7 24 7 24 5 12 4
10 21 111100213 (2)** 5 23 5 23 5 5 3 5 1
11 20 111209932 (3)* 3 21 3 19 3 16 4 15 4
12 20 XII:659357..662627 (4)* 4 19 4 3 0 37 9 36 6
13 19 III:210748..210748 (5)* 4 24 4 18 4 2 3 11 4
14 19 VIII:111679..111680 (6)* 3 20 3 19 3 3 3 12 2
15 19 VIII:111682..111690 (7)** 5 21 5 20 5 57 6 22 3
Total hits 75 74 70 59 49

 

aNumber of genes associated with the hotspot bThe chromosome position of the hotspot. CThe most signiﬁcant GO category enriched with the associated gene set. The
enrichment test was performed using DAVID (Huang et al., 2009). The gene function is deﬁned by GO category. The involved GO categories are: (i) telomere maintenance via
recombination; (ii) branched chain family amino acid biosynthetic process; (iii). regulation of mating-type speciﬁc transcription, DNA-dependent; (iv) sterol biosynthetic
process; (v) pheromone-dependent signal transduction involved in conjugation with cellular fusion; (vi) cytogamy; (vii) response to pheromone. dNumber of genes that have
enriched GO categories. e’g’l’kNumber of associated genes that can also be identiﬁed using GDL, G—Lasso, SIOL and LORS, respectively. f’h’j’lNumber of genes that have
enriched GO categories and can also be identified by GDL, G-Lasso, SIOL and LORS, respectively. Among these hotspots, hotspot (12) in bold cannot be detected by G-
Lasso. Hotspot (6) in italic cannot be detected by SIOL. Hotspot (3) in teletype cannot be detected by LORS. Adjusted P-values using permutation tests. *10_2~10_3,

**10—3~10-5, ***10-5~10-1°.

 

i147

112 /810's112u1noprOIXO's311eu1101u101q//:d11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

W.Cheng et al.

 

and real eQTL datasets demonstrate that our models outperform
alternative methods. In particular, the proposed dual reﬁnement
regularization can signiﬁcantly improve the performance of
eQTL mapping.

Funding: National Institutes of Health (grants R01HG006703
and P50 GM076468-08); NSF IIS-l313606; NSF IIS-1162374
and IIS-l218036.

Conﬂict of Interest: none declared.

REFERENCES

Biganzoli,E.M. et al. (2006) Artiﬁcial neural network for the joint modelling of
discrete cause-speciﬁc hazards. Artif Intell. Med, 37, 119—130.

Bochner,B.R. (2003) New technologies to assess genotype henotype relationships.
Nat. Rev. Genet., 4, 309—314.

Boyd,S. and Vandenberghe,L. (2004) Convex Optimization. Cambridge University
Press, Cambridge.

Brem,R.B. et al. (2005) Genetic interactions between polymorphisms that affect
gene expression in yeast. Nature, 436, 701—703.

Brem,Y.G. et al. (2003) Trans-acting regulatory variation in Saccharomyces cerevi—
siae and the role of transcription factors. Nat. Genet., 35, 57—64.

Charles Boone,H.B. and Andrews,B.J. (2007) Exploring genetic interactions and
networks with yeast. Nat. Rev. Genet., 8, 437C449.

Chen,X. et al. (2012) A two-graph guided multi-task lasso approach for eqtl map-
ping. In AIST AT S, pp. 208—217. La Palma, Canary Islands.

Chung,F.R.K. (1997) Spectral graph theory (reprinted with corrections). In: CBMS:
Conference Board of the Mathematical Sciences, Regional Conference Series. Vol.
92, Published for the Conference Board of the Mathematical Sciences,
Washington, DC.

Ding,C. et al. (2006) Orthogonal nonnegative matrix t-factorizations for clustering.
In KDD, ACM, New York, pp. 126—135.

Ding,C.H.Q. et al. (2010) Convex and semi-nonnegative matrix factorizations.
IEEE Trans. Pattern Anal. Mach. Intell., 32, 45—55.

Huang,D.A.W. et al. (2009) Systematic and integrative analysis of large gene lists
using DAVID bioinformatics resources. Nat. Protoc, 4, 44—57.

J enatton,R. et al. (2011) Structured variable selection with sparsity-inducing norms.
JMLR, 12, 2777—2824.

Kim,S. and Xing,E.P. (2009) Statistical estimation of correlated genome associ-
ations to a quantitative trait network. PLoS Genet., 5, 61000587.

Kim,S. and Xing,E.P. (2012) Tree-guided group lasso for multi-response regression
with structured sparsity, with applications to eQTL mapping. Ann. Appl. Stat,
6, 1095—1117.

Lander,E.S. (2011) Initial impact Of the sequencing of the human genome. Nature,
470, 187—197.

Lee,D.D. and Seung,H.S. (2000) Algorithms for non-negative matrix factorization.
NIPS, 13, 556—562.

Lee,S. and Xing,E.P. (2012) Leveraging input and output structures for joint map-
ping Of epistatic and marginal eQTLs. Bioinformatics, 28, il37—i146.

Lee,S. et al. (2010) Adaptive multi-task lasso: with application to eQTL detection.
NIPS, pp. 1306—1314, Vancouver, British Columbia, Canada.

Li,C. and Li,H. (2008) Network-constrained regularization and variable selection
for analysis of genomic data. Bioinformatics, 24, 1175—1182.

Listgarten,J. et al. (2010) Correction for hidden confounders in the genetic analysis
of gene expression. Proc. Natl Acad. Sci. USA., 107, 16465—16470.

Mazumder,R. et al. (2010) Spectral regularization algorithms for learning large
incomplete matrices. JMLR, 11, 2287—2322.

Michaelson,J. et al. (2009) Detection and interpretation of expression quantitative
trait loci (eQTL). Methods, 48, 265—276.

Musani,S.K. et al. (2007) Detection of gene x gene interactions in genome-wide
association studies of human population data. Hum. Hered, 63, 67—84.

Obozinski,G. and Taskar,B. (2006) Multi-task feature selection. Technical report
709. Statistics Department, University Of California, Berkeley.

Pujana,M.A. et al. (2007) Network modeling links breast cancer susceptibility and
centrosome dysfunction. Nat. Genet., 39, 1338—1349.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. Royal.
Statist. Soc. B, 58, 267—288.

von Mering,C. et al. (2002) Comparative assessment of large-scale data sets of
protein-protein interactions. Nature, 417, 399—403.

Yang,C. et al. (2013) Accounting for non-genetic factors by low-rank representation
and sparse regression for eQTL mapping. Bioinformatics, 29, 1026—1034.

 

i148

112 /810's112u1noprOIXO's311eu1101u101q//:d11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

