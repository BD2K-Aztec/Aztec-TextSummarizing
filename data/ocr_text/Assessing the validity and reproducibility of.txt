ORIGINAL PAPER

Vol. 29 no. 22 2013, pages 2844—2851
doi:10. 1093/bioinformatics/btt508

 

Genome analysis

Advance Access publication September 17, 2013

Assessing the validity and reproducibility of

genome-scale predictions

Lauren A. Sugden‘, Michael R. Tackettz, Yiannis A. Sawas, William A. Thompson1 and

Charles E. Lawrence1 ’*

1Center for Computational Molecular Biology and the Division of Applied Mathematics, Brown University, Providence,
RI 02912, USA, 2St. Laurent Institute, 317 New Boston St, Woburn, MA 01801, USA and 3Department of Molecular
Biology, Cell Biology and Biochemistry, Brown University, Providence, RI 02912, USA

Associate Editor: John Hancock

 

ABSTRACT

Motivation: Validation and reproducibility of results is a central and
pressing issue in genomics. Several recent embarrassing incidents
involving the irreproducibility of high-profile studies have illustrated
the importance of this issue and the need for rigorous methods for
the assessment of reproducibility.

Results: Here, we describe an existing statistical model that is very
well suited to this problem. We explain its utility for assessing the
reproducibility of validation experiments, and apply it to a genome-
scale study of adenosine deaminase acting on RNA (ADAR)-mediated
RNA editing in Drosophila. We also introduce a statistical method for
planning validation experiments that will obtain the tightest reproduci-
bility confidence limits, which, for a fixed total number of experiments,
returns the optimal number of replicates for the study.

Availability: Downloadable software and a web service for both the
analysis of data from a reproducibility study and for the optimal design
of these studies is provided at http://ccmbweb.ccv.brown.edu/repro
ducibility.html

Contact: Charles_Lawrence@Brown.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on June 5, 2013; revised on August 26, 2013; accepted on
August 28, 2013

1 INTRODUCTION

The issue of validation and reproducibility of scientiﬁc results
has recently been the subject of intense discussion in the scientiﬁc
community. Several eye-opening reports have either claimed
insufﬁcient validation of bold research ﬁndings or shown an
inability to replicate such results in genomics (DeVeale et al.,
2012; Gregg et al., 2010; Kleinman and Majewski, 2012; Lin
et al., 2012; Li et al., 2011b; Pickrell et al., 2012), genetics
(Hunt et al., 2012; Surolia et al., 2010), oncology (Begley and
Ellis, 2012), neuroscience (Button et al., 2013), pharmacology
(Prinz et al., 2011), proteomics (Bell et al., 2009) and psychology
(Shanks et al., 2013; Yong, 2012). Problems with reproducibility
have been demonstrated with widely-used technologies such as
microarrays (Ioannidis et al., 2009), siRNA-based screens

 

*To whom correspondence should be addressed.

(Barrows et al., 2010) and mass spectrometry (Bell et al., 2009).
This has led to appeals for increased statistical rigor (Macleod,
2011; Vaux, 2012), platforms for the publication of neutral stu-
dies (Macleod, 2011) and attempted replicates, whether success-
ful or not (Editorial, 2012a), and a system-wide committed effort
toward generating work that is reproducible (MacArthur, 2012),
placing at least as much emphasis on reproducibility as is cur-
rently placed on novelty (Editorial, 2012b; Russell, 2013).

Recent attempts at addressing the issue of reproducibility in-
clude the Reproducibility Initiative, which, for a fee, will carry
out independent validation of research ﬁndings and issue a ‘cer-
tiﬁcate of reproducibility’ for those studies that validate (https://
www.scienceexchangecom/reproducibility), and ScienceCheck,
which provides a platform for researchers to report on the
‘reproducibility and utility of the literature method(s) that they
have worked with’ (http://www.sciencecheck.org). Although
these are extremely important contributions, neither organiza-
tion provides a quantitative measure of reproducibility.

In light of this, there is an urgent need for statistical tools for
quantitatively evaluating reproducibility. To help address this
need, we introduce the application of a well-suited Bayesian hier-
archical model for assessing the reproducibility of validation
experiments in the context of evaluating top-tier predictions of
high-throughput genomic studies. We focus on studies in which a
large number of predictions are made concerning a biological
phenomenon of interest. There are many studies of this type in
the recent literature, in Drosophila alone; Hoskins et al. (2011)
predict 2000 new gene promoters, Li et al. (2008) identify thou-
sands of targets of six transcription factors involved in regulation
of the anterior—posterior axis in the embryo, Negre et al. (2010)
ﬁnd >14 000 binding sites of six proteins associated with insula-
tors, DNA sequences that block the spread of regions of mod-
iﬁed chromatin and interaction between other regulatory
elements, and Zeitlinger et al. (2007) ﬁnd evidence for 1600
genes whose transcription start sites are sites of polymerase II
stalling. Because validation of all predictions is typically infeas-
ible, often a few compelling and biologically interesting cases are
selected for further study (Hughes, 2009), leaving a long list of
unvalidated predictions. The reader is left unsure about both the
fraction of the list that is valid and the effect of biological and

sample preparation variation.
Our model takes advantage of multiple biological and tech-
nical replicates, in each of which validation of a random sample

 

2844 © The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

112 /810's112umo[pJOJXO'sopquJOJutoiq/ﬁd11q IIIOJJ papeolumoq

910K ‘09 lsnﬁnV no :2

Reproducibility of genome scale predictions

 

of the top-tier list is carried out. From these data, we can assess
the reproducibility of the validation studies and predict what
another investigator could reasonably expect to see in a follow-
up study.

The use of replicates, whether technical, biological or simu-
lated, has been shown to be useful in many contexts. McShane
et al. (2002) and Kerr and Churchill (2001) simulate microarray
replicates to determine the stability of clusters of genes that
exhibit similar expression patterns. In the search for differentially
expressed genes, technical replicates provide additional power for
microarrays (Pan et al., 2002), and biological replicates reduce
false positives in conclusions drawn from serial analysis of gene
expression data (Baggerly et al., 2003; Véncio et al., 2004) and
improve accuracy in calls made from RNAseq data (Glaus et al.,
2012). Xia et al. (2011) use replicate time series datasets to
capture time-delayed associations between microbes. In a
larger-scale take on the replicates, meta-analyses of genome-
wide association studies like those described in Zeggini and
Ioannidis (2009) combine datasets from multiple laboratories
to gain enough power to detect associations between particular
genes and diseases such as type II diabetes (Zeggini et al., 2008)
and Crohn’s disease (Barrett et al., 2008).

In some cases, replicates have been incorporated into optimal
study design. Many articles have been written on the number of
replicates required to detect a certain fold-change in gene expres-
sion Via microarray studies (Black and Doerge, 2002; Pan et al.,
2002; Tibshirani, 2005; Wei et al., 2004). For genome-wide asso-
ciation studies, Moonesinghe et al. (2008) ﬁnd the required
number of samples to replicate an association across studies
with a certain level of between-study heterogeneity, and
Pahl et al. (2009) propose multistage designs which, for a given
budget, maximize the power to ﬁnd associations. Auer and
Doerge (2010) advocate careful design of RNA-seq experiments,
including sampling, randomization, replication and blocking.
To our knowledge, no one in the biology community has used
biological or technical replicates to assess the reproducibility of
validation studies like those discussed here, or has proposed a
method for optimal design of such experiments with respect to
reproducibility.

There has been considerable work on assessing the reproduci-
bility of high-throughput experiments, especially in the context of
ranked lists of putative sites (Boulesteix and Slawski, 2009). In
this context, reproducibility is most closely related to precision
(or stability), in that the relevant issue is the similarity of two
ranked lists generated from biological replicates (or different
high-throughput platforms, different ranking algorithms,
etc. . .). There are many different measures used to assess the
similarity of two or more ranked lists, from Spearman’s rank
correlation (Kuo et al., 2006; MAQC Consortium, 2006) to over-
lap counts for the top k sites (Zhang et al., 2009) to weighted
overlap counts that emphasize correlation between high ranking
sites over that of low ranking sites Wang et al., 2006). Li et al.
(2011a) improve on these measures with a mixture model con-
sisting of reproducible and irreproducible sites, which assigns
each signal a reproducibility index based on its consistency
across replicates, which approximates its probability of being
reproducible. They deﬁne the ‘irreproducible discovery rate’
(IDR), an analog of the false discovery rate for multiple hypoth-
esis testing (Storey, 2002), which determines the ‘expected rate of

irreproducible discoveries’ for sites whose probability of being
irreproducible is below some threshold 3/. Their methods provide
a principled method for selecting sites for further study and for
evaluating ranking algorithms. Although here we also address
the issue of reproducibility, our focus is different. We are not
concerned with the precision of high-throughput technologies or
ranking algorithms, but rather with the reproducibility of inde-
pendent validation experiments that seek to verify ﬁndings of
such high-throughput experiments. The validation experiments
taken individually give us information about the accuracy of the
ﬁndings, whereas our model of biological replicates assesses the
reproducibility of the given validation scheme in the face of
biological and sample preparation variation.

Because the model we describe depends on validation of
random samples, here we ﬁrst review how a single simple
random sample drawn from the top-tier list can be used to esti-
mate the valid fraction of top-tier predictions. Because this
method does not account for biological and sample preparation
variability, it is not sufﬁcient to assess reproducibility, as factors
as seemingly benign as laboratory conditions, reagent lots, cell
generations and individual experimenter techniques have been
shown to affect results of biological experiments (Barrows
et al., 2010; Leek et al., 2010; Van Hijum et al., 2005). So moti-
vated, we describe how our hierarchical model uses data from
multiple replicates to compute a probability distribution of val-
idation results for an as-yet-unseen replicate. Hierarchical
models, described in many statistical textbooks including
Gelman et al. (2003), have many uses in computational genomics
(Ji and Liu, 2010), and are well suited to the task of assessing
reproducibility, as they provide a way to simultaneously model
similarities and differences between groups.

2 METHODS
2.1 Estimating validity

In a genome study with thousands of predictions, validation of select
predictions is an important step toward lending credibility to those
particular ﬁndings, but provides little, if any, support for the validity of
the other predictions of the study. Thus, follow-up studies must be carried
out without any conﬁdence in the validity of the ﬁndings they are pursu-
ing. If we ignore biological and sample preparation variability, a simple
yet rigorous way to address this is for the original investigators to draw a
random sample of their predictions to validate. The number valid can be
modeled by a binomial distribution, so the investigator can estimate the
fraction valid in the full top-tier list, complete with conﬁdence limits
to assess uncertainty (equations in Supplementary Table S1). These
conﬁdence limits can then be translated into lower conﬁdence bounds,
to give an idea of the worst-case scenario. For example, in a study with a
top-tier list of 1000 predictions, if the lower bound of the 90% conﬁdence
interval is 0.8, we are assured that at least 800 of the predictions are
valid with 95% conﬁdence. Our model builds on this idea by address-
ing the effects of biological and sample preparation variation on
reproducibility.

2.2 Hierarchical model and predictive distribution

We consider a result reproducible if it can be obtained in an independent
analysis, following the exact protocol provided by the original investiga-
tors, under the same experimental conditions. Because the conﬁdence
intervals described above are based on a single replicate and do not
take into account the effects of biological and sample preparation

 

2845

112 /810's112umo[pJOJXO'sopquJOJutoiq/ﬁd11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

L.A.Sugden et al.

 

variability, they do not provide a basis for assessing the reproducibility of
the validation study. In particular, such intervals will be deceptively
narrow for predicting what will happen in a new replicate. With this in
mind, we must allow the proportions found to be valid in each replicate
to vary from that in the original validation assay.

The hierarchical model provides a balance between treating each rep-
licate independently, leading to noisy estimates of the proportions valid in
each pool, and combining data from all replicates, ignoring inter-pool
variation. As shown in Figure 1, it achieves this balance by modeling the
similarities and differences in the proportions valid in each replicate with
a probabilistic function, f ( plat, ,8). Here, the function is a beta distribu-
tion, the natural conjugate distribution to the binomial. The proportions
p,- valid in each pool are assumed to be independent samples from this
distribution. Because predictions to be tested in each pool are drawn at
random, 1), is the probability that a randomly drawn prediction will be
valid in the ith pool; thus, as in the single replicate case above, we model
the number k, found valid in the ith pool with a binomial distribution,
f(k|p, N) in Figure 1. The distribution of parameters or and ,8 at the top of
the hierarchy is unknown, and thus must be inferred from the data
coming from all samples in all pools at the bottom of the hierarchy. If
the proportions in the pools are quite similar, the resulting distribution
will have a tight variance, reﬂecting the fact that the inter-pool variation
is small. On the other hand, if the proportions in the pools differ sub-
stantially, the variance of this distribution will be large. This model allows
us to make predictions about the results of a new replicate, as indicated
by pnew and knew in Figure 1, and allows for tighter conﬁdence limits in
each replicate, as illustrated in Supplementary Figure S1.

2.2.] Structure of the model Given NT total predictions from a
genome-wide study, we consider m biological and technical replicate
pools in which N,- < <NT randomly selected predictions are tested
(i = 1, ...,m). In a given replicate pool i, let k, be the number of the
Ni predictions that successfully validate. The number of predictions k,-
that validate in pool i is modeled as a draw from a binomial distribution
with parameters 1),- and Ni:

P(ki|piaNi) = )1)?“ _I7i)Ni—ki (1)
Raﬁ)

(xiii
11le ’N

9 p3 pnew
f(klp,N> 1
k1

k, k

Fig. 1. Diagram of the hierarchical model. k1, kg and k3 represent the
counts of successful validations in three experiments, out of N1, N2 and
N3 total validation experiments, respectively. Each k is a draw from a
binomial distribution with corresponding parameters 1) and N. Each 1), in
turn, is a draw from a beta distribution with parameters a and ,8. The
available data inform the common distribution of a and ,8, from which we
can simulate the results of a new experiment

—L

N? 4—;0

new

where p,- for each replicate is modeled as a draw from a common hier-
archical beta distribution with parameters a and ,8:

I1051—5) 05—1 _ _,3—1
F(Comp,- (1 p.) (2)

Since in general we expect that the number of replicates will be too small
to result in reliable point estimates for or and ,8, we use Bayesian inference
at the top level of the model to compute a probability distribution for
these parameters, ﬂat, ,8) in Figure 1. We use a hyperprior suggested by
Gelman et al. (2003) on or and ,8, a uniform distribution on transformed
axes (L 1  This distribution is ‘uninformative,’ or ‘diffuse’ in the

1x+,3 ’ m
sense that it does not place any large probability mass in any one place.
Because a and ,8 govern the position and shape of the beta distribution,
we use this uninformative prior to refrain from imposing prior assump-
tions about the amount of variation that actually exists among replicates.

f (pl-la, i3) =

2.2.2 Posterior distribution of hyperparameters There is no closed-
form expression for inference of the posterior distribution of or and ,8, so
we use the grid-based approach outlined in Gelman et al. (2003). The
un-normalized posterior distribution of or and ,8 given data
k,, i = 1, . . . ,m is given by the following expression:

f(a9ﬁ|kla - ° - 9km) (X
m m“ + :3) m“ + ki)F(.3 + Ni — ki) (3)
ﬂa’ ﬂ)  F(a)F(i3) F(a + 13 + N,-)

 

where ﬂat, ,8) represents the prior distribution. Details on computing and
drawing from this distribution can be found in Supplementary Methods.

2.2.3 The predictive distribution To ask what the results of a valid-
ation study in a new replicate might look like, we draw 1000 samples from
the posterior distribution on or and ,8, as described in Supplementary
Methods. For each pair (05,-, 13,-), j = 1, .. . , 1000, we draw pj from a
beta distribution with parameters 05,- and ,Bj. Given a sample size N, we
can then draw kj from a binomial distribution with parameters pj and N.
From these samples, we can approximate the predictive distribution and
compute informative statistics such as mean, variance and percentiles.

2.3 Optimal study design

The model described above can also be used to design a validation study
in a way that yields the most favorable (tightest) predictive distribution.
There are four necessary input parameters: the total number of predic-
tions to be validated (N), expected mean and standard deviation of the
distribution of validation rates across replicate pools (p E and 0E) and the
fraction of validation experiments expected to yield neither a positive nor
a negative result (qE). This last parameter acknowledges the fact that for
many protocols involving techniques such as polymerase chain reaction
(PCR) or other common tools, only a fraction of experiments will work
(e.g. primers may fail to bind). Given these four parameters, we can
determine the most effective way to distribute experiments across repli-
cates, under the assumption that the replicate pools all have approxi-
mately equal sample sizes. This is done via simulation as follows:

(1) Split N into a representative subset of the possible numbers of

replicates (Nrep) with Nexp = Ni experiments per replicate (see

Supplementary Table S2 for detrapils).
(2) For each Nrep (and corresponding Nexp):

(a) Generate samples from the hierarchical model:

(i) Generate Nrep values of p, from a beta distribution with
mean pE and standard deviation 0E

(ii) For i = 1, .. . , Nrep, simulate experiment failure by drawing
New from independent binomial distributions with param-
eters (N exp: QE)

 

2846

112 [glO'SIBILInO[plOJXO'SODBIIIJOJHIOIQ/[ldllq 111011 pep1201umoq

910K ‘09 lsnﬁnV no 2:

Reproducibility of genome scale predictions

 

(iii) For i: 1, ...,Nrep, draw the number of positive valid-
ations k,- from a binomial distribution with parameters
New- and p,-

(b) From generated samples k,- and New- for all i, infer the
posterior distribution of a and ,8, and use this distribution to
infer the predictive distribution of a new replicate as described
above, taking note of the standard deviation and 10th
percentile.

(0) Repeat from (a) 3000 times, averaging the results for standard
deviation and 10th percentile

(3) Select the value of Nrep with the most favorable characteristics (low
standard deviation, high 10th percentile). Often there is a range of
options that give similar results.

3 RESULTS
3.1 Application to adenosine deaminase acting on
RNA study

3.1.] Target prediction and validation In a related manuscript
(St. Laurent et al., 2013), we produced a number of top-tier-
predicted adenosine deaminase acting on RNA (ADAR) targets
in Drosophila. ADAR enzymes target double-stranded RNAs
(Nishikura et al., 1991), catalyzing the conversion of adenosine
(A) to inosine (I) through hydrolytic deamination (Bass and
Weintraub, 1988). This post-transcriptional mechanism, also
known as RNA editing, has the capacity to diversify genomes
via amino acid recoding in functionally important protein resi-
dues (Nishikura, 2010). Drosophila has a single ADAR protein
called dADAR, which has been shown to target protein-coding
genes involved in vesicular trafﬁcking, ion homeostasis, signal
transduction, ion channels and the cytoskeleton
(Hoopengardner et al., 2003; Stapleton et al., 2006), and is cru-
cial for normal adult nervous system function (Palladino et al.,
2000). Variability in RNA editing between ﬂies has even been
suggested as a mechanism for individual differences in behavior
and neuronal physiology (Jepson et al., 2012).

In the related study, we used a combination of single-molecule
sequencing, previously validated sites and machine learning
methods to predict 1782 top-tier sites of dADAR—mediated
RNA editing. As RNA editing converts an A to an I in the

RNA sequence, and sequencing machinery reads an I as a
G, validation consisted of Sanger sequencing to identify sites in
RNA that express a G (or a mixture of A and G) where the
genome contains an A. The ﬁrst validation experiment was car-
ried out in a single pool on 298 predicted sites. For ~30% of the
sites, the reads were of too poor quality to assess the absence or
presence of RNA editing, leaving 205 sites, of which 151 (74%)
successfully validated.

To obtain data on inter-pool variation from the combined
effect of biological variation and sample preparation technique,
we conducted independent Sanger validations in four additional
RNA pools created by a skilled investigator well versed in the
protocol, and in a ﬁfth pool created by an investigator with much
less experience, which we discarded after ﬁnding it to be of
much lower quality than the others (details in Supplementary
Table S3). All pools in this study were isolated from wild-type
Drosophila (Canton-S) raised at a constant 25°C on standard
molasses food and under 12-h day/night cycles. RNA was
extracted from adult, whole body, 1—2—day-old males (10 per
sample), using TRIzol reagent (Invitrogen). For validation,
cDNAs were ampliﬁed Via reverse transcriptase—polymerase
chain reaction using gene-speciﬁc primers. The data from the
ﬁnal ﬁve replicates are shown in Table 1. The ﬁrst pool was
larger than the others because the authors of the related manu-
script wanted to investigate sequence subcategories: exons,
introns and intergenic regions.

3.1.2 Predictive distribution and interpretation We used the
Bayesian procedure described in Section 2 to carry out predictive
inference based on the ﬁve replicates from the ADAR study. The
contour plot in Supplementary Figure S2 illustrates the variation
in the parameters of the beta distribution, revealing at least mod-
erate uncertainty in our knowledge of their values based on the
data, which supports our decision to avoid using point estimates.
The predictive distribution, illustrated in Figure 2, has a mean of
67%, meaning that in a new validation study following the same
protocol under the same experimental conditions, 67% of sites
would successfully validate on average. The 80% credibility limit
of the distribution has a lower bound of 55%, indicating that
90% of the time, at least 55% of sites will successfully validate.
It is possible that the requirement that further studies follow the
same protocol under the exact experimental conditions may be

Table 1. Data from ADAR replicates and cross-validation conﬁdence/credibility intervals

 

RNA pool Valid/Total Cross-validation 80% CI

Cross-validation 90% CI

Cross-validation 95% CI

 

Predictive distribution Pooled

 

Predictive distribution Pooled

Predictive distribution Pooled

 

1 151/205 (74%) (0.47, 0.77) (0.60, 0.71)
2 17/32 (53%) (0.60, 0.80) (0.69, 0.76)
3 21/30 (70%) (0.46, 0.82) (0.67, 0.74)
4 24/32 (75%) (0.47, 0.82) (0.67, 0.73)
5 18/29 (62%) (0.50, 0.83) (0.68, 0.75)

(0.36, 0.83) (0.58, 0.72) (0.23, 0.88) (0.57, 0.73)
(0.52, 0.84) (0.68, 0.77) (0.42, 0.89) (0.67, 0.77)
(0.38, 0.87) (0.66, 0.75) (0.29, 0.92) (0.65, 0.76)
(0.39, 0.87) (0.66, 0.74) (0.26, 0.92) (0.65, 0.75)
(0.40, 0.87) (0.67, 0.76) (0.33, 0.91) (0.66, 0.76)

 

Note: Column 2 shows the results from the ﬁve replicate datasets from the ADAR study. Columns 3—8 show the credibility/conﬁdence intervals generated in the cross-
validation studies described in Section 3.3.1, where the interval is generated either from the model predictive distribution, or the standard frequentist normal approximation on
the pooled data, where the left-out pool is the row pool (column 1).

 

2847

112 [3.10811211an[p.IOJXO'SODBIIIJOJIIIOIQ/[Z(11111 111011 pep1201umoq

910K ‘09 lsnﬁnV no 2:

L.A.Sugden et al.

 

difﬁcult to satisfy, in which case our limits may be too narrow.
Nevertheless, this is valuable information for an investigator
seeking to build on this study.

3.2 Optimal study design

As was the case in our ADAR study, the time and expense
involved in validation studies often come at the level of the val-
idation experiments themselves, and comparatively little effort is
needed for the creation of multiple replicate pools. In such a
situation, researchers have a good deal of ﬂexibility in deciding
how to divide a ﬁxed number of experiments into a variable
number of replicates. For example, if they are willing at the
outset to do 96 experiments, they could run 48 experiments
each in two replicate pools, two experiments each in 48 replicate
pools or any combination in between. The shape and spread of
the resulting predictive distribution will depend on these choices:
the number of replicates and the associated number of experi-
ments per replicate.

We used the experimental design procedure described in
Section 2 to compute the optimal number of replicates for studies
with a wide range of input parameters (total number of experi-
ments, expected mean and standard deviation and fraction of
experiments expected to fail from the start), for use by investi-
gators planning validation studies. An example of results for one
set of input variables is illustrated in Figure 3, which shows the
average standard deviation and 10th percentile of the predictive
distribution with increasing numbers of replicates. In most cases,
as in this one, uncertainty about reproducibility achieves a min-
imum value after an initial decrease. This change in uncertainty is
reﬂected in the 10th percentile, which at ﬁrst increases reaching a

0.06

 

- Sampled Proportions

Bayesian Confidence Limits:
95% _
90%
80%

 

0.05 -

 

 

 

Relative Frequency

 

 

 

 

 

 

 

 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Sampled Proportions

Fig. 2. The predictive distribution. The histogram represents the predict-
ive distribution of p for a new replicate following the same experimental
procedure that was performed in the ﬁve ADAR replicate pools. The
distribution has a mean of 0.67 with 95% Bayesian conﬁdence limits
running from 0.39 to 0.88, 90% conﬁdence limits running from 0.47 to
0.84 and 80% conﬁdence limits running from 0.54 to 0.79. We report
conﬁdence intervals to give a sense of best- and worst-case scenarios. In
particular, using the lower limits of the 80 and 90% conﬁdence intervals,
respectively, we see that only 10% of the probability mass lies <0.54 and
only 5% lies <0.47

maximum at nine replicates and then decreases. The initial
increase stems from the fact that for small numbers of replicates,
the increase in our knowledge about inter-pool variation out-
weighs the reduction of certainty about the proportion in each
replicate. This holds up to nine replicates in this case, at which
point intra-pool uncertainty stemming from the smaller sample
sizes in each replicate begins to outweigh the gain in certainty
about the inter-pool variation. The complementary trend is
embodied by the standard deviation curve. Because a predictive
distribution with narrow standard deviation and high 10th per-
centile is desirable, our algorithm would recommend around nine
replicates for this validation study.

In Figure 4, we illustrate the qualitative effect of the four par-
ameters in the system on reproducibility by displaying the 10th
percentile of the optimal predictive distribution for given 4-tuples
of parameters. This optimal 10th percentile rises with increasing
number of experiments, expected mean and fraction of successful
experiments, and falls with increasing standard deviation.
A more in-depth illustration of the effect of expected standard
deviation on the predictive distribution is given in Supplementary
Figure S3.

3.3 Cross-validation of ADAR replicates

Because it would require a very large number of replication
experiments to obtain a sufﬁciently large sample to have
enough data to persuasively validate the conﬁdence limits we
describe, we undertook a cross-validation study based on the
ﬁve available replicates. Leaving out each replicate in turn, we
computed the predictive distribution given the remaining four
replicates. For all ﬁve cross-validations, the proportion valid in
the left-out replicate fell within the 90% credibility interval of the

 

 

0.4 I I I I I I
9
'4:
c
<0
g 0.35
c
n- o
E "*3
$3 :g 0.3 -
E .5
_c_> 0 —Standard Deviation
13' <0
3 .2 0.25 - —10th Percentile
.H
CD .9
D 'o
e :5
g H_ 0.2 -
C O
(U
.H
U)

 

 

 

246151612 1'6 1'9 2'4 3'2 48
Number of Replicates

Fig. 3. Software-generated curves. Software inputs here are the total
number of experiments (96), the expected overall mean (0.6) and standard
deviation between replicates (0.09) and the percentage of experiments
expected to work (0.6). The red curve represents the standard deviation
of the predictive distribution for a new replicate, and the blue curve rep-
resents the 10th percentile of the same distribution, each computed for
different numbers of replicates to identify the optimal number of repli-
cates to use. Here, 9 replicates with 10—11 experiments each seem to give
the tightest predictive distribution with the highest 10th percentile

 

2848

112 /810's112umo[pJOJXO's01112u1101u101q/pd11q 111011 pep1201umoq

910K ‘09 lsnﬁnV no 2:

Reproducibility of genome scale predictions

 

 

M=0.7 M=0.7 M=0.7 0 75
N=192 SD=0.07 SD=0.07 '
PS=0.8 PS=0.8 N=192

0.65
/—-—"‘ o 55

K 0.45

- 0.35

\

 

 

 

 

 

 

 

 

 

 

I\ SI'

Ln. «2 N. 0°. 62 h in in 0° 0 N 0° «2 N. 0°. 62

o o o o o 8 8 0. $2 ". ‘1' a’ 2 a o o o o
o' o' ° o' °

Mean (M) Standard Deviation Total Number of Percent Successful

(SD) Experiments (N) Experiments (PS)

Fig. 4. Inﬂuence of parameters on optimal predictive distribution. Here
we see the effect of varying the four input parameters: expected mean,
expected standard deviation, expected fraction of successful experiments
and total number of experiments. For each curve, we held three param-
eters ﬁxed and varied the parameter of interest. For each 4-tuple, we
found the maximum 10th percentile over all predictive distributions for
all assignments of experiments to replicates. From the plot, we see that
the desirable higher 10th percentiles result from data with high mean, low
standard deviation, large numbers of experiments and high fraction of
successful experiments

corresponding predictive distribution, and for four of ﬁve, the
proportion valid fell within the 80% credibility interval. Thus,
these results are fully consistent with the model’s expectations.
However, because these results are based on a small cross-
Validation dataset, the support for our model from this analysis
is quite limited.

We carried out a similar cross-validation with our ﬁve pools,
not accounting for biological or sample preparation variability.
Leaving out each replicate in turn, we pooled the other four
replicates and computed standard frequentist conﬁdence limits
based on a normal approximation to the binomial distribution
(equations in Supplementary Table S1). In four of ﬁve trials, the
left-out proportion lay outside of both the 80% and 90% conﬁ-
dence intervals, and in three of ﬁve, the left-out proportion lay
outside of the 95% conﬁdence interval. Cross-validation inter-
vals for each method can be found in Table 1. These events,
although not statistically signiﬁcant at the standard 0.05 level,
are in stark contrast with the cross-validation results above,
revealing that the available data are less likely under the assump-
tion of no biological or sample preparation variation than under
our model including variation. This supports our contention that
such variation plays an important role in studies of editing
in Drosophila, and suggests that it may also be important in
other genome-scale studies. We found similar results in a cross-
Validation using Bayesian methods to compute credibility limits
for the pooled data (Supplementary Table S4), demonstrating
that our model performs better not because we use Bayesian
methods, but because we account for variation.

4 DISCUSSION

The need for reproducibility in scientiﬁc research has always
been central, but has only recently become a major focus of
the greater scientiﬁc community. Here, we present a procedure
that addresses these issues in the context of high-throughput
studies like that described in our companion article, where thou-
sands of predictions are made, and only a relatively small frac-
tion can be validated. We studied closely the example of ADAR

editing sites, but the method is generalizable, and could just as
easily apply to any of the high-throughput site prediction experi-
ments described earlier. Studies of this kind have become more
frequent with the advent of high-throughput technologies
like Illumina and large collaborations like ENCODE,
modENCODE and the 1000 Genomes Project. Most studies
already have their own schemes for validation, which they
carry out with varying levels of statistical rigor. The authors
and any investigators hoping to carry out follow-up studies
would all beneﬁt from a carefully designed validation study
using statistically random samples in biological replicates to
assess reproducibility.

As the accuracy of technology inevitably grows, it may be
tempting for investigators to assume a single replicate is sufﬁcient
to address reproducibility, implicitly assuming that technical
variation is the only variation that matters. However, even
with perfect technology, multiple biological replicates are still
necessary to assess the reproducibility of a set of results. It is
worth noting that each of our validation replicates was per-
formed in a pool of 10 ﬂies each. We expect that biological
replicates will be even more crucial in studies where replicates
consist of individual model organisms, such as mice or rats.

In our analysis of ADAR-mediated editing data, we found
that the 95% conﬁdence intervals for individual replicates
showed substantial variation. This was not unexpected, given
the documented variation in ADAR activity in individual ﬂies
(Jepson et al., 2012) and observations on the effects of experi-
mental conditions described earlier, and it underscored the need
for statistical tools that can address the effect of such variation
on the reproducibility of results. Our software predicted that a
new experiment using the same protocol under the same experi-
mental conditions would validate on average 67% of sites, and
that 90% of the time, the percentage validated would be at least
55 %. We emphasize the lower bound percentile, e.g. the 10th
percentile, because it represents a worst-case scenario for a
future experiment. Of course, because these results are affected
by our choice of the diffuse prior on a and ,6, the intervals we
generate may be too conservative in the case where more is
known about these parameters a priori.

There is a technical limitation to our model that arises from
the statistical formulation. The predictive distribution we
describe is well deﬁned everywhere except in the precise circum-
stance in which every replicate pool has a validation rate of either
100 or 0% (Gelman et al., 2003). We consider such extremes to
be very unlikely in most validation studies; however, this limita-
tion occasionally comes to bear in experimental design, where we
simulate thousands of distributions. This affects almost all simu-
lations with means >95% and most simulations with means
~90% and with wide variances. Otherwise, the effect is negli-
gible. Our software will not return results in the few non-
negligibly affected cases.

Our model makes two major assumptions: that the validation
experiments in each replicate follow a binomial distribution, and
that the proportions valid in each replicate follow a beta distri-
bution. Because we require that each replicate test a random
sample drawn from the whole population of predictions, the
results of each replicate follow a hypergeometric distribution,
which is very well approximated by a binomial distribution as
long as the number of predictions n tested in a single replicate is

 

2849

112 /810's112umo[pJOJXO's01112u1101u101q/pd11q 111011 pep1201umoq

910K ‘09 lsnﬁnV no 2:

L.A.Sugden et al.

 

much smaller than the total number of predictions N (a typical
rule of thumb is N Z 20n). The beta distribution was chosen for
the model primarily because as a conjugate prior to the binomial
distribution, it makes computation feasible. However, another
real advantage is that the beta distribution is extremely ﬂexible,
in that it can approximate most smooth unimodal distributions.
We illustrate this ﬂexibility in the context of our model in
Supplementary Figure S4. Together, this suggests that the
assumptions of our model will be appropriate for most
applications.

There are three places in our model where we rely on numer-
ical approximations: during predictive inference, we compute the
posterior distribution of the hyperparameters over a grid, as
there is no closed-form expression for computing it directly,
then we sample from the grid to approximate the predictive dis-
tribution; and in our optimal study design, we rely on a sampling
approximation to ﬁnd the average mean, standard deviation and
10th percentile of the population of predictive distributions
resulting from particular input parameters. For predictive infer-
ence, we follow the procedure outlined in Gelman et al., comput-
ing over a dense-enough grid that we believe captures the
important features of the posterior distribution, and sampling
a large number of points (1000) to approximate the predictive
distribution. We found that neither varying the grid density nor
increasing the number of samples noticeably changed the results
(data not shown). For study design, we sample a large number of
distributions (3000 is the default), and if the user downloads our
software, he or she can increase the number of samples if desired,
so as to obtain narrower error bars.

Finally, it should be noted that the results of any reproduci-
bility analysis can only be generalized to the population to which
the replicates belong (just as the results of any study should only
be generalized to the population from which the data are drawn).
We assume that follow-up validation studies follow the original
protocol under the exact experimental conditions as the original
experiments. To the extent that this is not possible, the credibility
intervals that we report may be too narrow to accurately reﬂect
the population of follow-up validation studies performed by
other investigators in other laboratories. Therefore, care must
be exercised in how claims of reproducibility are made, and
authors should be sure to specify the population to which their
results generalize. In some cases, large collaborations between
laboratories (such as those associated with modENCODE) will
be able to carry out replicates that represent a larger portion of
the possible variability, and will be able to make even stronger
claims about the reproducibility of their ﬁndings.

Validation and reproducibility are bedrock principles through-
out science that have until recently received limited attention. We
present this work as an aid in advancing these crucial principles
in the ﬁeld of genomics.

ACKNOWLEDGEMENTS

The authors thank their collaborators Robert Reenan, Georges
St. Laurent and Phipr Kapranov for valuable discussions
throughout the project, and the reviewers for their insightful
comments and suggestions.

Funding: This work was supported the National Institutes of
Health [NS074686] (in part) and by an Ellison Medical
Foundation Senior Scholar Award to RR.

Conﬂict of Interest: None declared.

REFERENCES

Auer,P.L. and Doerge,R.W. (2010) Statistical design and analysis of RNA sequen-
cing data. Genetics, 185, 405—416.

Baggerly,K.A. et al. (2003) Differential expression in SAGE: accounting for normal
between-library variation. Bioinformatics, 19, 1477—1483.

Barrett,J.C. et al. (2008) Genome-wide association deﬁnes more than 30 distinct
susceptibility loci for Crohn’s disease. Nat. Genet., 40, 955—962.

Barrows,N.J. et al. (2010) Factors affecting reproducibility between genome-scale
siRNA-based screens. J. Biomol. Screen, 15, 735—747.

Bass,B.L. and Weintraub,H. (1988) An unwinding activity that covalently modiﬁes
its double-stranded RNA substrate. Cell, 55, 1089—1098.

Begley,C.G. and Ellis,L.M. (2012) Drug development: raise standards for preclinical
cancer research. Nature, 483, 531—533.

Bell,A.W. et al. (2009) A HUPO test sample study reveals common problems in
mass spectrometry-based proteomics. Nat. Meth., 6, 423—430.

Black,M.A. and Doerge,R.W. (2002) Calculation of the minimum number of rep-
licate spots required for detection of signiﬁcant gene expression fold change in
microarray experiments. Bioinformatics, 18, 1609—1616.

Boulesteix,A.L. and Slawski,M. (2009) Stability and aggregation of ranked gene
lists. Brief. Bioinform, 10, 556—568.

Button,K.S. et al. (2013) Power failure: why small sample size undermines the
reliability of neuroscience. Nat. Rev. Neurosci, 14, 365—376.

DeVeale,B. et al. (2012) Critical evaluation of imprinted gene expression by RNA-
seq: a new perspective. PLoS Genet., 8, e1002600.

Editorial. (2012a) Further conﬁrmation needed. Nat. Biotechnol., 30, 806.

Editorial. (2012b) Error prone: biologists must realize the piffalls of work on mas-
sive amounts of data. Nature, 487, 406.

Gelman,A. et al. (2003) Hierarchical models. In: Bayesian Data Analysis. 2nd edn.
Chapman and Hall/CRC, pp. 120—160.

Glaus,P. et al. (2012) Identifying differentially expressed ranscripts from RNA-seq
data with biological variation. Bioinformatics, 28, 1721—1728.

Gregg,C. et al. (2010) High-resolution analysis of parent-of-origin allelic expression
in the mouse brain. Science, 329, 643—648.

Hoopengardner,B. et al. (2003) Nervous system targets of RNA editing identiﬁed by
comparative genomics. Science, 301, 832—836.

Hoskins,R.A. et al. (2011) Genome-wide analysis of promoter architecture in
Drosophila melanogaster. Genome Res., 21, 182—192.

Hughes,T.R. (2009) ‘Validation’ in genome-scale research. J. Biol., 8, 3—5.

Hunt,K.A. et al. (2012) Rare and functional SIAE variants are not associated with
autoimmune disease risk in up to 66,924 individuals of European ancestry.
Nat. Genet., 44, 3—5.

Ioannidis,J.P.A. et al. (2009) Repeatability of published microarray gene expression
analyses. Nat. Genet., 41, 149—155.

Jepson,J.E.C. et al. (2012) Visualizing adenosine-to-inosine RNA editing in the
drosophila nervous system. Nat. Meth., 9, 189—194.

Ji,H. and Liu,X.S. (2010) Analyzing omics data using hierarchical models. Nat.
Biotech., 28, 337—340.

Kerr,M.K. and Churchill,G.A. (2001) Bootstrapping cluster analysis: assessing the
reliability of conclusions from microarray experiments. PNAS, 98, 8961—8965.

Kleinman,C.L. and Majewski,J. (2012) Comment on Widespread RNA and DNA
Sequence Differences in the Human Transcriptome. Science, 335, 1302.

Kuo,W. et al. (2006) A sequence-oriented comparison of gene expression measure-
ments across different hybridization-based technologies. Nat. Biotechnol., 24,
832—840.

Leek,J.T. et al. (2010) Tackling the widespread and critical impact of batch effects in
high-throughput data. Nat. Rev. Genet., 11, 733—739.

Li,Q. et al. (2011a) Measuring reproducibility of high-throughput experiments. Ann.
App]. Stat., 5, 1752—1779.

Li,M. et al. (2011b) Widespread RNA and DNA sequence differences in the Human
Transcriptome. Science, 333, 53—58.

Li,X. et al. (2008) Transcription factors bind thousands of active and inactive
regions in the Drosophila blastoderm. PLoS Biol., 6, e27.

 

2850

112 /810's112umo[pJOJXO'sot112u1101utotq/pd11q 111011 pep1201umoq

910K ‘09 lsnﬁnV no 2:

Reproducibility of genome scale predictions

 

Lin,W. et al. (2012) Comment on widespread RNA and DNA sequence differences
in the human transcriptome. Science, 335, 1302.

MacArthur,D. (2012) Face up to false positives. Nature, 487, 427—428.

Macleod,M. (2011) Why animal research needs to improve. Nature, 477, 511—511.

MAQC Consortium. (2006) The microarray quality control (MAQC) project shows
inter- and intraplatforrn reproducibility of gene expression measurements. Nat.
Biotechnol., 24, 1151—1 161.

McShane,L.M. et al. (2002) Methods for assessing reproducibility of clustering
patterns observed in analysis of microarray data. Bioinformatics, 18, 1462—1469.

Moonesinghe,R. et al. (2008) Required sample size and nonreplicability thresholds
for heterogeneous genetic associations. PNAS, 105, 617—622.

Négre,N. et al. (2010) A comprehensive map of insulator elements for the
Drosophila genome. PLoS Genet., 6, e1000814.

Nishikura,K. et al. (1991) Substrate speciﬁcity of the dsRNA unwinding/modifying
activity. EMBO J., 10, 3523—3532.

Nishikura,K. (2010) Functions and regulation of RNA editing by ADAR deami-
nases. Annu. Rev. Biochem, 79, 321—349.

Pahl,R. et al. (2009) Optimal multistage designs — a general framework for efﬁcient
genome-wide association studies. Biostatistics, 10, 297—309.
Palladino,M.J. et al. (2000) A-to-I Pre-mRNA editing in Drosophila is primarily
involved in adult nervous system function and integrity. Cell, 102, 437—449.
Pan,W. et al. (2002) How many replicates of arrays are required to detect gene
expression changes in microarray experiments? A mixture model approach.
Genome Biol., 3 research 0022.

Pickrell,J.K. et al. (2012) Comment on widespread RNA and DNA sequence
differences in the human transcriptome. Science, 335, 1302.

Prinz,F. et al. (2011) Believe it or not: how much can we rely on published data on
potential drug targets? Nat. Rev. Drug Discov., 10, 712—712.

Russell,J .F. (2013) If a job is worth doing, it is worth doing twice. Nature, 496, 7—7.

Shanks,D.R. et al. (2013) Priming intelligent behavior: an elusive phenomenon.
PLoS One, 8, e56515.

St. Laurent,G. et al. (2013) Genome-wide analysis of A-to-I RNA editing via single
molecule sequencing in Drosophila. Nat. Struct. Mol. Biol, in press.

Stapleton,M. et al. (2006) RNA editing in Drosophila melanogaster: new targets and
functional consequences. RNA, 12, 1922—1932.

Storey,J.D. (2002) A direct approach to false discovery rates. J. R. Stat. Soc. Ser.
B Stat. Methodol., 64, 479—498.

Surolia,I. et al. (2010) Functionally defective germline variants of sialic acid acet-
ylesterase in autoimmunity. Nature, 466, 243—247.

Tibshirani,R. (2005) A simple method for assessing sample sizes in microarray
experiments. BM C Bioinformatics, 7, 106.

Van Hijum,S. et al. (2005) A generally applicable validation scheme for the assess-
ment of factors involved in reproducibility and quality of DNA-microarray
data. BMC Genomics, 6, 77.

Vaux,D.L. (2012) Know when your numbers are signiﬁcant. Nature, 492, 180—181.

Véncio,R.Z.N. et al. (2004) Bayesian model accounting for within-class biological
variability in Serial Analysis of Gene Expression (SAGE). BM C Bioinformatics,
5, 119.

Wei,C. et al. (2004) Sample size for detecting differentially expressed genes in micro-
array experiments. BM C Genomics, 5, 87.

Xia,L.C. et al. (2011) Extended local similarity analysis (eLSA) of microbial
community and other time series data with replicates. BMC Syst. Biol., 5,
S15.

Yang,X. et al. (2006) Similarities of ordered gene lists. J. Bioinform. Comput. Biol.,
4, 693—708.

Yong,E. (2012) Replication studies: Bad copy. Nature, 485, 298—300.

Zeggini,E. and Ioannidis,J.P.A. (2009) Meta-analysis in genome-wide association
studies. Pharmacogenomics, 10, 191—201.

Zeggini,E. et al. (2008) Meta-analysis of genome-wide association data and large-
scale replication identiﬁes additional susceptibility loci for type 2 diabetes. Nat.
Genet., 40, 638—645.

Zeitlinger,J. et al. (2007) RNA polymerase stalling at developmental control genes
in the Drosophila melanogaster embyo. Nat. Genet., 39, 1512—1516.

Zhang,M. et al. (2009) Evaluating reproducibility of differential expression discov-
eries in microarray studies by considering correlated molecular changes.
Bioinformatics, 25, 1662—1668.

 

2851

112 /810's112umo[pJOJXO'sot112u1101utotq/pd11q 111011 pep1201umoq

910K ‘09 lsnﬁnV no 2:

