ORIGINAL PAPER

Vol. 30 no. 11 2014, pages 1562—1568
doi:10. 1093/bioinformatics/btu040

 

Genetics and population analysis

Advance Access publication February 5, 2014

Learning phenotype densities conditional on many interacting

predictors

David C. Kessler1 ’*, Jack A. Taylor2 and David B. Dunson3

1Advanced Analytics Division, SAS Institute Inc., Cary, NC 27513, 2Molecular and Genetic Epidemiology Section,
Epidemiology Branch and Laboratory of Molecular Carcinogenesis, National Institute of Environmental Health Sciences,
Research Triangle Park, NC 27709 and 3Department of Statistical Science, Duke University, Durham, NC 27708

Associate Editor: Jeffrey Barrett

 

ABSTRACT

Motivation: Estimating a phenotype distribution conditional on a set of
discrete-valued predictors is a commonly encountered task. For ex-
ample, interest may be in how the density of a quantitative trait varies
with single nucleotide polymorphisms and patient characteristics. The
subset of important predictors is not usually known in advance. This
becomes more challenging with a high-dimensional predictor set
when there is the possibility of interaction.

Results: We demonstrate a novel non-parametric Bayes method
based on a tensor factorization of predictor-dependent weights for
Gaussian kernels. The method uses multistage predictor selection
for dimension reduction, providing succinct models for the phenotype
distribution. The resulting conditional density morphs flexibly with the
selected predictors. In a simulation study and an application to mo-
lecular epidemiology data, we demonstrate advantages over com-
monly used methods.

Availability and implementation: MATLAB code available at https://
googledrive.com/host/OBw6KIFB-k4lOOWQOdFJtSVZxNEO/ktdctf.
html

Contact: dave.kessler@gmail.com

Received on July 19, 2013; revised on December 29, 2013; accepted
on January 17, 2014

1 INTRODUCTION

Many areas of research are concerned with learning the distribu-
tion of a response conditional on numerous categorical (discrete)
predictors. The important predictors for characterization of this
distribution are not usually known in advance, and there may be
hundreds or thousands of candidates. Methods that attempt to
accommodate interactions among these predictors become mired
in the enormous model space. For example, in a moderate-
dimensional case involving p240 categorical predictors, each
with  = 4 possible realizations, considering all possible levels
of interaction leads to a space of 440 w 1024 possible models.
Parallelization and technical tricks may work for smaller ex-
amples, but data sparsity and the sheer volume of models force
us to consider different approaches. The conditional density may
vary in more than just location; Chung and Dunson (2009) illu-
strated this in an application to the conditional density of blood
glucose levels given insulin sensitivity and age. In the work that
follows, we present a novel non-parametric Bayes (NPB)

 

*To whom correspondence should be addressed.

approach to learning conditional densities that makes use of a
conditional tensor factorization to characterize the conditional
distribution given the predictor set, allowing for complex inter-
actions between the predictors. The particular form assumed for
the conditional density gives rise to an attractive predictor selec-
tion procedure, providing support for distinct predictor selection
steps. This addresses the challenges of high-dimensional data and
produces conditional density estimates that allow assessment of
tail risks and other complex quantities.

2 APPROACH

The primary goal of our work is to model the conditional density
f(y|x), where the form of this density for the response y changes
ﬂexibly with the predictor vector x. There is a large body of work
devoted to this idea of density regression in settings involving x
of dimension p 5 30, and such models have provided many op-
tions for that situation. We wish to develop techniques for prob-
lems involving much larger p, and ideally to scenarios where
p> 1000. We want to provide a method that performs variable
selection, assesses the probability of a predictors inclusion in the
model and provides easily interpretable estimates of the impact
of different predictors. This problem has been addressed with
variations on the finite mixture model,

K
f0) = 22mm; 9;.) (1)
[1:1

This is the basic form of the hierarchical mixture of experts
model [HME, Jordan and Jacobs (1994)]. In this representation,
K represents the number of contributing parametric kernels
IC(; 9],) distinguished by parameters 9],. The It], provides the
weights in this convex combination of kernels, where
2le 71’], = l and (m, . . . , JTK) e 8K_1, the K — l probability sim-
plex. The most straightforward forms rely on a pre-speciﬁed K
and include the predictors x in a linear model for the mean.
HME methods in the frequentist literature have often relied on
expectation maximization (EM) (Dempster et al., 1977) tech-
niques, which can suffer from overfitting (Bishop and Svensén,
2003). EM approaches in the Bayesian literature seek to avoid
this; Waterhouse et al. (1996) used EM to ﬁnd maximum a pos-
teriori estimates using the inherent Bayesian penalty against
complexity to regulate those estimates. In addition, the
Bayesian framework allows the quantiﬁcation of uncertainty
about the parameters in the model.

 

1562 © The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

112 ﬂJO'slcumo[pJOJXO'sopchogurotq/ﬁd11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

Learning phenotype densities with the CTF

 

NPB methods, such as the Dirichlet Process, prompted tech-
niques like that in Muller et al. (1996), which induced ﬂexible
conditional regression through joint modeling of the response
and predictors. Subsequent methods included the predictors in
71’], and/or 9;, via Dependent Dirichlet Process (DDP) mixtures.
De Iorio et al. (2004) proposed an ANOVA DDP model with
ﬁxed weights {717,} that used a small number of categorical pre-
dictors to index random distributions for the response. Grifﬁn
and Steel (2006) developed an ordered DDP, where the predictor
vectors were mapped to speciﬁc permutations of the weights
{7th}, yielding different density estimates for different predictor
vectors. Reich and Fuentes (2007) and Dunson and Park (2008)
used the kernel stick-breaking process to allow predictors to in-
ﬂuence the weights. Chung and Dunson (2009) presented a fur-
ther alternative in the probit stick-breaking process, which uses a
probit transform of a real-valued function of the predictors to
incorporate them into the weights. Methods that use joint mod-
eling of response and predictors (Shahbaba and Neal, 2009;
Hannah et al., 2011; Dunson and Xing, 2009) are popular and
can work well under many circumstances, but estimation of the
marginal distribution of the predictors is a burden. Methods not
relying on discrete mixtures also exist; Tokdar et al. (2010) de-
veloped a technique based on logistic Gaussian processes. Jara
and Hanson (2011) presented an approach using mixtures of
transformed Gaussian processes.

These and other methods of Bayesian density regression have
proven successful, but as datasets have grown in size and com-
plexity, these approaches encounter difﬁculties. This is even more
daunting when we consider interactions of discretely valued pre-
dictors because we must consider the factorial combinations of
those levels.

The associated challenges of variable selection and dimension-
ality reduction have been explored in Bayesian density regres-
sion. Dimensionality reduction has a goal similar to that of
variable selection, that of ﬁnding a minimal set of predictors
that account for variation in the response. The logistic
Gaussian process approach of Tokdar et al. (2010) includes a
subspace projection method to reduce the dimension of the pre-
dictor space. Reich et al. (2011) developed a technique for
Bayesian sufﬁcient dimensionality reduction based on a prior
for a central subspace. Although all of these approaches have
demonstrated their utility, they do not scale easily beyond p = 30
predictors.

There are also techniques like the random forest (Breiman,
2001) that aim to ﬁnd parsimonious models for density estima-
tion involving a large number of predictors. One disadvantage to
this type of ‘black box’ method is in interpreting the impact of
speciﬁc predictors on the response. Bayesian additive regression
trees (BART) (Chipman et al., 2006, 2010) focus on modeling the
conditional mean and assume a common residual distribution.
As previously noted, there are many questions that require learn-
ing about more than just the conditional mean of the response.
Another ﬂexible approach is the Bayes network (BN), which
considers the predictors and the response on equal footing to
develop a parsimonious network linking all variables (Pearl,
1988; Cowell et al., 1999; Lauritzen, 1992). The conditional dis-
tribution of the response given the predictors can be derived from
such a model, using developed BN techniques for mixed continu-
ous and discrete data (Lauritzen, 1992; Moral et al., 2001;

Langseth et al., 2012). A BN does estimate a joint density for
all of the predictors; the effort to estimate this very high-dimen-
sional nuisance parameter is unattractive, if the conditional dens-
ity is of primary interest.

We propose an approach based on a conditional tensor fac-
torization (CTF) for the mixing weights. As in the DDP and
certain of the kernel stick-breaking methods, the predictors in-
ﬂuence the mixing weights for this CTF model. The conditional
tensor factorization facilitates borrowing of information across
different proﬁles in a ﬂexible representation of the unknown
density. We focus our attention on situations involving continu-
ous responses and categorical predictors.

3 METHODS

We consider a univariate response y and a vector of p categorical pre-
dictors x = (x1, .. . , xp), where the jth predictor xj can take values
1, .. . ,  We would like a model that can ﬂexibly accommodate condi-
tional densities that change in complex ways with changes in the predictor
vector. In addition, we must consider situations where 1) >> n; there may
be no exemplars for certain predictor vectors. To address this, we propose
a Tucker-style factorization with the following general model for the
conditional density f(y|x):

k1

kp
fO’Ix) = Z ' ' ' Z 7Th1,...,hp(x)}~(y; 0h1,...,hp)

h1=l hp=l
p (I)
where 717,1,"th (x) = n ﬂhj (Xj). (2)
j=1
This form uses the maps n0), j = 1, . . . , p to associate the predictor vector

x with a separate weight for each combination of the latent identiﬁers
hl, ...,hp and thus with each of the k1 x  x kp kernels in the repre-
sentation. The xjih row of n03 is a vector of weights, one for each

hj = 1, ...,kj. These weights 7r(1D(xJ-), ...,7r,(€’3(xj) are all in [0,1] and
21,221 ngjkxj) = 1. The number of latent predictors p, is the same as

the number of observed predictors, but the form of the 7T2]? may mean

that different predictor vectors x result in the same sets of weights
m,,,,,1(x), ...,7rk1,m,kp (x). This provides the mechanism for dimension
reduction that we will develop. An important distinction from the
HME is in the treatment of the weights 717,1,  hp (x) as a tensor factoriza-
tion and the use of kernels A(y;0h1,m,hp), which do not depend on
the predictor vector x. This is similar in spirit to the classiﬁcation
approach proposed by Yang and Dunson, 2012, but we address
the problem of estimating an inﬁnite-dimensional conditional density
rather than the ﬁnite-dimensional problem of a categorical response dis-
tribution. In addition, we make distinct improvements in predictor selec-
tion to allow the approach to scale to larger numbers of candidate
predictors.

Tucker decompositions (Tucker, 1966) and other kinds of decompos-
itions have appeared in the machine learning literature before. Xu et al.
(2012) developed an ‘inﬁnite’ Tucker decomposition making use of latent
Gaussian processes rather than explicit treatment of tensors and matrices;
in comparison, the proposed method uses the Tucker decomposition to
characterize the mapping of predictors into weights. Other factorizations
have been used for similar problems; Hoff (2011) presented a reduced-
rank approach for table data, but this approach focused on the develop-
ment of estimates for the mean of a continuous response. Chu and
Ghahramani (2009) derive an approach for partially observed multi-
way data based on a Tucker decomposition; their objective is to learn

 

1563

112 /810's112umo[pJOJXO'sot1chOJurotw/2d11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

D. C.Kessler et al.

 

about the latent factors driving observations rather than the character-
ization of the response distribution or variable selection.

The collection across j = 1, ...,p forms a ‘soft’ clustering from the
d1 x - - - x 61,, possible realizations of the x vector to each of the
M 2 k1 x  x kp possible latent vectors. This means that a predictor
vector x is not exclusively associated with one of the M kernels, but has a
weight for each kernel determined by the product in (2). This allows each
observation to contribute some information about the inﬂuence of each
of the 1) sites, and thus allows borrowing of information across different
combinations of In , . . . , hp. In settings of extreme sparsity, where most of
the possible predictor vectors are not represented, this is an attractive
property. This uses many fewer parameters than a full factorial represen-
tation and is still ﬂexible enough to represent complex conditional distri-
butions. Finally, we assume normal kernels:

k1 kp P
ﬂyilxi) = Z ' " Z{N(yi; Mh1,...,h,,aTh_lj...,hp) X H”$(xij)} (3)
1:1

h1=l hp=l

This resembles other mixture-based approaches to density estimation as
originally speciﬁed in (1), but the proposed model for the weights pro-
vides the desired support for sparsity and information borrowing as pre-
viously discussed. In addition, the kernel-speciﬁc means ,uhhnth and
precisions 1711,"th are not functions of the predictor vector. Figure 1
shows a conditional dependence graph for the model parameters and
the observed data.

3.1 Predictor selection

The ﬁrst task in learning the conditional distribution is to identify those
predictors that provide the most information about the response; the
second task is to learn the form of the conditional distribution given
this set of informative predictors. The k1, ..., kp parameters indicate
the number of latent levels for each predictor. Because each kj can take
on the values 1, ... , a}, the possible combinations of different values for
k1, ...,kp can be immense, and including these as parameters in an
Markov chain Monte Carlo (MCMC) sampler is not an attractive option.

In the notation of (3), predictors exclusion is equivalent to identifying
those sites j such that kj = 1. Consequently, predictor vectors that differ
only at the jth position will have the same conditional density, and the jth
predictor can be excluded from the model. To identify those j such that
kj = 1, we use a predictor selection step based on a special form of the
it“). For eachj = 1,  ,p and each xj = 1,  , a}, we specify the n03 so
that ngjkxj) = 1 for exactly one hj and ngZ(xJ-) = 0 for all hk 7E 12,-. This
form for the n03 associates each predictor vector x with exactly one of the
M 2 k1 x  x kp kernels by giving that particular kernel a weight of
one. That is, if the set of maps n0), j = 1, ...,p is such that
Jrglll)(x,-1)= 1, ...,Jrg)(x,-p) = 1 for values In, ...,/21,, then only the
kernel indexed by In, ...,/21, will have non-zero weight. For computa-
tional convenience, we use conjugate priors and make the simplifying
assumption that the prior precision of each kernel mean ,uhhmh is the

P
same as the kernel precision 17,1,"th for each h1,...,hp, so that

Fig. 1. Conditional dependence graph showing the relationship between
the model parameters and the observed data

 

k1

at

H
—L

 

 

l

 

Mh1,...,h,|Th1,...,hp N N(0, Tiling) and Th1,...,h,, N Gamma(5t/2, 14/2).

Because the proposed form for Tim), ...,7r(1’) maps each predictor
vector to exactly one of the M groups, we can collect the observations
that map to each of the M groups and compute a marginal likelihood for
each group. Given the prior structure, the simplifying assumptions and
the clusterings deﬁned by the It“), ..., JIQ’), the log marginal likelihood
for the mth group is

Nm 1

Nm + 81‘ 8t 8t
+ log I‘(T) — log 1(3) + 5 log(y,)

(YnTtJNm)2

l

+ Vt}:

where Ym is the vector of responses, Nm is the number of observations in
group m and J Nm is a Nm x 1 vector of 1’s. The sum of these M approxi-
mated log-marginal likelihoods gives a score for the particular levels of
k1, ..., kp and the particular It“), . . . , 7T(p). Using these scores for differ-
ent levels of k1, ..., kp and different hard-clustering forms of
It“), ..., 7T(p), we can ﬁnd those predictors with inﬂuence on the condi-
tional density.

It is not generally feasible to evaluate every possible set of k1, .. . , kp,
even for moderately sized problems. Instead, we begin with the null
model, where k1 2 k2 =  = I, = 1 and propose random changes to
the different  and the associated it“). The randomly proposed changes
are of two types: ‘split’ and ‘merge’. A ‘split’ change at position j means
changing the no) map so that the distinct xij map to more levels. For
example, assume that the jth position has three observed levels (0',- = 3)
and the current form of n03 is such that all three observed levels of xj are

1
mapped to the same level. In this case, kj = 1 and n03 =  One pos-
1

sible ‘split’ move would propose it? so that Xj = 2 maps to the second

1 o
latent level, so that kj = 2 and n2) 2 [0 1]. Conversely, a ‘merge’ move
1 0

will decrease the number of mapped levels by one; using the deﬁnitions
above, one such merge move would be to replace it? with It“). If site j
already has kj 2 dj, then only merge moves are considered. Likewise, if
site j already has kj = 1, then only split moves are considered. We use a
Metropolis step to accept or reject the proposed change; the stochastic
search proceeds as:

(i) Set 121-20,ka 1;j= 1, ...,p; set no) =de,j= 1, ...,p; com-
pute the marginal likelihood MLC.

(ii) For j = 1, . .. ,p, draw from all possible split and merge moves
with equal probability. For a split, propose k; = kj + 1; for a
merge, propose k; = kj — 1.

(iii) Compute ML* for the proposed conﬁguration; accept the move
with probability 1 /\  If the new conﬁguration is accepted, set
kj = k; and ML" 2 ML*; ifkj> 1, set 12,- 2 nj +1.

(iv) After T iterations of steps 2-3, compute inclusion probabilities
pj=1;forj=1,...,p.

(v) Retain those predictors such that pj >01; using a = 0.5 is equiva-
lent to choosing the median probability model.

 

This stochastic search is similar to George and McCulloch (1997). The
approach we propose here is simple and appealing, but in our initial
simulation studies we noticed a tendency for this search to choose
overly complex models. Model selection was sensitive to the order in
which the predictors were considered. When the important features
were considered after many unimportant factors, randomly induced as-
sociations in the data and stochastic variation in the search led to com-
plex models that were not improved by addition of the important
predictors.

 

1564

112 /810's112umo[pJOJXO'sot112u1101u101q/ﬁd11q 111011 pepcolumoq

910K ‘09 lsnﬁnV no 22

Learning phenotype densities with the CTF

 

To combat this tendency, we introduced a preliminary predictor iden-
tiﬁcation step that considers each of the predictors in isolation. We can
represent the entire stochastic search on the jth predictor with a  x 
discrete-time Markov transition matrix derived from the acceptance and
move probabilities deﬁned above. We can then compute the long-run
proportion of time that the chain spends in states such that kj >1. This
can be done in an embarrassingly parallel fashion, and the computation
of each pj proceeds quickly. For the simulation case, where d]- : 4 for all j,
computation of each pj took ~0.3 s. At the conclusion of this single-pre-
dictor search step, we arrange the predictors in descending order of these
1)], retaining only those predictors such that pj >01, and proceed with the
full stochastic search to identify a ﬁnal predictor set.

3.2 Estimation after predictor selection

To estimate the parameters in the model using the selected predictors, we
introduce a prior precision to ~ Gamma(60 / 2, yo / 2) for each kernel mean
uh],  hp ~ N(0, to), a prior for each kernel precision
Th1,  ,hp ~ Gamma(6 t / 2, y, / 2) and separate Dirichlet priors for each
weight vector NU)(Xj) ~ Diri(kij , .. . , ii).

To facilitate computation, we augment the model proposed in (3) with
classiﬁcation vectors z,- that associates the ith observation with exactly one
kernel and gives a complete-data likelihood that can be expressed as a
product:

ﬁ’“ .211,

i=1 111:1 11,21

,  ,hp)]

1, IIZi=(h1
, —1
(12,-, M.....,h,.r,,,,.,,,.,) x wimp} (4)
j=1

The full conditional distributions are

(1) Mh1,...,h,,| ' " N NOLZIWJ, ,Iétzhmjpfl), where:
1:1,...,hp : To + Th1:"'3hpN:l-=1 IIZi : (hla ' ' ' 9hP)]
“’21,..th : {Thls"'5hp Zizl inIZi : (hl’ ' ' ' ’hP)]}/T;lk1,...,hp
(2) Th1,...,hpl - - - ~ Gamma(6*/2, y*/2), where:
5* =5,+:§ZV:V1 1[z,- = (h,  ,h,)]
V: = y. + 2.21 Hz. = (hi.  .hp)i(y.- — uh.,...,h,)2
k1J
(3) Tol - -- ~ Gammaaao + xvi/2.1m +1222. - - - 2,21 1111,”, 11/2)
(4) (NE/3(1), ...,ngjam . .. ~ Diri(klj + 2f; 1pc,-j = 111p,- = 1], .
$4211.11 l[x,-j=£] 1[z,-,-=kj]) fort: 1, ...,a} andj: 1, ...,p

 PrIZi :  E (hla ° ° °9hj—lam9hj+la °  ' ' ' (X  _ 
/‘L’Z;m] x ng2(x,-j) for m = 1, ...,kj within eachj: 1, ...,p; ¢(-)

indicates the standard normal density.

The updates for the ,uhl,  hp , 1711,},1, and n03 can be done blockwise, and
the z,- can be updated blockwise at each position j. Using the ﬁnal pre-
dictor set and the full conditionals, we produce a posterior sample for the
model parameters. This posterior sample allows us to compute condi-
tional densities and credible intervals around those estimates for various
combinations of the predictors.

4 DISCUSSION
4.1 Simulation study

To assess the variable selection and prediction performance of
the CTF, we conducted a simulation study, varying the number
of training observations N e {300, 500, 1000, 1500} and using a
consistent ground truth to produce simulated datasets with total
number of predictors p = 1000. In each case, the true model was
based on three predictors at positions 30, 201 and 801, each with
d]- = 4 levels and including three-way interactions among these
predictors. The resulting marginal density is an equally weighted
mixture of 64 Gaussians with different means and the same

residual precision ‘L'. In other words, an observation with
(xia30,x,-,201,x,-,801) =(3,2,1) is drawn from N(,u.3, 2,1,t_1), and
so forth for each of the 64 distinct predictor vectors.

For each of 20 training sets, we produced selected predictor
sets and posterior samples. We then made predictions for 20
validation sets drawn from the same underlying true distribution.
As competitor methods, we used random forests (RF) and
quantile regression random forests (QRF) (Meinshausen,
2006); these are implemented in the randomForest and
quantregForest packages in R. BART, as implemented in
the BayesTree package, was unable to run to completion on
any of the training sets, though we were able to use BART with
the real data example in Section 4.2. RF and QRF include pre-
dictor selection directly, and QRF directly addresses the idea of
coverage proportion. BART is another MCMC-based approach,
but it does not directly address variable selection, allowing us to
investigate the impact of the large predictor space. The implicit
cost in estimating the joint distribution of predictors and re-
sponse made Bayes networks unattractive.

We computed mean square prediction error (MSPE) as the
average squared difference between the response value predicted
by the model for a predictor vector from the validation set and
the actual response value for that observation. We deﬁned cover-
age proportion as the proportion of times that the 95% predic-
tion interval for an observation in the validation set included the
actual response value, averaged over the intervals for each pos-
terior sample. When comparing performance with that of the
competitors, we attempted to give those competitors whatever
advantages we could provide. In the case of RF, this meant that
we did two passes over the training data. The ﬁrst pass identiﬁed
important variables using the importance method in the
randomForest package. We used the ‘mean decrease in accur-
acy’ style of importance; this measurement is derived from the
impact of permuting out-of-bag data for each tree in the forest.
We then fed those variables identiﬁed as important as a pre-
selected set into a second run of RF. This generally improved
the MSPE performance of RF. An analogous method was not
available for QRF, so we could not treat that method in the same
manner. In each of the 20 cases for p = 1000 and training
N: 500, the CTF outperformed RF on mean square prediction
error and showed comparable 95 % coverage proportions to
those derived from QRF; this is summarized in Figure 2. The
CTF and RF showed comparable accuracy in identifying import-
ant predictors, but RF tended to include many unimportant pre-
dictors. In contrast, the CTF produced no false-positive results,
identifying the correct subset of predictors in each case. This
performance is particularly attractive given the large number of
possible interactions in the original predictor set. Both RF and
QRF may have suffered because of the strong interactions pre-
sent in these simulated data.

4.2 Molecular epidemiology application

We also consider an application to an epidemiology dataset,
comparing CTF performance with that of the same competitor
methods (RF, QRF and BART). The dataset concerns DNA
damage to instances of different cell lines when exposed to en-
vironmental chemicals. The exposure types are hydrogen perox-
ide (H202) and methyl methane sulfonate (MMS), and the

 

1565

112 /810's112umo[pJOJXO'sot112u1101u101q/ﬁd11q 111011 pep1201umoq

910K ‘09 lsnﬁnV no 22

D. C.Kessler et al.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

6
u? m + CTF,t=0.5 RF,t=0.5
5 w—- -+- CTF,1:=1 RF,t=1
213
'D
9 o_
D_ 1—
9
(U
E} x
‘2 2°- ‘ ~ .
(U x A
e ‘— -------- - - .1 ————————— ﬂ - -
| l l l l l
400 600 800 1 000 1200 1400

g In— + CTF,'c=0.5 QRF,'c=0.5
g 8_ ’ ’L ~ ~ uﬁ- CTF,1:=1 QRF/12:1
5 o k ’ ' T ~ - ~
0 - ~ 2 g _ _ _ _ _ - .. 4
E a “' ‘ ‘ ‘
5 02—
'“ o r
o\
8 ‘3

0,-

o'

 

| | | | | |
400 600 800 1000 1200 1400
Training sample size
Fig. 2. Simulation study results, comparing CTF with random forests
and quantile regression random forests

remainder of the predictor set is genotype information on 49 428
single nucleotide polymorphisms (SNPs). Rodriguez et al. (2009)
provide extensive details on the original experiments. One hun-
dred separate instances of each of 90 cell lines were exposed to
each chemical and examined at each of three time points (before
treatment, immediately after treatment and a longer time after
treatment). The nature of the measurement is destructive; at the
desired time interval, comet assay was performed on each cell
and the Olive tail moment (OTM) (Olive et al., 1991) was re-
corded; this assesses the amount of DNA damage in the cell, with
higher measurements indicating more damage. The cells from
each line are genetically identical, but the resulting distribution
of OTM has a different shape for each cell line. In addition, these
distributions are different at the separate time points; generally,
OTMs are smallest (least damage) before exposure to the chem-
ical, largest (most damage) immediately after exposure and
somewhere in-between after a longer recovery time.

We computed empirical quantiles of the OTM for each cell
line at each of the three time points and then derived a single-
number summary wl-j to tie these three quantile vectors together
for cell line i and exposure j. The summary measure Wij e (0, 1) is
the value that minimizes

31
Z IWijQij,N,h + (1 — Wij)Qij,L,h — Qij,A,hI (5)

11:17

Here, Qij; Nah indicates the h/32‘h quantile for the ith cell line’s
OTM distribution at the ‘No treatment’ time, with corresponding
quantities for the ‘Later’ time point and the ‘immediately After’
time point for the jth exposure. The use of only the higher quan-
tiles reﬂects our desire to learn more about the extremes of DNA
repair. We used a logit transform to derive our ﬁnal response

Wz‘j

yij = log 1_Wij); this is appropriate for the assumptions of the

 

model. Negative values of the response indicate that the OTM
distribution long after treatment is closer to the distribution right
after treatment; positive values indicate that the ‘long after’ dis-
tribution is closer to the distribution before treatment.

SNPS in genes thought to be associated with some aspect of
DNA repair were genotyped, leading to data on 49428 individ-
ual SNPs. Given the small number of cell lines and the fact that

Table 1. Details for SNPs included in the ﬁnal CTF model for the mo-
lecular epidemiology data

 

 

Gene SNP Position

IGFBP5 RS11575170 217256085
TGFBR3 RS 17880594 92118885
CHClL RS9331997 47986441
XPA RS3176745 99478631

 

Table 2. Comparison of MSPE, 95% coverage proportion and mean
computation time for different methods applied to molecular epidemi-
ology data

 

 

Metric CTF RF QRF BART
MSPE 0.263 0.353 — 0.425
95% Coverage 0.961 — 0.928 0.817
Time (s) 3317 80 88 2343

 

many individuals have two copies of the major allele for these
SNPs, many of the SNP proﬁles were identical or had no indi-
viduals with two copies of the minor allele. We recoded the
genotypes so that one indicated at most one copy of the major
allele and two indicated two copies of the major allele. After
recoding, we reduced the predictor set to those SNPS with dis-
tinct proﬁles, leaving 23 210 SNPS for analysis.

We used leave-one-out cross-validation to assess the perform-
ance of CTF against the three competitors RF, QRF and BART.
We ran the variable selection chain for 5000 burn-in iterations
and computed inclusion probabilities from 10000 samples. We
ran the MCMC chain for 40 000 burn-in iterations and retained a
sample of 20 000 iterations. Autocorrelation diagnostics indi-
cated an effective sample size of 15 000. We used the same
burn-in and posterior sample sizes for BART. As in the simula-
tion study, we used the results from a ﬁrst run of RF to seed a
ﬁnal run of RF.

CTF showed consistent selection of the treatment (H202 or
MMS) as the most important predictor and selected a set of
four SNPs (IGFBP5, TGFBR3, CHClL and XPA) as predictors;
information about these SNPS is summarized in Table 1. In con-
trast, RF chose the treatment variable in only 56 of the 180 cross-
validation scenarios and did not consistently identify any other
predictors. Comparison with the competitor methods showed pat-
terns similar to the simulation study; Table 2 compares the results
from each method. The interactions between the treatment and
the various SNPS may be weak enough that they do not contrib-
ute to the same elevated MSPE that RF demonstrated in the
simulation study. Even though the MSPE for RF was close to
that for the CTF, the CTF was able to achieve lower MSPE while
not sacriﬁcing coverage performance. This improved performance
offsets the CTF’s higher computational time requirement.
Figure 3 shows estimated conditional densities with 95 % credible
intervals from the full dataset given varying levels of the treatment
and of the IGFBP5 SNP while holding the other three SNPS at

 

112 /810's112umo[pJOJXO'sot112u1101u101q/ﬁd11q 111011 pep1201umoq

910K ‘09 lsnﬁnV no 22

Learning phenotype densities with the CTF

 

H202, IGFBP5=ZeroIOne Copy

co— co.

H202, IGFBP5=Two Copies

 

 

4 5
I I
4 5
I I

Estimated Density
3
I
3
I

 

 

 

 

O
I

l
l

 

 

MMS, IGFBP5=ZeroIOne Copy

co- co.

MMS, IGFBP5=Two Copies

 

 

5
I
5
I

4
I
4
I

Estimated Density
2 3
I I
2 3
I I

1
I

 

 

 

 

I I I
—1 O 1 2 3 —1 O 1 2 3
Logit of minimizing weight Logit of minimizing weight

0
I

 

 

Fig. 3. Selected conditional densities given different exposures and differ-
ent number of copies of the dominant allele at the IGBP5 SNP, holding
all other SNPs at the 0/1 level. Heavy black lines show the mean condi-
tional density and gray lines show the 95% credible interval

Table 3. Summary of conditional distribution characteristics

 

 

Proﬁle Mean Variance 90th %ILE
H202, IGFBP5 = 0/1 0.226 11.39 2.65
H202, IGFBP5 = 2 0.156 7.31 2.25
MMS, IGFBP5 = 0/1 0.023 9.76 2.07
MMS, IGFBP5 = 2 —0.023 6.11 1.88

 

the ‘Zero/One Copy’ level, and illustrates how the conditional
density changes in more than the conditional mean when the
predictor vector changes. In this case, the interaction between
MMS treatment and two copies of the major allele for this
IGFBP5 SNP tightens the density markedly, although it has a
more muted impact on the conditional mean. The change is less
dramatic under the exposure to H202. Here, the shift in the mean
response as treatment and genetic proﬁle change is less interesting
than the difference in conditional variance; under treatment with
H202, the mean response is slightly different than under treat-
ment with MMS, but the tail probabilities are noticeably differ-
ent. Table 3 summarizes these differences in conditional mean,
conditional variance and conditional 90th percentile for each scen-
ario. As suggested in Figure 3, the medians of the conditional
densities given the exposure (H202 or MMS) are close, but in
the tail of the distribution (the 90th percentile), the separation
between the estimated quantile curves is larger. This varying
shift in the 90th percentile reﬂects the interaction between the
exposure and the level of the IGFBP5 SNP.

5 CONCLUSION

We have presented a novel method for ﬂexible conditional dens-
ity regression in the common case of a continuous response and
categorical predictors. The simulation study and real data ex-
ample suggest that this conditional tensor factorization method
can have better performance than other modeling tools when

there is substantial interaction between the predictors of interest.
The CTF does have a higher computational time requirement
than the competitor methods, but the improvement in prediction
accuracy and coverage still make the CTF an attractive method.

A particularly appealing aspect of the CTF is predictor selec-
tion, which ﬁnds low-dimensional structure in the high-dimen-
sional predictor set. This reduction to more parsimonious
models yields a succinct description of the ways in which the
phenotype varies given exposure and SNPs. Finally, a distinct
advantage of the CTF is its ability to produce conditional density
estimates. This property of the CTF provides insight beyond a
simple conditional expectation and makes it possible to answer
more complex questions about the relationship between the re-
sponse and the predictors.

Funding: This research was supported in part by the Intramural
Research Program of the NIH, National Institute of
Environmental Health Sciences, 201 ES049032. David
Kessler’s work was partially supported by National Institute of
Environmental Health Sciences training grant T32ES007018.
David Dunson’s work was supported by Award Numbers
R01ES017240 and R01ES017436 from the National Institute
of Environmental Health Sciences.

Conflict of Interest: none declared.

REFERENCES

Bishop,C. and Svensén,M. (2003) Bayesian hierarchical mixtures of experts. In:
Proceedings of the Nineteenth Conference on Uncertainty in Artificial
Intelligence. pp. 57—64.

Breiman,L. (2001) Random forests. Mach. Leam., 45, 5—32.

Chipman,H. et al. (2006) Bayesian ensemble learning. In: Advances in Neural
Information Processing Systems. MIT Press, Cambridge, Massachusetts, USA,
pp. 265—272.

Chipman,H. et al. (2010) BART: Bayesian additive regression trees. Ann. Appl.
Stat., 4, 266—298.

Chu,W. and Ghahramani,Z. (2009) Probabilistic models for incomplete multi-
dimensional arrays. Proceedings of the 12th International Conference on
Artificial Intelligence and Statistics ( AIST AT S ), Clearwater Beach, Florida,
USA.

Chung,Y. and Dunson,D. (2009) Nonparametric Bayes conditional distribution
modeling with variable selection. J. Am. Stat. Assoc., 104, 1646—1660.

Cowell,R. et al. (1999) Probabilistic Networks and Expert Systems. Springer, New
York, USA.

De Iorio,M. et al. (2004) An ANOVA model for dependent random measures.
J. Am. Stat. Assoc., 99, 205—215.

Dempster,A.P. et al. (1977) Maximum likelihood from incomplete data via the EM
algorithm. J. Roy. Stat. Society B (Methodological), 39, 1—38.

Dunson,D. and Park,J. (2008) Kernel stick-breaking processes. Biometrika, 95,
307—323.

Dunson,D. and Xing,C. (2009) Nonparametric Bayes modeling of multivariate cat-
egorical data. J. Am. Stat. Assoc. (Theory and Methods), 104, 1042—1051.
George,E. and McCulloch,R. (1997) Approaches for Bayesian variable selection.

Stat. Sin., 7, 339—373.

Grifﬁn,J. and Steel,M. (2006) Order-based dependent Dirichlet processes. J. Am.
Stat. Assoc., 101, 179—194.

Hannah,L. et al. (2011) Dirichlet process mixtures of generalized linear models.
J. Mach. Learn. Res., 12, 1923—1953.

Hoff,P. (2011) Hierarchical multilinear models for multiway data. Comput. Stat.
Data Anal., 55, 530—543.

J ara,A. and Hanson,T. (2011) A class of mixtures of dependent tail-free processes.
Biometrika, 98, 553—566.

Jordan,M. and Jacobs,R. (1994) Hierarchical mixtures of experts and the EM
algorithm. Neural C0mput., 6, 181—214.

 

112 /810's112umo[pJOJXO'sot112u1101utotq/ﬁd11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no 22

D. C.Kessler et al.

 

Langseth,H. et al. (2012) Inference in hybrid Bayesian networks with mixtures of
truncated basis functions. In: Sixth European Workshop on Probabilistic
Graphical Models, pp. 171—178.

Lauritzen,S. (1992) Propagation of probabilities, means, and variances in mixed
graphical association models. J. Am. Stat. Assoc., 87, 1098—1108.

Meinshausen,N. (2006) Quantile regression forests. J. Mach. Learn. Res., 7,
983—999.

Moral,S. et al. (2001) Mixtures of truncated exponentials in hybrid Bayesian net-
works. In: Proceedings of the Sixth European Conference on Symbolic and
Quantitative Approaches to Reasoning Under Uncertainty (ECSQARU 2001).
New York, USA, pp. 156—167.

Muller,P. et al. (1996) Bayesian curve ﬁtting using multivariate normal mixtures.
Biometrika, 83, 67—79.

Olive,P. et al. (1991) DNA double-strand breaks measured in individual cells sub-
jected to gel electrophoresis. Cancer Res., 51, 4671—4676.

Pearl,J. (1988) Probabilistic Reasoning in Intelligent Systems: Networks 0fPlausible
Inference. Morgan Kaufmann, San Francisco, California, USA.

Reich,B. and Fuentes,M. (2007) A multivariate semiparametric Bayesian spatial
modeling framework for hurricane surface wind ﬁelds. Ann. Appl. Stat., 1,
249—264.

Reich,B. et al. (201 l) Sufﬁcient dimension reduction via Bayesian mixture modeling.
Biometrics, 67, 886—895.

Rodriguez,A. et al. (2009) Bayesian hierarchically weighted ﬁnite mixture models
for samples of distributions. Biostatistics, 10, 155—171.

Shahbaba,B. and Neal,R. (2009) Nonlinear models using Dirichlet process mixtures.
J. Mach. Learn. Res, 10, 1829—1850.

Tokdar,S. et al. (2010) Bayesian density regression with logistic Gaussian process
and subspace projection. Bayesian Anal., 5, 319—344.

Tucker,L. (1966) Some mathematical notes on 3-mode factor analysis.
Psychometrika, 31, 279.

Waterhouse,S. et al. (1996) Bayesian methods for mixtures of experts. In: Advances
in Neural Information Processing Systems. MIT Press, Cambridge,
Massachusetts, USA, pp. 351—357.

Yang,Y. and Dunson,D.B. (2012) Bayesian Conditional Tensor Factorizations for
High-Dimensional Classiﬁcation. Working Paper. Duke University, Durham,
USA.

Xu,Z. et al. (2012) Inﬁnite Tucker decomposition: nonparametric Bayesian models
for multiway data analysis. In: Proceedings of the 29th International Conference
on Machine Learning, Princeton, New Jersey, USA.

 

112 /810's112umo[pJOJXO'sot112u1101utotq/ﬁd11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no 22

