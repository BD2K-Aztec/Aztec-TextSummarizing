ORIGINAL PAPER

Vol. 30 no. 2 2014, pages 242-250
doi:10. 1093/bioinformatics/btt662

 

Data and text mining

Advance Access publication November 20, 2013

Effect of separate sampling on classification accuracy
Mohammad Shahrokh Esfahanil’2 and Edward R. Dougherty1’2’*

1Department of Electrical and Computer Engineering and 2Center for Bioinformatics and Genomic Systems Engineering,

Texas A&M University, College Station, TX 77843, USA
Associate Editor: Prof. Martin Bishop

 

ABSTRACT

Motivation: Measurements are commonly taken from two phenotypes
to build a classifier, where the number of data points from each class
is predetermined, not random. In this ‘separate sampling’ scenario,
the data cannot be used to estimate the class prior probabilities.
Moreover, predetermined class sizes can severely degrade classifier
performance, even for large samples.

Results: We employ simulations using both synthetic and real data
to show the detrimental effect of separate sampling on a variety of
classification rules. We establish propositions related to the effect on
the expected classifier error owing to a sampling ratio different from
the population class ratio. From these we derive a sample-based mini-
max sampling ratio and provide an algorithm for approximating it from
the data. We also extend to arbitrary distributions the classical popu-
lation-based Anderson linear discriminant analysis minimax sampling
ratio derived from the discriminant form of the Bayes classiﬁer.
Availability: All the codes for synthetic data and real data examples
are written in MATLAB. A function called mmratio, whose output is an
approximation of the minimax sampling ratio of a given dataset, is also
written in MATLAB. All the codes are available at: http://gsp.tamu.edu/
Publications/supplementary/shahrokh13b.

Contact: edward@ece.tamu.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on August 5, 2013; revised on November 9, 2013; accepted
on November 11, 2013

1 INTRODUCTION

The medical community is being confronted with serious prob-
lems of reproducibility in the development of biomarkers. The
issue has been highlighted by a recent report regarding comments
by Janet Woodcock, FDA drug division head. The report states,
‘Based on conversations Woodcock has had with genomics re-
searchers, she estimated that as much as 75 percent of published
biomarker associations are not replicable. “This poses a huge
challenge for industry in biomarker identiﬁcation and diagnos-
tics development,” she said (Ray, 2011).’ Many issues affect
reproducibility, including the measurement platform, specimen
handling, data normalization and sample compatibility between
the original and subsequent studies. These matters concern
experimental procedures and are not our concern here; rather,
we are interested in the methodology for designing classiﬁers.
One issue in this regard is the impact of inaccurate error estima-
tion owing to small samples. This has been previously quantiﬁed

 

*To whom correspondence should be addressed.

Wouseﬁ and Dougherty, 2012). Here we are interested in a dif-
ferent problem, one that will confront us even if we have large
samples and perfect error estimation: the effect of having prede-
termined sample sizes so that sampling is not random.

In classiﬁcation studies it is typically a tacit assumption that
sampling is random; indeed, it is commonplace for this assump-
tion to be made throughout a text on classiﬁcation. For instance,
Devroye et al. declare on page 2 of their text that all sampling is
random (Devroye, 1996). The assumption is so pervasive that it
can be applied without mention. With regard to the problem at
hand, Duda et al. (2001) state, ‘In typical supervised pattern
classiﬁcation problems, the estimation of the prior probabilities
presents no serious difﬁculties.’ But, in fact, there are often ser-
ious difﬁculties.

Under the assumption of random sampling, the data set,
Sn={(X1, Y1), . . . , (Xn, Yn)}, is drawn independently from a
ﬁxed distribution of feature-label pairs, (X, Y); in particular,
this means that if a sample of size n is drawn for a binary clas-
siﬁcation problem, then the numbers of sample points, no and 111,
in classes 0 and 1, respectively, are random variables such that
n0+n1= n. An immediate consequence of the random-sampling
assumption is that the prior probability c=Pr(Y= 0) can be
consistently estimated by the sampling ratio, namely, by 8 = 
Consistency is nothing but Bernoulli’s weak law of large num-
bers, namely, "7° —> c in probability. Thus, if the sample is large,
we can expect the sampling ratio to be close to the prior
probability.

Suppose the sampling is not random, in the sense that the
ratios 71—0 and "71 are chosen prior to sampling. In this ‘separate
(stratiﬁed) sampling’ case, S, = Sn0 U Sn], where the sample
points in Sn0 and S”, are selected randomly from 1'10 and
1'11 but, given 11, the individual class counts no and 111 are
not random. Then, in effect, we have no sensible estimate of
c. One could let 6 2 "7°, but there would be no reason to
do so.

Since our aim is to use the data to train a classiﬁer, does the
inability to consistently estimate c matter? Clearly in the case of
linear discriminant analysis (LDA) it does, since the LDA clas-
siﬁer is deﬁned by wn(x) = 1 if Dsamp(x) 5 O and wn(x) = 0 if
Dsm1p (x) > 0, where

it +i: 71—1 6
Dsmp(x)=(x—°T1) >2 (ﬁe—ﬁ1)—111 , , <1)

 

and £10 and ill are the sample means of the class-conditional
populations 1'10 and 1'11, respectively, and i is the pooled
sample covariance matrix. The rationale for the LDA discrimin-
ant is that the estimators converge to the population parameters

 

242 © The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

112 [glO'SIBILInO[plOJXO'SODBIILIOJHIOIQ/[ldllq IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

Effect of separate sampling on classification accuracy

 

as n —> oo, in which case the resulting discriminant, DBayes(x),
deﬁnes the Bayes (optimal) classiﬁer in the two-class Gaussian
model with common covariance matrix. It is obvious from
Equation (1) that an estimate of c is required for LDA and a
bad choice of 6 will negatively impact the classiﬁer. This fact,
which is a consequence of separate sampling, has long been
recognized (Anderson, 1951).

The situation is less transparent with model-free classiﬁcation
rules such as support vector machines. In this article we use
simulation to study the effect of separate sampling on several
different classiﬁcation rules, where the role of c does not
appear explicitly in classiﬁer learning. We generate separate sam-
ples with different ratios r = "7" and consider the expected error,
E[e,,|r], of the designed classiﬁer, given r, where the error of
classiﬁer 10,, is deﬁned by 8,, = Pr(w,,(X) 75 Y), the probability
of misclassiﬁcation. We will see that the penalty for separate
sampling without knowledge of c can be severe.

With random (or, ‘mixed’) sampling, rather than being ﬁxed
prior to sampling, r is a sample-dependent random variable. In
this case, E[e,,|r] denotes the expectation of the error conditioned
on r and the expected classiﬁcation error is given by
E[8,,] = Er[E[e,,|r]], where the outer expectation is relative to
the distribution of r. The classiﬁer error is likely to be smaller
when the sampling ratio r is close to 0. Hence, if one happens to
ﬁx r sufﬁciently close to c, then E[e,,|r] < E[8,,]. Because r —> c in
probability as n —> 00 for mixed sampling, as n gets larger the
distribution of r gets more tightly concentrated around c, so that
the distribution of E[e,,|r] (as function of r) gets more tightly
packed around E[8,,], which in turn means that to have
E[8,,|r]< E[e,,] one must choose r very close to c. To illustrate
this phenomenon, consider 2D Gaussian class-conditional den-
sities with means at (0.3, 0.3) and (0.8, 0.8), possessing common
covariance matrix 021, where I is the identity matrix and
02 = 0.4, and with c = 0.6. For this model, the Bayes error is
eBayes = 0.27. Figure 1 shows the difference E[8,,] — E[e,,|r] for

 

 

 

 

-0.03--

 

 

 

-0.04 -

 

 

 

 

-0.05 ' ' ' ' ' ' ' '

10 20 30 40 50 60 70 80 90 100
n

Fig. 1. The difference E[8,,(c)] — E[8,,(c)|r] for different values of r as a

function of sample size, n, with c=0.6. The class conditional densities

parameters are as follows: yo = (0.3,0.3),u1 = (08,08), and Bo =

21 = 0.41, leading to the Bayes error eBayes = 0.27

different values of r and different sample sizes when using
LDA. If r = 0.6, then E[e,,|r]< E[8,,] for all n. If r = 0.7,
which is fairly close to c = 0.6, then E[e,,|r]< E[8,,] for n 5 25.
Notice the lack of symmetry, both 0.5 and 0.7 being equally close
to c. This should not be surprising because we should not expect
the distribution of E[e,,|r] to be symmetric.

Let us examine Figure 1 from the practitioner’s perspective.
Suppose that cost limits the sample to a given size n. If the
sample is random, then the expected error of the designed clas-
siﬁer will be E[8,,], which is unknown since the feature-label dis-
tribution is unknown. Consider three cases: (i) if c is accurately
known from existing population statistics regarding the two
classes, say BRCA1 and BRCA2 breast cancer, then no matter
what the sample size, it is best to do separate sampling with
no % cn; (ii) if c is approximately known, meaning that the prac-
titioner believes that c is close to c’, then, for small n, it may be
best, or at least acceptable, to do separate sampling with
no % c’ n, and the results will likely still be acceptable for large
n, though not as good as with random sampling; (iii) if the prac-
titioner has no idea what c is, then sampling must be random
because the penalty for separate sampling can be very large.
While, at this point, these comments refer specially to Figure l,
which is for LDA, a salient point to be made in this article is that
they are quite general and, moreover, can be extended to the
commonplace separate sampling situation where one cannot
choose no and n1.

Why is all of this a major issue for bioinformatics? Simply put
separate sampling is ubiquitous in bioinformatics, in particular,
with genomic classiﬁcation, where a standard approach is to take
tissue samples from two classes, say, different types of cancer or
different stages of cancer, for which the number of specimens in
each class is not chosen randomly, and then to design a classiﬁer.
The Supplementary Material lists 20 published studies using sep-
arate sampling. In each case we give the classiﬁcation problem,
sample sizes, classiﬁcation rule and error estimator. Even if an
error estimate is exact for the problem at hand—that is, for the
sampling ratio represented by the data—what does it mean
relative to the classiﬁcation error for future observations (say,
patients)? That depends on the true prior probabilities, which we
do not know.

2 SYSTEMS AND METHODS

2.1 Effect of sampling ratio—synthetic data

We employ simulation to study the effect of the sampling ratio for dif-
ferent classiﬁcation rules using a general model based on multivariate
Gaussian distributions with a blocked covariance structure. This model
conforms to the setting where blocks represent correlated gene groups,
say common pathways, and between-block correlation is negligible
moughtery et al., 2007; Hua et al., 2005; Shmulevich and Dougherty,
2007). The model has several parameters that can generate a battery of
covariance matrices. For example, a 4-block covariance matrix with
block size 3 has the structure

13,, 0 0 0
_ 0 13,2 0 0

2y — 0 0 13,3 0 ’ (2)
0 0 0 13,4

 

243

112 [glO'SIBILInO[plOJXO'SODBIIIJOJHIOIQ/[ldllq IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

M.S.Esfahani and E.R.Dougherty

 

where
2 2 2
Uyz pit:y MO};

I 2 I 2 2
play play 0y

in which of, is the variance of each variable and the pi, i e {1, 2, 3, 4}, are
the correlation coefﬁcients inside blocks. We consider both identical and
unequal covariance matrices. We assume common correlation coefﬁcient,
p,- = p,i e {1,2,3,4}.

A typical microarray or next-gen RNA sequencing (Mortazavi et al.,
2008; Wang et al., 2009) experiment yields expressions for thousands of
genes, but a small number of sample points, typically <200. Therefore,
data-based feature selection is typically employed; however, since our sole
aim is to study the effect of the ratio r on the expected true error, we do
not consider feature selection and assume a model containing a reason-
able number, D, of features (which is equivalent to assuming that a set of
D genes has been chosen by the researcher based on prior biological
knowledge). We let D = 15. Two covariance matrix settings are con-
sidered: identical covariance matrices, 03 = 0% = 0.4, and unequal co-
variance matrices, 03 = 0.4, of = 1.6, with block size I = 5 and
correlation coefﬁcient p = 0.8 corresponding to tight correlation within
a block. The parameter settings are summarized in Table 1.

Seven classiﬁcation rules are considered: 3-nearest neighbor (3NN), 5-
nearest neighbor (5NN), linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), linear support vector machine (L-SVM),
radial basis function SVM (RBF-SVM) and decision tree (DT). The
SVM classiﬁers are trained from the package LibSVM written in
MATLAB (Chang and Lin, 2011). A decision tree classiﬁer is trained
using the MATLAB classregtree function.

2.2 Effect of sampling ratio—real data

Four microarray real datasets are used: pediatric acute lymphoblastic
leukemia (ALL) Weoh et al., 2002), acute myeloid leukemia (AML)
Walk et al., 2004), multiple myeloma (Zhan et al., 2006) and breast can-
cer (Desmedt et al., 2007). We follow the data preparation instructions
reported in the cited articles. The properties of these datasets are sum-
marized in Table 2. The right-most column in Table 2 contains the initial
feature size, number of sample points in classes 0 and 1, respectively, from
left to right. The Supplementary Material provides detailed descriptions
of these datasets. The same classiﬁcation rules as those used for the syn-
thetic data are applied to the real data. T-test feature selection is used to
reduce the original set of genes down to D = 15.

2.3 Holdout error estimation

Because we are going to use real data, we wish to use holdout error
estimation; however, the standard holdout procedure, which is unbiased
with random sampling, become biased, perhaps severely so, with separate
sampling. Therefore, we redeﬁne holdout for separate sampling.

Table 1. Distribution model parameters

 

Parameters Value/description

 

Mean [.60 = 0.310, [.61 = 
Covariance matrix 03 = 0.4, of = 0.4 (identical covariance)
02 = 0.4, 02 = 1.6 (unequal covariance)
o 1

Block size I = 5
Feature size D = 15
Feature block correlation p = 0.8

 

The true error of a designed classiﬁer (0,, is given by
8n = PIOMX) 7'5 Y)
= CPTWAX) 79 0| Y: 0)
+ (1 - 6')P1r(iﬂn(X)5ré 1|Y= 1)
= 682-1-(1— 698,11.

(4)

Relative to a random sample, S”, the expected true error is
Es..[8n] = cEs,[82] + (1 - C)ES,, [8,1,1 (5)

For standard holdout error estimation, the sample is split into t points
(the training set) to train the classiﬁer and m points (the test set) to
estimate the error, where in this scenario the notation indicates that the
total sample size is n = t+ m. Let S,, Sm, Smo, and Sm1 denote the set of
training data, the full set of test data, the class-0 test points, and the class-
1 test points, respectively. The holdout estimator is

A 1
8000:; Z MAXI-#10

(Xi, Yi)€Sm

m0 1
= Wm— : MAXI-#10
0 (Xi: Yi)ESm0 (6)

m1 1
+—— Z 1wn(X.-)¢Yi
m m1(Xi.Y.-)esm,

m1 A1

=Eﬁm+ MM,
m

m
where 5° and §1 denote the holdout estimators of 82 and 8,1,. Taking
expectations in (6) yields

ES,[§] = cES,[§°] + (1 — c)ES,[§1]. (7)

Because the test data are independent from the training data, the hold-
out estimator is unbiased given the training data, which means that
ESn[§ISt] = 8”. Taking the expectation relative to the training data
yields ESn[§] = ESt[ESn[§|St]] = Esn[8n]. Similar expressions apply to £0
and 51, namely, Esn[§°] = ESn[82] and ES" [51] = ESn[8,1,]. Thus,
a, £0, and 51 are unbiased estimators of an, 82, and 8,11, respectively, and
Expression (7) corresponds term by term to (5).

With separate sampling, taking expectations in (6) yields

A m A m1 A
hM=ﬁhWthﬂl

(8)

m m1
=Jaw+—am.
m m

because the ratio % is ﬁxed. Hence, 5 is not unbiased. The bias depends
on the difference between c and  If c is known, then the holdout
estimator can be redeﬁned as

5. = 0230 + (1 — c)§1, (9)

for both random and separate sampling. In both cases it is unbiased:
taking expectations on the right-hand side of (9) yields the right-hand

Table 2. Real datasets used in this article

 

Dataset Dataset type FeaturelSample size

 

Yeoh et al., 2002 Pediatric ALL
Valk et al., 2004 AML

Zhan et al., 2006 Multiple myeloma
Desmedt et al., 2007 Breast cancer

5077|149/99
22215|116/157
54613|156/78
22215|98/77

 

 

244

112 [glO'SIBILInO[p.IOJXO'SODBIIHOJIIIOIQ/ﬂ(11111 IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no 22

Effect of separate sampling on classification accuracy

 

(a) Noose), MULLED (b) 

l S.

sampling With sampling with St 1 d. S
7' — size of ’n and 7‘ — size of n — m and eXC 11 mg 15

 

Sn

 

 

 

 

the given ratio T from the data set

 

the given ratio r

 

St

 

feature selec—

tion (t—test)

\II —~‘ classiﬁer training
‘11 —v‘ classiﬁer training \

¢n
true error

computation

 

1% | §°

 

 

 

  
   

 

 

holdout error
estimation Sm

‘ I

an 2 059, + (1 — c)e,1,

 

 

 

 

 

 

a, = cég+ (1—c)é},

Synthetic data procedure Real datasets procedure

Fig. 2. Flowcharts of the processes implemented for the synthetic and
real dataset examples. The dashed boxes show one iteration of the MC
simulation, repeated to ﬁnd an approximation for the expected true error,
i.e. ES” [8,, (c)|r], and the expected holdout error estimation, i.e. E[§,, (c)|r],
respectively, for the synthetic and real dataset examples

side of (5). (We are unaware of this approach to holdout error estimation
in the case of separate sampling having been previously used.) Absent an
independent estimate of c, use of (6) for separate sampling is unacceptable
because it is biased and the extent of the bias is unknown. We use (9) for
our real-data examples.

2.4 Implementation

For a given synthetic model parameter setting, sample size n, ratio r and
classiﬁcation rule III, we approximate the expected true error rate
ESn[8,,(c)|r] via Monte-Carlo (MC) simulation. Each repetition of the
MC simulation is depicted in Figure 2(a).

The ﬁrst set of experiments is done using the ﬂowchart in Figure 2(a).
In these experiments, the sample size, n, is ﬁxed but the class sample sizes
vary according to the sampling ratio r. Samples S, are generated using the
model determined by (no, 20) and (pl, 21) described in Table 1 in accord-
ance with r and n. Assuming a classiﬁcation rule III, a classiﬁer is trained.
The last stage in Figure 2(a) is the true error computation for the designed
classiﬁers, which is also done via MC with 10 000 repetitions. The whole
procedure is repeated 5000 times.

The real datasets in Table 2 are sufﬁciently large to be divided for
training and testing and we use holdout error estimation, as previously
described for separate sampling. The procedure is graphically illustrated
in Figure 2(b).

Fixing the total sample size n, assuming different values for the par-
ameter r, we choose no 2 [r(n — m)_| points from class 0 and
n1 2 (n — m) — no from class 1, where la] is the smallest integer greater
than or equal to a. The remaining data points are used for holdout esti-
mation. The expected holdout estimate, E[§,,(c)|r], is computed via MC
approximation. The process is repeated 3000 times.

3 RESULTS AND DISCUSSION

The full set of results appears in the Supplementary Material.
Herein, we provide some results covering a variety of cases. We
show results of four classiﬁcation rules: 3NN, 5NN, L-SVM and
RBF-SVM. Results for the synthetic examples with dimension 15

are shown. Also, we only provide the results for n: 100 and
n = 80, for the synthetic and real datasets, respectively. For the
real datasets, a two-sample t-test is used to reduce the dimensions
in Table 2 to D: 15.

3.1 Expected true error

The expected true errors for synthetic data with common covari-
ance matrix are given in Figure 3(a)-(d), where each plot gives the
expected true error versus the parameter r for different class prior
probabilities, i.e. c e {0.3,0.4,0.5,0.6,0.7}. Similar behavior is
observed, regardless of classiﬁcation rule. There is, however, dif-
ferent sensitivsities of different rules to the sampling ratio r.
Results for the second model with different covariance matrices
are shown in Figure 3(e)—(h). In this case, there is more varied
behavior among the classiﬁcation rules. Note the point in each
ﬁgure where the curves cross. This point corresponds to the mini-
max solution and will be discussed in detail when we analyze the
properties of the curves in a later subsection. For equal covari-
ance matrices, the point is close to 0.5 for all the classiﬁcation
rules; however, it can be far from 0.5 for unequal covarinace
matrices, depending on the classiﬁcation rule.

Figure 3(d)—(p) show results for two real datasets, where the
performance is assessed via holdout error estimation. Each plot
includes the expected holdout error estimate versus r for ﬁve
values of c. In contrast to Figure 3(a)—(h), the curves in Figure
3(d)—(p) are not smooth, which is a result of discrete error esti-
mation. Nonetheless, there still is a crossing point. The solid
vertical lines in Figure 3(i)—(p) are ﬁxed on the initial sampling
ratios.

3.2 Properties of the error curves

The most obvious characteristic of the error curves is that in each
ﬁgure they appear to cross at a single value of r. We will now
examine this phenomenon. We must be careful because the ﬁg-
ures show continuous curves but r is a discrete variable. Hence,
we will have to carefully deﬁne what it means to ‘cross’.

According to the standard deﬁnition, a classiﬁcation rule is
said to be ‘smart’ if the expected value of the error is monoton-
ically decreasing as a function of sample size for all feature-label
distributions (Devroye, 1996). This is in accord with the intuition
(not always correct) that more data cannot hurt classiﬁer design.
We adapt the notion of smartness to the present circumstances
by deﬁning a classiﬁcation rule to be ‘class-wise smart’ relative to
a family of feature-label distributions fc(x, y) = cf(x|0)+
(1 — c)f(x|1), c 6 [0,1], if, for all c 6 [0,1] and r2>r1,E[82|r2]
5 E[82|r1] and E[8,1,|r2] Z E[8,1,|r1]. Intuitively, r2 >r1 means
that there are more data available from class 0 when design-
ing the classiﬁer when conditioning on r2 than when condi-
tioning on r1, so that one would intuitively expect that the
class-0 error when conditioning on r2 is not greater than
when conditioning on r1, which is what is stated by the ﬁrst
inequality. The situation reverses relative to the class-1 error
and that is what is stated by the second inequality. A classiﬁca-
tion rule is ‘strictly class-wise smart’ if r2 >r1 implies
Ei£2|r2l <Ei82|r1l and Ei8ilr2] >Ei8ilr1l-

In Figure 3 we observe that, if cz>cl, then for sufﬁciently
small r, E[en(cz)|r]>E[en(c1)|r], where the notation E[8,,(c)|r]

 

245

112 ﬁhO'spaumoprOJXO'sopchoguroiq/ﬁd11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no 22

M.S.Esfahani and E.R.Dougherty

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

020.3 020.4 020.5 020.6 020.7
(a) (b) (c) (d)
H 0./ 0.1 0.1 0.,
W 0.65 0.65 0.65 0.65
H 0.6 0.6 0.6 0.6
o 055 055 055 055
on 0.5 0.5 0.5 0.5
a 30 45 £0.45 £0.45 E045
a: :9 a: In
H 04 0.4 0.4 0.4
E 0 35 0.35 0.35 0.35
03 0.3 0.3 0.3
+
o 0 25 0.25 0.25 0.25
Q
0.2 0.2 0.2 0.2
0.1 0.2 0.3 0.4 0,5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0,5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 05 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0f5 0.6 0.7 0.8 0.9
(e) (f) (9) (h)

 

 

 

 

0.7 0.7 0. 0.

‘1

0.6 0.6 0. O.

0.

no +121 2 100, 20 75 21
Ei5nl
o o o o
N w A 01
EiEHl
o o o o
Eisnl
I MK
Eienl

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

0. 0.
01 0.1 01 01
0.1 0.2 0.3 0.4 0,5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0,5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 05 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0f5 0.6 0.7 0.8 0.9
i I k I
A () (I) < ) ()
N 0.35 0.35 0.35 o.3=
O
O
N 0.3 - 0.3 0.3 0.3
N'
c 025 - 025 025 025
h . . . .
m
'8 _ 0.2 - _ 0.2 _ 0.2 _ 0.2
{>4 :9 In a: :9
v 0.15 - 0.15 0.15 0.15
H
a.)
(I)
S 0.1 0.1 0.1 0.1
(3
Q 0.05 - 0.05 0.05 0.05
0.1 0.2 0.3 0.4 0,5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0,5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0,5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0,5 0.6 0.7 0.8 0.9
(n) (0) (P)
0.7 0.7 0.7

 

 

 

 

£0.45
:9

 

 

0.

Dataset (Valk et al., 2004)

P P .0 .°

M9009 901.00:

mum-rs ammo:
o

o: 4: 01 a:

o o

M0000

010-301»

0.25

 

 

 

 

 

 

 

 

 

 

 

0.65 0.65

O. 0.6

0.55 0.55

O. 0.5

£0.45 £0.45
0.35 .

 

 

 

 

06.1 0.2 0.3 0.4 05 0.6 0.7 0.8 0.9 011.1 0.2 0.3 0.4 0&5 0.6 0.7 0.8 0.9 013.1 0.2 0.3 0.4 0&5 0.6 0.7 0.8 0.9 013.1 0.2 0.3 0.4 01.5 0.6 0.7 0.8 0.9

3NN 5NN L-SVM RBF-SVM
Fig. 3. The ﬁrst and the second rows show the expected true error rates of four classiﬁcation rules when covariance matrices are identical and unequal,
respectively. In these plots no + n1 = 100 is ﬁxed, where no and n1 are chosen according to the ratio r. The third and the fourth rows show the expected
holdout error estimate of the datasets Weoh et al., 2002) and Walk at al., 2004), respectively, where no = l80r1 and n1 = 80 — no points are randomly
selected from classes 0 and 1, respectively, as the training set. The rest of points are held out for error estimation computed via (9)

indicates that the error is with respect to fc(x, y). To obtain For small r, the preponderance of data for classiﬁer design is in
intuition behind this phenomenon, decompose the error into class 1 with very little data in class 0. Hence, the class-0 error
class-wise errors: tends to be greater than the class-1 error, so that

E[8n(c)lr] = 0E[82|r] + (1 - C)E[8,1.|r]- (10) E[8n(02)lr] - E[8n(61)lr] = (02 - 61)(E[82|r] - EIEilr])>0- (11)

 

246

112 ﬁhO'slcumoprOJXO'soi112u1101uroiq//2d11q 111011 pepcolumoq

910K ‘09 lsnﬁnV no 22

Effect of separate sampling on classification accuracy

 

Analogously, for sufﬁcient large r,
E[8n(62)lr] - E[8n(61)lr]<0- (12)

We shall assume that whatever classiﬁcation rule and feature-
label distribution we are considering, (11) and (12) hold for suf-
ﬁciently small and sufﬁciently large r, respectively.

The next lemma, whose proof is in the Supplementary
Material, states a fundamental property of the error curves.

LEMMA 3.2.1 If a classification rule is strictly class-wise smart
relative to the family {fc(x,y)}, then, for c2>c1, E[8,,(c2)|r]—
E[8,,(c1)|r] is a strictly decreasing function of r.

If we only assume class-wise smart, then E[s,,(c2)|r]—
E[6,,(c1)|r] would only be sure to be a decreasing function of r.

The next lemma, whose proof is in the Supplementary
Material, shows that constraining c results in a corresponding
constraining of the expected error.

LEMMA 3.2.2 Suppose c1<c2<c3. IfE[8,,(c3)|r] 3 E[8,,(c1)|r],
the" Ei£n(c3)|r] Z Ei£n(02)|r] Z Ei£n(cl)|r]- If E[8n(c3)|r] f
Ei£n(cl)|r]a the" E[8n(c3)|r] f E[8n(02)|r] f E[8n(cl)|r]-

To ease notation, we will say that E[s,,(c2)|r] is between
Ei£n(cl)|r] and E[8n(c3)|r]  either E[8n(c3)|r] Z E[8n(02)|r] Z
Ei£n(cl)|r] or E[8n(c3)|r] f E[8n(02)|r] f E[8n(cl)|r]-

The salient proposition concerning the error curves involves a
strictly decreasing function g(r) of r that is positive for sufﬁ-
ciently small values of r and negative for sufﬁciently large
values of r. Since r is a discrete variable in (0,1), we have a
sequence of values 0<r1<  <r,,_1 <1. If g were continuous,
then there would exist a unique value r* such that g(r*) 2 0,
g(r)>0 for r<r*, and g(r)<0 for r>r*. But since g is discrete,
this basic proposition is slightly altered. Rather, there are two
possibilities: (i) there exists a unique value r* = r,- for some value
j such that g(r*) 2 0, g(r) >0 for r<r*, and g(r) <0 for r>r* or
(ii) there is a unique value rj such that g(r)>0 for r 5 r,- and
g(r)<0 for r 3 rj+1. In the second case, we select a
point r* e (rj, rj+1), say, the mid-point, and then we have
g(r)>0 for r<r* and g(r)<0 for r>r*, as in the ﬁrst case.
In the next theorem, whose proof is in the Supplementary
Material, we will be interested in a ‘unique’ point r* 6 (0,1).
For the second case, we interpret this to mean that there is a
unique interval (rj, rj+1) and r* is the selected point in that
interval.

Given the preceding discrete interpretation, we shall say that a
function p(r) ‘crosses’ function g(r) at r* if p(r) Z g(r) for r<r*
and if p(r) 5 q(r) for r>r*.

THEOREM 3.2.3 If a classification rule is strictly class-wise smart
relative to {fc(x, y)}, then there exists a unique point r* such that
for any c2 >c1, E[8,,(c2)|r] crosses E[8,,(c1)|r] at r*.

This is precisely the theorem we want because it means that all
error curves cross at r*.

In the error curves of Figure 3, we observe that r* provides a
minimax value; that is, rmm = r* yields the minimum value of
E[6,,(c)|r] when taking the maximum error over all values of
E[8,,(c)|r] for r 6 (0,1) :

rmm = arg minmax E[6,,(c)|r], (13)

where we must keep in mind that r e R = {r1, ...,r,,_1} is a
discrete variable. The next theorem, proven in the
Supplementary Material, formalizes this observation.

THEOREM 3.2.4 Consider a classification rule that is strictly
class-wise smart relative to {fc(x, y)} and let rmm be the minimax
value deﬁned by (13). If Theorem 3.2.3 yields a unique point
r* 2r], then rmm =rj; otherwise, if Theorem 3.2.3 yields an
interval (rj, rj+1), then either rj or rj+1 is the minimax ratio, deter-
mined by

rj if E182|rjl S E18ilrj+1l

. - (14)
rj+1 If E182|rjl >E[8,1,|rj+1]

rm=i

3.3 Practical implications of the error curves

Recall the practical implications we drew regarding Figure 1 in
the Section 1: (i) if c is known, then do separate sampling with
no 2 cn; where the equal sign means ‘as close to cn as possible’;
(ii) if c w c’, then for small n do separate sampling with no 2 c’n;
(iii) if one has no idea regarding the value of c, then sampling
must be random. Looking at the curves in Figure 3 (and similar
ﬁgures in the Supplementary Material), we see that the curve for
c has its minimum value at r: c or r = c’ w c and, in the latter
case, E[6,,(c)|r] % E[6,,(c’)|r]. Hence, the ﬁrst two recommenda-
tions hold for the other classiﬁcation rules examined.

Going beyond the case where c is known or approximately
known, consider the third implication, where one has no good
idea concerning the value of c. Then the minimax rmm is an
option. Its suitability depends upon the classiﬁcation rule and
feature-label distribution. As we can see from Figure 3, except
for extreme values of c, E[6,,(c)|rmm] tends not to be too much
greater than E[s,,(c)|c]. Of course, there is a practical problem:
while we may well know the classiﬁcation rule, we will not know
the feature-label distribution.

3.4 Algorithm to approximate rInIIl

Algorithm 1 provides an iterative algorithm for approximating
rmm when the feature-label distribution is unknown. The proced-
ure is an empirical illustration of Theorem 3.2.4, which requires
E[6,,(c)|r], which now needs to be approximated to approximate
rm. Algorithm 1 uses holdout error estimation. The expectation
of this error estimate is taken by iterative random sampling from
the dataset. Here we give a brief overview of the algorithm.
The inputs to the algorithm are: dataset denoted by SN, clas-
siﬁcation rule, number of points to be held out for error estima-
tion from classes 0 and 1, denoted, respectively, by ngest, and ntlest
and number of iterations, MaxIters, for computing the expected
holdout error estimate. The maximum number of points after
holding out test sample points is Nnew = N — (n?est + ntlest),
denoted class-wise as N36“, and MW. The algorithm searches
over possible values for r, from 0 to 1, until a stopping criterion
is met. Suppose we ﬁx the total sample size n. Then, considering
the ﬁrst extreme case, r = 0, we need to have at least n points in
class 1 to draw sample points from, randomly, i.e. N1 Z n. On

new

the other hand, when r: 1, we similarly should have NEW 2 n.

Hence, we should have n 5 min{N0 N1 }, whereby we set

. 1 new, new
n = m1n{NgeW, Nnew}.

 

247

112 ﬂJO'sleumoprOJXO'sot112u1101utotq//2d11q 111011 pepeolumoq

910K ‘09 lsnﬁnV no 22

M.S.Esfahani and E.R.Dougherty

 

The algorithm’s search criterion is based on Theorem 3.2.4: in
a ‘while loop’ over an increasing sequence of the ratios r, the
algorithm computes the estimated slope of the expected error (as
a function of c), this being slopenew = E[§2 |r] — E[§,11|r] (line 22 of
the algorithm), obtained by plugging the error estimates (lines
7—21 of the algorithm) into the unknown slope formula
E[62|r] — E[6,1,|r]. Because the classiﬁcation rule is strictly class-
wise smart, for sufﬁciently small r, the slope is positive, and it
becomes negative for sufﬁciently large r (refer to Supplementary
Material ﬁle for further explanation). Once a point is reached at
which the sign of the slope becomes non-positive, the ‘while loop’
stops increasing r. Thereafter, the three different possibilities
given by Theorem 3.2.4 are checked, in lines 26—32, and ﬁnally
a single rmm is returned. Although the returned minimax ratio is
only computed for sample size n deﬁned above, the class-sizes
can still be conservatively adjusted per rmm in the dataset SN
because, for a ratio given by the algorithm, if one increases the
sample size, then in the worst case the error is as large as the
minimax value returned by the algorithm.

 

Algorithm 1 Iterative algorithm to approximate rmm (an implementation

of Theorem 3.2.4 using an estimate of the expected error estimate)
Input: Dataset SN, Classiﬁcation rule III, n96“, néest, MaxIters

2: Output: rmm

3: Deﬁne: N36,, 2 N0 — 12965,, N11“,W = N1 — nlest

n =min{N0 N1 }

new ’ new

 

 

4 Initialize: j = 0, r = 0, slopenew = 1, sign 2 1

5 while sign > 0 do

6: Set: a <— E[§2|r], slopeold <— slopenew, r <—%

7 Reset: Epgm = 0, E[§,1,|r] = 0

8 for i = 1 to MaxIters do

9 52:0 <— ngest randomly drawn points from SR,
10:  <— néest randomly drawn points from So
u: nemwml

12:  «8919222231... «5292:2211

13: SNnew <— Sgﬂw U Sjvm

14: 52 <— rn randomly drawn points from 59%”
15: S}, <— (1 — r)n randomly drawn points from Sjvnew
16: Sn <— 82 U S},

17: 10,, <— 111(5),)

18: Compute 52,12}, of 10,, using 

19: Add 52, and 5,1, to E[§2|r] and E[§,1,|r], respectively
20: end for

21: Eiéilri «£2515... Eiéiiri «$5112

22: slopenew <— E[§g|r] — E[§,1,|r]

23: sign <— slopenewslopeold

24: j <— j + 1

25: end while

26: if sign 2 0 then

27: rmm <— r

28: else if a < E[§,1,|r] then
29: rmm <— r — i
30: else

31: rmm <— r
32: end if
33: return r111m

 

3.5 Adjusting sample sizes

Consider the common situation in which no and n1 have been
determined beforehand, but suppose one knows c. The curves of
Figure 3 still apply but we are not free to choose no and n1, so
that we cannot choose no 2 cn. Nevertheless, we desire the train-
ing data to be apportioned according to c and we want to use as
much data as is possible. These conditions mean that for training
we want class sample sizes mo and m1 such that m 2 mo + m1 is
maximized given the constraints mo 2 cm, m1 = (1 — c)m,
mo 5 no, and m1 5 m. The solution is to let m = [min "7", 
To see the effect of adjusting sample sizes, we consider the
difference, A(r, c) = E[6,,(c)|r] — E[sm(c)|c], between the ex-
pected true errors of two cases, n being the original sample size
and m the adjusted sample size. When the sampling ratio is r and
the true prior probability is c, A(r, c) can be interpreted as the
penalty incurred. Figure 4 shows A(r, c) for L-SVM and RBF-
SVM for the equal covariance model described in Table 1. The
result for the case with unequal covariance matrices can be found
in the Supplementary Material. The two parameters r and c take
values from 0.06 to 0.94 with the step size of 0.04. As expected,
as |r — c| increases, A(r, c) signiﬁcantly increases. When
r w c, A(r, c) w 0, which is always the minimum. The ﬁgure
shows that except when r is very close to c, A(r, c) > 0, meaning
that, even though m<n, a correct sampling ratio more than
compensates for the loss of data due to subsampling.

3.6 Population-based minimax theory

The minimax value in the error curves of Figure 3 depends on the
sampling distribution and results from the fact that E[6,,(c)|r] is
minimized over c for a single value r*. In Anderson (1951), a
population-based minimax approach was taken to arrive at a
‘best’ choice for 8 in (1) in the Gaussian model with common
covariance matrix under separate sampling. Here we extend the
population-based mimimax approach to arrive at much more
general solution than that given by Anderson. It is based upon
the fact that the Bayes classiﬁer can be determined via a discrim-
inant involving the class-conditional densities. Anderson also
utilized the Bayes classiﬁer in his analysis but he restricted it to
the Gaussian model with common covariance matrix, in which
case the Bayes classiﬁer is given by LDA using the actual par-
ameters rather than their estimates as in (1).

Given the class-conditional distributions and prior probabil-
ities, the Bayes classiﬁer, wBayeS, is determined by the
discriminant

 

 

f(XI0) 1 - c
— lo ,
f(Xl1) g c
Where WBayes(X) = 1 if DBayes(X) 5 O and WBayes(X) = 0 if
DBayes(x) >0. The regions assigned to the two classes are
R1 = {x : DBayes(X) 5 0} and R0 = {x : DBayes(X)>0}. If c is un-
known and replaced by 8, then the discriminant becomes

D Bayes (X) = log (15)

 

248

112 ﬂJO'sleumoprOJXO'soi112u1101uioiq//2d11q 111011 pepeolumoq

910K ‘09 lsnﬁnV no 22

Effect of separate sampling on classification accuracy

 

 

L-SVM

 

RBF-SVM

Fig. 4. The parameter A(r, c) for classiﬁcation rules, L-SVM and RBF-
SVM trained on the sample data with size n = 100 from the common
covariance matrix model described in Table 2

 

 

f(Xl0) 1 - 5

— 10 A ,
f(Xl1) g c
which deﬁnes the classiﬁer 105, with corresponding class regions
R1(E) = {x : Dpﬁor(x) 5 0} and Ro(8) = {X : Dprior(x)>0}. The
error associated with we is

e(a, c) = ce°(a) + (1 — c)e1(a), (17)

Dprior(X) = log 

where

8y(5) = f(le)an (18)
R1—y(5)
for y e {0, 1}. R1(E) and Ro(8) are strictly increasing and decreas-
ing, respectively, for increasing values of loglge . Hence, if the
conditional densities are strictly positive, then 61(8) and 60(6) are
strictly decreasing and increasing, respectively, for increasing
values of loglff.

The minimax choice selects the value of 8 that yields the min-
imum value of the error 6(8, c) when taking the maximum error

over all values of c e (0, 1] :

 

8mm = arg min max 6(6, c). (19)
c C

We state a lemma and a theorem, whose proofs are given in the
Supplementary Material that can be used to ﬁnd minimax
solutions.

LEMMA 3.6.1 If the class-conditional distributions are strictly
positive and 8y(5),y = 0,1, is a continuous function of a, then
there exists a unique point 8”“ such that 80(8m) = 81(6m) and
this point corresponds to the minimax solution defined in (19).

THEOREM 3.6.2 If the class-conditional distributions are strictly
positive and 821(6), y = 0,1, is a continuous function of a, then
the minimax solution for the discriminant in (16) is the value of c

 

 

 

 

 

0.3
 ' — — ~ ‘a -
“’
’\
I f \‘ \
I 1 '
O 2 ’ I~ \\ ‘K
’ .’ ‘ ’1
m r , \ .
g l I‘ \ \
e 0.15 ' I X ‘
m ~I \
Q) I \
,‘ \
0.1 - I ‘
~I~ 
~I \
, \
0.05- ,‘ 20 2 21 = 0,412 \ '
- — -20 = 0412,21 = 1-6I2
.—-—120 = 1.6I2,21 = 0.412
0 I . .

 

 

 

 

0 0.2 0.4 0.6 0.8 1
C

Fig. 5. Bayes error as a function of class prior probability c for different
settings with multivariate Gaussian distributions with no 2 (0.3, 0.3) and

that gives rise to the maximum Bayes error for the discrimination
problem of (15).

To apply the lemma to the Gaussian model with common
covariance matrix, note that the discriminant takes the form

 

 

 

 

Dpﬁoxx) = (x — win—10. — m) — In T A (20)
and the error of the classiﬁer induced by Dpﬁor is given by
8(636) = C") _W2_1(ﬂ0 — #1) —1Ogl%e
\/(ﬂ0 — Mfg—1010 — #1)
so“) (21)
+(1_ CM) @2400 -m) 40121? ,

 

 

\/(ﬂ0 — Mfg—1010 — #1)

87(6)

k

where q) is the standard normal cumulative distribution function.
It is immediate that for 8 = 0.5, 60(8) 2 61(6). Hence, 8mm 2 0.5,
which is precisely Anderson’s result for this special case.

To illustrate the effect of the underlying feature-label distribu-
tion on am, we consider a situation similar to that used for
Figure 1, except that we allow unequal covariance matrices.
Figure 5 contains Bayes-error curves as functions of c for differ-
ent covariance models. It shows that, except for a common
covariance matrix, 8mm 75 0.5, 8“““ being the value of c at
which the curve attains its maximum. The curve for equal co-
variance matrices is constructed analytically; for other values,
MC simulation is employed.

Obviously, if 8mm 2 c, then the minimax value will perform
well. But what happens when 8”“ 75 c? In fact, the minimax
value can work well so long at it is close to the true value,
how close depending on the particulars of the problem. For a
Gaussian model with common covariance matrix, as used for
Figure 1, we consider LDA with random sampling under three
scenarios: (i) known c, (ii) minimax 8mm and (iii) the maximum-
likelihood estimate 6”” =  Figure 6 shows the expected errors
(MC estimates) as a function of c for n = 20 and 80. In all cases,
known c is the best. When the sample is small, 8mm outperforms

 

249

112 ﬂm'sleumoprOJXO's01112u1101u101q//2d11q 111011 pepeolumoq

910K ‘09 lsnﬁnV no 22

M.S.Esfahani and E.R.Dougherty

 

 

 

 

 

 

 

 

 

 

 

 

0.34 0.34
(a) _ _ (b)
I l T ‘ ‘
1 ‘1‘ ‘, \
0.3 ' I I ‘.‘ ' 0.3 — - .
I I‘ \\ I, ' M \
I" \' ‘ I" ‘1:
I, \J ,1 1‘s
I~ ‘ I r
l—S' "I '\‘\ '—§' 1" \}\\
£ 0 26 ,’,~’ 0 H 0.26 .N ‘1
m Ii ‘3‘ :73 i ‘2
{I~ ‘_\ f ‘3
L’, \\ [I ‘1
l.’ ‘.‘ I 
0.22 a; t,- o.22 i 1
,7 —Anderson (6 2 6mm 2 0.5) ‘1 —Anderson ((3 2 (3mm 2 0.5)
- - 'LDA with 1;: am , - - 'LDA with e = 6”“ ‘1‘
i---ILDA withc=c(known c) r --I-1LDA withé=c(kn0wn c) 1

 

 

 

 

 

 

018.2 0.3 0:4 035 0.6 0.7 0.8 °"‘0.2 0.3 0:4 05 0.6 0.7 0.8
n = 20 n = 80
Fig. 6. Expected true error for different scenarios with random

sampling (ﬁxed no + n1 with random no and n1) for the same model as
used in Fig. 1

81111 for a fairly wide range of c, but this advantage disappears
rapidly as n grows. The reason for this behavior is the difﬁculty
of estimating c by 8”” for small samples. All curves show that
when c is large or small the minimax solution gives poor results.
Let us close this section by noting that the Bayes classiﬁer is
intrinsic to the feature-label distribution and, since the minimax
choice depends only on the form of the Bayes classiﬁer, it is
independent of any particular classiﬁcation rule.

3.7 Concluding remarks

We have shown, via simulations on both synthetic and real ex-
amples, that separate sampling with an inappropriate sampling
ratio can signiﬁcantly degrade classiﬁcation accuracy for classi-
ﬁcation rules that do not use an explicit estimate of the prior
probability. We have demonstrated some fundamental properties
of the expected-error curves, developed the minimax sample-
based theory for those curves, proposed an algorithm to approxi-
mate the minimax value in practice and extended the classical
Anderson minimax theory for prior probabilities. We have pro-
vided heuristics on how to proceed when the prior probability is
known (or known within a small range) and we have proposed a
subsampling methodology to implement these heuristics when
the class sample sizes are predetermined. Given the ubiquity of

separate sampling in biomedicine, it would behoove the medical
community to record incidence rates of patient sub-types (popu-
lation statistics), so that very accurate estimates of class prior
probabilities would be available. While this would certainly
incur some cost, that cost would be minuscule compared to the
costs incurred by the irreproducibility of classiﬁcation studies.

Conflict of Interest: none declared.

REFERENCES

Anderson,T.W. (1951) Classification by multivariate analysis. Psychometrika, 16,
31—50.

Chang,C.-C. and Lin,C.-J. (2011) LIBSVM: A library for support vector machines.
ACM Transact. Intell. Syst. T echnol., 2, 1—27.

Desmedt,C. et al. (2007) Strong time dependence of the 76-gene prognostic signa-
ture for node-negative breast cancer patients in the transbig multicenter inde-
pendent validation series. Clin. Cancer Res., 13, 3207—3214.

Devroye,L. (1996) A Probabilistic Theory of Pattern Recognition. Vol. 31, Springer,
New York.

Doughtery,E.R. et al. (2007) Validation of computational methods in genomics.
Curr. Genom., 8, 1.

Duda,R.O. et al. (2001) Pattern Classification. John Wiley, New York.

Hua,J. et al. (2005) Optimal number of features as a function of sample size for
various classiﬁcation rules. Bioinformatics, 21, 1509—1515.

Mortazavi,A. et al. (2008) Mapping and quantifying mammalian transcriptomes by
rna-seq. Nat. Methods, 5, 621—628.

Ray,T. (2011) FDA’s Woodcock says personalized drug development entering
‘long slog’ phase. Pharmacogen. Rep., http://www.genomeweb.com/mdx/
fdaswoodcock-says-personalized—drug-development—entering-long-slog—phase
(26 October 2011, date last accessed).

Shmulevich,I. and Dougherty,E.R. (2007) Genomic Signal Processing (Princeton
Series in Applied Mathematics). Princeton University Press, Princeton,
New Jersey.

Valk,P.J. et al. (2004) Prognostically useful gene-expression proﬁles in acute myeloid
leukemia. New England J. Med, 350, 1617—1628.

Wang,Z. et al. (2009) Rna-seq: a revolutionary tool for transcriptomics. Nat. Rev.
Genet, 10, 57—63.

Yeoh,E.-J. et al. (2002) Classiﬁcation, subtype discovery, and prediction of outcome
in pediatric acute lymphoblastic leukemia by gene expression proﬁling. Cancer
cell, 1, 133—144.

Youseﬁ,M.R. and Dougherty,E.R. (2012) Performance reproducibility index for
classiﬁcation. Bioinformatics, 28, 2824—2833.

Zhan,F. et al. (2006) The molecular classiﬁcation of multiple myeloma. Blood, 108,
2020—2028.

 

250

112 ﬂm'sleumoprOJXO's01112u1101u101q//2d11q 111011 pepeolumoq

910K ‘09 lsnﬁnV no 22

