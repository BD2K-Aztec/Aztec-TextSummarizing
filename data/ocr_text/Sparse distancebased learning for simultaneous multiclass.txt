ORIGINAL PAPER

Vol. 27 no. 23 2011, pages 3242-3249
doi: 10. 1093/bioinformatics/btr54 7

 

Sequence analysis

Advance Access publication October 7, 2011

Sparse distance-based learning for simultaneous multiclass
classification and feature selection of metagenomic data
Zhenqiu Liu1’2’*, William HsiaoZ, Brandi L. Cantarelz, Elliott Franco Dr bel<2

and Claire Fraser-Liggett2

1Department of Epidemiology and Public Health, University of Maryland Greenebaum Cancer Center and 2Institute
for Genome Sciences, University of Maryland School of Medicine, Baltimore, MD 21201, USA

Associate Editor: Alfonso Valencia

 

ABSTRACT

Motivation: Direct sequencing of microbes in human ecosystems
(the human microbiome) has complemented single genome
cultivation and sequencing to understand and explore the impact of
commensal microbes on human health. As sequencing technologies
improve and costs decline, the sophistication of data has outgrown
available computational methods. While several existing machine
learning methods have been adapted for analyzing microbiome data
recently, there is not yet an efficient and dedicated algorithm available
for multiclass classification of human microbiota.

Results: By combining instance-based and model-based learning,
we propose a novel sparse distance-based learning method
for simultaneous class prediction and feature (variable or taxa,
which is used interchangeably) selection from multiple treatment
populations on the basis of 16S rRNA sequence count data. Our
proposed method simultaneously minimizes the intraclass distance
and maximizes the interclass distance with many fewer estimated
parameters than other methods. It is very efficient for problems with
small sample sizes and unbalanced classes, which are common in
metagenomic studies. We implemented this method in a MATLAB
toolbox called MetaDistance. We also propose several approaches
for data normalization and variance stabilization transformation
in MetaDistance. We validate this method on several real and
simulated 16S rRNA datasets to show that it outperforms existing
methods for classifying metagenomic data. This article is the first
to address simultaneous multifeature selection and class prediction
with metagenomic count data.

Availability: The MATLAB toolbox is freely available online at
http://metadistance.igs.umaryland.edu/.

Contact: zliu@umm.edu

Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on April 4, 2011; revised on August 5, 2011; accepted on
September 28, 2011

1 INTRODUCTION

The human body is inhabited by on the order of 1014 bacteria,
collectively known as the human microbiota, which contains 100
times more genes (the microbiome) than in the human genome.

 

*To whom correspondence should be addressed.

Since these microbes interact with our bodies and provide functions
lacking in our genome, changes in the microbial community
structure are thought to impact our health. Given the vast number
of genes in the microbiome and our inability to sequence all
of them, a marker gene is often used for comparison among
samples. The 16S rRNA gene is the most common marker gene
used since it is universally present and well conserved among
prokaryotes. Sequencing of 16S rRNA in an environment containing
a mixed population allows the surveying of community structure
without biases from culture—based methods at a relatively low
cost. Whole genome shotgun (WGS) sequencing of the community
(a metagenome), on the other hand, can provide estimates of
functional capabilities of microbiome (Turnbaugh et al., 2007), but
the cost is substantially higher. A main promise of metagenomics
is that it will accelerate the discovery of novel genes and new
drug target and provide new insights into diseases with unknown
etiologies (Qin et al., 2010; Wooley et al., 2010).

The ﬁrst step of 16S rRNA metagenomic analysis usually
involves the classiﬁcation of sequences by organism to reduce
the dimensionality of the dataset (from millions of sequences to
thousands of organisms). In the process, the number of sequences
classiﬁed to each organism is kept to provide an estimate on
organism abundance. Classiﬁcation of sequences are done by
comparing sequences from a sample to 16S rRNA from known taxa
or by clustering of similar sequences into operational taxonomic
unit (OTU), which represent an unnamed taxon. The end result
is a series of sequence read counts associated with taxa in the
sample. A popular software package for assigning 16S rRNA
to known taxa based on k—mer frequencies is called ribosomal
database project (RDP) Classiﬁer (Wang et al., 2007). In addition
to sequence classiﬁcation, software such as Mothur (Schloss
et al., 2009) and QIIME (Caporaso et al., 2010; Lozupone and
Knights 2005) provides diversity metrics and sample comparison
statistics allowing comparison of microbiome proﬁles using alpha
(within—community) and beta (across—communities) diversities. For
graphical comparison, MEGAN (Huson et al., 2007) can compare
the OTU composition of frequency—normalized samples (Mitra
et al., 2009; Huson et al., 2009). From these analyses, we can
ﬁgure out what organisms are in each of the samples or classes
of samples. While these tools can be used to compare and cluster
samples based on microbiome proﬁles, they do not allow for
the identiﬁcation of differentially abundant microbes in samples.
Therefore, the important question of which organisms (by virtue
of their presence/absence or relative abundance) distinguish one

 

3242 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /§JO'SIBUJn0[pJOJXO'SOllBIHJOJUTOTQ/ﬁdnq mm; pepBOIUAAOG

9IOZ ‘091sn8nv uo ::

Sparse distance-based learning for metagenomic data

 

sample class from another cannot be answered by these software
packages. This can be done using MetaStats (White et al., 2009),
but this software can neither identify multiple differentially abundant
microbes simultaneously nor classify samples into multiple classes.
MetaStats also suffers the multiplicity problem with multiple tests.
Knights et al., (2010) applied some existing machine learning
approaches to the classiﬁcation of microbiome data, but novel and
efﬁcient software is not yet available for multiclass classiﬁcation of
microbiome count data using supervised learning methods.

Machine learning for multiclass (and more general multilabel)
classiﬁcation has been applied to microarray analysis, text mining
and image identiﬁcation (Allwein et al., 2001; Crammer and
Singer 2001; Vens and Struyf 2008; Xu et al., 2010; Liu et al.,
2010). The main objective of supervised learning is to predict
the class of a future sample given the class and metagenomic
count data. Most supervised learning methods fall into two general
categories: instance—based and model—based learning. Instance—
based learning (IBL), such as k—nearest neighbor (KNN) (Zhang
and Zhou 2007), predicts the class of a sample with unknown
class by considering the classes of k—nearest neighbors. It is more
robust for data with unbalanced classes and is efﬁcient for multiclass
classiﬁcation with a small number of features. However, accuracy
diminishes with increasing irrelevant features because of the curse of
dimensionality. On the other hand, model—based learning methods,
such as support vector machine (SVM) and logistic regression,
are mainly designed for binary classiﬁcation. They are designed to
separate two different classes as far as possible without considering
the intraclass distances. Multiclass problems are often handled by
combining binary classiﬁer outputs, such as one class against the
other (one versus one) or one class against the rest (one versus
rest). However, when sample sizes are small, accuracy is reduced
potentially due to noise and overﬁtting can occur since a high
number of parameters needed to be estimated from a small number
of samples [either C(C— 1)n/ 2 or (6— Dr; parameters needed to be
estimated with 6 classes and n features]. Furthermore, these methods
also create unbalanced classiﬁcation problems with the one versus
rest rule even if the original dataset is balanced. Instance—based
learning only takes into account the minimal distance, while model—
based learning incorporates maximizing the interclass distances
(e. g. maximizing the margin in SVM).

The integration of instance—based and model—based methods can
maximize the interclass distances while minimizing the intraclass
distances. Current integration (Cheng and Hullermeier 2009) only
considers the labels of neighborhood instances as additional
features for logistic regression, without utilizing the robustness
of instance—based learning for unbalanced classes. Moreover,
this method estimates many parameters and creates unbalanced
classes in multi—class classiﬁcations, even if the original dataset is
balanced. Because of the common issues associated with clinical
samples: (i) small sample size and (ii) unbalanced classes, we
propose a novel approach for multiclass classiﬁcation through
integrating instance—based and model—based learning to overcome
these challenges in metagenomic data. Our proposed approach
combines the KNN and SVM to simultaneously maximize the
interclass distance and minimize the intraclass distance. This
approach is robust for unbalanced classiﬁcation, can classify
multiple classes simultaneously without creating unbalanced classes
and perform simultaneous feature (variable) selection and multiclass
prediction with a simple parameter regulation, while estimating

fewer parameters than previous approaches (only the same as the
number of features). We apply our approach to 16S rRNA count
data from metagenomic samples to select microbial taxa (features)
that can distinguish one class of samples from others. The number
of microbial taxa (features) is determined through cross—validation
with smallest prediction error. We then use the selected features
to build a weighted KNN classiﬁer to predict a class for each
sample. Because the dependence of the variance of the metagenomic
count data for each taxon on the abundance of the taxa violates the
homogeneity of variance assumption required for the application
of many statistical methods, we developed variance stabilization
methods to make non—homoskedastic count data easily tractable
by standard machine learning methods. The current widely used
data normalization method with proportion (relative abundance)
only accounts for different levels of sampling across multiple
individuals without adjusting for differences in variance. In this
article, we describe several data normalization methods for variance
stabilization before applying our proposed classiﬁcation method. We
evaluate the performance of our tool (MetaDistance) using simulated
datasets and two publicly available real metagenomic datasets.
The proposed methods are robust for all datasets and efﬁcient for
microbial feature identiﬁcation and sample phenotype prediction.

2 METHODS

To understand the association between microbiota proﬁles and clinical
phenotypes such as obesity, it is crucial to develop new supervised learning
tools. We assume there are two or more populations with different clinical
phenotypes (e. g. obese and lean, or different treatments and controls), each
having multiple samples. We assume a set of non—overlapping taxa has been
chosen, e. g. all genus—level groups appearing in the data. For each sample, we
have one metagenomic count feature for each taxon, indicating the number
of 16S rRNA sequence reads from the given sample assigned to that taxon,
as shown in the following:

9511 x12  xlm yl

x21 x22  x2m 3’2
X: . . , . , and y: . ,

xnl x112  xnm yn

where X is the metagenomic count matrix with n samples and m features, xi]-
denotes the total number of reads assigned to feature j in sample i, and y is
the clinical phenotypes with g categories. y,- e C={C1,...,Cg}. Our goals are
to identify features whose abundance in different populations is different,
and to estimate the power of those identiﬁed features in predicting clinical
phenotypes.

2.1 Data normalization and transformation

There are two sources of bias in the metagenomic count data: (i) different
levels of reads (sampling) across multiple samples and (ii) dependence
of the variance of xi]- on its particular value. The large the count value,
the larger the variance. Validity of many statistical procedures relies upon
the assumptions of normal distribution and homogeneity of variances.
However, the metagenomic count and related percentage data have variances
that are functions of the mean and are not normally distributed but
instead are described by Poisson, binomial, negative binomial or other
discrete distributions. The variance heterogeneity and non—normality of the
metagenomic count data can seriously increase either Type I or 11 error and
make the statistical inferences invalid (Kim and Taylor 1996; Kasuya 2004).
Therefore, it is crucial to transform the count and percentage data prior
to any standard analysis in order to correct deﬁciencies in normality and

 

3243

112 /§.IO'S[BU.ITIOIP.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

Z.Liu et aI.

 

homogeneity of variance (Freeman and Tukey 1950; Foi 2009; Olivier 2010).
Our method for variance—stabilizing transformation and data normalization
consists of two steps:

( 1) Converting the raw abundance measure to a proportion (percentage)
representing the relative contribution of each feature to each sample.
This is to adjust for the sampling depth (read count) differences
across samples. Mathematically, we normalize the metagenomic
count matrix X into a proportion matrix P with

xi].
P:[plj]nXMa Where  m—.
ijlxij
(2) We then employ either the square root transformation or the arcsine

transformation to the metagenomic proportion matrix P (or original

count matrix X):

0 Square root transformation: This can be used either with the
proportion matrix P or the original count matrix X, the transformed
feature matrix Z = [zij]nxm with

1
Zij= pz‘j‘I‘E Of Zij= xz‘j‘I‘E

- Arcsine transformation: This is well suited for metagenomic
proportion data P with

Z = [ZijIrz x m 

zij = arcsin(\/p—,-j).

This is very similar to the arcsine transformation with original count
data X (Laubscher, 1961), which deﬁned as

zl-j =x/Zarcsin‘ / % +x/L—1arcsin 1%,

where L = max(X) + 4 is the largest count value in count matrix X
plus a constant number 4.

Before we do any transformations, we will compute the mean and variance
for each sample with matrix P or X, and then test the assumption of
homogeneity of variances with Bartlett’s test (Nagarsenker, 1984). Either
the square root or arcsine transformation will be used. Practically, if the
percentage data have homogeneous variances, no transformation is needed.
For data with variance heterogeneity, if the data lie in the range of 0—0.3 or
0.7—1 but not both, the square root transformation should be used. Otherwise,
the arcsine transformation should be used. In most cases, we ﬁnd both
transformations increase predictive power and have similar performance. In
this article, we therefore utilize the arcsine transformation with proportion
data for all of our experiments.

2.2 Sparse-weighted distance learning with integrated
KNN and SVM

A general multiclass classiﬁcation problem may be simply
described as follows. Given n samples, with normalized features,
D: {(Z1,y1), ...,(zn,yn)}, where z,- is a multidimensional feature vector
with dimension m and g classes with class label yl- e C = {C1, ...,cg}, ﬁnd a
classiﬁer f (2) such that for any normalized feature vector 2 with class label
y, f (z) predict class y correctly. Given two samples 2,- and Zj, we introduce
a general weighted distance functions for KNN as follows:

D(W,Zi,ZjaP)=W1|Zi1 _Zj1|p+"'+Wm|Zim —ij|p
m
=Zw.lz,-r—zjrlp=wtlz.-—szP, (1)
r=l

where H denotes the absolute value, wk 2 0 for k = 1, ...,m are the non—
negative weights and p is a positive free parameter. In particular, when p = 1
and p = 2, D(W, z), zj, 1) and D(w, z), Zj, 2) represent the weighted city—block
and Euclidean distances between 2,- and Zj, respectively. Given a new sample

2;, we calculate KNN of z; denoted by Nk(zl, cs) for each class cs, and then
take the average distance

ZzieNk(zlacs){D(wa Zla Ziap)}

k 7
as the distance of z; to class cs. Finally, we assign z; to a class cj by means
of a minimal distance vote.

D(Zl7CS):

 

371 =arg min{D(ZlaCj)}-
CjEC

2.3 Efﬁcient quadratic SVM method for weight
estimation

Now, the problem left is how to ﬁnd optimal w for high—dimensional
metagenomic data. As we discuss earlier, we want to choose W with
small intraclass distance and large interclass distances simultaneously and
automatically identify the features relevant to the phenotypes. We, therefore,
propose an efﬁcient quadratic SVM for weight estimations as follows:

rvrji§ZZ(sg-)2+ZZ@3)Z+AZW
’ i j i j k=1

s.t.wT(Iz.-—zj|{’):1+a: Vyi,yjec.,&zjeN/.(z,-,c.>

a
l]?

wTuzi—erozz—sgwyz-ec.,yjec.,&ssét (2)

53.30, 5330, and WkZO,Viajaka

where |z,- —Zj|.p =[(z,-1 —Zj1)p,..., (zim —ij)p]T is an element—wise
operation, and A, k and p will be determined through cross—validation. In
Equation (2), the ﬁrst constraint represents the KNN intraclass distances,
and we restrict them to a soft upper bound 1. The second constraint indicates
the interclass distances with a soft lower bound 2. Hence, we can enforce a
soft margin 1 between the intraclass and interclass distances. Therefore, the
solution of Equation (2) will guarantee a small KNN intraclass distance and
large interclass distance simultaneously. Finally, the reason we used KNN
instead of all the samples in the same class for the ﬁrst constraint is that
samples in one class may have multimodal distributions. It is too stringent
and unrealistic to require that all samples in one class have small distances.
Equation (2) is equivalent to the following problem:

'E: I WTz-—z-1”—12 
 224 I; ll. )+

i€cs,j€cs
m
+ ZZao—whzi—zjr’fHZWk (3)
iecs,j€ct k=1
s.t. wkZO,Vk=1,...,m,

1 ﬁwﬂm—qw>1

where 1,, = : V 371‘an E Cs, and Ib =

0 otherwise.

1 ﬁwﬂg—qw<2 . .

. ‘ V yl- Ecs, and yj EC). Equatron (3) 1s a much
0 otherwrse.
simpler truncated quadratic programming with non—negative constraints. It
can be solved very efﬁciently, even if the problem has both large sample
size and high dimension. The ﬁrst—order derivative for Equation (3) is as

follows:

8E T
B_Wk =ZZIa(w IZi—Zjl{)_1)lxik —xjk|p —---
ZECSJECS
_ Z ZIbQ—Wlei —z,-|{’)|x.-k —xjk|p+)‘ (4)
iecs,j€c,

Based on Equation (4) and wk 2 0, we implement a standard conjugate
gradient method (Hager and Zhang, 2006) with non—negative constraints in
M etaDistance. Because E is a convex optimization with a convex constraint,

 

3244

112 /B.IO'SIBUJnOIpJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse distance-based learning for metagenomic data

 

a global optimal solution is guaranteed theoretically. The global minimum of
E is reached when each element of Wk satisﬁes one of two conditions: either
(i) Wk > 0 and (BE/aw)“, =0 or (ii), Wk =0 and (ElE/Eiwkmv Z 0. In the ﬁrst
case, the feature is identiﬁed as important by receiving a positive weight
while the corresponding term in the gradient reaches zero. In the second case,
the feature is eliminated as the corresponding term in the gradient remains
positive even when Wk reaches zero, at the edge of the feasible region. Letting
g(w)= W , we have the following iterative gradient algorithm for E
maximization and optimal weight estimation:

Algorithm for optimal weight estimation: given p, k, )t and 6210—6,
initializing w1=(wi,w;,...,w,1,)T with unweighted w,1=1, for k = 1, 
Update W

0 Wt+1=Wt+Oltdt, where t: the number of iterations and at: the step
size.

0 dt is updated with the conjugate gradient method:

[g(WI)—g(WI‘1)ng(WI)
g(Wt‘1)Tg(Wt‘1)

Stop when th+1—WII<E or each element Wk satisﬁes the following

two conditions: either (i) Wk >0 and (BE/awkNWZO or (ii), Wk=0 and
(BE/BWk)lw :0.

 

dt=g(wt)+utdt and at:

2.3.] Evaluation criteria and choices of parameters The performance
of Metadistance for multiclass classiﬁcation is mainly evaluated with the
prediction (test) error. The small the prediction error, the better the prediction
accuracy. The average area under the ROC curve (AUC) is also used as a
performance measure. AUC is equal to the probability that a classiﬁer will
rank a randomly chosen positive instance higher than a randomly chosen
negative one. The large the AUC, the better the model performance. AUC
for each class versus the rest is calculated with the KNN distance between
the test data and each class. Intuitively, if a test sample is from that training
class, their distance will be small. The average prediction AUC is then used
as a measure of overall model performance.

The free parameters A, k and p are also determined by cross—validation with
the smallest prediction error. The regular parameter A controls the sparsity
of the model. The larger the value of A, the fewer microbial features will be
selected. If )t is too small, there will be overﬁtting and little sparsity. If )t is
too large, the produced classiﬁer will be very sparse (very small number of
features with non—zero weight) but have poor predictiveness. The optimal )t
and the number of predictive features (variables) with non—zero weights are
determined with smallest prediction error through 10—fold cross—validation.
The parameters p and the number of nearest neighbors k are also decided by
cross—validation. We limit k: 1,2, ...,min n), where min n,- is the smallest
sample size for one class. Our computational experiments with simulation

and real data show k is in the range 5—25 for the best performance. For
simplicity, we choose p=1 or 2 only in all the computational experiments,
but other choices of p do improve the predictive power of our method. Users
should feel free to choose different P values in their computations. Quadratic
SVMs are implemented in the MetaDistance software.

3 RESULTS

Simulation Data: in silico metagenomic datasets were generated
to contain ﬁve classes (groups) in four samples sizes (10, 20, 50 and
unbalanced sample size with 10, 20, 30, 40 and 50 for each class,
respectively). Datasets (xlj) were generated from negative binomial
(NB) distributions with different means and dispersion parameters.
The means for NB are simulated from the Gamma distribution with
a mean (,u.) of 100 and variance (02) of 1000. The variance of NB
is 01%,3 = u —l— 11.2 / scale, where scale: 1. We simulated 1000 features
for each sample from NB distributions, which contained the ﬁrst 10
relevant features having different distributions with distinguished
,us. We used 2—fold cross—validation to evaluate the method. First,
we normalized the data with proportion and arcsin transformation,
and then divided the data into training and test equal subsets. The
training subset was used for model construction, while the test subset
was used to evaluate performance. The model parameters k, p and
)1 are determined from only the training data with leave—one—out
cross—validation. Each simulation was performed 100 times for each
sample size (Table 1).

MetaDistance can identify differentiated features with high
accuracy even if the sample size is small or unbalanced (Table 1). As
the sample size increases so does frequency of correctly identiﬁed
features. At 10 samples per class, MetaDistance identiﬁes 80%
of relevant features with over 72% accuracy and 30% of features
with over 92% accuracy compared with 50% features with over
93% accuracy when there are 20 samples per class. Increasing the
sample size to 50 leads to identiﬁcation of 100% of relevant features
over 93% accuracy. Our method performs well even if the dataset
has highly unbalanced sample size. The method is able to identify
40% of relevant features accurately in all experiments with both
sample size of 50 and unbalanced. The average number of features
identiﬁed increases with the sample size for each class (Table 1).
For example, a sample size of 50 for each class identiﬁes 9.87
(of 10) relevant features on average. For comparison purpose, we

Table 1. Frequencies of correctly identiﬁed features with different sample sizes

 

 

Sample size/per—class 10 20 50 Unbalanced
parameters (2*, k*, p*) (5, 6, 1) (20, 5, 1) (100, 20, 1) (95, 5, 1)
W1 72 60 100 100

W2 73 93 100 99

W3 31 18 93 91

W4 38 10 93 98

W5 92 98 100 100

W6 94 68 97 94

W7 76 94 97 100

W3 72 96 99 99

W9 99 99 100 99

W10 75 98 100 100
Average no. of features selected 7.35 7.87 9.87 9.79

 

 

3245

112 /B.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Z.Liu et aI.

 

also apply F —test (ANOVA) to 100 simulation data with the sample
size of 50 for each class. In all, 76—91 features are identiﬁed and
the average number of features selected is 82.6 with P < 0.05. With
P2000005, 16—27 features are selected and the average number
of selected features is 20.2, which indicate that the false positive
rate is high even if we adjust the multiplicity problem for multiple
comparisons with a very conservative Bonferroni rule. M etadistance
identiﬁes multiple features simultaneously without encountering the
multiplicity problem and it is more accurate in identifying true
predictive features than the statistical test.

The prediction error was calculated and compared with KNN
and multinomial logistic regression (mlogit) in R (http://www.r—
project.org/) (Fig. 1). MetaDistance outperforms KNN and mlogit
in terms of prediction error rate. KNN had the highest error
rate, likely due to the curse of dimensionality, making feature
selection important when applied to high—dimensional data. Mlogit
also performs more poorly than MetaDistance, due to unbalanced
classiﬁcation problem and small sample size. Since we observe a

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

0.6
0.5
0.5
§ 0.4
,3 0.4
§ 0.3 0.3
0.2 n =10 0.2 “=20
E E
e. a a a
Z ... Z .2
2 2
E g E E g E
0.5 0.5
§ 0.4 + 0-4 %
LE 0
g 03 0.3
'—
02 n=50 + 02 Unbalanced k
E
a
(U
E
E

KNN
mlogit
KNN
MetaDist
mlogit

Fig. 1. Average prediction error with different sample sizes and different
methods—Left: KNN; middle: MetaDistance; right: mlogit.

Table 2. Identiﬁed taxa and their relevance counts

lower error rate in MetaDistance with larger sample sizes, these
results suggest an increases of sensitivity and speciﬁcity as sample
size increases. In addition, the average prediction error (0.22) with
the unbalanced dataset is slightly better than the prediction error
(0.23) with the dataset of sample size 50, indicating our method is
robust with unbalanced data.

Benchmark metagenomic data: we applied our method to 815 16S
rRNA metagenomic samples from six human body habitats (Costello
et al., 2009): external auditory canal (EAC), gut, hair, nostril, oral
cavity (OC) and skin. Since the sample sizes range from 14 to 612
per habitat, this highly unbalanced dataset is dominated by one class
(skin), which could create challenges for classiﬁcation. Sequencing
reads were classiﬁed into taxonomic groups using the RDP classiﬁer
using conﬁdence threshold 2 0.5 1. About 5% of the sequences are
not assigned to a genus by RDP. (56017/1070702 sequences are
not assigned ) (Wang et al., 2007). The aim of the analysis was to
identify taxonomic makers per habitat, whereas the original study
used a binary classiﬁer to determine whether samples originated
from the gut, 0C or other sites (Knights et al., 2010). Using 100
permutations, we split the data into training (2/3 of samples) and
test (1/3 of samples) and estimated parameters 2 , p and k with 10—
fold cross—validation with the training data only. Relevance count
was calculated by the number of permutations a taxon is selected
in a model. This analysis was performed at the bacterial family
and genus levels of taxonomic assignment, with optimal parameters
(2* ,p* , k>l<) of (410, 11, 1) for family and (450, 8, 1) for genus.
The performances for this dataset are not sensitive to the parameter
selections and the prediction errors are quite similar with a wide
range of values for the parameters (Table 2).

MetaDistance identiﬁed 11 taxonomic markers at the family and
genus level (Table 2), most of which have relevance counts of 100.
The mean relative abundance (reads) of selected taxa across samples
varies from 7 to 339 reads (column 3, 6). The abundance of these
taxa could be used as markers to distinguish samples from different
classes. Calculated prediction error rate of 0.075 (family) and 0.064
(genus) are smaller than reported for OTU analysis (Knights et al.,
2010). Using the abundances of these taxa, we are able to correctly
classify 94.1% of samples to their correct body habitat (Table 3).

 

Family level

 

Genus level

 

 

Taxa Relevance count Mean reads Genera Relevance count Mean reads
Chloroplast 100 58.3 Neisseria 100 27.5
Propionibacteriaceae 100 339.8 Streptophyta 100 58
Incertae Sedis XI 100 28.5 Bacteroides 100 35
Prevotellaceae 100 69 Prevotella 100 63.3
Corynebacteriaceae 100 80.9 Staphylococcus 100 126.2
Streptococcaceae 100 86.4 Actinomyces 100 33
Bacteroidaceae 100 35.2 Alloiococcus 88 23.7
Staphylococcaceae 100 131 Streptococcus 100 82.6
Pasteurellaceae 100 33.7 Propionibacterium 100 337.9
Carnobacteriaceae 100 34.5 Corynebacterium 100 65.5
Actinomycetaceae 100 33.5 Peptoniphilus 100 6.9
Test error 0.075 :I: 0.017 Test error 0.064 :I: 0.02

 

 

3246

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse distance-based learning for metagenomic data

 

This accuracy ranges by body habitat between 60% (Hair) to 100%
(Gut). The average AUC is 0.99 across six classes. These results
suggest that MetaDistance can accurately predict classes even with
highly unbalanced sample sizes. For comparison purpose, we also
analyzed the metagenomic OTU count data with similar procedure
using only 552 non—transplanted samples (details in Supplementary
Material). MetaDistance achieves the best predict error (0.08).
We also compute the average prediction AUC based on the KNN
distance between the test data and each class. The average prediction
AUC is 0.99 with only 13 OTUs, compared with 27 OTUs reported
by Knights et al. (2010). In addition, Gut and 0C are perfectly
separated from other classes, which is consistent with the result of
Costello et al. (2009).

Metagenomic data of skin sites: using this same dataset, we
repeated our analysis on 612 skin samples (Costello et al., 2009),
with the aim to classify each sample into a subhabitat (sample size)—
Class 1: axilla (28), Class 2: external nose (14), Class 3: forehead
(160), Class 4: glans penis (8), Class 5: labia minora (6), Class
6: lateral pinna (27), Class 7: palm (64), Class 8: palmar index
ﬁnger (28), Class 9: plantar foot (64), Class 10: popliteal fossa
(46), Class 11: umbilicus (12) and Class 12: volar forearm (155).
This dataset represents several challenges: (i) a highly unbalanced
classiﬁcation ranging from 6 to 160 sample per subhabitat and (ii)
previous methods have failed to separate compositional differences

Table 3. Predicted cross—classiﬁcation

 

Predicted classes

 

EAC Gut Hair Nostril OC Skin

 

 

for these subhabitats. Compared with the one—versus—one strategy,
which needs to estimate 66 models, only one model is needed with
MetaDistance to identify features which are differentially abundant
and capable of predicting classes. We divided the data into two parts:
one with % of the samples from each class as the training data and

the remaining % samples as the test data. The parameters 2, p and
k were estimated using 10—fold cross—validation with the training
data only. The parameter p has the choice of value 1 or 2 only, k is
chosen from 1 to 20, and )t is selected from 1 to 40 with steps of
1. To prevent bias arising from a speciﬁc partition, we split the data
100 times and reported the relevance counts of the the identiﬁed
taxa. The optimal parameters ( 2*, k*, p*) were ( 195, 3, 1) (family)
and (210, 5, 1) (genus).

As shown in Table 4, we selected 12 taxonomic marker at the
family and genus level assignment based on relative abundances,
with test errors at 0.30 (family) and 0.31 (genus), comparable to
previously reported best results (Knights et al., 2010). The mean
relative abundance (reads) of selected taxa across samples varies
from 11 to 390 reads (columns 3, 6). Many of these markers are
similar to those found in the habitat comparison, where skin samples
were compared with other body habitats. Some of these taxa have
been linked to disease, such as Prevotellaceae/Prevotella, which has
been shown to be less prevalent in lean subjects (Zhang et al., 2009).
Additionally, species from the family Acinetobacter have been
linked to disease and are target for health studies (Guner et al., 2011).

We also plot a receiver operating characteristic (ROC) curve for
each class versus the rest based on the KNN distance between the test
data and each class from one run. Intuitively, if a test sample is from
that training class, the KNN distance will be small. Otherwise, the
distance will be large. Figure 2 shows the ROC curves and predictive
AUC values at the bottom—right corner of each subplot. It is shown
that Class 9: plantar foot is the easiest skin site to be separated from

 

 

 

 

a EAtC 1(1) 1(5) 8 8 8 (3) other classes with the prediction AUC of 0.99, and Class 2: external
g Hit 0 0 3 0 0 2 nose is the hardest skin site to be classiﬁed correctly with the test
g Nostril 0 0 0 12 0 3 AUC of 0:74. The averagetest AUC for all classes is 0.88. The
a QC 0 0 0 0 14 4 results indicate that the 12 identiﬁed taxa at family level have the
Skin 0 0 0 3 1 200 predictive power for skin site discrimination. Obviously, it is more
crucial to select important taxa that are highly discriminative for
Table 4. Identiﬁed taxa and their relevance counts
Family level Genus level
Taxa Relevance count Mean reads Genera Relevance count Mean reads
Chloroplast 100 67 Acinetobacter 53 11.8
Propionibacteriaceae 100 392.5 Veillonella 57 40.5
Incertae Sedis XI 55 28.6 Neisseria 100 21.3
Prevotellaceae 80 58 Streptophyta 100 66.6
Corynebacteriaceae 100 74.6 Prevotella 100 53.5
Micrococcaceae 100 31.2 Rothia 91 18
Streptococcaceae 100 83 Staphylococcus 100 128.4
Veillonellaceae 85 47 Pseudomonas 51 11.6
Comamonadaceae 63 1 1 .4 Streptococcus 100 78.3
Staphylococcaceae 100 133 Propionibacterium 100 390.1
Moraxellaceae 69 29.6 Corynebacterium 100 73
Neisseriaceae 97 22.4 F usobacterium 77 13.2
Test error 0.30 :l: 0.04 Test error 0.31:l: 0.03

 

 

3247

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Z.Liu et al.

 

Class 2 vs. Rest

Class 1 vs. Rest

 

 

 

 

 

 

 

 

 

 

 

 

 

m
/.
/.
—I— /~
/.
./ AUC=O.84 ./ AUC=O.74
/ /
0
Class 5 vs. Rest Class 6 vs. Rest
1 /
/ /~
/ /
0.5 / v / / - /
-/ AUC=O.91 ./ AUC=O.86
O / /

 

 

 

 

 

 

Class 9 vs. Rest Class 10 vs. Rest

 

 

 

 

 

 

 

 

1 / _|_,—l—'——'— /
/ /‘
/ /‘
0.5 / / / /
. / ‘ /
./ AUC=O.99 ./ AUC=O.95

o / /

O 0.5 1 O 0.5 1

Fig. 2. ROC curves predicted with the eight selected taxa at family level.

this type of task, since the sites of sampling would most likely to be
known. However, MetaDistance can certainly be applied for both
taxa selection and class prediction with other types of metagenomic
data where the category labels (such as disease status) are more
expensive to obtain.

4 CONCLUSIONS

We have proposed a sparse distance learning method (MetaDistance)
for multiclass classiﬁcation through combining instance—based
(KNN) and model—based (SVM) learning methods. The proposed
method can identify phenotype—associated taxa and perform class
prediction simultaneously. It is robust for unbalanced classiﬁcation
and can classify multiple classes simultaneously without creating
unbalanced classes. In addition, this method estimates a small
number of parameters (only the same as the number of features)
and is very efﬁcient for problems with small sample sizes, high
dimensions and unbalanced classiﬁcations with many classes, which
is common in genomic data. Experiments with limited simulation
and real datasets demonstrated its effectiveness. While this method
was tested on 16S rRNA, it can easily be applied to identify marker
genes from WGS metagenomic and digital gene expression survey
(SAGE) analysis without modiﬁcation.

Funding: National Institutes of Health (1UH2DK083982—01 and
4UH3DK083991—02); National Cancer Institute (1R03CA133899—
01A210).

Conﬂict of Interest: none declared.

REFERENCES

Allwein,E.L. et al. (2001) Reducing multiclass to binary: a unifying approach for margin
classiﬁers. J. Mach. Learn. Res., 9, 113—141.

Class 3 vs. Rest Class 4 vs. Rest

 

 

 

./‘ AUC=0_88 ,/' AUC=O.87

/ /

 

 

 

 

 

 

Class 7 vs. Rest Class 8 vs. Rest

 

 

/‘ /~
/~ /‘
/‘ /‘
q/ ./‘

./‘ AUC=O.9 / AUC=O.81
/ /

 

 

 

 

 

 

Class 11 vs. Rest Class 12 VS- Rest

/ /
/ /
/ /

 

 

 

 

 

 

 

 

/' /V
/‘ /‘
./ AUC=0.91 ./ AUC=O.89
/ /
0 0.5 1 0 0.5 1

Caporaso,J.G. et al. (2010) QIIME allows analysis of high-throughput community
sequencing data. Nat. Methods, 7, 335—336.

Crammer,K. and Singer,Y. (2001) On the algorithmic implementation of multiclass
kernel-based vector machines. J. Mach. Learn. Res., 2, 265—292.

Cheng,W. and Hullermeier,E. (2009) Combining instance-based learning and logistic
regression for multilabel classiﬁcation. Mach. Learn, 76, 211—225.

Costello,E.K. et al. (2009) Bacterial community variation in human body habitats across
space and time. Science. 326, 1694—1697.

Foi,A. (2009) Clipped noisy images: heteroskedastic modeling and practical denoising.
Signal Process, 89, 2609—2629.

Freeman,M. and Tukey,J. (1950) Transformations related to the angular and the square
root. Ann. Math. Stat, 21, 607—611.

Guner,R. et al. (2011) Outcomes in patients infected with carbapenem-resistant
Acinetobacter baumannii and treated with tigecycline alone or in combination
therapy. Infection [Epub ahead of print, doi: 10.1007/s15010-011-0161-1, July 26,
2011].

Hagger,W.W. and Zhang,H. (2006) A survey of the nonlinear conjugate gradient
methods. Pac. J. Optim, 2, 35—58.

Huson,D.H. et al. (2007) MEGAN analysis of metagenomic data. Genome Res., 17,
377—386.

Huson,D. et al. (2009) Methods for comparative metagenomics. BMC Bioinformatics,
10, S1—Sl2.

Kasuya,E. (2004) Angular transformation - another effect of different sample sizes.
Ecol. Res., 19, 165—167.

Kim,D.K. and Taylor,J.M.G. (1994) Transform-both-sides approach for overdispersed
binomial data when N is unobserved. J. Am. Stat. Assoc, 89, 833—845.

Knights,D. et al. (2010) Supervised classiﬁcation of human microbiota, FEMS
Microbiol Rev. [Epub ahead of print, doi: 10.1111/j.1574-6976, October 7, 2010].

Laubscher,N.F. (1961) On stabilizing the binomial and negative binomial variances.
J. Am. Stat. Assoc, 56, 143—150.

Liu,Z. et al. (2010) Sparse support vector machines with Lp penalty for biomarker
identiﬁcation. IEEE/ACM Trans. Comput. Biol. Bioinform, 7, 100—107.

Lozupone,C. and Knights,R. (2005) UniFrac: a new phylogenetic method for comparing
microbial communities. Appl. Environ. Microbiol, 71, 8228—8235.

Mitra,S. et al. (2009) Visual and statistical comparison of metagenomes. Bioinformatics,
25, 1849—1855.

N agarsenker,P.B. (1984) On Bartlett’s test for homogeneity of variances. Biometrika,
71, 405—407.

Olivier,J. (2010) Positively slewed data: revisiting the Box-Cox transformation.
Int. J. Psychol. Res., 3, 69—78.

 

3248

112 /§.IO'SIBUJTIOIpJOJXO'SOIlBIHJOJUIOICI/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse distance-based learning for metagenomic data

 

Qin,J. et al. (2010) A human gut microbial gene catalogue established by metagenomic
sequencing. Nature. 464, 59—65.

Schloss,P.D. et al. (2009) Introducing mothur: open-source, platform-independent,
community-supported software for describing and comparing microbial
communities. Appl. Environ. Microbial, 75, 7537—7541.

Turnbaugh,P.J. et al. (2007) The human microbiome project. Nature, 449, 804—810.

Vens,C. and Struyf,J. (2008) Decision trees for hierarchical multi-label classiﬁcation.
Mach. Learn, 73, 185—214.

Wang,Q. et al. (2007) Naive Bayesian classiﬁer for rapid assignment of rRNA sequences
into the new bacterial taxonomy. Appl. Environ. Microbial, 73, 5261—5267.

White,J.R. et al. (2009) Statistical methods for detecting differentially abundant features
in clinical metagenomic samples. PLaS Comput. Biol, 5, e1000352

Wooley,J.C. et al. (2010) A primer on metagenomics. PLaS Comput. Biol, 6, e1000667.

Xu,Z. et al. (2010) Semi-supervised feature selection based on manifold regularization.
IEEE Trans. Neural NetW., 21, 1033—1047.

Zhang,M.L. and Zhou,Z.H. (2007) Ml-knn: a lazy learning approach to multi-label
learning. Pattern Recognit, 40, 2038—2048.

Zhang,H. et al. (2009) Human gut microbiota in obesity and after gastric bypass. Prac.
Natl Acad. Sci. USA, 106, 2365—2370.

 

3249

112 /§.IO'SIBUJTIOIpJOJXO'SOIlBIIlJOJUIOICI/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

