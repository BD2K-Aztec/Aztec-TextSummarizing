ORIGINAL PAPER

Vol. 28 no. 7 2012, pages 1001-1008
doi: 10. 1093/bioinformatics/btsO81

 

Data and text mining

Advance Access publication February 13, 2012

ShapePheno: unsupervised extraction of shape phenotypes from

biological image collections

Theofanis Karaletsoslnf, Oliver Stegle1’*, Christine Dreyer2, John Winn3 and

Karsten M. Borgwardt1 ’4

1 Machine Learning and Computational Biology Research Group, Max Planck Institute for Intelligent Systems and
Max Planck Institute for Developmental Biology, 2Department of Molecular Biology, Max Planck Institute for
Developmental Biology, 72076 T bingen, Germany, 8Machine Learning and Perception Group, Microsoft Research
Ltd, Cambridge CB3 OFB, UK and 4Zentrum f r Bioinformatik, Eberhard Karls Universit t, 72076 T bingen, Germany

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: Accurate large-scale phenotyping has recently gained
considerable importance in biology. For example, in genome-
wide association studies technological advances have rendered
genotyping cheap, leaving phenotype acquisition as the major
bottleneck. Automatic image analysis is one major strategy to
phenotype individuals in large numbers. Current approaches for
visual phenotyping focus predominantly on summarizing statistics
and geometric measures, such as height and width of an individual,
or color histograms and patterns. However, more subtle, but
biologically informative phenotypes, such as the local deformation of
the shape of an individual with respect to the population mean cannot
be automatically extracted and quantified by current techniques.
Results: We propose a probabilistic machine learning model that
allows for the extraction of deformation phenotypes from biological
images, making them available as quantitative traits for downstream
analysis. Our approach jointly models a collection of images using a
learned common template that is mapped onto each image through
a deformable smooth transformation. In a case study, we analyze the
shape deformations of 388 guppy fish (Poecilia reticulata). We find that
the flexible shape phenotypes our model extracts are complementary
to basic geometric measures. Moreover, these quantitative traits
assort the observations into distinct groups and can be mapped to
polymorphic genetic loci of the sample set.

Availability: Code is available under: http://bioweb.me/GEBI
Contact: theofanis.karaletsos@tuebingen.mpg.de; oliver.stegle@
tuebingen.mpg.de

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on September 22, 2011; revised on February 7, 2012;
accepted on February 9, 2012

1 INTRODUCTION

With the advent of high—throughput genotyping techniques an
unprecedented breadth of genotypic datasets can be generated,
opening doors to large—scale association studies, promising sufﬁcient
power to understand the genetic underpinning of more subtle
phenotypes that characterize the sample. As phenotyping often

 

*To whom correspondence should be addressed.

requires manual labor and expert knowledge, a major bottleneck now
lies with the identiﬁcation and quantiﬁcation of informative traits.
Currently, the quantiﬁcation of phenotypic traits is predominantly
done in a semi—manual fashion, rendering the task of analyzing large
datasets expensive, time—consuming and error—prone. In order to
address these shortcomings, the automated analysis of biological
images has become a staple in modern biology.

High—throughput imaging techniques for various types of
microscopy and other imaging modalities have become common
in the experimental environment. Automated image analysis for
bioimaging attempts to deal with the ﬂood of data and subsumes
a large variety of tasks and methods; for a comprehensive review,
see Peng (2008) and Walter et al. (2010). Common tasks include
the counting of cells in microscopy images and differential analysis
of distinct cell types (Fuchs et al., 2010; Pau et al., 2010). Key
challenges in bioimage informatics stem from the breadth and
individuality of natural variation within these images and dealing
with the inherent noise in biological imaging tasks. In order to
deal with these factors, machine learning techniques have raised
considerable attention and are used to tackle various complicated
tasks in realistic settings (Ning et al., 2005; Shamir et al., 2010).
For example, in the analysis of appearance phenotypes machine
Vision has been used to quantify the extent of existence of
predeﬁned Visual features or detect interesting appearance features
that characterize the data (Whibley et al., 2006). Visual appearance
features usually pertain to speciﬁc local properties of the depicted
objects. However, more general Visual phenotypes often are also
biologically informative, such as the description of the shape of
an object and the quantiﬁcation of global (including size and
height) as well as local (i.e. locally deformed parts of an image)
shape variations. An example where such a method is useful is
the characterization of the shapes of guppy ﬁsh, which so far can
only be analyzed by labor—intensive manual geometric phenotype
measurements on hundreds of ﬁsh, as performed in (Tripathi et al.,
2009).

Our goal is to automatically determine and quantify differences
among observed shapes in biological images in order to interpret
them as shape phenotypes and facilitate downstream analysis, for
instance association tests of traits with putative causal factors in the
genome. In this work, we propose an unsupervised machine learning
method to quantify shape variations of a given object class depicted

 

© The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 1001

112 /§JO'SIBUJn0[pJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq mm; popeoprmoq

9IOZ ‘091sn8nv uo ::

IKaraIetsos et aI.

 

Mapping Fields L

Fish images I — Reconstructions R

 

 

f
'4 )‘Nl

Template T

a“

 

a
ﬂ/VVVVVVVVVVVVVVV'N 1‘4 +
)VVVVVVVVVVVVVVV'M

l/V/VVV'ﬂ/V/V/‘l/VVVVVV')‘

ffffffﬁﬂﬂﬂﬁﬂ

f f f f )UUUUUUUUUUUU'
)VVVVVVVV/‘J/VVVVVVV')‘Fﬂﬂfflﬂfﬂﬂﬂﬂﬂflﬂﬂ)‘

Fﬂffffﬂﬂﬂﬂﬂﬂfffﬂﬂ

 

.
Tilllllllllllllllllllfv
H444++++++++++++++++++++++ If . 1
r+++++++++++++++++++++.. .44 .
H4++++++++++++++++++++++s2' .
MHH +++++++H++++H+an ,
r+++++ +++++++++++fﬂ /w 4 v
M H++H+++++ ‘ 2 .r I I
+M+M+++++ / r r r v v I
4444:, I . . . . .

+ H
I 1 r rm
¢< ¢<¢
gézéW

)‘FFFFFWWFFFFFFFFWZFFF.
)‘FFFFFWFFFFFFFFFWFFFF
)UUUUUUUUUUUUU'FFFFFF)‘ 2

 

20‘ +3 47;,"
 r « w r R
262%?” W 1

 

 

 

)'

)U'

)U')‘
)U')‘)')')')')',
)‘fﬁlﬂﬂﬂﬂ,
Iffﬂﬂﬂﬂ)‘

 

 

I3

 

R3

mm

Fig. 1. A schematic overview of the generative process underlying the proposed approach, illustrated using three random images. From left to right: the
learned common template T, describing the mean—shape and appearance for the entire image collection. Image—speciﬁc mapping ﬁelds L capture the translation
and deformation needed to map the template onto the observed images I. Every pixel of each image has a corresponding vector that points onto the template
pixel which it is drawn from. Images R denote the reconstruction of the corresponding raw image from the trained model.

in a set of images, one per individual or sample. We postulate the
existence of an unobserved reference shape, called a template. We
proceed with joint learning of this shared template and the image—
speciﬁc shape deviation from this reference, allowing every image
to be aligned to it. The resulting template iteratively converges to an
idealized mean image from which the observed images are generated
through deformation ﬁelds that explain the variation in shape of each
image (Fig. 1). The converged model can also be run backwards,
yielding a reconstruction of every image from the template and the
mapping ﬁelds (R in Fig. 1).

The general task of aligning two or more images is also known as
registration, where a correspondence between pixels of one image
and pixels of another image is established. For example Saalfeld
et al. (2010) perform a simpler form of registration, where images
are aligned to a known template. In contrast to previous studies,
our method does not require explicit knowledge of the template a
priori; neither is supervision like setting of landmarks or outline
selection/binarization on each image required. Instead, ShapePheno
discovers and objectively quantiﬁes deformation phenotypes on
unannotated images in a fully unsupervised fashion while retaining
interpretable features and results. Thus, our approach facilitates
obtaining accurate non—trivial measurements on large datasets where
human labor is costly and error—prone.

In Section 3, we present a case study of our method on
guppy ﬁsh, Poecilia reticulata. The individuals in this dataset are
subject to variation in appearance and shape. Interestingly, both
appearance and shape variability have previously been shown to
exhibit considerable genetic components (Tripathi et al., 2009). In
Section 3.3, we describe the basic approach of quantifying shape
phenotypes using our model. We demonstrate that these quantitative
traits are orthogonal to traditional geometric measures (Section 3.4)
and show practical utility of these traits in the context of two
fundamental types of downstream analyses. First, in Section 3.5, we
show how learned shape features allow for grouping the observed

images into plausible similarly shaped or deformed subgroups based
on characteristic deformation patterns. Second, in Section 3.6,
we show that quantitative shape phenotypes can be associated to
variable genetic loci in the guppy genome. This analysis serves both
as a step towards obtaining further knowledge as to the underlying
biological processes that lead to variation of these phenotypes as
well as a natural validation step, suggesting that the automatically
determined phenotypes are biologically relevant.

2 METHODS

Extraction and quantiﬁcation of shape features is carried out in two steps.
In Section 2.1, we discuss how a graphical model based on Markov random
ﬁelds (MRF) can be used to simultaneously learn the unknown template and
recover smooth mapping ﬁelds, performing a ﬂexible variant of deformable
registration. We decompose the mapping ﬁelds into a sum of a technical
translation component and shape—related deformation ﬁelds, both speciﬁc to
each image. The overall setup of our probabilistic model largely follows
the jigsaw model (Kannan et al., 2007). Under this model, a set of N
observed images I i , i=1,...,N is explained by a common latent template
image T. The training images are explained as function of the template
through learned mapping vectors Li between pixels in the template and
observed pixels in each image i. The coordinate mapping accounts for an
overall shift of the image with respect to the template, as well as local
deformations, compressing or stretching speciﬁc parts of each image to
match the common reference (Section 2.2). Subsequently, once the template
and the deformation ﬁelds are learned, we extract quantitative traits from
the information captured in the deformation ﬁelds. For this purpose, we
employ linear dimensionality reduction (Section 2.3), yielding a compact set
of features that explain the major axes of variation in the deformation ﬁelds
for each image. For comparison, we also show how our model can be used
to quantify length vectors within images, which can be directly related to
established manual measurements of shape traits. Both types of features can
be used for downstream analyses.

Summary overview of model parameters: In our model, we assume a set of N
images I i and corresponding mapping ﬁelds Li of dimension (c100 >< d(y)) for

 

1 002

112 /§.IO'S[BU.IHO[p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

ShapePheno

 

(a) @

 

(m

 

 

 

 

s s 8

Energy Cost

N
0

(may) 6 {d(w)vd(y)}
i: 1,...,N

 

 

 

14 15
p

4 6 _ 8 i 10 12
Mapping Distance

Fig. 2. Illustration of deformable registration model and deformation cost
function. (a) Graphical model representation of the core ShapePheno model.
Observed images I 1, ...,IN are modeled through common template image T
and a coordinate mapping function M. The mapping is parametrized by
smooth deformation ﬁelds LEW), denoting the coordinate offset between
each image pixel and the template. The prior belief of smoothness of L is
parameterized by an energy function for translation (EVT and deformation
EVD) deﬁned on pixel blocks U. (b) Example choices of mapping energy
function EVD of order a, inducing the cost of non—neighboring mappings as
a function of mapping distance. At maximum allowed deformation ,0, the
energy cost diverges, restricting the effective range of deformations learned
by the model.

i={1, ...,N}, where Li is a matrix with entries lix, 6Z2 for each image i.
The size of template image T is set to (too xt(y)). The model parameters
a0, a, b and ﬂ deﬁne a prior on the template T. The parameters determining
the smoothness prior are given as: the prior on the translation component of
the mapping ﬁeld is parametrized by an energy constant yt; the deformable
component of model is deﬁned by as based cost at saturation, yo, the maximal
allowed deformation distance ,0 as well as a, parametrizing the order of the
metric. U is describing the pixel block coupling.

2.1 Markov random ﬁelds for deformable registration
Each image pixel [(ix y) is related to the common template T, linked by a

transformation Mi that maps image coordinates (x, y) to the corresponding
coordinates within T :

Mi”, =(x,y)—l§,,,,. (1)

The mapping is parametrized using a ﬁeld of relative shifts Li:
Léoaoyu-aLédbdy), where contiguous (smooth) mappings of neighboring

pixels corresponds to constant entries in Li .
For a given mapping ﬁeld, L’, the generative model (Fig. 2a) of each
image is then

IiZTMi-I-lﬁ, (2)

where Mi is parameterized by Li and ti denotes the reconstruction error of
each pixel. Both, the template map T and the mapping ﬁelds Li are unknown a
priori and hence need to be learned from the image data alone. For a common
template T with set dimensions tog), t(y), a set of N observed images I i and

corresponding mapping ﬁelds L’, the joint probability under our model is

N
P(T,{I",L"}§V:1)= P(T) HP(I"|T,L") P(L’) . (3)
ﬁz—r AH

.2 1
template priorl likelihood mapping prior

The likelihood of the observation model corresponding to Equation (2) is
a Gaussian mixture model independent for each image pixel (x, y):

P(Ii|T,Li)= H [(1—JT)N(ng,y)IILMExy),T;iE )) +71 Uniform]. (4)
(xay) ’ x’y

Here, a and 1' correspond to the means and the precisions at each position
of the template image T and 71’ is the mixture coefﬁcient of the uniform
background model to explain outliers that are not compatible with the
template image. To ensure that the template is well—behaved, we choose
a normal—gamma prior on the values of the template,

P<T> = MN (M(x,y)|/L0, (trawl) Gammon,» lab). (5)
(xay)

Smoothness of the mapping ﬁelds Li is encouraged through the choice
of a Markov random ﬁeld prior that couples neighboring mapping offsets in
each image

P(Li) oc exp — Z

(x,y),(x’,y’)€8(x,y)

EV(l(x,y)a l(x’,y’)) - (6)

Here, 80,,” denotes the set of pixels in the direct neighborhood of (x, y) and
the pairwise energy term EV penalizes non—contiguous offsets of neighboring
pixels. In ShapePheno, the mapping ﬁelds Li are decomposed into a
translation and deformation component with individual smoothness priors.
The speciﬁc modeling choices will be discussed in Section 2.2.

Inference in the joint model implied by Equations (3—6) is feasible by
means of iterative learning using expectation maximization. In this approach,
we alternate between maximizing the joint probability (Equation (3)) with
respect to mapping ﬁelds Li and the unknown template T, keeping the other
variables ﬁxed. The ﬁrst task, determining the most probable template T
essentially boils down to parameter inference in a normal—gamma model,
where closed form updates for mean and precisions of the template ([1,,1')
are available (Bishop, 2006). Alternatively, for a ﬁxed template T, the
most probable mappings Ll =argmaxLip(Ii|T,Li)p(Li) can be determined
efﬁciently using graph cuts for each image (see Kannan et al. (2007)
and Boykov et al. (2001) for details).

2.2 Design choices for MRF energy functions

Since the alignment of template and observed images requires a combination
of translation and deformation, we choose L to be the sum of a translation and
deformation ﬁeld component L =Lt +Ld. In this stacked two—layer Markov
random ﬁeld, P(Lt) accounts for the global shift of images to the template
and P(Ld) speciﬁes the prior probability of local deformations (see also
Fig. 2.1). The joint prior probability of the mapping—ﬁeld components can
be expressed as

P(L7LtaLd) = 5(L, (Ld +Lz))P(Ld)P(Lz),

where 8() denotes the Dirac delta function. Accounting for both prior
contributions, the effective energy term in Equation (6) becomes EV:
EVD +EVT. Inference in the full model is done iteratively within the mapping
updates, by ﬁrst keeping Ld = 0 ﬁxed and updating Lt. Next, L, is kept at the
learned value while updating Ld. Both update steps can be done following
the standard jigsaw inference (Section 2.1 and Kannan et al. (2007)). In
the following, we will explain the modeling choices of each mapping prior
separately.

 

1 003

112 /§.IO'SIBUJHOTPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

IKaraIetsos et al.

 

2.2.1 Translation (rigid) model: Having deﬁned a template that is of equal
size as the images, the goal is to register images to it. In order to allow the
shift ﬁeld L, to incorporate translation behavior, we employ a Pott’s Model
prior with energy function EVT(lp, lq) = y, 8(lp, lq). Here, the cost parameter
7/, is set to large value, such that all mappings L,- are forced to take on identical
values, solely accounting for a constant overall image shift.

2.2.2 Deformable model: For the deformable prior P(Ld), we employ a
non—rigid smoothness prior that encourages smooth deformation ﬁelds. In
contrast to the rigid Pott’s model, the energy costs is distant—dependent,
favoring short—range deformations. More speciﬁcally, the energy function
EVD scales linearly with a particular choice of distance norm of order a:

_lq”ot for “lp_lq”a5p
otherwise.

l/cleflll

E l ,l = p 7

VD( p q) { 00 ( )

Here, ,0 denotes the maximum permitted range of deformation, and a is a

power (or order) where a e {1,2n} for n e N>0 and scaling parameter ydef =
r0

max(1,p1/“)'

Intuitively, one can imagine this function to apply elastic bands connecting
pieces (in our case pixels) to their four neighbours with the EVD part
of the mapping costs being the equivalent of the elastic potential of the
bands between all pieces. Figure 2b shows the energy function for two
choices of a.

2.2.3 Robustifying deformable registration: We use various constraints
on the registration to further improve the robustness of our method against
noise and non—standardized images and reliably produce good results. We
apply our deformation ﬁelds on pixel blocks, meaning that we constrain
groups of pixels of block—size to obtain the same mapping via the prior
U shown in Figure 2a. This leads to piecewise smooth deformation ﬁelds.
Additionally, we constrain the parameter ,0 of the deformation ﬁeld itself.
This makes large jumps prohibitively expensive and drives the model to use
smoothly varying local deformation patches. A positive side effect of this
constraint is a signiﬁcant boost in computational efﬁciency and robustness
against outlier—mistakes since the solution space is reduced. We also robustify
alignments against appearance outliers with a mixture of densities used as
the observation model in Equation (4), which allows for a background class.

2.3 Feature representation for deformation maps

The deformation ﬁelds Ld at pixel resolution, described in Section 2.2.2,
capture the relevant information to explain local shape deformations of the
samples in each image. Comparing deformation ﬁelds with non—equal objects
at non—equal positions is hard, since we face the problem of correspondence.
However, in our framework this problem can be elegantly circumvented
using the common template all images are aligned to. To render individual
deformation ﬁelds comparable between images, we ﬁrst project these maps
from observation space into the common reference coordinate system on the
template. For this purpose, we apply the same mapping operator we used for
image pixels now to the mappings themselves

i _ i
DMi_Ld7

resulting in deformation ﬁeld representations in the object—centered template
space. Thus, we obtain an easily interpretable shift—corrected ﬁeld D1 of
deformation for each image—mapping L2].

2.3.] Low-rank representations of deformation ﬁelds: In order to extract
meaningful features from the high—dimensional deformation ﬁelds, we
reduce their dimensionality by representing individual deformation ﬁelds
via a set of coefﬁcients over a small number of bases CD obtained via
standard principal component analysis (PCA) (Fig. 3). Prior to running
PCA, we can apply a binary mask to the template to select only relevant
template regions for consideration as variance components. The resulting
individual bases can be visualized and constitute local deformation factors.

 

 

 

PCA deformation components

Fig. 3. Illustration of the ﬁrst three PCA—bases of deformation ﬁelds in the
X and Y deformation direction. Individual highlighted image parts such as
black stripes correspond to image parts with most pronounced deformation.
Here, these areas correspond to compression and stretching of ﬁsh tails and
the main body (Section 3).

Building on the PCA basis functions, we use the corresponding coefﬁcients
for every image as a quantitative trait that characterizes the deformation
ﬁeld in a given sample. Formally, for a matrix of k linearized orthogonal
bases CD of dimensionality p x (d(x).d(y)) (dimensions of an image), a
coefﬁcient matrix W of dimensionality N xk and aN >< (dog) 'd(y)) observation
matrix of linearized matrices Di that contains the k—rank approximations
to the observed, corrected and template—projected deformation ﬁelds Di as
described in Section 2.3, the low—dimensional projections per image have
the form: Di =wi - CD.

2.3.2 Geometric measurements as shape traits: We also use our method
to measure distances in the images by exploiting the mapping ﬁelds to
the common template. We measure geometric traits by selecting points of
interest on the template and measuring their distance per image by projecting
the annotated template—points into the reconstructed image through the
mapping ﬁelds. Thus, a single annotation of the shared template yields
an exhaustive annotation of all images explained by the model. These
annotations can be used to measure geometric distances in the images, for
example. Importantly, these geometric measurements are local measurements
on an image in contrast to the holistic descriptor of shape we introduce in
Section 2.3.1.

3 RESULTS

We applied ShapePheno to a dataset that shows the lateral aspect
of male guppy ﬁsh, P. reticulata. The goal was to obtain local
deformation patterns that are informative about typical distortions
of the shape among the individuals, which also display considerable
variation of appearance traits and size. We demonstrate further use
of our method in two tasks: clustering of populations according to
deformation patterns and association studies to link genotypes to
deformation phenotypes.

3.1 Dataset

The 388 available individuals are second generation progeny (F2)
of two parents representing geographically and genetically distant
populations whose visual appearances also differ signiﬁcantly. The
male parent from Cumana (Ve) (Alexander and Breden, 2004) has a
slimmer posterior trunk and brighter orange ornaments as compared
to the maternal population from the Quare river in East Trinidad.

 

1 004

112 /3.IO'S[BUJHO[pJOJXO'SOIlBIIlJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

ShapePheno

 

This cross (157 Quare x Cumana) has been subject of a genotyping
project to establishing a comprehensive genetic map and to initiate
conventional QTL (quantitative trait locus) mapping (Tripathi et al.,
2009). The raw images were rescaled such that the ratio of pixels to
physical length is constant across the dataset. Due to rescaling, the
image size was variable, ranging between 75 x 226 and 83 x 250 pixel
size. To account for images taken at slightly varying distance, we
chose to embed all images according to original size of the ﬁsh into
empty images of the chosen format (83X 250). For this particular
experiment, there were few outliers and thus setting the outlier ratio
7T :0 yielded good results. For each of the 388 individuals, the
dataset included matching genotype information, covering a total of
1063 genome—wide single nucleotide polymorphisms (SNPs). After
ﬁltering, removing rare SNPs with a minimum rare allele frequency
<5 %, we obtained 814 polymorphisms that were considered for
analysis.

3.2 Experimental settings

We chose the following parameters for the deformation model:
yo :40, ,0: 10 and a: 1, which resulted in robust registration in
a series of test runs. We applied the deformation ﬁeld to 2 x 2 pixel
blocks in order to locally tie together image pixels to correct for
appearance differences and to prevent excessive local deformation.
The normal—gamma prior parameters (Equation (5)) were set such
that the prior reﬂects the ﬁrst and second empirical moments of
the distribution of the raw image pixels (see also Kannan et al.
(2007)). We ran a Python—based parallelized implementation of
ShapePheno on an 8—core Intel Xeon machine where the full dataset
could be run to convergence within 3 days. After convergence, we
manually segmented the template ﬁsh from the background template
to facilitate all downstream analyses (clustering and association
mapping on foreground information only).

3.3 Shape phenotype determination

The converged ShapePheno model yielded a sharp template that
resembles an average ﬁsh and mapping ﬁelds Li for every image in
the dataset (Fig. 1). The model perceives the shapes of the ﬁsh in
individual images as locally stretched or smoothly distorted versions
of the template and smoothly bypasses appearance differences
that would counteract shape alignment. This suggests that the
deformation ﬁelds Lil, capture shape information corrupted by
noise stemming from the difference in sizes of images and the
background color similarity to the ﬁsh corpora. Next, we used linear
dimension reduction (Section 2.3.1) to determine the corresponding
deformation factors of the converged model; Figure 3 depicts the ﬁrst
three PCA—bases. These three main deformation features appear to
divide the ﬁsh into the anterior and posterior part. Inﬂated anterior
parts at the belly region as well as distorted posterior trunks are
the main sources of shape variation. We also observed that local
structure in the bases matches appearance features of the template
that get distorted frequently. These ﬁndings reﬂect the set—up of
the experiment in Tripathi et al. (2009) in agreement with our
expectations, as the parents were originally chosen to exhibit these
shape differences and the offspring shows strong variation at these
features. Supplementary Figure S1 provides examples of inference
results for extreme outliers within the data, here a singleton shape—
mutant in our training set. Since the method is unsupervised, it

 

Fig. 4. Overview of the chosen points of interest on the ﬁsh and the
template and the measured geometric phenotypes with corresponding names
M1—M8. These measurements are chosen to capture shape differences
between anatomically ﬁxed points.

requires shape mutants to be well—represented in the data in order to
model their shape accurately.

3.4 Quantiﬁcation of geometric measurement accuracy

After the qualitative evaluation of the reconstructed shape template,
we next characterized the accuracy of the shape representation
captured by the model in a quantitative manner. For this purpose,
we used the converged model to automatically measure geometric
distances in images (Section 2.3.2). We comparatively evaluated
eight geometric trait measurements (described in Figs 4 and 6),
whose choice was motivated by primary analyses of the Guppy
dataset (Tripathi et al., 2009). Manual quantiﬁcation was done on
50 individuals from our dataset chosen at random, measuring all 8
geometric distances in each raw image by 3 independent experts,
as well as using the fully automated approach provided by the
ShapePheno model. We assessed the correlation between manual and
automated measurements, comparing the ShapePheno prediction to
the mean of the manual quantiﬁcation runs (Fig. 3.4). Encouragingly,
all automated geometric measurements were in good agreement
with the corresponding manual annotation. The correlation score for
pairs of corresponding automated and manual measurements ranged
between 0.65 (A2 versus M2) and 0.84 (A6 versus M6) with a mean
correlation score of 0.76.

To better understand the magnitude of the variation between
automated and manual measurements, we also considered the
pairwise correlation between two of the three manual runs (Fig. 5b),
yielding comparable results. Pairwise correlations here ranged from
0.81 (M7) up to 0.96 (M3) with a mean correlation score of 0.87.

This suggests that ShapePheno captures true variability in images
and yields high levels of accuracy when used to quantify geometric
measurements in place of a human expert. Detailed scatter plots,
showing the correlations between manually and automatically
determined traits are shown in Figure 6.

From either of the correlation analyses, it was also notable that
geometric measurements correlated well with each other, reﬂecting
the biological relatedness of growth—phenotypes that underlie
the geometric measurements under consideration. In contrast to
this observation, the correlation to the new PCA—deformation
phenotypes described in Section 3.3 was weak, which shows that
they capture orthogonal aspects of shape variation and hence are
complementary to geometric measurements.

3.5 Clustering of populations based on deformation
traits

We clustered populations of guppy ﬁsh according to their
characteristic local deformation patterns, without any prior

 

1 005

112 /§.IO'S[BU.IHO[p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

IKaraIetsos et al.

 

 

(a) 00
L1 I ' 1.0
L2 I
L3 h I I 0.5
L4
L5 °
L6 0.5
L7
L8 1.0

A1 A2 A3 A4 A5 A6 A7 A8 PX1PX2PY1PY2 M1 M2 M3 M4 M5 M6 M7 M8PX1PX2PY1PY2
Geometric Measurements PCA‘Def- Geometric Measurements PCA-Def.

Automated to Manual correlations Manual self-correlations

Fig. 5. Quantitative evaluation and comparison of the geometric traits
as determined by ShapePheno (Al—A8), manually measured counterparts
from a human expert (Ll—L8) and novel PCA—deformation phenotypes
(PXl —Py2). (3) Correlation between automatic geometric measurements,
manual measurements and PCA—deformation phenotypes. (b) Correlation
between two manual quantiﬁcation runs by an expert user (Ml—M8 versus
L1—L8). The results show that the reproducibility of manual expert labels is
similar to automated measurements by ShapePheno, suggesting the model
is able to achieve human—like results. Weak correlation between PCA—
deformation phenotypes and geometric phenotypes shows that these new
quantitative traits complement established measurements.

 

 

 

Automated Measurements

 

 

 

 

 

 

 

 

 

 

Manual measurements

Fig. 6. Scatter plots with error bars, showing the relationship between
manual measurements and automated geometric measurements as obtained
by ShapePheno. Shown is data for each of the 8 length phenotypes,
where each point corresponds to an instance of the 50 samples chosen
for quantiﬁcation. Error bars show :I:1SD, estimated separately for each
quantiﬁcation approach. For manual quantiﬁcation, error bars correspond
to the empirical variation between three independent annotation runs. For
automated quantiﬁcation, uncertainty estimates stem from the variation of
the 15 nearest neighbor assignments on the template to the selected point and
measuring their SD. The green diagonals show the expected ideal correlation.

knowledge of their genetic constitution. Morphometric prototypes
for the guppy have previously been determined from hand—annotated
images and correlated to sex and environmental factors (Hendry
et al., 2006). We clustered deformation ﬁelds according to a
linear kernel between low—rank projections of Di (as described in
Section 2.3.1) using afﬁnity propagation (Frey and Dueck, 2007), a
non—parametric clustering technique that uses deformation kernel
values as inputs and yields a ﬂexible number of clusters |C|.
Reconstructing the mean low—rank vector ﬁeld of each cluster
given by its embedding in deformation space yields cluster—speciﬁc
morphological deformation bases. Figure 7 provides a comparative
overview of three characteristic clusters, indicating that independent
factors can inﬂuence the shape of the anterior and posterior trunk of
the examined guppy ﬁsh. Deformation bases correspond to low—rank
projections of the cluster means, where only the characteristic local

deformations per cluster are considered as shared elements of cluster
members. The shape seen in an example image representing the
median deformation ﬁeld per cluster corresponds to our expectation
given the proﬁles. Different clusters portray signiﬁcant variability
between their proﬁles, such as the regional focus and the extent of
expected local deformation.

3.6 Association study of shape factors to genotype

Finally, we performed a genome—wide association study using
the previously learned phenotypes and their measurements. The
phenotypic measurements y are the per—image coefﬁcients wi of
PCA—deformation bases CID (Sections 2.3.1 and 3.3). We used a
linear model that assesses how well a particular phenotypic value is
modeled when genetic factors are taken into account, compared to
when they are ignored. The relevant quantity is the log—odds (LOD)
score,

10 n 
g1“ J. Pojlebck)

where sj is a SNP measurement and yj the phenotypic expression
value for the j—th individual. The terms 6, QbCk are parameters for the
genetic and background models, respectively. We thus obtain LOD
score plots over a large genomic region to obtain an association plot.
We used Storey’s method (Storey and Tibshirani, 2003), a variant of
Benjamini Hochberg, to assess genome—wide signiﬁcance.

Although the available data has sparse genetic marker coverage,
we still obtained statistically meaningful peaks as can be seen in
Figure 8. Previous genetic QTL mapping in overlapping data has
suggested markers of the proximal region of linkage group 12
(LG12) as relevant for size and body shape traits in male guppies,
and in addition phenotypic sex has impact on these traits (Tripathi
et al., 2009). Among the signiﬁcant hits in our mapping, Markers
398 (lod 7.7 on LG12) and 442 (lod 10.3, LG12) are found in the
proximal region of LG12 while marker 229 (lod 11.9, LG12) is the
most distal and closest to the putative male sex—determining locus.
Depending on the trait analyzed, signiﬁcant QTL were suggested
within a region spanning ~ 6 cM (~7 Mb (Tripathi et al., 2009))
in cross 157. Marker 442 was supported as a QTL for area of
the posterior trunk for cross 158 (Tripathi, 2009). Additional loci
were detected with good statistical support, in agreement with the
observation that co—factors on various linkage groups contribute to
complex traits.

4 DISCUSSION

We have proposed a generative probabilistic model that extracts
deformation phenotypes by registering images to a latent, learned
template in an unsupervised fashion. Our method presents a novel,
clean framework for researchers to quantify and describe subtle
local deformation patterns and use them for downstream analyses,
like clustering or genetic association tests. We applied our method
to a bioimaging task, where we discovered signiﬁcant deformation
patterns in images of guppy ﬁsh. We also showed that ShapePheno
can be used for automated quantiﬁcation of geometric measurements
and showed good correspondence to manually labeled data.

More important than accurate geometric traits, ShapePheno
yielded deformation ﬁelds that characterize the variability in
shape and could be used to identify low—rank PCA factors

 

1 006

112 /3.IO'S[BUJHOIpJOJXO'SOIlBIIlJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

ShapePheno

 

Cluster Example



 

Deformation Basis

. . . . . . . . . . . . . . . . . . . . b > _ _ , g . . . . . . . . .

. . . . .. K 5..., ,,
3:: g {V‘éthuq/xu— 

.
¢ 1
, , , . . . . . . . .. .
 1». :. . . , . . . . . .. 
 ,-*’l
 ‘. kk . . . . . . ..
 ‘1'

  
 

    

   
    
 
 

Cluster 3

Fig. 7. Illustration of the clustering results obtained when using the PCA—deformation phenotypes from ShapePheno. From top to bottom the ﬁgure shows:
exemplar of the cluster, illustrating the representative ﬁsh chosen by Afﬁnity propagation to represent the given cluster. Deformation basis, showing the
corresponding deformation of the exemplar projected onto the PCA—deformation ﬁelds used for clustering. The exemplars approximately correspond to visual
categories of inﬂated anterior body, elongated anterior body and deformed tail, as present in the image collection.

 

 

 

 

 

 

 

 

16 846 LG 13'
14- 229 — PCA X1
12. 442 — PCA X2
210—1 39? —::::;
g :1 R: 0.10 l | , 142 LG 9  I I _
9 41 ‘ 1 j l I I I II I '
"I n I I I ‘ ‘ . I I 1 I -
ZIIHII'NI'IMWII  III .II II will 1...». » 1.1.1.111 II .tlv.1.t.~lll  I», II.  1.. III In 1.
00 100 200 300 400 . , 500 600 700 800

marker LG12

Fig. 8. Genome—wide association plot, showing the association strengths with the ﬁrst two PCA—deformation phenotypes corresponding to the X and Y
direction. The signiﬁcance threshold of 10% false discovery rate (FDR) is shown as thin line in the diagram. SNPs are plotted in order of linkage groups,
while signiﬁcant hits on LG12 as described in Section 3.6 are highlighted. LG13 contains markers with function in sex determination yielding additional

informative peaks for shape determination.

of shape variability. While simple distance measurements inter—
correlate strongly, the deformation phenotypes we propose describe
orthogonal shape factors and are thus novel holistic descriptors
of shape. We showed practical utility of these PCA—deformation
phenotypes in the context of clustering, grouping the data into
clusters exhibiting characteristic deformation. We also performed a
GWAs with the same traits, which yielded biologically sound results
in agreement with previous results on geometric approximations of
shape (see Tripathi et al. (2009) and unpublished observations of
CD). We are convinced that comprehensive genomic analyses on
larger datasets can be performed by using this method with a rigorous
treatment of image acquisition, higher image resolution and higher
marker density.

Unsupervised extraction and quantiﬁcation of subtle
morphological phenotypes, as done here, is the logical next
step in automated image analysis. The relevance of these new types
of methods is expected to rise quickly as dataset sizes increase,
providing the necessary statistical power to identify and quantify
complex phenotypic variation.

Funding: T.K. was supported by a Microsoft Research Cambridge
stipend and OS. was supported by a fellowship from the Volkswagen
Foundation.

Conﬂict of Interest: none declared.

REFERENCES

Alexander,H.J. and Breden,F. (2004) Sexual isolation and extreme morphological
divergence in the cuman guppy: a possible case of incipient speciation. J. Evolution.
Biol, 17, 1238—1254.

Bishop,C. (2006) Pattern Recognition and Machine Learning. Vol. 4, Springer,
New York.

Boykov,Y. et al. (2001) Fast approximate energy minimization via graph cuts. IEEE
Trans. Pattern Anal. Mach. Intell, 23, 1222—1239.

Frey,B.J. and Dueck,D. (2007) Clustering by passing messages between data points.
Science, 315, 972—976.

Fuchs,F. et al. (2010) Clustering phenotype populations by genome-wide RNAi and
multiparametric imaging. Mol Syst. Biol, 6, 370.

Hendry,A.P. et al. (2006) Parallel evolution of the sexes? Effects of predation and
habitat features on the size and shape of wild guppies. J. Evolution. Biol, 19,
741—754.

Kannan,A. et al. (2007) Clustering appearance and shape by learning jigsaws. In
Scholkopf,B. et al. (eds) Advances in Neural Information Processing Systems. MIT
Press, p. 2006.

N ing,F. et al. (2005) Toward automatic phenotyping of developing embryos from videos.
IEEE Trans. Image Process, 14, 1360—1371.

Pau,G et al. (2010) Ebimage—an R package for image processing with applications to
cellular phenotypes. Bioinformatics, 26, 979—981.

Peng,H. (2008) Bioimage informatics: a new area of engineering biology.
Bioinformatics, 24, 1827—1836.

Saalfeld,S. et al. (2010) As-rigid-as-possible mosaicking and serial section registration
of large system datasets. Bioinformatics, 26, i57—i63.

Shamir,L. et al. (2010). Pattern recognition software and techniques for biological
image analysis. PLoS Comput. Biol, 6, e1000974.

 

1 007

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

IKaraIetsos et al.

 

Storey,J.D. and Tibshirani,R. (2003) Statistical signiﬁcance for genomewide studies.
Proc. Natl. Acad. Sci, 100, 9440—9445.

Tripathi,N. et al. (2009) Genetic linkage map of the guppy, poecilia reticulata, and
quantitative trait loci analysis of male size and colour variation. P. Roy. Soc. B Bio.,
276, 2195—2208.

Tripathi,N. (2009) PhD Thesis, University of Tuebingen.

Walter,T. et al. (2010) Visualization of image data from cells to organisms. Nat. Methods,
7(Suppl. 3), S26—S41.

Whibley,A.C. et al. (2006) Evolutionary paths underlying ﬂower color variation in
antirrhinum. Science, 313, 963—966.

 

1 008

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

