Vol. 29 lSMB/ECCB 2013, pages i217-i226
doi:10. 1093/bioinformatics/btt245

 

Multitask learning for host—pathogen protein interactions

Meghana Kshirsagarl, Jaime Carbonell1 and Judith KIein-Seetharamah

1,2,3,*

1Language Technologies Institute, School of Computer Science, Carnegie Mellon University, 5000 Forbes Ave., PA
15213, USA, 2Forschungszentrum JUIich, Institute of Complex Systems (ICS—5), 52425 JOIich, Germany and 8Systems
Biology Centre, University of Warwick, Coventry CV4 7AL, UK

 

ABSTRACT

Motivation: An important aspect of infectious disease research in-
volves understanding the differences and commonalities in the infec-
tion mechanisms underlying various diseases. Systems biology-based
approaches study infectious diseases by analyzing the interactions
between the host species and the pathogen organisms. This work
aims to combine the knowledge from experimental studies of host—
pathogen interactions in several diseases to build stronger predictive
models. Our approach is based on a formalism from machine learning
called ‘multitask learning’, which considers the problem of building
models across tasks that are related to each other. A ‘task’ in our
scenario is the set of host—pathogen protein interactions involved in
one disease. To integrate interactions from several tasks (i.e. dis-
eases), our method exploits the similarity in the infection process
across the diseases. In particular, we use the biological hypothesis
that similar pathogens target the same critical biological processes in
the host, in defining a common structure across the tasks.

Results: Our current work on host—pathogen protein interaction pre-
diction focuses on human as the host, and four bacterial species as
pathogens. The multitask learning technique we develop uses a task-
based regularization approach. We find that the resulting optimization
problem is a difference of convex (DC) functions. To optimize, we
implement a Convex—Concave procedure-based algorithm. We com-
pare our integrative approach to baseline methods that build models
on a single host—pathogen protein interaction dataset. Our results
show that our approach outperforms the baselines on the training
data. We further analyze the protein interaction predictions generated
by the models, and find some interesting insights.

Availability: The predictions and code are available at: http://www.cs.
cmu.edu/~mkshirsa/ismb2013_paper320.htm|

Contact: j.klein-seetharaman@warwick.ac.uk

Supplementary information: Supplementary data are available at
Bioinformatics online.

1 INTRODUCTION

Infectious diseases are a major health concern worldwide, caus-
ing millions of illnesses and deaths each year. Newly emerging
viral diseases, such as swine H1N1 inﬂuenza, severe acute re-
spiratory syndrome (SARS) and bacterial infections, such as
the recurrent Salmonella and Escherichia coli outbreaks not
only lead to wide-spread loss of life and health, but also result
in heavy economic losses. To better navigate this landscape of
infectious diseases, it is important to not only understand the
mechanisms of individual diseases, but also the commonalities
between them. Combining knowledge from related diseases will
give us deeper insights into infection and host immune response,

 

*To whom correspondence should be addressed.

will enhance our ability to comprehend new diseases and lead to
efficient development of therapeutics.

Key to the infection process are host—pathogen interactions at
the molecular level, where pathogen proteins physically bind
with human proteins. Via these protein interactions, the patho-
gen manipulates important biological processes in the host cell,
evades host immune response and multiplies within the host.
Interactions between host and pathogen proteins can be studied
using small-scale biochemical, biophysical and genetic experi-
ments or large-scale high-throughput screening methods like
yeast two-hybrid (Y2H) assays. Databases like PHI-base
Minnenburg et al., 2008), PIG (Driscoll et al., 2009), HPIDB
(Kumar and Nanduri, 2010), PHISTO (Tekir et al., 2012) aggre-
gate host—pathogen protein interactions from several small-scale
and high-throughput experiments via manual literature curation.
These databases are valuable sources of information for develop-
ing models of the modus operandi of pathogens.

However, interaction datasets from these databases are not
only small but are available for only a few well-studied patho-
gens. For example, the PHI-base Minnenburg et al., 2008)
database covers 64 diseases but has only 1335 interactions,
PIG (Driscoll et al., 2009) covers only 12 pathogens.
Computationally, this calls for techniques that combine datasets
and build joint models across several pathogens, which can then
be used to analyze the commonalities in the pathogens and also
to predict plausible interactions that are biased by this joint
understanding.

In our work, we study host—pathogen protein—protein inter-
action (PPI) where the host is ﬁxed and the pathogens are vari-
ous bacterial species (Fig. 1A). The host species we consider is
human and the bacterial species are Yersinia pestis, Francisella
tularensis, Salmonella and Bacillus anthracis, which cause the dis-
eases bubonic plague, acute pneumonia, typhoid and anthrax,
respectively.

Some recent work on infectious diseases has alluded to the
hypothesis that a’iﬂerent pathogens target essentially the same
critical biological processes in the human body. The analysis by
Chen et al. (2012) suggests that HIV infection shares common
molecular mechanisms with certain signaling pathways and can-
cers. Dyer et al. (2008) study bacterial and viral interactions with
human genes and find infection mechanisms common to multiple
pathogens. Experiments by Jubelin et al. (2010) show how vari-
ous bacterial cyclomodulins target the host cell cycle. The study
by Mukhtar et al. (2011) on plant pathogens, in particular,
Arabidopsis concludes that pathogens from different kingdoms
deploy independently evolved virulence proteins that interact
with a limited set of highly connected cellular hubs to facilitate
their diverse life cycle strategies. Figure 1B illustrates an example
depicting the commonality in various bacterial species, where

 

© The Author 2013. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non—Commercial License (http://creativecommons.org/Iicenses/
by—nc/3.0/), which permits non—commercial re—use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial

re—use, please contact journals.permissions@oup.com

112 /310's113umo [p.IOJXO'SOllBIIIJOJUIOIQ/ﬁdllq 11101; popeoIII/noq

9IOZ ‘091sn3nv uo ::

M. Kshirsagar et al.

 

A F. tularensis

B
pathway-:/'
I

F. tularensis

     

pathway-2 J__
O
.'
Human  h  pathway 3 anthracis
" .- I
B. anthracis

$5”

Hist-1’? S. typhi

5' typh' Human

Fig. 1. (A) Host—pathogen PPI prediction where the host is human and
the pathogens are bacteria. (B) An example depicting the commonality in
the bacterial attack of human proteins. Pathway-l and pathway-3 (high-
lighted) represent critical processes targeted by all bacterial species

they are targeting the same biological pathways in their human
host.

This biological hypothesis, which we henceforth call the com-
monality hypothesis, is exploited here to jointly learn PPI models
for multiple bacterial species. We translate the hypothesis into a
prior that will bias the learned models. We use a multitask learn-
ing—based approach, where each ‘task’ represents the interaction
of one bacterial species with human. The prior is represented in
the form of a regularizer that penalizes models to the degree that
the above hypothesis is violated.

2 BACKGROUND

The most reliable experimental methods for studying PPI are
often time-consuming and expensive, making it hard to investigate
the prohibitively large set of possible host—pathogen inter-
actions—for example, the bacterium B.anthracis, which causes
anthrax, has about 2321 proteins, which when coupled with the
25 000 or so human proteins gives approximately 60 million pro-
tein pairs to test, experimentally. Computational techniques com-
plement laboratory-based methods by predicting highly probable
PPIs—thereby enabling experimental biologists to focus on fewer
interactions and ruling out the vast majority of unlikely ones.

In particular, supervised machine learning—based methods use
the few experimentally discovered interactions as training data
and formulate the interaction prediction problem in a classifica-
tion setting, with target classes: ‘interacting’ or ‘non-interacting’.
Features are derived for each host—pathogen protein pair using
various attributes of the two proteins such as protein sequence,
gene expression, gene ontology (G0) etc. The general outline of
the supervised PPI prediction procedure is illustrated in
Supplementary Figure Sl.

Most of the prior work in PPI prediction has focussed on
building models separately for individual organisms (Chen and
Liu, 2005; Qi et al., 2006; Singh et al., 2006; Wu et al., 2006) or
on building a model specific to a disease in the case of host—
pathogen PPI prediction (Dyer et al., 2007; Kshirsagar et al.,
2012; Qi et al., 2009; Tastan et al., 2009). The use of PPI data

from several organisms has predominantly been in the form of (i)
features derived from various PPI datasets, (ii) use of common
structural properties of proteins across organisms (\Nang et al.,
2007) or (iii) methods that narrow down predicted interactions in
the organism of interest (Garcia et al., 2010). Some of these
methods use the concepts of ‘homologs’, ‘orthologs’ and ‘inter-
ologs’ to define a similarity measure between PPIs from various
organisms (Garcia et al., 2010).

There has been little work on combining PPI datasets with the
goal of improving prediction performance for multiple organ-
isms. Qi et al. (2010) proposed a semi-supervised multitask
framework to predict PPIs from partially labeled reference sets.
The basic idea is to perform multitask learning on a supervised
classification task and a semi-supervised auxiliary task via a
regularization term. Another line of work in PPI prediction
(Xu et al., 2010) uses the Collective Matrix Factorization
(CMF) approach proposed by Singh and Gordon (2008). The
CMF method learns models for multiple networks by simultan-
eously factorizing several adjacency matrices and sharing param-
eters amongst the factors. Xu et al. (2010) use these ideas in their
transfer learning setting, where the source network is a relatively
dense interaction network of proteins and the objective is to infer
PPI edges in a relatively sparse target network. To compute
similarities between the nodes in the source and target networks,
they use protein sequences and the topological structures of the
interaction networks.

3 APPROACH

Multitask learning is a family of machine learning methods that
addresses the issue of building models using data from multiple
problem domains (i.e. ‘tasks’) by exploiting the similarity be-
tween them. The goal is to achieve performance benefits for all
the tasks involved. This paradigm of building joint models has
been applied successfully in many areas including text mining,
computer vision, etc. Because bioinformatics datasets often rep-
resent an organism, a natural notion of a ‘task’ is an ‘organ—
ism’—for example, the work by Widmer et al. (2010) uses a
multitask learning approach for splice-site prediction across
many organisms. They use phylogenetic trees to incorporate
similarity between organisms (i.e. tasks). For a survey of multi-
task learning in computational biology, see Xu and Yang (2011).
Our multitask learning method is based on the task regular-
ization framework, which formulates the multitask learning
problem as an objective function with two terms: an empirical
loss term on the training data of all tasks, and a regularization
term that encodes the relationships between tasks. Equation (1)
shows the general form of such an objective, the term R being the
regularizer raised to the power p and with a q-norm. The work by
Evgeniou and Pontil (2004) is one of the early few to develop this
general approach. The function in Equation (1) represents a
simple multitask objective with a single regularizer R; many of
the formulations often involve a summation over multiple terms.
L = 2 Loss (task,) —I— AllRllZ (1)
ietasks

We optimize this function by modifying the regularizer R to
encode the biological hypothesis. Our approach differs greatly
from prior work because we propose a technique to translate a

 

i218

112 /310's113umo [p.IOJXO'SOllBIIIJOJUIOIQ/ﬁdllq 11101; popeoIII/noq

9IOZ ‘091sn3nv uo ::

Combining host-pathogen PPI data

 

problem-relevant biological hypothesis into a task regulariza-
tion—based approach rather than applying existing general for-
malisms on a dataset. Our tasks try to capture a naturally
occurring phenomenon. While our framework is developed in
the context of a speciﬁc hypothesis, we also illustrate the incorp-
oration of other hypotheses with an example. The key contribu-
tions of our work are as follows:

0 we present a novel way of combining experimental PPI data
coming from several organisms

0 we incorporate domain knowledge in designing a prior that
causes the learned models to exhibit the requisite common
structure across the tasks

0 to optimize the resulting non-convex objective function, we
implement a concave convex procedure (CCCP)-based
method

In the Methods section (Section 4), we describe details of the
PPI datasets and our multitask learning framework. The evalu-
ation metrics and description of experiments is in Section 6,
results and analysis in Section 7.

4 METHODS
4.1 Multitask pathway—based learning

In this section, we describe how we incorporate the commonality hypoth-
esis into our multitask classiﬁcation framework formulating it as an
optimization problem.

We consider each human-bacteria PPI prediction problem as one task.
The predicton problem is posed as a binary classiﬁcation task, with each
instance xi being a pair of proteins <b,h>, where one protein is the
bacterial protein ‘b’ (e.g. Y. pestis) and the other ‘h’ is the host protein (i.e.
human). The class-label y’ 6 {+1, — 1} represents interacting and non-
interacting proteins respectively. Features are deﬁned for every protein-
pair using various properties of the individual proteins and combining
them all into a single feature vector. The positive class in our training data
comprises the known human-bacterial PPI which are obtained from data-
bases like PHISTO (Tekir et al., 2012). The construction of the negative-
class data is explained in Section 5.

Our objective is to minimize the empirical error on the training data
while favoring models that are biased toward the commonality hypoth-
esis. To achieve this, we use a bias term in the form of a regularizer in our
objective function. For brevity and without loss of generality, we will
henceforth refer to each human—bacteria PPI prediction problem as a
‘task’ (We will also refer to a task by the name of the bacterial species
only, as the host species, i.e. human, is common across all tasks).

Our method ﬁrst combines all tasks in a pairwise manner, and ﬁnally
aggregates the output from the pairwise models. Let {7132; be the set of
tasks to be combined, where m is the number of tasks. Consider two tasks
TS and 7,. Let the training data for the task 7, be X, = {xgli = l...ns}
where each example x; 6 Rd“. Similarly, the training data for T, is
X, = {lei = l .. .n,} where x: 6 Rd’. n, and n, are the number of training
examples and d, and d, denote the number of features in the two tasks.
Let w, 6 Rd“, w, 6 [Rd’ represent the parameter vectors, i.e. the models
for the two tasks. We now describe how we combine these two tasks.
Section 4.3 will show how such pairwise models are aggregated.

The pathway-based Objective Biologists often represent the set of
human proteins involved in a particular biological process by a graph
called a ‘biological pathway’. One such example, the ‘glucose transport
pathway’ in human is shown in the Supplementary. To use this pathway
construct, we revise our hypothesis to ‘proteins from different bacterial

species are likely to interact with human proteins from the same biolo-
gical pathway’. Figure 1B illustrates an example where this hypothesis
holds. The pathway information for each human protein can be obtained
from pathway databases like Reactome (Matthews et al., 2009) and PID
(Schaefer et al., 2009). While pathways are generally represented as
graphs, for our current work we do not use the edges. We treat a pathway
as a set of proteins—a human protein h can be a member of several
pathways depending on the biological processes it is involved in. Let N
be the total number of pathways in human. For a protein pair
i: <b,h>, let pi e {0,1}N be the binary ‘pathway vector’ indicating
the pathway membership of h.

The commonality hypothesis suggests that the pathway memberships
of human proteins from interactions should be similar across tasks. We
deﬁne a pathway-summary function S, which aggregates all pathway
vectors for a given task 7,. Because our hypothesis is about interactions,
we only consider pathway vectors of positive examples. Let Xj, X f rep-
resent the set of positive examples from tasks 7, and 7,; let nj, nf be
their sizes. In Figure 2, we depict the aggregation done by S.
Mathematically, we have

S(Ts) =  Z p21p0s(WsTX§ (2)
‘ ieX:

where p; is the pathway vector for example i, and I 1,0,(2) = [(2 > 0). S sums
up the pathway vectors of examples predicted to be positive. We normalize
using n: to compensate for the different dataset sizes across tasks.

Let P, = {pgli = l . . . nj} be a matrix containing all pathway vectors
for positive examples from task 7,. Analogously, P, e {0,1}N""t+ is a
matrix for the positive examples from task 7,. Matrices PS and P, are
constant matrices and are known a priori. Let S(TS) and S(T,) be the
pathway summaries of the tasks. We want to penalize the dissimilarity
between these summaries. Our objective function thus has the following
general form:

L(ws,w,) = l(w,) + l(w,) + MIRIIE + amwSIIé + IIWzIIE)
(3)
where R = S(TS) — S(T,).

Here l(w,) and l(w,) can be any convex loss functions computed over
the two tasks. We use logistic loss in our work based on prior experience
with PPI datasets. The last two 62 norms over the parameter vectors w,
and w, control overﬁtting. The parameters A and a take positive values.

The indicator function I pm is non-differentiable. So we approximate I pa,
with the exponential function, which is a convex upper bound of the

X+ P
@TO NEW <— P1—>
' .:> : => :
WEE <— io‘—>
pathway vectors
for each example ﬂ 2
4o

Sm: 

Set of predicted

interactions from
given input X

10
summary 0

function

I I I I _
2 3 4 5 6 7
pathways

Fig. 2. A schematic illustrating the pathway summarizing function S for
a task T 1. On the left are the examples from the input predicted to be
positive, indicated by X +. The matrix P has the pathway vectors for each
example in X +. The summary function aggregates the pathway vectors to
get the distribution

 

i219

112 /310's112umo [p.IOJXO'SOIlBIHJOJUIOICI/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn3nv uo ::

M. Kshirsagar et al.

 

indicator function and will make optimization easier. Let ¢(z) = eZ/C,

where C is a positive constant. This function, for various values of C has
been plot in Figure 3. Small positive values of z = WTx’ indicate positive-
class predictions that are closer to the decision boundary of the classiﬁer.
Examples predicted to be positive with a high conﬁdence have a large 2.
With varying values of C, the function (I) gives varying importance to pre-
dictions based on their classiﬁer conﬁdence ‘2’. Negative values ofz, which
correspond to examples predicted to be negative, are given close to zero
importance by (p. The choice of an appropriate C is important so as to
ensure the proper behavior of the summary function S. A steeply increas-
ing curve (C = 1) is undesirable as it will assign too much weight to the
summary of only some of the examples. We chose a moderate value of
C = 30 for our experiments.

Replacing [pas by (p in Equation (2), our summary function S becomes
S(TS) =  21.6 X: p; >¢(W5Tx§). Putting everything together, our object-
ive with the logistic loss terms, the pathway summary function and the £2
regularizer terms has the following form:

"5 , , nt - -
L<ws,w,) = Zlogh + e-Ws’xéys) + Zlog(1 + e—WW)
i=1 1:1
2 (3)
1 i i 1 ' '
+1 —+ Z p.¢<w3x,) — n—. 2 p’,¢(w,TX’,) +Rt2<ws, w,)
n5 text I jex,+ 2
where Rz,(WS,Wz) = 0(IIWSII5 + IIWzIIE)
The objective in Equation (3) is non-convex, and with some algebraic
simpliﬁcations we can reduce it to a difference of convex functions (DC).

To optimize this function, we implement the CCCP algorithm, which was
originally introduced by Yuille and Rangarajan (2003).

4.2 Solving the optimization problem

The objective in Equation (3) is non-convex in the shown form. We tried to
optimize it directly using L-BFGS, but found that the objective does not
decrease consistently. Below, we show that (3) is a DC functions. The ﬁrst
two log-loss terms [we abbreviate them henceforth as [(WS, w,)] and the last
R32 term are all convex and do not pose any problem with optimization.

PROPOSITION l. The objective {3) is a DC functions.
L(w59wl) : F(w59wl) _ G(w59wl) 

PROOF. Expanding the pathway vectors p; and  and rewrit-
ing Equation (3) we get
L 2 [(WS, w,) + R32(WS, W,)
N 1 . . 1 . .
+ x <—+ 2 p2“ ¢(W,TX;) — —. 2 pr «mixer
k=1 n5 text nl jex,+

N , (5)
L = l(ws, W) + Rtst, Wt) + A Z (fk — gk) ,Where
[(21

1 . . l - ~
fk = n_+ 21):“  and gk = n—+ :17]? 

 

 

5 text ’jexf

15 I ,
: ---c=1o
,' --c=3o
10 ,' —c= 50

I
z/C l —C—80
e

 

 

 

 

 

 

 

 

Fig. 3. The exponential function eZ/C for different values of C

Note that fk and gk are non-negative convex functions. This
follows because ¢(z) = eZ/C is a positive convex function and the
matrices PS and P, are non-negative by construction. fk and gk
are both thus positive linear combinations of convex functions
and hence convex. We now decompose the squared term in
Equation (5) as follows.

N 2 N N 2
EVk—gk) =I§2ﬁ+gi)—I;l(fk+gk) (6)

We further observe that fi is convex. To derive this, we use the
following proposition: a composition of a monotonically increas-
ing convex function and a convex function is still convex. The
square function h(z) = 22 is a monotonically increasing function
for z 2 0, thus the composition with fk (i.e. h(fk)) is also convex
by the positivity offk. Analogously, gi is also convex. Further,
(fk —I— gk)2 is also convex by the same argument. Substituting (6)
back into Equation (5) we get our result.

N
L = [l(w,,w,) + Rt,(ws,w,) + A Z 203’. + gin
k=1

N 7)
— [A 2 (f1. + gm (
k=1

L = F(WS, W,) — G(WS, W,)

To optimize this function, we use a CCCP algorithm (Yuille
and Rangarajan, 2003). Our approach is inspired by the work
from Yu and J oachims (2009) on learning structural SVMs. The
idea is to compute a local upper bound on the concave function
(—G) and instead of optimizing L from Equation (4) directly, use
an approximation based on the upper bound of —G. Equation
(7) shows this function Lappmx. Let W represent the concatenation
of the two parameter vectors W5 and W,. Let Wk be the k-th iter-
ate. We have from Taylor’s ﬁrst order approximation that
—G(W) g —G(Wk) + (W — Wk)TVG for all W. This allows us to
obtain the following approximation, which we get by substituting
the above bound in place of —G in Equation (4):

ngn L.,,...(w) = Hgnlﬂvv) — G<wk) + (w — wk)TVG]

= Invinwm) + WTVG] (7)

since Wk is a constant. The optimization problem in Equation (7) is now
convex and can be solved using conventional techniques like L—BFGS,
conjugate gradient, etc. The outline of our CCCP-based procedure is
shown in Listing 1.

 

Algorithm 1 CCCP procedure
0

 

l: Initialize W = W
2: repeat

3: Compute VG using Wk

Compute current value Lappmx

Solve Wk+1 = argmin [F(W) + WT VG]
Set k = k + l w
Compute new value L
: a = L,,,,,,,,, — L
9: until 6 < r

/
approx

WE]???

/
approx

 

Yuille and Rangarajan (2003) show that such a CCCP-based algorithm
is guaranteed to decrease the objective function at every iteration and to
converge to a local minimum or saddle point. We observe a similar be-
havior in our experiments. Computationally, this algorithm is efﬁcient

 

i220

112 /§JO's112umO [pJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn3nv uo ::

Combining host-pathogen PPI data

 

because the regularizer works on a subset of the data—only the positive
examples, which are a small fraction of the complete training data.

Stopping criteria The convergence criterion for algorithm 1 is 6<r,
where r is a threshold. We used I = 1 in our experiments. Smaller values
required a long time to convergence. The inner optimization (line # 5),
which uses L-BFGS, had a convergence threshold of 0.0001. This step
took more iterations initially and fewer iterations getting closer to
convergence.

4.3 Combining pairwise models

In the previous sections, we described how we combine two tasks. In
particular, Equation (3) involves pairwise learning, which results in two
models W5 and W,. Because our current framework can combine only two
tasks at a time, for m tasks we perform 
ments and then combine their outputs. Each task will thus have m — 1
models as a result of pairing up with each of the other tasks. Let the set of
models for task 7, be M, = {W51,W52 ...W5m_1}. We treat MS as an en-
semble of models for this task and aggregate the output labels from all
models to get the ﬁnal labels on the test data. Let the output labels from
each model for a given test instance x be 0,, = {01, 02 . . . om_1}. Then the
ﬁnal output label y is computed by taking a vote and checking if it crosses
a threshold:

) pairwise learning experi-

1 if(ZI(oj =1)) 2 v

— 1 otherwise

, = (8)

where v is a vote threshold that should be crossed in order for the label to
be positive. In our experiments, we found that the predictions for T, from
all models in MS overlapped greatly. Hence, we used v = 1, which implies
that x is an interaction if any one of our four tasks labels it as such.

5 DATASET AND FEATURES

For Salmonella typhi, we used the list of 62 interacting protein
pairs reported in Schleker et al. (2012), which were obtained by
the authors by manual literature curation. These interactions
come from small-scale experiments. The other three PPI inter-
action datasets were obtained from the PHISTO database. Most
of the reported interactions for these three bacterial species come
from a single high-throughput experimental study reported in
Dyer et al. (2010). While F.tularensis, S.typhi and Y.pestis are
gram-negative gamma-protobacteria, B.anthracis is a gram-posi-
tive bacteria. The number of unique proteins in each bacterial

Table 1. Characteristics of all four interaction datasets used

species, the sizes of all datasets and the number of all possible
host—pathogen protein pairs are listed in Table l.

5.1 Feature set

For each protein pair, we compute features similar to the work in
Kshirsagar et al. (2012). Some features use both proteins in the
pair, while some others are based on either the host protein or
the pathogen protein. While the features used for S.typhi were
obtained directly from the authors, those for the other three
datasets were derived from the following attributes of proteins
available in public databases: protein sequences from Uniprot
(UniProt Consortium, 2011), gene ontology from GO database
(Ashburner et al., 2000), gene expression from GEO (Barrett
et al., 2011), properties of human proteins in the human PPI
network. Owing to the lack of space, we brieﬂy mention only
some of the prominent features here, and encourage the readers
to refer to the supplementary for details. The sequence features
count the frequency of amino acid—based n-grams or n-mers (for
n = 2, 3, 4, 5) in the protein sequence. The G0 features count the
co-occurrence of host—pathogen GO term combinations. The
human PPI network-based features compute various graph prop-
erties like node-degree, betweenness-centrality, clustering coefﬁ-
cient of the human protein.

Our features deﬁne a high-dimensional and sparse space (the
number of features is listed in Table 1). Because our features are
derived by integrating several databases, some of which are not
complete, there are many examples and features with missing
values. In our current work, we eliminate all examples with
>10% missing features. For the rest, we use mean value—based
feature imputation. Handling missing data effectively is an im-
portant aspect of the PPI prediction problem; however, it is not
the focus of this work. The remaining examples after elimination
and imputation are also shown in Table l.

5.2 Negative class examples

The interactions listed in the table form the positive class. Because
there is no experimental evidence about proteins that do not inter-
act, we construct the ‘non-interacting’ (i.e. negative) class using a
technique commonly used in PPI prediction literature. We use
random pairs of proteins sampled from the set of all possible
bacteria—human protein pairs. The number of random pairs
chosen as the negative class is decided by what we expect the

 

 

B.anthracis F.tularensis Y.pestis S. typhi

Total no. of bacterial proteins (‘reviewed’ protein set 2321 1086 4600 3592
from UniprotKB)

Total no. of human—bacteria protein pairs 59.4M 27.8 M 117.7 M 87.7 M
No. of known interactions 3073 1383 4059 62
No. of interactions with no missing features 655 491 839 62
Size of training data with 1:100 class ratio 66 155 49 591 84 739 6262
No. of unique features in the training data 69 4715 468 955 886 480 349 155

 

Note: Total no. of human proteins: 25 596; M, million. For each host—pathogen PPI dataset, the number of pathogen proteins, the size of the dataset and other such statistics

are shown.

 

112 /§JO's112umO [pJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

M. Kshirsagar et al.

 

interaction ratio to be. We chose a ratio of 1:100 meaning that we
expect 1 in every 100 random bacteria—human protein pairs to
interact with each other. In general, there is no basis for choosing
a more meaningful ratio, as there are few known interactions. We
rely on previous work on better-studied organisms, where a ratio
of 1:100 was used, based on the number of known interactions.
Further, prior studies (Dyer et al., 2007; Tastan et al., 2009) also
use a similar ratio. This random selection strategy is likely to
introduce ~l % false negatives into the training set.

5.3 Analyzing the known interactions

We analyze the known host—pathogen interactions from our
datasets. This analysis also motivates our choice of a multitask
approach that uses a pathway-based similarity across tasks. The
known PPIs are compared across datasets in two ways: (i) path-
way enrichment and (ii) presence of interologs.

(i) The human proteins involved in each interaction dataset
are used to obtain the human pathways that are enriched.
We use Fisher’s test (based on the hypergeometric distri-
bution) to compute the P—value of each pathway. We plot
these P—values for each pathway, and for each dataset in
the form of a heatmap shown in Figure 4. The heatmap
shows how there are several commonly enriched pathways
across the datasets (the black vertical lines spanning all
four rows). It also shows the difference in the enrichment

    

 

 

B. anthracis
0.8
F tularenSI | - O_6
|| I I - 0.4
0.2
' ' ' ' 0
500 1000 1500 2000
Pathways

Fig. 4. Heatmap showing pathways enriched in each bacterial—human
PPI interactions dataset. The horizontal axis represents the pathways
(about 2100 of them) and the vertical axis represents the four datasets.
Each entry in the heatmap represents the P—value of a pathway w.r.t one
dataset. Darker values represent more enrichment. The black columns
that span across all four rows show the commonly enriched pathways

for the S.typhi dataset, which comes from small-scale PPI
experiments.

(ii) We analyze the similarity between the PPIs from various
datasets. A natural way to determine similarity is to check
if proteins known to interact in one dataset have homolo-
gous proteins that are also interacting in another dataset.
Such pairs of proteins, also called ‘interologs’, are deﬁned
as a quadruple of proteins A, B, A’, B’, where A <—> B
(interaction) and A’ <—> 3’. Further, A, A’ are homologs
and B, B’ are also homologs. The number of such inter-
ologs existing between the four datasets is shown in
Table 2. To compute homologs of a protein, we used
BLASTP sequence alignment with an e-value cutoff of
0.1. As evident from Table 2, there are few interologs
across the bacterial PPIs. None of the high-throughput
datasets have an interolog in the small-scale S.typhi data-
set. This seems to indicate that interolog-based approaches
to compute task similarity are not relevant here. The phe-
nomenon governing the similarity of these host—pathogen
interactions is probably at a much higher level, rather than
at the level of individual proteins. We explore one such
possibility—the ‘commonality hypothesis’.

6 EXPERIMENTS

We use 10-fold cross validation (CV) to evaluate the perform-
ance of all algorithms. Our evaluation criteria do not use accur-
acy, which measures performance on both the classes. Because
our datasets are highly imbalanced with a large number of nega-
tive samples, a naive classiﬁer that always says ‘no’ would still
have a high accuracy. We instead use precision and recall com-
puted on the interacting pairs (positive class) because they can
deal with class imbalance.

number of true positives

 

P c c P = .
rec1s10n ( ) number of predicted positives,

number of true positives

 

eca ( ) total number of true positives in data’
2PR
F1 F1 2 —
score ( ) P + R

The baselines that we compare against are brieﬂy described
below.

Independent models (Indep.): We train models independently on
each task using two standard classiﬁers: Support Vector
Machines and Logistic regression with 51 and £2 regularization.
We used LibLinear (Fan et al., 2008) for these experiments and
found that logistic regression with 51 regularization performs the

Table 2. Conserved interactions in the form of interologs across the various host—bacterial datasets

 

 

Human—bacteria PPI datasets compared H-B versus H-B versus H-B versus H-F versus H-F versus H-Y versus
H-F H-Y H-S H-Y H-S H-S
Number of Interologs 2 3 0 3 0 0

 

Note: H-X: stands for human—pathogen where the pathogen ‘X’ can be B, F, Y and S referring to B.anthracis, F. tularensis, Y.pestis and S.typhi, respectively. The non-zero
entry ‘2’ for ‘H-B versus H-F’ means there are two PPIs in the H-B dataset that have interologs in the H-F dataset.

 

i222

112 /§JO's112umO IPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

Combining host-pathogen PPI data

 

best across all tasks. For conciseness, we report only the best
model’s performance.

Coupled models: This baseline was implemented so as to couple
the regularizer parameter across two tasks, thereby keeping the
basic framework similar to that in our technique. To achieve this
we optimize the function in Equation (9) and use the L-BFGS
implementation from Mallet. Note that the previous baseline has
separate regularization parameters for each task.

L = z" log(l + eTWsTXiyi) + :1og(1 + e-W?X€%)+

i=1 j:1

2 2
0(IIWSII2 +||Wt||2)

Independent models With pathway features (Indep. Path.): This
baseline incorporates the pathway information from the pathway
vectors p’ as features. For each example i, the feature vector is
appended by the pathway vector p’. While our method uses the
pathway vectors only for the positive class examples (via the
matrices PS and P,), this baseline uses the pathway information
for all examples via features. The performance of this baseline
will indicate whether using raw pathway information without
incorporating any biologically relevant coupling does well. We
learn independent models for each task as before, and ﬁnd that
logistic regression with 51 regularization does the best (only these
results are reported).

(9)

Mean Multi-task Learning (Mean MTL): This is a logistic regres-
sion-based implementation of the multitask SVM model pro-
posed by (Evgeniou and Pontil, 2004). The important feature
of this work is the use of a regularizer that penalizes the differ-
ence between a model and the ‘mean’ model formed by averaging
over models from all m tasks. In the original paper, the loss
functions l(W,) were all hinge loss. Because we ﬁnd that logistic
regression does better on our datasets, we replaced the original
hinge loss function by logistic loss. The objective we use is shown
in Equation (10).
2 m

+0; “Willi (10)
2 ’2

Ms

L = Wi—%ZWJ'

J

l

l(w,) + A Z
1 i=1

 

 

 

 

Multitask pathway-based learning: This refers to our technique,
which minimizes the sum of logistic loss over the two tasks with
an 62 regularization penalizing the difference between the path-
way summaries. We train two tasks at a time and compute the
performance for each task. Because we have four tasks, there are
six such pairwise learning experiments in all. While evaluating
performance during 10-fold CV, we obtain the F1 on l-fold of a
task 7, by averaging the F1 across all pairwise learning experi-
ments that involve 7, (see Section 4.3 for details). The ﬁnal CV
performance reported in our results is an average over 10-folds.

6.1 Parameter tuning

We followed an identical procedure for all algorithms. For the
10-fold CV experiments we train on 8-folds, use l-fold as held-
out and another as test. The optimal parameters (i.e. the best
model) were obtained by parameter tuning on the held-out fold.
The test fold was used to evaluate this best model—these results

are reported in Section 7. The range of values we tried during the
tuning of the regularization parameter (A) were 150—10_4. For a,
the parameter controlling overﬁtting in multitask pathway—
based learning (MTPL), we used a ﬁxed value of o = 1. For
Mean MTL, we tune both A and a. To handle the high-class
imbalance in our data, we used a weight-parameter Wpos to in-
crease the weight of the positive examples in the logistic loss
terms of our function. We tried three values and found

WM, 2 100 performed the best on training data.

7 RESULTS AND DISCUSSION

7.1 Overall performance

Table 3 reports for each bacterial species, the average Fl along
with the standard deviation for the 10-fold CV experiments. The
performance of all baselines is similar, and our method outper-
forms the best of the baselines by a margin of 4 points for
B.anthracis, 3.4 points for F.tularensis and 3.2 points for
Y.pestis and 3.3 for S.typhi. The overall performance of all meth-
ods on this dataset is twice as good as that on the others. We
believe that the difference in the nature of the datasets might
explain the above observations. While the S.typhi dataset com-
prises small-scale interaction studies, the other datasets come
from high-throughput experiments. Owing to its smaller size, it
has less variance making it an easier task. This dataset is also
likely to be a biased sample of interactions, as it comes from
focussed studies targeting select proteins.

The coupled learner (Coupled) performs slightly worse than
Indep. This is explained by the fact that Indep. has more ﬂexi-
bility in setting the regularization parameter for each task separ-
ately, which is not the case in Coupled. It is interesting to note
that the independent models that use the pathway matrices PS
and P, as features (i.e. Indep-Path) show a slightly worse per-
formance than the Indep. models that do not use them. This
seems to suggest that the cross-task pathway similarity structure
that we enforce using our regularizer has more information than
simply the pathway membership of proteins used as features.
Precision-Recall curves: We also plot the P-R curves for
MTPL. Please see the Supplementary Section 3.

7.2 Paired t—tests for statistical signiﬁcance

Given two paired sets of k measured values, the paired t—test
determines whether they differ from each other in a signiﬁcant

Table 3. Averaged 10—fold CV performance for all methods for a posi-
tive:negative class ratio of 1:100

 

 

Method B.anthracis F tularensis Y.pestis S. typhi

Indep. 27.8:I:4 25.7:I:5.4 28.8:I:4 72.5:I: 11.4
Coupled 27 :I: 3.9 25.5 :I: 5 27.9 :I: 3.4 69.8 :I: 12.4
Indep. Path. 26.5 :I: 4.7 26.1 :I: 6.9 26.7 :I: 4.3 69.1 :I: 12.7
Mean MTL 25.2:I:4.9 26.7:I:4 27.5:I:6.3 69.4:I: 12.1
MTPL 31.8 :I: 3.9 30.1 :I: 5.8 32.1 :I: 2.5 75.8 :I: 12.1

 

Note: Accuracy is reported as the F1 measure computed on the positive class. The
standard deviation over the 10-folds is also reported. Bold values indicate the high-
est Fl value for each column (i.e. for that PPI dataset).

 

112 /§JO's112umO IPJOJXO'SOIlBIIIJOJUIOIQ/ﬁ(11111 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

M. Kshirsagar et al.

 

way. We compare MTPL with Indep.—the best baseline from
the 10-fold CV results. Because the 10-fold CV results from the
previous section give insufﬁcient samples (i.e. only 10 samples),
we instead use 50 bootstrap sampling experiments and use the
results to compute the P—values. Each bootstrap sampling experi-
ment consists of the following procedure: we ﬁrst make two
random splits of 80 and 20% of the data, such that the class
ratio of 1:100 is maintained in both. The training set is then
constructed using a bootstrap sample from the 80% split and
the test data from the 20% split. A total of 50 models are thus
trained and evaluated. We do not tune parameters again for each
model and instead use the optimal setting of parameter values
from our 10-fold CV experiments. The F1 is computed for each
experiment thereby giving us 50 values, which will be our samples
for the hypothesis test.

Because t-tests assume a normal distribution of the samples,
we ﬁrst did a normality test on each set of 50 F1 values. We
performed the Shapiro—Wilk test with a signiﬁcance level of
a = 0.00001 and found that our samples satisfy normality.

The averaged Fl over the 50 bootstrap experiments for the
four tasks with MTPL and Indep. has been tabulated in the
Supplementary Table S1. We observe that MTPL does better
than Indep. for the three high-throughput datasets and margin-
ally underperforms for the S.typhi. dataset. Table 4 shows the
P—values on applying the paired t-tests to the 50 F1 values. For
three of the four tasks, the performance improvement by MTPL
is clearly statistically signiﬁcant. For the fourth task, which in-
volves S.typhi, the baseline has a slightly better averaged per-
formance but the P—value does not indicate statistical
signiﬁcance. Hence we can say that the performance of MTPL
and Indep. is similar for this task.

7.3 Pairwise performance of tasks in MTPL

The previous section gave a summary of the aggregated perform-
ance of MTPL for every task. Here we present the performance
of every pairwise learning experiment of MTPL in Table 5. This
gives an idea of how various tasks beneﬁt from being paired up
with other tasks. For each task, we check the task-pairing that
gave the best performance (best F1 for each task is shown in
bold). For instance, the best Fl of 32.3 for Y.pestis was obtained
in the pairwise model learned with S.typhi. It is evident that
coupling a model with one additional task seems to improve
the performance over the baseline.

7.4 Feature importance across tasks

To get an understanding of inter-task model similarity, we com-
pared the parameter vectors ‘W’ of all tasks with each other (each
W was learned on the entire training data). Because the number
of features is large, we computed the cosine similarity between
them. Note that we only use features that are common across
tasks for this comparison. Gene expression features for instance
were not used as they vary with regard to the number of expres-
sion time points, the experiment protocol, etc.

We found that the feature weights vary greatly across
models—the cosine similarity ranges between 0.1 and 0.13. We
also analyzed which features had the highest absolute weight. We
found that the node-degree feature (computed using the human
PPI graph) has a high positive weight across all tasks. Gene

Table 4. P—values from pairwise t-tests of statistical signiﬁcance

 

B.anthracis F. tularensis Y.pestis S. typhi

 

P—value 4.1e-04a 9.1e-04a 2.2e-07a 0.1b

 

Note: We compare MTPL with the best baseline ‘Indep.’, using results from 50
bootstrap sampling experiments. The null hypothesis is ‘there is no signiﬁcant dif-
ference between the performance of MTPL and Indep.’.

Null hypothesis: MTPL = Indep.

aAlt. hypothesis: MTPL> Indep.

bAlt. hypothesis: MTPL< Indep.

Table 5. Pairwise model performance of MTPL

 

 

 

Pairwise tasks F1

Task-1, Task-2 Task-1 Task-2
B.anthracis, F.tularensis 31.4 30.1
B.anthracis, S.typhi 32 76.3
B. anthracis, Y.pestis 31.6 32
F.tularensis, S.typhi 30.3 73
F.tularensis, Y.pestis 30 32.1
S.typhi, Y.pestis 74.2 32.3

 

Note: F1 computed during 10-fold CV of various pairwise models from MTPL.
Positive: negative class ratio was 1:100. The best F1 achieved for each task (i.e. for
each bacterial species) is shown in bold. For example, B.anthracis has the best
performance of 32 when it is coupled with S.typhi.

expression features have large negative weights across all tasks.
In general, the GO and protein sequence—based n-gram features
have different weights across tasks.

This seems to imply that having similar parameter values
across models is not particularly important for this multitask
problem. This explains why one of our baselines: the Mean
MTL method, which penalizes differences between parameter
vectors, does not perform well. Instead, regularization using the
pathway summaries seems key in giving a better performance.

Sparsity of weights: We use 62 regularization in our optimiza-
tion function, which does not produce sparse weight vectors. We
observe that ~50% of the features have 0 weight in all tasks.
About 75—80% of the features have small weights in the range of
(0.001 to —0.001).

7.5 Analysis of predictions

The F1 measure gave us a quantitative idea of the performance
of each method on training data. In this section, we present a
qualitative analysis of the new interactions that our models pre-
dict. We ﬁrst construct, for each task ‘T,’, a random set R, of
protein pairs that is disjoint from the training dataset. We train
the pairwise models on the training data and obtain predictions
on R,. The method described in Section 4.3 is used to aggregate
predictions from all pairwise models. The subset of R, labeled as
‘positive’ is used for the analysis described below.

 

i224

112 /§JO's112umO IPJOJXO'SOIlBIIIJOJUIOIQ/ﬁ(11111 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

Combining host-pathogen PPI data

 

F. TUIHI‘E I'lSIS

1". Pestis

    

El.Anthrat:is

Fig. 5. The intersection of enriched human pathways from predicted
interactions. The total number of enriched pathways for each bacterial
species are B.anthracis: 250, F.tularensis: 164, Y.pestis: 400 and S.typhi:
40. The size of the intersection between all tasks’ enriched pathways is 17.
The size of this intersection for the high-throughput datasets (excluding
S.typhi) is much larger: 104

Enriched human pathways We perform enrichment analysis on
the human pathways from the positive predictions of MTPL. We
use Fisher’s exact test with the hypergeometric distribution. We
intersect the top enriched pathways that satisfy P—value g le-07
from each task to get the commonly enriched pathways. The
sizes of the various intersections are shown in Figure 5.
Seventeen pathways are commonly enriched across all four
tasks. One hundred four pathways are enriched across the
three high-throughput datasets, which is a signiﬁcant fraction
of the total number of pathways considered. This result indicates
that the bias produced by our regularizer does produce predic-
tions satisfying the commonality hypothesis.

Table 6 shows some of the common enriched pathways. The
‘Integrin alpha IIb beta3 (aIIb ,83) signaling’ pathway is enriched
only in B.anthracis and Y.pestis in the training data. However, in
the predictions it is enriched in all four bacterial datasets.
Integrin-aIIb ,83 is a transmembrane receptor expressed in
mast cells and plays an important role in innate immune re-
sponses against pathogens.

We also analyze the overlap between the pathways enriched in
the gold-standard positives and those enriched in the predictions.
Please see the Supplementary Section 5 for details.

7.6 Incorporating other biological hypotheses

The regularizer in Equation (3) uses the pathway information
matrix to enforce pathway-level similarity. The matrix can be
used to represent any other common structure. For example,
consider the hypothesis ‘all pathogens target hub proteins in the
host’, which implies that bacterial proteins are often found to
interact with host proteins that have a high node degree in the
PPI network of the host. We tried two variants to incorporate
this hypothesis4(i) we identify ‘hubs’ in the human PPI graph
and use the binary vectors pi as an indicator of the ‘hub’ protein
targeted by the bacterial protein, (ii) instead of a discrete ‘hub’/
‘not hub’ indicator we use p’ to represent the node degree [each
component of pi represents one node-degree bin say (10—20)]. We

Table 6. Five of the 17 commonly enriched pathways in the predicted
interactions from MTPL

 

Platelet activation, signaling and aggregation

Integrin alpha IIb beta3 signaling

Stabilization & expansion of E-cadherin adherens junction
Post-translational regulation of adherens junction stability & disassembly
Signaling by NGF

 

found that using (i) gives us an improvement of upto 2.5 F points
over the baseline methods.

8 CONCLUSION

We presented a method that uses biological knowledge in jointly
learning multiple PPI prediction tasks. Using a task regulariza-
tion—based multitask learning technique, we were able to encode
a biological hypothesis into the optimization framework effect-
ively, thus enabling the commonality hypothesis to be tested. Our
results indicate that the tasks beneﬁt from this joint learning and
we see an improvement of 4 F points over the baseline methods.

While our current results were presented on four bacterial spe-
cies, we plan to extend our analysis to several other pathogens.
Another direction to explore is the case where the pathogen is
ﬁxed and the hosts are different.

Our current approach integrates multiple tasks in a pairwise
manner, which is inefﬁcient because it does not scale well while
integrating several PPI datasets. The most straightforward way
of extending Equation (3) to learning m tasks simultaneously
involves loss terms for each of the tasks and 0(m2) pairwise
regularization terms, which unfortunately makes the optimiza-
tion problem more complex and inefﬁcient. A more promising
and efﬁcient direction would be to consider model selection at
the task level where only the most relevant and useful tasks are
used for multitask learning.

ACKNOWLEDGEMENT

We would like to thank Kevin Gimpel for a helpful discussion of
DC programming and the reviewers for their excellent feedback.

Funding: The work has been supported in part by the Richard
King Mellon Foundation, the EraSysBio+ grant funds from the
European Union and BMBF to the Salmonella Host Interactions
Project European Consortium, SHIPREC, as well as NIH grants
P50GM082251 and 2ROlLM007994-05, and NSF grant CCF-
1144281.

Conflict of Interest: none declared.

REFERENCES

Ashburner,M. et al. (2000) Gene ontology: tool for the uniﬁcation of biology. Nat.
Genet., 25, 25—29.

Barrett,T. et al. (2011) NCBI GEO: archive for functional genomics data set3710
years on. Nucleic Acids Res, 39, D1005—D1010.

Chen,X. and Liu,M. (2005) Prediction of protein-protein interactions using random
decision forest framework. Bioinformatics, 21, 4394—4400.

 

112 /810's112um0 prOJXO'SOIlBIIlJOJUIOIQ/ﬂdllq 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

M. Kshirsagar et al.

 

Chen,K.C. et al. (2012) Associations between HIV and human pathways revealed
by protein-protein interactions and correlated gene expression proﬁles. PLoS
One, 7, 634240.

Driscoll,T. et al. (2009) Pig—the pathogen interaction gateway. Nucleic Acids Res,
37, D647—D650.

Dyer,M. et al. (2007) Computational prediction of host-pathogen protein-protein
interactions. Bioinformatics, 23, i159—i166.

Dyer,M. et al. (2008) The landscape of human proteins interacting with viruses and
other pathogens. PLoS Pathog, 4, e32.

Dyer,M. et al. (2010) The human-bacterial pathogen protein interaction networks of
Bacillus anthracis, Francisella tularensis, and Yersinia pestis. PLoS One, 5,
612089.

Evgeniou,T. and Pontil,M. (2004) Regularized multi-task learning. In: SIGKDD.
ACM, New York.

Fan,R. et al. (2008) Liblinear: a library for large linear classiﬁcation. J. Mach.
Learn. Res, 9, 1871—1874.

Garcia,J. et al. (2010) Biana: a software framework for compiling biological inter-
actions and analyzing networks. BM C Bioinformatics, 11, 56.

Jubelin,G. et al. (2010) Pathogenic bacteria target NEDD8-conjugated cullins to
hijack host-cell signaling pathways. PLoS Pathog, 6, 61001128.

Kshirsagar,M. et al. (2012) Techniques to cope with missing data in host-pathogen
protein interaction prediction. Bioinformatics, 28, i466—i472.

Kumar,R. and Nanduri,B. (2010) HPIDB—a uniﬁed resource for host-pathogen
interactions. BM C Bioinformatics, 11 (Suppl. 6), $16.

Matthews,L. et al. (2009) Reactome knowledgebase of human biological pathways
and processes. Nucleic Acids Res, 37, D619—D622.

Mukhtar,M.S. et al. (2011) Independently evolved virulence effectors converge onto
hubs in a plant immune system network. Science, 333, 596—601.

Qi,Y. et al. (2006) Evaluation of different biological data and computational clas-
siﬁcation methods for use in protein interaction prediction. Proteins, 63,
490—500.

Qi,Y. et al. (2009) Systematic prediction of human membrane receptor interactions.
Proteomics, 23, 5243—5255.

Qi,Y. et al. (2010) Semi-supervised multi-task learning for predicting interactions
between HIV-1 and human proteins. Bioinformatics, 26, i645—i652.

Schaefer,C.F. et al. (2009) PID: the pathway interaction database. Nucleic Acids
Res, 37, D674—D679.

Schleker,S. et al. (2012) The current salmonella-host interactome. Proteomics Clin.
Appl., 6, 117—133.

Singh,A.P. and Gordon,G.J. (2008) Relational learning via collective matrix factor-
ization. In: KDD. ACM, New York.

Singh,R. et al. (2006) Struct2net: integrating structure into protein-protein inter-
action prediction. Pac. Symp. Biocomput., 403—414.

Tastan,O. et al. (2009) Prediction of interactions between HIV-1 and human pro-
teins by information integration. Pac. Symp. Biocomput., 516—527.

Tekir,S.D. et al. (2012) Infection strategies of bacterial and viral pathogens through
pathogen-host protein—protein interactions. Front. M icrobiol., 3, 46.

UniProt Consortium. (2011) Ongoing and future developments at the universal
protein resource. Nucleic. Acids Res, 39, D214—D219.

Wang,R.S. et al. (2007) Analysis on multi-domain cooperation for predicting pro-
tein-protein interactions. BM C Bioinformatics, 8, 39.

Widmer,C. et al. (2010) Leveraging sequence classiﬁcation by taxonomy-based
multitask learning. In: RECOMB. Springer-Verilag, Berlin.

Winnenburg,R. et al. (2008) Phi-base update: additions to the pathogen host inter-
action database. Nucleic Acids Res, 36, D572—D576.

Wu,X. et al. (2006) Prediction of yeast protein-protein interaction network: in-
sights from the gene ontology and annotations. Nucleic Acids Res, 34, 2137—2150.

Xu,Q. and Yang,Q. (2011) A survey Of transfer and multitask learning in bioinfor-
matics. J. Comput. Sci. Eng, 5, 257—268.

Xu,Q. et al. (2010) Protein-protein interaction prediction via collective matrix fac-
torization. In: International Conference on Bioinformatics and Biomedicine.
IEEE, Hong Kong.

Yu,J. and J oachims,T. (2009) Learning structural SVMS with latent variables. In:
International Conference on Machine Learning. ACM, New York.

Yuille,A. and Rangarajan,A. (2003) The concave-convex procedure. Neural
Comput., 15, 915—936.

 

i226

112 /810's112umo IPJOJXO'SOIlBIIIJOJUIOIQ/ﬁ(11111 111011 pop1201umoq

9IOZ ‘091sn8uv uo ::

