APPLICATIONS NOTE V°" 333%.539232755322533/215333

 

Systems biology

Advance Access publication July 7, 2014

MCMC_CLlB-an advanced MCMC sampling package for ODE models

Andrei Kramer1 ’*, Vassilios StathOpOUIOSZ, Mark Giirolami3 and Nicole Radde1

1Institute for Systems Theory and Automatic Control, University of Stuttgart, 70569 Stuttgart, Germany, 2Department of
Statistical Science, University College, London WC1 E BBT, UK and 3Department of Statistics, University of Wan/Vick,

Coventry CV4 7AL, UK

Associate Editor: Jonathan Wren

 

ABSTRACT

Summary: We present a new C implementation of an advanced
Markov chain Monte Carlo (MCMC) method for the sampling of
ordinary differential equation (ODE) model parameters. The software
MCMC_CLIB uses the simplified manifold Metropolis-adjusted Langevin
algorithm (SMMALA), which is locally adaptive; it uses the parameter
manifold’s geometry (the Fisher information) to make efficient moves.
This adaptation does not diminish with MC length, which is highly
advantageous compared with adaptive Metropolis techniques when
the parameters have large correlations and/or posteriors substantially
differ from multivariate Gaussians. The software is standalone (not a
toolbox), though dependencies include the GNU scientiﬁc library and
sundials libraries for ODE integration and sensitivity analysis.
Availability and implementation: The source code and binary files are
freely available for download at http://a—kramer.github.io/mcmc_clib/.
This also includes example files and data. A detailed documentation, an
example model and user manual are provided with the software.
Contact: andrei.kramer@ist.uni-stuttgart.de

Received on February 2, 2014; revised on June 28, 2014; accepted on
June 30, 2014

1 INTRODUCTION

Modeling intracellular reaction networks by ordinary differential
equations (ODES) has become a standard approach in systems
biology. ODES provide a quantitative and dynamic description
of the underlying biochemical processes. However, parameter
estimation for these models is a challenging task, as typically
only few data points are available, and the respective optimiza-
tion problem is ill-posed. That is, the data do not contain sufﬁ-
cient information to identify parameter values uniquely. In a
statistical Bayesian framework, this results in high correlations
of parameters and/or multiple modes in the posterior distribu-
tion. Standard sampling algorithms frequently fail when facing
those problems. The Metropolis algorithm in its original form
(Hastings, 1970; Metropolis et al., 1953), for example, is elegant
and easy to implement but uses an isotropic distribution to sug-
gest the next set of parameters and struggles with complicated
posteriors. In the case of strongly correlated parameters, some
directions are highly prohibitive because of low likelihoods,
whereas others are permissive because of low sensitivity of the
model output in these directions, i.e. ﬂat likelihoods. This results
in highly auto-correlated samples and extremely slow conver-
gence rates. Adaptive variants of Metropolis that try to amend

 

*To whom correspondence should be addressed.

this problem do exist; they use diminishing adaptation that can
deal with strong correlations of sampling variables in some cases.
One such implementation is described in Haario et al., 2006. The
adaptive Metropolis algorithm adapts to the posterior by esti-
mating its covariance from previously sampled points. While this
can drastically increase sampling efﬁciency for densities that are
similar to Gaussians, it struggles with more complex cases. To
date, sampling techniques that put more computational effort
into the proposal step are often implemented in high-level pro-
gramming languages only and so are not efﬁcient computation-
ally. This prompted our efforts to provide a highly efﬁcient
specialized tool that spends most of its running time on the
solution of initial value problems.

We introduce MCMC_CLIB, an implementation of SMMALA
(simpliﬁed manifold Metropolis-adjusted Langevin algorithm),
a sampling algorithm, described in Girolami and Calderhead,
2011 that has been shown to outperform simpler sampling
algorithms.

2 APPROACH

Our approach is motivated by modeling practices in systems
biology, where model parameters pk are typically required to
be of ﬁxed sign for model stability. Otherwise, the model
structure is fairly generic. Although the software will always
supply pk =exp (9k)>0 to the model (see Availability and
Implementation section), the framework can be manipulated to
accommodate sign switching. Using 9 for sampling, i.e. sampling
in logarithmic space, has the beneﬁt of easily covering several
orders of magnitude. An important novel feature is the ability of
the implemented likelihood function to deal with relative data
like Western blots in arbitrary units.

2.1 Modeling

We assume an ODE model given in state-space representation:

ic=f(x;p,uk), xeR”, peRi, (1)

yjk = CxUj; P, ilk) +5jka yjk E Rm, (2)

with additive Gaussian noise 6,), e R’"~N(O, Ejk). We assume
the relation between state and measurement to be linear,
characterized by C e Rm” (it may, however, be scaled by an
unknown parameter, i.e. data may be in arbitrary units). Index
k= 1, ...,nE enumerates different experiments (described by
different inputs uk 6 IR], e. g. drug dosage), whereas j marks dif-
ferent measurement time instances. For simplicity, we assume

 

© The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 2991

112 ﬂJO'slcumo[pJOJXO'sopchogutotq/ﬁd11q IIIOJJ papeolumoq

910K ‘09 lsnﬁnV no :2

A.Kramer et al.

 

 

model VFGEN GCC

pﬁor

 

given

 

 

 

 

Fig. 1. Flowchart of the usage structure

independent noise with standard deviation ol-J-k for each experi-
ment, each output variable and each time point, such that EJ- =
diag(o.J-k) contains variances of outputs yl-jk (i= 1, . . . , m). Given
data D = {yiJ-k, 0121.51,“, the likelihood reads

, 2
LD(0)=CXP (-%2 (W _ :1 Ciwj’a’ 1"”)  (3)

2',ch 027"“

 

Using Bayesian learning, we construct the posterior distribu-
tion p(0|D) oc LD(0)p(0) with prior distribution p(0)~N(/1., E)
that is implemented as a multivariate Gaussian distribution

H

with mean ,u. and covariance matrix ca.

3 METHODS

The original and the SMMALA algorithms are comprehensively
described in Girolami and Calderhead, 2011 and Calderhead, 2012,
respectively, and we only give a glimpse of its structure here.

3.1 The simpliﬁed manifold MALA algorithm SMMALA
The Markov chain (MC) is a discrete time approximation of the SDE

d0= %G_1(0)V0V(0; D)dt+ choI(G-1(0»db(t), (4)

where V(0;D) = log (p(0|D)) and db(t) is a Wiener process. The natural
metric G(0) of the parameter space is the Fisher information; chol de-
notes the Cholesky decomposition, a function that returns the Cholesky
factor. We can write the MC proposal 0 —> 0’ with step size 6 e IR+ as

1909’ I0) = N (1109, 6), M09, 6)) (5)
v(0, e) = 0 + g c—1(0)v0 V(0; D) (6)
M(0, 6)=€2G_1(0). (7)

This step proposes a parameter vector from a Gaussian distribution
with mean and covariance that take information about the local shape
of the posterior distribution via the Fisher matrix and the gradient
into account. The metric tensor G is calculated using sensitivity analysis
(Sy : = dCx/d0) of the solution to the initial value problem,

0(0) = 2 5,09; 0, 1495,73 S,(t,—; 0, uk). (8)
j,k

3.2 Implementation details

Figure 1 shows a ﬂowchart of the typical MCMC_CLIB usage.

Input The ODE model has to be provided as an XML ﬁle in VFGENS vf
format, together with a data ﬁle that contains measurement time points
and a data set D. Additionally, this data ﬁle also speciﬁes sample size,
initial step size, desired acceptance rate and the parameters ,u and 8‘1 for
the prior distribution.

Sampling algorithm To evaluate the likelihood function, ODES are nu-
merically integrated using CVODES with ﬁxed relative and absolute toler-
ance. For further details, we refer to the supplement in the Availability and
Implementation section (manual . pdf and documentation . pdf).

I conﬁguration  prior I
data I xml model I IC model I shared library SMMALA posterior sample

 

I I I.J-I‘.l l-l_.

 

26

Clié'l-LdOIIQ—I‘OAM

  

 

 

41...;lllllll
-9 -8 -7-6-5-4 -3 -2 -1

 

Table 1. Sample properties for the provided example model

 

 

Property Value
Sample size N = 1 x 106
Target acceptance 0.500
Observed acceptance 0.498
Auto-correlation length tin“ 340 :l: 30
Sampling time ts 74h

Effective sampling speed v (5.50 :l: 0.49) x 10—3 s—1

 

Note: The effective sampling speed is deﬁned as v=N/(2tint_,1ts).

Output The user can select a text or binary ﬁle output, which con-
tains the results of the sampling procedure: the sampled log-parameters
and their log-posterior values. The sample can be further processed with
other numerical software (e.g. GNU OCTAVE, MATLAB).

3.3 Test results for the provided demo

For demonstration purposes, we supply a model with 11 state variables,
26 parameters and 4 input variables. The data, obtained in a real experi-
ment, are given in arbitrary units, stem from nE = 4 experiments and are
only meaningful in relation to the reference experiment (also included).
The initial conditions are unknown stable steady states of the unper-
turbed system. At t = 0, a constant disturbance, parametrized by u, is
activated and perturbs the system indeﬁnitely. Then measurements are
taken. The activation is modeled by a logistic function centered at t = 0.

Performance of the sampling algorithm is summarized in Table 1.
Further insight into this example is provided as part of the software
package. The analysis reveals that not all data points can be ﬁtted equally
well by the model; a considerable uncertainty remains for several param-
eters and consequently model predictions. Hence, we consider this as a
realistic test case, which covers many of the difﬁculties that emerge in
current research.

Funding: AK. and NR. acknowledge support by the German Research
Foundation (DFG) within the Cluster of Excellence in Simulation
Technology (EXC 310/1) at the University of Stuttgart.

Conﬂict of interest: none declared.

REFERENCES

Calderhead,B. (2012) Differential geometric MCMC methods and applications. PhD
Thesis, Department of Computer Science, University of Glasgow, Glasgow, UK.

Girolami,M. and Calderhead,B. (2011) Riemann manifold Langevin and
Hamiltonian Monte Carlo methods. J. R. Stat. Soc. B, 73, 123—214.

Haario,H. et al. (2006) DRAM: efﬁcient adaptive MCMC. Stat. C0mput., 16,
339—354.

Hastings,W.K. (1970) Monte Carlo sampling methods using Markov chain and
their applications. Biometrika, 57, 97—109.

Metropolis,N. et al. (1953) Equation of state calculations by fast computing ma-
chines. ]. Chem. Phys., 21, 1087—1092.

 

2992

112 [glO'SIBILInO[p.IOJXO'SODBIIIJOJIIIOIQ/ﬂdllq IIIOJJ papeolumoq

910K ‘09 lsnﬁnV no :2

