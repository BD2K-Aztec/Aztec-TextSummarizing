Bioinformatics Advance Access published August 13, 2016

 

Subject Section

CGDM: Collaborative Genomic Data Model for
Molecular Profiling Data Using NoSQL

Shicai Wang‘, Mihaela A. Mares1 and Yi-ke Guo1’ 2’*

1 Data Science Institute, Imperial College London, London, UK and
2School of Computer Science, Shanghai University, Shanghai, China.

*To whom correspondence should be addressed.

Associate Editor: Dr. Jonathan Wren

Abstract

Motivation: High-throughput molecular profiling has greatly improved patient stratification and mechanistic
understanding of diseases. With the increasing amount of data used in translational medicine studies in
recent years, there is a need to improve the performance of data warehouses in terms of data retrieval
and statistical processing. Both relational and Key Value models have been used for managing molecular
profiling data. Key Value models such as Square have been shown to be particularly advantageous
in terms of query processing speed for large datasets. However, more improvement can be achieved,
particularly through better indexing techniques of the Key Value models, taking advantage of the types of
queries which are specific for the high-throughput molecular profiling data.

Results: In this paper, we introduce a Collaborative Genomic Data Model (CGDM), aimed at significantly
increasing the query processing speed for the main classes of queries on genomic databases. CGDM
creates three Collaborative Global Clustering Index Tables (CGCITs) to solve the velocity and variety
issues at the cost of limited extra volume. Several benchmarking experiments were carried out, comparing
CGDM implemented on HBase to the traditional SQL data model (TDM) implemented on both HBase and
MySQL Cluster, using large publicly available molecular profiling datasets taken from NCBI and HapMap.
In the microarray case, CGDM on HBase performed up to 246 times faster than TDM on HBase and
7 times faster than TDM on MySQL Cluster. In single nucleotide polymorphism (SNP) case, CGDM on
HBase outperformed TDM on HBase by up to 351 times and TDM on MySQL Cluster by up to 9 times.
Contact: y.guo@imperial.ac.uk

 

1 Introduction databases are meant to retrieve subjects based on their features as

Molecular proﬁling refers to the study of speciﬁc patterns or signatures,
such as DNA polymorphism, mRNA gene expression proﬁling, RNA
proﬁling, proteomics and metabolic polymorphism. Biomedical research
is moving towards using more high-throughput molecular proﬁling data to
improve disease understanding.

A typical molecular proﬁling database contains values of features
from individuals of interest or samples, to be used for several medical
studies. A subject of study is usually a particular sample which we want to
analyze for the speciﬁc study. At high level, queries on bioinformatics

searching conditions. We investigated massive molecular proﬁling data
based analysis applications, such as Van’t Veer et al. (2002), Cross and
Burmester (2004), Sotiriou et al. (2006) and Bissonnette et al. (2016), and
participated in popular European medical projects, such as U-BIOPRED
(Unbiased BIOmarkers in PREDiction of respiratory disease outcomes)
(Wheelock et al., 2013) and eTRIKS (European Translational Information
and Knowledge Management Services) (Pandis et al., 2015). We found
three most frequently used steps for disease understanding. The ﬁrst step
is the marker selection. The next step is to discover the relationship
between the selected markers and a particular disease. If the relationship
is validated, the last step is to detect potential patients with the disease

© The Author (2016). Published by Oxford University Press. All rights reserved. For Permissions, please email:

journals.permissions@oup.com

9mg ‘09 1sn8nV uo salaﬁuV s01 ‘erulomeg JO AusraAru [1 112 /3.IO'S[BII.IHO[plOJXO'SODBIILIOJIIlOlQ/ﬂdllq mog papeolumoq

 

based on their markers. According to the three steps, we consider the three
classes of queries for disease understanding:

In the ﬁrst step, multiple cohorts with all markers in the database are
collected for a particular study, for example "asthma".

select * from table

where subjects in {the cohorts}
and study = ’asthma’;

——Query 1

In the second step, the selected markers of all the samples in related studies
are retrieved to discover the relationship between the markers and the
disease. For example, we may want to retrieve the samples genotyped at a
particular DNA position in a "asthma" study.

select * from table

where markers in {the DNA positions}
and study = ’asthma’;

——Query 2

If a relationship between features and diseases is validated, all the samples
with these markers in the database are selected to detect potential patients
for a particular disease. For example, we may want to retrieve all the
samples genotyped at a particular DNA position.

select * from table
where markers in {the DNA position};
——Query 3

Many management systems can enable scientists to do that, such as
tranSMART (Athey et al., 2013), NCBI (Barrett et al., 2013) and Square
(O’Connor et al., 2010). The storage of the ﬁrst two platforms uses an
SQL model and the last one uses a Key Value model. TranSMART is a
biomedical data warehouse and analytics software platform that integrates
clinical and genomics data using SQL models. The NCBI dbSNP (Sherry
et al., 2001) individual genotype data schema is a widely used relational
data model for SNP data. The Square implements a Key Value model
based on HBase with secondary indices.

In the big data era, molecular proﬁling data size increases sharply due
to new biological techniques, such as next generation sequencing. None of
the existing databases work well whilst considering the three "V" features
of big data (Volume, Variety, and Velocity). An alternative solution, and
what forms the contribution described in this paper, is to take advantage of
emerging NoSQL techniques with high speed indices to speed up queries
over molecular proﬁling data.

Google Bigtable (Chang etal., 2008) is aNoSQL database to store large
volumes of structured data in the range of petabytes across thousands of
machines. The database model of Bigtable is a set of processors known as
clusters. Each cluster controls a set of table partitions. A table in Bigtable
is a sparse, distributed, persistent and dynamic sorted tree, known as Log-
structured merge-tree (LSM-tree) (OONeil et al., 1996), and the data is
organized into three dimensions: rows, columns, and timestamps. Many
open source projects are implemented based on the Bigtable design, such
as Accumulo (Sen et al., 2013), Cassandra (Lakshman and Malik, 2010),
HBase (George, 2011), Hypertable (Khetrapal and Ganesh, 2006), Druid
(Yang et al., 2014) and Open Neptune (Atzori and Dessi, 2011).

Other NoSQL databases, such as MongoDB (Chodorow, 2013),
Couchbase (Brown, 2012), Redis (Carlson, 2013) and Memcached
(Petrovic, 2008), MemcacheDB (Tudorica and Bucur, 2011) and
ClusterPoint (Rats and Ernestsons, 2013) have also become a popular
solution for managing large volumes of data. For example, MongoDB is a
cross-platform document-oriented database, which avoids the traditional
relational database structure in favor of J SON-like documents with

 

 

pk study subject position info maintable

index table

1 st1 sub1 posl inf01
st2 sub2 pos2 infoZ
st3 sub3 pos3 inf03

 

 

 

 

 

 

 

 

 

 

Fig. 1. Traditional molecular proﬁling data model

dynamic schemas (BSON), making the integration of data in certain types
of applications easier and faster.

Moreover, range scan query in Bigtable can perform even faster
than indices of other NoSQL databases for molecular proﬁling data. For
example, Wang et al. (2014a) indicates HBase data query performs up to
7x faster than MongoDB for microarray data. Yet queries by non-row-key
columns in Bigtable can be extremely slow due to random read operations
usually being slower than range scan operations. Thus, in Bigtable the
secondary index (Liu and Yoneda, 2001) has a smaller space requirement
but performs slow; the clustering index (Zou et al., 2010) has larger storage
overhead but performs fast. The global index has high network trafﬁc but is
easy to implement; the local index has low network trafﬁc but is difﬁcult to
implement. Feng et al. (2015) did a performance test on all those indices.
The global clustering index performs best with a reasonable high storage
overhead.

However, none of existing molecular proﬁling data models work
well for Bigtable. The distinctive characteristic of our approach is to
create a collaborative genomic data model (CGDM), which uses three
complementary global clustering index tables (CGCITs) for each of the
classes of queries described above. Each CGCIT uses vertical partitions
to further speed up the queries and increase the data availability. Thus,
CGDM can solve the velocity and variety issues with limited extra volume
cost. We index and order each table based on the speciﬁc selection criteria
of the query (i.e. study, subject, DNA position) and then distribute the
queries to the right table based on the speciﬁc class or selection criteria it
belongs to. Two benchmarks for gene expression and SNP are used to test.
The experiments show CGDM implemented in HB ase greatly outperforms
the traditional model implemented in both HB ase and MySQL Cluster.

2 Methods
2.1 Traditional secondary index for the queries

A typical traditional molecular proﬁling data model (TDM) used in
tranSMART includes a main table and three secondary index tables, as
shown in Figure 1. The pk is a unique id. Main table stores data ordered
by the id. Three secondary indices are created to speed up the three typical
queries above. The subject id is usually associated with its study, so study
and subject together can be a unique id for Query 1. The combination of
study and DNA position is the fastest index to locate a DNA postion in
a speciﬁc study for Query 2. The position alone can be used to search
information on a speciﬁc DNA position in the database for Query 3.
There are two types of secondary indices that can be used for this
purpose. One stores pk values in the main table, while the pk column in
the other index stores pointers linking to the physical location in the main
table. In a distributed database, it is very difﬁcult to maintain the second
type of index. For example, if the distributed table blocks are merged

9mg ‘09 1sn8nV uo salaﬁuV s01 ‘BIIIJOJHBD JO AusraAru [1 112 /810's112umo[pJOJXO'sor1chOJurorw/zd11q IIIOJJ papeolumoq

 

CGCIT1 for Query 1 CGCITZ for Query 2

         

info info

infol infol
infoZ

inf03

info primary index

infol other indices

content

 

infoZ

infoB

 

Fig. 2. CGDM structure.

or split, all the related physical locations in the pk column will be re-
calculated. Thus, the pk in the index table usually contains the pk value
in the main table. That means that if we sequentially search the index
table to ﬁnd the pk, we need to randomly search the pk in the main table
to ﬁnd the data. The random search leads to a big challenge to design a
multi-dimension index for a large range query in Bigtable.

2.2 An optimized cluster index for Bigtable

A clustering index may be introduced to solve the random search problem
from the secondary index. But the clustering index leads to a big storage
overhead, as each index table is a full copy of the main table. In order to
reduce the storage overhead, the backup copies in the database can be re-
used. Fortunately, in Bigtable there are usually three copies of each table
to make sure the data consistence and fault toleration. A main table with
two index tables does not increase any overhead. Further more, if the main
table can be even re-used, three collaborating clustering index tables can
be created for the 3 typical queries.

2.3 Collaborative global clustering index table

The remaining work consists in presenting the design of the index for each
query. From the analysis point of View, the study, subject and the DNA
position are frequently used as search conditions; From the database point
of View, Bigtable offers a hierarchical composite primary key, including
row key (primary index) and column key (assistant index). Bigtable
horizontally partitions a table by row key and and vertically partitions
the table by column key. The row key is the primary index of the key, so
the main index columns in this index table are stored here. The other index
columns can be added in the column key. Based on the principles above, a
collaborative data model for molecular proﬁling data is created, as shown
in Figure 2.

2.4 Column Family separating different types of data

Except the usage of horizontal index, CGDM also considers utilizing the
Bigtable vertical partition feature, i.e. Family, to further speed up data
query, as shown in Figure 3. Family is a special concept, related to a group
of columns that contain the same type of data. The same type of data is
usually stored together and retrieved together. For example, three different
types of gene expression data may be stored, such as raw value, logarithm
value and zscore value, and a normal query only focuses on one type of
data. If three Families are used for the three types of data, the query could
be speeded up by searching about 1/3 of the data only.

CGCIT

 

FamilyZ in CGCIT

   

Fig. 3. Structure of multiple types of data.

2.5 The space overhead estimation

The disadvantage of Clustering index is the space overhead. However, the
CGDM design uses an even smaller space than the secondary index and
that is due to the optimization.

The secondary index space includes the main table and three indices.
Each table has three copies of data. The space of each record S,- is the sum
of Main table space Sm and three index table space S“, 512 and Sig.

Sm = ([1171: ‘i' Lstudy ‘i' Lsubject + LPOS’it’ion + Linfo) * 3 (1)

Si1 = (Lstudy + Lsubject + ka) * 3 (2)
522 = (Lstudy ‘i‘ Lposition ‘i‘ ka) * 3 (3)
823 = (Lposition ‘i' ka) * 3 (4)

Si = Sm -I- 5'11 -I- 5'22 -I- 5'23
= 12 * ka -i- 9 * Lstudy ‘i— 6 * Lsubject ‘i‘ 9 * Lposz’tion (5)
‘i' 3Lz’nfo

Where ka is the length of primary key, L study is the length of study
column, Lposition is the length of DNA position column, Linfo is the
length of non-indexed columns and data replica factor is 3.

The CGDM space Sc consists of the space of three CGCITs Scl, Sag,
Sag.

Sol 2 (Lstudy ‘i' Lsubject ‘i' Lposition) * N ‘i' Linfo (6)

502 = (Lstudy ‘i' Lposit’ion ‘i' Lsubject) * N ‘i' Linfo (7)

Sc3 = (Lposition ‘i' Lsubject ‘i' Lstudy)N + Linfo (8)

Sc = Sol ‘1' Sc2 ‘i‘ Sc3

=  * Lstudy ‘i‘  * Lsubject +  * Lposition + 3 * 
(9)

Where N is the number of Family in each CGCIT.

9mg ‘09 1sn8nV uo salaﬁuV s01 ‘BIIIJOJHBD JO AusraAru [1 112 /810's112umo[pJOJXO'sor1chOJuiorw/zdug IIIOJJ papeolumoq

 

 

.0
.1:

Overhead ratio
0
LI)

 

 

 

 

Fig. 4. The overhead ratio of CGDM to TDM.
The overhead ratio of CGDM to TDM is:
(Sc _  =  _ 3) * Lstudy ‘i' (N _ 2) * Lsubject

‘i' (N _ 3) * Lposz’tion _ 4 * ka)/
(4 * ka + 3 * Lstudy‘i'

(10)

2 * Lsubject ‘i‘ 3 * Lpos’ition + LinfO)

Assume pk, study, subject and position have the same length L then,
the ratio of the info length to any other index column length R is:

R: Linfo/L (11)

Then, the overhead ratio becomes:

(Sc — SID/Si = (3N — 12)/(12 -i- R) (12)

If N<5, there will not be any overhead. For N from 5 to 7, we plotted
the Figure 4. The overhead ratio drops signiﬁcantly as the R increases and
the N decreases, which indicates that CGDM should have less columns to
index and all index columns should have small length to avoid big space
overhead. If N changes from 5 to 7 and the R changes from 10 to 30, then
the overhead changes from 7.14% to 52.94%.

2.6 Fault Tolerance

In CGDM, CGCITs will have no replica to avoid huge storage overhead,
and cause the problem of fault tolerance. The basic idea is that CGCITs
will replicate and recover each other at record level. As both CGCIT1 and
CGCIT2 row keys begin with study column, rows in CGCIT1 and CGCIT2
are both ordered by study. When a region of a CGCIT1 is damaged,
we can quarantine the regions of the studies involved and use regions
containing the same studies in CGCIT2 to reconstruct the regions. For
CGCIT2, the situation is similar. For CGCIT3, we can only isolate DNA
positions involved and fetch corresponding information by scanning the
whole CGCIT2. During the scan, we can use DNA position range to skip
unnecessary regions.

3 Results
3.1 Experiment Environment

There are many implementations of Bigtable, such as Hypertable
(Khetrapal and Ganesh, 2006), Apache HB ase (George, 2011) and Apache
Cassandra Lakshman and Malik (2010). We chose the most popular one -
HBase. A monolithic relational database is difﬁcult to be compared to
HBase clusters, but many relational database clusters can be used for
this purpose, such as MySQL Cluster (Ronstrom and Thalmann, 2004),

PostgreSQL XE (Momjian, 2001) and SQL Server Cluster (Campbell etal.,
2010). Compared to other database clusters, MySQL Cluster, supported
by Oracle, is one of the most powerful and easy to deploy clusters.

Both HBase and MySQL Cluster were running on a OpenStack
platform Sefraoui et al. (2012). HBase was used to implement both
the TDM and CGDM, while MySQL Cluster implemented TDM. Each
database was conﬁgured as follows:

HBase (Version 0.96.0 on Hadoop 1.0.3): One master server node and
three slave nodes with HBase conﬁgured in fully distributed mode. The
master server was conﬁgured as a Virtual machine (VM) with 4 CPU cores
and 8 GB memory, while each slave node was conﬁgured as VMs with
4 CPU cores and 8 GB memory. Each VM used a 100 GB disk. Three
copies is the minimum number for the Hadoop Distributed File System to
guarantee data consistency.

MySQL Cluster (Version 5.6.11-ndb-7.3.2): Four VMs, with one as a
manager node and three data nodes. The manager node consists of a MGM,
a MySQL and a NDB using a VM with 4 CPU cores and 8 GB memory.
Each VM used a 100 GB disk. Each data node consisted of a NDB. Two
data copies were used to guarantee its data consistency.

3.2 Microarray benchmark

Our experiment was performed using a public Multiple Myeloma
(MULTMYEL) (Raab et al., 2009; Hanamura et al., 2006) dataset
[GEO:GSE24080] (Shi et al., 2010). The reason we chose MULTMYEL as
our test dataset was that it is one of the largest datasets, consisting of 559
samples and 54,675 probesets for each sample, totalling approximately
30.5 million records. In each record there are three columns in the info
part, the raw value and two normalized values (logarithm and zscore).
These values are usually used separately for different purposes, so three
data types (Family) are set for the three values.

3.2.1 Query 1 Evaluation

We refer to the use cases and part of the results from the paper (Wang et al.,
2014b) and add an new baseline test to see how TDM works for HB ase. The
searching conditions in these use cases are mostly related to the subject,
so CGCIT1 is selected to perform the tests. In Figure 5, CGDM in HB ase
demonstrates an average 5.24 and 73.89 times of increase compared to
TDM implemented on MySQL Cluster and HBase.

Compared to TDM on HB ase, in 5/11 of cases, CGDM performs more
than 80x faster than TDM on HBase. In 3/11 of cases, CGDM is more than
90x faster. With the subject number increasing, the CGDM’s advantage
is more obvious. In the last and biggest case, CGDM outperforms the
traditional one by 101x.

Compared to TDM on HBase, in 6/11 of cases, CGDM is more than
5x faster than traditional model on MySQL Cluster. In 10/11 of cases,
CGDM is more than 4x faster than TDM on MySQL Cluster. In the worst
case, A2, CGDM is more than 3.61 times faster than traditional model on
MySQL Cluster.

3.2.2 Query 2 Evaluation

A normal marker selection returns about 100 potential probe sets, which
need to be veriﬁed by other datasets of similar studies. Based on the design
CGCIT1 will perform sub-standard for this purpose, but CGCIT2 can
perform much faster. We randomly generated 100 probes and test CGDM
against the one. The query of Key Value model is based on the Random
Read operation.

CGDM demonstrates an average 174.52 times of increase compared
to TDM implemented on HBase. In 4/10 of cases, CGDM on HBase is
more than 170 times faster than TDM on HBase. In 9/ 10 of cases, CGDM
on HBase performs 140 times faster than TDM on HBase.

9mg ‘09 1sn8nV uo salaﬁuV s01 ‘121u10111123 10 A11s19A1uf1 112 /810's112umo[p101x0'sor112u1101u101q/ﬁd11q 111011 papeolumoq

 

 

 5139.09 5520.67 6183.56

    
 
 
 

4502.12
382 5.54
304439 $404.24

   
 
    
 

2401.53
1537.61
1148.91

1000 7 794.27

223 5? 25335 259132 292.16 299.82 315.95
132.35 214‘“ '

141.54

123.40

111.01

 _ 5553 5551 51.21 59.36 51.21

39.23 32.40 38.33 38.67
26.27

Execution time (secondsl

14.96

 

 

 

0 100 200 300 400 500 600
Subject number
+CGDM on HBase +TDM on MySQL Cluster +TDM on HBase

Fig. 5. Result of Query 1.

 

1000

407.55 456.94 497.78 533.06
326.55 367.21
282.71

248.37
174.93 19734

._.
D
D

1153 12.80

915 10.40

Execution time (seconds)
H
O

 

H
I

 

 

 

Thread number
+CGDM on HBase +TDM on MySQL Cluster +TDM on HBase

Fig. 6. Result of Query 2.

CGDM demonstrates an average 3.06 times of increase compared to
TDM implemented on MySQL Cluster. In 6/ 10 of cases, CGDM is more
than 3 times faster than traditional model on MySQL Cluster. In 9/ 10 of
cases, CGDM is more than 2.5 times faster than TDM on MySQL Cluster.
In the worst case, 20-thread, CGDM is more than 2.47 times faster than
traditional model on MySQL Cluster. HB ase performs as stable as MySQL
Cluster in most of the cases, as show by the greater average result deviations
in Figure 6.

3.2.3 Query 3 Evaluation
A further validation is performed using three GSE24080 studies. We
copied GSE24080 dataset twice and arbitrarily named them GSE24081
and GSE24082 and re-used the 100 probe sets in the query by position
experiment. MySQL Cluster failed to load these three datasets due to a
memory limitation. We compared the query using both CGDM and TDM
on HBase.

CGDM demonstrates an average 246.48 times of increase compared
to TDM, as shown in Figure 7. In 6/ 10 of cases, CGDM is more than 220
times faster than TDM. In 3/ 10 of cases, CGDM is more than 250 times
faster than TDM. In largest case, CGDM is more than 210 times faster
than TDM.

3.3 SNP benchmark

The large Microarray benchmark shows the performance of CGDM
throughput, without considering the response time for small queries. If
only one subject or a few DNA position are required, a large overhead may
damage the users’ experience. The SNP benchmark not only illustrates the
throughput of CGDM but also its response time of small queries. Also, the
TDM on HB ase performs much worse than the other two implementations

 

10000

151537 1792.19
94341 1137.34 1251.25 1359-37
355.13 -

1000 ' 525.21 633.90 737.37

100 '

Execution time [seconds]

10 ' 4.43 M39 5.48

 

 

 

0 20 4O 60 80 100
Thread number
+CG D M +TDM

Fig. 7. Result of Query 3.

in all the tests above. So, the TDM on HBase will not be performed any
more.

The experiments are performed using three SNP datasets (ASW, 87
samples and 119,487 SNPs; CEU, 165 samples and 119,487 SNPs; CHB,
137 samples and 317,642 SNPs) from the 2010-05 phase III consensus
datasets of HapMap project (Consortium, 2007). This SNP dataset has
only one type of data.

3.3.1 Query 1 Evaluation

Usually, the ﬁrst step of SNP based medical research is to ﬁnd signiﬁcant
differential SNPs with clinical information in a study. All available SNPs
of the speciﬁed subjects with typical features are loaded for further tests
to ﬁnd the expected SNPs. Thus, the ﬁrst query is usually applied on the
main cohorts (almost all subject) at the beginning of a study to ﬁnd the
most relevant SNPs. For this type of case, we searched all subjects in each
dataset and as shown in Figure 8, CGDM on HBase is on average 269.39
times faster than TDM on HBase. CGDM on HBase outperforms TDM
on HBase by 351.33 times when tested with CHB dataset, while when
using datasets ASW and CEU, TDM it performs 215.15 and 241.68 times
faster than TDM on HB ase. CGDM on HB ase performed about 4 or more
times faster for the data retrieval time than TDM on MySQL. Particularly
in ASW, CGDM on HB ase outperforms TDM on MySQL Cluster by 9.12
times. While in datasets CEU and CHB, CGDM respectively performed
5.12 and 4.51 times faster than TDM on MySQL Cluster.

In the response time test of one subject queries over 119,487 SNPs,
CGDM on HBase is 148.25 times faster than TDM on HBase on average,
as shown in Figure 9, while the queries over 317,462 SNPs using CGDM
outperformed TDM on MySQL Cluster by 99.20 times. The response time
of 119,487 SNPs queries with CGDM are 2.61 times faster than TDM on
MySQL Cluster on average, while the queries over 317,462 SNPs using
CGDM outperformed TDM on MySQL Cluster by 1.57 times.

3.3.2 Query 2 and 3 Evaluation

After signiﬁcant differential SNPs are calculated, the top hundred
differential SNPs are usually tested using other datasets to conﬁrm or
further reduce the number of these SNPs. If SNPs associated with certain
diseases are found, queries over several SNPs across datasets are generated
to detect whether a patient is vulnerable to certain diseases based on his
genotype information from the SNPs.

While querying 100 discrete SNP by position, CGDM on HBase
performs on average 15.40 times faster than TDM on HBase. But TDM
on MySQL Cluster is 1.84 times faster than CGDM on HB ase on average,
as shown in Figure 10, due to the slow HBase Random Read.

However, with concurrent query thread number increasing, MySQL
Cluster does not scale well, as shown in Figure 11. CGDM on HBase
demonstrates an average 4.66 times of increase in query processing speed

9mg ‘09 1sn8nV uo salaﬁuV s01 ‘121u10111123 10 A11s19A1uf1 112 /810's112umo[p101x0'sor112u1101u101q/ﬁd11q 111011 popcolumoq

 

 

ICGDM ITDM on MySQL ITDM on HBase

 

9,412.03

10000 ‘ 4,435.52
2,524.53

1000

100

Execute time (second)

10

 

 

 

ASW CEU CHB

Fig. 8. Result of Query 1 for throughput test.

 

ICGDM ITDM on MySQL ITDM on HBase
100000 70402 70057
27418 26495 27845

 

10000 '

1000 '

100 '

Response time (millisecond)

 

 

 

119487 119487 119487 119487 317642 317642
SNP number

Fig. 9. Result of Query 1 for response time test.

 

I CGDM .TDM on MySQL ITDM on HBase
9.03 9.51 9.35 9.9?

 

H
D
I

   
 

Execute time (second)
H
I

 

 

0.1 ‘

ASW CEU CHB All

Fig. 10. Result of Query 2 and 3 for response time test.

compared to TDM implemented on MySQL Cluster. In 6/10 of cases,
CGDM is more than 5 times faster than TDM on MySQL Cluster. In
the worst case, when using only 10 threads, CGDM is more than 3.07
times faster than TDM on MySQL Cluster. Compared to TDM on HB ase,
CGDM performs 105.40 times faster on average. Especially, in 6/ 10 of
cases, CGDM is more than 100 times faster than TDM on HBase.

4 Discussion

The most common queries for disease understanding do not cover all the
commonly used queries, for example wildcard query. Generally, there are
two main types of wildcard queries. One begins with a wildcard (%search-
string...). This query will perform a scan operation on a full or large range
of the table. The other one starts with a search string (search-string%...).
This query will perform a relatively small seek operation based on the

 

 

 

1000
1?
g 100 ' , __......—-—-—-n-""" " 13031 19583
155.41 . .
3 9219 110.59 122.35 138.50 152.59
" 77.35 51.54 .
In
H 12.3?
a 2.40 8.57 9.95 11.13
g 10 am 534 614
F 2.22
o 1.23
1:
a 3.29
a 1 1 55 1.39 2 12
w 089 090 100 1-30
071 .
0.49
0 I I I I I

 

 

 

Concurrent thread number
+CGDM +TDM on MySQL +TDM on HBase

 

 

Fig. 11. Result of Query 3 for throughput test.

preﬁx ’search-string’. Though tests for wildcard query are not speciﬁed,
some of the results show some hints about how CGDM may perform. The
last query in section 3.2.1, which performed a scan on the whole table,
indicates the performance of a test similar to (%search-string...). The query
tests in section 3.2.3 performed queries similar to queries with wildcard
(GSE2408%) on HBase. Due to the hierarchical design of CGCIT, CGCIT
has three types of wildcard queries, the wildcard query on primary index,
the wildcard query on other indices and the wildcard query on content. The
last query in section 3.2.1 shows a wildcard query on primary key and the
queries in section 3.2.3 indicates wildcard queries on other indices. Any
one of these three wildcard queries may not be the best choice for certain
cases, while a mixed dynamically wildcard query plan may perform better
than statically choosing one.

5 Conclusion

In this paper, CGDM has been created for high-throughput molecular
proﬁling data to improve the query processing speed for the three
main classes of queries on genomic databases. Multiple benchmarking
experiments were carried out, comparing CGDM implemented on HB ase
to TDM implemented on both HBase and MySQL Cluster, using large
publicly available molecular proﬁling datasets. In the microarray case,
CGDM on HBase performed up to 246 times faster than TDM on HBase
and 7 times faster than TDM on MySQL Cluster. In the SNP case, CGDM
on HBase outperformed TDM on HBase by up to 351 times and TDM on
MySQL Cluster by up to 9 times.

Funding

This research is partially supported by the Innovative R&D Team Support
Program of Guangdong Province (NO. 201001D0104726115), China,
Johnson & Johnson Pharmaceutical and Research Comp, the European
Commission Innovative Medicines Initiative (IMI) under the European
Translational Information & Knowledge Management Services project
(eTRIKS: Grant Agreement NO. 115446).

References

Athey, B. D., Braxenthaler, M., Haas, M., and Guo, Y. (2013). tranSMART: An Open
Source and Community-Driven Informatics and Data Sharing Platform for Clinical
and Translational Research. AMIA Summits on Translational Science proceedings
AMIA Summit on Translational Science, 2013, 6—8.

Atzori, M. and Dessi, N. (2011). Dataspaces: Where structure and schema meet. In
Learning Structure and Schemas from Documents, pages 97—119. Springer.

9mg ‘09 1sn8nV uo salaﬁuV s01 ‘121u10111123 10 A1rs19Aruf1 112 /810's112umo[p101x0'sor112u1101urorq/ﬁd11q 111011 papeolumoq

 

Barrett, T., Wilhite, S. E., Ledoux, P., Evangelista, C., Kim, I. F., Tomashevsky, M.,
Marshall, K. a., Phillippy, K. H., Sherman, P. M., Holko, M., Yefanov, A., Lee, H.,
Zhang, N., Robertson, C. L., Serova, N., Davis, S., and Soboleva, A. (2013). NCBI
GEO: archive for functional genomics data sets—update. Nucleic acids research,
41(Database issue), D991—5.

Bissonnette, R., Suarez-Fariﬁas, M., Li, X., Bonifacio, K. M., Brodmerkel, C.,
Fuentes-Duculan, J ., and Krueger, J. G. (2016). Based on molecular proﬁling of
gene expression, palmoplantar pustulosis and palmoplantar pustular psoriasis are
highly related diseases that appear to be distinct from psoriasis vulgaris. PloS one,
11(5), e0155215.

Brown, M. C. (2012). Getting Started with Couchbase Server. " O’Reilly Media,
Inc.".

Campbell, D. G., Kakivaya, G., and Ellis, N. (2010). Extreme scale with full sql
language support in microsoft sql azure. In Proceedings of the 201 0ACM SIGM OD
International Conference on Management of data, pages 1021—1024. ACM.

Carlson, J. L. (2013). Redis in Action. Manning Publications Co.

Chang, E, Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A., Burrows, M.,
Chandra, T., Fikes, A., and Gruber, R. E. (2008). Bigtable: A distributed storage
system for structured data. ACM Transactions on ComputerSystems (TOCS), 26(2),
4.

Chodorow, K. (2013). MongoDB: the deﬁnitive guide. " O’Reilly Media, Inc.".

Consortium, T. I. H. (2007). A second generation human haplotype map of over 3.1
million SNPs. Nature, 449, 851—61.

Cross, D. and Burmester, J. K. (2004). The promise of molecular proﬁling for cancer
identiﬁcation and treatment. Clinical medicine & research, 2(3), 147—150.

Feng, C., Yang, X., Liang, F., Sun, X.-H., and Xu, Z. (2015). Lcindex: A local and
clustering index on distributed ordered tables for ﬂexible multi-dimensional range
queries. In Parallel Processing (ICPP), 2015 44th International Conference on,
pages 719—728. IEEE.

George, L. (2011). HBase: the deﬁnitive guide. " O’Reilly Media, Inc.".

Hanamura, 1., Huang, Y, Zhan, F., Barlogie, B., and Shaughnessy, J. (2006).
Prognostic value of cyclin d2 mrna expression in newly diagnosed multiple
myeloma treated with high-dose chemotherapy and tandem autologous stem cell
transplantations. Leukemia, 20(7), 1288—1290.

Khetrapal, A. and Ganesh, V. (2006). HBase and Hypertable for large scale
distributed storage systems A Performance evaluation for Open Source BigTable
Implementations. Evaluation, page 8.

Lakshman, A. and Malik, P (2010). Cassandra: a decentralized structured storage
system. ACM SIGOPS Operating Systems Review, 44, 35.

Liu, L.-C. H. and Yoneda, K. (2001). Secondary index search. US Patent 6,266,660.

Momjian, B. (2001). PostgreSQL: introduction and concepts, volume 192. Addison-
Wesley New York.

O’Connor, B. D., Merriman, B., and Nelson, S. F. (2010). Square Query Engine:
storing and searching sequence data in the cloud. BMC bioinformatics, 11 Suppl
12, 82.

OONeil, P., Cheng, E., Gawlick, D., and OONeil, E. (1996). The log-structured
merge-tree (LSM-tree).

Pandis, I., Guo, Y, Guitton, F., Yang, X., Sun, K., Wang, S., Jullian, N., Sousa, A.,
Bansal, A., Corﬁeld, J ., et al. (2015). etriks it platfroms for large-scale biomedical
research. European Respiratory Journal, 46(suppl 59), PA3976.

Petrovic, J. (2008). Using memcached for data distribution in industrial environment.
In ICONS, pages 368—372.

Raab, M. S., Podar, K., Breitkreutz, 1., Richardson, P G., and Anderson, K. C.
(2009). Multiple myeloma. Lancet, 374, 324—339.

Rats, J. and Ernestsons, G. (2013). Clustering and ranked search for enterprise content
management. International Journal of E—Entrepreneurship and Innovation (IJEEI),
4(4), 20—31.

Ronstrom, M. and Thalmann, L. (2004). Mysql cluster architecture overview. MySQL
Technical White Paper.

Sefraoui, O., Aissaoui, M., and Eleuldj, M. (2012). Openstack: toward an
open-source solution for cloud computing. International Journal of Computer
Applications, 55(3), 38—42.

Sen, R., Farris, A., and Guerra, P (2013). Benchmarking apache accumulo bigdata
distributed table store using its continuous test suite. In Big Data (BigData
Congress), 2013 IEEE International Congress on, pages 334—341. IEEE.

Sherry, S. T., Ward, M. H., Kholodov, M., Baker, J ., Phan, L., Smigielski, E. M.,
and Sirotkin, K. (2001). dbSNP: the NCBI database of genetic variation. Nucleic
acids research, 29, 308—311.

Shi, L., Campbell, G., Jones, W. D., Campagne, F., Wen, 2., Walker, S. J., Su,
Z., Chu, T.-M., Goodsaid, F. M., Pusztai, L., Shaughnessy, J. D., Oberthuer, A.,
Thomas, R. S., Paules, R. S., Fielden, M., Barlogie, B., Chen, W., Du, P., Fischer,
M., Furlanello, C., Gallas, B. D., Ge, X., Megherbi, D. B., Symmans, W. R,
Wang, M. D., Zhang, J., Bitter, H., Brors, B., Bushel, P. R., Bylesjo, M., Chen,
M., Cheng, J., Cheng, J., Chou, J., Davison, T. S., Delorenzi, M., Deng, Y,
Devanarayan, V., Dix, D. J., Dopazo, J ., Dorff, K. C., Elloumi, F., Fan, J ., Fan,

S., Fan, X., Fang, H., Gonzaludo, N., Hess, K. R., Hong, H., Huan, J ., Irizarry,
R. A., Judson, R., Juraeva, D., Lababidi, S., Lambert, C. G., Li, L., Li, Y, Li, 2.,
Lin, S. M., Liu, G., Lobenhofer, E. K., Luo, J ., Luo, W., McCall, M. N., Nikolsky,
Y, Pennello, G. A., Perkins, R. G., Philip, R., Popovici, V., Price, N. D., Qian,
F., Scherer, A., Shi, T., Shi, W., Sung, J., Thierry-Mieg, D., Thierry-Mieg, J.,
Thodima, V., Trygg, J., Vishnuvajjala, L., Wang, S. J., Wu, J., Wu, Y, Xie, Q.,
Yousef, W. A., Zhang, L., Zhang, X., Zhong, S., Zhou, Y, Zhu, S., Arasappan,
D., Bao, W., Lucas, A. B., Berthold, F., Brennan, R. J., Buness, A., Catalano,
J. G., Chang, C., Chen, R., Cheng, Y, Cui, J ., Czika, W., Demichelis, F., Deng,
X., Dosymbekov, D., Eils, R., Feng, Y, Fostel, J ., Fulmer-Smentek, S., Fuscoe,
J. C., Gatto, L., Ge, W., Goldstein, D. R., Guo, L., Halbert, D. N., Han, J ., Harris,
S. C., Hatzis, C., Herman, D., Huang, J ., Jensen, R. V., Jiang, R., Johnson, C. D.,
Jurman, G., Kahlert, Y, Khuder, S. A., Kohl, M., Li, J., Li, M., Li, Q.-Z., Li,
S., Li, Z., Liu, J ., Liu, Y., Liu, Z., Meng, L., Madera, M., Martinez-Murillo, F.,
Medina, I., Meehan, J ., Miclaus, K., Mofﬁtt, R. A., Montaner, D., Mukherjee, P.,
Mulligan, G. J ., Neville, P., Nikolskaya, T., Ning, B., Page, G. R, Parker, J ., Parry,
R. M., Peng, X., Peterson, R. L., Phan, J. H., Quanz, B., Ren, Y, Riccadonna, S.,
Roter, A. H., Samuelson, F. W., Schumacher, M. M., Shambaugh, J. D., Shi, Q.,
Shippy, R., Si, S., Smalter, A., Sotiriou, C., Soukup, M., Staedtler, F., Steiner, G.,
Stokes, T. H., Sun, Q., Tan, P-Y., Tang, R., Tezak, Z., Thorn, B., Tsyganova, M.,
Turpaz, Y, Vega, S. C., Visintainer, R., von Frese, J ., Wang, C., Wang, E., Wang,
J ., Wang, W., Westermann, F., Willey, J. C., Woods, M., Wu, S., Xiao, N., Xu,
J., Xu, L., Yang, L., Zeng, X., Zhang, J., Zhang, L., Zhang, M., Zhao, C., Puri,
R. K., Scherf, U., Tong, W., and Wolﬁnger, R. D. (2010). The MicroArray Quality
Control (MAQC)-II study of common practices for the development and validation
of microarray-based predictive models. Nature biotechnology, 28(8), 827—38.

Sotiriou, C., Wirapati, P, Loi, S., Harris, A., Fox, S., Smeds, J., Nordgren, H.,
Farmer, P., Praz, V., Haibe-Kains, B., et al. (2006). Gene expression proﬁling in
breast cancer: understanding the molecular basis of histologic grade to improve
prognosis. Journal of the National Cancer Institute, 98(4), 262—272.

Tudorica, B. G. and Bucur, C. (201 1). A comparison between several nosql databases
with comments and notes. In RoedunetIntemational Conference (RoEduNet), 201 1
10th, pages 1—5. IEEE.

Van’t Veer, L. J., Dai, H., Van De Vijver, M. J., He, Y. D., Hart, A. A., Mao, M.,
Peterse, H. L., van der Kooy, K., Marton, M. J ., Witteveen, A. T., et al. (2002). Gene
expression proﬁling predicts clinical outcome of breast cancer. nature, 415(6871),
530—536.

Wang, S., Pandis, 1., Wu, C., He, S., Johnson, D., Emam, I., Guitton, F., and Guo,
Y. (2014a). High dimensional biological data retrieval optimization with NoSQL
technology. BMC genomics, 15 Suppl 8(Suppl 8), S3.

Wang, S., Pandis, 1., Wu, C., He, S., Johnson, D., Emam, I., Guitton, F., and Guo,
Y (2014b). High dimensional biological data retrieval optimization with nosql
technology. BMC genomics, 15(Suppl 8), S3.

Wheelock, C. E., Goss, V. M., Balgoma, D., Nicholas, B., Brandsma, J ., Skipp, P. J .,
Snowden, S., Burg, D., D’Amico, A., Horvath, I., et al. (2013). Application
of’omics technologies to biomarker discovery in inﬂammatory lung diseases.
European Respiratory Journal, 42(3), 802—825.

Yang, F., Tschetter, E., Léauté, X., Ray, N., Merlino, G., and Ganguli, D. (2014).
Druid: a real-time analytical data store. In Proceedings of the 2014 ACM SI GM OD
international conference on Management of data, pages 157—168. ACM.

Zou, Y, Liu, J ., Wang, S., Zha, L., and Xu, Z. (2010). CCIndex : A Complemental
Clustering Index on Distributed Ordered Tables for Multi-dimensional Range. In
the 9th IFIP International Conference on Network and Parallel Computing, pages
247—261.

9mg ‘09 1sn8nV uo soloﬁuV soq ‘BIHJOJHBD JO KHSJQAIII [1 1e /810'S{12umo[pJOJXO'sopeuuoguiorq/ﬁd11q won popeolumoq

