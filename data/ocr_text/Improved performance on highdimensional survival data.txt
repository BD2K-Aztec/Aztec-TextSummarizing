ORIGINAL PAPER

Vol. 27 no. 1 2011, pages 87—94
doi: 1 0. 1 093/bioinformatics/btq61 7

 

Gene expression

Advance Access publication November 8, 2010

Improved performance on high-dimensional survival data by

application of Survival-SVM

V. Van Belle”, K. Pelckmans2, S. Van Huffel1 and J. A. K. Suykens1

1Department of Electrical Engineering (ESA'I), Katholieke Universiteit Leuven, Kasteelpark Arenberg 10, B—3001
Leuven, Belgium and 2Department of Information Technology, Division Syscon, Uppsala University, ITC Building 2,

SE—751 05 Uppsala, Sweden

Associate Editor: John Quackenbush

 

ABSTRACT

Motivation: New application areas of survival analysis as for
example based on micro-array expression data call for novel
tools able to handle high-dimensional data. While classical
(semi-) parametric techniques as based on likelihood or partial
likelihood functions are omnipresent in clinical studies, they are
often inadequate for modelling in case when there are less
observations than features in the data. Support vector machines
(SVMS) and extensions are in general found particularly useful
for such cases, both conceptually (non-parametric approach),
computationally (boiling down to a convex program which can
be solved efficiently), theoretically (for its intrinsic relation with
learning theory) as well as empirically. This article discusses such an
extension of SVMS which is tuned towards survival data. A particularly
useful feature is that this method can incorporate such additional
structure as additive models, positivity constraints of the parameters
or regression constraints.

Results: Besides discussion of the proposed methods, an empirical
case study is conducted on both clinical as well as micro-array
gene expression data in the context of cancer studies. Results are
expressed based on the logrank statistic, concordance index and
the hazard ratio. The reported performances indicate that the present
method yields better models for high-dimensional data, while it gives
results which are comparable to what classical techniques based on
a proportional hazard model give for clinical data.

Contact: vanya.vanbelle@esat.kuleuven.be

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on July 8, 2010; revised on October 13, 2010; accepted on
October 29, 2010

1 INTRODUCTION

Survival analysis concerns the study of the time to occurrence of
a certain event. It is best known in cancer studies where one is
interested in characterizing which patients will relapse after surgery
or pass away. Other applications can be found in electronic or
mechanic components where the characterization of the lifetime
is useful for optimizing maintenance strategies. Typically, survival
data contain censored observations. Censoring indicates a lack
of information on the outcome. For example, in a clinical study

 

*To whom correspondence should be addressed.

examining relapse of breast cancer patients, where patients are
included between 1990 and 2000 and followed until 2008, not all
patients will have experienced relapse. For these patients, the failure
time is right censored.

The statistical literature describes different models for the analysis
of failure time data, an overview of which can be found in
Kalbﬁeisch and Prentice (2002). The largest breakthrough in
modelling survival data came in 1972 when Cox proposed his
proportional hazard model (PH) (Cox, 1972). The PH model is a
semi-parametric model assuming that the hazard of an observation
(the instantaneous risk of occurrence of the event given that the event
did not occur before) is proportional to a ‘baseline’ hazard common
to all observations. Proportionality is modelled as the exponential
of a linear function of the covariates. The semi-parametric character
of this model comes from the fact that the baseline hazard is left
unspeciﬁed. Success of this model is to a certain extend due to
the description and analysis of a corresponding partial likelihood
function whose properties are proven to be quite similar to ordinary
likelihood functions.

Although the PH model is perhaps the most common survival
model, some drawbacks remain. At ﬁrst, the model is based on the
assumption that hazards for different subjects are proportional to
each other, an assumption which is not always realistic. A second
drawback is the restrictive parametric form in which the variables
affect the outcome. During the last decade, different methods dealing
with one or both of those drawbacks have been proposed. The linear
parametric form was generalized by means of ANOVA models, splines
and artiﬁcial neural networks [see Bakker et al. (2004); Huang
et al. (2000); Leblanc and Crowley (1999) and references therein].
Models dealing with non-linear covariate effects and not imposing
the proportional hazards assumption were proposed in the ﬁeld of
artiﬁcial neural networks (Biganzoli et al. , 1998; Faraggi and Simon,
1995; Lisboa et al., 2003). Due to the good performances obtained
with machine learning methods in regression and classiﬁcation
(Schblkopf and Smola, 2002; Shawe-Taylor and Cristianini, 2004;
Suykens et al., 2002; Vapnik, 1998), ideas underlying methods
of machine learning started to pervade also traditional modelling
areas. Support vector machine (SVM) regression for censored data
was proposed by Shivaswamy et al. (2007) and a rank regression
approach was given in Evers and Messow (2008); Van Belle et al.
(2007, 2009a). In Van Belle et al. (2010a), a least-squares SVMS
approach making use of ranking and regression constraints was used.

This work compares an SVM-based method incorporating ranking
and regression constraints (survival support vector machines: SSVM)

 

© The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 87

112 [3.10'811211an[p.IOJXO'SODBIIIJOJIIlOlCI/[I(11111 IIIOJJ pepcolumoq

9IOZ ‘ISlsnﬁnV uo ::

MVan Belle et al.

 

as proposed in Van Belle et al. (2009b) with the linear and non—linear
PH model (Cox, 1972) and with the partial logistic artiﬁcial neural
network model with automatic relevance detection (PLANNARD)
(Lisboa et al., 2003). Due to the growing interest in micro-array
data within survival studies, the need for survival methods which
perform well on high—dimensional data is increasing. Therefore,
SSVM is compared with other survival methods, speciﬁcally adapted
for high—dimensional data. Merely a few papers till date approach
this problem in the context of SVMS, see for example Evers
and Messow (2008). Many earlier published studies propose to
reduce the problem of estimating a good prognostic index to
a classiﬁcation problem. Essentially, they discriminate between
observations with (i) a poor prognosis (non-survivors) and (ii) a good
prognosis (survivors). Although this approach is plausible in case all
observations are followed for an equally long follow-up period, it
remains open how this approach can be applied when censoring can
occur at arbitrary moments. For more details, we refer to Callas et al.
(1998); Green and Symons (1983).

This article is organized as follows. Section 2 describes the setup
of survival analysis and gives some more details about the PH, PLANN
and SSVM methods. After a short introduction of the SSVM method,
a feature selection technique is proposed. This method results in
a sparse coefﬁcient vector and will turn out to be useful for high—
dimensional datasets. In Section 3, a comparison is made between
SSVM, the PH and the PLANNARD approaches. In a ﬁrst experiment,
the methods are compared on clinical cancer datasets. A second
experiment involves high-dimensional micro-array data. The article
ends with a discussion and some concluding remarks.

2 METHODS

This section describes three different approaches for estimating prognostic
indices for survival problems. In the remainder of this text, the p-th covariate
of observation i will be denoted by  ; the vector containing all covariates
of the i-th observation is represented as xi. The p-th element of a vector w
will be denoted by wp.

Consider a dataset D = {(xi,y,-,6,-)};‘=1, where xi,y,- and 6,- represent
the covariate vector, a positive survival time and an event indicator for
observation i, respectively. The event indicator is equal to 1 if an event
occurred, and zero if the subject is (right) censored at time yi. The survival
function S(t) = P(y > t) is deﬁned as the probability of not having experienced
the event until time t. The hazard Mt) is deﬁned as the risk of the event to
occur at time t, given that the event did not occur before that time:
P(t5y<t+AtIth)

At (1)

Atzl'
0 Alﬁe

2.1 The proportional hazards model

The proportional hazard model (Cox, 1972) is built on two assumptions:
(i) proportional hazards and (ii) linear parametric form in the covariates. Let
A(x, t) be the hazard, x a speciﬁc covariate vector and t the time at which
one wants to estimate the hazard. The model becomes

A(x, t) = A0(t)exp(wa). (2)
The risk associated with an observation with covariates x,- is related to wai,

also called a prognostic index. The parameters w are inferred by maximizing
the partial likelihood function deﬁned as

L(W)=ﬁ M (3)
j=1 ZzeRjCXPWTxl) ’

where Rj represents all patients at risk at the j-th failure time and k is
the number of distinct failure times. In practice, the log partial likelihood
£(w) = log(L(w)) is maximized.

2.1.1 Penalized proportional hazard regression In order to reduce the
problem of overﬁtting, penalized forms of the likelihood function were
introduced. A penalization term that is often used is called ridge regression
or weight decay [see e.g. Hastie et al. (2001) and references therein]. The
penalized log partial likelihood function then becomes

mw) =£(w) — ngw, (4)

with A Z 0 a regularization constant. We will refer to this model as PH12 in the
remainder of this work. A second penalized model (PHll) maximizes the log
partial likelihood subject to Zd=1|wp|5s, where s needs to be optimized
[see Goeman (2010); Tibshirani (1997)].

2.1.2 Including non-linearities: PH with penalized smoothing splines To
relax the PH model towards non-parametric effects of the covariates, the use
of penalized splines in PH models was proposed by Eilers and Marx (1996),
among others. The PH model is rewritten as

Mx, t) =Ao(t)6XP(f(x)), (5)

where f (x) represents a function of x. When using B-splines, this function is
a linear combination of m basis functions Ba:

f<x)=Zw..Ba<x). <6)

a=1
The penalized partial likelihood function then becomes

A m
2;.(w>=e(w>— 5 Z (Wa —2Wa_1+Wa_2)2, (7)
a=3
with A a positive regularization constant. This approach will be used for
comparison and will be denoted by PHpsplme.

2.1.3 Practical approaches for handling high-dimensional data When
dealing with high-dimensional data, the PH model needs to be adapted
to avoid overﬁtting as before. Three different practical adaptations of the
standard PH model are studied and implemented in Bevelstad et al. (2007),
and matlab code was provided by the authors. A ﬁrst method (PCR) uses
(unsupervised) principal component analysis (PCA) to select a number of
principal components of the expression data accounting for the largest
variation in gene expression proﬁles. The selected principal components
are then used as covariates (Hastie et al., 2001) in a standard PH model.
The SPCR method selects a subset of genes which correlate best with the
observed survival using a univariate PH model and applies PCR on the resulting
genes (Bair et al., 2006). The PLS method creates new features as a linear
combination of the covariates and uses these as input variables for the
PH model (Nygéird et al., 2008).

2.2 Multi-layer perceptron models

2.2.1 The partial logistic artiﬁcial neural network model Biganzoli et al.
(1998) proposed a partial logistic artiﬁcial neural network (PLANN) for the
analysis of survival data. This multi-layer perceptron contains three layers:
(i) an input layer containing a neuron for each input p: 1, ...,d, and one
neuron for the time variable; (ii) a hidden layer containing h = 1, . . . ,H hidden
neurons; and (iii) an output layer with one output neuron. The model is trained
as follows. First the time in which observations were followed is divided into
time intervals [tk_1,tk],k= 1, ...,K. The goal is then to estimate the chance
that an observation will experience the event at study within each of these
intervals, given that they did not relapse before. Therefore, each data point is
replicated for each time interval in which the outcome is known. The input
of the model consists of two parts: the covariate vector and a time variable
indicating the time interval. The output is one if the patient experienced the
event under study within the considered time interval, zero otherwise. For
discrete time studies, the output of the model will represent the predicted
hazard within each time interval.

 

88

1e [3.10'SIBIIJHO[p.IOJXO'SODBIIIJOJIIIOICI/[Z(11111 IIIOJJ pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

Improved performance with Survival-SVM

 

2.2.2 Feature selection: PLANN model with automatic relevance detection
Lisboa et al. (2003) proposed to incorporate Bayesian automatic relevance
determination (ARD) in PLANN. A penalization term a is linked with
each parameter of PLANN and Bayes’ theorem is used as a regularization
framework. As the PH model, PLANN estimates parameters by optimizing the
likelihood function of the parameters w, given the data D, the penalization
terms a and the model hypothesis space H. According to Bayes’ theorem,
this can be expressed as:
P(D |w, a, H)P(w|a, H)

P(wlD,a,7-l)= P(Dla H) . (8)

 

The PLANNARD procedure has three levels. On the ﬁrst level, the regularization
parameters a are assumed to be ﬁxed and the prior distribution P(w|a,7-l)
of the weights w is assumed to be normal, centered at zero, with variance
l/a. P(w|D,a,7—l) can then be optimized in function of w. On a second
level, the regularization parameters are estimated. On a third level, the
evidence in support of a particular model hypothesis H is estimated. The
prior distribution P(aIH) is assumed to be log-normal. A ﬂat prior is assumed
for the model space. The optimal parameters are found by iterating over the
three levels. The values of a are inversely proportional to the relevance of
the variables. See Lisboa et al. (2003) for more details.

2.3 Support vector machines

2.3.1 Survival support vector machines An SVM-based method
formulating the problem of estimating a prognostic index as a ranking
problem was proposed in Evers and Messow (2008); Van Belle et al. (2007),
and computationally simpliﬁed in Van Belle et al. (2008). Van Belle et al.
(2009b) proposed the use of an SVM approach for survival analysis with
ranking and regression constraints. Comparing these methods with those
using only ranking constraints, the former performed signiﬁcantly better.
In this article, we refer to MODEL 2, as discussed in the latter article as to
survival SVM (SSVM).

The approach taken in SSVM is quite different from the other methods
discussed in the previous subsections. Instead of inferring the hazard directly,
a utility function of the covariates is searched such that the resulting
utility values are as concordant as possible with the corresponding observed
failures. For a thorough investigation of this reasoning and its relation with
the technique of maximizing the margin in standard SVMS, see Van Belle et al.
(2009a). A good concordance is found by implementing ranking constraints,
penalizing misranking between pairs of observations. In addition, regression
constraints are included. These direct the estimate towards prediction of
the events. In Van Belle et al. (2009b), it is empirically observed that such
mechanism yields improved estimates over a pure ranking-based approach.
Technically, consider a transformation with feature map (0(x) of the covariates
x such that the utility estimate for observation i equals u(x,-) = wT(p(x,-). The
problem is then formulated as:

. 1 " "
mm —WTW+VZ€i,i—1+ILZ(Si+S;k)

W,€i,i—1,Ei,5§" 2 i=2 i=1
’ WT§0(xi)—WT§0(xi—1)+€i,i—1 Zyi —yi—1
Vi=2,...,n
wT(0(x,-)+$,-Zy,- Vi=1,...,n (9)
st. 4 —8in(p(xi)+$§kZ—6iyi Vi=1,...,n
ism-420 Vi=2,...,n
5,30 Vi=1,...,n
ﬁgsz Vi=1,...,n,

 

with y and ,u positive regularization constants and 6i,,-_1,$,- and $1?“ slack
variables. In the above formulation, the data points are sorted according to
their failure (or censoring) time such that y,- 2 yi_1. The ﬁrst set of constraints
leads to a solution for which u(x,-) Z u(x,-_1) if y,- Zyi_1 is mostly satisﬁed:
those are referred to as to the ranking constraints. The second and third set
of constraints are referred to as the regression constraints, where the equality

between y,- and u(x,-) is desired only for non-censored observations. Since
5* = 0 for right censored cases (6,- : 0), the outcome is targeted higher than
the survival time for these cases. Non-linearities are modelled by the choice
of (p(x). However, thanks to the kernel trick or K (xi,xj) =(p(x,-)T(o(xj), this
transformation function does not need to be speciﬁed explicitly. Common
kernels include: (i) the linear kernel: K (x,z)=xTz; (ii) the polynomial
kernel of degree a: K (x,z) = (‘L’ +xTz)“ with ‘L’ 2 0; and (iii) the REF kernel
_ IIx—zII%
0.2

deﬁned as K (x,z)=exp  More recently, a kernel for clinical

data (Daemen and De Moor, 2009) was proposed as an additive kernel
K (x,z)=ZZ=1Kp(x,z), where Kp(-, -) depends on the type of the variable
x”. For continuous and ordinal variables, Kp(-, -) is deﬁned as
c— xp — 1”
Kp(xp’zp)= ¥’ (10)
c

with xp the p-th covariate of observation x, c = maxp —minp, with minp and
maxp the minimal and maximal value of the p-th covariate in the given
training dataset D. For categorical and binary data, Kp(-, -) is deﬁned as

If p: 1"
KP<xP,zP)= T x z (11)
01fxp75zp.

The estimated outcome u(x*) for any new observation x* is then given as

no.) = Zai[K(xi,x.)—K(xi_1,x.)1

i=2
+Z(ﬂi-6iﬂi‘)K(xz-,x*), (12)
i=1

with {01,-},{,8,-} and {,Bf} the sets of Lagrange multipliers corresponding to the
ﬁrst, second and third set of constraints in (9). For more information on this
subject, we refer to Van Belle et al. (2009a, b). The resulting optimization
problem is a convex Quadratic Programming (QP) problem, which can be
solved efﬁciently using contemporary solvers.

2.3.2 Feature selection in linear models: positivity constraints In clinical
applications, one is not only interested in trying to ﬁnd a ‘good’ prognostic
index. Searching which variables/genes are relevant (and should be measured
in the future) and which are not needed, is equally important. Feature
selection is included in SSVM by constraining the weights w to positive
weights and will be denoted as SSVMP. If the true parameters were positive,
this constraint would not introduce extra bias unlike an L1 approach. This
estimate is obtained by solving the problem

. 1 " "
m1n * EWTW-i-VZELi—l +ILZ($;' +5?)

Wﬁij—LEhE) i=2 i=1
' wai—wai_1+€i,i—1Zyi—yi—l

Vi=2,...,n

WTxi+5iZYi ViZI’m’” (13)
81. { —8inxi+$§’Z—5iyi Vi=1u~un
wp20 Vp=1,...,d
€l,l—IZO Vi=2,...,n
$1.20 Vi=1,...,n
k $20 Vi=1,...,n.

 

This set of constraints is only relevant after preprocessing the data, in order
to ensure that negative relations are not ignored. Therefore, the concordance
(Harrell et al., 1988) between each variable and the failure time is calculated.
Each variable xp with a concordance less than 0.5 is switched as xP <— —xp.
Constraining the weights to be positive will then lead to weights close to
zero for irrelevant variables, and higher weights for relevant variables. This
technique is closely related with the non-negative least squares estimator,
and with the non-negative garotte estimator (Breiman, 1995).

 

89

1e [3.10'SIBIIJHO[p.IOJXO'SOpBIIIJOJIIIOIQ/ﬂ(11111 IIIOJJ pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

MVan Belle et al.

 

2.3.3 Feature selection in non-linear models: a two-step approach Since
the above formulation cannot be extended directly to the case non-linear
kernels are used, we propose a two-step approach in order to include feature
selection [see also Van Belle et al. (2010a)]. In a ﬁrst step, the method as
described in (9) is solved. Using a non-linear but additive kernel K (xi,xj) =
23:11? (xf ,xf ), the estimated non-linear transformation of each covariate
p can be estimated as

stapes = Zai[KP<xf,x€)—Kp<x§’_1,x5)]
i=2

+Z(ﬂi—6iﬂi)KP(x§’,x€). (14)
i=1

The second step performs feature selection by considering if as the covariates
and applying method (13) with a linear kernel. We will refer to this approach
using a clinical kernel in the ﬁrst step as SSVMPclmical.

3 RESULTS

In subsection 3.1, the performance of SSVM, PH and PLANNARD
approaches are compared based on clinical datasets. In subsection
3.2, SSVM is compared with other methods, able to deal with high-
dimensional data. The design parameters y, ,u, A and s are tuned
using a 10-fold cross-validation criterion.

The different methods will be compared using three performance
measures. Clinicians are typically interested in groups of patients
with higher or lower risk proﬁles. Therefore, a ﬁrst performance
measure denotes the concordance between the predicted and
observed order of relapse: the concordance index (c-index) (Harrell
et al., 1988). In the same perspective, patients are divided into
two risk groups according to the prognostic index obtained with
each model. The median prognostic index is used as threshold for
deﬁning the two groups. The logrank test x2 statistic, measuring
the difference in survival between both groups will be reported. As
a third measure the hazard ratio as calculated by a univariate PH
model using the estimated prognostic indices, normalized to the
unit interval, is reported. For all three measures, a higher value
corresponds to a better performance. In all experiments, datasets
were 50 times randomly divided into training (2/3 of the data) and
test (1/3 of the data) sets. Models were estimated based on the
training data. Results are calculated based on the test data. Table 1
gives an overview of the different methods and their properties.

Complementary to the reported ﬁgures relating performance and
clinical usefulness of the presented methods, a comparison regarding
the effective CPU time required to train each method is reported
in the Supplementary Material. Those results conﬁrm the claim of
fast and effective implementations compared with state-of—the—art
approaches if a suitable contemporary optimization solver is used.

3.1 Clinical cancer data

This subsection compares performances obtained by SSVM, SSVMP,
PH and PLANNARD methods applied on six different clinical datasets1 :

0 The leukemia dataset (Emerson and Banks, 1994) contains
observations of 129 patients with leukemia. The available
variables are as follows: treatment (daunorubicin or idarubicin),
sex, age, Karnofsky score (indicating how well a patient

 

1Data available on http://lib.stat.cmu.edu/datasets and http://cran.r—
project.org/web/packages/survival/index.html.

having cancer is functioning, expressed on a scale from 0% to
100% ), baseline white blood cells, baseline platelets, baseline
haemoglobin, kidney transplant (binary), complete remission
and time until complete remission. In a ﬁrst experiment, the
endpoint is time until death (LD). In a second experiment, the
endpoint is time until complete remission (LCR) and time until
death is not taken into account.

0 The Veteran’s Administration Lung Cancer Trial (VLC;
Kalbﬁeisch and Prentice, 2002; Prentice, 1974) incorporated
137 men with advanced inoperable lung cancer. Patients were
randomized to a standard or test chemotherapy. Only nine
patients were still alive at the end of the study. The available
variables are as follows: the Karnofsky performance score,
age, prior therapy (binary), histological type of the tumour,
treatment and months between diagnosis and randomization.

° The data on prostatic cancer (PC; Byar and Green, 1980)
contains 506 patients with four types of treatment (placebo,
0.2, 1.0 or 5.0 mg diethylstilbestrol daily). Although this dataset
contains information on competing risks, we use it to estimate
survival where death due to prostatic cancer is the event under
study. The variables are as follows: treatment, age, weight
index, performance index, history of cardiovascular disease,
size of the tumour, a combined index of stage and histologic
grade and serum haemoglobin. Of total, 483 patients had
complete information on the variables mentioned above. In
all, 125 (26%) patients died due to prostatic cancer during the
study. All other patients were right censored at their date of last
follow-up.

0 The Mayo Clinic Lung Cancer Data (MLC; Therneau and
Grambsch, 2000) is a subset of data concerning advanced
lung cancer patients, conducted at the North Central Cancer
Treatment Group, Rochester Minnesota (Loprinzi et al. , 1994).
The subset used here contains 167 patients with full information
on the following variables: age, sex, the physician’s estimate of
the ECOG performance and Karnofsky score, patient’s estimate
of the Karnofsky score, calories consumed at meals and weight
loss in the last 6 months. In all, 120 (72%) patients died during
the study period.

0 The German Breast Cancer Study (BC; Sauerbrei and Royston,
1999; Schumacher et al., 1994) is a dataset containing
observations of 720 breast cancer patients, who were recruited
in 41 centres between July 1984 and December 1989.
Available variables are as follows: hormonal therapy (binary),
menopausal status (binary), patient’s age at diagnosis, tumour
grade, tumour size, the number of positive lymph nodes,
expression of the progesterone and oestrogen receptors (in
fmol). The study is performed on the 686 cases with complete
data. In all, 299 (44%) of these patients had a breast cancer-
related event (remission) during the study period.

The SSVM and SSVMP methods are tested using two different kernels:
the linear kernel (SSVMh-near and SSVMPh-near) and the clinical kernel
(SSVMch-nical and SSVMPch-nical). Earlier publications of the authors
compare with results obtained with a RBF kernel, see Van Belle
et al. (2010b). The PH model is tested using a linear parametric form
of the covariates (PHlinear), using a ridge regression penalized partial
likelihood function (Ple), using a LASSO penalization (PHll) and
with penalized smoothing splines (PHpspline). Table 2 illustrates the

 

90

1e [3.10'SIBIIJHO[p.IOJXO'SOpBIIIJOJIIIOIQ/ﬂ(11111 IIIOJJ pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

Improved performance with Survival-SVM

 

Table 1. Summary of the different methods used in the experiments

 

 

Model (Equation) Non- Covariate High Hazard Kernel Software Model
[References] linear selection dim. selection
SSVMlmear (9) \/ Linear mosek, matlab CV
(9) \/ \/ Clinical mosek, matlab CV
SSVMPh'near (13) \/ \/ Linear mosek, matlab CV
SSVMPclmical (14) \/ \/ \/ Clinical mosek, matlab CV
PHlmear Cox (1972) \/ matlab
PH12 B¢velstad et al. (2007) \/ \/ matlab CV
PH” Goeman (2010) \/ \/ \/ \/ R CV
PHpsph'ne Eilers and Marx (1996) \/ \/ R CV
PLANNARD Lisboa et al. (2003) \/ \/ \/ matlab Bayesian
PCR Hastie et al. (2001) \/ \/ matlab CV
SPCR Bair et al. (2006) \/ \/ matlab CV
PLS Nygard et al. (2008) \/ \/ matlab CV

 

It is indicated whether the methods are able (a) to model non-linear effects of the covariates, (b) to perform covariate selection, (c) to handle high-dimensional
data well, ((1) whether or not the hazard is estimated, (e) which kernel is used for kemel-based methods, (i) which software program was used and (g) how model

selection was done in this article. CV: cross-validation.

results. None of the methods performs overall better or worse than
the other methods.

3.2 High-dimensional micro-array data

This subsection reports performances of the described survival
methods when used to model high-dimensional data. Three different
micro-array datasets are used:

0 The Dutch Breast Cancer Data (DBCD) from van Houwelingen
et al. (2006) is a subset of the data from van de Vijver et al.
(2002) and contains information on 4919 gene expression
levels of a series of 295 women with breast cancer.
Measurements were taken from the fresh-frozen tissue bank
of The Netherlands Cancer Institute. All 295 tumours were
primary invasive breast carcinoma less than 5 cm in diameter.
The women were 52 years or younger. The diagnosis was made
between 1984 and 1995 and there was no previous history
of cancer, except non-melanoma skin cancer. In 79 (26.78%)
patients, distant metastases were noted within the study period.
The median follow-up was 6.7 years (range 005—183).

0 The diffuse large B—cell lymphoma data (DLBCL) from
Rosenwald et al. (2002) contains data of 240 patients with
diffuse large B-cell lymphoma. For each patient, one observed
7399 different gene expression measurements. The median
follow-up time was 2.8 years and 58% of the patients passed
away during the study period.

0 The Norway/Stanford breast cancer data (NSBCD) as presented
in S¢rlie et al. (2003) contains gene expression measurements
of 115 women with breast cancer. Of total, 549 intrinsic genes
introduced in S¢rlie et al. (2003) were measured. Missing
values were previously imputed using 10-nearest neighbour
imputation. In all, 38 (33%) patients experienced an event
within the study period.

The SSVM and SSVMP methods will be compared with PH models
adapted for the high-dimensionality in these datasets. Since variable
selection is a major issue in high-dimensional datasets, the number of

coefﬁcients wp with an absolute value larger than 10‘8 is reported as
# weights. Table 3 indicates that the clinical kernel performs better
than the linear one. However, when including a feature selection
algorithm, the results of the linear kernel become signiﬁcantly
better. The SSVMP method signiﬁcantly outperforms all other tested
methods. In addition to a better performance, the SSVMP method with
a linear kernel results in a sparser model than the other methods. Due
to the fact that most PH models approach dimensionality reduction by
composing new features as a function of the measured covariates,
nearly all gene expressions need to be obtained for any test case
[see e. g. Nguyen and Rocke (2002); Park et al. (2002)]. The SSVMP
method on the other hand is able to obtain a high performance based
on less gene expressions.

4 DISCUSSION AND CONCLUSIONS

The PH model is most often used in clinical applications, thanks to
its easy applicability and interpretability. The disadvantages are that
it assumes in its standard form proportional hazards, as well as a
linear parametric form of the covariates. Both assumptions can be
relaxed, the ﬁrst one e. g. by including time-dependent variables, the
second one e.g. by using regression splines. The PLANN model is
a multi-layer perceptron model incorporating non—linearities in the
covariates as well as interactions. The main disadvantage of such
models is the difﬁculty to interpret them. Clinicians are generally
interested in the contribution of each covariate to the estimated risk.
Due to the complex architecture of multi-layer perceptrons, this
contribution is less straightforward to recover. Including automatic
relevance determination in the PLANN framework is a step in the right
direction, although two problems remain. The ﬁrst disadvantage
of PLANN and PLANNARD is that they both reduce the survival
problem to time-dependent classiﬁcation problems. Therefore, all
patients need to be replicated at each time at which they are
at risk. This replication leads to an exponential increase of the
complexity of the estimation problem. The second problem occurs
when dealing with high-dimensional data. For these datasets, the
parameter space for multi-layer perceptrons becomes very large,

 

91

112 Bio'SIBuinoprOJxo'soiiemJOJuioiw/zdnq IIIOJJ pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

MVan Belle et al.

 

Table 2. Performances for different survival methods for red6 clinical datasets

 

 

Dataset Method c-index Logrank x2 Hazard ratio
VLC SSVMlmear 0.68 d: 0.02*** 4.19 d: 3.17*** 7.07 d: 2.75***
SSVMclmical 0.70 d: 0.02 7.84 d: 2.85 10.24 i: 4.36*
SSVMPclmical 0.69 d: 0.02* 6.84 :l: 2.93 7.80 d: 3.20***
PHh'near 0.68 d: 0.03*** 5.45 d: 1.97* 10.61 :I: 2.99**
PH12 0.70 d: 0.02 6.17 d: 0.98 31.26 :I: 7.47*
PH“ 0.70 d: 0.02* 7.04 d: 2.85** 11.41 :I: 4.47**
PHpsph-ne 0.70 d: 0.03 8.11 d: 3.29*** 14.29 i: 6.76**
PLANNARD 0.71 :l: 0.03 6.76 :l: 2.82 11.18 :I: 6.25
LCR SSVMlmear 0.59 d: 0.02 1.50 d: 1.30 3.57 d: 1.54
SSVMclmical 0.58 d: 0.04 1.05 d: 0.96 2.79 :l: 1.36
SSVMPclmical 0.59 d: 0.03 1.14 d: 0.97 3.10 d: 1.43
PHlinear 0.60 d: 0.02 1.60 d: 1.18 4.05 d: 1.34
PH12 0.60 d: 0.02 1.80 d: 1.58 3.66 d: 1.04
PH11 0.57 d: 0.03* 1.11 d: 0.89 3.22 d: 1.37
PHpsph'ne 0.56 d: 0.03* 1.12 d: 0.91 3.42 :l: 1.69
PLANNARD 0.56 d: 0.03 0.72 d: 0.68 2.22 d: 1.16
LD SSVMlmear 0.67 d: 0.03*** 5.94 d: 3.31*** 16.04 :I: 9.27
SSVMclmical 0.69 d: 0.03* 9.06 d: 3.99* 20.98 i: 9.90
SSVMPclmical 0.70 d: 0.03 7.52 d: 4.06*** 29.39 d: 13.20*
PHlinear 0.69 d: 0.03** 9.73 d: 4.23 22.21 :I: 13.02
PH12 0.68 d: 0.02 7.13 d: 3.48 13.9 d: 7.00
PH11 0.69 d: 0.03* 6.97 d: 3.93 21.13 :I: 1206*
PHpsph'ne 0.67 d: 0.03 5.1 d: 3.56 14.96 i: 8.80
PLANNARD 0.69 :l: 0.03** 8.07 :l: 3.86* 11.95 :I: 7.20*
SSVMclmical 0.61 d: 0.03 2.51 d: 2.00 3.53 d: 1.27**
SSVMPclinical 0.60 d: 0.03** 1.64 d: 1.03* 3.06 d: 1.20***
PHlinear 0.61 d: 0.03* 2.89 :l: 1.95 6.04 d: 2.67
PH12 0.62 d: 0.03 4.10 d: 2.61 3.97 d: 1.74
PH“ 0.60 d: 0.03*** 1.46 d: 1.32** 3.55 d: 2.16
PHpsph-ne 0.56 d: 0.03*** 1.06 d: 0.92** 2.41 :l: 1.29***
PLANNARD 0.63 :l: 0.02 4.46 :l: 2.20 3.81 :l: 1.50
PC SSVMlmear 0.76 d: 0.02 11.27 i: 3.34 239.91 :I: 123.05
SSVMclmical 0.78 d: 0.02 13.80 :I: 4.49* 149.23 :I: 79.55*
SSVMPclinical 0.78 d: 0.02 13.61 i: 3.91** 144.33 3: 73.35***
PHlinear 0.77 d: 0.02 11.39 i: 3.46 266.26 3: 150.48
PH12 0.76 d: 0.02 11.19 :I: 3.44 228.04 3: 107.23
PH11 0.76 d: 0.02 10.49 i: 3.27 247.56 3: 128.35
PHpsph'ne 0.78 :l: 0.02 12.61 i: 4.26 207.16 3: 88.44
PLANNARD 0.76 :l: 0.02 10.87 :I: 3.80 49.44 :I: 31.14***
BC SSVMlinear 0.67 :l: 0.02 16.97 i: 4.84 79.38 :I: 49.57
SSVMclmical 0.68 d: 0.02 20.93 i: 4.64** 22.21 :I: 8.04***
SSVMPclmical 0.68 d: 0.02 24.14 i: 7 .56** 20.27 i: 6.95**
PHlinear 0.67 d: 0.02 18.11 i: 4.83 102.78 3: 61.38
PH12 0.68 d: 0.02 20.16 :I: 4.2 48.19 d: 18.49**
PH11 0.67 d: 0.02 15.20 i: 3.99 193.77 3: 139.29**
PHpsph'ne 0.68 d: 0.02 21.99 i: 6.69** 158.69 :I: 107.79
PLANNARD 0.67 d: 0.02 15.00 :I: 5.11 19.63 i: 9.28***

 

Median and median absolute deviation of 50 random splits into train-test sets are given. Statistically signiﬁcant differences between SSVMP
(reference model: indicated in grey) and the other methods are indicated as: *P < 0.05 ; **P < 0.01; ***P < 0.001. The best results are typeset

in bold.

 

92

112 Bio's112umofp101xo'soi112u1101uioiq/pd11q 111011 pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

Improved performance with Survival-SVM

 

Table 3. Performances obtained on three micro-array datasets

 

 

Dataset Method c-index Logrank x2 Hazard ratio # weights
nsbcd SSVMlmear 0.60 :l: 0.08*** 0.85 :l: 0.79*** 4.35 :l: 3.70***
SSVMclmical 0.71 :l: 0.03*** 4.46 :l: 2.31 16.90 :I: 9.46***
SSVMPclmical 0.68 :l: 0.04*** 3.72 :l: 2.10 16.49 :I: 10.18*** 288 :l: 183***
PCR 0.72 :l: 0.03** 4.67 :l: 2.51 24.23 :I: 14.84*** 549 :l: 0***
SPCR 0.71 :l: 0.03*** 4.88 :l: 2.66 13.55 :I: 7.46*** 137 :l: 136**
PLS 0.73 :l: 0.04* 4.00 :l: 2.09* 21.39 :I: 11.58*** 549 :l: 0***
PH12 0.66 :l: 0.05*** 1.91 :l: 1.45*** 11.76 :I: 6.97*** 549 :l: 0***
PH11 0.71 :l: 0.04*** 4.73 :l: 1.89 17.97 :I: 8.21*** 119.5 :I: 14***
dlbcl SSVMh'near 0.61 :l: 0.03*** 4.14 :l: 2.41*** 6.22 :l: 3.21***
SSVMclmical 0.63 :l: 0.03*** 5.97 :l: 3.42*** 7.45 :l: 3.15***
SSVMPclmical 0.62 :l: 0.02*** 5.71 :l: 2.50*** 7.53 :l: 3.02*** 7027 :l: 266***
PCR 0.60 :l: 0.02*** 2.88 :l: 1.83*** 4.66 :l: 1.90*** 7399 :l: 0***
SPCR 0.59 :l: 0.03*** 2.08 :l: 1.84*** 4.49 :l: 2.09*** 7399 :l: 0***
PLS 0.58 :l: 0.03*** 1.65 :l: 1.50*** 3.49 :l: 1.63*** 7399 :l: 0***
PH12 0.64 :l: 0.03*** 6.04 :l: 2.64*** 8.71 :l: 4.27*** 7399 :l: 0***
PH11 0.63 :l: 0.03*** 5.41 :l: 2.85*** 7.57 :l: 3.83*** 3960.5 :l: 2477
dbcd SSVMlmear 0.64 :l: 0.02*** 3.49 :l: 1.48*** 7.74 :l: 2.62***
SSVMclmical 0.75 :l: 0.03*** 13.30 :I: 4.00*** 33.61 :I: 15.88***
SSVMPclim-cal 0.75 :l: 0.03*** 12.57 :I: 4.18*** 33.78 :I: 17.00*** 4232 :l: 271***
PCR 0.73 :l: 0.02*** 9.63 :l: 2.68*** 28.10 :I: 16.96*** 4919 :l: 0***
SPCR 0.73 :l: 0.03*** 10.31 :I: 3.02*** 24.34 :I: 12.09*** 860 :l: 848
PLS 0.74 :l: 0.02*** 11.23 :I: 3.97*** 10.21 :I: 3.34*** 4919 :l: 0***
PH12 0.71 :l: 0.03*** 10.7 :I: 3.41*** 23.06 :I: 12.38*** 4919 :l: 0***
PH“ 0.71 :l: 0.06*** 11.14 :I: 6.51*** 15.58 :I: 13.37*** 1786 :l: 896

 

The SSVM and SSVMP methods are compared with four PH models with different regularization mechanisms to deal with high-dimensional data. Median and median absolute
deviations of the performances for 50 randomly splits in train-test sets are given. Statistically signiﬁcant differences between SSVMP (reference model: indicated in grey) and

the other methods are denoted as: *P < 0.05; ** p < 0.01; ***P < 0.001. The best results are typeset in bold.

with an increasing risk of overﬁtting the training data. Thanks to
regularization mechanisms, the number of effective parameters can
be much lower than the number of weights. Nevertheless, for high-
dimensional data, training remains difﬁcult and/or time consuming,
as illustrated in the Supplementary Material report. Finally, multi—
layer perceptrons are known to be non-convex and the optimal
solution will only be locally optimal.

The SSVM method does not assume a linear effect of the
covariates, and has the additional computational advantage that
data points do not need to be replicated. This is the result
of reformulating the survival problem as a combined ranking-
regression approach instead of as a time-dependent classiﬁcation
problem. The disadvantage of this approach is that estimation
of the hazard is not directly incorporated in the model. The
cumulative hazard can be estimated after categorizing patients into
risk groups according to the estimated risk using the Nelson—Aalen
estimator. The most important advantage of the SSVM method is the
applicability and performance obtained on problems involving hi gh—
dimensional data. Additionally, since the estimation problem boils
down to solving a convex QP standard efﬁcient solvers can be used,
and the estimate is a guaranteed global optimum.

The experiments give evidence that the SSVMP method
outperforms standard techniques on high-dimensional micro—array
data, while they perform similar to other methods on clinical data.

ACKNOWLEDGEMENT

We kindly acknowledge the support and constructive remarks of
E. Biganzoli and P. Boracchi.

Funding: Research Council KUL: GOA-AMBioRICS, GOA
MaNet, CoE EF/05/006 Optimization in Engineering (OPTEC),
several PhD/postdoc and fellow grants; Flemish Government:
FWO: PhD/postdoc grants, projects, G.0302.07 (SVM), research
communities (ICCoS, ANMMM); IWT: TBM070706-IOTA3,
PhD Grants; Belgian Federal Science Policy Ofﬁce IUAP
P6/04 (DYSCO, ‘Dynamical systems, control and optimization’,
2007—2011); EU: FAST (FP6—MC—RTN—035801), Neuromath
(COST-BM0601). V.V.B. is supported by a grant from the IWT.
K.P. is an associate professor (‘Forskarassistent’) at the University
of Uppsala, Sweden. S.V.H. is a full professor and J.A.K.S. is a
professor at the Katholieke Universiteit Leuven, Belgium.

Conﬂict of Interest: none declared.

REFERENCES

Bair,E. et al. (2006) Prediction by supervised principal components. J. Am. Stat. Assoc. ,
101, 119—137.

Bakker,B. et al. (2004) Improving Cox survival analysis with a neural-Bayesian
approach. Stat. Med, 23, 2989—3012.

 

93

112 Bio's112umofp101xo'soi112u1101uioiq/pd11q 111011 pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

MVan Belle et al.

 

Biganzoli,E. et al. (1998) Feedforward neural networks for the analysis of censored
survival data: a partial logistic regression approach. Stat. Med, 17, 1169—1186.
B¢velstad,H.M.M. et al. (2007) Predicting survival from microarray data - a

comparative study. Bioinformatics, 23, 2080—2087.

Breiman,L. (1995) Better subset regression using the nonnegative garrote.
Technometrics, 37, 373—384.

Byar,D. and Green,S. (1980) Prognostic variables for survival in a randomized
comparison of treatments for prostatic cancer. Bull. Cancer, 67, 477—490.

Callas,P.W. et al. (1998) Empirical comparisons of proportional hazards, poisson, and
logistic regression modeling of occupational cohort data. Am. J. Indust. Med, 33,
33—47.

Cox,D.R. (1972) Regression models and life-tables (with discussion). J. R. Stat. Soc.
Ser: B, 34, 187—220.

Daemen,A. and De Moor,B. (2009) Development of a kernel function for clinical
data. In Proceedings of the 31th Annual International Conference of the
IEEE Engineering in Medicine and Biology Society (EMBS), IEEE, Piscataway,
pp. 5913—5917.

Eilers,P.H. and Marx,B.D. (1996) Flexible smoothing with B-splines and penalties. Stat.
Sci, 11, 89—121.

Emerson,S.S. and Banks,P.L.C. (1994) Interpretation of a leukemia trial stopped early.
In Lange,N. et al. (eds) Case Studies in Biometry, Wiley-Interscience, New York,
pp. 275—299.

Evers,L. and Messow,C.M. (2008) Sparse kernel methods for high-dimensional survival
data. Bioinformatics, 24, 1632—1638.

Faraggi,D. and Simon,R. (1995) A neural network model for survival data. Stat. Med,
14, 73—82.

Goeman,J.J. (2010) L1 penalized estimation in the Cox proportional hazards model.
Biometr: J., 52, 70—84.

Green,M.S. and Symons,M.J. (1983) A comparison of the logistic risk function and the
proportional hazards model in prospective epidemiologic studies. J. Chronic Dis.,
36, 715—723.

Harrell,F. et al. (1988) Regression models in clinical studies: determining relationships
between predictors and response. J. Natl Cancer Inst, 80, 1198—1202.

Hastie,T. et al. (2001). The Elements of Statistical Learning. Springer.

Huang,J.Z. et al. (2000) Functional ANOVA modeling for proportional hazards
regression. Ann. Stat, 28, 961—999.

Kalbﬂeisch,J.D. and Prentice,R.L. (2002) The Statistical Analysis of Failure Time Data.
Wiley series in probability and statistics, New York.

Leblanc,M. and Crowley,J. (1999) Adaptive regression splines in the Cox model.
Biometrics, 55, 204—213.

Lisboa,P. et al. (2003) A Bayesian neural network approach for modelling censored
data with an application to prognosis after surgery for breast cancer. Art. Intell.
Med, 28, 1—25.

Loprinzi,C.L. et al. (1994) Prospective evaluation of prognostic variables from patient-
completed questionnaires: north central cancer treatment group. J. Clin. Oncol, 12,
601—607.

Nguyen,D.V. and Rocke,D.M. (2002) Partial least squares proportional hazard
regression for application to DNA microarray survival data. Bioinformatics, 18,
1625—1632.

Nygard,S. et al. (2008) Partial least squares Cox regression for genome-wide data.
Lifetime Data Anal, 14, 179—195.

Park,P.J. et al. (2002) Linking gene expression data with patient survival times using
partial least squares. Bioinformatics, 18 (Suppl. 1), 8120—8127.

Prentice,R.L. (1974) A log gamma model and its maximum likelihood estimation.
Biometrika, 61, 539—544.

Rosenwald,A. et al. (2002) The use of molecular proﬁling to predict survival after
chemotherapy for diffuse large-B-cell lymphoma. N. Engl. J. Med, 346, 1937—1947.

Sauerbrei,W. and Royston,P. (1999) Building multivariable prognostic and diagnostic
models: transformation of the predictors by using fractional polynomials. J. R. Stat.
Soc. Ser: A, 162, 71—94.

Sch61kopf,B. and Smola,A. (2002) Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT Press, Cambridge.

Schumacher,M. et al. (1994) Randomized 2 x 2 trial evaluating hormonal treatment
and the duration of chemotherapy in node-positive breast cancer patients. J. Clin.
Oncol., 12, 2086—2093.

Shawe-Taylor,J. and Cristianini,N. (2004) Kernel Methods for Pattern Analysis.
Cambridge University Press, Cambridge, UK.

Shivaswamy,P.K. et al. (2007) A support vector approach to censored targets. In
Proceedings of the 2007 Seventh IEEE International Conference on Data Mining
(ICDM). IEEE Computer Society, CA, pp. 655—660.

S¢rlie,T. et al. (2003) Repeated observation of breast tumor subtypes in independent
gene expression data sets. Proc. Natl Acad. Sci. USA, 100, 8418—8423.

Suykens,J.A.K. (2002) Least Squares Support Vector Machines. World Scientiﬁc,
Singapore.

Themeau,T.M. and Grambsch,P.M. (2000) Modeling Survival Data: Extending the Cox
Model, 2nd edn. Springer, New York.

Tibshirani,R. (1997) The lasso method for variable selection in the Cox model. Stat.
Med, 16, 267—288.

Van Belle,V. et al. (2007) Support Vector Machines for Survival Analysis. In Ifeachor,E.
and Anastasiou,A. (eds) Proceedings of the Third International Conference on
Computational Intelligence in Medicine and Healthcare (CIMED). University of
Plymouth, Plymouth, UK.

Van Belle,V. et al. (2008) Survival SVM: a practical scalable algorithm. In Verleysen,M.
(ed.) Proceedings of the 16th European Symposium on Artiﬁcial Neural Networks
(ESANN). d-side, Evere, pp. 89—94.

Van Belle,V. et al. (2009a) Learning transformation models for ranking and survival
analysis. Technical report, 09-135, ESAT-SISTA, K ULeuven (Leuven, Belgium)
2009. Available at ftp://ftp.esat.kuleuven.ac.be/pub/SISTA/vanbelle/reports/09-
135.pdf (last accessed date November 19, 2010).

Van Belle,V. et al. (2009b) Support vector methods for survival analysis: a
comparison between ranking and regression approaches. Technical report,
09-235, ESAT-SISTA, KULeuven (Leuven, Belgium) 2009. Available at
ftp://ftp.esat.kuleuven.ac.be/pub/SISTA/vanbelle/reports/09-235.pdf (last accessed
date November 19, 2010).

Van Belle,V. et al. (2010a) Additive survival least squares support vector machines.
Stat. Med, 29, 296—308.

Van Belle,V. et al. (2010b) On the use of a clinical kernel in survival analysis. In
Verleysen,M. (ed.) Proceedings of the European Symposium on Artiﬁcial Neural
Networks (ESANN2010), d-side, Evere, pp. 451—456.

van de Vijver,M.J. et al. (2002) A gene-expression signature as a predictor of survival
in breast cancer. N Engl. J. Med, 347, 1999—2009.

van Houwelingen,H.C. et al. (2006) Cross-validated cox regression on microarray gene
expression data. Stat. Med, 25, 3201—3216.

Vapnik,V. (1998) Statistical Learning Theory. Wiley and Sons, New York.

 

94

112 Bio's112umofp101xo'soi112u1101uioiq/pd11q 111011 pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

