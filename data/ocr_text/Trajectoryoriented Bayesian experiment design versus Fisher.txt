Vol. 28 ECCB 2012, pages i535-i541
doi: 10. 1093/bioinformatics/bts3 77

 

Trajectory-oriented Bayesian experiment design versus Fisher
A-optimal design: an in depth comparison study

Patrick Weber“, Andrei Kramerl, Clemens Dingler1 and Nicole Radde1
1Institute for Systems Theory and Automatic Control, University of Stuttgart, Pfaffenwaldring 9, Stuttgart 70550,

Germany

 

ABSTRACT

Motivation: Experiment design strategies for biomedical models
with the purpose of parameter estimation or model discrimination
are in the focus of intense research. Experimental limitations such as
sparse and noisy data result in unidentifiable parameters and render-
related design tasks challenging problems. Often, the temporal
resolution of data is a limiting factor and the amount of possible
experimental interventions is finite. To address this issue, we propose
a Bayesian experiment design algorithm to minimize the prediction
uncertainty for a given set of experiments and compare it to
traditional A-optimal design.

Results: In an in depth numerical study involving an ordinary
differential equation model of the trans-Golgi network with 12 partly
non-identifiable parameters, we minimized the prediction uncertainty
efficiently for predefined scenarios. The introduced method results
in twice the prediction precision as the same amount of A-optimal
designed experiments while introducing a useful stopping criterion.
The simulation intensity of the algorithm’s major design step is
thereby reasonably affordable. Besides smaller variances in the
predicted trajectories compared with Fisher design, we could also
achieve smaller parameter posterior distribution entropies, rendering
this method superior to A-optimal Fisher design also in the parameter
space.

Availability: Necessary software/toolbox information are available
in the supplementary material. The project script including
example data can be downloaded from http://www.ist.uni-
stuttgart.de/%7eweber/BayesFisher2012.

Contact: patrick.weber@ist.uni-stuttgart.de

Supplementary Information: Supplementary data are available at
Bioinformatics online.

1 INTRODUCTION

Regulation models are preferably used to describe intra— and
inter—cellular interactions of biomolecules. The Biomodel Database
of the European Bioinformatics Institute comprises over 400
published and curated models in its repository, most of them being
ordinary differential equation (ODE) models (Li et al., 2010). The
usefulness of a biomedical ODE model is often assessed through
its capability to predict possible scenarios of interest. In order to
calibrate a biological ODE model, expensive and time—consuming
experiments are performed to gain the needed experimental training
data. Despite continuously improving measurement methods, the
data for most models remain scarce, resulting in practically non—
identiﬁable parameters (Gutenkunst et al., 2007). To support the

 

*To whom correspondence should be addressed.

learning process, several experiment design methods have been
successfully developed including classical Fisher information matrix
(FIM) design (Bandara et al., 2009), Bayesian methods (Chaloner
and Verdinelli, 1995; Kramer and Radde, 2010; Wilkinson, 2007)
and optimization—based methods (Kreutz et al., 2011). Claiming
that ODE model predictions improve when reducing parameter
uncertainty intervals, these methods design optimal experiments
for parameter estimation (OED/PE). However, models can be used
to make precise predictions despite sloppy parameters with large
conﬁdence intervals (Brown et al., 2004; Klinke, 2009). This is
rendering OED/PE an indirect method to improve predictions.
Methods have been developed to directly address experiment
design for better model predictions (Casey et al., 2007; Vanlier
et al., 2012), which we here refer to as OED/MP. In this work,
we also propose to go this direct way, by predeﬁning sets of
experimentally feasible prediction scenarios of interest. We use a
Bayesian posterior inference method and predict experimentally
feasible scenarios to purposively reduce uncertainty in the model
trajectories. In addition, we deﬁne a reasonable stopping criterion
to avoid further experimentation when the desired predictions meet
a certain precision compared with the expected data quality. The
method is successfully applied to reduce prediction uncertainty of an
ODE model of secretory pathway control at the trans—Golgi network.
In an intense comparison study, we were able to outperform A—
optimal designed experiments in both prediction uncertainty and
posterior distribution entropy, with affordable computational effort.

2 METHODS
System

In this work, we address the problem of efﬁciently selecting experiments to
improve the prediction capabilities of a given ODE model of the following
form:

5c(t; u, 6) =f(x(t; u, 6), u(t), 6)
W: u,6)=h(x(t; m9), MU)a 9),

x(0;u,6)=x0(u,6) (1)

with state x(t;u,6)eR$‘, input u(t)elR{:“, parameter 6 ER? and output
y(t; u, 6) 6 R13; . The vector ﬁeld f is assumed to be continuously differentiable
to guarantee the existence of a unique solution. For intracellular models, f
is often derived by using chemical reaction kinetics. The output mapping
It deﬁnes the measurable outputs as functions of model states. To include
typical experimental pretreatments, we allow the initial conditions x0(u, 6) to
depend on the system parameters 6 and initial experimental inﬂuences u. The
system parameter vector 6 consists of positive rate coefﬁcients, Michaelis
Menten parameters or degradation and synthesis rates. Given positivity,
these constants are assumed to be loglO—transformed, 6={61,...,6,,9}=
{log10(k1), ...,log10(kn9)}, to guarantee numerical stability in subsequently
introduced numerical computations.

 

© The Author(s) 2012. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which
permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.

112 /§.IO'S[BU.IHO[p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

H Weber et aI.

 

Experiments The goal of improving the model prediction is restricted to
a predeﬁned set of experiments {802.21. In this work, we consider each
prediction scenario also a potential experiment. Each experiment index i
is connected to an experimental setup. An experiment 6’" is deﬁned by an
experimentally feasible input vector Lil-(1‘) and a set of measurable quantities
yi =(yi1,...,y;; ). Typical experiments include for example initial changes

of Lil-(1‘) at to describing vector overexpression or siRNA experiments. Also
popular are speciﬁc changes at a predeﬁned time instance that refer to culture

medium changes or external stimuli. Each measurable quantity y}, j = 1, . . . , n’y

can be measured at one discrete time point if? within the set i]? e T: {to, to +
At, t0+2At,...,tend}.

Data and likelihood function We assume that the measurement process is
prone to measurement noise, and we choose a log—normal error model here, in
accordance with recent studies (Gassmann et 01., 2009; Kreutz et 01., 2007).
Thus, whenever a measurement is performed, the true system is assumed to
provide a data point of the form

wwwﬁgmw, m

with r} ~logN(0,0*2), 6* denoting the true system parametrization and 0*
denoting the true error model parameter. For simplicity, we assume the same
parameter 0 for all outputs and experiments, which is no general restriction
for our proposed method. We collect the data from experiment i in the set
Di 2 {Z1130}? )}j:lmn; . Given this error model and assuming independence of
all measured data points, the likelihood function for the system states

He
ambmww
i=1

(3)

 

~. . ~. . . 2
H 1 1 z}(t})—y}(ti;u’,6)

2 ex —— — ,
. . V2710 p 2 0

with the log transformations 2130;.) =1og(y;i(ti‘ ; ul‘ , 9)) and 21130;) =1og(z;3(t;‘)).
The maximum—likelihood estimate (MLE) for the parameters is given by
maximizing the likelihood function:

(W = argmeaxp(D|6). (4)

2.1 Trajectory-oriented Bayesian design

The objective function in the Bayesian framework is the posterior distribution
p(6|D), which is a distribution over the parameters 6 after having seen the
data. Values of the posterior distribution can be calculated using Bayes’
theorem:

P(D|9)P(9)

P(QID)— P(D) , (5)
where P(6), the prior distribution, reﬂects our prior assumptions about the
parameters. P(6) can be used to restrict the search space to regions of
plausible values. P(6lD) can be investigated through Markov chain Monte
Carlo sampling, and we use an adaptive Metropolis algorithm for this
purpose.

In the following, we denote the representative sample for the current
posterior distribution {65:s=I,...,N} or short {65}. Each sample step
requires evaluation of the likelihood function, for which the ODE system
has to be solved numerically. Thus, together with a sample {65}, we get
automatically for each experiment i and each measurable output )2} a set
of trajectories {yJi(t;6S,ui):s=1,...,N}. From those trajectories, we can
estimate variances according to

A. . 1 . . . _A. . . 2
TWPEIZ@%WHWW) @
6€{6S}

with estimated mean

A...1 ...
ww%ﬁ2mmm m

939$}

for each i, j and each time point t]? e T. In the ﬁrst step, we determine the
set of measurement times if? that maximize the expected variances in the
trajectories
2;. = argmaij’(tJ’-), (8)
556T

and collect the set of respective variance estimates in the sets vi =
{V}(i})}j:lmn; . These sets are the basis for our experiment selection
procedure. At this point, we introduce a stopping criterion: measurement on
y}: in experiment i is performed only if the respective variance  is still above
a certain threshold, which we set to the pooled empirical variance estimate
62 here. This means that we propose to measure only if we can still expect
a decrease in I7? even when we take the precision 6—2 of the measurement
process into account. To address this stopping criterion mathematically, we
set I7” = {25* :=max(I7ji —62,0)}. The successor experiment 2 is selected
by maximizing the sum of expected variances within each experiment

i=argmax Z  (9)

Once i has been determined, we suggest to measure at time instances  for
which  750.

To illustrate this approach, we refer to a fetch—ahead of our numerical study
given in Figure 1, which shows the sets of trajectories {yJi(t;6S,ui)} of the
posterior sample {65 } for each measurable quantity y}: after a ﬁrst experiment
was performed (upper row), along with the proposed measurement time
instances 2;, which are indicated with vertical lines here, the estimated

variances  and the current values of the pooled measurement error

estimates 62. The second line shows predictions subject to the current sample
{65} for a different experiment. In this case, the algorithm would decide to
measure all quantities y]? once again, since the stopping criterion is fulﬁlled
for none of them.

2.2 FIM experiment design

To compare our proposed Bayesian experiment selection to a
computationally inexpensive and well—established experiment design
routine, we shortly introduce the well—known FIM based experiment design
routine (Atkinson and Donev, 1992). The improvement of FIM optimal
experiment design for time—series measurements in biomedical model
discrimination (OED/MD), OED/PE or the combination of both disciplines
is in the focus of recent publication series (Donckels et al., 2012, 2009),
rendering it a solid reference method. In order to apply the method, the FIM
must be evaluated at the current MLE 6ml for the current index set L, of all
already performed experiments:

.meb=§jﬁem) am

161,,

"i
1 ~... T ~...
=Zzghwmmﬂ%hwmmﬂ%.

ieijzl

The superposition principle of the FIMs holds for experiments, outputs
and measurement time points. In the next step, anticipatory update FIMs

17;(6m1,tji) for all candidate experiments i =1,...,ng and j =1,...,n; are

evaluated at the current MLE. The update FIMs 17;(6m1,t;) are added
individually to the current FIM, forming the overall predicted FIMs
13*; (éml,tj)=F(émI)+F{ (ém1,t;). A design criterion c1>(13*{(ém1,t;)) has to be
applied to the update FIMs to decide upon the successor experiment for
the current design step. Under the threat of non—identiﬁable parameters,
we decided to apply the modiﬁed A—optimal criterion CI>(FJ?(6m1,tJ?))=

 

i536

112 /B.IO'S[BU.IHO[p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

Trajectory-oriented Bayesian experiment design

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

3 PKD" D PHI-(III? D CERT" D DAG U Ceramide
1D 1., : 1E] 10 - "ID 1D
r.——=.'—l
performe'd = T 5"":
experiment‘l
indefizg 1:15 11:1'5 MT“ in"2 in":
10 -5 -11) 4 .4. '
10 ll] 10 “ID 'IIII
[I 20 40 EU 0 ED 40 ED 2|] 40 El] III 20 40 EU CI 20 4D 50
u
10 1D _ ‘ID if}
..._———___ =—
axperimentE
index i=2? 10.5  10.2 10-2
5 ~10 .4
ID 1D 1E} __.—._._ c. . _.  _
El 2111 40 BE] L'I ED 40 ED El] 40 BID Elli =10 60 El 2E] 40 60
time h] time [it] time [hi time [h] time [h]

1 l'i-Ijﬁ'i,"1II1'-li'!-i yiij

— Lt'.‘-iji-r.1ur}'1'2it'ieit1c-t-I-Ilj

— ti'xjiii'ifllstl {'I'I'L'ii' IIIUIIE‘I 1.1-1 :‘ii'IJIIZ'i-J n3

[lEiEI-I iii]
.‘1 | I .13.. mi jr‘rtm'f:

[il'iillith'i‘fi firm: iifIIIE‘HHIIJ'E‘IIH‘IIIIL

 

Fig. 1. Predicted trajectories for the experiment outcome after an initial training experiment is performed. Trajectories are depicted together with the trajectories’
empirical variance. The ﬁrst row depicts the model ﬁt for the initial experiment. The measurement time instances suggested by our design algorithm are
depicted by vertical lines. The second row evinces the proposed measurements for the second successor experiment. For experimental inputs corresponding

to experiment index i, we refer to Supplementary Table S3

tr(l:“;(6m1,tji)) to completely avoid the inversion of close to singular FIMs.
For fairness of comparison, we need to have the same amount of overall
measurements m for the next experiment, which is determined by the
Bayesian design. If m outputs are to be measured at yet unknown ti, these
are chosen according to

f]? =argmaxtr(f7;(6mlt If», (11)

ti

J
A(K,§,)= Z tr(13}?(éml,?})), (12)

jGKt‘h

 =argmaxA(K,it)a (13)

Kilt
i=argmaxA(§,it)a (14)

l

where Kg, denotes the index subset of the set of all measurable outputs y;
with m elements. The successor output set and time points if contain all
needed information to plan the experiment. We note that other FIM design
criteria such as minimizing the determinant of the inverse matrix are more
involved computationally, since the superposition principle does not hold
any more and maximization cannot be decoupled any longer.

3 DESIGN OF THE STUDY

3.1 A model for secretion control at the trans-Golgi
network

Protein secretion in mammalian cells is a highly regulated process,
in which the regulation of the formation of transport vesicles
at the trans—Golgi network plays a crucial role. Understanding
these regulatory processes is for example important for further
optimization of producer cell lines, where it has been shown that for

high messenger RNA (mRNA) copy numbers, there are bottlenecks
further downstream, and one example is the trans—Golgi network
(Becker et al., 2008). Vesicle formation is mediated by a network
of interacting proteins and lipids, involving the protein kinase D
(PKD), the ceramide tranfer protein CERT, the lipid kinase PI4KIIIB,
ceramide and diacylglycerol (DAG). Figure 2 shows a scheme of our
regulatory network model. DAG is located in the Golgi membrane
and recruits and thereby activates PKD (Bard and Malhotra, 2006).
Active PKD has a dual effect on CERT transport activity: on the
one hand, it directly phosphorylates and thereby deactivates CERT
(Fugmann et al., 2007), on the other hand, it activates CERT
indirectly through activating PI4KIIIB, which triggers synthesis of
phosphatidyl—inositol—4—phosphate (PI4P). PI4P enables binding of
CERT to the TGN and release of ceramide (Fugmann et al., 2007).
The feedback loop between CERT and PKD is closed by conversion
of phosphatidylcholine (PC) and ceramide to sphingomyelin (SM)
at the Golgi, where DAG occurs as a byproduct. Both PKD and
DAG have also been shown to play a direct role in vesicle formation
(Bard and Malhotra, 2006; Hausser et al., 2005). Although the main
interactions are known, the kinetics of this system has not yet been
investigated quantitatively, and there are currently no appropriate
experimental data available for model ﬁtting. Thus, we use artiﬁcial
data here.

A differential equation model for the regulation process was
established which involves 9 molecular species, 17 chemical
reactions, 12 interaction parameters and 17 further parameters such
as basal production and degradation rates. Up to ﬁve model outputs
are measurable upon request, namely active PKD (PKD*), active
PI4KIIIB (PI4KIIIB*), active CERT (CERT*), overall DAG and
overall ceramide. The four remaining model status variables are

 

i537

112 /B.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

H Weber et aI.

 

    
  

 

 

 

PIA-Kiln!
SM synthesis
“WEED” phesphsrylatler
4—
:41:35 [Jim ry |.':I l- L1|'

  

ll

CERT+ PI4P 
bﬁ Li 

CERT ectluetienireeruitment

PMKIIIE

Fig. 2. Model for secretion control at the trans—Golgi network. Grey
nodes denote latent variables, black nodes measurable species. Active
species are denoted with an asterisk. Self—loops describe linear degradation
terms. Dashed boxes depict turnover reactions. The inputs ‘u’ indicate
experimentally possible perturbations of synthesis rates. Blue text explains
biological processes depicted by corresponding edge. SM and PC are not
contained in the model but shown for completeness of the SM synthesis
reaction

deﬁned as latent variables. Equations and parameter values are listed
in Supplementary Appendix.

3.2 Experiment design testing

In order to test our newly suggested algorithm, we performed a
biologically motivated in silico study. Our Bayesian and a classical
A—optimal design routine are allowed to make suggestions from
a set of 27 experiments, which describe all possible perturbation
permutations of the three species that can experimentally be accessed
through overexpression or siRNA silencing, namely PKD, CERT
and PI4KIIIB. The valid experiment time horizon has been set
to tend=72h, which is a realistic time frame for a cell culture
experiment with proliferation and conﬂuence effects. In the case
of silencing, the respective basal expression rate is damped by a
factor of 10, while in the vector overexpression case, the basal rate is
10—fold higher. These input perturbation parameter values for siRNA
silencing and vector overexpres sion are experimentally motivated by
typical suppression rates (Pillai et al., 2005). Although the Bayesian
approach can in principle incorporate input uncertainty, this is not
considered in this comparison study to avoid shifting the focus.
We assume all 12 interaction parameters to be initially unknown
and choose a bounded log uniform prior supporting four orders of
magnitude for each of them. Parameters are estimated and sampled

in logarithmic space, while basal production and degradation rates
are set to biologically plausible values (Eden et al., 2011). To
account for limitations in the quantity of sampling time instances
in protein quantiﬁcation (e.g. western blotting: 18 lanes per gel),
both algorithms were allowed to demand for a maximum of one
data triplet at one sampling time instance for each output. Thus, if for
example data for all ﬁve outputs are requested this would result in 15
data points in total. The requested data are generated by the unknown
true system and noise corrupted using a log—normal error model with
0* 20.15. The exercise is stopped as soon as one algorithm trained
the model in a way that it fulﬁlls our stopping criterion. Details on the
choice of the joint initial experiment are given in the Supplementary
Material. To make the study more realistic, we estimate the variance
62 in each simulation scenario using a pooled variance estimate. In
order to compare the two algorithms, the 12—dimensional parameter
posterior distributions have been estimated through Markov chain
Monte Carlo (MCMC) sampling after each newly obtained dataset.
For their computation, the sampling algorithms of the MCMC
toolbox (Haario et al., 2006) have been combined with the fast
ODE integrators of the SBtoolbox2 (Schmidt and Jirstrand, 2006)
and the parallel computing capabilities of Matlab. Model prediction
and entropy estimation in the following sections are based on
these posterior distributions. For the prediction of our proposed
Bayesian method, 1000 parameters were drawn from the posterior
distribution according to the description in ‘Methods’ section, and all
27 experiments are simulated. The time horizon of 72 h was divided
into 105 equidistant discrete sampling times, leaving realistic 40 min
of reaction time for an experimentalist between two subsequent wet
lab sample preparation procedures. The whole design exercise has
been repeated ﬁve times for each algorithm.

4 RESULTS

4.1 Bayesian experiment design results in twice the
prediction quality than A-optimal design

Figure 3 depicts the expected uncertainty of the predictions,
represented here as the overall mean of all expected variances W01?)
after being trained with experiments selected by the competing
algorithms. We additionally averaged over ﬁve runs. After each
new training experiment, a prediction for all experiments has been
performed. The stopping criterion was reached by the Bayesian
routine after four experiments in all ﬁve runs, while requesting a
total of 12 data points. This means that we expect the trained model
to be able to make predictions for all 27 experiments in which all
trajectory variances are below the threshold.

The evolution of the mean trajectory variance D, averaged over
all 27 experiments, outputs and time points, is depicted in Figure 3.
It reaches a value of 0.06 after four experiments for the Bayesian
design, making the model prediction twice as good as the A—optimal
trained model with a value of 0.12. After the terminal experiment
6 =0.15 :I:0.2 was close to 0* (see Supplementary Appendix).

4.2 Bayesian experiment design gains more
information about parameters than A-optimal
design

Since A—optimal design reduces the overall spread of the posterior

distribution, while the proposed method reduces the observable

 

i538

112 /BJO'S[BUJn0[p.IOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Trajectory-oriented Bayesian experiment design

 

 

- Beyeman
A-eptimel "

[1.3
0.25-

IILL‘ELIL D 2 _
Iil'tttllfiflilﬂ
uncertainty 0.15
L1
[3.1

0.05- i i ~.

 

D. .—,—_.
1 2 3 4

perfin'rneil experitnents

Fig. 3. Mean expected prediction uncertainty for all 27 experiments after the
model has been trained with up to four experiments chosen by the Bayesian or
the A—optimal design criterion. We additionally averaged over ﬁve different
runs

 

 

5 .
I - Bayesian
ilk—optimal
D _
posterior
entrnpy
estiniete -5 I
5 I
-19 { .
1 2 e. 4 I

perfm'ined experimean

Fig. 4. Mean posterior distribution entropy estimates after the model has
been trained with up to four experiments chosen by the Bayesian or the A—
optimal design criterion, respectively. Five runs of the overall experiment
design exercise have been performed

trajectory variance, we should also consider the posterior size in
a fair comparison. The entropy S reﬂects the spread of a probability
distribution regardless of shape complexity. Therefore, we include
a numerical approximation of the entropy given by

A 1
Sz—ﬁezw:}lnP(6|D)%S=E[—lnP(6|D)] (15)
E S

in this analysis as a measure for the information content of the
posterior sample of the parameters. Figure 4 depicts the evolution of
the Shannon entropy estimated from the posterior samples using a
parallel multivariate kernel density estimator. Our prediction—based
Bayesian design routine achieved an average entropy estimate of
— 10 nats, which is 8 nats lower than the A—optimal experiments with
an average value of —2 nats. Our design is superior to A—optimal
design already after the second experiment.

As entropy is an abstract measure, an additional visual insight
is given by Figure 5, which shows a comparison of the marginal
posterior distributions for three parameters of one exemplary
design run. We have exemplary chosen a parameter that is expected
to be very well identiﬁable after the design exercise with four

 

 

 

 

 

 

 

.... I ,5, ,_._ Bayesian
a a [ﬁx a Elie-optima:-
e l e: l r e
e. e. l ,r’ E:
L i
—2 2 —2 2

 

e
91

Fig. 5. Estimated marginal distributions of three parameters of the posterior
sample after the model has been trained with four experiments chosen by
the Bayesian or the A—optimal design criterion. Y —axis limits denote the
boundaries of the log uniform prior

Table 1. Qualitative properties of the model parameters

 

 

Algorithm Two boundaries One boundary Prior bounded
Bayes design 5.6 :l: 0.5 4.2 :l: 0.7 2.2 :l: 0.4
A—optimal design 4.6 :l: 1.4 3.6 :l: 1 3.8 :l: 1.2

 

Average number of parameter boundaries distinct from the initially set prior boundaries
established after the fourth experiment in ﬁve runs. The categories two boundaries,
one boundary and prior bounded parameters are determined by visually examining the
marginal distributions (Figure 5).

successive experiments, one that is vaguely identiﬁable and one
that is expected to be not identiﬁable at all. Axis ranges have been
set equal to the prior ranges. We have also visually inspected all
marginal distributions of ﬁve different design runs and categorized
the established parameter boundaries in Table 1. Although this is
not an established quantitative measure as, e. g. entropy, it is a good
visualization for the differences of the two designs in the parameter
space.

In our example, the parameter 61 describes recruitment and
thereby activation of PKD by DAG The ﬂux for this reaction
depends in our model linearly on both variables. Furthermore, PKDa
and DAG are both measurable, such that it is intuitive that 61
becomes identiﬁable. In the ﬁve runs, the Bayesian design routine
established an average of one more double bounded parameters such
as 61, than A—optimal design. Double bounded parameters with tight
conﬁdence intervals are most useful to draw biological conclusions,
as they restrict model ﬂuxes. As each ﬂux stands for a different effect
(e. g. transport or activation), the relative importance of these effects
becomes comparable.

The parameter 67 describes recruitment of CERT to the TGN
through PI4P and its activation. Although CERT can be disturbed
and is also directly measurable, both are not the case for PI4P, and
so the experiments are less informative for this parameter. Still, an
average of 0.6 more one—sided parameter boundaries such as the
lower bound of 67 are identiﬁed by the proposed method. With the
same line of arguments, upper or lower bounded parameters are
useful to identify, e.g. a tendency towards higher or lower inﬂuence
of a modelled effect.

Finally, 611 describes conversion of DAG to ceramide. Although
both DAG and ceramide are observables, this parameter is hardly
identiﬁable, since the ﬂux of this reaction is small in our model and
both variables cannot be directly perturbed. Bayesian design ended
up with an average of 1.6 less of these uninformative parameters. An
example plot of all 12 marginal distributions of the parameter vector

 

i539

112 /B.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

H Weber et aI.

 

m
(J1
i
q
i
l
I

_|._m_.._|._._.._|._._~m
Amuemmumm'ﬂ
I I I I I

_.

umwhmmumma
.. .. .

Expel'il'l'ient. index i

 

 

 

0204050 0204050 0204050 {1204050 [1204060

PIG—J“ PHKIIIEI‘ CPI—{Ti DAG Cizl‘iitnitii:

Fig. 6. Repeated decision making (n=100) of the Bayesian (black) and
the A—optimal Fisher (grey) experiment design. Big spots indicate more
frequently chosen combinations of experiments and sampling times

is given in Supplementary Figure S2. Summarizing it can be said
that the Bayesian design enables a more detailed interpretation of the
model with the same amount of data measured by more efﬁciently
restricting the models parameter values.

4.3 Bayesian design makes more consistent decisions

Figures 3 and 4 indicate that the highest incremental improvement
in predictions and entropy is achieved after the second experiment.
We used this particular design step to further investigate differences
in decision making of the competing algorithms. We tested
two potential weak points of each method. Fisher design is
assumed to be highly dependent on the MLE. Therefore, we
repeated the second design step 100 times, while each time re—
estimating the MLE. Bayesian design is limited by the amount
of representative posterior trajectories that can be simulated for
decision making; therefore, we redraw 100 times the N: 1000
prediction trajectories from the posterior distribution. The statistic
for the proposal of the second experiment is depicted in Figure 6.
Trajectory—based Bayesian design prefers (beside one single outlier)
experiment 27, overexpression of all three species, while merely
altering in sampling time. A—optimal decisions are mostly bi—modal
between experiments 9 and 18, in which PI4KIIIB and CERT are
overexpressed, while about 15% of the decisions are scattered over
various experiments and time points. It is clearly visible that the
suggestions of the Bayesian design are much more consistent.

4.4 Computational effort

The main computational effort of this study stems from the sampling
of the posterior distribution, which serves as a common criterion
to compare the two different methods. This involved over a
quarter of a billion ODE integrations and a detailed Markov chain
convergence analysis. Parallel Markov chains have been merged
for each posterior distribution estimate containing at least 3.5
million sample points each. Not converged sub—chains have been
discharged. Note that experiments are typically scheduled on a
weekly basis, so that differences in computation time might not

be crucial. The posterior sampling has to be performed only after
new data is received from the wet lab. For a fair comparison of
the computational effort of the two design routines, we focus on
the number of necessary ODE integrations needed for the actual
design or proposal steps which are, respectively, used to assess
new potential experiments. The calculation of the A—optimality of a
candidate experiment with a 12 X 12 FIM (simple ﬁnite differences)
requires 48 model simulations; the Bayesian concept required 1000
simulations per prediction, rendering it 20 times more simulation
intensive than the Fisher design in this example. Still, any state of
the art Desktop PC should be able to assess 100—1000 experiments
per hour using the Bayesian method.

5 DISCUSSION AND CONCLUSION

In this work, we introduced a trajectory—based Bayesian experiment
design approach and compared this with A—optimal Fisher design
for a regulatory network of transport vesicle formation at the
trans—Golgi network in mammalian cells with 12 parameters. In
this example, which contains several sloppy parameters, Bayesian
design clearly outperformed Fisher design with respect to reducing
uncertainty in predictions and also in the parameter space.

Our results demonstrate that design criteria that rely on local
approximations of the posterior distribution are not suited in case of
sloppy or non—identiﬁable parameters. Fisher design, for example,
assumes that the posterior distribution can be approximated by a
multivariate Gaussian about its maximum. In our study, we could
further identify vulnerable dependence of FIM design decision
making to the obviously non—reliable MLE. Since the appearance
of sloppy parameters is ubiquitous for intracellular regulation
networks, we strongly encourage to use and further develop methods
that are based on output or trajectory optimization rather than
parameter identiﬁcation.

One of the reviewers brought up the important point of
model errors, which leads to further discrepancies between model
simulations and ‘real’ observations. This study uses artiﬁcial data
that are generated by the model under consideration, which has of
course the advantage that all true parameters are known and results
are controllable. However, we are aware that this simpliﬁes analyses
a lot compared with more realistic settings with real experimental
data. For example, if the model is erroneous and does not really
capture the dynamics of the underlying processes, this might lead to
likelihood functions for different datasets that are not consistent with
respect to the model and favour different regions in the parameter
space, which complicates the PE step and thus also the whole
experiment design method. This fact has for sure to be taken into
account when working with real data and can for example be
addressed by a design strategy that is optimized to discriminate
between different model hypotheses, which is one of the main issues
for our future work in this project.

When summarizing our numerical results, we became aware of
a very recent study on prediction—based experiment design (Vanlier
et al., 2012). The authors focus on posterior predictive distributions,
which assign probabilities to new simulation scenarios. Although
this is an elegant approach in general, calculating this distribution
requires the evaluation of further integrals and thus becomes rapidly
computationally expensive for high—dimensional design spaces, e.g.
if multiple candidate measurements are planned within a single
experiment.

 

i540

112 /B.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Trajectory-oriented Bayesian experiment design

 

Overall, model—based experiment design that optimizes prediction
uncertainty is a challenging new ﬁeld, which is very suitable
especially for intracellular network models with sparse data and
should further be investigated.

6 AUTHOR CONTRIBUTIONS

P.W. and A.K. developed the trajectory—based Bayesian design
algorithm. P.W. implemented the algorithm and created the results
with support from A.K. C.D. created results for the Fisher design.
P.W., A.K. and NR. developed the model and wrote the manuscript.
All authors read and approved the ﬁnal manuscript.

ACKNOWLEDGEMENT

We especially thank Prof. Dr. Monilola Olayioye and Dr. Angelika
Hausser from the Institute of Cell Biology and Immunology in
Stuttgart for their collaboration in the trans—Golgi network project.

Funding: Deutsche Forschungsgemeinschaft (DFG, GZ: RA 1840/
1 — 1) and German Research Foundation within the Cluster of
Excellence in Simulation Technology (EXC 310/1) at the University
of Stuttgart.

Conﬂict of Interest: none declared.

REFERENCES

Atkinson,A. and Donev,A. ( 1992) Optimum Experimental Designs. Oxford Statistical
Science Series, vol. 8, Oxford: Clarendon Press.

Bandara,S. et al. (2009) Optimal experimental design for parameter estimation of a cell
signaling model. PLoS Comput. Biol, 5, 61000558.

Bard,F. and Malhotra,V. (2006) The formation of TGN-to-plasma-membrane transport
carriers. Annu. Rev. Cell Dev. Biol, 22, 439—455.

Becker,E. (2008) An xbp-l dependent bottle-neck in production of igg subtype
antibodies in chemically deﬁned serum-free Chinese hamster ovary (cho) fed-batch
processes. J. Biotechnol, 135, 217—223.

Brown,K.S. et al. (2004) The statistical mechanics of complex signaling networks:
nerve growth factor signaling. Phys. Biol, 1, 184—195.

Casey,F.P. et al. (2007) Optimal experimental design in an epidermal growth factor
receptor signalling and down-regulation model. IET Syst. Biol, 1, 190—202.

Chaloner,K. and Verdinelli,l. (1995) Bayesian experimental design: a review. Stat. Sci,
10, 273—304.

Donckels,B.M. et al. (2012) Performance assessment of the anticipatory approach to
optimal experimental design for model discrimination. Chemomet. Intell. Lab. Syst,
110, 20—31.

Donckels,B.M.R. et al. (2009) A kernel-based method to determine optimal sampling
times for the simultaneous estimation of the parameters of rival mathematical
models. J. Comput. Chem., 30, 2064—2077.

Eden,E. et al. (2011) Proteome half-life dynamics in living human cells. Science, 331,
764—768.

Fugmann,T. et al. (2007) Regulation of secretory transport by protein kinase d-mediated
phosphorylation of the ceramide transfer protein. J. Cell Biol, 178, 15—22.

Gassmann,M. et al. (2009) Quantifying western blots: pitfalls of densitometry.
Electrophoresis, 30, 1845—1855.

Gutenkunst,R.N. et al. (2007) Universally sloppy parameter sensitivities in systems
biology models. PLoS Comput. Biol, 3, 1871—1878.

Haario,H. et al. (2006) Dram: efﬁcient adaptive mcmc. Stat. Comput, 16, 339—354.

Hausser,A. et al. (2005) Protein kinase D regulates vesicular transport by
phosphorylation and activation of phosphatidylinositol-4 kinase III B at the Golgi.
Nat. Cell Biol, 7, 880—886.

Klinke,D.J. (2009) An empirical bayesian approach for model-based inference of
cellular signaling networks. BMC Bioinformatics, 10, 371.

Kramer,A. and Radde,N. (2010) Towards experimental design using a Bayesian
framework for parameter identiﬁcation in dynamic intracellular network models.
Procedia Comput. Sci, 1, 1639—1647.

Kreutz,C. et al. (2007) An error model for protein quantiﬁcation. Bioinformatics, 23,
2747—2753.

Kreutz,C. (2011) Likelihood based observability analysis and conﬁdence intervals for
predictions of dynamic models. ArXiv e-prints, 1107.0013.

Li,C. et al. (2010) Biomodels database: an enhanced, curated and annotated resource
for published quantitative kinetic models. BMC Syst. Biol, 4, 92.

Pillai,R.S. et al. (2005) Inhibition of translational initiation by let-7 microrna in human
cells. Science, 309, 1573—1576.

Schmidt,H. and Jirstrand,M. (2006) Systems biology toolbox for matlab: a
computational platform for research in systems biology. Bioinformatics, 22,
5 14—5 15.

Vanlier,J. et al. (2012). A bayesian approach to targeted experiment design.
Bioinformatics, 28, 1136—1142.

Wilkinson,D.J. (2007) Bayesian methods in bioinformatics and computational systems
biology. Brief. Bioinform., 8, 109—116.

 

i541

112 /B.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

