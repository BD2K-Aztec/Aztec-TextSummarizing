Bioinformatics, 32, 2016, i37—i43
doi: 10.1093/bioinformatics/btw249
ISMB 2016

 

Novel applications of multitask learning and
multiple output regression to multiple genetic
trait prediction

Dan He1, David Kuhn2 and Laxmi Parida1

1IBM T.J. Watson Research, Yorktown Heights, NY, USA and 2USDA-ABS Subtropical Horticultural Research
Station, Miami, FL, USA

Abstract

Given a set of biallelic molecular markers, such as SNPs, with genotype values encoded numerically
on a collection of plant, animal or human samples, the goal of genetic trait prediction is to predict the
quantitative trait values by simultaneously modeling all marker effects. Genetic trait prediction is usu—
ally represented as linear regression models. In many cases, for the same set of samples and
markers, multiple traits are observed. Some of these traits might be correlated with each other.
Therefore, modeling all the multiple traits together may improve the prediction accuracy. In this work,
we view the multitrait prediction problem from a machine learning angle: as either a multitask learn—
ing problem or a multiple output regression problem, depending on whether different traits share the
same genotype matrix or not. We then adapted multitask learning algorithms and multiple output
regression algorithms to solve the multitrait prediction problem. We proposed a few strategies to im—
prove the least square error ofthe prediction from these algorithms. Our experiments show that mod—
eling multiple traits together could improve the prediction accuracy for correlated traits.

Availability and implementation: The programs we used are either public or directly from
the referred authors, such as MALSAR (http://VWNW.public.asu.edu/~jye02/Software/MALSARl)

 

package. The Avocado data set has not been published yet and is available upon request.

Contact: dhe@us.ibm.com

 

1 Introduction

Whole genome prediction of complex phenotypic traits using high-
density genotyping arrays has attracted lots of attention, as it is rele—
vant to the fields of plant and animal breeding and genetic epidemi-
ology (Cleveland et 61]., 2012; Hayes et 61]., 2009; Heffner et 61].,
2009; Jannink et 61]., 2010; Lande and Thompson, 1990; Meuwissen
et 61]., 2001; Rincent et 61]., 2012; Xu and Crouch, 2008). Given a set
of biallelic molecular markers, such as SNPs, with genotype values
typically encoded as {0, 1, 2} on a collection of plant, animal or
human samples, the goal is to predict the quantitative trait values by
simultaneously modeling all marker effects. The problem is called
genetic trait prediction or genomic selection.

More specifically, the genetic trait prediction problem is defined
as follows. Given n training samples, each with in >> n genotype
values (we use ‘feature’, ‘marker’, ‘genotype’, ‘SNP’ interchange-
ably) and a trait value, and a set of n’ test samples each with the
same set of genotype values but without trait value, the task is
to train a predictive model from the training samples to predict
the trait value, or phenotype of each test sample based on their geno—
type values. Let Y be the trait value of the training samples. The

©The Author 2016. Published by Oxford University Press.

problem is usually represented as the following linear regression
model:

Y=ﬂ0+ZﬂiXi+el (1)
i=1

where X,- is the ith genotype value, m is the total number of geno—
types and ﬂ,- is the regression coefficient for the ith genotype, e; is the
error term.

There have been lots of work on predicting genetic trait values
from genotype data, such as rrBLUP (Ridge regression BLUP)
(Meuwissen et 61]., 2001), Elastic—Net, Lasso, Ridge Regression
(Tibshirani, 1994; Shaobing Chen et 61]., 1998), Bayes A, Bayes B
(Meuwissen et 61]., 2001), Bayes C,I (Kizilkaya et 61]., 2010) and
Bayesian Lasso (Legarra et 61]., 2011; Park and Casella, 2008), as
well as other machine learning methods. Most of the work assumes
that for each set of samples there is only one trait, and therefore, a
single regression is conducted to predict the trait value. However, in
reality, it is quite often the case that we could observe and measure
multiple traits rather than one, especially for crops and animals. For
example, for plant dataset, once we obtain a fruit, we could measure
its weight, size, etc. This will give us multiple traits. Obviously some

i37

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/Iicenses/by-nc/4.0/),
which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact

journals.permissions@oup.com

1e /§JO'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTQ/ﬁdllq 11101; pepeommoq

9IOZ ‘09 lsnﬁnv uo ::

i38

D. He et al.

 

of the traits are correlated, such as weight and size. Leveraging such
correlations in the predictive model might improve the prediction
accuracy. Therefore, we call the problem of modeling multiple traits
at once as multitrait prediction problem.

Lots of work have been proposed for the multitrait prediction
problem from genotype data, such as multitrait GBLUP (Clark and
van der Werf, 2013), Multitrait BayesA (Jia and Jannink, 2012),
Bayesian multivariate antedependence model (Jiang et al., 2015).
GBLUP and multitrait BayesA are mainly based on the framework
of linear regression. The Bayesian multivariate antedependence
model considers nonstationary correlations between SNP markers
through assuming a linear relationship between the effects of adja—
cent markers. These methods are shown to have superior perform—
ance compared with single trait prediction methods.

In this work, we study the multitrait prediction problem from a
machine learning angle. We consider the multitrait prediction prob—
lem as a multitask learning problem or a multiple output regression
problem. When there are multiple sets of samples, each having a sep—
arate set of genotypes on the same set of markers as well as a corres—
ponding trait, the multitrait prediction problem can be converted
into a multitask learning problem. This is shown in Figure 1. We
can see that there are three sample sets and three different genotype
matrices. These genotype matrices, however, share the same set of
markers. Each sample set has a different trait. When there is only set
of samples and one set of genotypes but multiple traits, the problem
can be converted into a multiple output regression problem, as
shown in Figure 2. There is only one sample set and one genotype
matrix. There are three different traits. Although lots of work have
been done for the multitrait prediction problem, this is indeed the
first time the problem is modeled as a multitask learning and a mul—
tiple output regression problem.

We can see that for the multitask learning problem, if we learn
each task independently, we only use a small portion of the samples.

l K1312!!! IIIIE XE KT KB Y1 Y2 Y3

 

Earnpla Sat 1

 

 

Sample Set 2

 

 

Sample Sat 3

 

 

 

 

Fig. 1. An example of multitasking learning

I1 123(3)“ 915 IE I? IS- ‘i"! 1"? Y3

 

sample Set

 

 

 

Fig. 2. An example of multiple output regression

If we model all the tasks at the same time, we leveraged the informa—
tion from the complete set of samples and the improvement could be
significant when the number of tasks is large. In the multiple output
regression problem, although all the tasks share the same set of sam—
ples and there is no advantage on the sample size, the prediction per—
formance could still be improved by the modeling the correlations
among the tasks.

In this work, we adapt the state—of—the—art multitask learning al-
gorithms and multiple output regression problems to solve the mul—
tiple trait prediction problem. For the multiple output regression
problem, we conduct an iterative algorithm to learning the variable
one at a time with others fixed. The objective function is convex
when we only optimize one variable with others fixed, and there—
fore, efficient optimization is allowed. We observed that a direct ap—
plication of these algorithms to the multiple trait prediction problem
usually leads to poor least square error. We applied strategies such
as centering the genotype matrix to improve the prediction perform-
ance. We showed that modeling all the traits together could improve
the prediction compared with predicting each trait independently,
especially for the correlated traits.

2 Preliminaries

Given the traditional encoding of genotypes as {0, 1, 2}, lots of tech—
niques have been applied to the genetic trait prediction problem
defined in Equation (1). Consider the typical situation for linear re-
gression, where we have the training set y E R’, x E RIX", in a stand—
ard linear regression, we wish to find parameters ﬂoﬁ such that the
sum of square residuals, Ell-:1 (y,- — B0 —  [3)2, is minimized.

Many machine learning methods have been applied to the gen—
etic single trait prediction problem, such as Elastic—Net, Lasso,
Ridge Regression (Shaobing Chen et al., 1998; Tibshirani, 1994),
Bayes A, Bayes B (Meuwissen et al., 2001), Bayes C,r (Kizilkaya
et al., 2010) and Bayesian Lasso (Legarra et al., 2011; Park and
Casella, 2008). They could be applied to predict the multiple traits
where each trait is predicted independently. In this work, we
applied ridge regression (Hoerl and Kennard, 1970) for single trait
prediction, which aims to minimize the following objective
function.

I n
min Zn,- — xiTpf + 2:13;], (2)
i=1

i=1
The solution of ridge regression is given by:
[3 = (xTx + M)_1XTy (3)

which is similar to the ordinal least square solution, but with the
addition of a ‘ridge’ down the diagonal. Ridge regression has been
shown to have certain bias as —1(XTX —l— M)_1ﬂ. The unbiased ver—
sion of rrBLUP (Ridge regression BLUP) (Meuwissen et al., 2001;
Whittaker et al., 2000) is one of the most popular methods for gen—
etic trait prediction. rrBLUP simply is ridge regression with a specific
choice of i in (2). Specifically, Meuwissen et al. (2001) assumes that
the B coefficients are iid from a normal distribution such that
B,- N N (0, 0%,). Then the choice of i = 03/0"; where a? is the residual
error. In this case, the ridge regression penalized estimator is equiva—
lent to best linear unbiased predictor (BLUP) (Ruppert et al., 2003).
Many methods for multitrait prediction where all the traits are
modeled together have also been proposed, such as multitrait
GBLUP (Clark and van der Werf, 2013), Multitrait BayesA (Jia and
Jannink, 2012), Bayesian multivariate antedependence model (Jiang

1e /§JO'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(11111 11101; pepeommoq

9IOZ ‘09 lsnﬁnv uo ::

Novel applications of multitask learning

i39

 

et al., 2015). In multivariate models with m traits, marker effects on
phenotypic traits were estimated from the mixed linear model
below:

P
y = u + Zxﬂp, + e (4)
i=1

where y is a n x m matrix with n samples and m traits, a,- is a 1 X m
vector for the effects of the jth marker on all m traits which is
assumed to be normally distributed a,- N N (0, 2a,), 20,]. is m x m
variance—covariance matrix for the jth marker, e is a n x m matrix
for residual error that follows a normal distribution.

In multitrait GBLUP (Cleveland et al., 2012), unstructured covari—
ance matrix among traits was assumed and the relationship matrix
derived from SNPs were fit in ASReml (Gilmour et al., 2009). The
multitrait BayesA model (Jia and Jannink, 2012) assumes the prior of
2a,. follows a scaled inverse—Wishart distribution, which were given a
ﬂat prior and estimated from the data using the Metropolis algorithm
to sample from the joint posterior distribution. Gibbs sampling and
MCMC are applied to estimate the parameters. In the Bayesian multi-
variate antedependence model (Jiang et al., 2015), it is assumed that
the adjacent markers are correlated as below:

5- j: 1
“i—{ ’ . <5)
ti7i_1oc,-_1 +5," [=2,...,p

where ti7i_1 is the scalar antedependence parameter of or,- on oci_1.
Again, the parameters are estimated Via Gibbs sampling and MCMC.

3 Multitrait prediction

As we have discussed before, there are two types of multitrait pre—
diction problem:

° For each trait, the genotype matrix is different: the problem can
be formalized as a multitask learning problem.

° For each trait, the genotype matrix is the same: the problem can
be formalized as a multiple output regression problem.

3.1 Multitask learning

Many algorithms have been proposed (Abernethy et al., 2006, 2009;
Agarwal et al., 2010; Argyriou et al., 2007; Chen et al., 2012;
Evgeniou and Pontil, 2004; Liu et al., 2009; Zhou et al., 2011) for
the multitask learning problem. Here we mainly focused on four al—
gorithms: Cluster—based MTL (CMTL) (Zhou et al., 2011), €1—norm
regularized MTL, Egg-norm regularized MTL (Liu et al., 2009) and
Trace-norm Regularized MTL (Abernethy et al., 2009).

Lasso (Tibshirani, 1996) is a well—known method that uses the
€1—norm (or Lasso) regularizer to reduce model complexity and learn
features. It can be easily extended for single task learning to multi-
task learning. The objective function for €1-norm regularized MTL
is based on least square lasso:

t
. 2 2
manZHWiTXi_YillF+p1||W||1+pL2||WHF (6)
i=1

where X,- denotes the input matrix of the ith task, Y,- denotes the ith
trait, W,- is the coefficient matrix for task i, the regularization par-
ameter p1 controls sparsity and the optional )oL2 regularization par-
ameter controls the EZ—norm penalty. Note that both €1—norm and
EZ-norm penalties are used in Elastic Net.

Besides a simple €1—norm regularizer, we could constrain all coef—
ficient matrices to share a common set of features. This motivates

the group sparsity, i.e. the €1/ﬂz—norm, or Egg—norm, regularized
learning (Liu et al., 2009). The objective function for Egg—norm regu—
larized MTL is also based on least square lasso:

t
. 2 2
manZIIWI-sz — Yzllp + pillWll2,1 + pLZHWHF (7)
i=1

where X,- denotes the input matrix of the ith task, Y,- denotes the ith
trait, W,- is the coefficient matrix for task i, the regularization par-
ameter p1 controls sparsity and the optional )oL2 regularization par-
ameter controls the EZ—norm penalty. Notice the difference of the
objective functions in Equations (6) (||W||1) and (7) (||W||271).

We could also constrain the coefficient matrices from different
tasks to share a low—dimensional subspace, i.e. W is of low rank. By
replacing the rank of W with trace norm  |>k : Z,- 5)(W), the ob—

jective function becomes:
mimic/MW) + V||P||1

(8)
subject to: W: P—I—Q,  <2 ‘E

where the task coefficient matrices W is decomposed into two com—
ponents: a sparse part P and a low—rank part Q.

The advantage of CMTL is that prior knowledge on the cluster
structure of the traits can be embedded into the objective function.
For our multitrait problem setting, we always know what are the
traits and what they measured. Thus it is very often we know which
traits are more correlated with each other. For example, fruit weight
and fruit size are known to be highly correlated. These more corre—
lated traits should be in one cluster. By leveraging such cluster infor-
mation, we could improve the multitrait prediction algorithm. The
objective function for CMTL is based on the spectral relaxed k—
means clustering:

minWaFZFTFZIkL(W) + oc(tr(WT W) — “(FT WT WP» + [max/T W)
(9)

where k is the number of clusters and F captures the relaxed cluster
assignment information. As the above objective function is not con—
vex, a convex relaxation cCMTL is also proposed as below:

minWL(W) + p1n(1 + inﬂux/(271 + M)" WT)

. pz (10)

subject to : tr(M) : k,M g I,M E Si,” 2 _
P1

Accelerated Projected Gradient (Zhou et al., 2011) is applied to

optimize the above objective function.

3.2 Multiple output regression

Given a set of training data consisting of N samples, each sample is
associated with a genotype matrix X of D—dimension and a trait matrix
Y of C—dimension, the multioutput regression model is shown below:

Y=XB+E (11)

where B : [B1, . . . , BC] is a D x C regression coefficient matrix, each
element B,- is the vector of the regression coefficient for the jth trait.
E : [€1,...,€N]T is an N>< C matrix, where e,- : (6)-1,...,e,-c) E RC
denotes the residual errors on each trait prediction introduced by
the ith sample.

Multiple output regression has been widely used in a variety of
domains such as stock prices prediction, pollution prediction, etc. It
was first noticed by Breiman (2000) and Friedman that through uti—
lizing correlations between outputs the regression accuracy can be
improved. In general there are two types of correlations: the task

12 /§JO'Sjeu1no [p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(11111 11101; pepeommoq

9IOZ ‘09 lsnﬁnv uo ::

i40

D. He et al.

 

correlation and the noise correlation. Most of the work focus on
modeling only one type of correlation, either task correlation or
noise correlation. Some recent works (Cai et al., 2014; Rai et al.,
2012) consider both types of correlation, which are shown to
achieve better regression performance. The work of Rai et al. (2012)
and Cai et al. (2014) are essentially the same in that they both aim
to optimize the following objective function:

aigminmijyi = tr((Y — XB)o—1(Y — XB)T)
—Nlog|Q_1| + 21tr(BBT) + imam—113T ) (12)
—Dlog|2_1| + 23tr(Q_1) + 24tr(2_1)

where  denotes the determinant of a matrix.

The inverse covariance matrix 9—1 couples the correlated noise
across tags and similarly, 2‘1 obtained relationships among the
multiple tasks’ regression coefficients. Apparently, both 9—1 and
21—1 are learnt from the training data rather than pre-defined prior
knowledge. The last two terms tr(Q_1) and tr(2_1) are the regular—
izers, which impose the matrix variate Gaussian priors on both
9—1/2 and 21—1/2 to solve the overfitting issue.

The objective function in Equation (12) of the multiple output re—
gression model is not jointly convex in all variables but individually
convex in each variable while others are fixed. Therefore, in order to
optimize the objective function, an iterative algorithm is applied: (i)
Fix 9—1 and 2—1 and estimate B, (ii) Fix 9—1 and B and estimate 2—1
and (iii) Fix 2‘1 and B and estimate 9—1. The process iterates and
stops when the value of the objective function does not change or
when the number of iterations exceeds a pre—defined threshold.

As the multiple output regression model is convex in each vari—
able while others are fixed, when we optimize a variable, we can
take the derivative of the variable to estimate its optimal value. To
estimate B with fixed 9—1 and 21—1, we set the derivative of B over
the objective function as 0 and we obtain:

2xTx13o—1 + 22113 + 22232—1 = 2me—1
=> xTXB + 21139 + 22132—112 = xTY

Both works (Cai et al., 2014; Rai et al., 2012) applied the above
algorithm. However, when optimizing B, the work (Rai et al., 2012)
applies Kronecker product which generates a DC >< DC matrix
whose complexity might be high for large D and C. Then work Cai
et al. (2014) MOR improved the complexity by applying Cholesky
factorization and singular value decomposition and they showed
that the efficiency of the optimization process can be significantly
improved. Please refer to Rai et al. (2012) and Cai et al. (2014) for
the details of the two approaches.

As 21!) —1— 2221—19 is systemic and positive—definite, the
Cholesky factorization is performed on it to produce lower triangu—
lar matrix P:

2112 + 122—112 = PPT

By setting X : U1Z1V1T and P : U222 V2T be the SVD of X and
P, respectively, we obtain the following:

V1Z1UTU1Z1VTB + BU2Z2VTV2Z2UT = xTY
1 1 2 2

By setting B : V1T B U2 and S : V1T XTYU2, we could obtain B
as:

B = V1BU2T

When optimize 2‘1 with fixed 9—1 and B, we set the derivative
of 2‘1 over the objective function as 0 and we obtain:

212BTB — D2 + MC 2 0

lzBTB + W4IC

424% D >

When optimize 9—1 with fixed 2‘1 and B, we set the derivative
of 2‘1 over the objective function as 0 and we obtain:

(Y — XB)T(Y — XB) — N9 + MC 2 0

(Y — XB)T(Y — XB) + MC _1
N )

 

=>Q‘1:(

where I C is an C X C identity matrix and M‘1 denotes the inverse ma—
trix of the matrix M. The 21,22,122, are selected by cross—validation.

In Cai et al. (2014), dimensionality reduction is applied on both
feature space and target (trait) space. Feature space is the space for
all the features, which are the genotypes in our setting. Target space
is the space for all the target variables, which are the multiple gen-
etic traits in our setting. On feature space, PCA is applied to reduce
the dimensionality. On target space, a regularizer is applied to re—
duce the dimensionality. In this work, we did not conduct dimen—
sionality reduction as in the dataset we studied, the number of
features (in thousands) and the number of traits (eight) are not very
big.

Notice the MOR method without the task correlations and noise
correlations can be reduced to a standard ridge regression. We
observed that a direct application of the MOR method usually leads
to poor accuracy, as the predicted values are usually far off the true
values. In order to address this issue, we centered the input data ma—
trix X as W, where Mean(X) computes the column—wise
mean of X, Std(X) computes the column—wise standard deviation of
X. We call it Centered M OR. It turns out that the centering strategy
significantly improved the least square error of MOR.

4 Results

4.1 Simulated data

We first simulate the data using the Equation (11). Recall that in
this equation, B : [B1, . . . ,BC] is a D x C regression coefficient ma-
trix, each element B,- is the vector of the regression coefficient for the
j—th trait. In order to add task correlation among all the Bj’s, we
sample Bj’s from a standard normal distribution. Similarly, to add
noise correlation, we sample the residual errors E from a standard
normal distribution. The genotype matrix X is randomly sampled
from the values [0, 1, 2]. The traits Y are then computed as
Y : XB —I— E. We simulated four traits for 200 samples, each with
2000 markers. We repeat all the experiments 10 times and com—
puted the average performance. To evaluate the performance of the
prediction methods, we used r2 (r—square, the square of the person’s
correlation coefficient between the predicted trait values and the
real trait values, a popular metric for genomic selection. For gen-
omic selection, the r2 is almost consistent with least square error).
For r2, the larger the better.

Notice we do not compare multitask learning and multiple out-
put regression methods directly as they will be used in different scen—
arios. Multitask learning is used when we have unique set of
samples for different traits. Multiple output regression is used when
we have the same set of samples for all the traits.

4. 1. 1 Multitask learning
We randomly split the 200 samples into 4 subsets, each with 50
samples and one corresponding trait. All of these subsets share the

112 /§JO'Sjeu1no [p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(11111 11101; pepeommoq

9IOZ ‘09 lsnﬁnv uo ::

 

 

   

    
     

Novel applications of multitask learning i41
Trait 1 Trait 2 1
0.35
0.30
0.30 as
025 0'25  
g g 0.20 DE
3 020 3 Water loss
‘1’ 0.15 ‘l’ 0'15
I 0.10 m 0.10 Meﬁocarpwerght I“
0.05 0-05 Seed “112101
0.00 0.00 02
ﬁ ﬁ ﬁ ﬁ 8. ﬁ ﬁ ﬁ ﬁ 8. Seed mm
2 2 2 E 12 2 2 2 E .9
0 j a 8 0: 0 E E 8 D:
A g 4 g Seed want t.
Trait 3 Flu“ Elrctmf
- Ln:
0'4 0-25 FrUIt length '5"
93 93 0.20
g 0.3 g Frult WE'I'QI'II
w m 0.15 J”
0': 0.2 0': <6" '3‘
0.10 .57» t.
'1"
0.1 3) ﬂ. 1:? .03
0.05 «(3? ‘23“ ‘55:} we? :35:th 4.; g 4:!” 4
0.0 0.00 c? Q
: e e e 3. e e e e 3. <1 «at -1
2 2 2 E 9 2 2 2 E .9
0 j a 8 0: 0 E E 8 D:
—| E _l E
l— I—

Fig. 3. The 12 of the four multitask learning algorithms versus the single trait
ridge regression algorithm on the simulated data

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

Trait1 Trait2
05 0.5
0.4 0.4
9 9
‘3 0.3 ‘3 0-3
D' D'
"r 1’
II 0.2 a: 0.2
0.1 0.1
0.0 [I < (D (D 0.0 [I < (D (D
o 3 g a o 3 g a
(2) (B U) E (2) E’ U) I
“3 °>’. m 9
«S «S
m m
Trait3 Trait4
0-5 0.5
9 9
g 0.3 g 0.3
D' D'
"r "r
a; 0.2 a: 0.2
0.1 0.1
0.0 [I < (D (D 0.0 [I < (D (D
o a 5 a o a 5 a
2 E‘ U) I 2 E‘ U) I
O m g 0 m g
«S «S
m m

Fig. 4. The 12 of the centered MOR method, the multitrait BayesA algorithm,
the Bayesian multivariate antedependence model versus the single trait ridge
regression algorithm on the simulated data

same 2000 set of markers, with their corresponding genotype values.
We tested the performance of the four multitask learning algorithms
[Cluster—based MTL (CMTL), L1-norm regularized MTL, L2,1-
norm regularized MTL, Trace-norm Regularized MTL] against the
single trait ridge regression algorithm. Notice for the single trait al—
gorithm, for each trait, we train ridge regression only on one subset
of 50 samples. For CMTL, as we do not have specific clusters, we
just randomly group two of the traits in one cluster and the other
two in another cluster.

We show the results in Figure 3. We can see that the four multi—
task learning algorithms in general achieved much better results
than that of the single trait ridge regression. This is reasonable as the

Fig. 5. The heat map of the correlation of the eight traits

single trait ridge regression only uses 50 samples for the prediction.
The CMTL does not have any advantage over other methods as the
traits are indeed not in clusters. The trace—norm MTL achieved the
best results.

4. 1.2 Multiple output regression

Here we conducted 10—fold cross validation for each trait and we
compare the performance of the the single trait prediction method:
single trait ridge regression, the multiple output regression methods:
the centered MOR method and the state—of—the—art multitrait predic—
tion methods: the multitrait BayesA algorithm, the Bayesian multi-
variate antedependence model. We did not show the performance of
MOR here as it in general has poor performance.

The results are shown in Figure 4. We can see that the multitrait
algorithms have better performance than the single trait ridge regres—
sion does. The Bayesian multivariate antedependence model does not
outperform BayesA in that we do not insert LD in our dataset. The
centered MOR has the best performance. We also see that the multi—
trait prediction methods and multiple output regression methods
made improvements on all four traits over the single trait prediction
method. This is because all the four traits are correlated as they are
sampled from the same standard normal distribution. As we will
show later in the experiments on real data, when the traits are not cor—
related, multitrait prediction does not make obvious improvements.

4.2 Real data

Next we evaluate the performance of the multitrait prediction meth-
ods on a real plant dataset, the avocado dataset, which contains 8
traits, 160 samples and 2663 markers. The eight traits are: fruit
weight, seed weight, fruit length, fruit width, fruit diameter, number
of fruit (log), mesocarp weight and water loss percentage. From the
name of the traits, we know which traits are more correlated with
each other. We show the heat map of the correlation of these traits
in Figure 5. Notice in the Figure there are two more traits ‘seed
width’ and ‘seed length’. They are not included in the experiments.
From the heat map, we can see that the first six traits are more cor—
related with each other and the last two traits are less correlated
with the remaining traits.

1e /810'S[eu1no IpJOJXO'SOTlBIHJOJUTOTQ/ﬂClllq 11101; pepeommoq

9IOZ ‘09 lsnﬁnv uo ::

i42

D. He et al.

 

4.2.1 Multitask learning

We randomly divide the genotype matrix into eight subsets, each
with 20 samples. Notice the eight genotype matrices share the same
set of markers. For each subset, we keep only one trait for the cor—
responding samples in the subset. Therefore, we ended up with eight
datasets, each with a single trait.

For single trait prediction, to predict the j—trait, we first train a
predictive model on the jth dataset. Then we take all the other data—
sets as input and apply the predictive model on the other datasets to
predict the jth trait for them. The predictive model we used here is

 
 

 

 

   

 

 

 

fruit weight seed weight
0.35 —
0.30 —
0.30 —
0.25 —
0.25 -
a, a, 0.20—
23 0.20 — 26
3 3
O" O"
«I: <1? 0.15—
g: 0.15 - g:
OJ 0 _ 0.10 —
0.05 - 0.05 —
0.00 — 0.00 —
_l _l _l _l a) _l _l _l _l a)
l— l— l— l— a) l— l— l— l— a)
2 2 2 2 9 2 2 2 2 9
0 5 a 8 “I 0 5 a 8 “I
_l m _l g
F 1—
fruit diameter number of fruits
0.25 —
0.10 —
0.20 —
0.08 —
(1.) (1.)
26 0.15 - 26
g g 0.06 —
‘1’ ‘1’
D: D:
0'10 0.04 —
0.05 - 0.02 —
0.00 — 0.00 —
_l _l _l _l a) _l _l _l _l a)
l— l— l— l— a) l— l— l— l— a)
2 2 2 2 9 2 2 2 2 9
0 5 a 8 “I 0 5 a 8 “I
_l m _l g
F l—

 

ridge regression. For the multitrait prediction, we used four algo—
rithms: Cluster—based MTL (CMTL), L1-norm regularized MTL,
L2,1-norm regularized MTL and Trace-norm Regularized MTL.
For CMTL, we applied our prior knowledge on the structure of the
clusters, namely the traits fruit weight, fruit length, fruit width, fruit
diameter are highly correlated and they should be in one cluster.

We compare the performance of the four multitask learning al-
gorithms with the single trait prediction. We show the results in
Figure 6. We can see the obviously the multitrait prediction signifi-
cantly outperforms the single trait prediction, as the single trait

fruit width

Illlt

water loss percentage

fruit length

0.5 —

0.30—
0.4 — 025-
03 _ 0.20—

0.15—
0.2 —

0.10-
0.1 -

0.05—
0.0 — 0.00—

R—square

 
 

 

 

CMTL

L1 MTL
L21 MTL
TraceMTL
Ridge
CMTL

L1 MTL
L21 MTL
TraceMTL
Ridge

mesocarp weight

 

 

Fig. 6. The :2 of the four multitask learning algorithms versus the single trait ridge regression algorithm

fruit weight seed weight

 

 

 

R—square
R—square

 

 

 

 

 

0.35 - 030 _
0.30 -

0.25 -
0.25 -

0.20 -
0.20 -

0.15 -
0.15 -
010 _ 0.10 -
0.05 - 0.05 -
0.00 - 0.00 -

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

E 75 9 8 E if, 95’ 8
a) C ‘o a) ‘o
E >. < -- E > < ._
O 3 10/; D: O 3 10/; D:
m «1
£0 £0
fruit diameter number of fruits
0.25 — 0-10 '
0.20 — 0.08 —
9 9
g 0-15 - g 0.06 -
‘1’ ‘1’
D: D:
0.10 - 0_04 _
0.05 — 0.02 -
0.00 — 0.00 —
D: < 0 0 0: <1: a) a)
0 8 E 8‘ 0 8 E 8’
E > < ._ E > < ._
m «1
£0 £0

R—square

R—square

 

 

 

 

 
 

 

 

 

 

 

 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

0.05—
0.04—
9 0.03—
m
3
O"
‘1’
0: 0.02—
0.01—
0.00—
_l _l _l _l a) _l _l _l _l a)
l— l— l— l— a) l— l— l— l— a)
2 2 2 2 9 2 2 2 2 .9
0 5 a 8 “I 0 5 a 8 “I
_l g _l g
l— 1—
fruit length fruit width
0.35 —
0.5 —
0.30 —
0.4 — 0.25 —
93
0.3 — 8’ 0'20 '
O"
‘1’
n: 0.15 —
0.2 —
0.10 —
0.1 -
0.05 —
0.0 — 0.00 —
(1.) (1.) (1.) (1.)
g B E 8’ g § E g!
E a ‘8 E E a ‘8 E
O m g 0 m g
m «1
£0 £0
mesocarp weight water loss percentage
0.35 —
0.04 —
0.30 -
0.25 — 0.03 —
93
0.20 — g
3
I 0.02 -
0.15 — 0:
0.10 -
0.01 -
0.05 —
0.00 — 0.00 —
(1.) (1.) (1.) (1.)
g B E 8’ g § E g!
E a ‘8 E E a ‘8 E
O m g 0 m g
m «1
£0 120

Fig. 7. The 12 of the centered MOR method, the multitrait BayesA algorithm, the Bayesian multivariate antedependence model versus the single trait ridge regres-

sion algorithm

112 /810'S[eu1n0 [p.IOJXO'SOTl‘BIHJOJUTOTQ/ﬂ€13,111 11101; pepeommoq

9IOZ ‘OE lsnﬁnv uo ::

Novel applications of multitask learning

i43

 

prediction used only one—eighth of the complete data to predict each
trait. We also observed that CMTL and the TraceNormMTL
achieved better results than the other two MTL methods, as they
conducted more complicated strategies rather than simple regular-
ization. The TraceNormMTL achieved slightly better results than
CMTL, indicating that reducing the original problem into a lower
dimensional subspace is indeed an effective strategy when the
dimensionality of the original problem is high.

4.2.2 Multiple output regression

For the multiple output regression methods, as there is only one data—
set, we conducted 10—fold cross validation. We evaluated the perform—
ance of the single trait prediction method: single trait ridge regression,
the multiple output regression methods: the centered MOR method
and the state—of—the—art multitrait prediction methods: the multitrait
BayesA algorithm, the Bayesian multivariate antedependence model.
Again we do not include MOR here as it has poor performance.
Notice we do not include the multitrait GBLUP algorithm here as the
two multitrait prediction methods have been shown to have superior
performance over the multitrait GBLUP algorithm. The performance
is again evaluated by the two metrics: r2 and the least square error.

As we can see in Figure 7, both the multitrait and multiple out—
put regression methods outperform single trait ridge regression. The
centered MOR method achieved better performance than the ridge
regression does, indicating that the centering strategy is critical for
the genetic trait prediction problem. The centered MOR method
also shows competitive performance compared with the multitrait
prediction methods on most of the traits. Also we can observe that
the improvement are mainly made on the first six traits, which are
highly correlated with each other. For the last two traits, the multi—
trait prediction does not show advantages.

5 Conclusions and future work

In this work, we studied the multitrait prediction problem where the
multiple quantitative trait values of a set of samples are predicted from
their corresponding genotypes. We modeled the problem from a ma—
chine learning perspective. We considered the problem as either a mul—
titask learning problem or a multiple output regression problem. By
adapting the state—of—the—art machine learning algorithms, we showed
that the prediction accuracy can be improved by modeling all the traits
together and we also showed that the machine learning methods are in-
deed very competitive with the existing statistical methods.

We also observed that the MOR method without the task correl—
ations and noise correlations can be reduced into a standard ridge
regression. From our previous study on single genetic trait predic—
tion (Haws et al., 2015), we observed that rrBLUP (the unbiased
version of ridge regression) achieves better performance than ridge
regression does. In our future work, we would like to extend the
MOR method to take the form of rrBLUP rather than ridge regres-
sion, which might improve its performance.

Conﬂict of Interest: none declared.

References

Abernethy,]. et al. (2006) Low-rank matrix factorization with attributes.
arXiv Preprint Cs/0611124.

Abernethy,]. et al. (2009) A new approach to collaborative ﬁltering: operator
estimation with spectral regularization. ]. Mach. Learn. Res., 10, 803—826.
Agarwal,A. et al. (2010) Learning multiple tasks using manifold regulariza-

tion. Adv. Neural Inf. Process. Syst., 46—54.

Argyriou,A. et al. (2007) A spectral regularization framework for multi-task
structure learning. Adv. Neural I Process. Syst., 25—32.

Breiman,L. (2000) Randomizing outputs to increase prediction accuracy.
Mach. Learn., 40, 229—242.

Cai,H. et al. (2014) Multi-output regression with tag correlation analysis for
effective image tagging. In: Lecture Notes in Computer Science., 8422, pp.
31—46. Springer, Berlin.

Chen,]. et al. (2012) Learning incoherent sparse and low-rank patterns from
multiple tasks. ACM Trans. Knowl. Discov. Data (TKDD), 5, 22.

Clark,S.A. and van der Werf,]. (2013) Genomic best linear unbiased prediction
(gblup) for the estimation of genomic breeding values. In: Genome—Wide
Association Studies and Genomic Prediction, pp. 321—330. Springer, Berlin.

Cleveland,M.A. et al. (2012) A common dataset for genomic analysis of live-
stock populations. G3: Genes— Genomes— Genetics, 2, 429—435.

Evgeniou,T. and Pontil,M. (2004) Regularized multi—task learning. In:
Proceedings of the tenth ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 109—1 17. ACM.

Gilmour,A.R. et al. (2009) ASReml user guide release 3.0. VSN International
Ltd, Hemel Hempstead, UK.

Haws,D.C. et al. (2015) Variable-selection emerges on top in empirical com-
parison of whole-genome complex-trait prediction methods. PloS One, 10,
e0138903.

Hayes,B.]. et al. (2009) Genomic selection in dairy cattle: Progress and chal-
lenges. ]. Dairy Sci., 92, 433—443.

Heffner,E.L. et al. (2009) Genomic selection for crop improvement. Crop Sci.,
49, 1—12.

Hoerl,A.E. and Kennard,R.W. (1970) Ridge regression: biased estimation for
nonorthogonal problems. Technometrics, 12, 55—67.

Jannink,].-L. et al. (2010) Genomic selection in plant breeding: from theory to
practice. Brief. Funct. Genomics, 9, 166—177.

Jia,Y. and Jannink,].-L. (2012) Multiple-trait genomic selection methods in-
crease genetic value prediction accuracy. Genetics, 192, 1513—1522.

Jiang,]. et al. (2015) Joint prediction of multiple quantitative traits using a
bayesian multivariate antedependence model. Heredity, 115, 29—3 6.

Kizilkaya,K. et al. (2010) Genomic prediction of simulated multibreed and
purebred performance using observed ﬁfty thousand single nucleotide poly-
morphism genotypes. ]. Anim. Sci., 88, 544—55 1.

Lande,R. and Thompson,R. (1990) Efﬁciency of marker-assisted selection in
the improvement of quantitative traits. Genetics, 124, 743—75 6.

Legarra,A. et al. (2011) Improved lasso for genomic selection. Genet. Res., 93,
77.

Liu,]. et al. (2009) Multi-task feature learning via efﬁcient I 2, 1-norm mini-
mization. In: Proceedings of the twenty—ﬁfth conference on uncertainty in
artiﬁcial intelligence, pp. 339—348. AUAI Press.

Meuwissen,T.H.E. et al. (2001) Prediction of total genetic value using
genome-wide dense marker maps. Genetics, 157, 1819—1829.

Park,T. and Casella,G. (June 2008) The bayesian lasso. ]. Am. Stat. Assoc.,
103, 681—686.

Rai,P. et al. (2012) Simultaneously leveraging output and task structures for
multiple-output regression. Adv. Neural Inf. Process. Syst., 3185—3193.

Rincent,R. et al. (2012) Maximizing the reliability of genomic selection by opti-
mizing the calibration set of reference individuals: Comparison of methods in
two diverse groups of maize inbreds (zea mays 1.). Genetics, 192, 715—728.

Ruppert,D. et al. (2003) Semiparametric Regression. In: Cambridge Series in
Statistical and Probabilistic Mathematics. Cambridge University Press, New
York, NY.

Shaobing Chen,S. et al. (1998) Atomic decomposition by basis pursuit. SIAM
]. Sci. Comput., 20, 33—61.

Tibshirani,R. (1994) Regression shrinkage and selection via the lasso. ]. R.
Stat. Soc. Ser. B, 58, 267—288.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. ]. R.
Stat. Soc. Ser. B (Methodological), 267—288. p

Whittaker,].C. et al. (2000) Marker-assisted selection using ridge regression.
Genet. Res., 75, 249—252.

Xu,Y. and Crouch,].H. (2008) Marker-assisted selection in plant breeding:
from publications to practice. Crop Sci., 48, 391—407.

Zhou,]. et al. (201 1) Clustered multi-task learning via alternating structure op-
timization. Adv. Neural Inf. Process. Syst., 702—710.

112 /810'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(11111 1110131 popeoprmoq

9IOZ ‘09 lsnﬁnv uo ::

