ORIGINAL PAPER

Vol. 26 no. 10 2010, pages 1348—1356
doi: 10. 1093/bioinformatics/btq 140

 

Data and text mining

Advance Access publication April 7, 2010

A CROC stronger than ROC: measuring, visualizing and

optimizing early retrieval

8. Joshua Swamidassl’2’3, Chlo —Agathe Azencott1’2, Kenny Daily“2 and

Pierre Baldi1’2’4’*

1Division of Laboratory and Genomic Medicine, Department of Pathology and Immunology, Washington University,
St. Louis, MO 63110, 2Institute for Genomics and Bioinformatics, University of California, Irvine, CA 92697,
3Department of Immunology and Pathology, Washington University, Saint Louis, MO 63110

and 4Department of Biological Chemistry, University of California, Irvine, CA 92697, USA

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: The performance of classifiers is often assessed using
Receiver Operating Characteristic ROC [or (AC) accumulation curve
or enrichment curve] curves and the corresponding areas under the
curves (AUCs). However, in many fundamental problems ranging
from information retrieval to drug discovery, only the very top of the
ranked list of predictions is of any interest and R003 and AUCs are
not very useful. New metrics, visualizations and optimization tools
are needed to address this ‘early retrieval’ problem.

Results: To address the early retrieval problem, we develop the
general concentrated ROC (CROC) framework. In this framework,
any relevant portion of the ROC (or AC) curve is magnified smoothly
by an appropriate continuous transformation of the coordinates
with a corresponding magnification factor. Appropriate families of
magnification functions confined to the unit square are derived
and their properties are analyzed together with the resulting CROC
curves. The area under the CROC curve (AUC[CROC]) can be used to
assess early retrieval. The general framework is demonstrated on a
drug discovery problem and used to discriminate more accurately
the early retrieval performance of five different predictors. From
this framework, we propose a novel metric and visualization—the
CROC(exp), an exponential transform of the ROC curve—as an
alternative to other methods. The CROC(exp) provides a principled,
flexible and effective way for measuring and visualizing early
retrieval performance with excellent statistical power. Corresponding
methods for optimizing early retrieval are also described in the
Appendix.

Availability: Datasets are publicly available. Python code and
command-line utilities implementing CROC curves and metrics are
available at http://pypi.python.org/pypi/CROC/

Contact: pfbaldi@ics.uci.edu

Received on July 20, 2009; revised on March 18, 2010; accepted on
March 30, 2010

1 INTRODUCTION

One of the most widely used tools to assess the performance of a
classiﬁcation or ranking algorithm in statistics and machine learning
is the Receiver Operating Characteristic (ROC) curve, plotting true

 

*To whom correspondence should be addressed.

versus false positive rate, together with the corresponding area
under the ROC curve (AUC[ROC]) metric. However, in many
applications, ranging from information retrieval to drug discovery,
the ROC curve and the area under the curve (AUC) metric are
not very useful. This is because the total number of objects to be
classiﬁed or ranked, such as web pages or chemical molecules, tends
to be very large relative to the number of objects toward the top of the
list that are practically useful or testable due to, for instance, ﬁnancial
constraints. Speciﬁcally, consider a typical drug discovery situation
where one is interested in discovering molecules that may be ‘active’
among a library of 1000 000 molecules by computational Virtual
screening methods, such as docking or similarity search. Depending
on the ﬁnancial conditions and the details of the corresponding
experimental setup, experimentalists may be able to test in the
laboratory, for instance, only the top 1000 hits on the list. In
such conditions, the majority of the ROC curve is without much
relevance and the AUC is useless. Only the early portion of the
ROC curve is relevant. Furthermore, the precise ranking of the
molecules, particularly for the bottom 999 000 molecules, is also
of very minor interest. Similar observations can be made in many
other areas, ranging from fraud detection to web page retrieval. What
one is really interested in across all these cases is the notion of early
enrichment/recognition/retrieval, having as many true positives as
possible within the list of top hits.

To further drive this point, consider the following three cases
from Truchon and Bayly (2007) corresponding to an algorithm that
either: (1) ranks half the positive candidates at the top of the list and
half at the bottom; (2) distributes the positive candidates uniformly
throughout the list; or (3) ranks all the positive candidates exactly in
the middle of the list. All three cases yield an AUC of 0.5 although,
if only the top few hits can be experimentally tested, Case 1 is
clearly better than Case 2 which, in turn, is better than Case 3. Good
early recognition metrics and Visualization tools ought to easily
discriminate between these cases and rank them appropriately.

Several metrics have been suggested in the literature to address the
early retrieval problem, but none seems entirely satisfactory. Some
metrics lack smoothness and require setting arbitrary thresholds,
including looking at the derivative of the smoothed ROC curve at
or near the origin, or looking at the ROC curve and its area over
the interval [0,t] for some threshold t (e.g. t=0.05). However,
as we shall see, methods with hard threshold cutoffs tend to be
less statistically powerful than methods without threshold cutoffs,
because they discard meaningful differences in the performance of

 

1348 © The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org

112 Bro'SIBulnoprOJxo'soi1cu110juioiq//:d11q IIIOJJ pepcolumoq

9IOZ ‘ISlsnﬁnV uo ::

A CROC stronger than ROC

 

the classiﬁers just beyond the cutoff threshold. Other interesting
metrics that explicitly try to avoid setting an arbitrary threshold
(Clark and Webster-Clark, 2008; Sheridan et al., 2001; Truchon and
Bayly, 2007) have other limitations, such as being untunable, lacking
good Visualization or generality, or requiring the non—principled
choice of a transformation (see Section 6).

In this article, the early retrieval problem is addressed by
introducing and studying a general framework—the concentrated
ROC (CROC) framework—for both measuring and Visualizing early
retrieval performance in a principled way that is more suitable
than the ROC curve and the AUC measure. The overall approach
is demonstrated on a drug discovery problem. Finally, CROC is
discussed in relation to other metrics in the literature and shown
to provide a general unifying framework. Methods for maximizing
early retrieval performance are described in the Appendix A.

2 THE CROC FRAMEWORK
2.1 The ROC

Classiﬁcation (0r ranking) performance can be assessed using single-
threshold metrics. For a given threshold, the number of true positives (TP),
true negatives (TN), false positives (FF) and false negatives (FN) can be
used to compute quantities such as sensitivity, speciﬁcity, precision, recall
and accuracy (e.g. Baldi et al., 2000). Measuring performance at a single
threshold, however, is somewhat arbitrary and unsatisfactory. To obviate this
problem and capture performance across multiple thresholds, it is standard
to use ROC or AC (accumulation curve or enrichment curve) curves and
the area under these curves. The ROC curve plots the TP rate (TPR), as a
function of the FP rate (FPR). The AC plots the TP rate as a function of
the FDP, the fraction of the data classiﬁed as positive at a given threshold,
and is occasionally used to assess Virtual screening methods (Seifert, 2006).
The ROC curve and AC are related by a linear transformation of the x-
axis. The area under the ROC curve (AUC[ROC]) or area under the AC
(AUC[AC]) can be used to quantify global performance. The AUC[AC] is
a linear transform of the AUC[ROC] and the two metrics are approximately
equivalent as the size of the dataset increases (Truchon and Bayly, 2007).

Within the ROC or AC framework, a classiﬁer can be assessed by
comparing its performance to the performance of a random classiﬁer, or
to the best possible and worst possible classiﬁers. The random classiﬁer
corresponds to an algorithm that randomly and uniformly distributes all
the positive candidates throughout its prediction-sorted list. This is exactly
equivalent to using a random number generator, uniform in [0, 1], to produce
class membership probabilities. For both the ROC and AC plots, averaging
the performance curves of a large number of random trials of a random
classiﬁer constructed in this manner yields a straight line from the origin
to the point (1, 1), with an area 0.5. Classiﬁers worse than random,
therefore, yield an AUC[ROC] <0.5. Furthermore, the variance of the random
AUC[ROC] 0r AUC[AC] can be estimated analytically or by Monte Carlo
simulations and used, for instance, to derive a Z-score for the area. The best
and worst classiﬁers assume that all the positive candidates are ranked at the
top or bottom, respectively, of the prediction sorted list. These also provide
a useful baseline, especially for Visualization of the AC curve by providing
extreme curve boundaries. Masking out the portion of the plot above the
best possible curve and below the worst possible curve highlights the region
within which the AC curve is conﬁned (Fig. 1).

However, despite their usefulness, the ROC and AC curves and their AUCs
measure and Visualize classiﬁcation performance uniformly across the entire
data, and therefore are poorly suited to measure and Visualize early retrieval
performance. Thus, to address this shortcoming, we next introduce the CROC
framework using ROC curves as the starting point, but similar ideas apply
immediately to AC curves.

(b)

(3) AC

    

TPR

 

0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0
FDP FPR
pAC pROC

TPR

 

1.2 —0.8 —0.4 0 1.2 —0.8 —0.4 0
p(FDP) p(FPR)
CAC(log) CROC(log)

TPR

 

0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0
f (FDP) f(FPR)
CAC (power) CROC(power

TPR

 

0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0
f(FDP) f(FPR)

CAC (exp) CROC(exp)

TPR

 

0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0
f(FDP) f(FPR)

Fig. 1. Different performance curves for a dataset with 10 instances and
positives ranked 1, 2, 4, 5 and 8 by a hypothetical prediction algorithm.
(a) AC curves. (b) ROC curves. pAC and pROC use a logarithmic
transformation described in Clark and Webster-Clark (2008) (see Section
6). For each row from the top, the x-coordinates are: (i) untransformed;
(ii) transformed by logarithmic equation f (x)=log10(max[x,0.5 /N]); (iii)
transformed by logarithmic equation f(x)=log(1+ax)/log(1+a); (iv)
transformed by power equation f(x)=x[1/(1+°‘)]; and (V) transformed
by exponential equation f(x)=(1—e_°‘x)/(1—e_°‘). a is the global
magniﬁcation factor and N the total number of examples. a is chosen so
that f (0.1) 20.5. For AC curves, areas above the best and below the worst
possible curves are shaded. Dotted lines represent the performance of the
random classiﬁer.

2.2 The CROC

Here, we propose a different approach whereby any portion of the ROC
curve of interest is magniﬁed smoothly using an appropriate continuous and

 

1 349

112 Bro'SIBumoprOJXO'sopcuimjuroiq/ﬁdnq IIIOJJ pepcolumoq

9IOZ ‘ISlsnﬁnV uo ::

S.J.Swamidass et al.

 

monotone (hence one to one) function f from [0, 1] to [0, 1], satisfying f (0) =
0 and f (1): 1. Since this general approach tends to concentrate resources
on a particular portion of the ROC curve, we call it the Concentrated ROC
(CROC) approach [and concentrated AC (CAC)]. The approach can be used
to magnify the x- axis or the y-axis, or both. Here, we focus primarily on
magniﬁcation along the x-axis, since this is the most critical in early retrieval,
but the same ideas can be applied to the y-axis if necessary. Since we are
interested in the early portion of the ROC curve, we need a function that
expands the early part of the [0,1] interval, and contracts the latter part. This
is easily achieved using a function f that is concave down ((1), i.e with a
negative second derivative in the differentiable case. Many such functions
are possible and it is reasonable to further require that such functions have
a simple expression, with at most one parameter to control the overall level
of magniﬁcation. Examples of natural choices for the function f, using
exponential, power, or logarithmic representations include

1— e-ax 1 log(1+orx)
: /<a+1) =
l—e—O‘ ’ﬂx) x ’ “)0 log(1+or)’

where or > 0 is the global magniﬁcation factor. The logarithmic
transformation f (x): [log2(1 +x)]1/ (“14) is also possible but yields curves
that are fairly similar to the power transformation above, as can be seen
with a Taylor expansion. Of course, the common technique of displaying the
initial portion of the ROC curve is equivalent to using the transform function

f(x)=min[x(1+or),1]=min[x/t,1], (2)

where t=1/(1+or) is the hard threshold cutoff after which the ranks of
positive instances are ignored. For all these transformations, at a given
point x, the derivative f ’ (x) measures the local degree of magniﬁcation: a
small interval of length dx near x is transformed into an interval of length
f ’ (x)dx. If f ’ (x) > 1, the region around x is being stretched. If f ’ (x) < 1, the
region around x is being contracted. Thus, the functions under consideration
magnify the early part of the x-axis until the point where f ’ (x)=1 where
there is no magniﬁcation. Past this point, the functions contract the x-axis.
In contrast, the threshold cutoff function of Equation (2) corresponds to a
constant magniﬁcation factor of 1 / t over [0, t].

 

f(x) = (1)

2.3 Choice of magniﬁcation curve and parameters

The choice of magniﬁcation curve f and magniﬁcation parameter or can be
done using additional considerations, such as ‘user behavior’. For instance,
in a web page retrieval application, one may know on average how often
the ﬁrst, second, third, ...records are clicked. The corresponding decreasing
curve of how relevant each rank is provides valuable information for
the range and magniﬁcation levels required. While it is possible to use
the empirical relevance levels recorded in a particular application, for a
more principled approach here we can imagine that this relevance g(r/N)
decays exponentially or as a power law, as a function of the rank r (the
scaling factor N is the total number of examples). Thus g(r/N) = Ce‘ﬂr/N
or g(r/N)=C/(r/N)ﬂ+1, with ,8>0. It is reasonable to require the local
magniﬁcation factor to be proportional to the corresponding relevance so that

f ’(x) =Dg(x) (3)

for some proportionality constant D, so that f (x) =DG(x) +K , where G is a
primitive of g and K is another constant. In the exponential and power-
law cases, Equation (3) can be solved exactly taking into account the
boundary values f (0) = 0 and f (1) = 1. When g decays exponentially, a simple

 

calculation yields precisely the magniﬁcation function f (x): 11:88:? thus
052,8. When g decays like a power law, one obtains precisely f (x)=x_*3
thus 04:,8— 1. Thus, in short, the magniﬁcation function can be derived
precisely from the relevance function.

In the application to be considered, an exponentially decaying relevance
ﬁts well with the drug discovery setting. Thus, in the rest of the paper, for both
theoretical and practical reasons we use the exponential transformation f =
(1—e_“x)/(1—e_°‘) with magniﬁcation factor or. The same ideas, however,
can be applied immediately with any other transformation.

For setting the parameter or, several strategies can be used. Solving f ’ (x) =
1 yields the point x1 where the local magniﬁcation is 1. For the exponential
transformation, this gives

x1:_$log(1—e_°‘) N log(or). (4)

O! O!

the approximation being valid for large values of or. Alternatively, one could
choose or to sensibly map a particular value of x to another one [6. g. f (0. 1) =
0.5], or to match other particular conventions (e. g. or = 20 in the exponential
case corresponds roughly to 8% enrichment).

2.4 Area under the CROC curve (AUC[CROC]) and
random classiﬁers

The early recognition performance of a classiﬁer can be measured by the
AUC[CROC]. This area depends on the magniﬁcation factor or, and therefore
both the area and or must be reported. Under smoothness assumptions, it
can be shown by Taylor expanding the ROC function, that in the limit of
very large or the AUC[CROC] goes to 0 like ROC’(0)/or, where ROC’(0)
represents the derivative of the ROC curve at the origin. Note that if both the
x and y axes are magniﬁed with the same transform using the same or, then
the limit is ROC’(0). Thus, asymptotically the AUC[CROC] depends on the
tangent of the ROC curve at the origin, although in practice CROC curves
are interesting primarily for ﬁnite values of or (see Section 5).

As for the AUC[ROC], the AUC[CROC] can also be compared to the
same area computed for a random classiﬁer. The ROC curve of a completely
random classiﬁer is the straight diagonal line y=x between the points (0,
0) and (1, 1). The CROC curve of a random classiﬁer is easily obtained
by applying the magniﬁcation function f to this line. In the case of the
exponential transform, the equation of the corresponding curve is given by

log[1—x(1—e_°‘)]

_ (5)
a
This curve is approximately a line y%x(1—e_°‘)/or when or is small. But
otherwise the curve is far from being a line (see Figs 1 and 2). The AUC[ROC]
of a random classiﬁer is 0.5. The AUC[CROCLand of a random classiﬁer is

given by

/1 log[1—x(1—e_°‘)]_—ore_°‘—e_°‘+1_1 rm
0

 

or 05(1—e—0‘) _or l—e—O‘

 

(6)

and so it behaves like 1 /or for large values of the magniﬁcation factor or.

2.5 Assessing signiﬁcance

Assessing the statistical signiﬁcance of the difference in performance
between two classiﬁers is a fundamentally more important and general task
than assessing the signiﬁcance between a classiﬁer’s performance and a
random classiﬁer. In most interesting settings, several classiﬁers are available
that clearly perform better than random. The differences in performance
between these classiﬁers, however, can be more subtle and require statistical
assessments. In simple terms, how signiﬁcant is a difference in AUC[ROC]
for classiﬁcation or AUC[CROC] for early enrichment between two different
algorithms? We propose and compare six ways of assessing the signiﬁcance
of the difference between two early retrieval performances: (i) an unpaired
permutation test; (ii) a paired permutation test; (iii) an unpaired t-test;
(iv) a paired t-test; (V) an unpaired Wilcoxon test; and (Vi) a paired
Wilcoxon test.

The unpaired permutation test follows from the observation that the
performance of two algorithms being compared is exactly deﬁned by the
size N of the entire dataset and the exact ranks assigned by each algorithm
to the positive instances in this dataset (Zhao et al., 2009). Next, the
ranks of the positive instances from both algorithms are pooled together
and randomly partitioned into equally sized sets of ranks. Once the ranks
are sampled in this way, the difference in performance between these two
randomly derived methods is computed using, for instance, AUC[ROC] or

 

1 350

112 Bro's1120mofp101xo'sor112u11010101q”:d11q 111011 pepaolumoq

9IOZ ‘ISlsnﬁnV uo ::

A CROC stronger than ROC

 

1.0

 

 

 

 

A -vy55-5'5‘64 —
’15-‘THTJ’T- I
g .2117 I "
//:.  'Fﬂ.
/,. ,.
I, 
#4.}!
to I' "
o [..I
’5
m -.'
n. .'
E n .'
.'
'I
<1 .-
o g,
E
I
1
l
1
3 ' —RANDOM
— — IRV
—- SVM
MAX—SIM
—kNN
C
d I | | I I
oo 02 04 06 08 10
FPR
Q
C ‘— — RANDOM

TPR

 

 

 

f(FPR)

 

TPR

TPR

1.0

 

— RANDOM I
---— IRV '
——- SVM 4.;
---— MAX—SIM i
kNN

 

 

 

 

f(FPR)

 

 

 

 

 

f(FPR)

Fig. 2. 10-fold cross-validated performance of the MAX-SIM, kNN, SVM and IRV algorithms on the HIV data displayed using: (a) standard ROC curves;
(b) CROC curves with a = 7 [f(0.1) 20.5]; (c) CROC curves with a: 14 [f(0.05) 20.5]; and (d) CROC curves with a = 80 [f(0.0086) 20.5], displaying the

ﬁrst 1000 hits for the IRV on the ﬁrst half of the curve.

AUC[CROC]. Several samples of this difference in performance can be
drawn by repeatedly partitioning the ranks and compared to the observed
value of interest. Signiﬁcance can be assessed by estimating a P-Value
corresponding to the percentage of times that the sampled differences in
performance are greater than the observed difference. Enough samples must
be drawn to establish the desired level of conﬁdence in the P-Value. For
example, if the performance difference between algorithms A and B is 0.13,
then this difference would be considered signiﬁcant at a P: 0.05 level if at
most 50 out of 1000 sampled performance differences are >0.13.

The paired permutation test is novel in this context and computed in
a nearly identical manner. The only difference is the way the ranks are
partitioned. For the paired permutation test, the ranks are partitioned so as
to ensure that each instance’s rank is included exactly one time in each
partition. This is equivalent to ﬂipping an unbiased coin once for each
instance in the dataset. If the coin comes up heads, the rank of this instance
in the ﬁrst algorithm’s list is put in the ﬁrst partition and the corresponding
rank produced by the second algorithm is put in the second partition. If the
coin comes up tails, these ranks are placed in the opposite partitions. Once
the ranks have been partitioned, the Monte Carlo procedure is repeated a
sufﬁcient number of times to determine a corresponding P-Value.

The permutation tests we have described make few assumptions about
the data but do require substantial amounts of computations, especially for
the reliable estimation of small P-Values. A faster approach is obtained by
performing the appropriate unpaired and paired t-tests. These t-tests rely
on normality assumptions or approximations, which may be more or less
accurate, and therefore the t-tests can be expected to be less precise than
extensive permutation testing.

The t-tests are derived by observing that computing the AUC[CROC] is
equivalent to computing the mean of the transformed FPRs of the positive
instances. For example, if a method ranks ﬁve positive instances at ranks
{1, 2,4,5 ,7} in a list of 10 total instances, this corresponds to FPRs of
{0,0,0.2,0.2,0.4} for each positive instance, and to transformed FPRs of
{f (0), f (0), f (0.2), f (0.2), f (0.4)}. It can easily be seen that the AUC[CROC]
is exactly the mean of {1—f(0),1—f(0),1—f(0.2),1—f(0.2),1—f(0.4)}.
From this observation, one can compute a t-test between the population of
transformed FPRs produced by one classiﬁer against the similar population
produced by another classiﬁer. This approximates the results of the unpaired
permutation test. Similarly, a paired t-test between the two populations
approximates the results of the paired permutation test. An even faster
approximation for the unpaired case is obtained by applying the t-test using
the mean and SD of each classiﬁer’s performance derived by cross-validation
methods.

A parallel derivation shows that computing the AUC[CAC] is equivalent
to computing the mean of the transformed ranks. Using the same example,
this would correspond to computing the mean of {1—f(0.1),1—f(0.2),1—
f(0.4),1—f(0.5),1—f(0.7)}. From this observation, it is easy to see how
a t-test approximation of the permutation test can be constructed for the
AUC[CAC] as well.

The accuracy of the t-test approximations depend on the normality of the
distribution of the transformed ranks. Clearly, the transformed ranks can be
very skewed, and therefore Violate the t-tests’ normality assumptions. This
motivates the last two signiﬁcance tests implemented here, which use either
a paired or unpaired Wilcoxon test on the same population of transformed
ranks used for the t-tests. The Wilcoxon tests are more robust to outliers

 

1351

112 Bro's1120mofp101xo'sor112u11010101q”:d11q 111011 pepaolumoq

9IOZ ‘ISlsnﬁnV uo ::

S.J.Swamidass et al.

 

and do not make normality assumptions; however, they are not designed to
analytically approximate the permutation tests.

3 DRUG DISCOVERY DATA, REPRESENTATIONS,
AND SIMILARITY MEASURES

Drug discovery data: we use the benchmark dataset associated with the Drug
Therapeutics Program (DTP) AIDS Antiviral Screen made available by the
National Cancer Institute (NCI) (ht tp: / / dtp . nci . nih . gov/ docs /
aids / aids_data . html). The set contains assay results for 42 678
chemicals experimentally tested for their ability to inhibit the replication
of the human immunodeﬁciency virus in vitro. Initially, the data is divided
into three classes: inactive, active and moderately active. In what follows,
we combine the active and moderately active classes into a single ‘active’
class containing about 3.5% of the data. Thus, the ﬁnal dataset contains
1503 active molecules and 41 175 inactive molecules. All performance
metrics are computed using 10-fold cross-validation experiments.
Chemical representations: molecules are described by their labeled
molecular graph, where the vertices correspond to the atoms of the molecule,
labeled by the atom type [6. g. carbon (C), nitrogen (N) and oxygen (0)], and
the edges correspond to the bonds connecting these atoms together, labeled by
the bond type (e.g. single and double). In chemoinformatics, these variable-
size molecular graphs are routinely converted into long, ﬁxed-size, binary
ﬁngerprint vectors. Each bit in a ﬁngerprint representation corresponds to the
presence or absence of a particular feature in the molecular graph. Typical
sets of features used in the literature (Hassan et al., 2006; Leach and Gillet,
2005; Swamidass and Baldi, 2007) are labeled paths up to a certain length, or
labeled trees up to a certain depth. While the methods to be presented can be
applied with any set of features, here we use circular substructures (Hassan
et al., 2006), corresponding to labeled trees of depth up to 2. Each vertex is
labeled by the atomic symbol of the corresponding atom together with the
number of non-hydrogen atoms to which it is bonded (e.g. C3 for a carbon
with three non-hydrogen atoms attached), and each bond is labeled according
to its bond type (single, double, triple or aromatic). We choose this particular
labeling scheme and set of features because the resulting representations
seem to yield reasonable performance based on experiments performed using
other datasets (not reported).
Chemical similarity measures: although there are several possible ways of
deﬁning similarity between ﬁngerprint vectors (Holliday et al., 2002), by
far the most widely used approach is the well-known Jaccard—Tanimoto
similarity measure (Leach and Gillet, 2005). If A and I} are the ﬁngerprint
vectors representing molecules A and B, the Jaccard—Tanimoto similarity
between A and B is deﬁned by
s A B s 21’ 73 NIB 7

( , )— ( , )— m, ( )
where A ()3 is the number of 1 bits in the intersection A AND E, and A UB is
the number of 1 bits that appear in the union A OR  It is well known that
the J accard—Tanimoto similarity measure satisﬁes the Mercer’s conditions
and, hence, yields a Mercer kernel (Swamidass et al., 2005a).

4 CLASSIFIERS

In the experiments, we use the CROC framework to compare the early
retrieval performance of ﬁve different classiﬁers: RANDOM, MAX-SIM,
k-Nearest—Neighbors (kNN), support vector machine (SVM) and Inﬂuence
Relevance Voter (IRV). Although the details of the algorithms are not
relevant to this study, we describe them brieﬂy for completeness. RANDOM
corresponds to a random classiﬁer, which assigns a random probability
chosen uniformly over [0,1] to each example. The maximum-similarity
algorithm MAX-SIM (Hert et al., 2004, 2005) is one of the simplest methods
for virtual high-throughput screening by which a molecule is scored by its
highest (Tanimoto) similarity to a known active molecule. Formally, if all
known active compounds are denoted by {X1, ...,Xp}, the score of a new

molecule X is given by
P
z(X)=maIXS(X,Xi)- (8)
l:

MAX-SIM has been well studied and is straightforward to implement.
However, it is a rather unreﬁned method, which takes very little information
into account ignoring, for instance, all information about known inactive
molecules. A slightly more reﬁned method is the k-nearest—neighbor
algorithm, where one scores a molecule using the fraction of its k-nearest—
neighbors that are active. For the SVM algorithm, it is well known that the
Tanimoto similarity satisﬁes Mercer’s condition (Swamidass et al., 2005a),
and therefore it deﬁnes a kernel that can be used in an SVM for classiﬁcation
(see Azencott et al., 2007; Mahé et al., 2006; Swamidass et al., 2005b, for
other related kernels). Finally, we use the IRV algorithm (Swamidass et al.,
2009), which can be viewed as a reﬁnement of kNN. At a high-level, the
IRV is deﬁned by a preprocessing step, during which all the neighbors—as
deﬁned by the Tanimoto similarity metric—of a test molecule are identiﬁed,
and a processing step during which information from each neighbor is fed into
a neural network to produce a prediction. The prediction is computed from
the ‘inﬂuence’ of each neighbor, which in turn is computed as the product
of each neighbor’s ‘relevance’, encoding how much each neighbor should
affect the prediction, and ‘Vote’, encoding the direction towards which the
prediction should be pushed. The output probability of membership in the
active class is computed as

k
z<X>=<r<wZ+ZIi<X>> (9)

i=1

where X is the test molecule, i ranges from 1 to k over all the k-nearest—
neighbors, I,- the ‘inﬂuence’ of the ith neighbor on the output, wZ the bias of
the output node and o(x) the logistic function 1/(1+e_x). These inﬂuences
indicate exactly how much, and in which direction, each training example
contributes to the prediction and can be used to interpret each ﬁnal prediction.
The inﬂuence of the ith node is deﬁned multiplicatively by I,- =R,-(X )V, (X),
where R,(X) is the relevance and V,(X) is the vote of the ith neighbor. Here
the relevance is deﬁned by

Ri(X)=0(Wy+wSSi(X)+Wrri(X)), (10)

where s,- is the similarity S(X,N,-) of the ith closest neighbor to the test
compound, r,- the rank of the ith neighbor in the similarity-sorted list of
neighbors, ws and w, parameters tuning the importance of different inputs,
and wy the bias of this logistic function. For this study, we deﬁne the vote
by: V,- =w0 if c,- :0 and V,- =w1 if c,=1, where wo is the weight associated
with inactive neighbors, W1 the weight associated with active neighbors
and cie{0,1} the class of the ith neighbor. The IRV is trained by gradient
descent on the relative entropy (or negative log-likelihood). Thus, this version
of the IRV has only six tunable parameters: three parameters w, w, and
wy shared across all k neighbors, and three additional parameters, W1, wo,
and wz—making overﬁtting unlikely. In the experiments, the neighborhood
parameter k is set to 20 for both the kNN and IRV algorithms.

5 RESULTS

5.1 CROC metric and visualization

First, the ROC and CROC frameworks are compared by examining
how they assess the early retrieval performance of the classiﬁers.
Figure 2a shows the ROC curves of the various classiﬁers on the
HIV data. It is virtually impossible to disentangle their performances
in terms of early recognition using the ROC curves. In contrast,
Figure 2b, c and (1 obtained by using exponential CROC curves with
different global magniﬁcation factors a, show immediately that the
CROC framework disambiguates the early retrieval performance

 

1 352

112 Bro's1120mofp101xo'soi1eu11010101q//:d11q 111011 papeolumoq

9IOZ ‘ISlsnﬁnV uo ::

A CROC stronger than ROC

 

Table 1. AUC[ROC] and AUC[CROC] achieved by the ﬁve classiﬁers obtained using 10—fold cross-validations on the HIV data

 

 

 

AUC[ROC] AUC[CROC]
or = 7 or = 14 or = 80
RANDOM 0.500 0.142 0.071 0.013
MAXSIM 0.806 (:I: 0.002) 0.592 (:I: 0.002) 0.497 (:I: 0.003) 0.237 (:I: 0.004)
kNN 0.742 (:I: 0.003) 0.638 (:I: 0.004) 0.559 (:I: 0.005) 0.365 (:I: 0.005)
SVM 0.852 (:I: 0.004) 0.644 (:I: 0.003) 0.557 (:I: 0.002) 0.310 (:I: 0.006)
IRV 0.845 (:I: 0.002) 0.656 (:I: 0.003) 0.584 (:I: 0.003) 0.400 (:I: 0.004)

 

Best performances are in bold and second-best performances are in italics. SDs are in parentheses.

Table 2. Estimated P-values for the difference in performance for SVM versus IRV and kNN versus IRV

 

 

 

 

 

Permutation t-test Wilcoxon
unpaired paired unpaired paired unpaired paired
SVM versus IRV
AUC[ROC] 0.094 0.008 0.220 0.042 0.210 0. 028
AUC [AC] 0.096 0.008 0.235 0.045 0.396 0.103
AUC[pROC] 0.016 0.019 0.090 0.086 0.028 0.085
AUC[pAC] 0. 01 1 0.008 0.437 0.054 0.395 0.067
AUC[ROC, FP = 50] 0.059 0.051 0.116 0. 01 7 0.202 0.060
AUC[ROC, t = 0.0086] 0.189 0.148 0.149 0.005 0.263 0.053
AUC[ROC, t= 0.1] 0.012 0.150 0.432 0.113 0.255 0.059
AUC[ROC, t = 0.5] 0.092 0.248 0.395 0.179 0.248 0.040
AUC[CAC, a = 7] 0.003 0.003 0.450 0.050 0.396 0.110
AUC[CAC, or = 14] 0.001 0.003 0.457 0.034 0.394 0.064
AUC[CAC, a = 80] 0.000 0.008 0.580 0.019 0.447 0.026
AUC[CROC, a = 7] 0.001 0.002 0.400 0.037 0.211 0.024
AUC[CROC, a = 14] 0.002 0.003 0.371 0.016 0.394 0.008
AUC[CROC, a = 80] 0.002 0.005 0.241 0.002 0.244 0.002
KNN versus IRV

AUC[ROC] 0.094 0.010 0.164 0.137 0.015 0.073
AUC[AC] 0.083 0.011 0.182 0.140 0.087 0.112
AUC[pROC] 0.025 0.020 0.285 0.201 0.157 0.185
AUC[pAC] 0.027 0.014 0.225 0.210 0.184 0.193
AUC[ROC, FP = 50] 0. 016 0.057 0. 022 0. 020 0. 028 0.141
AUC[ROC, t = 0.0086] 0. 015 0.028 0.030 0. 013 0.140 0.129
AUC[ROC, t=0.1] 0.055 0.148 0.619 0.617 0.291 0.279
AUC[ROC, t= 0.5] 0.136 0.237 0.309 0.300 0.135 0.659
AUC[CAC, a = 7] 0.050 0. 013 0.264 0.258 0.431 0.263
AUC[CAC, or = 14] 0.030 0.010 0.165 0.161 0.080 0.208
AUC[CAC, a = 80] 0.011 0. 015 0.244 0. 024 0.189 0.021
AUC[CROC, a = 7] 0.055 0.011 0.215 0.206 0.135 0.147
AUC[CROC, a = 14] 0.022 0.009 0.100 0.091 0.324 0.106
AUC[CROC, a = 80] 0.010 0.004 0. 016 0. 014 0.039 0. 025

 

Permutation tests were sampled 10 000 times. P < 0.01 are in bold and P-values between 0.01 and 0.05 are in italics.

of the classiﬁers. In these plots, increasing magniﬁcation factors
of 01:7, 14 and 80 are used, corresponding to magniﬁcation
functions that send the points x 20.1, 0.05 and 0.0086 onto x = 0.5,
respectively. The value 0.0086 is chosen because it is the FPR

obtained by looking at the 1000 compounds that are top-ranked by
the IRV. With the magniﬁcation, it becomes immediately apparent
that in this case the IRV has better early recognition performance,
followed by the kNN, SVM and MAX-SIM in that order, although

 

1 353

112 Bro's1120mofp101xo'soi1eu11010101q//:d11q 111011 papeolumoq

9IOZ ‘ISlsnﬁnV uo ::

S.J.Swamidass et al.

 

there is a range of low values where MAX-SIM may be slightly
better than the SVM. Among the top 1000 ranked compounds, the
IRV retrieves 645 positives, to be compared to 603 for the kNN
and 616 for the SVM. Furthermore, the results are consistent at all
magniﬁcation scales, thus the CROC approach appears to be robust
with respect to the choice of 01.

These visualization results are conﬁrmed in Table 1 that displays
the AUC[ROC] and AUC[CROC] of the various algorithms, for
different values of a. While the SVM has a slightly better
AUC[ROC] than the IRV (0.852 versus 0.845), the CROC metrics
conﬁrm the advantage of the IRV algorithm for this task in terms
of early recognition, consistently across different values of a.
For example, for 01:80, the IRV achieves an AUC[CROC] of
0.400 compared to 0.310 for the SVM. Although, the differences
between the best methods—IRV, SVM and kNN—are small, they
are signiﬁcant, as demonstrated in the Section 5.2.

5.2 Assessing signiﬁcance

We have applied the six proposed methods for assessing signiﬁcance
(paired and unpaired permutation tests, paired and unpaired t-tests,
paired and unpaired Wilcoxon tests) to all the known metrics
for assessing early retrieval [e.g. AUC[ROC], AUC[ROC] with
hard threshold cutoff t, AUC[CROC], AUC[pROC] and Boltzman-
enhanced discrimination of the ROC (BEDROC)] and to all
ﬁve classiﬁers. For the methods with hard threshold cutoffs, the
thresholds were chosen to correspond to general practice (FP= 50
corresponds to ROC50) and to the values of the magniﬁcation factor
a we tested, thus t: 0.5 corresponds to a = 7, t=0.1 corresponds to
01:14 and t=0.0086 corresponds to 01:80. In total, we obtained
well over 100 P-value measurements. Only the most salient results
are reported here.

Using the permutation test of signiﬁcance several variants of
the CROC yield P<0.05 and 0.01 (Table 2) conﬁrming that the
difference is statistically signiﬁcant. Several additional patterns are
discernible in the signiﬁcance tests across different performance
metrics. First, as expected, the methods with hard threshold cutoffs
perform better than AUC[ROC] but are substantially weaker than
several of the metrics that do not use hard cutoffs. Second, consistent
with prior studies on statistical power, the AUC[pAC] has better
power than the AUC[pROC], which has better power than the
AUC[ROC] (Zhao et al., 2009). Third, CROC variants have more
power than CAC variants, pROC and ROC. Other metrics reviewed
in Section 6, such as robust initial enhancement (RIE), BEDROC
and sum of log ranks (SLR) correspond to afﬁne transforms of
AUC[CAC] or AUC[pAC], and therefore yield the same P-values
as AUC[CAC] or AUC[pAC].

Most importantly, the signiﬁcance of the early retrieval
performance differences between the classiﬁcation algorithms can
be established with high conﬁdence using the AUC[CROC(exp)]
on a dataset where the AUC[ROC] cannot establish signiﬁcance.
Furthermore, the paired permutation test appears to be the most
powerful method for assessing signiﬁcance. While the t-test and
Wilcoxon methods produce reasonable approximations, their results
are somewhat different from the results produced by the permutation
tests and also less powerful. For the t-test, this is probably as a result
of the violations of the underlying normality assumptions. If a t-test
is to be used to save computational time, the paired t-test is preferable
over the unpaired t-test.

6 DISCUSSION AND CONCLUSION

There have been other attempts to try to address the early enrichment
problem beyond using a low threshold cutoff for the ROC or AC
curves. Clark and Webster-Clark (2008) propose to apply a different
logarithmic transformation to the ROC curve by transforming its
x-axis using

p(x) =log10(max[x,0.5/N]). (11)

In this formula, the max function is used to avoid a 0 argument in
the logarithm when x = 0. The transformed curve is called a pROC
curve by these authors, and obviously the same transformation could
be used to derive a pAC curve, which does not require the zero—
point correction. ROC and AC curves transformed by this equation
extend from (log10(0.5/N),0) to (0,1) (Fig. 1). This logarithmic
transformation, however, has two deﬁciencies. First, the width of
the curve varies based on the number of examples. This means that
performance curves from datasets with different sizes cannot be
meaningfully compared on the same plot. Furthermore, there is no
way to tune the importance of the early portion of the curve. In some
applications, only the top 10% may be interesting or experimentally
testable, while in other applications only the top 1% may be useful.
Ideally, a metric should be tunable to these differences.

Although derived from different standpoints, additional metrics
proposed in the literature are linear afﬁne transforms of our metrics
and can be written in the present notation. Speciﬁcally, Sheridan
et al. (2001), Truchon and Bayly (2007) and Zhao et al. (2009)
have, respectively, proposed to use the RIE, the BEDROC and the
SLR metrics. The RIE metric corresponds to

AUC[CAC(exp)] — 1+ %
R1E~ 1 exp(1 a) ’ (12)
AUC[CAC(6XP)lrand - 1 + m

 

where AUC[CAC(exp)]rand is the area under the concentrated
AC curve obtained by exponential transformation [CAC(exp)] 0f
the random classiﬁer. The BEDROC metric corresponds to
AUC[CAC(eXp)] — AUC[CAC(exp)]min
AUC[CAC(exp)]maX — AUC[CAC(exp)]min ’
where AUC[CAC(exp)]max and AUC[CAC(exp)]min are,

respectively, the area under the best and worst possible CAC(exp)
curves. The SLR metric can be approximated as

 

(13)

SLR%N+(AUC[pAC]+logN), (14)

where N is the total number of test candidates being scored and
N+ the number of positives. Thus RIE and BEDROC are afﬁne
transforms of AUC[CAC] and have the same statistical power
as AUC[CAC]. SLR is an afﬁne transform of AUC[pAC] and
have the same statistical power as AUC[CAC]. Note that the
approximation error in Equations (12) and (14) is negligible since
it is less than the ﬂuctuations introduced by the ambiguity in the
deﬁnition of the AC curve and its area, depending on whether
one uses the rectangular or trapezoidal rule, corresponding to
whether consecutive points are connected by discrete jumps or linear
interpolation. These ﬂuctuations become also irrelevant on large
datasets. Ignoring these ﬂuctuations, which only subtly alter the
computations, RBI and BEDROC have a geometric interpretation
depicted in Figure 3. The particular standpoints originally used to
derive these other metrics did not lend themselves to deriving a more
general magniﬁcation framework that can be extended, for instance,

 

1 354

112 Bro's1120mofp101xo'soi1eu11010101q//:d11q 111011 papeolumoq

9IOZ ‘ISlsnﬁnV uo ::

A CROC stronger than ROC

 

 

 

 

 

 

1.0 -
0.8 :
A  RIE _ C+D+E+F
D: 0.6  _ D+E+F
0.4 a...» — 
....‘09  — 
O . 2 “.“soou D E
0 .
o 0.2 0.4 0.6 0.8 1.0

f(FDP)

1/(1 — e_°‘)

Fig. 3. The geometric relationship between the CAC(exp) curve and two
early recognition metrics. Each letter, from A to F, corresponds to an area
delimited by curves and the boundary of the plot. Both the RE and BEDROC
metrics can be expressed as ratios of particular areas of the plot.

from AC to ROC curves, or to different magniﬁcation functions, or
to derive corresponding visualizations. Furthermore, both the RIE
and SLR metrics are larger than one and their ranges vary with the
dataset to which they are applied, which is somewhat at odds with
most other commonly used metrics for classiﬁcation. The unifying
framework presented here addresses all these issues.

Our data also suggests that the CROC(exp), the variant we
propose for general use in early recognition, has better statistical
power than other commonly used metrics (ROC, pROC, SLR,
RBI and BEDROC). It is tempting to perform more exhaustive
benchmarks of statistical power on simulated datasets. However,
the best strategy for simulating the appropriate data randomly is
unclear. One strategy has been suggested by Truchon and Bayly
(2007) and used by others. This strategy, however, assumes that the
cumulative distribution of ranks is exactly the exp transform. This
assumption is made without clear justiﬁcation and could introduce
some circularity and artiﬁcially bias tests of power toward metrics
based on the exp transform. Although less exhaustive, we expect that
direct comparison of metrics on real, rather than simulated, dataset
will continue to provide unbiased insight into the utility of early
recognition performance metrics.

Here, we have presented a general-principled approach for
‘putting a microscope’ on any portion of the ROC (or AC)
curve, particularly the early part, to amplify events of interest and
disambiguate the performance of various classiﬁers by measuring
the relevant aspects of their performance. The magniﬁcation is
mediated by a concave-down transformation f with a global
magniﬁcation parameter a that allows one to smoothly control
the level of magniﬁcation. Rational approaches for choosing the
transformation and magniﬁcation factors have been proposed.

This magniﬁcation approach is sensitive to the rank of every
positive candidate, and therefore should be preferred over all single
threshold metrics. At the same time, the approach is sensitive to early
recognition performance and can discriminate between predictors
where standard methods fail, as seen in the drug discovery example
used here. Unlike some of the previous visualizations and metrics
(RIE, pROC and SLR), the CROC and CAC are both normalized
to ﬁt within the unit square and lead to an efﬁcient visualization
and measurement of early retrieval performance, with robust and
tunable magniﬁcation. The overall approach has been demonstrated
on a standard, publicly available, drug discovery benchmark dataset

and a Python package that implements the CROC methodology is
publicly available.

Funding: Laurel Wilkening Faculty Innovation award; an NIH
Biomedical Informatics Training grant (LM-07443-01); NSF grants
EIA— 0321390, CCF—0725370, and IIS—0513376 (to P.B.); IBM PhD
Fellowship (to C.A.); Physician Scientist Training Program of the
Washington University Pathology Department (to S.J.S.).

Conﬂict of Interest: none declared.

REFERENCES

Azencott,C. et al. (2007) One- to four-dimensional kernels for small molecules and
predictive regression of physical, chemical, and biological properties. J. Chem. Inf.
Model, 47, 965—974.

Baldi,P. et al. (2000) Assessing the accuracy of prediction algorithms for classiﬁcation:
an overview. Bioinformatics, 16, 412—424.

Clark,R.D. and Webster-Clark,D.J. (2008) Managing bias in ROC curves. J. Comput.
Aided M0l Des., 22, 141—146.

Hassan,M. et al. (2006) Cheminformatics analysis and learning in a data pipelining
environment. Mol Divers, 10, 283—299.

Hert,J. et al. (2004) Comparison of ﬁngerprint-based methods for virtual screening
using multiple bioactive reference structures. J. Chem. Inf. Model, 44, 1177—1185.

Hert,J. et al. (2005) Enhancing the effectiveness of similarity-based virtual screening
using nearest-neighbor information. J. Med. Chem, 48, 7049—54.

Holliday,J.D. et al. (2002) Grouping of coefﬁcients for the calculation of inter-molecular
similarity and dissimilarity using 2D fragment bit-strings. Comb. Chem. High
Throughput Screen., 5, 155—66.

Leach,A.R. and Gillet,V.J. (2005) An Introduction to Chemoinformatics. Springer,
Dordrecht, Netherlands.

Mahé,P. et al. (2006) The pharmacophore kernel for virtual screening with support
vector machines. J. Chem. Inf. Model, 46, 2003—2014.

Seifert,M.H.J. (2006) Assessing the discriminatory power of scoring functions for
virtual screening. J. Chem. Inf. Model, 46, 1456—1465.

Sheridan,R. et al. (2001) Protocols for bridging the peptide to nonpeptide gap in
topological similarity searches. J. Chem. Inf. Comput. Sci, 41, 1395—1406.

Swamidass,S. et al. (2009) The Inﬂuence Relevance Voter: An Accurate And
Interpretable Virtual High Throughput Screening Method. J. Chem. Inf. Model,
49, 756.

Swamidass,S.]. and Baldi,P. (2007) Bounds and algorithms for exact searches of
chemical ﬁngerprints in linear and sub-linear time. J. Chem. Inf. Model, 47,
302—317.

Swamidass,S.]. et al. (2005a) Kernels for small molecules and the predicition of
mutagenicity, toxicity, and anti-cancer activity. Bioinformatics, 21 (Suppl. 1), 359.

Swamidass,S.]. et al. (2005b) Kernels for small molecules and the prediction of
mutagenicity, toxicity, and anti-cancer activity. Bioinformatics, 21 (Suppl. 1),
i359—368.

Truchon,J.F. and Bayly,C.I. (2007) Evaluating virtual screening methods: Good and
bad metrics for the “early recognition” problem. J. Chem. Inf. Model, 47, 488—508.

Zhao,W. et al. (2009) A statistical framework to evaluate virtual screening. BMC
bioinformatics, 10, 225.

APPENDIX A
A.1 MAXIMIZING EARLY RETRIEVAL

An important question is whether a learning algorithm can
be developed to emphasize early recognition, as opposed to
classiﬁcation or ranking over the entire data. A possible approach
to address this issue is to try to optimize CROC behavior, rather
than ROC behavior by, for instance, maximizing the AUC[CROC]
rather than the AUC[ROC]. To this end, we propose an iterative
algorithm where the training examples are reweighted after each
training epoch giving, everything else being equal, higher weights
to higher ranked examples. We describe several variations on this
basic idea and explore them in the simulations. We assume that there
are N training examples. All the examples are initialized uniformly,

 

1 355

112 Bro's112umofp101xo'sor1eu11010101q//:d11q 111011 papeolumoq

9IOZ ‘ISlsnﬁnV uo ::

S.J.Swamidass et al.

 

Table A1. Comparison of the AUC[CROC] (or = 80) of several weight update schemes for the IRV, after three training iterations and after convergence of the

algorithm, together with the total number of iterations for convergence

 

 

 

 

 

NO  Wr :e_r/N wr : 1_:_y e_yr/N
Regular x g(r) Regular x g(r)
AUC[CROC] 0.368 0.383 0.364 0.3 74 0.383
After three iterations
AUC[CROC] 0.404 0.388 0.381 0.383 0.385
After convergence
Iterations 101 8 4 4 5

 

Results are 10-fold cross-validated on the HIV dataset. Best performances are in bold and second-best performances are in italics.

Table A2. Comparison of the AUC[CROC] (01:7 and or: 80) of the regular IRV and the IRV with weights updated by w, =1/r multiplied or not by the
output density g(r), presented after one and three iterations and after convergence of the algorithm

 

AUC[CROC] (or = 7)

AUC[CROC] (or = 80)

 

 

 

w,=1 wr=1/r w,=g(r)/r w,=1 wr=1/r w,=g(r)/r
After one iteration 0.569 0.598 0.611 0.348 0.365 0.366
After three iterations 0.641 0.646 0.652 0.368 0.380 0.386
After convergence 0.660 0.660 0.661 0.404 0.403 0.404

 

The IRV converges after 101 iterations if the weights are not updated, 42 if they are updated by wr = 1 / r and 33 if they are updated by wr = g(r) / r. Results are 10—fold cross-validated
on the HIV dataset. Best performances are in bold and second-best performances are in italics.

with a weight equal to 1 /N. After each training epoch t, the weights
are recomputed according to a polynomial or exponential decaying
scheme of the form

wr(t+1)=Ce_yr(t) or wr(t+1)= (A1)

[f(t)]1’+1 ’
where wr (t+ 1) denotes the weight of the example with ranking
r: r(t) after the ﬁrst t epochs. The constant C is used to normalize
the weights so that their sum remain constant.

A ﬁrst variation on this idea is to multiply the weight update rule
by the density of the scores that are in the neighborhood of the score
of the example ranked r so that

Wr(t+ 1) = Ce_”rg(r), Wr(t+ 1) = L80”)- (A2)

rV‘H

Here, g(r) represents the empirically measured or continuously
smoothed density of the classiﬁer scores around the score of the
r-th ranked example. Intuitively, if this density is high, slightly
improving the classiﬁcation score of the corresponding example
ought to have a larger effect than if the density is low. Other schemes
not explored here could use differentially the density of positive and
negative examples.

A second variation on this idea is to change the parameter y during
training, increasing it monotonically at each epoch, according to
some schedule, linear or other. As in simulated annealing, the goal
is to start with low values of y to avoid getting stuck in early local
minima.

In the corresponding series of experiments, for clarity we focus on
the IRV exclusively, but in principle the same ideas can be applied
to any learning method that allows for weighting the importance of

the training instances. Table A1 shows the results for exponential
weighting schemes, which greatly speed up the convergence of the
algorithm, by a factor of 10 or so. Four to eight epochs of training
with exponential weighting of the examples is sufﬁcient to reach
convergence. In contrast, the unweighted IRV requires 101 epochs.
Although the ﬁnal results obtained with exponential weighting
are slightly inferior to those obtained without any weighting, the
weighted IRV still outperforms the SVM and kNN algorithms. When
a multiplicative factor associated with the density of the classiﬁer
scores around a given example is introduced into the weighting
scheme, mixed results are observed.

As shown in Table A2, the 1 / r power-law weighting scheme
performs the best in this case, converging three times faster than the
unweighted IRV, and reaching a set of weights that gives the same
level of performance, in terms of early enrichment or AUC[CROC],
as in the unweighted case. Convergence takes 42 epochs, and only
33 epochs if multiplied by the density g(r), as opposed to 101 epochs
in the unweighted case. Although after convergence the AUC[ROC]
in the weighted and unweighted cases seem identical (0.396), the
compounds are ranked similarly but not identically. This is why the
AUC[CROC] are slightly different for a = 7 and, with more decimal
precision, the AUC[CROC] with 01:80 can be differentiated. In
any case, on this dataset, we see that for the power (1 / r) weighting
scheme, multiplication by the density g(r) slightly improves both
the speed of convergence and the quality of the ﬁnal classiﬁers. In
summary, this learning scheme can sometimes slightly improve the
performance of a classiﬁer, while quite often dramatically speeding
up the time it takes to train a model by gradient descent. In our study,
this speed-up can be close to 2-fold.

 

1 356

112 Bro's112umofp101xo'sor1eu11010101q//:d11q 111011 papeolumoq

9IOZ ‘ISlsnﬁnV uo ::

