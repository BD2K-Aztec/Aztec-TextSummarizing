Bioinformatics, 31 (1 5), 2015, 2595—2597

doi: 10.1093/bioinformatics/btv153

Advance Access Publication Date: 24 March 2015
Applications Note

 

Data and text mining

PRROC: computing and visualizing
precision-recall and receiver operating
characteristic curves in R

Jan Grau1'*'T, lvo Grosse1'2 and Jens Keilwagen3'T

1Institute of Computer Science and Universitatszentrum lnformatik, Martin Luther University Halle-Wittenberg, Halle
(Saale), Germany, 2German Centre for Integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Leipzig, Germany
and 3Institute for Biosafety in Plant Biotechnology, Julius Kiihn-Institut (JKI) - Federal Research Centre for Cultivated
Plants, Ouedlinburg, Germany

*To whom correspondence should be addressed.

TThe authors wish it to be known that, in their opinion, the first and last authors should be regarded as Joint First
Authors.

Associate Editor: Jonathan Wren

Received on January 20, 2015; revised on February 27, 2015; accepted on March 11, 2015

Abstract

Summary: Precision—recall (PR) and receiver operating characteristic (ROC) curves are valuable
measures of classifier performance. Here, we present the R—package PRROC, which allows for com—
puting and visualizing both PR and ROC curves. In contrast to available R—packages, PRROC allows
for computing PR and ROC curves and areas under these curves for soft—labeled data using a con—
tinuous interpolation between the points of PR curves. In addition, PRROC provides a generic plot

 

function for generating publication—quality graphics of PR and ROC curves.
Availability and implementation: PRROC is available from CRAN and is licensed under GPL 3.

Contact: grau@informatik.uni—halle.de

 

1 Introduction

The assessment of classifier performance is a recurring task in ma-
chine learning and data mining, and in particular in bioinformatics
applications. It assists researchers in identifying the most promising
approach for the classification problem at hand. For binary classifi-
cation tasks, the receiver operating characteristic (ROC) curve and
the area under this curve (AUC—ROC) are widely accepted as a gen—
eral measure of classifier performance. In many bioinformatics ap-
plications, however, positive examples are substantially less
abundant than negative examples, resulting in a highly imbalanced
class ratio. For instance, the number of target genes of a microRNA
is substantially smaller than the number of non—target genes. In such
cases, the precision—recall (PR) curve and AUC (AUC—PR) is better
suited for comparing the performance of individual classifiers than
the ROC curve and AUC—ROC (Davis et 61]., 2005).

Often, the decision for the true class labels of a given data point
is arguable and, for instance, based on an arbitrary threshold for

©The Author 2015. Published by Oxford University Press.

some continuous measurement or based on multiple, possibly
contradictory, expert labelings. However, the choice of this thresh—
old decisively influences classifier training and assessment. One solu—
tion to this problem is the transition from hard—labeling to soft—
labeling, where each data point is assigned to both classes with a cer—
tain probability that reﬂects confidence in the labeling (Grau et 61].,
2013; Mihaljevic et 61]., 2014). Although soft—labeling has been used
extensively for classifier training in the past, it has been neglected
for classifier assessment (Keilwagen et 61]., 2014).

Computing empirical AUC—PR and AUC—ROC values from a
limited set of test data points requires interpolation between discrete
supporting points corresponding to a series of classification thresh—
old affecting the classification result. AUC—ROC can be computed
by linear interpolation between the supporting points of the curve
for hard—labeled and soft—labeled data. In contrast, Davis and
Goadrich (2006) show that for AUC—PR an interpolation along the
true positives is more accurate than linear interpolation for hard-

2595

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/Iicenses/by-nc/4.0/),
which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact

journals.permissions@oup.com

112 /§JO'S{eumo [p.IOJXO'SSUBUHOJUIOIQ/ﬁdllq 11101; popeoIII/vxoq

9IOZ ‘09 lsnﬁnv uo ::

2596

J. Grau et al.

 

Table 1. R-packages for computing PR and ROC curves, and their
AUCs; "both": AUC and curve can be computed; "linear": linear in-
terpolation, "DG": interpolation of Davis and Goadrich (2006),
"con.”: interpolation of Boyd et al. (2013); Keilwagen et al. (2014)

 

 

Package AUC PerfMeas pRO Ca RO CRb PRROC
Version 0.3.0 1.2.1 1.7.3 1.0—5 1.1
PR curve
Hard—labeled No Both No Curve Both
Interpolation N/A Linear N/A Linear DG/con
Soft—labeled No No No No Both
ROC curve
Hard—labeled Both AUC Both Both Both
Soft—labeled No No No No Both
Plotting Yes Std. R Yes Yes Yes

 

aRobin et al. (2011); bSing et al. (2005).

labeled data, while Boyd et al. (2013) and Keilwagen et al. (2014)
propose a more fine—grained, continuous interpolation between the
supporting points of the PR curve. Only the latter can also be used
for soft—labeled data (Keilwagen et al., 2014).

In Table 1, we list several common R—packages for computing
PR or ROC curves or their AUCs, which in some cases provide fur—
ther performance measures. For PR curves, however, none of the
previous packages uses the more accurate interpolation of Davis and
Goadrich (2006) or the continuous interpolation of Boyd et al.
(2013) and Keilwagen et al. (2014). Hence, none of these packages
is applicable to soft—labeled data.

In this article, we present the R—package PRROC, which closes
both gaps by (i) using the continuous interpolation of Keilwagen
et al. (2014) for computing and drawing PR curves and, by this
means, (ii) enabling the computation of PR and ROC curves, and
AUC—PR and AUC—ROC for soft—labeled and hard—labeled data. In
addition, PRROC optionally computes curves and AUC values for
the optimal, the worst and the random classifier as a reference.
These references are particularly useful for (a) PR curves and (b)
ROC and PR curves in case of soft—labeled data, where, for instance,
the minimum and maximum AUC might differ from 0 and 1, re-
spectively. Finally, PRROC provides a plotting function
for generating publication—quality plots of PR and ROC curves.

2 Use cases

In this section, we present typical applications of the PRROC R-
package. Complete listings of the corresponding R—code and further
examples are available in the R vignette of PRROC.

First, we consider the scenario that we developed a novel ap—
proach for a classification problem with ‘hard class labels’ and now
want to assess its performance on an independent test dataset.
Further assume that the classification scores of the data points be—
longing to the positive class are stored in a vector fg, and those of
the negative class in bg. Using PRROC, we can compute the ROC
and PR curve, respectively, by

roc< — roc . curve (fg, bg , curvezT);
pr< —pr . curve (fg , bg , curvezT);

obtain the AUC values with print(roc); print(pr), and plot the curves
with plot(roc); plot(pr). An ROC curve obtained by this procedure
is shown in the left panel of Figure 1.

Alternatively, classification scores for both classes may be stored
in one joint vector (x) and the corresponding class labels (1/0 for

ROC curve (hard labels)

AUC = 03064067 PR curves (soft labels)

 

 

 

 

 

 

 

 

 

 

o_ _ O. _
CO (D
o" _ o" _
3‘ to C co
3 d _ :5 d _
(I) O
C <1- 2 <r
g o' ‘ o. :5 ‘
N (\l
o' _ o" _
O. _ O. _ 
o | I l | | | o | | | | | |
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
FPR Recall

Fig. 1. Plots of ROC (left) and PR (right) curves generated by PRROC. For the
ROC curve, we consider hard-labeled data and show the plotting variant with
a color scale that indicates classification thresholds yielding the points on the
curve. For the PR curve, we consider soft-labeled data and show a compara-
tive plot for two classifiers as solid and dashed lines. We also include the
maximal and minimal possible curves and the curve of a random classifier
for the given soft-labels

positive/negative class) in another vector (lab). In this case, we can
compute the ROC and PR curves by

roc< —roc . curve (x, weights . classO=lab, curvezT);
pr< —pr . curve (x, weights . classO=lab, curvezT);

Second, we consider a scenario for a classification problem
with ‘soft class labels’, where each data point belongs to the positive
class with probability P and to the negative class with probability
(1—P). We assume that the classification scores are again stored in
one joint vector (x) and the soft—labels, i.e., the probability of
belonging to the positive class for each data point, in another vector
(w). Using PRROC, we can compute the PR curve as well as the
minimum and maximum curve, and the curve for the random
classifier by

pr . l< —pr . curve (x, weights . classO:w, curvezT,
max . compute=T , min . compute=T , rand . computezT)

Analogously, we compute the PR curve pr . 2 for another classi—
fier and plot both curves together with the maximum and minimum
curve, and the curve of the random classifier by

plot (pr . l , col=2 , max . plot=T , min . plot=T ,
rand . plotzT, fill . areazT, auc . mainzF);
plot (pr . 2 , col=3 , addzT)

A plot obtained by this procedure is shown in the right panel of
Figure 1. We clearly see the difference in performance of the two
classifiers and may conclude that the ranking implied by the classifi-
cation scores behind the solid curve reconstructs the soft—labels with
greater accuracy.

3 Discussion

We present PRROC, an R—package for computing PR and ROC
curves as well as their AUCs for soft—labeled and hard—labeled data,
which may be beneficial for typical bioinformatics applications.
Additionally, PRROC provides a function for plotting PR and ROC
curves within R. The PRROC package provides R documentation
files and a vignette.

Conﬂict of Interest: none declared.

112 /§JO'S{eumo [p.IOJXO'SSUBUHOJUIOIQ/ﬁdllq 11101; popeoIII/vxoq

9IOZ ‘09 lsnﬁnv uo ::

Computing and visualizing PR and ROC curves

2597

 

References

B0yd,K. et al. (2013) Area under the precision-recall curve: point estimates
and conﬁdence intervals. In: Blockeel, H., Kersting, K., Nijssen, S., and
Zvielezny, F. (ed.) Machine Learning and Knowledge Discovery in Databases.
Vol. 8190 of LNCS. Springer, Berlin, pp. 451—466.

Davis,]. and G0adrich,M. (2006) The relationship between precision-recall
and ROC curves. In: Proceedings of the 23rd International Conference on
Machine Learning. ACM, New York, pp. 233—240.

Davis,]. et al. (2005) View learning for statistical relational learning: with an
application to mammography. In: Proceeding of the 19th International
joint Conference on Artiﬁcial Intelligence, pp. 677—6 83.

Grau,]. et al. (2013) A general approach for discriminative de
novo motif discovery from high-throughput data. Nucleic Acids Res.,
41,e197.

Keilwagen,]. et al. (2014) Area under precision-recall curves for weighted and
unweighted data. PLOS One, 9, e92209.

Mihaljevic,B. et al. (2014) Multi-dimensional classiﬁcation of GABAergic
interneurons with Bayesian network-modeled label uncertainty. Front.
Comput. Neurosci, 8, 150.

Robin,X. et al. (2011) pROC: an open-source package for R and 8+ to analyze
and compare ROC curves. BMC Bioinformatics, 12, 77.

Sing,T. et al. (2005 ) ROCR: Visualizing classiﬁer performance in R.
Bioinformatics, 21, 3940—3941.

112 /§JO'S{eumo [p.IOJXO'SSUBUHOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

