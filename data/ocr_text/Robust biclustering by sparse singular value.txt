ORIGINAL PAPER

Vol. 27 no. 15 2011, pages 2089-2097
doi: 10. 1093/bioinformatics/btr322

 

Gene expression

Advance Access publication June 2, 2011

Robust biclustering by sparse singular value decomposition

incorporating stability selection

Martin Si||1’*, Sebastian KaiserZ, Axel Benner1 and Annette Kopp-Schneider1
1Division of Biostatistics, DKFZ, 69120 Heidelberg and 2Working Group Computational Statistics, LMU, 80539

M nchen, Germany
Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: Over the past decade, several biclustering approaches
have been published in the field of gene expression data analysis.
Despite of huge diversity regarding the mathematical concepts of
the different biclustering methods, many of them can be related
to the singular value decomposition (SVD). Recently, a sparse
SVD approach (SSVD) has been proposed to reveal biclusters in
gene expression data. In this article, we propose to incorporate
stability selection to improve this method. Stability selection is a
subsampling-based variable selection that allows to control Type
I error rates. The here proposed S4VD algorithm incorporates this
subsampling approach to find stable biclusters, and to estimate
the selection probabilities of genes and samples to belong to the
biclusters.

Results: So far, the S4VD method is the first biclustering approach
that takes the cluster stability regarding perturbations of the
data into account. Application of the S4VD algorithm to a lung
cancer microarray dataset revealed biclusters that correspond to
coregulated genes associated with cancer subtypes. Marker genes
for different lung cancer subtypes showed high selection probabilities
to belong to the corresponding biclusters. Moreover, the genes
associated with the biclusters belong to significantly enriched
cancer-related Gene Ontology categories. In a simulation study, the
S4VD algorithm outperformed the SSVD algorithm and two other
SVD-related biclustering methods in recovering artificial biclusters
and in being robust to noisy data.

Availability: R-Code of the S4VD algorithm as well as a
documentation can be found at http://s4vd.r-forge.r-project.org/.
Contact: m.sill@dkfz.de

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on March 1, 2011; revised on April 11, 2011; accepted on
May 10, 2011

1 INTRODUCTION

Clustering methods belong to the most commonly used statistical
tools in the analysis of high—dimensional datasets. If additional
information about the sample class labels is lacking, other types
of analysis like supervised classiﬁcation methods or testing for
differentially expressed genes cannot be performed. In this case,
unsupervised clustering allows to reveal unknown structures that are

 

*To whom correspondence should be addressed.

possibly hidden in the gene expression data matrix. These structures
may be characterized by groups of genes that are coregulated by a
common transcription factor and thus belong to the same pathway
or samples that share a similar gene expression pattern.

One disadvantage of commonly used clustering algorithms like
hierarchical clustering or k—means clustering is that the cluster
assignment of objects are based on the complete feature space, e. g. in
case of clustering the samples, the resulting clusters are derived with
respect to all genes. But groups of genes may only be coregulated
within a subset of the samples and samples may share a common
gene expression pattern only for a subset of genes. Such clusters
that exist only in a subspace of the feature space can hardly be
detected by these classical one—way clustering algorithms. To ﬁnd
such clusters, other clustering concepts are needed.

In the past decade, the concept of biclustering has emerged in the
ﬁeld of gene expression analysis. Biclustering which is also known
as coclustering or two—way clustering describes the simultaneous
clustering of the rows and the columns of a data matrix. The ﬁrst
biclustering algorithm, the so—called Block Clustering, has been
developed by Hartigan (1972). Cheng and Church (2000) proposed
the ﬁrst biclustering algorithm for the analysis of high—dimensional
gene expression data. Since then, many different biclustering
algorithms have been developed. Currently, there exists a diverse
spectrum of biclustering tools that follow different strategies and
algorithmic concepts. Among others, popular algorithms are the
Coupled Two—Way Clustering (CTWC) by Getz et al. (2000) , Order
Preserving Sub Matrix (OPSM) algorithm by Ben—Dor et al. (2003),
the Iterative Signature Algorithm (ISA) by Bergmann et al. (2003),
the Plaid Model by Lazzeroni and Owen (2002) and the improved
Plaid Model (Turner et al., 2005), SAMBA by Tanay et al. (2004),
biclustering by non—smooth non—negative matrix factorization by
Carmona—Saez et al. (2006), the Bi—correlation clustering algorithm
(BCCA) by Bhattacharya and De (2009) and factor analysis for
bicluster acquisition (FABIA; Hochreiter et al., 2010). Prelic et
al. (2006) developed a fast divide—and—conquer algorithm (Bimax)
and conducted a systematic comparison of different biclustering
algorithms. Santamaria et al. (2007) published an article on
validation indices for the evaluation of biclustering results and
the comparison for biclustering algorithms. Comprehensive reviews
about the concept of biclustering and the different biclustering
approaches have been written by Madeira and Oliveira (2004) and
Van Mechelen et al. (2004).

In a more theoretical review, Busygin et al. (2008) emphasized
the mathematical concepts behind several biclustering algorithms
and pointed out that the SVD represents a capable tool for ﬁnding

 

© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 2089

112 /§JO'SIBUJn0[pJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq U101} popeommoq

9IOZ ‘091sn8nv uo ::

M.SiII et aI.

 

biclusters. Furthermore, most existing biclustering algorithms use
the SVD directly or have a strong association with it. To keep track
of the huge diversity, regarding the mathematical properties of the
existing biclustering algorithms, Busygin et al. (2008) suggest to
relate new and existing biclustering algorithms to the SVD.

A major drawback of many biclustering methods is that they rely
on random starting seeds and thus are inconsistent and results may
vary even when the algorithm is applied to the same dataset. As often
in unsupervised clustering it is difﬁcult to judge the biclustering
results regarding their stability. For one—way clustering, several
resampling approaches to validate the stability of the clustering
results are known, e. g. multiscale bootstrap hierarchical clustering
(Suzuki and Shimodeira, 2006) and consensus clustering (Monti,
2005). In case of biclustering, similar methods that take the stability
of the results into account are not yet available.

Recently, Lee et al. (2010) proposed a sparse SVD (SSVD)
method to ﬁnd biclusters in gene expression data. Singular vectors
of an SVD are interpreted as regression coefﬁcients of a linear
regression model. The SSVD algorithm alternately ﬁts penalized
regression models to the singular vector pair to obtain a sparse matrix
decomposition. The sparseness of the resulting singular vectors
strongly depends on the choice of the penalization parameter. In
this article, we propose to choose the penalization parameters by
stability selection (Meinshausen and Buhlmann, 2010), which is a
subsampling procedure that can be applied to penalized regression
models to select stable variables. In addition, stability selection
offers the possibility to control Type I error rates (Dudoit et al.,
2003), e. g. the per—family error rate (PFER) or the per—comparison
wise error rate (PCER). Applying the new combined algorithm, the
sparse SVD algorithm with nested stability selection (S4VD) to a
lung cancer gene expression dataset reveals biclusters that represent
lung cancer subtypes characterized by relevant sets of coregulated
genes. In a simulation study, we compare the S4VD with the SSVD
algorithm as well as the improved Plaid Model (Turner et al., 2005)
and the ISA (Bergmann et al., 2003).

2 METHODS
2.1 SVD and biclustering

Let X=(x,-j) 6R1” X” be the gene expression matrix with indices 1': 1, ..., p
and j: 1, ...,n. The number of genes p is usually by multiple greater than
the number of samples n. The SVD of X can be written as:

x~UDvT=deukv,{, (1)
k=1

where r is the rank of X and the columns of the matrix U: (m, ...,ur) are
the orthonormal left—singular vectors and the columns of V =(v1, ...,vr) are
the orthonormal right—singular vectors. The elements of the diagonal matrix
D are the corresponding positive singular values d1 2 d2 2  Z d, > 0. Thus,
the SVD is the sum of rank of one matrices dkukvg, herein after also called
SVD—layers. According to Busygin et al. (2008), biclustering can be related
to the SVD by considering an idealized data matrix. This matrix has a block
diagonal structure where each block represents a bicluster and the elements
outside these blocks are equal to zero:

x1 0  0
0 x2 0

X: . . , , (2)
: : '. 0
0 0 x,

where Xk, k = 1, . . . , r are submatrices of X. If we decompose X by the SVD,
then each submatrix Xk will be associated with a singular vector pair (uk, vk)
such that the non—zero coefﬁcients in uk represent the rows that belong to Xk
and the non—zero coefﬁcients vk represent the columns that belong to Xk. In
the presence of noise and if the data matrix has no block diagonal structure,
the SVD will still be able to detect the rows and columns of the submatrices
as the prominent coefﬁcients in the singular vector pair. These properties
make the SVD a practical method for biclustering.

2.2 The SSVD algorithm

A sparse SVD method for biclustering high—dimensional gene expression data
has been proposed by Lee et al. (2010). The idea is to interpret the singular
vectors of a regular SVD as regression coefﬁcients of a linear regression and
use sparsity—inducing penalties to obtain sparse singular vector pairs.
According to Eckart and Young (1936), the ﬁrst SVD—layer gives us the
best rank—one approximation of X with respect to the squared Frobenius
norm, i.e.
(d1,u1,v1)=arg min “X—duvT  (3)
d,u,v

where ||'||12F indicates the squared Frobenius norm, which is the sum of
squared elements of the matrix. Lee et al. (2010) showed how this rank—one
approximation can be related to linear regression. Suppose 111 is ﬁxed, then
the minimization of (3) with respect to (d1 , v1) is equivalent to a minimization
with respect to 91 =(d1v1). Accordingly, the loss function can be written as:

llX—uﬁﬂli:lly—(Inoulwl

 

 

, (4)

where y=(x1T, )T ER” with Xj being the j—th column of X. Then the
minimization of (4) can be interpreted as least squares problem with y as
the response vector, In®u1 as the design matrix and the 61 as vector of
regression coefﬁcients. The least squares estimator of 91 is:

2 —1

v1 = {(Inoulfanoun} (In®u1)Ty (5)
= (ulTx1,...,u1Tx,,)T
= XTul.

In the same way, we can derive the least squares estimator for the product
of the ﬁrst left singular vector multiplied with the ﬁrst singular value in. So
without loss of generality with v1 ﬁxed, the minimization of (3) with respect
to in =(d1u1) is given by the minimization of:

IIX—ﬁlvilli=Ilz—<In®v1>ﬁ1

 

 

, (6)

where z=(x1, ...,Xp)T ER” with  being the i—th row of X. Here z is the
response vector and (In ®v1) is the design matrix.
Finally, the least squares estimator of in is given by:

61 = {(In®v1)T(In®v1)}‘1(Inovlfz (7)
=(X1Tv1,...,xTv1)
= Xv1.
In order to obtain sparse singular vector pairs, Lee et al. (2010) suggest

to ﬁnd the ﬁrst SVD—layer that minimizes the Frobenius norm subject to
sparsity—inducing penalty terms P1(d1u1) and P2(d1v1):

2
“X—dlulvlT “F+ku1P1(d1u1)+lv1P2(d1V1), (8)

where Au, and AV, are tuning parameters. Possible penalty functions are the
adaptive lasso penalties (Zou, 2006). The corresponding penalized function
is given by:
P n
P1 (d1u1)=d1 Zwljluljla P2(d1V1)=d1 ZW2J|V1J|a (9)
i=1 j=1

where w1,,- and wzd- are weights that can be chosen according to Zou (2006),
e. g. for w1,,- = wzd- = 1 we obtain the lasso penalty. Thus, the penalty functions

 

2090

112 /§JO'SIBUJn0[pJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; popaommoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse SVD incorporating stability selection

 

are weighted sums of the absolute values of the elements of the ﬁrst singular
vector pair. Fixing 111 and using the adaptive lasso penalty, the minimization
of (8) becomes:

r1
||X_d1u1vI||:+AV1 sz.j|V1.j| (10)
j=1

n
=||X||%+Z{izij—2i21,j(XTu1)j+Avlwz,j|i21,jl}.
j=1

To solve this penalized regression and estimate the sparse right singular
vector, Lee et al. (2010) proposed an algorithm that incorporates a
simple component—wise thresholding rule. The component—wise minimizer
of (10) is:

$1,-=sign{(XTui)j}(I(XTui)j| —Avlw2,,-/2)+. (11)

This is the well—known soft threshold estimator proposed by Tibshirani
(1996). Then 91 2 (171,1 , ...,i71,,,)T, is an estimate for the product of the ﬁrst
right singular vector multiplied with the ﬁrst singular value. In order to get
an estimate for the ﬁrst sparse right singular vector, we have to update the
ﬁrst singular value. The ﬁrst update of d1 is dlavl = “$71” and accordingly
the estimated sparse singular vector becomes €71 =31 /d1,V1. The penalized
regression for the left singular vector can be solved in the same way. For
ﬁxed v1 and with the adaptive lasso penalty, the loss function of (8) becomes:

P
||X_d1u1vI||12!7+Au1Zwljluljl (12)
i=1

p
= IIXIIin-l-Z  —25t1,i(XV1)i+lu1W1,i|5t1,i| 
i=1

The component—wise minimizer of (12) is:
fit.- =sign{(Xvi),-}(I(Xvi)t| —Au,w1,i/2)+. (13)

The updated singular value is dlaulzllﬁlll, with ﬁl=(lfl1,1,...,§i1,p)T.
Finally, the estimated sparse left singular vector is in =61 /d1,u1.

The degree of sparsity, which is deﬁned as the number of non—zero
coefﬁcients in the singular vector pair, depends on the choice of the
penalty parameters. Lee et al. (2010) proposed to choose the optimal
degree of sparsity by computing the complete penalization path and
apply the penalty parameter that minimizes the Bayesian information
criterion (BIC). In the SSVD algorithm, the two regressions with the
corresponding parameter tuning are alternated until convergence is reached,
which is if either “v1 —€71  < e or ||u1 —ﬁ1  < e, where e > 0 is an arbitrary
convergence threshold. After convergence the ﬁnal estimate of the ﬁrst
singular value of the sparse SVD—layer is d1 =ﬁ1Tle. The next sparse rank—
one approximation can be obtained by subtracting the sparse SVD—layer and
applying the SSVD method to the residual matrix X —d1ﬁ1v1T.

 

The SSVD algorithm

 

1. Apply the standard SVD to X. Let {d1,u1,v1} denote the ﬁrst SVD
triplet.

2. Update:
(a) Set 5H,;-=sign{(XV1)t}(l(XV1)tl—ku1wtt/2)+, where Au,

minimizes the BIC. Let ﬁl =(tt1,1,...,tt1,p)T, dLu1 = Ilﬁl n, and
ﬁ1=l~11/d1,u1.

(b

V

Set 91,,-=sign{(xTﬁl)j (|(xTﬁ1)j|—xvlwz,j/2)+, where xv,
minimizes the BIC. Let {’1 2071,], ...,1~/1,n)T, (11",1 =IIT’1II, and
9129/6113“.

(c) Set v1 = €71, u1 = in and repeat 2(a) and 2(b) until convergence.

3. Set it =ﬁ1Txv1.
4. To obtain the next layer apply steps 1—3 to the residual matrix X—
d 1am]?

 

In practice, we observed that choosing the regularization parameters
according to the BIC results in singular vector pairs with a relative low
degree of sparsity. In addition, the SSVD algorithm does not offer a stopping
criterion and so the choice of the number of SVD—layers is arbitrary.

2.3 Stability selection

In this article, we propose to choose the penalization parameters and to
control the degree of sparsity of the resulting SVD—layers using stability
selection (Meinshausen and Biihlmann, 2010). The idea of stability selection
is to combine resampling with variable selection methods, e.g. penalized
regression models. For each variable, its probability of being selected is
estimated by resampling the data and calculating relative frequencies of
being selected. Meinshausen and Bijhlmann (2010) provide a theoretical
framework for controlling Type I error rates of falsely selecting variables
based on the maximum of these selection probabilities over the range of
regularization parameters.

Suppose we want to infer the true set of non—zero coefﬁcients in the
left singular vector Su1={i:u1,,-7£0}. The set of possible penalization
parameters that can be applied within the adaptive lasso regression is Aul.
Each Au, 6 Alll leads to a different estimated subset of indices of non—zero

AAu . .
coefﬁcients Sul1 §{1,..., p}. Mernshausen and Bijhlmann (2010) 111ustrate
the stability selection with the so—called stability paths that show the selection
probabilities of each coefﬁcient along the range of penalization parameters.

. . Al . .
Grven any A111, the estrmated set Sui” can be written as a functron of the

AA.“ AA.“ .
samples J={1,...,n}, e.g. 51111 251111 (J). If J* CJ 1s a subsample drawn

without replacement, then the estimated selection probability is:

A Au . AAu

I'll. 1=P(l€Sull (J*)). (14)
The selection probability can be estimated by calculating the relative
selection frequencies of i with regard to all subsamples. Given an arbitrary

threshold 71W 6 (0.5, 1) and the set of penalization parameters Aul, the set of
non—zero coefﬁcients estimated with the stability selection is:

Sztfble = :i: max Fl?“1 37%,}. (15)
Aul eAul

According to Meinshausen and Bijhlmann (2010), the value of am” has a
negligible inﬂuence and they recommend to choose values in the range of
[0.6,0.9]. Let SAN =Uiu1€Aul SAN be the union of the estimated sets of
selected coefﬁcients with regard to all Au, 6 Alll . Then the average number
of selected coefﬁcients is un1=E(|SA“1(J*)|). Let Nlll denote the set
of zero coefﬁcients, then the number of falsely selected coefﬁcients with
stability selection is given by VIII 2 |Nlll ﬂSﬁtfblel. Following Theorem 1 in
Meinshausen and Bijhlmann (2010), the expected number of falsely selected
coefﬁcients is bounded by:

2
E(V )< —1 qA‘”

“1 — (zetr— 1) p
Interpreting Equation (16), the expected number of falsely selected
coefﬁcients decreases by either reducing the average number of selected
coefﬁcients q Aul or by increasing the threshold 71W. Supposing that 71th,
is ﬁxed, the stability selection controls the desired error level of E(Vu1) as
long as the average number of selected coefﬁcients is less then e Aul, where

e Aul =./E(Vu1)p(27£thr — 1) is an upper bound for the average number of
selected coefﬁcients that can be controlled by reducing the length of the

regularization path Aul. In multiple testing, the expected number of falsely
selected variables is also known as PFER and if divided by the total number
of the variables, it will become the PCER (Dudoit et al., 2003). The stability
selection allows to control these Type I error rates.

 

(16)

 

2091

112 /§JO'S[BUJn0[pJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 11101; popeoIIr/vioq

9IOZ ‘09 lsnﬁnv uo ::

M.SiII et aI.

 

2.4 The SSVD algorithm with nested stability selection

Here, we propose to replace the BIC—based penalty parameter selection of the
SSVD algorithm by the stability selection. This combined approach allows
to control the expected number of falsely selected non—zero coefﬁcients in
the singular vector pair and therefore the degree of sparsity of the resulting
SVD—layers. Furthermore, the error control also serves as stopping criterion
for the improved SSVD algorithm and determines the number of reasonable
layers.

We aim to estimate the left singular vector in and at the same time infer
the true set of non—zero coefﬁcients Sul. For each possible A111, we draw

- . . . . A A ,
subsamples and estrmate the selectron probab111t1es 1'11. “1. Gwen a threshold
mm and the desired Type I error E(Vu1) the regularization region Au1 is
deﬁned so that q Aul 5 e Aul. Then the estimated set of non—zero coefﬁcients is:

A . A A.
Sztfble=:l2 max I'll.lll 3mm} (17)

Aul eAul

To estimate ﬁl, we apply the component—wise minimizer of Lee et al. (2010)

with the smallest penalization value of the regularization path A313“.

fit,-=sign{(Xvi)t}(I(Xvi)t| —A{:i“w1,t/2)+ (18)

Like in the priginal SSVD approach, the ﬁrst update of the singular value
is d1,u1=||ﬁ1||, with ﬁ1=(i:il,1,...,i:il,n)T. The estimated sparse singular
vector is til =ﬁ1 /d1,u1. Without loss of generality, we estimate the sparse
right singular vector €71 and infer the respective set of non—zero coefﬁcients

SV1 . The selection probabilities 121)}V1 for each AV, are estimated by drawing
subsets of the genes 1* CI, where I = {1, ..., p}. Again, given the desired Type
I error E(VV1) and the threshold mm the regularization region is delimited
such that q An 5 e Avl, where eAV1 = E (VV1 )n(2ﬂ’thr — 1). Consequently, the
estimated set of non—zero coefﬁcients in the right singular vector is:

A A A

Sitable = 1': max H - V1 2 mm (19)
1 xvl eAVl 1

Given the smallest parameter of the penalization path Arvnlin,

of 91 are:

the components

31,,- = sign{(XTui),-}(I(XTui),-I —A$1§“w2,j/2)+ (20)

Finally let 912(91,1,...,91,H)T, the updated ﬁrst singular value is dl,V1 =
“61” and estimated sparse singular vector is 651/611,“.

These two penalized regression models with the nested stability selection
are alternated until convergence, e.g. that is if either ||v1—vlll <6 or
||u1 —ﬁ1 H < e, where e > 0. After convergence, the estimated singular value
is d1=ﬁ1Tle and ﬁnally those coefﬁcients that are not in the two sets
of stable coefﬁcients Sﬁtfble and Sitlable are set to zero. So the components
of in become a1,,=1(ieS§;fble)a1,, and the components of €71 become
131,]- : 1(j e Siaableﬁ/Lj, where 1(') is an indicator function.

The high degree of sparsity of the resulting SVD—layers may lead to a
poor matrix factorization that might induce noise to the residual matrix
when subtracted from the data matrix. Like for multivariate regression
models, this can be seen as a trade—off between a high degree of sparsity
and hence interpretability for the cost of losing prediction power. Regarding
the sequential ﬁtting procedure of the S4VD algorithm, the acceptance
of a poor matrix approximation might induce noise into the residual
matrix. This induced noise may perturb the ﬁtting process for subsequent
biclusters. In order to avoid a propagation of errors induced by a poor
matrix approximation, we propose to apply the regular SVD to the submatrix
deﬁned by the stable subsets of rows and columns identiﬁed with the
S4VD algorithm. According to Eckart and Young (1936), the rank—one SVD
approximation of this submatrix is the best rank—one approximation of the
submatrix with respect to the Frobenius norm. The next bicluster can be
detected by subtracting this rank—one approximation of the submatrix from
the corresponding submatrix of the input data matrix and applying the S4VD
algorithm to the resulting residual matrix.

Alternatively non—overlapping biclusters can be detected by excluding
either the rows or the columns (or both) that correspond to the non—zero
coefﬁcients in the singular vector pair and apply the S4VD method to the
submatrix. By incorporating the stability selection, a stopping criterion can
be deﬁned. If in any iteration an estimated set of non—zero coefﬁcients is an
empty set, the sequential ﬁtting of sparse rank—one layers will be interrupted.

Due to the element of resampling the S4VD algorithm will not necessarily
converge to the exact same result when applied to the same dataset. However,
the element of resampling also allows to take the bicluster stability into
account by controlling the Type I error levels of falsely assigning rows and
columns. As demonstrated by the simulations presented in Section 3, the
S4VD algorithm shows a better performance in revealing the true bicluster
structure and is more robust to noise.

 

The S4VD algorithm

 

1. Apply the standard SVD to X. Let {d1,u1,v1} denote the ﬁrst SVD
triplet. Choose the desired Type I errors E(VV1) and E(Vu1) and the
threshold at)”.

2. Update:

, A x
(a) For each Au, draw subsamples J * and estrmate l'liul. Deﬁne
Alll such that q Aul SeAlll and estimate the set of non—zero

coefﬁcients Sﬁtfble.

Set flu =Sign{(XV1)i}(l(XV1)i| —l{1nlinW1,i/2)+

Let Isl] 203151, ...,Iilapﬂ", dlaul =  II, and uA1=lgl/d1,u1

For each AV, draw subsamples 1* and estimate 121M. Deﬁne
AV1 such that qAVl SeAV1 and estimate the set of non—zero
coefﬁcients Saab”.

Set 31,,- = sign{(XTﬁ1)j} (|(xTﬁ1),-| 4311mm,- /2)+

Let {’1 =(171,1, ...,171,n)T, d1,v1 = “(’1 II, and 91:7/611,“

(b

V

(c) Set v1 = €71, u1 = in and repeat 2(a) and 2(b) until convergence.
3. After convergence set d1 =ﬁ1Tle.
The components of in become 01,,- = 1(i e Sﬁtfbleﬁiu.
The components of €71 become 131,]- : 1(j e Siaableﬁzlj.
4. To obtain the next layer, apply steps 1—3 to the residual matrix after

subtracting the rank—one approximation derived by applying a regular
SVD to the submatrix deﬁned by Sﬁtfble and Sitlable.

5. Stop steps 1—4 if either Saab” = 0 or Sﬁtfble = 0.

 

The subsampling steps of the stability selection makes the S4VD
algorithm computationally very demanding. To reduce the computation
time, we implemented the pointwise error control suggested by Meinshausen
and Bijhlmann (2010). To examine how the pointwise error control reduces
the computation time, the runtimes of the SSVD algorithm, S4VD algorithm
and S4VD algorithm with the pointwise error control have been compared.
In the ﬁrst part of the simulation study described in the Section 3 at a noise
level of 0.5, the mean runtime of the SSVD algorithm was 33.9 s, for the
S4VD it was 181.7 s and for the S4VD with the pointwise error control it
was 5.8 s. The simulations have been carried out using a notebook with an
Intel®CoreTM2 Duo Processor T7700 2.4 GHz and 4 GB DDR2 SDRAM.
Details about the pointwise error control and boxplots of the runtimes are
shown in the Supplementary Material.

3 RESULTS

In order to demonstrate that the here proposed S4VD algorithm is
able to ﬁnd biologically relevant biclusters, we applied it to a known

 

2092

112 /§.IO'S[BU.IT10[p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; popeoIIr/vioq

9IOZ ‘09 lsnﬁnv uo ::

Sparse SVD incorporating stability selection

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

__ _ _L _ _ _ Q
_ (O
—O
F
LO
l
'9322:9BEEEEEEEEEEEEEEEEEEEEEEEECCCCCCCCCCCC======§EEEECEEE
OOOOOOOOOOOO‘D‘DCD‘D‘D‘D O
822222232222Egg§§§§§EggEggggﬁggggggggggggggggggEEEEEEEEg
00000000000000 0 mmmmmmooooo
aeeaeaeeaaeaeazzzzzzzzzzzzzze eeeeseeaeea ZZZ
00000000000000 0 wwwwwwooooo

Fig. 1. Heatmap showing the biclusters identiﬁed in the lung cancer dataset. Note that the heatmap shows only those genes that have been selected in at least
one bicluster. The rectangles indicate the genes and samples that correspond to the three biclusters (the rectangle on the left side corresponds to Bicluster 1,
the two rectangles in the middle correspond to Bicluster 2 and the two rectangles on the right side to Bicluster 3).

lung cancer gene expression dataset (Bhattacharyee et al., 2001).
Furthermore, to examine the inﬂuence of increasing levels of noise
regarding the performance of the S4VD algorithm, we performed
a simulation study. The S4VD algorithm was compared with the
SSVD method, the improved Plaid Model (Turner et al., 2005) and
the ISA (Bergmann et al., 2003). The ISA and the Plaid Model are
known to be closely related to the SVD.

3.1 Evaluation of the lung cancer dataset

Here we analyzed the same subset of the lung cancer gene expression
data set (Bhattacharyee et al., 2001) that was used by Lee et al.
(2010) to illustrate the SSVD algorithm. This dataset contain 56
samples and gene expression values of 12625 genes measured
using the Affymetrix 95av2 GeneChip. The samples comprise
20 pulmonary carcinoid samples (Carcinoid), 13 colon cancer
metastasis samples (Colon), 17 normal lung samples (Normal)
and 6 small cell lung carcinoma samples (SmallCell). Lee et al.
(2010) applied the SSVD method to this gene expression matrix
and decomposed it into the ﬁrst three sparse SVD—layers. For each of
the resulting SVD—layers, the degree of sparsity was relatively low,
e. g. for the three singular vectors that correspond to the samples,
the number of non—zero coefﬁcients were 55 for the ﬁrst two and
47 for the third. The singular vectors that correspond to the genes
contained 3205, 2511 and 1221 non—zero coefﬁcients. Scatterplots
of the sample singular vectors showed a clear grouping of the
samples into the different cancer subtypes. In addition, Lee et al.
(2010) formed 27 gene sets according to the sign of the coefﬁcients
in the three gene singular vectors. The mean expression proﬁles
of these gene sets showed clear differences between the cancer

subtypes. However, despite these results a direct interpretation of
each singular vector pair is not possible. To obtain SVD—layers
with a higher degree of sparsity that can be interpreted as single
biclusters, we applied the S4VD algorithm controlling a PCER of
0.5 for falsely selecting coefﬁcients in the sample singular vector
and a PCER of 0.01 for falsely selecting coefﬁcients in the gene
singular vector. Furthermore, we did not allow the samples to
overlap, e. g. each sample is assigned to only one bicluster. Therefore,
after a sparse SVD—layer is ﬁtted, we exclude the corresponding
columns from the data matrix and applied the S4VD algorithm
to the resulting submatrix. According to the stopping criterion of
the S4VD algorithm, three biclusters have been obtained and are
shown in the heatmap in Figure 1. The ﬁrst bicluster corresponds
to a subset of 550 genes and a subset of 28 samples including 14
Normal samples and 14 Carcinoid samples. The second bicluster
comprises 12 Colon samples and one falsely assigned Carcinoid
sample together with a subset of 506 genes. The third bicluster
consists of 6 SmallCell samples and 344 genes. All other samples
and genes have not been assigned to any bicluster. To illustrate
that the selected genes represent genes that are associated with the
cancer subtypes, we performed a geneset enrichment analysis (Alexa
et al., 2006). Tables of all signiﬁcantly enriched Gene Ontology
(GO) terms (p<0.01) as well as a description of the analysis
can be found in the Supplementary Material. Bhattacharjee et al.
(2001) identiﬁed several possible marker genes for the different
cancer subtypes. A list of eight of these genes together with
the corresponding selection probabilities with respect to the three
biclusters are shown in Table 1. TGF-ﬂ receptor 11, tetranectin,
retinoic acid receptor responder 3 and ﬁcolin 3 are known to be
highly expressed in normal lung tissue compared with carcinoid

 

2093

112 /§.IO'S[BU.IT10[p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; popeoIIr/vioq

9IOZ ‘09 lsnﬁnv uo ::

M.SiII et aI.

 

Table 1. Selection probabilities of lung cancer subclass marker genes

 

Gene

Bicluster 1 Bicluster 2 Bicluster 3

 

Retinoic acid receptor responder 3

Transforming growth factor, 6 receptor II (70/80kDa)

C—type lectin domain family 3, member B (tetranectin)

Ficolin (collagen/ﬁbrinogen domain containing) 3 (Hakata antigen)
v—myc myelocytomatosis viral oncogene homolog

Integrin, a 6

Cyclin—dependent kinase inhibitor 2C (p18)

Thymosin p

0.99 0.00 0.00
1.00 0.00 0.86
1.00 0.74 0.68
1.00 0.71 0.98
0.00 1.00 0.20
0.00 0.93 0.00
0.00 0.00 0.91
0.00 0.15 0.98

 

tissue and thus have high selection probabilities for the ﬁrst bicluster.
This coincides with the GO analysis, e.g. 2 of the 62 GO terms
that are signiﬁcantly enriched by the genes corresponding to the
ﬁrst bicluster are TGF,B receptor signaling pathway (GO:0007179)
and response to retinoic acid (GO:0032526). Integrin,a6 as well
as v-myc (c-myc) are usually overexpressed in colon cancer. These
genes have high selection probabilities with respect to the second
bicluster. In addition, among the 61 signiﬁcantly enriched GO terms
corresponding to the second bicluster is the term endothelial cell
migration (GO:0043542), which coincides with the fact that the
associated samples correspond to colon cancer metastases. The
cell-cycle inhibitor protein p18 and thymosin-ﬂ are markers for
small cell carcinomas and show high selection probabilities in the
third bicluster. Among the 97 GO terms signiﬁcantly enriched in
the third bicluster are many cell cycle—associated terms, e.g. cell
division (GO:0051301), mitotic spindle organization (GO:0007052)
and cell cycle checkpoint (GO:0000075). Furthermore, for the
ﬁrst bicluster as well as for the third bicluster the GO term
positive regulation of Notch signaling pathway (GO:0045747) is
signiﬁcantly enriched. Alterations of the Notch signaling cascade
are known to be associated with several human cancer types.

3.2 Simulation study

In the ﬁrst part of the simulations, we generated 100 artiﬁcial data
matrices comprising p: 1000 genes and n: 100 samples, where
each entry of the data matrix is set to 0. In each dataset, we randomly
assigned 100 genes and 10 samples to a bicluster that shows constant
upregulated gene expression represented by a value of 1 in the
data matrix. Normally distributed noise N(0, 02) was added to each
entry of the data matrix. We examined different noise levels in
the range of o: (0,0.1, . . ., 1). In the second part of the simulation
study, 100 data matrices of the same dimension were generated.
This time four biclusters were included where each consists of
100 genes and 10 samples. Constant up— and downregulation was
represented by values of 1,—1,0.5 and —0.5. For both scenarios, the
performance of the S4VD algorithm was examined in comparison to
the original SSVD algorithm, the improved Plaid Model (PM; Turner
et al., 2005) and the ISA (Bergmann et al., 2003). Since the SSVD
algorithm does not include a stopping criterion, we considered
only the ﬁrst SVD—layer as result in the ﬁrst scenario and the ﬁrst
four SVD—layers as the biclustering result in the second scenario.
The clustering results were validated by application of an external
validation index based on the Jaccard coefﬁcient. In addition, the
stability of the clustering results was assessed through the average

proportion of falsely selected rows and columns. Details on the
validation indices, the remaining biclustering algorithms and their
relation to the SVD are provided in the Supplementary Material.

3.2.] Scenario 1 The simulation results of the ﬁrst scenario are
shown in Figure 2. For low noise levels up to o = 0.3, all biclustering
algorithms except the SSVD show an almost perfect performance
with relevance and recovery scores mostly equal to one and no
falsely selected rows and columns. For noise levels of 0.1 to 0.7,
all biclusters proposed by the SSVD algorithm are too large and
on average a proportion around 0.015 of the rows and 0.012 of the
columns are falsely assigned. This results in relevance and recovery
scores around 0.8. In case of larger noise levels, the SSVD algorithm
often fails to converge and thus the relevance scores and the number
of falsely assigned rows and columns approach zero. For noise levels
above 0.3, the ﬁrst bicluster detected by the Plaid Model usually
consists of a strict subset of those rows and columns that belong to
the true artiﬁcial bicluster in the data. Thus, the performance of the
Plaid Model regarding the relevance and the recovery decreases with
noise. Furthermore, the algorithm starts to ﬁt the noise and proposes
a number of further small biclusters. This explains why the relevance
score is inferior compared with the recovery score. Most of these
small biclusters correspond to parts of the true artiﬁcial bicluster
and hence the proportions of falsely assigned rows and columns are
close to zero. Beginning with a noise level of 0.5, the ISA proposes
an increasing number of biclusters of which only one shows a strong
agreement with the true bicluster. Even after applying the additional
ﬁltering functions available in the isa2 R—package (Csardi et al.,
2010), some nonsense biclusters remain. Thus, both scores start to
decrease with noise but are superior to the Plaid Model. The number
of falsely assigned rows and columns increases with the noise level
indicating that some of the detected biclusters correspond to ﬁtted
noise. Regardless of the noise level, the S4VD algorithm always
detects a single bicluster that agrees with the true bicluster. For
noise levels above 0.6, the proposed bicluster becomes smaller and
represents only a part of the true bicluster. Therefore, both scores
start to decrease with noise but are superior to that of all other
biclustering methods considered in the simulation study.

3.2.2 Scenario 2 The results of the second part of the simulation
study are shown in Figure 3. For noise levels below 0.3, the ISA
and the S4VD showed relevance and recovery scores around 1
and accordingly the average proportions of falsely assigned objects
are near zero. This indicates that both algorithms are able to
correctly detect all the four artiﬁcial biclusters present in the data.

 

2094

112 /810'sieurnofproarxosor1eu1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse SVD incorporating stability selection

 

 

 

 

 

 

 

 

 

 ‘— - - I - 9 9 - i v I, g T  ‘— - - - - 9 v - i ,I g T
T T T TT T T! It T T T T .T T! .t
I I I I T's o I I I I T'8 °
I 1 I
I - l o I -
T—I III'“§i° T—I III*"‘§i°
I .
o : I I I : I TEI T o : I I I : Tl Tgl T
I I I I T
a : T : 1 :1 T I I“: i! T a : T l l 1 T T F: -! T
g“ g _ 01. o . I o J. I I 8 ' LL:  _ J. o J. l I l -:- i
E : u : T T E a : T
m o a I l l : In I
I l
“I _ a I I I ' N — 1
o o I,
l 1 : I I a
1 ° 0
o _ ﬁ 1'.- l- o — 0 a- l-
I I I I I I I I I I I I I I I I I I I I
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
I ISA
0' 0' I SSVD
I S4VD
(on, (d) .N.
3 3 I
\— ‘_ I
o' — cs — :
. T :
El: E: o 8 o o : T I T I
" B o I
9: In T Q LO 0 0° g 3° :o l I : I
> O. - T I > O. - T T 0T °T °T I :T I
o I, T .I. T I I O I I °I °I 8I I
0 0 0 9 T 0
8 o  I I ' I I I 0| 0I aI I I
I °: SI 2? :I  i I | : : : z: : T : °
' ' ' I I' ' ' I I I I II I I o
I I o o o oo
0. IT. I. «1.121-; o-  - - --1--:--
I I I I I I I I I I I I I I I I I I I I
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0' 0'

Fig. 2. Simulation results of the ﬁrst scenario. The relevance score M (G, F), recovery score M (F , G) and the average proportions of falsely assigned rows
V1(G, F) and columns VJ(G, F) are described in the Supplementary Material. The boxplots show the distribution of these validation indices with respect to
the 100 simulated datasets. 0 indicates the considered noise level. (a) Relevance; (b) recovery; (c) falsely selected rows; ((1) falsely selected columns.

(a)-_ -..H (ble— q”

 

 

 

 

 

 

 

 

, TI- ' T o 'I'
T
a II- 9 'I-
a r . I i : . I I I . 1
ID I LO I
I‘. - I I I I ; T a - I‘. - I I T E
O I °° I I. I O I ,I, .I.
I I I, l o T I T I I I I, T
C J. I : I o I T a ‘- I I I
- I
g g — I 1' $ 1 I L\L;  — : I I o I o
2 0 ° I I o 1 E l S J. T f
1. 8 I I I I o 1- 8 T 3
I I I I 8 § ‘ 1 T
a) _ J- | I T ‘Li‘) _ é o T T E
o' l i i 1i *7 'I I d I i i i T g ! T I.
I: 5 I I ‘- 1 I I’ i 1. I 5 i o
e 0
O — 0 Q A 1.0 Q — o O I 1.0
I I I I I I I I I I I I I I I I I I I I
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
I ISA
6 o I SSVD
I S4VD
(0)10 (d) e
a — e —
c5 c5
3- — 3- —
"'2 “Z i
91 Q, T I
>— 8. — >‘ 8. — T T . :
o o T ' '
T O I I I
T T T I 3 I ' I
o T I ' ' I § 2 a I
T T a i I I I I ° I °7 5 I o 8 I
T T ° T T
°. 9 ‘- 1' o 8 o 0o
c, _ -T. J. .1. at- .4. i. ... *1; .i ..i o _ -1. -1. .1. Ha .1. it i- a- Ii- Ii-
I I I I I I I I I I I I I I I I I I I I
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
6 0'

Fig. 3. Simulation results of the second scenario. The relevance score M (G, F), recovery score M (F , G) and the average proportions of falsely assigned rows
V1(G, F) and columns VJ(G, F) are described in the Supplementary Material. The boxplots show the distribution of these validation indices with respect to
the 100 simulated datasets. 0 indicates the considered noise level. (a) Relevance; (b) recovery; (c) falsely selected rows; ((1) falsely selected columns.

 

2095

112 /3JO'S[BUJn0IpJOJXO'SOI]BUIJOJUIOIQ//Zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

M.SiII et al.

 

The Plaid Model algorithm in some cases perfectly revealed the
hidden structure, but in other situations depending on the randomly
chosen starting values and the noise level, the algorithm falsely
assigns rows and columns to the biclusters. The stopping criterion
of the algorithm depends on a permutation test that can fail to reject
unimportant biclusters that correspond to noise. On the other hand
for higher noise levels, the permutation test also tends to reject
biclusters early in the ﬁtting process so that only three or less
biclusters are detected. Thus, the resulting relevance and recovery
scores are highly variable and decrease with noise. Regarding low
noise levels, the SSVD algorithm mostly identiﬁes the correct
biclusters but usually falsely assigns some additional rows and
columns. This behavior maintains for higher noise levels, but
additionally the number of correctly identiﬁed biclusters becomes
less. For noise levels above 0.7, both the SSVD algorithm and the
Plaid Model mostly do not detect any of the artiﬁcial biclusters and
hence the average proportions of falsely assigned rows and columns
approach zero. The performance of the ISA decreases due to an
increasing number of identiﬁed irrelevant biclusters, starting with
noise levels above 0.2. For noise level 0.5, the medians of both
similarity scores are around 0.5, and the relevance scores show
a high variability. For noise levels above 0.5, the two embedded
biclusters generated to have a constant up— and downregulation of
0.5 and —0.5 are masked by noise, and hence, the ISA as well
as the S4VD algorithm tend to miss these clusters. This results
in a slight increase of their relevance scores while the recovery
scores decrease. Moreover, the relevance scores for both algorithms
show a high variability at noise level 0.6. In summary, the S4VD
algorithm outperforms all other biclustering algorithms considered
in the simulation study regarding the relevance and the recovery
of the artiﬁcial biclusters for all simulation scenarios. Furthermore,
due to the stability selection the S4VD algorithm rarely assigns false
rows and columns to the proposed bicluster and does not detect any
additional nonsense clusters. Thus for all simulation scenarios, the
average proportions of falsely assigned rows and columns are close
to zero.

4 DISCUSSION AND CONCLUSION

In this article, we propose a new biclustering algorithm that
combines the SSVD algorithm suggested by Lee et al. (2010) with
the stability selection of Meinshausen and Buhlmann (2010). In
brief, the model selection—based parameter tuning of the penalized
regression models of the SSVD algorithm is replaced by a
subsampling—based variable selection that controls Type I error
rates. The S4VD approach here presented allows to control the
degree of sparsity of the resulting SVD—layers by choosing desired
Type I error levels. The stability selection estimates the selection
probabilities of the rows and columns to belong to a bicluster.
Depending on the chosen Type I error levels, the results are
robust biclusters represented by rows and columns that have high
selection probabilities. If the noise level is getting too high, the
stopping criterion leads to an interruption of the S4VD algorithm
preventing from ﬁtting additional SVD—layers that correspond to
noise. So far, the S4VD method is the only biclustering approach
that takes the cluster stability regarding perturbations of the
data into account. We applied the S4VD algorithm to evaluate
a lung cancer microarray dataset and showed that the resulting
biclusters represent tumor subclasses together with coregulated

genes. Marker genes for the different tumor subclasses showed
high selection probabilities in the respective biclusters. In addition,
a gene set enrichment analysis revealed that the genes associated
with identiﬁed biclusters belong to signiﬁcantly enriched cancer—
related GO terms. In a simulation study, the S4VD algorithm was
compared with the SSVD algorithm, the improved Plaid Model
(Turner et al., 2005) and the ISA (Bergmann, 2003). The S4VD
algorithm showed the best performance regarding the recovery of
biclusters and was more robust to noisy data compared with the
other methods. The subsampling steps of the stability selection make
the S4VD algorithm computationally very demanding. However, an
improvement that strongly reduces the computation time is presented
in the Supplementary Material.

Conﬂict of Interest: none declared.

REFERENCES

Alexa,A. et al. (2006) Improved scoring of functional groups from gene expression data
by decorrelating GO graph structure. Bioinformatics, 22, 1600—1607.

Ben-Dor,A. et al. (2003) Discovering local structure in gene expression data: the order-
preserving submatrix problem. J. Comput. Biol, 10, 373—384.

Bergmann,S. et al. (2003) Iterative signature algorithm for the analysis of large-scale
gene expression data. Phys. Rev. E. Stat. Nonlin. Soft. Matter Phys, 67(3 Pt 1),
031902.

Bhattacharya,A. and K De,R. (2009) Bi-correlation clustering algorithm for determining
a set of co-regulated genes. Bioinformatics, 25, 2795—2801.

Bhattacharjee,A. et al. (2001) Classiﬁcation of human lung carcinomas by mRNA
expression proﬁling reveals distinct adenocarcinoma subclasses. Proc. Natl Acad.
Sci. USA, 98, 13790—13795.

Busygin,S. et al. (2008) Biclustering in data mining. Comput. Oper. Res., 35,
2964—2987.

Carmona—Saez,P. et al. (2006) Biclustering of gene expression data by non-smooth
non-negative matrix factorization. BMC Bioinformatics, 7, 78.

Cheng,Y. and Church,G.M. (2000) Biclustering of expression data. Proc. Int. Conf.
Intell. Syst. Mol. Biol, 8, 93—103.

Csardi,G. et al. (2010) Modular analysis of gene expression data with r. Bioinformatics,
26, 1376—1377.

Dudoit,S. et al. (2003) Multiple hypothesis testing in microarray experiments. Stat. Sci.,
18, 71—103.

Eckart,C. and Young,G. (1936) The approximation of one matrix by another of lower
rank. Psychometrika, 1, 211—218.

Getz,G. et al. (2000) Coupled two-way clustering analysis of gene microarray data.
Proc. Natl Acad. Sci. USA, 97, 12079—12084.

Hartigan,J.A. (1972) Direct clustering of a data matrix. J. Am. Stat. Assoc., 67,
123—129.

Hochreiter,S. et al. (2010) Fabia: factor analysis for bicluster acquisition.
Bioinformatics, 26, 1520—1527.

Lazzeroni,L. and Owen,A. (2000) Plaid models for gene expression data. Stat. Sin., 12,
61—86.

Lee,M. et al. (2010) Biclustering via sparse singular value decomposition. Biometrics,
66, 1087—1095.

Madeira,S.C. and Oliveira,A.L. (2004) Biclustering algorithms for biological data
analysis: a survey. IEEE/ACM Trans. Comput. Biol. Bioinformatics, 1, 24—45.
Meinshausen,N. and Biihlmann,P. (2010) Stability selection. J. R. Stat. Soc. Ser. B Stat.

Methodol, 72, 417—473.

Monti,S. et al. (2003) Consensus clustering: a resampling-based method for class
discovery and visualization of gene expression microarray data. Mach. Learn, 52,
91—1 18.

Prelic,A. et al. (2006) A systematic comparison and evaluation of biclustering methods
for gene expression data. Bioinformatics, 22, 1122—1129.

Santamaria,R. et al. (2007) Methods to bicluster validation and comparison
in microarray data. In Proceedings of the 8th International Conference on
IntelligentData Engineering and Automated Learning, IDEAL’ 07. Springer, Berlin,
Heidelberg, pp. 780—789.

Suzuki,R. and Shimodaira,H. (2006) Pvclust: an r package for assessing the uncertainty
in hierarchical clustering. Bioinformatics, 22, 1540—1542.

 

2096

112 /§.IO'SIBU.IHOIP.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; popnommoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse SVD incorporating stability selection

 

Tanay,A. et al. (2004) Revealing modularity and organization in the yeast molecular
network by integrated analysis of highly heterogeneous genomewide data. Proc.
Natl Acad. Sci. USA, 101, 2981—2986.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. R. Stat. Soc.
Ser. B Methodol, 58, 267—288.

Turner,H.L. et al. (2005) Biclustering models for structured microarray data. IEEE/ACM
Trans. Comput. Biol. Bioinform., 2, 316—329.

Van Mechelen,l. et al. (2004) Two-mode clustering methods: a structured overview.

Stat. Methods Med. Res., 13, 363—394.

Zou,H. (2006) The adaptive lasso and its oracle properties. J. Am. Stat. Assoc., 101,

1418—1429.

 

2097

112 /§.IO'SIBU.IHOIP.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; popnommoq

9IOZ ‘09 lsnﬁnv uo ::

