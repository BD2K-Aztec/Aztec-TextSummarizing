Bioinformatics, 31 (1 9), 2015, 3172—3180

doi: 10.1093/bioinformatics/btv349

Advance Access Publication Date: 4 June 2015
Original Paper

 

Systems biology

CCLasso: correlation inference for
compositional data through Lasso

Huaying Fang1'2'3, Chengcheng Huang‘, Hongyu Zhao5 and
Minghua Deng1'3'6'*

1LMAN, School of Mathematical Sciences,2Beijing International Center for Mathematical Research, 3Center for
Quantitative Biology, Academy for Advanced Interdisciplinary Studies, Peking University, Beijing 100871, China,
4College of Global Change and Earth System Science, Beijing Normal University, Beijing 100875, China,
5Department of Biostatistics, Yale School of Public Health, New Haven, CT 06510, USA and 6Center for Statistical
Science, Peking University, Beijing 100871, China

*To whom correspondence should be addressed.
Associate Editor: Igor Jurisica

Received on January 13, 2015; revised on May 28, 2015; accepted on May 29, 2015

Abstract

Motivation: Direct analysis of microbial communities in the environment and human body has
become more convenient and reliable owing to the advancements of high-throughput sequencing
techniques for 165 rRNA gene profiling. Inferring the correlation relationship among members of
microbial communities is of fundamental importance for genomic survey study. Traditional
Pearson correlation analysis treating the observed data as absolute abundances of the microbes
may lead to spurious results because the data only represent relative abundances. Special care
and appropriate methods are required prior to correlation analysis for these compositional data.
Results: In this article, we first discuss the correlation definition of latent variables for compos-
itional data. We then propose a novel method called CCLasso based on least squares with 61 pen-
alty to infer the correlation network for latent variables of compositional data from metagenomic
data. An effective alternating direction algorithm from augmented Lagrangian method is used to
solve the optimization problem. The simulation results show that CCLasso outperforms existing
methods, e.g. SparCC, in edge recovery for compositional data. It also compares well with SparCC
in estimating correlation network of microbe species from the Human Microbiome Project.
Availability and implementation: CCLasso is open source and freely available from https://github.
com/huayingfang/CCLasso under GNU LGPL v3.

Contact: dengmh@pku.edu.cn

Supplementary information: Supplementary data are available at Bioinformatics online.

 

 

1 IntrOduct'on human life on our food, health and medicine (Gill et al., 2006). The

Microbes play an important role in the environment and human life.
Bacteria and archaea have been found in extreme conditions such as
deep sea vents with high temperatures and rocks of boreholes
beneath the Earth’s surface (Pikuta et al., 2007). The microorgan-
isms affect environments where they exist, and vice versa. It is esti-
mated that there are about 10 times microbe cells inhabiting our
human body than human cells (Savage, 1977). Microbes affect the

way in which microbes affect the human health remains largely
unknown. Analysis of the human microbiome may help us better
understand our own genome.

The increasing quality and reducing cost of sequencing
technologies provide great opportunity to analyze the microbe com-
munities through sequencing. This represents a great improvement
over traditional microbe studies which are hindered by several

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3172

9mg ‘09 isnﬁnV uo sejeﬁuV socl ‘erulomeg JO AirSJeAru [1 112 [3.10811211an[plOJXO'SODBIILIOJIIlOIQ/ﬂ(11111 wort pepeolumoq

CClasso

3173

 

limiting factors. First, only a small proportion of microbes can be
cultured under laboratory conditions. Second, only single microbe
can be studied in laboratories but it is well known that most
microbes need other microbes to survive. In contrast, sequencing
technologies allow researchers to collect information from the whole
genomes of all microbes in a community directly from their nat-
ural environment, facilitating mixed genomic surveys (Handelsman
etal., 1998).

When data are available across many communities, the depend-
encies among microbes, which can be measured by correlations,
may provide important clues on the interactions among microbes.
However, one unique feature of sequencing-based survey data is
that they only provide relative abundances of different microbes in a
community because the sequencing results are a function of sequenc-
ing depth and the biological sample size (Ni et al., 2013). Therefore,
metagenomic data collected from mixed genomic survey studies
belong to the so-called class of compositional data in statistics. It
was pointed out by Pearson (1897) more than one century ago that
correlation analysis method designed for absolute values could lead
to spurious correlations for compositional data. Great attention and
specialized methods are needed to appropriately analyze and inter-
pret compositional data. Filzmoser and Hron (2009) proposed a
procedure based on balances to measure correlations for compos-
itional data, but the groups defined by the balances cannot always
be clearly defined and separated from each other. Faust et al. (2012)
proposed CCREPE based on permutation and bootstrap to infer the
correlated significance but it’s difficult to explain the difference
between the permutation and bootstrap samples. Friedman and Alm
(2012) introduced correlation concepts of latent variables based on
log-ratio transformation of compositional data and proposed an
approximation method called SparCC to infer the correlation matrix
under sparse assumption. But SparCC does not consider the influ-
ence of errors in compositional data which may reduce the estima-
tion accuracy. In addition, there is no guarantee that the inferred
covariance matrix from SparCC is positive definite and even the cor-
relation coefficients may fall outside [—1, 1].

In this article, we propose a novel method based on least squares
with £1 penalty after log ratio transformation for raw compositional
data to infer the correlations among microbes through a latent vari-
able model, called Correlation inference for Compositional data
through Lasso (CCLasso). Similar to SparCC, CCLasso explicitly
considers the compositional nature of the metagenomic data in cor-
relation analysis, and it has the additional benefit that the estimated
correlation matrix of the latent variables for compositional data is
positive definite. We also propose an efficient alternating direction
algorithm of augmented Lagrangian method to solve the optimiza-
tion problem involved in our method. The tuning parameter that
balances the loss function and sparse assumption is chosen through
cross validation.

The performance of CCLasso is compared with SparCC through
simulation studies, using several correlation network structures and
sample sizes. The simulation results show that CCLasso gives more
accurate estimation for correlation matrix than SparCC as well as bet-
ter edge recovery. When CCLasso and SparCC are applied to estimate
the correlation networks of microbes from Human Microbiome
Project (HMP), we find that CCLasso is comparable with SparCC in
view of consistent accuracy and reproducibility. But for shufﬂed HMP
datasets there are supposed to be no correlations for any species,
SparCC always results some small correlations while CCLasso shrinks
these small values into 0. We believe that CCLasso can be applied to
study correlations of compositional data arising from metagenomic
data in natural environment and human body, and it is also broadly

applicable in many other contexts where there is interest to assess cor-
relations of variables from compositional data.

2 Methods

2.1 Correlation of latent variables for compositional

data

Suppose there are p microbe species and their absolute abundances
are random vector y = (y1, . . . , yp) which cannot be directly
observed in practice. Instead, only the compositional random vector
x = (x1,  ,xp),

x,- =—,,” , <1)

ZE:Yk

k=1

can be observed from biological experiments. The absolute abun-
dances y are called latent variables since they cannot be directly
observed. The additive log normal distribution (Aitchison and Shen,
1980) is a special case for Equation (1) when y is from a multivariate
logarithm normal distribution. The relations among y are of more
relevance than x’s in both practice and theory. The interactions
among microbe species are described by y while there is a negative
correlation trend for compositional vector x from the constant sum
constraint even in the absence of any correlations among y,

P
Zxk = 1 => 2 Cov(x,—,xk) = —Var(x,~).
k=1 k¢i

P
Let w = Z yk be the total absolute abundance for microbe species.
k=1
Covariances between the latent absolute abundance 3), which cannot
be observed, and those of its compositional representation x, which
are observed, can be related through the base equation (1),

Cov(ln x,-, ln xi) = Cov(ln 3),, ln 3),) — Cov(ln 3),, ln w)
—Cov(ln w, ln 3),) + Var(ln w),

since ln x,- = ln )1,- — ln w. Let 21H x = Var(ln x), 21,, y = Var(ln y)
and a = Cov(ln y,ln w) — Var(ln w)1p/2 where 11, is a p X 1 vector
of 1's, then the matrix form of connection between 21,, x and 21,, y
can be described as

21,, x = 21,, y — a1T — 1f. (2)

We can focus on the correlation among log transforms of y and we
also call ln y latent variables. When x is from additive logistic nor-
mal, the independences among ln y are equivalent to y. Since there is
information loss from y to x through normalization procedure
(Equation 1), the problem of estimating 21,, y from the sample esti-
mation of 21,, x is undefined without any assumptions. This can be
easily seen from Equation (2) that there are p(p + 1) /2 equations
but p(p + 1) / 2 + p unknown parameters.

One way to get around this problem is to assume that 21,, y is
sparse which means that the interaction network among microbe spe-
cies has a small proportion of all possible edges present compared to
the fully connected network. Sparse structure is a very common as-
sumption for under-determinated problems such as linear regression
models (Tibshirani, 1996), Gaussian graphical models (Yuan and Lin,
2007) and compressed sensing (Candes and Tao, 2005) where the
number of unknown parameters is larger, sometimes much larger,
than the number of data points. For compositional data, there may
exist several sparse networks corresponding to the same 21,, x because

9mg ‘09 isnﬁnV uo sejeﬁuV socl ‘erulomeg JO AirSJeAru [1 112 [3.10811211an[plOJXO'SODBIILIOJIIIOIQ/ﬂ(11111 wort pepeolumoq

3174

H.Fang et al.

 

y and its scaled form C(y)y (C(y) is any arbitrary positive random
variable which is a scaling factor) cannot be distinguished from the
base equation (1) if both 21,, y and 21,, (COW) are sparse. The sparse
level of 21,, y is the key because there is at most one sparse network
21,, 3, whose edge density is no greater than % — lﬁ corresponding to
the same 21,, x. And this sparse density condition cannot be relaxed
(See Supplementary Material). There are very few statistical methods
available to investigate the correlation among the latent variables ln y
with the exception of some recently introduced methods, e.g. SparCC
(Friedman and Alm, 2012).

To remove a in the right hand of Equation (2), we can choose a
matrix F with Rank(F) = p — 1 and F11, = 0 and multiple F on both

sides in Equation (2),
F21n xFT = F21n ,FT — FallfFT — FlagFT = F21n ,FT. (3)

The left hand of Equation (3) is the variance of Fln x and the right
corresponding Fln y. And their relationship can be seen as

Fln x = F(ln y — lpln w) = Fln y.

The above relation can explain the two constraints for F. Rank(F)
= p — 1 ensures that there is a 1 — 1 correspondence between x and
Fln x since there is the constant sum constraint for x. So there is
no loss of information in statistical inference from Fln x instead of
x. F11, = 0 helps to cancel the common denominator w after log
transformation. There are many such transformation matrices sat-
isfying the two constraints, e.g. F = (Ep_1, —1p_1) is the linear trans-
formation for additive log ratio where the reference variables is xp
and F 2 E1, — 11,117; /p for the centered log ratio for compositional
data where E), is a p X p identity matrix (Aitchison, 1982).

Let 2 = 21,, y = [oil-ijib. The sample version S of 21,, x can be
obtained after the fraction estimation from raw data such as metage-
nomic data through the Bayesian pseudo count method (Agresti and
Hitchcock, 2005). From Equation (3) and the sample estimation
S for 21,, x, we can get the following estimation equation,

FzFT = FSFT. (4)

Since Rank(F) = p — 1 and 2 is a p X p positive definite matrix,
2 cannot be directly estimated through Equation (3). The additional
sparse assumption for 2 is reasonable in many application contexts,
such as metagenomic data, since most of variable pairs are not
expected to be correlated when the number of components is large.
Therefore, we can impose some sparsity constraints to help model
and infer 2 without other prior information.

2.2 SparCC and its limitations
Friedman and Alm (2012) proposed an iterative approximation ap-
proach called SparCC to solve the estimation equation (4) for a
number of special forms of transformation matrix F. In short,
SparCC first obtains a rough estimation for variance of latent vari-
able ln y,- and the corresponding correlation matrix. Then it uses a
threshold to remove the most correlated pair and repeatedly esti-
mates the variances and correlations until some terminating condi-
tions are met.

Under the above notation, SparCC’s algorithm can be summar-
ized as follows. First, SparCC obtains an estimation for the diagonal
of 2 from a rough approximation that

:0}; = 0, Vi. (5)
#i

The rough approximation (Equation 5) supplies additional

p equations for Equation (4). This assumption means that every com-
ponent has no correlations with others on average. Let F1 2 (—1p_1,
Ep_1) and 212 = (221)T = Cov(ln y1,ln y_1), 222 = Var(ln y_1)
where ln y_1 = (ln yz, . . . ,ln yp)T, then Equation (4) can be written
as follows,

(—1p—1’Ep—1)2(—1p—1’Ep—1)T = FISF1T

=> 11.40111;1 — 1,4221 — 2121,:1 + 222 = FlSFlT.

Computing the trace for both sides of above equation, we have

P P
(p — 1)O‘11 -)- 20'1"; — 22 0'1) 2 tr(F1SF}").
i=2 —2

P P
If :01): 0, then (p — 1)a11 + Z 0,,- = tr(F1SFf). Let F,-, i = 2,
i=2 i=2

. ,p be the additive log ratio transformation matrix where the x,- is
the reference variable, for example, F1, 2 (E, —1p_1). Then similar to
F1, we have (1) — 1)o,-,- + 2 Ci,- 2 tr(F,-SFZT) for i: 2,  ,p from

#i

the assumption (Equation 5). The corresponding solution is

_ 1 . T 1 p . T
0,,- _ mar-(F313. ) — m  tr(F,SFi )), (6)

since (E1, +%1p1;) 1 2 (E1, —ﬁ1p1;). Then the basic coeffi-
cients can be obtained after substituting Equation (6) into Equation
(4). In fact, the above procedure is just a way to solve Equation (4)
and Equation (5). This is called basic SparCC in Friedman and Alm
(2012). A potential problem for SparCC is that 17,-,- in Equation (6)
can be negative, and so a minimal value Vmin is required to replace
negative O'ii. Second, SparCC employs an iterative refinement scheme
through excluding the strongest correlated pair if the corresponding
magnitude exceeds a given threshold or. The 17,-,- is updated through
removing the most significant correlation pair based on another
assumption like Equation (5 ),

2017' = 0, (7)

HQ

where C,- denotes the set of indices of ln 3)) identified to be strongly
correlated with ln 3),. Finally, SparCC repeats the former two steps to
update the variance 17,-,- and the correlation matrix of ln )1 through the
threshold or for a given iteration time or until no new strongly corre-
lated pair is identified or only three components left. And SparCC se-
lects a correlation threshold to give an interaction network.

As far as we are aware, SparCC is the first method to infer the cor-
relations among latent variables ln y for compositional data. Its se-
cond step is an effective method to remove the strong assumption
(Equation 5) in the first approximation step and Equation (7) is
approximately right after removing the strongest pairs. Although it
represents a significant advance in analyzing compositional data,
SparCC has some limitations in the approximations. First, SparCC
directly solves Equation (4) with a series of approximate assumptions,
and the accuracy of Equation (4) is inﬂuenced by the errors resulting
from these approximations. Second, there is no consideration for the
overall property of the estimated correlation matrix. SparCC cannot
guarantee the inferred correlation matrix to be positive definite and
even the estimated correlations may fall out of [—1, 1].

2.3 CCLasso
We first note that the vectorization version of Equation (3)
together with sample variance S is a = (F®F)vec(2—S), where

9mg ‘09 1sn8nV uo sejeﬁuV soc) ‘121u101n123 10 A1rSJeAruf1 112 /810'S{12umo[p101x0'831112u1101urorq/ﬁd11q 111011 pepeolumoq

CCLasso

3175

 

8 satisfies E(a) = 0 and Var(a) = (F (X) F)Var(vec(S))(FT (8) FT). Let
VS 2 Var(vec(S)), then an inverse variance weighted loss function
can be given as follows,

LOSSIQ) : $01642 _ S))T(FT ® FT)((F ® F)VS(FT ® FT))—1 (8)
(F e F)(ve<:(2 — 5)),

where the inverse symbol M‘1 is the Moore-Penrose pseudo
inverse of M (Penrose and Todd, 1955). The solution to minimize
LOSSl(E) in Equation (8) satisfies the estimation equation (4). One
important property of the loss function (Equation 8) is that it is
invariant for any choice of the linear transformation matrix F. This
property results from the fact that the information for E in the ori-
ginal data is kept after log ratio transformation.

The loss function LOSS1 (E) in Equation (8) is too complex to be
handle for the high-dimensional covariance matrix VS (p2 X 122).
Inspired by Zhang and Zou (2012) for variance approximation of
sample variance, we can use the following loss function to substitute
Equation (8),

LOSSl’(E) = imam — S)FT)(FSFT)_1(F(E — S)FT)). (9)

The transformation matrix F in Equation (9) should be chosen rea-
sonably. Considering the symmetry of components, let
F0 2 Ep — % 11,117; be the transformation matrix of centered log ratio
with symmetric projection property F3 2 F0, F3; 2 F0. It is sug-
gested that treating the weighting covariance matrix in loss func-
tions as diagonal performs well in some high-dimensional problems
(Chen et al., 2013). Let V = (diag(F0SF(7;))_l. We may consider an-
other substitute for loss function,

LOSS(2) = gamma — S)F0T)V(Fo(2 — S)F0T))
(10)
=§IIF0<2 — S)Folliz-

The diagonal matrix V can be seen as a standardization matrix for
F0 (E — S)F0. The key idea of our method is that we use the loss func-
tion (Equation 10) because of its simplicity.

A reasonable approach to incorporating the sparse assumption
for E is to minimize loss function plus a suitable penalty. An ideal
penalty function is the number of non-zero elements in E‘ which is
the off diagonal of E. But it is computationally intractable where the
optimization involving ||E_ | |0 is a combinatorial optimization prob-
lem with an exponential complexity. A commonly used approach is
to replace Eo-norm by El-norm (Tibshirani, 1996; Yuan and Lin,
2007). We consider the following objective function combining loss
function and £1 penalty,

f<2> = Loss<2> +PEN<2> =§||Fo<2—S>Folrv +lnllz‘lll, (11)

where PEN(E) = 2,,||E‘ | |1. The tuning parameter 2,,20 in
Equation (1 1) is used to balance the fit of model (3) and the sparsity
assumption of E. CCLasso aims to find a positive definite matrix E
so that

A _ _ 1 _
E = arg min  = arg min —||F0(E —S)Fo||%, +2n||E H1, (12)
2>0 2>0 2

where E > 0 means E should be positive definite. The corresponding
correlation matrix estimation can be derived from standardizing the
diagonal elements of E. The optimization problem involved in
Equation (12) is convex since both the objective function f (E) and

the constraint region {E|E > 0} are convex. So the local minimiza-
tion of Equation (12) is global.

Compared with SparCC, CCLasso explicitly considers the error
terms behind the estimation equation (4) through the loss function
(Equation 10). The sparse assumption is directly handled through an
additional Zl-type penalty function in contrast to the additional
assumption (Equation 7) for SparCC. The estimated correlation
matrix from Equation (12) is positive definite and its elements are
located in [—1, 1] from the positive definite restriction.

2.4 Optimization algorithm and choice of 2,,

We develop an efficient algorithm based on the alternating direction
method to solve the constrained optimization problem in CCLasso
(Zhang and Zou, 2012). A relaxed version for Equation (12) can be
obtained after removing the positive definite constraint,

~ _ 1 _
2 = argmm EIIFo<z—S>FOIIZV +2112 II1- <13)
2=2T

If the solution E in Equation (13) is positive definite, E = E.
Otherwise the nearest positive definite matrix to E is used as E.

To derive an alternating direction method for Equation (13),
we introduce a new matrix E1 and rewrite Equation (13) as follows,

~ ~ _ 1 _
(2,21) = arg mm §||F0(2 — S>FoIIZV +2112. Ill-
2=2T,21=2

We consider the augmented Lagrangian function
1 _
L(E,21,A) = E ||Fo(2 — S)F0||i/ + 1n||21||1
+t1‘(1\(2 — 21)) + (1)/2)”2 — Elllis,

where  - F is the matrix Frobenius norm. Let (Ek, Elf, Ak) be the so-
lution at step k, we update (E, E1, A) according to

EkJr1 = arg min L(E,E’i‘,Ak), (14)
2=2T
Elf“ = arg min L(Ek+1,E1,Ak), (15)

21

and Ak+1 = Ak + p(Ek+1 — Elf“). Let Ek+1 = S + Ak+1 for (14), we
can write

_ 1
Ak+1 = arg min §||F0AFolliz+(p/2)||All12:
A=AT
+ tr(A(Ak + p(S — 21‘)»-

The above objective function on the right is quadratic for A and

Ale“ is the solution of the following equation,

1
5 (FOVFOAFO + FoAFoVFo) + pA = —(A’< + p(S — 21))-

Let F0 2 UDo UT, (UTVU)11/2p + Ep_1/2 = UODUE; (the subscript
11 means removing the last row and column) be the eigenvalue
decomposition for the corresponding matrix and
M = —UT(Ak + p(S — E'f))U/ p, then the solution for the above
equation is

U0{(U3M11U0) - C}Ug M12

Ak-I-l : 
M21 M22

)UT, (16)

where ° is the Hadamard product of matrices and Ci,- 2 ﬂ.
Elie” 11 II

To update , we define an operator G(A, 2) as follows,

9mg ‘09 1sn8nV uo sejeﬁuV socl ‘121u101n123 10 A1rSJeAruf1 112 /810'S{12umo[p101x0'831112u1101urorq/ﬁd11q 111011 pepeolumoq

3176

H.Fang et al.

 

Aii i=1,
Aii—l 1.757., Ail->2,
A,,-+2 2'75), A,,-<—,1,
0, 2'75), —2gA,-,-g,1.

G(A,/1),]. =

From Equation (15 ), we write

Elf“ = argzmin (p/ 2)||21||12: — t1“(21(Ak + rim»,
1

then the solution of the above problem is Elf“ = G A7}: + Ek+1,%2.
The following algorithm summarizes the details to carry out t e
above alternating direction method to solve the optimization prob-
lem (Equation 12) for CCLasso.

1. Initialization: k = 0, A0, 25’ = E,.
2. Repeat (a)-(d) until Ek and Elf converge:
1. EkJr1 <— S + Ak+1 where Ak+1 is given in Equation (16);
2. 2?: <— G£A7k+ SHIP);
3. A + <— A +,o(2’<+1 — 21H);
4. k <— k + 1.
3. Return the converged Ek as the solution for E deﬁned in
Equation (13).

The tuning parameter 2,120 in Equation (11) has to be tuned
since it controls the balance between fitness of model (Equation 3)
and the sparsity assumption. A K-fold general cross validation of
loss function (Equation 10) is used to choose 2,, in this article. First,
all samples are divided into K disjoint subgroups as folds noted by lie
for k = 1,  ,K. These folds will be used as the training set and
testing set in turn. Second, for each k = 1,  ,K, compute Sk and
S_k corresponding to the sample estimation of Var(ln p) through 1k
and 11, . . . ,Ik_1, Ik+1, . . . ,IK. The subscript —k means using all sam-
ples with the k-th fold left out. The weight matrix V for both train-
ing data and testing are based on all data. Thirdly, let S = S_k and
compute the estimation E_k through Equation (12) for each
1SkSK. Then compute the mean of K-fold cross validated errors for
the tuning parameter 2",

1 K 1 .
cw”) — Z§||F0(E—k — Sk)Foll1z-
k=1

_ E
Finally, we choose 2:; = arg min ,(ﬂCV(/in) as the final tuning
parameter.

3 Results

3.1 Simulation studies
Though a goal of a genomic survey study is to infer the correlations
among members of microbe communities from the abundance count
matrix, the estimation accuracy of correlation matrix using either
CCLasso or SparCC can be compared to assess their relative perform-
ance since they are both based on the same latent assumptions described
in Equation (1). The essential difference between these two methods is
the estimation procedure after obtaining the fraction estimation.

The compositional data are simulated from the additive logistic

normal distribution with a given mean and covariance matrix,
1 N 2 — yi
my N (.u’ )2 xi — p—

29]..

12:1

The variation parameter of )1 controls the unbalance of components.

Every element of u is generated from a uniform distribution of
[—0.5, 0.5]. We focus on performance comparison between SparCC
and CCLasso on sparse correlation matrix with varying levels of
sparsity in our simulations. Five covariance structures are
considered:

1. Random Model: Every pair of components is connected with a
given probability 0.3 and the correlation strength is :0.15 with
equal probability 0.5.

2. Neighbor Model: Randomly select 12 points in the [0,1]2 plane.
Then connect the 10 nearest neighbors for each point with the
correlation strength 0.5 .

3. AR(4) Model: Connect pair (2', j) if |i — 7134, and set the correl-
ation strength as 0.4, 0.2, 0.2 or 0.1 as the distance is 1, 2, 3 or
4, respectively.

4. Hub Model: Randomly select 3 points as hubs and the other 12 —
3 points as common points. Then connect each hub to others
with a probability 0.7 while creating edges with probability 0.2
among the common points and all edge strength is set to be 0.2.

5. Block Model: Divide 12 points into 5 blocks equally. Connect
each pair in the same block with probability 0.6 and correlation
strength 0.4 while connecting points in different blocks with
probability 0.2 and correlation strength 0.2.

To make the covariance matrix positive definite, the diagonal
elements of E are set large enough and then normalized all as 1. The
random model is a very common graph model in which every pos-
sible edge occurs independently with the same probability. Through
setting the strength as :0.15 with equal probability, the random
model roughly satisfies assumptions Equations (5) and (7) of
SparCC. The neighbor model is a 2-dimensional geography model in
which edges exist among nearest neighbors. The AR(4) model can
be considered as a model where points are ordered linearly along a
line where edges exist between those nodes whose distance is no
more than 4, and the correlation decreases as the distance increases.
The hub model describes a graph where some special nodes, called
hubs, are connected to others with a higher probability than the con-
nection probabilities among other nodes. The block model defines
network clustering where edge probabilities are higher within
groups than between groups. All models are sparse with different de-
grees of sparsity. The expect number of edges in the neighbor and
AR(4) model is proportional to 12 while 122 for the random, hub and
block model.

For all the models, we set 12 = 50 and consider different sample
sizes 11 = 200, 300 and 500. For each model, and three combinations
of (p, n), we repeat simulations 100 times. The tuning parameter 2,,
is determined through 3-folds cross validation, and all data are used
to estimate E and the correlation matrix. We reimplemented
SparCC using R and the default tuning parameters a = 0.1,
kmaxz 10 and Vmin = 10‘4 are used while the final correlation is
truncated by —1 and 1 as the lower and upper limit. SparCC is ro-
bust for its tuning parameters since only the strongest pair is
removed in each iteration (Supplementary Fig. S1).

To compare the performance between CCLasso and SparCC for
each combination of model setting and sample size, we define the

correlation inference accuracy by the mean absolute error d103, p)

= $2 |f),-l- — pii| and the Frobenius norm distance dp(f),p) =
z</

Hf) — p||F between the estimated correlation matrix [3 and the true

one p. The area under the receiver operation characteristics curve

(AUC) is used to assess the performance of CCLasso and SparCC on

9mg ‘09 1sn8nV uo sejeﬁuV socl ‘121u10111123 10 A1rSJeAruf1 112 ﬁlO'SIBIIan[p.IOJXO'SODBIIHOJIIIOIQ/ﬂ(11111 111011 pepeolumoq

CCLasso

3177

 

recovering the non-zero entries in the sparse covariance matrix E to
avoid the threshold parameter selection.

Table 1 summarizes the performance of CCLasso and SparCC
for simulation studies in view of d1 and dF distances and AUC. As
the sample size increases from 200 to 500, both d1 and dF decrease
for both CCLasso and SparCC in each simulation setting. The esti-
mation errors of CCLasso are smaller than SparCC. And the corres-
ponding results suggest that CCLasso performs better than SparCC
in simulations. This may be due to the fact that CCLasso considers
random errors while SparCC does not. For edge recovery, CCLasso
also performs better than SparCC except for the random graph
model when sample size is 200 and 300. This can be explained by
the fact that the random model roughly satisfies assumptions
Equations (5 ) and (7) of SparCC. The accuracy and AUC are not
vey consistent such as d1 and dF for CCLasso is smaller than SparCC
but the AUC for SparCC is larger than CCLasso in the random mod-
el. This phenomenon’s reason is that the accuracy measures the con-
tinuous distance between the estimation and the true one while AUC
shows the discrimination between the non-zeros and zeros.

More detailed results for ROC are shown in Figure 1. As the sam-
ple size increases, the gap between CCLasso and SparCC increases.

Table 1. Performance comparisons of CCLasso and SparCC based
on simulation results

 

 

n Method d1 d1: AUC
Random Model
200 CCLasso 0.033(0.001) 2.954(0.049) 0.791(0.015)
SparCC 0.057(0.001) 3.528(0.080) 0.823(0.014)
300 CCLasso 0.028(0.001) 2.409(0.057) 0.885(0.012)
SparCC 0.047(0.001) 2.901(0.059) 0.891(0.011)
500 CCLasso 0.023(0.001) 1.994(0.053) 0.953(0.007)
SparCC 0.038(0.001) 2.332(0.056) 0.951(0.006)
Neighbor Model
200 CCLasso 0.039(0.003) 3.355(0.206) 0.948(0.015)
SparCC 0.076(0.001) 4.606(0.081) 0.888(0.014)
300 CCLasso 0.033(0.002) 2.675(0.151) 0.986(0.006)
SparCC 0.070(0.001) 4.176(0.060) 0.931(0.009)
500 CCLasso 0.026(0.002) 2.064(0.121) 0.999(0.001)
SparCC 0.065(0.001) 3.800(0.041) 0.967(0.006)
AR(4) Model
200 CCLasso 0.021(0.001) 2.444(0.134) 0.885(0.021)
SparCC 0.061(0.001) 3.766(0.087) 0.858(0.019)
300 CCLasso 0.018(0.001) 1.994(0.133) 0.922(0.017)
SparCC 0.052(0.001) 3.210(0.078) 0.890(0.017)
500 CCLasso 0.015(0.001) 1.549(0.087) 0.958(0.011)
SparCC 0.044(0.001) 2.693(0.059) 0.918(0.011)
Hub Model
200 CCLasso 0.037(0.001) 3.453(0.037) 0.749(0.021)
SparCC 0.067(0.001) 4.194(0.070) 0.690(0.014)
300 CCLasso 0.036(0.001) 3.133(0.047) 0.768(0.021)
SparCC 0.059(0.001) 3.686(0.049) 0.735(0.012)
500 CCLasso 0.032(0.001) 2.918(0.048) 0.828(0.018)
SparCC 0.051(0.001) 3.248(0.043) 0.788(0.010)
Block Model
200 CCLasso 0.039(0.001) 3.307(0.113) 0.782(0.014)
SparCC 0.070(0.001) 4.268(0.072) 0.734(0.010)
300 CCLasso 0.035(0.001) 2.773(0.079) 0.854(0.014)
SparCC 0.062(0.001) 3.788(0.052) 0.765(0.011)
500 CCLasso 0.029(0.001) 2.258(0.076) 0.924(0.011)
SparCC 0.057(0.001) 3.374(0.038) 0.796(0.012)

 

all and d1: are the two distances between the estimated correlation matrix

and the true one deﬁned in the text. AUC is the area under the receiver oper-
ation characteristics curve. The results are the averages over 100 simulation
runs with standard deviations in brackets.

For the low false-positive rate such as 0.1, the true-positive rate for
CCLasso is larger than SparCC except the random graph model. An
interesting phenomenon is that both CCLasso and SparCC perform
poorly for the hub model, but as sample size increases the estimation
efficiency improves. One should use a much larger sample size for
some special graphical structures such as the hub model and the block
model than others for given precision. We also compare CCREPE
with CCLasso through ROC and find the performance of CCREPE is
similar to SparCC (Supplementary Fig. S2).

3.2 HMP data

Because of close relationships between ourselves and the microbes in
our body, the Human Microbiome Project Consortium (2012a,b)
aims to investigate the fundamental roles of the microbes in human
health and disease. The high-quality sequencing reads in 16S vari-
able regions 3—5 (V35) of HMP healthy individuals are used to
explore the correlation interactions among the microbes in 18 body
sites and the corresponding operational taxonomic units (OTUs) are
obtained from the HMPOC dataset, available at http://www.
hmpdacc.org/HMMCPl. We consider Phase I production study
(May1, 2010) and the first sample collected for multiple samples
from the sample body site of the same individual. The data are fur-
ther filtered by removing samples with less than 500 reads or more
than 60% OS are collected and by removing OTUs that are repre-
sented by less than 2 reads per sample on average or more than 60%
0s. The transformation from counts to compositional data cannot
be directly normalized since both CCLasso and SparCC assume that
the OS in the OTU counts are not real 0 fractions. CCLasso adds all
counts by the maximum rounding error 0.5 and then normalizes the
counts to get compositional data. Friedman and Alm (2012) pro-
vided Bayesian framework to estimate the fractions from counts for
SparCC. The final estimation for SparCC is the median of estima-
tions in 20 replicated samples from the posterior distribution of
fractions.

Since there is no prior information for true correlation network
of taxon—taxon interaction in real data, we use consistent accuracy
and reproducibility to compare the performance of CCLasso and
SparCC. First, all data are used to construct a gold standard refer-
ence correlation matrix for CCLasso and SparCC. The estimated
correlation matrix in this step is treated as “known” since all data
are used. Second, we randomly select half samples to estimate the
correlation matrix through CCLasso and SparCC. The consistent ac-
curacy is measured by the Frobenius norm distance between the esti-
mated correlation matrices of the first and second step. The
consistent reproducibility is measured by the fraction of the same
edges shared for these two steps in the first gold reference network
which only the top 1/4 edges is used. This procedure is repeated 20
times for stable results.

The results are summarized in Table 2. CCLasso and SparCC
have similar performance in terms of consistent accuracy and repro-
ducibility. When the sample size is small, the reproducibility is low.
Even for large sample size such as left Antecubital fossa, the repro-
ducibility is only 0.64 for both CCLasso and SparCC. We can find
consistent accuracy and reproducibility are not good criteria from
the simulation data (Supplementary Table S1). Since there are sev-
eral optimization procedures for the cross validation of CCLasso,
SparCC is faster than CCLasso (Supplementary Table 52). The
reproducibility is robust for the top edges’ choice (Supplementary
Table S3). We also compare the inferred correlation network from
CCLasso and SparCC using all samples for all body sites and find
their results are very similar (Supplementary Fig. S3 and Table S4).

9mg ‘09 1sn8nV uo sejeﬁuV socl ‘121u10111123 10 A1rSJeAruf1 112 [BJO'SIBILIHO[p.IOJXO'SODBIIHOJIIIOIQ/ﬂ(11111 111011 pepeolumoq

3178 H.Fang et al.

 

 

 

 

     

 

 

 

 

 

 

 

 

 

 

 

 

 

 

     

 

 

 

 

 

 

 

 

 

 

 

 

     

 

 

 

 

 

 

 

 

 

 

 

 

     

 

 

 

 

 

 

 

 

 

 

 

 

     

Randomln=200 Randomln=300 Randomln=500
8 8 8
mm x. mm
.8 ° .8 ° .8 °
8: r: r:
8 8 8
n. 8 - n. 8 - n. 8 -
a.) a.) a.)
E E E
l— g I l— g I l— g I
— CCLasso — CCLasso — CCLasso
g _ - - SparCC g _ - - SparCC g _ - - SparCC
0'0 0'2 0i4 I I 0i6 I 110 0i0 I 0i4 I I 016 I 1i0 0i0 I 0i4 I I 0i6 I 1i0
False Posmve Rate False Posmve Rate False Posmve Rate
Neighborln=200 Neighborln=300 Neighborln=500
8 - 8 - 8 -
8 8 8
m GD m GD m GD
.8 ° .8 ° .8 °
8: r: :1:
8 8 8
n. 8 - n. 8 - n. 8 - I
a.) a.) a.) I
3 3 3
r: r: r: I
8 - 8 - I 8 - '
I I
I — CCLasso I — CCLasso — CCLasso
g _ - - SparCC g _ - - SparCC g _ - - SparCC
0i0 I 0i4 I I 0i6 I 1i0 0i0 I 0i4 I I 0i6 I 1i0 0i0 I 0i4 I I 0i6 I 1i0
False Posmve Rate False Posmve Rate False Posmve Rate
AR(4)In=200 AR(4)In=3oo AR(4)In=500
8 - 8 - 8 -
9. 9. 9.
co co co
m GD m GD m GD
.8 ° .8 ° .8 °
:1: :1: :1:
8 8 8
n. 8 - n. 8 - n. 8 -
a.) a.) a.)
3 3 3
1': 1': 1': '
— CCLasso — CCLasso — CCLasso
g _ - - SparCC g _ - - SparCC g _ - - SparCC
0i0 0'2 0i4 I I 0i6 I 1i0 0i0 I 0i4 I I 0i6 I 1i0 0i0 I 0i4 I I 0i6 I 1i0
False Posmve Rate False Posmve Rate False Posmve Rate
Hubln=200 Hubln=300 Hubln=500
8 8 8
m GD m GD m GD
:1: :1: :1:
8 8 8
n. 8 - n. 8 - n. 8 -
a.) a.) a.)
8 8 8
l—
— CCLasso — CCLasso — CCLasso
g _ - - SparCC g _ - - SparCC g _ - - SparCC
0i0 I 0i4 I I 0i6 I 1i0 0i0 I 0i4 I I 0i6 I 1i0 0i0 I 0i4 I I 0i6 I 1i0
False Posmve Rate False Posmve Rate False Posmve Rate
BlockIn=200 BlockIn=300 BlockIn=500
8 8 8
m GD m GD m GD
:1: :1: r:
8 8 8
n. 8 - n. 8 - n. 8 -
a.) a.) a.)
8 8 8
l—
8 8 8 I
— CCLasso I — CCLasso — CCLasso
g _ - - SparCC g _ - - SparCC g _ - - SparCC

 

 

 

 

 

 

 

 

 

1.0 0.0 0.4 I I 0.6 1.0 0.0 0.4 I I 0i6
False Posmve Rate False Posmve Rate

I I
1.0

Falsg PositiveoleRate

Fig. 1. ROC curves of CCLasso and SparCC. The true-positive rate is averaged over 100 replications after fixing the false-positive rate and the gray line is baseline
reference

9mg ‘09 1sn8nV uo sejeﬁuV socl ‘121u10111123 10 A1rSJeAruf1 112 [BJO'SIBILIHO[p.IOJXO'SODBIIHOJIIIOIQ/ﬂ(11111 111011 pepeolumoq

CCLasso

3179

 

Table 2. Consistent Frobenius accuracy and reproducibility for CCLasso and SparCC in different body sites from HMP data

 

 

Body Sample Frobenius Accuracy Reproducibility

Site Size CCLasso SparCC CCLasso SparCC
AntNar 152 2.28(0.17) 2.22 0.12) 0.70(0.05) 0.68(0.05)
AKerGin 193 1.71(0.13) 1.56 0.14) 0.75(0.04) 0.77(0.05)
BucMuc 196 2.47(0.17) 2.11 0.11) 0.71(0.03) 0.72(0.03)
HarPal 197 2.57(0.18) 2.24 0.13) 0.79(0.03) 0.80(0.03)
LAntFos 51 4.35(0.31) 6.57 0.51) 0.64(0.05) 0.64(0.04)
LRetCre 123 2.24(0.21) 2.30 0.15) 0.69(0.04) 0.66(0.04)
MidVag 45 2.69(0.48) 3.35 0.59) 0.64(0.08) 0.64(0.07)
PalTon 203 2.76(0.17) 2.31 0.17) 0.83(0.02) 0.83(0.02)
PosFor 22 3.34(0.68) 2.98 0.80) 0.67(0.14) 0.68(0.11)
RAntFos 54 3.31(0.37) 6.32 0.32) 0.54(0.03) 0.60(0.04)
RRetCre 85 2.64(0.17) 3.50 0.20) 0.63(0.04) 0.61(0.05)
Saliva 184 2.95(0.14) 2.75 0.13) 0.75(0.03) 0.77(0.03)
Stool 190 1.81(0.13) 2.10 0.16) 0.72(0.04) 0.71(0.03)
SubPla 205 2.89(0.23) 2.45 0.18) 0.82(0.02) 0.84(0.03)
SupPla 207 2.74(0.22) 2.31 0.12) 0.82(0.02) 0.85(0.02)
Throat 197 2.79(0.15) 2.48 0.14) 0.79(0.03) 0.80(0.02)
TonDor 207 2.52(0.22) 2.00 0.17) 0.83(0.02) 0.84(0.02)
VagInt 52 2.93(0.27) 2.94 0.21) 0.65(0.06) 0.63(0.05)

 

AntNar, anterior nares; AKerGin, attached keratinized gingiva; BucMuc, Buccal mucosa; HarPal, hard palate; LAntFos, left antecubital fossa; LRetCre, left ret-

roauricular crease; MidVag, mid vagina; PalTon, palatine tonsils; PosFor, posterior fornix; RAntFos, right antecubital fossa; RRetCre, right retroauricular crease;

SubPla, subgingival plaque; SupPla, supragingival plaque; TonDor, tongue dorsum; VagInt, vaginal introitus.

The results are the averages over 20 replication runs with standard deviations in brackets.

We also compare the performances between CCLasso and
SparCC through shuffled HMP data. The individual counts are per-
muted for each OTUs, so it is supposed not to find any correlations
among species. Figure 2 shows the histograms of estimated correl-
ations through CCLasso and SparCC for the shufﬂed datasets.
Almost none correlations are detected by CCLasso but there are
always some small correlations inferred from SparCC. In this way,
CCLasso outperform SparCC. We use CCLasso and SparCC for the
other dataset and find SparCC detects too many strong meaningless
edges (Supplementary Fig. S4).

4 Discussion

Although compositional data arise naturally in many practical prob-
lems, researchers are generally more interested in the latent variables
that underlie these data. For example, in genomic survey studies, it
is of great interest to infer the dependency among different bacteria
from the observed relative abundance, instead of the absolute abun-
dance, of the bacteria. Therefore, there is a need to infer the correl-
ation matrix among the latent variables for the compositional data.
In this article, we have proposed a novel method to infer the correl-
ations among the latent variables for compositional data. We use
the sparse assumption to help estimate the correlation matrix of
latent variables through solving the constant sum constraint prob-
lem. The simulation results show that CCLasso has better perform-
ance than SparCC, the only available method in the literature that
we are aware of that attempts to solve this problem from a latent
variable viewpoint. For the HMP data, CCLasso has similar consist-
ent accuracy and reproducibilities as SparCC. But from the shufﬂed
HMP datasets, we find that SparCC always gives some nonzero
estimations.

Though CCLasso performs better than SparCC in simulation
studies, it has similar difficulties as SparCC such as reliable compo-
nent fraction estimation and only linear relation explained. We
adopt the simple pseudo count 0.5 to avoid 0 components for HMP
datasets. There are other normalization methods that accounts for

 

 

 

 

 

 

 

 

 

 

 

 

 

 

[0,0.001) [0.001,0.1) (0.103) (031.1) (00001) [0.001,0.1) (0.10.3) (0.31.1) [0.001,0.1) (0.103) (031.1)
VagInt

I CCLasso

II Throat TonDor

E I CCLasso I CCLasso

o u SparCC n SparCC |:| SparCC
8 J] _I:I

8 _= _=

[0,0.001) [0.001,0.1) [0.1,0.3) [0.311) [0,0.001) [0.001,0.1) [0.1,0.3) [0.3,1.1) [0,0.001) [0.001,0.1) [0.1,0.3) [0.311)

AntNar AKerGin 8 BucMuc
§ I CCLasso § I CCLasso ‘° I CCLasso
El SparCC El SparCC 8 El SparCC
o _|:l a _I=I a _I=l
[0,0.001) [0.001,0.1) (0.103) (031.1) [0,0.001) [0.001,0.1) (0.10.3) (031.1) (00001) (0001,01) (0.103) (031.1)
8 HarPal LAntFos LRetCre
‘° I CCLasso I CCLasso § I CCLasso
§ El SparCC 8 J] El SparCC § 1 El SparCC
GD
o a _D a _|:I
[0,0.001) [0.001,0.1) (0.103) (031.1) [0,0.001) [0.001,0.1) (0.10.3) (031.1) (00001) (0001,01) (0.103) (031.1)
8 M‘dVag PalTon PosFor
N I CCLasso § I CCLasso g I CCLasso
8 n SparCC o n SparCC n SparCC
8
' JJ 41 8 3 j j
8 8 _I=I 8 _|:I
[0,0.001) [0.001,0.1) (0.103) (031.1) [0,0.001) [0.001,0.1) (0.10.3) (031.1) (00001) (0001,01) (0.103) (031.1)
a RAntFos RRetCre 8 Saliva
E I CCLasso § I CCLasso °° I CCLasso
8 El SparCC c j n SparCC 8 E El SparCC
a; 8 ‘t
8 j _l:l 8 _|:| 8 _|:I
[0,0.001) [0.001,0.1) (0.103) (031.1) [0,0.001) [0.001,0.1) (0.10.3) (031.1) (00001) (0001,01) (0.103) (031.1)
Stool 8 SubPla o SupPla
I CCLasso °° I CCLasso E I CCLasso
c 3 El SparCC 8 n SparCC a El SparCC
3 8 8
8 _|=l 8 _|=| 8 _|:I
[0,0.001)

0 200 500
0 150 300

 

 

 

Fig. 2. Histogram of estimated correlations through CCLasso and SparCC for
shuffled HMP datasets

under sampling such as Paulson et al. (2013) introduced a method-
ology to assess differential abundance in sparse high-throughput
microbial marker-gene survey data. Recently, Biswas et al. (2014)
proposed a Poisson-multivariate normal hierarchical model to learn
direct interactions removing confounding predictors’ effect from
metagenomics sequencing experiments. The assumption of Biswas et
al. (2014) is similar to CCLasso from the latent model view, but the
essential difference between them is the compositional assumption.
Future work will concentrate on improvement of the components
estimation accounting for under sampling and exploring nonlinear
relationships among microbes.

Funding

This work was supported by the National Natural Science
Foundation of China (Nos. 31171262, 31428012, 31471246),
and the National Key Basic Research Project of China

9mg ‘09 1sn8nV uo sejeﬁuV socl ‘121u10111123 10 A1rSJeAruf1 112 [BJO'SIBILIHO[p.IOJXO'SODBIIHOJIIIOIQ/ﬂ(11111 111011 pep1201umoq

3180

H.Fang et al.

 

(No. 2015CB910303). H.F.’s work was supported in part by a fel-
lowship from Graduate School of Peking University. H.Z.’s work
was supported in part by NIH GMS 95 07.

Conﬂict of Interest: none declared.

References

Agresti,A. and Hitchcock,D.B. (2005). Bayesian inference for categorical data
analysis. Stat. Method Appl., 14, 297—330.

Aitchison,]. (1982). The statistical analysis of compositional data. ]. R. Stat.
Soc. B, 44, 139—177.

Aitchison,]. and Shen,S.M. (1980). Logistic-normal distributions: Some prop-
erties and uses. Biometrika, 67, 261—272.

Biswas,S. et al. (2014). Learning microbial interaction networks from metage-
nomic cout data. arXiv:1412.0207v1 [q-bio.QM].

Candes,E.]. and Tao,T. (2005). Decoding by linear programming. IEEE T.
Inform. Theory, 51, 4203—4215.

Chen,]. et al. (2013). Structure-constrained sparse canonical correlation ana-
lysis with an application to microbiome data analysis. Biostatistics, 14,
244—25 8.

Faust,K. et al. (2012). Microbial co-occurrence relationships in the human
microbiome. PLoS Comput. Biol., 8, e1002606.

Filzmoser,P. and Hron,K. (2009). Correlation analysis for compositional data.
Math. Geosci., 41, 905—919.

Friedman,]. and Alm,E.]. (2012). Inferring correlation networks from genomic
survey data. PLoS Comput. Biol., 8, e1002687.

Gill,S.R. et al. (2006). Metagenomic analysis of the human distal gut micro-
biome. Science, 312, 1355—1359.

HandelsmanJ. et al. (1998). Molecular biological access to the chemistry of
unknown soil microbes: a new frontier for natural products. Chem. Biol., 5,
R245—R249.

Human Microbiome Project Consortium. (2012a). A framework for human
microbiome research. Nature, 486, 215—221.

Human Microbiome Project Consortium. (2012b). Structure, function and
diversity of the healthy human microbiome. Nature, 486, 207—214.

Ni,]. et al. (2013). How much metagenomic sequencing is enough to achieve a
given goalP. Sci. Rep., 3, 1968, doi:10.1038/srep01968.

Paulson,J.N. et al. (2013). Differential abundance analysis for microbial
marker-gene surveys. Nat. Methods, 10, 1200—1202.

Pearson,K. ( 1897). On a form of spurious correlation which may arise when
indices are used in the measurement of organs. Proc. R. Soc. Lond., 60,
489—502.

Penrose,R. and Todd,].A. (1955). A generalized inverse for matrices. Math.
Proc. Cambridge, 51.

Pikuta,E.V. et al. (2007). Microbial extremophiles at the limits of life. Crit.
Rev. Microbiol., 33, 183—209.

Savage,D.C. (1977). Microbial ecology of the gastrointestinal tract. Annu.
Rev. Microbiol., 31, 107—133.

Tibshirani,R. (1996). Regression shrinkage and selection via the lasso. ]. R.
Stat. Soc. B, 58, 267—288.

Yuan,M. and Lin,Y. (2007). Model selection and estimation in the gaussian
graphical model. Biometrika, 94, 19—35.

Zhang,T. and Zou,H. (2012). Sparse precision matrix estimation via lasso
penalized d-trace loss. Biometrika, 99, 1—18.

9mg ‘09 isnﬁnV uo seleﬁuV s01 ‘eiulomeg JO AIISJQAIII [1 112 ﬁhO'sleumo[pJOJXO'soneuuoguioiq/ﬁdnq won pepeolumoq

