APPLICATIONS NOTE V°" 23.2%.?éé‘iéiaiﬁgiifaﬁff/Ziﬁi

 

Data and text mining

Advance Access publication August 31, 2012

FSelector: a Ruby gem for feature selection
Tiejun Cheng, Yanli Wang" and Stephen H. Bryant"

Computational Biology Branch, National Center for Biotechnology Information, National Library of Medicine, National
Institutes of Health, 8600 Rockville Pike, Bethesda, MD 20894, USA

Associate Editor: Jonathan Wren

 

ABSTRACT

Summary: The FSelector package contains a comprehensive list of
feature selection algorithms for supporting bioinformatics and machine
learning research. FSelector primarily collects and implements the
filter type of feature selection techniques, which are computationally
efficient for mining large datasets. In particular, FSelector allows
ensemble feature selection that takes advantage of multiple feature
selection algorithms to yield more robust results. FSelector also pro-
vides many useful auxiliary tools, including normalization, discret-
ization and missing data imputation.

Availability: FSelector, written in the Ruby programming language, is
free and open-source software that runs on all Ruby supporting
platforms, including Windows, Linux and Mac OS X. FSelector is avail-
able from https://rubygems.org/gems/fselector and can be installed
like a breeze via the command gem install fselector. The source
code is available (https://github.com/need47/fselector) and is fully
documented (http://rubydoc.info/gems/fselector/frames).

Contact: ywang@ncbi.nlm.nih.gov or bryant@ncbi.nlm.nih.gov
Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on June 20, 2012; revised on July 31, 2012; accepted on
August 21, 2012

1 INTRODUCTION

Feature selection is of great importance for building statistical
models when mining large datasets of high dimension, such as
those generated from microarray and mass spectra analysis
(Saeys et al., 2007). It proves to be effective in the data mining
and bioinformatics ﬁelds for reducing dimensionality, selecting
relevant and removing redundant features, increasing predictive
accuracy and improving model interpretability (Guyon and
Elisseeff, 2003).

Depending on how they interact with the learning method,
various feature selection techniques roughly fall into three cate-
gories: ﬁlters, wrappers and embedded methods (Guyon, 2006).
Filters investigate only the intrinsic characteristics of a given
dataset and have the advantage of being fast as well as being
independent of learning method. Basically, there are two types of
ﬁlters: ﬁlter-by—feature-weighting and ﬁlter-by—feature—searching.
The former measures independently the relevance of each feature
to the target problem according to a certain evaluation criterion.
It provides a weight or ranking list as output, and features are
usually selected based on a given threshold. The latter also takes
inter-feature correlation into account and generates a subset of

 

*To whom correspondence should be addressed.

the original feature set that tends to be both relevant and
non-redundant. Wrappers wrap around a speciﬁc learning
method and conduct a search in the space of feature subset for
optimal model performance. They often report superior results
than ﬁlters, but coupled with increased computational load (Inza
et al., 2004). Embedded methods seek a trade-off between per-
formance and computational cost, by use of the internal infor-
mation of a learning method. In many applications, speciﬁcally
in bioinformatics, the datasets are often huge with numerous
samples and/or very high-dimension features. It is thus more
practical to apply ﬁlters for feature selection because of their
computational efﬁciency. To this end, we primarily implemented
ﬁlters in F Selector.

A wide variety of feature selection algorithms have been
proposed in the past decades (Guyon, 2006); however, most of
them are scattered in literature and not readily available to
researchers. Feature selection algorithms that come in a collec-
tion can be found in several machine learning software such as
PyML (http://pyml.sourceforge.net/), written in Python and
Weka (http://www.cs.waikato.ac.nz/ml/weka), written in Java,
or serve as additional libraries for statistical platform including
R (http: / /cran.r-proj ect.org/web /packages /F Selector /index.html)
and Matlab (http://featureselection.asu.edu/software.php).

Ruby is a high-level dynamic scripting language with a focus
on simplicity and productivity, which has become increasingly
popular in bioinformatics and other scientiﬁc research ﬁelds
(Aerts and Law, 2009; Dahl and Crawford, 2008; Goto et al.,
2010). To our best knowledge, the only Ruby package related to
feature selection is the feature_selecti0n gem that implements
only three algorithms (https://rubygems.org/gems/feature_selec-
tion). In this work, we presented F Selector, providing a substan-
tially larger collection of ﬁlters with 40+ feature selection
algorithms implemented in Ruby.

2 FEATURES OF FSELECTOR

F Selector works at the intermediate layer between data and ma-
chine learning approaches, such as random forest and support
vector machines. It simply takes a local or remote dataset in
CSV, ARFF or LibSVM ﬁle format as input and generates a
reduced dataset with only selected features. The output dataset is
interchangeable among different ﬁle formats and is compatible
with several popular machine learning software, including Weka
and LibSVM (http://www.csie.ntu.edu.tw/~cjlin/libsvm/).

F Selector gathers and implements a rich list of feature selec-
tion algorithms (Supplementary Table Sl) that have been put
into practice by researchers for a wide variety of applications.

 

Published by Oxford University Press 2012. All rights reserved. For Permissions, please e-mail: journals.permissions.com 2851

112 [3.10811211an[plOJXO'SODBIILIOJIIlOlQ/ﬂ(11111 IIIOJJ popcolumoq

910K ‘09 lsnﬁnV no :2

T.Cheng et al.

 

require ‘ fse'l eater”

H.-

r = FSelector::1nfurmatfunﬁain.n&w

'13“:

3H:

select the tap—ranked three features
.selettjeaturLerank! [' {:3 'J

.1

&

. datLtiL'l ihsvmf ‘hreast—w. 'I ii:st ']

.1

use Infurmatiunﬁain as a feature seiectinn algurithm

read remote UEI Breast Cancer Wisconsin dataset {Weka ARFF file format]
.datﬂ_frum_ur1ﬂ‘http:ffrepusitury.seasr.urngatasetstCIIarfffbreast—w.arff‘, :weka)

save dataset [with selected features} for 'Iater use by Libﬁ'u'rui {e.g. classification]

Fig. 1. Example of feature selection by using information gain as the feature selection algorithm. Note that line contents following ‘#’ are comments in

the Ruby programming language

It is very straightforward to get started with F Selector using a
single algorithm: choose a desired algorithm, read in a data ﬁle
and perform feature selection. For different algorithms,
F Selector maintains a consistent interface for feature selection,
depending on the algorithm type (i.e. ﬁlter-by-feature—weighting
or ﬁlter-by—feature-searching). As an example, Fig. 1 shows the
codes that use information gain as criterion to select the top three
informative features.

F Selector supports ensemble feature selection that takes ad-
vantage of multiple feature selection algorithms to yield more
robust results (Saeys et al., 2008). It follows the same procedure
as using a single algorithm and shares the same feature selection
interface as well. Components in ensemble can be different algo-
rithms of the same type (Supplementary Fig. SlA) or same
algorithms with sampled data created by instance perturbation
(Supplementary Fig. S1B). Depending on the type of component
algorithm, results from individual algorithms are combined using
various strategies (Supplementary Fig. S1).

F Selector also offers several data pre-processing techniques
related to feature selection, such as normalization, discretization
and missing data imputation. Normalization techniques may be
useful to clean and standardize raw data. Real datasets often
consist of continuous features, while many feature selection
algorithms expect feature to be discrete. Discretization tech-
niques are thus necessary prior to the use of such algorithms.
Likewise, for feature selection algorithms that work on complete
datasets, missing data imputation techniques are helpful to
replace missing values with desired ones.

F Selector has on-line tutorials and code examples for each
feature selection algorithm and for auxiliary normalization, dis-
cretization and missing data imputation techniques as well.
F Selector is hosted at the largest and most popular Ruby gem
repository (http://rubygems.org/), with source code available
under the Git version control. F Selector is 100% documented
including summaries and references, with intuitive layout gener-
ated by using the YARD tool (http://yardoc.org/). This support
can lower the barrier for end users to write their own feature
selection algorithms based on F Selector.

New features and updates will be added to F Selector in future.
We hope the release of source code of F Selector into the public

domain will encourage the community to contribute to the de-
velopment and help to improve F Selector.

3 CONCLUSION

F Selector is a valuable Ruby gem that offers easy and public
access to 40+ prevalent feature selection algorithms through a
consistent interface. We hope the rich collection of algorithms
together with other utilities (including various ﬁle formats sup-
port, auxiliary data pre-processing techniques and comprehen-
sive help documentation) will make F Selector a useful tool for
supporting various bioinformatics research, such as text mining,
microarray analysis and mass spectra analysis.

ACKNOWLEDGEMENTS

Funding: Intramural Research Program of the National Institutes
of Health, National Library of Medicine.

Conflict of Interest: None declared.

REFERENCES

Aerts,J. and Law,A. (2009) An introduction to scripting in Ruby for biologists.
BMC Bioinf., 10, 221.

Dahl,D.B. and Crawford,S. (2008) Rinruby: accessing the r interpreter from pure
ruby. J. Stat. Softw., 29, 1—18.

Goto,N. et al. (2010) BioRuby: bioinformatics software for the Ruby programming
language. Bioinformatics, 26, 2617—2619.

Guyon,I. (2006) Feature Extraction: Foundations and Applications. Springer Verlag.

Guyon,I. and Elisseeff,A. (2003) An introduction to variable and feature selection.
JMLR, 3, 1157—1182.

Inza,I. et al. (2004) Filter versus wrapper gene selection approaches in DNA micro-
array domains. Artif. Intell. Med, 31, 91—103.

Saeys,Y. et al. (2008) Robust feature selection using ensemble feature selection
techniques. In Proceedings of the European conference on Machine Learning
and Knowledge Discovery in Databases-Part II. Springer-Verlag, Antwerp,
Belgium, pp. 313—325.

Saeys,Y. et al. (2007) A review of feature selection techniques in bioinformatics.
Bioinformatics, 23, 2507—2517.

 

2852

112 ﬂJO'slcumo[pJOJXO'sopchogutotq/ﬁd11q IIIOJJ popcolumoq

910K ‘09 lsnﬁnV no :2

