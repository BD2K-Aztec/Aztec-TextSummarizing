ORIGINAL PAPER

Vol. 27 no. 21 2011, pages 3050-3055
doi: 1 0. 1 093/bioinformatics/btr5 1 6

 

Data and text mining

Advance Access publication September 9, 2011

AUC-based biomarker ensemble with an application on gene
scores predicting low bone mineral density

X. G. Zhaol’, W. Dai2, Y. Li2 and L. Tian3’*

1Department of Bone and Joint Surgery, The First Affiliated Hospital of Xi’an Medical University, Xi’an 710077,
Shaanxi Province, PR. China, 2Department of Biostatistics, Harvard University, Boston, MA 02115 and 8Department
of Health Research and Policy, Stanford University, Palo Alto, CA 94301, USA

Associate editor: John Quackenbush

 

ABSTRACT

Motivation: The area under the receiver operating characteristic
(ROC) curve (AUC), long regarded as a ‘golden’ measure for the
predictiveness of a continuous score, has propelled the need to
develop AUC-based predictors. However, the AUC-based ensemble
methods are rather scant, largely due to the fact that the associated
objective function is neither continuous nor concave. Indeed, there is
no reliable numerical algorithm identifying optimal combination of a
set of biomarkers to maximize the AUC, especially when the number
of biomarkers is large.

Results: We have proposed a novel AUC-based statistical ensemble
methods for combining multiple biomarkers to differentiate a binary
response of interest. Specifically, we propose to replace the non-
continuous and non-convex AUC objective function by a convex
surrogate loss function, whose minimizer can be efficiently identified.
With the established framework, the lasso and other regularization
techniques enable feature selections. Extensive simulations have
demonstrated the superiority of the new methods to the existing
methods. The proposal has been applied to a gene expression
dataset to construct gene expression scores to differentiate elderly
women with low bone mineral density (BMD) and those with normal
BMD. The AUCs of the resulting scores in the independent test
dataset has been satisfactory.

Conclusion: Aiming for directly maximizing AUC, the proposed
AUC-based ensemble method provides an efficient means of
generating a stable combination of multiple biomarkers, which is
especially useful under the high-dimensional settings.

Contact: lutian@stanford.edu

Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on May 10, 2011; revised on August 29, 2011; accepted
on September 1, 2011

1 INTRODUCTION

Given that there are multiple biomarkers and a binary response of
interest (e.g. case and control), it is often of substantial interest
to combine the biomarkers to form a ‘strong’ scoring system for
the differentiation of cases from controls. While the choice of the
predictive measure is not unique, the most appealing choice is
the area under the receiver operating characteristic (ROC) curve
(AUC) in the case—control study (Pepe, 2003; Zhou et al., 2002).

 

*To whom correspondence should be addressed.

For ﬁnite samples, AUC is simply the non—parametric two—sample
Mann—Whitney U test statistics. Unlike the measures such as
misclassiﬁcation rate, the AUC reﬂects the intrinsic predictive value
of a score in that it does not depend on the prevalence of the cases
and thus is invariant under the case—control sampling. Therefore, it
is natural to combine biomarkers by maximizing the AUC under
ROC curve (Ma and Huang, 2005, 2007; Pepe et al., 2006; Ye
et al., 2007; Zhou et al., 2011). However, it is notoriously difﬁcult
to maximize the AUC numerically since the objective function is
neither continuous nor convex. Ad hoc methods have been proposed
to tackle the numerical problem. For example, sigmoid function has
been used to approximate the indicator function used in calculating
AUC (Ma and Huang, 2007). However, the smoothed objective
function may still have multiple local maximums, with no guarantee
of locating the global maximizer by using the commonly used
numerical algorithms. In view of these challenges, we propose
a class of ensemble methods aiming for maximizing AUC with
multiple biomarkers. Speciﬁcally, we introduce a class of convex
surrogate loss functions to approximate the non—convex AUC,
greatly facilitating computation and optimization.

2 METHODS

2.1 Surrogate loss functions

Assume that X1,...,X,, are n independently identically distributed (i.i.d)
copies of p—dimensional random vector X, representing, for example, p
biomarkers for cases, and Y1,...,Ym are m i.i.d copies of p—dimensional
random vector Y for controls. Suppose that we want to construct a score
as a linear combination of the p biomarkers with the aim of maximizing the
AUC under ROC curves. Speciﬁcally, we want to ﬁnd a vector ﬂ to maximize
the objective function

n m
AUC(16)=(nm)‘1 [Em/X.- > ﬁ’Yj).

i=1 j=1
Ideally, we would want the score for cases to be higher than that for
controls, which yields 1 for the AUC and completely differentiates cases
and controls. However, several challenges are prominent. First, since the
objective is invariant in ﬂ —> cop for arbitrary co > 0, there is no unique
maximizer for the objective function. One often needs to subjectively select
an anchor biomarker with its weight in the linear combination being one and
maximize the remaining p — 1 components in ﬂ. The performance of the score
heavily depends on the selection of the anchor biomarker. Second, even with
a given anchor biomarker the objective function is still neither continuous nor
concave and therefore it is very likely that conventionally used optimization
iterations have been trapped around local maximum points depending on the

subjectively selected initial point.

 

3050 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /§.IO'S[BU.IHO[p.IOJXO'SOllBIHJOJUTOTQ/ﬁdnq urorj popeoIIJ/noq

9IOZ ‘091sn8nv uo ::

AUC-based biomarker ensemble

 

We are now in a position to propose a method addressing these
two challenges. By noting that l—AUC(ﬁ) can be interpreted as the
misclassiﬁcation rate of using the binary rule ﬁ/(Xi—Yj)>0 to classify
a binary response always taking value 1, we may borrow the popular
classiﬁcation approaches aimed for minimizing the misclassiﬁcation rate
in the data mining literature (Friendman et al., 2000; Hastie and Zhu,
2006). Speciﬁcally, instead of directly maximizing AU C (p) or equivalently
minimizing l—AUC(ﬁ), it is sensible to minimize a surrogate loss function.
We propose the following two surrogate functions

M1(13)=(nm)‘1ZZIOgIHeXpI—ﬁ/(Xt— ij,

i=1 j=1
and

n m
M2(ﬁ)=(nm)—1ZZu—ﬁKXi—mn,
i=1 j=1
which correspond to the negative log likelihood function raised in the
conditional logistic regression and hinge loss function used in support
vector machine, respectively, where x+ = x1 (x > 0). Since these two functions
are continuous convex function, their minimizers are well deﬁned and
can be reliably located. Numerically, it amounts to replace the indicator
function I(x<0) by log{l+exp(—x)} and (l —x)+, respectively. Previous
data mining literature reveals that algorithms minimizing such surrogate
loss functions often result in models with good performance in minimizing
the misclassiﬁcation rate. Analogously, the estimated scores tend to render
satisfactory AUCs.
With moderate dimension p, while M1(ﬁ) can be minimized via the
scoring algorithm in ﬁtting a generalized linear model, we can use linear
programming technique to minimize M2 (ﬂ). Speciﬁcally, it is equivalent to

n m
minimizingE E Elj+

i=1 j=1

Sij 2517+ _§ij—
subject to a,- =1—13’(Xt—Xj) 15:91.1 SjSm.
Sij-l' Z 07 51.j— Z 0

Several advantages of the proposal are obvious. First, minimizing either
M1(ﬁ) or M2(ﬁ) does not require selecting an anchor biomarker a priori,
which is especially appealing for high—dimensional case. Second, in the
limiting case, the maximizer of the AUC under the ROC curve

E {AUC(g)} =pr{g(X ) > 8(Y)},

is g(')=m{f1(')/f0(')} for any strictly monotone increasing function m(~),
where f1(') and f0(') are the underlying density functions of X and Y,
respectively (Jin and Lu, 2008). The minimizer of

E{M1(g)}=Elog[1 +e—{g(X>—g<r>}],

is log {f1 (-)/f0(-)} and thus minimizing E {M 1 (g)} is equivalent to maximizing
the AUC under the ROC curve. The minimizer of

E{M2(g)}=E{1—g(X)+g(Y)}+,

is more complicated. Numerical studies point that the minimizer may also
be a monotone transformation of f1( ') / f0( ), as opposed to the conventional
support vector machine whose solution approximates the optimal decision
boundary.

2.1.1 Adaptive generalizations As neither M1(ﬁ) nor M2(ﬁ) provides a
good approximation to l—AUC(ﬁ) (indeed no convex functions accurately
approximate the indicator function), we employ an iterative algorithm to
approximately minimize l—AUC(ﬁ). More speciﬁcally, given that

(C —x)+

m x =
c() C+lc_x|

—>I(x<0) as c—>0+,

we expect that the minimizer of 227:1 1mc{ﬁ’(X,- —Yj)} approximates
that of l —AUC(ﬁ) for small 0 > 0. Figure 1 illustrates the 0—1 loss function

 

Loss function

 

 

 

 

Fig. 1. mc(x) against the 0—1 loss function. The 0—1 loss function I (x<0)
(black); hinging loss function (red); green: m1(x); m0.5(x) (blue); m0.25(x)
(cyan); 10g{1+eXp(—x)} (purple).

against log{l +exp(—x)}, the hinge loss function and mc(') with different cs.
Noting that mc( H x) 2 m1 {( ﬁ/ c)/ x}, the scoring system minimizing mc( 0 x) is
equivalent to that minimizing m1 (0 x). Thus, we may employ the following
adaptive algorithm

(1) Set the initial

ﬂ <— argminﬁMzw)
(2) Update p as
ﬂ <_argminyz:z: {1—7/ (Xi—Yj)}+

i=1 j=1 1+|1_BI/c—1(XI_YJ.)|,

where the minimization can be solved via linear programming
technique.

(3) Repeat Step (2) until convergence or the number of iteration reaches
a pre—speciﬁed number.

Our numerical results (reported in the Supplementary Material) show that
the adaptive iteration may increase the resulting AUC especially when there
are potential outliers. The robustness of the method is not a surprise, because
the inﬂuence from individual observations on the objective function via me (x)
is always bounded. Furthermore, we ﬁnd that one or two iterations often
sufﬁce to harvest most of the gain in maximizing AU C ( ﬂ) and thus, in general,
there is no need to continue the iteration until convergence.

2.1.2 Extension to survival outcomes When the outcome is survival time
subject to potential right censoring, the 0— index as the generalized AUC is
often computed as

:11 Law/Z.- > ﬁ’zjﬂﬁi < im-
21-; 2,1110.- < Tm.-

 

C(13):

where (T,, A,,Z,-), i: l,  are n i.i.d copies of (T, A,Z), T is the minimum
of the censoring and failure times, A is the binary censoring indicator and Z
is the covariate vector (Cai and Cheng, 2008). Similarly, one may ﬁnd ﬂ by
minimizing

M1(ﬁ)=2210g[1+exp{—ﬁ’(zi—n)}11('fi{Emu

i=1 j=1
and
A712(13)=ZZ{1—/3/(Zt—%)}+I(Tt< fjmi,
i=1 j=1
the counterparts of M1(ﬁ) and M2(ﬁ), respectively. Indeed, the log—partial
likelihood function also can be viewed as surrogate to the c—index in that

 

3051

112 /§.IO'S[BU.IHO[p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdllq 11101; prBOIIIAAOG

9IOZ ‘09 lsnﬁnv uo ::

X.G.Zhao et aI.

 

both
K

K
log [1 +ZeTxk:| and Zlogﬂ +e_xk),
k=1

k=1
may serve as a surrogate to

K
2100, < 0).
j=1

This may explain that the Cox model—based c—index is often high.

2.2 Regularization for high-dimensional covariates

When p is high, the proposed surrogate loss function can be conveniently
regularized for feature selection. While many regularization methods can
be used, we hereafter pursue the popular lasso regularization for illustration
(Tibshirani, 1996). Speciﬁcally, we propose to minimize

1V1j(13)+k|13|,j=1,2
where ﬁ=(ﬁ1,...,ﬁp)’ and |ﬁ|= gllﬁjl. For M1(ﬁ), one may use

the following iterative coordinate descending algorithm to minimize the
objective function with a given A

(1) Set an initial estimator ﬂ.

(2) Update ﬂ

ﬁeargminﬁZZWU-{Zywin—Inna

i=1 j=1
where Wij =pij(l —pij),
Zij = ﬁ/(Xi — Yj)+Pij(1—Pij)a

and
l

1+6XP{13/(Xt — 1(1)},
for i=1,...,n and j: l, ...,m. The standard coordinate descending

algorithm can be used to minimize the weighted L2 loss function in
this step (Friedman et al., 2010).

pij =

(3) Repeat Step 2 until convergence.

The coordinate descending algorithm is not directly applicable for the non—
differentiable surrogate loss function M2(ﬁ). However, since the objective
function M2(ﬁ) is convex and piecewise linear, the exact solution path with
A varying from 00 to 0 is also piecewise linear and can be computed
using the generalized LARS algorithm (Cai et al., 2009; Rosset and Zhu,
2007). When the dimension p or sample size n is high, the computation is
demanding due to dense joints in the solution path. As a remedy, we propose
a forward stagewise algorithm that generates an approximate solution path
of p (Friedman and Popescu, 2004).

(1) Set an initial estimator ﬂ =0 and small positive number 6 > 0.
(2) At step k =1,..., identify the coordinate j with the largest decrease
maX{M2(ﬁ) —M2(ﬁ+ej6)aM2(ﬁ) —M2(ﬁ— €j€)},
and update
ﬂ <— ﬁ + sjeje,
where e > 0 is an small constant selected a priori, ej is a p—dimensional

vector with all the components being zero except the j—th component,
which is l and sj = 2I{M2(ﬁ+ eje) < M2(ﬁ — eje)} — 1.

Repeat Step 2 until the number of non—zero components of p reaches
a prespeciﬁed number or the AU C(ﬁ) becomes 1.

(3

V

When the exact lasso solution is desirable, one may employ an ad hoc two—
stage approach. Speciﬁcally, one may ﬁrst implement the aforementioned
forward stagewise algorithm to screen informative features. The forward
stagewise algorithm stops when the number of selected features reaches a

prespeciﬁed maximum number of biomarkers to be used for constructing the
score in practice, say 30. At the second step, the exact lasso solution path
can be computed with only the selected features. In either case, the penalty
parameter can be selected via cross—validation. The objective function used
in the cross—validation can be either [Ill-(ﬂ) itself or the AUC under ROC
curve.

3 SIMULATION

Extensive simulations are conducted to examine the ﬁnite sample
performance of the proposed method. We generate the covariates
{X1,...,Xn} and {Y1,...,Ym} from the following models:

..d ..d
(l) (multivariate normal) XilrlvN(,U.1,21) and lerlvN(,U.2,22),
where X,- and  are p—dimensional random vectors.
..d
Q)mmmd1mmm@ xgnstbzp+ozmpgzg md

Yj1’1\(’10.8N(/Lz,22)+0.2N(M3,E3), Le. 20% of the markers
values in both cases and controls are contaminated by a
common error distribution.
..d ..d
(3) (multivariate log—normal) log(X,-)1r1vN(,a1, 21) and log(YJ-)1*1v
NW2, 232)-
..d
(4) (log—normal mixture) log(X,-)1r1\/0.8N(,U.1, 21H—
0.2N(,a3, E3) and log(I{,-)1*1\E10.8N(,a2, 22)—I—0.2N(,a3, 23).

In the above settings, we let ,a1=(l,0,0,...,0)’, M2:
(0,1,0,...,0)’, M3=(l,l,l,0,...,0)’,

1 1/3 ..1/3
21=22= 11.3 E. 1:: 11.3 7
1/3 1/3... 1

E3 =51p, [p is the identity matrix. We considered several
conﬁgurations of n, m and p to investigate the operational
characteristics of the proposed method.

First, we examine the scenario where the number of covariates is
low relative to the sample size. To this end, we let p = 3 and n = m =
50. For each generated dataset, we construct a linear combination of
the covariates as a score differentiating cases from controls, where
the weights of the linear combinations are estimated by minimizing
(i) M1(,B) (ii) M2(,8) (iii) the loss function

71 m 1
5(ﬂ)=—ZZW—m—x.—m’ (1)

i=1j=1

proposed in Ma and Huang (2007) and (iv) by ﬁtting a regular
logistic regression. We also implement the popular ada—boosting
with 300 iterations using the simple stump as the base classiﬁer
(Friendman et al., 2000). The continuous class probability based on
ada—boosting trained ensemble is used to generate the ROC curve.
In minimizing S(,B), we ﬁrst identify the ‘anchor covariate’ with
the most signiﬁcant p value from t—test comparing the covariate
distribution between cases and controls and set its regression
coefﬁcient at +1 or —1 depending on the sign of the t—statistics.
The a in S(,B) is then selected as 20% of the mean group difference
of the anchor covariate as suggested in Ma and Huang (2007). We
then calculate the AUCs in an independent test set consisting of
2000 cases and 2000 controls for all the ﬁve constructed scores.
The boxplots of AUCs over 250 replications in each setting are

 

3052

112 /§.IO'SIBUJHOprOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

AUC-based biomarker ensemble

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

8.- : ﬁ
1— ! I
a . E
d 5 c»,
E O
8.- . ..
(230 a a a a a 3.0” H H  H H
g: 3: a; e 1 ° 5 a, a, a a
Z  i i ..-s  3;: a E
g- 8 a  o  1 
8
§ 0
A B1 B2 c D E A B1 B2 c D E
(C)8 = (003 a
F E 3
L0 T O5
O5 : O
O a
o 5 co
0') ' I I 1
..o H H .H 0° 9H 15 a3 EH EB =
D I ' Dlx :I '- l- :I II E
<3 3 E B. 5 5 <0 'i  at   ‘
o 1 IE =5 3 5‘ 6 o ‘ .E E
g 3   a: . ‘0 °: ° §:
8 9 ° 1 as °' 5 O 8 ° ° °
0 8 l g 2 Lo 0
LO 0
N a
0 II
A B1 B2 c D E A B1 B2 c D E

Fig. 2. Empirical AUC in both training and validation sets for different
methods [A: M1 (8); Bl: M2(8); B2: one—step adaptive M2(8); C: 5(8); D:
lasso—regularized logistic regression; E: ada—boosting using stumps with 300
iterations] with low—dimensional covariates. Gray box: training set; empty
box: validation set. (a) Simulation setting I (normal); (b) simulation setting
II (normal mixture); (c) simulation setting III (log—normal); (d) simulation
setting IV (log—normal mixture).

plotted in Figure 2. The AUCs in the training sets are higher than
their counterparts in the validation sets as expected. In most cases
including the ﬁrst setting, where the logistic regression estimates
the optimal combination in terms of maximizing the AUC, the
scores based on M 1 ( ,8), M2 (,8), S (,8) and logistic regression perform
similarly in terms of AUC in the validation sets. In general, the
score based on ada—boosting has the lowest AUC, which could be
due to overﬁtting indicated by the high AUCs in the training set.
Furthermore, the score based on the one—step adaptive hinge loss
function performs similar or slightly superior to that based on hinge
loss function itself.

Second, we have examined the performance of the proposed
method for covariates with moderate dimension. In this case,
we let p=200 and n=m=50 and the lasso regularization is
used for selecting the important features in logistic regression.
The forward stagewise algorithm similar to that presented in
Section 2.2 for minimizing M2(,8) is also used to minimize S(,8).
We choose the popular lasso penalty mainly for the purpose of
fair comparison, i.e. evaluating the relative performance of various
methods under similar regularization schemes. The boxplots of
AUCs in independent test sets over 250 replications are plotted in
Figure 3. In general, the scores based on M,( ,8) perform better than
that based on the alternatives in terms of average AUC in the test
sets. Furthermore, the AUCs from scores constructed via MJ-(,8) also
tend to have smaller variability than their counterparts. In the most
challenging fourth setting, the empirical average AUC in the test
sets is 0.66 for score minimizing M1 (,8), 0.66 for score minimizing
the hinge loss, M2(,8), 0.60 for scores minimizing 5(8), 0.60 for
score from the logistic regression ﬁtting and 0.63 for score from
ada—boosting using three markers. An increase from 0.60—63 to 0.66
in the AUC is often considered non—trivial in clinical practice.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(av 11 11 1,5, :11 ([3)?   1? ii?
I; 11 f. 35;  a- " ,== :1 gii ,
00 So a :0 °e :- o . - 1:
C5— 5 =3 1:9 $9 H O_ H lH  EH 
a 11 151° 5.  3 a; a, 1.; a 
OE  03.     
2°  20'   ' 81: 
 8- 1: :1: 1i 5:: 
<9- 8 0' 1°  ' °ai 15
° 18 313 81° 15
3 a is s 53 
g- g. .— 11: ll: 1 n: 
O I, II. II, II, II.
A B C D E A B C D E

c 'v '1 - d v ii 7
() ’31 '11 as If 1. ()E    .
09_ ,g 151, , .aH  °    
O o o 3 “ o v 1: 515 ii
 1 as 15. HH 3    
< 8 UH <0   .
a ° 3 S   
O In - 5!
In ' a:
g a e i. E I g 
A B IO D E A B C D IEI

Fig. 3. Empirical AUC in the validation sets for different methods [A: M1(8);
B: M2(8); C: 5(8); D: lasso—regularized logistic regression; E: ada—boosting
using stumps with 300 iterations] with moderate dimensional covariates.
Empty box: AUC with one selected covariate; gray box: AUC with three
selected covariates; dark gray box, AUC with 10 selected covariates. (a)
Simulation setting I (normal); (b) simulation setting II (normal mixture);
(c) simulation setting III (log—normal); (d) simulation setting IV (log—normal
mixture).

Lastly, we have examined the cases for high—dimensional
covariate. Here, we let p=20 000 and n=m=50. To save
computational time, the ada—boosting is only applied to top 500
features selected based on signiﬁcance levels of t—test comparing
cases and controls in the training set. The simulation results are
presented in Figure 4. For the high—dimensional covariates, the
relative performance of the proposed methods is even better than
that in the previous case where p=200. For example, in the third
setting, the empirical average AUC in the test sets is 0.85 for score
minimizing M1 ( ,8), 0.85 for score minimizing the hinge loss, 0.76 for
scores minimizing 5(8), 0.74 for score from the logistic regression
ﬁtting and 0.64 for score from ada—boosting using three markers.
Similarly, in the fourth setting, the empirical average AUC in the
test sets are 0.56, 0.56, 0.53, 0.53 and 0.53 for aforementioned ﬁve
methods.

4 ANALYSIS OF THE BONE MINERAL DENSITY
STUDY

We apply the proposed method to a dataset (Reppe et al., 2010)
arising from a study that recruited 301 non—related post—menopausal
ethnic Norwegian women at the Lovisenberg Deacon Hospital.
Among them, bone mineral density (BMD) and gene expression
levels (Affymetric array) were measured for 84 women. Since low
BMD is associated with higher fracture rates (Cooper, 1997), it
is of interest to identify a linear combination of gene expression
levels to differentiate the osteopenia or osteoporosis (low BMD)
from normal among post—menopausal women. Bone biopsies show
that there are 39 from 84 women having osteopenia or osteoporosis.
All the normalized gene expression level are log—transformed. After
screening out ~25% probesets with lowest variation, we have 40 4 ll
probesets for each patient. We randomly split the data into training
and validation sets and apply the proposed method to the training

 

3053

112 /810's12u1nofp101x0'sor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

X.G.Zhao et aI.

 

 

 

11 111

5%

0.8

AUC
0.6 07
1::
AUC
0.50 0.55 0.60 0.65 0.70 0.75 0.80
13:1-
4::

 

 

 

 

0.5

 

 

 

 

I'

0 8
OOOOQDD-'ED-<

.0 III
E u
mm
o
(I
>-1:|:1-<
>—1=|:n-<
.-:n----.
El:

000 0 mm a. on
.4 000mm 0.: mm 0
"-400.13 0
m

a:
m
a:
- - ‘EI:
41- m
— 43:1»
- --1:|=1----
— 1411-4
— pun:
— >-—l:l=l-

AUC
0.16 07
4::-
1::
AUC
050 055 060 065 070 075
I:l:l

 

0.I5

 

 

 

 

—n..
>

 

 

>
m
0
U
ITI
>
m
0
U
ITI

Fig. 4. Empirical AUC in the validation sets for different methods [A: M 1 ( 8);
B: M2(8); C: 5(8); D: lasso—regularized logistic regression; E: ada—boosting
using stumps with 300 iterations] with high—dimensional covariates. Empty
box: AUC with one selected covariate; gray box: AUC with three selected
covariates; dark gray box, AUC with 10 selected covariates. (a) Simulation
setting I (normal); (b) simulation setting II (normal mixture); (c) simulation
setting III (log—normal); (d) simulation setting IV (log—normal mixture).

set consisting of 26 cases and 30 controls. For the purpose of
comparisons, we also construct gene scores based (1) 5(8) proposed
in Ma and Huang (2007) (2) lasso—regularized logistic regression
and (3) ada—boosting using stumps as base classiﬁers. The anchor
gene and o in 5(8) are determined using the same method as that
presented in the simulation study. To save computational time, ada—
boosting is only applied to the top 2000 genes according to their
signiﬁcance level in t—test comparing average gene expression levels
between cases and controls. With the estimated scores based on
M1(8), M2(8), 5(8), the regularized logistic regression and ada—
boosting, we examine their corresponding AUC in the validation
set. The results are shown in Figure 5. It can be seen that in general
scores based on M1(8) and M2(8) yield higher AUC than that
based on 5(8), the commonly used logistic regression and ada—
boosting with the same number of covariates in the validation set.
In Figure 5, we also plot the AUC in the training set. Since the
number of covariates is much higher than the sample size, the
maximum AUC (AUC: 1) corresponding to complete separation
between case and control in the training set is reached with 20—
30 covariates for all the methods. The highest AUCs for scores
based on 51(8), regularized logistic regression and ada—boosting
are 0.764, 0.687 and 0.728, respectively, while the highest AUC
is 0.708 for score based on M1(8) and 0.764 for score based on
M2(8). As a reference, the AUC for age is only 0.669 in this cohort.
Furthermore, while the optimal scores with M1 (8) and M2 (8) use 9
and 13 genes, respectively, their counterparts based 5(8), regularized
logistic regression and ada—boosting use 37, 12 and 45 genes,
respectively. These comparisons suggest that the genes score based
on M2 (8) possesses the best combination of sparsity and prediction
performance: it attains the highest AUC in the validation set with
only 13 genes. The gene lists selected by these methods are heavily
overlapping. For example, there are seven common genes shared
by at least three out of four linear combinations constructed based

 

,g/

- /\r\

AUC under the ROC curve
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

 

 

 

0 10 20 30 40
number of genes

Fig. 5. AUC of scores for differentiating the women having low and normal
BMD. Thick lines: AUC in validation set; thin lines: AUC in training set;
red: score based on M1(8); green: score based on M2(8); blue: score based
on 5(8); black: score based on logistic regression; cyan: score based on
ada—boosting.

M1(8), M2(8), 5(8) and logistic regression. The probe set AFFX—
M27830_M_at, which is shared by all four scores, is a member of the
eight core genes reported by (Reppe et al., 2010). Furthermore, gene
SOST (Affymetrix ID 223869_at) shared by scores based on M1(8),
M2(8) and logistic regression is also a member of the eight core
genes explaining the variation of BMD and sits in the ‘center’ of the
constructed intermolecular network sharing signiﬁcant associations
reported in the original paper (Reppe et al., 2010). The selected
genes and their corresponding weights are summarized in Table l,
where the weights are standardized such that the probe set AFFX—
M27830_M_at has the unit weight for comparison purpose. One
interesting and reassuring observation is that signs of all non—zero
weights were consistent across methods.

We also repeat analysis based on other random training test
splitting and obtain similar results.

5 DISCUSSION

Motivated by recent advances in data mining, we have proposed
a class of methods combining biomarkers to construct a scoring
system, boosting the resulting AUC under the ROC curve, a
prevalence—free summarization of intrinsic predictive values of a
continuous score. The method is easily adapted to high—dimensional
cases, wherein one may need to identify informative features
from thousands of candidate biomarkers. In high—dimensional case,
we propose to apply lasso regularization to yield a parsimonious
combination maximizing the AUC. On the other hand, lasso is
neither the unique nor the universally optimal regularization method
for analyzing high—dimensional data. Due to the convexity of the
proposed loss function, it is straightforward to couple MJ-(8) with
other penalty functions such as elastic net, adaptive lasso and SCAD,
which may have superior performance to simple lasso in speciﬁc
settings (Zou, 2006; Zou and Hastie, 2005; Zou and Li, 2008). The
key proposal is to target a convex surrogate loss function instead of
a discontinuous Mann—Whitney rank statistic.

While in this article, we have focused on the hinge loss function
(corresponding to the l— norm support vector machine), our results
can be extended to accommodate other versions of SVM loss

 

3054

112 /810's12u1nofp101x0'sor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

AUC-based biomarker ensemble

 

functions, such as

(anI ZZH —/3’(X.- — m1“,

i=U=1

Table 1. The estimated scores for differentiating low and normal BMD

 

 

 

Affymatrix ID Weights
M1 (1‘3) M203) 503) LR

AFFX—M27830_M_at 1 1 1 1
211769_x_at 0 0 0 0.946
215887_at 0 0 0 1.915
2l776l_at 0 0 0 —0.447
220900_at 0 0 0 1 .073
223869_at 0.587 0.159 0 0.112
227405_s_at 0.055 0 0 0.024
231599_x_at 0 0 0 —0. 175
235102_x_at 0.983 0.113 0 2.822
237739_at 0.779 0.093 0 0.896
238020_at 0 0 0 — 1.625
239498_at 1.161 0.185 0 1.146
206742_at —0.068 —0.06 —0.026 0
222735_at —0.639 —0. 185 —0.082 0
244035_at 0.692 0.013 0 0
219747_at 0 0.007 0 0
235439_at 0 0.033 0 0
238705_at 0 0.212 0 0
238946_at 0 0.06 0.02 0
1552477_a_at 0 —0.02 0 0
206273_at 0 0 0.008 0
206307_s_at 0 0 0.006 0
206326_at 0 0 0.038 0
207369_at 0 0 —0.036 0
210045_at 0 0 —0.002 0
210174_at 0 0 —0.036 0
214412_at 0 0 —0.004 0
215196_at 0 0 —0.008 0
215431_at 0 0 0.002 0
219566_at 0 0 —0.01 0
220554_at 0 0 0.09 0
2205 84_at 0 0 —0.012 0
221631_at 0 0 —0.01 0
227440_at 0 0 —0.078 0
229201_at 0 0 —0.108 0
230349_at 0 0 —0.01 0
230839_at 0 0 —0.024 0
231231_at 0 0 0.01 0
231468_at 0 0 —0. 1 12 0
231759_at 0 0 —0. 106 0
231828_at 0 0 —0.004 0
232114_at 0 0 0.09 0
234259_at 0 0 0.012 0
234421_s_at 0 0 —0.036 0
234604_at 0 0 0.058 0
241736_at 0 0 0.048 0
243673_at 0 0 —0.016 0
243889_at 0 0 —0.068 0
244338_at 0 0 0.022 0
1553027_a_at 0 0 —0.008 0
1556803_at 0 0 0.044 0
1556938_a_at 0 0 0.048 0

15 60779_a_at 0 0 0.032 0

 

LR, logistic regression.

for any given or Z 1. Another alternative is the exponential function
used in boosting algorithm

n m
(mm—1 226—18/(Xi—Yj).

i=U=1

Finally, while the AUC under the entire ROC curve is a useful
global measure, the AUC under partial ROC curve has recently
emerged as a useful problem—speciﬁc measure in practice (Komori
and Equchi, 2010). Therefore, an numerically efﬁcient algorithm
combining multiple biomarkers to maximize the AUC under partial
ROC curves or sensitivity for given speciﬁcity level is worth further
investigations.

F unding: R01 HL089778—04 (to L. Tian) and R01 CA95747 (to Y.
Li) from National Institute of Health.

Conﬂict ofInterest: none declared.

REFERENCES

Cai,T. and Cheng,S. (2008) Robust combination of multiple diagnostic tests for
classifying censored event times. Biostatistics, 9, 216—233.

Cai,T. et al. (2009) Regularized estimation for the accelerated failure time model.
Biometrics, 65, 394—404.

Cooper,C. (1997) The crippling consequences of fractures and their impact on quality
of life. Am. J. Med., 103, l2S—l7S.

Friedman,J. and Popescu,B. (2004) Gradient directed regularization for linear regression
and classiﬁcation. Technical Report. Department of Statistics, Stanford University.

Friedman,J. et al. (2010) Regularization paths for generalized linear models via
coordinate descent. J. Stat. Soﬁwr, 33, 1—22.

Friendman,J. et al. (2000) Additive logistic regression: a statistical view of boosting.
Ann. Stat, 28, 337—407.

Hastie,T. and Zhu,]. (2006) Discussion of “support vector machines with applications"
by Javier Moguerza and Alberto Munoz. Stat. Sci., 21, 352—357.

J in,H. and Lu,Y. (2008) A procedure for determining whether a simple combination of
diagnostic tests may be noninferior to the theoretical optimum combination. Med.
Decis Making, 28, 909—916.

Komori,O. and Equchi,S. (2010) A boosting method for maximizing the partial area
under the ROC curve. BMC Bioinformatics, 11, 314—330.

Ma,S. and Huang,J. (2005) Regularized roc method for disease classiﬁcation and
biomarker selection with microarray data. Bioinformatics, 21, 4356—4362.

Ma,S. and Huang,J. (2007) Combining multiple markers for classiﬁcation using ROC.
Biometrics, 63, 751—757.

Pepe,M. (2003) The Statistical Evaluation of Medical Tests for Classiﬁcation and
Prediction. Oxford University Press, Oxford.

Pepe,M. et al. (2006) Combining predictors for classiﬁcation using the area under the
receiver operating characteristic curve. Biometrics, 62, 221—229.

Reppe,S. et al. (2010) Eight genes are highly associated with bmd variation in
postmenopausal caucasian women. Bone, 46, 604—612.

Rosset,S. and Zhu,]. (2007) Piecewise linear regularized solution paths. Ann. Stat, 35,
1012—1030.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. R. Stat. Soc.,
Ser. B, 58, 267—288.

Ye,J. et al. (2007) On the analysis of glycomics mass spectrometry data via the
regularized area under the ROC curve. Bioinformatics, 8, 477—488.

Zhou,X. et al. (2002) Statistical Methods in Diagnostic Medicine. John Wiley & Sons,
Inc., New York.

Zhou,X. et al. (2011) Variable selection using the optimal roc curve: An application to a
traditional Chinese medicine study on osteoporosis disease. Stat. Med. [Epub ahead
of print, doi:10.1002/sim.3980].

Zou,H. (2006) The adaptive lasso and its oracle properties. J. Am. Stat. Assoc, 101,
1418—1429.

Zou,H. and Hastie,T. (2005) Regularization and variable selection via the elastic net. J.
R. Stat. Soc. Ser. B, 67, 301—320.

Zou,H. and Li,R. (2008) One-step sparse estimates in nonconcave penalized likelihood
models. Ann. Stat, 36, 1509—1533.

 

3055

112 /810's112u1nofp101x0'sor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

