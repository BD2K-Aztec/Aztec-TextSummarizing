Bioinformatics, 32(2), 2016, 173—180

doi: 10.1093/bioinformatics/btv561

Advance Access Publication Date: 30 September 2015
Original Paper

 

Sequence analysis

smallWig: parallel compression of RNA-seq
WIG files

Zhiying Wang1'*, Tsachy Weissman1 and Olgica Milenkovic2

1Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA and 2Department of Electrical
and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA

*To whom correspondence should be addressed.
Associate Editor: lvo Hofacker

Received on November 27, 2014; revised on September4, 2015; accepted on September 23, 2015

Abstract

Contributions: We developed a new lossless compression method for WIG data, named smallWig,
offering the best known compression rates for RNA—seq data and featuring random access func—
tionalities that enable visualization, summary statistics analysis and fast queries from the com—
pressed files. Our approach results in order of magnitude improvements compared with bigWig
and ensures compression rates only a fraction of those produced by cWig. The key features of the
smallWig algorithm are statistical data analysis and a combination of source coding methods that
ensure high flexibility and make the algorithm suitable for different applications. Furthermore, for
general—purpose file compression, the compression rate of smallWig approaches the empirical en—
tropy of the tested WIG data. For compression with random query features, smallWig uses a simple
block—based compression scheme that introduces only a minor overhead in the compression rate.
For archival or storage space—sensitive applications, the method relies on context mixing tech—
niques that lead to further improvements of the compression rate. Implementations of smallWig
can be executed in parallel on different sets of chromosomes using multiple processors, thereby
enabling desirable scaling for future transcriptome Big Data platforms.

Motivation: The development of next—generation sequencing technologies has led to a dramatic
decrease in the cost of DNA/RNA sequencing and expression profiling. RNA—seq has emerged as
an important and inexpensive technology that provides information about whole transcriptomes of
various species and organisms, as well as different organs and cellular communities. The vast vol—
ume of data generated by RNA—seq experiments has significantly increased data storage costs and
communication bandwidth requirements. Current compression tools for RNA—seq data such as
bigWig and cWig either use general—purpose compressors (gzip) or suboptimal compression
schemes that leave significant room for improvement. To substantiate this claim, we performed a
statistical analysis of expression data in different transform domains and developed accompanying
entropy coding methods that bridge the gap between theoretical and practical WIG file compres—
sion rates.

Results: We tested different variants of the smallWig compression algorithm on a number of integer—
and real— (floating point) valued RNA—seq WIG files generated by the ENCODE project. The results reveal
that, on average, smallWig offers 18—fold compression rate improvements, up to 2.5—fold compression
time improvements, and 1.5—fold decompression time improvements when compared with bigWig. On
the tested files, the memory usage of the algorithm never exceeded 90 KB. When more elaborate context
mixing compressors were used within smallWig, the obtained compression rates were as much as 23
times better than those of bigWig. For smallWig used in the random query mode, which also supports
retrieval of the summary statistics, an overhead in the compression rate of roughly 3—17% was

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

 

OXFORD

173

91% ‘09 1sn3nv uo sopﬁuv $01 111110;th JO Anus/xtqu 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(1111] 11101; popeommoq

174

Z. Wang et al.

 

introduced depending on the chosen system parameters. An increase in encoding and decoding time
of 30% and 55% represents an additional performance loss caused by enabling random data access.
We also implemented smallWig using multi—processor programming. This parallelization feature de—
creases the encoding delay 2—3.4 times compared with that of a single—processor implementation, with
the number of processors used ranging from 2 to 8; in the same parameter regime, the decoding delay

decreased 2—5.2 times.

Availability and implementation: The smallWig software can be downloaded from: http://stanford.edu/
~zhiyingw/smallWig/smallwig.html, http://publish.illinois.edu/milenkovicl, http://web.stanford.edu/~tsachy/.

Contact: zhiyingw@stanford.edu

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

Next—generation sequencing technologies have resulted in a dramatic
decrease of genomic data sequencing time and cost. As an illustrative
example, the HiSeq X machines introduced by Illumina in 2014 en-
able whole human genome sequencing in less than 15 h and at a cost
of only $1000 (http://www.illumina.com/systems/hiseq—x—sequencing—
system.ilmn). A suite of other—seq techniques has closely followed this
development (for a comprehensive overview, see http://res.illumina.
com/documents/products/research_reviews/), including the by now
well—documented RNA—seq method. RNA—seq is a shotgun sequenc—
ing technique for whole transcriptomes (Marioni et al. 2008) used for
quantitative and functional genomic studies. In addition to generating
sequence—related information, RNA—seq methods also provide dy—
namic information about gene or functional RNA activities as meas—
ured by their expression (abundance) values. This makes RNA—seq
techniques indispensable for applications such as mutation discovery,
fusion transcript detection and genomic medicine (Wang et al. 2009).
As a result, the volume of data produced by RNA—seq methods can be
foreseen to increase at a much faster rate than Moore’s law. It is
therefore imperative to develop highly efficient lossless compression
methods for RNA—seq data.

The problem of DNA and RNA sequence and expression com—
pression has received much attention in the bioinformatics commu-
nity. Compression methods for whole genomes include direct
sequence compression (e.g. Cao et al. 2007; Pinho et al. 2011;
Tabus and Korodi 2008) and reference—based compression schemes
(e.g. Kuruppu et al. 2011; Pinho et al. 2012; Wang and Zhang
2011). The former class of methods explores properties of genomic
sequences such as small alphabet size and large number of repeats.
The latter techniques use previously sequenced genomes as refer—
ences with which to compare the target genome or sequencing reads,
leading to dramatic reductions in compressed file sizes. Related simi—
larity—discovery—based schemes are usually applied to a large collec—
tion of genomes and they achieve very small per genome
compression rates (e.g. Deorowicz et al. 2013). Moreover, recent
work also includes the compressive genomics paradigm, which
allows for direct computation and alignment on compressed data
(Loh et al. 2012). The aforementioned methods and some informa—
tion—theoretic techniques to biological data compression were re—
viewed in (Vinga 2013).

For every base pair in the genome, an RNA—seq WIG file con—
tains an integer or ﬂoating—point expression value. Human tran—
scriptome WIG files may contain hundreds of millions of expression
values, which amounts to GB of storage space (e.g. one of the subse—
quently analyzed WIG files randomly chosen from the ENCODE
(Encode Project Consortium 2004) project has a size of 5 GB). WIG
files are usually compressed by bigWig (Kent et al. 2010), which ba—
sically performs gzip compression on straightforwardly preprocessed

data. Unfortunately, the bigWig format does not appear to offer sig—
nificant data volume reductions and about 10% of the tracks from
the UCSC ENCODE hg19 browser in bigWig format take up 31%
in storage space (Hoang and Sung 2014). Recently, another com—
pression suite, termed cWig (Hoang and Sung 2014), was imple-
mented as an alternative to bigWig. The cWig method outperforms
bigWig in terms of compression rate, and random query time, al—
though it still relies on suboptimal compression techniques such as
Elias delta and gamma coding (Salomon 2007).

This work focuses on transform and arithmetic compression
methods for expression data in the WIG format. Since WIG files
capture expressions of correlated RNA sequence blocks, modeling
these values as independent and identically distributed random vari—
ables is inadequate for the purpose of compression. Hence, we first
perform a statistical analysis of expression values to explore their
dependencies/correlations and then proceed to devise a new suite of
compression algorithms for WIG files. Since the WIG format is not
limited to RNA—seq data, our compression methods are also suitable
for other types of dense data, or quantitative measurements, such as
GC content values, probability scores, proteomic measurements and
metabolomic information. The main analytic and algorithmic con-
tributions of our work are as follows:

i. Devising a new combination of run length and delta encoding
that allows for representing the expression data in highly compact
form. As part of this procedure, we identiﬁed runs of locations
with the same expression value and then computed the differences
of adjacent run values. The resulting transformed sequences are
referred to as run difference sequences and specialized statistical
analysis of difference sequences constitutes an important step to—
wards identifying near—optimal compression strategies.

ii. Analyzing the probability distributions of the difference se—
quences and inferring mixture Markov models for the data. As
part of this step, we estimated information—theoretic quantities,
such as the (conditional) entropy, to guide us in our design and
evaluation process. More precisely, we ﬁrst ﬁtted power—law
distributions to the empirical probability distributions of the
difference sequences. Second, we showed that strong correl—
ations exist between adjacent run differences, while there exists
only a relatively small correlation between the sequences of run
length differences and that of the corresponding run expression
differences. These ﬁndings provide a strong basis for performing
separate compression of the run length and the expression
information.

iii. Developing arithmetic encoders for compression of the differ—
ence sequences, including options such as basic arithmetic cod—
ing and context—mixing coding based on the work in Mahoney
(2002). In this step, we were guided by the results of the

91% ‘09 1sn3nv uo sojoﬁuv soq ‘BTUJOJTIBD aIo Amie/xtqu 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(1111] 11101; popeommoq

smallWig: parallel compression of RNA-seq WIG ﬁles

175

 

statistical analysis and performed alphabet size reduction in the
difference sequences and subsequent run length and run expres—
sion compression. With this step, we were able to achieve 17—
fold improvements in the compression rate when compared
with bigWig: as an illustration, a typical WIG ﬁle of size 5 GB
was compressed to roughly 64—69 MB, depending on the user—
deﬁned operational mode; in comparison, traditional gzip and
the bigWig (Kent et al. 2010) compressors produced ﬁles of
sizes 1.1 GB and 1.2 GB, respectively.

Our new compression algorithm follows the standard require—
ments for expression data representation/visualization by allowing
random access features via data blocking and separate block com—
pression. It also encodes data summary statistics, akin to bigWig
data formats. Furthermore, smallWig has two implementation
modes, one of which runs on a single processor and another which
uses multiple processors in parallel. The parallelized version of the
algorithm offers significant savings in computational time, with
identical rate performance as the serial version.

The remainder of the article is organized as follows. Section 2
provides the idea behind our sequence transformations and coding
methods. Section 3 contains our statistical analysis. A detailed de—
scription of the smallWig algorithm is provided in Section 4.
Compression results and a comparative study of compression meth—
ods is given in Section 5. A discussion of our findings and concluding
remarks are given in Sections 6 and 7, respectively.

2 Methods: sequence transformations

We start our analysis by introducing WIG data transforms that
allow for efficient run length encoding and by explaining how to use
difference values in subsequent compression steps. To illustrate
some of the concepts behind our analysis, we make use of a WIG file
from the ENCODE project (Encode Project Consortium 2004) per—
taining to RNASeq cell line GM12878, for which the RNA fraction
is Long PolyA—l— and the compartment is Nucleus.

Throughout, we use capital letters to represent sequences, and
lower case letters to represent elements in sequences. We write
[a] : {1,2,  ,a}, for any positive integer a E NT, and
[a, h] = {a, a —I— 1, ... ,h}, for two positive integers a g h.

The WIG files of interest comprise two sequences:

° The Location Sequence A : (a1,a2,  ,aM), where M denotes
the length of the sequence. Sequence A contains chromosomal
positions (or locations) satisfying a,- E NT for all iE [M], and
a,- < a,-+1 for all iE [M — 1]. This sequence typically contains
consecutive indices of the base pairs, for which a,-+1 : a,- —l— 1, ex-
cept for skipped locations with expression value equal to zero,
for which a,-+1 > a,- —I— 1.

° The Expression Sequence B : (h, [92, . .. ,hM). The sequence B
contains expression values [9,- E IR. The his indicate the number
of RNA transcripts that include location a,-. Note that the se—
quences A and B have the same length.

A run is defined as a sequence of consecutive locations with identical
expression value. The number of locations in the run is called the
run length, and the corresponding expression value is called the run
expression. Note that if for some integers i g 1', one has at+1 : at —I— 1
for all t E [i,j — 1], and h,- = [9,11 = . . . : by, then the sequence cor—
responds to a run of length j — i —I— 1 with run expression equal to b,.
On the other hand, if for some integer i there exist skipped locations,
i.e. locations for which a,-+1 > a,- —I— 1, and hi, [9,11 7E 0, then the run
is of length a,-+1 — a,- — 1 with corresponding run expression value

equal to 0. Thus, there is a 1—1 mapping from the sequences A and B
to the run length and run expression sequences described below:

,cN), c,- E N+,i E [N],
a sequence of run lengths that describes the runs of consecutive

° The Run length Sequence C : (c1,c2, 

locations with identical expression values. Alternatively, one
may deﬁne the sequence by stating that for locations conﬁned to

[2:21 ct —I— 1, ct], the expression values are identical. Here,
N denotes the number of runs. If No denotes the number of
skipped runs of value 0 in a WIG ﬁle, then  c,- : M —1— N0.

° The Run Expression Sequence D : (d1,d2,  ,dN), d,- E IR,
i E [N], a sequence of expression values that corresponds to the
runs. More precisely, the sequence speciﬁes that the ith run has
expression value d,, for all i E 

For our running example, the original WIG sequences were of length
N : 3.2e —l— 8, while the run and difference sequences were of length
M : 3.9e —I— 7. Note that a similar transform is used in the bigBed
format, which provides a more succinct representation of sparse
WIG data (we will revisit the Bed format in the results section).
Since adjacent runs tend to have similar lengths and expressions, the
differences between consecutive runs may lead to further compac—
tion of WIG information. To describe the difference sequences, let
(:0 = 0,610 = 0.

° The Run length Difference Sequence, X : (x1,x2,  ,xN), is
deﬁned by x,- = c,- — c,-_1, so that x,- E Z for all i E 

° Run Expression Difference Sequence, Y = (321,322, . . . ,yN), is
deﬁned by y,- = d,- — di_1, so that y,- E IR for all i E 

Here, we also point out that a related run length transformation
has been investigated in BedGraph format, while the idea of run
length and difference—value transformations has been studied in
(Hoang and Sung 2014). In the latter work, the authors also demon—
strated the potential benefit of using these transformations on WIG
data.

3 Methods: statistical data analysis

In what follows, we describe how to fit empirical probability mass
functions and compute empirical entropies for the run length and
run expression difference sequences X, Y. Moreover, we study
higher order dependencies of adjacent elements within the sequences
X and Y, as well as dependencies between the sequences X and Y.

3.1 Fitting empirical probability mass functions
We computed the empirical frequencies for the run expression and
run length difference sequences. Both sequences roughly follow a
power—law distribution, with probability density function

w» = “£1 (2)“

which can consequently be used to approximate the empirical prob—

 

ability mass function [similar to the method in Pavlichin et al.
(2013)]. Therefore, we can parameterize the two empirical distribu—
tions with only two parameters, or and ﬂ. Goodness of fit may be
estimated via the standard Kolmogorov—Smirnov statistics or some
other means. For a more detailed analysis of the empirical probabil-
ity distributions, the reader is referred to the Supplementary
Material.

91% ‘09 1sn8nv uo sojoﬁuv soq ‘121u103up23 aIo Amie/xtqu 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁ(1111] 11101; pop1201umoq

176

Z. Wang et al.

 

3.2 Empirical entropy computation
Next, we propose three correlation models for the difference se—
quences X, Y and estimate the entropy of their underlying
distributions.

A detailed description of these models may be found in the
Supplementary Material.

Let Z, W be discrete random variables with alphabet Z, W, re-
spectively. The Shannon entropy of Z is defined as

H(Z) = —Z Pz(z)log2PZ(z).

zEZ

Similarly, the conditional entropy of Z given W is defined as

H (ZIW) = — Z PMWZ lew(zlw)10g2lew(zlw)-
wEW zEZ

In what follows, we assume that X, Y are two random sequences of
lengthNofthe form (X1,X2,  ,XN) and (Y1, Y2,  ,YN).

Independent run difference model. Assume that the sequences X,
Y are independent and that the elements of X (Y) are independent
and identically distributed. The entropy in this case reads as
H] = H(X1) -I— H(Y1).

Markov run difference model. Again we assume that the se-
quences X, Y are independent, but let X (Y) be a first order Markov
sequence. We write the entropy under this model as
HM =  -I— 

Paired sequence model. We assume the sequence of pairs ((X1,
Y1), (X2, Y2), . . . , (XN, YN)) is a first order Markov sequence.
Thus, the entropy formula reads as Hp : H (X2, Y2|X1, Y1).

In the Supplementary Material, we list the entropies of all the afore—
mentioned models and for 14 Wig and 10 different BedGraph files,
computed using the information theoretic tools described in (Jiao et al.
2015). As one may see, the entropy is 19—5 6% smaller for the Markov
model than the independent run difference model. In most cases, this
reduction in entropy may be attributed to dependencies in the run
length differences. In other words, run length values are more likely to
be affected by adjacent run length values. On the other hand, consider—
ing dependencies between the run length differences and run expression
differences only reduces the entropy by about 5—16%. As a result, the
most effective compression strategy appears to be separate compression
of the difference sequences X and Y.

Because of the large variations in the run expression and run
length difference values, computing and storing all conditional prob—
abilities (about 1010 such entries) under the Markov model requires
very large memory. Hence, we first focus on a compression algo—
rithm for the independent model and then discuss our generalized
compression scheme based on context mixing, which requires speci-
alized means for overcoming the memory overﬂow problem.

4 Compression algorithms

We start by describing our basic compression algorithms based on
arithmetic coding and then show how to enable random queries
within the given algorithmic coding framework. In addition, we de—
scribe how to reduce the compressed file sizes even further via con—
text—mixing methods. We conclude this section by introducing
parallelization techniques for the proposed algorithms. Diagrams of
our compression and decompression architectures are given in the
Supplementary Material.

To compress the RNA—seq expressions, we used two individual
arithmetic encoders and decoders for the difference sequences X, Y
defined in Section 2. We observe that since expression values can be
real valued, any errors in computing the expression differences may

cause error propagation during decompression. As a result, the ex—
pression difference alphabet has to be stored as well, with a preci—
sion large enough to allow for correct decompression.

Arithmetic compression (Rissanen and Langdon 1979) is an en—
tropy coding method that converts the entire input sequence into a
range of values (interval) determined by its cumulative frequency.
On length—n sequences in Z", one defines a total order W 4 Z by
requiring that W precedes Z lexicographically. For a sequence Z, its
code word is the binary representation of a real number between
ZWAXRZ P(W) and ZWAXRZ P(W) —I— P(Z). For sequences with in—
dependent and identically distributed entries, arithmetic coding is an
entropy—approaching compression scheme, given that the distribu—
tion of the sequence is known.

To ensure small computational complexity, we used arithmetic
compression algorithms with range encoding (Martin 1979) and
some techniques from the package rangemapper by Polar (http://
ezcodesample.com/reanatomy.htmlPSource=To—I—article—I—and—I—sour
ce—I—code). Our implementation is similar to the original version of
arithmetic coding, except that the underlying probabilities are repre—
sented with binary sequences of fixed length, which allows for more
efficient computations. Moreover, encoding/decoding may be per—
formed in a streaming fashion. A buffer is used to store the “unre—
solved range” depending on the yet unobserved part of the sequence.
In our implementation, the precision of the buffer was limited to 30
bits so as to control the number of operations performed. Unlike
range encoding, in which the calculations are performed base 256,
we used base 2 so as to achieve the best compression rate.

To facilitate random queries during decompression, we divided
the difference sequences into blocks of fixed length. The length—
subsequently termed bloc/e size—can be chosen by the user, to allow
for desired trade—offs between compression rate and query time.
Since the compressed sequences have lengths that vary from block to
block, we also store the address of each block. Moreover, to quickly
obtain the original sequences from the difference sequences, we also
store the (start location, expression, run length) triple (a,, h,, c,) for
every starting element in a block. For this purpose, we implemented
a simple binary search procedure originating from the start location
to identify the blocks corresponding to a random query.

For fast visualization of the WIG data and summary statistics
analysis, we stored an additional summary vector for every block.
The summary vector contains the following six values: (i) the min-
imum expression value in the block; (ii) the maximum expression
value in the block; (iii) the mean value of the block; (iv) the standard
deviation of the block; (v) the number of locations covered in the
block and (vi) the total number of locations within the block. If a
random region is queried, the aggregated summary vector for
the queried region is computed as follows. First, all blocks that are
completely included in the queried region are identified, and their
summary information is computed from the summary vectors of the
blocks; then, the starting and ending blocks partially contained
within the query region are retrieved and their summary information
is computed directly from the symbols in the blocks. The complexity
of the statistics query is linear in the number of queried blocks and
in the block size.

To explore dependencies among elements in the run sequences,
we used the context—mixing algorithm implemented as part of the
lpaq1 package (Mahoney 2002). To illustrate the idea behind con—
text mixing, we focus on context—tree weighting (Willems et al.
1995). In this model, we assume that every element xt E {0, 1} in a
sequence is generated based on a suffix set S, which can be repre—
sented by a degree—two tree of depth not more than D. Here, D is a
parameter that indicates the dependency between symbols at a

91% ‘09 1sn8nv uo sojoﬁuv soq ‘121u103up23 aIo Amie/xtqu 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁ(1111] 11101; pop1201umoq

smallWig: parallel compression of RNA-seq WIG ﬁles

177

 

certain distance in the sequence and consequently determines the mem—
ory requirements of the algorithm. The root of the tree is indexed by
the empty string, while the left (or right) edge of every node represents
a 1 (or 0), and every node corresponds to the string associated with its
path to the root. Each suffix/leaf s is associated with a parameter 0,,
which equals the probability of the next source symbol xt+1

being equal to 1 conditioned on the suffix of the semi—infinite sequence

1‘
t

the conditional probability of a symbol given the preceding |s| symbols

 xt_2xt_1xt beings. Let 05(xﬁ) : P(xt+1 : 1|x _|S|+1 : s) denotes

being equal to s, where we denote by  the string
x,-,x,-+1,  ,x,-, for is]:

The probability 0, (called the model parameter) is usually not
known but can be estimated using the Krichevsky—Trofimov (KT)
method (Krichevsky and Trofimov 1981). Moreover, the actual tree
generating the sequence (called the model) is also unknown. The
context—tree weighting algorithm takes a weighted sum over all tree
models with depth not exceeding D. The redundancy introduced by
the lack of knowledge of both the parameter and the model is
bounded, and context—tree weighting is optimal since it achieves the
lower bound of redundancy derived in Rissanen (1984). More de—
tails regarding this method are provided in the Supplementary
Material.

The context-mixing algorithm lpaq1 (Mahoney 2002) that we
used in our implementation predicts the next bit based on the previ—
ous six bytes, as well as the last matching context. Hash tables are
built to store the history and the context. During compression, the
algorithm adaptively updates the probability distribution of the next
bit based on its current prediction and uses arithmetic coding with
time varying probability values. Since adaptive schemes perform
poorly for short sequence lengths, the context—mixing scheme is only
recommended in the one—block compression mode which does not
allow random query. As a result, context—mixing compression
should be used for archival storage.

To speed up compression/decompression, we also implemented a
parallel scheme for arithmetic coders with random query. The
scheme partitions the original sequences based on its chromosome
index and compresses each substring on a separate processor.
Details of this implementation and its performance are discussed in
the Results Section.

5 Results

We tested our compression algorithm on 14 integer—valued WIG
files with sizes ranging from 1.5 to 5.3 GB and on 10 integer and
real—valued BedGraph files. All Wig files contain human transcrip-
tome RNA—seq data from the ENCODE hg19 browser. Since
smallWig is designed for WIG files, here we mainly focus on the 14-
file set. A more detailed report on the performance of smallWig on
both file sets can be found in Supplementary Material.

We measured the performance of smallWig and other existing al-
gorithms through the:

° Compression rate (compression ratio), the compressed ﬁle size
divided by the original ﬁle size.

° Running time of: (i) the encoding, (ii) the decoding and (iii) the
random query process.

Figure 1 shows the compression rates achieved by various vari—
ants of smallWig, compared with the rates of gzip, bigWig and cWig
through BedGraph. The depicted entropy is under the independent
run difference model. With arithmetic coding, our algorithm offers

18—fold rate improvements compared with bigWig. In fact, the com—
pressed file size of our running example is only 1/80 of the original
WIG file. Furthermore, the compression rate is only 1.6% larger
than the empirical entropy and may be attributed to storing the em—
pirical probabilities. With context-mixing, one can further improve
the compression rate to 23 times compared with bigWig. For com—
pression with random queries, smallWig offers 17—fold rate improve—
ments compared with bigWig.

According to the report in Hoang and Sung (2014), the compres—
sion rate of the state—of—the art cWig method is about 3.1 times bet—
ter than that of bigWig. However, we found that one can obtain an
even better rate by first converting a WIG file into a BedGraph file
and then converting the BedGraph file to cWig with some simple
additional processing (BedGraph files are compact representations
of WIG files that fundamentally rely on run length coding). Our se—
quential WIG—BedGraph—cWig pipeline performs about 8.5 times
better than bigWig. The newly introduced smallWig method still
performs twice as well as the proposed modification of cWig. For
databases containing TB/PB of WIG files, a 2-fold reduction in file
sizes may lead to exceptionally important storage cost savings.

In Figure 2, we present the running time of smallWig encoding/
decoding schemes, as well as those of gzip, bigWig and cWig. With
arithmetic coding, smallWig has a 2.5 times smaller encoding and
1.5 times smaller decoding time compared with that of bigWig.
Arithmetic coding with random query has 1.9 times smaller encod—
ing time than bigWig. Context-mixing algorithms are computation—
ally intensive compared with arithmetic coding and require
significantly longer running time.

To compare the effect of different block sizes used for random
query on compression rate and encoding/decoding time, we refer the
reader to Figure 3. In the experiments, the block sizes ranged from
512 to 4096. To enable random query, we introduced a 3—17%
overhead in compression rate and a 30% and 55% overhead in
encoding and decoding time, respectively.

Table 1 lists the random query time. Note that the start positions
(and for long queries the end positions) of the queries were gener—
ated uniformly at random among all allowed chromosomal loca—
tions for every chromosome. For short queries, the query length was
fixed to 1000, so that one query falls within a single block; in this
case, the query time corresponds to the time needed to retrieve the
corresponding block. One can see that smallWig is comparable in
performance to bigWig for short queries and runs about three times
faster for long queries. It is also comparable to cWig for both types

Compression Rate

0-2500 0.2301
0.2126
0.2000
0.1500
0.1000
00500 0.0270
0.0125 0.0136 00102 0.0123

0.0000

gzip bigWig cWig with our smallWig smallWig w/ smallWig w/ empirical

modification random query context mixing entropy

Fig. 1. Compression rates achieved by gzip, bigWig (Kent at al. 2010), cWig
(Hoang and Sung 2014) through BedGraph and smallWig methods, which en-
compass arithmetic coding, arithmetic coding on blocks of size 1024 and con-
text-mixing algorithms using lpaq1 (Mahoney 2002). To test cWig, we
constructed our own WlG-BedGraph-cWig pipeline. All presented results are
averaged over 14 sample files taken from ENCODE hg19. A more detailed
table is included in the Supplementary Material

91% ‘09 1sn8nv uo sojoﬁuv soq ‘BTUJOJTIBD aIo Amie/xtqu 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁdllq 11101; pop1201umoq

 

 

 

 

 

 

178 Z. Wang et al.
Running Time Table 2. Maximum memory usage during encoding and decoding
025 in bytes for gzip, bigWig, cWig, smallWig and smallWig with
€ 62 context mixing on the tested files
3 .
g 0-15 gzip bigWig cWig smallWig smallWig cntx
3" 0.1
O
E 0.05 _ = _ I _ enc. 664 2315M 1935M 89K 90K—1200M
0 ' _ dec. 932 24K 45K 19K 90K—1200M
gzip bigWig cWig w/ our smallWig smallWig w/ smallWig w/
modiﬁcation random query context
mixing . _
Compression Decompression Running Time v.s. #Processors

Fig.2. Encoding and decoding time of gzip, bigWig and smallWig algorithms
using arithmetic coding, arithmetic coding on blocks of size 1024 and our con-
text-mixing algorithm. The encoding/decoding time is expressed in seconds
per MB of the original WIG file. All the results were averaged over 14 sample
files from ENCODE hg19. The error bars indicate the standard deviation. A
more detailed table is included in the Supplementary Material

Compression for Different Block Sizes

0.05 0.015
1: 0.04 0.012 ,1,
OJ H
a m
m M
c 0.03 0.009 g
.9 .5
3 m
2 0.02 0.006 g
g e

O
8 0.01 0.003 U

0 0

no block block = 512 block = 1024 block = 2048 block = 4096

compression speed (s/MB) decompression speed (s/MB) rate

Fig. 3. Compression rate, encoding time and decoding time given different
block sizes fed to arithmetic encoders. The label "no block" indicates that the
whole sequence is compressed as a single block. The encoding/decoding
time is expressed in seconds per MB in the original file. The y-label is for both
the rate and the speed (s/MB). All the results are averaged over 14 sample
files from ENCODE hg19

Table 1. Comparison of smallWig, bigWig and cWig with respect to
random query time

 

 

Query Type Measure bigWig cWig smallWig

Long query Average (s/bp) 1.99E—7 5.20E—8 5.87E—8
std 4.74E—6 6.48E—8 6.08E—7

Short query Average (5) 0.0565 0.0574 0.0711
std 0.0515 0.1360 0.0176

 

We list the average query time in seconds per queried location for long
queries, and the average query time in seconds for short queries, together with
the corresponding standard deviations, over all 14 WIG ﬁles and 240 queries
on each ﬁle.

of queries. Moreover, to facilitate visualization, in the random query
functions, smallWig outputs the exact summary information to—
gether with the queried location—expression pairs. On the other
hand, the bigWig summary function only outputs information cor—
responding to the overlapped blocks but not to that of the exact
queried region.

We observe that for all the tested files, smallWig with arithmetic
coding had a relatively small memory usage, as listed in Table 2. In
particular, during most of the compression tests, the memory usage
was less than 10 KB. With different user—defined parameters,
smallWig with context mixing had higher and more variable mem-
ory usage, ranging from 90 KB to 1200 MB. We note here that since
gzip does not offer random access and summary information, its
memory usage is smaller than that of the other algorithms.

 

 

 

 

 

 

 

0.04
Compression
- - - Decompression
0.03
‘57
1% I
E \
} 0.02 ‘.
. \\
(D
.§ \
l- \:5
‘s2
0.01 ~~~~
' - - - - -
- i - - - -
- - l
0 I l l I
0 2 4 6 8

Number of Processors

Fig. 4. Compression time versus the number of processors used in smallWig.
The time is expressed in seconds per MB of the original WIG file. The block
size is 512, and the results are averaged over 14 sample files from ENCODE
hg19. Error bars indicate the standard deviation

In Figure 4, we show the running time of parallel multiprocessor
compression methods. The encoding time is decreased by 2—3.4
times as the number of processors increases from 2 to 8.
Furthermore, the decoding time is decreased by 2—5.2 times. The
time does not decrease linearly since we used a uniform sequence
partition procedure for individual chromosomes, and chromosomes
have largely different lengths. Moreover, after every step in the algo—
rithm (e.g. sequence transformation, empirical probability computa—
tion, arithmetic coding), some components of the pipeline have to
pause until all processors have finished their computations and their
information is aggregated.

We also tested smallWig on 10 WIG files that were generated
from BedGraph files including integer—valued as well as floating—
point—valued expressions. The average compression rates are shown
in Figure 5. Note that BedGraph already takes into account the run
length transformations and hence the compression rate improve—
ments for these files are not as large as those for WIG files. For inte—
ger—valued files, smallWig is 5 and 1.8 times more efficient than
bigWig and cWig, respectively. For floating point—valued files,
smallWig is 4.3 and 1.9 times more efficient than bigWig and cWig,
respectively. More details about these tests can be found in
Supplementary Material.

6 Discussion

In what follows, we describe the differences in compression strategies
used by various methods and attempt to intuitively explain the
improved performance of smallWig compared with cWig and bigWig.
All three algorithms—bigWig, cWig and smallWig—use run
length encoding. Both cWig and smallWig use delta encoding.
Moreover, all three algorithms use blocks of a certain size for ran-
dom query purposes: bigWig and cWig only operate with fixed

91% ‘09 1sn§nv uo SQIQBIIV soq ‘121u10111123 aIo Amie/qua 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁdllq 11101; pop1201umoq

smallWig: parallel compression of RNA-seq WIG ﬁles

179

 

Compression Rate for Bed Graph

 

0.25

0.2

 

0.15 -—

Rate

0.05 —— — _

 

 

int float

gzip bigWig cWig w/ our modiﬁcation smallWig w/ rand. access

Fig. 5. Compression rates for BedGraph files. The bars on the left correspond
to compression rates for integer-valued files, and the bars on the right corres-
pond to compression rates for floating-point-valued file formats. Since the
tested files have a large variance on their sizes, we take the sum of the com-
pressed file sizes divided by the sum of the original BedGraph file sizes as
our average rate

blocks of size 512, while smallWig allows for variable sizes.
Furthermore, smallWig also allows for flexible indexing density,
with indexing used to record the chromosomal locations that corres—
pond to the blocks. Compared with bigWig and cWig, smallWig al-
gorithms use a fairly simple data structure for indexing data and
block addresses. As a combined result of these properties, for all the
files compressed during testing, smallWig had a comparable running
time for random query to that of bigWig.

To compress the run sequences, bigWig uses gzip [based on
LZ77 (Ziv and Lempel 1977)] on each block. As already pointed
out, gzip is a universal source coding scheme that does not rely on
prior knowledge about the probability distributions. It approaches
the entropy rate if the source is stationary and ergodic and as the se—
quence length goes to infinity. Since the alphabet sizes of the se—
quences are fairly large (a few thousand to several tens of thousand)
but the block sizes are only 512, gzip offers somewhat poor per—
formance. On the other hand, cWig uses Huffman codes for fre—
quent values, and Elias delta codes for less frequent values. Both
codes perform symbol—by—symbol encoding.

Assume that the data source is producing independent and iden—
tically distributed outputs with probability mass function  Since
the code word of a symbol x must be represented by a binary se—
quence, say of length €(x), the individual symbol redundancy r(x)
:€(x) — logzﬁ is a real number in [0,oo). Even for Huffman
encoding (i.e. the optimal prefix encoding), the expected per—symbol
redundancy may be large enough to create “visible” rate losses.
There exist a number of results on the upper and lower bounds of
the expected redundancy r : EX(r(X)) for a random variable X. For
example, Gallager (1978) showed an upper bound based on the larg—
est symbol probability; Capocelli and De Santis (1991) bounded the
redundancy both from above and below based on the largest and the
smallest symbol probability and Mohajer et al. (2012) showed a
tight upper and lower bound based on one known symbol probabil—
ity p. In particular, in the latter case, the redundancy is lower
bounded by r2 mp — H(p) — (1 — p)log2(1 — 2"”), where m > 0 is
either [—logzp] or[—log2p], depending on which value minimizes
the overall expression. Here, H denotes the binary Shannon entropy
function: H(p) : —plog2p — (1 — p)log2(1 — p). For our running
example, the run expression difference x:—1 has the largest prob—
ability, p(—1) : 0.3374, which leads to the corresponding redun—
dancy of Huffman coding r20.0275. For a given distribution and a
given symbol—by—symbol codebook which may not be optimal, there

exists a non—negative and non—negligible coding redundancy r; on
blocks of length 512, the overall redundancy equals 512r, which is
at least 14 bits per block for Huffman codes. If a different subopti—
mal code or unmatched Huffman code is used, this redundancy may
be even larger. At the same time, arithmetic coding only causes a re—
dundancy up to 2 bits per block if the probability distribution is
known. As a result, smallWig files are significantly smaller than
cWig files. Furthermore, smallWig is ﬂexible in terms of the block
size and enables context—mixing as well as parallel processing.

7 Conclusions

We studied compression methods for RNA—seq expression data. We
proposed a new algorithm, termed smallWig, which achieves a com—
pression ratio that is at least one order of magnitude better than cur—
rently used algorithms. At the same time, the algorithm also improves
the running time and ﬂexibility of random access. The presented results
included detailed performance evaluations of smallWig in the standard,
random access, context mixing and parallel operation mode.

Acknowledgements

The authors thank the editor and the anonymous reviewers for their com-
ments and suggestions.

Funding

The work is partially supported by the Center for Science of Information
(CSoI), funded under grant agreement CCF-0939370, and by NIH BD2K
1U01CA198943-01.

Conﬂict of Interest: none declared.

References

Cao,M.D. et al. (2007) A simple statistical algorithm for biological sequence com-
pression. In: Data Compression Conference, 2007. DCC’07. IEEE, pp. 43—52.
Capocelli,R. and De Santis,A. (1991) New bounds on the redundancy of

Huffman codes. IEEE Trans. Inf. Theory, 37, 1095—1104.

Deorowicz,S. et al. (2013) Genome compression: a novel approach for large
collections. B ioinformatics, 29, 25 72—25 78.

Encode Project Consortium. (2004) The ENCODE (ENCyclopedia of DNA
elements) project. Science, 306, 636—640.

Gallager,R.G. (1978) Variations on a theme by Huffman. IEEE Trans. Inf.
Theory, 24, 66 8—674.

Hoang,D.H. and Sung,W.-K. (2014) CWig: compressed representation of Wig-
gle/bedgraph format. Bioinformatics, 30, 2543—2550.

Jiao,]. et al. (2015) Minimax estimation of functionals of discrete distribu-
tions. IEEE Trans. Inf. Theory, 61, 2835—28 85.

Kent,W.]. et al. (2010) Bigwig and bigbed: enabling browsing of large distrib-
uted datasets. Bioinformatics, 26, 2204—2207.

Krichevsky,R. and Troﬁmov,V. (1981) The performance of universal encod-
ing. IEEE Trans. Inf. Theory, 27, 199—207.

Kuruppu,S. et al. (2011) Optimized relative Lempel-Ziv compression of gen-
omes. In: Proceedings of the Thirty-Fourth Australasian Computer Science
Conference-Volume 1 13. Australian Computer Society, Inc., pp. 91—98.

Loh,P.-R. et al. (2012) Compressive genomics. Nat. Biotechnol, 30, 627—630.

Mahoney,M.V. (2002) The paq1 data compression program. Draft, jan, 20.

Marioni,].C. et al. (2008) RNA-seq: an assessment of technical reproducibility
and comparison with gene expression arrays. Genome Res., 18, 15 09—15 17.

Martin,G.N.N. (1979) Range encoding: an algorithm for removing redundancy
from a digitised message. In: Proceedings Institution of Electronic and Radio
Engineers International Conference on Video and Data Recording.
Institution of Electronic and Radio Engineers.

91% ‘09 1sn§nv uo SQIQBIIV soq ‘121u10111123 aIo Amie/qua 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁdllq 11101; pop1201umoq

180

Z. Wang et al.

 

Mohajer,S. et al. (2012) Tight bounds on the redundancy of Huffman codes.
IEEE Trans. Inf. Theory, 58, 6737—6746.

Pavlichin,D.S. et al. (2013) The human genome contracts again.
Bioinformatics, 29, 2199—2202.

Pinho,A.]. et al. (2011) On the representability of complete genomes
by multiple competing ﬁnite-context (Markov) models. PLoS One, 6,
e215 88.

Pinho,A.]. et al. (2012) GReEn: a tool for efﬁcient compression of genome
resequencing data. Nucleic Acids Res., 40, e27.

Rissanen,]. (1984) Universal coding, information, prediction, and estimation.
IEEE Trans. Inf. Theory, 30, 629—63 6.

Rissanen,]. and Langdon,G.G. Jr. (1979) Arithmetic coding. IBM]. Res. Deu,
23, 149—162.

Salomon,D. (2007) Variable—Length Codes for Data Compression. Vol. 140.
Springer Science 85 Business Media.

Tabus,I. and Korodi,G. (2008) Genome compression using normalized
maximum likelihood models for constrained Markov sources. In: IEEE
Information Theory Workshop, 2008. ITW’08. IEEE, pp. 261—265.

Vinga,S. (2013) Information theory applications for biological sequence ana-
lysis. Brief. Bioinform, 15, 376—389.

Wang,C. and Zhang,D. (2011) A novel compression tool for efﬁcient storage
of genome resequencing data. Nucleic Acids Res., 39, e45.

Wang,Z. et al. (2009) RNA-seq: a revolutionary tool for transcriptomics. Nat.
Rev. Genet., 10, 57—63.

Willems,F.M. et al. (1995) The context-tree weighting method: basic proper-
ties. IEEE Trans. Inf. Theory, 41, 653—664.

Ziv,]. and Lempel,A. (1977) A universal algorithm for sequential data com-
pression. IEEE Trans. Inf. Theory, 23, 337—343.

91% ‘09 1sn§nv uo SQIQBIIV soq ‘121u10111123 aIo Amie/qua 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁdllq 11101; pop1201umoq

