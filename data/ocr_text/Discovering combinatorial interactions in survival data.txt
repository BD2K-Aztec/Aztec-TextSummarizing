ORIGINAL PAPER

Vol. 29 no. 23 2013, pages 3053—3059
doi:10. 1093/bioinformatics/btt532

 

Systems biology

Advance Access publication September 13, 2013

Discovering combinatorial interactions in survival data
David A. duVerle1 ’*, Ichiro Takeuchi2, Yuko Murakami—Tonami3’4, Kenji Kadomatsu4 and

Koji Tsuda1

1Computational Biology Research Center, National Institute of Advanced Industrial Science and Technology, Tokyo,
Japan, 2Department of Computer Science, Nagoya Institute of Technology, Nagoya, Japan, 3Division of Molecular
Oncology, Aichi Cancer Center, Nagoya, Japan and 4Department of Molecular Biology, Nagoya University Graduate

School of Medicine, Nagoya, Japan
Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: Although several methods exist to relate high-dimensional
gene expression data to various clinical phenotypes, finding combin-
ations of features in such input remains a challenge, particularly when
fitting complex statistical models such as those used for survival
studies.

Results: Our proposed method builds on existing ‘regularization path-
following’ techniques to produce regression models that can extract
arbitrarily complex patterns of input features (such as gene combin-
ations) from large-scale data that relate to a known clinical outcome.
Through the use of the data’s structure and itemset mining techniques,
we are able to avoid combinatorial complexity issues typically encoun-
tered with such methods, and our algorithm performs in similar orders
of duration as single-variable versions. Applied to data from various
clinical studies of cancer patient survival time, our method was able to
produce a number of promising gene-interaction candidates whose
tumour-related roles appear confirmed by literature.

Availability: An R implementation of the algorithm described in this
article can be found at https://github.com/david-duverIe/regularisa
tion-path-following

Contact: dave.duverle@aist.go.jp

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on May 20, 2013; revised on August 19, 2013; accepted on
September 6, 2013

1 INTRODUCTION

From their inception, high-dimensional genomic data, such as
obtained through genome-wide expression microarrays, have
been used to identify genes that affects survival or tumour re-
occurrence time spans among cancer patients (Bovelstad et al.,
2007; Van De Vijver et al., 2002). Survival data generally contain
partially known observations (e. g. when clinical follow-up of the
patient ends before a decisive event) requiring the use of regres-
sion models that can speciﬁcally handle censored data. Cox pro-
portional hazards model (Cox, 1972) is one such model that
combines advantages of both parametric and non-parametric
approaches to statistical inference, making it ideally adapted to
the type of data obtained in clinical trials.

 

*To whom correspondence should be addressed.

Owing to the high dimensionality and small sample size of
gene expression data, it is desirable to add a penalization com-
ponent in ﬁtting the Cox model (Dudoit et al., 2002; Ghosh,
2003; Van De Vijver et al., 2002), with £1-norm often preferred
for its ability to drive sparsity of the model and select a concise
set of variables (gene expression values, mutation types, etc.)
(Gui and Li, 2005; Tibshirani et al., 1997). Different methods
have been suggested (Gui and Li, 2005; Lin and Wei, 1989; Park
and Hastie, 2007) for ﬁtting til-penalized Cox model. Park and
Hastie (2007), in particular, proposed a method to compute the
regularization path of til-penalized Cox model, producing a series
of Cox models that have different levels of complexity and
sparsity.

As for many models in systems biology, it has been widely
shown (Hanahan and Weinberg, 2000; Tibshirani et al.,
2002) that the gene regulatory pathways of cancer involve
non-linear gene interactions. Although models based on linear
combinations of gene expression may accurately approximate
more complex interactions for some tasks, it can be desirable
to speciﬁcally identify combinatorial covariates for such purpose
as the identiﬁcation of synthetic lethal genes (Kaelin, 2005).
However, all current methods rely on the ability to enumerate
potential input variables: although it is computationally feasible
to examine each single gene in such a way (even for a large
microarray), issues of exponential complexity quickly arise
when considering interactions between more than one gene at
a time.

In this article, we extend the approach in Park and Hastie
(2007) to handle combinatorial interactions among genes. We
deal with issues of combinatorial explosion and computational
complexity by taking advantage of itemset mining techniques
(Uno et al., 2004). Using this approach, Virtually limitless com-
binations of genes and phenotypes, grouped in itemsets of boo-
lean variables, can be used as single predictor variables in the
model. Our proposed algorithm computes the regularization
path of til-penalized Cox models that account for the effects of
combinatorial gene interactions on survival.

Beyond proportional hazards models, our itemset-based
method can be applied to any regression model with convex
loss, each time making use of the input’s structure and sparsity
to sidestep complexity issues, while at the same time guarantee-
ing that events along the regularization path (values of the reg-
ularization parameter for which a change occurs in the model
structure) are exhaustively explored.

 

© The Author 2013. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/Iicenses/by/3.0/), which
permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

112 /810's112umo[pJOJXO'sopeuJJOJutotq//:d11q IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no :2

D.A.duVerle et al.

 

In the rest of this article, section 2 ﬁrst outlines our general
approach for adapting existing path regularization techniques to
work with patterns of discretized input features instead of single
continuous values. Section 3 details the mathematical basis for
our algorithm and illustrates its application to proportional
hazard models using Cox’s partial likelihood as loss function
(with further detailed proofs as Supplementary Material).
Finally, section 4 presents qualitative and quantitative results
obtained by applying our method to different survival datasets.

2 APPROACH

2.1 El-penalized maximum likelihood estimation

A common deﬁning feature to many major regression models,
such as generalized linear models (GLM) or previously men-
tioned Cox model, is the use of a loss function to ﬁt the param-
eters of otherwise analytically intractable problems. Adding an £1
penalty term to the original loss criterion results in the typical
estimation problem:

BO») = afglgrlin(—C(y; Xﬁ) + A llﬁ Hi) (1)

where £ denotes the log-likelihood function with respect to the
given data (X, y), B is the vector of coefﬁcients that needs to be
estimated and A the regularization parameter.

For values of A tending towards inﬁnity, all coefﬁcients in B
will be forced to 0, whereas as A decreases, more coefﬁcients will
have non-null values (i.e more predictor variables will be used in
the model estimation).

2.2 Regularization path-following algorithm

Among various methods for solving til-regularized problems
similar to (l), the use of so-called ‘regularization path-following’
algorithms (Hastie et al., 2005; Park and Hastie, 2007) is of par-
ticular interest for their ability to ﬁnely control the number of
active variables in the model, regardless of the dimensionality of
the input. The general idea behind path-following is to study
variations of the A parameter in the space of B coefﬁcient
values (see Fig. l): by decreasing the value of A, starting from
the maximum Amax for which B is non-null, we can ﬁnd a se-
quence of all discrete values of A, for which new coefﬁcients of B
change between null and non-null (corresponding to a particular
predictor variable exiting or entering the regression model).

ﬂog.) @

ﬁ(>\2)

3&1)

 

Fig. 1. Schematic representation of the regularization path in the space of

. . . 
,8. Success1ve values of ,8(Ak) can be approx1mated us1ng 3T

The resulting sequence of Ak and associated optimal B(Ak)
allow us to model the data at varying levels of sparsity.

Park and Hastie (2007) suggested a path-following algorithm
for £1 -regularized GLM that uses a predictor-corrector approach
to efﬁciently ﬁnd all Ak and the coefﬁcients of the model asso-
ciated with each level of regularization. If we deﬁne the ‘active
set’, AM, as the set of non-null indices in the coefﬁcient vector
B(Ak), their algorithm can be deﬁned as a loop over four main
steps:

(1) Predict: Starting with a known B(Ak_1) and Ak: the next
target value of A, estimate B(Ak) using a piecewise linear
approximation of B, under the assumption that A remains
unchanged.

(2) Correct: Solve the associated convex optimization problem
to ﬁnd the exact value of B(Ak) (using the linear approxi-
mation as a warm start).

(3) Update active set: By confronting the new values of B to
the optimality conditions of the problem, update A (i.e.
add/remove predictors from the model). Repeat step 3 if
necessary to adjust B.

(4) Decrement A: Analytically ﬁnd the exact value of Ak+1, at
which the active set will next change.

It is worth noting that, when an til-regularized model is ﬁtted
to high-dimensional small sample data, sparse models are usually
selected (based on some model selection criteria). Therefore, we
do not really have to compute the ‘entire’ regularization path
(from A0 to 0). The algorithm is usually terminated for a value
of A where the size of the active set A is still much smaller than
the input dimension.

Because steps 1 and 2 only use variables in the current active
set A, they can be performed at little computing cost for values
of A where |A| remains much smaller than the number of vari-
ables. Steps 3 and 4 require solving simple equations for each
possible input variable (in linear time of the input’s dimension).

In their work, Park and Hastie (2007) showed that, along with
GLM, their algorithm could also easily be applied to the Cox
proportional hazards model. In fact, it can be shown that their
results hold for any loss-based model ﬁtting task, provided a loss
function that exhibits certain mathematical properties (see sec-
tion 3 and Supplementary Material).

2.3 Finding combinatorial covariates

When the linear model is extended to combinatorial interaction
terms, the input dimension increases exponentially because of the
combinatorial explosion of gene interactions. Of the steps enum-
erated in section 2.2, the predictor and corrector steps only deal
with the small subset of covariates currently in the active set A,
and therefore do not need to be changed. On the other hand,
updating the active set in step 3 and ﬁnding the next value of A at
which an update event will occur in step 4, both potentially re-
quire examining a number of feature combinations that grows
exponentially with the order of the interactions considered.
One practical approach to dealing with issues of combinatorial
explosion and computational complexities in steps 3 and 4 is to
take advantage of the input’s structure to efﬁciently explore its
space. By discretizing our input (gene expressions or other

 

3054

112 [3.10811211an[plOJXO'SODBIIIJOJIIIOIQ/[Zdinq IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV uo ::

Discovering combinatorial interactions

 

clinical data) and considering all possible sets of such binary
variables, we can use itemset mining techniques (Saigo et al.,
2007; Uno et al., 2004) to preserve the computational efﬁciency
of the path-following algorithm despite a high dimensional input.

We show that step 3 can be reduced to a weighted itemset
mining problem, easily solvable using existing optimization tech-
niques (see Methods section 3.1.3), whereas step 4 requires sol-
ving a particular form of fractional programming problem, for
which we developed an efﬁcient pruning approach (see Methods
section 3.1.4). Our method can therefore overcome those com-
putational complexity issues, and identify complex interactions
(between two or more factors) that contribute to the response
model, at varying degrees of sparsity (controlled by the penaliza-
tion component).

2.4 Application to Cox proportional hazards model

We applied our modiﬁed version of the path-following algorithm
to the Cox proportional hazards model, where patient survival
(or any timed event) is used as a response, allowing for missing
data because of right censorship. To estimate this model, we seek
to maximize a so-called log partial likelihood function (see
Methods section 3.2) for a given set of data. As predictors, we
use discretized values of the gene expression levels (see section
4.1).

3 METHODS

In this section, we give a quick overview of the path-following algorithm
ﬁrst presented by Park and Hastie (2007) and the necessary changes to
work on combinatorial interactions:

3.1 Path-following algorithm
Let J(B) be the criterion from (1):

J(B) I= -£(y; Xﬁ) + Alllflll (2)

In the regularization path, we consider the optimal parameter vector B

as a function of the regularization parameter A, and represent the optimal

parameter vector at A as B(A). We can write the optimality condition as
follows:

Harem :=%”Iﬁzm = 0 (3)

Our goal is to compute the path of solutions of (3) for all the A. If we
only consider the range of A where the active set A does not change
(noting B A: the restriction of B to the active set A), the partial change
of the optimality condition (3) with respect to A must satisfy:

moon) 2 g1 gag _

_ 4
8A 8A 313A 8A ( )

3.1.1 Predictor step In each predictor step, we assume that the cur-
rent active set, A, does not change. In the k-th predictor step, we use a
linear approximation to predict B with the current active set:

MAO»)

ﬁAO‘k+l) 1= 3,40%) +0~k+1 - Ale) 3}»

 

|A=Ak (5)

3.1.2 Corrector step We also assume that the active set A does not
change during each corrector step. Any convex optimization algorithm
can be used to minimize the penalized loss function (2). The use of

B A(Ak+1) as an initial starting point ensures that an optimal solution
can be found in a small number of iterations.

3.1.3 Active set update After each corrector step, it is necessary to
identify all new features that should enter A. If we consider the set 73 of
all possible patterns, up to a given length, of binarized input features (6. g.
‘gene A over-expressed and gene B under-expressed) and assign each such
pattern an index value, for any K e {1, . . . , IPI}, we note xi 6 B” (where n
is the total number of observations) the indicator vector for the matching
pattern. Our goal is to identify such values of E that contribute to mini-
mize the loss function (2), and for which the matching value of the par-
ameter vector B should be non-null (noted as Be being ‘active’ and B being
in the ‘active set’ A).
With the feature notation X :2 {xij},,j, we deﬁne:

w- '— in c '— iw-x- (6)
z -— ap-l-xi: Z -— i=1 1 ti
Assuming strong complementarity slackness, we obtain the following
result (see Supplementary Material for detailed proof):

THEOREM 1.
Be is active <=> legl = A (7)

Therefore, if ICgI Z Ak+1 after the corrector step, K (and its associated
parameter Be) must be added to the active set A.

If K were an easily enumerable feature (such as in the case of single gene
expression level), it would be computationally feasible to exhaustively
enumerate all values of Cg for all possible 6. In our case, however, K
can match an arbitrarily long pattern drawn from the power set of all
binarized features; the number of such features grows exponentially with
the maximum size of the patterns, making the problem highly impractical
for sets of >2 or 3 items. However, as long as ce can be rewritten as linear
sums of xii, ﬁnding all such K can be accomplished in reasonable time,
using frequent itemset enumeration techniques.

Because the values w, in the linear sum deﬁned in (6) do not depend on
K (and are constant for A e [Ak+1,Ak]), ﬁnding all items c5 2 Ak+1 is
equivalent to ﬁnding all itemsets with weighted support above Ak+1 (the
symmetrical problem of also ﬁnding {eel — ce 2 Ak+1} is then trivial). To
solve this problem, we use the LCM program (http://research.nii.ac.jp/
~uno/codes.htm) (Uno et al., 2004), which provides an exhaustive enu-
meration of frequent itemsets in guaranteed polynomial time per itemset.

If any variable is added to the active set A, or removed (indices
{6 E AIBg = 0}), we go back to the corrector step (where the new
values of Cg are ﬁrst recomputed). These two steps are repeated until
the active set does not change, thus guaranteeing that the solutions are
optimal.

3.1.4 Step length To determine the optimal step length (the minimal
value by which the regularization parameter must be decreased in order
for the active set to change), we need to solve a similar problem, this time
involving the ratio of two separate frequent itemset mining optimization
problems.

If we deﬁne the step length:

Akk = Ak+1 — Ak

the minimum decrement of A for which the active set A changes (a vari-
able is added or removed), it can be shown (see Supplementary Material
for detailed proof) that:

 

 

THEOREM 2.
T Ak—Clg Ak+clg
A), :—m1n , ,A _ -,A
k eez{d£_l _d£_1 non active k

 

3055

112 /810's112umofpiOJXO'sot112u1101utotq//2d11q 111011 pepeolumoq

910K ‘09 lsnﬁnV no 22

D.A.duVerle et al.

 

where min+ is the smallest strictly positive value, dg :2 33% and An0n_acg,-ve
are obtained by:

. 3p ‘1
Anon—active =    |A=Ak) :| 

We note that An0n_ac,,-ve only depends on the variables in the active set
and can be easily computed. On the other hand, much like in section
3.1.3, exhaustively computing the values of the ﬁrst two expressions in (2)
for all K in .71 is not computationally feasible given the dimension Of our
input.

We designed an exploratory approach using bounds on each sub-
problem to efﬁciently prune the search tree and drastically reduce the
number of solutions explored.

First, we observe that both expressions can be rewritten as Optimiza-
tion problems of the form:

+ Kp ‘1' 21719611
‘ l

21% Kq +  Qixu (9)
l

where Vi : p,, q,- 6 IR only depend on the variables in the active set A (and

can therefore be easily computed) and K1,, ICqZ constant terms

(i—Ak, — 19Ak91})-

We consider a relaxed form of (9), known as unconstrained fractional
0—1 programming, problem (Hammer et al., 1968) and frequently encoun-
tered in the ﬁelds of scheduling or database query optimization (Hansen
et al., 1990):

Kp + Zpixi
l

= min —
{ML-63" Kq + Z Qixi
i

452’ (10)

where n is the number of non-zero values for the itemset B being con-
sidered. {pi},- 6 IR” and {qi}, 6 IR”.

Although the general form of this problem is shown to be NP-hard (by
association to the well-known NP-complete subset sum decision problem),
it has an easy polynomial solution (Boros and Hammer, 2002; Hammer
et al., 1968) if certain conditions hold.

With the following notation, separating positive and negative terms in
the sums of p,- and q,-:

V1312 =p,-+ —p,- 219?,19,‘ >0; 132 1= prxu; 13g— :: prxu

Vi:Qi = q? - q,- Iq,-+,q,-‘ >0; q"? 1= Zqixu; q”; 1= Zqi‘xu
we have the following result:

THEOREM 3. For a given itemset K, it is not necessary to explore any super-
sets of E if either of the following conditions holds:

(Kq — 97; Z 0) A (a): Z curmin)

(Kq + 972“ 5 0) A (a): Z curmin)

where curmin is the current minimum value found by the algorithm up
until itemset K.

A much faster (0(1)), albeit slightly weaker, pruning condition can
also be Obtained (see proof in Supplementary Material):

THEOREM 4. For a given itemset K, it is not necessary to explore any super-
sets of E if either of the following conditions holds:

_ ”- ~+
(Kq — 6.7g— 2 0) /\ [(3 +16% 2 curmin) V (m < 0)]

 

 

q 2 K4 _ q; —
~+ — ~—
(Kq + at s o) A  z curmin) v (“p ’31 s 0)]
"q _ ‘12 “‘1 + ‘15

Although this pruning-based method loses some of its efﬁciency as the
regularization parameter A decreases and the model becomes less sparse,
for the range of values Of Ak treated, it remains well within the reach of
standard computing equipment (under a minute on a single 3.2 GHz CPU
core).

3.2 Application to Cox proportional hazards model

To demonstrate the potential of our method, we applied it to the Cox
model. This model uses survival data Of the general form {(xi, yi, 6,-)};’:1,
where x,- 6 Rd is the vector of risk factors, for instance gene expression
levels. In practice the x,- used by our method is vector of binary indicators
of under— or over-expression (possibly in combination); y,- > 0 is the time
Observed (survival until an event or censoring); 6,- 6 {0,1} is a binary
variable indicating whether an event has taken place (8,- : 1) or the Ob-
servation was right censored (8,- : 0).

The Cox regression model (Cox, 1972) for the hazard of death at time t
can be expressed as:

’10) = 120(1) eXp(lfTX) (11)
where ho(t) is the baseline hazard function, B 6 Rd is the vector of par-
ameters and X = {X 1, . . . , Xd} is the vector of risk factor variables with

corresponding sample value of x,- for the i-th sample.

However, it is not necessary to know ho(t) to infer the regression par-
ameters, thanks to the use Of the log partial likelihood function Of the
Cox model (Tibshirani et al., 1997), deﬁned as:

cur) = Z (BTxi-10g< Z exp(rixj))) (12)

i33i=1 jiijJ’i

Refer to the Supplementary Material for the exact computation of the
criterion Cg (6) in the case of the Cox proportional model.

3.3 Gathering synthetic candidates

To extract as many interaction candidates as possible, while avoiding the
risk of overﬁtting the data, we repeatedly run the path-following algo-
rithm on a randomly chosen subset of the input. It has been shown
(Meinshausen and Buhlmann, 2010) that the use of such sampling
method with regularized methods Of variable selection provides a good
estimator of the original data. On each run of the algorithm, we keep
feature combinations that show a signiﬁcantly improved predictive power
over the linear models (likelihood ratio test P-value <0.01). We aggre-
gate all such combinations and rank them by Kaplan—Meier test P-value
to produce a list of candidate interactions positively or negatively affect-
ing the timed outcome.

As could be expected, a few combinations will tend to reoccur multiple
times across successive iterations of the algorithm, whereas a large
number only occurs once or twice. We hypothesized and veriﬁed a pos-
teriori (see Supplementary Material) that combinations with low number
of occurrences might be overﬁtting a particular iteration’s training subset
and have poor generalization power. We therefore set an additional
screening thresholds on the list of interactions, keeping only those that
occur in at least four (out of 100) iterations. This threshold value was
selected as giving the best compromise between ratio of false positives and
overall number of interactions found (see details in Supplementary
Material).

Independent testing shows remarkable stability of the list of selected
interactions for a large-enough number Of iterations. With our chosen
occurrence and P—value thresholds, the ﬁnal list of variables sees little
change after ~50 iterations (see plot in Supplementary Material). This
trend is also conﬁrmed when using an independent test: none of the rarely
occurring combinations added in later iterations turn out to be signiﬁcant
in the test subset. For our experiment, we therefore set the total number

 

3056

112 /810's112umofpiOJXO'sot112u1101utotq/pd11q 111011 pepeolumoq

910K ‘09 lsnﬁnV no 22

Discovering combinatorial interactions

 

of total iterations to 100, a value that once again seems to Offer a good
compromise between exhaustivity and the risk of false discovery.

4 EVALUATION
4.1 Datasets

To test our method, we used two datasets publicly available:
survival studies of neuroblastoma (Oberthuer et al., 2006) and
breast cancer 01 an De Vijver et al., 2002) patients. In both stu-
dies, complementary DNA microarray assays of gene expression
(10163 probes for 9878 unique genes and 24158 probes for
23 031 unique genes, respectively), along with (right-censored)
survival data, were available for n = 251 and n = 295 patients,
respectively. In both cases, after setting aside a test subset (25%
of all instances), the algorithm was iteratively applied on rando-
mized subsets of the training data (95%) in a method similar to
the leave-one-out procedure (Kearns and Ron, 1999).

For each study, gene expression data were normalized across
arrays using standard methods Wang and Thorne, 2003), then
discretized in two binary classes depending on their distance to
the mean (,u.) using a threshold proportional to the standard
deviation (0): genes that are over-expressed (expression value
above ,u. +00, where 9 is a thresholding parameter, set to 1.5
in this instance) or under-expressed (below ,u. — 00).

To compare the hi gher- order interactions found by our method
with a linear combination search, we ran the original Park and
Hastie (2007) algorithm on the same training datasets and ranked
the resulting variables found by the order in which they entered
the regularized model. These ranks appear in the result tables
under the column ‘single-variable rank’ (‘NA’, standing for ‘not
applicable’, indicates a variable that did not appear in any of the
models ﬁtted by the single-variable version of the algorithm before
one of its default termination conditions were reached).

4.2 Analysis of breast cancer data

The list of interactions found for Van De Vijver et al. (2002) (see
Table 1) not only features a large number of genes strongly
associated with breast cancer prognosis in the medical literature,
such as SLC2A3 (Sternlicht et al., 2006), CA9 (Span et al., 2003),
RAB6B (van’t Veer et al., 2002), BBC3 (Cobleigh et al., 2005) or

Table 1. Interaction results for Van De Vijver et al. (2002)

KIAA0882 (Abba et al., 2005), many of which do not appear at
all in single-variable model ﬁts (see single-variable ranks); it also
features interesting examples of synthetic interactions: e. g. the
Kaplan—Meier plot for the interaction between BBC3 and
KIAA0882 (Fig. 2) shows perfect prediction of survival of all
test samples (P<0.0003), compared with the much less signiﬁ-
cant plot for BBC3 alone (P = 0.03), whereas a strong synthetic
effect can be observed with BBC3 over-expressed (logrank
P—value: 0.008, see plots in Supplementary Material).

Despite the overall small number of samples and difﬁculties to
obtain good generalization power from such small training and
test subsets, these results hold fairly well in test. Logrank
P—values computed over an independent test subset for all se-
lected combinations show 6 of 9 (66.7%) to be signiﬁcant
(P< 0.05), with 4 combinations (44%) still signiﬁcant after
Bonferroni correction for multiple-hypotheses testing.

4.3 Analysis of neuroblastoma data

The even smaller number of samples for Oberthuer et al. (2006)
makes it difﬁcult to obtain good generalized results (Table 2);
however, the single interaction validated on the test subset (out
of four interactions in total selected by our algorithm) not only
shows strong predicting power on both subsets, but also involves
two sequences strongly tied to breast cancer in literature. Locus
BC046178 is associated with CENPW (previously known as
C6orf 173 or CUG2), a well-studied oncogene associated with
apoptotic behaviours in tumour cells (Lee et al., 2007, 2010).
Probe Hs458148 is a match for multiple genes including

dn.BBC3*dn. KIAA0882

dn.BBCa'dnKlAM‘lﬂlZ dn.Baca dn.KIAmaz
p-val: 0.000276 p-val: 0.03268 p—val: 0.000602

 

 

 

 

 

 

 

 

 

 

 

 

Fig. 2. Kaplan—Meier plots for genes BBC3 and KLAA0882 (separately

and in combination) in data used by Van De Vijver et al. (2002) (using
test subset independent from training data used to compute Table 1)

 

Gene combination LR test P-value

Logrank P-value

NO. of occurrences Test logrank P—value Single-variable rank

 

up.SLC2A3 * up.CA9 0.00153 0.000175
dn.Contig56307 * up.RAB6B 0.00168 0.000392
dn.BBC3 * dn.KIAA0882 5.21e-05 0.00043
up.KLAA0964 * up.SLC2A3 0.000254 0.00132
up.GADD153 * up.SLC31A1 0.0147 0.0022
dn.Contig41887_RC * dn.KIAA0252 0.0151 0.00387
up.RAD51C * up.TIMELESS 0.0367 0.0168
up.TGFBI * up.ITGA5 0.0195 0.0298
dn.Contig41887_RC * up.UGT8 0.000172 0.0329

65 0.003432472 NA NA
15 0.09744396 NA NA
22 0.0002761196 NA NA
23 0.04875811 NA NA

5 0.2596054 540 NA
13 0.01261452 NA NA

4 0.001651706 NA NA
11 0.2221538 NA NA
51 0.003772726 NA NA

 

Note: Selected feature combinations, ranked by Kaplan—Meier P-value. Bonferroni-significant Kaplan—Meier test P-values are in bold (correction factor: m: 71). Total
variables found with single-variable model: 585. Combinations of two genes (or more) are indicated by the symbol ‘*’, while ‘up.’ and ‘dn.’ preﬁxes indicate up- and down-

regulated genes, respectively.

 

3057

112 /810's112umofpiOJXO'sot112u1101utotq/pd11q IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

D.A.duVerle et al.

 

Table 2. Interaction results for Oberthuer et al. (2006)

 

Gene combination LR test P-value

Logrank P-value

NO. of occurrences Test logrank P-value Single-variable rank

 

up.BC046178 * up.Hs458148.20 0.0131 2.16e-07
dn.THC1529413 * up.Hs172998.2 0.0199 0.00142
dn.I_3233919 * up.USPl 0.0164 0.00561
dn.U92981 * dn.SLC14A2 0.0147 0.0369

36 0.01003018 NA 67

20 0.3228081 NA NA

61 0.2413684 NA 89
9 0.1264266 NA NA

 

Note: Selected feature combinations, ranked by Kaplan—Meier P-value. Bonferroni-signiﬁcant Kaplan—Meier test P-values are in bold (m: 48). Total variables found with

single-variable model: 474. Using same notations as Table 1.

RPL10: a ribosomal protein-coding gene that has been found to
be over-expressed in breast cancer tumours (Nagai et al., 2004).
Although Hs458148 could also match other genes, its expression
values in this dataset are highly correlated (Pearson’s coefﬁcient:
0.63) with two other probes exclusively matching RPL10.

4.4 Model validity and computation time

Although our goal is primarily not to create a predictor, but to
gather input feature combinations (with promising synthetic le-
thality properties, in the case of cancer studies), we could still
conﬁrm that the model estimates produced by our method were
sound and consistent with previous methods. Separating the ori-
ginal dataset in a training (75%), model-selection (12.5%) and
test (12.5%) subsets and running nested cross-validation (100
iterations at the training level, each evaluated over 100 partition-
ing of the model-selection and evaluation subsets), we were able
to compare the average log partial likelihood for both our algo-
rithm and that of Park and Hastie (2007) (who use a til-penalized
path-following algorithm that only selects single variables, here-
after referred to as single-variable algorithm or single-variable
model), both on the test subset.

Using the breast cancer survival data from Van De Vijver et al.
(2002), our algorithm gave a mean log partial likelihood of
—121.00 (SD: 27.56) compared with —117.10 (SD: 26.85) for
the single-variable algorithm by Park and Hastie (2007), both
signiﬁcantly (P<2.2e — 16) higher than the null model
(—123.28, SD: 27.88), where no variables are used. With both
algorithms, a large variance in the cross-validated results and
overall middling performances are to be expected due to the
small sizes of training, model-selection and testing subsets
along with the typically high level of noise in microarray data.
However, as the validation of the results in section 4.2 shows,
there is still enough signal to detect meaningful covariates.

Additionally, we ran our algorithm on a randomized version
of the breast cancer data, where survival data had been shuﬁled
so as to no longer match its particular gene expression data.
Using the same experimental set-up as described in 4.1, the al-
gorithm produced only two signiﬁcant interactions (P<0.05):
one of which only occurred once (and therefore would not be
selected under normal conditions), whereas the other, with a
P-value of 0.03, was no longer signiﬁcant after Bonferroni cor-
rection (correction factor: 36) for multiple-hypotheses testing.
This is to be contrasted with the multiple Bonferroni-signiﬁcant
interactions found in regular data (see section 4.2).

Computing time, although consistently longer for our algo-
rithm was still within reasonable distance of the single-variable
version: with similar termination conditions and the same input
data, a single run of our path-following algorithm took on aver-
age <5 min (281 s:l:83 s) on a quad-core 3.2GHz CPU, com-
pared with a little under a minute for Park and Hastie (2007)
(36 s :I: 6 s).

5 CONCLUSION

In this article, we presented an algorithm to follow the regular-
ization path of any Kl-regularized linear model ﬁtting, using
combinatorial interactions as covariates. Although the path-fol-
lowing method has been applied to microarray data in the past
(Park and Hastie, 2007), it was until now only able to deal with
single-valued features, ignoring possible higher-order effect of
gene interactions.

Our method makes uses of existing frequent itemset mining
techniques and novel imports from fractional programming to
avoid the intractability issues of combinatorial input and pro-
duce a regression model of accuracy and run time comparable
with the linear case. By running multiple iterations of the algo-
rithm on subsampled datasets, we can produce ordered lists of
candidate interactions with strong predicting power.

The interactions found by applying our method to cancer
study survival data include many genes that could not be
found through linear models, yet show up in literature as
strongly tied to these conditions, conﬁrming the crucial import-
ance of taking interaction effects into account to detect some of
the weaker signal in gene expression data. Although most sig-
niﬁcant interactions found by our method on experimental data
were limited to two or three genes, there are no theoretical limi-
tations to the size of interactions that can be searched, at no
particularly higher computational cost, setting this method
apart from other recent work on penalized selection of inter-
actions in high-dimensional data (Bien et al., 2012).

The strong noise inherent to gene expression microarray likely
prevents the detection of weaker signals between more than three
genes, making it an attractive prospect to work with less noisy
types of data where larger interactions might be detectable. In the
future, we plan to extend our ﬁeld of application to a wider range
of biomedical data, such as the identiﬁcation of SNP interactions
(Schwender and Ickstadt, 2008), as well as leverage our model’s
ability to deal with heterogeneous input, for example by

 

3058

112 /810's112umofpiOJXO'sot112u1101utotq/pd11q IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

Discovering combinatorial interactions

 

including a wide range of clinical data in addition to the large-
scale numeric data.

ACKNOWLEDGEMENTS

The authors would like to thank Hiroshi Mamitsuka and
Timothy Hancock, of Kyoto University, for their helpful feed-
back and suggestions.

Funding: Grant-in-Aid for J SPS Fellows (No. 24-02709 in part to
D.dV.); MEXT KAKENHI (No. 23700165 to IT); Grant-in-
Aid for Scientiﬁc Research on Innovative Areas (No. 23110002
to K.K.) from the Ministry of Education, Culture, Sports,
Science and Technology (MEXT) of Japan; Grant-in-Aid (No.
20390092 to K.K.; 24590376 to Y.M.-T.) from MEXT; and
funds from the Global COE program, MEXT, to Nagoya
University. FIRST program and J ST ERATO Minato Project
(in part to K.T.).

Conflict of Interest: none declared.

REFERENCES

Abba,M. et al. (2005) Gene expression signature of estrogen receptor or status in
breast cancer. BM C Genomics, 6, 37.

Bien,J. et al. (2012) A lasso for hierarchical testing of interactions. arX iv preprint
arXiv,I2II.I344.

Boros,E. and Hammer,P. (2002) Pseudo-boolean optimization. Discrete Appl.
Math, 123, 155—225.

Bovelstad,H. et al. (2007) Predicting survival from microarray data a comparative
study. Bioinformatics, 23, 2080—2087.

Cobleigh,M.A. et al. (2005) Tumor gene expression and prognosis in breast cancer
patients with 10 or more positive lymph nodes. Clin. Cancer Res., 11,
8623—8631.

Cox,D. (1972) Regression models and life-tables. J. Roy. Stat. Soc. Ser. B, 34,
187—220.

Dudoit,S. et al. (2002) Comparison of discrimination methods for the classiﬁcation
of tumors using gene expression data. J. Am. Stat. Assoc., 97, 77—87.

Ghosh,D. (2003) Penalized discriminant methods for the classiﬁcation of tumors
from gene expression data. Biometrics, 59, 992—1000.

Gui,J. and Li,H. (2005) Penalized cox regression analysis in the high-dimensional
and low-sample size settings, with applications to microarray gene expression
data. Bioinformatics, 21, 3001—3008.

Hammer,P. et al. (1968) Boolean methods in operations research and related areas.
Vol. 5, Springer-Verlag, New York.

Hanahan,D. and Weinberg,R.A. (2000) The hallmarks of cancer. cell, 100, 57—70.

Hansen,P. et al. (1990) Boolean query optimization and the 0-1 hyperbolic sum
problem. Ann. Math. Artif. Intell., 1, 97—109.

Hastie,T. et al. (2005) The entire regularization path for the support vector machine.
J. Mach. Learn. Res., 5, 1391.

Kaelin,W. (2005) The concept of synthetic lethality in the context of anticancer
therapy. Nat. Rev. Cancer, 5, 689—698.

Keams,M. and Ron,D. (1999) Algorithmic stability and sanity-check bounds for
leave-one-out cross-validation. Neural Comput., 11, 1427—1453.

Lee,S. et al. (2007) Molecular cloning and functional analysis of a novel oncogene,
cancer-upregulated gene 2 (cug2). Biochem. Biophys. Res. Commun., 360,
633—639.

Lee,S. et al. (2010) Cancer-upregulated gene 2 (cug2) overexpression induces apop-
tosis in skov-3 cells. Cell Biochem. cht., 28, 461—468.

Lin,D. and Wei,L. (1989) The robust inference for the cox proportional hazards
model. J. Am. Stat. Assoc., 84, 1074—1078.

Meinshausen,N. and Bﬁhlmann,P. (2010) Stability selection. J. Roy. Stat. Soc. Ser.
B, 72, 417—473.

Nagai,M.A. et al. (2004) Gene expression proﬁles in breast tumors regarding the
presence or absence of estrogen and progesterone receptors. Int. J. Cancer, 111,
892—899.

Oberthuer,A. et al. (2006) Customized oligonucleotide microarray gene expression—
based classiﬁcation of neuroblastoma patients outperforms current clinical risk
stratiﬁcation. J. Clin. Oncol., 24, 5070—5078.

Park,M. and Hastie,T. (2007) Ll-regularization path algorithm for generalized
linear models. J. Roy. Stat. Soc. Ser. B, 69, 659—677.

Saigo,H. et al. (2007) Mining complex genotypic features for predicting HIV-1 drug
resistance. Bioinformatics, 23, 2455—2462.

Schwender,H. and Ickstadt,K. (2008) Identiﬁcation of SNP interactions using logic
regression. Biostatistics, 9, 187—198.

Span,P. et al. (2003) Carbonic anhydrase-9 expression levels and prognosis in
human breast cancer: association with treatment outcome. Br. J. Cancer, 89,
271—276.

Stemlicht,M.D. et al. (2006) Prognostic value of pail in invasive breast cancer:
evidence that tumor-speciﬁc factors are more important than genetic variation
in regulating pail expression. Cancer Epidemiol. Biomarkers Prev., 15,
2107—21 14.

Tibshirani,R. et al. (1997) The LASSO method for variable selection in the Cox
model. Stat. Med, 16, 385—395.

Tibshirani,R. et al. (2002) Diagnosis of multiple cancer types by shrunken centroids
of gene expression. Proc. Natl. Acad. Sci. USA, 99, 6567—6572.

Uno,T. et al. (2004) An efﬁcient algorithm for enumerating closed patterns in trans-
action databases. In: Discovery Science. Springer, Heidelberg, pp. 57—59.

Van De Vijver,M. et al. (2002) A gene-expression signature as a predictor of survival
in breast cancer. N Engl. J. Med, 347, 1999—2009.

van’t Veer,L.J. et al. (2002) Gene expression proﬁling predicts clinical outcome of
breast cancer. Nature, 415, 530—536.

Yang,Y. and Thorne,N. (2003) Normalization for two-color cDNA microarray
data. In: Lecture Notes-Monograph Series. Institute of Mathematical Studies,
Beachwood, pp. 403—418.

 

3059

112 /810's112umofpinXO'sot112u1101utotq/pd11q uroii pepeolumoq

910K ‘09 lsnﬁnV no 22

