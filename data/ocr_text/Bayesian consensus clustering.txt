ORIGINAL PAPER

Vol. 29 no. 20 2013, pages 2610—2616
doi:10. 1093/bioinformatics/btt425

 

Systems biology

Bayesian consensus clustering
Eric F. Lock1’2’* and David B. Dunson‘

Advance Access publication August 28, 2013

1Department of Statistical Science, Duke University, Durham, NC 27708, USA and 2Center for Human Genetics, Duke

University Medical Center, Durham, NC 27710, USA

Associate Editor: Alfonso Valencia

 

ABSTRACT

Motivation: In biomedical research a growing number of platforms
and technologies are used to measure diverse but related information,
and the task of clustering a set of objects based on multiple sources of
data arises in several applications. Most current approaches to multi-
source clustering either independently determine a separate clustering
for each data source or determine a single ‘joint’ clustering for all data
sources. There is a need for more flexible approaches that simultan-
eously model the dependence and the heterogeneity of the data
sources.

Results: We propose an integrative statistical model that permits a
separate clustering of the objects for each data source. These separ-
ate clusterings adhere loosely to an overall consensus clustering, and
hence they are not independent. We describe a computationally scal-
able Bayesian framework for simultaneous estimation of both the
consensus clustering and the source-specific clusterings. We demon-
strate that this flexible approach is more robust than joint clustering of
all data sources, and is more powerful than clustering each data
source independently. We present an application to subtype identifi-
cation of breast cancer tumor samples using publicly available data
from The Cancer Genome Atlas.

Availability: R code with instructions and examples is available at
http://people.duke.edu/%7Eel113/software.html.

Contact: Eric.Lock@duke.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on April 3, 2013; revised on June 18, 2013; accepted on
July 18, 2013

1 INTRODUCTION
1.1 Motivation

Several ﬁelds of research now analyze multisource data (also
called multimodal data), in which multiple heterogeneous data-
sets describe a common set of objects. Each dataset represents a
distinct mode of measurement or domain.

While the methodology described in this article is broadly ap-
plicable, our primary motivation is the integrated analysis of
heterogeneous biomedical data. The diversity of platforms and
technologies that are used to collect genomic data, in particular,
is expanding rapidly. Often multiple types of genomic data,
measuring various biological components, are collected for a
common set of samples. For example, The Cancer Genome
Atlas (TCGA) is a large-scale collaborative effort to collect

 

*To whom correspondence should be addressed

and catalog data from several genomic technologies. The inte-
grative analysis of data from these disparate sources provides a
more comprehensive understanding of cancer genetics and
molecular biology.

Separate analyses of each data source may lack power and will
not capture intersource associations. At the other extreme, a
joint analysis that ignores the heterogeneity of the data may
not capture important features that are speciﬁc to each data
source. Exploratory methods that simultaneously model shared
features and features that are speciﬁc to each data source have
recently been developed as ﬂexible alternatives (Lock et al., 2013;
L6fstedt and Trygg, 2011; Ray et al., 2012; Zhou et al., 2012).
The demand for such integrative methods motivates a dynamic
area of statistics and bioinformatics.

This article concerns integrative clustering. Clustering is a
widely used exploratory tool to identify similar groups of objects
(for example, clinically relevant disease subtypes). Hundreds of
general algorithms to perform clustering have been proposed.
However, our work is motivated by the need for an integrative
clustering method that is computationally scalable and robust to
the unique features of each data source.

In Section 3.3, we apply our integrative clustering method to
mRNA expression, DNA methylation, microRNA expression
and proteomic data from TCGA for a common set of breast
cancer tumor samples. These four data sources represent differ-
ent but highly related and dependent biological components.
Moreover, breast cancer tumors are recognized to have import-
ant distinctions that are present across several diverse genomic
and molecular variables. A fully integrative clustering approach
is necessary to effectively combine the discriminatory power of
each data source.

1.2 Related work

Most applications of clustering multisource data follow one of
two general approaches:

(1) Clustering of each data source separately, potentially fol-
lowed by a post hoc integration of these separate
clusterings.

(2) Combining all data sources to determine a single ‘joint’
clustering.

Under approach (1), the level of agreement between the sep-
arate clusterings may be measured by the adjusted Rand index
(Hubert and Arabie, 1985) or a similar statistic. Furthermore,
consensus clustering (also called ensemble clustering) can be used
to determine an overall partition of the objects that agrees the
most with the source-speciﬁc clusterings. Several objective

 

2610 © The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

112 /810'S112umo[pJOJXO'sot1emJOJutotw/2d11q IIIOJJ pepeommoq

910K ‘09 isnﬁnV no :2

Bayesian consensus clustering

 

functions and algorithms to perform consensus clustering have
been proposed [for a survey see Nguyen and Caruana (2007)].
Most of these methods do not inherently model uncertainty, and
statistical models assume that the separate clusterings are known
in advance Wang et al., 2010, 2011). Consensus clustering is
most commonly used to combine multiple clustering algorithms,
or multiple realizations of the same clustering algorithm, on a
single dataset. Consensus clustering has also been used to inte-
grate multisource biomedical data (Cancer Genome Atlas
Network, 2012). Such an approach is attractive in that it
models source-speciﬁc features, yet still determines an overall
clustering, which is often of practical interest. However, the
two stage process of performing entirely separate clusterings fol-
lowed by post hoc integration limits the power to identify and
exploit shared structure (see Section 3.2 for an illustration of this
phenomenon).

Approach (2) effectively exploits shared structure, at the ex-
pense of failing to recognize features that are speciﬁc to each
data source. Within a model-based statistical framework, one
can ﬁnd the clustering that maximizes a joint likelihood.
Assuming that each source is conditionally independent given
the clustering, the joint likelihood is the product of the likelihood
functions for each data source. This approach is used by
Kormaksson et al. (2012) in the context of integrating gene ex-
pression and DNA methylation data. The iCluster method (Mo
et al., 2013; Shen et al., 2009) performs clustering by ﬁrst ﬁtting a
Gaussian latent factor model to the joint likelihood; clusters are
then determined by K—means clustering of the factor scores. Rey
and Roth (2012) propose a dependency-seeking model in which
the goal is to ﬁnd a clustering that accounts for associations
across the data sources.

More ﬂexible methods allow for separate but dependent
source clusterings. Dependent models have been used to simul-
taneously cluster gene expression and proteomic data (Rogers
et al., 2008), gene expression and transcription factor binding
data (Savage et al., 2010) and gene expression and copy
number data Wuan et al., 2011). Kirk et al. (2012) describe a
more general dependence model for two or more data sources.
Their approach, called Multiple Dataset Integration (MDI), uses
a statistical framework to cluster each data source while simul-
taneously modeling the pairwise dependence between clusterings.
Savage et al. (2013) use MDI to integrate gene expression,
methylation, microRNA and copy number data for glioblastoma
tumor samples from TCGA. The pairwise dependence model
does not explicitly model adherence to an overall clustering,
which is often of practical interest.

2 METHODS

2.1 Finite Dirichlet mixture models

Here we briefly describe the ﬁnite Dirichlet mixture model for clustering a
single dataset, with the purpose of laying the groundwork for the inte-
grative model given in Section 2.2. Given data Xn for N objects
(n = 1, .. . , N), the goal is to partition these objects into at most K clus-
ters. Typically X” is a multidimensional vector, but we present the model
in sufﬁcient generality to allow for more complex data structures. Let
ﬂXnIQ) deﬁne a probability model for X” given parameter(s) 0. For ex-
ample, f may be a Gaussian density deﬁned by the mean and variance
0 = (,u, 02). Each X” is drawn independently from a mixture distribution

with K components, speciﬁed by the parameters 01, ...,OK. Let
C” e {1, ...,K} represent the component corresponding to X”, and Tl'k
be the probability that an arbitrary object belongs to cluster k:

mC = P(C,, = k).

Then, the generative model is
X n ~ f(-|0k) with probability nk.

Under a Bayesian framework, one can put a prior distribution on
1'1 2 (7n, ...,JTK) and the parameter set E) = (01, ...,QK). It is natural
to use a Dirichlet prior distribution for 1'1. Standard computational meth-
ods such as Gibbs sampling can then be used to approximate the poster-
ior distribution for 1'1, 6) and C = (C1, ..., CN). The Dirichlet prior is
characterized by a K-dimensional concentration parameter ,8 of positive
reals. Low prior concentration (for example, ,Bk 5 1) will allow some of
the estimated 71} to be small, and therefore N objects may not represent
all K clusters. Letting K —> 00 gives a Dirichlet process.

2.2 Integrative model

We extend the Dirichlet mixture model to accommodate data from M
sources X1, . . . , XM. Each data source is available for a common set of N
objects, where an represents data m for object n. Each data source
requires a probability model fm(X,,|0m) parametrized by 0m. Under the
general framework presented here, each Xm may have disparate structure.
For example, X1” may give an image where f1 deﬁnes the spectral density
for a Gaussian random ﬁeld, while X 2,, may give a categorical vector
where f2 deﬁnes a multivariate probability mass function.

We assume there is a separate clustering of the objects for each data
source, but that these adhere loosely to an overall clustering. Formally,
each an n = 1, ...,N is drawn independently from a K-component
mixture distribution speciﬁed by the parameters Oml, .. . , OmK. Let
Lmn e {1, .. . , K} represent the component corresponding to an.
Furthermore, let C, e {1, ...,K} represent the overall mixture compo-
nent for object n. The source-speciﬁc clusterings ﬂ. = (Lml , . . . , LmN) are
dependent on the overall clustering C = (C1, ... , CN):

P(Lmn =  2 Wk, Cm am)

where am adjusts the dependence function v. The data Xm are independ-
ent of C conditional on the source-speciﬁc clustering ﬂ_m. Hence, C serves
only to unify |]_1, . . . , ILM. The conditional model is

P(Lmn 2 lemn: Cm ka) (X v(k, Cm
Throughout this article, we assume v has the simple form

1)(Lmna Cm 05m) = {  (1)
where am e [%, 1] controls the adherence of data source m to the overall
clustering. More simply am is the probability that Lmn = C”. So, if
am 2 1, then [Lm = C. The am are estimated from the data together
with C and 1.1, ..., ﬂ_m. In practice we estimate each am separately, or
assume that a1 = . . . = aM and hence each data source adheres equally to
the overall clustering. The latter is favored when M: 2 for identiﬁability
reasons. More complex models that permit dependence of the Oth are
also potentially useful.

Let It} be the probability that an object belongs to the overall cluster k:

m, = P(C,, = k).

We assume a Dirichlet(,8) prior distribution for 1'1 2 (7n, ...,JTK). The
probability that an object belongs to a given source-speciﬁc cluster fol-
lows directly:

1 — am
K — 1 '

 

P(Lmn =  = ﬂkam +  _ ﬂk) 

 

261 1

112 /810'S112umo[pJOJXO'sot1emJOJutotw/2d11q Incl; pepeommoq

910K ‘09 isnﬁnV no :2

E.F.Lock and D.B.Dunson

 

Table 1. Notation

 

N Number of objects

M Number of data sources
K Number of clusters

Xm Data source m

an Data for object n, source m

fm Probability model for source 112

Omk Parameters for fm, cluster k

pm Prior distribution for Omk

C” Overall cluster for object n

nk Probability that C, = k

Lmn Cluster speciﬁc to Km

1) Dependence function for C” and Lmn
am Probability that Lmn = C,

 

Moreover, a simple application of Bayes rule gives the conditional distri-
bution of C:

M
P(Cn =  H: a) (X nk l—l V(meka am):

m=1

where v is deﬁned as in (1).

The number of possible clusters K is the same for 1.1, . . . , ILM and C.
The link function 1) naturally aligns the cluster labels, as cases in which
the clusterings are not well aligned (a permutation of the labels would
give better agreement) will have low posterior probability. The number of
clusters that are actually represented may vary, and generally the source-
speciﬁc clusterings II”, will represent more clusters than C, rather than
vice versa. This follows from Equation (2) and is illustrated in Section 2
of the Supplementary Material. Intuitively if object n is not allocated to
any overall cluster in data source m (i.e. Lmn¢C), then an does not
conform well to any overall pattern in the data.

Table 1 summarizes the mathematical notation used for the integrative
model.

2.3 Marginal forms

Integrating over the overall clustering C gives the joint marginal distri-
bution of |]_1, ..., ILM:

K M
P({Lmn = mlnﬂle 11-1205) (X Zﬂk l—I 1)(km: k: 05m)- 
k=1 m=1
Under the assumption that a1 = . .. = a M the model simpliﬁes:
K
P({Lmn = m},1‘f=1|1'1,a) oc chU‘k (4)
k=1

where tk is the number of clusters equal to k and U = % Z 1. This
marginal form facilitates comparison with the MDI method for depend-
ent clustering. In the MDI model (ﬁg->0 control the strength of associ-
ation between the clusterings IL,- and II]:

M
P({Lmn = m}2‘f=11ﬁ,<1>)o< Hank, 1'1 (1+¢ij) (5)

m=l 

where ﬁmk = P(Lm,, = k). For K = 2 and in. = IT}, it is straightforward
to show that (4) and (5) are functionally equivalent under a parameter
substitution (see Section 3 of the Supplementary Material). There is no
such general equivalence between the models for K > 2 or M > 2, regard-
less of restrictions on 1:1 and d). This is not surprising, as MDI gives a
general model of pairwise dependence between clusterings rather than a
model of adherence to an overall clustering.

2.4 Estimation

Here we present a general Bayesian framework for estimation of the
integrative clustering model. We use a Gibbs sampling procedure to ap-
proximate the posterior distribution for the parameters introduced in
Section 2.2. The algorithm is general in that we do not assume any spe-
ciﬁc form for the fm and the parameters Omk. We use conjugate prior
distributions for am, 1'1 and (if possible) Omk.

0 am ~ TBeta(am,bm, ﬁ), the Beta(am,bm) distribution truncated
below by  By default we choose am = bm = 1, so that the prior
for am is uniformly distributed between % and 1.

0 1'1 ~ Dirichlet(,80). By default we choose ,80 = (1, 1, . . . , 1), so that the
prior for 1'1 is uniformly distributed on the standard (M — 1)-simplex.

o The ka have prior distribution pm. In practice, one should choose pm
so that sampling from the conditional posterior pm(0mk|Xm, IL") is
feasible.

Markov chain Monte Carlo (MCMC) proceeds by iteratively sampling
from the following conditional posterior distributions:

0 @mlxm, ILm ~pm(0mk|Xm, IL”) for k = 1, ...,K.
o ILmIXm, @m,am,C ~ P(k|Xm,,, Cn,0mk,am) for n = 1, ...,N, where
P(k|Xmm Cm 6m) (X Wk, Cm

amlC, [Lm ~ TBeta(am + rm, bm + N — rm, %), where rm is the num-
ber of samples n satisfying Lmn = C”.

Clle, 1'1,a ~ P(k|1'1,{Lmn,am}nAf_1) for n = 1, ...,N, where

M
P(k|H:{meam}nA14=1) (X 71k H v(kaLmnaam)

m=1

o 1'1|C ~ Dirichlet(,80 + p), where pk is the number of samples allo-
cated to cluster k in C.

This algorithm can be suitably modiﬁed under the assumption that
a1 =  = aM (see Section 1.2 of the Supplementary Material).

Each sampling iteration produces a different realization of the cluster-
ings C, |]_1, - - - , IL", and together these samples approximate the posterior
distribution for the overall and source-speciﬁc clusterings. However, a
point estimate may be desired for each of C, 1.1, - - - , II”, to facilitate in-
terpretation of the clusters. In this respect, methods that aggregate over
the MCMC iterations to produce a single clustering, such as that
described in Dahl (2006), can be used.

It is possible to derive a similar sampling procedure using only the
marginal form for the source-speciﬁc clusterings given in Equation (3).
However, the overall clustering C is also of interest in most applications.
Furthermore, incorporating C into the algorithm can actually improve
computational efﬁciency dramatically, especially if M is large. As pre-
sented, each MCMC iteration can be completed in 0(MNK) operations.
If the full joint marginal distribution of L1, .. . , LM is used the compu-
tational burden increases exponentially with M (this presents a bottleneck
for the MDI method).

For each iteration, C” is determined randomly from a distribution that
gives higher probability to clusters that are prevalent in {L1 n, . . . , Lmn}. In
this sense, C is determined by a random consensus clustering of the
source-speciﬁc clusterings. Hence, we refer to this approach as Bayesian
consensus clustering (BCC). BCC differs from traditional consensus clus-
tering in three key aspects.

(1) Both the source-speciﬁc clusterings and the consensus clustering
are modeled in a statistical way that allows for uncertainty in all
parameters.

(2) The source-speciﬁc clusterings and the consensus clustering are
estimated simultaneously, rather than in two stages. This permits

 

261 2

112 /810'S112umo[pJOJXO'sot112u1101utotq/ﬁd11q 111011 p9p1201umoq

910K ‘09 isnﬁnV no 22

Bayesian consensus clustering

 

borrowing of information across sources for more accurate cluster
assignments.

(3) The strength of association to the consensus clustering for each data
source is learned from the data and accounted for in the model.

We have developed software for the R environment for statistical
computing (R Development Core Team, 2012) to perform BCC on multi-
variate continuous data using a Normal-Gamma conjugate prior distri-
bution for cluster-speciﬁc means and variances. Full computational
details for this implementation are given in Section 1.1 of the
Supplementary Material. This software is open source and may be mod-
iﬁed for use with alternative likelihood models (e.g. for categorical or
functional data).

2.5 Choice of K

One can infer the number of clusters in the model by specifying a large
value for the maximum number of clusters K, for example K: N. The
number of clusters realized in C and the [Lm may still be small. However,
we ﬁnd that this is not the case for high-dimensional structured data such
as that used for the genomics application in Section 3.3. The model tends
to select a large number of clusters even if the Dirichlet prior concentra-
tion parameters ,80 are small. The number of clusters realized using a
Dirichlet process increases with the sample size; hence, if the number of
mixture component is indeed ﬁnite, the estimated number of clusters is
inconsistent as N —> 00 (Miller and Harrison, 2013). This is undesirable
for exploratory applications in which the goal is to identify a small
number of interpretable clusters.

Alternatively, we consider a heuristic approach that selects the value of
K that gives maximum adherence to an overall clustering. For each K, the
estimated adherence parameters am e [%, 1] are mapped to the unit inter-
val by the linear transformation

* _ Kam — 1
0"" T K— 1
We then select the value of K that results in the highest mean adjusted
adherence

 

1 M

m=1

This approach will generally select a small number of clusters that reveal
shared structure across the data sources.

3 RESULTS

3.1 Accuracy of 0?

We ﬁnd that with reasonable signal the am can generally be
estimated with accuracy and without substantial bias. To illus-
trate, we generate simulated datasets X1 : 1 x 200 and
X2 : 1 x 200 as follows:

(1) Let C deﬁne two clusters, where C” = 1 for
n e {1, ...,100} and C” :2 for n e {101, ...,200}.

(2) Draw a from a Uniform(0.5,1) distribution.

(3) Form =1,2andn = 1, ...,200,generateLm,, e {1,2}with
probabilities P(Lm,, = C”) = a and P(Lm,, 75 C”) = 1 — a.

(4) For m = 1, 2, draw values an from a Normal(1.5,1) dis-
tribution if Lmn = 1 and from a Normal(—1.5, 1) distribu-
tion if Lmn = 2.

We generate 100 realizations of the above simulation, and es-
timate the model Via BCC for each realization. We assume

True Vs. Estimated 0c

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

0. _
0°. _
o
co - _
<25 0' T
«g < < V
f .
g )l/f) -
s h <
o _ o >/)
/ _
‘ o
<
‘0. _ < < o
o >
>
LQ _ _' m‘[ --
o
| | | | | |
0.5 0.6 0.7 0.8 0.9 1.0
Trueoc

Fig. 1. Estimated 6: versus true a for 100 randomly generated simula-
tions. For each simulation, the mean value 6: is shown with a 95% cred-
ible interval

a1 = 052 in our estimation and use a uniform prior; further com-
putational details are given in Section 4 of the Supplementary
Material. Figure 1 displays 0?, the best estimate for both a1 and
052, versus the true a for each realization. The point estimate
displayed is the mean over MCMC draws, and we also display
a 95 % credible interval based on the 2.5—97.5 percentiles of the
MCMC draws. The estimated 0? are generally close to the true a,
and the credible interval contains the true value in 91 of 100
simulations. See Section 4 of the Supplementary Material for a
more detailed study, including a simulation illustrating the effect
of the prior distribution on 0?.

3.2 Clustering accuracy

To illustrate the ﬂexibility and advantages of BCC in terms of
clustering accuracy, we generate simulated data sources X1 and
X2 as in Section 3.1 but with Normal(1,1) and Normal(—1,1) as
our mixture distributions. Hence, the signal distinguishing the
two clusters is weak enough so that there is substantial overlap
within each simulated data source. We generate 100 simulations
and compare the results for four model-based clustering
approaches:

(1) Separate clustering, in which a ﬁnite Dirichlet mixture
model is used to determine a clustering separately for X1
and X2.

(2) Joint clustering, in which a ﬁnite Dirichlet mixture model
is used to determine a single clustering for the concate-
nated data [X’1 X’Z]’.

(3) Dependent clustering, in which we model the pairwise
dependence between each data source, in the spirit of
MDI.

(4) Bayesian consensus clustering.

The full implementation details for each method are given in
Section 5 of the Supplementary Material.

 

261 3

112 /810'S112umo[pJOJXO'sot112u1101utotq/ﬁd11q 111011 p9p1201umoq

910K ‘09 isnﬁnV no 22

E.F.Lock and D.B.Dunson

 

We consider the relative error for each model in terms of the
average number of incorrect cluster assignments:

M N A
Z Z1]{Lmn 7E Lmn}

 

S = m=1 11:1
ource error MN ,
N A
2 1HQ: 79 Cn}
O 11 2 "=1—
Vera error N ,

where 1]  the indicator function. For joint clustering, the source
clusters ILm are identical. For separate and dependent clustering,
we determine an overall clustering by maximizing the posterior
expected adjusted Rand index (Fritsch and Ickstadt, 2009) of the
source clusterings.

The relative error for each clustering method with M: 2 and
M: 3 sources is shown in Figure 2. Smooth curves are ﬁt to the
results for each method using LOESS local regression
(Cleveland, 1979) and display the relative clustering error for
each method as a function of a. Not surprisingly, joint clustering
performs well for a w 1 (perfect agreement) and separate cluster-
ing performs well when a w 0.5 (no relationship). BCC and de-
pendent clustering learn the level of cluster agreement, and hence
serve as a ﬂexible bridge between these two extremes. Dependent
clustering does not perform as well with M: 3 sources, as the

Source error M-2

 

 

 
 
 
   

 

 

 

 

 

 

 

 

 
 
 
 
 
   

 

 

 

l0
0Q _
o — Joint
o — Separate
cq _ — Dependent
° — BCC (estimated at)
m — BCC (true on)
(\I _
O
 a _
Lu 0
:2
o'
2
0'
LO
Q _
O I I I I I I
0 5 0 6 0.7 0 8 0 9 1 0
Trueoc
Overall error M=2
L0. _
O
— Joint
— Separate
— Dependent
:- — — BCC (estimated a)
— BCC (true at)
5 <0. _
2 C
Li
“I _
0
g _ II
I

 

 

 

0.5 0.6 0.7 0.8 0.9 1.0

True 01

pairwise dependence model does not assume an overall cluster-
ing and therefore has less power to learn the underlying structure
for M > 2.

3.3 Application to genomic data

We apply BCC to multisource genomic data on breast cancer
tumor samples from TCGA. For a common set of 348 tumor
samples, our full dataset includes

0 RNA gene expression (GE) data for 645 genes.

0 DNA methylation (ME) data for 574 probes.

e miRNA expression (miRNA) data for 423 miRNAs.

0 Reverse phase protein array (RPPA) data for 171 proteins.

These four data sources are measured on different platforms
and represent different biological components. However, they all
represent genomic data for the same sample set and it is reason-
able to expect some shared structure. These data are publicly
available from the TCGA Data Portal. See http://people.duke.
edu/%7Eell13/software.html for R code to completely repro-
duce the following analysis, including instructions on how to
download and process these data from the TCGA Data Portal.

Breast cancer is a heterogeneous disease and is therefore a nat-
ural candidate for clustering. Previous studies have found

Source error M-3

 

 

Joint
Separate
Dependent
BCC (estimated or)
BCC (true on)

  
    
    

 

 

 

Error
0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40

 

 

 

True 0t

Overall error M=3

 

 

 

 

 

“’- — .-
O
— Joint
— Separate
¢ — Dependent
o' T — BCC (estimated oc)
— BCC (true on)
<2. _
§ 0
Li:
N _
O
3 _

 

 

 

0.5 0.6 0.7 0.8 0.9 1.0

True a

Fig. 2. Source-speciﬁc and overall clustering error for 100 simulations with M: 2 and M: 3 data sources, shown for joint clustering, separate clustering,
dependent clustering, BCC and BCC using the true a. A LOESS curve displays clustering error as a function of a for each method

 

261 4

112 /810'S112umo[pJOJXO'sot112u1101utotq/ﬁd11q 111011 popcorn/110g

910K ‘09 isnﬁnV no 22

Bayesian consensus clustering

 

anywhere from 2 (Duan, 2013) to 10 (Curtis et al., 2012) distinct
clusters based on a variety of characteristics. In particular, 4 com-
prehensive sample subtypes were previously identiﬁed based on a
multisource consensus clustering of the TCGA data (Cancer
Genome Atlas Network, 2012). These correspond closely to the
well-known molecular subtypes Basal, Luminal A, Luminal B and
HER2. These subtypes were shown to be clinically relevant, as
they may be used for more targeted therapies and prognosis.

We use the heuristic described in Section 2.5 to select the
number of clusters for BCC, with intent to determine a clustering
that is well-represented across the four genomic data sources. We
select K: 3 clusters, and posterior probability estimates were
converted to hard clusterings Via Dahl (2006) to facilitate com-
parison and Visualization. Table 2 shows a matching matrix com-
paring the overall clustering C with the comprehensive subtypes
deﬁned by TCGA, as well as summary data for the BCC clusters.

The TCGA and BCC clusters show different structure but are
not independent (P-Value <0.01; Fisher’s exact test). BCC clus-
ter 1 corresponds to the Basal subtype, which is characterized by
basal-like expression and a relatively poor clinical prognosis.
BCC cluster 2 is primarily a subset of the Luminal A samples,
which are genomically and clinically heterogeneous. DNA copy
number alterations, in particular, are a source of diversity for
Luminal A. On independent datasets Curtis et al. (2012) and
J onsson et al. (2010) identify a subgroup of Luminal A that is
characterized by fewer copy number alterations and a more
favorable clinical prognosis (clusters IntClust 3 and Luminal-
simple, respectively). As a measure of copy number activity, we
compute the fraction of the genome altered (FGA) as described
in Cancer Genome Atlas Network (2012) Supplementary Section

Table 2. BCC cluster versus TCGA comprehensive subtype matching
matrix and summary data for BCC clusters

 

BCC cluster

 

 

 

1 2 3

TCGA subtype

1 (Her2) 13 6 20

2 (Basal) 66 2 4

3 (Lum A) 3 80 78

4 (Lum B) 0 3 73
5-year survival 0.67 :I: 0.20 0.94 :I: 0.08 0.81 :I: 0.11
FGA 0.22 :I: 0.04 0.10 :I: 0.02 0.20 :I: 0.02
ER+ 1 3% 92% 94%
PR+ 7% 86% 75 %
HER2+ 15% 12% 18%
8p1 1 ampliﬁcation 32% 19% 42%
8q24 ampliﬁcation 79% 39% 67%
5q13 deletion 61% 3% 14%
16q23 deletion 19% 66% 61 %

 

Note: Summary data includes 5-year survival probabilities using the Kaplan—Meier
estimator, with 95% conﬁdence interval; mean fraction of the genome altered
(FGA) using threshold T: 0.5, with 95% conﬁdence interval; receptor status for
estrogen (ER), progesteron (PR) and human epidermal growth factor 2 (HER2);
and copy number status for ampliﬁcation at sites 8p11 and 8q23 and deletion at sites
5q13 and 16q23.

VII (with threshold T=0.50) for each BCC cluster. Clusters 1
and 3 had an FGA above 0.2, while Cluster 2 had an FGA of
0.10 (Table 2). For comparison, those Luminal A samples that
were not included in Cluster 2 had a substantially higher average
FGA of 0.17 :I: 0.02. Cluster 3 primarily includes those samples
that are receptor (estrogen and/or progesterone) positive and
have higher F GA. These results suggest that copy number vari-
ation may contribute to breast tumor heterogeneity across sev-
eral genomic sources.

Figure 3 provides a point-cloud View of each dataset given by a
scatter plot of the ﬁrst two principal components. The overall
and source-speciﬁc cluster index is shown for each sample, as
well as a point estimate and ~95% credible interval for the ad-
herence parameter a. The GE data has by far the highest adher-
ence to the overall clustering (a = 0.91); this makes biological
sense, as RNA expression is thought to have a direct causal re-
lationship with each of the other three data sources. The four
data sources show different sample structure, and the source-
speciﬁc clusters are more well-distinguished than the overall
clusters in each plot. However, the overall clusters are clearly
represented to some degree in all four plots. Hence, the ﬂexible,
yet integrative, approach of BCC seems justiﬁed for these data.

Further details regarding the above analysis are given in
Section 6 of the Supplementary Material. These include the
prior speciﬁcations for the model, charts that illustrate mixing
over the MCMC draws, a comparison of the source-speciﬁc
clusterings Lmn to source-speciﬁc subtypes deﬁned by TCGA,
clustering heatmaps for each data source and short-term survival
curves for each overall cluster.

4 DISCUSSION

This work was motivated by the perceived need for a general,
ﬂexible and computationally scalable approach to clustering
multisource biomedical data. We propose BCC, which models
both an overall clustering and a clustering speciﬁc to each data
source. We View BCC as a form of consensus clustering, with
advantages over traditional methods in terms of modeling uncer-
tainty and the ability to borrow information across sources.

The BCC model assumes a simple and general dependence
between data sources. When an overall clustering is not
sought, or when such a clustering does not make sense as an
assumption, a more general model of cluster dependence (such
as MDI) may be more appropriate. Furthermore, a context-spe-
ciﬁc approach may be necessary when more is known about the
underlying dependence of the data. For example, Nguyen and
Gelfand (2011) exploit functional covariance models for time-
course data to determine overall and time-speciﬁc clusters.

Our implementation of BCC assumes the data are normally
distributed with cluster-speciﬁc mean and variance parameters. It
is straightforward to extend this approach to more complex clus-
tering models. In particular, models that assume clusters exist on
a sparse feature set (Tadesse et al., 2005) or allow for more gen-
eral covariance structure (Ghahramani and Beal, 1999) are grow-
ing in popularity.

While we focus on multisource biomedical data, the applica-
tions of BCC are potentially widespread. In addition to multi-
source data, BCC may be used to compare clusterings from
different statistical models for a single homogeneous dataset.

 

261 5

112 /810'S112umo[pJOJXO'sot112u1101utotq/ﬁd11q 111011 popcorn/110g

910K ‘09 isnﬁnV no 22

E.F.Lock and D.B.Dunson

 

GE (11:11.91 i006)

 

10 20 30
I

PCZ

-10 0

 

—30
I

 

 

 

10

PC2

 

 

-15

 

 

PC1

ME (oc=0.69 i 0.06)

 

PCZ

 

 

 

 

PC2
0

 

 

 

 

PC1

Fig. 3. PCA plots for each data source. Sample points are colored by overall cluster; cluster 1 is black, cluster 2 is red and cluster 3 is blue. Symbols
indicate source-speciﬁc cluster; cluster 1 is indicated by ﬁlled circles, cluster 2 is indicated by plus signs and cluster 3 is indicated by asterisks

Funding: National Institute of Environmental Health Sciences
(NIEHS) (R01-ES017436).

Conflict of Interest: none declared.

REFERENCES

Cancer Genome Atlas Network. (2012) Comprehensive molecular portraits of
human breast tumours. Nature, 490, 61—70.

Cleveland,W.S. (1979) Robust locally weighted regression and smoothing scatter-
plots. J. Am. Stat. Assoc., 74, 829—836.

Curtis,C. et al. (2012) The genomic and transcriptomic architecture of 2,000 breast
tumours reveals novel subgroups. Nature, 486, 346—352.

Dahl,D. (2006) M odel-Based Clustering for Expression Data via a Dirichlet Process
Mixture Model. Cambridge University Press, Cambridge, UK.

Duan,Q. et al. (2013) Metasignatures identify two major subtypes of breast cancer.
CPT Pharmacom. Syst. Pharmacol., 3, e35.

Fritsch,A. and Ickstadt,K. (2009) Improved criteria for clustering based on the
posterior similarity matrix. Bayesian Anal., 4, 367—391.

Ghahramani,Z. and Beal,M.J. (1999) Variational inference for bayesian mixtures of
factor analysers. In: Solla,S.A. et al. (ed.) Advances in Neural Information
Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November
29—December 4, I999]. The MIT Press, Cambridge, MA, USA, pp. 449—455.

Hubert,L. and Arabie,P. (1985) Comparing partitions. J. Classif, 2, 193—218.

J6nsson,G. et al. (2010) Genomic subtypes of breast cancer identiﬁed by array-
comparative genomic hybridization display distinct molecular and clinical char-
acteristics. Breast Cancer Res., 12, R42.

Kirk,P. et al. (2012) Bayesian correlated clustering to integrate multiple datasets.
Bioinformatics, 28, 3290—3297.

Kormaksson,M. et al. (2012) Integrative model-based clustering of microarray
methylation and expression data. Ann. Appl. Stat., 6, 1327—1347.

Lock,E. et al. (2013) Joint and Individual Variation Explained (JIVE) for integrated
analysis of multiple data types. Ann. Appl. Stat., 7, 523—542.

L6fstedt,T. and Trygg,J. (2011) Onplsa novel multiblock method for the modelling
of predictive and orthogonal variation. J. Chemom., 25, 441—455.

Miller,J.W. and Harrison,M.T. (2013) A simple example of dirichlet process mixture
inconsistency for the number of components. arX iv preprint arXiv:1301.2708.

Mo,Q. et al. (2013) Pattern discovery and cancer gene identiﬁcation in integrated
cancer genomic data. Proc. Natl Acad. Sci. USA, 110, 4245—4250.

Nguyen,N. and Caruana,R. (2007) Consensus clusterings. In: Proceedings of the 7th
IEEE International Conference on Data Mining (ICDM 2007), October 28-31,
2007, Omaha, Nebraska, USA. pages 607—612. IEEE Computer Society.

Nguyen,X. and Gelfand,A.E. (2011) The Dirichlet labeling process for clustering
functional data. Stat. Sin., 21, 1249—1289.

R Development Core Team. (2012) R: A Language and Environment for Statistical
Computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-
900051-07-0.

Ray,P. et al. (2012) Bayesian joint analysis of heterogeneous data. Preprint.

Rey,M. and Roth,V. (2012) Copula mixture model for dependency-seeking cluster-
ing. In: Langford,J. and Pineau,J. (eds) Proceedings of the 29th International
Conference on Machine Learning ( I CML-I2 ). ICML’ 12, p. 927—934, New York,
NY, Omnipress.

Rogers,S. et al. (2008) Investigating the correspondence between transcriptomic and
proteomic expression proﬁles using coupled cluster models. Bioinformatics, 24,
2894—2900.

Savage,R.S. et al. (2010) Discovering transcriptional modules by bayesian data
integration. Bioinformatics, 26, i158—i167.

Savage,R.S. et al. (2013) Identifying cancer subtypes in glioblastoma by combining
genomic, transcriptomic and epigenomic data. arX iv preprint arX iv.'I 304.35 77.

Shen,R. et al. (2009) Integrative clustering of multiple genomic data types using a
joint latent variable model with application to breast and lung cancer subtype
analysis. Bioinformatics, 25, 2906—2912.

Tadesse,M.G. et al. (2005) Bayesian variable selection in clustering high-dimen-
sional data. J. Am. Stat. Assoc., 100, 602—617.

Wang,H. et al. (2011) Bayesian cluster ensembles. Stat. Anal. Data Mining, 4, 54—70.

Wang,P. et al. (2010) Nonparametric bayesian clustering ensembles. In: Machine
Learning and Knowledge Discovery in Databases. Springer, Berlin - Heidelberg,
pp. 435—450.

Yuan,Y. et al. (2011) Patient-speciﬁc data fusion deﬁnes prognostic cancer subtypes.
PLoS Comput. Biol., 7, e1002227.

Zhou,G. et al. (2012) Common and individual features analysis: beyond canonical
correlation analysis. Arxiv preprint arXiv:1212.3913.

 

261 6

112 /810'S112umo[pJOJXO'sot112u1101utotq/pd11q 111011 popcorn/110g

910K ‘09 isnﬁnV no 22

