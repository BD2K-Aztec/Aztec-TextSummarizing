ORIGINAL PAPER

Vol. 28 no. 17 2012, pages 2256-2264
doi: 10. 1093/bioinformatics/bts455

 

Gene expression

Advance Access publication July 20, 2012

Discriminative local subspaces in gene expression data
for effective gene function prediction

Tomas Puelma1’2, Rodrigo A. Gutierrez” and Alvaro Soto“

1Department of Molecular Genetics and Microbiology, FONDAP Center for Genome Regulation, Millennium Nucleus
Center for Plant Functional Genomics and 2Department of Computer Science, Millennium Nucleus Center for Plant
Functional Genomics, Pontificia Universidad Catolica de Chile, Santiago, Chile

Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: Massive amounts of genome-wide gene expression data
have become available, motivating the development of computational
approaches that leverage this information to predict gene function.
Among successful approaches, supervised machine learning meth-
ods, such as Support Vector Machines (SVMs), have shown superior
prediction accuracy. However, these methods lack the simple biolo-
gical intuition provided by co-expression networks (CNs), limiting their
practical usefulness.

Results: In this work, we present Discriminative Local Subspaces
(DLS), a novel method that combines supervised machine learning
and co-expression techniques with the goal of systematically predict
genes involved in specific biological processes of interest. Unlike trad-
itional CNs, DLS uses the knowledge available in Gene Ontology (GO)
to generate informative training sets that guide the discovery of ex-
pression signatures: expression patterns that are discriminative for
genes involved in the biological process of interest. By linking genes
co-expressed with these signatures, DLS is able to construct a dis-
criminative CN that links both, known and previously uncharacterized
genes, for the selected biological process. This article focuses on the
algorithm behind DLS and shows its predictive power using an
Arabidopsis thaliana dataset and a representative set of 101 GO
terms from the Biological Process Ontology. Our results show that
DLS has a superior average accuracy than both SVMs and CNs.
Thus, DLS is able to provide the prediction accuracy of supervised
learning methods while maintaining the intuitive understanding of CNs.
Availability: A MATLAB® implementation of DLS is available at http://
virtualplant.bio.puc.cI/cgi-bin/Lab/tools.cgi

Contact: tfpuelma@uc.cl

Supplementary Information: Supplementary data are available
at http://bioinformatics.mpimp-golm.mpg.de/.

Received on October 3, 2011; revised on June 20, 2012; accepted on
July 16, 2012

1 INTRODUCTION

Discovering the biological processes that genes carry out inside
the cell is a major challenge to understand gene function
at a genome-wide scale. Unfortunately, many organisms lack
in-depth understanding about the genes involved in speciﬁc bio-
logical processes. As an example, in the favorite model in plant
biology, Arabidopsis thaliana, 16319 (52%) of its genes lack

 

*To whom correspondence should be addressed.

annotations about their biological processes in the Gene
Ontology (GO) database (GO annotations date: November
9, 2010) (Ashburner et al., 2000; http://www.geneontology.org).

Machine learning (Mitchell, 1997) has emerged as one of the
key technologies to support gene function discovery. In particu-
lar, many methods have been proposed to take advantage of the
massive amounts of microarray expression data available
(see Valafar, 2002; Zhao et al., 2008 for reviews). These predic-
tion methods can be classiﬁed into two broad groups: supervised
and semi-supervised approaches. On one hand, supervised tech-
niques use a labeled training set of genes to learn how to dis-
criminate the genes of each label or function. On the other hand,
semi-supervised approaches first group genes in an unsupervised
manner, without using any functional information, and then a
prediction is performed, usually by propagating the over-
represented functions among the genes of each group (“guilt-
by-association’ rule, Walker et al., 1999).

Among supervised machine learning techniques, Support
Vector Machines (SVMs) (Cortes and Vapnik, 1995) have been
one of the most successful approaches to predict gene function,
as has been shown by several works (Brown et al., 2000; Mateos
et al., 2002; Yang, 2004; Barutcuoglu et al., 2006). However,
despite their theoretical advantage in terms of classiﬁcation
accuracy, in practice, SVMs present the mayor inconvenience
of operating as a black-box (Barakat and Bradley, 2010).
Although additional techniques can be applied to extract com-
prehensible semantic information from SVM models, their ap-
plication is not straightforward and is usually restricted to
linear-SVM models (Guyon et al., 2002; Fung et al., 2005;
Wang et al., 2009). In the general case of non-linear SVMs, the
transformation of the data to high-dimensional spaces compli-
cates any interpretation of the SVM solution. In our experience,
this is a major limitation for gene function discovery as under-
standing the predictions is a key aspect to evaluate their biolo-
gical soundness and guide research. This aspect is even more
critical considering the incomplete nature of annotations and
the capability of genes to have multiple functions, which prevents
obtaining an error-free gold standard, and thus evaluating the
absolute accuracy of the methods (false-negative problem;
Mateos et al., 2002; Jansen and Gerstein, 2004).

In contrast to supervised methods, many semi-supervised
approaches have emerged based on simpler, but biologically
sound concepts, such as co-expression and the ‘guilt-by-associa—
tion’ rule (Eisen et al., 1998; Walker et al., 1999; Kim et al., 2001;

 

© The Author 2012. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which
permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.

112 /§JO'S{Bumo [p.IOJXO'SSUBIHJOJUIOIQ/ﬁdllq 11101; pepBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

Discriminative local subspaces

 

Stuart et al., 2003; Blom et al., 2008; Horan et al., 2008;
Vandepoele et al., 2009; Lee et al., 2010; Ogata et al., 2010;
Bassel et al., 2011). The basic assumption in these methods is
that if a group of genes shows synchronized (correlated) expres-
Sion patterns, then there is a high chance for them to participate
in a common biological process. Common techniques used to
group genes are clustering (Eisen et al., 1998; Alon et al., 1999;
Horan et al., 2008), biclustering (see Madeira and Oliveira, 2004;
Tanay et al., 2005; Prelié et al., 2006 for reviews) and
co-expreSSion networks (CNS; Stuart et al., 2003; Vandepoele
et al., 2009; Bassel et al., 2011).

Unfortunately, current methods based on CNS do not offer
the accuracy of supervised methods to predict gene function, as
we show in this work by comparing the performances of CNS
and SVMS. Furthermore, their classiﬁcation strategy poses some
relevant inconveniences. In particular, the selection of a suitable
correlation threshold to deﬁne co-expressed genes is often difﬁ-
cult and arbitrary. Furthermore, both CNS and clustering rely on
global co-expreSSion patterns, meaning that geneS need to be
co-expressed in a large proportion of the data in order to be
grouped together. Usually, these data involve hundreds or thou-
sandS of microarray experiments, each measured under a wide
range of experimental conditions, such aS different time points,
tissues, environmental conditions, genetic backgrounds and mu-
tationS. In thiS scenario, expecting global co-expreSSion becomes
a strong imposition and limitation.

The previous observation haS motivated the development of
biclustering algorithms (Cheng and Church, 2000). The main
idea behind biclustering iS to ﬁnd clusters of geneS that
co-expreSS in subsets of experimental conditions. After the sem-
inal work by Cheng and Church (2000), an extensive list of
biclustering approaches haS been developed (see Madeira and
Oliveira, 2004; Tanay et al., 2005; Prelié et al., 2006 for reviews).
However, beSideS their theoretical advantages, these approaches
have not been extensively used in practice. Based on our experi-
ence, the unsupervised local search of experimental conditions
often leadS to clusters with geneS from a broad range of func-
tionS, thuS, limiting their discriminative properties. ThiS problem
iS even worse considering the noisy nature of microarray data,
which often leadS to the discovery of biologically meaningleSS
biclusterS. Selecting datasetS in a ‘condition-dependent’ faShion
Should more precisely identify gene interactions relevant to a
Speciﬁc biological question at hand (Bassel et al., 2011).
However, given the amount of expreSSion data available today,
manual selection of the relevant conditions iS not a practical
solution in most cases.

To overcome the state of the art limitations exposed above and
aid gene functional research, we present Discriminative Local
Subspaces (DLS), a novel machine learning method that discri-
minatively predicts new geneS involved in a biological proceSS of
interest by building a discriminative CN. DLS takeS advantage
of the discriminative nature of supervised learning while main-
taining the expressiveness of CN approaches.

Unlike other co-expreSSion-based methodS, DLS exploits the
existing knowledge available in GO to construct informative
training setS. These training setS guide the search of suitable sub-
setS of experimental conditions containing expression signatures.
An expreSSion Signature corresponds to a discriminative expres-
Sion pattern with two key properties: (i) it iS deﬁned in a local

subspace of the data (i.e. a particular gene and a subset of ex-
perimental conditions) and (ii) it iS highly discriminative
(exclusive) for the positive training geneS associated to a biolo-
gical proceSS of interest. AS a further feature and to tackle the
inherent noise of negative training setS (geneS not related to a
biological proceSS), DLS incorporates a procedure that itera-
tively predicts false-negative (FN) geneS and reﬁneS the training
set in order to improve itS prediction performance.

The discriminative nature of expreSSion Signatures allows DLS
to reveal novel co-expreSSion associations for the selected pro-
ceSS. In contrast to discriminative black-box modelS, such aS
SVMS, these predicted associations can be exposed in the context
of a discriminative CN, giving the scientist the possibility to visu-
alize, evaluate and interpret the predicted associations.

Unlike traditional CN, DLS doeS not rely on a predeﬁned and
ﬁxed correlation threshold to construct the networkS. Instead,
DLS useS a Bayesian probabilistic approach that adaptively
deriveS a conﬁdence score for each predicted association. A net-
work iS then constructed based on a deSired minimum conﬁ-
dence, which iS translated into different correlation threSholdS
depending on the discriminative level of each Signature.

In order to test the prediction power of our method, we use an
A. thaliana expreSSion dataset containing 2017 microarray
hybridizationS. We compare DLS performance with respect to
CN and two verSionS of SVM, linear-SVM and radial baSiS
kernel (RBF)-SVM. The accuracy and predictive power of the
methodS are tested uSing cross-validation and also testing the
enrichment of year 2008 predictions with respect to new 2010
annotations, uSing 101 representative GO termS from the
Biological ProceSS Ontology. Our results reveal that DLS attainS
superior average accuracy and Similar predictive power than
RBF-SVM. Furthermore, they Show a clear advantage for
DLS over linear-SVM and CN in both testS. Remarkably, they
Show that unlike SVM and CN, DLS iS able to systematically
improve itS predictive power when increasing the number of
available experimental conditions.

The rest of the article presents the detailS behind DLS method
(Section 2), our experimental setup (Section 3), the main
results (Section 4) and our principal conclusions of thiS work
(Section 5).

2 METHODS

DLS consists of four main consecutive steps: pre—processing of raw data,
construction of a labeled training set, training and classiﬁcation (or pre-
diction). Additionally, DLS has two relevant steps for gene function pre-
diction: the construction of a discriminative CN of predictions and the
discovery of potential FNs in the training set. We detail next each of these
main parts that compose the proposed DLS method.

2.1 Expression data pre-processing

A key aspect to use massive microarray data to perform effective
gene functional predictions is to apply suitable pre—processing steps to
extract informative features and to handle the noisy nature of raw
expression data. We consider a generic case, in which we have a dataset
containing multiple microarray experiments, each performed in replicates
among several experimental conditions and coming from different
sources. We organize this dataset in M control—test pairs of experimental
conditions. These pairs can be manually deﬁned by an expert or by

 

2257

112 /§JO'S{Bumo [p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdllq 11101; pepBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

T.Puelma et aI.

 

using the automatic procedure described in the following paragraph.
For each deﬁned pair, we apply the RankProducts algorithm
(Breitling et al., 2004), which provides a statistical methodology to ﬁnd
the signiﬁcance level between expression changes of genes over two
experimental conditions with replicates. From this procedure, we obtain
a N x M matrix XLR with N genes and M log-ratio expression features,
each corresponding to the logarithm of the fold change between the gene
expressions in the test with respect to the control condition. The statistical
signiﬁcance of each change is provided in a second N X M X FDR contain-
ing false discovery rates (FDRS). In few words, a small FDR value indi-
cates that the corresponding change has a highly consistent rank among
the replicates of the compared experiments and thus a low probability of
being a false-positive detection (Breitling et al., 2004). The X FDR matrix is
used by DLS to guide the search of discriminative expression pattern in
X LR, by favoring the features with signiﬁcant expression changes. A sche-
matic view of this process can be seen in Supplementary Figure S1.

Manual deﬁnition of control—test pairs of experimental conditions can
be a tedious and time-consuming task when using public databases con-
taining thousands of microarray slides. Unfortunately, few public data-
bases provide well—formatted annotations and labels for the available
slides. Thus, in most cases it is impossible to systematically ﬁnd the
control—test pairs of conditions originally deﬁned for each experiment.
However, in many cases, it is possible to deﬁne which slides are replicates
and which are part of the same experimental set. Thus, we propose an
automatic procedure that uses this information in order to generate all
possible pairs of conditions within a given experimental set, generating one
log-ratio feature vector for each of them using the RankProducts method.
Thus, if an experiment has NC different conditions, our procedure gener-
ates NC (NC — l)/2 log-ratio expression features. In order to minimize the
redundancy that this procedure might generate, we consider a feature
vector only if it does not have a ‘high’ correlation with any of the already
added features of the same experiment. In the dataset used in this work, we
deﬁne as ‘high’ a correlation >0.9. Although this procedure might generate
some biologically meaningless comparisons, they should not affect the
performance of DLS because its automatic selection of discriminative
features should ﬁlter non-informative features. Moreover, even if some
unexpected informative comparisons are found, these may provide new
biological insights about the predictions and the process.

2.2 Training set: acquisition of functional labels

In order to search for discriminative expression patterns for a speciﬁc
Biological Process of interest (BP), DLS needs a labeled training set of
genes. Each training gene must be labeled as positive or negative, depend-
ing on whether the gene participates or does not participate in BP,
respectively. DLS derives these labels using the gene annotations avail-
able in GO (Ashburner et al., 2000). These annotations are organized
hierarchically as a directed acyclic graph (DAG) of functional terms,
going from the most general term, at the root node, down to the most
speciﬁc terms, at the leaves of the graph. A relevant fact of this hierarch-
ical organization is the upward propagation of functional annotations.
More precisely, genes that receive a direct annotation at a speciﬁc level of
the hierarchy also inherit all the functional annotations of their more
general ancestors in the hierarchy.

The derivation of positive class C2,}, consists of selecting the genes
annotated directly or by inheritance in GO terms related to BP.
Optionally, this list can also be customized by the user. The derivation
of the negative class Con is a more ambiguous task, mainly due to missing
functional labels. In effect, the list of annotations in G0 is still incom-
plete, therefore it does not preclude that a gene not annotated with a
particular biological process might indeed participate in it. Furthermore,
the almost total absence of negative annotations and the ability of genes
to be involved in multiple biological processes add extra complications.
We face these inconveniences by using the multiple GO annotations of
the positive genes to build a set Con composed of genes that have a ‘low

chance’ of being involved in BP. Our main intuition is that GO terms
containing a substantial number of positive genes are likely to be func-
tionally related to BP, and hence, they have a high chance to contain
genes involved in BP. Following this intuition, we consider a GO term as
‘negative’ if it contains no more than a percentage P of genes already
included in C21,. Consequently, the negative training set C93,, is formed by
genes that have at least one direct annotation in a ‘negative’ GO term and
do not have annotations in positive (non-negative) GO terms. According
to our experiments, a value of P = 5% provides a good trade-off between
the rates of false and true negatives. To handle the case of mislabeled
genes, DLS also incorporates a false negatives discovery option that helps
to reﬁne the training set (details in Section 2.6).

2.3 Training: identifying expression signatures

The aim of the training scheme used by DLS is to identify a set of suitable
expression signatures for the biological process of interest BP. Each ex-
pression signature is deﬁned by a discriminative local subspace of the
expression data matrix XLR described in Section 2.1. The core of this
scheme is based on four concepts about gene expression:

(1) Co-expression: genes exhibiting co-expression patterns are likely to
be co-regulated, and hence, they are likely to participate in a
common biological process. Consequently, DLS uses the positive
genes C2,}, to search for characteristic co-expression patterns for
genes involved in BP.

(2) Subspaces: genes participating in the same biological process
are usually not co-regulated under all cellular conditions.
Consequently, DLS searches for co-expression patterns among
subsets of experimental conditions.

(3) Discrimination: genes not sharing a common biological
process may co-express under some experimental conditions.
Consequently, DLS uses the negative genes Con to ﬁlter out
non-discriminative subsets of conditions where positive and nega-
tive genes show co-expression patterns.

(4) Locality: genes participating in the same biological process might
be regulated by different transcription factors and hence, they
might co-express under different experimental conditions.
Consequently, DLS independently searches for a suitable subset
of discriminative conditions for each positive gene in C21,.

In agreement with the previous concepts, the core of the training pro-
cess consists of a feature selection algorithm that looks for a suitable
expression signature for each gene g,- 6 C21,. We achieve this by selecting
a subset of features where g, shows ‘strong’ co-expression with genes in
C2,}, and ‘weak’ co-expression with genes in BP. This feature selection
algorithm explores the space of possible subsets of features using the
Expression Signature Score (ESS) presented in equation (1). This score
evaluates the discriminative power of each potential subset (pattern).
Once the feature selection scheme is ﬁnished, each positive gene
g,- 6 C2,}, has an associated subset of features fse; corresponding to the
most discriminative expression pattern found by DLS. However, only
expression patterns having an ESS(g,[]”Se;])>0 are selected as valid ex-
pression signatures and used in the classiﬁcation process. We describe
next the details of the ESS score and then the main steps behind the
operation of the feature selection scheme.

2.3.] Expression Signature Score Let vector fse; be a subset of the
total set of available features. Furthermore, let g,[fseg] be the expression
pattern of gene g,- considering only the features in fsel
(i.e. g,[fseg] = X LR(i,fseg). The ESS of gene g, for a subset of features fse;
is deﬁned as

ESS(gi[fsez]) = W1 - SCOV€1(gi[fsez]) — W0 - SCOV€0(gi[fsez]) (1)

where Score1(-) and Score0(-) are functions that quantify the level of

 

2258

112 /§JO'S{12umo [p.IOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Discriminative local subspaces

 

co-expression of gene g, with respect to the set of genes in C2,}, and COBP,
respectively, considering only features in fseg. More precisely

Score...(g,-[fsez]) = Z Sgd(00€xp(gimellsgjlﬂell»a (2)

jeCEP

where coexp(-,-) measures the co-expression between two patterns and
sgd(-) corresponds to a sigmoidal function used to establish a continuous
threshold to separate ‘strong’ from ‘weak’ co-expressions. The shape of
this sigmoidal function was tuned for best performance for function pre-
diction using tests described in Section 3 and taking into account our
biological and mathematical knowledge (Supplementary Fig. S2). As a
result, the function returns values between 0 and 1, being close to 0 for
co-expressions with values below 0.6 (weak) and above 0.5 for
co-expressions above 0.8 (strong).

To measure co-expression between the expression patterns of two
genes g,- and g considering features in fsel, we use the absolute value of
the cosine correlation, which can be expressed as the dot product of two
vectors, normalized by their respective magnitudes:

COGXMgilfsengjl/‘selD = abS(COS —0077(gi[l§e1],gj[ﬂe1]))
z abs( gilfsez] 'gjlfsez] ) (3)
|| gi[fsel] || || gj[fsel] II o

The cosine correlation (cos_corr) returns a continuous value between 1
and —1, taking a value of 1 if the two patterns are correlated, —1 if they
are negatively correlated and 0 if they change independently. We use the
absolute value abs(-) to capture positive and negative correlations indis-
tinctively among genes, which improves the prediction performance in
our test. Despite its simplicity, we consider this measure more suited than
the traditional Pearson correlation coefﬁcient (PCC) to measure
co-expression in log-ratio expression data, in which each feature is a
comparison in itself between two conditions. This can be more clearly
seen by the following example: consider the log-ratio expression patterns
of genes g1 = [l,l,0,0] and g2 = [0,0, — l, — 1]. Analyzing these two
patterns, we intuitively do not expect any relation between their corres-
ponding genes because the expression of gene g1 is not affected at all
when gene g2 changes (i.e. g1 2: 0 <—> g2 75 0) and vice versa. This is
very well expressed by the cosine correlation, which returns a value
cos_corr(p1,p2) = 0. Contrarily, the PCC only considers the relative
changes within the features of the patterns, which in this example are
perfectly synchronized, thus returning a PCC(p1,p2) = l, the opposite
from what we expect.

In equation (1), W1 and wo weight the inﬂuence of Score1(-) and
Score0(-), respectively; W1 is deﬁned by a function used to penalize
expression signatures with a small number of features (details
in Section 1 of Supplementary material), whereas wo is a predeﬁned par-
ameter that allows us to adjust the level of discrimination of the expres-
sion signatures in order to avoid overﬁtting the training samples.

2.3.] Feature selection The feature selection algorithm uses the
ESS score in equation (1) to ﬁnd a suitable expression signature for
each positive gene g, 6 C2,). An exhaustive search, however, is not pos-
sible because it requires the evaluation of 2M — 1 possible subsets of fea-
tures for each positive gene. Consequently, we use an iterative and fast
exploration scheme, referred as signFS, which uses suitable heuristics to
efﬁciently search for discriminative expression signatures.

Given a gene expression pattern g, = X LR(i, :), signFS starts by select-
ing an initial set fseg(0) of features where gene 1' signiﬁcantly changes its
level of expression. We deﬁne as signiﬁcant, a change with a FDR value
<0.l in X FD R(i, :). Afterwards, signFS performs an iterative process that,
at each iteration t, adds and/or removes a suitable subset of features F,,
from fse;(t). These changes must increase the expression pattern score
ESS(-). As a consequence, the new subset fse;(t+ 1) should provide
better discriminative properties for gene function prediction. To favor
the exploration of changes that increase discrimination, only a 20% of

the total features, showing the lowest FDR and not in fseg(t), can be
added at each iteration. This fosters the inclusion of features in
fse;(t+ 1) that show the most signiﬁcant expression changes. Details
about the scheme used to select subset F, can be seen in Section 2 of
Supplementary material. This iterative process continues until consecu-
tive modiﬁcations of fseg(t) do not increase the respective score ESS(-).

2.4 Classiﬁcation: using expression signatures to predict
new gene associations

The aim of the classiﬁcation scheme used by DLS is to predict new genes
for a biological process of interest BP. Brieﬂy, as expression signatures
are discriminative, DLS considers that if a gene is highly co-expressed
with the expression signature of a gene in C21,, then it is likely to be
involved in BP.

A relevant issue with respect to the previous classiﬁcation scheme is
that not all the expression signatures have the same potential to predict
functional associations. In effect, this potential depends on several factors
such as type of gene, type of biological process, level of noise in the data
and biological complexity of interprocess co-regulations. DLS overcomes
these issues by using a Bayesian inference approach that allows it to
adaptively decide the minimum co-expression level needed by each sig-
nature to predict a gene with a given conﬁdence. Consider a hypothesis, [1,
representing that an unknown gene gj belongs to the positive class C2,).
In addition, consider evidence, e, indicating that gene g has a
co-expression level, L, with respect to the expression signature of gene
g,, ES(g,-). We can estimate the posterior probability P(h|e) by using the
Bayes rule: P(h|e) = P(e|h) - P(h)/P(e).

Prior probability P (k) can be estimated directly from training data by
calculating the proportion of positive versus negative genes in the training
set. However, the estimation of the likelihood term P(e|h) is not so
straightforward as we need to estimate the probability density function
of the co-expressions with respect to ES(g,-). In this work, we estimate
P(e|h) using a kernel-based density function estimation (Parzen, 1962).
Given a data sample x,- and a bandwidth 0, we use a Gaussian kernel
function K(x) = N(x,-, a), which measures the inﬂuence of sample x,- in a
location x of the input space. The bandwidth 0 is a parameter that con-
trols the smoothness of the density estimation and it is optimized using
the cross-validation analysis described in Section 3.

Using the previous procedure, a gene g is predicted as positive by an
expression signature ES (g1), if the co-expression L between them results in
a conﬁdence P (hle) greater than a desired threshold. A graphical example
of the above-mentioned procedure can be seen in Supplementary
Figure S3.

2.5 Construction of a discriminative CN

One of the main features behind DLS is its ability to represent its pre-
dictions as a discriminative co-expression network (DCN), providing
additional insights about the predictions and the biological process of
interest. Formally, a DCN for a biological process BP is deﬁned by a
graph G 3}) = <V,E>, where vertices in set Vrepresent genes, and edges in
set E represent predictions from expression signatures to other genes.
More precisely, there is an edge from gene g, 6 C2,, to gene gj, if there
is an expression signature ES (g,) predicting that g,- is related to BP with a
conﬁdence greater than a pre—deﬁned threshold. In order to construct a
DCN that involves all the genes related to BP, DLS applies the classiﬁ-
cation method to all the N genes in matrix X, including the ones in CBP
used for training. This not only allows DLS to display a network descrip-
tion of the relations between training genes and predicted genes, but also
to expose relevant relations among the positive genes, known to be
involved in BP. A network description allows application of tools and
concepts (Strogatz, 2001) developed in ﬁelds such as graph theory, phys-
ics and sociology that have dealt with network problems before (Alon,
2003). For example, a simple calculation of the node degree of the genes

 

2259

112 /§JO'S{12umo [p.IOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

T.Puelma et al.

 

in the DCN can give relevant insights to discover central and highly
coordinated genes in the biological process of interest.

2.6 Overcoming the FNs problem

One of the most relevant issues in using supervised learning methods to
predict gene function is the FNs problem. In Section 2.2, we present a
method to obtain an informative negative training set C931,.
Unfortunately, due to the inherent complexity of gene behavior and the
incompleteness of annotations, it is not possible to obtain a negative set
without mislabeled genes, which may damage the prediction performance
and evaluation.

To tackle the previous problem, we add to our training algorithm the
option of a bootstrap step, which is able to automatically identify and
temporarily discard from the set COBP, genes that are potential FNs. More
speciﬁcally, this strategy is applied at the start of each iteration t of the
feature selection algorithm signFS, performed in the training of each
positive gene g,. The strategy discards a negative gene g from iteration
t if its co-expression with gene gi, using the selected features in fseg(t),
satisﬁes two conditions: (i) it has a value of at least min_FN_coexp and (ii)
it is among the top pf" % most highly co-expressed negative genes. Notice
that these potential FN genes are not discarded permanently from the
negative training set but they are only not considered in the evaluations of
the patterns generated during step I. At the end of the training process,
the method outputs the potential FNs detected by each expression
signature.

The bootstrap option explained earlier in the text allows us to avoid
overﬁtting problems due to the presence of FNs in the training set, how-
ever, it may affect the discriminative level of the signatures by ignoring
some true negatives during the training process. Thus, we develop an
iterative method, False-Negatives Discovery (FND), that takes advantage
of this option in order to predict FN genes in a more precise and inform-
ative manner.

Initially, the list GFN of potential FNs contains all negative genes in
Con (GFN = COBP). Then, each iteration of the method applies three con-
secutive steps, used to incrementally bound and reﬁne the list GFN. In the
ﬁrst step, a model is trained using the bootstrap option explained in the
previous paragraph. GFN is then bounded to the potential FN genes de-
tected by at least one trained signature. In the second step, the trained
signatures are used to classify the genes in GFN, ﬁltering out the ones not
predicted as positive. Finally, in the third step, the training algorithm is
used to search for a suitable expression signature for each gene in
GFN. This algorithm is used without the bootstrap option. Then, a gene
g in GFN is predicted as a FN if the method is able to ﬁnd an
expression signature ES(gJ-) that satisﬁes two conditions: (i)
ESS(gJ-[fseg])>0 [equation (1)] and (ii) Score1(gj[fseg]) [equation (2)] is
greater than the average Score1(-) obtained among the valid expression
signatures of the positive class C2,). The ﬁrst condition imposes to the
predicted FNs to be discriminativer connected to other positive genes,
whereas the second condition imposes them to be at least as connected as
an average positive gene.

The three steps described earlier are executed iteratively by the FND
method, automatically moving the predicted FNs to the positive set of the
next iteration. The method stops if no new FNs are predicted or if a
maximum number of iterations are reached. After performing the FND
method, the training set can be reﬁned, either by eliminating the predicted
FNs from the negative set or by moving them to the positive set. This
reﬁned set is then used to train a DLS model and obtain the ﬁnal
predictions.

3 EXPERIMENTAL SETUP

A systematic evaluation waS performed uSing an A. thaliana ex-
pression dataset and 101 GO biological processes. We compare

the performance of DLS against two widely used state-of-the-art
algorithms: SVMS (Brown et al., 2000) and CNS (Vandepoele
et al., 2009).

In our testS, we used two expression datasetS, pre—processed aS
described in Section 2.1, but uSing different procedures to deﬁne
the control—test pairs of experimental conditions. In the ﬁrst
dataset, M: 643 pairs were manually deﬁned by an expert, start-
ing from a raw dataset containing 2017 A. thaliana ATHl micro-
array SlideS (including replicates). We refer to thiS dataset aS the
‘expert—dataset’. For the second dataset, a total of M: 3911 fea-
tureS were derived by the automatic procedure described in
Section 2.1, starting from an updated raw dataset containing
3352 SlideS. We refer to thiS dataset aS the ‘automated-dataset’.
Most SlideS were obtained from the International Affymetrix
Service of the Nottingham Arabidopsis Stock Centre (NASC,
www.affymetrix.arabidopSiS.info).

The evaluations consider the selection of 101 representative
GO-termS from the 3500+ GO-termS available for A. thaliana
in the biological process ontology. ThiS selection waS performed
uSing the annotations available in G0 on May 8, 2008. First, we
ﬁltered out all the annotations with IEA evidence code (Inferred
from Electronic Annotation), aS they are not reviewed by a cur-
ator. Then, we selected representative functional GO-termS uSing
a depth-ﬁrst strategy, searching for the ﬁrst GO-term of each
branch containing between 30 and 500 annotated geneS. ThuS,
thiS selection iS representative of the Space of possible biological
processes in the sense that all the branches of the GO DAG are
represented by at least one GO-term in our selection. In other
wordS, all the GO-termS that were ﬁltered out are either subca-
tegories (descendants) or broader categories (ancestors) of at
least one of the selected GO-termS. The selected GO-termS are
LevelS 2—6 in the GO hierarchy and cover a wide range of bio-
logical processes, such aS responses to different stimulus and
various metabolic and developmental processes. The complete
list of selected GO-termS iS available in a Supplementary material
Spreadsheet ﬁle.

We derived a labeled training set for each selected GO-term, aS
described in Section 2.2. The number of positive geneS in these
training setS varieS from 30 to 474, with an average of 162 geneS.
The number of negative geneS varies from 1011 to 4112, with an
average of 3105 geneS. AS can be seen, negative training setS are
much bigger than positive oneS, which iS expected, because most
geneS are not involved in a particular biological process.

Cross-validation testS were performed uSing three lO-fold
cross-validation testS over each GO-term. Each test waS per-
formed using a different lO-fold partition. AS evaluation metrics
we used:

P . . lTPl R [I lTPl
rectszon = — , eca = —
|TP| -l- IFPI |TP| —l— |FN|
1 2
Ff; — score = %,

 

recall precision

where |T Pl, |FP| and |FN| correspond to the number of true
positives, false positives and false negatives, respectively.
Precision measures the proportion of positive predictions that
are correct. Recall measures the proportion of positive geneS
that are predicted aS positive. Finally, the Fﬁ score provides a
joint evaluation of both precision and recall, by calculating their

 

2260

112 /§JO'S{12umo [p.IOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Discriminative local subspaces

 

harmonic mean. The ,6 parameter controls the weight given to
precision with respect to recall. In our testS, we used ,6 = 2 (F2
score), in order to favor accurate models over models with high
recalls but large false positive rateS.

In addition to the cross-validation analySiS, an alternative,
more realistic evaluation waS performed, testing the enrichment
of new annotations available on September 7, 2010, in the
positive predictions of each method trained uSing the annota-
tionS of year 2008. ThiS enrichment waS tested uSing a hypergeo-
metric distribution and a P—value threshold of 0.1 to consider
enrichment.

In order to facilitate the analySiS of our results, we summarize
them uSing three criteria. The ﬁrst criterium consists of counting
the number of GO-termS in which each method attainS useful
predictions. In the case of cross-validations, we consider aS useful
the GO-termS with precisionS >0.33, meaning that at least one of
three predictions are correct (Fig. 1A). In the case of enrichment
analyses, we consider aS ‘useful’ the GO-termS attaining enriched
predictions (Fig. 2A). The second criterium consists of evaluating
the average performances of the methods considering the 101
tested GO-termS. In cross-validations, we include precision,
recall and F2-Score averages (FigS lB—D), whereas we include
P—value averages for the enrichment analyses (Fig. 2). Finally,
the third criterium consists of a pairwise comparison of the per-
formances of the methods over each GO-term. Given two meth-
odS, A and B, we count the number of GO-termS in which
A outperforms B and vice versa. Only GO-termS with useful
predictions are counted. The F2-ScoreS and P—valueS were con-
Sidered aS performance measures for cross-validations (Fig. 1E)
and enrichment analyses (Fig. 2C), respectively.

The workﬂow used for the evaluations iS aS follows. We ﬁrst
perform cross-validation and enrichment analyses uSing the
expert-dataset aS described earlier in the text. The automated

dataset iS evidently more prone to both useleSS and redundant
features, aS some of them may be deﬁned uSing biologically
meaningless comparisons. ThuS, the expert-dataset iS used in
order to ensure quality control—test condition pairs for the evalu-
ationS. In addition, we perform enrichment analyses uSing the
automated-dataset with two Speciﬁc aims: (i) test the potential of
the automated-dataset for function prediction and (ii) test the
performance of the methods in datasetS with an increasing
number of conditions (features). ThuS, in addition to the com-
plete dataset of M: 3911 features, we use two additional smaller
datasetS, deﬁned by a random selection of M ={1000, 2000}
features.

In termS of the evaluated methods, we use the conﬁguration
and parameters providing the highest average F2-Score in the
cross-validation analySiS. In the particular case of the proposed
method, we report the performance of two alternative conﬁgur-
ationS, one uSing the False Negative Discovery method
(FND-DLS), described in Section 2.6 and other without uSing
it (DLS). In the case of FND-DLS, the ﬁnal predicted FNS are
added to the positive training set.

For the SVM method, we use the implementations available aS
part of the library for support vector machines (LIBSVM)
(Chang and Lin, 2001). Similar to Brown et al. (2000), we
tested four types of kernels: RBF, linear kernel and two polyno-
mial kernelS with degrees equal to two and three, respectively. AS
reported by Brown et al. (2000), RBF—SVM Shows the best per-
formance. However, aS linear-SVM haS the advantage of being
more eaSily interpreted, it provides a good reference point to
compare the performance of our method. Consequently, we
report the results of both, RBF-SVM and linear-SVM. In
termS of the selection of relevant parameters for the different
SVM models, we tested different conﬁgurations following the
default values suggested by the LIBSVM library and the

A Eiuﬂ-terrnt with precision :- 0.33 _ .- _-'--'- :-' --1'-:| B Average FI-Store _|’.- I c Average prerlsjon _I: ': D Ave-rage recall | 'I If
 m m m m
REF-Sm m m m
DLS —El _EI _EEI -:EI

'u- '-.-.-' 1
E Pairwise FZ-seore tomparison E-'= _.-;: :.|."."l'-'1_
FND-m5 _— = ‘- DLS
run-ms #- —:— 
run-ms  .5- 
mn-ms M - :1 cm
ms “ —_ mm
“L5 — __ “NW-5w

DLS

l Method shown it lift his but!" FISH!"-

l

HIithl-r mlthod I'll! incur-it- proditt'ront

{'N

l Method shown at rilhl. hi'l hotter F-Sliﬂl'l'

Fig. 1. Results of the lO-fold cross-validation analyses performed over 101 representative GO biological processes. FND-DLS consistently shows the
best overall performance, demonstrating the power of our method and the importance of handling false negatives (FNS) present in training sets. Despite
FNS, all the tested supervised methods show superior prediction performances than the semi-supervised method CN. (E) shows the number of GO-terms
in which one method attains better Fz-scores than the other (details in Section 3).

 

2261

112 /810'S[12umo [pJOJXO'SOIJ’BIIIJOJUIOIQ/ﬂdllq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

T.Puelma et al.

 

A (ED-terms with pualue f I11 | - oi unr r 'wrl .'_r1-1Ir'r"-- B Average pvalue _i-'.1I.-'.--rr  l:-=-'.1|-rr_
FND-DLS 515E [Li-B
“emu m _EEI
m 5 _El _Iﬂ
. '£€:‘-i£ ﬁt'i'ﬁ

c Pairwise pvalue :ompariiun_r'..|n‘r1rrn‘ gn-ter11'.]
"mm H ‘— Uta
FHDDL'E 23 H _— mar-5w
moms 4? “ LINEnR 5w
rue-m5 —_ =0 ‘- crv
ms ‘— 4“ .— rem-sum
[II-5 n ‘— LINEAR-EUM
an n 4% m- u.-

l Method shown at left has better enrichment Neither method has enriched predictions I Method ﬂ'lown at right has better enrinrJIment

Fig. 2. Summary of the results of an enrichment analysis performed over 101 representative GO biological processes. The analysis consists of testing the
enrichment of new annotations from year 2010, in the predictions done by each method using annotation from year 2008. FND-DLS and RBF-SVM
show the best overall performances, with a small advantage for FND-DLS. Figure (C) shows the number of GO-terms in which one method attains

better enrichments (lower P-values) than the other (details in Section 3).

optimization methods proposed by Brown et al. (2000). More
detailS about the parameters selected for the SVMS can be found
in Section 6 of Supplementary Material.

For the CN method, we construct the networks uSing the
cosine correlation metric. Predictions are performed uSing a
guilt-by-association criteria, uSing the hypergeometric distribu-
tion and Bonferroni correction for multiple testS. We tested ﬁve
networks, applying correlation thresholds of 0.5, 0.6, 0.7, 0.8 and
0.9, respectively. In addition, we tested three P—value thresholds,
0.1, 0.05 and 0.01. We report the results of the CN model uSing a
correlation and P—value value thresholds of 0.6 and 0.1, respect-
ively, aS thiS provides the highest average F2-SCOI‘C.

The code and data to run these analyses over each method are
available for MATLAB® programming software and can be
downloaded from the link provided in the Availability Section.

4 RESULTS AND DISCUSSION

The results of cross-validation and enrichment analyses uSing the
expert-matrix are summarized in Figures 1 and 2, respectively.
The results of the enrichment analySiS uSing the automated-
matrix are summarized in Figure 3. The complete report of
results can be found in Supplementary material Spreadsheets
available online. The rest of thiS Section presents and discusses
these results.

4.1 FND-DLS shows the best overall prediction
performance

Our results Show that FND-DLS outperforms

all competing methods, whereas RBF-SVM consistently

attainS the second best performance. In the case of the cross-

validation analySiS, FND-DLS attainS useful predictions

Ell. 3
[lard {1.3-1 + DLS-

ﬂa‘m + REF-SUM

+LlNEAH-5VM

 

Enrichment pualue

I].':"—." EN

'1 "i" 3.5::-

[143 mm ELIGIL'I 39 1. 1
Number of featum In expression matrix

Fig. 3. Results of enrichment analyses done over gene expression datasets
with an increasing number of features (details in Section 3). In contrast to
both SVM and CN, DLS shows a remarkable ability to systematically
increase its performance when more features are added to the dataset. In
fact, for the datasets with M Z 1000 DLS outperforms all other methods,
showing the greatest potential to exploit the increasing amounts of gene
expression data.

(precision>0.33) in 96% of the considered GO-termS, corres-
ponding to 15%, 24%, 33% and 49% more GO-termS than
RBF-SVM, DLS, linear-SVM and CN, respectively (Fig. 1A).
In addition, it attainS an average F2-SCOI‘C of 0.44, whereas
RBF-SVM, DLS, linear-SVM and CN attain averages equal to
0.29, 0.22, 0.20 and 0.15, respectively (Fig. 1B). Although
FND-DLS attainS better average precisionS than the other meth-
odS (Fig. 1C), itS supremacy in termS of the F2-SCOI‘C iS mostly
explained by itS higher recalls. FND-DLS attainS an average
recall of 0.29, whereas RBF-SVM, DLS, linear-SVM and CN
attain average recalls equal to 0.11, 0.11, 0.08 and 0.10,
respectively (Fig. 1D).

We see four main factor that may explain the overall small
level of recalls obtained by the methods: (i) some geneS may be

 

2262

112 /810'S[12umo [pJOJXO'SOIJ’BIIIJOJUIOIQ/ﬂdllq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Discriminative local subspaces

 

regulated under experimental conditions not available in the ex-
preSSion dataset; (ii) some geneS may not be regulated at a tran-
scriptional level and thuS, may not have (common) expression
patterns; (iii) due to miSSing functional labels, some geneS may
only be regulated by (or regulate) geneS not present in the posi-
tive training set and thuS, it may be impossible for the methods to
discriminate them and (iv) FNS geneS may Share and maSk some
discriminative patterns present among positive geneS. The higher
levelS of recall achieved by FND-DLS over the other methods
remark the importance of the last two factors described above.

The higher precisionS obtained by FND-DLS supports the
effectiveness of the FND process. The FND process iteratively
moveS the predicted FNS to the positive set. ThuS, if FND-DLS
wrongly predicted FNS, thiS geneS would become false positive
geneS which, in turn, would decrease the precision of FND-DLS.

Cross-validation iS useful to asseSS the relative performance of
the methods; however, itS results must be considered with cau-
tion (Varma and Simon, 2006). To tackle thiS, we use an enrich-
ment analySiS over a completely new set of labeled geneS (i.e. new
annotations from year 2010) to asseSS the performance of our
method in an alternative and more realistic scenario (details in
Section 3).

Interestingly, the results of the enrichment analySiS conﬁrm the
supremacy of FND-DLS over RBF-SVM, although itS overall
advantage iS smaller than in the cross-validation test (Fig. 2).
FND-DLS attainS enriched predictions in 53% of the GO-
termS, whereas RBF—SVM, DLS, linear-SVM and CN attain
enriched predictions in 52%, 44%, 43% and 39% of the
GO-termS, respectively (Fig. 2A). Note that some GO-termS
have few or no new geneS annotated on year 2010 with respect
to year 2008 and thuS, it iS very difﬁcult or even impossible for
the predictions to be enriched. In addition, the enrichment per-
formance iS affected by the same four factors exposed above for
cross-validation. In terms of enrichment P-value (lower P-value
represent higher enrichments), FND-DLS attainS an average of
0.39, whereas RBF—SVM, DLS, linear-SVM and CN attain aver-
ageS equal to 0.40, 0.46, 0.54 and 0.55, respectively (Fig. 2B).

4.2 Discriminative methods, DLS and SVM, provide more
accurate gene function predictions than CNS

According to our experiments, both verSionS of DLS and SVM
outperform CN. Although CN obtains Similar average recall
levelS than SVMS and DLS (without FND), it failS in providing
predictions aS precisely aS them (Fig. 1C and D). These results
Show the advantages of uSing discriminative training techniques
in contrast to semi-supervised techniques in attaining accurate
gene functional predictions. ThiS assertion iS further supported
by the results of the enrichment analySiS.

4.3 There is no method to rule them all

Although FND-DLS and RBF-SVM Show the best overall per-
formances, when comparing the performance at a term-by-term
scale, we can only conclude that there iS no method able to attain
the best performance through all GO-termS (FigS. 1E and 2C).
There are many factors that can biaS the predictability of geneS
of a biological process toward one method or another. For
example, in GO-termS related to responses, we see a biaS in the
predictability toward DLS in expense of CN, aS the responses are

usually expressed under Speciﬁc environmental or physiological
conditions, which DLS iS able to detect due to itS local search for
discriminative features.

4.4 The discriminative and local expression patterns of
DLS provide effective and meaningful predictions

According to the FDR matrix X FDR, 96.2% of the expression
changes in the log-ratio matrix X LR are not Signiﬁcant in the
expert-dataset (considering an FDR < 0.1 for Signiﬁcance), mean-
ing that on average, geneS Show differential expression in only 24
(3.8%) of the 643 features. ThiS SparseneSS emphasizes the im-
portance of the selection of relevant features to achieve effective
predictions, aS the one performed by DLS. SVMS perform tranS-
formations to higher dimensions, which can also be interpreted
aS an implicit selection of relevant features. However, these tranS-
formations complicate the interpretation of the predictions and
the extraction of further knowledge. Consequently, beSideS the
prediction power of DLS, a key advantage over SVMS and other
discriminative state of the art prediction methods iS itS ability to
provide biologically meaningful and interpretable predictions
while maintaining highly accurate predictions. Unlike SVMS,
DLS iS able to visually expose itS predictions in the form of a
network. ThiS network delivers a much richer interpretability to
the user than SVM, providing key information about the regu-
latory linkages that may exist between the geneS of the functional
claSS of interest. Finally, unlike both SVMS and CNS, DLS iS
able to explicitly reveal the experimental conditions and geneS
that are relevant for each prediction, by extracting the features
and geneS that deﬁne each expression Signature.

4.5 DLS systematically improves its performance as more
experimental conditions are added to the dataset

AS stated above, the lack of informative features iS one of the
factors that may affect the prediction potential of the methods.
In thiS sense, the increasing amount and variety of gene expres-
Sion experiments represent both an opportunity and a challenge.
If the number of available experiments increases, chances to ﬁnd
informative features among them also increase. However, the
amount of uninformative and redundant features Should also
increase, adding extra noise that must be correctly handled by
the prediction methods.

The results of the enrichment analyses performed uSing the
automated-dataset support our previous hypotheSiS and one of
the most remarkable features of our method (Fig. 3). When uSing
the expert-dataset, containing 643 features, DLS achieves an
overall P-value of 0.46. Interestingly, when uSing the auto-
mated-datasets, containing 1000, 2000 and 3911 features, itS
average P-value improves to 0.42, 0.34 and 0.32, respectively.
In contrast, RBF-SVM iS not able to improve itS performance,
linear-SVM Shows little improvement, and the performance of
CN even getS deteriorated. Notice that when uSing the
automated-dataset, DLS achieves the highest overall perfonn—
ance in termS of enrichment, even without uSing the FND
procedure.

These results Show that DLS iS able to overcome the under-
lying noise added by the automated-dataset by effectively
extracting relevant and informative features. In addition, they

 

2263

112 /§JO'S{12umo [p.IOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

T.Puelma et al.

 

support the usefulness of our automatic procedure to generate
log-ratio expression datasets from poorly annotated experiments.
But, most remarkably, they suggest that DLS Should be the most
beneﬁted method aS, in the future, more microarray experimen-
tal data becomes available.

5 CONCLUSION

In thiS work, we described DLS, a novel method that combines
supervised machine learning and co-expreSSion approaches to
effectively predict new geneS involved in a biological process of
interest. We introduced four key concepts that allow DLS to
effectively predict gene function: the derivation of informative
training setS of geneS by discovering FN training geneS, the
supervised search of discriminative expression patterns in subsets
of experimental conditions (expression Signatures), a Bayesian
probabilistic approach to derive the conﬁdence for each predic-
tion and the construction of discriminative CNS to represent
predictions.

By uSing an A. thaliana expression dataset and 101 GO biolo-
gical processes, our experiments Showed that DLS iS able to
provide effective gene functional predictions, with accuracies
comparable to the highly discriminative SVMS, while maintain-
ing the expressiveness of CNS. Interestingly, they also Show that,
unlike SVMS and CNS, DLS systematically improves itS predic-
tion performance aS more experimental conditions are added to
the dataset. ThuS, we believe that the supervised use of
co-expreSSion proposed in thiS work opens new opportunities
to extract meaningful biological hypothesis from the increasing
amounts of expression data, and therefore, to cope with the need
to understand gene functions and biological processes.

Funding: ThiS research waS funded by International Early Career
Scientist program from Howard Hughes Medical Institute,
Fondo de Desarrollo de AreaS PrioritariaS (FONDAP) Center
for Genome Regulation (15090007), Millennium Nucleus Center
for Plant Functional Genomics (P10-062-F), Fondo Nacional
de Desarrollo Cientiﬁco y Tecnologico (1100698), ComiSion
Nacional de Investigacion Cientiﬁca y Tecnologica-ANR pro-
gram (ANR-007) and Corporacion de Fomento de la
Produccion Genome Program (CORFOO7Genoma01).

Conﬂict of Interest: none declared.

REFERENCES

Alon,U. (2003) Biological networks: the tinkerer as an engineer. Science, 301,
1866—1867.

Alon,U. et al. (1999) Broad patterns of gene expression revealed by clustering ana-
lysis of tumor and normal colon tissues probed by oligonucleotide arrays. Proc.
Nat. Acad. Sci. USA, 96, 6745—6750.

Ashburner,M. et al. (2000) Gene ontology: tool for the uniﬁcation of biology. Nat.
Genet., 25, 25—29.

Barakat,N. and Bradley,A.P. (2010) Rule extraction from support vector machines:
a review. Neurocomputing, 74, 178—190.

Barutcuoglu,Z. et al. (2006) Hierarchical multi-label prediction of gene function.
Bioinformatics, 22, 830—836.

Bassel,G.W. et al. (2011) Genome-wide network model capturing seed germination
reveals coordinated regulation of plant cellular phase transitions. Proc.Natl.
Acad. Sci. USA., 108, 9709—9714.

Blom,E. et al. (2008) Prosecutor: parameter-free inference of gene function for
prokaryotes using DNA microarray data, genomic context and multiple gene
annotation sources. BM C Genomics, 9, 495.

Breitling,R. et al. (2004) Rank products: a simple, yet powerful, new method to
detect differentially regulated genes in replicated microarray experiments. FEBS
Lett., 573, 83—92.

Brown,M.P.S. et al. (2000) Knowledge-based analysis of microarray gene expression
data by using support vector machines. Proc. Natl. Acad. Sci. USA, 97, 262—267.

Chang,C.C. and Lin,C.J. (2011) ACM Transactions on Intelligent Systems and
Technology. LIBS VM .' a library for support vector machines, 3, 1—27.

Cheng,Y. and Church,G.M. (2000) Biclustering of expression data. Proc. Int. Conf
Intell. Syst. Mol. Biol., 8, 93—103.

Cortes,C. and Vapnik,V. (1995) Support-vector networks. Mach. Learn., 20,
273—297.

Eisen,M.B. et al. (1998) Cluster analysis and display of genome-wide expression
patterns. Proc. Natl Acad. Sci. USA, 95, 14863—14868.

Fung,G. et al. (2005) Rule extraction from linear support vector machines. In
Proceedings of the eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, KDD ’05. ACM, New York, NY, pp. 32—40.

Guyon,I. et al. (2002) Gene selection for cancer classiﬁcation using support vector
machines. Mach. Learn, 46, 389—422.

Horan,K. et al. (2008) Annotating genes of known and unknown function by
large-scale coexpression analysis. Plant Physiol, 147, 41—57.

J ansen,R. and Gerstein,M. (2004) Analyzing protein function on a genomic scale:
the importance of gold-standard positives and negatives for network prediction.
Curr. Opin. Microbiol, 7, 535—545.

Kim,S.K. et al. (2001) A gene expression map for Caenorhabditis elegans. Science,
293, 2087—2092.

Lee,I. et al. (2010) Rational association of genes with traits using a genome-scale
gene network for Arabidopsis thaliana. Nat. Biotechnol, 28, 149—156.

Madeira,S.C. and Oliveira,A.L. (2004) Biclustering algorithms for biological data
analysis: a survey. IEEE/ACM Trans. Comput. Biol. Bioinform., 1, 24—45.

Mateos,A. et al. (2002) Systematic learning of gene functional classes from DNA
array expression data by using multilayer peroeptrons. Genome Res, 12,
1703—1715.

Mitchell,T.M. (1997) Machine Learning. 1 edn. McGraw—Hill Science/Engineering/
Math, pp. 1—2.

Ogata,Y. et al. (2010) CoP: a database for characterizing co-expressed gene modules
with biological information in plants. Bioinformatics, 26, 1267—1268.

Parzen,E. (1962) On estimation of a probability density function and mode. Ann.
Math. Stat., 33, 1065—1076.

Prelié,A. et al. (2006) A systematic comparison and evaluation of biclustering meth-
ods for gene expression data. Bioinformatics, 22, 1122—1129.

Strogatz,S.H. (2001) Exploring complex networks. Nature, 410, 268—276.

Stuart,J.M. et al. (2003) A gene-coexpression network for global discovery of con-
served genetic modules. Science, 302, 249—255.

Tanay,A. et al. (2005) Biclustering algorithms: a survey. In Aluru,S. (ed.) Handbook
of Computational Molecular Biology. Chapman & Hall/CRC Computer and
Information Science Series, doi: 10.1.1.1339434.

Valafar,F. (2002) Pattern recognition techniques in microarray data analysis: a
survey. Ann. N Y Acad. Sci, 980, 41—64.

Vandepoele,K. et al. (2009) Unraveling transcriptional control in arabidopsis using
cis-regulatory elements and coexpression networks. Plant Physiol, 150, 535—546.

Varma,S. and Simon,R. (2006) Bias in error estimation when using cross-validation
for model selection. BM C Bioinformatics, 7, 91.

Walker,M.G. et al. (1999) Prediction of gene function by genome-scale expression
analysis: prostate cancer-associated genes. Genome Res, 9, 1198—1203.

Wang,X. et al. (2009) An HOG-LBP human detector with partial occlusion hand-
ling. In Computer Vision, 2009 IEEE 12th International Conference on.
pp. 32—39. IEEE, Kyoto.

Yang,Z.R. (2004) Biological applications of support vector machines. Brief.
Bioinform., 5, 328—338.

Zhao,X.-M. et al. (2008) Protein function prediction with high-throughput data.
Amino Acids, 35, 517—530.

 

2264

112 /810'S{12umo [p.IOJXO'SOIlBIIIJOJUIOIQ/ﬁ(11111 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

