Vol. 28 E003 2012, pages i4 73—i4 79
doi: 10. 1 093/bioinformatics/bts3 70

 

Identifying functional modules in interaction networks through

overlapping Markov clustering

Yu—Keng Shih and Srinivasan Parthasarathy*

Department of Computer Science and Engineering, the Ohio State University, Columbus 43210—1277, OH USA.

 

ABSTRACT

Motivation: In recent years, Markov clustering (MCL) has emerged
as an effective algorithm for clustering biological networks—for
instance clustering protein—protein interaction (PPI) networks to
identify functional modules. However, a limitation of MCL and its
variants (e.g. regularized MCL) is that it only supports hard clustering
often leading to an impedance mismatch given that there is often a
significant overlap of proteins across functional modules.

Results: In this article, we seek to redress this limitation. We
propose a soft variation of Regularized MCL (Fl-MCL) based on the
idea of iteratively (re-)executing R-MCL while ensuring that multiple
executions do not always converge to the same clustering result
thus allowing for highly overlapped clusters. The resulting algorithm,
denoted soft regularized Markov clustering, is shown to outperform
a range of extant state-of—the—art approaches in terms of accuracy
of identifying functional modules on three real PPI networks.
Availability: All data and codes are freely available upon request.
Contact: srini@cse.ohio-state.edu

Supplementary Information: Supplementary data are available at
Bioinformatics online.

1 INTRODUCTION

Advances in technology have enabled scientists to determine,
identify and validate pairwise protein interactions through
a range of experimental approaches. Recently, several high-
throughput approaches have produced a large scale of protein—
protein interaction (PPI) datasets. These approaches include yeast
two-hybrid, protein co-immunoprecipitation followed by mass
spectrometry (MS), protein chip technologies and tandem afﬁnity
puriﬁcation (TAP) with MS. Such data have led researchers to
discover protein functions through PPI networks, in which a node
represents a protein and an edge mimics an interaction between two
proteins. A fundamental goal here is to discover functional modules
or protein complexes in order to predict the function of unannotated
proteins.

The fundamental concept of identifying functional modules is
that a pair of proteins interacting with each other has higher
probability of sharing the same function than two proteins not
interacting with each other. The dense sub-networks in a PPI
network can therefore be identiﬁed as functional modules. Thus,
identifying functional modules is similar to detecting communities
(clusters) in a network (graph). However, traditional community
detection algorithms are usually ‘hard’ clustering algorithms, i.e.
they produces non-overlapped clusters, whereas functional modules
are highly ‘overlapped’ (Li et al., 2010). As a result, a number of

 

*To whom correspondence should be addressed.

‘soft’ clustering algorithms have been recently proposed to identify
functional modules in a PPI network, and they can be grouped into
three categories.

The ﬁrst category includes algorithms such as Peacock (Gregory,
2009), hub-duplication (Ucar et al., 2006) and DECAFF (Li et al.,
2007). These algorithms identify the bridge nodes at the beginning,
i.e. nodes belong to multiple clusters, and then either duplicate
or remove the bridge nodes from the network. A hard clustering
algorithm is then applied on the modiﬁed network. The problem with
this approach is that only the identiﬁed bridge nodes can belong to
multiple clusters, and it is conﬂicted with the literature (Ashburner
et al., 2000) that a large fraction of proteins belong to multiple
functional modules. For example, in the yeast network in BioGRID
database (Stark et al., 2011), there are 3085 proteins annotated by
low-level Gene Ontology (GO) terms, whose information content
(see Sec 4.1) is higher than 2.5, and 2392 of 3085 proteins are
annotated by at least two of these GO terms.

Algorithms in the second category adopt line-graph
transformation. These algorithms (Ahn et al., 2010; PereiraLeal
et al., 2004) ﬁrst transform the input network into a line graph, in
which a node represents an edge in the original network. Then, a
hard clustering algorithm is applied on the line graph, so edges
are clustered instead of nodes. A node in the original network
belongs to multiple clusters if its incident edges are clustered
into different clusters. It has been pointed out in the literature
(Fortunato, 2010) that clustering edges has a similar issue as
clustering nodes: a ‘bridge edge’ that connects nodes of different
clusters can only be clustered into one cluster by the line-graph
technique. Furthermore, while functional modules are so highly
overlapped that an interaction might belong to multiple modules,
these algorithms cannot successfully identify all overlapped
functional modules.

Algorithms in the third category aim to ﬁnd local dense sub-
networks instead of globally clustering a graph. Each node forms
a singleton cluster at the beginning, and then each cluster iteratively
adds a neighbor node according to different criteria. Algorithms in
this category include MCODE (Bader and Hogue, 2003), CFinder
(Adamcsek et al., 2006), DPClus (Altaf-Ul-Amin et al. , 2006), IPCA
(Li et al., 2008), MoNet (Luo et al., 2007), CORE (Leung et al.,
2009), COACH (Wu et al., 2009), DME (Georgii et al., 2009),
RRW (Macropol et al. , 2009), NWE (Maruyama and Chihara, 2011),
SPICi (Jiang and Singh, 2010), HUNTER (Chin et al., 2010) and
HC—PIN (Wang et al., 2011).

Although the resulting clusters could be highly overlapped, one
main drawback of those algorithms is that the criterion for adding
a node usually considers relatively local topology. Given that PPI
networks are estimated to be quite noisy (Brohee and Helden,
2006), these algorithms could add several nodes connected by noisy
edges.

 

© The Author(s) 2012. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which
permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.

112 [3.10'811211an[p.IOJXO'SODBIIIJOJIIlOlCI/[I(11111 IIIOJJ popeolumoq

910K ‘09 lsnﬁnV uo ::

K-K.Shih and S.Parthasarathy

 

In addition to those in the above three categories, there are
some other algorithms, such as RNSC (King et al., 2004), principal
component analysis (PCA)-based consensus clustering (Asur et al.,
2007) and Markov clustering (MCL) algorithm (Dongen, 2000),
which have targeted identiﬁcation of functional modules. The
detail of most above-mentioned algorithms can be found in recent
surveys (Fortunato, 2010; Li et al., 2010). MCL, which is based on
manipulation of transition probabilities or stochastic ﬂows between
nodes of the graph, is shown to be particularly noise-tolerant as
well as effective in identifying high-quality functional modules
(Brohee and Helden, 2006; Vlasblom and Wodak, 2009). Several
studies, such as (Friedel et al., 2009), (Moschopoulos et al., 2008)
and (Srihari et al., 2009), have adopted MCL as a base algorithm
to produce more accurate results. Recently, (Satuluri et al., 2010)
propose an efﬁcient and robust variation of MCL, called Regularized
MCL (R—MCL). They show that R-MCL’s regularize operation
and balance parameter can improve the accuracy of identifying
functional modules. Nevertheless, MCL and R-MCL only generate
non-overlapped clusters, and they always assign all proteins into
clusters while not all proteins are functionally annotated. As a result,
MCL and R—MCL usually produce more false-positive clusters than
other algorithms (Brohee and Helden, 2006; Li et al., 2010).

In this article, we redress the limitation of R-MCL and propose
a new variation called ‘Soft’ R-MCL (SR-MCL), which produces
overlapped clusters. The intuition of SR—MCL is to produce
overlapped clusters by iteratively re-executing R—MCL while
ensuring the resulting clusters are not always the same. In order
to produce different clusterings in each iteration, the stochastic
ﬂows are penalized if they ﬂow into a node that was an attractor
node in previous iterations. Since iteratively re—executing R-MCL
would produce several redundant and low-quality clusters, a post-
processing is applied to remove those clusters. Only a cluster that
is not removed by the post-processing is predicted as a functional
module, so not all proteins are assigned into clusters.

We have conducted a series of experiments on three networks in
Saccharomyces cerevisiae. Based on the gold standard annotation,
GO terms (Ashburner et al., 2000), we ﬁnd that SR-MCL
has signiﬁcantly higher accuracy than R-MCL. SR—MCL also
outperforms a range of algorithms on these three networks. Since
it has been pointed out that there are different scales of potential
functional relevance within a PPI network (Lewis et al., 2010),
we also demonstrate that R-MCL is capable of identifying both the
parent module as well as the child module in the GO hierarchy.

2 TERMINOLOGY

Let G: (V,E) denote a PPI network, which is an undirected graph
excluding self-loops, where V is the set of nodes (proteins), E is
the set of edges (interactions) and n = |V|. Each edge is denoted by
(vi,vj), vi,vj e V. w((vi,vj)) is the weight of an edge (vi,vj), which
represents the conﬁdence level of the interaction in a weighted PPI
network. If the network is unweighted, the weight of an edge is
always 1. Let A be the adjacency matrix of the graph such that

w(vi,vj) if (vi,vj) 6E
A(i,j)= maxx¢iw(vi,vx) if1 vi 2v]-
e se

A column stochastic matrix M is a n by 11 matrix that can be
interpreted as the matrix of the transition probabilities of a random

walk (or a Markov chain) deﬁned on the graph. Speciﬁcally, M0,]-

represents the probability of a transition from vj to vi. We also refer
to the transition probability from vi to vj as the ﬂow from vi to vj. The
canonical ﬂow matrix MG is deﬁned as MGO’D =A(i’j) /Z"=1A(x’j).

3 METHOD

3.1 Prior work on Markov clustering

MCL and R-MCL are graph clustering algorithms based on a simulation of
stochastic ﬂows on the graph. MCL consists of two operations on a stochastic
matrix: ‘Expand’ and ‘Inﬂate.’ The Expand operation is simply M =M x M,
and the Inﬂate operation raises each entry in the matrix M to the inﬂation
parameter r (r> 1, and typically set to 2) followed by re-normalizing the
sum of each column to 1. These two operations are applied in alternation
iteratively, starting with M =Mg, where MG is the canonical ﬂow matrix.
In R-MCL, Expand is replaced by ‘Regularize’, which is M =M ng.
The Expand and Regularize operations spread the ﬂow out of a vertex to
potentially new nodes. This has the effect of enhancing within-cluster ﬂows
as there are more paths between two nodes that are in the same cluster than
between those in different clusters. At the start of this process, the distribution
of ﬂows out of a node is relatively smooth and uniform; as more iterations
are executed, all the nodes within a tightly linked group of nodes will start
to ﬂow to one node within the group. This allows us to identify all the nodes
that ﬂow to the same ‘attractor node’ as belonging to one cluster. (Satuluri
et al., 2010) additionally introduce a balance parameter into R-MCL. For a
complete description of MCL and R-MCL, the reader is referred elsewhere
(Dongen, 2000; Satuluri and Parthasarathy, 2009; Satuluri et al., 2010).

3.2 Overlapping MCL

Although R-MCL is effective and efﬁcient in hard clustering, it has three
issues in identifying functional modules, which are usually hierarchical and
highly overlapped. First, R-MCL usually merges functional modules sharing
the same (bridge) node(s). A bridge node usually interacts with a large number
of nodes in a PPI network, so it is likely to become an attractor node. As
shown in Figure 1a, two modules are clustered together by R-MCL with the
bridge node V5 being the attractor node. Second, R-MCL cannot identify
modules with large overlaps since it is a hard clustering algorithm. For
example, R-MCL cannot produce any cluster similar to the green module
in Figure 1b. Third, again, because R-MCL is a hard clustering algorithm,
R-MCL is unable to identify hierarchical modules. As shown in Figure 10,
R-MCL only produces two clusters matching the two children (blue and red)
modules, while no cluster can match the parent (green) module.

In order to overcome these three issues, we propose a variation, SR—MCL.
The intuition of SR-MCL is to iteratively re—execute R-MCL while ensuring
the clusters produced are not always the same. Thus, the resulting clusters
can be overlapped if clusters produced in all iterations are incorporated. The
clustering produced in each iteration is differentiated by ‘penalizing’ the

(a) ' (b) ‘

-
N v" ~‘~
s o ~
I c ~ .
I ¢ I I
s I Q ,
c I I
o I ‘ ,
I I ‘ ,
’ I I
I l V
I I
I I
I
I ' .
I ' ‘
’ I
I
I
' I
' I
I
I

  
     

.__ ¢ ¢
- - . . . - ‘ ‘ ~ _ -‘

clusters with the hierarchical

highly overlapped
same bridge node(s) modules modules

Fig. 1. Three toy examples pointing out the problems of R—MCL. All edges
have the weight 1 except that the thin edges in (c) have weight 0.5. The
color of the nodes represents the result of R-MCL, and the red/blue numbers
indicate the attractor nodes w.r.t. the red/blue clusters. The dash circles
indicate functional modules

 

i474

112 /810'spaumofplogxo'sopchOJquIq/ﬁd11q IIIOJJ popcolumoq

9IOZ ‘09 lsnﬁnV uo ::

Identifying functional modules in interaction networks

 

 

Algorithm 1 SR-MCL

 

Algorithm 2 post-processing

 

Input: The canonical ﬂow matrix Mg, the balance parameter b, the inﬂation
parameter r, the penalized ratio ,8 and the number of iterations t.
Output: A set of clusters C.
1. C = {}
2. count: {0,0....,0} //An array with n values initialized to zero
3. for iter=1—>t d0

4 repeat

5. MR =Regularizati0nMatrix(M , MG , b)

6 M =M *MR //Regularize operation

7 M =Inﬂate(M , r, count, ,8) //Introducing penalty on attractor
nodes

8. M = Prune(M )

9. until M converges //iter-time execution of R-MCL
10. Tim 2 attractors(M ) //Resulting attractor nodes from iter-time
execution of R-MCL
11. for all v,- 6 Tim d0

12. c0unt[i] = c0unt[i] + 1
13. Cite, = clusters(M ) //Resulting clusters from iter-time execution of
R-MCL

 C = C U Cher
15. C = post-process...(C)

 

attractor nodes, i.e. decreasing the ﬂow to a node that has been an attractor
node in previous iterations. Speciﬁcally, the penalty is introduced as follows:
in the Inﬂate operation, the ﬂow to a node that has been an attractor node
x times in previous iterations is raised to r x ,8", where ,8 >1 is the user-
speciﬁed penalty ratio. Therefore, if a node has not been an attractor node,
the ﬂow to it is still raised to the inﬂation parameter r, and more times
a node has been an attractor node, more severe penalty is introduced to
it. The penalty results in possibly different attractor nodes and therefore
possibly different clusters. Moreover, since the bridge node is likely to be the
attractor in R-MCL, by penalizing attractor nodes, we can correctly produce
clusters sharing the same bridge node. For example, in Figure 1a, R-MCL
only identiﬁes one cluster in the ﬁrst iteration, but in the second iteration,
the attractor node V5 is penalized so SR-MCL could produce two clusters
matching the two modules.

The procedure of SR-MCL is shown in Algorithm 1. Line 4 to Line 9
are the same as R-MCL except introducing the penalty ratio ,8 [the detail of
R-MCL, including the usage of parameters r and b, is illustrated elsewhere
(Satuluri et al., 2010)]. In the Inﬂation operation, each entry Mi]- in the matrix
M is raised to r x ,8 Count[i], and then the sum of each column is re-normalized
to 1. Count[i] is the number of times that v,- has been an attractor node, and t is
the number of times that R-MCL is executed. Since R-MCL is very efﬁcient
(only takes less than 1 s in a modern dual-core machine) in clustering a PPI
network, which typically contains less than 10 000 nodes and 100 000 edges,
and the difference between clusterings produced by each iteration should be
so slight that every possible cluster is produced, we suggest that t is set to
a large number from 10 to 50 and ,8 is set to a relative small number (1.25
in default). Although this setting would result in several redundant clusters,
the post-processing, which will be introduced in the next section, can ﬁlter
out those clusters.

3.3 Post-processing

As the resulting clustering from iterative execution of R-MCL could contain
several redundant and low-quality clusters, those clusters should be removed.
The pseudo code of our post-processing is shown in Algorithm 2. First, we
use one of three simple quality functions, ‘density, clustering coefﬁcient’, and
density multiply by the square root of size (denoted by density x sqrt(size)), to
evaluate the quality of each cluster. The reason of using densityx sqrt(size) is
that a PPI network is usually relatively sparse, so simply adopting density as
the quality function might result in a huge number of too small clusters. Note
that other quality functions, such as those discussed by (Lewis et al., 2010),

 

Input: A set of clusters C 2 {c0, ...ck}, the quality function qf, the quality
threshold 0) and the overlap threshold p.

Output: A set of qualiﬁed clusters C

1. for i=0—>k d0

2. if qf(c,-) < a) or size(c,-) 5 2 then
Remove c,- from C
. Sort C in descending order of qf values
. for i = 0 —> k do
if c,- was not removed from C then

forj=i+1—>k do
if NA(c,-,cj) >2 p then
Remove cj from C

pwsemew

 

can also be applied. Here, we aim to show that a simple quality function can
make SR-MCL produce clusters accurately matching functional modules.
The chosen quality function is denoted by qf . We remove all clusters whose
qf value is below a user-speciﬁed threshold (0. The value of 0) depends on
qf and the network’s property. Furthermore, all clusters whose size is 52
are also removed.

After removing low-quality clusters, we examine whether each cluster
is redundant or not in the descending order of its qf value. A cluster cj is
removed if there exists a cluster c,- that qf(c,-) >= qf(cj) and NA(c,-,cj) > p,
where p is another user-speciﬁed threshold and NA is neighborhood afﬁnity
(Bader and Hogue, 2003):

Ici nle2

NA(Ci,Cj)= - (1)
Ici|* chl

 

Thus, p is used to control the degree of overlap among clusters. The higher
p produces higher overlapped clusters and vice versa. As the functional
modules are highly overlapped, we suggest that p is set from 0.3 to
0.8. If 0) becomes larger and p is decreased, the post-processing removes
more clusters, so the remaining high-quality clusters can precisely match
functional modules, but more functional modules could not be identiﬁed. On
the other hand, less a) and larger p result in more clusters, so the resulting
clusters can identify more functional modules; however, the result contains
relatively redundant and low-quality clusters, so a number of resulting
clusters cannot precisely match functional modules.

4 RESULTS

4.1 Datasets and metrics

We report results on three PPI networks of S. cerevisiae extracted
from DIP (version October 27, 2011) (Salwinski et al., 2004),
BioGRID (version 3.1.81) (Stark et al., 2011) and WI-PHI (Kiemer
et al., 2007), respectively. DIP and BioGRID are unweighted
networks and WI-PHI is a weighted network. The weight of WI-
PHI is adjusted by min-max normalization. All self-loops are
removed, and for BioGRID, we only extracted low-throughput
experiments that were used since these interactions have higher
precision (Paccanaro et al., 2005). In order to evaluate the functional
modules identiﬁcation result, we used GO (Ashburner et al., 2000)
as the gold standard clusters for ground truth validation. GO terms
is a set of hierarchical annotations, in which low-level (general)
terms contain most proteins in a network. As most functional
modules identiﬁcation algorithms produce small clusters which can
be identiﬁed as high-level (speciﬁc) GO terms, we aim to evaluate
the results only based on high-level GO terms. Therefore, for each
network, we only use the GO terms whose information content
(IC) is higher than 2. The information content of a GO term g is
deﬁned as IC(g)= —log(| g | / |root|), where root is the corresponding

 

i475

112 /810'spaumofplogxo'sopchOJquIq/ﬁd11q IIIOJJ popcolumoq

9IOZ ‘09 lsnﬁnV uo ::

K-K.Shih and S.Parthasarathy

 

Table 1. Information of the three yeast networks used in the experiment

 

 

Name IVI  IV E G0| avg(G0) |GO|

BioGRID 4364 25464 3771 10.73 3033
DIP 4995 21875 3822 11.30 3038
WI-PHI 5953 49607 4338 12.19 3262

 

W e G0| is the number of proteins annotated by any GO term we used; avg (GO) is the
average GO term size and |GO| is the number of GO terms.

GO category (biological process, molecular function or cellular
component) of g. We moreover remove GO terms annotating 2 or
less proteins. The detail of these three networks are shown in Table 1.
Except comparing with existing algorithms that can only be applied
on unweighted networks, we mainly show the results on WI—PHI,
since the edge weight is useful for all weighted network clustering
algorithms, including MCL and R—MCL.

We adopt the widely used metric F -measure (Li et al., 2010) to
evaluate the accuracy of clusters produced. F -measure can evaluate
not only the accuracy of the clusters matching functional modules
but also the accuracy of functional modules matching the clusters.
Given a clustering result C : {01 , 02, ..., ck}, in which singleton
clusters are removed and the gold standard clusters (e. g. GO terms)
9 : {g1 , g2, ..., g}, F -measure, based on neighborhood afﬁnity (1),
is the harmonic mean of precision and recall, which are deﬁned as

|{Ci ECIEIgj EQ,NA(Ci,gj)59}|

 

 

precision: ICI (2)
- El - C,NA -, - £9
recall: Hg] Egi Cl 6  (Cl 8])  , (3)

where 9 is set to a typically value 0.25. We moreover propose a
new version of F -measure which does not require the threshold
9. The equations of new precision and new recall are shown in
Supplementary. (The performance of these new metrics is shown in
Supplementary.)

The usage and the suggested range of each parameter are listed in
Supplementary Table 1. Generally, only r, p and a) need to be tuned
and the parameter tuning is straightforward. The parameters (r, b, t,
,8, p) of SR—MCL were set to default values (2.0,0.5,30, 1.25,0.6),
respectively, in all experiments unless otherwise noted.

4.2 The choice of quality function

We compared the three quality functions mentioned in Section 3.3:
density, clustering coefﬁcient and density X sqrt(size). For each
quality function, we varied the quality threshold on in order to
yield different clusterings in various ‘coverages’, which are the
numbers of nodes assigned to any cluster. The result is shown in
Figure 2a. Since a too large coverage yields very small precision,
and a too small coverage yields very small recall, all quality
functions obtain maximal F -measure when the coverage is ~2000
to 3500. It is clear that clustering coefﬁcient produces the worst
result and densityxsqrt(size) produces the best. As mentioned in
Section 3.3, purely using density along has slightly lower F -measure
as it is biased towards small clusters, since the PPI networks are
generally sparse. For simplicity, in the following experiments, we
use densityxsqrt(size) as the quality function.

(a) Different quality functions (b) SoftR-MLC and R-MCL

' —e— sn-mc
—|— R—MCL
[L35 7 + MCL 7

Sis/A“

' ' 0.1 ' ‘ ‘ '
1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000
Coverage Coverage

 

 

F—measure
F-measure
O
a

 

    

+ density
-6- density * sqrt(size)
0.22 ' —E— clustering coeff.

 

 

 

 

 

 

 

 

 

Fig. 2. The comparison of F-measure on WI-PHI

4.3 Comparison with MCL and R-MCL

In this section, we report the beneﬁt of iterative executing R—MCL
with penalty on attractor nodes. We use density X sqrt(size) as
the quality function to prune the results of MCL, R—MCL, and
SR-MCL. The parameters r of MCL and (r, b) of R-MCL are
set to the same values as SR—MCL. Again, we varied on to yield
different clusterings with various coverages. As can be seen in
Figure 2b, SR—MCL always yields signiﬁcantly higher F -measure
than R—MCL, while R—MCL’s F—measure is higher than MCL’s.
Drilling down, we observe that the improvement primarily stems
from corresponding improvement of recall and to a lesser extent
precision (see Supplementary Figs. 1 and S2). This demonstrates
that the clusters produced by SR-MCL can more accurately match
functional modules as attractor nodes are penalized.

4.4 Comparison with state-of-the-art algorithms

We compare SR-MCL with MCL (Dongen, 2000), MCODE (Bader
and Hogue, 2003), RNSC (King et al., 2004), CFinder (Adamcsek
et al., 2006), DPClus (Altaf—Ul—Amin et al., 2006), IPCA (Li et al.,
2008), CORE (Leung et al., 2009), COACH (Wu et al., 2009),
RRW (Macropol et al., 2009), HUNTER (Chin et al., 2010), R-
MCL (Satuluri et al., 2010), SPICi (Jiang and Singh, 2010), Link
Community (LinkCom) (Ahn et al. , 2010) and NWE (Maruyama and
Chihara, 2011). RRW, NWE, HUNTER and SPICi are designed for
weighted networks, so we compared SR-MCL with them on WI—
PHI. LinkCom can be applied on both unweighted and weighted
network, so all three networks are used. The rest of these algorithms
can only be applied on unweighted networks, so they are compared
with SR—MCL on DIP and BioGRID. We tuned each algorithm to its
best parameter setting but generally found that the default parameter
setting generates the best results. The values of each algorithm’s
parameters are reported in Supplementary Table S2. For R—MCL
and MCL, here we show the results without using any quality
function to prune out their results. For SR—MCL, we set p to 0.8
in this experiment, and we choose 0.4 and 1.2 as a) for WI-PHI and
BioGRID/DIP, respectively, since these parameters generally yield
the best F -measure. Finally, we prune out all clusters whose size is
less than or equal to 2 in all algorithms’ clustering results.

The information of all clustering results is reported in Tables 2
and 3. Since other algorithms generally produce smaller clusters than
SR-MCL with above parameter setting, we additionally evaluated
the results based on a set of smaller GO terms GOspec, which
contains GO terms whose information content is higher than 2.5.
Detailed information can be found in Supplementary Table S3. For
GOSpec, we changed the inﬂation parameter r of SR—MCL to 4.5,

 

i476

112 [3.10'811211an[p.IOJXO'SODBIIIJOJIIIOICI/[I(11111 wort popcolumoq

9IOZ ‘09 lsnﬁnV uo ::

Identifying functional modules in interaction networks

 

Table 2. The information of clusters produced by SR-MCL and other weighted network clustering algorithms on WI—PHI. avg(C) is the average size of

 

 

 

 

 

 

 

 

 

clusters
Algorithm RRW NWE HUNTER SPICi LinkCom MCL R-MCL SR-MCL
# clusters 1014 442 46 127 4219 649 897 1828
avg(C) 6.22 5.85 34.80 7.82 4.93 8.33 5.82 9.54
coverage 3523 2134 1370 994 3467 5407 5217 3118
Table 3. The information of clusters produced by SR-MCL and other unweighted network clustering algorithms on BioGRID and DIP
Algorithm CFinder COACH CORE DPClus IPCA MCODE RNSC LinkCom MCL R-MCL SR—MCL
# clusters 816 1248 615 591 1526 81 579 3160 475 559 2611
BioGRID avg(C) 6.57 9.37 7.38 6.54 14.89 4.51 3.71 8.08 7.18 9.59
coverage 2959 2764 2696 2976 2414 1206 2615 3048 3838 4018 3381
# clusters 609 903 772 827 63 549 949 623 848 1038
DIP avg(C) 6.18 8.90 5.30 5.28 19.00 3.89 4.00 6.57 5.25 9.84
coverage 2135 1999 2471 3258 1525 1032 2133 1412 4096 4456 2047
o 4 _ Although MCODE’s precision is higher than other algorithms except
- RRW SR—MCL, its F -measure is lower than most other algorithms. SPICi
e o 3_ - NWE also has very small coverage, but its clusters size is moderate,
: ' - HUNTER . . . . .
g -SPICi resulting 1n very h1gh pre01s1on and low recall, and the F-measure
“E’ 02- - LinkCom is slightly lower than most of other algoirthms. (ii) CORE, DPClus,
u'_ =“Rncn'I-ICL RNSC, MLC and R—MCL all have higher coverage and produce
o_1 - - SR_MCL more and smaller clusters on the sparser network DIP than the
denser network BioGRID. However, since the average size of

 

 

 

WIPHIIGO

WIPHIIGOspec

Fig. 3. F—measure of each weighted network clustering algorithms on
WI-PHI

which results in smaller clusters. For example, in BioGRID, the
average cluster size is reduced from 9.59 to 7.23 if r is increased
from 2.0 to 4.5.

SR—MCL has the highest precision among all algorithms except
SPICi (See Supplementary Figs. 4 and 7), since SPICi produces
less clusters that are only in dense subgraphs. With regard to
recall, among all weighted network clustering algorithms, SR—
MCL is the highest; and among all unweighted network clustering
algorithms, SR—MCL and LinkCom are the highest on BioGRID, but
RNSC is higher than SR—MCL on DIP (see Supplementary Figs. 5
and 9). This is simply because RNSC has much higher coverage
(>4000) than SR-MCL (~2000), and therefore more GO-terms are
identiﬁed. Nevertheless, as shown in Figures 3 and 4, SR—MCL
has the highest F -measure among all existing algorithms on either
unweighted or weighted networks. This is probably because SR-
MCL has high precision due to the original design of R-MCL and the
post-processing, and SR-MCL generally has high recall because it
produces overlapped clusters after executing R—MCL multiple times
with penalizing attractor nodes.

Moreover, we have following observations about other
algorithms: (i) HUNTER and MCODE’s coverage are much smaller
than other algorithms and their clusters are too large to identify
small functional modules, resulting in extremely low (<5%) recall.

gold standard are roughly equal in different networks and high—
quality clusters are rarer in a sparser network, a functional modules
identiﬁcation algorithm should have lower coverage and produce
less and roughly equal-size clusters on a sparser network. As a
result, these algorithms have a relatively (compared with COACH
and IPCA) lower F—measure on DIP than on BioGRID with regular—
size GO terms. (iii) LinkCom clearly outperforms other algorithms
except SR—MCL on BioGRID, but, LinkCom is just above average
on DIP. This is because BioGRID is a denser network than DIP, and
LinkCom can identify highly overlapped clusters in a denser network
due to the line-graph transformation. (iv) RNSC, which is a hard
clustering algorithms, surprisingly has average F -measure among
all unweighted network clustering algorithms. Although RNSC’s
poor performance were previously reported by (Li et al., 2010),
after simply removing clusters containing two or less proteins, it
can still have average performance, so it might be also interesting
to extend RNSC into a soft clustering algorithm.

4.5 Identifying hierarchical annotations

In this section, we demonstrate that SR—MCL is capable of
identifying hierarchical functional modules by showing two top
cases. Each case contains two clusters matching a parent GO term
and its child GO term.

In the ﬁrst example, shown in Figure 5, the large nodes are all
annotated by the child term ‘holo TFIIH complex’ (GO:0005675),
and all of the medium nodes and the large nodes are annotated by
the parent term ‘TFIIK complex’ (GO:0070985). SR—MCL produces
two clusters matching these two terms. In an earlier iteration, a large
cluster roughly matching the parent term with the attractor node

 

i477

112 [3.10'S1120an[plOJXO'SODBIIIJOJIIIOIQ/[i(11111 moxj papeolumoq

9IOZ ‘09 lsnﬁnV uo ::

K-K.Shih and S.Parthasarathy

 

 

 

 

|:l CFinder
0.4 — — - COACH
- CORE
g 03— _ -DPCIus
m - IPCA
8 o 2 _ _ - MCODE
E ' - RNSC
Li. - LinkCom
0.1 — ‘ - MCL
- R—MCL
0 - SR-MCL

BioGRID/GO BioGRID/GOspec

 

 

 

 

DIP/GO DIPIGOspec

Fig. 4. F -measure of each unweighted network clustering algorithms on BioGRID and DIP

 

Fig. 5. Two clusters matching GO:0005675 and its child term GO:0070985
in WI-PHI. The thickness of an edge represents the weight. The size of a
node indicates the GO term annotation: The large nodes are annotated by
GO:0070985 (and thus GO:0005675); the medium nodes are only annotated
by GO:0005 675; and the small nodes are are not annotated by either of these
two terms. The color indicates the clustering result: All nodes form a cluster
matching GO:0005675, and the dark green nodes form a cluster matching
GO:0070985

RAD3 is produced. In a later iteration, after several nodes including
RAD3 being penalized, a small cluster consisting of RAD3, TFB3,
KIN28 and CCL1 is produced, where TFB3, KIN28 and CCL1 are
the only three nodes annotated by the child term in WI—PHI.

The second example is presented in Figure 6. The large nodes are
all annotated by the child term ‘U4/U 6 X U5 tri-snRNP complex’
(GO:0046540), and the medium nodes and the large nodes are
all annotated by the parent term ‘small nuclear ribonucleoprotein
complex’ (GO:0030532). SR-MCL produces two clusters matching
these two terms: The cluster containing the green nodes and the
blue node, named Cluster A, can roughly match the child term
(GO:0046540), and the cluster containing the yellow nodes and the
green nodes, named Cluster B, can roughly match the parent term
(GO:0030532). Cluster A is produced in an earlier iteration with
the attractor node LSM2; in a later iteration, as some other nodes
in Cluster A were penalized in previous iterations, PRP6, which is
roughly the center of nodes annotated by GO:0030532, becomes the
attractor node of Cluster B.

5 CONCLUSIONS

In this article, we proposed a new approach for identifying functional
modules in PPI networks—SR-MCL. We empirically found that
SR-MCL outperforms a range of extant algorithms in terms of its
accuracy in identifying functional modules. As part of future work,
we are interested in auto-tuning some of the parameters of SR-MCL

and adopting SR-MCL for TAP data (Gavin et al., 2006; Krogan
et al., 2006).

ACKNOWLEDGEMENT

We thank V. Satuluri and the anonymous reviewers for helpful
suggestions to improve the presentation of this work.

Funding: This material is based upon work supported by the
National Science Foundation under Grant No. CCF-0702587, IIS-
0917070 and IIS—1141828. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those of the
author(s) and do not necessarily reﬂect the views of the National
Science Foundation.

Conﬂict of Interest: none declared.

REFERENCES

Adamcsek,B. et al. (2006) Cﬁnder: locating cliques and overlapping modules in
biological networks. Bioinformatics, 22, 1021—1023.

Ahn,Y.Y. et al. (2010) Link communities reveal multiscale complexity in networks.
Nature, 466, 761—764.

Altaf-Ul-Amin,M. et al. (2006) Development and implementation of an algorithm for
detection of protein complexes in large interaction networks. BM C Bioinformatics,
7, 207.

Ashbumer,M. et al. (2000) Gene ontology: tool for the uniﬁcation of biology. Nat
Genet, 25, 25—29.

Asur,S. et al. (2007) An ensemble framework for clustering protein—protein interaction
networks. Bioinformatics, 23, i29—i40.

Bader,G and Hogue,C. (2003) An automated method for ﬁnding molecular complexes
in large protein interaction networks. BM C Bioinformatics, 4, 2.

Brohee,S. and Helden,].V. (2006) Evaluation of clustering algorithms for protein—
protein interaction networks. BMC Bioinformatics, 7, 488.

Chin,C.H. et al. (2010) A hub-attachment based method to detect functional
modules from conﬁdence-scored protein interactions and expression proﬁles. BM C
Bioinformatics, 11(Suppl 1), S25.

Dongen,S.V. (2000) Graph clustering by ﬂow simulation. PhD thesis, University of
Utrecht, Netherlands.

Fortunato,S. (2010) Community detection in graphs. Phys. Reports, 486, 75—174.

Friedel,C.C. et al. (2009) Bootstrapping the interactome: unsupervised identiﬁcation of
protein complexes in yeast. J. Comput. Biol., 16, 971—987.

Gavin,A.C. et al. (2006) Proteome survey reveals modularity of the yeast cell machinery.
Nature, 440, 631—636.

Georgii,E. et al. (2009) Enumeration of condition-dependent dense modules in protein
interaction networks. Bioinformatics, 25, 933—940.

Gregory,S. (2009) Finding overlapping communities using disjoint community
detection algorithms. Complex Networks, Springer-Verlag, Berlin, Heidelberg,
47—61.

Jiang,P. and Singh,M. (2010) Spici: a fast clustering algorithm for large biological
networks. Bioinformatics, 26, 1105—1111.

 

i478

1e [3.10'S1120an[plOJXO'SODBIIIJOJIIIOIQ/[i(11111 uroxj papeolumoq

9IOZ ‘09 lsnﬁnV uo ::

Identifying functional modules in interaction networks

 

 

NU1

 

 

 

 

Fig. 6. Two clusters matching GO:0030532 and its child term GO:0046540 in WI-PHI. The thickness of an edge represents the weight. The size of a
node indicates the GO term annotation: The large nodes are annotated by GO:0046540 (and thus GO:0030532); the medium nodes are only annotated by
GO:0030532; and the small nodes are are not annotated by either of these two terms. The color indicates the clustering result: The green nodes are the overlap
of two clusters. The yellow nodes and green nodes form a cluster matching GO:0030532, and the blue node (PAT1) and green nodes form a cluster matching

GO:0046540.

Kiemer,L. et al. (2007) Wiphi: a weighted yeast interactome enriched for direct physical
interactions. Proteomics, 7, 932—943.

King,A.D. et al. (2004) Protein complex prediction Via cost-based clustering.
Bioinformatics, 20, 3013—3020.

Krogan,N.J. et al. (2006) Global landscape of protein complexes in the yeast
Saccharomyces cerevisiae. Nature, 440, 637—643.

Leung,H.C.M. et al. (2009) Predicting protein complexes from ppi data: a core-
attachment approach. J. Comput. Biol., 16, 133—144.

Lewis,A. et al. (2010) The function of communities in protein interaction networks at
multiple scales. BMC Syst. Biol., 4, 100.

Li, M. et al. (2008) Modifying the dpclus algorithm for identifying protein complexes
based on new topological structures. BMC Bioinformatics, 9, 398.

Li,X. et al. (2010) Computational approaches for detecting protein complexes from
protein interaction networks: a survey. BMC Genomics, 11(Suppl 1), S3.

Li,X.L. et al. (2007) Discovering protein complexes in dense reliable neighborhoods of
protein interaction networks. In Computational Systems Bio informatics Conference,
Vol. 6, pp. 157—168.

Luo,F. (2007) Modular organization of protein interaction networks. Bioinformatics,
23, 207.

Macropol,K. (2009) RRW: repeated random walks on genome-scale protein networks
for local cluster discovery. BMC bioinformatics, 10, 283.

Maruyama,0. and Chihara,A. (2011) NWE: node-weighted expansion for protein
complex prediction using random walk distances. Proteome Sci., 9(Suppl 1),
814.

Moschopoulos,C.N. et al. (2008) An enhanced markov clustering method for detecting
protein complexes. In 8th IEEE International Conference on BioInformatics and
BioEngineering (BIBE 2008): 8—10 October, IEEE, 1—6.

Paccanaro,A. et al. (2005) Inferring protein—protein interactions using interaction
network topologies. In IJCNN, Vol.1, pp. 161—166.

PereiraLeal,J.B. et al. (2004) Detection of functional modules from protein interaction
networks. PROTEINS: Struct F unct Bioinform, 54, 49—57.

Salwinski,L. et al. (2004) The database of interacting proteins: 2004 update. Nucleic
Acids Res., 32(Suppl 1), D449—D451.

Satuluri,V. and Parthasarathy,S. (2009) Scalable graph clustering using stochastic
ﬂows: applications to community discovery. In Proceedings of SIGKDD, ACM,
pp. 737—746.

Satuluri,V. et al. (2010) Markov clustering of protein interaction networks with
improved balance and scalability. In Proceedings of the ACM Conference on
Bioinformatics and Computational Biology, ACM, pp. 247—256.

Srihari,S. et al. (2009) Reﬁning markov clustering for protein complex prediction by
incorporating core-attachment structure. Genome Inform. Series, 23, 159—168.
Stark,C. et al. (2011) The biogrid interaction database: 2011 update. Nucleic Acids Res. ,

39(Suppl 1), D698.

Ucar,D. et al. (2006) Improving functional modularity in protein—protein interactions
graphs using hub-induced subgraphs. In PKDD, pp. 371—382.

Vlasblom,J. and Wodak,S. (2009) Markov clustering versus afﬁnity propagation for the
partitioning of protein interaction graphs. BMC Bioinformatics, 10, 99.

Wang,J. et al. (2011) A fast hierarchical clustering algorithm for functional modules
discovery in protein interaction networks. IEEE/ACM Trans. Comput. Biol.
Bioinformatics, 8, 607—620.

Wu,M. et al. (2009) A core-attachment based method to detect protein complexes in
ppi networks. BMC Bioinformatics, 10, 169.

 

i479

1e [3.10'S1120an[plOJXO'SODBIIIJOJIIIOIQ/[i(11111 uroxj papeolumoq

9IOZ ‘09 lsnﬁnV uo ::

