Vol. 28 ISMB 2012, pages i16—i24
doi: 1 0. 1 093/bioinforma tics/b ts220

 

Joint stage recognition and anatomical annotation of drosophila

gene expression patterns

Xiao Cai, Hua Wang, Heng Huang* and Chris Ding
Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, 76019, USA

 

ABSTRACT

Motivation: Staining the mRNA of a gene via in situ hybridization
(ISH) during the development of a Drosophila melanogaster
embryo delivers the detailed spatio-temporal patterns of the gene
expression. Many related biological problems such as the detection
of co-expressed genes, co-regulated genes and transcription factor
binding motifs rely heavily on the analysis of these image patterns.
To provide the text-based pattern searching for facilitating related
biological studies, the images in the Berkeley Drosophila Genome
Project (BDGP) study are annotated with developmental stage term
and anatomical ontology terms manually by domain experts. Due to
the rapid increase in the number of such images and the inevitable
bias annotations by human curators, it is necessary to develop
an automatic method to recognize the developmental stage and
annotate anatomical terms.

Results: In this article, we propose a novel computational model
for jointly stage classification and anatomical terms annotation
of Drosophila gene expression patterns. We propose a novel
iii-Relational Graph (TG) model that comprises the data graph,
anatomical term graph, developmental stage term graph, and
connect them by two additional graphs induced from stage or
annotation label assignments. Upon the TG model, we introduce
a Preferential Random Walk (PRW) method to jointly recognize
developmental stage and annotate anatomical terms by utilizing the
interrelations between two tasks. The experimental results on two
refined BDGP datasets demonstrate that our joint learning method
can achieve superior prediction results on both tasks than the
state-of-the-art methods.

Availability: http://ranger.uta.edu/%7eheng/Drosophila/

Contact: heng@uta.edu

1 INTRODUCTION

The mRNA in situ hybridization (ISH) provides an effective way to
visualize gene expression patterns. The ISH technique can precisely
document the localization of gene expression at the cellular level
via visualizing the probe by colorimetric or ﬂuorescent microscopy
to allow the production of high quality images recording the
spatial location and intensity of the gene expression (Fowlkes
et al., 2008; Hendriks et al., 2006; L’ecuyer et al., 2007; Megason
and Fraser, 2007). Such spatial and temporal characterizations of
expressions paved the way for inferring regulatory networks based
on spatio—temporal dynamics. The raw data produced from such
experiments includes digital images of the Drosophila embryo
(examples are visualized in Fig. 1) showing a particular gene
expression pattern revealed by a gene—speciﬁc probe (Grumbling

 

*To whom correspondence should be addressed.

et al., 2006; Lyne et al., 2007; Tomancak et al., 2002, 2007). The
fruit ﬂy Drosophila melanogaster is one of the most used model
organisms in developmental biology.

Traditionally, such ISH images are analyzed directly by the
inspection of microscope images and available from well—known
databases, such as the Berkeley Drosophila Genome Project (BDGP)
gene expression pattern database (Tomancak et al., 2002, 2007)
and Fly—FISH (L’ecuyer et al., 2007). To facilitate spatio—temporal
Drosophila gene expression pattern studies, researchers needed
to solve two challenging tasks ﬁrst: Drosophila gene expression
pattern stage recognition (temporal descriptions) and anatomical
annotation (spatial descriptions). As shown in Figure l, Drosophila
embryogenesis has been subdivided into 17 embryonic stages. These
stages are deﬁned by prominent features that are distinguishable in
living Drosophila embryos (Weigmann et al., 2003). To recognize
the stages of the Drosophila, embryos provide their time course
patterns. On the other hand, the Drosophila gene expression patterns
are often recorded by controlled vocabularies from the biologist’s
perspective (Tomancak et al., 2002). Such anatomical ontology
terms describe the spatial biological patterns and often cross stages.
What is more, because the ISH images are attached to each other
collectively becoming bags of images, the corresponding stage label
as well as anatomical controlled terms are the descriptions of the
whole group of images instead of each individual image inside
the bag. A Drosophila embryo ISH image bag belongs to only
one stage, but has multiple related anatomical terms. Previously,
those two tasks are tackled by domain experts. However, due to
the rapid increase in the number of such images and the inevitable
bias annotation by human curators, it is necessary to develop an
automatic method to classify the developmental stage and annotate
anatomical structure using controlled vocabulary.

Recently, a lot of research works have been proposed to solve
the above two problems. They considered the stage recognition
as a single—label multi—class classiﬁcation problem while the
anatomical annotation was treated as a multi—label multi—class
classiﬁcation problem. (Kumar et al., 2002) ﬁrst developed an
embryo enclosing algorithm to ﬁnd the embryo outline and
extract the binary expression patterns via adaptive thresholding.
(Peng and Myers, 2004) and (Peng et al., 2007) developed
new ways to represent ISH images based on Gaussian mixture
models, principal component analysis and wavelet functions.
Besides that, they utilized min—redundancy max—relevance to do
the feature selection and automatically classify gene expression
pattern developmental stages. Recently, (Puniyani et al., 2010)
constructed a system (called SPEXZ) and concluded that the
local regression (LR) method taking advantage of the controlled
term—term interactions can get the best enhanced anatomical
controlled term annotation results. The LR method was proposed
by Ji et al. and developed based on their previous works

 

© The Author(s) 2012. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non—Commercial License (http://creativecommons.org/licenses/
by—nc/3.0), which permits unrestricted non—commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.

112 /§JO'SIBUJn0[pJOJXO'SOTlBIHJOJUTOTQ/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

Drosophila gene expression patterns

 

 

 

 

Stage images anatomical terms.
_ Lellua' blasted-emf

:LE‘ a protephalit ectudermal'lage'nsiamr'uasceu

Anterior endoderm eulage in stem nascenti
I Eubbm.

Pagieriur entiutlerm anlege Ill 5.13m name-rim
dnrsa EL‘tﬂdErn'Iﬂril'l'lDFdiul'l'l

7-3 I I'erugu'. enluge

I: ylreolab'um anlage

'FftirLgu'. pril'rmrdlurn
9-10 '- ' H ‘- ‘B - 1" “r I salivary gland body-specific enlage
antene’ endodefm pri’nerdium
neste'im er‘dodErm primordiLIr“:
Lr unit ruesuderrn prirrim Ilium

head niesuderm primal dium P2

 

adli'irur‘ir gland Lindy |J| iIrIu-rdlum
11—12 ' .— antene’midguteremewl-Jm
nested-3r midgut primordial-I1
fr urlk flit-‘ﬁﬂlijli-‘lrll prirrml Ilium

head mesoderm prlmsrdium

hi'ttngt pauper primereium

 

- ' ‘ ' '  Hmhryﬁl‘lirdﬂlhﬂlFFHI‘lE—‘lrr'ili

13-16 ;   :

:5 ‘-

e: nhr-j.I en i i. hea u e p ider-ms

El'llbr'glﬁl'll:'.'E"|1.|'-&|E:||dli‘l"l"l'||5

 

Fig. 1. Examples of Drosophila embryo ISH images and associated
anatomical annotation terms in the stages 4—6, 7—8, 9—10, 11—12 and 13—16
in the BDGP database. The darker stained region highlights the place where
the gene is expressed. The darker color the region has, the higher the gene
expression level is

(Ji et al., 2008, 2010; Li et al., 2009; Shuiwang et al., 2009). All of
the above methods have provided new inspirations and insights
for classifying or annotating Drosophila gene expression patterns
captured by ISH. However, none of them considered doing those two
tasks simultaneously. As we know, intuitively, anatomical controlled
vocabulary terms provide evidence for the stage label and vice versa.
For example, the early stage range is more likely annotated with the
controlled terms such as ‘statu nascendi’ and ‘celluar’ than the terms
‘embryonic’ and ‘epidermis’. Therefore, besides the image—stage
and image—annotation relationships which have been well studied
and applied in the previous research, it is necessary to take advantage
of the correlations between stage classes and annotation terms.

In this article, we propose a novel Tri-Relational Graph (TG)
model that comprises the data graph, anatomical controlled terms
graph, developmental stage label graph to jointly classify the
stage of images and annotate anatomical terms simultaneously.
Upon the TG model, we introduce a Preferential Random
Walk (PRW) method to simultaneously produce image—to—stage,
image—to—annotation, image—to—image, stage—to—image, stage—to—
annotation, stage—to—stage, annotation—to—image, annotation—to—stage
and annotation—to—annotation relevances to jointly learn the salient
patterns among images that are predictive of their stage label
and anatomical annotation terms. Our method achieves superior
developmental stage classiﬁcation performance and anatomical
terms annotation results compared with the state—of—the—art methods.

We consider each image bag as a data point and extract the bag—
of—word features that are widely used in computer vision research as
the corresponding descriptors. Since the real object is 3D and each

image can only provide 2D observation from a certain perspective,
we integrate the bag—of—word features for different views. We
summarize our contributions as follows:

(1) This article is the ﬁrst one to propose a novel solution to
the questions ‘What is the developmental stage?’ and ‘What
are the anatomical annotations’ simultaneously, given an
unlabeled image bag.

(2) Via the new TG model that we constructed, the relationships
between stage label and anatomical controlled terms as well
as the correlations among anatomical terms can be naturally
and explicitly exploited by the graph—based semi—supervised
learning methods.

(3) We propose a new PRW method to seek the hidden
annotation—annotation and annotation—stage relevances.
Other than only using image—to—image relevance conducted
by existing methods, we can directly predict the stage label
and annotate anatomical controlled terms for unknown image
bags.

2 DATA DESCRIPTORS

As we known, the Drosophila embryos are 3D objects. However,
the corresponding image data can only demonstrate 2D information
from a certain view. Since recent study has shown that incorporating
images from different views can improve the classiﬁcation
performance consistently (Ji et al., 2008), we will use the images
taken from multiple views instead of one perspective as the data
descriptor. We only consider the lateral, dorsal and ventral images
in our experiment due to the fact that the number of images taken
from other views is much less than that of the above three views. All
the images from BDGP database have been pre—processed, including
alignment and resizing to 128 X 320 gray images. For the sake of
simplicity, we extract the popular SIFT (Lowe, 2004) features from
the regular patches with the radius as well as the spacing as 16 pixels
(Shuiwang et al., 2009), which is shown in Figure 2. Speciﬁcally,
we extract one SIFT descriptor with 128 dimensions on each patch
and each image is represented by 133 (7 X 19) SIFT descriptors.
Nevertheless, the above SIFT features cannot be directly used to
measure similarity between data points (image bags), because the
number of images in each image bag is different. In order to get a
desired equal length descriptor to release the burden of later learning
task, we need to build codebook for all extracted SIFT features ﬁrst
and then redo the data representations for each image bag based on
the constructed codebook.

2.1 Codebook construction

Usually the codebook is established by conducting the clustering
algorithms on a subset of the local features, and the cluster centers
are then chosen as the visual words of the codebook. In our study,
we use K —means to do the clustering on the training image bags.
Since the result of K —means depends on the initial centers, we repeat
it with 10 random initializations from which the one resulting in
the smallest objective function value is selected. The number of
clusters is set to 1000, 500 and 250 for lateral, dorsal and ventral
images, respectively, according to the total number of images for
each view as shown in Table 1. (Other codebook sizes gave similar
performance.)

 

i1?

112 /810'smumofpmjxosor113u110jurorq//zd11q 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

X. Cai et al.

 

   

         
   
    
  
   
   

"e     £3 .23 $35353 .
anﬁfnqevqnmamaewnﬁﬁ@
‘_. I:  .._. i- I 5...: I! II   2,: J... I r 

    
   

1t 1mg rat a“! Ir.

,1 ." _:_ 1.. _. :I: gat'l'i. ..

x4 it's-1v gin-'34
«an- r E-h “I r' ‘1“:

    
    

 
     

 

  

 _ 5. ..- __  .
r  I. -' F- I
I'I"|:' II x
 ‘zediﬁ- .L  . inane,
'. T' _ - r_. I.- 1‘ _, - . 1‘ '. ..

:_ _ -:_L ..-.I -:,~ 11.1.: _' __ :1},-'___ in . II -_  fl ﬂ :3;
L':' hi ..n-J ': -' i... \" ‘aJL-J-JL -_l 5* -- il

.- . '  » . .. - . “TIFF-'15"

. ~ . {am-ii.

E?

Fig. 2. Demonstration of the regular patches. We extract one SIFT feature
on one patch, where the radius and spacing of the regular patches are set to
16 pixels

Table 1. The statistics summary of the reﬁned BDGP images with 79 terms

 

 

Stage range 4—6 7—8 9—10 11—1 2 13—1 6 Total
Size of control term 11 12 12 20 31 79
N0. of image bags 500 500 500 500 500 2500

No. of lateral images 1514 812 727 1356 1004 5414
No. of dorsal images 226 324 431 447 724 2152
No. of ventral images 164 137 81 214 216 812

 

2.2 Data (image bag) representations

After we get three codebooks, the images in each bag are quantized
separately for each view. Features computed from patches on images
with a certain view are compared with the visual words in the
corresponding codebook and the visual word closest to the feature
in terms of Euclidean distance is utilized to represent it. Therefore,
if an image bag encompasses the images from three views, then
it could be represented by three bags of words, one for each
view. We concatenate the three vectors so that the images with
different views (lateral, dorsal and ventral) in one bag can be
represented by one vector. To be speciﬁc, Let x1 E R1000,xd E R500
and XV E R250 denote the bag—of—words vector for images in a bag
with lateral, dorsal and ventral view, respectively. The descriptor for
this image bag can be represented as x: [x1 ; xd ; xv] e R1750. Since
not all the image bags enclose the images from all three views, the
corresponding bag—of—words representation is a vector of zeroes if a
speciﬁc view is absent. Moreover, in order to capture the variability
of the number of images in each view and each bag, we normalized
the bag—of—words vector to unit length. At last, each image bag is
represented by a normalized vector x.

3 METHODS

In this section, we ﬁrst construct a TG to model Drosophila gene expression
patterns followed by proposing a novel PRW method. Using PRW on TG, we
jointly make stage classiﬁcation and annotate anatomical terms ofDrosophila
gene expression patterns.

For the Drosophila gene expression pattern data, we have n gene
expression images bags X = {x1 , ,xn}, where each image bag is abstracted
as a data point xieRp. Each data point x,- belongs to one of KC stage
classes C={Cl,--- ,cKC} represented by yf e {0, 1}KC, such that yf(k)=l if
x,- is classiﬁed into class ck, and 0 otherwise. Meanwhile, each image
bag x,- is also annotated with a number of anatomical ontology terms
A={a1,m,aKa} represented by y? e{0, 1}Ka, such that yf'(k)=1 if x,- is
annotated with term ak, and 0 otherwise. Also, for convenience, we write

y,- = [yfT,yf’T]T e {0, 1}KC+K0. Without loss of generality, we assume the ﬁrst
i < n image bags are already labeled, which are denoted as T 2 {xi, y,-}ll.:1 . Our
task is to learn a function f :X —> {0, 1}KC+Ka from T that is able to classify an
unlabeled data point xl-(l +1 5 i 5 n) into one stage class in C and to annotate
it with a number of anatomical terms in A at the same time. For simplicity, we
write YC=[y§, ~~,y;], Ya =[y‘1',m,yZ], and Y=[y1, m,yn]. As introduced
in Section 1, the stage class and anatomical terms have some relations. We
utilize the following afﬁnity matrix to model their interrelations, R e RKC xKa,
where R(i, j) indicates how closely class c,- and term aj are related. In this
work, we compute it as

y; > (1)

R(i,j)=cos(yf,y§’)=<yf,y§'>/( y?

 

 

 

 

 

 

 

 

where  is the i—th row of YC and  is the j—th row of Ya. Throughout this
article, we denote a vector as a bold lowercase character and a matrix as an
uppercase character. We denote the i—th entry of a vector v as v(i), and the
entry at the i—th row and j—th column of a matrix M as M (i, j). ||v|| denotes
the Euclidian norm of vector v. And the inner product of two vector v1 and
V2 is deﬁned as < v1,vz >=v1Tvz.

3.1 The construction of TG

Given the dataset X, pairwise similarity WX e Rnx" between data points can
be computed using the Gaussian kernel function,

.._ exp(—“X,-—Xj“2/202, igéj
WX(Z’J)_ : 0, otherwise (2)

where the vector x is calculated using bag—of—word features for one image
bag. Regarding the parameter 0, we resort to self—tuning method (Zelnik—
Manor and Perona, 2004). WX (i , j) indicates how closely x,- and Xj are related.
From WX, a graph 9X =(VX,8X) can be induced, where VX =X and 8X g
VX >< VX. And we use kNN graph. To be speciﬁc, we connect xi,xj if one
of them is among the other’s k nearest neighbor and deﬁne the value of
the edge connecting them by Equation (2). Because QX characterizes the
relationships between data points, it is usually called as data graph, such
as the middle subgraph in Figure 3. Existing graph—based semi—supervised
learning methods (Kang et al., 2006; Zha et al., 2008) only make use of the
data graph, on which the class label information is propagated.

Different from conventional single—label classiﬁcation learning problem in
which classes are mutual exclusive, the anatomical terms are interrelated with
one another. Again, we resort to cosine similarity to calculate the controlled
term afﬁnity matrix WA, where WA(i, j) indicates the correlation between
a,- and aj. Thus, a graph GA 2 (VA,5A) is induced, where VA 2A and 8A g
VA >< VA. We call GA as annotation label subgraph, which is shown as the
right subgraph in Figure 3. Similarly, stage classiﬁcation label graph shown
as the left subgraph in Figure 3, QC =(VC,8C) can be constructed from
stage classiﬁcation labels, where VC 2 C and 8C g VC >< VC, where we deﬁne
the value of the edge connecting two stage labels as WC(i, j) 2 “Sb,- —Sbj| F,
where || H F means Frobenius norm and Sb,- denotes the between class scatter
matrix for stage i. Connecting QX and GA by the annotation associations via
the green dashed lines, connecting QX and QC by the class associations via
the blue dashed lines and connecting QC and GA by the stage—term association
via the purple dashed lines, we construct a TG as following:

 

G=(VXUVCUVAagXUgAUgXCUgXAUgCA)a (3)

which is illustrated in Figure 3. Obviously, the subgraph QAX = (VX , VA , EXA)
connects GX and GA, whose adjacency matrix is Y J. Similarly, the
adjacency matrix of QCX = (VX , VC,5XC) is YCT. The subgraph (Vol/A, SCA)
characterizes the associations between stage classes and anatomical terms
whose adjacency matrix is R deﬁned in Equation (1).

In contrast to existing graph—based semi—supervised learning methods that
only use information conveyed by 9X, we aim to simultaneously classify and
annotate an unlabeled data point using all the information encoded in Q . Since
all data points (gene expression image bags), stage terms and annotation
terms are equally regarded as vertices on 9, our task is to measure the

 

HS

112 /810's112u1n0fp101x0'sor112u1101urorq//:d11q 111011 pop1201um0q

9IOZ ‘09 lsnﬁnv uo ::

Drosophila gene expression patterns

 

Stage 7-8

  

Stage 13-16

Classification label
subgraph

  
 
  

 

 

 

Foregut
anlage

Clypeolabrum I
anlage

 
  
 
     

Cellular
blastoderm

 

 

 

 

 

 

 

 

 

 

. l Annotation label
/// | GA A’EA) subgraph )
—\— .4 _ / — T ————————— — —
/r / EA
‘ ' . EX
// EC
: ' . — . — 
Data _ E
subgraphl — ox
_ _ _ _/ — -? — Unknown

Fig. 3. The TG constructed from the gene expression data. Solid lines indicate afﬁnity between vertices within in a same subgraph, dashed lines indicates

associations between vertices in two different subgraphs

relevance between a class/anntotation term vertex and a data point vertex.
As each class/annotation term has a set of associated training data points,
which convey the same biological record information as the class/annotation
term, we consider both a class/annotation term vertex and its labeled training
image bag vertices as a group set,

Gk=CkU{Xi|yl-(k)=1}a (4)

which is illustrated as the vertices with orange boundary in 3. As a result,
instead of measuring vertex—to—vertex relevance between a class/annotatation
term vertex and an unlabeled data point vertex, we may measure the set—to—
vertex relevance between the group set and the data point. Motivated by Erin
and Page (1998); Tong et al. (2006), we consider to further develop standard
random walk and use its equilibrium probability to measure the relevance
between a group set and an unlabeled data point.

3.2 Preferential random walk

Standard random walk on a graph W can be described as a Markov process
with transition probability M =D‘1W, where (L =ZjW(i, j) is the degree
of vertex i and D=diag(d1, m,d,,). Clearly, MT 75M and ZjM(i,j)=1.
When W is symmetric, it corresponds to an undirected graph. When W
is asymmetric, it corresponds to a directed graph and d,- is the out degree
of vertex i. Let p“) be the distribution of the random walker at time t,
the distribution at t+l is given by p<t+1>(j)=z,p<t>(i)M(i, j). Thus, the
equilibrium (stationary) distribution of the random walk p*=p<t=°°> is
determined by M Tp* = p*. It is well known that the solution is simply given
by p* = We/(Zidi)=d/(Zidi), where (1: [d1, m,dn]T.

It can be seen that the equilibrium distribution of a standard random walk
is solely determined by the graph itself, but independent of the location where
the random walk is initiated. In order to incorporate label information, we
propose the following PRW:

p“+1>(i)=(1—a)Z,p<t>(i)M(i.j)+oh-, (5)

where 0 g a g 1 is a ﬁxed parameter, and h, called preferential distribution,
is a probability distribution such that h(i) 2 0 and Zih(i) = 1. Equation (5)
describes a random walk process in which the random walker hops on

the graph W according to the transition matrix M with probability 1—a,
and meanwhile it takes a preference to go to other vertices speciﬁed by h
with probability a. The equilibrium distribution of PRW in Equation (5) is
determined by p* = (1 —a)MTp* +ah, which leads to:

p*=a[I—(l—a)MT]_1h. (6)

Due to Perron-Frobenius theorem, the maximum eigenvalue of M is less than
maxiZjM (i, j): 1. Thus, I —(1—a)MT is positive deﬁnite and invertible.
Equation (5) takes a similar form to two existing works: random walk with
restart (RWR) method (Tong et al., 2006) and PageRank algorithm (Erin and
Page, 1998). In the former, h is a vector with all entries to be 0 except one
entry to be 1 indicating the vertex where the random walk could be restarted;
while in the latter, h is a constant vector called as damping factor (Erin and
Page, 1998). In contrast, the preferential distribution vector h in Equation (5)
is a generic probability distribution, which is ﬂexible thereby more powerful.
Most importantly, through h we can assess group—to—vertex relevance, while
RWR and PageRank methods measure vertex—to—vertex relevance.

Similar to RWR (Tong et al., 2006), when we set the h to be a probability
distribution in which all the entries are 0 except for those corresponding to
Gk, p*(i) measures how relevant the k—th group is to the i—th vertex on G.

3.3 Preferential random walk on TG

In order to classify and annotate unlabeled data points using the equilibrium
probabilities in Equation (6) of the PRW on TG, we need to construct the
transition matrix M and the preferential distribution h from 9.
Construction of the transition matrix M :

Let

Mx ch MXA
M: MCX MC MCA , (7)
MAX MAC MA

where MX, MC and MA are the intrasubgraph transition matrices of 9X,
QC and GA respectively, and the rest six sub—matrices are the intersubgraph
transition matrices among 9X, QC and QA. Let 61 e[0,1] be the jumping
probability, i.e. the probability that a random walker hops from QX to QC
and vice versa. And let 62 e[0,1] be the jumping probability from QX to

 

HQ

112 /810's112u1nofp101x0'sot112u1101utotq//zd11q 111011 pop1201um0q

9IOZ ‘09 lsnﬁnv uo ::

X. Cai et al.

 

CA or vice versa. Therefore, 181 and ﬁg regulates the reinforcement between
QX and one of the other two subgraphs. When both 181 =0 and ﬁg =0, the
random walk are performed independently on 9X, which is equivalent to
existing graph—based semi—supervised learning methods only using the data
graph QX. Similarly, we deﬁne )t as the jumping probability from QC to CA
or vice versa.

During a random walk process, if the random walker is on a vertex of the
data subgraph which has at least one connection to the label subgraph, such
as vertex x1 in Figure 3, she can hops to the class label or annotation subgraph
with probability 181 or annotation subgraph with probability ﬁg, or stay on
the data subgraph with probability 1— 181 — ﬁg and hop to other vertices of
the data subgraph. If the random walker is on a vertex of the data subgraph
without a connection to the class label or annotation subgraph, she stays on
the data sub graph and hops to other vertices on it as in standard random walk

T
process. To be more precise, let diYC = Z]. YCT(i , j), the transition probability
from x,- to Cj is deﬁned as following:

T ' ' dYCT dYCT 0
p(cjlxi-)=MXC(i,j)= ﬁiYc(z,J)/,- , l. > , (8)
0, otherwise.

Similarly, let diYC = Z]. Yc(i , j), the transition probability from c,- to xj is:

ﬁch(i,j)/d,-Y‘, d,YC>0,

. _ = M . . :
pallet) CXU’J) { 0, otherwise.

(9)

Following the same deﬁnition, the rest four inter—subgraph transition
probability matrices are deﬁned as:

YT . . dYaT . dYaT 0
P(aj|Xi)=MXA(iJ)= (2 0W” i ’ if, i > ’ (10)
0, otherw1se.
.. dYa .dya
p(x-Iai)=MAX(i.j)= Amman/,- , If :- >Ov (11)
J 0, otherwise.

T
where all“ = Z]. YaT(i, j) and a,“ = Z]. Ya(i, j), and

.. ART  dRT, ‘ ctRT 0,
p(cjla.-)=Mie(zu)=: (“V I if» > (12)
0, otherw1se.
I _ _ . . _ AR(i,j)/dl.R, ifdiR>0,
p(ajlci)—Mcn(z.J)—{ 0, otherwise (13)

where ell-RT zszTUJ) and dl-R = ZjRUJ)

Let d? = ZijUJ), d? = erou), d?“ =2,- QC. (13]), c496 =Z,Qc(i,j)
where Q, =R+ Ya and QC =RT + YC.
The data subgraph intra transition probability from x,- to Xj is computed as:

(1—31 —/32>Wr<i,j>/d§‘, if d,” >0

19(yl l) x(l J) { WX(i,j)/diX, otherWISe

(14)

Similarly, let df‘ =Zj WA(i, j), the annotation label subgraph intra
transition probability from a,- to aj is:

. . . QT
.._ --_ 1—ﬂz—A>Wi(z,J)/de,u‘d.a>0
(a la)—M (z, )— ( z i (15)
p J l A J { WA(i,j)/dlf4, otherwise

let diczzj WC(i, j), the classiﬁcation label subgraph intra transition
probability from c,- to Cj is:

. . . QT
_ . . _ 1—,31—}~)WC(Z,J)/d-C,lfd-‘>0
c-c')_M (l, )_ ( z i (16)
p<J|l C J { WC(i,j)/dl.C, otherwise

It can be easily veriﬁed that, ZJ-M (i , j): 1, i.e. M is a stochastic matrix.

Construction of the preferential distribution H: the preferential distribution
vector speciﬁes a group of vertices to which the random walker prefers
to moving in every iteration step. The relevance between this group and
an vertex is measured by the equilibrium distribution of the random
walk process. Therefore, we construct K =KC +Ka preferential distribution
vectors, one for each semantic group Gk:

yhac)
h<k>= (1 331(k) eR’fK (17)

where hgﬁki) = 1 /Z,y, (k) if y, (k) = 1 and hgﬁki) = 0, otherwise; h(ﬁk)(i) = 1,
if i=k, y e[0,1] controls how much the random walker prefers to go to the
data subgraph GA; and other two subgraphs QC, GA. It can be veriﬁed that
Zih(k)(i)= 1, i.e, W is a probability distribution. Let [K be the identity
matrix of size K x K, we write

H=[h(1)’m’h(K)]=[07/1731 l (18)
_ K

PRW on T G: given the TG of a dataset, using the transition matrix M
deﬁned in Equation (7) and the preferential probability matrix H deﬁned in
Equation ( 18), we can perform PRW on the TG. According to Equation (6),
its equilibrium distribution matrix P* is computed as:

P*=0([I—(l—O()MT]_1H, (19)

P* 2 [p7, ~~,p}"{] eR(”+K)XK, and p: is the equilibrium distribution of the
PRW taking the k—th semantic group as preference. Therefore, p:(i) (1+
1 g i gn) measures the relevance between the k—th class and an unlabeled
image bag X). We can predict the stage class from the sub—matrix Pic by
Equation (20) and annotate the controlled terms for x,- using the adaptive
decision boundary method (Wang et al., 2009) on the submatrix Pia by
Equation (21).

P*(l+1,1)  P*(l+1,KC)
13;: 3  3 (20)
P*(n, 1)  P*(n,Kc)

 “‘ 
P211: :  : (21)
P*(n7KC+1) P*(naKc‘l—Ka)

Since the stage prediction is a single—label classiﬁcation problem, we select
the stage label y(i)c* with the maximum probability as the stage label for
image bag xi.
y(i)C* =argmax(13“f),vi‘=l+ 1,l+2,  (22)
k
where  is the i—th row vector of matrix Pic. Therefore, we can do the stage
classiﬁcation and anatomical controlled term annotation simultaneously.

4 DATA REFINEMENT

In this section, we will introduce the details of the data used in
our experiment. Drosophila embryogenesis has 17 stages, which
are divided into 6 major ranges, i.e. stages 1—3, 4—6, 7—8, 9—10,
11—12 and 13—16 (the stage 17 is usually studied individually), in
the BDGP database (Tomancak et al., 2002). Each image bag is
labeled with one stage term and many controlled vocabulary terms.
The total number of anatomical controlled vocabulary terms is 303.

We used the following way to reﬁne the dataset. First, we only
keep the image bag data with lateral, dorsal and ventral view
information. And then, we eliminate six common annotation terms,
that is, ‘no staining, ubiquitous, strong ubiquitous, faint ubiquitous,

 

i20

112 /810's12u1nofp101x0'sol112u1101ulolq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Drosophila gene expression patterns

 

maternal, rapidly degraded’, which can be regarded as outliers
because they can neither provide stage—speciﬁc information nor
record anatomical structures. After that, we remove the anatomical
terms whose data sample is <50. We ignore the stage 1—3 data since
the number of anatomical terms after the above procedure becomes
2, too small to be compared with other stages. And ﬁnally we get 79
anatomical annotation terms in total that we will consider to annotate
the unlabeled image bag.

We reﬁne the data mainly based on the following two reasons.
On one hand, the annotation terms which appear in too few image
bags are statistically too weak to be learned effectively. On the other
hand, since we will use 5—fold cross—validations in our experiments,
we have to guarantee there is at least one data point associated with
each anatomical term in each fold. Moreover, in order to balance the
number of image bags for different stages, we randomly sample 500
image bags as the data points for each stage. At last, the summary
of the reﬁned dataset is shown in Table 1.

5 EXPERIMENT

In this section, we will conduct experiments to evaluate PRW
empirically on the reﬁned dataset and compare it with other
state—of—art classiﬁcation methods. Since our method can do joint
classiﬁcation, in order to evaluate the beneﬁt of joint learning, we
compare its performance with that of the state—of—art multiclass
single label or multiclass multilabel algorithms which can only
handle either stage classiﬁcation or anatomical term annotation
problem. Our procedure is to train our model with stage labeled
and anatomical term annotated image bags. All testing image
bags are unlabeled with developmental stage and unannotated with
anatomical controlled terms.

5.1 Experimental setup

When constructing PRW on TG, we used kNN graph setting k :9.
We used ‘inverse’ 5—fold cross—validation to determine the values
of the following ﬁve parameters, that is, using 1—fold for training
and using the remaining 4—folds for testing to mimic the scenario
in the real application where the number of training data is much
less than the testing data. In our experiment, we found that the
following ﬁve parameters are not sensitive in certain ranges with
good performances. ,81, ,82 and 2 controls the jumping between
different subgraphs and cannot affect the result much if they are
assigned in the range of (01,045). or controls initial preference of
the random walker and will get stable result if it is assigned in the
range of (0,0.1). y controls how much the random walker prefers
to go to the data subgraph or to go to two other subgraphs and it is
usually in the range of (0.1,0.3).

Besides those parameters, we also need to initialize the stage
as well as anatomical controlled terms for the testing image bag
xi, where i=l—l—1,...,n, l is the number of training image bag. In
our experiment, we used k—nearest neighbor (KNN) method to do
the initializations for both stage classiﬁcation and anatomical term
annotations tasks because of its simplicity and clear intuition. To
be speciﬁc, we use k=1 and we abbreviate it as 1NN. Our joint
classiﬁcation framework will self—consistently amend the incorrect
labels for stage and controlled terms. We perform 10 random splits of
the data and report the average performance over the 10 trials. Please

note that, in each trial, we still do ‘inverse’ 5—fold cross validation
and record the average performance result as the result of that trial.

5.2 Image bag stage classiﬁcation

Drosophila gene expression pattern stage categorization is a single—
label multi—class problem. We compare the result of our method with
that of support vector machine (SVM) with radial basis function
(RBF) kernel (Chang and Lin, 2001). We use the optimal parameter
values for C and y got from cross—validation as well. We also
compare the classiﬁcation result of 1NN that we use to do the
initialization. We assess the classiﬁcation in terms of the average
classiﬁcation accuracy and the average confusion matrices. Since the
data that we used is class balanced, the mean value of the entries on
the diagonal of the confusion matrix is also the average classiﬁcation
accuracy. From the resulting average confusion matrices shown in
Figure 5, we can see that the average prediction accuracy of our
method is better than that of the other two state—of—art methods,
especially in the last stage 13—16, where the number of anatomical
terms is greatly larger than that of the other stages.

5.3 Image bag controlled vocabulary terms annotation

Besides the stage classiﬁcation task, we also validate our method
by predicting the anatomical controlled terms for the Drosophila
gene expression patterns, which can be considered as a multi—class
multi—label classiﬁcation problem. The conventional classiﬁcation
performance metrics in statistical learning, precision and F1 score,
are utilized to evaluate the proposed methods. For every anatomical
term, the precision and F1 score are computed following the standard
deﬁnition for the binary classiﬁcation problem. To address the multi—
label scenario, following Tsoumakas and Vlahavas (2007), macro
and micro average of precision and F1 score are used to assess the
overall performance across multiple labels. We compared four state
of art multi—label classiﬁcation methods: local shared subspace (LS)
(Ji et al., 2008), local regression (LR) (Ji et al., 2009), harmonic
function (HF) (Zhu et al., 2003) and random walk (RW) (Zhou
and Schblkopf, 2004). All of them are proposed recently to solve
the multilabel annotation problem. In addition, we compare the
results of 1NN as well. For the ﬁrst three methods we use the
published codes posted on the corresponding author’s websites. And
we implement the RW method following the original work (Zhou
and Schblkopf, 2004). For HF and RW methods, we follow the
original work to solve the multilabel annotation only. Therefore, we
only evaluate those two methods on data subgraph and annotation
label subgraph without using any information derived from the
classiﬁcation label subgraph such as the stage—term correlation.
Table 2 shows the average anatomical annotation performance of
79—term dataset. Compared to the above ﬁve stat—of—the—art methods,
our method has the best results by all metrics. Figure 6 illustrates
the average Micro F1 score of our method, 1NN, LS, LR, RW and
HF approaches for all the anatomical terms on 79—term dataset. And
again, our method consistently achieves best performance for most
of the anatomical controlled terms.

5.4 The advantage of joint learning

Unlike the traditional work, our proposed method can take advantage
of all the information to do the stage classiﬁcation and anatomical
term annotation simultaneously. Therefore, when the number of
training data is scare, we can resort to both intrarelations and

 

i21

112 /810's12u1nofp101x0'sol112u1101ulolq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

X. Cai et al.

 

Anatomical Stage mage _
mms -mr 1mg Terms—Stages Correlation
1"- -.- \ 1.11.15111 his“
’ Nearest Neighbor \ sag-£0?

in Data Subgraph

12.:lorul

“Marl-LE".

Terms 18, 29, 75 all
have high correlation
with stage 13-16

   

Stage
Unknown 18 29 75
Test data

1

  
 
   

‘5

The similarities with
the 1st and 2nd Nearest
Neighbor on the Data
subgraph is not high

Train 5 13 56
data



2nd Nearest Neighbor
\\ in Data Subgraph ’/ m

~-----—-‘ Jﬂﬂalmu 0‘

        
   
 
 
  
  
 
 
 
 
 
 
     
   
   
     

Terms—Terms Correlation

--0.¢

 

 

0

Fig. 4. The middle part demonstrates the tenns—stages correlation and the right part shows the terms—tenns correlation of 79 terms. The stage unknown test
data shown in the left part is classiﬁed correctly as Stage 13—16, because of the strong correlation between the predicted stage and its predicted anatomical
terms and vice versa, NOT the similarity of its ﬁrst and second nearest neighboring data induced from the data graph only

Table 2. Annotation prediction performance comparison
on the 79—tenn dataset

 

 

Method Ma Pre Ma F1 Mi Pre Mi F1

1NN 0.3455 0.3595 0.2318 0.2230
LS 0.5640 0.3778 0.3516 0.1903
LR 0.6049 0.4425 0.3953 0.2243
RW 0.4019 0.3385 0.2808 0.1835
HF 0.3727 0.3296 0.2756 0.1733

Our method 0.6125 0.4434 0.4057 0.2336

 

Ma Pre, Avg. Macro Precision; Ma F 1, Avg. Macro F 1; Mi Pre,
Avg. Micro Precision; Mi F 1, Avg. Micro F 1.

interrelations to make the decision for stage classiﬁcation and
anatomical controlled term annotation simultaneously. When there
are strong correlation between those two tasks, we expect that
the performance of both tasks will be enhanced by joint learning
work than treating them individually and independently. Figure 4
shows the pairwise label correlations of the 79 terms and stage—term
correlations between 5 stages and 79 terms. As highlighted by purple
arrows, we can observe that there are high pairwise correlations
between the terms ‘embryonic brain’,‘ventral nerve cord’ as well
as ‘embryonic/larval muscle system’. Moreover, all the above three
terms have high correlations with the stage 13—16, which can provide
strong evidence that the given testing image bag could belong to
the last developmental stage besides the induction from the data
graph only. If our joint classiﬁcation framework annotates it with all
those three terms, although from the data similarity we cannot get

(a) (b) (c)
4to6 7t08 9t010 lltolZ l3tol6 41:06 7mg 9t010 11t01213tol6 4t06 7t08 9tolO lltolZ 13t016
um um cm 0.0-1 mm _ up: U-EI Ill-DE 0.04: MI MN

     
   

     
    

           

m [1.111 . _ 11.112 0.05
11.113
um um

0.03 0.01 0.00

1NN: acc. 77.40% our: acc. 85.20%

SVM: acc. 84.50%

Fig. 5. Stage classiﬁcation results in terms of confusion matrices on 79—term
dataset: (a) the confusion matrix calculated by SVM (b) the confusion matrix
calculated by 1NN. (c) the confusion matrix calculated by our method. (a)
SVM: acc. 84.50%; (b) 1NN: acc. 77.40%; (0) our: acc. 85.20%

strong evidence for the stage prediction, we can take advantage of
the term—term as well as term—stage high correlations to adjust its
stage to stage 13—16. In other words, relevant anatomical terms could
help us to predict the stage label since they provide the spatial and
temporal information of local structure corresponding to a speciﬁc
embryo development stage. Nevertheless, not all anatomical terms
will deﬁnitely beneﬁt stage classiﬁcation, which is consistent with
our stage classiﬁcation result. From Figure 5, we can see that our
method may have competitive result compared with SVM with
respect to some certain stage. However, given more anatomical
term information, the performance of our method will gradually
outperform the other methods, especially for the prediction result of
stage 13—16.

 

i22

112 /BJO'SIBUJnoprOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Drosophila gene expression patterns

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

°'8 W I I I I lour
0'6 I I I I I ll
0.5 i I I. I
l I ll ill I
0.3 III
0.2'
0.1-
0 . .- .  . .  .. .  .. . . . . . .. ..
_ .. . m... . my “ . .._.d__ = .. . w=...d ..= . a . . . . .. ..
EtﬁgEEEEEEEEE_%5353£E==E£5EE=EEE£>E%3B$E&EwEEEggggEEmw§§§=$EEwE>E¢E§§5§§£gEaEEE
35mm3=h_a;=w=E~EQEOEEMMSEurzugamEgmmha;om.2°=.Euow.E“=.EN‘-’umwmmaautgsﬁ=wwuqu=°m°Og
__ -—~|_- .—u._ L‘- _ ‘_ “ugh-u “'5 __|_-n_=u u_.u ° _ u._ _ ﬁ- _ ma._ ._ u hug—99
3ag5822533?«2%oﬂwﬂSagas3g=2agegwguaga§8n55§m233535225%338§EggsheEaEggaggagwﬁwwE
:13 1300 OuOEO._-=&=._:_n_°u-u._.:n_‘a U Emil!“ amg>ﬂ q; ommﬂ°so°mnuuu uumﬁcO Ommits._ a 33°
I: v: n. In: a... n. .-u: n.-_— _ E Eon: E 5E :::I- 99 goo n._ 1: E
a EE=E> ENENE 3m _ 1,130 uo ._ «In: _ a —_ :3 M = a w Eng-uNE “my who”
._ w ._ = .— _ ._ ._ 0 I. _;_) ° m 0 D. Q, -— I) -— a. g = W > > U N _ m m N M -- -- u -— > ._ -— u q; q; q; _2 h — ._ ._ _ _ a) I! = .—
ﬁﬁégaie ifEE533%ggba3u§§3839gsﬁﬁagiﬁgeagea EEEEEEEEE 3335%83§%;5 iﬁiéeeéiasééa
3:00:33. .EI'TuG’EENmrsﬁﬂ-E': “oruﬂ Emmbw2<~££—°‘5 ENNEL—EEB -:-,-_-:1;,'°¢umﬂ.'a~'° EGCoEguogwa-I-2
- 13131» a mih'ﬁhoﬂ‘: uo °£°°.i:.n-— -° uu=<\u—\\\“'u hw‘” “Vi-om wwwot“ >5 $531! =>=EEEU
“Cccum I-—-n°w‘Uo°-nw 13° 0 1:.2 2'---wu.2--\Uu.2 Il-c-c “Cold- 0.!!- "-—‘“ .:""o°“”w "I
._ ‘5 _ u u EE._ug E-c: ._ :u-—._ U o-— “E “"“9-0 |-—— 9" >> “3960:
5 mm:.— swag-u uz-E ._ Z‘ ._ o: = > I: ._:=: n. '5 ,_ mum on; a... u u > >:
w to o :.2.n-=0=:o o‘”o°° 50°: 0 o o g>m ooo uuu a '4 >>E
m... E 060» A.» c o E z. 00 0° .. 9- amm angmoo “: - 0 —
oo -1: o E 0 r z- r-- are a w u _ —.... .- _ m _
«1.-.- n. a)“, E oE.u at. .n snag a it w 15 on in m n. ,5
—~- 0 n. w .n E u g-n 4: o a in a ”- .:°-°- “ u ..
c ._ .D .1: .9 u > I-
ngg a _>§.-. ES 52 mg aggreesgsge E a $3 a -,-, a 5 3
«In "' "'52 me "u w '90” will” In .5 EE 8 In W > .2
c o q, E ‘1’ o .= > >
It: 1, a) S ‘6.

 

 

Fig. 6. The Avg. Micro Fl score of ﬁve methods on each term in 79—term dataset. (It is better to be viewed in colorful and zoomed in mode.)

5.5 The more meaningful asymmetric correlation
matrix

When we build the TG at ﬁrst, we assume the term—term correlation
and stage—term correlation are both symmetric, since we used
cosine similarity to represent their correlations. However, the above
assumption does not always hold in the real data. In Drosophila
embryo gene expression images, we found that the conditional
probability of the occurrence of term ‘ventral nerve cord’ given term
‘embryonic brain’ is higher than that of the ‘embryonic brain’ given
‘ventral nerve cord’, which satisﬁes the biology meaning that ventral
nerve cord occurs earlier than embryonic brain. After learning, our
method can automatically discover the above hidden asymmetric
correlation information, that is,

P*(n—l—KC—l—1,KC—l—l)  P*(n—l—KC—l—1,K)
P*(n—l—K,KC—l—l) P*(n—l—K,K)

In order to see the learned asymmetric term—term correlation
more clearly, in Figure 7, we show the difference matrix got by
P2}, —P§aT. Taking those more accurate asymmetric correlation
into consideration, our method can potentially improve both stage
classiﬁcation and anatomical annotation results even more.

6 CONCLUSION

In this article, we proposed a novel TG model to learn the
task interrelations between stage recognition and anatomical terms
annotation of Drosophila gene expression patterns. The standard
bag-of—word features and three major views (lateral, dorsal and
ventral) were used to describe the 3D Drosophila images. A new
PRW method was introduced to simultaneously propagate the
stage labels and anatomical controlled terms via TG model. Both
stage classiﬁcation and anatomical controlled term annotation tasks
are jointly completed. We evaluated the proposed method using one
reﬁned BDGP dataset. The experimental results demonstrated in
the real application, when the number of training data is scarce,
our joint learning method can achieve superior prediction results on
both tasks than the state-of—the-art methods. What is more, we can
discovery more accurate asymmetric term—term correlation, which
can potentially improve the results of both tasks even more.

I
--'--Ii'--l
I .- I I.

 

Fig. 7. The learned difference matrix. (It is better to be viewed in colorful and
zoomed in mode.) In order to see the asymmetric entries more clearly, we plot
Pja —P;l"aT. After PRW, the entries marked as brighter square have higher
conditional probability (positive correlation) than its counterpart which is
marked as darker color. This asymmetric reﬂects more accurate term—tenn
correlation than the original symmetric assumption

ACKNOWLEDGEMENT

The author would like to thank Dr Sudhir Kumar for his help in data
collection.

Funding: [This research was supported by National Science
Foundation Grants CCF—0830780, CCF—0917274, DMS—0915228
and IIS—1117965].

REFERENCES

Brin,S. and Page,L. (1998) The anatomy of a large-scale hypertextual web search
engine. In International Conference on World Wide Web (WWW), Elsevier Science
Pubhshers,pp.107—117.

Chang,C. and Lin,C. (2011) LIBSVM : a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2, 1—27.

Fowlkes,C. et al. (2008) A quantitative spatiotemporal atlas of gene expression in the
Drosophila blastoderm. Cell, 133, 364—374.

Grumbling,G. et al. (2006) FlyBase: anatomical data, images and queries. Nucleic Acids
Res., 34, D484—D488.

Hendriks,C. L. et al. (2006) Three dimensional morphology and gene expression in the
Drosophila blastoderm at cellular resolution I: data acquisition pipeline. Genome
Biol, 7, R123.

 

i23

112 /3.IO'SIBUJHOlpJOJXO'SOIlBUIJOJUIOlq/ﬂdnq 111011 pop1201umoq

9IOZ ‘OE lsnﬁnv uo ::

X. Cai et al.

 

Ji,S. et al. (2008) Extracting shared subspace for multi-label classiﬁcation. In ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining
2009, pp. 381—389.

Ji,S. et al. (2009) Drosophila gene expression pattern annotation using sparse
features and term-term interactions. In ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 407—416.

Ji,S. et al. (2010) A shared-subspace learning framework for multi-label classiﬁcation.
ACM Transactions on Knowledge Discovery from Data (TKDD), 4, 1—29.

Kang,F. et al. (2006) Correlated label propagation with application to multi-label
learning. In IEEE International Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 1719—1726.

Kumar,S. et al. (2002) BEST: A novel computational approach for comparing gene
expression patterns from early stages of Drosophila melanogaster development.
Genetics, 162, 2037—2047.

L’ecuyer,E. et al. (2007) Global analysis of mRNA localization reveals a prominent
role in organizing cellular architecture and function. Cell, 131, 174—187.

Li,Y. et al. (2009) Drosophila gene expression pattern annotation through multi-instance
multi-label learning. In Proceedings of the 21st International Joint Conference on
Artiﬁcial Intelligence, AAAI press, pp. 1445—1450.

Lowe,D. (2004) Distinctive image features from scale-invariant keypoints. Int.
J. Comput. st., 60, 91—110.

Lyne,R. et al. (2007) FlyMine: an integrated database for Drosophila and anopheles
genomics. Genome Biol, 8, R129.

Megason,S. and Fraser,S. (2007) Imaging in systems biology. Cell, 130, 784—795.

Peng,H. and Myers,E.W. (2004) Comparing in situ mRNA expression patterns of
drosophila embryos. In International Conference on Research in Computational
Molecular Biology (RECOMB), ACM, pp. 157—166.

Peng,H. et al. (2007) Automatic image analysis for gene expression patterns of ﬂy
embryos. BMC Cell Biol, 8(Suppl. 1), S7.

Puniyani,K. et al. (2010) SPEXZ: Automated Concise Extraction of Spatial Gene
Expression Patterns from Fly Embryo ISH Images. Intell. Sys. Mol Biol, 26,
147—156.

ShuiwangJ. et al. (2009) A bag-of—words approach for Drosophila gene expression
pattern annotation. BMC Bioinformatics, 10, 119.

Tomancak,P. et al. (2002) Systematic determination of patterns of gene expression
during Drosophila embryogenesis. Genome Biol, 3, 88.

Tomancak,P. et al. (2007) Global analysis of patterns of gene expression during
Drosophila embryogenesis. Genome Biol, 8, R145.

Tong,H. et al. (2006) Fast random walk with restart and its applications. In IEEE
International Conference on Data Mining (ICDM), pp. 613—622.

Tsoumakas,G. and Vlahavas,I.P. (2007) Random k-labelsets: An ensemble method for
multilabel classiﬁcation. In European conference on Machine Learning, Springer-
Verlag, pp. 406—417.

Wang,H. et al. (2009) Image annotation using multi-label correlated Green’s function.
In IEEE International Conference on Computer Vision, pp. 2029—2034.

Weigmann,K. et al. (2003) FlyMove — a new way to look at development of Drosophila.
Trends Genet. 19, 310—311.

Zelnik-Manor,L. and Perona,P. (2004) Self-tuning spectral clustering. Advances in
neural information processing systems, 17, 16.

Zha,Z. et al. (2008) Graph-based semi-supervised learning with multi-label.
In IEEE International Conference on Multimedia and Expo (ICME),
pp. 1321—1324.

Zhou,D. and Scholkopf,B. (2004) Learning from labeled and unlabeled data using
random walks. In Annual Symposium of the German Association for Pattern
Recognition (DAGM), Springer, pp. 237—244.

Zhu,X. et al. (2003) Semi-supervised learning using gaussian ﬁelds and harmonic
functions. In International Conference on Machine Learning (ICML), ACM press,
pp. 912—919.

 

i24

112 /B.IO'S{BU.IHO[p.IOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

