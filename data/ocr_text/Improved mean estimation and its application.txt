ORIGINAL PAPER

doi: 1 0. 1 093/bioinforma tics/btr690

 

Gene expression

Advance Access publication December 14, 2011

Improved mean estimation and its application to diagonal

discriminant analysis

Tiejun Tong 3*, Liang Chen 2 and Hongyu Zhao 3’4

1Department of Mathematics, Hong Kong Baptist University, Kowloon Tong, Hong Kong, 2Department of Biological
Sciences, University of Southern California, Los Angeles, CA 90089, 3Department of Epidemiology and Public Health
and 4Department of Genetics, Yale University School of Medicine, New Haven, CT 06520, USA

Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: High-dimensional data such as microarrays have
created new challenges to traditional statistical methods. One such
example is on class prediction with high-dimension, low-sample size
data. Due to the small sample size, the sample mean estimates
are usually unreliable. As a consequence, the performance of the
class prediction methods using the sample mean may also be
unsatisfactory. To obtain more accurate estimation of parameters
some statistical methods, such as regularizations through shrinkage,
are often desired.

Results: In this article, we investigate the family of shrinkage
estimators for the mean value under the quadratic loss function.
The optimal shrinkage parameter is proposed under the scenario
when the sample size is fixed and the dimension is large. We then
construct a shrinkage-based diagonal discriminant rule by replacing
the sample mean by the proposed shrinkage mean. Finally, we
demonstrate via simulation studies and real data analysis that the
proposed shrinkage-based rule outperforms its original competitor
in a wide range of settings.

Contact: tongt@hkbu.edu.hk

Received on August 24, 2011; revised on November 24, 2011;
accepted on December 8, 2011

1 INTRODUCTION

With the advent of high—throughput technologies, learning high—
dimensional complex models is critical in many disciplines such
as biology, genetics, epidemiology, geology, ecology, neurology
and engineering. One such example is microarray data, where the
expression levels of thousands of genes are measured simultaneously
from each sample. Due to the cost and/or other experimental
difﬁculties such as the availabilities of biological materials, it is
common that high—throughput data are collected only in a limited
number of samples. They are referred to as high—dimensional
data with small sample size, or ‘large G small 11’ data, where
G is the number of dimensions and n is the sample size. High—
dimensional data pose many challenges to traditional statistics
methods. Speciﬁcally, due to the small n, there are more uncertainties
associated with standard estimations of parameters such as the mean
and variance estimations. As a consequence, statistical analyses
based on such parameter estimation are usually unreliable. To obtain

 

*To whom correspondence should be addressed.

more accurate estimation of parameters some statistical methods,
such as regularizations through shrinkage, may yield better results.

Shrinkage—based methods have been proposed in recent years to
improve the variance estimation for ‘large G small 11’ data. See
for example, Baldi and Long (2001), Storey and Tibshirani (2003),
Wright and Simon (2003), Smyth (2004), Cui et al. (2005), Tong
and Wang (2007), Opgen—Rhein and Strimmer (2007) and Wang
et al. (2009), among many others. In contrast to the advances on
variance estimation, little attention has been paid to improving the
mean estimation for high—dimensional data until recently (Hausser
and Strimmer, 2009; Hwang and Liu, 2010). In this article, we
investigate the family of shrinkage estimators for the mean value
which is tailored to the high—dimensional data such as microarrays.
Speciﬁcally, we will propose the optimal shrinkage parameter under
the quadratic loss function for the data when G tends to be inﬁnite.

Class prediction with high—dimensional data has been recognized
as a very important problem and received much attention in different
ﬁelds such as genomics, proteomics, brain images, medicine and
machine learning. For high—dimensional data with small sample
sizes, it is known that the traditional classiﬁcation methods such
as the linear discriminant analysis are not applicable as the sample
covariance is going to singular when G is greater than n. To
overcome the singularity problem, Dudoit et al. (2002) introduced
two diagonalized discriminant rules, the diagonal linear discriminant
analysis (DLDA) and the diagonal quadratic discriminant analysis
(DQDA). When the sample size is small, DLDA performed
remarkably well compared with more sophisticated classiﬁers in
terms of both accuracy and stability (Dettling, 2004; Lee et al.,
2005). In addition, DLDA is easy to implement and is not sensitive
to the number of predictor variables.

Though DLDA performed well for high—dimensional small
sample size data, there is still room to improve it (Huang et al., 2010;
Pang et al., 2009; Pang et al., 2010). In particular, we notice that
the mean estimation (the sample mean) in DLDA will be unreliable
when the sample size is not sufﬁciently large. As a consequence,
the performance of DLDA may also be unsatisfactory. With this
insight, we propose in this article an improved version of DLDA,
which replaces the sample mean by the optimal shrinkage estimator.
We expect that the proposed shrinkage—based DLDA will improve
the classiﬁcation accuracy in practice.

The remainder of the article is organized as follows. In Section 2,
we investigate the family of shrinkage estimators for the mean
value under the quadratic loss function. With the nature of high—
dimensional data, we assume that the variances are unequal and
unknown. Under regularity conditions, we discuss the choices of the

 

© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 531

Vol. 28 no. 4 2012, pages 531-537

112 /§JO'SIBUJn0[pJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘091snﬁnv uo ::

ITong et aI.

 

shrinkage parameter and then evaluate their practical performance
via numerical studies. In Section 3, we construct a shrinkage—based
diagonal discriminant rule by replacing the sample mean by the
proposed shrinkage mean. Simulation studies will also be conducted
to evaluate its performance over its original competitor. In Section 4,
we use the leukemia data to demonstrate that the proposed method
is widely applicable and performs well. Finally, we conclude the
article in Section 5 with a brief discussion.

2 IMPROVED MEAN ESTIMATION

Let X j 2 (X1 j, . . . ,XGj)T, j: 1, . . . , n, be independent G—dimensional
vectors normally distributed with mean pt =(M1, ...,Mg)T and
covariance matrix 2. Let X: Zf=1Xj/n be the sample mean. Let

— — T— — — T — . . .
||X||2 =X X and ||X||§J =X E‘lX for any invertible matrix 2. In
this section, we consider estimating pt with respect to the quadratic
loss function.

2.1 Motivation

Shrinkage estimation of means has a long history starting with the
seminal paper of James and Stein (1961) that the sample mean is
inadmissible for G 2 3. When 2 :1, James and Stein (1961) showed

that
A (G—2)/n) _
llel2

dominates X for any G2 3. When 2:02] with 02 unknown, the
J ames—Stein type estimator is given as (Baranchik, 1970)

- (G—zvn -
= 1_—_ X,
M ( “XIV/83)

where 83 is the pooled estimator of 02. More generally, when E is
non—diagonal and unknown, the J ames—Stein type estimator has the
form (Lin and Tsai, 1973; Gleser, 1986)

- 1 (G—zvn -
ILJS: __—
IIXII2i

7

where 2: 3:10;,- —X)(Xj —X)T/(n—G+2) is the pooled
sample covariance matrix. To guarantee 2) non—singular, we require
that n > G. Note that this is not the case for high—dimensional data
where G can be much larger than n. Therefore, the existing shrinkage
methods for estimating pt break down and cannot apply to the
high—dimensional data directly.

2.2 Proposed mean estimator

To overcome the singularity problem, we assume 2 is diagonal and
denote it by D=diag(012,...,og;). As mentioned in Hwang et al.
(2009), most existing shrinkage estimators of pt in the literature
required the variances of, i: 1, ..., G, to be either equal or unequal
but known. When at? = 02 for all i, the problem reduces to D :02]
with 02 unknown (James and Stein, 1961; Montazeri et al., 2010).
When the at? are unequal but known, the problem reduces to the case
considered in Efron and Morris (1973). In this article, we consider
the assumption of at? being both unequal and unknown. Note that the
same assumption has been commonly used in the recent literature,

e.g. in Berger and Bock (1976), Dudoit et al. (2002), Bickel and
Levina (2004), Cui et al. (2005), Tong and Wang (2007) and Hwang
and Liu (2010) where the correlations among genes are ignored due
to the small sample size.

Consider the following hierarchical Bayesian model with
conjugate priors,

21.1.(1. 2
leIMiaO'i N NUMan )7
Milaiz ~N(0,a,-2/ro),
0,2 ~Inv—X2(v0,o(2)),

where i=1,...,G, j=1,...,n, N(-) is the standard normal
distribution, and Inv—X2(-) is the scaled inverse chi—squared
distribution with unknown hyperparameters (:0, 120,03). The joint

prior density of ,ul- and a? is given as

_ 1
f(Mi,Ui2)0<(Ul-2) (”0+3)/26Xp{—ﬁ(v00§+T0/L3)}-
0'.
I
Let x,- =Z§l=1x,;,-/n and s3 = f=1(x,-j —x,-)2/(n— 1) be the
sample mean and the sample variance for gene i, respectively. Let
X=(x1, ...,xG)T and S=diag(s%, . . . , sé). By Gelman et al. (2004),
the posterior distribution of (Ml-,0?) can be represented as
2 0'2 2 2
l
f(/1«i,0l- lXi1,...,Xin)=N(Mi,p, ﬁllnv-X (V0+n,0i,p)

 

 

where
n _ 5
Ml,p— t0+n l7
1 nt _
2 2 2 0 2

The posterior mean estimator of ,ul- is then

I
A-: - = 1—— X', '21,...,G. 1
M1 M1,}? ( n/t0+1) l l ( )

Note that ‘50 in (1) is an unknown parameter. We propose to
estimate n/to—l—l by the form of ||X||§/r, where r is a shrinkage
parameter to be tuned. We then have the following estimator for pt,

r _
A = 1—— X. 2
Mr) ( HXHE) ()

Under the quadratic loss function
A n A
L(u,u,D)=5IIu—uu%), (3)

Fourdrinier et al. (2003) showed that the estimator (2) dominates X
when 0 < r < 2(G—2)/(n—l— 1).

2.3 Optimal shrinkage parameter

In this section, we propose the optimal shrinkage parameter of r > 0
within the family of estimator (2). In Appendix A, we show that the
optimal shrinkage parameter is given as
_ - 2
 (G _2>f<1/ngs> . (4)
nE[IIXIID/(IIXHS)2]

 

 

532

112 /§JO'SIBUJn0[pJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘091snﬁnv uo ::

Improved mean estimation

 

Note that for high—dimensional data, n is usually small but G is large.
We have the follow asymptotic result.

LEMMA 1. Deﬁne Nm=#{k:(ak/Zf=1ai)2 l/m}, where ai=
XiZ/ol.2 and #{A} denotes the total number of elements in set A.

Assume that supm(Nm / m) < 00 almost surely. Then for any ﬁxed
2 as.

n24, we have ||X||%)/||X||S —> (n—3)/(n— 1) as G—> 00.

The proof of Lemma 1 is given in Appendix B. By Lemma 1,
for high—dimensional data, we have fopt N [(n — 1)(G — 2)] / [n(n — 3)]
and the optimal shrinkage estimator is

 

A A f‘opt _
p. r = 1— _ X. (5)
(C’pt) ( IIXIIE)

Let f:(G—2)/(n—l—1) be the middle point of the range 0<r<
2(G—2)/(n—l— 1) in Fourdrinier et al. (2003). It is clear that when
n is large, iopt and f are asymptotically equivalent. While for high—
dimensional data with small sample sizes, the difference between
these two estimators can be large. For instance, when n=5, the
ratio iopt/f is given as 2.4; and when n=6, the ratio is 1.9. The
practical performance of these two estimators will be studied in the
next section.

2.4 Evaluation

The ﬁrst simulation is to evaluate the performance of mop.) with
the estimators X, [1 JS and 11(2). Assume that G: 100. We consider
four different values of n, ranging from 5, 10, 20 to 50, to represent
different levels of sample sizes. We draw a? from the scaled chi—
square distribution xi_1/(n— 1), and ,ul- from N (0, ‘52), where t:
0.2, 0.6 or 1 representing different levels of mean heterogeneity. For
each gene i, we simulate n observations from the normal distribution
N (Ml-,0?) We repeat the process 5000 times for each setting and

report the simulated average risk AR=Zg2010 L(11,pt,D) /5000 in
Table 1 for the four estimators, respectively.

Under the quadratic loss function (3), the sample mean X has
a constant risk at 1, as reported in the simulations. The standard
errors of these average risks are all around 0.14 / \/ 500020.0020.
Therefore, the improvements of the three shrinkage estimators
over X are all statistically signiﬁcant. We observe that 11(iopt)
has a smaller average risk than both [1 JS and [1(2) in most
settings, especially when the sample size is small. In addition,
the improvement of .100“) over X increases when the mean
heterogeneity decreases, especially when ‘E is near zero. We also
observe that the improvements of the shrinkage estimators over the
sample mean become smaller when n becomes larger. This indicates
that for the large sample size scenario, it is no longer necessary
to borrow information from other genes to improving the mean
estimation.

Note that we have assumed the grand mean to be zero in the above
simulation. Note that when the grand mean has a shift from zero,
the term  ,ul2 /ol.2 will tend to be larger so that the improvement
of 11(iopt) over X will be diminished correspondingly. In such
situations, Lindley (1962) suggested to apply the shrinkage method
to the deviations le —X.. rather than to the original observations
Xij, which leads to the following variation of the estimator (5),

°Pt ||X—X..||S

Table 1. Average risks of the estimators under various settings

 

 

r n=5 n=10 n=20 n=50
0.2 X 0.997 1.001 0.996 1.000
[115 0.337 0.362 0.485 0.682
1107) 0.514 0.408 0.493 0.683
[1030“) 0.339 0.359 0.483 0.682
0.6 X 1.004 1.001 1.003 0.999
[115 0.889 0.836 0.897 0.951
[1(7) 0.851 0.840 0.898 0.951
[1030“) 0.801 0.827 0.895 0.951
1 X 0.998 0.997 0.998 1.001
[115 0.973 0.932 0.957 0.984
[1(7) 0.933 0.932 0.957 0.984
11030“) 0.912 0.927 0.956 0.983

 

Table 2. Average risks of the estimators (5) and (6) under various settings

 

 

11.0 n=5 n=lO n=20 n=50
0 [1030“) 0.732 0.770 0.849 0.926
[1030“) 0.736 0.772 0.850 0.926
1 [1030“) 0.928 0.940 0.966 0.982
[1030“) 0.737 0.771 0.852 0.928
2 [1030“) 0.977 0.982 0.993 0.993
[1030“) 0.738 0.773 0.853 0.928

 

where X.. =  Zf=1Xij/(nG) is the grand sample mean across

all the genes, and X..=(X..,...,X..)T is a vector of size G with
common values.

To evaluate the performance of the Lindley—type estimator (6),
we simulate ,ul- from N (120,052) for three different values of ,uO:
0, 1 and 2. All other settings remain the same as before. We repeat
5000 simulations for each setting and report the average risks of both
11(iopt) and 11(iopt) in Table 2. We observe that the two shrinkage
estimators perform similarly when ,uo :0. When ,uo is away from
zero, 11(iopt) may perform unsatisfactory while the performance
of 11(iopt) remains the same. This indicates that the Lindley—type
estimator is robust to the shift of the grand mean. In the remainder
of the article, the estimator (6) will be adopted to estimate the mean
value unless otherwise speciﬁed.

3 IMPROVED DIAGONAL DISCRIMINANT
ANALYSIS

As mentioned in Section 1, class prediction with high—dimensional
data is an important problem in high dimensional data analysis. The
objective of class prediction is to assign a new observation to one
of the K classes based on its given proﬁle. For ease of notation, we
deﬁne the class labels to be integers ranging from 1 to K with nk
observations belonging to class k,

i.i.d.
Xk,1,...,Xk,nk ~ MVNg(uk,Ek), k=1,...,K,

where ptk and 22k are the mean value and covariance matrix
of the G—dimensional multivariate normal distribution for class

 

533

112 /B.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201um0q

9IOZ ‘091sn3nv uo ::

ITong et al.

 

k, respectively. Let N =n1 —I— - - - —I—nK be the total number of
observations.

For high—dimensional data such as microarrays, it is common that
the dimension is much larger than the sample size. As a consequence,
traditional classiﬁcation methods such as the linear discriminant
analysis are likely to be inapplicable to such high—dimensional data
directly. For instance, the sample covariance matrix is singular when
G is larger than N. To overcome the singularity problem, Dudoit et al.
(2002) introduced DLDA and DQDA that ignored the correlations
among genes. Speciﬁcally, under the assumption that 22k = E for
all k, DLDA classiﬁes a new observation y =(y1, ...,yg) to class k
which minimizes the following discriminant score

Elke) = tv—XnTIdiagén—ltv—Xk)—2logﬁk (7)

G
=Zui—anz/8E—2logﬁk, k=1,...,K,

i=1
where Xk=Z;;1Xk,j/nk=(Xk1,...,Xk(;) is the sample mean
of class k, E=Z§=l(nk—1)Ek/(N—K)=diag(8%,...,8é) is

the pooled sample covariance matrix with ik=Z;;1(Xk,j—

Xk)(Xk,j —Xk)T/(nk — 1), and ftk =nk/N are the prior probabilities
of which the next new observation is coming from class k. DLDA is
also called a ‘naive Bayes’ classiﬁer as it arises in a Bayesian setting
(Bickel and Levina, 2004). It has been widely adopted to analyze
high—dimensional data in the real sciences. See for example Speed
(2003), Noushath et al. (2006), Asyali et al. (2006) and Heilemann
and Schuhr (2008), among others.

Note that DLDA uses the sample mean. We propose here a
modiﬁed version of DLDA by replacing the sample mean X k in
formula (7) by the proposed optimal shrinkage estimates [1k(l\‘0pt)
for each class k. We then classify a new observation y to class
argminkgllg (y), where

Elie)=tv—I1k(ropt))TIdiag(i)I—1tv—ak(?opt)>—2logﬁk. (8)

We refer to it as shrinkage—mean—based DLDA (SmDLDA).
Similarly, we can propose a shrinkage—mean—based version for
DQDA as well. The behavior of the proposed SmDLDA will be
studied in the next section under various scenarios.

3.1 Simulated study

In this section, we conduct simulations to compare the performance
of SmDLDA with DLDA. We consider a binary classiﬁcation with
simulate data from multivariate normal distributions MVN(pt1, E)
and MVN(|L2, 2), respectively.

The ﬁrst simulation study considers an identity covariance matrix,
i.e. 2 :10. To differentiate the two classes, without loss of generality
we assume the ﬁrst d, OfdfG, components of [L1 and 112 are
the same and the left ones are different. Speciﬁcally, we let “.12
{0,---,0,M1,d+1,---,M1G} and ILZZ—Ikla where {M1,d+1,---,M1G}
is a random sample of size G—d from the uniform distribution
U (0, 0.5). We consider two choices of G (50 and 200), and for each
G, we consider six different d values at 0, 0.1 X G, ..., 0.5 X G,
respectively. For each simulation, we generate a training set of size
nk for each class k under the simulation setting described above,
and a test set of size 5nk under the same setting to assess the
misclassiﬁcation rate. The overall misclassiﬁcation rate is calculated
by the percentage of misclas siﬁed observations over the total number

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

G=50, n=10 G=50, n=20

8-

° ——- DLDA ’0 — ——- DLDA ,o
9 — SmDLDA ,/ o 9 N — SmDLDA ,' 0
6:“ :9_ ,o/ 6:“ o'— 0'
5° X/ o 5 — z’ 0
“a, o/ “a, l/

.2 ,’ .2 oo .0
<7) 0 _o’ O (7) o-— ,’ 0
c3 5‘ x / go o/
0 o’ o _ 2’ 0
<2 ,2 <2 2
E o’ o/ E v 0’ , I g/
_ /
3—0/ 3 0
O
l l l I I I I I I I I I
O 5 1O 15 20 25 O 5 1O 15 20 25
d d
G=200, n=10 G=200, n=20

LO

Q—

0 --- DLDA _ --- DLDA
9 g_ — SmDLDA O 9 a — SmDLDA
(U o- / (U 0—
II I/ II 0'
.5 ‘8.— ,' .5 _
.8 o e’ o .8 1’
2": l 3": O /
2% / / £5.— ,'
g o” o g o o,
S 5— ’ S — ,"o

0 g , o O/ ’ 

__ r o /
0 I I I I I o' I I I I I I
20 4O 60 80 100 20 4O 60 80 100
d d

Fig. 1. Plots of the average misclassiﬁcation rates for DLDA and SmDLDA
when the observations are independent.

of observations in the test sets. Finally, we repeat the procedure 1000
times and report the average misclassiﬁcation rates in Figure 1 for
n1 =n2 = 10 and 20, respectively. It is evident that SmDLDA has a
smaller misclassiﬁcation rate than DLDA in all settings.

To evaluate and compare the performance of DLDA and
SmDLDA under more realistic situations, we conduct here another
simulation study in the case where the observations are correlated.
Similarly as in Guo et al. (2007) and Pang et al. (2009), we use
the following block diagonal structure to mimic the true covariance
matrix,

2,, 0
0 z_,, 0 .
E: . . ,
- 0 2,, 0 -
' 0 z_,, E
  .. GxG

where each diagonal block has the following auto—regressive
structure

1p..p9
p1--p8
.9 .
p"'p110x10

We consider ﬁve different values of the correlation coefﬁcient ,0,
ranging between 0, 0.2, 0.4, 0.6 and 0.8, to represent different
levels of dependence. Note that ,0 = 0 corresponds to the independent
situation in the previous simulation. All other settings are the same
as before except that we set d=0.1>< G in this new simulation.
We repeat the procedure 1000 times and report the average

 

534

112 /B.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201um0q

9IOZ ‘091sn3nv uo ::

Improved mean estimation

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

G=50, n=10 G=50, n=20
8— e
o' — — - DLDA o" - — - DLDA 9
a, — SmDLDA a, _ — SmDLDA
8 8
c 59_ c .9-
.9 o' .g o'
.8 ,2 .8 _ °
(_t5 0 , '0 cc 8- / ’
.8 5‘ o2 ’ I o E d » ’ 8
E 0 ‘ ’ ' " / E , o/
o_ _
o/O <- 0/0
Q—
| I I I I o I I I I I
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
rho rho
G=200, n=10 G=200, n=20
— - - - DLDA §_ - — - DLDA
a, g — SmDLDA o a, o- — SmDLDA
I? C5 I, I516 — o
C l’ C a ’
5:) — I O 5:) Q—
.8 ,’ .8 °
’5 N I ’5 —
<3 3_ ’0’ (:3 ,9
.8 , I g g—
2 — / ’ O E
, o _ ,
o o____o-'/o/ o ———8/
q_ o o 8_ o — - — -o=’/
0 I I I I I o' I I I I I
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
rho rho

Fig. 2. Plots of the average misclassiﬁcation rates for DLDA and SmDLDA
when the observations are correlated.

misclassiﬁcation rates in Figure 2 for both DLDA and SmDLDA.
Once again, SmDLDA outperforms DLDA in most situations. The
comparison result is more evident when the correlation coefﬁcient
,0 is not large.

Though we have restricted to a balanced binary classiﬁcation with
n1 2 n2 due to the page limitation, extensive simulations (not shown)
indicate that the above comparative conclusions remain the same for
unbalanced designs as well as for other simulation settings, including
the multiclass comparison problems.

4 APPLICATION TO LEUKEMIA DATA

In this section, we apply the proposed discriminant rule, SmDLDA,
to the leukemia data of Golub et al. (1999). The dataset is
available in the website of www.bioconductor.org. By following
the same pre—processing steps (thresholding, ﬁltering and logarithm
transformation) as described in Dudoit et al. (2002), we end up
with a gene expression dataset with a total of 3571 genes for 47
acute lymphoblastic leukemia (ALL) patients and 25 acute myeloid
leukemia (AML) patients. We further standardize the dataset so that
each array has mean zero and variance one across genes.

Note that in general, the shrinkage methods work well for dense
data rather than sparse data. Thus, to better reveal the practical
performance of SmDLDA we perform a preliminary screen of
informative features before the case study. Different screen methods
are available in the literature so as to identify biologically signiﬁcant
gene functional groups or pathways via Gene Ontology annotations
(Pan, 2006; Tai and Pan, 2007). In this study for simplicity, we will
not carry out the screen by integrating the real biological knowledge,
but instead, will perform it based on the ratio of the between—group
to within—group sums of squares as in Dudoit et al. (2002). Note
that the gene selection can also be based on other proposals, see for

top 50 genes top 200 genes

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

LO O
8— a
O O
- - - DLDA - - - DLDA
o — SmDLDA 0‘ — SmDLDA
CO_ Ln \\
Q N x
O O__ \
o \
Q) Q)
t a s
0.
so §§_
.8 .8 c5 .
E c; E “
.8 .8 :2 ‘~
2 E O- \o‘
In 0 ‘x
:— ‘O~~_~ o “ o
o ~~~o \
\o g o
O Q—
5- 0
c5
I I I I I I I I
5 1O 15 2O 5 1O 15 20

training set size training set size

Fig. 3. Plots of the average misclassiﬁcation rates for DLDA and SmDLDA
using leukemia data.

example, Bayesian variable selection (Lee et al., 2003), analysis of
variance (Draghici et al., 2003) and independent component analysis
(Cale et al., 2005). Let ALL be class 1 and AML be class 2. The
ratio for gene j is given as

2 _ _
Zk=1 21:1 (Xk-j _X--j)2

2 _ 7
Zk=1:?;1(inj_Xuj)2

where X. J- is the averaged expression values across all samples and

3W0):

 

ij is that across samples belonging to class k. We select the top G
genes (50 and 200) with the largest BW ratios for further study.

To assess the misclassiﬁcation rates for both SmDLDA and
DLDA, we randomly divide the total 72 samples into training sets
and test sets. We let the training set size for each class ranging
from 5, 10, 15 to 20, respectively. The remaining samples are then
used as the test sets. Recall that the improvement of shrinkage is
inversely proportional to the mean heterogeneity (Section 4). To
better improve the performance of the shrinkage—based rule, we
propose an adaptive procedure that aims to reduce the possibly
large level of mean heterogeneity that may appear in real data.
Speciﬁcally, we shrink the distances between the class centroids and
the overall centroids rather than to shrink the class centroids directly.
This is a similar idea as in Tibshirani et al. (2003). Finally, for each
setting, we repeat the procedure 1000 times and report the average
misclassiﬁcation rates in Figure 3. Similarly as in the simulation
studies, it is evident again that SmDLDA outperforms DLDA in all
settings.

5 DISCUSSION

In this article, we proposed an optimal shrinkage estimator for the
mean value under the ‘large G small n’ scenario. We then applied
the proposed shrinkage estimator to high—dimensional classiﬁcation
problem by constructing a shrinkage—based diagonal discriminant
rule. Its improvement over the original competitor was demonstrated
through both simulations and real data analysis.

Though the independence assumption in this article is popular in
the literature (Bickel and Levina, 2004; Dudoit et al., 2002; Hwang
et al., 2009; Tong and Wang, 2007), it is unlikely to be true in
practice and so certain remedy might be necessary for a further
improvement when additional information is available. Langaas

 

535

112 /§.IO'SIBUJnOprOJXO'SOIlBIHJOJUIOICI/ﬁdnq 111011 pop1201um0q

9IOZ ‘091sn8nv uo ::

ITong et al.

 

et al. (2005) suggested that the clumpy dependence is a likely
form of dependence, where the clumpy dependence means that
the genes are dependent within groups and independent among
groups. Inspired by that, one natural extension would be to propose
new shrinkage estimators for the mean value under the clumpy
dependence structure. To avoid the singularity problem, we might
need to assume that the largest group size is not larger than the
number of samples. Another future work is to examine if the
proposed optimal shrinkage estimator has any good in its own right,
or if it can be further improved by its positive—part estimator.
Finally, we note that the proposed SmDLDA in this article is a
shrinkage—mean—only—based DLDA, whereas in Pang et al. (2009)
the authors proposed a shrinkage—variance—only—based DLDA.
As both the mean and variance estimations are crucial in the
statistical analysis, further research might be needed to develop new
classiﬁcation rules that shrink both the mean value and the variance.
Possible approaches can be either by plugging—in the existing
shrinkage estimators, respectively, or by proposing new shrinkage
estimators for the mean value and variances simultaneously.

ACKNOWLEDGEMENTS

The authors are grateful to the editor, the associate editor and
three referees for their constructive comments and suggestions
that have led to a substantial improvement in the article. The
authors are also grateful to Professor J .T. Gene Hwang for helpful
discussions.

Funding: Hong Kong RGC grant (HKBU202711) and Hong
Kong Baptist University FRG grant (FRG2/10—11/020) to T.T.;
AFAR research grant and National Institutes of Health grant
(P50HG002790) to L.C; National Institutes of Health grant
(GM59507) and NSF grant (DMS0714817) to H.Z.

Conﬂict of Interest: none declared.

REFERENCES

Assani,I. (1997) Strong laws for weighted sums of independent identically distributed
random variables. Duke Math. J., 88, 217—246.

Asyali,M.H. et al. (2006) Gene expression proﬁle classiﬁcation: a review. Curr.
Bioinformatics, 1, 55—73.

Baldi,P. and L0ng,A.D. (2001) A Bayesian framework for the analysis of microarray
expression data: regularized t-test and statistical inferences of gene changes.
Bioinformatics, 17, 509—519.

Baranchik,A.J. (1970) A family of rninimax estimators of the mean of a multivariate
normal distribution. Ann. Math. Stat, 41, 642—645.

Berger,J.O. and B0ck,M.E. (1976) Combining independent normal mean estimation
problems with unknown variances. Ann. Stat, 4, 642—648.

Bickel,P.J. and Levina,E. (2004) Some theory of Fisher’s linear discriminant function,
‘naive Bayes’, and some alternatives when there are many more variables than
observations. Bernoulli, 10, 989—1010.

Calo,D.G. et al. (2005) Variable selection in classiﬁcation problems: a strategy based
on independent component analysis. In Vichi,M. et al. (eds) New Developments
in Classification and Data Analysis. Studies in Classification, Data Analysis, and
Knowledge Organization. Springer, Berlin, pp. 21—30.

Cui,X. et al. (2005) Improved statistical tests for differential gene expression by
shrinking variance components estimates. Biostatistics, 6, 59—75.

Dettling,M. (2005) Bagboosting for tumor classiﬁcation with gene expression data.
Bioinformatics, 20, 3583—3593.

Draghici,S. et al. (2003) Noise sampling method: an ANOVA approach allowing
robust selection of differentially regulated genes measured by DNA microarrays.
Bioinformatics, 19, 1348—1359.

Dudoit,S. et al. (2002) Comparison of discrimination methods for the classiﬁcation of
tumors using gene expression data. J. Am. Stat. Assoc, 97, 77—87.

Efron,B. and Morris,C. (1973) Stein’s estimation rule and its competitors - an empirical
Bayes approach. J. Am. Stat. Assoc, 68, 117—130.

Fourdrinier,D. et al. (2003) Robust shrinkage estimation for elliptically symmetric
distributions with unknown covariance matrix. J. Multivar. Anal, 85,
24—39.

Gelman,A. et al. (2004) Bayesian Data Analysis, 2nd edn. Chapman and Hall, London.

Gleser,L.J. (1986) Minimax estimators of a normal mean vector for arbitrary quadratic
loss and unknown covariance matrix. Ann. Stat, 14, 1625—1633.

Golub,T.R. et al. ( 1999) Molecular classiﬁcation of cancer: class discovery and class
prediction by gene expression monitoring. Science, 286, 531—537.

Guo,Y. et al. (2007) Regularized linear discriminant analysis and its application in
microarrays. Biostatistics, 8, 86—100.

Hausser,J. and Strimmer,K. (2009) Entropy inference and the James-Stein estimator,
with application to nonlinear gene association networks. J. Mach. Learn. Res.,
10, 1469—1484.

Heilemann,U. and Schuhr,R. (2008) On the evolution of german business cycles
1958—2004. J. Econ. Stat, 228, 84—109.

Huang,S. et al. (2010) Bias-corrected diagonal discriminant rules for high-dimensional
classiﬁcation. Biometrics, 66, 1096—1106.

Hwang,J.T.G. and Liu,P. (2010) Optimal tests shrinking both means and variances
applicable to microarray data analysis. Stat. Appl. Genet. Mol Biol, 9, 36.

Hwang,J.T.G. et al. (2009) Empirical Bayes conﬁdence intervals shrinking both means
and variances. J. R. Stat. Soc Ser. B, 71, 265—285.

James,W. and Stein,C. ( 1961) Estimation with quadratic loss. Proc. Fourth Berkeley
Symp. Math. Stat. Probab, 1, 361—379.

Langaas,M. et al. (2005) Estimating the proportion of true null hypotheses, with
application to DNA microarray data. J. R. Stat. Soc Ser. B, 67, 555—572.

Lee,K.E. et al. (2003) Gene Selection: a Bayesian variable selection approach.
Bioinformatics, 19, 90—97.

Lee,J.W. et al. (2005) An extensive comparison of recent classiﬁcation tools applied to
microarray data. Comput. Stat. Data Anal, 48, 869—885.

Lin,P.E. and Tsai,H.L. ( 1973) Generalized Bayes rninimax estimators of the multivariate
normal mean with unknown covariance matrix. Ann. Stat, 1, 142—145.

Lindley,D.V. (1962) Discussion of professor Stein’s paper: conﬁdence sets for the mean
of a multivariate normal distribution. J. R. Stat. Soc Ser. B, 24, 285—287.

Montazeri,Z. et al. (2010) Shrinkage estimation of effect sizes as an alternative
to hypothesis testing followed by estimation in high-dimensional biology:
Applications to differential gene expression. Stat. Appl. Genet. Mol Biol, 9, 23.

Noushath,S. et al. (2006) Diagonal Fisher linear discriminant analysis for efﬁcient face
recognition. Neurocomputing, 69, 1711—1716.

Opgen-Rhein,R. and Strimmer,K. (2007) Accurate ranking of differentially expressed
genes by a distribution-free shrinkage approach. Stat. Appl. Genet. Mol Biol, 6, 9.

Pan,W. (2006) Incorporating gene functions as priors in model-based clustering of
microarray gene expression data. Bioinformatics, 22, 795—801.

Pang,H. et al. (2009) Shrinkage-based diagonal discriminant analysis and its
applications in high-dimensional data. Biometrics, 65, 1021—1029.

Pang,H. et al. (2010) Analyzing breast cancer microarrays from african americans using
shrinkage-based discriminant analysis. Hum. Genomics, 5, 5—16.

Smyth,G.K. (2004) Linear models and empirical Bayes methods for assessing
differential expression in microarray experiment. Stat. Appl. Genet. Mol Biol, 3, 1.

Speed,R. (2003) Statistical Analysis of Gene Expression Microarray Data. Chapman
and Hall, London.

Storey,J.D. and Tibshirani,R. (2003) SAM thresholding and false discovery rates for
detecting differential gene expression in DNA microarrays. In Parmigiani,G. et al.
(eds) The Analysis of Gene Expression Data: Methods and Software. Springer,
New York.

Tai,F. and Pan,W. (2007) Incorporating prior knowledge of gene functional groups
into regularized discriminant analysis of microarray data. Bioinformatics, 23,
3170—3177.

Tibshirani,R. et al. (2003) Class prediction by nearest shrunken centroids, with
applications to DNA microarrays. Stat. Sci., 18, 104—117.

Tong,T. and Wang,Y. (2007) Optimal shrinkage estimation of variances with
applications to microarray data analysis. J. Am. Stat. Assoc, 102, 113—122.

Wang,Y. et al. (2009) Variance estimation in the analysis of microarray data. J. R. Stat.
Soc Ser. B, 71, 425—445.

Wright,G.W. and Simon,R.M. (2003) A random variance model for detection of
differential gene expression in small microarray experiments, Bioinformatics, 19,
2448—2455.

 

536

112 /§.IO'SIBUJnOprOJXO'SOIlBIHJOJUIOICI/ﬁdnq 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

Improved mean estimation

 

APPENDIX A
A1. DERIVATION OF FORMULA (4)
Noting that 11 — pt 2 (X — pt) — rX/ “X “3., the risk function is

A n A _ A
Romp) = Emu—ND 1(IL—IL)

%E[(X—IL)TD_1(X—IL)
+ r2 _ T _1_
(IIXIIE)2

 

f 2 (X—IL)TD_1X
IIXIIS

 

_T _1_
— _ XD (X—IL)
IIXII§ 1

2 ' 2
nr “X”
= 1+—E _ ZDZ
G (IIXIIS)

G _ _
_2nr E[ 1 Xxx—m]
' 2 2 '
G,=, IIXIIS 0-

l

 

By Stein formula, Eg(X)(X — a) = eZEg’ (X) where x ~ N(,u, 02),
we have

E 1 Xi(Xi—Mi) 21E Xi Xi—Mi
|le|§ 0,2 n _ IIXIIE (IE/n

1 _ a x,-
= —E T _ 2
n _8X.- IIXIIS

_1E_ 1 2x,2
n _IIXII§ (IIXIIEPSE '

 

 

 

 

This leads to

- nr2 17%] 240—2) ( 1 >
R( , ,D)=—E _ — E _ .
M G [(||X||§)2 G IIXII§

Finally, by minimizing the above quantity, we have

 

(G—2)E(1/IIXII§)
7’0 t: _ _ -
nE[IIXII%,/(IIXII§)2]

 

APPENDIX B
B1. PROOF OF LEMMA

Noting that f (x) = 1 /x is a continuous function in (0, 00), it sufﬁces

to show that ||X||§/||X||%)g'pn as G—>oo. By Assani (1997), the
following strong law holds under the condition supm(Nm/m) < 00
almost surely,

G
1
A—G Za, [2,- —E(Z,-)] & 0, as G—> oo.
i=1

Further, we have Zl-GZIaiZi/Agg'Elel/(n—3) for any
n24. This proves the lemma by noting that AG2||X||%) and
||X||§=(n—1)ZiG=1aiZi-

 

537

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

