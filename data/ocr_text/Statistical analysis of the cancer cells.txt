ORIGINAL PAPER

Vol. 27 no. 4 2011, pages 556—563
doi: 10. 1 093/bioinformatics/btq 704

 

Systems biology

Advance Access publication December 20, 2010

Statistical analysis of the cancer cell’s molecular entropy using

high-throughput data

Wessel N. van Wieringen1’2’* and Aad W. van der Vaart2

1Department of Epidemiology and Biostatistics, VU University Medical Center, PO Box 7075, 1007 MB Amsterdam
and 2Department of Mathematics, VU University Amsterdam, De Boelelaan 1081a, 1081 HV Amsterdam,

The Netherlands

Associate Editor: Trey Ideker

 

ABSTRACT

Motivation: As cancer progresses, DNA copy number aberrations
accumulate and the genomic entropy (chromosomal disorganization)
increases. For this surge to have any oncogenetic effect, it should (to
some extent) be reflected at other molecular levels of the cancer cell,
in particular that of the transcriptome. Such a coincidence of cancer
progression and the propagation of an entropy increase through the
molecular levels of the cancer cell would enhance the understanding
of cancer evolution.

Results: A statistical argument reveals that (under some
assumptions) an entropy increase in one random variable (DNA
copy number) leads to an entropy increase in another (gene
expression). Statistical methodology is provided to investigate the
relation between the genomic and transcriptomic entropy using
high-throughput data. Analyses of multiple high-throughput datasets
using this methodology show a close, concordant relation among
the genomic and transcriptomic entropy. Hence, as cancer evolves,
and the genomic entropy increases, the transcriptomic entropy is
also expected to surge.

Contact: wvanwie@few.vu.nl

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on September 14, 2010; revised on December 2, 2010;
accepted on December 15, 2010

1 INTRODUCTION

DNA copy number aberrations are one of the key characteristics
of cancer (Stratton et al., 2009). In fact, the accumulation of
DNA copy number aberrations is the most consistent feature of
cancer progression (Fearon and Vogelstein, 1990). The entropy at
the genomic level (chromosomal disorganization) of cancer cells
thus exceeds that of healthy cells and tends to surge as cancer
progresses. DNA copy number aberrations affect mRN A expression
levels as the central dogma of molecular biology suggests and
numerous high-throughput studies have shown (e.g. Pollack et al.,
2002). Aberrations need not only affect the expression of its driver
gene, but may also alter expression levels of the other genes
that map to the aberrated segment (Kauraniemi and Kallioniemi,
2006; Van Wieringen et al., 2010). In fact, genomic aberrations
also affect expression levels of other transcripts like microRNAs
(e.g. Zhang et al., 2006). The close relation between the genome

 

*To whom correspondence should be addressed.

and transcriptome suggests that the entropy increase spreads to other
molecular levels of the cell’s regulatory system, and is expected to
manifest itself most prominently in the transcriptome. Indeed, for
this surge in genomic entropy to have any phenotypic (oncogenetic)
effect, it needs (to some extent) to propagate to the transcriptomic
level and beyond.

Cancer may be considered an evolutionary process, driven by
random variation and natural selection (Merlo et al., 2006; Stratton
et al., 2009). During its life a cell may undergo heritable genetic
alterations (e.g. DNA copy number aberrations). Such alterations
may be neutral but may also affect the cell’s phenotype. Irrespective
of the type of alteration, any cell is subject to natural selection: it
has to survive in the environment of the organism’s tissue. Within
this framework, a cancer cell can be thought of as having acquired
alterations that resulted in beneﬁcial traits to survive, proliferate and
metastasize (Hanahan and Weinberg, 2000).

Evolution explores different paths via random variation, and the
path that leads to a faster entropy increase is naturally selected (Kaila
and Annila, 2008). As cancer evolves/progresses, the entropy at the
genomic level increases (Castro et al., 2006; Hoglund et al., 2005).
Here we investigate, using high-throughput studies, whether this is
reﬂected at the transcriptomic level (as suggested by Tsafrir et al.,
2006). If so, this may shed light on the path of evolution of the
cancer cell.

Before we facilitate the study of the propagation of increased
entropy of genome to transcriptome in the cancer cell, we ﬁrst
provide a statistical argument that suggests this indeed seems
plausible. We then present statistical methodology to analyze high-
throughput genomic experiments in order to answer the following
related questions:

0 Is the entropy of a cancer sample’s transcriptome higher than
that of a normal sample?
0 Is a cancer sample’s genomic entropy associated to that of its

transcriptome?

These questions are portrayed schematically in Figure 1. We
illustrate how the discussed methodology may be utilized by
application to multiple datasets.

2 METHODS
2.1 Motivation

We provide statistical motivations for the hypothesis that an increase of
the entropy at the genomic level leads to an increase of the entropy at the

 

556 © The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 [3.10'811211an[p.IOJXO'SODBIIIJOJIIIOICI/[I(11111 meg pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

Analysis of the cancer cell’s molecular entropy

 

normal samples cancer samples

normal copy observation: copy

number Hnormals < Hcancers number
assumed data
|_|

hypothesis:

H increase

propagated
gene hypothesis: gene

expression expression
data data

Fig. 1. Schemata of entropy relations of genome and transcriptome within
and between normal and cancer samples.

transcriptomic level. To do so, we need the deﬁnition of the (diﬁferential)
entropy of a continuous random variable X with density fX as (Cover and
Thomas, 2006):

H00 2: —f fx(x)10g[fx(x)]dx-

For our ﬁrst motivation, we assume that DNA copy number and gene
expression are both measured without error and their relation at any loci
may be described by a strict monotone increasing function g(-): Y =g(X).
In addition, we note that, according to Theorem 1 of Ramsay (1998), any
smooth monotone function g(x) may be represented as:

x t
g(x) = c0+clf exp w(s)ds] dt, (1)

where c0 and c1 are constants and w is a coefﬁcient function that is
Lebesque square integrable. By construction, w satisﬁes the second-order
differential equation d2g(x)/dx2 =wdg(x)/dx. For w(s)=0, g(-) is linear;
w(s)=a implies g(-) is an exponential function.

PROPOSITION 1. Let X1 and X2 be random variables with symmetric and
zero-centered densities le and fXZ, respectively, and g(-) a strict monotone
function generated by Representation (I) using an even coeﬁ‘icient function
w(s), 116- W(3)=W(—S)- Then, H (X1)SH (X2) implies H (g(X1))SH (g(X2))
To show that H(g(X1)) 5 H (g(X2)), we ﬁrst note that the entropies of X1 and
g(Xl) are linked via:

dg

H<g<X1» = H(X1)+ f fx1(x)10g%dx-

It thus sufﬁces to show that:

00 d 00
/ fx1(x)10g (51:6) 6196 S / fx2(x)108

 

 

dg(x)
dx.
dx
In case g(-) is a linear map, g(x)=ax, this is immediate. To prove the

desired inequality for other choices of g(-), we note that, appealing to
Representation (1), the original inequality follows if we prove:

/ 00 le (x) f x w(s)dsdx 5 foofX2(x) fx w(s)dsdx. (2)

Integration by parts yields:

foofx1(x) fx w(s)dsdx = foo w(s)ds— foo FX1(x)w(x)dx.

From the assumption that X1 and X2 are symmetrically distributed around
zero and the symmetry of w(s), w(s) = w(—s), the second term on the right-
hand side equals fooo w(s)ds and equality in Equation (2) follows. Hence,
under the assumptions of Proposition 1, we have shown that H (X1) 5 H (X2)
implies H (80(1)) 5 H (g(X2))-

If one is not willing to assume that DNA copy number and gene expression
are known without error, we provide an alternative motivation. Hereto we

introduce the concept of dispersive ordering between two random variables
(Shaked and Shanthikumar, 2007). Let X1 and X2 be two random variables
with distributions F X1 and F X2, respectively. Let F £1 1 and F £21 be the right
continuous inverses of F X1 and F X2, and assume that:

F; (b) — 14; 1 (a) 5 17;; (b) — 17;; (a) for all 0 5 a 5 b 5 1.
Then X1 is said to be smaller than X2 in the dispersive order, denoted by
X1 Sdisp X2.

COROLLARY 1. Let X1 ,X2 and 8 be independent random variables with log-
concave densities le ,fX2 and f8, respectively. Then, ile fdisp X2 and ,8 Z 0:

,3X1 +8 Edisp ,3X2 +8-

Corollary 1 follows directly from Theorems 3.B.4 and 3.B.9 of Shaked and
Shanthikumar (2007).

To interpret Corollary 1, let X1 and X2 be random variables representing
the DNA copy number changes at two different loci and Y1 and Y2 the
expression levels of two genes that map to these loci. Furthermore, assume
that the relation between DNA copy number changes and gene expression
at both loci may be described by Y =,8X +8 with X and 8 independent,
E(Y |X)= ,BX, and ,8 Z 0 to reﬂect the empirical observation that a(n)
increase/decrease in DNA copy number leads to a(n) increase/decrease in
gene expression. Theorem 1 then tells us that if locus 2 is more prone to be
aberrated at the genomic level than locus 1 (statistically operationalized as
X1 5disp X2), this is reﬂected in the transcriptome (under the assumption of
a simple linear model) and locus 2 will exhibit more abnormal expression
levels than locus 1.

Corollary 1 is formulated in terms of the dispersive ordering, whereas
our interest is in the entropy ordering. The relevance of Theorem 1
stems from the fact that dispersive ordering implies entropy ordering,
i.e. X1 5diSpX2 =>H(X1)5H(X2) (Oja, 1981). Corollary 1 is illustrated
for the entropy ordering by two examples. In the ﬁrst, let X1 ~N(0,o}2(1),
X2 ~N(0, (52(2), 81 ~N(0, 082) and 62 ~N(0, 082), all are independent. Then:
H(,BX2 +82) —H(,BX1 +81) 2 %10g(,820}2(2 +082)—%10g(,820}2(1 +082). In case
H (X2) 2H (X1), and thus (5%2 Z (7)2“, this difference is non-negative. In
the second example, let X1~Cauchy(0,yX1), X2 ~Cauchy(0, yXZ), 81 ~
Cauchy(0, ya) and 82 ~Cauchy(0, ya), all are independent. Then, using
a result from Blyth (1986): H(,BX2+82)—H(,BX1+81)=log(,B)/X2 +y8)—
log(,B)/X1 + ye). In case H (X2) 2 H (X1), and thus yxz 2 ml, this difference is
non-negative.

2.2 Experiments

Two types of experiments are considered. The ﬁrst (referred to as experiment
of Type 1) involves nC samples from cancerous tissue of a particular same
type. For all samples, both a DNA copy number and a gene expression proﬁle
are assumed to be available. The random variables X; and Yij represent
the DNA copy number and the expression level of gene j, j: 1, ..., p, of
sample i, i: 1, . . . , nC, respectively. Together the DNA copy number proﬁles
of all samples make up X = (Xij)i=1,...,nc;j=1,m,p, the n x p genomic aberration
matrix, which we assume to contain no missing values. The gene expression
matrix Y is deﬁned similarly, also without missing values. The DNA copy
number and gene expressions proﬁles of sample i are thus the i-th rows of
matrices X and Y, respectively, and will be denoted X,- and Y).

The second type of experiment (Type II) involves n =nN+nC samples,
of which nN originate from normal, healthy tissue and nC from cancerous
tissue of the same type. For all samples, only a gene expression proﬁle is
available. The gene expression data of experiments of Type II is denoted as
above by Y = (Yij)i=1,...,n;j=1,...,p-

2.3 Entropy

In the present context, entropy measures the diversity (at the molecular level)
of the samples under study. Here diversity (entropy) at the transcriptomic

 

557

112 Bio'sleuinofplogxo'sopeuuogurorq/ﬁdnq wort pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

W.N. van Wieringen and A.W.van der Vaart

 

level is compared between normal and cancer samples. At the genomic level,
the entropy of normal samples reaches a minimum (the DNA copy number
of its autosomale genome equals two), whereas that of cancer samples can
be anything as DNA copy number aberrations may be abound. From Section
2.1 we thus expect that this propagates to the expression levels, resulting
in a higher entropy of the cancer transcriptome. To test this, we use the
entropy difference between the normal and cancer samples as test statistic,
and generate its null distribution by resampling. In order to calculate the
test statistic, we ﬁrst point out how the entropy of a set of samples may be
calculated from high-dimensional data (p > n), then discuss the test.

The entropy of a multivariate normally distributed random variable, YiT ~
N01,, 2), is given by %log[det(2)] + %p[1+log(27t)]. For low-dimensional
situations (p < n) the determinant of 2, and thus the entropy of Y),
is straightforwardly obtained by estimating 2 by S: %(Y—ﬁ)T(Y—ﬁ),
where ﬁ=(}l:?=1Y,-1,..., %Z?=1Yip), and computing the product of its
eigenvalues. However, in case p > n the estimate S is singular and
consequently det(S) = 0. This problem can be overcome when using a
shrunken estimate of 2 (Schafer and Strimmer, 2005). The shrunken estimate
of the covariance matrix is given by: f: = (1 —A)S+AT, where S as before,
T a diagonal mthrix with diag(T) = diag(S) and the optimal (in some sense) A
equals [Zingar(rij)]/[Zi¢j rizj] with rij =(S),-j/ (S),-,-(S)jj (confer Schafer
and Strimmer, 2005). The determinant of the shrunken covariance estimate
is given by:

det(fl) = det(Tl/ZT—l/Zir—l/ZTW)
= det(T1/2)2det[(1—A)T_1/ZST_1/2+AI]
= det(T)det[(1—A)S+AI]

:e

P
sfjn[(1—A)uj+t],
j=1

j 1
where up 2 Z vp_n Z 0: vp_n_1 = . . . 2 v1 are the eigenvalues of S =
T‘l/ZST_1/2. The non-zero vs coincide with the square of the singular
values of (Y—ﬁ)T_1/2/ﬁ. Hence, we are able to estimate the entropy
when p > n.

To contrast the multivariate normality-based entropy, we also use a non-
parametric motivated entropy estimate (Kozachenko and Leonenko, 1987;
Leonenko et al., 2008). Hereto we need the k-th nearest neighbor probability
density estimate of f (-) given by:

A k I‘ 2+1) 1
MY» = — “IV—2 —,
n—1 W [we]?
where nP/Z/ r(p/2+ 1) the volume of the unit ball in RP, and dk(Y,-) the
Euclidean distance between Y,- and its k nearest neighbor. The entropy is
then estimated by:

Elton = —%Zlog[ft<Yt-)]. (3)
i=1

the average entropy of the observations (as determined within the sample).

To test for a difference in entropy, we use the following test statistics:
T=H(C) —H(N), where 1160 and H(N) are the estimated entropy in cancer
and normal sample, respectively. To obtain the null distribution of this test
statistic, we permute1 the sample labels and recalculate the test statistic.
This process is repeated L times. The signiﬁcance level of the tests is now
calculated by {#ZITobngg for £=1,...,L}/L, the proportion of the null
distribution that exceeds the observed test statistic.

2.4 Mutual information

To investigate the genomic—transcriptomic entropy association, we use the
concept of mutual information, a general measure of dependence between

 

1The non-parametric bootstrap seems inappropriate here (and in Section 2.4)
as it draws datasets of the same dimensions with identical replicate samples,
which inﬂates the compactness (decreases entropy) of the dataset.

two random variables. Here, it measures the amount of information shared
by genome and transcriptome. Or, loosely, how much knowledge of the
genomic entropy tells us about the transcriptomic entropy. Formally, mutual
information is deﬁned as (Cover and Thomas, 2006):

ﬁY.X)(y,x))
; 2: , l — ddx. 4
“Y X) f / 11”)“ ’0 og<fy(y)fx(x) y U

Mutual information and entropy are related via I (Y ;X ) =H (Y ) —H (Y |X) =
H(Y)+H(X)—H(Y,X), where H(Y|X) is the conditional entropy of Y
given X and H(Y,X) the joint entropy of Y and X. Hence, by studying
the mutual information between Y and X, we compare the unconditional
entropy of the gene expression to its conditional counterpart, conditional
on DNA copy number. In case, gene expression is independent of DNA
copy number: H (Y | X) =H (Y) and I (Y; X) = 0. Mutual information can thus
be used to test whether the transcriptomic entropy is associated with the
genomic entropy, using resampling to assess whether mutual information
deviates signiﬁcantly from zero. We describe how to estimate I (Y; X) for the
distributions considered in Section 2.3.
In case of multivariate normality, Z := (X,Y)T:

N MX 2:XX 2:YX _,
Z N((MY)’(2XY 2”» FNMZJZZ)’

the mutual information in the high-dimensional setting of p>n is
estimated by:

P P
éZlog [(1 42)qu +AZ]+%Zlog[(1—Az)v}( +Az]

i=1 i=1

2p
1
—E E log [(1 —Az)vJ-Z+AZ] ,
i=1

where AZ the shrinkage parameter of shrunken estimate of 222,  the

eigenvalues of ngl/Zszzrgzl/Z with $22 = %(Z—ﬁz)(Z—ﬁz)T and T22 a

diagonal matrix with diag(Tzz) = diag(Szz), and v}, and  deﬁned similarly.

Again to contrast the multivariate normality assumption, we also estimate
the mutual information under the assumption of the k-th nearest neighbor
distribution. We follow the approach described in Kraskov et al. (2004).
The joint entropy, the entropy of Z=(X,Y)T, can be estimated using (3).
To obtain the mutual information we need to subtract if from the entropies
of X and Y, using the same k. However, this results in a biased mutual
information estimate (Kraskov et al., 2004). To resolve this Kraskov et al.
(2004) propose, in the estimation of the marginal entropies, to ﬁx the nearest
neighbor distance instead of k. Let dk(Z,-) be the distance (with respect to
the uniform norm) of Zl- to its k-th nearest neighbor, and kx(i) and ky(i)
the number of samples that fall marginally in the balls B(X,-,dk(Z,-)) and
B(Y,-, dk (Zi)). Now, following Wang et al. (2009), the marginal entropies are
estimated, by, e. g.:

. 1 "
Ht,(X) = 7 Zitikx<i)i+iog<n— 1) (5)
i=1
ﬂp/Z 1 n
— _ . . P
+log[r(p/2+1)] + n ;10g[dkx(t)(xt)l ],

where (0 is the digamma function and k2 refers to the k chosen for Z. The
mutual information is then estimated by sz (X) +sz (Y) —Hk (Z):

A 1 "
I (Y; X) = w(kz) — ; Zhﬂkxu‘» + 1009(0)] + w(n), (6)
i=1

where last term of entropy estimates (S) cancel out as the same nearest
neighbor distances are used in the calculation of j oint and marginal entropies.
Estimator (6) is less biased than a mutual information estimate using the same
k for the estimation of H (X),H (Y) and H (Z) (Kraskov et al., 2004; Wang
et al., 2009). Note that the estimator (6) is not scale invariant, and Kraskov

 

558

112 Bio'sleuinofplogxo'sopeuuogurorq/ﬁdnq wort pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

Analysis of the cancer cell’s molecular entropy

 

et al. (2004) recommend to scale X and Y to comparable scales before the
estimation of mutual information.

The null distribution of the test statistic is estimated by permuting the
columns of the gene expression matrix Y, while the DNA copy number matrix
X is left unchanged (as is done in Van Wieringen and Van de Wiel, 2009).
Under the null hypothesis, there is no association between copy number and
expression, consequently the permutation only yields the random behavior
of the test statistic. For each permutation, the test statistic is recalculated.
After L permutations, the P-value is calculated as in Section 2.3.

3 SIMULATION

Here we report a simulation study that serves three ends: (i) to make
an informed choice on the nearest neighbor parameter k for entropy
and mutual information estimation, (ii) to investigate the behavior of
the entropy and mutual information estimators under increased high
dimensionality and (iii) to study the relation between the two entropy
estimators (similarly for the two mutual information estimators). The
simulation study involves artiﬁcial data, which facilitates insight
on all three ends, as they are sampled from a known distribution
with known entropy and mutual information. The setup is described
next.

Artiﬁcial datasets involve n = 20 samples from a p-variate, p 2 an
with a varying from 0.5 to 50, normal distribution N (0, 23). Prior
to the generation of each dataset, the covariance matrix 22 is itself
randomly drawn from an inverse Wishart distribution W_1(d, S2),
with d =p+2 (allowing 2 to vary around S2) and S2 =VTAV.
The p x p dimensional matrix V is diagonal with diagonal elements
sampled randomly from the empirical distribution of MADs of gene
expression data from individual genes of the Chitale dataset. The
p x p dimensional matrix A is the correlation matrix estimated from
the expression data of the same p genes of the Chitale dataset
(see Section 4), with correlations estimated by Kendall’s ‘L', which
ensures the positive deﬁniteness of A. As, for d =p+2, E(Z): S2,
the sampled covariance structure mimics that found in real datasets.
For each a, multiple datasets are drawn, for which the entropy is
estimated (assuming normality and k-NN) and the true entropy using
the corresponding drawn 2 is calculated. For the investigation of
mutual information datasets with twice the number of genes are
drawn, which is randomly split in two equal sized parts. In the
estimation of the k-nearest neighbor entropy and mutual information
k: ,8n , where ,8 ranges from 0.05 to 0.95.

The simulation results are ﬁrst used to ﬁnd an optimal ,8 for
entropy and mutual information estimation (when assuming k-NN).
Hereto the correlation between Hknn and Htrue and between lknn
and Itrue are calculated for all combinations of a and ,8. Ideally,
these correlations are high, and an optimal choice of ,8 results in
the highest correlations over the whole range of a. The correlations
are displayed as a contour plot in Figure 2. From Figure 2, it is
clear that both entropy and mutual information estimation are served
best by choosing a small ,8 (irrespective of the high—dimensionality
parameter a).

To investigate the effect of increased high dimensionality on
entropy and mutual information, we again calculate the correlations
between H and Htrue and between l and Itrue (assuming both

norm knn norm

normality and k-NN), which are denoted pg H , pg H, pi I and

$011111“. These correlations are plotted against a (plots not shown, but

cah be deduced from Figure 2 for the k-NN assumption). This reveals
that the correlations at ﬁrst decay rapidly as a is increased, but that

entropy mutual information

1.0 1.0

alpha

 

 

0.0

   

Fig. 2. Contour plots of the correlations between Hknn and Htrue and
between [knn and [true in relation to high dimensionality (a = p /n) and the
number of nearest neighbors (,8 = k /n).

this decay levels off to settle at ,0??? m 0.30 and $011111“ @040 after

Ol=20 and p113“; ~0.30 and pﬁn=o30 after Ol=20. The decay in
itself is not surprising, but the, leveling off is encouraging, as the
proposed estimators, although noisy, do provide information on the
quantities of interest.

Finally, we investigate whether the estimates of the different
operationalizations of entropy and mutual information induced by
the different distributional assumptions yield concordant results.
We now correlate Hnorm with Hknn and inorm with lknn. For
the simulated data, Spearman’s correlation between the entropy
estimates is high (varying from 0.80 to 0.95, with highest
values assumed for small k) and between the mutual information
estimates is substantial (varying from 0.40 to 0.75, with highest
values assumed for small k). The concordance of the results
from both entropy and mutual information operationalizations
indicates that they indeed measure the same quantity. By lack of
information on the true entropy and mutual information, which is
the case when analyzing real data, corroboration between the two
operationalizations enhances the conﬁdence in the results.

4 APPLICATION TO CANCER DATA

The aforementioned methodology is applied to publicly available
datasets representing both experiment types described in Section
2.2. The ﬁrst ﬁve columns of Table 1 give an overview of the
datasets analyzed. Details on array platforms, preprocessing, etc.
are provided in the Supplementary Material. Here we only point
out that we have used normalized instead of segmented or called
copy number data (refer to Van Wieringen et al., 2007, for details
on the differences between these data) in the analysis of Type I
experiments, as beforehand the multivariate normality assumption
is unlikely to hold for the latter two. See Section 4.4 for a discussion
on normalization.

4.1 Analyses of Type II experiments

We analyzed the Type II experiments listed in Table 1 by calculating
the entropy difference between cancer and normal groups under
the normality and k-th nearest neighbor distributional assumptions
discussed in Section 2.3. Permutation tests with L = 1000 were used
to evaluate the null hypothesis of no entropy difference between
the groups. The results are displayed in Table 1. All Type II datasets
show a signiﬁcant (P < 0.10 for all analyses, but more often than not

 

559

112 Bio'sleuinofplogxo'sopeuuogurorq/ﬁdnq wort pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

W.N. van Wieringen and A.W.van der Vaart

 

Table 1. Datasets used (more details are provided in the Supplementary Material) and their analysis results

 

 

Dataset Cancer Experiment nN nC Entropy Entropy Mutual information Mutual information

name type type P-value P-value P-value P-value
(normality) (knn, k = 1) (normality) (knn, k = 1)

Chandran Prostate II 54 54 0.012 0.026 — —

D’Errico Gastric II 3 1 38 0.000 0.000 — —

Kim Prostate II 15 32 0.008 0.002 — —

Landi Lung 11 49 58 0.000 0.000 — —

Mougeot Ovary II 14 32 0.000 0.000 — —

Scotto Cervix II 25 41 0.070 0.000 — —

Singh Prostate II 50 52 0.000 0.046 — —

Stirewalt AML II 1 8 26 0.000 0.000 — —

Bergamaschi Breast I — 85 — — 0.000 0.000

Carvalho Colon I — 62 — — 0.000 0.002

Chitale Lung I — 88 — — 0.000 0.000

Kim Prostate I — 17 — — 0.664 0. 184

Lenz DLB CL I — 203 — — 0.000 0. 106

Oudej ans DLB CL I — 42 — — 0.000 0.002

Pollack Breast I — 41 — — 0.000 0.020

Zhang Breast I — 263 — — 0.000 0.000

 

Note that the Kim dataset is a combination of a Type I and a Type II expen'ment and therefore appears twice in the table below. Its nC differs, for the analysis of its Type I version
the cancer samples are limited to those that have both DNA copy number and gene expression proﬁles available.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

D’Errico data Kim data
I normal
A A cancer A“ ‘
 .-  x" \
:  .~ ." '. ; l’ \
e--. 2..   “a. = : .a r
'1 2“. 3; {AF-a  AA.  ', ," I‘ I'
\ a‘. . tux-A“. '~ :95;  2 i :1 ti,’
r’H.  A §:A"-_A  I ‘, ll
“‘IE I‘ .l 'A; I .' :35; A  ‘ I I I
M \l” 12: 2A}? '- 3‘ ~ ' II ”
"I | t... .-  _~ :1 ' _- I \ II
I A, . - ; - . _-
ﬁlth: '1 a“; :-  .. I! 7‘
l l 1* .' - i -' ‘ I
\"i. .. A i - I
“J -_‘ - A. . . . ' . I. A
. A . . . . , , . ' _ H;
‘k' ‘ l normal
- A cancer
Mougeot data Stirewalt data
A .
 l normal I normal
._A_  A cancer “A A cancer
     
  FF”: 13 5A.: l '
  :;:-.; I
_' . ' . \ t . j 1.
an? '- :H-t 2; .~ ~: -:-“-: I
1.9;; ._ .‘. i=5 $.- 1'11- .'
:::'.: ‘. I 'I-Ii \ ZA: 5:: : I'n' I
A 2:1'A1  I i i i 1 1: 2 : I'u' I
::._; A 1 ‘ .l 5 L's-3'": IIIII'
 . .‘ l ‘ , I ; .'. . ~. . A I. I
A I A \ I I . 
‘  " '  )I'III'
..A'

 

 

Fig. 3. The dotted and dashed lines result from convex hull peeling per
group. For clarity, the groups have been artiﬁcially separated by adding a
constant to the ﬁrst principal component of the normal samples.

P < 0.01) difference in entropy: the cancer groups exhibit a higher
entropy than the normal group, under normality and k-NN.

In order to visualize the entropy difference, we have projected the
gene expression matrix Y on its ﬁrst two right principal components
v1v1 and v2 v2, where K1 and K2 are the largest singular values
of Y and v1 and v2 the corresponding right singular vectors. The
projected data for four datasets have been plotted in Figure 3. From

Table 2. Additional results for the Kim dataset: P-values under both
distributional assumptions

 

 

 

Cancer Distributional Cancer stage
stage assumption
PIN Cancer Metastasis

Normal Normal 0.468 0.012 0.000
Normal knn, k = 1 0.356 0.004 0.000

PIN Normal 0.016 0.000

PIN knn, k = 1 0.014 0.000
Cancer Normal 0.008
Cancer knn, k: 1 0.008

 

this ﬁgure, it is clear that the cancer samples are much more spread
out, indeed indicating a higher entropy.

The Kim and Mougeot datasets are of particular interest as they
comprise, next to the analyzed normal and cancer samples, samples
from other cancer stages. The former also yields samples from
an intermediate and a more advanced stage: ‘pin’ (13 samples)
and ‘metastatic’ (20 samples), whereas the latter contains two
intermediate categories: ‘benign’ (18 samples) and ‘borderline
malignant’ (3 samples). This allows further investigation of the
transcriptomic entropy increase with advanced cancer stage. All
possible stage pairs are compared, testing for larger entropy in the
more advanced cancer stage. Tables 2 and 3 contain the results. In the
Mougeot dataset, not all entropy comparisons are signiﬁcant, which
is (to a large extent) due to the low sample size of the ‘borderline
malignant’ group (consisting of only three samples). Nonetheless,
the general picture that emerges from the pairwise analyses in both
datasets is that the entropy of a cancer stage exceeds that of preceding
stages.

 

560

112 Bro'SIBurnoprOJxo'sorteul10jurorq//:dnq wort pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

Analysis of the cancer cell’s molecular entropy

 

Table 3. Additional results for the Mougeot dataset: P-values under both
distributional assumptions

 

 

 

Cancer stage Distributional Cancer stage
assumption
Benign Borderline Malignant
malignant

Normal Normal 0. 104 0.019 0.000
Normal knn, k = 1 0.450 0.048 0.000
Benign Normal 0.004 0.000
Benign knn, k = 1 0.082 0.004
Borderline Normal 0.446

malignant
Borderline knn, k = 1 0.340

malignant

 

The entropy surge may be interpreted as an increased diversity
among the cancer samples. This may suggest that cancers develop
along many different routes, though all leading to unproliferated
growth. Consequently, it may prove difﬁcult to develop one
therapy that will beneﬁt all individuals with the same cancer.
A large (genomic and transcriptomic) diversity increases chances
of individuals being and becoming resistant against the therapy, as
has been observed in many cases.

4.2 Analyses of Type I experiments

For all Type I experiments listed in Table 1, the mutual information
I (Y; X) between gene expression and DNA copy number is
calculated. To evaluate whether I (Y; X) deviates signiﬁcantly from
zero, a permutation test as described in Section 2.4 with L=1000
permutations is conducted. The results are presented in Table 1.
Due to very different (and in these high-dimensional situations
hard to verify) distributional assumptions, the P-values may differ
between the two presented tests. Nonetheless, in almost all Type I
datasets the mutual information deviates signiﬁcantly from zero,
under both distributional assumptions. Hence, the unconditional
transcriptomic entropy H (Y) signiﬁcantly exceeds the conditional
transcriptomic entropy H (YlX), conditional on the DNA copy
number. The Kim dataset, however, shows deviating behavior with
clear non-signiﬁcant P-values. This seems due to the fact that
these cancer samples contain relatively few (compared to, e. g. the
Bergamaschi dataset) genomic aberrations (as determined by the
calling method CGHcall, Van de Wiel et al., 2007). When analyzing
the metastatic samples of the Kim dataset, that contain more genomic
aberrations, the permutation test for H0 :1 (Y; X) = 0 yields P-values
0.076 (normality) and 0.064 (k-NN). This suggests that also in
prostate cancer the genomic entropy surge is propagated to the
transcriptome, although perhaps at a later stage in the progression
of the disease.

To accompany the above test results, we propose a visualization.
The k-NN entropy statistic (3) is composed of the entropies at
each observation. Each sample’s contribution to the k-th nearest
neighbor genomic entropy estimate may then be plotted against
its contribution to the k-th nearest neighbor transcriptomic entropy
estimate. If indeed the entropies of the two molecular levels
are closely related, we expect the ‘marginal’ entropies at each

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

Chitale Oudejans
>. >. o
a - s a —
If. If. m _
.2 8 .2 o
E 8 - E s -
T2 ‘— 2 N
8 o 8 T
e 8 — e g
E °° E g ‘
I- to I— o _
E E 8 . .
I I I I I
-16000 -12000 —8000 —2000 —1000 —500 0
marginal genomic entropy marginal genomic entropy
Pollack Zhang
> >. —
a 8 ° 5
.2 - . .2 ‘°
5 8 _ S —
.g- E .g
g 8 _ g “‘
_ o _
m <0 m
C C
 o _ o o o  o
g 8 - .0 g 8 - 00
ID I I I I I I N I I I I I I I
—2000 0 1000 2000 3000 —2000 —1000 0 500

marginal genomic entropy marginal genomic entropy

Fig. 4. In all panels, the marginal genomic and transcriptomic entropies are
plotted against each other. Each graph contains the isotonic regression ﬁt in
red.

observation, log [fk (Y,)] and log [fk (X,)], to be positively associated.
This is visualized for two datasets in the upper panels of Figure 4.
Indeed, as the test results reported in Table 1 suggest, the ‘marginal’
entropies of the two molecular levels reveal a positive association.

The concordant surge in entropy of the cancer cell’s molecular
levels may be interpreted as follows. Hereto we exploit the reciprocal
link between entropy and information (Gatenby and Frieden, 2004).
As cancer progresses, the information content of a cancer cell’s
genome declines until some minimum necessary for the cell to
function and proliferate is reached. The above shows that this
information decline is reﬂected at the transcriptomic level, which
is a prerequisite for it to have any phenotypic/oncogenic effect.
Together these observations suggest that in the evolution (which
naturally selects the path of steepest entropy ascent, Kaila and Annila
2008) of the cancer cell, natural selection acts on those parts of
the genome and transcriptome that contribute to the cancer cell’s
minimum information content that enable it to realize its small but
focused agenda, making more copies of itself.

4.3 Potential

Besides providing insight into cancer progression, entropy may
be used in clinical cancer research. We illustrate this by two
examples, which can be the basis for further research. We shall
refer to the observation that molecular entropy increases with cancer
progression as the Entropy Assumption. In the ﬁrst example, we
aim to reconstruct (unsupervisedly) the order of the samples’ cancer
advancement using the Entropy Assumption. A simple unsupervised
procedure to achieve this would be to start with all samples together,
and remove them one by one in accordance with the largest
entropy decrease of the expression levels caused by a sample’s
removal. If the Entropy Assumption has some value, the order of
removal is, negatively related to the samples’ cancer advancement:

 

561

112 Bro'SIBurnoprOJxo'sorteul10jurorq//:dnq wort pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

W.N. van Wieringen and A.W.van der Vaart

 

 

 

 

 

 

 

 

 

 

 

 

40

 

 

 

removal order

 

 

 

 

 

 

 

I
I
_I_

 

 

 

| I I I
normal pin cancer metastatic

Fig. 5. Order in which samples are removed (by the unsupervised procedure)
against cancer stage.

samples removed ﬁrst cause the highest increase in entropy and are
(according to the Entropy Assumption) furthest advanced, whereas
samples removed at the end are least advanced. Application of
this procedure to the Kim dataset (as used for generating Table 2)
allows comparison of the reconstructed order to the sample’s known
cancer stages (a proxy for cancer progression). In Figure 5, the
reconstructed order (removal rank) for the Kim dataset (using k-
NN entropy; results are similar under the normality assumption)
is plotted against the known cancer stage. Clearly, the metastatic
samples tend to be removed earlier than the other samples; the cancer
samples tend to be removed earlier than the normal and PIN samples.
The samples from the pin stage do not seem to be removed earlier
than the normal samples, which is in line with the non—signiﬁcance
of their comparison (Table 2). In addition, the Spearman’s rank
correlation coefﬁcient between the reconstructed order and cancer
stage equals —0.41, also indicating a strong relationship between
the two.

In the second example, we interpret entropy increase as
increased stochasticity. Higher variability of gene expression levels
among cancer samples (compared to normal samples) may be an
indication of different regulation. At the level of the pathway
(to which presented techniques can be directly applied), such
increased randomness in expression patterns may have functional
implications. To test this idea, consider the p53 signalling pathway,
which is an ideal test case, as its main function is to guard the
integrity of the genome (Weinberg, 2006). The p53 gene, central
to this pathway, is silenced in many cancers, causing loss of
function and leading to genomic instability (Weinberg, 2006). This
suggests that the p53 pathway should exhibit an entropy increase as
cancer progresses. Again the Kim dataset is used to investigate this.
Hereto the (k-NN) entropy of the pathway’s (deﬁned by the KEGG
repository, ID = 04115, Ogata et al., 1999) expression levels at
each cancer stage is estimated: H(norma1)= —8.44, H(Pin) = —5.10,
H(cancer) = —2.78 and H(metasmtj") = 5 .34. The p53 pathway entropy
increases monotonely with the cancer’s advancement. To assess
whether this increase is not due to chance, we assess the signiﬁcance
of this monotonic trend. A simple measure of monotonicity would
be the area under the entropy curve (when connecting the entropy
of the cancer stages), using H(nomlal) as a base line. Using this as
a test statistic and generating the null distribution by permutation

of the cancer stage labels, yields a P-value of 0.009 (using 1000
permutations).

4.4 Discussion

Analysis results depend to some extent on the normalization method
applied. It may therefore be argued that we should have preprocessed
the data with more than one normalization method. In fact, we
analyzed two different preprocessed version of the Singh dataset
(Type II), one preprocessed by the RMA approach of Irizarry et al.
(2003) and the other by the MAS approach of Affymetrix. This
yielded comparable signiﬁcant P-values. In addition, we re-analyzed
the Chitale dataset (Type I) with segmented (instead of normalized)
DNA copy number data, also leading to identical results. For
other datasets (when preprocessed differently, we expect similarly
consistent analysis results.

Perhaps more convincing, is the fact that, e. g. an entropy
difference among normal and cancer is observed over many
datasets, involving many different tissue types, generated on various
platforms and preprocessed with a diverse array of normalization
methods. This despite the fact that normalization aims to remove
differences between hybridizations, and is thus likely to obscure (at
least partially) the entropy signal. In addition, results are, within
tissue type, consistent over datasets (for Type II experiments, the
prostate cancer datasets Chandran, Kim and Singh reveal concordant
results; for Type I experiments, the results of the breast cancer
datasets Bergamaschi, Pollack and Zhang all agree).

In future, the data analyzed here (DNA copy number and gene
expression data) will be generated by means of massive parallel
sequencing (MPS). Apart from the improved resolution, such data
will be less noisy and normalization is expected to be less of a
nuisance for entropy comparison.

5 CONCLUSION

We provided a motivating statistical argument which suggests that
an increase in genomic entropy is reﬂected in the transcriptome.
Statistical methodology that facilitates the investigation of this
hypothesis through analysis of high-throughput DNA copy number
and gene expression data is presented. The methodology is illustrated
on a multitude of datasets from cancers of various tissues. In
addition, the results from analyses of these data suggest that the
hypothesis of a related genomic and transcriptomic entropy in cancer
has more than only face value.

Conﬂict of Interest: none declared.

REFERENCES

Blyth,C.R. (1986) Convolutions of Cauchy dist11'butions. Am. Math. Mon. , 93, 645—647.

Castro,M.A.A. et al. (2006) Chromosome aberrations in solid tumors have a stochastic
nature. Mutat. Res., 600, 150—164.

Cover,T.M. and Thomas,J.A. (2006) Elements of Information Theory, 2nd edn. John
Wiley, New York.

Fearon,E.R. and Vogelstein,B. (1990) A genetic model for colorectal tumon'genesis.
Cell, 61, 759—767.

Gatenby,R.A. and F11'eden,B.R. (2004) Information dynamics in carcinogenesis and
tumor growth. Mutat. Res., 568, 259—273.

Hanahan,D. and Weinberg,R. (2000) The halhnarks of cancer. Cell, 100, 57—70.

H6glund,M. et al. (2005) Statistical behavior of complex cancer karyotypes. Genes
Chromosomes Cancer, 42, 327—341.

 

562

112 Bro'SIBurnoprOJxo'sorteul10jurorq//:dnq wort pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

Analysis of the cancer cell’s molecular entropy

 

Irizarry,R.A. et al. (2003) Exploration, normalization, and summaries of high density
oligonucleotide array probe level data. Biostatistics, 4, 249—264.

Kaila,V.R.I. and Annila,A. (2008) Natural selection for least action. Proc. R. Soc. A,
464, 3055—3070.

Kauraniemi,P. and Kallioniemi,A. (2006) Activation of multiple cancer-associated
genes at the ERBB2 amplicon in breast cancer. Endocr: Relat. Cancer, 13, 39—49.

Kozachenko,L.F. and Leonenko,N.N. (1987) On statistical estimation of entropy of a
random vector. Problems Inform. Transm, 23, 95—101.

Kraskov,A. et al. (2004) Estimating mutual information. Phys. Rev. E, 69, 066138.

Leonenko,N. et al. (2008) Estimation of entropies and divergences via nearest
neighbors. Tatra Mountains Math. Publications, 39, 265—273.

Merlo,L.M.F. et al. (2006) Cancer as an evolutionary and ecological process. Nat. Rev.
Cancer, 6, 924—935.

Ogata,H. et al. (1999) KEGG: Kyoto Encyclopedia of Genes and Genomes. Nucleic
Acids Res., 27, 29—34.

Oja,H. (1981) On location, scale, skewness and kurtosis of univariate dist11'butions.
Scand. J. Stat, 8, 154—168.

Pollack,J.R. et al. (2002) Microarray analysis reveals a major direct role of DNA copy
number alteration in the transcn'ptional program of human breast tumors. Proc. Natl
Acad. Sci., 99, 12963—12968.

Ramsay,J.O. (1998) Estimating smooth monotone functions. J. R. Stat. Soc. Ser. B, 60,
365—375.

SchaferJ. and Stn'mmer,K. (2005) A shrinkage approach to large-scale covariance
matn'x estimation and implications for functional genomics. Stat. Appl. Genet. Mol.
Biol., 4, Article 32.

Shaked,M. and Shanthikumar,J.G (2007) Stochastic Orders and Their Applications.
Springer, New York.

Stratton,M.R. et al. (2009) The cancer genome. Nature, 458, 719—724.

Tsafrir,D. et al. (2006) Relationship of gene expression and chromosomal abnormalities
in colorectal cancer. Cancer Res., 66, 2129—2137.

Van de Wiel,M.A et al. (2007) CGHcall: calling aberrations for array CGH tumor
proﬁles. Bioinformatics, 23, 892—894.

Van Wien'ngen,W.N. and Van de Wiel,M.A. (2009) Nonparametric testing for DNA
copy number induced differential mRNA gene expression. Biometrics, 65, 19—29.

Van Wien'ngen,W.N. et al. (2007) Normalized, segmented or called aCGH data? Cancer
Inform, 3, 331—337.

Van Wie11'ngen,W.N. et al. (2010) A random coefﬁcients model for regional co-
expression associated with DNA copy number aberrations. Stat. Appl. Genet. Mol.
Biol., 9, Article 25: 1—28.

Wang,Q. et al. (2009) Divergence estimation for multidimensional densities via k-
nearest-neighbor distances. IEEE Trans. Inform. Theory, 55, 2392—2405.

Weinberg,R.A. (2006) The Biology of Cancer. Garland Science, New York.

Zhang,L. et al. (2006) MicroRNAs exhibit high frequency genomic alterations in human
cancer. Proc. Natl Acad. Sci. USA, 103, 9136—9141.

 

563

112 Bro'SIBurnoprOJxo'sortemJOJHIOIW/zdnq wort pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

