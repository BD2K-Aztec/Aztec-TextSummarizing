ORIGINAL PAPER

Vol. 27 no. 20 2011, pages 2812—2819
doi: 1 0. 1 093/bioinformatics/btr494

 

Sequence analysis

Advance Access publication August 27, 2011

SCLpred: protein subcellular localization prediction by N-to-1

neural networks

Catherine Mooney1’2’3’4

, Yong-Hong Wang5 and Gianluca Pollastri1’3’*

1School of Computer Science and Informatics, 2School of Medicine and Medical Science, 3Complex and Adaptive
Systems Laboratory, 4Conway Institute of Biomolecular and Biomedical Science, University College Dublin, Belfield,
Ireland and 5Biophysics Institute, Hebei University of Technology, Tianjin, China

Associate Editor: Burkhard Rost

 

ABSTRACT

Summary: Knowledge of the subcellular location of a protein
provides valuable information about its function and possible
interaction with other proteins. In the post-genomic era, fast and
accurate predictors of subcellular location are required if this
abundance of sequence data is to be fully exploited. We have
developed a subcellular localization predictor (SCLpred), which
predicts the location of a protein into four classes for animals and
fungi and five classes for plants (secreted, cytoplasm, nucleus,
mitochondrion and chloroplast) using machine learning models
trained on large non-redundant sets of protein sequences. The
algorithm powering SCLpred is a novel Neural Network (N-to-1
Neural Network, or N1 -NN) we have developed, which is capable of
mapping whole sequences into single properties (a functional class,
in this work) without resorting to predefined transformations, but
rather by adaptively compressing the sequence into a hidden feature
vector. We benchmark SCLpred against other publicly available
predictors using two benchmarks including a new subset of Swiss-
Prot Release 2010_06. We show that SCLpred surpasses the state of
the art. The N1 -NN algorithm is fully general and may be applied to a
host of problems of similar shape, that is, in which a whole sequence
needs to be mapped into a fixed-size array of properties, and the
adaptive compression it operates may shed light on the space of
protein sequences.

Availability: The predictive systems described in this article are
publicly available as a web server at http://distill.ucd.ie/distill/.
Contact: gianluca.pollastri@ucd.ie

Received on June 17, 2011; revised on August 5, 2011; accepted on
August 22, 2011

1 INTRODUCTION

With the recent advances in hi gh-throughput sequencing technology,
there has been a rapid increase in the availability of sequence
information. To fully exploit, this information sequences need to be
annotated quickly and accurately, which has led to the development
of automated annotation systems. A major step toward determining
the function of a protein is determining its subcellular localization
(SCL). Knowledge of the location of the protein sheds light not
only on where it might function but also what other proteins it
might interact with, as, in order to interact, proteins must inhabit

 

*To whom correspondence should be addressed.

the same location or physically adjacent compartments, at least
temporarily. There is a growing gap between the number of proteins
that have reliable SCL annotations and the number of known protein
sequences. Experimental approaches to SCL prediction are time-
consuming and expensive, whereas computational methods can
provide fast and increasingly accurate localization predictions.

There are various different mechanisms by which a protein
is directed to a particular location in the cell, and there are
many possible compartments in which eukaryotic proteins may be
located. Some nuclear proteins have a nuclear localization signal
(NLS), which may occur anywhere in the sequence (Cokol et al.,
2000). Most secreted, mitochondrial and chloroplastic proteins have
N—terminal cleavable peptides (SP, mTP and cTP), but many proteins
have no known motif (Emanuelsson, 2002; Nair and Rest, 2005),
and many are known not to have N—terminal peptides (Bendtsen
et al., 2004a). Even in these cases, the information contained in a
protein sequence may be sufﬁcient to predict the protein’s location
in the cell, given that residue and k—residue frequencies correlate
with locations (Emanuelsson, 2002; Nair and Rest, 2003, 2005;
Nakashima and Nishikawa, 1994).

There are many methods for the prediction of SCL that can
be roughly divided into two groups: homology or knowledge-
based, which rely on similarity to another sequence of known
location, or other known information about the sequence or similar
sequences, for example WOLF PSORT (Horton et al., 2007) or
SherLoc (Shatkay et al., 2007); and de novo or ab initio, sequence-
based methods, which may use evolutionary information in the form
of multiple sequence alignments (MSAs), but do not depend on
similarity to sequences of known location, for example BaCelLo
(Pierleoni et al., 2006).

We predict SCL for eukaryotes only, which we divide into
animals, plants and fungi. There are many potential classes of
subcellular localization, and different prediction systems sometimes
use different class subdivisions, ranging from 3 (Béden and
Hawkins, 2005; Emanuelsson et al., 2000; Hawkins and Béden,
2006) up to more than 10 classes (Horton et al., 2007). Here,
similarly to BaCelLo (Pierleoni et al., 2006), to which we directly
compare our results, we consider four subcellular localizations
for animals and fungi and ﬁve for plants: nucleus, cytoplasm,
mitochondrion, chloroplast and secreted. In a ﬁrst series of tests,
we adopt essentially the same experimental setting as in (Casadio
et al., 2008) and (Pierleoni et al., 2006), to which we compare our
predictor. We then take a further step by developing new, redundancy
reduced training and testing sets starting with Swiss-Prot Release

 

2812 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 [3.10'8112(1an[plOJXO'SODBIIIJOJIIlOlQ/[i(11111 IIIOJJ pepeorumoq

9IOZ ‘09 lsnﬁnV uo ::

Protein subcellular localization prediction

 

2010_06 (Boeckmann et al. , 2003) and benchmark SCLpred on
these sets against six state-Of-the-art, publicly available predictors Of
SCL: BaCelLO, LOCtree, SherLoc, Protein Prowler, TARGETp and
WOLF PSORT, which we brieﬂy describe in the following sections.

BaCelLo: BaCelLO (Pierleoni et al., 2006) uses a hierarchy
Of binary support vector machines (SVMs) tO predict SCL for
eukaryotes intO four classes for animals and fungi and ﬁve for
plants: secreted, cytoplasm, nucleus, mitochondrion and chloroplast.
BaCelLO is trained on a non-redundant set Of sequences from Swiss-
Prot 48. Predictions are made from the full sequence, from the
N— and C-terminal regions and evolutionary information. BaCelLO
is available at http://gpcr.biocomp.unibO.it/bacellO/.

LOCtree: LOCtree (Nair and Rost, 2005) uses binary SVMs tO
predict SCL. Three versions Of the predictor are available, for
plants, non-plants and prokaryotes. For prokaryotes, predictions are
dived intO three classes: secreted, periplasm and cytoplasm. For
eukaryotes, predictions are divided intO six classes: extracellular
space, nucleus, cytoplasm, chloroplast, mitochondrion and other
organelles. LOCtree is trained on a redundancy reduced subset
Of Swiss-Prot 40. Predictions are made from the full sequence,
a 50-residue N—terminal region, predicted secondary structure and
the output Of SIGNALp (for eukaryotes). LOCtree is available at
http://www.predictpr0tein.org/.

SherLoc: SherLOC (Shatkay et al., 2007) uses SVM that integrate
sequence and text-based features. There are three predictors (animal,
fungi, plant) which predict 10 locations for animals and fungi:
cytoplasm, endoplasmic reticulum, extracellular, Golgi, lysosome,
mitochondrion, nucleus, peroxisome, plasma membrane, vacuole
and an extra class, chloroplast, for plants. The predictors are
trained on sequences extracted from Swiss-Prot 42. http://www-
bs.informatik.uni-tuebingen.de/SerVices/SherLOCI.

TargetP: TargetP (Emanuelsson et al., 2000) uses a feed-forward
neural network for the prediction Of plant and non-plant SCL
intO three and four classes, respectively, based on the N—terminal
sequence. The prediction is based on the presence Of a chloroplast
transit peptide (cTP), a mitochondrial targeting peptide (mTP) or
a secretory pathway signal peptide (SP). TargetP is available at
http://www.cbs.dtu.dk/serVices/TargetP/.

Protein Prowler: Protein Prowler (BOden and Hawkins, 2005;
Hawkins and BOden, 2006) is based on the ideas behind TargetP and
trained on a subset Of Swiss-Prot 37 and 38. The predictor uses neural
networks and SVMs specialized for the prediction Of plants or non—
plants and predicts intO the following classes: secretory pathway,
mitochondrion, chloroplast and other. Protein Prowler is available
at http://ppr0wler.itee.uq.edu.au/.

WOLF PSORT: WOLF PSORT (Horton et al., 2007) is a version Of
the PSORT family Of SCL predictors for the prediction Of eukaryotic
proteins based on their sequence. Based on a number Of features
(residue composition, presence Of known sorting signal and target
peptides, etc.), WOLF PSORT uses a k-nearest neighbor classiﬁer,
comparing these features tO other Swiss-Prot—annotated proteins,
resulting in a ranked list Of up tO 12 possible locations: chloroplast,
cytOSOl, cytoskeleton, endoplasmic reticulum, extracellular, GOlgi

Table 1. Number Of sequences per class for each Of the three kingdoms in
the BaCelLO training set and the BaCelLO_2008 test set

 

BaCelLO training set BaCelLO_2008 test set

 

 

Animals Fungi Plants Animals Fungi Plants

 

Cytoplasm 439 211 58 846 331 102
Mitochondrion 188 188 67 241 104 38
Nucleus 1166 711 121 979 256 99
Secreted 804 88 41 722 26 18
Chloroplast 204 1345
Total 2597 1198 491 2788 717 1602

 

apparatus, lysosome, mitochondrion, nuclear, peroxisome, plasma
membrane and vacuOlar membrane. WOLF PSORT is available at
http://wolfps0rt.0rg/.

2 MATERIALS AND METHODS

2.1 Datasets

The ﬁrst dataset that we use tO train and test SCLpred is the dataset used
by Pierleoni et al. (2006) tO train BaCelLO in 10-fOld cross-validation, for a
direct comparison with this predictor. We call this set the BaCelLO training
set. We also test SCLpred on the test dataset used in CasadiO et al. (2008)
(BaCelLO_2008 test set), which is based on Swiss-Prot 54 (Table 1). The
BaCelLO_2008 test set is redundancy reduced excluding all sequences with
a BLAST hit (e:10‘3) tO the BaCelLO training set. Next, we create a new
training and test set starting from Swiss-Prot Release 2010_06. We start
from 97 939 Metazoa, 27 540 Fungi and 28 998 Viridiplantae sequences.
Of these 74 724, 20196 and 22 442, respectively, have a ‘SUBCELLULAR
LOCATION’. We remove membrane proteins and sequences that have non-
experimental qualiﬁers (Potential, Probable, By similarity), leaving 16406,
3339 and 7116 sequences, respectively. We internally redundancy reduce
each Of these sets using an all-against—all BLAST (Altschul et al., 1997)
search (with e = 10‘3) removing any sequence with a hit with >30% sequence
identity tO any other sequence in the set. All the sequences added tO Swiss-
Prot earlier than 2009 in the set are used as a training set (2010_06 training
set). Sequences added tO Swiss-Prot in 2009 or later are used for testing, as
these sequences have <30% sequence similarity tO any sequences used tO
train any Of the predictors tested in this article. We refer tO as the 2009+ test
set. Table 2 shows the number Of sequences per class for each Of the three
kingdoms in these new training (2010_06 training set) and test sets (2009+
test set).

The BaCelLO datasets are available on the BaCelLO website:
http://gpcr.biocomp.unibo.it/bacello/datasethtm and the SCLpred datasets
are available upon request from the authors.

MSAs are extracted from uniref90 (Suzek et al., 2007) from February
2010 containing 6 464 895 sequences. The alignments are generated by three
runs Of PSI-BLAST with parameters b=3000 (maximum number Of hits)
and e = 10‘3 (expectation Of a random hit).

2.2 Predictive architecture: Nl-NN

We call the model that we describe in this work N-tO-l Neural Network
or Nl-NN. The model is based and on our framework tO design Neural
Networks for structured data (Baldi and POllastri, 2003; Walsh et al.,
2009). The aim Of the model is tO map a sequence Of variable length
N into a single property or ﬁxed-width array Of properties. Other models
transform/compress the sequence into a ﬁxed number Of descriptors (or into
descriptors Of pairwise relations between sequences) beforehand, and they
then map these descriptors into the property Of interest. These descriptors are

 

2813

112 Bro'srcumoprOJXO'sor1cmrogurorq//:d11q urorr popcorn/hog

9IOZ ‘09 lsnﬁnV uo ::

C.Mooney et al.

 

Table 2. Number Of sequences per class for each Of the three kingdoms in
the 2010_06 training set and the 2009+ test set

 

2010_06 training set 2009+ test set

 

Animals Fungi Plants Animals Fungi Plants

 

Cytoplasm 1364 890 133 20 34 8
Mitochondrion 315 413 81 5 19 7
Nucleus 1830 1150 259 25 36 54
Secreted 1584 111 98 68 15 8
Chloroplast 523 29
Total 5095 2564 1094 118 104 106

 

typically frequencies Of residues or k-mers, sometimes computed separately
on different parts Of the sequence [e.g. around the termini, as in (Pierleoni
et al., 2006)]. In some cases whole sections Of the sequence are directly
taken into account (again, typically the termini, where some signals are tO
be found), but even in this case the size Of this section needs tO be ﬁxed and
decided beforehand.

In Nl-NN, instead, we dO not compress all the information Of a sequence
into a handful Of predeﬁned features (e.g. k-mer frequencies, sequence
length, etc.). Rather, we decide beforehand only how many features we
want tO compress a sequence into. If these features are stored in a vector
f = (fl , . . . ,fh), and if we represent the i-th residue in the sequence as r,, then
f is Obtained as:

N
f=kZN<h>(r.-_c, ...,r.-+.) (1)
i=1
where N01) is a non-linear function, which we implement by a two-layered
feed-forward Neural Network with h non-linear output units (the sequence-
tO-feature network). N01) is replicated N times (N being the sequence length),
and k is a normalization constant. The feature vector f is Obtained by
combining information coming from all windows Of 2c+1 residues in the
protein. If c = 20, as in all the tests in this article, the motifs have a length Of
41 residues. The feature vector f thus Obtained is mapped into the property
Of interest 0 (for instance, cellular component class), as follows:

0=N(")(f) (2)

where N(") is a non-linear function that we implement by a second two-
layered feed-forward neural network (the feature-tO-0utput network). The
whole neural network (the cascade Of N replicas Of the sequence-tO-feature
vector network and one feature-to-output network) is itself a feed-forward
neural network, and thus can be trained by gradient descent via the back-
propagation algorithm. As there are N copies Of N01) for a sequence Of length
N, there will be N contributions tO the gradient for this network, which are
added together. A graphical representation Of N-tO-l NN is shown in Fig. 1.

The feature vector f is a compression Of the sequence into h real-valued
descriptors. These descriptors are automatically determined/leamed in order
tO minimize the output error, hence tO be most informative tO predict the
property Of interest. Although there is a daunting number Of possible motifs
Of length 2c + 1, the model does not need tO count them or represent them all.
Only a relatively small number Of free parameters is available tO represent
all the motifs in a sequence. This prevents overparametrization and model
ﬁtting problems that arise when one counts frequencies Of n-mers as soon as
n > 2—3. If training is successful, only (soft) motifs relevant tO the task at
hand are represented inf. Thus, f is effectively a compressed version Of the
sequence into a ﬁxed-size array. The compression is property driven, meaning
that different predictive targets generally induce different representations Of
a sequence.

The number Of free parameters in the overall Nl-NN can be controlled by:
the number Of units in the hidden layer Of the sequence-tO-feature network
N01) 0, NH ; the number Of hidden units in the feature-tO-0utput network

   

QARHSVEPTGKLSNE IANQKEN * '1' SNKKRKTTVHKQKGS QEK'NSSIKYFIQKGRECQQIS QLGAKPQRTKLLYDE

Fig. 1. An N-tO-l Neural Network. N copies Of the N01) network (only
three represented for simplicity) process all the (overlapping) motifs Of a
predeﬁned length in a sequence. The vectorial outputs fk Of these networks
are added up, and the resulting feature vector f is input tO the N(") network
tO produce the localization prediction.

N(")(), N 51 ; the number Of hidden states in the feature vector f , which is also
the number Of output units in the sequence-tO-feature network, Nf. Given that
only one instance Of the sequence-tO-feature network (i.e. only one set Of free
parameters) is replicated for all positions in the sequence, and there is only
one feature-tO-0utput network, the overall number Of free parameters Np Of
the Nl-NN is:

N, =(N,-+1)N;1+(N;1+1)Nf+(Nf+1)Nf+(Nf+1)N0 (3)

where N,- is the size Of the input vector representing one residue (including
its context) and N0 is the number Of output classes. The number Of free
parameters can be controlled by NH, Nf and N51, while N0 is governed by
the property being predicted, and N,- depends on the input representation and,
importantly, by the size Of the motifs being considered [2c+1 in Equation
(1)]. The input at each residue is coded as a letter out Of an alphabet Of
25. Beside the 20 standard amino acids, B (aspartic acid or asparagine), U
(selenocysteine), X (unknown), Z (glutamic acid or glutamine) and . (gap)
are considered. The input presented tO the networks is the frequency Of each
Of the 24 non-gap symbols, plus the total frequency Of gaps in each column
Of the MSA.

Training: for each training experiment (i.e. training on the BaCelLO
training set and training on the 2010_06 training set), we implement three
predictors, one for each Of the three kingdoms Of animals, fungi and plants.
Each training is conducted by 10-fOld cross-validation, Le. 10 different sets
Of training runs are performed in which a different tenth Of the overall set
is reserved for testing. The 10 tenths are roughly equally sized, disjoint and
their union covers the whole set. For each training, the 9/10 Of the set that
are not reserved for testing are split into a validation set (1/10 Of the overall
set) and a proper training set. Given that some classes are far less numerous
than others, in order tO rebalance the training set we repeat the number Of
instances in the various classes until we have roughly the same number Of
examples in each Of them. Examples in the testing and validation sets are not
replicated. The training set is used tO learn the free parameters Of the network
by gradient descent, while the validation set is used tO monitor the training
process. For each different architecture, we run three trainings, which differ
only in the training versus validation split. Excluding different validation
sets ensures that the resulting models are different, which yields larger gains
when ensembling them.

During preliminary experiments (run on the BaCelLO plant training set
split into 2/3 for training and 1/3 for testing), we tested N51 values Of 6, 8 and
10, which all yielded similar performances. When choosing a motif size, we
considered that the average size for known signal peptides in eukaryotes is
~20 residues (Bendtsen et al., 2004b), and 35—40 is an upper size bound for
most known signals and NLS (Bendtsen et al., 2004b; COkOl et al., 2000).

 

2814

112 Bro's112umofp101xo'sor112u1101urorq/ﬁd11q 111011 popcorn/110g

9IOZ ‘09 lsnﬁnV uo ::

Protein subcellular localization prediction

 

It should be noted that, since all (overlapping) motifs Of length 2c+1 are
considered by an N-tO-l NN, it is not strictly necessary for 2c+1 tO cover
all motif sizes, because signal larger than 2c+1 is still input tO an N-tO-l
NN as all its overlapping substrings Of length 2c+1, although this may lead
tO the loss Of some positional information. During preliminary experiments,
we tested c values Of 10 and 15, which performed marginally less well than
c=20. We kept N151 and Nf ﬁxed at 10 in all experiments. During the ﬁnal
cross-validations, we use exactly the same architecture for all sets and all
kingdoms, in which NJ? =Nf 2N},LI :10 and c: 20.

All trainings are also identical in that the weights in the networks are
updated every 10 examples (proteins) and 2000 epochs Of training are
performed, which brings the training error tO near zerO in all cases. In all
cases, we save networks at epochs 1800, 1900 and 2000, ensemble average
them and evaluate them on the corresponding test set. Saving the models that
perform best on validation yields very similar results. The ﬁnal results for
each 10-fOld cross-validation (different kingdoms, BaCelLO and 2010_06
training sets) are the average Of the results on each test set. When testing on
an independent set from the one used during training (BaCelLO for training
and BaCelLO_2008 for testing, 2010_06 for training and 2009+ for testing),
we ensemble-combine all the models from all cross-validation fOlds Of the
best architecture.

Training is performed by gradient descent on the error, which we model
as the relative entropy between the target class and the output Of the network.
The overall output Of the network [output layer Of N(")()] is implemented as
a softmax function, while all internal squashing functions are implemented
as hyperbolic tangents. The examples are shufﬂed between epochs. We use
a momentum term Of 0.9. Although this does not signiﬁcantly affect the
ﬁnal results, it speeds up overall training times by a factor 10. The learning
rate is kept ﬁxed at 0.2 throughout the training. Training one model on a
state Of the art core tOOk between 8 h and 4 days, depending on the size Of
the training set. Predicting the localization Of an average-sized protein from
the sequence and MSA takes less than a second, in fact running BLAST tO
generate MSA is far costlier (minutes) than Obtaining the actual prediction
from an ensemble Of N-tO-l NN.

Evaluating performance: tO evaluate the performance Of SCLpred against
other predictors, we use the following global indices:

 

 

— 

(4)
where:

0 zij: the number Of sequences Of class i predicted tO be in class j.

0 eij: the number Of sequences Of class i expected tO be predicted in class
j by chance.

0 N: the number Of sequences.
0 K: the number Of classes.

TO measure performances for a given class i we use:

 

 

 

TP
Spec 2
TP+FP
TP
Sens =
TP+FN
FP
FPR =
FP+ TN

C _ TPxTN—FPXFN
_ \/(TP+FP)(TP+FN)(TN+FP)(TN+FN)

 

 

(5)

Table 3. Results for BaCelLO [from Pierleoni et al. (2006)] and SCLpred,
trained and tested in 10-fOld cross-validation on the BaCelLO training set
(Pierleoni et al., 2006), extracted from Swiss-Prot 48

 

SCLpred BaCelLO SCLpred BaCelLO SCLpred BaCelLO

 

Spec Sens Spec Sens Spec Sens Spec Sens Spec Sens Spec Sens

 

Animals Fungi Plants

 

Cth 0.68 0.79 0.76 0.73
CytO 0.58 0.54 0.41 0.65 0.46 0.39 0.39 0.60 0.39 0.36 0.47 0.52
MitO 0.77 0.74 0.66 0.76 0.72 0.78 0.72 0.81 0.49 0.34 0.54 0.51
Nucl 0.83 0.85 0.85 0.65 0.83 0.82 0.85 0.67 0.83 0.76 0.76 0.72
Secr 0.93 0.93 0.91 0.91 0.86 0.85 0.85 0.94 0.89 0.85 0.65 0.85

 

GC 0.72 0.67 0.67 0.66 0.63 0.59
Q 0.82 0.74 0.75 0.70 0.68 0.68

 

Deviations are :I:2 for both Q and GC for Plants, and :I:1 for Fungi and Animals.

where:

0 True positives (TP): Zﬁ.

- False positives (FP): 2.1-75%,}.

0 True negatives (TN): 21¢,- Zjﬂzjv.
- False negatives (FN): 2.1-751217.

We emphasize performances based on GC [see Baldi et al. (2000) for
more details], as this index minimizes the effect Of class sizes. For some
Of the experiments, we extract performances Of other predictors from the
literature, hence not all indices are reported at all times.

3 RESULTS AND DISCUSSION

In previous tests, BaCelLO (Pierleoni et al., 2006) was shown
tO outperform the fOllowing publicly available methOds for the
prediction Of the subcellular localization: LOCtree (Nair and Rost,
2005), PSORT II (Nakai and Horton, 1999), SubLOC (Hua and Sun,
2001), ESLpred (Bhasin and Raghava, 2004), LOCSVMpsi (Xie
et al., 2005), SLP-local (Matsuda et al., 2005), Protein Prowler
(BOden and Hawkins, 2005), TARGETp (Emanuelsson et al.,
2000), PredOTar (Small et al., 2004) and pTARGET (Guda and
Subramaniam, 2005).

In Table 3, we show the performance Of SCLpred compared with
BaCelLO on the BaCelLO training set (Pierleoni et al., 2006). Both
predictors are assessed by 10-fOld cross-validation on the same set.
Overall SCLpred is far more accurate for animals (Q 82% versus
74% and GC 72% versus 67%) and fungi (Q 75% versus 70%
and GC 67% versus 66%) while the accuracy for plants (Q) is
the same (68%), but GC is still considerably higher for SCLpred
(63% versus 59%).

Table 4 shows the accuracy Of the same version Of SCLpred
tested on the BaCelLO_2008 test dataset from CasadiO et al. (2008)
compared with the other ﬁve SCL predictors tested on the same
dataset [results from CasadiO et al. (2008)]. Notice that two Of the
predictors (Protein Prowler and TARGETp) use a different class
assignment (‘easier’ as comprised by fewer classes) and are thus
not directly comparable tO SCLpred. The results refer tO versions Of
the various predictors that were trained on datasets extracted from
Swiss-Prot release 48 or earlier. Since the BaCelLO_2008 test set

 

2815

112 Bro's112umofp101xo'sor112u1101urorq/ﬁd11q 111011 pepeorumoq

9IOZ ‘09 lsnﬁnV uo ::

C.Mooney et al.

 

Table 4. Results for SCLpred, trained on the BaCelLO training set from Swiss-Prot 48 (Pierleoni et al., 2006), compared with BaCelLO (Pierleoni et al., 2006),
LOCtree (Nair and Rost, 2005), WOLF PSORT (Horton et al., 2007), Protein Prowler (Hawkins and BOden, 2006) and TARGETp (Emanuelsson et al., 2000)

 

 

 

 

 

 

 

 

 

SCLpred BaCelLO LOCtree Protein Prowler TARGETp WOLF PSORT

Sens MCC FPR Sens MCC FPR Sens MCC FPR Sens MCC FPR Sens MCC FPR Sens MCC FPR
Animals
CytO 0.68 0.68 0.05 0.72 0.55 0.15 0.67 0.60 0.09 0.65 0.59 0.09
MitO 0.86 0.83 0.02 0.90 0.68 0.07 0.79 0.73 0.03 0.47 0.62 0.01 0.69 0.52 0.07 0.79 0.71 0.04
Nucl 0.92 0.78 0.12 0.62 0.58 0.08 0.80 0.65 0.14 0.84 0.70 0.13
Secr 0.96 0.92 0.03 0.93 0.89 0.04 0.90 0.85 0.05 0.82 0.86 0.01 0.83 0.86 0.02 0.92 0.89 0.02
Other 0.98 0.77 0.26 0.89 0.69 0.19
GC 0.81 0.70 0.72 0.74 0.7 0.75
Q 0.85 0.75 0.78 0.89 0.86 0.81
Fungi
CytO 0.39 0.37 0.04 0.45 0.33 0.15 0.49 0.34 0.17 0.27 0.35 0.03
MitO 0.72 0.48 0.09 0.80 0.53 0.15 0.49 0.47 0.06 0.35 0.53 0.00 0.50 0.34 0.10 0.66 0.51 0.09
Nucl 0.85 0.49 0.43 0.66 0.39 0.26 0.68 0.34 0.32 0.91 0.43 0.48
Secr 0.85 0.74 0.02 1.00 0.75 0.03 0.93 0.51 0.08 0.89 0.78 0.01 0.96 0.66 0.03 0.96 0.81 0.02
Other 0.98 0.59 0.52 0.87 0.43 0.35
GC 0.57 0.56 0.44 0.67 0.53 0.59
Q 0.60 0.59 0.57 0.89 0.84 0.5 8
Plants
Cth 0.78 0.42 0.25 0.79 0.48 0.19 0.48 0.26 0.13 0.07 0.03 0.05 0.14 0.06 0.09 0.15 0.01 0.16
CytO 0.63 0.63 0.02 0.43 0.32 0.06 0.78 0.52 0.08 0.78 0.17 0.42
MitO 0.37 0.09 0.15 0.29 0.53 0.00 0.55 0.11 0.24 0.71 0.13 0.32 0.66 0.14 0.26 0.50 0.30 0.05
Nucl 0.79 0.82 0.01 0.84 0.48 0.11 0.80 0.41 0.14 0.77 0.26 0.27
Secr 0.83 0.48 0.02 0.94 0.42 0.04 0.83 0.56 0.02 0.79 0.32 0.06 0.83 0.35 0.05 0.33 0.15 0.04

Other

0.83 0.23 0.49 0.83 0.22 0.50

 

GC 0.58 0.46 0.44
Q 0.76 0.76 52

0.24 0.25 0.25
0.19 0.24 0.24

 

Tested on the BaCelLO_2008 test set (see text). Results for the predictors other than SCLpred from Casadio et al. (2008). Results in italics are for predictors using a fewer classes,
hence not directly comparable tO SCLpred. For these predictors ‘Other’ is the class Of proteins that cannot be classiﬁed as mitochondrion, secreted or chloroplast based on the
presence Of a known SP, mTP or cTP. Deviations are :I:2 for both GC and Q for Fungi and :I:1 for Plant and Animal. These are for our predictor (SCLpred). We have no access tO
the raw data for the other predictors as these are Obtained from the literature and were not reported.

is extracted from Swiss-Prot release 54 and redundancy reduced
against Swiss-Prot 48, there is nO signiﬁcant overlap between the
training sets Of any predictors in the table and the BaCelLO_2008
test set. For animals we Obtain a Q Of 85% and GC Of 81%, higher
than the second best predictor that is directly comparable (WOLF
PSORT, with 81 and 75%, respectively). SCLpred also performs
better than the two predictors that are not directly comparable on
the two classes that are common (mitochondrion and secreted). On
fungi, SCLpred has the best Q (60% versus BaCelLO’s 59%) and
the second best GC (57% versus WOLF PSORT’s 59%). On plants,
SCLpred has by far the highest GC (58% versus BaCelLO’s 46%)
and the jOint highest Q (76%, again with BaCelLO).

It should be noted that BaCelLO was Optimized for balanced
class accuracies (Pierleoni et al., 2006), that is, tO maximize
average class sensitivity (nQ measure). Based on nQ, SCLpred still
outperforms BaCelLO on both the BaCelLO and BaCelLO_2008 set
for animal proteins (by 2.3 and 6.2%, respectively), BaCelLO fares

better on fungi (by 4.5 and 2%), while on plants BaCelLO does
better on the BaCelLO training set (by 4.6%) and SCLpred on the
BaCelLO_2008 test set (by 2.2%). Overall BaCelLO shows a more
balanced sensitivity across classes than SCLpred, although in the
case Of animal proteins this is at a lower average level.

We repeat the experiments on a new training set extracted from
the 2010_06 release Of Swiss-Prot, which is approximately twice
the size Of the BaCelLO set for all three kingdoms. The accuracy Of
this new version Of SCLpred is shown in Table 5. On animal and
fungi, overall performances are lower, in absolute value, tO those
Obtained on the BaCelLO set. We attribute this tO the more balanced
nature Of the 2010_06 training set, which is thus intrinsically
‘harder’. Assigning proteins randomly tO classes with a probability
proportional tO class frequencies yields a Q measure 3% higher on
the BaCelLO animal training set than on the 2010_06 set (33.1%
versus 30.1%) and 6.3% higher on fungi (41.3% versus 35.0%).
Always predicting the most numerous class yields a 9% higher Q

 

2816

112 Bro's112umofp101xo'sor112u1101urorq/ﬁd11q 111011 pepeorumoq

9IOZ ‘09 lsnﬁnV uo ::

Protein subcellular localization prediction

 

Table 5. SCLpred, trained and tested in 10-fOld cross-validation on the
2010_06 training set

 

Animals Fungi Plants

 

Spec Sens MCC Spec Sens MCC Spec Sens MCC

 

Cth 0.74 0.83 0.56
CytO 0.62 0.65 0.50 0.5 8 0.57 0.35 0.42 0.29 0.27
MitO 0.73 0.59 0.64 0.69 0.62 0.59 0.23 0.14 0.13
Nucl 0.76 0.76 0.62 0.72 0.75 0.51 0.84 0.83 0.78
Secr 0.91 0.91 0.87 0.83 0.84 0.82 0.72 0.76 0.70

 

GC 0.68 0.63 0.5 8
Q 0.77 0.67 0.71

 

on the BaCelLO set compared with 2010_06 for animals (44.9%
versus 35.9%) and 14.4% higher for fungi (59.3% versus 44.9%).
Moreover, in both kingdoms the class which is overrepresented
in the 2010_06 set compared with BaCelLO is cytOplasm (26.8%
versus 16.9% Of the examples for animal, 34.7% versus 17.6% Of
the examples for fungi), which in all out tests is the hardest tO
predict. Hence not only is 2010_06 more challenging because Of
its distribution Of examples, but also because it contains a higher
proportion Of difﬁcult instances. On plants, Q is higher on the
2010_06 training than on the BaCelLO training set (71% versus
68%) while GC is lower (58% versus 63%). This is the result Of a
larger chloroplast class (which is well predicted) in 2010_06, and Of
the mitochondrion class being only 7% Of the 2010_06 set (versus
14% in BaCelLO), which results in infrequent predictions for this
class. While the improvement on the much larger chloroplast class
dominates in terms Of Q measure, the reduction Of performances
on mitochondrion dominates with respect tO GC, which weighs all
classes equally. Overall it should be noted that, because Of different
class composition, it is hard tO compare Q and GC measures across
different datasets, and different predictors should always be ranked
on the same dataset, as we dO throughout this article.

We then test the version Of SCLpred trained on the 2010_06 set
on the 2009+ dataset (a subset Of Swiss-Prot 2010_06 with <30%
sequence similarity tO the training set, described in Section 2.1).
We compare its accuracy with BaCelLO, SherLoc, WOLF PSORT,
Protein Prowler and TARGETp (Table 6).

Results for TARGETp and Protein Prowler are based on three
class predictions for animals and fungi, and four for plants, whereas
for WOLF PSORT and SherLOC prediction is possible for more
four/ ﬁve classes. For WOLF PS ORT, we count any proteins predicted
as ‘vacu’, ‘lyso’, ‘E.R.’, ‘gOlg’ or ‘plas’ as secreted, and any ‘cytO’,
‘cysk’, ‘cytO_nucl’ as cytoplasmic and any ‘nucl’ or ‘cytO_nucl’ as
nuclear. For SherLoc, any sequences predicted as ‘extracellular’,
‘ER’, ‘vacuOlar’, ‘peroxisomal’, ‘GOlgi’ or ‘plasma’ are counted as
secreted.

On 2009+, SCLpred again performs best Of all predictors. On
animals, Q is 89%, more than 20% better than the second best
directly comparable predictor (B aCelLO, with 66.3%), and over 10%
better than predictors using one less class. GC, at 79%, is also 10%
higher than BaCelLO, and higher than that Of the two predictors
with one less class. On fungi, both Q and GC (72% and 69%) are
the highest Of all four class predictors, and similar tO those Obtained

by the three class predictors. On plants, again Q (at 80%) is by far
the highest (SherLoc in this case being the second best ﬁve class
predictor at 68%), and GC (66%) is at least 9% higher than all other
ﬁve class predictors, and only lower than Protein Prowler’s (69%)
which tackles the simpler four class prOblem. In this case, SCLpred
also outperforms BaCelLO by nQ on all three kingdoms.

4 CONCLUSION AND FUTURE WORK

As the amount Of sequence information churned out by experimental
methOds keeps expanding at an ever-increasing pace, it is crucial
tO develOp and make available fast and accurate computational
methOds tO make sense Of it. SCL prediction is a step toward
bridging the gap between a protein sequence and the protein’s
function and can provide information about potential protein—protein
interactions and insight intO possible drug targets and disease
processes. As different SCL predictors are specialized for prediction
intO different classes and number Of classes, and as some predictors
are more accurate than others at prediction intO any one class,
this information can be explOited tO lead tO more accurate overall
consensus predictions, especially if the predictors are diverse in their
behavior.

In this article, we have developed a new methOd for SCL
prediction (SCLpred) based on a novel Neural Network architecture
(N1-NN). The architecture can map a sequence Of any length
mm a set Of individual properties for the whOle sequence. We
have developed three kingdom speciﬁc predictors for animals,
fungi and plants and predict intO four classes for animals and
fungi (nucleus, cytoplasm, mitochondrion and the secreted) and
an additional ﬁfth class for plants (chloroplast). We have trained
SCLpred in 10-fOld cross-validation on large non-redundant subsets
Of annotated proteins from Swiss-Prot 2010_06 and benchmarked
it against ﬁve other state-Of-the-art SCL prediction servers on an
independent set Of recently annotated proteins. SCLpred performs
favorably on these benchmarks, Often by consistent margins, and
we expect that its prediction accuracy will continue tO improve
with frequent retrainings tO take advantage Of larger, more diverse,
datasets Of annotated proteins as they become available, and as our
understanding Of the underlying biological mechanisms improves.
We expect larger datasets tO be especially beneﬁcial tO our mOdels, as
these incorporate information from the whOle sequence and normally
have a higher number Of free parameters than the alternatives.

In this work, we have used the primary sequence and multiple
sequence alignments as inputs tO the network. Additional residue-
level information may be included, such as predicted secondary
structure, solvent accessibility, location Of binding sites, etc.
Incorporating diverse information intO the input tO SCLpred is one Of
our future directions Of investigation, as is the inclusion Of putative
homology tO ‘templates’ or proteins Of known localization/ structure
[e. g. by techniques similar tO those in Mooney and POllastri (2009)].
In this work, we predict subcellular localizations intO a small
number Of classes (four for animal and fungi, ﬁve for plants), tO
allow the comparison Of our novel algorithms against a a number
Of existing predictors, and direct comparison against BaCelLO in
particular, which has been shown as one Of the best-performing
ab initio systems tO date. We are currently testing our methOds on
a wider set Of localization classes, as well as different functional
tasks. A further direction Of research is studying the space Of f
vectors (i.e. compressed, property-driven representations Of whOle

 

2817

112 Bro's112umofp101xo'sor112u1101urorq/ﬁd11q 111011 pepeorumoq

9IOZ ‘09 lsnﬁnV uo ::

C.Mooney et al.

 

Table 6. Results for SCLpred, trained on the 2010_06 set, compared with BaCelLO (Pierleoni et al., 2006), SherLoc (Shatkay et al., 2007), WOLF PSORT
(Horton et al., 2007), Protein Prowler (Hawkins and BOden, 2006) and TARGETp (Emanuelsson et al., 2000)

 

SCLpred BaCelLO LOCtree

SherLoc

Protein Prowler TARGETp WOLF PSORT

 

Spec Sens MCC FPR Spec Sens MCC FPR Spec Sens MCC FPR Spec Sens

MCC FPR Spec Sens MCC FPR Spec Sens MCC FPR Spec Sens MCC FPR

 

Animal
CytO 0.76 0.65 0.65 0.01 0.40 0.30 0.23 0.09 0.28 0.25 0.12 0.13 0.20
MitO 1.00 0.60 0.77 0.01 1.00 0.60 0.77 0.00 0.33 0.60 0.42 0.05 0.75
Nucl 0.72 0.92 0.76 0.09 0.62 0.84 0.63 0.14 0.49 0.68 0.44 0.19 0.63
Secr 1.00 0.97 0.97 0.02 0.95 0.91 0.85 0.06 0.96 0.79 0.75 0.04 0.90
Other

0.35
0.60
0.76
0.65

0.05 0.29 0.48 0.50 0.38 0.11

0.66 0.01 1.00 0.80 0.89 0.00 0.36 0.80 0.51 0.06 0.50 0.40 0.43 0.02

0.60 0.12 0.69 0.80 0.67 0.10

0.55 0.10 0.98 0.60 0.60 0.02 1.00 0.65 0.66 0.00 0.94 0.88 0.80 0.08
0.61 0.96 0.57 0.38 0.60 0.84 0.49 0.34

 

 

GC 0.79 0.69 0.58 0.59 0.75 0.58 0.60

Q 0.89 0.66 0.51 0.59 0.77 0.76 0.65

Fungi

CytO 0.58 0.65 0.41 0.11 0.57 0.50 0.33 0.19 0.71 0.29 0.33 0.06 0.50 0.32 0.19 0.16 0.60 0.08 0.13 0.03
MitO 0.79 0.79 0.74 0.04 0.71 0.89 0.75 0.08 0.42 0.53 0.33 0.16 0.71 0.53 0.54 0.05 0.89 0.42 0.56 0.01 0.61 0.58 0.51 0.08 0.71 0.79 0.69 0.7
Nucl 0.77 0.75 0.64 0.06 0.64 0.64 0.45 0.19 0.70 0.89 0.65 0.21 0.66 0.86 0.60 0.24 0.54 0.94 0.50 0.43
Secr 0.92 0.73 0.79 0.01 0.86 0.80 0.80 0.02 0.60 0.80 0.63 0.09 0.52 0.73 0.54 0.11 0.80 0.80 0.77 0.03 0.86 0.80 0.80 0.02 0.73 0.73 0.69 0.04

Other

0.82 0.93 0.57 0.41 0.86 0.89 0.60 0.29

 

 

GC 0.69 0.66 0.63 0.52 0.67 0.63 0.58
Q 0.72 0.71 0.53 0.61 0.72 0.73 0.64
Plants

Cth 0.82 0.93 0.82 0.08 0.63 0.69 0.52 0.16 0.50 0.45 0.29 0.17 0.82
CytO 0.30 0.38 0.27 0.02 0.75 0.38 0.51 0.01 0.37 0.50 0.37 0.07 0.20
MitO 0.50 0.29 0.35 0.00 0.00 0.00—0.03 0.01 0.14 0.29 0.12 0.12 0.43
Nucl 0.89 0.87 0.75 0.13 0.80 0.91 0.68 0.23 0.87 0.74 0.63 0.12 0.95
Secr 1.00 0.75 0.86 0.00 0.63 0.63 0.59 0.03 0.44 0.50 0.43 0.05 0.42
Other

0.31
0.50
0.86
0.74
1.00

0.42 0.03 1.00 0.41 0.58 0.00 0.65 0.52 0.45 0.10 0.48 0.48 0.29 0.19

0.23 0.16 0.35 0.75 0.46 0.11

0.57 0.08 0.23 0.86 0.38 0.20 0.29 0.57 0.35 0.10 0.50 0.29 0.35 0.02

0.72 0.04 0.88 0.83 0.72 0.12

0.61 0.11 1.00 0.88 0.93 0.00 0.88 0.88 0.86 0.01 0.60 0.38 0.44 0.02
0.87 0.84 0.65 0.18 0.85 0.84 0.63 0.20

 

GC 0.66 0.54 0.49 0.57
Q 0.80 0.52 0.45 0.68

0.69 0.62 0.50
0.75 0.70 0.55

 

Tested on the 2009+ set. Results in italics are for predictors using fewer Of classes, hence not directly comparable tO SCLpred. For these predictors ’Other’ is the class Of proteins
that cannot be classiﬁed as mitochondrion, secreted or chloroplast based on the presence Of a known SP, mTP or cTP. Deviations for both GC and Q are :I:4 for Plant and Fungi and

:I:3 for Animal.

proteins as ﬁxed-size arrays) induced by different output targets
(functional classes, protein folds/families), tO determine whether
they are satisfactory representations toward protein comparison, and
whether they yield insights intO the structure Of the protein space.

SCLpred is available as part Of our web servers for protein
sequence annotation. Up tO 32 768 residues can be handled in a
single submission. The servers are freely available for academic
users at http://distill.ucd.ie/distill/. Predictions are Obtained by an
ensemble Of all mOdels trained on the 2010_06 training set (as
in Table 6). Linux binaries and the benchmarking sets are freely
available for academic users upon request.

ACKNOWLEDGEMENTS

We thank the authors Of BaCelLO for making their datasets
publicly available, Dr Andrea Pierleoni for assistance with the
BaCelLO predictions and Tatyana Goldberg for providing LOCtree
predictions. We wish tO acknowledge UCD IT Services for the
provision Of computational facilities and support.

Funding: Science Foundation Ireland Grant (08/IN.1/B1864 tO
C.M.); SFI RFP grant (10/RFP/GEN2749 to GP).

Conﬂict of Interest: none declared.

REFERENCES

Altschul,S. et al. (1997) Gapped BLAST and PSI-BLAST: a new generation Of protein
database search programs. Nucleic Acids Res., 25, 3389—3402.

Baldi,P. and POllastri,G (2003) The principled design Of large-scale recursive neural
network architectures — DAG-RNNs and the protein structure prediction problem.
J. Mach. Learn. Res., 4, 575—602.

Baldi,P. et al. (2000) Assessing the accuracy Of prediction algorithms for classiﬁcation:
an overview. Bioinformatics, 16, 412—424.

Bendtsen,J. et al. (2004a) Feature-based prediction Of non-classical and leaderless
protein secretion. Protein Eng, 17, 349—356.

Bendtsen,J. et al. (2004b) Improved prediction Of signal peptides: Signalp 3.0. J. Mol.
Biol, 340, 783—795.

Bhasin,M. and Raghava,G (2004) ESLpred: SVM-based method for subcellular
localization Of eukaryotic proteins using dipeptide composition and PSI-BLAST.
Nucleic Acids Res., 32, W414—W419.

BOden,M. and Hawkins,J. (2005) Prediction Of subcellular localization using sequence-
biased recurrent networks. Bioinformatics, 21, 2279—2286.

Boeckmann,B. et al. (2003) The Swiss-Prot protein knowledgebase and its supplement
TrEMBL in 2003. Nucleic Acids Res., 31, 365—370.

CasadiO,R. et al. (2008) The prediction Of protein subcellular localization from
sequence: a shortcut tO functional genome annotation. Brief. F unct. Genomic
Proteomic, 7, 63—73.

COkOl,M. et al. (2000) Finding nuclear localization signals. EMBO Reports, 1, 411—415.

 

2818

112 Bro's112umofp101xo'sor112u1101urorq/ﬁd11q 111011 popcorn/hog

9IOZ ‘09 lsnﬁnV uo ::

Protein subcellular localization prediction

 

Emanuelsson,0. et al. (2000) Predicting subcellular localization Of proteins based on
their N-terminal amino acid sequence. J. Mol. Biol., 300, 1005—1016.

Emanuelsson,0. (2002) Predicting protein subcellular localisation from amino acid
sequence information. Brief. Bioinform, 3, 361—376.

Guda,C. and Subramaniam,S. (2005) pTARGET: a new method for predicting protein
subcellular localization in eukaryotes. Bioinformatics, 21, 3963—3969.

Hawkins,J. and BOden,M. (2006) Detecting and sorting targeting peptides with recurrent
networks and support vector machines. J. Bioinformatics Comput. Biol., 4, 1—18.

Horton,P. et al. (2007) WOLF PSORszrotein localization predictor. Nucleic Acids Res.,
35, W585—W587.

Hua,S. and Sun,Z. (2001) Support vector machine approach for protein subcellular
localization prediction. Bioinformatics, 17, 721—728.

Matsuda,S. et al. (2005) A novel representation Of protein sequences for prediction Of
subcellular location using support vector machines. Protein Sci, 14, 2804—2813.

Mooney,C. and POllastri,G. (2009) Beyond the twilight zone: Automated prediction Of
structural properties Of proteins by recursive neural networks and remote homology
information. Proteins, 77, 181—190.

Nair,R. and Rost,B. (2003) Better prediction Of sub-cellular localization by combining
evolutionary and structural information. Proteins, 53, 917—930.

Nair,R. and Rost,B. (2005) Mimicking cellular sorting improves prediction Of
subcellular localization. J. Mol. Biol., 348, 85—100.

Nakai,K. and Horton,P. (1999) PSORT: a program for detecting the sorting signals
Of proteins and predicting their subcellular localization. Trends Biochem. Sci., 24,
34—35.

Nakashirna,H. and Nishikawa,K. (1994) Discrimination Of intracellular and
extracellular proteins using amino acid composition and residue-pair frequencies.
J. Mol. Biol., 238, 54—61.

Pierleoni,A. et al. (2006) BaCelLO: a balanced subcellular localization predictor.
Bioinformatics, 422, 408—416.

Shatkay,H. et al. (2007) Sherloc: high-accuracy prediction Of protein subcellular
localization by integrating text and protein sequence data. Bioinformatics, 23,
1410—1417.

Small,I. et al. (2004) Predotar: a tOOl for rapidly screening proteomes for N-terminal
targeting sequences. Proteomics, 6, 1581—1590.

Suzek,B. et al. (2007) Uniref: comprehensive and non-redundant uniprot reference
clusters. Bioinformatics, 23, 1282—1288.

Walsh,I. et al. (2009) Recursive neural networks for undirected graphs for learning
molecular endpoints. In Pattern Recognition in Bioinformatics, Vol. 5780 Of Lecture
Notes in Bioinformatics. Springer, Berlin/Heidelberg.

Xie,D. et al. (2005) LOCSVMPSI: a web server for subcellular localization Of
eukaryotic proteins using SVM and proﬁle Of PSI-BLAST. Nucleic Acids Res.,
33, W105—W110.

 

2819

112 Bro's112umofp101xo'sor112u1101urorq/ﬁd11q 111011 popcorn/hog

9IOZ ‘09 lsnﬁnV uo ::

