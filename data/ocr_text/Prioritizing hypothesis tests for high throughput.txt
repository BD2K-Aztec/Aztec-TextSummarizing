Bioinformatics, 32(6), 2016, 850—858

doi: 10.1093/bioinformatics/btv608

Advance Access Publication Date: 16 November 2015
Original Paper

 

 

Genetic and population analysis

Prioritizing hypothesis tests for high
throughput data

Sangjin Kim and Paul Schliekelman*

Department of Statistics, University of Georgia, Athens, GA 30602, USA

*To whom correspondence should be addressed.
Associate Editor: Alfonso Valencia

Received on December 22, 2014; revised on September 11, 2015; accepted on October 16, 2015

Abstract

Motivation: The advent of high throughput data has led to a massive increase in the number of
hypothesis tests conducted in many types of biological studies and a concomitant increase in stringency
of significance thresholds. Filtering methods, which use independent information to eliminate less prom-
ising tests and thus reduce multiple testing, have been widely and successfully applied. However, key
questions remain about how to best apply them: When is filtering beneficial and when is it detrimental?
How good does the independent information need to be in order for filtering to be effective? How should
one choose the filter cutoff that separates tests that pass the filter from those that don’t?

Result: We quantify the effect of the quality of the filter information, the filter cutoff and other factors
on the effectiveness of the filter and show a number of results: If the filter has a high probability (e.g.
70%) of ranking true positive features highly (e.g. top 10%), then filtering can lead to dramatic increase
(e.g. 10-fold) in discovery probability when there is high redundancy in information between hypoth-
esis tests. Filtering is less effective when there is low redundancy between hypothesis tests and its
benefit decreases rapidly as the quality of the filter information decreases. Furthermore, the outcome
is highly dependent on the choice of filter cutoff. Choosing the cutoff without reference to the data will
often lead to a large loss in discovery probability. However, naive optimization of the cutoff using the
data will lead to inflated type I error. We introduce a data-based method for choosing the cutoff that
maintains control of the family-wise error rate via a correction factor to the significance threshold.
Application of this approach offers as much as a several-fold advantage in discovery probability rela-
tive to no filtering, while maintaining type I error control. We also introduce a closely related method
of P-value weighting that further improves performance.

Availability and implementation: R code for calculating the correction factor is available at http://
www.stat.uga.edu/people/faculty/paul-schliekelman.

Contact: pdschlie@stat.uga.edu

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

A dominant trend in biology in recent years has been the develop-
ment of high throughput techniques and the dramatic increase in the
resolution of available data. However, most of the information
gained is not relevant for any particular question at hand and comes
at the cost of more hypothesis tests and thus more stringent statis-
tical thresholds. There is often high redundancy between tests and
the gain in information may be slower than the increase in

resolution. Thus, higher resolution will not always lead to higher
probability of discovery.

Given the realities of multiple testing, it is unlikely that a mere
increase in throughput and resolution will greatly increase discov-
eries. Rather, it will be necessary to combine high throughput data
with other sources of information in order to better target investiga-
tions. The problems of multiple testing are well understood and
many methods have been proposed for using external information

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 850

9mg ‘09 isnﬁnV uo seleﬁuV socl ‘erulomeg JO AirSJQAru [1 112 ﬂJO'sleumo[pJOJXO'sopeuuogurorq/ﬁdnq IIIOJJ pepeolumoq

Prioritizing hypothesis tests for high-throughput data

851

 

to filter for the most promising features of the data. Such methods
typically have two stages: first, some filtering criterion is used to se-
lect the most promising features. Then, only those features are tested
for the effect of interest. These include methods for microarrays
(Bourgon, et al., 2010; Clevert, et al., 2013; Hackstadt and Hess,
2009; Jiang and Doerge, 2006; Lu, et al., 2011; McClintick and
Edenberg, 2006; Talloen, et al., 2007; Talloen, et al., 2010), RNA-
Seq (Bottomly, et al., 2011; Ramskold, et al., 2009; Rau, et al.,
2013a, b; Sultan, et al., 2008), genome-wide association studies
(Calle, et al., 2008; Dai, et al., 2012; Degnan, et al., 2008; Li, et al.,
2013; Patwardhan, et al., 2014; Roeder and Wasserman, 2009; Van
Steen, et al., 2005) and epistasis (Emily, et al., 2009; Evans, et al.,
2006; Pattin and Moore, 2008; Yang, et al., 2009).

Despite the popularity of filtering methods, key questions remain
about their general statistical properties. Bourgon et al. (2010) dis-
cussed the conditions sufficient for maintaining Type I error control
and showed that the key requirement is that the null hypothesis dis-
tribution of the test statistics after filtering should be the same as the
null hypothesis distribution before filtering. They showed that some
filtering techniques in use can violate this requirement. Their focus
was on the conditions for filtering to be valid. Little is known about
the conditions required for filtering to be successful in significantly
increasing discovery probabilities, and our focus is on this question.

In this paper, we address two major issues. First, we evaluate the
usefulness of filtering and determine major factors affecting its be-
havior. We quantify the effect of the filtering statistic in terms of its
probability of ranking true effects highly. We show that a strong fil-
ter that has a high probability of ranking true positives in e.g. the
top 10% can greatly increase discovery probability (as much as
20-fold) when there is high redundancy between features and when
a good cutoff is known in advance. Even a random filter can in-
crease power when the cutoff point is well chosen. On the other
hand, filtering is less effective when there is low redundancy be-
tween features and if the filter statistic is not able to reliably rank
true positive features highly.

Second, most applications of filtering methods have used ad-hoc
approaches to choosing the filter cutoff point. The filter cutoff point
refers to the value of the filtering criterion which separates features
that will be included in the second stage and those which will not.
We show that the filter cutoff has a large effect on the performance
of filtering methods. A good choice can make a several-fold differ-
ence in discovery probability relative to a poor choice. Furthermore,
inappropriate ad-hoc methods can greatly inflate false positive rates.
We introduce a general and rigorous method for choosing the filter
cutoff and show that this approach can increase discovery probabil-
ities by several-fold relative to no filtering. We also introduce a sim-
ple and intuitive method for weighting p-values that is closely
related to our filtering method and improves the performance
further.

2 Methods

2.1 Optimized filtering

Consider a generic high throughput study. There are L true effects in
the data and m potential hypothesis tests that could be performed in
order to find those effects. In a QTL mapping study, the true effects
would be genetic polymorphisms that affect the trait of interest and
the hypothesis tests would be genetic markers. In an RNA-seq study,
the true effects would be genes that are differentially expressed be-
tween treatment conditions and the hypothesis tests would be the
genes that are tested.

Now, suppose that the hypothesis tests are ordered by a filtering
criterion that attempts to identify the tests most likely to be able to
identify true effects with the best candidate test being ranked #1, se-
cond best #2 and so on. The filtering criterion will be based on some
biological knowledge of the hypothesis tests derived from independ-
ent data. The top 12 tests on this filter-ordered list will be conducted
and the remaining tests will be discarded. Discarding tests will re-
duce multiple testing and potentially increase power. Our goal is
determining what 12 should be in order to maximize power.

We choose a vector 5 = (111, ...nR) of proposed values for v,
where n,- is the number of filter-ordered tests, R is the total number
of subsets of tests, 111 < 112 <, . . . , < 11R 3 m and each subset is
included in the larger ones. Thus, we test 1 to 111, 1 to 112, etc. Take
r1,r2, . . . . ,rm to be the P-values for the filter-ordered hypothesis
tests. The test set nk refers to the filter-ordered set of hypothesis tests
1 to k. A hypothesis test 2' in test set nk is significant if

(1)

Ti < A X 11k
where a is the desired family-wise error rate (FWER). That is, a
Bonferroni correction for the number of tests 11k is applied with a
correction factor A that will correct the multiple testing adjustment
for the fact that we will optimize v overﬁ. We then take whichever
test set maximizes the number of rejected null hypotheses. We show
in the SI that

a 711 R—1 a "HI—"i
FWER< 1 — 1 — — 1 — — 2
_ ( l X 111) ( l X 1114.1) ( )

This has a simple interpretation: in order for no tests to be signifi-
cant, then no p-value can be lower than the least-stringent threshold
at which it is tested. The least-stringent threshold to which the first
111 tests are compared is oc/(li X 111). The least stringent threshold to
which the next 112—111 tests are compared is oc/ (x1 x n2),etc. We obtain
the correction factor 1 required to control FWER by equating (2) to
or and solving for li.The resulting equation does not have a closed
form solution, but can easily be solved numerically. We conducted
simulations to verify that this expression correctly controls FWER
(Supplementary Fig. 51). Supplementary Figures 51 and S2 show a
plot of calculated correction factors as a function of the number of
elements of Z. The Supplementary Material also shows approximate
closed-form solutions for the correction factor.

We focus on the control of FWER in this paper, which is import-
ant for applications such as QTL and genetic association mapping.
A future paper will address control of FDR.

Some filtering schemes will result in tied ranks. Provided that
tied tests lie in the same block between consecutive elements of the
vector 5, then they will be tested at the same significance threshold.

Our approach assumes independence between tests in calculating
the correction factor. However, there will usually be substantial cor-
relation between tests. An alternative is to estimate an effective num-
ber of independent tests. Such approaches can be implemented in
our method by calculating an effective number of tests within each
subset. Simulation results in the SI show that this approach can im-
prove power while maintaining Type I error.

We illustrate the above method with a simple example in
Table 1. There are m = 9 total hypothesis tests and we take 5 = [3,
6, 9]. The filter-ranked P-values are shown in the table. Note that
the filter-ranking will not correspond exactly to the p-value order
and thus the p-values are not ordered. First consider the case with
no correction to the significance threshold. In the first test set, the
P-values are tested against a threshold of 0.05/3 2 0.0167 and two

9mg ‘09 isnﬁnV uo sejeﬁuV s01 ‘etulomeg JO KitSJQAtu [1 112 [3.10811211an[plOJXO'SODBIILIOJIIIOIQ/ﬂ(11111 wort pepeolumoq

852

S.Kim and P.Schliekelman

 

Table 1. Toy example with m: 9 P—values and test sets of 3 = [3, 6, 9]

 

 

 

 

 

 

P-value Without correction factor (A = 1) With correction factor (A = 1.793)

Test set Test set

111:3 112:6 113:9 111:3 112:6 113:9
Signiﬁcance Threshold .0167 .0083 .0056 .0093 .0046 .0031
P1 .0011 .0011 .0011 .0011 .0011 .0011
P2 .0092 .0092 .0092 .0092 .0092 .0092
P3 .0201 .0201 .0201 .0201 .0201 .0201
P4 .0089 .0089 .0089 .0089
P5 .0091 .0091 .0091 .0091
P6 .0064 .0064 .0064 .0064
P7 .0022 .0022
P3 .0861 .0861
P9 .0045 .0045

 

The signiﬁcance thresholds are calculated as 0.05/n,- without the correction factor and 0.05/(n,- x 1.793) with the correction factor. The P-values in bold are less

than the signiﬁcance threshold for that test set.

are significant (shown in bold). In the second test set, the threshold
becomes 0.05/6 2 0.0083. One of the previously significant P-values
0.0092 becomes non-significant, but a newly added P-value 0.0064
is significant. In the third test set, the P-values are tested against a
threshold of 0.05/9 2 0.0056. Now the 0.0064 P-value drops out of
significance, but two new P-values 0.002 and 0.0045 are significant.
The third test set has the highest number (three) significant P-values
and thus we would take 12 = 9 as the optimal cutoff.

However, we have not accounted for the effect of this optimiza-
tion on the FWER. We are maximizing rejections over three sets and
the expected number of false positives will be increased by this
maximization. The probability of type I error is the probability that
there is at least one false positive across the three sets. The first set
in which a P-value appears has the least stringent threshold that
p-values will be tested against. If it is not rejected in that test set,
then it will not be rejected in any (see SI for more details). Thus, the
probability of type I error is

0.05 2 13(52 1)
= 1 — P(S : 0)
= 1 — (1 — 0.05/3,1)3(1 — 0.05/6,1)(6_3)(1 — 0.05/9,1)(9—6)

where 0.05 is a target FWER, S is the number of type I errors and A
is the correction factor based on 3 candidate subsets. We numeric-
ally solve this equation for i, yielding Ii 2 1.793. After applying the
correction factor, the significance thresholds decrease to the values
shown in the table and the number of significant tests decreases as
well. Now, test sets 1 and 3 both have two significant tests and thus
we can take either 12 = 3 or v = 9 as the optimal cutoff.

2.2 Power gains from filtering
Our first goal is to explore the potential gain in statistical power
from filtering. Consider again the generic high-throughput experi-
ment discussed earlier. We will focus on one specific true effect that
we label as effect ‘6 . There are m potential hypothesis tests that
could be performed in order to find this effect and r of these tests
can actually detect the effect ‘6 . ‘ ‘6 -tests’ refers to these tests. We
will make the approximation that all of these 1' tests have the same
power to detect ‘6 .

Suppose that all of the tests are ordered by a filtering criterion
and the first 12 such tests are conducted and the remainder discarded.
For the moment, we will assume that v is fixed in advance so that a

simple Bonferroni correction will suffice and no adjustment to the
significance criterion needs to be made. The probability that at least
one of the r ‘6 -tests successfully detects ‘C is

W) = 2’) v.<s><p.<s>
s=1

where v is the number of filter-ordered hypothesis tests conducted,
32,,(5) = probability that 5 out of the r ‘6 -tests are included in the top
12 tests, and (p1,(s) = probability that at least one of the s ‘6 -tests de-
tects ‘C . We refer to ¢(v)as the discovery probability. We use this
terminology to distinguish it from the power for a single test. We as-
sume that the test statistics for the r ‘6 -tests follow a multi-normal
distribution with mean 0 when the null hypothesis is true and with
the mean determined by the effect size when the null hypothesis is
false. The correlation coefficients will be varied and reﬂect the
amount of correlation between tests.

The function yy(s)quantifies the effectiveness of the filter. An ef-
fective filter will rank hypothesis tests highly that are able to detect
true effects. We assume that the top 12 filter-ordered tests are inde-
pendently sampled without replacement from the m total tests, of
which 1' are ‘6 -tests. Take q(u)as the probability that a given ‘6 -test
is included in the top proportion u of all hypothesis tests. q(u) is
modeled with a beta CDF, giving great ﬂexibility for determining
the properties and effectiveness of the filter function. If sampling
was with replacement, then the function 32,,(5) would follow the bi-
nomial distribution with 12 trials and q(v/m) as the probability of suc-
cess. However it instead follows Wallenius’ noncentral
hypergeometric distribution (Fog, 2008a, b) because sampling is
without replacement. If the filtering is effective, then the ‘6 -tests
will have a higher probability of being selected, which distinguishes
the distribution from a standard hypergeometric. Wallenius’ non-
central hypergeometric accounts for this. See SI for further details.

The assumption of independence in filter rank may be violated in
some data sets and the benefits of filtering will be lower (see SI). The
discovery probability formula is implemented in an R program that
is included in the SI.

Example Scenarios. In our calculations below, we will consider
two basic scenarios. The first scenario has a single test capable of de-
tecting each true effect. This is characteristic of, for example, micro-
array and RNA-seq studies. In these studies, we are interested in
determining which genes are differentially expressed between

9mg ‘09 isnﬁnV uo sejeﬁuV s01 ‘etulomeg JO KitSJQAtu [1 112 /810'S{12umo[pJOJXO'sopeuuogutotq/ﬁdnq wort pepeolumoq

Prioritizing hypothesis tests for high-throughput data

853

 

biological conditions. Typically, each gene is its own hypothesis test
and true effects are genes that are differentially expressed. We as-
sume 10000 hypothesis tests (that is, 10000 genes being tested).
This is an example with no redundancy between tests. That is, each
true effect (differentially expressed gene) can be detected only by a
single test.

In the second scenario there are 20 tests capable of detecting the
true effect and the tests are correlated. This is characteristic of, for
example, QTL mapping, where each marker is a hypothesis test and
the true effect is the QTL. Typically, there are multiple markers that
are correlated with the QTL and each other. We assume 1000 hy-
pothesis tests (that is, 1000 markers being tested). In this case, there
is redundancy between tests because multiple markers can detect the
same true effect. The level of redundancy depends on the amount of
correlation between tests.

2.3 Weighted P—values

Next, we consider a closely related approach based on the weighted
P-value framework (Benjamini and Hochberg, 1997; Finos and
Salmaso, 2007; Genovese, et al., 2006; Holm, 1979; Kropf, et al.,
2004; Roeder and Wasserman, 2009; Roquain and van de Wiel,
2009; Rubin, et al., 2006; Wasserman, et al., 2006; Westfall, et al.,
2004). Suppose that we have P-values p1 to pm (not ordered). A
weight w,- is calculated for each P-value p,- such that the correspond-
ing null hypothesis is rejected if

' OC
n<_
10,- m

Equation (1) above suggests the following weight for the jth filter-
ordered P-value ri,

m

wi — l X 11kg) 
where nkU)is the smallest value from the vector 5 that is greater than
i and 2 is the correction factor computed from Eq. (2). This leads to
a different procedure than the optimal filter approach proposed
above. In the optimal filtered approach, we maximize over the val-
ues of 5. That is, a test is only significant if its filter rank is less than
now and its P-value is less than oc/li >< nopt, where nopt is the value of
Z for which the number of significant tests is maximized. In con-
trast, the optimization step does not occur in the weighted P-value
approach and a test is rejected if its P-value is less than oc/li >< 11kg).
Thus, there are potentially more rejected null hypotheses in the se-
cond procedure. We call this block weighting, where blocks refer to
the groups 1 to n1, 111 + 1 to 112, etc.

Consider the example in Table 1. After applying the correction
factor, we found two significant tests in both the first and third test
sets. Under the optimal filtering procedure, we would choose one of
these test sets as optimal and have two significant tests. Under the
block weighting procedure, the first three P-values are compared to
0.0093, the second three are compared to 0.0046 and the third three
are compared to 0.0031. All tests that meet their respective signifi-
cance criterion are rejected and thus there are three significant tests.

Surprisingly, these two procedures have the same FWER bound.
Although the weighted P-value approach will clearly produce more
false positives under some scenarios, the probability of producing
zero false positives is the same between the two procedures and
FWER is based on this probability (see SI). Because there is no
downside from the FWER perspective in using the block weighting
scheme and it will produce more true positives in some scenarios,
then it is advantageous to use the weighted P-value scheme

(3) instead of the optimal filtering one. We will return to this point
in the data application. This weighting scheme can be viewed as a
generalization of that of Ionita-Laza et al. (2007), who uses 111 = k,
11,-211,4 + 2i_1k,i = 2,. . . for some integer k and then weight
appropriately.

3 Results

3.1 Random filter

We start with a random filter. That is, v of the total m tests are ran-
domly chosen, with all tests being chosen with equal probability re-
gardless of whether they are null or non-null. Figure 1 shows the
discovery probability as a function of v for several scenarios. The
first is parameterized (r: 1, m = 10 000) for the RNA-seq type ex-
periment described above. We see that the discovery probability in-
creases rapidly with v initially, but then flattens out as 12 increases
past about 1000 and then increases slowly after that. A 5-fold in-
crease of 12 from 2000 to 10 000 results in a roughly 2-fold increase
in discovery probability.

There are two opposing forces in action as v is increased. The
probability that the t-test is included in the top 12 is v/m. If this was
the only effect in play, then doubling the number of tests would dou-
ble the probability of discovery. However, the statistical power for
each test decreases as the number of tests increases. Thus, the shape
of the curve in Figure 1a is the result of the interaction of these two
effects. The initial rapid increase is due to the linear increase in v/m.
However, the decreasing power per test causes the curve to flatten
out.

Figure 1b—d are parameterized for the QTL mapping scenario
described previously (r220, m: 1000). Figure 1b has low correl-
ation (0.2) between the 20 t-tests. The dynamics are similar to those
in Figure 1a. However, the beneficial component of increasing 1) is
saturating. That is, the higher 12 is, then the higher the probability is

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

1'1“ r-1.nn5o, m'1D.DDD “3} rﬂzﬂ.n-1U.cor=ll.2, ml‘lﬂDD
3- a
:i :1 '
D
E- e
i. ci 5-1 _ '
E E E“
E a
5 § 5 E.
E‘ r: 3' Id
5 ¥
:1 E c.
"E E, E E _
d D
|l'.l
3 _ 3
1:!
a D RUDD d-DDD DMD SDDD 1WDD D EDD 4W DDD' BED 1WD
numb-er of male 1! nunherofteatsv
{c} =20.n=tu,cor=ﬂ.5. m=1ﬂﬂﬂ {d} r=20.n=2flrﬂ.¢m=fl.5. m=tDﬂﬂ
e. E-
:5
' In. _
A.‘ En 1n 1:.
CI
1 i-
E‘ if." '3'
II E I”
E 2' i
F a n! _
D
Q .
D =1 _
r 'I I I T 'I D 'I I f T T I
a- zoo IUD EDD em moc- o 2m: 4m em am 1:01
numb-at I:Il teal:- Ir mimblr-uf busts 1r

Fig. 1. Plots of the probability of detecting a true effect with a random filter. r
is the number of hypothesis tests able to detect the effect, cor is the pairwise
correlation between those tests, and n is the sample size. The probability of a
test being in the top vis v/m. Waviness in the curves is due to numerical error
in the R routine pmvnorm for calculating multivariate normal probabilities

9mg ‘09 1sn8nV uo sejeﬁuV s01 ‘121u10111123 10 A11s19Atu [1 112 /810'S{12umo[pJOJXO'sot112u1101utotq//:d11q 111011 pepeolumoq

854

S.Kim and P.Schliekelman

 

that one of the 20 t-tests has already detected the effect and there-
fore the benefit in increasing 1) is lowered. Thus, after the initial
period of rapid increase, the discovery probability curve increases
even more slowly than in Figure 1a. A 5 -fold increase in the number
of tests from 200 to 1000 hypothesis tests results in only a 40% in-
crease in discovery probability.

The impact of this saturation is more dramatic with high
correlation between tests. Figures 1c has identical parameters to
Figure 1b, except that the correlation between t-tests is 0.9. Such a
level of correlation is consistent with high density markers in QTL
mapping, for example in genotype-by-sequencing studies. In this
case, the discovery probability reaches a peak at 78 tests and there-
after decreases with increasing v to about half of peak discovery
probability. When the correlation between t-tests is high, then there
is a high level of redundancy between tests. Increasing 1) beyond this
point is counter-productive because the information gain is minimal
and is outweighed by the increased multiple testing correction.

Figure 1d is identical to Figure 1c, except that the sample size is
increased to 200. We see that discovery probability reaches a peak
of about 80% at v = 200 and then decreases. However, in this case
the decrease is minimal and the discovery probability does not drop
below 75%. When the power for single tests is high, then the effect
of the multiple testing correction is much lower than when the
power is low. If the test statistic follows a normal distribution, then
the power for a test is 1 — (I) (1)—1(1 — oc/v) — é , where (I) is the stand-
ard normal cdf, 5 is the standardized effect size and (1)—1(1 — oc/v) is
the Bonferroni-corrected significance threshold. When 5 is small, then
changes in significance threshold from increasing 1) are a dominating ef-
fect. However, when 5 is large, then changes in the significance thresh-
old cause only a small change in (1)—1(1 — oc/v) — 5. It is primarily the
low power situation that is of interest in this paper. When power is
high, then special efforts to optimize discovery probability are not
needed. Figures 1a—c have peak discovery probabilities on the order of
1%. In many situations of practical interest, discovery probabilities are
of this order or lower. The interaction between effect size and filtering
effectiveness is explored more thoroughly in the SI.

In summary, we see that a random filter can be effective for
applications such as QTL mapping where there is high redun-
dancy between tests, but will be detrimental for applications such
a transcriptomics where there is minimal redundancy between
tests.

3.2 Non-random filter

Next, we consider the case of filters that are not random, but rather
are able to preferentially identify tests that are associated with true
effects. We quantify the effectiveness of the filter by the probability
q(u) that a given 1: -test is included in the top proportion u of all hy-
pothesis tests.

Figure 2a shows the probability density functions for three differ-
ent filter functions that we will consider. The corresponding cdfs give
the probability q(u) for a test to be ranked in the upper proportion u
of tests. These are each modeled with a beta distribution with param-
eters as shown in the figure legend. The a = 1, [f = 25 is a very
effective filter that has a maximum probability density at u = 0
and 93% probability of ranking a t-test in the top 10%.
The a = 1, [f = 10 curve is a less strong but still effective filter that
has a 65% probability of ranking a t-test in the top 10% of tests. The
third filter or = 2, [f = 10 has peak probability at 0.1. It has a
low probability of ranking a t-test very highly, but has a high prob-
ability (67%) of ranking them in the top 20%. These filter-functions

span a range from highly effective at increasing discovery probabilities
to only marginally effective or even detrimental (see Figs 2 and 4).

The other three panels show the discovery probabilities for these
filters under the same three low power scenarios considered in
Figure 1. A strong filter can increase discovery probability greatly
compared to no filtering (which occurs when 12 = m at the extreme
right end of the plot). With the optimal cutoff point, the
or = 1, [f = 25 filter increases discovery probability by a factor of
19 for the high correlation case, a factor of 7 for the low correl-
ation case, and a factor of 4 for the r: 1 case. The a = 1, [f = 10
increases power by a factor of 9 for the high correlation case and a
factor of 4 for the lower correlation case, and a factor of 2.5 for
the r: 1 case. The third filter (a = 2,6 = 10) increases discovery
probability by 2- to 4-fold over the three scenarios.

Unlike with the random filter case, these filters are effective in
increasing discovery probabilities for both the low-redundancy tran-
scriptomic scenario and the high redundancy QTL mapping scen-
ario. However, the benefit is much greater for the QTL mapping
scenario.

The performance of the filters is strongly dependent on the
choice of v. The filter has a sharp peak, especially in the high correl-
ation case. For this case, the peak is at v = 3. Discovery probability
is 40% of the maximum at v = 50 and 25% of maximum at v = 100.
The peaks are less sharp for other cases, but in most cases the dis-
covery probability is heavily influenced by the choice of v. The sharp
peaks in the high correlation case (Fig. 2d) are at least partially
caused by the assumption of independence in filter-ranks between
tests. Results presented in the SI explore the effect of this
assumption.

3.3 Optimizing the filter

The above results show that a properly chosen filter can greatly in-
crease the probability for detecting target effects. However, these re-
sults assume that we know the optimal filter cutoff in advance.

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

tn) ﬁlter 111) F1
In- . I:-
W ['1'
q _
D
a .
W
3., — u=1.|I=25
:- ": a —— u=1.)l-=1D
III _
.= “d 5 c! - u=2_II=1I:I
E '— — t1:""|.|3‘25 a D
E'.‘ -- (“1141:1131 a
g - 11:2.n=1e :-
s E- E E.
E a 5.-
'EJ I:
m. -
D
r: -  -
I I I I I I III I I I I I I
{III} 0.2 0.1 I15 [1.1! 1.0 {I 2001} 40120 1512-210 3000 10001:]
1: number 11112513 I
[1:] F213. cor-11.2 {.11 N FED. cor-0.9
E
In E _
F“ — «Imp-25 ‘3
a - - «=1.|1=1DI g- E
E I = :1 E - _
E a “ 21‘ a g ':' u=1_|I=2£-
E " ' E —— u=1.|l.-1D
u G a. 3. - - map-1n
E D
an a a. -
e g- a D
E _
U
D- D
CII . :1 _
a r r r I 1 1 ﬂ 1 I I I I r
D ZED- |14DD EDD EDD 10W D EDD JDD DDD EDD ‘ID'DD

number aftﬁt: 1r numbaf Elf fax-15 «-

Fig. 2. Effectiveness of Filter. (a) The three different filter functions. The X-axis
quantity P is the quantile at which a true effect test is ranked. The y-axis is the
probability density for that ranking. (b—d) The discovery probabilities for
these three filters for three different scenarios. The x—axis is the number of
ranked tests conducted and the y-axis is the discovery probability

9mg ‘09 1sn8nV uo sejeﬁuV s01 ‘121u10111123 10 A11s19Atu [1 112 /810'S{12umo[pJOJXO'sot112u1101utotq//:d11q 111011 pepeolumoq

Prioritizing hypothesis tests for high-throughput data

855

 

Unfortunately, the optimizing process will inﬂate Type I error unless
properly accounted for. We demonstrate this via simulation in
Figure 3. In this simulation we assume that we search for the opti-
mal filter cutoff v by trying different values and choosing the one
that gives the highest number of significant tests. We see that the
Type I error increases dramatically as we increase the number of val-
ues of 12 that we optimize across (see SI for more details).

In the Section 2, we introduced a method for choosing the best
value of v in a statistically principled fashion. We will examine its ef-
fect on discovery probabilities here. A key factor in applying this
method is the set of values over which 12 is optimized. Choosing the
best set of values is not straightforward. The sharper the peak in the
discovery probability with respect to v, the more benefit in searching
over a finer grid. However, this comes at the cost of a higher correc-
tion factor for type I error. We will not delve deeply into this issue
here, but show one example in Figure 4. This is identical to Figure 2,
but with a correction factor of 2 = 2.86. This corresponds to search-
ing 12 from 500 to 5000 in increments of 500 for the case with
10 000 total tests (Fig. 4b) or searching I) from 50 to 500 in incre-
ments of 50 for the case with 1000 total tests (Fig. 4c, d). The verti-
cal lines in each plot show the closest value of v to the peak value
that will be obtained in this search scheme. The horizontal line in
each plot shows the discovery probability with no filtering.

Note that while the shape of these curves is identical to Figure 2,
the height of the curves has been reduced by the introduction of the
correction factor. The weaker filter (cc 2 2, [f = 10) has little or no
benefit after the optimization over 12 is accounted for. However, the
other two filters still bring major benefit. For the r: 1 scenario, the
best searched value of 12 comes very close to the true optimum for all
three cases. For the best (cc 2 1, [f = 25) filter, there is an increase in
discovery probability of 2.2-fold. For the second best filter
(cc 2 1, [f = 10), there is an increased in discovery probability of
35%. The optimized filters are more effective for the r=20 cases.
With correlation between tests of 0.2, the best searched value of v

 

 

 

 

u u u

D— n ..

D '5 II

n. a I D e
I an “than”: “I
e a "a.
nu- II'II- an, D page a a
a, a e I. u
n I: ﬁg” 0
ﬁ_ né a nﬂé null" u
:1 g E II n. "I
III-IIEII
D
a "a In: an
‘5 in“ u
'I: a
ED 9 I
.12.- ° 1 °
no
a. H
II
n
a
e
1:] II
Ci-
I}...
i-

In

:3.

I: II

I I I I I I
D EDD III-DD EDD EDD 1DDD

number of tested eete

Fig. 3. Type I error rate at the level of or: 0.05 in naive optimization of the filter
cutoff. The y-axis is type I error. The x—axis is the number of tested sets. For
example, if there were 1000 total tests and we tried v=100, 200,..., 1000,
then there would be 10 sets tested. Each point represents the proportion of
rejected null hypotheses in 1000 simulation replicates, with P—values gener-
ated from a uniform distribution

hits exactly the true optimum for the (cc 2 1,6 = 10) filter, but
misses substantially for the (cc 2 1, [f = 25) case (true optimum at
v = 19, searched peak at v = 50). The increase in discovery probabil-
ity relative to no filter is 3.08-fold for the (cc 2 1, [f = 25) and 1.82-
fold for the (or = 1, [f = 10) case. The peaks are very
sharp for the high correlation case. The best searched values of
12:50 miss the true optimums of 2 and 5 for the (cc 2 1, [f = 25)
and (or = 1, [f = 10) filters by a large margin. Still, the increase in
discovery probability relative to no filtering is 3.4-fold and 2.7-fold,
respectively. It is clear that we could do better if we knew something
about the shape of the filter function and therefore how finely to
search 12. In absence of this information, the decision is more difficult
and the filter effectiveness is likely to be decreased.

3.4 Application of the filtering/correction factor
procedure to a mouse obesity data
We applied our filtering approach to the QTL mapping data set of
Ghazalpour et al. (2006), using gene expression information to rank
the markers. The data set includes 1065 SNP markers, expression
values for 3421 genes, and measurements of several phenotypes
including body weight for 135 female mice. The goal is to identify
QTLs for body weight. We used the Mouse Gene Expression
Database(Smith, et al., 2014) to identify a set of 37 genes present in
the data that have previously been shown to be associated with body
weight in mice. LOD scores were calculated for each SNP with re-
spect to each of these 37 genes. The filter statistic for each SNP was
the median value over the 37 genes. These were then sorted to give
filter ranks. The SNPs were then tested for linkage with body weight
QTLs. See the SI for more details.

The reasoning in using this filtering function is that SNPs that
drive expression of genes associated with body weight are good

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

ta] ﬁlter {In} :1
In ' '
H I
g" I
e - e i
l
is E :
E '13 - E g _. I
e — 1:111:25 .1 g.
E — Imp-1o If
g E _ - - wilful“) g T r - — - — - — - — - — I
I
e E . -
'GI' ; i
In - | D
l I
l
' i I
e - g ~ ' I 2
I ¢' I I I I I
D D D 2 D If. D 3 DB 1D D 2ND IDDD DDDD DDDD 1DEDD
b number chests Ir
in) I=2D, uaI=D.2 1‘} =20, eur=D.9
: l I
H
e - I e I
: a : — r1-1.|I-25
— rJ=1_|'|=1D
F D- : Fri : -- 11:21:11:
% c1 : g I i - run-ﬁlter
I
E - f e. -
a - I Q I
g..- t g :
w I I In E - I
"E - - —I"r - — - — - — - — - — I "E '
g- _ '- ‘f :
a '- .
,I' : 3 .
I
a I ll'I- .  d IllI
'5 'r 7 r r r I D I I I I I I
D 2DD MID EDD EDD 1DDD D 213D 413D EDD EDD 1DDD
numbru‘flacetev number EI'f1B'EbS Ir

Fig. 4. Effectiveness of filter after optimizing. (a) The three different filter func-
tions. (b—d) The discovery probabilities for these three filters for three differ-
ent scenarios, with a correction factor of 2: 2.86. This correction factor
corresponds to optimizing v over the values 500, 1000, 1500, . . . , 5000 for (b)
and 50, 100, 150,. .., 500 for (c) and (d). The horizontal line shows the discov-
ery probability with no filtering. The vertical line shows the closest value of v
to the peak that will be obtained under the search scheme, with the line types
of the vertical lines being the same as the filter to which they correspond

9mg ‘09 1sn8nV uo sejeﬁuV s01 ‘121u10111123 10 A11s19Atu [1 112 /810'S{12umo[pJOJXO'sot112u1101utotq//:d11q 111011 pepeolumoq

856

S.Kim and P.Schliekelman

 

candidates for being body weight QTLs. Because we did not use the
body weight data in calculating the filter ranks, then the filter ranks
will be independent of the LOD scores with respect to body weight
under the null hypothesis that the SNP is not a body weight QTL.
We consider both the optimal filtering approach (Eq. (1)) and the
P-value weighting approach (Eq. (3)).

There were four chromosomal regions with some evidence for
body weight QTLs, although no SNP is significant under the stand-
ard Bonferroni correction with 1065 tests. For the SNP with the best
chance of being detected in each of the four QTL regions, Table 2
shows the filter rank and the maximum number of tests for which it
would be significant under a Bonferroni correction. The maximum
number of tests gives a measure of how significant the test is. If the
maximum tests equals or exceeds m = 1065, then the SNP would be
significant under a standard Bonferroni correction. The lower this
quantity is below m, then the better the filter will have to perform in
order for the SNP to be significant.

The SNP shown in the table for each QTL region is the SNP with
the highest value of (max tests to be significant)/(filter rank). The
most significant SNPs are on chromosome 19. The strongest would
be significant if tested among 591 or fewer SNPS. These SNPS have
filter ranks of about 400 (among 1065). Three other SNPs in the
same region are moderately weaker, but are filter-ranked at about
the 17th percentile. The most significant SNP of these is shown in
the Table 2. It would be significant if tested among 444 or fewer
markers. The next most significant region is on chromosome 15.
The most significant markers would be significant if tested among
190 or fewer markers. These markers are filter-ranked very highly at
about the 1 % point. The third most significant region is on chromo-
some 5. The strongest SNP would be significant if tested among 124
or fewer markers. The filtering ranking is ineffective with this SNP,
placing it about the 33rd percentile. The fourth strongest region is
on chromosome 1. Although several markers are filter-ranked at
about the 10th percentile, the most significant SNPs would have to
be filter-ranked in the top 5% to be significant. No other regions
show any evidence of QTLs. The most significant SNP outside of
these four regions would only be significant if tested among six or
fewer markers and most of the remaining P-values are greater than
0.05. Supplementary Table S1 in the SI shows the top 20 most sig-
nificant SNPs.

Table 2 shows the results of using three different schemes for
optimizing the filter: 2: 100—500 in increments of 100, 2:25—
1050 in increments of 25 and Z = 5 0—200 in increments of 50. For
each combination of SNP and search scheme, we show the lowest
value (labeled ‘Next highest inc.’) of E that is higher than the filter
rank (i.e. the value at which the SNP will be tested), and that value
multiplied by the correction factor (i.e. the effective number of hy-
pothesis tests in the Bonferroni correction for that SNP, labeled
‘Corrected’). The SNP will be significant if this corrected value is

less than or equal to the maximum Bonferroni-corrected number of
tests for that SNP (column 3 in the Table). The corrected values are
bold in cases where this occurs.

Under the first scheme, the QTL on chromosome 19 is signifi-
cant. It has filter rank 182 and thus the next highest increment is
200. After multiplying by the correction factor of 222.22, the ef-
fective number of tests is 444 and the marker is significant. No other
markers are significant. Although the marker on chromosome 15 is
very highly ranked (12th), the next highest increment is 100 under
this relatively coarse search scheme. This marker is too weak to be
significant under the resulting 222 effective number of tests. These
results are the same whether we use the optimal filtering approach
or the weighted P-value approach.

The second search scheme uses increments of 25. This causes the
correction factor to increase from 2.22 to 4.17. However, this re-
sults in a decrease from 100 to 25 in the next highest increment for
the 12th-ranked chromosome 15 marker. Even after applying the
higher correction factor, the marker is still significant. On the other
hand, the chromosome 19 marker is no longer significant. The finer
search grid makes no difference in the next highest increment (200
in both cases) and the higher correction factor causes the marker to
lose significance.

The third search scheme uses increments of 50, but only for the
top 200 filter-ordered tests. The 12th-ranked chromosome 15
marker has a next highest increment of 50. After applying the cor-
rection factor of 2.03 the effective number of tests is 104 and it is
significant. The 180th-ranked chromosome 19 marker again has a
next highest increment of 200. With the lower correction factor of
2.03, the effective number of tests is 406 and it is also significant.
Under the optimal filtering approach, we would have a choice of
taking the filter cutoff at either 50 or 200 and get one QTL either
way. Under the weighted p-value approach, both QTLs are
significant.

These examples show the tradeoffs in choosing different search
schemes. Smaller search increments will tend to favor highly
ranked features, because they can make a large proportional differ-
ence in the minimum number of tests including that feature. The
chromosome 15 marker benefits greatly from being tested among
25 features rather than 100, even after correction factor. On the
other hand, a lower ranked feature gets less proportional benefit
and may decrease in discovery probability because of an increase
in 2.

We have shown the results of several different search schemes in
order to demonstrate important aspects of our method. However, it
should be noted that the search scheme must be chosen in advance
of seeing the data, or the Type I error rate will be inflated. Nai've ad-
justment of the search scheme to get the largest number of positive
results will inflate false positives just as nai've adjustment of the filter
cutoff does.

Table 2. The SNPs with the highest value of (max tests)/ (filter rank) for each of the four putative QTL regions

 

Search scheme

 

100—500 by 100 A = 2.22

25—1050 by 25 Ii 2 4.17 50—200 by 50 Ii 2 2.03

 

 

 

 

ID CHR. Max tests Filter rank Next Highest Inc. Corrected Next Highest Inc. Corrected Next HighestInc. Corrected
p45915 19 444 182 200 444 200 834 200 406
p44593 15 190 12 100 222 25 104 50 102
p45558 5 124 353 400 888 375 1564 — —
p46339 1 53 101 200 444 125 521 150 305

 

9mg ‘09 1sn8nV uo s9198uV s01 ‘aiulomag JO AnsmAiu [1 112 [glO'SIBILInO[plOJXO'SODBIILIOJHIOIQ/[Zdllq wort pepaolumoq

Prioritizing hypothesis tests for high-throughput data

857

 

4 Discussion

Although filtering methods have been in common use throughout
the genomic era, their general statistical properties are not well
understood. In this study, we have introduced a framework that
quantifies filter effectiveness in terms of the probability of a feature
associated with a true effect being ranked at or above a specified
quantile. Using this approach, we have examined the conditions for
filtering to be successful.

First, we have shown that filters can be very effective at increas-
ing discovery probabilities for weak effects. However, the filter must
have a substantial probability of ranking true positive features
highly (e.g. the top 10%). Furthermore, the benefit of filtering is
greater when there is high redundancy in information between hy-
pothesis tests. In this case, a filter with a high probability of ranking
true positive features in the upper 10% can increase discovery prob-
abilities by 10-fold or more. The gain is less with lower redundancy
between tests, but there is a several-fold (and higher) benefit over a
wide range of conditions of filter effectiveness and redundancy in
tests. In applications such as QTL mapping and GWAS, there are
commonly many tests capable of detecting each true effect. In this
case, filtering can be highly effective. In applications such as stand-
ard transcriptomics or proteomics where there is a single test per
true effect, then redundancy is minimal and filtering less effective
but can still be beneficial.

A major caveat is that the increased benefit in the case of high re-
dundancy is contingent on the correlation in filtering ranks being
low after conditioning on shared causative mechanism. If there is
additional correlation (as in our QTL example where the ranks are
all derived from the same data), this benefit is reduced (SI).

Second, the gain from filtering is highly dependent on the choice
of filter cutoff. The choice of cutoff can easily make a 2- to 3-fold
difference in discovery probability in the lower redundancy situation
and a bigger difference in the high redundancy case. Most applica-
tions of filtering choose the cutoff in an arbitrary fashion, poten-
tially leading to large loss in discovery probabilities relative to what
is possible. Even worse, however, is choosing the cutoff based on the
outcome without properly accounting for type I error. FWER can be
greatly inflated when the cutoff is naively chosen based on the data.

Third, we have introduced a method for choosing the filter cutoff
that finds the best value and properly accounts for the effect on
FWER. Even after adjusting for the search procedure, there can still
be a gain of several-fold in discovery probability relative to an arbi-
trary choice of cutoff. This data-dependent filtering procedure leads
naturally to a closely related data-independent p-value weighting
technique. In this approach, tests are filter-ordered in blocks and
weighted by the inverse of the filter percentile rank for the lowest
ranked member of the block, adjusted by a correction factor that en-
sures the target FWER is maintained. This method will always per-
form at least as well as optimal filtering and will sometimes result in
more rejected null hypotheses while maintaining the same FWER.

The benefit of filtering depends strongly on effectiveness that the
filter statistic is at ranking true effects highly. According to our re-
sults, a filter statistic with a high probability (say 70%) of ranking
true effects in the top 10% can substantially improve discovery
probabilities. Filters that are substantially less effective than this will
not be likely to improve discovery probabilities and may make them
worse. Determining whether filtering statistics can be expected to
routinely perform this well is a crucial question that should be ad-
dressed in future research.

It is important to emphasize that P-value weighting/optimal fil-
tering does come at a cost relative to the case where we know the

correct filter cutoff in advance. A comparison of Figures 2 and 4
shows that discovery probabilities can be reduced by a factor of two
or more between the case where we know the optimal filter cutoff
and the case where we have to search for it. Furthermore, there is a
tradeoff in deciding how finely we should search for the peak.
Searching more finely makes it more likely to find the peak, but
comes at a cost of a bigger FWER correction. When there is high
correlation between hypothesis tests, then peaks tend to be sharper
and searches with a finer grid may be beneficial. Future research
should investigate the shape of filter functions for different types of
data and filtering information. This would provide insight into the
best choice of the vector 2. Ideally, we would determine where good
cutoff points tend to be for particular types of data. Then, optimiza-
tion of the filter would not be required and the full benefits of filter-
ing could be realized.

Another finding from this work (Figure 1d and SI) is that filter-
ing can greatly increase the relative discovery probability for weak
effects, but it is constrained in terms of the absolute gains in power
that are possible. Whether such gains for weak effects are of much
value depends on the distribution of effect sizes. For example, accu-
mulating evidence suggests that complex traits in humans are often
driven by very large numbers of very low effect genetic variants (e.g.
Gibson, 2011). In such a scenario, a boost in discovery probability
from, for example, 0.1 to 1% for many such variants would be very
significant. On the other hand, filters will be less effective at, for ex-
ample, boosting power from 20% to 80% unless the filter is very ef-
fective at ranking true positive features highly.

A number of previous studies (Benjamini and Hochberg, 1997;
Finos and Salmaso, 2007; Genovese, et al., 2006; Holm, 1979; Ionita-
Laza, et al., 2007; Kropf, et al., 2004; Roeder and Wasserman, 2009;
Roquain and van de Wiel, 2009; Rubin, et al., 2006; Wasserman,
et al., 2006; Westfall, et al., 2004) have proposed alternative methods
for P-value weighting and several studies (Roeder and Wasserman,
2009; Rubin, et al., 2006; Wasserman, et al., 2006) have derived opti-
mal weights based on the true effect size. However, true effect size is
never known and optimality is unclear under schemes for estimating
it. Our correction factor approach is based solely on the filter-ranks.
It will tend to perform better for sufficiently good filter-ranks, but the
relative merits in practical circumstances are unclear. A future manu-
script will explore these issues more thoroughly.

Funding

This study was supported in part by the University of Georgia Research
Computing Center, a partnership between the Ofﬁce of the Vice President for
Research and the Ofﬁce of the Chief Information Ofﬁcer of the University of
Georgia.

Conﬂict of Interest: none declared.

References

Benjamini,Y. and Hochberg,Y. (1997) Multiple hypotheses testing with
weights. Scand. ]. Stat., 24, 407—418.

Bourgon,R. (2010) Independent ﬁltering increases detection power for high-
throughput experiments. Proc. Natl Acad. Sci., 107, 9546—955 1.

Bourgon,R. et al. (2010) Reply to Talloen et al.: independent ﬁltering is a gen-
eric approach that needs domain speciﬁc adaptation. Proc. Natl Acad. Sci.,
107, E175—E175.

Calle,M.L. et al. (2008) Improving strategies for detecting genetic patterns of
disease susceptibility in association studies. Stat. Med., 27, 65 32—6546.

Dai,].Y. et al. (2012) Two-stage testing procedures with independent ﬁltering
for genome-wide gene-environment interaction. Biometrika, 99, 929—944.

mm ‘09 1sn8nV uo sejeﬁuv s01 ‘atulomag JO AnsmAtu [1 112 /810's112umo[pJOJXO'sotiamJOJutotw/zdnq wort papaolumoq

858

S.Kim and P.Schliekelman

 

Degnan,].H. et al. (2008) Genomics and genome-wide association studies: an
integrative approach to expression QTL mapping. Genomics, 92, 129—133.

Evans,D.M. et al. (2006) Two-stage two-locus models in genome-wide associ-
ation. PLoS Genet, 2, e157.

Finos,L. and Salmaso,L. (2007) FDR- and FWE-controlling methods using
data-driven weights. ]. Stat. Plan. Inference, 137, 3859—3870.

Fog,A. (2008a) Calculation methods for Wallenius’ noncentral hypergeome-
tric distribution. Commun. Stat. Simul. C, 37, 25 8—273.

Fog,A. (2008b) Sampling methods for Wallenius’ and Fisher’s noncentral
hypergeometric distributions. Commun. Stat. Simul. C, 37, 241—257.

Genovese,C.R. et al. (2006) False discovery control with p-value weighting.
Biometrika, 93, 5 09—524.

Ghazalpour,A. et al. (2006) Integrating genetic and network analysis to char-
acterize genes related to mouse weight. Plos Genet. 2, 1 1 82—1 192.

Gibson,G. (2011) Rare and common variants: twenty arguments. Nat. Rev.
Genet., 13, 135—145.

Hackstadt,A.]. and Hess,A.M. (2009) Filtering for increased power for micro-
array data analysis. BMC Bioinf., 10, 11.

Holm,S. (1979) A simple sequentially rejective multiple test procedure. Scand.
]. Stat., 6, 65—70.

Ionita-Laza,I. et al. (2007) Genomewide weighted hypothesis testing in fam-
ily-based association studies, with an application to a 100K scan. Am. ].
Hum. Genet., 81, 607—614.

Jiang,H. and Doerge,R.W. (2006) A two-step multiple comparison procedure
for a large number of tests and multiple treatments. Stat. Appl. Genet. Mol.
Biol., 5, Article28.

Kropf,S. et al. (2004) Nonparametric multiple test procedures with data-
driven order of hypotheses and with weighted hypotheses. ]. Stat. Plan.
Inference, 125, 31—47.

Li,L. et al. (2013) Using eQTL weights to improve power for genome-wide
association studies: a genetic study of childhood asthma. Front. Genet., 4, 103.

Lu,]. et al. (201 1) Principal component analysis-based ﬁltering improves detec-
tion for Affymetrix gene expression arrays. Nucleic Acids Res., 39, e86.

McClintick,].N. and Edenberg,H.]. (2006) Effects of ﬁltering by Present call
on analysis of microarray experiments. BMC Bioinf., 7, 49.

Pattin,K.A. and Moore,].H. (2008) Exploiting the proteome to improve the
genome-wide genetic analysis of epistasis in common human diseases. Hum.
Genet., 124, 19—29.

Patwardhan,A. et al. (2014) Variant priorization and analysis incorporati-ng
problematic regions of the genome. Pac. Symp. Biocomput., 277—287.

Ramskold,D. et al. (2009) An abundance of ubiquitously expressed genes revealed
by tissue transcriptome sequence data. PLoS Comput. Biol., 5, e1000598.

Rau,A. et al. (2013a) HTSFilter : independent data-based ﬁltering for repli-
cated transcriptome sequencing experiments. 1—14, web document found at
https://www.bioconductor.org/packages/release/bioc/vignettes/I-ITSFilter/inst/
doc/HTSFilter.pdf.

Rau,A. et al. (2013b) Data-based ﬁltering for replicated high-throughput tran-
scriptome sequencing experiments. Bioinformatics, 29, 2146—2152.

Roeder,K. and Wasserman,L. (2009) Genome-wide signiﬁcance levels and
weighted hypothesis testing. Stat. Sci. Rev. ]. Inst. Math. Stat., 24, 398—413.

Roquain,E. and van de Wiel,M.A. (2009) Optimal weighting for false discov-
ery rate control. Electron. ]. Stat., 3, 678—711.

Rubin,D. et al. (2006 ) A method to increase the power of multiple testing pro-
cedures through sample splitting. Stat. Appl. Genet. Mol. Biol., 5, 19.

Smith,C.M. et al. (2014) The mouse Gene Expression Database (GXD): 2014
update. Nucleic Acids Res., 42, D81 8—D824.

Sultan,M. et al. (2008) A global view of gene activity and alternative splicing
by deep sequencing of the human transcriptome. Science (New York, N. Y),
321, 956—960.

Talloen,W. et al. (2007) I/NI-calls for the exclusion of non-informative genes:
a highly effective ﬁltering tool for microarray data. Bioinformatics (Oxford,
England), 23, 2897—2902.

Talloen,W. et al. (2010) Filtering data from high-throughput experiments
based on measurement reliability. Proc. Natl Acad. Sci. USA, 107, E173—
E174.

Wasserman,L. et al. (2006) Genome-wide signiﬁcance levels and weighted hy-
pothesis testing. Stat. Sci. 2009, 24, 398—411.

Westfall,P.H. et al. (2004) Weighted FWE-controlling methods in high-dimen-
sional situations. Lect. Notes Monogr. Ser. Recent Dev. Multiple
Comparison Proced., 47, 143—154.

mm ‘09 1sn8nV uo sejeﬁuv s01 ‘etulomeg JO AnsmAtu [1 112 /810's112umo[pJOJXO'sotiemJOJutotw/zdnq wort papeolumoq

