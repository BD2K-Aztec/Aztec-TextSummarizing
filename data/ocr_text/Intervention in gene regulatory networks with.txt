ORIGINAL PAPER

Vol. 29 no. 14 2013, pages 1758—1767
doi:10. 1093/bioinformatics/btt242

 

Systems biology

Advance Access publication April 29, 2013

Intervention in gene regulatory networks with maximal phenotype

alteration

Mohammadmahdi R. Yousefi1 and Edward R. Dougherty1’2’*

1Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX 77843 and
2Computational Biology Division, Translational Genomics Research Institute, Phoenix, AZ 85004, USA

Associate Editor: Igor Jurisica

 

ABSTRACT

Motivation: A basic issue for translational genomics is to model gene
interaction via gene regulatory networks (GRNs) and thereby provide
an informatics environment to study the effects of intervention (say, via
drugs) and to derive effective intervention strategies. Taking the view
that the phenotype is characterized by the long-run behavior (steady-
state distribution) of the network, we desire interventions to optimally
move the probability mass from undesirable to desirable states
Heretofore, two external control approaches have been taken to
shift the steady-state mass of a GRN: (i) use a user-defined cost func-
tion for which desirable shift of the steady-state mass is a by—product
and (ii) use heuristics to design a greedy algorithm. Neither approach
provides an optimal control policy relative to long-run behavior.
Results: We use a linear programming approach to optimally shift the
steady-state mass from undesirable to desirable states, i.e. optimiza-
tion is directly based on the amount of shift and therefore must out-
perform previously proposed methods. Moreover, the same basic
linear programming structure is used for both unconstrained and con-
strained optimization, where in the latter case, constraints on the op-
timization limit the amount of mass that may be shifted to ‘ambiguous’
states, these being states that are not directly undesirable relative to
the pathology of interest but which bear some perceived risk. We
apply the method to probabilistic Boolean networks, but the theory
applies to any Markovian GRN.

Availability: Supplementary materials, including the simulation
results, MATLAB source code and description of suboptimal methods
are available at http://gsp.tamu.edu/Publications/supplementary/
yousefi13b.

Contact: edward@ece.tamu.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on February 20, 2013; revised on April 11, 2013; accepted
on April 24, 2013

1 INTRODUCTION

Genetic regulatory networks (GRNs) refer to a class of models
describing the multivariate functional relationships among a
cohort of genes or their products. From a graphical perspective,
genes are nodes in this network, and edges describe regulatory
relationships between genes. These networks aim to model cellu-
lar control and how abnormal cell functions result from one or
several breakdowns in the regulatory mechanisms. Thus, GRNs

 

*To whom correspondence should be addressed.

are an essential part of translational medicine, whose ultimate
goal is to develop therapies based on the disruption or mitigation
of aberrant gene function contributing to the pathology of a
disease (Dougherty et al., 2010). Therapies usually involve
some procedure and several drug candidates acting on various
gene products with the aim of mitigating undesirable gene
functions.

Developing therapeutic methods in the context of GRNs in-
volves designing intervention strategies to alter the dynamics of
the gene activity proﬁles (GAPS) of the network in some desired
manner, thereby identifying potential drug targets (Datta and
Dougherty, 2006; Dougherty and Datta, 2005; Dougherty
et al., 2010). Modeling GRNs via Markovian dynamical net-
works has received much attention because they capture uncer-
tainty intrinsic to the interactions among genes or gene products
at different levels. Furthermore, one can use the rich theory of
Markov decision processes (MDPs) to formulate optimal inter-
vention problems. In the present article, we choose probabilistic
Boolean networks (PBNs) (Shmulevich et al., 2002a) as our ref-
erence model for GRNs. The transition probabilities of PBNs are
characterized by their associated Markov chains, and PBNs have
played the major role in intervention studies.

Assuming that the collection of all possible GAPs constitutes
the state space, we can partition it into desirable and undesirable
cellular states according to the expression values of a given set of
genes. An undesirable state can be associated with a phenotype
representative of a cancerous state; e. g. cell growth in the absence
of growth factors is undesirable. Although there might exist some
states in the desirable set that do not represent healthy condi-
tions, they can be classiﬁed as ‘desirable’ because they are not
associated with the particular pathology deﬁning the undesirable
set (Qian and Dougherty, 2012). Once a model for the underlying
GRN is assumed and desirable and undesirable sets are recog-
nized, the objective is to find an optimal therapeutic strategy
(control policy) with respect to a predeﬁned objective function
to drive the dynamics of the network from undesirable states to
the desirable ones. This problem has been extensively studied
within two basic intervention approaches in the context of
PBNs, external control and structural intervention.

External control is generally based on ﬂipping (or not ﬂipping)
the value of a speciﬁc gene (or possibly more than one), called
the control gene. As the dynamic behavior of a PBN can be
represented by a ﬁnite-state Markov chain, the first proposed
intervention approach was to determine an optimal single-gene
perturbation to the network based on mean ﬁrst-passage times in
Markov chains (Shmulevich et al., 2002b). Attention, later,

 

1758 © The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 ﬁle'snaumo[pJOJXO'sot1emJOJutotw/2d11q IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no :2

Intervention in GRNs

 

turned to dynamic programming-based ﬁnite-horizon (Datta
et al., 2003) and inﬁnite-horizon external control in which the
steady-state distribution (SSD) is altered (Pal et al., 2006). These
works were followed by several articles developing approaches to
external intervention that take into account practical issues that
arise from therapeutic constraints (Ching et al., 2009; Youseﬁ
et al., 2012), biological complexity (Pal et al., 2008) and compu-
tational limitations (Faryabi et al., 2007; Ivanov et al., 2010).
Structural intervention, on the other hand, involves a one-time
change in the network regulatory structure (wiring) so that the
long-run behavior of the network is altered in a desired manner.
Given a collection of potential structural changes, the problem is
to ﬁnd an optimal structural intervention resulting in a max-
imum alteration of the SSD toward the direction of desirable
states and away from undesirable states (Qian and Dougherty,
2008; Shmulevich et al., 2002c; Xiao and Dougherty, 2007).

In the main, optimal inﬁnite-horizon intervention for GRNs
has involved the speciﬁcation of a cost function based on the
current state of the system and the desirability of potential tran-
sitions. Thus, the long-run effect of the optimal policy on the
network dynamics becomes a by-product of this cost function-
based optimization problem. As phenotype is associated with
steady-state behavior, as in the case of attractor cycles in BNs
and PBNs (Kauffman, 1993), from a practical perspective, it
would be better to determine optimality directly in terms of
long-run behavior. Direct optimization has been addressed to
some extent in Qian et al. (2009), where the authors propose
three classes of greedy stationary intervention policies that
bypass the need for a user-deﬁned cost function and directly use
long-run behavior as the optimization criterion to reduce the mass
of the SSD corresponding to undesirable states and increase the
mass corresponding to desirable states (Qian et al., 2009).

In this article, we rigorously formulate this intervention prob-
lem and provide an optimal intervention policy with a computa-
tional complexity equivalent to solving a linear program (LP)
optimization problem in Section 2.4 for a general cost function
and in Section 2.5 for maximal phenotype alteration. We also
consider a variant of this optimization problem where one might
constrain the steady-state probability mass of some ‘ambiguous’
states in the network while ﬁnding the optimal intervention policy.
This is especially important in therapeutic methods because it is
prudent to avoid introducing new probability mass to states asso-
ciated with unknown phenotypes (Qian and Dougherty, 2012).
We demonstrate the performance of optimal policies using syn-
thetically generated networks and two real networks derived from
the metastatic melanoma and mammalian cell cycle.

2 SYSTEMS AND METHODS
2.1 PBNs

PBNs address uncertainty in the structure of BNs or the dynamics of state
transitions (Shmulevich et al., 2002a). We restrict ourselves to binary
PBNs, it being assumed that the value of each gene is quantized to two
levels 0 or 1, where 0 corresponds to an unexpressed (OFF) gene and 1
corresponds to an expressed (ON) gene. The mathematical theory extends
directly to PBNs with discrete-valued nodes.

DEFINITION 1. A Boolean network BN( V, F) is fully characterized by a
set of n nodes, V 2 {v1, v2, . . . , vn}, (representing genes {g1, g2, . . . , g"} or

their products) and a list of Boolean functions F = {1‘1 ,f2, . . . ,f"} describ-
ing the functional relationships between the nodes. The Boolean function
f : {0, 1}j‘I—>{0, 1} determines the value of node i at time k+ 1, given the
value of its predictors at time k by

v2+1=fi(v§€1, v22, . . . , vii),

where {vi1, viz, . . . , viji } corresponds to values of {gi1, giz, . . . , gij" } as j, pre-
dictors of gene g’.

In a BN, all genes are assumed to update synchronously in accordance
with the Boolean functions assigned to them. A BN( V, P) evolves accord-
ing to a genome-wide GAP, V], = (vllc, vi, . . . , v”) at time k, forming the
state space of size 2”. There is a bijection between v and its decimal
representation x e :9 = {0, 1, . . . , 2” — 1} by x = 2:721 2”‘ivi. In this con-
text, genes that are reﬂective of an undesirable state are called target
genes. 3 is partitioned into subsets of desirable and undesirable states,
denoted by T) and U, respectively. Given an initial state, a BN will even-
tually enter a set of states, called an attractor cycle. The set of states
leading to that attractor cycle is known as the basin of attraction
(BOA) mougherty et al., 2010).

DEFINITION 2. A PBN( V, F, P, q, p) is characterized by a set of n nodes,
V 2 {v1, v2, . . . , v”}, and a set ofm constituent BNs, F = {F1,F2, . . . ,Pm},
called contexts, a selection probability vector P = {121,122, . . . , pm} over F,
a network switching probability q and a random gene perturbation prob-
ability p. Random switching and gene perturbation are also assumed to
be mutually exclusive.

At any time point, with probability q, the network dynamics may
switch from the current governing constituent BN to another according
to the selection probability vector P. It is assumed that the probability of
switching to a new constituent network is independent of the current
network. We also allow that the current network may switch to itself
when a switch is called for (Dougherty et al., 2010). In addition, with
probability p, the current state of each gene in the network can be ran-
domly ﬂipped. The PBN is said to be context-sensitive if q< 1, the inter-
pretation being that there are latent variables outside the network whose
changes cause the model network to behave stochastically (Brun et al.,
2005). If q: 1, the PBN is called instantaneously random, the interpret-
ation being that the uncertainty in the PBN arises from uncertainty in
model inference (Shmulevich et al., 2002a). Averaging over the various
contexts reduces the transition probability matrix (TPM) of a context-
sensitive PBN to the instantaneously random PBN with identical param-
eters (Faryabi et al., 2009). By deﬁnition, a PBN inherits the attractor
structures from its context BNs without perturbation. With sufﬁciently
small perturbation probability p, the long-run behavior of a PBN will
reﬂect the attractor structures within the context BNs (Brun et al., 2005).
A Boolean network with perturbation, BNp, is a PBN in which m = 1.

Transition rules of any PBN can be modeled by a homogeneous
Markov chain, whose states of the TPM are the GAPS of the underlying
regulatory network (Shmulevich et al., 2002a). Let S = {(x, y) : x E :9,
y 6 {1,2, . . . ,m}} denote the state space of the PBN. The sets
D and U corresponding to the set of desirable, and undesirable
states can be deﬁned as D = {(x,y) : x e 17y 6 {1,2, . . . ,m}}
and U = {(x,y) : x e bl,y 6 {1,2, . . . ,m}}. We denote by
{Zk e S,k = 0,1,...} the stochastic process of the state of the PBN
that has both the information about the current constituent BN and
GAP of the underlying network. Originating from state ie 8, the
successor state j e S is selected randomly according to the transition
probability 7?, with its ijth element deﬁned by pij é P(Zk+1 = j|Zk = i)
for all k = 0,1,.... The transition probabilities of this Markov chain
can be calculated as explained in Dougherty et al. (2010). Owing to
the random gene perturbation, the equivalent Markov chain is erg-
odic and has a unique invariant measure, It, equal to its limiting
distribution.

 

1 759

112 [3.10811211an[p.IOJXO'SOImIIIJOJIIIOIQ/ﬂdllq IIIOII pepeolumoq

910K ‘09 lsnﬁnV no :2

M.R. Yousefi and ER. Daugherty

 

We assume that the PBN admits an external control input A from a set
Of actions, A, specifying the type Of intervention on a set Of control genes.
For instance, A = 0 may indicate nO-intervention, and A = 1 may indicate
that the expression level Of a single gene, gc, c e {1, 2,. . . , n}, is ﬂipped. In
this intervention scenario, the control action A = 1 at state (x, y) replaces
the row corresponding to the state (x, y) in the original TPM Of the
underlying Markov chain by the row corresponding to the state (Sc, y),
where the binary representation Of 5c is the same as x except in bit vc,
where it is ﬂipped. Let {Ak e A,k = 0, 1, . . .} denote the stochastic pro-
cess of actions taken. The law of motion for the controlled network is
represented by a matrix 73(a) with its ijth element deﬁned as

FIJI“) = P(Zk+1 = jIZk = LAk = a), (1)

this being the probability Of going to j at time k + 1 starting from state i
and taking action a at time k. As the original Markov chain is ergodic, the
controlled chain will also be ergodic having a unique invariant measure.
The action process is stochastic in two ways: (i) the state process is sto-
chastic, and the action process is a mapping from the entire history Of
states and actions to the action space so that the action process is also
stochastic and (ii) we allow the actions to be random depending on the
history and current state of the system. Indeed, the pair (Zk, A1,) is a
stochastic process.

2.2 Constrained MDPs

Discrete time MDPs constitute a class of sequential decision making
problems where the system of interest evolves stochastically at discrete
time units. One can observe system states at times k = 0, 1, . . . , N, where
N, called the ‘horizon’ may be ﬁnite or inﬁnite. At each k, a decision
maker can alter the costs or dynamics of the system by choosing some
parameters, called ‘actions’. The dynamical movement of the system from
one state to another is completely characterized, given the current state
and action taken at this state via a probability vector over all possible
next states. The goal is to optimize an objective function (e. g. the inﬁnite-
horizon expected average cost) and ﬁnd an optimal control policy indu-
cing the optimal cost. More than one objective cost may exist, and the
decision maker minimizes one Of the objectives subject to constraints on
the others. This class of MDPs are referred to as constrained MDPs
(Altman, 1999) and mathematically deﬁned as follows.

DEFINITION 3. A tuple {8, A, 73(a),g, r} deﬁnes a constrained MDP,
where 8 denotes the state space of a ﬁnite-state Markov chain, A is a
ﬁnite set of actions, A(i) g A denoting the set of actions available at state
i (we also denote by IC = {(i, a) : i E S,a E A(i)} the set of state-action
pairs), 73(a) defined in Equation (1) is the TPM of the controlled
Markov chain, g : [C —> IR is an immediate cost, and r : [C —> IRD is a
D-dimensional vector of immediate costs deﬁning the D constraints.

Denote by {zk,k= 0,1,...} and {ak,k= 0,1,...} the sequences Of
Observed states and actions. A policy is a prescription for taking actions
at each k. Actions may be taken in accordance with a random mechan-
ism, possibly a function Of the entire history Of the system up to time k.
To make this precise, let H k be the random vector Of previous states and
actions occurring up to time k and hk be the Observed history, i.e.
hk = (zo,ao,zl,a1,...,zk,ak). A policy ,u = (,uo,,u1,...,,uN) is a se-
quence prescribed by the decision maker that steers the dynamics of
the underlying system. If the history hk_1 is Observed up to time k,
then the decision maker chooses an action a e A(zk) with probability
Mk(a|hk—1 , Zk): Where

0 S Mk(a|hk—1,Zk) S 1,

Z Mk(a|hk—1,Zk) = 1-

aeA(zk)

The class of all control policies It is denoted by M. The initial state i of
the Markov chain and any given policy ,u determine a unique probability
measure Pf over the space of all trajectories of states and actions, which

correspondingly deﬁnes the stochastic processes Z, and Ak of the states
and actions for the controlled system.  denotes the corresponding ex-
pectation relative to which cost criteria are deﬁned.

DEFINITION 4. The inﬁnite-horizon expected average costs are

 

 

1 N
Gi, =limsu Ei‘ Z,A ,
( u) Napr+1kEZO .g( k k)
d 1 N d
R ' =l'm E.“ Z A f d=12... D.
(W) leolipN+1kX=g 1” k’ k)’ or ” ’

Given real numbers V1, V2,..., VD, we can formulate the Optimization
problem in its most general form: for an initial state i,

0P . minuEM I'll):
1 ' subject to Rd (i, ,u) 5 V1 for all d: 1,2,...,D.

The set of all policies that satisfy the constraints is called the feasible
region. Let G*(i) denote the Optimal value achieved by the Optimization
procedure. A feasible policy ,u* is Optimal if G*(i) = G(i, ,u*), for i e S.
The search space M is inﬁnite (possibly uncountable); however, one may
identify classes within M that possess beneﬁcial properties. We consider
three classes: Markov policies, stationary policies and stationary deter-
minist policies, denoted by MM, MS and M D, respectively. MM in-
cludes policies for which ,uk is only a function of Z], for any k. MS is a
subset of MM and includes policies for which ,uk is time invariant (not
dependent on k). This means that the probabilities that the control policy
,u assigns to different actions only depend on the state and not the time.
For example, for a PBN with transition probabilities deﬁned in Equation
(1) and any policy ,u 6 M5, the underlying stochastic process becomes a
stationary Markov chain with the set, Q(,u), of transition probabilities
deﬁned by

ail-(M) = Z pij(a)u(ali), (2)

aeA(i)

for all i, j e S. The third class, MD, is the set of all stationary determin-
istic policies such that ,u(a|i) is either 0 or 1 for every i e S. In this case,
,u :8 —> A is a single-valued transformation from the states to the
actions.

Usually in the context Of MDP problems, minimizing an Objective
function (the infmite—horizon expected total discounted cost or the ex-
pected average cost) with no constraints, i.e. D20, is carried out by
formulating a set of dynamic programming functional equations,
known as the Bellman optimality equations. Using these equations, it
can be shown that the Optimal control policies belong to the class MD
of policies. Hence, instead of searching the entire space of all history-
dependent randomized control policies for the Optimal solution, one can
focus only on a set Of coupled minimization problems over a (much
smaller) set of actions.

2.3 Suboptimal phenotype alteration

Our goal is to ﬁnd an intervention policy to maximally shift the long-run
probability mass of undesirable states to desirable ones. Several algo-
rithms have been proposed for intervention in Markovian GRNs that
avoid using a user-deﬁned cost function and work directly with the tran-
sition probabilities of the Markov chain associated with the network to
approximate this goal. These algorithms are motivated by heuristics and
Obtain suboptimal policies. Owing to space limitations, here we only list
the algorithms, with detailed descriptions being given in the
Supplementary Material: (i) mean-ﬁrst-passage-time (MFPT) control
policy (V ahedi et al., 2008), (ii) BOA control policy (Qian et al., 2009),
(iii) SSD control policy (Qian et al., 2009) (iv) and conservative SSD
(CSSD) control policy (Qian et al., 2009).

 

1 760

112 ﬁlm'spaumo[pIOIXO'soImuIIOIuIOIq/ﬁdnq IIIOII pepcolumoq

910K ‘09 lsnﬁnV no 2:

Intervention in GRNs

 

Under the MFPT, BOA, SSD and CSSD control policies, as well as
for user-deﬁned cost functions, there might be an introduction Of signiﬁ-
cant mass at states representing complications and harmful side effects to
healthy cells, even though these are classiﬁed in the set of desirable states
because they are not undesirable relative to the particular pathology Of
interest.

In Section 2.8.2, we will discuss a 10-gene network involving the gene
WNT5A and explain why an intervention that downregulates WNT5A
may have the beneﬁcial effect of suppressing metastatic phenotypes. For
this reason, networks involving WNT5A have been used in a number Of
control studies (Datta et al., 2003; Faryabi et al., 2009; Pal et al., 2006). In
particular, in Qian and Dougherty (2008), structural intervention consist-
ing Of a one-time change to the governing regulatory logic was applied to
a seven- gene network containing WNT5A with the aim downregulating it
in the long-run. Among the genes in the network, the minimum steady-
state probability mass for upregulated WNT5A was Obtained by perturb-
ing the function determining the status Of WNT5A. Unfortunately, this
intervention had the effect of placing a large mass (> 0.4) at a state that
had virtually no mass in the original SSD and in which STC2 is upregu-
lated. Several studies have shown that STC2 plays a role in carcinogen-
esis. For instance, STC2 is upregulated in breast and ovarian cancer cells,
following exposure to hypoxia (Law and Wong, 2010a), and high levels Of
STC2 expression are associated with increased invasiveness and metasta-
sis in ovarian cancer cells (Law and Wong, 2010b). Hence, the uncon-
strained Optimal structural intervention should be avoided. A better
alternative would be to perturb the machinery governing RETl, where
the results in Qian and Dougherty (2008) show that an appropriate per-
turbation of its regulatory logic reduces the steady-state mass of the
upregulated WNT5A states almost as much as direct WNT5A interven-
tion, and without the side effect Of signiﬁcantly upregulating STC2.

From a general perspective, it may be prudent to restrict the newly
introduced mass to states associated with known healthy phenotypes. To
achieve this end, the set D of desirable states can be further partitioned
into two categories, Dh representing known healthy states (those known
to be associated with a healthy phenotype) and Da representing ambigu-
ous states that may (or certainly will) lead to phenotypes that, although
not undesirable relative to the pathology Of immediate interest, are them-
selves known tO be undesirable or Of which nothing is known. The goal is
to ﬁnd a control policy that shifts the long-run probability mass of un-
desirable states as much as possible and at the same time keeps the long-
run probability mass Of each ambiguous state below some level. Being
that phenotypes are associated with attractors, a conservative approach is
to deﬁne the ambiguous states as those states in D that belong to the
nonattractor states Of the original network so that the control will not
introduce new attractors (phenotypes). This approach is not only conser-
vative but also does not require prior knowledge Of which states in D are,
or are not, associated with pathological phenotypes. In the WNT5A ex-
ample just cited, the intervention leading to the high-mass state in which
STC2 was upregulated would not have been allowed under this criterion.
The constrained SSD (conSSD) and constrained CSSD (conCSSD) con-
trol policies aim to ﬁnd such (suboptimal) interventions (Qian and
Dougherty, 2012).

None Of the six long-run—based heuristic algorithms use a rigorous
Optimization to ﬁnd Optimal intervention policies with respect to the Ob-
jectives and constraints.

2.4 Occupation measures and the primal LP

To proceed optimally in the case Of constrained problems, we note that it
has been shown that the Objective functions for different cost criteria are a
linear function of ‘Occupation measures’ for stationary control policies. A
salient consequence is that the original problem Of ﬁnding the Optimal
cost and control policy can be transformed into an LP, referred to as the
primal LP, where the Optimization variables are the occupation measures
GDerman, 1970; Kallenberg, 1983). The Optimal solutions to this primal

LP determine, as a one-tO-One relationship, the Optimal stationary control
policies (Altman, 1999). Let us ﬁrst state an assumption that we will make
throughout the rest Of this article.

ASSUMPTION 1. The MDP is assumed to be unichain, meaning that for
any ,u e MD, the corresponding Markov chain described by the transi-
tion probabilities Q(,u), deﬁned in Equation (2), has at most one ergodic
class and a (perhaps empty) set of transient states.

This assumption holds for the case of controlled PBNs, as the Markov
chain corresponding to a given PBN is ergodic due to random gene per-
turbation. We now deﬁne the occupation measures.

DEFINITION 5. For any given initial state i and policy ,u, and any state-
action pair j and a e A(j), the occupation measure is

1

N
— Pr(zk=j.Ak=a).
NH;

ngaa 1”“) =

In other words, under any policy ,u and given Z0 2 i, vﬁﬁ, ,u) is the

expected frequency, up to time N, Of entrances into state j when action a

is taken. Let vN (i, ,u) denote the matrix Of vﬁﬁ, ,u) over all j and a. Let

V(i, ,u) be the nonempty set of all limit points of the sequence
{vN(i,,u),N = 0,1,...}. Then, for any i e S, ,u E M and v E V(i,,u),

Z Z 11,-aged s G(i,.u),

jeS aeAG)
with equality holding for some I) e V(i, ,u)—in particular, it holds when
,u 6 MS (Altman, 1999). Moreover, the union Of all V(i, ,u) over all
,u E M, where the V(i, ,u) are singletons, is equal to the union Of all
V(i, ,u) over all ,u 6 MS (Derman, 1970). If ,u 6 MS, then the V(i, ,u)
are all singletons and independent Of the initial state i. Furthermore,
for vja(i, ,u) e V(i, ,u), the Markov chain with transition probabilities
Q(,u), deﬁned in Equation (2), has a unique invariant probability measure
vector 7t(,u), which satisﬁes

Til-(IL) = Z 11,-,6, a), for all i, j e s, (3)
aeA(j)
nu) = about). (4)
anm) = 1, mm) 2 0, for allj e s. (5)
jeS

Therefore, one only needs to search the space Of MS for the Optimal
solution by solving the following LP problem (Altman, 1999):

Elvin: Z vjagﬁaa),

jeS aeA(j)

LP2 I
Z Z vjardG,a)s Vd,d=1,2,...,D,
jeSaeAG)
2 VIP: 2 ViaPij(a),j€5,
subject to aGAG) iesaeAﬁ)
Z Z 1)ja=1:

jeS aeA(j)
via 2 0, for all j e S,a E A(j).

Let v* be a minimizing argument Of the LP2 problem. Under
Assumption 1, one can recover an Optimal randomized stationary
policy ,u* by

1)?“
* . _ ja
Mum—Efﬁ. @
aeA(j)
whenever 20640) 1);, >0, and if 20640) 1);, = 0, then ,u*(a|j) = 1 for some
a e A(j). As the Markov chain for a controlled PBN is ergodic, there are
no transient states. Hence, 2,1640) via 75 0.

 

1761

112 /810'S112umo[pIOIXO'soI112uIIOIHIOIq/ﬁd11q 111011 pepcolumoq

910K ‘09 lsnﬁnV uo 2:

M.R. Yousefi and ER. Daugherty

 

THEOREM 1. (Altman, 1999) The Optimization problems 0P1 and LP2
are equivalent, and there is a one-tO-One correspondence between their
feasible (and Optimal) solutions.

THEOREM 2. (Ross, 1989) If LP2 is feasible and ,u* is an Optimal policy
constructed by the aforementioned procedure, then there exists a list Of at
most |S| + D actions for ,u* so that ,u* requires randomization in at most
D states.

If there are no constraints on the expected average cost criteria,
i.e. D=0, then there is no randomization and ,u* e MD. The main
result of this section can be summarized as follows: Under
Assumption 1, for every policy in M, there exists a stationary policy
in MS that achieves the same limit point for the occupation measures;
hence, we only need to consider the set MS for any Optimization prob-
lem involving occupation measures of state and action in its Objective
and constraints.

2.5 Maximal phenotype alteration

We desire an intervention policy that maximally shifts the long-run prob-
ability mass of undesirable states to desirable states. This Objective func-
tion essentially concerns the long-run behavior Of the occupation
measures marginalized over the actions. Thus, based on the discussion
at the end of Section 2.4, we can limit the policy space to MS without loss
Of generality.

Let A 2 AG) 2 {0, 1} for allj e S. If policy ,u 6 MS, then the amount
of shift in the aggregated probability of undesirable states for a PBN
controlled under ,u is deﬁned as

AWOL) = 2 7Ti — Z 71101“): (7)

jeu jeu

where n and 7t(,u) are the unique vectors Of the invariant probability
measure for the Markov chains governed under the TPMs 73 and
Q(,u), respectively, which also satisfy Equations (3) to (5). In general,
—1 5 Anu(,u) 5 1 and our goal is to maximize it.

LEMMA 1. Atty is maximized by solving the LP2 problem with the
immediate cost function g being 1 for undesirable states and 0 otherwise.

PROOF. It can be easily seen from Equation (7) that maximizing
Artu(,u) is equivalent to minimizing Zieu nj(,u), as n is ﬁxed. Also, if
we let the immediate cost function g(Zk, Ak) take the form:

, 1, if j G Ll,
go’ a) _ {0, otherwise,

we have

Z Z vjaﬁ, M)g(j,a) = Z Z vita. u) = Zulu).

jeS aeA jeu aeA jeu

for all i and any ,u 6 MS, where we used Equation (3) for the second
equality. Therefore, it can be veriﬁed that we need to again solve LP2,
with the suggested choice of g, to ﬁnd the Optimal I), which at the same
time maximizes the shift mm.

2.5 .1 Unconstrained optimal intervention If there are no constraints
on the cost criteria (D = 0) for the shift maximization problem, then we
can rewrite LP2 as

min 2 Z vja:
v jeUaeA
Z 1)ja= Z Z 1)ia pij(a)aj E 8,
LP3 I aeA ieS aeA
subject to Z 2 via: 1,
jeS aeA
via 2 0,for all j e S,a E A.

The unconstrained (UC) Optimal intervention policy ,uf‘lc can be
constructed using Equation (6) when v* yields the minimum in LP3. As
there are no constraints, ,uf‘lc e MD so that ,uf‘lc is stationary and
deterministic.

2.5.2 Phenotypically constrained optimal intervention The LP3
problem aims to locate an Optimal control policy inducing maximal
shift from the undesirable mass to eradicate phenotypes associated with
the pathology Of interest. However, when there are ambiguous states in
the network, the Optimization problem can be stated as locating a control
policy that maximizes the shift of undesirable mass while satisfying an
upper bound constraint for newly introduced steady-state mass into am-
biguous states in Da. Similar to the choice of g in the proof Of Lemma 1,
one can deﬁne rd for d = 1,2, . . . , |Da|, each corresponding to a state
jd 6 Da. The phenotypically constrained (PC) Optimization problem can
be formulated as

min: 2 via,
v jeUaeA
2% 5 Vd,d=1,2,...,|Da|,jd e 1),,
(IE
LP4 3 _ Z 1)ja= Z Z 1)ia pij(a)9j E 8,

subject to aeA ieS aeA
Z 2 via: 1:
jeSaeA

via 2 0,forallj ES,aEA.

One can assume that Vd = A for all d. The PC Optimal intervention
policy use can be constructed using Equation (6) when v* yields the
minimum in LP4. As there are |Da| additional constraints on the Opti-
mization problem, age 6 MS and it requires randomization in at most
|Da| states.

2.6 Computational complexity

In any Optimization problem, computational complexity is a concern. In
particular, how well does the algorithm scale to larger networks? In our
case, there are |S| x |A| decision variables and |S| equality constraints in
both LP3 and LP4 problems. LP4 has |Da| additional inequality con-
straints. We use IBM ILOG CPLEX Optimizer using a dual simplex
method to solve both LP3 and LP4 problems. Although, the worst-case
computational complexity Of the simplex method has been shown to be
exponential in the number of decision variables and constraints, prob-
abilistic analyses of the simplex method have indicated that the average
complexity Of this method is polynomial. Moreover, it has been proven
that solving an LP, in general, requires a number Of Operations polyno-
mial in the number of decision variables and constraints. Speciﬁcally,
Karmarkar’s algorithm, which belongs to the general class Of interior
point methods, solves LPs in polynomial time with reasonable efﬁciency
(Megiddo, 1987). The computational complexities Of all the suboptimal
policies mentioned in the article are also reported to be either linear or
polynomial.

Besides the guaranteed Optimality Of our method over the greedy al-
gorithms, the linear programming approach is particularly useful when
Optimization problems involving nonlinear functions or side constraints
are considered GDeman, 1970). In such cases, using a dynamic program-
ming approach makes the analysis much more complicated and compu-
tationally prohibitive. Based on our estimates and the reported size Of the
linear programming problems that have been solved using IBM ILOG
CPLEX Optimizer, it is possible with the current technology to ﬁnd the
Optimal UC and PC policies for networks with 20 genes. Hence, it avoids
the kinds Of model reduction methodologies that have been developed in
the context Of GRN control (Ghaffari et al., 2010; Ivanov et al., 2007,
2010; Qian et al., 2010), thereby avoiding the loss Of Optimality engen-
dered by reductions.

 

1 762

112 /810'S112umo[pIOIXO'soI112uIIOIHIOIq/ﬁd11q 111011 pepcolumoq

910K ‘09 lsnﬁnV uo 2:

Intervention in GRNs

 

2.7 Robustness

As with any Optimization procedure, the algorithm is Optimal so long as
the assumed model is correct. In the case of GRNs, model ﬁdelity is
affected by several factors: data extraction, discretization, gene selection
and network generation. Hence, robustness becomes an issue. Robustness
refers to an algorithm’s performance on models that are close to, but not
equal to, the model on which it has been derived. The robustness Of
stationary control for PBNs has been considered in Pal et al. (2008),
and a robust intervention strategy has been Obtained by minimizing the
worst-case cost over an uncertainty class of networks. Such a minimax
control approach is typically conservative because it gives equal import-
ance to extreme cases. Thus, a Bayesian approach was formulated in Pal
et al. (2009). A Bayesian approach requires a prior distribution on the
uncertainty class of networks and therefore is dependent on signiﬁcant
prior knowledge, which may not be available. Here, we describe the per-
turbation bounds discussed in Pal et al. (2008), where uncertainty is
studied in the context Of the transition probabilities. The intervention
strategy is derived on~the estimated TPM, 73, and is applied to the
actual network TPM, 73. The basic question is that, for a given control
policy, how does the mismatch between 73 and 75 affect the SSD of the
controlled network? Although the discussion in Pal et al. (2008) applies to
stationary deterministic control and the equivalent and our general set-
ting is Of wider scope, both the UC and PC algorithms have stationary
control so that the analysis of Pal et al. (2008) is applicable.

Given that the class of allowed interventions consists of ﬂipping the
value Of a gene, application Of a stationary policy ,u derived from the
uncontrolled estimated TPM 73 converts 73 to a controlled TPM 9(a),
where 9(a) = T (a)? and T(u) represents a stochastic matrix, which has
at most |A| nonzero entries adding up to 1 in each row (these nonzero
entries are found from the control policy ,u). Let It and 7t(,u) denote the
SSDs corresponding to 73 and Q(,u), respectively. With 75 being the actual
uncontrolled TPM, gm) 2 T (1075 is the controlled TPM resulting from
applying T(u) to  Let 7? and ﬂu) denote the SSDs of 75 and éﬂt),
respectively. Robustness concerns the difference 7i(,u) — 7t(,u) based on
the estimation error E é 73 — 

Assuming that the actual and estimated networks possess the same
state space, which allows for different topologies and different regulatory
functions, the SSD difference can be bounded by In— ftqu K||E||00
where q: 1 or 00 and K >0 are some constants. K is called a condition
number. Some condition numbers will yield tighter bounds than the
others (ChO and Meyer, 2001). Pal et al. (2008) considered the ergodicity
coeﬂficient

11(7)) = sup |xT73
xT 1=1
le=0

 

19

where 1 denotes a column vector Of appropriate length having all entries
equal to 1 (Seneta, 1988). If 11(73) 75 1, then @al et al., 2008)

~ 1
WC“) — 7T(I1«)|15 WHEN“;-

Hence if, the error is small, then the SSD of the controlled actual
process is close to the SSD of controlled estimated process from which
the control policy has been derived, the bound depending on the ergodi-
city coefﬁcient. Other perturbation bounds using different condition
numbers were examined via simulations in Pal et al. (2008).

Having discussed the general case of Markov chains, Pal et al. (2008)
went on to relate the general results to PBNs by showing how different
classes of possible uncertainties for a PBN would translate into uncer-
tainties in the TPM for the corresponding Markov chain.

2.8 Simulation setup

We design simulation studies on synthetically generated and two real
networks. We ﬁnd Optimal and suboptimal control policies for both

UC and constrained problems and calculate their performance under
different parameter choices. To reduce the complexity Of graphs and
tables, we do not consider the MFPT and BOA policies. From the per-
spective Of demonstrating the Optimality Of the UC and PC algorithms,
there is no loss in this omission, as it has been amply demonstrated that
the SSD and CSSD policies generally outperform the MFPT and BOA
policies relative to the criterion Of shifting the SSD (Qian et al., 2009). For
constrained Optimization problems, the set of ambiguous states, Da, and
an upper-bound A must be deﬁned. In practice, these should be deﬁned
based on biomedical knowledge related to the GRN and treatment Ob-
jectives. Here, we take an approach similar to Qian and Dougherty
(2012). For PBNs, we deﬁne Da = {i e D : m 5 t}, where n is the invari-
ant measure Of the TPM Of the uncontrolled network and t is a user-
deﬁned threshold. We let I = 1/2” and set A = maxiepa m. For BNps,
ambiguous states are the states in D that belong to the nonattractor states
Of the original BNs so that the control will not introduce new attractors.

2.8.1 Synthetic networks We generate 2500 random PBNs and 2500
random BNps with different parameters and report the average perform-
ance across all networks as well as some statistics on the performance of
each intervention policy. The performance Of Optimal policies might not
substantially exceed that of suboptimal policies when averaged across
randomly generated networks for a couple Of reasons. First, randomly
generated networks may have certain structures making them unrespon-
sive to the intervention policies. Second, many networks might possess
structure for which the suboptimal and Optimal policies are almost iden-
tical. Be that as it may, the key point is that there are networks for which
the Optimal policy signiﬁcantly outperforms the suboptimal ones. Here,
we use the difference in the amounts of shifts made by Optimal and sub-
optimal policies to quantify the gain made by using the Optimal policy:

Au) = Mam) — Axum) = Zulu) — Eva-(Ir).

jeu jeu

where 7t(,u*) and 7t(,u) denote the invariant probability measures for the
network under the Optimal and suboptimal policies, respectively. Clearly,
0 5 A(u) 5 1. For each network A(u) is calculated for all previously
discussed suboptimal policies, for the UC and constrained problems sep-
arately. As the networks are randomly generated, A(u) is a random
variable. To show the effectiveness of Optimal policies, we graph the
tail probabilities, i.e. the empirical complementary cumulative distribu-
tion function (CCDF), Of A(u).

To keep the computational time tractable, we consider instantaneously
random PBNs, the states representing the GAPs at any given time, and
n = 6,7 or 8 genes. The state space is 8: {0,1,2, ...,2” — 1} and is
generated from four equally likely constituent BNs with the maximum
number Of predictors (i,- 5 K for all i) for each Boolean function set to two
or three. The bias of each PBN is the probability that each Boolean
regulatory function takes on the value 1. We assume that it is taken
randomly from a beta distribution with mean 0 e {0.25,0.5,0.75} and
the standard deviation 0.01 , thereby affecting the dynamics of a randomly
generated BN. The gene perturbation probability is either 0.001 or 0.01.
A similar setting is used for generating BNps, except that there is only one
constituent BN. For any given BNp and PBN, the TPMs Of the corres-
ponding Markov chains are computed as explained in Section 2.1. We
choose the control and target genes to be the least and most signiﬁcant
bits in the binary representation of states, respectively, and assume that
downregulation Of the target gene is undesirable.

The actual construction is done in the following manner. For each
constituent BN in a random PBN, we ﬁrst randomly select the predictors
Of each gene with all genes having the same probability Of being selected.
The value Of each gene given the values Of its predictors is now deter-
mined by a Bernoulli random variable whose probability Of being 1
equals the bias (the bias is also randomly generated from a beta distri-
bution with a mean 0 and a given standard deviation). This process will
construct a random truth table corresponding to a BN. Having all the

 

1 763

112 /810'S112umo[pIOIXO'soI112uIIOIHIOIq/ﬁd11q 111011 pepcolumoq

910K ‘09 lsnﬁnV uo 2:

M.R. Yousefi and ER. Daugherty

 

constituent BNs and gene perturbation probability deﬁned, we directly
calculate the TPM of an instantaneously random PBN without actually
constructing it.

2.8.2 Metastatic melanoma network A metastatic melanoma net-
work is derived from gene expression data collected in a study Of meta-
static melanoma (Bittner et al., 2000; Dissanayake et al., 2008;
Weeraratna et al., 2002). By manipulating the concentration level Of
WNT5A protein secreted by a melanoma cell line, one can directly
affect the metastatic status of the cell as measured by the standard
in vitro assays for metastasis. For intervention purposes, antibodies can
be designed to bind with WNT5A and block it from activating its recep-
tor. This will signiﬁcantly weaken its ability to induce a metastatic pheno-
type, which suggests that reducing the WNT5A gene’s action could
reduce the chance of a melanoma metastasizing. Therefore, it is desirable
to downregulate WNT5A as much as possible through appropriate inter-
vention mechanisms (Datta et al., 2003; Qian and Daugherty, 2008). We
use a network composed Of 10 genes from a set Of 587 genes (Qian and
Daugherty, 2012). The genes, listed from the most signiﬁcant bit to the
least signiﬁcant bit, are WNT5A, pirin, S100P, RETl, MMP3, PHOC,
MARTl, HADHB, synuclein and STC2. This ordering Of genes is only
for demonstration purposes and does not affect our analysis. The regu-
latory relationships between these genes are presented in Table 1. We
construct a BNp with p=0.001 and assume that the upregulation of
WNT5A is undesirable. For the control genes, we choose each gene
in the network as the control gene and ﬁnd the Optimal and subopti-
mal intervention policies in addition to their effects on the steady-state
shift.

2.8.3 Mammalian cellcycle network To characterize the dynamical
behavior Of normal mammalian cells during the cell cycle, Faure et al.
(2006) proposed a Boolean-logic regulatory network containing three
key genes: Cyclin D (Cch), retinoblastoma (Rb) and p27. For a
normal mammalian organism, cell division is coordinated with overall
growth via extracellular signals controlling the activation of Cch.
These signals indicate whether a cell should undergo cell division or
remain in a resting state. Rb is a tumor-suppressor gene and is ex-
pressed in the absence of the cyclins that inhibit Rb by phosphorylation.
Gene p27 is also active in the absence of the cyclins. An active p27
blocks the action of Cch or CycA and, hence, Rb can also be ex-
pressed, even in the presence Of Cch or CycA, resulting in a stop in the
cell cycle. In the wild-type cell-cycle network, when p27 is active, the cell
cycle can be stopped. Following a proposed mutation for this network,
we assume that p27 can never be activated (always OFF), thereby
creating a situation where both Cch and Rb might be inactive
(Faure et al., 2006). Under these conditions, the cell can cycle in the
absence of any growth factor, thereby causing undesirable proliferation.
The mutated network has nine genes, Cch, Rb, E2F, Cch, CycA,
Cdc20, th1, chH10 and CycB, ordered from the most signiﬁcant bit
to the least signiﬁcant bit in the binary representation. Similar to the
metastatic melanoma network, this ordering is only for the sake of
presentation. Table 2 lists the regulatory functions Of the mutated
cell-cycle network following Boolean logic.

We construct an instantaneously random PBN for this mutated net-
work, where depending On the state Of the extracellular signal that deter-
mines the state Of Cch as being ON or OFF, there are two constituent
BNs, which we assume to be equally likely. We set p=0.001. As cell
growth in the absence of growth factors is undesirable, the undesirable
states are those for which Cch and Rb are both downregulated (OFF).
We consider all genes except Cch and Rb as the possible control gene
and ﬁnd the Optimal and suboptimal intervention policies in addition to
their effects on the steady-state shift.

Table 1. Regulatory functions of a metastatic melanoma network

 

 

Gene Node Predictor functions

WNTSA v1 (V3 A vs A v_6) v (v_5 A v6)

pirin v2 (V—l A W A vs) V (v1 A W A V_5)

 V3 V7

RETl v4 (v_1 /\ v2 /\ v4) V (V? /\ V4)

MMP3 v5 (V4 /\ V9) V 

PHOC v6 (v_4 A W) v (V4 A V7 A v10)

 V7 V7

HADHB v8 (v1 A vs) v (v_5 A V—9) v (v1 A v_5 A V9)
synuclein v9 (v_1 A W A v—lo) v (V4 A W A v10) v V7
 V10 V—3

 

Table 2. Regulatory functions of a mutated mammalian cell-cycle
network

 

 

Gene Node Predictor functions

Cch v1 Extracellular signal

Rb v2 (V—l A W A V_5 A W)

E2F v3 (v? A V_5 A W)

CYCE V4 (V3 /\ W)

CycA V5 (v3 A n A V_6 A W)
V(vs Av—zAVsAWD

 V6 V9

th1 v7 (v_5 A W) v v6

chHlO v8 W v (v7 /\ vg /\ (v6 V vs V v9))

CycB V9 (V—6 A W)

 

3 RESULTS AND DISCUSSION

The entire set of simulation results can be found in the
Supplementary Material. Here, we provide some results that rep-
resent the general trends observed in the simulations.

3.1 Synthetic networks

We present results for the synthetic networks with seven genes.
Corresponding results for six and eight genes are given in the
Supplementary Material, where it is evident that the same trends
are observed. Figure 1 shows the shift in the probability mass of
undesirable states induced by the optimal and suboptimal poli-
cies, Equation (7), averaged over all random networks with vari-
ous structural properties and for different values of 9. These
graphs clearly show the optimality of [12:0 and age compared
with their suboptimal counterparts. In general, the suboptimal
policy ,ucssd yields the best results, close to optimal, among the
suboptimal policies. However, ,uconcssd does not induce as much
shift as age, which is due to the fact that the search space of
conCSSD policy is M D, whereas it is MS in the LP4 problem.
The amount of average shift induced by age is signiﬁcantly less
than ,uﬁc owing to the additional constraints imposed on the
Optimization LP4 problem.

 

1 764

112 /810'S112umo[pIOIXO'soI112uIIOIHIOIq/ﬁd11q 111011 pepcolumoq

910K ‘09 lsnﬁnV uo 2:

Intervention in GRNs

 

 

SSD -CSSD -UC I:|conSSD .conCSSD -Pc|
0.3

 

 

A
III
. v

 

 

 

 

 

 

 

 

 

Average shift
Average shift

   

0.25 0.5 0.75 0.25 0.5 0.75
Bias (9) Bias (0)

 

 

 

 

 

 

 

 

 

 

Average shift
Average shift

   

0.25 0.5 0.75 0.25 0.5 0.75
Bias (0) Bias (6)

 

(b)| SSD -CSSD -UC I:IconSSD .conCSSD -Pc|
0.3

 

 

 

 

 

 

 

 

 

 

 

   

it}
.5 0.2
“6’0
3
> 0.1
<

0 0

0.25 0.5 0.75 0.25 0.5 0.75
Bias (0) Bias (6)

 

_o
W
o
w

 

 

 

 

 

 

 

 

 

 

 

 

 

K = 3 K = 3
d3 elf.
a 0.2 p = 0.01 a 0.2 p = 0.001
30.1   30-1
0

 
       

0.25 0.5 0.75 0.25 0.5 0.75
Bias (9) Bias (0)

Fig. 1. Shift in the steady-state mass of undesirable states averaged across
randomly generated networks with seven genes, various 1c and gene per-
turbation probabilities p: (a) PBN; GI) BNp

Figure 2 shows the empirical CCDF Of A(,u) for different
suboptimal intervention policies ,u, and for the UC and PC prob-
lems separately, across 2500 random PBNs with seven genes,
IC = 3 and p=0.001. The graphs on the left column indicate
that UC optimal and CSSD policies induce similar amounts Of
shift with a high probability. However, for the constrained prob-
lem, it is clear that PC optimal policies outperform conCSSD
policies with a (realistically) signiﬁcant probability. For example,
for Optimal policy ago, with probability 0.1, we achieve an
improvement of > 5% over ,uconcssd suboptimal policy.

The results are somewhat different for BNps. Figure 3 dem-
onstrates the empirical CCDF of A(,u.) for different suboptimal
intervention policies ,u, corresponding to the UC and PC prob-
lems, across 2500 randomly generated BNps with seven genes,
IC = 3 and p = 0.001. With high probability, higher than those for
PBNs, we achieve better performance by using optimal policies.
For instance when 9 = 0.5, this ﬁgure shows that the optimal
policy achieves an improvement of > 5% over ,uconcssd with prob-
ability 0.2.

3.2 Real networks

As there is only one network, we cannot report the average shift
or CCDF Of A(,u.) for different suboptimal intervention policies.

 

 

 

 

 

 

 

 

 

 

 

 

 

1 1
= — ﬂ = — I“

0.8 0  —.ussd 0.8 " 6  ‘— .uconssd |
m _ - . pcssd m | - - - #concssd
Q 0.6 Q 0.6 -,

80.4 80.4 ‘.‘
0.2 0.2 
c 0 ’ ‘ ‘ '
0 0.05 0.1 0.15 0.2 0.25 0.3 0 0.05 0.1 0.15 0.2 0.25 0.3
Shift difference (A) Shift difference (A)
1 1
0.8 0 = 0.5 0.8 g‘ 0 = 0.5
|
g o 6 ‘5 0.6 ‘.
8 o 4 8 0.4 
o 2 0.2 ‘X
c c “ ~ -
0 0.05 0.1 0.15 0.2 0.25 0.3 0 0.05 0.1 0.15 0.2 0.25 0.3
Shift difference (A) Shift difference (A)
1 1

Q8 19 = 0.75 0.8: (9 = 0.75
‘5 0.6 E 0.6 
80.4 80.4 ‘.‘

0.2 0.2 ‘x‘

G ' ' ' c - 7 ‘ ‘ -
0 0.05 .1 0.15 0.2 0.25 0.3 0 0.05 0.1 0.15 0.2 0.25 0.3

 

 

 

 

 

 

0
Shift difference (A) Shift difference (A)

Fig. 2. Empirical CCDF Of A for different intervention policies across
2500 random PBNs with seven genes, K = 3 and p = 0.001. The left and
right columns correspond to the UC and PC Optimal policies, respectively

 

 

= — l1
0.8 0  — ﬂssd 0.8
- - - .ucssd

= — ﬂ
0  ‘— IuCOIlSSd H
- - - ﬂconcssd

 

 

  

 

 

O 0.05

 

 

 

 

 

 

 

    

 

 

 

 

 

 

0.1 0.15 0.2 0.25 0.3 0 0.05 0.1 0.15 0.2 0.25 0.3
Shift difference (A) Shift difference (A)
1 1
o 8 0 = 0.5 _ o 8 t9 = 0.5
127-1 H" I
8 0.6 Q 06
00.4 80.4 .
0.2 : o 2 _________ _ _
c I - - - - - c I I - - T T ' ' ' - - - - - -
0 0.05 0.1 0.15 0.2 0.25 0.3 0 0.05 0.1 0.15 0.2 0.25 0.3
Shift difference (A) Shift difference (A)
1 1
08 0:0.75 08 19:0.75
‘5 0.6
80.4
0.2
G .
0 0.25 0.3 0 0.0 0.25 0.3

0.05 011 0.15 0.2
Shift difference (A)
Fig. 3. Empirical CCDF Of A for different intervention policies across

2500 random BNps with seven genes, K = 3 and p = 0.001. The left and
right columns correspond to the UC and PC Optimal policies, respectively

Table 3 shows the shifts made by all control policies for different
potential control genes for the metastatic melanoma network.
The aggregated probability mass of undesirable states for the
uncontrolled network is Zieu nj 2 0.1711. The maximum shift
induced by the UC optimal policy occurs when either MMP3 or
PHOC are the control genes. However, the maximum shift for
the PC optimal policy occurs when MARTl is the control gene,
which is an immunogen that the body sometimes recognizes and
attacks via the immune system (Dissanayake et al., 2008; Qian
and Daugherty, 2012). A good amount of shift is achieved for
both constrained and UC problems with MARTl. Thus,
MARTl can be viewed as a potential control gene for interven-
tion. Overall, however, the results indicate that the best control
gene depends on ones objectives and how much risk one is

 

1 765

112 /810'S112umo[pIOIXO'soI112uIIOIHIOIq/ﬁd11q 111011 pepcolumoq

910K ‘09 lsnﬁnV 00 2:

M.R. Yousefi and ER. Daugherty

 

Table 3. Shift in the steady-state mass of undesirable states of the metastatic melanoma network for different control genes and different control policies

 

 

Control WNT5A pirin S100P RETl MMP3 PHOC MARTl HADHB synuclein STC2

SSD 0.0632 0.1630 0.1679 0.1646 0.1701 0.1701 0.1684 0.0000 0.0029 0.1674
CSSD 0.0675 0.1632 0.1679 0.1681 0.1701 0.1701 0.1684 0.0000 0.0029 0.1674
UC 0.0677 0.1632 0.1681 0.1681 0.1701 0.1701 0.1684 0.0000 0.0029 0.1674
conSSD 0.0000 0.0003 0.1548 0.0000 0.0054 0.0041 0.0165 0.0000 0.0013 0.1146
conCSSD 0.0675 0.0043 0.0080 0.0005 0.0059 0.0057 0.0016 0.0000 0.0029 0.0040
PC 0.0677 0.0548 0.0139 0.1473 0.0076 0.0076 0.1682 0.0000 0.0029 0.0070

 

Table 4. Shift in the steady-state mass of undesirable states of the mutated mammalian cell-cycle network for different control genes and different control

 

 

policies

Control E2F Cch CycA Cdc20 th1 chH10 CycB
SSD 0.1972 0.0573 0.0334 0.0565 0.0835 0.0122 0.0598
CSSD 0.1972 0.0573 0.0335 0.0836 0.0837 0.0138 0.0843
UC 0.1972 0.0573 0.0335 0.0836 0.0837 0.0138 0.0843
conSSD 0.0374 0.0207 0.0329 0.0119 0.0327 0.0001 0.0244
conCSSD 0.0571 0.0573 0.0332 0.0332 0.0506 0.0118 0.0254
PC 0.1857 0.0573 0.0332 0.0565 0.0509 0.0120 0.0481

 

willing to take. The seemingly intuitive choice of directly perturb-
ing WNT5A to control WNT5A (ﬁrst column) only produces
the best result with the conSSD policy. This is because of feed-
back in the network. Not only can a direct approach absent
constraint produce unwanted phenotypes, a direct approach
may not even achieve the desired reduction in long-run probabil-
ity mass of the target gene on account of complicated feedback
loops.

The aggregated mass of undesirable states for the uncontrolled
mammalian cell cycle network is Zieu nj 2 0.2012. Table 4 gives
the amount of shift made by all control policies for different
control gene candidates. The table shows that choosing E2F as
the control gene induces the maximum shift under different con-
trol policies. Thus, E2F can be considered as a potential control
gene for intervention. It is also evident that the difference be-
tween the performances of PC optimal policies and conCSSD is
signiﬁcant.

4 CONCLUDING REMARKS

Heretofore, two external control approaches have been taken to
shift the steady-state mass of a GRN: (i) use a subjectively
deﬁned cost function for which desirable shift of the steady-
state mass is a by-product and (ii) use heuristics to design a
greedy algorithm. The present article uses a linear programming
approach to optimally shift the steady-state mass and therefore
outperforms both of the preceding approaches. Moreover, it
does with less computational overhead and therefore can be
applied to larger networks. Owing to its generality, the same
basic LP structure is used for both unconstrained and con-
strained optimization problems. Thus, at least in the case of

Markovian GRNs, in particular, PBNs, previously proposed
methods can be abandoned whenever the goal is to optimally
shift the steady-state mass.

Conflict of Interest: none declared.

REFERENCES

Altman,E. (1999) Constrained Markov Decision Processes. Chapman & Hall/CRC,
Boca Raton, FL, USA.

Bittner,M. et al. (2000) Molecular classiﬁcation of cutaneous malignant melanoma
by gene expression proﬁling. Nature, 406, 536—540.

Brun,M. et al. (2005) Steady-state probabilities for attractors in probabilistic
Boolean networks. Signal. Process, 85, 1993—2013.

Ching,W.K. et al. (2009) Optimal control policy for probabilistic boolean networks
with hard constraints. IE T Syst. Biol., 3, 90—99.

Cho,G.E. and Meyer,C.D. (2001) Comparison of perturbation bounds for the sta-
tionary distribution of a Markov chain. Linear Algebra Appl., 335, 137—150.

Datta,A. et al. (2003) External control in Markovian genetic regulatory networks.
Mach. Learn, 52, 169—191.

Datta,A. and Dougherty,E.R. (2006) Introduction to Genomic Signal Processing with
Control. CRC Press.

Derman,C. (1970) Finite State Markovian Decision Processes. Academic Press, New
York.

Dissanayake,S.K. et al. (2008) Wnt5a regulates expression of tumor-associated anti-
gens in melanoma via changes in signal transducers and activators of transcrip-
tion 3 phosphorylation. Cancer Res, 68, 10205—10214.

Dougherty,E.R. and Datta,A. (2005) Genomic signal processing: diagnosis and
therapy. IEEE Signal Process. Mag., 22, 107—112.

Dougherty,E.R. et al. (2010) Stationary and structural control in gene regulatory
networks: basic concepts. Int. J. Syst. Sci, 41, 5—16.

Faryabi,B. et al. (2007) On approximate stochastic control in genetic regulatory
networks. IET Syst. Biol., 1, 361—368.

Faryabi,B. et al. (2009) Intervention in context-sensitive probabilistic
Boolean networks revisited. EURASIP J. Bioinform. Syst. Biol., 2009, Aritcle
ID 360864.

 

1 766

112 /810'S112umo[pIOIXO'soI112uIIOIHIOIq/ﬁd11q 111011 pepcolumoq

910K ‘09 lsnﬁnV 00 2:

Intervention in GRNs

 

Faure,A. et al. (2006) Dynamical analysis of a generic Boolean model for the control
of the mammalian cell cycle. Bioinformatics, 22, e124—e131.

Ghaffari,N. et al. (2010) A COD-based reduction algorithm for des-
igning stationary control policies on Boolean networks. Bioinformatics, 26,
1556—1563.

Ivanov,I. et al. (2007) Dynamics preserving size reduction mappings for probabil-
istic Boolean networks. IEEE Trans. Signal Process, 55, 2310—2322.

Ivanov,I. et al. (2010) Selection policy-induced reduction mappings for Boolean
networks. IEEE Trans. Signal Process, 58, 4871—4882.

Kallenberg,L.C.M. (1983) Linear Programming and Finite Markovian Control
Problems. Mathematisch Centrurn, Amsterdam.

Kauffman,S.A. (1993) The Origins of Order: Self-Organization and Selection in
Evolution. Oxford University Press, New York, NY, USA.

Law,A. and Wong,C. (2010a) Stanniocalcin-2 is a Hif-l target gene that promotes
cell proliferation in hypoxia. Exp. Cell Res, 316, 466—476.

Law,A. and Wong,C. (2010b) Stanniocalcin-2 promotes epitheliahnesenchymal
transition and invasiveness in hypoxic human ovarian cancer cells. Exp. Cell
Res, 316, 3425—3434.

Megiddo,N. (1987) On the complexity of linear programming. In: Advances in
Economic Theory: Fifth World Congress. Cambridge University Press, New
York, NY, USA, pp. 225—268.

Pal,R. et al. (2006) Optimal inﬁnite-horizon control for probabilistic Boolean net-
works. IEEE Trans. Signal Process, 54, 2375—2387.

Pal,R. et al. (2008) Robust intervention in probabilistic Boolean networks. IEEE
Trans. Signal Process, 56, 1280—1294.

Pal,R. et al. (2009) Bayesian robustness in the control of gene regulatory networks.
IEEE Trans. Signal Process, 57, 3667—3678.

Qian,X. and Dougherty,E.R. (2008) Effect of function perturbation on the steady-
state distribution of genetic regulatory networks: optimal structural interven-
tion. IEEE Trans. Signal Process, 56, 4966—4976.

Qian,X. and Dougherty,E.R. (2012) Intervention in gene regulatory networks via
phenotypically constrained control policies based on long-run behavior. IEEE
ACM Trans. Comput. Biol., 9, 123—136.

Qian,X. et al. (2009) Intervention in gene regulatory networks via greedy control
policies based on long-run behavior. BM C Syst. Biol., 3, 61.

Qian,X. et al. (2010) State reduction for network intervention with probabilistic
Boolean networks. Bioinformatics, 26, 3098—3194.

Ross,K.W. (1989) Randomized and past-dependent policies for Markov decision
processes with multiple constraints. Oper. Res, 37, 474—477.

Seneta,E. (1988) Perturbation of the stationary distribution measured by ergodicity
coefﬁcients. Adv. Appl. Probab., 20, 228—230.

Shmulevich,I. et al. (2002a) Probabilistic Boolean networks: a rule-based uncer-
tainty model for gene regulatory networks. Bioinformatics, 18, 261—274.

Shmulevich,I. et al. (2002b) Gene perturbation and intervention in probabilistic
Boolean networks. Bioinformatics, 18, 1319—1331.

Shmulevich,I. et al. (2002c) Control of stationary behavior in probabilistic Boolean
networks by means of structural intervention. J. Biol. Syst., 10, 431—445.

Vahedi,G. et al. (2008) Intervention in gene regulatory networks via a stationary
mean-ﬁrst-passage—time control policy. IEEE Trans. Biomed. Eng., 55,
2319—2331.

Weeraratna,A.T. et al. (2002) Wnt5a signaling directly affects cell motility and in-
vasion of metastatic melanoma. Cancer Cell, 1, 279—288.

Xiao,Y. and Dougherty,E.R. (2007) The impact of function perturbations in
Boolean networks. Bioinformatics, 23, 1265—1273.

Youseﬁ,M.R. et al. (2012) Optimal intervention strategies for therapeutic methods
with ﬁxed-length duration of drug effectiveness. IEEE Trans. Signal Process,
60, 4930—4944.

 

1 767

112 /810'S112umo[pIOIXO'soI112uIIOIHIOIq/ﬁd11q 111011 pepcolumoq

910K ‘09 lsnﬁnV 00 2:

