ORIGINAL PAPER

Vol. 29 no. 18 2013, pages 2245—2252
doi:10. 1093/bioinformatics/btt386

 

Sequence analysis

Advance Access publication July 3, 2013

Leveraging reads that span multiple single nucleotide
polymorphisms for haplotype inference from sequencing data

Wen-Yun Yang‘ ’2, Farhad Hormozdiari1,Zhanyong Wangl, Dan Hes, Bogdan Pasaniuc

and Eleazar Eskin1’2’6’*

2,4,5!*

1Department of Computer Science and 2Inter—Departmental Program in Bioinformatics, University of California,
Los Angeles, CA 90095, USA, 3IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA, 4Department of
Pathology and Laboratory Medicine, 5Jonsson Comprehensive Cancer Center and 6Department of Human Genetics,

University of California, Los Angeles, CA 90095, USA
Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: Haplotypes, defined as the sequence of alleles on one
chromosome, are crucial for many genetic analyses. As experimental
determination of haplotypes is extremely expensive, haplotypes are
traditionally inferred using computational approaches from genotype
data, i.e. the mixture of the genetic information from both haplotypes.
Best performing approaches for haplotype inference rely on Hidden
Markov Models, with the underlying assumption that the haplotypes of
a given individual can be represented as a mosaic of segments from
other haplotypes in the same population. Such algorithms use this
model to predict the most likely haplotypes that explain the observed
genotype data conditional on reference panel of haplotypes. With
rapid advances in short read sequencing technologies, sequencing
is quickly establishing as a powerful approach for collecting genetic
variation information. As opposed to traditional genotyping-array tech-
nologies that independently call genotypes at polymorphic sites, short
read sequencing often collects haplotypic information; a read span-
ning more than one polymorphic locus (multi-single nucleotide poly-
morphic read) contains information on the haplotype from which the
read originates. However, this information is generally ignored in exist-
ing approaches for haplotype phasing and genotype-calling from short
read data.

Results: In this article, we propose a novel framework for haplotype
inference from short read sequencing that leverages multi-single nu-
cleotide polymorphic reads together with a reference panel of haplo-
types. The basis of our approach is a new probabilistic model that
finds the most likely haplotype segments from the reference panel to
explain the short read sequencing data for a given individual. We
devised an efficient sampling method within a probabilistic model to
achieve superior performance than existing methods. Using simulated
sequencing reads from real individual genotypes in the HapMap data
and the 1000 Genomes projects, we show that our method is highly
accurate and computationally efficient. Our haplotype predictions
improve accuracy over the basic haplotype copying model by
~20% with comparable computational time, and over another recently
proposed approach Hap-SeqX by ~10% with significantly reduced
computational time and memory usage.

Availability: Publicly available software is available at http://genetics.
cs.ucla.edu/harsh

Contact: bpasaniuc@mednet.ucla.edu or eeskin@cs.ucla.edu

 

*To whom correspondence should be addressed.

Received on April 17, 2012; revised on June 19, 2013; accepted on
June 28, 2013

1 INTRODUCTION

Humans are diploid organisms with two copies of each chromo-
some, one inherited from the father and the other from the
mother. The two copies are similar to each other and only
differ at a small fraction (~ 0.1%) of sites. Most of the variation
is contained at single nucleotide polymorphic (SNP) sites. The
sequence of alleles on each chromosome is referred to as the
haplotype. Haplotype information is centrally important for a
wide variety of applications, including association studies and
ancestry inference (Fearnhead and Donnelly, 2001; Hugot
et al., 2001; Lazzeroni, 2001; Myers and Grifﬁths, 2003; Rioux
et al., 2001; Sabeti et al., 2002). Unfortunately, standard methods
for probing genetic variation are able to collect only genotype
information but not haplotypes. A large number of computa-
tional methods, referred to as haplotype phasing approaches,
have been proposed to infer haplotypes from genotypes. The
most successful methods use a set of reference haplotypes to
build a probabilistic model of the haplotypes in the population
(Howie et al., 2009; Howie et al., 2011; Kang et al., 2010; Li
et al., 2010; Long et al., 2009). Using a population genetics
model for the haplotype distribution, these models predict the
most likely haplotype data that can explain the observed
genotypes.

Rapid advances in high-throughput sequencing (HTS) tech-
nologies provide new opportunities for haplotype phasing meth-
ods. HTS yields short segments of the DNA (reads) where each
read originates from one of the pair of chromosomes. Therefore,
all the alleles in this read are from the same haplotype. Although
reads that cover multiple SNPs (multi-SNP reads) could be used
to improve haplotype inference, existing methods generally
ignore this information, partially owing to computational difﬁ-
culty associated with modeling such reads.

Several methods have been proposed to predict haplotypes
directly from the reads. These methods, referred to as haplotype
assembly methods, use overlapping reads to construct the haplo-
type (Aguiar and Istrail, 2012; Bansal and Bafna, 2008; Bansal
et al., 2008; Duitama et al., 2010, 2012; He et al., 2010; Xie et al.,
2012). The most commonly used objective function for haplotype

 

© The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 2245

112 /810's112umo[pJOJXO'sopquJOJurorq/ﬁd11q IIIOJJ popcolumoq

910K ‘09 lsnﬁnV no :2

W.-Y.Yang et al.

 

assembly is the minimum error correction (MEC). The MEC
objective function aims at ﬁnding the minimum number of
edits such that the reads can be partitioned into two disjoint
sets, and each set of reads originates from one of the haplotypes.
However, as these methods do not use the information in the
reference haplotype panel, they signiﬁcantly underperform
standard phasing methods that ignore read information but
use reference panel (He et al., 2010). Recently, one of these
methods has been extended to use the reference (He and Eskin,
2013; He et al., 2012). Unfortunately, this method has prohibitive
memory and time requirements, thus making it unfeasible for
moderate to large datasets.

Here, we propose a novel approach called Haplotyping with
Reference and Sequencing technology (HARSH) for haplotype
phasing. We use a probabilistic model to incorporate the multi-
SNP read information together with a reference panel of haplo-
types. We use an efﬁcient Gibbs sampling method to ﬁnd sample
from the posterior distribution. This algorithm has the advan-
tages of being computationally efﬁcient, scalable in memory
usage and accurate in genotyping and phasing prediction. We
evaluate our method on simulations from real haplotypes from
the HapMap project. At 1x coverage, HARSH gives ~10%
improvement in terms of total error rate compared with standard
phasing approaches that do not use the multi-SNP read infor-
mation, thus showing the beneﬁts of modeling multi-SNP reads.
We also evaluate HARSH and the basic model for varying
coverage and read length, showing the beneﬁts of our approach
in higher coverage and longer read length. Additionally, we test
our method on simulations starting from real sequencing data of
1000 Genomes project, where the density of SNPs is much higher
than that in HapMap data. Through extensive simulations we
show that the gain in performance of our approach over existing
models extends to realistic read lengths (e.g. 100—400 bp), making
our approach readily applicable to existing sequencing datasets.
With recent works showing that short read sequencing can dra-
matically increase association power in genome-wide association
study over genotyping arrays (Pasaniuc et al., 2012), we expect
our approaches to further increase power in genome-wide asso-
ciation study by increasing accuracy in genotype calling and
phasing from short read data.

2 METHODS

The best performing approaches for haplotype inference rely on Hidden
Markov Models (HMMs) for describing the distribution of haplotypes in
the population. These approaches generally ignore multi-SNP informa-
tion in the reads, thus implementing the model as a linear chain graph.
The model structure becomes complicated when we are considering multi-
SNP information, as it is not trivial to perform standard operations (e. g.
Viterbi decoding) to a non-linear chain graph. Previous methods [e.g.
Hap-SeqX die and Eskin, 2013)] have attempted to extend the Viterbi
algorithm to the complex graph induced by multi-SNP reads and refer-
ence haplotypes, but the approach is expensive in both time and memory
usage. As opposed to previous approaches, in this work, we use a Gibbs
sampler—based method for fast inference. The main advantage of this
approach is that the computations are efﬁcient and it can achieve the
optimal or close to optimal solution in a feasible amount of time.
However, all other current methods are either not optimal or not
practical in terms of computational time or memory usage.

2.1 Gibbs sampler preliminaries

A Gibbs sampler serves as the basis for our method. We ﬁrst introduce
the general idea of Gibbs sampling before we use it to solve the haplotype
problem. Consider the following distribution typically used to perform
optimization in graphical models:

P(X) = % exp (IL 2 Z ¢ij (Xi, 99))

i=1 j=1

where X = (x1, x2, -sxd) is a d—dimensional vector and Z is a normaliza-
tion factor. The function ([5 speciﬁes the edge potential for two variables
with an edge between them. We would like to collect samples of X based
on this distribution P(X).

Gibbs sampler is a special case of Monte Carlo Markov Chain method
(Geman and Geman, 1984), which is guaranteed to converge to the
equilibrium distribution after sufﬁcient burn-in rounds. In each round,
it randomly samples one variable x,- based on the conditional probability
P(x,-|x[_,]) when all other variables le = (x1, ...,x,-_1,x,-+1, ...,xd)
are ﬁxed. Formally, this conditional probability can be written as
follows:

P(xi = LXI—1])
Zr; P(x,- = t’,x[_,])

A more complete treatment of Monte Carlo Markov Chain is available in
(Liu, 2008).

P(x,- = tlx[_,'_|) = (1)

2.2 Haplotype assembly with sequencing data

Sequencing technologies provide us with a set of reads, each of which is a
short fragment from one of the chromosomes. Haplotype assembly aims
to assemble the entire haplotype based on only read information. An
illustrative example is given in Figure 1.

We can formalize this problem as follows. Suppose that we only
consider L biallelic SNPs and M reads. Each read is represented
by X]- : {—1,1,0}L, where 0 stands for unobserved SNP in jth
read, —1 and 1 stand for observed minor and major alleles, respectively.

m
m e o @ (D G) ,0». e
@ ® 6) © (9 © © ©
References (9 6;.) ® a 0 0 
 © ©
6) ® @  ®

 

Haplotypel: IA c G T A T T c>

 

HaplotypeZ: :c G T c G c G Tn}

........................... ..

Alqwassv adM0|deH Bugseqd adA10|deH

"9 .......  ....... 
IT'IIIIIIIICIIIIIII[EXCISE
Reads A T T
G T A

Fig. 1. An illustration of haplotype inference problems. The two chromo-
somes for an individual are unknown to us at ﬁrst. Sequencing technol-
ogy produces a set of reads, each of which originates from one of the two
chromosomes. We also have a set of reference haplotypes, which are from
the same population as the donor. Haplotype assembly aims to assemble
the two donor haplotypes by only using the read information. Haplotype
phasing problem aims to phase the two haplotypes by mosaic copies from
the reference haplotypes. However, our approach HARSH takes into
account both read information and reference panel for more accurate
haplotype inference

 

2246

112 /810's112umo[pJOJXO'sor1chOJurorw/2d11q IIIOJJ papeolumoq

9IOZ ‘09 lsnﬁnV no :2

Leveraging multi-SNP reads from sequencing data

 

Read 1 Read 2

Haplotype

Fig. 2. A graphical model for haplotype assembly. In this example, two
reads and four heterozygous SNPs are considered. Read 1 covers the
SNPs 1, 2 and 3. Read 2 covers SNPs 2, 3 and 4. The variables
h e {1, — 1} stands for the haplotype. The variable r e {1, — 1} stands
for whether the read is from haplotype h or the complementary h

Because the homozygous site does not affect the haplotype phasing, we
only consider heterozygous sites. Therefore, the objective is to ﬁnd a
sequence of haplotype and its complementary {h, h} where
h = —h e {—1, 1}L, to minimize the total number of ﬂipped loci within
reads, such that every read can be perfectly assigned to one of the haplo-
types. Another necessary variable for the model is the read origin indi-
cator rj e {—1, 1}. If rj = 1, the jth read is assumed to have been
generated from haplotype h, and if rj = —1, the jth read is from the
complementary haplotype h. We assume the read generation process is
as follows. First, we randomly pick one of the haplotypes (h, h) with equal
probability, and then sample the read starting position from one of the
L possible positions in the genome. If we consider the read generation
processing is error free, then we have xij = hirj. However, if the read
generation process is error-prone and 8 indicates the rate of sequencing
error then with probability 1 — 8, we have xij = —h,-rj, and with probabil-
ity 8, we have xij = —h,-rj. An illustrative example is given in Figure 2.

We can formalize the connection between the haplotypes and read
origin variables into the following probabilistic distribution. For each
possible values of the haplotypes and read origin variables, we can cal-
culate its probability as follows:

P(R, H; X)
1 (2)
= 2 exp 1‘ Z 9110“: rj) + Z Tiij(hz" rj)
ij:x,-j=1 ij;xij:_
where

Oz'j(hi, rj) 2 {ms h. # r1
1 J
(h ) ln8 h,- = rj
Th] 19 r] _  _ 8) hi 7E rj'

and the variables R = (mi-:1, H = (hi):1 and X = (xv-M are vectors and
matrix composed of scalar variables r, h and x. The variable Z is a nor-
malization constant to ensure ZR, H P(R, H; X) = 1. The functions 0 and
1} specify edge potentials that favor h and r to be of equal values and
opposite values, respectively. The model parameter ,u controls the ‘heat’
of the probabilistic model. Generally speaking, the probability distribu-
tion is smoother when ,u is small and sharper when ,u is large.

LEMMA 1. The maximum a posteriori (MAP ) assignment of (2) corres-
ponds to the MEC haplotype for any €<0.5.

PROOF. We can prove by constructing the MEC haplotype from MAP
assignment. Let H* and R* denote the MAP assignment of our probabil-
istic model, and the corresponding probability calculated from (2) will be

P(H*, R*; X) = % exp(,u(n ln(l — 8) + m In 8))

where n is the number of edges getting potential ln(1 — e) and m is the
number of edges getting potential In 6 based on the conﬁguration H* and
R*. As ln(1 — 8)> ln8 for €<0.5 and the number of edges is ﬁxed, this
MAP assignment H* and R* is actually minimizing the number of edges

getting potential In 6. We can use this haplotype H * and ﬂip every read bit
corresponding to the edge getting potential In 8. The resulting MEC score
for H* will be m, which is minimized.

Suppose that there exists another haplotype H’ with MEC score
m’ <m. It suggests that we can ﬂip only m’ read bit then all the reads
will be perfectly assigned to one of the haplotypes. We keep those assign-
ments into the variable R’. Thus, we should have

P(H’, R’; X) = % exp(,u((n + m — m’) ln(1 — 8) + m’ In 8)).

By deﬁnition, m’ <m; thus, P(H’,R’; X)>P(H*,R*; X), which contra-
dicts the fact that H* and R* is the MAP assignment maximizing the
conﬁguration probability. By this contradiction, we can conclude that
there does not exist H’ and R’ with MEC score m’ <m.

2.3 Haplotype phasing with sequencing data and reference

Current haplotype assembly methods mainly focus on de novo assembly,
which uses short reads as the only information source. This is partially
owing to the complexity of extending the method to the scenario of
assembly using reference. On the other hand, current haplotype phasing
methods only use the reference panel and genotype likelihood in each
SNP but ignore the multi-SNP information in the reads. We aim to use
both the reference panel and sequencing data to perform haplotype phas-
ing as shown in Figure 1. Formally, suppose that we are only considering
L biallelic SNPs, M reads and N reference haplotypes. Each read is rep-
resented by Xj = {—1, 1, 0}L, where 0 stands for unobserved SNP in jth
read. The objective is to ﬁnd two haplotypes, H = {h1, h2}, where
hl, h2 e {—1, 1}L. We want to ﬁnd the two haplotypes with small
number of inconsistent loci with reads, as well as more consistent with
reference haplotypes. We use another set of variables, S = {s1, s2}, where
s1,s2 6 {1,2, ..., N}L, to stand for the assignment of each loci to refer-
ence haplotypes. We also need a set of variables R = {r1,r2, ...,rM},
where r,- e {—1, 1} stands for the haplotype that each read originates
from. An illustrative example of the graph structure is given in Figure 3.

Similar to the previous section, we can formalize the connection
between the three variables H, R and S into the following probabilistic
distribution. For each possible values of H, R and S, we can calculate its
probability as follows:

1
P(H,R,S;X)=Eexp ,u- 2 002,1, —rj)+ Z 7101}, —"j)

ij:x,-j=1 ij:x,-j=—1
L L—l
+Zé(h},s}) + Zr(s}.s}+1,i)
i=1 i=1
‘1' Z 9(h§,rj)+ Z 7101,2917)

ij:x,-j=1 ij;xij:_
L L—1
2 2 2 2 .
+ 2502,- .s,-) + Eros,- .s,.,,. 0
i=1 i=1

where we have four edge potential functions. The functions 0 and n are
deﬁned similarly as in (2) except that there would be no penalty if the read
is assigned by r to the other haplotype.

ln(1—8) rj=1,h,-=1
0(hi,rj)= ln8 rj=1,h,-=—1,
0 rj= —1
ln8 rj =1,h,-=1
n(h,-,rj)= ln(1—8) rj=1,h,-=—1.
0 rj= —1

The edge potential function E speciﬁes the ‘haplotype copying’, which
is motivated that the predicted haplotype is a mosaic of reference

 

2247

112 /810's112umo[pJOJXO'sor1emJOJurorw/2d11q IIIOJJ papeolumoq

9IOZ ‘09 lsnﬁnV no 22

W.-Y.Yang et al.

 

 

Haplotype 1

Haplotype 2

 

 

Reference

Fig. 3. A graphical model for haplotype phasing with reference. The
variables h1 and hz stand for the ﬁrst and second haplotypes. The vari-
ables r,- = {—1, 1} specify whether the read comes from the ﬁrst haplotype
or second haplotype. The variable s1 and 52 specify which haplotype in the
reference is generating the haplotype h1 and h2, respectively

haplotypes with a small number of differences. In this case, the predicted
haplotypes are similar to reference haplotype s1 and s2 at position i.

ln(l—w) 12,1201

si,i

l l _
$(higsi)—{lnw hll7éGS1J

where Gij stands for the jth allele in ith reference haplotype. Thus, GSIJ
stands for the ith allele in s11th reference haplotype. Moreover, we use the
following function to model the transition probability in haplotype copy-
ing model (Li and Stephens, 2003).

. exp(—ﬂ)+(1—exp(—ﬂ))/N Si=Si1
7(Siasi+1”)={(1—e:$(—%))/N N sass;

where p,- = 4Ner,- and r,~ is the per generation genetic distance between site
i and site i + 1, and N, is a constant.

This probabilistic model provides us a disciplined way to infer the most
probable haplotype given a set of reads and a set of reference haplotypes.
It extends the haplotype copying model (Li and Stephens, 2003) from
genotype input to sequencing data input. It also extends the haplotype
assembly problem in previous section to a more general case where the
reference panel can be used to improve the phasing. We are then able to
design efﬁcient sampling approach to ﬁnd the most possible conﬁgur-
ations of H, R and S that maximize the probability given in Equation (3).

2.4 Efﬁcient sampling

Haplotype assembly without reference. The bipartite structure in
Figure 2 suggests an efﬁcient procedure for sampling. For ﬁxed one
layer of the bipartite graph, the variables in the other layer will be inde-
pendent on each other. Thus, the conditional probability in Equation (1)
of Gibbs sampler can be signiﬁcantly reduced. Formally, following the
standard procedure of Gibbs sampling, we can sample haplotype from
the conditional probability for ﬁxed read origins. The sampling ratio
8,- : P(hi = —1|R) can be calculated as follows:

expC: 0(—1.r.-)+ : new»)

 

X,--=1 ':X,--=—1
6,: ’ ’ ’ . (4)
“PC: 9(—1,rj)+ Z U(—1,rj))

+exp<lz 0(1,rj)+ Z 71020))

 

Similarly, we can also do a similar Gibbs sampling step for read origin for
ﬁxed haplotype. The sampling ratio pj = P(rj = —1|H) can be calculated
as follows:

exp( 2 002,-. —1)+_ : n(h.-. —1))

 

i:Xij:1 1:X,-j=—1
p1 - - (5)
exp< Z 002,-, —1)+ Z n(h,-, —1))
i:Xij:1 i:X,-j=—1

+6Xp 2 9(hi: 1) + 2 7101131))
i:X,-j=1 i:X,-j=—1
The complete sampling algorithm for haplotype assembly is shown in
Algorithm 1. As default, we use 10000 rounds for sampling.

Haplotype phasing with reference. The sampling for haplotype
phasing with both sequencing data and reference from the graph in
Figure 3 is more challenging. However, we can still take advantages of
the special structure of the graph and perform efﬁcient sampling
procedure. Following the idea of Gibbs sampler, we will alternatively (i)
sample read origin R for ﬁxed haplotype H and reference assignment S; (ii)
sample S for ﬁxed R and H; (iii) sample H for ﬁxed R and S. The step (i) is
similar with that in haplotype assembly. Formally, the sampling ratio
P(rj = —1|H, S) for read origin can be calculated by

exp( : 0(h},1)+_ : 17022.0)

 IXijZ—l

 

IXijZ—l

P1 = - (6)
6Xp( Z 9(h},1)+ 2 1101.120)
i:X,-j=1 i

+exp( : 0(h%,1)+_ : 17022.0)

 IXijZ—l

 

Algorithm 1 Sampling Algorithm for Haplotype Assembly

 

1: Randomly initialize haplotype H.

2: For ﬁxed haplotype H, sample read origin R. For probability pj,
we get rj = —1, and for probability 1 — pj, we get rj = 1, where the
ratio )0 can be calculated as in (5).

3: For ﬁxed read origin R, sample haplotype H. For probability 6,,
we get h,- = —1, and for probability 1 — 6,, we get h,- = 1, where the
ratio 8 can be calculated as in (4).

4: Repeat steps 2 and 3 for sufﬁcient rounds until equilibrium.

5: Collect samples by repeating steps 2 and 3, and output the one
with highest probability.

 

The step (iii), sampling of haplotype H for ﬁxed read origin R and refer-
ence assignment S is a straightforward extension from Equation (4). The
modiﬁcation is based on the extra edge between reference penal variables S
and haplotype H. Formally, the sampling ratio P(h} = —1|R, S) for the
ﬁrst haplotype can be calculated by

i: —“( 1) (7)
a(—1) + 01(1)

where

am) = exp 2 002. — r.) + Z n(h. — rj) +£01.81) .
j:X,-j=1 j:X,-j=—1
The sampling ratio P(hlz = —1|R, S) is similar with P(hll = —1|R, S).
Similarly, we can obtain the sampling ratio for the second haplotype as
follows:
6; a—l)

’ = ﬁ(-1)+ 13(1) (8)

 

2248

112 /810's112umo[pJOJXO'sor1emJOJurorw/2d11q wort popcorn/hog

9IOZ ‘09 lsnﬁnV no 22

Leveraging multi-SNP reads from sequencing data

 

where

 = exp( 2 0029 rj) + Z 7’02: rj) +  '
j:X,-j=1 j:X,-j=—

The step (ii), sampling for the haplotype reference panel variables S for
ﬁxed read origin R and haplotype H is challenging. The difﬁculty comes
from the dependency between the variables s, and si+1, and the large
number of possible values for each s,. Note that unlike the binary vari-
ables h and r, the variable s,- 6 {1,2, ...,N}, where N is the number of
reference haplotypes. Thus, straightforward Gibbs sampler would be in-
efﬁcient in this case. To tackle this computational challenge, we resort to
the following Markov chain sampling procedure (Liu, 2008). The joint
distribution over all variables in S can be written as follows:

L—l
P(SIH) = %exp<¢o(51) + Z¢i(Si,Si+1)) (9)
i=1

where

¢0(S1) = $(h1,S1)
¢i(Si, Si+1) = T(Si,Si+1, 1) + $(hi+1,Si+1)-

Sampling directly from P(SIH) is still tedious. However, we can convert
the P(SlH) to multiplication series of probability functions as follows:
P(S1IS2,H)P(S2IS3,H) - sP(sL_1|sL,H)P(sL,H). Then sampling from
P(sL) and sampling backward using those conditional probabilities
becomes trivial. We can use dynamic programming to convert the
P(SIH) distribution to the alternative form. We deﬁne

V102) = 2 exp ((150 (s) «m (s, s»)

seS
and
V,(s,-+1) = Z V,_1(y) exp (¢,- (y, s,+1)) for i = 2, . . . ,L.
yeS

Thus, we can compute the normalization factor Z 2 End VL_1(sL)
efﬁciently using dynamic programming, and then we can compute the
marginal probability P(sL, H) = (VL_1(sL))/Z. Moreover, we can back-
ward compute P(s,-|s,-+1, H) similarly. Note that a naive implementation
of this step would result in a complexity of quadratic in the number of
reference haplotypes. We take advantage of the symmetry in the haplo-
type coping model to reuse computation to achieve runtime linear in the
number of reference haplotypes.

An outline of the sampling algorithm for haplotype phasing with
sequencing data and a reference panel is given in Algorithm 2. As default,
we use 10000 rounds of sampling.

 

Algorithm 2 Sampling Algorithm for Haplotype Phasing

 

1: Randomly initialize haplotype H

2: For ﬁxed haplotype H, sample read origin R using sampling ratio

3: For ﬁxed haplotype H sample haplotype reference S following
Markov chain sampling procedure described after (9).

4: For ﬁxed read origin R, and haplotype reference S, sample
haplotype H using sampling ratio 6,- in (7).

Repeat steps 2, 3 and 4 for sufﬁcient rounds until equilibrium.

Collect samples by repeating steps 2, 3 and 4. Output samples with
highest probability.

 

3 EXPERIMENTAL RESULTS

3.1 Datasets and experimental settings

We performed simulation experiments using HapMap Phase II
data (International HapMap Consortium, 2005) and 1000

Genomes data (Durbin, R. et al., 2010). For our simulations,
we used the 60 parental individuals of CEU populations from
HapMap Phase II as well as 60 individuals randomly chosen
from the European populations for 1000 Genomes data.
Although our method is scalable to the entire genome, for the
purpose of demonstration, we use only chromosome 22 as rep-
resentative of the rest of the genome, as it is the shortest chromo-
some. Because we are performing many simulations, we restrict
our results to the 35 421 SNPs in chromosome 22 of the HapMap
data, and the ﬁrst 30 000 SNPs in chromosome 22 of 1000
Genomes data, which span ~3 Mb. The datasets are publicly
available at http://mathgen.stats.ox.ac.uk/impute/ and http://
hapmap.ncbi.nlm.nih.gov/.

We evaluate our method using a leave-one—out procedure. In
each round, we infer the haplotype for one individual using
simulated sequencing data and the haplotypes of the other 59
individuals as reference panel. This procedure is repeated 60
times and all the evaluation metrics are averaged. The reads
are simulated uniformly across chromosome 22 for a given
coverage. The read length in each end of a pair-end read is
ﬁxed but the gap between the two ends follow a normal distri-
bution with ﬁxed mean and standard deviation. Errors are in-
serted in the read at a rate 8.

We evaluate our method HARSH using the standard metric
for genotyping and phasing accuracy: genotyping error rate and
switching error rate. The genotyping error rate is the proportion
of wrongly predicted genotypes, and the switching error is the
proportion of switches in the inferred haplotypes to recover the
correct phase in an individual. The total error rate is the sum of
genotyping error rate and switching error rate. We also use per-
centage improvement when comparing two methods. The per-
centage improvement is computed as the error rate difference
between two methods normalized by the error rate of baseline
method. For example, suppose that HARSH has error rate x and
baseline method has error rate y, the improvement of HARSH
over the baseline method would be ()2 — x) / y.

We ﬁxed the parameters ,u. = 1, a) = 0.002 and e = 0.01 for all
our experiments. From our experience, the performance of the
proposed method is not sensitive to parameter tuning. Using ,u
from 1 to 10 and a) from 0.001 to 0.005 does not affect the
performance signiﬁcantly. The sequencing error e = 0.01 is
standard sequencing error rate.

All experiments are performed in a cluster machine where each
node has 8—16 cores 3.0GHz CPU and 1—16 GB memory. Jobs
are submitted in a parallel manner but each job uses only one
node.

3.2 HapMap simulations

We use HapMap dataset to evaluate our method HARSH. We
compare our method with three other state-of—the-art methods:
the HMM at the core of the IMPUTE method (Howie et al.,
2009), BEAGLE (Browning and Browning, 2009) and Hap-
SeqX (He and Eskin, 2013). Because IMPUTE does not support
haplotype phasing for uncovered SNPs, for a fair comparison,
we re—implemented the basic HMM model of the IMPUTE v1.0,
which uses the pre—deﬁned genetic map information for transi-
tion probability. We will refer to our implementation of the
HMM model in IMPUTE method as IMPUTE*. In our

 

2249

112 /810's112umo[pJOJXO'soriemJOJurorw/2dnq wort papeolumoq

9IOZ ‘09 lsnﬁnV no 22

W.-Y.Yang et al.

 

 

    
   

.Assembly
.BEAGLE

.IMPUTE"
I:]HARSH

No. of Switches
_\ _\ N
O 01 O
O O O
O O 0

U1
0
O

 

 

 

1 2 4 6 8 10 1000 2000 3000 4000
Coverage Read Length

Fig. 4. The number of switches within heterozygous SNPs for haplotype
assembly, BEAGLE, IMPUTE* and HARSH. The number of switches
of haplotype assembly is estimated by the lowest bound. (a) Varying
coverage for ﬁxed read length 1000 bp. 0)) Varying read length for
ﬁxed coverage 4X

modiﬁed version, we use the read count for each SNP as input to
IMPUTE* method. The likelihood of read count from genotype
is used as the emission probability for the HMM model. Then
the Viterbi algorithm is used to decode two paths from the ref-
erence panel, which are most likely to generate the read counts in
each SNP. The two paths in reference panel also give the two
predicted haplotypes. Because the latest implementation of
IMPUTE (Howie et al., 2009) is not able to phase, we also
compared our approach with BEAGLE 3.3.2 (Browning and
Browning, 2009), a widely used approach for haplotype phasing
and imputation.

We ﬁrst use the HapMap dataset to show that haplotype as-
sembly without a reference panel will underperform haplotype
phasing with a reference panel. The main reason is that there are
not enough long reads covering all continuous heterozygous
SNPs. Thus, haplotype assembly cannot do more than random
guess between two continuous heterozygous SNPs if there is no
read spanning them. We can compute a lower bound of the
number of switches for haplotype assembly as K/2 where K is
the number of those gaps, assuming the MEC score to be zero.
For pair-end reads with ﬁxed length 1000 bp mean and 100 bp
standard deviation, we evaluate our method using six levels of
sequencing coverages: 1x, 2x, 4x, 6x, 8x and 10x. As shown
in Figure 4a, higher coverage does not help haplotype assembly
to achieve similar performance than haplotype phasing methods.
At ﬁxed coverage 4x, we simulated pair-end reads with 1000,
2000, 3000 and 4000 bp in each end. As shown in Figure 4b, we
can observe that the lower bound of haplotype assembly achieves
similar performance as haplotype phasing only under the unreal-
istic read length 4000 bp. Also, at 4x coverage, we can observe
that our method can improve ~44% over BEAGLE and ~37%
over IMPUTE in terms of numbers of switches.

For simulated pair-end reads with 1000 bp for each end at 1x
coverage, only 32% reads contain one SNP and ~26% of the
reads contain more than three SNPs. On average, every read
contains around 2.8 SNPs. Following the procedure similar to
that of He and Eskin (2013), we divide the chromosome into
overlapping chunks containing 1200 SNPs each and run our
method on each chunk independently. The ﬁnal haplotypes are
then constructed by stitching together the haplotypes from each
chunk. Chromosome 22 is divided into 36 chunks. The total
error rate for both IMPUTE* and HARSH are shown in
Figure 5. We can observe from the ﬁgure that HARSH consist-
ently performs better than IMPUTE* across all 36 chunks. The

 

- IMPUTE*
I:I HARSH

A

OD

 

 

 

15 20

o_ 5 1o 25
Contig Index (1000 bp)

Fig. 5. The error rate for IMPUTE* and our method for each chunk of
length 1200 SNPs in chromosome 22. The error rate consists of both
genotyping error for all SNPs and switch error within heterozygous SNPs

 

 

 

N

 

 

 

30 35 40

average improvement over IMPUTE* is 7.6%. We then conca-
tenated those haplotype chunks by minimizing the mismatches in
the overlap region between two adjacent chunks. After concat-
enation, the overall error rate for HARSH is 4.01% for chromo-
some 22, compared with 4.42% for IMPUTE*. The overall
improvement is 9.3% over IMPUTE*.

We compare HARSH with a previous method for combining
multi-SNP reads with a reference panel, Hap-SeqX (He and
Eskin, 2013). Hap-SeqX is an approximation to the dynamic
programming approach of the Hap-Seq method (He et al.,
2012), which optimizes a similar objective function to HARSH.
Hap-SeqX only searches a fraction of the search space compared
with Hap-Seq by only storing the top values at each state.
However, Hap-SeqX is still an expensive method in both time
and memory usage. In this experiment, we use the default par-
ameters of Hap-SeqX, where t = 0.01 speciﬁes that the algorithm
saves the top 1% of values for each state. On addition, Hap-Seq
and Hap-SeqX, unlike HARSH, can only handle up to three
SNPs in a read and split reads containing more SNPs into mul-
tiple reads. The performance comparisons are shown in Table 1.
HARSH and IMPUTE* have similar running time. HARSH
takes ~10 min compared with IMPUTE* 5 min on chromosome
22. Both these methods compare favorably with Hap-SeqX,
which takes 5 h for the same dataset. Cross validation of 60 in-
dividuals would be prohibitive for Hap-SeqX. Thus, we compare
all these three methods using only the ﬁrst individual in HapMap
dataset. The results averaged more than 36 chunks. We can see
that Hap-SeqX improves by ~12.53% from the baseline method
IMPUTE*, and HARSH signiﬁcantly improves by 21.34% from
IMPUTE*. We conducted signiﬁcance test (paired-sample t—test)
on the improvement of HARSH over Hap-SeqX and IMPUTE*.
The test results show that HARSH signiﬁcantly outperforms
both Hap-SeqX and IMPUTE* with P<1 x 10‘3 and
P<1 x 10‘7, respectively. Overall, the comparison shows that
HARSH is the most accurate and practical method among exist-
ing methods.

To fully evaluate the performance of our method, we apply
our method to cases with different coverages and read lengths.
For pair-end reads with ﬁxed length 1000 bp mean and 100 bp
standard deviation, we evaluate our method using six levels of
sequencing coverages: 1x, 2x, 4x, 6x, 8x and 10x. The result is
shown in Figure 6a. As expected, the performance improvement
of HARSH over BEAGLE and IMPUTE* becomes more sig-
niﬁcant when the coverage increases. The reason we expect this is
that the higher the coverage, the larger number of reads that

 

2250

112 /810's112umo[pJOJXO'soriemJOJurorw/2dnq wort papeolumoq

9IOZ ‘09 lsnﬁnV no 22

Leveraging multi-SNP reads from sequencing data

 

Table 1. Comparison between IMPUTE*, Hap-SeqX and HARSH on a
HapMap dataset with 1 donor individual, 59 reference individuals and
35 421 SNPs

 

 

 

 

 

 

 

 

 

 

 

 

Methods Error rate (switch, genotyping) Time
IMPUTE* 0.04836 (0.00804, 0.04033) ~5 min
Hap-SeqX 0.04230 (0.00726, 0.03504) ~5h
HARSH 0.03804 (0.00664, 0.03140) ~10 min
Note: Read length of 1000 bp and 1X coverage are simulated.
(a)0.06 (b) 0.02
0.05 a 0.018   
q) 0.04 a, 0.016 """ 
g“ 0.03  0.014 ’ 1:25:82;
u“; .E —HARSH
0.02 0.012
0.01 0.01
0o 5 1o 15 0'cc‘o’o 1000 2000 3000 4000 5000
Coverage Read Length

Fig. 6. Performance of BEAGLE, IMPUTE* and HARSH for varying
coverage and read length on HapMap. (a) Varying coverage for ﬁxed
read length 1000 bp. 0)) Varying read length for ﬁxed coverage 4X

span multiple SNPs. HARSH is able to take advantage of the
multi-SNP information within those reads but BEAGLE and
IMPUTE* can not take advantage of that. In Table 2, we
show the genotyping and switching error rate of HARSH and
IMPUTE* method for different coverages. It can be observed
that both genotyping error and switching error are signiﬁcantly
reduced by HARSH over BEAGLE and IMPUTE*. It is also
worth mentioning that 4x seems to be the best choice in terms of
the compromise between the cost of coverage and achieved ac-
curacy. The coverage 4x gives 0.28% genotyping error and
0.62% switching error. However, the improvement of higher
coverage than 4x is limited.

We also evaluate HARSH with different read lengths. At ﬁxed
coverage 4x, we simulated pair-end reads with 1000, 2000, 3000
and 4000 bp in each end. The results are shown in Figure 6b. It is
not immediately intuitive why the genotyping error rates for
BEAGLE, IMPUTE* and HARSH increase when the read
length increases. A possible reason is that longer reads for a
ﬁxed coverage result in fewer total reads and larger gaps without
any coverage. In other words, longer reads result in less random
read bits across the chromosome. An extreme example is that the
gap will be half of the genome on average if the read length is equal
to the genome size and coverage is 1x. Sequentially, larger gap
where no reads cover will potentially harm the imputation and
haplotype phasing accuracy. However, we can still see that the
performance gap between BEAGLE or IMPUTE* and HARSH
is enlarged while the read length increases. This is attributed to the
ability of HARSH to leverage the multi-SNP information in
longer reads. In Table 3, we show the improvement of HARSH
over BEAGLE and IMPUTE*. The improvement is basically
from the reduced switching error, which is reduced from 0.62 to
0.48% by HARSH but not by IMPUTE*. The genotyping error
for both methods increases at the same pace because of the larger
gaps caused by longer reads. The error rates for BEAGLE,

Table 2. Genotyping and switching errors (%) for varying coverages on
HapMap dataset

 

Coverage 1x 2x 4x 6x 8x 10x

 

Genotyping Error
BEAGLE 4.21 1.94 0.59 0.22 0.10 0.04
IMPUTE* 3.59 1.53 0.56 0.30 0.17 0.12
HARSH 3.42 1.28 0.28 0.08 0.04 0.02
Switching Error
BEAGLE 0.97 1.04 1.05 1.11 1.23 1.23
IMPUTE* 0.82 0.87 0.90 0.94 0.97 0.98
HARSH 0.72 0.67 0.62 0.63 0.65 0.65

 

Note: Read length is ﬁxed to be 1000 bp.

IMPUTE* and HARSH increase from 0.59 to 0.79%, from
0.56 to 0.85% and from 0.28 to 0.48%, respectively, when the
read length increases from 1000 to 4000 bp. But HARSH consist-
ently performs better than BEAGLE and IMPUTE even while the
genotyping error rate is increasing.

3.3 1000 Genomes simulations

The 1000 Genomes project is an ongoing project that uses HTS
technology to collect the genetic variant data across many indi-
viduals with the goal of characterizing rare variants, which are not
present in HapMap. This provides us the opportunity to evaluate
our method using simulations that will realistically capture the
distributions of rare variants and more accurately reﬂect a tubal
performance. We simulate realistic paired end reads, which have
100 bp for each end, and a gap size following a normal distribution
with 100 bp mean and standard deviation of 10 bp. Only 22%
reads contain only one SNP and ~55% reads contain more
than three SNPs. On average, every read covers around 3.1
SNPs. Following the same settings as what we did for HapMap
data, we test HARSH for different coverages and read lengths.
The results for coverage 1x, 2x, 4x, 8x, 16x and 32x are shown
in Figure 7a. We observe that the error rate does not further drop
after coverage 8x. At coverage 8 x, the improvement of HARSH
over IMPUTE* is 29% from 0.021 to 0.015 in terms of error rate.
Thus, for ﬁxed coverage 8 x, we simulate pair-end reads with 100,
200, 300 and 400 bp in each end. The results are shown in Figure
7b. We observe that, HARSH, unlike IMPUTE*, beneﬁts from
using longer reads, as it contains more multi-SNP reads than
shorter reads. Thus, as expected, the performance gap between
IMPUTE* and HARSH increases as the read length increases.
However, in Figure 7b, we do not see that the error rate increases
when the read length increases as in Figure 6b. A possible reason is
that the SNPs are much denser in 1000 Genomes data than
HapMap data, and we simulated much shorter reads for 1000
Genomes data. Thus, the gap caused by 400 bp read length
would be much shorter than previous 4000 bp read length for
HapMap dataset. The reference haplotype panel could well take
advantage of Linkage Disequilibrium effect to recover those gaps.
Therefore, the error rate for IMPUTE* keeps almost the same for
different read lengths but our method HARSH reduces the error
rate by incorporating more multi-SNP read information when the
read length increases.

 

2251

112 /810's112umo[p101x0'sot112u1101u101qﬂ2d11q 111011 papeolumoq

9IOZ ‘09 lsnﬁnV no 22

W.-Y.Yang et al.

 

Table 3. Genotyping and switching errors (%) for varying read lengths
on HapMap dataset

 

Read length 1000 hp 2000 hp 3000 hp 4000 bp

 

Genotyping error

BEAGLE 0.59 0.67 0.74 0.79

IMPUTE* 0.56 0.70 0.77 0.85

HARSH 0.28 0.37 0.40 0.48
Switching error

BEAGLE 1.05 1.10 1.07 1.07

IMPUTE* 0.90 0.93 0.94 0.94

HARSH 0.62 0.57 0.49 0.48

 

Note: Coverage is ﬁxed to be 4X.

(a) 0.03
- - - IMPUTE*
0.07 — HARSH

0.06

(b) 0.025

0.02 - - - IMPUTE*
—HARSH

.0
o
01

Error Rate
Error Rate
0
'3
01

.0
O
.5

.0
o
o:

0.01

.0
o
N

 

 

 

.0

o

_.
o

 

10 2o 30 40 0'cwo 100 200 300 400 500
Coverage Read Length

Fig. 7. Performance of IMPUTE* and HARSH for varying coverage
and read length on 1000 genomes. (a) Varying coverage for ﬁxed read
length 1000 bp. 0)) Varying read length for ﬁxed coverage 4X

 

14 CONCLUSKMJANDINSCUSSKNHS

Haplotype phasing plays an important role in a wide variety of
genetic applications. Although it is possible to determine haplo-
types using laboratory—based experimental techniques, these
approaches are expensive and time-consuming. Recently,
Kitzman et al. (2011) were able to generate the complete
phased sequence of a Gujarati individual using a F osmid library.
Unfortunately, this method is not easily scalable to phasing more
than one individual. Thus, the need for a practical computational
method for haplotype phasing remains.

We have presented HARSH, an efﬁcient method that com-
bines multi-SNP read information with reference panels of
haplotypes for improved genotype and haplotype inference in
sequencing data. Unlike previous phasing methods that use
read counts at each SNP as input, our method takes into account
the information from reads spanning multiple SNPs. HARSH is
able to efﬁciently ﬁnd the likely haplotypes in terms of the mar-
ginal probability over the genotype data. Using simulations from
HapMap and 1000 Genomes data, we show that our method
achieves superior accuracy than existing approaches with
decreased computational requirements. In addition, we evaluate
our method as function of coverage and read length, showing
that our method continues to improve as read length and cover-
age increases.

Funding: National Science Foundation (0513612, 0731455,
0729049, 0916676, 1065276 and 1320589 to W.Y., F .H., Z,W.
and EB); National Institutes of Health (K25-HL080079,
U01-DA024417, P01-HL30568 and POl-HL28481 to W.Y.,

F.H., Z,W. and BE; R03-CA162200 and R01-GM053275 to
BB).

Conﬂict of Interest: none declared.

REFERENCES

Aguiar,D. and Istrail,S. (2012) HapCompass: a fast cycle basis algorithm for accur-
ate haplotype assembly of sequence data. J. Comput. Biol., 19, 577—590.

Bansal,V. and Bafna,V. (2008) HapCUT: an efﬁcient and accurate algorithm for the
haplotype assembly problem. Bioinformatics, 24, i153—i159.

Bansal,V. et al. (2008) An MCMC algorithm for haplotype assembly from whole-
genome sequence data. Genome Res., 18, 1336—1346.

Browning,B.L. and Browning,S.R. (2009) A uniﬁed approach to genotype imput-
ation and haplotype-phase inference for large data sets of trios and unrelated
individuals. Am. J. Hum. Genet., 84, 210—223.

Duitama,J. et al. (2010) Refhap: a reliable and fast algorithm for single individual
haplotyping. In: Proceedings of the First ACM International Conference on
Bioinformatics and Computational Biology. ACM, New York, NY, pp. 160—169.

Duitama,J. et al. (2012) Fosmid-based whole genome haplotyping of a hapmap trio
child: evaluation of single individual haplotyping techniques. Nucleic Acids Res.,
40, 2041—2053.

Durbin,R. et al. (2010) A map of human genome variation from population-scale
sequencing. Nature, 467, 1061—1073.

Fearnhead,P. and Donnelly,P. (2001) Estimating recombination rates from popu-
lation genetic data. Genetics, 159, 1299—1318.

Geman,S. and Geman,D. (1984) Stochastic relaxation, gibbs distributions, and the
bayesian restoration of images. IEEE Trans. Pattern Anal. Mach. Intell., 6,
721—741.

He,D. and Eskin,E. (2013) Hap-seqX: expedite algorithm for haplotype phasing
with imputation using sequence data. Gene, 518, 2—6.

He,D. et al. (2010) Optimal algorithms for haplotype assembly from whole-genome
sequence data. Bioinformatics, 26, i183—i190.

He,D. et al. (2012) Hap-seq: an optimal algorithm for haplotype phasing with im-
putation using sequencing data. In: Proceedings of the 16th Annual International
Conference on Research in Computational Molecular Biology (RECOMB).
Springer, New York, NY, pp. 64—78.

Howie,B. et al. (2011) Genotype imputation with thousands of genomes. G3
(Bethesda), 1, 457—470.

Howie,B.N. et al. (2009) A ﬂexible and accurate genotype imputation method for the
next generation of genome-wide association studies. PLoS Genet., 5, e1000529.

Hugot,J.P. et al. (2001) Association of nod2 leucine-rich repeat variants with sus-
ceptibility to crohn’s disease. Nature, 411, 599—603.

International HapMap Consortium. (2005) A haplotype map of the human
genome. Nature, 437, 1299—1320.

Kang,H.M. et al. (2010) EMINIM: an adaptive and memory-efﬁcient algorithm for
genotype imputation. J. Comput. Biol., 17, 547—560.

Kitzman,J.O. et al. (2011) Haplotype-resolved genome sequencing of a gujarati
indian individual. Nat. Biotechnol., 29, 59—63.

Lazzeroni,L.C. (2001) A chronology of ﬁne-scale gene mapping by linkage disequi-
librium. Stat. Methods Med. Res, 10, 57—76.

Li,N. and Stephens,M. (2003) Modeling linkage disequilibrium and identifying re-
combination hotspots using single-nucleotide polymorphism data. Genetics, 165,
2213—2233.

Li,Y. et al. (2010) MaCH: using sequence and genotype data to estimate haplotypes
and unobserved genotypes. Genet. Epidemiol, 34, 816—834.

Liu,J .S. (2008) Monte Carlo Strategies in Scientific Computing. Springer, New York,
NY.

Long,Q. et al. (2009) HI: haplotype irnprover using paired-end short reads.
Bioinformatics, 25, 2436—2437.

Myers,S.R. and Grifﬁths,R.C. (2003) Bounds on the minimum number of recom-
bination events in a sample history. Genetics, 163, 375—394.

Pasaniuc,B. et al. (2012) Extremely low-coverage sequencing and imputation in-
creases power for genome-wide association studies. Nat. Genet, 44, 63 1—63 5.

Rioux,J.D. et al. (2001) Genetic variation in the 5q31 cytokine gene cluster confers
susceptibility to Crohn disease. Nat. Genet, 29, 223—228.

Sabeti,P.C. et al. (2002) Detecting recent positive selection in the human genome
from haplotype structure. Nature, 419, 832—837.

Xie,M. et al. (2012) A fast and accurate algorithm for single individual haplotyping.
BMC Syst. Biol., 6 (Suppl 2), S8.

 

2252

112 /810's112umo[p101x0'sot112u1101u101qﬂ2d11q u1011 papeolumoq

9IOZ ‘09 lsnﬁnV no 22

