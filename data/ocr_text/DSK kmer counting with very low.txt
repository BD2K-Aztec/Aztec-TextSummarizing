APPLICA TIONS NOTE V‘Zlg.ffoil’agii‘ilifé’ﬁiicifgﬁiﬁ

 

Sequence analysis

Advance Access publication January 16, 2013

DSK: k-mer counting with very low memory usage

Guillaume Rizkl, Dominique Lavenier2 and Rayan Chikhi2’*
1Algorizk, 75013 Paris and 2ENS Cachan Brittany/IRISA, Campus de Beaulieu, 35700 Rennes, France

Associate Editor: Michael Brudno

 

ABSTRACT

Summary: Counting all the k—mers (substrings of length k) in DNA/RNA
sequencing reads is the preliminary step of many bioinformatics ap-
plications. However, state of the art k—mer counting methods require
that a large data structure resides in memory. Such structure typically
grows with the number of distinct k—mers to count. We present a new
streaming algorithm for k—mer counting, called DSK (disk streaming of
k—mers), which only requires a fixed user-defined amount of memory
and disk space. This approach realizes a memory, time and disk
trade-off. The multi-set of all k—mers present in the reads is partitioned,
and partitions are saved to disk. Then, each partition is separately
loaded in memory in a temporary hash table. The k—mer counts are
returned by traversing each hash table. Low-abundance k—mers are
optionally filtered. DSK is the first approach that is able to count all the
27—mers of a human genome dataset using only 4.0 GB of memory
and moderate disk space (160 GB), in 17.9 h. DSK can replace a
popular k—mer counting software (Jellyfish) on small-memory servers.
Availability: http://minia.genouest.org/dsk

Contact: rayan.chikhi@ens-cachan.org

Received on October 19, 2012; revised on December 26, 2012;
accepted on January 9, 2013

1 INTRODUCTION

Determining the abundance of each distinct k—mer in a set of
sequencing reads is a conceptually simple yet fundamental
task. It is used in many bioinformatics applications related to
sequencing, e. g. genome and transcriptome assembly, variants
detection and read error correction. For de novo assembly, one
is often interested in counting k—mers to discard those with low
abundance, which likely stem from sequencing errors.

State of the art methods for k—mer counting rely on hash tables
(Jellyﬁsh; Marcais and Kingsford, 2011) and/or Bloom ﬁlters
(BF Counter; Melsted and Pritchard, 2011). These structures
need to reside in memory for random access. Sequencing errors
induce erroneous k—mers, in a volume typically greater or com-
parable with that of correct k—mers. Hence, counting k—mers for a
human dataset with either a single hash table or a Bloom ﬁlter is
a task that requires tens of gigabytes of memory. In Section 2, we
describe a ﬁxed-memory and ﬁxed-disk space streaming algo-
rithm, DSK (disk streaming of k-mers), and its worst-case com-
plexity is analysed in function of the desired memory and disk
usage. In Section 3, DSK is used to count all the 27-mers of a
whole-genome human dataset. The trade-off between memory
and disk space is analysed on two smaller datasets. We conclude

 

*To whom correspondence should be addressed.

 

Algorithm 1. The DSK algorithm

1: Input: The set S of sequences, k—mer length k, target memory usage M
(bits), target disk space D (bits) and hash function h(-)
2: v <— 2 (|s| — k + 1) (Number of k—mers)

seS
3: mm <— [v - 211°g2(2k)l /D_|

v(211°g2(2k)l + 32)
4: np <— —
0.7nitersM

 

(Number of iterations)

(Number of partitions)

5: for each iteration i = 0mm,rs do

6: Initialize a set of empty lists {do, ..., dnp} stored on disk
7 for each sequence s in S do

8 for each k—mer m in .9 do

9: if (h(m) mod niters) = i then
10: j <— h(m)/nit,,rs mod np
11: Write m to disk in 

12: for each indexj = 0..np do
13: Initialize a hash table T with M bits of memory

14: for each k—mer m in  do

_ T[m] + 1, if m is present in T
15' Hm] (— { 1, otherwise
16: output (m, T[m]) for each m in T

17: Delete T
18: Delete {(10, ...,dnp}

 

with a discussion of the advantages of DSK over related
methods.

2 METHODS

Algorithm 1 describes the DSK k—mer counting algorithm. The hash
function h(-) maps a k—mer to a numeric value in [0; H], where H is a
large integer (typically 264). In the following analysis, we make a simplify-
ing assumption. Let d be the total number of distinct k—mers in the input;
we assume that the number of distinct k—mers having a given hash value
x e [0; H] is at most l—d/H-l. In other words, the set of distinct k—mer
values can be uniformly partitioned by this hash function. Each k—mer is
encoded using the classical 2 bits representation in the smallest available
integer type, i.e. using 211°g2(2k)l bits. The abundance of each k—mer is
stored as a 32 bits integer. For convenience, let b = 2llog2(2k)l.

Each k—mer m present in S is examined niters = [vb/D] times (once per
iteration) and is written to disk only once, at the (h(m) mod nitm)-th it-
eration. Using the uniform repartition hypothesis, a multi-set of
v/nitﬁrs 5 [D/b] k—mers are written to disk at each iteration. As each
k-mer is encoded using b bits, the maximal disk usage of the algorithm
is D bits.

The maximal memory usage of the algorithm is M bits, as Steps 7—11
require constant memory, and Steps 12—17 load a single partition in T
that requires exactly M bits. With an open-addressing mechanism, each
distinct k-mer occupies exactly (b + 32) bits in T. To prove that the algo-
rithm terminates, it sufﬁces to show that T never overﬂows, i.e. strictly

 

652 © The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e—mail: journals.permissions@oup.com

112 [glO'SIBILInO[plOJXO'SODBIILIOJHIOIQ/[ldllq mm; pepcolumoq

910K ‘09 lsnﬁnV no :2

DSK: k-mer counting

 

Table 1. Wall-clock time and memory usage for counting 27-mers in
whole-genome human data

 

 

Program Time (h) Memory (GB) Disk (GB)
DSK 17.9 4 160
DSK-SSDa 3.5 4 240
BFCounter 41.2 56 0
Jellyﬁsh 3.5 70 21 1

 

The dataset used is the NA18507 human genome (SRX016231), unﬁltered, consist-
ing of 1.4 billion reads of average length 100 bp (160 GB ﬁle size). Jellyﬁsh used
eight threads, DSK-SSD used four threads and DSK and BFCounter are
single-threaded. The disk column indicates the temporary amount of disk space
used by each method.

aExecuted on a desktop computer equipped with two hard drives, including an SSD.

less than M /(b + 32) distinct k-mers are inserted in T. At each iteration
(v/niters), k-mers are split into np partitions. Each partition contains at
most v/(nitersnp) 5 [0.7M / (b + 32)] k-mers. In the worst case, all these k-
mers are distinct; thus, the load factor is upper-bounded by 0.7 (a classical
threshold above which hash table performance degrades).

The time complexity of Steps 7—11 (including the iteration loop) is
0(v2b/D). The algorithm creates (nitmnp) 5 [v(b + 32)/(0.71%)] tempor-
ary hash tables, inserting at most [(0.7M/(b + 32)] elements in each.
Hash tables accesses and insertions (Step 15) are done in constant ex-
pected time with open-addressing, as long as the load factor is strictly <1
(which was proved earlier in the text). Hence, the expected time complex-
ity of Steps 12—17 (including the iteration loop) is 0(v). Thus, Algorithm
1 runs in expected time 0(v2b/D). The algorithm runs in expected linear
time with respect to v when D = @(v), e.g. setting D equal to the sum of
input bases. In practice, the simplifying assumption on the uniform re-
partition of the hash function h does not hold exactly. Some partitions
contain a slightly larger number of distinct k-mers than l—v/H]. Hence,
the actual disk usage of the algorithm is slightly above D, and the load
factor of T could, in theory, be >0.7 (because of high k-mer redundancy,
this is not the case in practice).

3 RESULTS

In Table 1, we compared the execution time and memory usage
of DSK with Jellyﬁsh (version 1.1.5) and BFCounter (version
0.2) on a human genome Illumina dataset. The target disk
usage of DSK was set to 160 GB, equal to the size of the
reads ﬁle. As the algorithm relies heavily on I/O to the disk,
we also tested DSK with a solid-state drive (DSK-SSD). The
reads ﬁle was placed on a standard hard disk drive, and parti-
tions of redundant k-mers were written on a 256 GB SSD. In this
conﬁguration, we noticed the algorithm is no longer limited by
disk I/O and could beneﬁt from multi-threading. The two for
loops lines 7 and 12 were parallelized using openMP (four
threads). DSK-SSD ran for 3.5h using 4 x 1 GB of memory.
Although this experiment required speciﬁc hardware, it is
worth noting that the running time of DSK can be greatly
reduced with an SSD and multi-core parallelism.

To further assess the trade-off between time, memory and disk
usage, we executed DSK (using a standard hard drive) on two
smaller Escherichia coli and Drosophila ananassae datasets, with
various target memory and disk usage parameters (Figure 1). For
the executions with 100 MB and 1 GB memory usage, the run-
ning time of DSK on both datasets decreases as the target disk

 

 

   

 

 

 

 

 

 

E. coli DNA Drosophila RNA
8 _
L”, _ in, g —
<1) <1)
.§ 8 — .§ 0
I— N I— 8 —
O — o _
I I I I I I I I I I I I
1 15 1 150 4600 148 743 2975
Disk space (MB) Disk space (MB)
Memory (MB)
10 - - 100 — 1000

Fig. 1. Execution time of DSK (k = 21) as a function of memory and disk
usage, on the E.coli (Illumina DNA SRR001665, 20.8 - 106 reads of aver-
age length 36 bp) and D.ananassae datasets (Illumina RNA-Seq
SRR332538, 9.1 - 106 reads of average length 150 bp)

space increases. This is a consequence of the decreasing number
of iterations niters. The running times reach a plateau at roughly
the reads ﬁle size (where niters = 1). The execution time generally
seems to be unaffected by the target memory usage. However, at
the smallest tested memory usage (10 MB), the execution time on
both datasets is slightly higher, possibly because of consecutive
disk writes to a large number of partitions. Note that in practice,
the memory usage of DSK cannot be arbitrarily low: it is limited
by the number of ﬁles that can be simultaneously opened on the
system (partitions {(10, . . . , dnp} are all opened simultaneously).

4 DISCUSSION

Contrary to other methods, DSK does not provide random
access to k-mer counts. However, it beneﬁts from three strong
points:

0 Low-memory usage: Only an arbitrarily small subset of k-
mers is loaded in memory at any time. In contrast,
BFCounter stores all the k-mers with count 2 2 in a hash
table. In principle, Jellyﬁsh can use arbitrarily small hash
tables; however, storing the intermediate results requires a
prohibitive amount of disk (3 1 TB for human genome
reads using a hash table of size 5 GB).

0 Parameters are automatically inferred: The only mandatory
argument is the k-mer length. Optionally, target memory
and disk usages can be speciﬁed. Jellyﬁsh and BFCounter
require the user to specify a hash table size and an
upper-bound on the number of distinct k-mers, respectively.

0 Supports arbitrarily large values of k: As opposed to up to 32
for Jellyﬁsh (unbounded for BF Counter).

Funding: ANR MAPPI, ANR-10-COSI-0004.

Conﬂict of Interest: none declared.

REFERENCES

Marcais,G. and Kingsford,C. (2011) A fast, lock-free approach for efﬁcient parallel
counting of occurrences of k-mers. Bioinformatics, 27, 764—770.

Melsted,P. and Pritchard,J. (2011) Efﬁcient counting of k-mers in DNA sequences
using a bloom ﬁlter. BM C Bioinformatics, 12, 333.

 

653

112 ﬂJO'spaumo[pJOJXO'sorIBmJOJurorw/zdnq IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

