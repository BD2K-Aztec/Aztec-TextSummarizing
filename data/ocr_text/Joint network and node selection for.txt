ORIGINAL PAPER

Vol. 29 no. 16 2013, pages 1987—1996
doi:10. 1093/bioinformatics/btt335

 

Gene expression

Advance Access publication June 8, 2013

Joint network and node selection for pathway-based genomic

data analysis

Shandian Zhe‘, Syed A. Z. Naqvi‘, Yifan Yang2 and Yuan Qi1’3’*

1Department of Computer Science, 2Department of Biology, and 3Department of Statistics, Purdue University,

West Lafayette, IN 47907, USA
Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: By capturing various biochemical interactions, biological
pathways provide insight into underlying biological processes. Given
high-dimensional microarray or RNA-sequencing data, a critical chal-
lenge is how to integrate them with rich information from pathway
databases to jointly select relevant pathways and genes for phenotype
prediction or disease prognosis. Addressing this challenge can help us
deepen biological understanding of phenotypes and diseases from a
systems perspective.

Results: In this article, we propose a novel sparse Bayesian model for
joint network and node selection. This model integrates information
from networks (e.g. pathways) and nodes (e.g. genes) by a hybrid of
conditional and generative components. For the conditional compo-
nent, we propose a sparse prior based on graph Laplacian matrices,
each of which encodes detailed correlation structures between net-
work nodes. For the generative component, we use a spike and slab
prior over network nodes. The integration of these two components,
coupled with efficient variational inference, enables the selection of
networks as well as correlated network nodes in the selected
networks.

Simulation results demonstrate improved predictive performance and
selection accuracy of our method over alternative methods. Based on
three expression datasets for cancer study and the KEGG pathway
database, we selected relevant genes and pathways, many of which
are supported by biological literature. In addition to pathway analysis,
our method is expected to have a wide range of applications in
selecting relevant groups of correlated high-dimensional biomarkers.
Availability: The code can be downloaded at www.cs.purdue.edu/
homes/szhe/software.html.

Contact: alanqi@purdue.edu

Received on February 9, 2013; revised on June 5, 2013; accepted on
June 6, 2013

1 INTRODUCTION

With the popularity of high-throughput biological data such as
microarray and RNA-sequencing data, many variable selection
methods—such as lasso (Tibshirani, 1996) and elastic net
(Zou and Hastie, 2005)7have been proposed and applied to
select relevant genes for disease diagnosis or prognosis.
Nevertheless, these approaches ignore invaluable biological

 

*To whom correspondence should be addressed.

pathway information accumulated over decades of research;
hence, their selection results can be difﬁcult to interpret biologic-
ally and their predictive performance can be limited by a small
sample size of expression proﬁles. To overcome these limitations,
a promising direction is to integrate expression proﬁles with rich
biological knowledge in pathway databases. Because pathways
organize genes into biologically functional groups and model
their interactions that capture correlation between genes, this
information integration can improve not only the predictive
performance but also interpretability of the selection results.
Thus, a critical need is to integrate pathway information with
expression proﬁles for joint selection of pathways and genes
associated with a phenotype or disease.

Despite their success in many applications, previous sparse
learning methods are limited by several factors for the integra-
tion of pathway information with expression proﬁles. For
example, group lasso Wuan and Lin, 2007) can be used to utilize
memberships of genes in pathways via a [1/2 norm to select
groups of genes, but they ignore pathway structural information.
An excellent work by Li and Li (2008) overcomes this limitation
by incorporating pathway structures in a Laplacian matrix of a
global graph to guide the selection of relevant genes. In addition
to graph Laplacians, binary Markov random ﬁeld priors can be
used to represent pathway information to inﬂuence gene selec-
tion (Li and Zhang, 2010; Stingo and Vannucci, 2010; Wei and
Li, 2007, 2008). These network-regularized approaches do not
explicitly select pathways. However, not all pathways are rele-
vant, and pathway selection can yield insight into underlying
biological processes. A pioneering approach to joint pathway
and gene selection by Stingo et al. (2011) uses binary Markov
random ﬁeld priors and couples gene and pathway selection by
hard constraints—for example, if a gene is selected, all the path-
ways it belongs to will be selected. However, this consistency
constraint might be too rigid from a biological perspective: an
active gene for cancer progression does not necessarily imply that
all the pathways it belongs to are active. Given the Markov
random ﬁeld priors and the nonlinear constraints, posterior dis-
tributions are inferred by a Markov Chain Monte Carlo
(MCMC) method (Stingo et al., 2011). But the convergence of
MCMC for high-dimensional problems is known to take a long
time.

To overcome these limitations, we propose a new sparse
Bayesian approach, called Network and NOde Selection
(NaNOS), for joint pathway and gene selection. NaNOS is a
sparse hybrid Bayesian model that integrates conditional and
generative components in a principled Bayesian framework

 

© The Author 2013. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by—nc/3.0/), which permits
non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com

112 /810'S112umo[pJOJXO'soi1chOJuioiw/2d11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

S.Zhe et al.

 

(Lasserre et al., 2006). For the conditional component, we use a
graph Laplacian matrix to encode information of each network
(e.g. a pathway) and incorporate it into a sparse prior distribu-
tion to select individual networks. For the generative component,
we use a spike and slab prior distribution to choose relevant
nodes (e.g. genes) in selected networks. For this hybrid model,
we do not impose the hard consistency constraints used by
Stingo et al. (2011). Furthermore, the prior distribution of our
model does not contain intractable partition functions. This en-
ables us to give a full Bayesian treatment over model parameters
and develop an efﬁcient variational inference algorithm to obtain
approximate posterior distributions for Bayesian estimation. As
described in Section 3, our inference algorithm is designed to
handle both continuous and discrete outcomes.

Simulation results in Section 4 demonstrate superior perform-
ance of our method over alternative methods for predicting
continuous or binary responses, as well as comparable or im-
proved performance for selecting relevant genes and pathways.
Furthermore, on real expression data for diffuse large B cell
lymphoma (DLBCL), pancreatic ductal adenocarcinoma
(PDAC) and colorectal cancer (CRC), our results yield meaning-
ful biological interpretations supported by biological literature.

2 MODEL

In this section, we present the hybrid Bayesian model, NaNOS,
for network and node selection. First, let us start from the clas-
sical variable selection problem. Suppose we have N independent
and identically distributed samples 1) = {(X1,t1), ...,(XN, tN)},
where x,- and t,- are the explanatory variables and the response
of the i-th sample, respectively. The explanatory variables can be
various biomarkers, such as gene expression levels or single-nu-
cleotide polymorphisms. Following the tradition in variable
selection, we normalize the values of each variable so that its
mean and standard deviation are 0 and 1, respectively. The
response can be certain phenotype or disease status. We aim to
predict the response vector t: [t1, ...,tN]T based on the
explanatory variables X 2 [X1, ...,XN]T and to select a small
number of variables relevant for the prediction. Because the
number of variables (e. g. genes) is often much bigger than the
number of samples, the prediction and selection tasks are statis-
tically challenging.

To reduce the difﬁculty of variable selection, we can use
valuable information from networks, each of which contains
certain variables as nodes and represents their interactions.
For example, biological pathways cluster genes into functional
groups, revealing various gene interactions. Based on
M networks, we organize the explanatory variables x,- into
M subvectors, each of which comprises the values of explanatory
variables in its corresponding network. If a variable (i.e. a gene)
appears in multiple networks (i.e. pathways), we duplicate its
value in these networks. Note that networks here are exchange-
able with graphs; we can use them to represent not only
biological pathways but also linkage disequilibrium structures
for genetic variation analysis.

Our model is a Bayesian hybrid of conditional and generative
models based on a general framework proposed by
(Lasserre et al., 2006). The conditional component selects
individual networks via ‘discriminative’ training, the generative

component chooses relevant nodes in the selected networks and
the two models are glued together through a joint prior
distribution, so that the selected networks can guide node selec-
tion and, in return, the selected nodes can inﬂuence network
selection.

Speciﬁcally, for the conditional model, we use a Gaussian data
likelihood function for the continuous response

N
palx, w, r) = HNm-Ixiw, fl) (1)
i=1

where w are regression weights, each of which represents the
contribution of the corresponding node to the response, and
‘L' is the precision parameter. For the unknown variance 1', we
assign an uninformative diffuse Gamma prior, Gam(t| g, h) with
g = h = 10—6.

For the binary response, we use a logistic likelihood

N
palx, w) = 1'[ a(xiw)‘i[1 — a(x.-Tw)]1—“ (2)
i=1

where t,- e {0, 1}, w are classiﬁer weights and o(-) is the logistic
function [i.e. 00/) = (1 + exp(—y))_1]. Based on the M networks,
we partition w into M groups, so that w 2 [W1, . . . ,WM]T where
wk are the weights for the explanatory variables in the k—th
network.

To incorporate the topological information of a network, we
use its normalized Laplacian matrix representation. Speciﬁcally,
given an adjacent matrix Gk that represents the edges (i.e. inter-
actions) between nodes in the k—th network, the normalized
Laplacian matrix Lk is deﬁned as

1 i=j and deg(i) 75 O
. . _ 1 . . . .
Lk(z,]) = —deg(i)degw 1 7’5] and Gk(la.]) 7’5 0
0 otherwise

where deg(i) = Z]. Gk(i, j) is the degree of the i-th node in the
k—th network.

Based on the graph Laplacian matrices, we design the follow-
ing mixture prior over wk to select relevant networks:

19(Wkl06k) = N(Wk|0, S1111: 1)akN(Wk|09 521k)1_ak (3)

where oak is a binary variable indicating whether the k—th network
is selected, 51 >52, 52 w 0 and Ik is an identity matrix. We set the
hyperparameters $1 and 52 based on cross-validation (CV) in our
experiments. To make sure Lk is strictly positive-deﬁnite, we add
a diagonal matrix 10—6Ik to Lk. In (3), Lk captures the
correlation information between nodes in the k—th network.
Note that if we replace Lk by Ik in the slab component, the
prior (3) becomes a simple generalization of the classical spike
and slab prior (George and McCulloch, 1997) for group
selection. When oak = 1, the k—th network is selected and the
elements of wk are encouraged to be similar to each other due
to the Laplacian matrix Lk; when oak = 0, because 52 is close to
zero, the corresponding Gaussian prior prunes wk. We use a
Bernoulli prior distribution to reﬂect the uncertainty in
oak, p(ak) = (uk)°"‘ (1 — uk)1_°‘k where uk 6 [0,1] is the selection
probability. Without any prior preference over selecting or prun-
ing the k—th network, we assign a uniform prior over uk:
p(uk) = 1 [i.e. p(uk) = Beta(uk; a, b) where a = b = 1].

 

1988

112 /810'S112umo[pJOJXO'soi1chOJuioiw/2d11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

Joint network and node selection for pathway-based genomic data analysis

 

To identify relevant nodes, we introduce a latent vector Wk in
the generative model for each network k, which is tightly linked
to wk as explained later. We use a spike and slab prior:

Pk
pawl/3k) = “NW/«110,r1)’3’VN(vT/kj|0,r2)1_/3'v

1:1

Pk N . N 1— . (4)
=  r1)ﬂkJN(O|WkJ-, r2) I319

]=1
2  18k)

where pk is the number of nodes in the k—th network, r2 % O and
ﬂy is a binary variable indicating whether to select the j-th node
in the k—th network. We give ﬂy a Bernoulli prior,
PCB/q) = (vkj)’3"i(1 — vkj)1_’3ki, and a uniform prior over vkj:
p(ka-) = 1 (i.e. p(ka-) = Beta(ka-|c, d) where c = d = 1). As
shown above, the spike and slab prior p(Wk|ﬁk) has the same
form as p(0|Wk, ,Bk), which can be viewed as a generative
model—in other words, the observation 0 is sampled from Wk.
This view enables us to combine the sparse conditional model for
network selection with the sparse generative model for node
selection via a principled hybrid Bayesian model.

Speciﬁcally, to link the conditional and generative models
together, we introduce a prior on Wk:

Pﬁklwk) = N(Wklwka M) (5)

where the variance A controls how similar Wk and wk are in our
joint model. For simplicity, we set A = 0 so that
p(Wk|wk) = 8(v'7'k — wk) where 8(f) = 1 if f=O and 8(f) = 0
otherwise. The graphical model representation of the joint
model is given in Figure 1.

The network and node selections are consistent with each
other in a probabilistic sense. If a network is pruned, all its
node are removed. Because wk 2 Wk is enforced by the prior
8(v'7'k — wk), when oak = 0, wk 2 0 implies Wk 2 0. As a result,
the spike component in (4) will be selected for all the nodes in
the k—th network (i.e. ,Bkj = O for j = 1, ...,pk) with a higher
probability than the slab component. On the other hand, it is
easy to see that if one or multiple nodes in a network are selected,
then this network will be selected too. Note that if a node

 

 

 

31,32 .—

 

:T1,T'2

 

 

 

 

 

 

 

 

 

 

Fig. 1. The graphical model representation of NaNOS

appears in multiple networks and is selected, our model will
not force all the networks that contain this node to be chosen.
The reason is that we duplicate the value of this node in the
networks and treat their corresponding regression or classiﬁca-
tion weights as separate model parameters.

3 ALGORITHM

In this section, we present the variational Bayesian algorithm for
model estimation. Speciﬁcally, we develop the variational
updates to efﬁciently approximate the posterior distribution of
weights w, the network-selection indicators 01, the node-selection
indicators ,6, the network- and node-selection probabilities u and
v and the precision parameter ‘E for regression. Based on the
posteriors of oz and ,6, we can decide which networks and
nodes are selected.

For regression, based on the model speciﬁcation in Section 2,
the posterior distribution of our model is

p(w7 W) a) ﬂ) “7 V7  
l
= ZA/(thW, I‘ll)Gamma(t)-

11pm lak)P(VVk IWk)P(0|VVk, 13k) Bern(otk|uk) Beta(uk)- (6)
k

n Bern(,BkJ-| ij)Beta(ij)
j

where p(wk|ak) and p(0|v7'k, ,Bk) are deﬁned in (3) and (4),
p(Vvk|wk) = 8(Wk — wk) and Z is the normalization constant.
For classiﬁcation, the posterior distribution is similar to (6),
except that we replace the Gaussian likelihood (1) by the logistic
function (2) and remove the precision parameter ‘E and its prior
for regression in (6).

Classical Markov chain Monte Carlo methods can be applied
to approximate the posterior distribution. However, given the
high dimensionality of the parameters (e.g. w and 01), it would
take a long time for a sampler to converge. In practice, it is even
difﬁcult to judge the sampler’s convergence. Thus, we resort to a
computationally efﬁcient variational approximation to (6).

Speciﬁcally, we approximate the exact posterior
distribution in (6) by a factorized distribution: Q(9) =
Q(w)Q(oz)Q(,B)Q(u)Q(v)Q(r), where 9 denotes all the latent vari-
ables. Note that, for classiﬁcation, we do not have Q,(t).
Because we set p(v7|w) = 8(v'7' — w), we do not need a separate
distribution Q07). To solve Q(9), we minimize the Kullback—
Leibler (KL) divergence between the exact and approximate
posterior distributions of 9:

Q(9)
19(9lt, X)

Applying coordinate descent for the minimization of (7), we
obtain efﬁcient updates for the variational distributions as
described in the following sections. The updates are iterative:
we update one of the variational distributions at a time while
having all the other variational distributions ﬁxed, and iterate
these updates until convergence. Because these updates
monotonically decrease the value of the KL divergence (7),
which is lower bounded by zero, they are guaranteed to converge
in terms of the KL value (Bishop, 2006).

 

KL(Q(9)llp(9lt,X)) = / Q(9)1n d9 (7)

 

112 /810's112umo[pJOJXO'soi1chOJuioiw/2d11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no 22

S.Zhe et al.

 

3.1 Regression

The variational distributions for regression have the following
forms:

Q(W) =N<w1m,2) (8)
can = 11 r2211 — Mal—“k (9)
Q(ﬁ) = 11 1'1]. (nap/3m — ref—""2 (10)
Q(u) cc 1} (Lark—la — air—1 (11)
Q(V) oc 1'1, 1'1, (ij)5""_1(1 — wig—1 <12)
Q(r) = mag, 1?) (13)
Their parameters are iteratively updated as follows:

2 = (A + (r)xTX)-1 m = (1:)EXTt (14)
szzyk+a 13k=1—yk+b (15)
akj=nkj+c 2,: 1 —nk,-+d (16)
yk = 1/(1+ exp((ln(1 — uk)) — (1n uk) +1§1n:—: 17
—%ln|Lk| +%tr((wka)(%Lk —i1k)) ( )

na- = 1/(1 + exp (<1n(1 — ij)) — <1nva>
+%1n:—:+%<(wa)2>(%—%))) “8’
i? = h+§tTt—meTt+%Zix,T(wa)x, (19)
§=g+ﬂ am

2

where A = ﬁdiagGJ/kL/Jk) + idiagﬂﬂ - Vk)1k}k)+%diag(n) +
édiagﬂ — 17) [note that diag({ykLk}k) is a block-diagonal ma-
trix], (-) means expectation over the corresponding variational
distribution, and the required moments in the above equations
are

(WWT) = E + mmT (1:) = g/li

(Inuk) = «1(a) — «1(a) <1n(1 — uk» = Mia) — «1(a)

(111 W) = W512]? — 10(ka (111(1 — ij» = dej) — 10%)

where woe) = $111 r(x), a = ak + 13,, and ﬂ,- = 5,,- + 51,-.

3.2 Classiﬁcation

Compared with regression, the classiﬁcation task is more
challenging. Because of the logistic function (2), we cannot dir-
ectly solve the variational distribution Q(w). Therefore, we use a

lower bound proposed by (J aakkola and Jordan, 2000) to replace
the logistic function in the joint distribution:
0(VY(1 - 00))17‘
2r — 1 — (21)
2 ac) exp (# —/(5>(<2r — 1>2y2 — 52))

where f(x) = 4i5tanh(é/ 2), and S is a variational parameter. Note
that the equality is achieved when 52(21— 1)y. Because the
logarithm of the lower bound (21) is quadratic in y, it essentially
converts the logistic function into a Gaussian form so that the
variational inference becomes tractable.

Combining the maximization of the lower bound (21) with the
minimization of the KL divergence (7), we obtain the variational
updates for classiﬁcation. They are the same as those for the
regression task, except for that Q(w) = N(W|m, 2), now we have

2 = (A + 2 Ziﬁsgaxffl m = % 2xT(2t — 1) (22)

where A is the same as in the regression.
In addition, maximization of the lower bound of the logistic
function gives the update for the variational parameter 5,:

$12 = X} (WWT)Xi. (23)

3.3 Computational cost

The computational cost of the proposed algorithm is dominated
by (14) for regression and (22) for classiﬁcation. For both cases,
it takes 0(p3) for matrix inversion to obtain 2 and 0(Np + p2)
to obtain m for each iteration. Thus, the total cost is 0(p3 + Np)
and, for most applications where p>N, it simpliﬁes to 0(p3).

4 EXPERIMENTS

In this section, we apply NaNOS to synthetic and real gene
expression data to select pathways (i.e. networks) and genes
(i.e. nodes), and provide biological analysis of our results. We
also compare NaNOS with alternative methods, including lasso
(Tibshirani, 1996), elastic net (Zou and Hastie, 2005), group
lasso (Jacob et al., 2009; Yuan and Lin, 2007), the network-con-
strained regularization approach [Li and Li (2008), henceforth
‘LL’] and the sparse Bayesian model with the classical spike and
slab prior (George and McCulloch, 1997). For lasso and elastic
net, we used the Glmnet software package (www-stat.stanford.
edu/~tibs/glmnet—matlab/). For group lasso, we treat each path-
way as a group. To handle genes appearing in multiple pathways
(i.e. groups), we ﬁrst duplicated their expression levels for each
group—as suggested by (Jacob et al., 2009)—and then used the
SLEP software package (www.public.asu.edu/~jyeO2/Software/
SLEP/) for group lasso estimation. For the spike and slab
model, we implemented variational inference similar to our
updates in Section 3. Just as NaNOS, all these software packages
use the Gaussian likelihood for regression and the logistic likeli-
hood for classiﬁcation. We used the default conﬁguration of
these software packages for the maximum number of iterations,
initial values and the threshold for convergence. To tune regu-
larization weights in lasso, group lasso and the LL approach, we
conducted thorough 10-fold CV on training data (i.e. not using
the test data) using a large computer cluster. The CV grids on the

 

1990

112 [3.10811211an[p.IOJXO'SODBIIHOJIIIOIQ/ﬂ(11111 111011 pepcolumoq

910K ‘09 lsnﬁnV no 22

Joint network and node selection for pathway-based genomic data analysis

 

free parameters are summarized here: for lasso, a = [0 : 0.01 : 1];
for elastic net, at = [0 : 0.01 : 1] and ,6 = [0 : 0.01 : 1]; for
group lasso (both regression and logistic regression),
a = [0 : 0.01 : 1]; and for the LL approach, A1 = [1 : 25 : 300]
and A2 = [1 : 25 : 300] (we also did a second-level CV after we
pruned the range of A1 and A2 values based on the ﬁrst-level CW.
Finally, for NaNOS, the CV grids are $1 = r1 = [0.1, 1, 3] and
s2 = r2 =[10—3,10—4,10—5,10—6].

On the synthetic data for which we knew the true relevant
pathways, we also compared NaNOS with a popular tool for
gene set enrichment analysis (GSEA) (Mootha et al., 2003;
Subramanian et al., 2005). We treated each pathway as a set,
used GSEA’s default conﬁguration and applied its suggested
criterion false discovery rate (FDR) < 25% to discover enriched
pathways. We then identiﬁed all the genes in these enriched path-
ways as target genes. Because GSEA cannot provide predictions
on responses t, we did not include it for comparison on the real
data.

4.1 Simulation studies

We ﬁrst compare all the methods on synthetic data in the
following three experiments.

Experiment 1. We followed the ﬁrst and second data gener-
ation models used by Li and Li (2008). Speciﬁcally, we simulated
expression levels of 200 transcription factors (TFs), each control-
ling 10 genes in a simple tree-structured regulatory network, and
assumed that four pathways—including all of their genes—have
effect on the response t. We sampled the expression levels of each
TF from a standard normal distribution, xTF ~ N(0, 1) and the
expression level of each gene that this TF regulates from
N(0.7xTF,0.51). This implies a correlation of 0.7 between the
TF and its target genes.

For the ﬁrst model with the continuous response, we designed
a weight vector for each pathway, p = [1, ﬁ, . . . , ﬁ],
corresponding to the TF and 10 genes it regulates, and then
sampled t as follows:

w 2 [5p, — 5p, 3p, — 3p, 0T]T
t = XW + e

where 6 ~ N(0, 062) and 0 is a vector of all zeros.

The second model is the same as the ﬁrst one, except that the
genes regulated by the same TF can have either positive or nega-
tive effect on the response t. Speciﬁcally, we set

—1 —1 —1 1 1

= 19—9—9—9—9-“9— '

p ./10 ./10 ./10 ./10 ./10
\Q—J

7

For the ﬁrst and second models, the noise variance was set to be
062 = (ij})/4 so that the signal-to-noise ratio was 12.85 and
7.54, respectively.

For the binary response, we followed the same procedure as
for the continuous response to generate expression proﬁles X and
the parameters w. Then we sampled t from (2).

For each of the settings, we simulated 100 samples for training
and 100 samples for test. We repeated the simulation 50 times.
To evaluate the predictive performance, we calculated the

prediction mean-squared error for regression and the error
rate for classiﬁcation. To examine the accuracy of gene and
pathway selection, we also computed sensitivity and
speciﬁcity and summarized them in the F1 score, F1 2 2>
(sensitivity x speciﬁcity) / (sensitivity + speciﬁcity). The bigger
the F1 score, the higher the selection accuracy.

All the results are summarized in Figure 2, in which the error
bars represent the standard errors. For all the settings, NaNOS
gives smaller errors and higher F1 scores for gene selection than
the other methods, except that, for classiﬁcation of the samples
from the second data model, NaNOS and group lasso obtain the
comparable F1 scores. All the improvements are signiﬁcant under
the two-sample t-test (P<0.05). We also show the accuracy of
group lasso, GSEA and NaNOS for pathway selection in
Figure 5. Again, NaNOS achieves signiﬁcantly higher selection
accuracy. Because the LL approach was developed for regression,
we did not have its classiﬁcation results. While the LL approach
uses the topological information of all the pathways, they are
merged together into a global network for regularization. In con-
trast, using a sparse prior over individual pathways, NaNOS can
explicitly select pathways relevant to the response, guiding the
gene selection. This may contribute to its improved performance.

Experiment 2. For the second experiment, we did not require
all genes in relevant pathways to have effect on the response.
Speciﬁcally, we simulated expression levels of 100 TFs, each
regulating 21 genes in a simple regulatory network. We sampled
the expression levels of the TFs, the regulated genes and their
response in the same way as in Experiment 1, except that we set

— 1 1 1 0 0
I0_ amau'7m) 7'1'1')
\‘,—J

10

for the ﬁrst data generation model and

_1—1—1—11 100(24)
10— )mﬂmﬂmﬂmﬂu'9‘/2—) 9'1'1')

7

for the second data generation model. Note that the last 11 zero
elements in p indicate that the corresponding genes have no effect
on the response t, even in the four relevant pathways.

The results for both the continuous and binary responses are
summarized in Figures 3 and 5. For regression based on the ﬁrst
data model, NaNOS and LL obtain the comparable F1 scores;
for all the other cases, NaNOS signiﬁcantly outperforms the
alternative methods in terms of both prediction and selection
accuracy (P< 0.05).

Experiment 3. Finally, we simulated the data as in Experiment
2, except that we replaced J2—1 in the denominators in (24) with
21, to obtain a weaker regulatory effect of the TF. Again, as
shown in Figures 4 and 5, NaNOS outperforms the competing
methods signiﬁcantly.

4.2 Application to expression data

Now we demonstrate the proposed method by analyzing
gene expression datasets for the cancer studies of DLBCL
(Rosenwald et al., 2002), CRC (Ancona et al., 2006) and

 

1991

112 [3.10811211an[p.IOJXO'SODBIIHOJIIIOIQ/ﬂ(11111 111011 pepcolumoq

910K ‘09 lsnﬁnV no 22

S.Zhe et al.

 

 

 

 

 

(a) so

80

(b)

(0)20

0.8

 

60

0.6

PMSE
F1
Error rate (%)
or E3 31

A
F1 0.
o o v

N o ‘1

01 01 01

40 05

Data 1 Data 2 ' Data 1 Data 2 Data 1 Data 2 Data 1 Data 2

Regression: PMSE Regression: F1 Classiﬁcation: Error rate Classiﬁcation: F1

Fig. 2. Prediction errors and F1 scores for gene selection in Experiment 1. ENet, S&S and GLasso stand for elastic net, the spike and slab model and
group lasso, respectively; Data 1 and 2 indicate the ﬁrst and second data generation models

 

 

20

 

 

)60

(b) 1

0.8

(a

50

‘—
LL

 

40 0.6

PMSE
A
Error rate (%) 9,
8 a
A
F1 0.
h 6) (X? A

30 04

Data 1 Data 2 ' Data 1 Data 2 5 Data 1 Data 2 I Data 1 Data 2

Regression: PMSE Regression: F1 Classiﬁcation: Error rate Classiﬁcation: F1

Fig. 3. Prediction errors and F1 scores for gene selection in Experiment 2

 

 

 

 

(a) 23

(b) 1 (0) 2°

  

0.75

‘—
LL

PMSE

 

0.5

   

Error rate (%)
01 6‘ a

 

0.25

Regression: PMSE Classiﬁcation: Error rate Classiﬁcation: F1

Regression: F1

Fig. 4. Prediction errors and F1 scores for gene selection in Experiment 3

 

 

PDAC (Badea et al., 2008). We used the probeset—to-gene map- (a)
ping provided in these studies. For the CRC and PDAC datasets 0.98
in which multiple probes were mapped to the same genes, we

took the average expression level of these probes. We used the E 094

pathway information from the KEGG pathway database (www.
genome.jp/kegg/pathway.html) by mapping genes from the
cancer studies into the database, particularly in the categories
of Environmental Information Processing, Cellular Processes
and Organismal Systems.

(b) 1 .GSEA
.GLasso
0.95 .NaNOS
0.9
0.85

EXP1-D1-D2 EXP2-D1-D2 EXP3

F1

0.9
EXP1-D1-D2 EXP2-D1-D2 EXP3

Regression: F1 Classiﬁcation: F1

Fig. 5. F1 scores for pathway selection. ‘EXP’ stands for ‘Experiment’

4.2.1 Diffuse large B cell lymphoma We used gene expression and D Stands for Data mOdel

proﬁles of 240 DLBCL patients from an uncensored study in the
Lymphoma and Leukemia Molecular Proﬁling Project consist of components from multiple overlapped pathways. In

(Rosenwald et al., 2002). From 7399 probes, we found 752
genes and 46 pathways in the KEGG dataset. The median sur-
vival time of the patients is 2.8 years after diagnosis and chemo-
therapy. We used the logarithm of survival times of patients as
the response variable in our analysis.

We randomly split the dataset into 120 training and 120 test
samples 100 times and ran all the competing methods on each
partition. The test performance is visualized in Figure 6a.
NaNOS signiﬁcantly outperforms lasso, elastic net and group
lasso. Although the results of the LL approach can contain con-
nected subnetworks, these subnetworks do not necessarily cor-
respond to (part of) a biological pathway. For instance, they may

contrast, NaNOS explicitly selects relevant pathways. Four path-
ways had the selection posterior probabilities larger than 0.95
and they were consistently chosen in all the 100 splits. Two of
these pathways are discussed below.

First, NaNOS selected the antigen processing and presentation
pathway. The part of this pathway containing selected genes is
visualized in Figure 7a. A selected regulator CIITA was shown to
regulate two classes of antigens MHC I and II in DLBCL (Cycon
et al., 2009). The loss of MHC II on lymphoma cells—including
the selected HLA-DMB, -DQB1, -DMA, -DRA, -DRB1, -DPA1,
-DPB1 and -DQA1—was shown to be related to poor prognosis
and reduced survival in DLBCL patients (Rosenwald et al., 2002).

 

1992

112 /810's112umo[pJOJXO'sot112u1101uiotq//2d11q 111011 pepcolumoq

910K ‘09 tsnﬁnV no 22

Joint network and node selection for pathway-based genomic data analysis

 

 

 

(a) 2.75

A
0'
v
N
O

 

PMSE
Error rate (%)
8

 

2.45

Diffuse large B cell lymphoma

Fig. 6. Predictive performance on three gene expression studies of cancer

Cell membrane

(a) “’l

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

Colorectal cancer

 

A
O
v
_‘
w
(.11

    

 

Error rate (%)

9

Pancreatic ductal adenocarcinoma

Cell Membrane

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

l 
Endoplasmic BMPRl _> Smad1/5/8
Reticulum _
A BMPRZ
Nucleus '
RFX MHC' TNF Oi Transcription
| CIITA |—> CREB ———— ——> MHC" IFNG Smad6/7 factor "" EMT
NFY l' D74
I/C LTBPl Smurf1/2
(b)  _  _
I TGF_B Ia TGFB R2 Smad2/3
MadZ Smad4
Bule —[ APC/C  Securin I DCN
| Bubl |—a Bub3 Cdc20

 

 

 

 

 

Fig. 7. Examples of part of identiﬁed pathways. (a) The antigen processing and presentation pathway for DLBCL; 0)) the cell cycle pathway for CRC;
(c) the TGF-,8 signaling pathway for PDAC. Shaded and unshaded boxes indicate selected and not selected genes, respectively

The selected MHC I (e.g. HLA-A,-B,-C,-G) was reported to be
absent from the cell surface, allowing the escape from immuno-
surveillance of lymphoma (Amiot et al., 1998). And the selected Ii/
CD74 and HLA-DRB were proposed to be monoclonal antibody
targets for DLBCL drug design (Dupire and Coifﬁer, 2010).

Second, NaNOS chose cell adhesion molecules (CAMs).
Adhesive interactions between lymphocytes and the extracellular
matrix (ECM) are essential for lymphocytes’ migration and
homing. For example, the selected CD99 is known to be overex-
pressed in DLBCL and correlated with survival times (Lee et al.,
2011), and LFA-l (ITGB2/ITGAL) can bind to ICAM on the
cell surface and further lead to the invasion of lymphoma cells
into hepatocytes (Terol et al., 1999).

4.2.2 Colorectal cancer We applied our model to a CRC data-
set (Ancona et al., 2006). It contains gene expression proﬁles
from 22 normal and 25 tumor tissues. We mapped 2455 genes
from 22 283 probes into 67 KEGG pathways. The goal was to
predict whether a tissue has the CRC or not and select relevant
pathways and genes.

We randomly split the dataset into 23 training and 24 test sam-
ples 50 times and ran all the methods on each partition. The test
performance is visualized in Figure 6b. Again, based on a two-
sample t-test, NaNOS outperforms the alternatives signiﬁcantly
(P< 0.05). Three out of the four pathways with the selection pos-
terior probabilities larger than 0.95 are discussed below. They
were selected 20, 50 and 50 times in the 50 splits.

First, NaNOS selected the cell cycle pathway. This selection is
consistent with the original result by Ancona et al. (2006). As
shown in Figure 7b, NaNOS selected mitotic spindle assembly
related genes. Speciﬁcally, Bubl and Madl may regulate the

checkpoint complex (MCC) containing Mad2, Bule and
Bub3. The upregulated MCC may in turn inhibit ability of
APC/C to ubiquitinate securin and further lead to mitotic event
extension in CRC (Menssen et al., 2007). NaNOS also chose
cyclin/CDK complexes, among which Cch/CDK4 overexpres-
sion is found in mouse colon tumor and CDK1, CDK2, Cch are
increased in human CRC Wermeulen et al., 2003; Wang et al.,
1998). NaNOS further identiﬁed the minichromosome mainte-
nance (MCM) complex—including MCM2 and MCM5—which
are biomarkers for the CRC stage identiﬁcation (Giaginis et al.,
2009). Moreover, the selected TP53 and c-Myc are known to be
closely related to CRC (Menssen et al., 2007).

Second, NaNOS chose the intestinal immune network for IgA
production. A greatly increased level of IgA—as a result of long-
term intestinal inﬂammation—can increase the chance of CRC
(Rizzo et al., 2011) and serve as an effective biomarker for early
diagnosis of CRC (Chalkias et al., 2011). Also, selected chem-
kines in this pathway, such as CXCR4 and CXCL12, may con-
tribute to CRC progression (Sakai et al., 2012).

Third, NaNOS selected the cytokine—cytokine receptor inter-
action pathway as well as several well-known CRC-related
molecules in this pathway. For instance, CXCL13 is a biomarker
for stage II CRC prognosis (Agesen et al., 2012), CXCL10 dra-
matically increases with CRC progression (Toiyama et al., 2012)
and IL10 secreted by CRC cells can accelerate tumor prolifer-
ation and be used for the prognosis of CRC progression
(Toiyama et al., 2010).

4.2.3 Pancreatic ductal adenocarcinoma This cancer dataset in-
cludes expression proﬁles from 39 PDAC and 39 normal subjects
(Badea et al., 2008). By mapping 2781 genes from 54 677 probes

 

1993

112 /810's112umo[pJOJXO'sot112u1101uiotq//2d11q 111011 pepcolumoq

910K ‘09 tsnﬁnV no 22

S.Zhe et al.

 

 

11 -A-NaNOS
................................ .. "_ENet

10.5-

9.5-

Error rate (%)

 

 

 

1) 2'0 50 80 1
Percentage of deleted edges (%)

Fig. 8. The predictive performance of NaNOS when the pathway struc-
tures are inaccurate. When more edges are randomly selected and
removed from each pathway, the performance of NaNOS degrades
smoothly, but still better than the competing methods

into KEGG pathways, we obtained 67 pathways. Our goal was
to predict whether a subject has the pancreatic cancer and select
relevant pathways and genes. We randomly split the dataset into
39 training and 39 test samples 50 times and ran all the methods
on each partition. The test performance is visualized in Figure 60.
Based on a two-sample t—test, NaNOS signiﬁcantly outperforms
lasso, elastic net and group lasso.

To investigate the sensitivity of NaNOS to the structural noise
in the pathway database, we randomly chose 20, 50, 80 and
100% edges in each pathway and removed them. We tested
NaNOS for each case and reported the average test error rate
in the new Figure 8. As expected, the error rate of NaNOS grad-
ually increases with more edges being removed because less topo-
logical information in pathways is available. But NaNOS still
consistently outperformed all the alternative methods such as
elastic net, the second best method on this dataset. This experi-
ment demonstrates (i) that by exploiting subtle correlation infor-
mation embedded in the pathway topology, NaNOS can boost
its modeling power and predictive performance, and (ii) that
NaNOS is robust to small perturbation in pathway topology.

We also examined the impact of the important prior distribu-
tions on pathway and gene selection probabilities ak and ij. As
described in Section 2, we used the uniform priors [i.e. the
Beta(1,1) prior] over ak and ij, indicating no prior preference
over selecting a pathway or gene or not. The average test error
based on the uninformative priors is 9.15 :l: 0.5, as visualized in
Figure 60. If we change the prior to an informative one,
Beta(1,10) (mean 0.09 and standard deviation 0.083) that
strongly prefers sparsity, then the average test error increases
slightly to 10.0 :l: 0.4. This minor increase in error may stem
from the overspariﬁcation caused by the sparsity prior that are
overconﬁdent (suggested by a small variance). Now if we use
another informative prior Beta(10,1) (mean 0.91 and standard
deviation 0.083) that strongly prefers dense—instead of
sparse—estimation, then the average test error increases to
11.2 :l: 0.5. This relatively larger error increase is exactly what
we expected because now the wrong dense prior aims to select
most pathways and genes. What is important is that, no matter
which of these two informative priors we chose, NaNOS consist-
ently outperformed lasso and group lasso in Figure 60. Between
these two extreme cases, if we use an uninformative or weak
sparse prior [e.g. Beta(0.5,0.5)], we ﬁnd that similar prediction
error rates were obtained for NaNOS as in Figure 6c. The above
analysis indicates that NaNOS is robust to the prior choice.

In addition to using the even splitting strategy with the same
number of training and test samples, we also tested the perform-
ance of all the algorithms in another setting with more training
samples—speciﬁcally, 62 training and 16 test samples. We
repeated the random partitioning 50 times. The average error
rates for NaNOS, elastic net, lasso and group lasso are
8.00 :l: 0.89, 9.90 :l: 1.00, 12.0 :l: 1.0 and 11.0 :l: 0.14, respect-
ively. Again, the two-sample t—test indicates that NaNOS outper-
forms the alternative methods signiﬁcantly (P< 0.05).

Three out of the ﬁve pathways with the selection posterior
probabilities larger than 0.95 are discussed below. They were
selected 35, 50 and 50 times in the 50 splits.

The ﬁrst selected pathway was the transforming growth factor
beta (TGF-,B) signaling pathway. It is essential in epithelial-mes-
enchymal transition (EMT)7a critical component for develop-
mental and cancer processes—and related to PDAC (Krantz
et al., 2012). The selected part of this pathway is visualized in
Figure 70. It shows that IFNG, TNF-a, LTBPl, DCN, TGF-,6
and its receptor TGF-,B R1 were selected. The TGF-,B ligand—
via its receptor—propagates the signal through phosphorylation
of Smads including the selected Smad 4, which in turn translocate
into the nucleus and interact with Snail TFs to regulate EMT
(Krantz et al., 2012). The selected BMP ligand (i.e. BMP2) is
bound to BMP R1 and R2 receptors to activate Smadl, which
is in a protein complex including Smad4. Gordon et al. (2009)
showed that in PANC-l cell line, this protein complex mediates
EMT partially by increasing the activity of MMP-2.

The second identiﬁed pathway was ECM—receptor interaction.
It is associated with desmoplastic reaction, a hallmark in PDAC
(Shields et al., 2012). In this pathway, NaNOS selected the in-
tegrin receptors—including ITGBl, ITGA2, ITGA3, ITGA5,
ITGA6—and the ECM proteins—collagens including COL1A1
and COL1A2, and laminins including LAMC2 and LAMB3.
Important interactions among them were revealed in a previous
study by Weinel et al. (1992).

The third chosen pathway was CAMs. CAMs are pivotal in
pancreatic cancer invasion by mediating cell—cell signal transduc-
tion and cell—matrix communication (Keleg et al., 2003). In this
pathway, the selected molecules include calcium-dependent cad-
herin family molecules (CDH2, CDH3) and neural-related mol-
ecules (MAG); both of them have shown to be related to PDAC
(Kameda et al., 1999; Keleg et al., 2003).

5 DISCUSSION

As shown in the previous section, the new Bayesian approach,
NaNOS, outperformed the alternative sparse learning methods
on both simulation and real data by a large margin. Now we
discuss three factors that may contribute to the improved per-
formance of NaNOS.

First, the spike and slab prior (3) and its generalization (4) in
NaNOS separate weight regularization from the selection of vari-
ables (pathways or genes). Both the (generalized) spike and slab
prior and elastic net can be viewed as mixture models in which
one component encourages the selection of variables and the
other helps remove irrelevant ones. However, unlike the elastic
net where the weights over l1 and l2 penalty functions are ﬁxed,
the spike and slab prior has the selection indicators over these
two components estimated from data. When a variable is

 

1994

112 /810's112umo[pJOJXO'sot112u1101uiotq//2d11q 111011 pepcolumoq

910K ‘09 tsnﬁnV no 22

Joint network and node selection for pathway-based genomic data analysis

 

selected, the model has a Gaussian prior over its value (i.e.
weight) that is equivalent to a l2 regularizer (as in ridge regres-
sion) and does not shrink the value of the selected variable as l1
penalty would do. By contrast, lasso or elastic net, with a ﬁxed
mixture weight, has sparsity penalty over both pruned and se-
lected variables, which can greatly shrink the values of selected
variables and hurt predictive performance.

Second, NaNOS incorporates correlation structures encoded
in pathways for variable selection. Speciﬁcally, it uses pathway
structures into the extended spike and slab prior distribution to
explicitly model the detailed relationships between correlated
genes. In contrast, lasso and elastic net do not use this valuable
correlation information in their models. By comparing prediction
accuracies of NaNOS when 0 and 100% edges are removed from
pathways (Fig. 8), we can see that the detailed correlation infor-
mation captured by the pathway topology can greatly improve
modeling quality.

Third, NaNOS has the capability of selecting both relevant
pathways and genes due to its two-layer sparse structure. By
contrast, with l1 / l2 penalty, group lasso encourages the selection
of all the genes in chosen pathways, leading to dense estimation.
This may be undesirable in practice and deteriorate the predictive
performance of group lasso. NaNOS enhances the ﬂexibility of
group lasso by conducting sparse estimation at both the pathway
(or group) and gene levels. Meanwhile, our Bayesian estimation
effectively avoids overﬁtting, a problem often plaguing ﬂexible
models.

NaNOS has been applied to joint pathway and gene selection
in this article. Inspired by the seminal works in (Chuang et al.,
2007; Fr6hlich et al., 2006; Srivastava et al., 2008; Zycinski et al.,
2013), we can use NaNOS in a variety of biomedical applications
where there are abundant high-dimensional biomarkers of indi-
vidual samples and other information sources—for example, the
gene ontology (GO) and protein—protein interaction networks
information—that capture correlation in the high-dimensional
space. Here we discuss two approaches to apply NaNOS when
we have only GO or other group information without network
topology. The ﬁrst approach is to compute some distance or
similarity scores between genes based on the GO information
[e.g. following the approach by Srivastava et al. (2008)] and
then estimate the network topology based on a network learning
method, for example, graphical lasso (Friedman et al., 2008).
With the estimated network topology, we can compute the
graph Laplacian matrices and apply NaNOS to select genes
and groups of genes. The second approach is to directly use
the group membership information in NaNOS by replacing the
graph Laplacian matrices with identity matrices. This approach
becomes useful when we even do not have any information avail-
able to learn the network topology. As shown in Figure 8, even
when all the edges were removed and we had only group infor-
mation, NaNOS still outperformed the second best method, elas-
tic net, in terms of prediction accuracy.

Funding: This work was supported by NSF IIS-0916443, NSF
CAREER Award IIS-1054903, and the Center for Science of
Information (CSoI), an NSF Science and Technology Center,
under grant agreement CCF—0939370.

Conflict of Interest: none declared.

REFERENCES

Agesen,T. et al. (2012) ColoGuideEx: a robust gene classiﬁer speciﬁc for stage II
colorectal cancer prognosis. Gut, 61, 1560—1567.

Amiot,L. et al. (1998) Loss of HLA molecules in B lymphomas is associated with an
aggressive clinical course. Br. J. Haematol, 100, 655—663.

Ancona,N. et al. (2006) On the statistical assessment of classiﬁers using DNA
microarray data. BM C Bioinformatics, 7, 387.

Badea,L. et al. (2008) Combined gene expression analysis of whole-tissue and micro-
dissected pancreatic ductal adenocarcinoma identiﬁes genes speciﬁcally overex-
pressed in tumor epithelia. Hepatogastroenterology, 55, 2016—2027.

Bishop,C.M. (2006) Pattern Recognition and Machine Learning (Information Science
and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ.

Chalkias,A. et al. (2011) Patients with colorectal cancer are characterized by
increased concentration of fecal hb-hp complex, myeloperoxidase, and secretory
IgA. Am. J. Clin. Oncol., 34, 561—566.

Chuang,H. et al. (2007) Network-based classiﬁcation of breast cancer metastasis.
Mol. Syst. Biol, 3, 140.

Cycon,K. et al. (2009) Alterations in CIITA constitute a common mechanism
accounting for downregulation of MHC class II expression in diffuse large
B-cell lymphoma (DLBCL). Exp. Hematol, 37, 184—194.

Dupire,S. and Coiffier,B. (2010) Targeted treatment and new agents in diffuse large
B cell lymphoma. Int. J. Hematol, 92, 12—24.

Friedman,J. et al. (2008) Sparse inverse covariance estimation with the graphical
lasso. Biostatistics, 9, 432—441.

Fr6hlich,H. et al. (2006) Kernel based functional gene grouping. In: International
Joint Conference on Neural Networks. IEEE Computer Society, Los Alamitos,
CA, USA, pp. 3580—3585.

George,E.I. and McCulloch,R.E. (1997) Approaches for bayesian variable selection.
Statistica Sinica, 7, 339—373.

Giaginis,C. et al. (2009) Clinical signiﬁcance of MCM-2 and MCM-5 expression in
colon cancer: association with clinicopathological parameters and tumor prolif-
erative capacity. Dig. Dis. Sci., 54, 282—291.

Gordon,K. et al. (2009) Bone morphogenetic proteins induce pancreatic cancer cell
invasiveness through a Smadl-dependent mechanism that involves matrix
metalloproteinase-2. Carcinogenesis, 30, 238—248.

J aakkola,T.S. and J ordan,M.I. (2000) Bayesian parameter estimation through vara-
tional methods. Stat. C0mput., 10, 25—37.

Jacob,L. et al. (2009) Group lasso with overlap and graph lasso. In: Proceedings
of the 26th International Conference on Machine Learning. New York,
pp. 433—440.

Kameda,K. et al. (1999) Expression of highly polysialylated neural cell
adhesion molecule in pancreatic cancer neural invasive lesion. Cancer Lett.,
137, 201—207.

Keleg,S. et al. (2003) Invasion and metastasis in pancreatic cancer. Mol. Cancer,
2, 14.

Krantz,S. et al. (2012) Contribution of epithelial-to-mesenchymal transition
and cancer stem cells to pancreatic cancer progression. J. Surg. Res., 173,
105—112.

Lasserre,J. et al. (2006) Principled hybrids of generative and discriminative models.
In: IEEE Computer Society Conference on Computer Vision and Pattern
Recognition. Vol. 1, IEEE Computer Society, Washington, DC, USA,
pp. 87—94.

Lee,S. et al. (2011) Clinicopathologic characteristics of CD99-positive diffuse large
B-cell lymphoma. Acta. Haematol, 125, 167—174.

Li,C. and Li,H. (2008) Network-constrained regularization and variable selection
for analysis of genomics data. Bioinformatics, 24, 1175—1182.

Li,F. and Zhang,N. (2010) Bayesian variable selection in structured high-dimen-
sional covariate space with applications in genomics. J. Am. Stat. Assoc., 105,
1202—1214.

Menssen,A. et al. (2007) c-MYC delays prometaphase by direct transactivation of
MAD2 and Bule: identiﬁcation of mechanisms underlying c-MYC-induoed
DNA damage and chromosomal instability. Cell Cycle, 6, 339—352.

Mootha,V.K. et al. (2003) PGC-la-responsive genes involved in oxidative phos-
phorylation are coordinately downregulath in human diabetes. Nat. Genet, 34,
267—273.

Rizzo,A. et al. (2011) Intestinal inﬂammation and colorectal cancer: a double-edged
sword? World J. Gastroenterol., 17, 3092—3100.

Rosenwald,A. et al. (2002) The use of molecular proﬁling to predict survival after
chemotherapy for diffuse large-B-cell lymphoma. N. Engl. J. Med., 346,
1937—1947.

 

112 /810's112umo[pJOJXO'sot112u1101utotq//2d11q 111011 pepcolumoq

910K ‘09 tsnﬁnV no 22

S.Zhe et al.

 

Sakai,N. et al. (2012) CXCR4/CXCL12 expression proﬁle is associated with tumor
microenvironment and clinical outcome of liver metastases of colorectal cancer.
Clin. Exp. Metastasis, 29, 101—110.

Shields,M. et al. (2012) Biochemical role of the collagen-rich tumour microenviron-
ment in pancreatic cancer progression. Biochem. J., 441, 541—552.

Srivastava,S. et al. (2008) A novel method incorporating gene ontology information
for unsupervised clustering and feature selection. PLoS One, 3, l2.

Stingo,F.C. and Vannucci,M. (2010) Variable selection for discriminant analysis
with Markov random ﬁeld priors for the analysis of microarray data.
Bioinformatics, 27, 495—501.

Stingo,F.C. et al. (2011) Incorporating biological information into linear models: A
Bayesian approach to the selection of pathways and genes. Ann. Appl. Stat., 5,
1978—2002.

Subramanian,A. et al. (2005) Gene set enrichment analysis: a knowledge-based ap-
proach for interpreting genome-wide expression proﬁles. PNAS, 102,
15545—15550.

Terol,M. et al. (1999) Expression of beta-integrin adhesion molecules in
non-Hodgkin’s lymphoma: correlation with clinical and evolutive features.
J. Clin. Oncol., 17, 1869—1875.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. R Stat.
Soc., B, 58, 267—288.

Toiyama,Y. et al. (2010) Loss of tissue expression of interleukin-10 promotes the
disease progression of colorectal carcinoma. Surg. Today, 40, 46—53.

Toiyama,Y. et al. (2012) Evaluation of CXCL10 as a novel serum marker for pre-
dicting liver metastasis and prognosis in colorectal cancer. Int. J. Oncol., 40,
560—566.

Vermeulen,K. et al. (2003) The cell cycle: a review of regulation, deregulation and
therapeutic targets in cancer. Cell Prolif., 36, 131—149.

Wang,Q. et al. (1998) Altered expression of cyclin D1 and cyclin-dependent kinase 4
in azoxymethane-induced mouse colon tumorigenesis. Carcinogenesis, 19,
2001—2006.

Wei,Z. and Li,H. (2007) A Markov random ﬁeld model for network-based analysis
of genomic data. Bioinformatics, 23, 1537—1544.

Wei,Z. and Li,H. (2008) A hidden spatial-temporal Markov random ﬁeld model for
network-based analysis of time course gene expression data. Ann. Appl. Stat., 2,
408—429.

Weinel,R. et al. (1992) Expression and function of VLA-a2, -a3, -a5 and —alpha6-
integrin receptors in pancreatic carcinoma. Int. J. Cancer, 52, 827—833.

Yuan,M. and Lin,Y. (2007) Model selection and estimation in regression with
grouped variables. J. R Stat. Soc., B, 68, 49—67.

Zou,H. and Hastie,T. (2005) Regularization and variable selection via the elastic
net. J. R Stat. Soc., B, 67, 301—320.

Zycinski,G. et al. (2013) Knowledge Driven Variable Selection (KDVS) a new ap-
proach to enrichment analysis of gene signatures obtained from high-through-
put data. Source Code Biol. Med., 8, 2.

 

112 /BJO's112umo[pJOJXO'sot112u1101utotq//2d11q 111011 pepcolumoq

910K ‘09 tsnﬁnV no 22

