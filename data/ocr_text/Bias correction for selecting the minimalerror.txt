ORIGINAL PAPER

Vol. 30 no. 22 2014, pages 3152—3158
doi:10. 1093/bioinformatics/btu520

 

Genome analysis

Advance Access publication August 1, 2014

Bias correction for selecting the minimal-error classifier from

many machine learning models

Ying Dingl’z’l, Shaowu Tangz’l, Serena G. Lia02, Jia Jia2, Steffi Oesterreichs, Yan Lin2 and

George C. Tsengl’2’*

1Joint Carnegie Mellon University—University of Pittsburgh Ph.D. Program in Computational Biology, 2Department
of Biostatistics, Graduate School of Public Health, University of Pittsburgh, Pittsburgh, PA 15261, USA and
3Magee—Womens Research Institute, Pittsburgh, PA 15213, USA

Associate Editor: John Hancock

 

ABSTRACT

Motivation: Supervised machine learning is commonly applied in gen-
omic research to construct a classiﬁer from the training data that is
generalizable to predict independent testing data. When test datasets
are not available, cross-validation is commonly used to estimate the
error rate. Many machine learning methods are available, and it is well
known that no universally best method exists in general. It has
been a common practice to apply many machine learning methods
and report the method that produces the smallest cross-validation
error rate. Theoretically, such a procedure produces a selection bias.
Consequently, many clinical studies with moderate sample sizes (e.g.
n = 30—60) risk reporting a falsely small cross-validation error rate that
could not be validated later in independent cohorts.

Results: In this article, we illustrated the probabilistic framework of
the problem and explored the statistical and asymptotic properties.
We proposed a new bias correction method based on learning curve
fitting by inverse power law (IPL) and compared it with three existing
methods: nested cross-validation, weighted mean correction and
Tibshirani-Tibshirani procedure. All methods were compared in simu-
lation datasets, five moderate size real datasets and two large breast
cancer datasets. The result showed that lPL outperforms the other
methods in bias correction with smaller variance, and it has an add-
itional advantage to extrapolate error estimates for larger sample
sizes, a practical feature to recommend whether more samples
should be recruited to improve the classifier and accuracy. An R pack-
age ‘MLbias’ and all source files are publicly available.

Availability and implementation: tsenglab.biostat.pitt.edu/
softwarehtm.

Contact: ctseng@pitt.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on December 21, 2013; revised on July 10, 2014; accepted
on July 26, 2014

1 INTRODUCTION

In the past two decades, fast development in bioinformatics
was accompanied by the rapid production of high-throughput
genomic data, such as gene expression, genotyping and various

 

*To whom correspondence should be addressed.
TThe authors wish it be known that, in their opinion, the ﬁrst two authors
should be regarded as Joint First Authors.

types of next-generation sequencing data. Such high-dimensional
data usually come with small sample sizes and a large number
of genes/features (also known as ‘large p, small 11’ problem) and
pose many new challenges in statistical learning and data mining.
In the content below, we focus on machine learning of gene
expression proﬁle data, but the concept and theoretical issues
also apply to other high-throughput genomic (e. g. copy
number variation, DNA methylation) or proteomic data. In
gene expression proﬁle analysis, it is of great interest to predict
or diagnose a disease status (e. g. classify cases versus controls or
treatment responders versus non-responders). Because no univer-
sally best machine learning method exists in general (Allison
et al., 2006), to fulﬁll this task, multiple models are often con-
structed with different combinations of features (genes), different
machine learning methods as well as different tuning parameters
in the methods. To choose among such a large number of clas-
siﬁers (models), it is common practice to select the model with
the smallest cross-validation error rate, called the minimal-error
classiﬁer (MEC), and report its associated error rate.

The MEC error rate is, however, generally downward biased
and an overly optimistic estimator of the true optimal classiﬁca-
tion error rate. This is because taking the minimum of cross-val-
idation error rates, where the estimates are random variables, will
inevitably yield a downward bias. Such a selection bias has great
adverse impact in many biomedical pilot studies with moderate
sample sizes (e.g. n = 30—60). The problem, however, has often
been overlooked in applications. For example, one can examine
the small pilot data using ~10 popular machine learning methods,
and simultaneously choose among many different numbers of
features and tuning parameters in each method. This easily in-
creases the number of tested classiﬁers to several hundreds and
selects the MEC with a falsely small error rate because of the
selection bias. When the model proceeds to a large cohort valid-
ation for translational research, it will likely fail. In the Molecular
Taxonomy of Breast Cancer International Consortium
(METABRIC) example that will be demonstrated in Section
3.3, we will show that the MEC bias can mistakenly reduce the
error rate from 28.2% to an overly optimistic 19.1% in early- and
late-stage classiﬁcation (i.e. a —9.1% error rate bias). Many re-
searchers have recognized this problem (Bernau et al., 2013;
Berrar et al., 2006; Efron, 2009; Fu et al., 2005; Tibshirani and
Tibshirani, 2009; Varma and Simon, 2006; Wood et al., 2007).
Dupuy and Simon (2007) recommended to ‘report the estimates

 

3152 © The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

112 [3.10811211an[plOJXO'SODBIIIJOJIIIOIQ/[idlflq IIIOJJ popcorn/hog

910K ‘09 lsnﬁnV no :2

Bias correction for selecting the minimal-error classiﬁer

 

for all the classiﬁcation algorithms not just the minimal error rate’.
Some proposed comparing the minimal error rate with the median
error rate from the original datasets with permuted class labels
(Boulesteix and Strobl, 2009). These suggestions, however, did not
provide a real solution. Youseﬁ et al. (2011) proposed an ap-
proach to estimate the bias by a multivariate Gaussian distribu-
tion assumption between the minimal estimated error rate and the
true minimal error rate. The Gaussian assumption is, however,
generally questionable, and the method may not be accurate with
a small sample size.

In this article, three applicable bias correction methods pro-
posed in the literature will be compared with our proposed
inverse power law (IPL) method: nested cross-validation
(nestedCV) Warma and Simon, 2006), Tibshirani-Tibshirani
procedure (TT) (Tibshirani and Tibshirani, 2009) and weighted
mean correction methods WVMC/WMCS) (Bernau et al., 2013).
Tibshirani and Tibshirani proposed a simple bias estimation
method that is computationally efﬁcient and could be calculated
through a traditional K—fold cross-validation. They claimed that
the bias is only an issue when p >> 11 where p is the number of
genes and n is the number of samples. The nestedCV, proposed
by Varma and Simon, introduced another outer loop of
cross-validation, so that the model selection stage is wrapped
in the training samples of the outer loop. This double loop
procedure, which amounts to nested double leave-one-out
cross-validation (LOOCV), is computationally expensive with
complexity of 0(n2). WMC/WMCS (Bernau et al., 2013) was
proposed as a smooth analytical alternative to nestedCV based
on subsampling, which yielded a competitive estimate compared
with nestedCV at a much lower computational price.
Theoretically, according to Bernau et al. (2013), the TT
method does not apply subsampling in the bias correction, and
its estimation target is the conditional error rate (conditional on
the given samples). The nestedCV and WMC/WMCS (and the
IPL method we will propose) target on the unconditional error
rate by repeated subsampling. In addition, both nestedCV and
WMC/WMCS methods target on the error rate of a wrapper
algorithm (multiple algorithms and/or rules to decide which
one shall be used), which is slightly different from the MEC
error rate we discuss in this article. We, however, compare all
methods side-by-side because biologically they all conceptually
aim to correct MEC bias from many machine learning models.

This article is structured as follows. We ﬁrst illustrate the
MEC bias by a 2D toy example and discuss its asymptotic
theory and statistical properties. The performance of the
nestedCV, WMC/WMCS and TT will be examined. A subsam-
pling-based IPL method will be proposed for the bias correction
and compared with the three existing methods in both simulated
and real datasets. In real data evaluation, we will use ﬁve Gene
Expression Omnibus (GEO) datasets and two large breast cancer
datasets [the Cancer Genome Atlas (TCGA) and METABRIC].

2 METHODS
2.1 Problem setting and formulation under simulation
scheme

Assume that an observed dataset D with sample size n and number of
features 1) is to be analyzed for machine learning. Assume that D is

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Given n samples and M classifiers: :lmfm a“ a: uﬂﬂﬂﬂnﬂ "u.
i SEMI-“mad dmﬁﬁtﬁ = {ill-I Pun-Lb}!!! “it mr rat:
I '- PMIIB} Fill.” = m'tnlﬂilsﬂ FIJI'I
F11    PHI“
E Eu] B: Eaiimllld trul
% F Irmr ra‘ll 'l'mrn Ilmullﬂon
g “.mrb -
T: _ {In-'3“- : limﬂrsm F. 
E 1m“ : P;:JF{B] 'W-‘Etimsﬂpu this}
1n:ch P;_"=l'ima_.m 1",“. "{3}
_— . Fania}
  a + I + a 
in"; = mil-11m in.”
Bax c: Averaged HEB Hrs-r rate- Eu: D: Estimated IIIEC bias
Fania} = (33-1 ﬁrms)” bu.“ =5 {Fans} — Fruit
£{Pﬂﬂj} = Iimﬂ—Iw ﬁn,” [3} =[lmH—rm-FJLMKB] — pilgrim}

 

 

 

 

 

 

Fig. 1. The framework of estimating the true optimal classiﬁcation error
rate, MEC error rate and its bias from independently simulated datasets

generated from an underlying data distribution An. M Z 2 classiﬁcation
methods are used to learn a good classiﬁer for future prediction. M can
be large (6. g. several hundred), as different feature selection or different
parameter settings under a machine learning method are considered dif-
ferent classiﬁers. Suppose the unknown true error rate of classiﬁcation
method m for data distribution An is Pam. The theoretical best machine
learning method for data distribution An is m* =argminmPn,m and the
resulting error rate is PZM=min15m5MPmm (Box A of Fig. 1).

We will illustrate the problem in a simulation framework in Figure 1
because the underlying truth and error rates can be estimated well from
repeated simulations. Suppose B datasets Db(1 5 b 5 B) are indepen-
dently generated from An. We use a cross-validated error rate (from
LOOCV) to approximate the error rate for Db of sample size n using
method m, denoted as Emmi. As 13mm], is the LOOCV error rate, it
is almost an unbiased error estimator (i.e. E(IA’,,,,,,J,) g PM"). We have
PM, = lim 3%, P,,,,,(B), where P,,,,,(B) = (zleﬁn,m,b)/B. Denote by
M(B)=min15n,§MIA’n,m(B). We will show later that PZM=min15mEM
PM, = lim 390° Pn’M(B) (see Box B of Fig. 1). In other words, the true
best classiﬁer error rate PZ’M can be estimated by Pn’M(B) when many
datasets (i.e. B is large) can be repeatedly simulated from An. In real data
analysis, such repeated simulation is, however, not possible. When a
single simulated dataset Db is given, the MEC is chosen by the minimal
error rate: my“) =argmin15m5MFn,m,b and Pn’M’b =min15m5MIA’n,m,b.
The expected value of the MEC error rate can be estimated as
E(Pn,M,b) =  B—>oo Pn,M(B)9 Where Pn,M(B) =(lef=113n,M,b)/B (BOX
C in Fig. 1). Finally, the bias of the MEC error rate can be estimated
as I)“, = E(1”>,,,M,b) — P2,, = hurma P,,M(B) — PZ’M(B), where the ﬁrst
term is the estimated expectation of the MEC error rate and the second
term is the estimated true best classiﬁer error rate. In Section 2.2, a 2D
toy example will be used to demonstrate the issue and properties of the
MEC bias law. In Section 2.3, we will show that E(I~’:’M’b)<P:’M is
always true and the MEC error rate is always downward (optimistically)
biased (i.e. me < 0).

2.2 Illustration by a 2D toy model

Below we present the problem in a 2D toy simulation model. Although the
simple simulation model is not intended to mimic a real gene expression
proﬁle setting, it illustrates the MEC bias issue with known underlying
truth. We simulate B = 1000 training sets D1, ---, D1000 from An where An
contains n = (20, 30, 40, 60, 80, 120, 160, 320, 640, 1280, 2560) data points
in a 2D Euclidean space. Data points from two equal-size classes are
simulated, one (with n/2 data points) from N((?) (i 1)), and the
other from N(( _°2), (} ;)). M = 10 classiﬁers are applied to each
dataset: k—nearest neighbors (KNN) with k = 1, 3, 5, diagonal linear

 

3153

112 [3.10811211an[plOJXO'SODBIIIJOJIIIOIQ/[idlflq IIIOJJ popcorn/hog

910K ‘09 lsnﬁnV no :2

Y.Ding et al.

 

 

 

 

 

 

ILD
"l_l'r _
D
“E
I—
E
as E _ a
I: a. f! ‘5.
1:3 “In... J '
-— nap-‘0‘ .I-ﬁ- "-.._
E
L“: '3 +._
H II _
I 1. HIP-l.-
'I+_—|--+'I-—_ “I: u'-"':
«I1:- \_< _,___ ——-a.._ L+e_---+---'
E  _ i—F +--'.‘:..:_L-._:t:-_--t
"" r:- + ‘“"---+
n_n——-D—ﬂau
-I—._u_u
I I I I I I I I I I I
I:- 1:: I: 41:: c:
a a a e... .2. e. a g a

sample; sige (Ill)

‘3.“ _
.— --EI-- knn k=1 /°
 I-cnn k=3 I:
E muss—- Knn K=5 /
E g ... _....:s.-_- dicta a
m —E— qda
g -—~s-—- par'n
m m: _ —l— sum-linear n-
3 D ——-l—- sun's-radial
a... 1k -—--I—-- nnat a!“
E =3 ' /
g u aid-"Tn
E N .Tn-ﬂ-_ ﬂ“!
EL :5 _ u _u‘ _‘__.A-__‘___L__
fz':‘i'F-n""$----n "M...
i + “bl—l... i-'—"  1‘55
rah-‘2'.  EE‘FW -r-- 'E.-_--n-_-— “a.
g _ ﬁ:—E*E—_'§-_—_'  “TEE...
I I

 

 

 

 

 

 

II II III!
aeeeaaeggeg

sample size—(N) T

Fig. 2. Left: The estimated classiﬁcation error rate of each method at different sample sizes from 1000 independent simulation of the 2D toy model in
which QDA was the top classiﬁer in this context. Right: Probability for each classiﬁer to be chosen as the minimal error rate classiﬁer in the 1000

simulations

discriminant analysis (DLDA), quadratic discriminant analysis (QDA),
shrunken centroids discriminant analysis (SCDA), support vector
machines (SVM-linear) with linear kernel, support vector machines
(SVM-non—linear) with radial basis kernel, random forest (RF) and
neural networks with one hidden layer GVN ET). The R package
Classiﬁcation for MicroArrays (CMA) (Slawski et al., 2008) is applied to
implement all the classiﬁcation methods. Given the Gaussian assumptions
and non-identical covariance matrixes in two classes, QDA is expected to
be the optimal Bayes classiﬁer, and the best decision boundary is of a
quadratic form. Figure 2, left, shows the averaged error rate for classiﬁca-
tion method 112 at sample size n [i.e. Pn,m(B)] estimated from B = 1000
independently simulated datasets. As expected, QDA (solid line) is the
optimal Bayes classiﬁer and has the lowest error rates for all different n.
However, in real data analysis, we are given only one observed dataset.
Figure 2, right, shows the probability of the methods chosen as the error
classiﬁer [MEC; i.e. ﬁthEO] for different n. When the sample size is small,
MEC may not necessarily select the best method QDA. Particularly, when
n = 20 and 30, KNN methods (K = 1 with dashed line and K = 3 with
dotted line) generate a smaller error rate than QDA with higher probabil-
ity. When sample size becomes large (n > 160), the dataset contains enough
information for QDA to be dominantly (>50% probability) selected by
MEC. In Figure 3a, the true minimal error rate P;M(B) g PZ’M from
QDA (cross) and expectation of MEC error rate Pn,M(B) g E(P,,,M,b)
(circle) are shown for different n. In Figure 3b, the estimated MEC
biases 19A”,M(B) = Pn,M(B) — M(B) are shown for different 12. The result
clearly demonstrates a downward bias of the MEC error rate, and the bias
is greater for small sample sizes and diminishes to zero when sample size is
large. It is notable that the bias can be up to 3—5% for n = 20—30. Figure
3c shows the MEC bias for different n when the number of classiﬁers
examined reduces from 10 methods (circle) to four methods (QDA,
LDA, SVM-linear and SVM-non—linear) (triangle) or two methods
(QDA and LDA) (cross). The result shows that the bias increases as
more methods were compared. To our knowledge, only few studies have
recognized the increasing trend of MEC bias magnitude when sample size
is small or when the searching space of machine learning methods is large.
For example, Boulesteix and Strobl’s indirectly showed that bias increases
with decreasing sample size by showing a non-decreasing trend of the
MEC error rate when sample size gets large.

To further study the relationship between the bias and number of
classiﬁers used, we ﬁx n = 20, starting by using the classiﬁer of KNN
with k = 1 and keep adding one classiﬁer at a time until all 10 classiﬁers

 

 

 

 

 

 

 

 

 

 

 

 

 

55;. mt. r__,,--.._-..--.
Edi. gw- Iir..II--:"'
:5 i E'Ct. 1"”
._E_- I __-.-:. _
E11 3' I I l I I E ' f!’

DI . I I 1'
E - i q u 'ﬂ I I -E$- g!
as. n --
Etc. E-

iiiatlli'a'ltlnllaéi- assassins 'siJIiI'IiI'I'm'_3aI_srieIle:IssiIII

[c] sampleslts [II] sand-E SEE III]

$_ I ‘1, L n. l a _- I: I _=_
m”  '_'.‘::..I~***-__ r -.r. ---- —-+--

E; ea-“gsr-riﬂm-F'" "

q' I .II' 'I" -
ﬁe a...» if.” +¢+2dsssIﬁsrs
it- I“ F -l- 4 dass'rliers
53- t" +1ilstassiliers

‘1 at r. s te Ii:- rt .1: , eh eh eh eh
m; n-Eﬂl mph 5'39 ["1 1'5] “.211
Is“! .
fit. as:

E3" 3* I

m i I I :I I: I I h I ll ﬁm' i

ﬁgs ' i s 33' I 1

Ed: I' I . I i l  I I I

is: t. i :

 

 

 

 

IiiI'n ii 11-:

t J. t t r-
classiﬁers applied il-ii

.-
M-

5 4' i i i in
classiﬁers applied llIll
Fig. 3. Illustration of the downward bias of MEC error rate. (a) Trend of
true optimal classiﬁcation error rate (cross) and MEC error rate (circle) as
sample size increased. (b) Bias estimate of the MEC error rate diminishes
as the sample size increased. (0) Bias estimate when using all 10 classiﬁers
(cross), two classiﬁers QDA and DLDA (circle) and four classiﬁers QDA,
DLDA, SVM-linear and SVM-non-linear (triangle). (d) For a ﬁxed
sample size n = 20, the true minimal error rate (cross) and bias of
MEC error rates (circle) as more classiﬁers were added in the sequence
of KNN k = 1, QDA, DLDA, KNN k = 5, PAM, KNN K = 3, SVM-
linear, SVM-radial basis, RF and NNET. (e) The corresponding bias as

the number of classiﬁers used increased from 2 to 10

are used. The true optimal classiﬁcation error rates 13:, M(B) (cross) and
MEC error rate Pn,M(B)(circle) (Fig. 3d) and the MEC error rate biases
(Fig. 3e) for different number of classiﬁers are shown. The result shows
that the bias increased from ~2% for two classiﬁers (KNN K = 1 and
QDA) to ~5% for 10 classiﬁers, while the true optimal error rate  M(B)
(cross) no longer decreases once QDA is added as the second classiﬁer.
This result indicates that including additional low-performing classiﬁers

 

3154

112 ﬁle'srcumo[pJOJXO'sor112u1101urorq/ﬁd11q 111011 popcorn/hog

910K ‘09 lsnﬁnV no 22

Bias correction for selecting the minimal-error classiﬁer

 

inﬂates the bias tO the extent that may not be compensated by the de-
crease Of the true best classiﬁcation error rate. In other words, examining
tOO many classiﬁers and choosing the best is not a good practice, espe-
cially if the added classiﬁers are likely not the top performers. Therefore,
caution is called in the small sample size regime when reporting the min-
imal error rate from multiple classiﬁers in practice, and it is advantageous
if the optimal classiﬁers can be applied as early as possible without adding
more low-performing classiﬁers. The theorems in the next subsection
show that the increasing magnitude Of bias for small sample sizes or
large numbers Of examined classiﬁers are common statistical properties
in data analysis.

2.3 Properties and asymptotic theorems of MEC bias
The proofs Of the following theorems are included in the Appendix:

THEOREM 1. Given a smaller set Of classiﬁers, adding more classiﬁers
will decrease the true best classiﬁcation error rate (i.e. P; M 5 PZ’MZ if
M1 > M 2).

THEOREM 2. For a given observed dataset D”) from An, E(P:’M) < PZM,
where 15:, M = minlSmSM 13mm and 13mm is the cross-validation error rate of
D”) using classiﬁer m. In other words, the bias Of MEC error rate me
= E(P:’M) — PZ’M is strictly <0.

THEOREM 3. For Observed datasets D”) from An Of varying n and a
ﬁxed number Of classiﬁers M Z 2, it holds that lim ,Hoo Pn’M =
limnaoongoo PZ’M. In other words, b”, M —> 0as n —> 00 for ﬁxed M.

Theorem 1 shows that when we have two sets Of classiﬁers and the
smaller set is a subset Of the larger set, the larger set classiﬁers will yield
a smaller true best classiﬁcation error rate. Theorem 2 shows that the
expected MEC error rate E 15:, M always underestimates the true min-
imal error rate PZ’M and the negative bias always strictly holds. This is
consistent with the result in Figure 3. In Theorem 3, when the number Of
classiﬁers M is ﬁxed, the bias diminishes tO zero as the sample size n
increases to inﬁnity. This is also consistent with the 10-classiﬁer result
in Figure 3b where the bias diminishes to around zero when n is beyond
320. According to Figure 3d, we Observe that although the true best
classiﬁcation error rate does not decrease after the QDA is applied, the
MEC bias estimate continued tO decrease as more classiﬁers are included.
This theoretical result brings clear caution to use MEC without bias
correction. In other words, if a researcher runs n = 20—30 samples Of
pilot study and examines M = 300 classiﬁers via conventional cross-
validation tO choose the best, the minimal error rate from the 300
classiﬁers will likely generate low (or almost zero) error rate, while the
underlying true error rate may stay high. The researcher may be misled tO
expand the study to a larger cohort or a prospective clinical trial, and
eventually ﬁnd it difﬁcult tO validate the model and cannot translate into
a clinically useful diagnostic tool.

2.4 Three existing bias correction methods

In the literature, several methods have been developed to correct the
downward bias Of the MEC error rate, and most have focused on cor-
recting the bias Of parameter estimation via cross-validation for a given
machine learning method. Below, we introduce four bias correction meth-
Ods that we will compare in this article (Bernau et al., 2013; Tibshirani
and Tibshirani, 2009; Varma and Simon, 2006). Bernau et al. (2013) as-
sessed the condition with multiple machine learning methods, while the
others focused on correcting the bias Of parameter tuning via cross-val-
idation (e. g. estimate K for KNN) for a given machine learning method.
In practice, if one considers many machine learning models along with
feature selection and parameter tuning, the number Of classiﬁers (M)
examined can easily reach several hundreds. All three methods considered
here can be generalized tO this situation.

Nested cross validation (nestedCV): Instead Of using a single loop
cross-validation tO ﬁnd the minimal error estimate for a particular clas-
siﬁer, nestedCV uses two CV lOOps (shown in Supplementary Fig. S1).
The dataset is initially divided into training and testing sets. Then
LOOCV is applied on the training set using all the classiﬁers, and the
classiﬁer with the smallest error rate is selected and used tO build the
model based on the training set and then evaluate the error rate on the
testing set in the end. Therefore, the testing set is independent Of the
model selection stage, including the selection Of MEC. Finally, the prO-
cess is repeated until each sample acts as the testing set once; thus it is a
double LOOCV with two CV lOOps. The computation therefore scales
with the square Of the sample size. Instead Of LOOCV, it is possible tO use
5-fold or 10-fold cross-validation to accelerate the computing when the
sample size is large.

Weighted mean correction WMC/WMCS): The method (Bernau
et al., 2013) is proposed to be a smooth analytical alternative tO
nestedCV, which is a weighted mean Of the resampling error rates, Ob-
tained using the different machine learning models/parameter values.
Instead Of using cross-validation, it is based on repeated subsampling.
Then it estimates the unconditional error rate as a weighted sum Of the
error rate Of every classiﬁer on all the subsamples. The weights are esti-
mated with two variants, WMC and WMCS. Compared with nestedCV
in the original paper, the method is more stable and has a much lower
computational demand. We apply the R package CMA tO implement this
method, and the subsampling fraction is chosen tO be 0.8 in this study.

Tibshirani’s procedure (TT): 3 applies the idea Of estimating the bias
and adding back to the minimal error rate estimate tO correct for the bias
in the setting Of K-fold cross-validation. It estimates the true best classi-
ﬁcation error rate as 2Pn,M — ﬁZlePnMJ, where PmM is the biased
MEC error rate when sample size is n with M classiﬁers and PM”C is
the minimal error rate in the kth-fOld among all classiﬁers (Tibshirani and
Tibshirani, 2009). It does not require a signiﬁcant amount Of additional
computation as in nestedCV and scales linearly with the number Of cross-
validation folds. Owing to the calculation Of Pn’M’k, Tibshiranis’ method
is not suitable for LOOCV or when the size Of the left-out test set is tOO
small, and this estimate was shown to over-estimate the bias in some
settings (Bernau et al., 2011; http://epub.ub.uni-muenchen.de/12231/).
The Tibshiranis’ approach targets correcting the Optimization bias Of
the conditional minimal error rate, which heavily depends on the single
Observed dataset, and the results are more variable. On the contrary, the
IPL approach proposed below considers correcting the Optimization Of
the unconditional minimal error rate by using a resampling technique tO
take into account the sampling variation and, therefore, the results are
more reliable and stable.

 

 

 

2.5 The resampling-based IPL method

In this section, we propose a new resampling-based IPL method to cor-
rect the MEC error rate bias and estimate the true Optimal classiﬁcation
error rate PZM. By constructing learning curves for each individual clas-
siﬁer from repeated resampling Of the original dataset at different sub-
sample sizes (Mukherjee et al., 2003), we could estimate the error rate Of
each classiﬁer by ﬁtting a learning curve. Supplement Figure S1 shows the
concept Of IPL for learning curve ﬁtting using 2D simulated data from
Section 2.2. Five simulations in each Of the various sample sizes (n) are
performed, and the LOOCV error rates (P) from QDA are demonstrated.
The trend Of decreasing error rates with increasing sample sizes is clear.
By ﬁtting an IPL function (P = a - n‘“ + b; a, b, or > 0), the learning curve
can be well estimated.

Our proposed method applies the IPL concept using repeated subsam-
pling as follows. Consider sample sizes 15n1<n2<---<nL<n. For a
given machine learning method m, assume that the true error rate
equals PW" and these true error rates follow an IPL function:
PM”, = amnf‘x’" + bm. Normally, we assume am, bm, am > 0, as theoretically
larger sample size contains more information tO produce a lower

 

3155

112 /810's112umo[pJOJXO'sor112uHOJurOrq/ﬁd11q 111011 popcorn/hog

910K ‘09 lsnﬁnV no 22

Y.Ding et al.

 

prediction error rate. TO estimate am, bm and am, we ﬁrst estimate the
underlying PW” from subsampling 121 samples from the whole data and
repeat the procedure for B times. The resulting Observed cross-validated
. ASu
error rate Of each Of the sub-sampled data 1s denoted by P” b
. Asub AB Asub ﬁnk

(I 5 b 5 B), and the averaged error rate 1s Pnl’m =(Zb21Pnl’m’b)/B. The
least squared error method is then used to estimate am, bm and am.

L

A A A c Asub —(X 2

(am, bm, am) =arg mmamﬁmﬂm E (Pnl’m — amnl ’" — bm)
[=1

s.t. am, bm,orm Z 0

The IPL has been found tO ﬁt well in simulation and many real
datasets (Mukherjee AeItPLaL, 2003). It has the advantage tO Obtain an
accurate estimate Of Pn’m =émn‘“m +bm for PM, for any sample size n.
Alglihe bias Of MEC error rate can then be estimated by b,I,PL(B) = PmM —
Pn ,Alﬂhere PmMAIgEnotes the MEC error rate for a ﬁxed dataset
and Pn =minm Pn’m.

The IPL approach has two advantages. First, through subsampling
and ﬁtting by constructing learning curves, the IPL method borrows
information from neighboring estimates at different sample sizes, which
has the potential to reduce the random noise Of the true best classiﬁ-
cation error rate estimate, and the estimator will be more stable and
accurate. The second advantage for IPL is its potential tO extrapolate
the learning curves to estimate the true best classiﬁcation error rate
beyond the current sample size so that it can provide prediction on
how the error rate will further decline if more samples are included in
future studies. For example, from an existing Observed data Of n = 40
samples, IPL can estimate the expected accuracy at n = 100 or n = 250
samples and inform researchers whether it is worthwhile tO extend the
study tO a larger cohort.

3 RESULTS

3.1 Bias correction of the simulated 2D example

We evaluated performance Of different bias correction methods
(TT, nestedCV, WMC/WMCS and IPL) on the 2D toy model.
We calculated both bias-corrected estimates with M = 2 classi-
ﬁers (DLDA and QDA) and all M = 10 classiﬁers and com-
pared them with the true best classiﬁcation error rate at sample
size n = 20, 40, 80, 160, 640, 1280 with B = 100 simulated
datasets. As shown in Figure 4, left, the nestedCV method
generally overestimated the true best classiﬁcation error rate
and the overestimated bias was larger when using 10 classiﬁers
compared with two classiﬁers. Considering each inner cross-
validation model selection stage Of nestedCV, if the true best
classiﬁer (QDA in this example) was not selected frequently (i.e.
nigMEC) 75 m* using the notation in Section 2.1), the ﬁnal min-
imal error rate was estimated with another suboptimal mix Of
classiﬁers. In this respect, on average, without being able tO
select the true best classiﬁer tO estimate the error rate on the
test dataset, nestedCV resulted in an overestimate Of the true
best classiﬁcation error rate. The downward bias problem could
get more severe when more classiﬁers are included. Figure 4
shows that nestedCV generally overcorrects the bias. TT also
overcorrects the bias at n = 40 and 80 when M = 2 and n = 80
and 160 when M = 10. The overcorrection Of TT is consistent
with the results Of the original paper (Tibshirani and Tibshirani,
2009) as well as a previous technical report (Bernau et al.,
2011). WMC/WMCS perform well at M = 2 but ﬂuctuates
when M = 10. IPL performs overall the best. It slightly

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

a, o nestedCV a, o 11'
13‘ N - a “i -
 o x lrue optimal error rate  o x trueoplimalerrorrate
3 m o nestedcvm=2 3 m o 1TM=2
2 ‘T. ' 2 ‘T. '
E O A nestedCVM=1o g 0 A 1TM=10
a x E E a x
t 0 § 5 t o § 5
‘1’ F. - X ‘1’ F. - X § 5
go x§ x xoéxoﬁ go §X 3‘“ XQAXQA
s u, E r.
C C
-— o - -— o -
E d I E o' I l
O O o o o o o o O o O o
N °° 5‘3 3 a N ‘* °° $9 3 8
o IPL _ o wmc _
o a:
is N - a “4 -
 o x lrue optimal error rate  o x trueopllmalerrorrate
3 u, o IPLM=2 3 .n o WMCM=2
a) 1— - a) 1— -
4.. - ... -
g o x A IPLM=10 g 0 xi A qucM=1o
E O § § x E at, 0 § x § 3
.- _ .- _
go §x§§XQ5x°AX°A go x§ xﬁﬁxu‘xeA
E E
'E U) 'E In B
_Q- _Q-
E o I I I E o I I
O O O o o G O O O o O o
N m 5‘3 E 3 N ‘* "° 59 E 8
WMCS .— .—
x lrue optimal error rate
E o wmcs M=2
A WIIIlcs mm
x E

 

 

éx§§x§ xgaxuﬁx°A

 

 

minimum error rate estimate

0.05 0.10 0.15 0.20
I I I I

 

I I I
O O O
N v- In

1280 -

8 s

Fig. 4. Comparison Of minimal error rate estimates from all four bias
correction methods with M = 2 classiﬁers (QDA and DLDA), as well as
with M = 10 classiﬁers

listed dataset n:4ll

 

[1.20
I

0.15
I

I'l‘lll'llt'l‘lJ'l'l'l EHUF rate Eﬁ-[ll'l'lalﬁ
H
I—e—I

am
at
l—ID—l
in:

F64

 

 

0.115
I

 

20 so 40 so at}
sample site in]

FigPLS. Estimated minimum error rate (circle) using IPL extrapolation
(P ) for a ﬁxed dataset with n = 40. The bars reﬂect 95% coverage
interval from 100 simulations. The true minimum error rate (PZ’M) is
shown (cross) as a reference

underestimates the error rate at n = 20 but Obtains accurate
estimates for n = 40—1280.

TO illustrate performance Of IPL extrapolation for estimating
prediction ability in a larger sample size, Figure 5 shows the
estimated best error ratesAfor sample sizes 11 = 20, 30, 40, 60
and 80 using IPL (i.e. P” ) from an Observed dataset Of
n = 40. The true best error rates (PZM) are marked by a ‘cross’
for reference. The IPL extrapolations generally estimated the
truth pretty well. The result shows that increasing sample size
from n = 40 tO n = 80 only slightly improved the prediction ac-
curacy, and it is probably not worthwhile tO collect an additional
40 samples.

3.2 Application on ﬁve GEO datasets

We applied the methods tO ﬁve randomly selected GEO real
datasets (see Supplementary ﬁle and Supplementary Table S1

 

3156

112 /810's112umo[pJOJXO'sor112uHOJurOrq/ﬁd11q 111011 pepeorumoq

910K ‘09 lsnﬁnV no 22

Bias correction for selecting the minimal-error classiﬁer

 

Table 1. MEC error rate and corrected error rates by TT (5-fold and 10—fold cross-validation), nestedCV, WMC, WMCS and IPL

 

 

GDS n MEC TT(5) TT(10) nestedCV WMC WMCS IPL
1627 42 0.000 0.00 0.00 0.02 0.04 0.04 0.003
2190 61 0.323 0.45 0.44 0.39 0.45 0.42 0.40
2362 49 0.000 0.00 0.00 0.06 0.04 0.03 0.002
2415 59 0.320 0.47 0.45 0.49 0.44 0.42 0.38
2520 44 0.022 0.04 0.05 0.05 0.08 0.08 0.03

 

aTT targets on conditional error rate and is not directly comparable with the other methods.

for details on selection criteria). Because TT is not applicable for
LOOCV, we applied it to 5-fold cross-validation and 10-fold
cross-validation. For WMC/WMCS, B = 30 subsampling of
80% was applied. Ten machine learning methods were applied:
KNN (K—nearest neighbor with k = 1, 3 and 5), DLDA, QDA,
NNET, SVM with linear kernel, SVM with non-linear kernel
(radial), RF and SCDA. Feature selection was done by perform-
ing a simple t—test and then selecting 2—30 top features by P-
values in each cross-validation to construct the classiﬁers; there-
fore, a total of 290 classiﬁers were used.

Table 1 shows the best error rate after bias correction by all
methods. The result shows that MEC has a signiﬁcant down-
ward bias compared with the estimates from those correction
methods, especially for datasets GDS2190 and GDS2415. IPL
generally gives a smaller estimate compared with the other meth-
ods, which is consistent with the simulation result that nestedCV
and TT usually overcorrect the bias. WMC/WMCS also seems
to give a large bias correction in these cases.

3.3 TCGA and METABRIC breast cancer data

Owing to lack of the underlying truth in the real data in Section
3.2, the results could not be conclusive. To circumvent this short-
coming, we applied three methods to two large breast cancer
gene expression proﬁles, one from TCGA and the other from
METABRIC. The TCGA breast cancer dataset was downloaded
from TCGA Web site (http://tcga—data.nci.nih.gov/tcga) in
October 2012. Level 3 RNA-Seq data were extracted from the
Illumina HiSeq 2000 platform. We selected the TCGA breast
cancer dataset that contained expression data of n = 406
tumor samples. We deﬁned two classiﬁcation problems: one is
to classify between ER positive (11 = 391) and ER negative
(11 = 89), and the second is to classify between early-stage
(stages I and II, n = 292) and late-stage (stages III and IV,
11 = 114) tumors. The METABRIC gene expression and clinical
data are retrieved from Synapse (https://www.synapse.org/
#1Synapse:syn2133309). We obtained 1897 samples, consisting
of 945 early-stage (stages I and II) and 952 late-stage (stages
III and IW tumors (Curtis et al., 2012). We applied the same
set of 290 classiﬁers in Section 3.2 to both datasets. Because the
dataset contained a large sample size, we mimicked the simula-
tion scheme described in Section 2.1 and randomly split the data
into equal parts of ~40 samples. Under this setting, we pretended
that we obtained B = 10 independent datasets of n = 40—41 from
an unknown underlying distribution An in the TCGA dataset.
Similarly, we have B = 47 and n = 40—41 for the METABRIC

"ECG-HER 51911112.: TCGhljtumnr Eta-99,1 hElABHIEiturn-ur angel

 

 

 

I15 -

.I.
I
I
0.5 - I
I

--—--4

 

 

 

 

 

 

 

 

 

T
T |
n20- | M T
I .14. T ' I
§ I TT 5 '3 |
E | l' E E I
$.15 " g T €113-1-
E II I! DT E : :3
E T T EM lEJ“ " g I :I
1110 TI | II II
1- J. J. M" IIIIJ'
E I‘L'I
| IM J. IJ.
ﬂ.D5- I J- Eq-l I J_
II J- I1I-J_
IIII
n,m_J.J.J.J.
urI—I—Jduii: or—'—4L'>m5 ml"— 4'60:
I.|.I|— $1 LIJl—BEEL'iz |.|.II— E3 ‘5
E E a: E :4 E g a:
ti 33 E a; :3
El II {I
E = I:

Fig. 6. MEC error rate estimates from all bias correction methods along
with the true best optimal classiﬁcation error rate and MEC error rates
on the TCGA and METABRIC datasets. Left: classiﬁcation between
ER positive and ER negative in TCGA. Middle and right: classiﬁcation
between early and late tumor stage in TCGA and METABRIC

dataset. The random partition was also constrained such that
sample sizes in two classes are as balanced as possible. By fol-
lowing the wgrkﬂow of Figure 1, we generated the estimated best
error rate P”,M(B) (denoted as ‘truth’) and MEC error rates
13,1, Mi, (1 5 b 5 B) (denoted as ‘MEC’) in Figure 6. Five-fold
cross-validation was used for TT; leave-one—out was used for
nestedCV and IPL. Thirty subsamplings were used for both
IPL and WMC/WMCS. For WMC/WMCS, subsampling was
at a proportion of 80%. The results showed that nestedCV pro-
duced the most variable error estimates spanning a large range,
which is conﬁrmed also by Bernau et al. (2013). WMC/WMCS
yielded more stable estimates than nestedCV but created a large
overcorrection in TCGA (tumor stage). TT gave relatively stable
estimates but can either overcorrect or undercorrect the bias in
terms of an unconditional error rate, although it theoretically
estimates the conditional error rates (Bernau et al., 2013). IPL
generated the most stable and accurate error rates in all three
cases. The averaged MEC bias is as large as 1.69% in TCGA
ER status classiﬁcation, 6.15% in the TCGA tumor stage clas-
siﬁcation and 9.11% in the METABRIC tumor stage classiﬁca-
tion. Without bias correction, an overly optimistic conclusion
would have been drawn.

 

3157

112 [3.10811211an[plOJXO'SODBIILIOJIIIOIQ/ﬂ(11111 11101; pepeolumoq

910K ‘09 lsnﬁnV no :2

Y.Ding et al.

 

4 CONCLUSION AND DISCUSSION

With the advances of high-throughput genomic and proteomic
techniques, data are generated in an unprecedentedly increasing
pace. Machine learning methods have become a powerful tool in
almost all biomedical research of complex diseases to seek new
diagnostic or treatment selection tools. In most studies, small
sample sizes are encountered (n = 30—60), and researchers are
tempted to test many classiﬁers and select the best to report
(i.e. applying the MEC). In this article, we illustrated the down-
ward bias of MEC error rate when selecting from many machine
learning models in biomedical classiﬁcation problems. In the
application of high-throughput genomic data, this problem is es-
pecially magniﬁed because the addition of feature selection easily
increases the number of classiﬁcation models to several hundreds.
We ﬁrst demonstrated the problem using a 2D toy example where
QDA is known to be the best classiﬁer. The simulation results and
asymptotic theoretical results both illustrated the need of bias
correction for MEC, especially when sample size (n) is limited
and the number of classiﬁers examined (M) is large. We discussed
three existing methods (nestedCV, WMC/WMCS and TT) and
developed a new IPL method from the concept of learning curve
ﬁtting. Application of all four methods to the 2D toy example,
ﬁve selected GEO datasets and two large breast cancer datasets
concluded that nestedCV and TT overestimated the error rate,
whereas WMC/WMCS produces a ﬂuctuating estimate around
the true estimates. IPL provided a stable and accurate solution.
The method has an additional advantage to extrapolate and pre-
dict the optimal error rate for larger sample sizes, a useful feature to
help decide whether it is worthwhile to expand the study to recruit
more samples. We note that nestedCV and WMC/WMCS target
the error rate of the wrapper algorithm, which is slightly different
from the MEC error rate we target in this article. This is consistent
with the result that bias corrections by nestedCV and WMC/
WMCS are less accurate and more ﬂuctuating. Our proposed
IPL method has the advantage of directly targeting the MEC
error rate and completely avoid the wrapper algorithm issue.

Our article provides a careful framework and theoretical in-
vestigation of the problem, and our result shows that severe bias
can be generated for MEC with small sample size (e. g. n = 30—
60) and a large number of classiﬁers (e.g. M = 300). Without
bias correction, one runs the risk of obtaining an overly optimis-
tic error estimate of the classiﬁcation model, excitedly expanding
the investigation to larger independent cohorts and, eventually,
failing to validate and translate into a useful clinical tool. The
IPL method we proposed in this article not only generates more
accurate bias correction but also provides extrapolation esti-
mates to determine whether larger cohorts might warrant im-
proved accuracy. In the era of pursuing translational research
and personalized (or precision) medicine, rigorous evaluation
and interpretation of the machine learning results are essential
to evaluate the clinical potential of a research ﬁnding.

There are a few limitations or considerations for our study.
First, the IPL method has the modeling assumption that learning
curves of each classiﬁer could be ﬁtted well by IPL. Although
there is no theoretical proof that this always holds, our simula-
tion and real data showed good ﬁt to the assumption. Second,
the curve ﬁtting of the IPL method relies on subsampling at
smaller sample sizes. Therefore, if the original sample size is

small, IPL will yield an unstable estimate. Third, the IPL meth-
ods are more costly compared with WMC/WMCS because of its
need to subsample at different sample sizes. However, because all
the classiﬁers are ﬁtted independently, it is easy to parallelize the
computation. Last, we sum up all different feature selections,
machine learning methods and their associated parameter setting
into M classiﬁers in the investigation. Theoretically different
sources of classiﬁers have different correlated performance.
Understanding their correlations may elucidate the contribution
of bias from different sources and develop a better solution. In
addition, we demonstrated the idea that one should include high-
performing machine learning methods in the selection and avoid
adding low-performing methods. In practice, one may determine
high- and low-performance methods from empirical studies (e. g.
comparison of performance in similar studies in large databases,
such as GEO). How to systematically integrate the information
to decide the set of classiﬁers for investigation is still an
open question. All code and source ﬁles are available at http://
tsenglab.biostat.pitt.edu/publicationhtm to reproduce the results
in the article. An R package ‘MLbias’ is also available.

ACKNOWLEDGEMENT

The authors would like to thank suggestions from the reviewers
that have signiﬁcantly improved this article.

Funding: (NIH R21MH094862).

Conﬂict of interest: none declared.

REFERENCES

Allison,D.B. et al. (2006) Microarray data analysis: from disarray to consolidation
and consensus. Nat. Rev. Genet, 7, 55—65.

Bemau,C. et al. (2011) Correcting the optimally selected resampling-based error
rate: a smooth analytical alternative to nested cross-validation. In: Technical
report. Department of Statistics, University of Munich.

Bemau,C. et al. (2013) Correcting the optimal resampling-based error rate by esti-
mating the error rate of wrapper algorithms. Biometrics, 69, 693—702.

Berrar,D. et al. (2006) Avoiding model selection bias in small-sample genomic
datasets. Bioinformatics, 22, 1245—1250.

Boulesteix,A.-L. and Strobl,C. (2009) Optimal classiﬁer selection and negative bias
in error rate estimation: an empirical study on high-dimensional prediction.
BMC Med. Res. Methodol., 9, 85.

Curtis,C. et al. (2012) The genomic and transcriptomic architecture of 2,000 breast
tumours reveals novel subgroups. Nature, 486, 346—352.

Dupuy,A. and Simon,R.M. (2007) Critical review of published microarray studies
for cancer outcome and guidelines on statistical analysis and reporting. J. Natl
Cancer Inst., 99, 147—157.

Efron,B. (2009) Empirical Bayes estimates for large-scale prediction problems.
J. Am. Stat. Assoc., 104, 1015—1028.

Fu,W.J. et al. (2005) Estimating misclassiﬁcation error with small samples via boot-
strap cross-validation. Bioinformatics, 21, 1979—1986.

Mukherjee,S. et al. (2003) Estimating dataset size requirements for classifying DNA
microarray data. J. Comput. Biol., 10, 119—142.

Slawski,M. et al. (2008) CMA: a comprehensive bioconductor package for super-
vised classiﬁcation with high dimensional data. BM C Bioinformatics, 9, 439.
Tibshirani,R.J. and Tibshirani,R. (2009) A bias correction for the minimum error

rate in cross-validation. Ann. Appl. Stat., 3, 822—829.

Varma,S. and Simon,R. (2006) Bias in error estimation when using cross-validation
for model selection. BM C Bioinformatics, 7, 91.

Wood,I.A. et al. (2007) Classiﬁcation based upon gene expression data: bias and
precision of error rates. Bioinformatics, 23, 1363—1370.

Youseﬁ,M.R. et al. (2010) Reporting bias when using real data sets to analyze
classiﬁcation performance. Bioinformatics, 26, 68—76.

 

3158

112 [glO'SIBILInO[plOJXO'SODBIILIOJHIOIQ/[ldllq 11101; pepeolumoq

910K ‘09 lsnﬁnV no :2

