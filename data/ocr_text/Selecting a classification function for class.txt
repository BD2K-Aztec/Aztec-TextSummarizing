Bioinformatics, 32(12), 2016, 1814—1822

doi: 10.1093/bioinformatics/btw034

Advance Access Publication Date: 11 February 2016
Original Paper

 

Gene expression

Selecting a classification function for class
prediction with gene expression data

Victor L. Jong1'2'*, Putri w. Novianti1'3, Kit 6.3. Bees1 and
Marinus J.C. Eijkemans1

1Biostatistics & Research Support, Julius Center for Health Sciences and Primary Care, University Medical Center
Utrecht, 3508 GA, Utrecht, The Netherlands, 2Viroscience Lab, Erasmus Medical Center Rotterdam, Rotterdam, CE
3015, The Netherlands and 3Epidemiology & Biostatistics Department, Vrije University Medical Center Amsterdam,
HV Amsterdam 1081, The Netherlands

*To whom correspondence should be addressed.
Associate Editor: Ziv Bar-Joseph

Received on 12 October 2015; revised on 18 December 2015; accepted on 15 January 2016

Abstract

Motivation: Class predicting with gene expression is widely used to generate diagnostic and/or
prognostic models. The literature reveals that classification functions perform differently across
gene expression datasets. The question, which classification function should be used for a given
dataset remains to be answered. In this study, a predictive model for choosing an optimal function
for class prediction on a given dataset was devised.

Results: To achieve this, gene expression data were simulated for different values of gene—pairs
correlations, sample size, genes’ variances, deferentially expressed genes and fold changes. For
each simulated dataset, ten classifiers were built and evaluated using ten classification functions.
The resulting accuracies from 1152 different simulation scenarios by ten classification functions
were then modeled using a linear mixed effects regression on the studied data characteristics,
yielding a model that predicts the accuracy of the functions on a given data. An application of our
model on eight real—life datasets showed positive correlations (0.33—0.82) between the predicted
and expected accuracies.

Conclusion: The here presented predictive model might serve as a guide to choose an optimal clas—
sification function among the 10 studied functions, for any given gene expression data.

Availability and implementation: The R source code for the analysis and an R—package
‘SPreFuGED’ are available at Bioinformatics online.

Contact: v.l.jong@umcutecht.nl

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

 

Microarray gene expression profiling has become a widely used
tool to identify particular disease subpopulations and to perform
diagnostic and prognostic predictions (Huang et 61]., 2010; van
’t Veer et 61]., 2002). In clinical practice, they are used in diagnostic
and prognostic analyses while in preclinical studies (toxicogenom—
ics), they involve predicting the toxicity of compounds in animal
models with the goal of speeding up the evaluation of toxicity for

new drug candidates (Shi et 61]., 2010). Though class prediction
analysis is a common practice, the question that remains to be ad—
dressed is, given the wide availability of classification functions
nowadays, which classification function do we use for a particular
dataset? Classification functions have been shown to perform dif-
ferently across gene expression datasets (Lee et 61]., 2005).
Moreover, the MAQC—II initiative has pointed out that classifica—
tion function is one of the variables that explains the variability

©The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1814

9mg ‘09 1sn3nv uo sopﬁuv soq 111110;th aIo Amie/xtqu 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(1111] 11101; popeommoq

Classiﬁcation in gene expression

1815

 

between gene expression class prediction performance (Shi et al.,
2010).

While substantial amount of information is known about the
characteristics of classification functions and class prediction build—
ing procedures, little is known about which data characteristics have
impact on the performance of a class prediction model. For instance,
diagonal linear discriminant analysis (DLDA) assumes no covari—
ances and hence no correlations between variables and might fail if
the data is highly correlated. On the other hand, linear discriminant
analysis (LDA) assumes a common covariance matrix for the classes
and thus to some extent, accounts for correlations (Hastie et al.,
2003). In addition, pernalized regressions like ridge, lasso, elastic
net are capable to handle correlated variables. Support Vector
Machine (SVM), though commonly understood as a method of find—
ing the maximum-margin hyperplane, may also be seen as a regular—
ization function estimation problem, corresponding to a hinge loss
function with a quadratic penalty as that of ridge regression (Hastie
et al., 2003; Ye et al., 2011). And it has been shown by (Yang et al.,
2006) that if a group of non—distinct variables are selected as input
variable set, its training time lengthened and the errors become big—
ger. On the other hand, tree—based methods are by nature designed
to capture interactions between variables while neural networks
might capture other complex structures within a given dataset.

Given the above observations, it is obvious that the performance
of these functions depends on the characteristics of the data in ques—
tion. Despite this, the literature on how to choose a classification
function for a given dataset is sparse. A common practice is compar—
ing several classification functions and selecting the one with the
minimum error rate but this has been pointed by Bernau et al.
(2013), Ding et al. (2014), Tibshirani and Tibshirani (2009) and
Varma and Simon (2006 ) to lead to selection bias. As such, some ex—
perimenters adhere to one or a few classification functions irrespect—
ive of the dataset, disease or medical question being addressed.
While others choose a classification function for their datasets by af—
finity or familiarity without taking into account the characteristics
of such data.

A simulation study by Kim and Simon (2011) shows that correl—
ation is one of the data characteristics that affect the performance of
most probabilistic classification functions. In addition, Jong et al.
(2014) showed that correlation structures differ across gene expres—
sion data of different etiological diseases. The study by Novianti
et al. (2015) shows that microarray gene expression data character—
istics like logz fold change of expression values, number of deferen—
tially expressed genes and pairwise correlations between genes are
associated to the accuracy of classification functions. However, this
study was conducted in real—life gene expression datasets, where the
magnitude and/or direction of association might have been con—
founded by unobserved data characteristics.

In this study, we aim to provide a guideline for making a choice
of a classification function for a binary class prediction problem
based on observed magnitudes and directions of the data character—
istics, using accuracy as a measure of evaluation. We investigate the
effect of sample size, proportion of deferentially expressed (DE)
genes, genes’ variances, log fold changes, pairwise correlations be—
tween DE and noisy genes on the accuracy of classification functions
using extensive simulations.

The remainder of this article is organized as follows: method—
ology to simulate data, classification functions considered and the
building and evaluation of class prediction models are presented in
Section 2; Section 3 contains a predictive summary of the results of
class prediction models for different simulated scenarios; Section 4
provides an application of our predictive model from the simulated

results on real—life microarray gene expression datasets and Section
5 presents a discussion.

2 Methods

2.1 Simulated data (scenarios)

To simulated gene expression data, we hypothesized that sample
size, proportion of DE genes, genes’ variances, log fold changes,
pairwise correlations between DE and noisy genes might be associ—
ated to the performance of classification functions. These six vari-
ables were to be systematically varied in our simulations.

From observed correlation structures in real—life gene expression
datasets (Jong et al., 2014), we generalized the structure as shown in
Figure 1, containing three clusters referred to as up—regulated (UR),
down—regulated (DR) and noisy genes. The absolute values of pair—
wise correlation for DE genes (p) were varied as 0.00, 0.25, 0.50
and 0.75 with UR cluster taking oppositely—signed correlation values
for DR cluster. The pairwise correlations both within the noisy clus—
ter and between the noisy and the DE clusters were per gene—pair
randomly drawn from a normal distribution centered at zero with a
standard deviation 0 i.e. N(0, 0) where 020.00, 0.25, 0.50, 0.75.
The scenario p=0=0.00 corresponds to complete independence.
Resulting correlation values lying outside the interval [—1, 1] were
uniformly converted to the intervals [—1, —0.15] and [0.15, 1] for
negative and positive values respectively. The variances of the genes
(0'2 = %) were drawn from an exponential distribution i.e. exp(2)
where 2 = 0.25 , 0.50, 1.00, 1.50. The distributional assumptions
were made based on observation from real—life datasets as experi—
enced by Jong et al. (2014) and Novianti et al. (2015). With the cor—
relation values and the variances, the within covariance matrices
20 and 21 were constructed for the two classes. In addition, the
proportion of DE genes (TE) was also allowed to take up 1, 3 and 5%
of the total number of genes, as values. This resulted to 192 different
complex covariance matrices that were used to simulate the data for
different values of other variables.

Finally, two different values of absolute logz fold change (A) and
three different sample sizes (11) were considered (Table 1). For a fixed
number of genes (p = 1000) and n samples, the samples’ labels (0,1)
were generated from a Bernoulli distribution with a probability 0.5
and the gene expression data of p X n dimension was generated from
a multivariate normal distribution with mean vectors from a uniform
distribution, U(6,10) of length p and the covariance matrices corres—
ponding to the above description, using Cholesky decomposition
Golub and van Loan (1996) as a method to determine the root of the
covariance matrix. The mean logz expression values of DE genes
were incremented or decremented with the corresponding logz fold
change value for samples in class 1. The choice of multivariate normal

' ' 211: within UBGenes
2:11 £12 £13 223:withinDBGenes

El 2 E 233: within Nun—DE Genes
12 22 23 212: Between [JR BDRGenes

El 2’ E 213: Between UR Ba lien—DE Genes
13 23 33. 223: Between BBB lien—DE Genes

 

 

Fig. 1. Assumed correlation structure. Contains 3 clusters of up-regulated
(UR), down-regulated (DR) and noisy (Non-DE) genes

91% ‘09 1sn3nv uo sopﬁuv soq 111110;th aIo Amie/xtqu 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(1111] 11101; popeommoq

1816

V.L.Jong et al.

 

distribution and mean vector corresponds to the practical assumption
that gene expression data are normally distributed in logz scale and
based on observation that the logz expression values often fall in the
interval (0, 16). For each combination of the values of the data char—
acteristics, the dataset was simulated as shown in Figure 2 (Algorithm
1), yielding 1152 different simulation scenarios, each of which was
randomly replicated 1000 times.

2.2 Classification functions

Ten elective choices of classification functions were chosen to repre—
sent the broad list in the literature that falls within the categories:
discriminant analyses or Bayes classifiers, tree—based, regularization

Table 1. Simulated gene expression data characteristics

 

 

Data characteristics Values
Sample size (n) 20, 50, 100
Proportion of DE genes (7:) 1%, 3%, 5%

Logz fold change of DE genes (A) 0.5, 1

0, 0.25, 0.5, 0.75
A: 0.25, 0.50, 1, 1.5
Pairwise correlations of noisy genes (32) ~N(0,0) 0 = 0, 0.25, 0.5, 0.75

Pairwise correlations of DE genes (p)
Gene’ variances (0'2 = 1M) ~EXp(/l)

 

50% of TC were each up- and down-regulated.

Algorithm 1

and shrinkage, nearest neighbors and neural networks methods. For
discriminant analyses, linear discriminant analysis (LDA), quadratic
discriminant analysis (McLachlan, 1992) and shrunken centroid dis—
criminant analysis (SCDA) or prediction analysis of microarrays
(PAM) (Tibshirani et al., 2002) were selected. Random forest (RF)
(Breiman, 2002) was chosen as tree—based method while support
vector machines (SVM) (Scholkopf and Smola, 2002), E1 penalized
logistic regression or Lasso (PLR1) (Tibshirani, 1996), ﬁg penalized
logistic regression or Ridge (PLR2) (Zhu, 2004) as well as £1 and £2
penalized logistic regression or Elastic net (PLR12) (Zou and Trevo,
2005) were considered for regularized and shrinkage methods.
Finally, k-nearest neighbors (KNN) and feed—forward neural net—
work (NNET) (Ripley, 1996) were the lone choices for nearest
neighbors and neural networks respectively.

In machine learning, opinions are that super learners might pro—
vide good class predictions but model complexities of these learners
are usually high. As such, super learners might not be useful in clin-
ical practice where physicians often want simple class prediction
models, that might yield a subset of genes (and possibly coefficients)
for easy interpretation. This is because given a subset of genes, focus
can be geared toward these genes rather than the entire genome for
which experiments are often costly and time consuming. Thus, our
choices of classification functions were driven by the choices often
made and considered useful in clinical practice.

Fur pl‘rlpﬂf'ffrm It in new, 3% and 5% rilpr : 1000 are: diﬁ'wwurirriift' L’.‘l.‘f."l"t’.‘a'.'a'[’r:f{

ii'm' inglﬁw’d change I). in (1.5, i f

I‘II’JI‘ rrhn‘nhrir! pairwise L‘m‘nsfuiiun rgf'rfiﬂrh‘rsuiiuff 1‘ tttjlrinns'xr'd gents p in f}. fifi. 11.35, (150', ii. :75;
Her: pairwise r'rir'rwiarirm refresher: genes (H -—N{ﬂ. 5'): i} in H. Elli. i125. 11.50. (1153‘
Fr»: gene's" variance [4:12) ~Exp[l}; .5. i125. Fiji}, Hm, 115i};

Fru- .eeimpir: wine rt in Iii, iii, NH! ,5

‘1

Her ireer'rtiirm 5' in i,  Milli} {

Fig. 2. Algorithm to simulate data, build and validate class prediction models. For each value of the six variables, the covariance matrix was constructed in step 1,
the learning and test data were simulated at steps 2—4 and class prediction models were built and validated in step 5. Steps 2—5 were then repeated 1000 times

91% ‘09 1sn3nv uo sopﬁuv soq 111110;th aIo Amie/xtqu 112 /§.IO'S[BU.IHO[p.IOJXO'SOTlBIHJOJUTOTQ/ﬁ(1111] 11101; popeommoq

Classiﬁcation in gene expression

1817

 

2.3 Building and evaluating classifiers

To assess the dependency of the chosen classification functions on
characteristics of the simulated gene expression data, we built on
each simulated dataset, class prediction models with all the classifi-
cation functions listed above. The simulated dataset was considered
as a learning set and for classifiers that require pre—selection of genes
because of their limitation to accommodate a number of parameters
greater than the number of samples (i.e. LDA, QDA and NNET),
the genes were ranked by their moderated t statistics (Smyth, 2004)
using the learning set. The learning set was split into a % inner— test
set and % learning set using 5—fold Monte—Carlo—cross—validation
(MCCV) with stratification .

The parameter(s) of the classification functions were subse—
quently tuned using the inner—learning set and evaluated with the
inner-test set. These tuning parameters were: number of genes
(top 12) for LDA and QDA; shrinkage intensity of class centroids
for SCDA; with a fixed forest size of 500 trees, the number of
variables randomly sampled as candidates at each split and min—
imum size of terminal nodes for RF; with a linear kernel, the cost
of regularization for SVM; £1 penalty for Lasso; £2 penalty for
Ridge; £1 & £2 penalties for Elastic net; number of nearest neigh-
bors for KNN and finally, the number of genes (top 12), number
of units in a hidden layer and decay weights for NNET. With the
optimal parameter(s) for each classification function, the class
prediction models were built using the learning set. The resulting
models were evaluated on a test set consisting of 5000 samples
generated from the same model as the learning set (see Fig. 2).
The error rates of the classification functions on this test set were
recorded. The process was repeated 1000 times (sampling both
learning and test sets) for each simulation scenario and the result—
ing error rates over the 1000 replications were used for further
analyses.

2.4 Random effects linear regression

An average of the error rates of each and every classification func—
tion over 1000 replications for each simulated scenario was com—
puted yielding 11 520 data points resulting from the 1152 different
simulation scenarios by 10 classification functions. The error rates
were then transformed to accuracies (1 — [error rate —1— 0001]) and
these accuracies were modeled using a linear random effects regres—
sion model with the classification function as the random effects
clustering variable, by transforming the accuracies to an unbounded
range using the logit function. For the £th standardized study factor,
the random effects model is written as:

108 (—1 it Sign» = Yii 2 Ac + 1901 + (A1 + 1911))(1’1 + 611
where 0 < 7'E(Xij) < 1 is the average accuracy in scenario i for classi-
fication function j, 1% : (190i, 191i), N N(0, D) are respectively the
random intercepts and slopes of the classification functions while
eij N N(0,02) are the independent and identically distributed re—
siduals, also independent from the random effects 1%. D is a 2 x 2
covariance matrix of the random effects. All the aforementioned
study factors were evaluated by univariate and multivariate linear
random effects regression models. Multivariate regression evalu—
ation was done by a backward selection approach. In each step,
two nested models, with and without a particular study factor,
were compared by log—likelihood ratio test at 5% significance.

Each factor £ was also evaluated by its explained—variation
defined as:

MSEnull — M513)
MSEnull

 

Vary =

where MSEnuu and MSE1 are the mean square errors of the null (ran-
dom intercept only) and the 1th standardized study factor models re—
spectively. The explained variation of the selected multivariate
model was also evaluated.

2.5 Software

All statistical analyses were performed in R software version 3.2.0,
and Bioconductor (Gentleman et al., 2009) using the following
packages: mz/morm (Genz and Bretz, 2009) for simulating data,
limma (Ritchie et al., 2015) for ranking genes via linear models,
CMA (Slawski et al., 2008) for predictive classification modeling,
lattice (Sarkar, 2008) for visualization and 114164 (Bates et al., 2015 )
for linear random effects modeling. Additionally, we have developed
an R package called ‘SPreFuGED’: Selecting a Predictive Function
for Gene Expression Data, that allows researchers to determine an
optimal function for a given dataset.

3 Results

Figure 3 shows the average error rates over the 1000 random repli—
cates (y—axis) of the functions (x—axis) for different combinations of
variances, pairwise correlations of noisy (non-DE) genes and DE
genes for a fixed sample size (n = 100), proportion of DE genes
(TE : 5%) and logz fold change (A = 1). From this figure, one sees
that, the error rates for all functions increase with increasing vari-
ances (from top— to bottom—row), pairwise correlation values of
non-DE genes (from left- to right-column) and pairwise correlation
values of DE genes (different colored lines). On the other hand,
other scenarios for different values of sample size, proportion of DE
genes and logz fold change (Supplementary Fig. SlA—C) indicate a
negative association of sample size, proportion of DE gene and logz
fold change to the error rates. The non—constant variability of the
error rates between classification functions across scenarios indi—
cates a scenario—specific optimality for each and every classification
function.

The average accuracies (1 — [average error rates —1— 0001]) of
the simulations were summarized to a data matrix as shown on
Table 2. For each of the predictive variables, a linear random effects
regression model was fitted as described in the method section. The
individually explained variances of the study factors are depicted on
Figure 4. This figure shows that sample size, pairwise correlations of
non-DE genes and the proportion of DE genes are the leading factors
respectively accounting for approximately 17, 14 and 13% of the
null variance. While genes’ variances and fold change respectively
account for 8 and 7% of the null variance, pairwise correlations be—
tween DE genes accounts for simply 1%. As observed graphically,
the univariate models (results not shown) confirmed a positive asso—
ciation of sample size, proportion of DE genes and fold change, and
a negative association of pairwise correlations of non-DE, DE and
the genes’ variances to the accuracies.

For the multivariate linear random effects regression model, we
started with a complex model of random intercepts and slopes and
three ways interactions of the predictive factors. Starting with pair—
wise correlation between DE genes because of its low individually

9mg ‘09 1sn8nv uo sopﬁuv soq ‘1211110111123 10 Amie/xtqu 112 /§.IO'SIBLLIHO[p.IOJXO'SOTlBIIIJOJUTOTCI/ﬁdllq 111011 pop1201umoq

1818

V.L.Jong et al.

 

Wilt “Mlﬂﬂﬂﬂ'ﬂﬂﬂ H'Mf I'IMI

MI 31211 macaw-mm {Ir'L'I' IHIIIE

DE rho
um — U25-

1..." .4. In.- I“ nu. I . .u .1 I|

.u

— 05H] — ll'n'ﬁ

 

L'ilassnnr

Fig. 3. Average misclassification error rates of the ten classification functions for sample size of 100, 1092 fold change of 1 and 5% DE genes. Top-row to bottom

row indicate increase in variance (1M) while from left-column to right-column indicate increase in the pairwise correlation of non-DE genes and the different col-
ored lines from (blue—red) indicate increase in the pairwise correlations of DE genes (Color version of this figure is available at Bioinformatics online.)

Table 2. Structure of the performance data generated from evaluat-
ing the classification functions on the simulated data

 

ID Classifier SampSize propDE Variance deCorr otherCorr log2FC Acc

 

1 SVM 100 5 0.667 0.00 0.00 1 0.999
2 SVM 100 5 0.667 0.25 0.00 1 0.984
3 SVM 100 5 0.667 0.50 0.00 1 0.961
4 SVM 100 5 0.667 0.75 0.00 1 0.950
5 KNN 100 5 0.667 0.00 0.25 1 0.970
11515 LDA 20 1 4 0.50 0.75 0.5 0.498
11516 LDA 20 1 4 0.75 0.75 0.5 0.499
11517 QDA 20 1 4 0.00 0.75 0.5 0.500
11518 QDA 20 1 4 0.25 0.75 0.5 0.499
11519 QDA 20 1 4 0.50 0.75 0.5 0.499
11520 QDA 20 1 4 0.75 0.75 0.5 0.498

 

explained variance, we eliminated variables using the log—likelihood
ratio test. We ended up with the model presented on Table 3 consist—
ing of the fixed effects two ways interactions of all the six predictive
factors, random intercepts and slopes. This model explains approxi-
mately 70% of the null variance as illustrated on Figure 4. The left
panel of Table 3 presents the estimates of fixed effects, the standard
errors and the t statistics while the top—right panel presents the net
effect of a standard deviation (SD) unit increase of a given fac—
tor conditional on common values of other factors. Finally, the

bottom—right panel presents the performances of the classification
functions at different values of the predictive factors.

From the top—right panel of this table, one notices that a 1 SD
unit increase in sample size, corresponding to n = 89.67 will lead to
an increase in the Log odds (accuracy), with the highest increase
observed when other variables are at their highest values. A similar
effect is observed for a 1 SD unit increase in the proportion of DE
genes. Though a 1 SD unit increase in fold change leads to an crease
in the Log odds, as sample size and proportion of DE genes, its effect
is highest when the other variables are at their lowest values. While
on the average a 1 SD unit increase in the genes’ variances, pairwise
correlations of non—DE and DE genes will lead to a decrease in the
accuracy, these effects become very severe when other variables are
at their highest values. For very low values of other variables, a 1 SD
unit increase of pairwise correlations between DE genes could even
lead to an increase (a positive effect) on the accuracy as was previ—
ously observed and illustrated diagrammatically by Novianti et al.
(2015 ). A similar effect is observed for a 1 SD unit increase in the
genes’ variances at very low values of other variables. These varying
effects, indicate the complex interactions between the study factors
and hence illustrate why classification functions will perform differ-
ently on different datasets.

Lastly, the bottom—right panel of the table shows that all classifi—
cation functions will perform reasonably well if the predictive fac—
tors are at their average values (0 SD) with PLR1, PLR12, LDA and
QDA having outstanding performances. For extremely small values

9mg ‘Og 1sn8nv uo sopﬁuv s01 ‘1211110111123 JO [{1ts19Atun 112 /3.IO'S[BIIJHO[pJOJXO'SOTlBIIIJOJUTOTQ/ﬂClllq 111011 pep1201umoq

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Classiﬁcation in gene expression 1819
Table 3. Fixed effects estimates (left panel) and their conditional on other factors net effects (right panel)
Fixed effects Conditional net effects of study factors
Parameter Estimate Std. tvalue 1 SD unit increase
Error
Intercept 1.026 0.220 4.663 Other N N ~2 N N N
. 11 7t 0' ,0 6 A
variables
StdSampSize H) 0.573 0.123 4.660 25D Logodds 0.650 0.715 —0.983 —0.502 —1.078 0.259
SthropDE(7Nt) 0.508 0.108 4.695 1 SD 0.612 0.473 —0.691 —0.330 —0.819 0.318
Sthariance(&2) —0.400 0.081 —4.915 OSD 0.573 0.508 —0.400 —0.158 —0.560 0.378
SthECorr(,3) —0.158 0.027 —5.878 —1 SD 0.534 0.543 —0.108 0.015 —0.300 0.438
StdOtherCorr(6) —0.560 0.085 —6.557 —2 SD 0.496 0.578 0.184 0.187 —0.041 0.498
StdLog2FC (A) 0.378 0.068 5.565
StdSampSize’FStdLogZFC 0.109 0.009 12.551
SthropDE’FStdLogZFC 0.145 0.009 16.718
Sthariance’FStdLogZFC —0.109 0.009 —12.588
SthECorr’FStdLogZFC —0.053 0.009 —6.130 15D correspondsto 11:89.67 7:24.63 0223.22 p=0.65 620.65 A=1.00
StdSampSize’FStdOtherCorr —0.091 0.009 —10.542
SthropDE’FStdOtherCorr —0.138 0.009 —15.927
Sthariance’FStdOtherCorr 0.102 0.009 11.819
SthECorr’FStdOtherCorr 0.019 0.009 2.182 Classiﬁcation functions
StdSampSize’FSthECorr —0.067 0.009 —7.706 Other KNN LDA NNET PAM PLR1 PLR12 PLRZ QDA RF SVM
variables
SthropDE’iSthECorr —0.119 0.009 —13.745 25D Log odds —1.797 1.237 —0.511 —1.540 2.117 2.037 —0.977 0.957 0.265 —1.018
Sthariance’FSthECorr 0.048 0.009 5.519 1 SD —0.445 1.835 0.550 —0.232 2.451 2.396 0.178 1.632 1.085 0.150
StdSampSize’FSthariance —0.161 0.009 —18.571 OSD 0.089 1.617 0.794 0.258 1.968 1.938 0.517 1.491 1.088 0.500
SthropDE’FSthariance —0.172 0.009 —19.815 —1 SD —0.193 0.580 0.221 —0.070 0.667 0.662 0.038 0.532 0.273 0.033
StdSampSize’FSthropDE 0.249 0.009 28.729 —2 SD —1.294 —1.273 —1.170 —1.214 —1.451 —1.430 —1.258 —1.245 —1.359 —1.251
Selected Model
SampSize
otherCorr
PropDE
Variance
logZFC
deCorr
l
0.00 10.00 20.00 30.00 40.00 50.00 60.00 70.00 80.00

Fig. 4. Proportion of the null variance explained by each and every studied factor. The selected model refers to the predictive model presented on Table 3

(—2 SD) of the studied factors, all functions fail. An indication that
the positively associated factors (sample size, proportion of DE
genes and fold change) have a high combined net effect than the
negatively associated factors (pairwise correlations between non—DE
and DE genes and genes’ variances). Additionally, for extremely
large values (2 SD) of all predictive variables, classification functions
like PLR1 and PLR12 clearly demonstrate their abilities to handle
correlated variables and higher variances. That notwithstanding, the
optimality of a function is scenario specific as illustrated on
Supplementary Table 81 where both PLR1 and PLR12 fail when

other variables are fixed at —2SD and otherCorr or DECorr is varied
from —1SD to 2SD. It must be noted however that the combination
of all other variables simultaneously being at —2, or at +2, is highly
unlikely.

4 Application

To evaluate the predictive ability of the here presented random ef—
fects regression model on real—life data, eight Affymetrix gene

9mg ‘Og 1sn8nv uo sepﬁuv s01 ‘1211110111123 JO [{1ts19Atun 112 /3.IO'S[BIIJHO[pJOJXO'SOTlBIIIJOJUTOTQ/ﬂClllq 111011 pep1201umoq

1820

V.L.Jong et al.

 

Table 4. Characteristics of the eight datasets used for evaluating the predictive model

 

 

No. Study ID + Affymetrix Platform Probesets SampSize propDE Variance deCorr otherCorr log2FC
1 CF"‘ E—GEOD—10406 HG U133 Plus 2.0 54 675 15 (09, 06) 0.267 0.143 1.205 0.414 0.408
2 CS E—MEXP—2236 HG U133 Plus 2.0 5422 20 (10, 10) 1.881 0.654 1.200 0.368 0.418
3 Dia2 E—CBIL—3O HG U133A 1749 26 (18, 08) 2.859 0.444 0.604 0.611 0.434
4 HIV2 E—GEOD—14278 HG U133 Plus 2.0 11 286 18 (09, 09) 1.435 0.523 1.157 0.616 0.393
5 UC2"‘ E—GEOD—21231 HG 1.0 ST 32 321 40 (20, 20) 0.402 0.139 0.631 0.257 0.268
6 UC3 E—GEOD—36807 HG U133 Plus 2.0 6541 28 (15, 13) 6.849 0.735 1.381 0.715 0.503
7 UC5 E—MTAB—331 HG 1.0 ST/HG 1.1 ST 1402 59 (30, 29) 1.427 0.461 1.390 0.951 0.286
8 UC7"‘ E—GEOD—6731 HG U95AV2 12 625 28 (11, 19) 0.135 0.116 1.280 0.474 0.311

 

"‘No ﬁltering was performed; —l—: ArrayEXpress accessing ID.

The sixth to the eleventh columns correspond to the variable under study. (.,.) represent the sample sizes for each class.

expression datasets of the 25 non—cancerous datasets described in
one of our previous studies (Novianti et al., 2015 ) were used. These
datasets were selected to include a variety of Array platforms, both
class—balance and class—imbalance, number of DE probesets, as well
as various sample sizes. Three of these datasets were preprocessed
without filtering while the other five were preprocessed and filtered
as described by Novianti et al. (2015 ). We quantified the data char—
acteristics studied and presented on Table 4 as follows: (i) sampSize,
by counting the samples in the study, (ii) propDE, by ranking the
probesets using limma (Ritchie et al., 2015 ) and computing the pro—
portion of DE probesets based on a logz fold change cutoff of 1 if
the number of DE is 210 or 0.5 otherwise, (iii) variance, was deter—
mined as the mean of the variances of all the probesets, (iv) log2FC,
computed as the mean logz fold changes of the DE probesets, (v)
deCorr as the mean of the elements of the upper— (lower—) triangular
of the correlation matrix of the DE probesets and (vi) otherCorr,
was computed as the standard deviation (SD) of the elements of the
upper— (lower—) triangular of the correlation matrix of non-DE pro-
besets. This matrix was computed from all non—DE probesets if they
were less than 20 000 or a sample of 20 000 from these non-DE pro-
besets otherwise.

These data characteristics were standardized using the mean and
SD of the respective variables from the simulated data. And our
model was used to predict the accuracies for all classification func-
tions in each dataset (Supplementary Fig. S2). We then built and
evaluated classifiers using the classification functions by splitting the
data into % learning set and % test set with stratification and a 3—fold
inner cross—validation on the learning set for parameters optimiza—
tion. This step was repeated a hundred times, each time predicting
the accuracies of classification functions on the learning set using
the random effects model and also recording the expected (observed)
accuracies on the test set. These predicted and observed accuracies
over the 100 repetitions are respectively presented on
Supplementary Figure 83A and B. To compare the predicted to
observed accuracies, and considering that we are interested in the
ordering of performance (i.e. determining an optimal function for a
given data), we used the ranked base Spearman correlation between
the average predicted accuracies and the average observed
accuracies.

The results of this comparison for each dataset are presented on
Figure 5. The positive correlation values on this figure indicate
agreement between our predicted and observed accuracies. Though
these correlations are not very high in some datasets, our model
more or less determined an optimal classification function for all the
datasets except for UC7 where Ridge regression and SVM emerged
first instead of fourth as predicted (i.e. 87.5% sensitivity).
Nevertheless, the model was able to rule out on which

classification(s) will perform poor on a given dataset, with approxi—
mately 100% certainty. As expected, the performance of the func—
tions deteriorate on CF (small sample size and low proportion of DE
probesets), Dia2 (high class—imbalance and small fold changes), UC2
(low proportion of DE probesets and small fold changes), UC3
(large variances and high correlations) and UC7 (low proportion of
DE probesets). From Figure 5 and Supplementary Figure 83A and B,
one sees that except on the UC3 data, our model’s accuracies are
less than or equal to observed accuracies. The model performs well
on dataset with large sample sizes and balanced classes (UC2, UC3
and UC5). It attained its lowest performance on Dia2 where there is
high class—imbalance and hence few samples of the small class in the
learning set and on HIV2 and CF datasets with small sample sizes.

5 Discussion

We hypothesized that the performance of classification functions on
gene expression data depends on sample size, proportion of DE
genes, genes’ variances, logz fold changes between DE genes and
magnitude of the pairwise correlation within DE genes and non-DE
genes, and showed their association to the accuracies of ten often
used and clinically relevant classification functions using simula-
tions. Additionally, we built a predictive model to determine an op—
timal classification function among the studied functions using the
simulation results. An application of the predictive model on eight
non—cancerous real—life gene expression datasets predicted optimal
function(s) for seven out of the eight and was able to rule out func—
tion(s) that will perform poor on almost all the datasets. This model
may serve as a guide for choosing a classification function for a
given gene expression data.

Classification functions have been shown to perform differently
across gene expression datasets (Lee et al., 2005 ) and data character—
istics have been shown to differ across datasets and are associated to
the performance of classification functions (Jong et al., 2014;
Novianti et al., 2015). While sufficient knowledge is available on
the properties of most classification functions and procedures to
build class prediction models using gene expression data have been
outlined by Wessels et al. (2005), little is known about data charac—
teristics that accounts for the variability in the performance of classi-
fication functions and how to use these characteristics to choose an
optimal classification function for a specific dataset. As such, most
researchers adhere to specific classification function(s) or randomly
choose a classification for their class prediction models irrespective
of the disease or data under study. A common practice is to evaluate
several classification functions and select the one with smallest

9mg ‘09 1sn8nv uo sopﬁuv s01 ‘1211110111123 10 Amie/xtqu 112 /§.IO'SIBU.IHO[p.IOJXO'SOTlBIIIJOJUTOTCI/ﬁdllq 111011 pop1201umoq

 

 

 

 

 

Classiﬁcation in gene expression 1821
Classification functions
El KNN SCI LDA EB NNET PAM W PLR1 PLR12 FLFIE ll [IDA i FIF I SVM
0.4 0.5 0.3 1.0 0.4 0.5 0.3 1.1}
I I I I I I I I I I I I I I I I
U62 U03 UC5 UG?
- -'LO
_ I E i — 
E t
'12?!
g i E ¢ ‘- h *-
m m
T u 1. It — as
it
:5...I _ _ Ell-4
U
E
:-
U
R
(IF C3 DiaE HIV2
'3 _
a
(I) 1 _ _
E .D 
D #-
I _
I. 1‘:- _
 — I E D i—
1.
IE 13; E!
E
E Q . * LII“
ns— t, i. _
E
[1.4 - -
I I I I I I I I I I I I I I I I

 

 

 

 

 

 

0.4 DE [1.3 1.0

114 DE 113 1.0

Predicted accuracy

Fig. 5. Predicted versus Expected (Observed) accuracies. Cor represents Spearman correlations between the predicted and observed accuracies (Color version of

this figure is available at Bioinformatics online.)

misclassification error but this leads to selection bias (Ding et al.,
2014)

In this study, we outlined data characteristics together with clin—
ically relevant and often used classification functions and investi—
gated their effects on classification performance using simulation
studies. Based on these simulation studies, we provided a guide for
choosing an optimal classification function for a specific dataset
using the data’s characteristics and the studied classification func—
tions through a linear random effects predictive model. As a meta—
model one would expect it to explain close to 100% of the variance
in the simulated data but our predictive model accounts for approxi—
mately 70% of the variability in the simulated data. The remaining
30% unexplained variance may be associated to sampling variability
stemming from the several (192) random covariance matrices used
to generate both learning and test sets as well as the different learn—
ing and test sets generated at each iteration.

Although we used different classification functions and evaluated
these functions using accuracy, our simulation results confirm the
findings of Kim and Simon (2011) that classifiers tend to have poor
performance on highly correlated data. Our results also agree with
those of Novianti et al. (2015 ) that correlations, the absolute logz
fold changes and the number of DE probesets are associated to the
accuracy of a class prediction model. In addition, these results spe—
cify clearly the directions of the association and point out the effects
of other data characteristics like sample size, genes’ variances that
were not previously identified.

Most importantly, we have provided a predictive model that can
serve as a guide to choose a classification function for a given dataset
and its application on eight real—life datasets (both filtered and unfil—
tered) indicated a good predictive ability of the model. Although our
model was reasonably good in its prediction on real—life data, we
want to point out that it might have failed in some datasets because of
the following reasons: (i) most of the eight non—cancerous datasets
had small sample sizes and splitting these datasets to learning and test
sets yielded even smaller sample sizes of the learning sets and hence
might have led to poor estimates of the characteristics under study
and (ii) the observed accuracies might not be the true accuracies be—
cause of the few Bootstrap samples. It could have been better if we
had the means to perform several Bootstraps but due to the small
sample sizes, the number of independent Bootstrap samples is limited.
The fact that our predictions were most often slightly lower than the
observed accuracies for almost all classification functions might indi-
cate the general trend that the performance of a model usually de—
creases on an independent dataset. Hence, our model’s predictions
might reﬂect expected accuracies on independent datasets.

In the simulated data, we assumed exponential and normal distri-
butions for the variances and pair—wise correlation of non-DE genes
respectively. These distributional assumptions might be violated in
some datasets. As such, it will be worth trying different distributions.
Also, we used accuracy as a measure of evaluation by minimizing the
loss function but in clinical applications, probabilities are more in-
formative than simple yes or no predictions because they quantify the

91% ‘09 1sn8nv uo sopﬁuv s01 ‘1211110111123 10 Amie/xtqu 112 /§.IO'SIBU.IHO[p.IOJXO'SOIlBIIIJOJUIOICI/ﬁdllq 111011 pop1201umoq

1822

V.L.Jong et al.

 

uncertainty of a prediction (Pepe, 2005). As such, it is worth evaluat—
ing these data characteristics on probabilistic classification functions
where by the log—likelihood function is optimized, this might possibly
provide a predictive model that will be most useful in clinical applica-
tions. Despite these limitations, our model was found to work well
with data containing reasonably large and balanced sample sizes
(n 2 30). As such, our results apply to balanced class data. For data
with class—imbalance some classification functions will have deterio—
rating performance, for which several solutions are proposed.
However, this topic is outside the focus of the current study. In sum—
mary, our results serve as a guide to use data characteristics to choose
an optimal classification function for a given dataset.

Acknowledgements

The authors would like to thank the VIRGO consortium and its sponsors, the
Netherlands Genomics Initiative and the Dutch Government for the ﬁnancial
support. Next to this, we thank Martin Marinus and the HPC-team at UMC
Utrecht for the high performing computing facilities.

Funding
This work has been supported by the VIRGO consortium, which is funded by

the Netherlands Genomics Initiative and by the Dutch Government
(FESO908). The funding agencies in no way inﬂuenced the outcome or conclu-

sions of the study.

Conﬂict of Interest: none declared.

References

Bates,D. et al. (2015 ). lme4: Linear mixed—effects models using Eigen and S4.
R package version 1. 1-9

Bernau,C. et al. (2013) Correcting the optimal resampling-based error rate by
estimating the error rate of wrapper algorithms. Biometrics, 69, 693—702.

Breiman,L. (2002) Random forest. Mac/7. Learn., 45, 5—32.

Ding,Y. et al. (2014) Bias correction for selecting the minimal-error classiﬁer
from many machine learning models. Bioinformatics, 30, 3152—3158.

Gentleman,R. et al. (2009) Bioconductor: open software development for
computational biology and bioinformatics. Genome B iol., 5, R80.

Genz,A. and Bretz,F. (2009) Computation of Multivariate Normal and T
Probabilities. Springer-Verlag, Heidelberg, Germany.

Golub,G. and Van Loan,C. (1996). Matrix Computations. 3rd edn. Johns
Hopkins, Baltimore, USA.

Hastie,T. et al. (2003). The Elements of Statistical Learning: Data Mining,
Inference and Prediction. Springer, NY, USA.

Huang,]. et al. (2010) Genomic indicators in the blood predict drug-induced
liver injury. Pkarmacogenomics ]., 10, 267—277.

Jong,V. et al. (2014) Exploring homogeneity of correlation structures within
and between gene expression datasets of different etiological disease catego-
ries. Stat. Appl. Genet. Mol. Biol., 13, 717—732.

Kim,K. and Simon,R. (2011) Probabilistic classiﬁers with high-dimensional
data. Biostatistics, 12, 399—412.

Lee,]. et al. (2005) An extensive comparison of recent classiﬁcation tools
applied to microarray data. Comput. Statist. Data Anal., 48, 869—8 85.

McLachlan,G. (1992) Discriminant Analysis and Statistical Pattern
Recognition. Wiley, NY, USA.

Novianti,P. et al. (2015 ) Factors affecting the accuracy of a class prediction
model in gene expression data. BMC Bioinf., 16, 199.

Pepe,M. (2005) Evaluating technologies for classiﬁcation and prediction in
medicine. Stat. Med., 24, 3687—3696.

Ripley,B. (1996) Pattern Recognition and Neural Networks. Cambridge
University Press, MA, USA.

Ritchie,M. et al. (2015) Limma powers differential expression analyses for
RNA-sequencing and microarray studies. Nucleic Acids Res., 43, e47.

Sarkar,D. (2008) Lattice: Multivariate Data Visualization wit/7 R. Springer,
NY, USA

Scholkopf,B. and Smola,A. (2002). Learning wit/7 Kernels. MIT Press, MA,
USA.

Shi,L. et al. (2010) The MicroArray Quality Control (MAQC)-II study of com-
mon practices for the development and validation of microarray-based pre-
dictive models. Nat. Biotechnol, 28, 827—838.

Slawski,M. et al. (2008) CMA-a comprehensive Bioconductor package for
supervised classiﬁcation with high dimensional data. BMC B ioinf., 9, 439.
Smyth,G. (2004) Linear models and empirical Bayes methods for assessing dif-
ferential expression in microarray experiments. Stat. Appl. Genet. Mol.

Biol., 3, Article3

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. ]. R.
Statist. Soc. B, 58, 267—28 8.

Tibshirani,R. and Tibshirani,R. (2009) A bias correction for the minimum
error rate in cross-validation. Ann. Appl. Stat., 3, 822—829.

Tibshirani,R. et al. (2002) Diagnosis of multiple cancer types by shrunken cen-
troids of gene expression. Proc. Nat. Acad. Sci. USA, 99, 65 67—65 72.

van ’t Veer,L. et al. (2002) Gene expression proﬁling predicts clinical outcome
of breast cancer. Nature, 415, 530—536.

Varma,S. and Simon,R. (2006) Bias in error estimation when using cross-val-
idation for model selection. BMC Bioinf., 7, 91.

Wessels,L. et al. (2005) A protocol for building and evaluating predictors of
disease state based on microarray data. Bioinformatics, 21, 375 5—3 762.

Yang,K. et al. (2006) Correlation coefﬁcient method for support vector ma-
chine input samples. Mack. Learn. Cybern, 285 7—2861.

Ye,G. et al. (2011) Efﬁcient variable selection in support vector machines via
the alternating direction method of multipliers. Artif. Intell. Statist., 15,
832—840.

Zhu,]. (2004) Classiﬁcation of gene expression microarrays by penalized lin-
ear regression. Biostatistics, 5, 427—443.

Zou,H. and Trevo,H. (2005) Regularization and variable selection via the
elastic net. ]. R. Statist. Soc. B, 67, 301—320.

91% ‘09 1sn8nv uo sopﬁuv s01 ‘1211110111123 10 Amie/xtqu 112 /§.IO'SIBU.IHO[p.IOJXO'SOIlBIIIJOJUIOICI/ﬁdllq 111011 pop1201umoq

