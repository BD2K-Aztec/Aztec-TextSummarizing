ORIGINAL PAPER

Vol. 26 no. 21 2010, pages 2705—2712
doi: 1 0. 1 093/bioinformatics/btq498

 

Gene expression

Advance Access publication August 29, 2010

Model-based clustering of microarray expression data via latent

Gaussian mixture models

Paul D. McNicholas” and Thomas Brendan Murphy2

1Department of Mathematics & Statistics, University of Guelph, Guelph, ON, Canada, N1G2W1 and 2School of
Mathematical Sciences, University College Dublin, Belfield, Dublin 4, Ireland

Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: In recent years, work has been carried out on clustering
gene expression microarray data. Some approaches are developed
from an algorithmic viewpoint whereas others are developed via
the application of mixture models. In this article, a family of
eight mixture models which utilizes the factor analysis covariance
structure is extended to 12 models and applied to gene expression
microarray data. This modelling approach builds on previous work by
introducing a modified factor analysis covariance structure, leading
to a family of 12 mixture models, including parsimonious models.
This family of models allows for the modelling of the correlation
between gene expression levels even when the number of samples
is small. Parameter estimation is carried out using a variant of the
expectation—maximization algorithm and model selection is achieved
using the Bayesian information criterion. This expanded family of
Gaussian mixture models, known as the expanded parsimonious
Gaussian mixture model (EPGMM) family, is then applied to two
well-known gene expression data sets.

Results: The performance of the EPGMM family of models is
quantified using the adjusted Rand index. This family of models
gives very good performance, relative to existing popular clustering
techniques, when applied to real gene expression microarray data.
Availability: The reduced, preprocessed data that were analysed are
available at www.paulmcnicholas.info

Contact: pmcnicho@uoguelph.ca

Received on October 31, 2009; revised on August 11 , 2010; accepted
on August 26, 2010

1 INTRODUCTION
1.1 Model-based clustering

Cluster analysis methods are used to ﬁnd subgroups in a
population. Clustering is of particular interest when analysing gene
expression data because it can be used to ﬁnd subgroups that
are well distinguished by their expression proﬁles. A number of
clustering techniques are commonly used including agglomerative
hierarchical, divisive hierarchical, k-means, k-medoids and model-
based clustering. Model-based clustering is a technique for
estimating group membership based on parametric ﬁnite mixture
models. The density of a parametric ﬁnite mixture model can be

 

*To whom correspondence should be addressed.

written

G
r(xm,...,er,01,...,0G)=anr(xlog),
g=1

where the[0,l], such that 2G21ng21, is the probability of
membership of subpopulation g, and r(x | 0g) is the density of a
multivariate random variable X with parameters 0g. Overviews of
ﬁnite mixture models are given by McLachlan and Feel (2000a) and
Frﬁhwirth—Schnatter (2006).

In the model-based clustering literature, the ﬁnite Gaussian
mixture model is most commonly used (examples include Fraley and
Raftery, 2002; McLachlan et al., 2002; McNicholas and Murphy,
2008, 2010). The density of a ﬁnite Gaussian mixture model is
given by,

G
r(xw)=an¢(xmg,zg), (1)

g=1
where ¢(x| ug,Zg) is the density of a multivariate Gaussian
random variable X with mean [Lg and covariance matrix 23g,
and0=(7t1,...,7t(;,/L1,...,/L(;,21,...,Z(;).NotethattheGaussian
mixture model has been used within the bioinformatics literature
for purposes other than clustering: for example, McLachlan et al.
(2006) apply a two-component mixture model to detect differential

gene expression.

Gaussian mixture models offer an advantage over other
commonly used approaches because the covariance structure can
potentially account for correlation between expression levels within
an expression proﬁle. Consequently, these models are more ﬂexible
than k-means or hierarchical clustering which commonly use
Euclidean distance. However, due to the high-dimensional nature
of expression data, additional structure needs to be assumed
for the covariance matrices, so that the model can be ﬁtted in
high—dimensional settings. The MCLUST (Fraley and Raftery,
2002) approach to model-based clustering, which utilizes eigen—
decomposed covariance matrices, can only be applied to clustering
expression proﬁles if a diagonal covariance structure is assumed;
Yeung et al. (2001) were able to cluster genes using MCLUST but
not expression proﬁles. By assuming a highly parsimonious but non-
diagonal covariance structure, it is possible to cluster expression
proﬁles while allowing for correlation between gene expressions.

In general, a structure like that given in Equation (1) can
be used to model such data. Then the parameters, and hence
group memberships, can be estimated using some variant of the
expectation—maximization (EM) algorithm (Dempster et al., 1977).

 

© The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org 2705

112 [3.10'811211an[p.IOJXO'SODBIIIJOJIIlOlCI/[I(11111 IIIOJJ popcolumoq

910E ‘ISlsnﬁnV uo ::

P.D.McNichoIas and IB.Murphy

 

Table 1. The covariance structure of each parsimonious Gaussian mixture
model—note that the UCU, UUC and UUU models previously existed under
different names, as described in Section 1.2

 

 

A g = A \11g 2 \II Isotropic Covariance structure
C C C 2g=AA’+¢Ip

C C U 2g=AA’+\II

C U C 2g=AA’+¢ng

C U U 2g=AA’+\IIg

U C C ZnggAig+¢Ip
U C U ZnggAé+W

U U C ZnggAé+¢glp
U U U ZnggAé+Wg

 

C, constrained; U, unconstrained.

The covariance matrices 22g can be decomposed to allow the
construction of more parsimonious models.

1.2 Parsimonious Gaussian mixture models

The factor analysis model (Spearman, 1904) assumes that a
p-dimensional random vector X i can be modelled using a
q-dimensional vector of latent factors U i, where q<< p. The model
can be written X i = [L-I- AU ,- +6i, where A is a p X q matrix of factor
weights, the latent variables U ,- ~N (0,I q) and 6,- ~N(0, \II), where
‘1! is a p X p diagonal matrix. Therefore, the marginal distribution of
X,- is NUL, AA’+\II).

To illustrate the implications of the covariance matrix attached to
this marginal distribution, 22 =AA’ +‘II, suppose that X17 and Xik
are expression levels from a sample X i. Then, Cov(Xij,Xik) =ojk =

2:121 Ajskks for j 74k, and Var(Xij) =ojj : 2:121 his + wqq. Hence,
the matrix A models the covariance between expression levels, and
a combination of the A and ‘1! matrices models the variance of
expression levels. The factor analysis model allows for the modelling
of a high—dimensional non-diagonal covariance matrix with a low
number of parameters.

Ghahramani and Hinton (1997) proposed a mixture of factor
analysers model given by the ﬁnite Gaussian mixture model in
Equation (1), with 22g =AgA(’g,+\II. McLachlan and Feel (2000b)
used the more general covariance structure 2g =AgAé+\Ilg.
Tipping and Bishop (1999) proposed the mixtures of probabilistic
principal component analysers model, for which the component
covariance matrix is 22g 2 A gAg, + wglp.

McNicholas and Murphy (2008) further generalized the factor
analysis covariance structure by including the possibility of
imposing the constraints: Ag=A, \Ilg=\II and \Ilgzipglp. The
result of imposing, or not, each of these three constraints is the
family of eight parsimonious Gaussian mixture models (PGMMs)
that are described in Table 1. Each member of this family of
models has a number of covariance parameters that is linear in
data dimensionality. This is one of the reasons that this family of
models is particularly well suited to the analysis of high—dimensional
data. The constraints allow for assuming common structure in the
component covariance matrix 23g, if appropriate. By assuming
common covariance structure, a more parsimonious model can be
used and this can be estimated in a more stable manner.

The PGMM family has another signiﬁcant advantage that is
particularly important in applications involving high—dimensional

data. When running the alternating expectation-conditional
maximization (AECM) algorithm (Meng and van Dyk, 1997) for
these models, it is advantageous to make use of the Woodbury
identity (Woodbury, 1950) to avoid inverting any non—diagonal p X p
matrices. Given an n X 11 matrix A, an n X k matrix H, a k X k matrix
C and a k X 11 matrix V, the Woodbury identity states that

(A+HCV)_1=A_1—A_1H(C_1+VA_1H)_1VA_1. (2)
Setting H =A, V=A’, A 2‘1! and C =Iq in Equation (2) gives
(\II+AA’)_1=\II_1—\II_1A(Iq+A’\II_1A)_1A’\II_1. (3)

Now, the left-hand side of Equation (3) involves inversion of a
p X p matrix, but the right-hand side leaves only diagonal and q X q
matrices to be inverted. This is a major computational advantage
when modelling expression data, since q << p. A related identity for
the determinant of the covariance matrix is given by

|AA’+\II|=|\II|/|Iq—A’(AA’+\II)_1A|. (4)

Equations (3 and 4) are used by McLachlan and Feel (2000b) for the
mixtures of factor analysers model and by McNicholas and Murphy
(2008) and McNicholas et al. (2010) for the PGMM family.

2 METHODOLOGY

2.1 Modiﬁed factor analysis covariance structure

The factor analysis covariance structure (cf. McLachlan and Feel,
2000b) can be further parameterized by writing \Ilg 2005, A g, where
wgeR+ and Ag=diag{81,82,...,8p} such that |Ag|=1, for g:
1,2, ...,G. The resulting covariance structure ZnggAé+ngg
shall be known as the modiﬁed factor analysis covariance structure.
Now, this covariance structure can be used within the model-based
clustering framework, opening up the possibility of models that
are more parsimonious than their PGMM counterparts. Speciﬁcally,
constraints can be imposed on the parameters A g, cog and Ag
leading to the 12 Gaussian mixture models illustrated in Table 2.
The family of models in Table 2 will be referred to as the expanded
PGMM (EPGMM) family hereafter. Table 2 contains a total of
four new, parsimonious, models when compared with Table 1.
Notably, all 12 members of the EPGMM family have a number
of covariance parameters that is linear in the dimensionality of the
data. Furthermore, the identities given in Equations (3 and 4) can be
used for all 12 models.

2.2 Parameter estimation for the EPGMM family

2.2.] Introduction Estimation of the model parameters, via the
AECM algorithm, is analogous to that of the PGMM parameter
estimation procedure described by McNicholas and Murphy (2008).
The estimates for the eight pre-existing models are obtained from
the PGMM estimates by writing \Ilg=|\Ilg|1/P\IIg/|\Ilg|1/P, and
then setting wgzlqlgll/P and Ag=\IIg/|\Ilg|1/P. However, the
derivation of the maximum likelihood estimates of the model
parameters for the new models, requires the method of Lagrange
multipliers (Lagrange, 1788). Parameter estimates for the CCUU
model are derived in Section 2.2.2 and derivations for the other
three new models are given at the end of said section.

2.2.2 AECM algorithm The EM algorithm is an iterative
technique for ﬁnding maximum likelihood estimates when data are

 

2706

112 [3.10'811211an[p.IOJXO'SODBIIIJOJIIIOICl/[I(11111 IIIOJJ popcolumoq

9IOZ ‘ISlsnﬁnV uo ::

Model-based clustering of microarray expression data

 

Table 2. The covariance structure, number of covariance parameters and nomenclature for each member of the EPGMM family, along with the name of the

equivalent member of the PGMM family where applicable

 

 

 

EPGMM nomenclature PGMM equivalent Covariance structure Number of covariance parameters
Ag=A Ag=A cog—a) Agzlp

C C C C CCC 2g=AA’+a)Ip [pq—q(q—1)/2]+1

C C U C CUC 2g=AA’+a)ng [pq—q(q—1)/2]+G

U C C C UCC ZnggAé+wIp G[pq—q(q—1)/2]+1

U C U C UUC ZnggAé+ngp G[pq—q(q—1)/2]+G

C C C U CCU 2g=AAI+wA [pq—q(q—1)/2]+p

C C U U — ZgZAA/‘l'ng [Pq—Q(q—1)/2l+[G+(P—1)l
U C C U UCU ZnggAé+wA G[pq—q(q—1)/2]+p

U C U U — 2g=AgAig+ng G[pq—q<q— 1)/2]+[G+(p—1)]
C U C U — 2g=AA’+wAg [pq—q<q— 1)/2]+[1+G(p— 1)]
C U U U CUU 2g=AAl+ngg [pq—q(q—1)/2]+Gp

U U C U — ZnggAé+wAg G[pq—q(q—1)/2]+[1+G(p—1)]
U U U U UUU ZnggAig+ngg G[pq—q(q—1)/2]+Gp

 

C, constrained, U, unconstrained.

incomplete, or are treated as incomplete. In the expectation step
(E-step), the expected value of the complete-data log-likelihood (Q,
say) is computed, where the complete-data is the missing data plus
the observed data. Then in the maximization step (M-step), Q is
maximized with respect to the model parameters.

In the expectation-conditional maximization (ECM) algorithm
(Meng and Rubin, 1993), the M—step is replaced by a number of
conditional maximization (CM) steps. The AECM algorithm (Meng
and van Dyk, 1997) is an extension of the ECM algorithm that
permits different speciﬁcation of the complete-data at each stage.
Extensive details on the EM algorithm and variants thereof are given
by McLachlan and Krishnan (2008).

Since there are two sources of missing data for the EPGMM
family, the group memberships and the latent factors, the AECM
algorithm is used for parameter estimation. We shall use zig to denote
the group membership of sample i, so that zig :1 if sample i is in
group g and zig = 0 otherwise. At the ﬁrst stage of the algorithm, the
complete-data are (xi,zig) and in the E-step the zig are replaced by
their expected values

221mm.- mh, Ah, Ami.)

 

E[Zig|ﬁgsﬁgsAgsAgsa)g]=

to give the expected value of the complete-data log-likelihood,
Ql say. In the interest of brevity, the expected value of Zig will
be denoted Eig herein. The function Q1 is then maximized in
the CM-step to give ﬁg=Z?:12igxi/ng and ftgzng/n, where
ng = 2?:121'5, and 11: 2:321:15).

At the second stage, the complete-data is (xi,zig,uig) and in the
E-step the zig are replaced by 21-5. and the sufﬁcient statistics for the
factors U ig are replaced by

E[Uig lxisll'ga Agswg, Ag] =ﬂg(xi _M'g)s
 lxisll’gaAgawgs  2

lg _ﬂgAg +ﬂg(xi _”'g)(xi _ILg)/ﬂfga

respectively, where ﬂg=Aé(AgA(’g+a)gAg)_1, to give Qz. The
CM-step at this second stage will depend on the model. Consider
the CCUU model, so that Ag=A and Ang. In this case, the
expected complete-data log-likelihood QZ(A,a)g, A) can be written

G
1 —1 —1 —1 —1
C+§Zlng[plogwg +log|A |—a)g tr{A Sg}
g:

+2wgltr{A—1A[9gsg}—w;1tr{A’A—1Aog}],

where C is constant with respect to A, cog and A, and 9g =Iq —
pgA+pgsgﬁfg.

To maximize Q2 with respect to A, cog and A, it is necessary
to use the method of Lagrange multipliers. First, form the Lagrange
function L(A, cog, A, IC) = Q(A, cog, A) —Ic(|A| — 1). Note that we use
IC to denote the Lagrange multiplier to avoid confusion with the

elements of the matrix A. Differentiating L with respect to A, 00—1 ,

AT1 and IC, respectively, gives the following score functions.

G
aL .
31(A,wg, Am): — =2  [A—lsgp’ —A—1Aog],
g

8
3A Fl
BL 11
32(A,wg, Am): —1 = —g [pwg —tr{A_1Sg}
am; 2

+2tr{A—1Afsgsg}—tr{A—1AogA’}],

3L 1 G
S3(A,a)g, Am): —_1 = — Eng“ —wg—1$g

+2w;1(ABgsg)’—wglxogx’]+K|A|A,
3L
S A, ,A, =—= A —l.
4( 60g K) 3K I I
Note that S4 is included for completeness only and solving
S4 (A,a)g,A,Ic)=0 just returns the constraint |A|=1. Now, solving

 

2707

112 [3.10'811211an[p.IOJXO'SODBIIIJOJIIIOICl/[I(11111 IIIOJJ popcolumoq

9IOZ ‘ISlsnﬁnV uo ::

P.D.McNichoIas and IB.Murphy

 

S1(Anew,&)g, A,Ic) :0 gives
G n I G n —1
A new _ g A g
A — 25758357 Zeb—98 ’
and solving 52(Anew, (0%)”, A, IC) :0 gives
A 1 A —1 A new A A new A new
(mg)new = _tr{A [5g —2A pgsg+A egm y] 
12
Solving diag{S3(Anew, (agyew, 8“”, IC)} :0 leads to

A new

 

A =
G n
dia —g[s —2A“ew“ s Ame A“"'w’] .
But Anew is a diagonal matrix with |Anew| = 1, therefore

P

P
n+2Ic= HSJ- ,

.21
where Sj is the j-th element along the diagonal of the matrix

G

ng A new A A new A new
2  [5g —2A pgsg+A eg(A )’].
51:1 g
Therefore, it follows that

1

p p
K:E 115,- —n. (5)

The derivations for the other three new models are similar. The
estimates in the UCUU case are

A new

_ " —1
Ag —Sg,8g®g ,

A new 1 A—l A—l
(mg) =Etr{A sg—A Ag ﬁgsg},

G
Anew 1 n Anew"
A = d' g [s —A s ] ,
n+2K lag ;(é‘)g)new g g ﬁg g

 

 

where IC is as deﬁned in Equation (5) but, in this case, Sj is the j-th
element along the diagonal of the matrix

G
2 —”g is. — m sg]
A new g g '
g:1 (00g)
In the CUCU case, the estimate for A is derived in a row-by-row
fashion as
G —1
Anew ng
Xi =I'i Z  9g ,
g=1 g l
for i: 1, . . . , p where r; is the i-th row of the matrix
2:321 (ng /8g(i))Sg [32,, and Ego) is the i-th element along the diagonal

of the matrix Ag. The other estimates are
1 G _1
(@1162; Zﬁgupg [sg—zA pgsg—A og(A )’]},
51:1

A new "g

= . _ 2Anew A Anew Anew I
g —(.)m(.,+2.,) w w >1

1

P

ng 1 p
Kg=3 (.3)... 111ng- —1 ,

 

where 5g,- is the j-th element along the diagonal of the matrix

A new

sg —2A [sgsg +Amwegdl‘w)’.
In the UUCU case, the parameter estimates are given by

A new

_ " —1
Ag —Sg,8g®g ,

A new A

G
A 1 A A _
(WVCWZEE :ﬂgtrlAg1(Sg_Ag ﬁgsg)}’
51:1

A new 1 A new A
= — ' _ A
g (mm (1 +2Kg/ng) dlag{Sg ﬁgsg},

and Kg is as in the CUCU case but with 5g,- given by the j-th element

along the diagonal of the matrix S g — A ,8 5,5 g.

Note that the predicted clustering for each member of the EPGMM
family is given by the maximum a posteriori (MAP) classiﬁcation.
That is, the posterior predicted component membership of tissue i
is the value of g for which Eig is the greatest.

2.3 Convergence and model selection

2.3.1 Convergence criterion Aitken’s acceleration (Aitken, 1926)
is used in the analyses herein to estimate the asymptotic maximum
of the log-likelihood at each iteration. This allows a decision to be
made about whether or not a given AECM algorithm has converged.
Aitken’s acceleration at iteration t is given by

l<r+l> _ p)

(r) _
a — la) _l(r—1>’

where 10‘“), l“) and [0—1) are the log-likelihood values from
iterations t+ 1, t and t— 1, respectively. The asymptotic estimate
of the log-likelihood at iteration t+1 is given by

[(9:1) :10) + #0041) _ 1(0)
1 — a“)
(Bohning et al., 1994). Herein, the stopping criterion proposed by
McNicholas et al. (2010) is used, so that the algorithm can be stopped

when 18:1) —l(t) <6. More speciﬁcally, 6:01 is used. Note that
this criterion is very similar to that proposed by Lindsay (1995),

who suggested stopping when [SSH—10+” < 6.

2.3.2 Model selection The Bayesian information criterion (BIC
Schwarz, 1978) is used to select the best member of the EPGMM
family, in terms of both model and number of factors. Note
that the BIC can also be used to select the number of mixture
components (cf. Fraley and Raftery, 1999; McNicholas and Murphy,
2008) but this is not necessary for the analyses herein since
we ﬁx G22. For a model with parameters 0, the BIC is

 

2708

112 Bio's112umofp101xo'sot112u1101u101q”:d11q 111011 pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

Model-based clustering of microarray expression data

 

given by BIC=2l(x,0)—mlogn, where l(x,6AI) is the maximized
log—likelihood, 0 is the maximum likelihood estimate of 0, m is
the number of free parameters in the model and n is the number of
observations. The effectiveness of the BIC for choosing the number
of factors in a factor analysis model has been established by Lopes
and West (2004), while McNicholas et al. (2010) provide practical
evidence that the BIC performs well in choosing the number of
factors for the PGMM family of models.

A number of other model selection criteria could be used including
the Akaike information criterion (AIC; Akaike, 1974), the integrated
completed likelihood (ICL; Biernacki et al., 2000) and clustering
stability (cf. von Luxburg, 2009). However, we found that the BIC
gave a quick solution and generally good clustering results.

3 ANALYSES

3.1 Dimensionality reduction

McLachlan et al. (2002) analysed two microarray gene expression
data sets—one on leukaemia data and another on colon tissue
samples—using the EMMIX-GENE approach. The ﬁrst stage of this
approach focuses on data reduction where, initially, one- and two-
component mixtures of t-distributions are ﬁtted to the data. Then a
gene is retained only if two conditions are satisﬁed.

One of these conditions is that the minimum cluster size exceeds
some prespeciﬁed threshold al. The other condition concerns the
result of a likelihood ratio test, or tests. First, the hypothesis H0:
G: 1 is tested against H1 :G=2 and the gene is retained if

—2logh > a2, (6)

where )1 is the likelihood ratio statistic. However, if the condition
in Equation (6) is not met then the hypothesis H0 : G=2 is tested
against H1 :G=3 and the gene is retained if the same condition is
satisﬁed, with the same a2, for this test statistic )1 and at least two
of the three components contain at least a1 tissues.

When ﬁtting the two- and three-component mixture models for
this purpose, starting values for the component memberships are
deﬁned randomly or by using starting values based on k-means
clustering results. This whole process represents the ﬁrst stage
of the EMMIX-GENE approach and can be carried out using
the select—genes software that accompanies McLachlan et al.
(2004). For the analyses herein, the select—genes software is
used with thresholds a1 = a2 = 8, as in McLachlan et al. (2002), and
50 random and 50 k-means starts.

3.2 Leukaemia data

3.2.1 The data Golub et al. (1999) presented data on two forms
of acute leukaemia: acute lymphoblastic leukaemia (ALL) and
acute myeloid leukaemia (AML). Affymetrix arrays were used to
collect measurements for 7129 genes on 72 tissues. There were
a total of 47 ALL tissues and 25 with AML. The data were
sourced from the web site accompanying McLachlan et al. (2004,
www .maths . uq . edu . au/Ngjm/ emmix—gene/) and so they
had been preprocessed (Dudoit et al., 2002; McLachlan et al., 2002)
as follows.

(1) Genes with expression less than 100 or greater than 16000
were removed.

Table 3. The BIC for the best q for each of the 12 members of the EPGMM
family for the leukaemia data

 

 

Model q BIC Model q BIC

CCCC 3 —411 646.50 CCUC 3 —411 566.29
UCCC 1 —416 954.56 UCUC 1 —416 803.57
CCCU 4 —414 615.22 CCUUa 5 —413 207.29
UCCU 1 —423 354.79 UCUUa 1 —422 089.38
CUCUa 4 —413 966.90 CUUU 5 —413 978.04
UUCUa 1 —423 933.46 UUUU 1 —423 532.04

 

aOne of the four new models.

Table 4. Estimated group membership for the best EPGMM model for the
leukaemia data

 

 

ALL 42 0
AML 5 25

 

(2) Genes with expressions satisfying max/minfS and
max—min5500 were removed.

(3) The natural logarithm was taken.

Following this preprocessing, a total of 3731 genes remained. This
number was further reduced to 2030 following application of the
select—genes software (cf. Section 3.1).

3.2.2 The EPGMM approach Treating this as a clustering
problem where the form of leukaemia is unknown, all 12 members
of the EPGMM family (Table 2) were ﬁtted to these data for G: 2,
q: 1, ...,6 and 10 different random starting values for the 21-5,. The
BIC for the best q for each of the 12 members of the EPGMM family
is given in Table 3.

The best of these models, in terms of BIC, was a CCUC
model with q=3 latent factors. The chosen model has a non-
diagonal covariance structure where the covariance between pairs
of genes is equal across different clusters but the variance of each
gene is unequal across different clusters (Section 1.2). The MAP
classiﬁcations arising from the parameter estimates associated with
this model are given in Table 4; only ﬁve tissue samples were
misclassiﬁed.

3.2.3 Hierarchical clustering, k-means, k-medoids and MCLUST
In addition to the EPGMM technique, several other techniques
were applied to these data using the R software (R Development
Core Team, 2010). Agglomerative hierarchical clustering was used,
with Euclidean distance and three different linkage methods:
complete, average and single. The k-means (cf. Hartigan and
Wong, 1979) and k-medoids techniques were also used. In the
latter case, the partitioning around medoids (PAM; cf. Kaufman
and Rousseeuw, 1990, Chapter 2) algorithm was used. Finally,
in order to compare our model-based clustering approach to the
well-established MCLUST approach, we used the mo lus t package
(Fraley and Raftery, 1999) for the R software.

The results, which are summarized in Table 5, give the Rand
and adjusted Rand indices as measures of class agreement. The
Rand index (Rand, 1971) is based on pairwise agreements and

 

2709

112 Bio's112umofp101xo'sot112u1101u101q”:d11q 111011 pepeolumoq

9IOZ ‘ISlsnﬁnV uo ::

P.D.McNichoIas and IB.Murphy

 

Table 5. Summary results for all of the clustering techniques that were
applied to the leukaemia data

Table 6. The BIC for the best q for each of the 12 members of the EPGMM
family for the colon data

 

 

BIC Rand index Adjusted Rand index Model q BIC Model q BIC

Hierarchical (complete) — 0.532 0.05 8 CCCC 4 —79 085.73 CCUC 6 —70 937.72
Hierarchical (average) — 0.525 —0.024 UCCC 3 —77 267.65 UCUC 3 —77 268.16
Hierarchical (single) — 0.5 32 —0.01 3 CCCU 8 —71 064.11 CCUUa 7 —71 063.71
k-means — 0.593 0.187 UCCU 3 —77 310.19 UCUUa 4 —77 532.97
PAM — 0.518 0.023 CUCUa 8 —71 609.10 CUUU 8 —71631.35
MCLUST (VII) —416 293.2 0.593 0.186 UUCUa 2 —78 458.83 UUUU 2 —78 306.33
EPGMM (CCUC, q=3) —411566.3 0.869 0.738

 

disagreements, and the adjusted Rand index (Hubert and Arabic,
1985) is effectively the Rand index corrected for random chance.
These indices reveal that the best of the non-model-based approaches
was k-means clustering, with an adjusted Rand index of 0.187. In
fact, k-means clustering narrowly outperformed mc lust on these
data, but the EPGMM model with the greatest BIC (CCUC, q=3)
was the best model overall.

3.2.4 The EMMIX—GENE approach McLachlan et al. (2002)
analysed the same data using the EMMIX-GENE approach with
four random and four k-means starts in the ﬁrst stage, which reduced
the number of genes to 2015. In the second stage, a mixture of 40
normal distributions with isotropic covariance structure was ﬁtted
to the 2015 genes. Two of these groups (Groups 1 and 3) provided
clusterings that were most similar to the type of leukaemia—of
course, in a real clustering scenario this could not be established. A
two-component mixture of factor analysers, with q = 6 factors, was
ﬁtted to the data using the genes from Groups 1 and 3, respectively.
Using the genes from Group 1 led to the misclassiﬁcation of 13
tissues and using those from Group 3 led to the misclassiﬁcation
of six tissues. Note that McLachlan et al. (2002) did not specify
how many different random starts were used but, based on other
analyses, it seems likely that 50 random and 50 k-means starts were
used.

3.2.5 Two other approaches In addition to the EMMIX—Gene
approach, McLachlan et al. (2002) used two other approaches to
cluster the leukaemia tissues. In both cases, the ﬁrst stage was
identical to that described in Section 3.2.4. The ﬁrst alternative
approach was to cluster the tissues based on the 40 ﬁtted group
means and the top 50 of the 2015 genes. Fitting a two-component
mixture of factor analysers with q=8 factors to these data, using
50 random and 50 k-means starts, led to the misclassiﬁcation of
just one tissue. The second alternative approach was to base the
analysis on the top 50 genes. Fitting a two-component mixture of
factor analysers, with q=8 factors to these data, using 50 random
and 50 k-means starts, led to the misclassiﬁcation of 10 tissues.

3.2.6 Comments The EPGMM approach gave very good
clustering performance when applied to the leukaemia data. This
approach used 10 random starts and led to the misclassiﬁcation of
just ﬁve tissues. This performance far exceeds that of agglomerative
hierarchical clustering, k-means clustering, PAM and MCLUST.
In fact, the best of all of these techniques had an adjusted Rand
index of 0.187, while the best EPGMM model had an adjusted
Rand index of 0.738. Although, in one instance, one of the

 

aOne of the four new models.

approaches of McLachlan et al. (2002) returned a better predicted
classiﬁcation, it is difﬁcult to make a direct comparison to the
EPGMM approach. This difﬁculty arises because the EPGMM
approach is a genuine clustering approach, while the methods
described in Sections 3.2.4 and 3.2.5 assumed, to some extent,
knowledge of the truth. This knowledge was clearly used in the
analyses described in Section 3.2.4 but was used in a less obvious
fashion in the analyses given in Section 3.2.5. In this latter case,
the choice of the number of clusters (40) was validated in some
sense by the fact that two of the groups give classiﬁcations that
were similar to the true leukaemia type. In fact, as mentioned
by McLachlan et al. (2002), an objective technique for choosing
this number is not possible since genes cannot be assumed to be
independently distributed within a tissue sample. Furthermore, it is
quite likely that the number of factors q was selected, in each case,
to give the best classiﬁcation. This could be done objectively, as in
Section 3.2.2, using the BIC. Finally, any comparison between the
EPGMM approach and the approaches of McLachlan et al. (2002)
would have to be taken in context with the fact that different subsets
of the 3731 genes are used in each case.

3.3 Colon data

3.3.1 The data Alon et al. (1999) presented gene expression data
on 62 colon tissue samples, of which 40 were tumours and the
remaining 22 were normal. Affymetrix arrays were used to collect
measurements for 6500 gene expressions on all 62 tissues. Following
Alon et al. (1999) and McLachlan et al. (2002), only the 2000 genes
with the highest minimal intensity are focused upon. The data were
again sourced from the web site mentioned in Section 3.2.1 and, this
time, the only preprocessing was the taking of natural logarithms,
followed by normalization. Application of the select—genes
software, with the settings speciﬁed in Section 3.1, led to the
reduction of the number of genes from 2000 to just 461.

3.3.2 The EPGMM approach Treating this as a clustering
problem where the type of tissue is unknown, all 12 members of
the EPGMM family (Table 2) were ﬁtted to these data for G22,
q = 1, . . ., 10 and 10 different random starting values for the 21-5,. The
BIC for the best q for each of the 12 members of the EPGMM family
is given in Table 6.

The best of these models, again in terms of BIC, was a CCUC
model with q = 6 latent factors; the covariance structure in this model
is the same as that chosen for the leukaemia data (Section 3.2.2).
The MAP classiﬁcations given by the parameter estimates associated

 

2710

112 Bio's112umofp101xo'sot112u1101u101q”:d11q 111011 p9p1201umoq

9IOZ ‘ISlsnﬁnV uo ::

Model-based clustering of microarray expression data

 

Table 7. Estimated group membership for the best EPGMM model for the
colon data

Table 9. Estimated group membership for the second best EPGMM model
for the colon data

 

 

 

Tumour 37 3
Normal 2 20

 

Poly detector 19 3
Total extraction of RNA 5 35

 

Table 8. Summary results for all of the clustering techniques that were
applied to the colon data

 

 

BIC Rand index Adjusted Rand index
Hierarchical (complete) — 0.497 —0.01 8
Hierarchical (average) — 0.526 —0.005
Hierarchical (single) — 0.526 —0.014
k-means — 0.494 —0.0 1 6
PAM — 0.611 0.218
MCLUST (VII) —81 124.36 0.500 —0.006
EPGMM (CCUC, q = 6) —70 937.72 0.849 0.697

 

with this model are given in Table 7; only ﬁve tissue samples were
misclassiﬁed.

3.3.3 Hierarchical clustering, k-means, k-medoids and MCLUST
In addition to the EPGMM technique, the methods used in
Section 3.2.3 were run on these colon data using the R software.
The results, which are summarized in Table 8, suggest that
the best of the non-model-based approaches was PAM, with an
adjusted Rand index of 0.218. This time, mc lust outperformed
k-means clustering but PAM outperformed mc lust. The EPGMM
model with the greatest BIC (CCUC, q=6) was the best model,
misclassifying just ﬁve tissues based on 10 random starts.

3.3.4 Correspondence with McLachlan et al. (2002 ) Using
various techniques, McLachlan et al. (2002) found ﬁve different
clusterings of these data. However, none of these clusterings
corresponded to the tissue type. While, once again, the EPGMM
results are not directly comparable with those of McLachlan et al.
(2002), it is interesting to look at the second best of the EPGMM
models. The second best of the EPGMM models, in terms of BIC,
was a CCUU model with q=7 latent factors. Note that this is one
of the four new models that were introduced herein and, again,
this model has equal covariance between pairs of genes; however,
the variance structure is more complex than for the CCUC model.
The MAP classiﬁcation given by the parameter estimates associated
with this CCUU model do not separate tumour from normal tissue.
However, they are similar to what McLachlan et al. (2002) call C1, in
that they seem sensible when one considers that there was a change
of protocol during the experiment (Getz et al., 2000; McLachlan
et al., 2002). Speciﬁcally, tissues 1—11 and 41—5 1 were all extracted
from the ﬁrst 11 patients using a poly detector, while the remaining
samples were taken from the other patients using total extraction of
RNA. Looking at the tissues by extraction method, rather than by
tissue type, leads to the estimated classiﬁcations given in Table 9;
only eight of the tissues were misclassiﬁed by this CCUU model
when the data are considered by extraction method.

 

Table 10. Summary results, by extraction method, for all of the clustering
techniques that were applied to the colon data

 

 

BIC Rand index Adjusted Rand index
Hierarchical (complete) — 0.5 18 0.024
Hierarchical (average) — 0.545 0.035
Hierarchical (single) — 0.526 —0.014
k-means — 0.526 0.048
PAM — 0.581 0.158
MCLUST (VII) —81 124.36 0.526 0.045
EPGMM (CCUU, q = 7) —71 063.71 0.772 0.542

 

The results from applying the other methods to the colon data
(cf. Table 8), can also be viewed in terms of extraction method,
rather than tissue type. These results are given, along with our best
CCUU model, in Table 10. From this table, it is clear that our
CCUU model gives the best clustering performance of all of the
approaches. Furthermore, the hierarchical (complete and average
linkage), k-means, and mo lus t clustering results are all better when
viewed in terms of extraction method.

3.3.5 Comments The EPGMM approach gave very good
clustering performance when applied to the colon data. Our approach
led to the misclassiﬁcation of just ﬁve tissues, when these data were
viewed by tissue type. This performance far exceeded that of all
of the other techniques that were used—in fact, the performance of
these other approaches was surprisingly poor, with only PAM giving
better than random classiﬁcations (cf. Table 8). This phenomenon is
partly explained when one looks at the classiﬁcations by extraction
method, rather than by tissue type (cf. Table 10). In this case, only
one method performed worse than random, which might suggest
that techniques such as k-means clustering and MCLUST were
picking up extraction method more— so than tissue type. That said, the
performance of these methods was only slightly better than random
which suggests that the restrictive cluster shapes imposed by k-
means clustering and MCLUST were not at all suited to the data.
On the other hand, the best of the new EPGMM models gave the
based clustering performance, misclassifying just eight samples.

4 DISCUSSION

The EPGMM family of models has been shown to give good
clustering performance when applied to gene expression microarray
data. These applications, concerning leukaemia and colon tissue
data, respectively, were conducted as genuine clustering examples.
That is, no information on the true tissue classiﬁcation was used for
parameter estimation or model selection. In fact, this information
was only used to assess the performance of the selected model.
In this context, the clustering performance of the EPGMM family
can be looked upon favourably. Moreover, the performance of the

 

2711

112 Bio's112umofp101xo'sot112u1101u101q”:d11q 111011 p9p1201umoq

9IOZ ‘ISlsnﬁnV uo ::

P.D.McNichoIas and IB.Murphy

 

EPGMM family on both data sets far exceeded that of a number of
popular clustering techniques, including agglomerative hierarchical
clustering and k-means clustering.

However, like the techniques of McLachlan et al. (2002), the
EPGMM family relies on multiple random starts. In addition to the
obvious drawback of the sensitivity of results to the starting values,
there is the computation time that is required. Furthermore, there is
no guarantee that increasing the number of random starts will lead
to better clustering results. This is due, in the main, to the fact that
models with greater BIC do not necessarily give better clustering
performance. This phenomenon has been observed previously and
work into ﬁnding better model selection techniques is ongoing.
That said, the EPGMM family did perform well in the analyses
in Section 3, based on random starting values and using the BIC.

5 CONCLUSION

The EPGMM family of mixture models has been introduced and
used for the model-based clustering of gene expression microarray
data. This family of models is an extension of the PGMM family of
models which, in turn, is an extension of the mixtures of factor
analysers model. The EPGMM family of models are very well
suited to the analysis of high-dimensional data. The reason for this
suitability is 3-fold. First, each member of the EPGMM family
has a number of covariance parameters that is linear in the data
dimensionality. Second, as shown herein, the Woodbury identity can
be used to avoid the inversion of any non-diagonal p X p matrices,
leading to efﬁcient computation. Thirdly, as shown by McNicholas
et al. (2010) in the context of the PGMM family, these models
are ‘trivially parallelizable’, opening up the possibility of even
more efﬁcient parameter estimation using parallel computing. The
EPGMM family was applied to two well-known gene expression
microarray data sets. In both cases, the EPGMM family performed
well and gave much better clusterings than several popular clustering
techniques. Herein, we took G=2 for all of the analysis but future
work will focus on the selection of G.

ACKNOWLEDGEMENTS

The authors gratefully acknowledge the insightful and helpful
comments of three anonymous reviewers.

Funding: This work was supported by a Discovery Grant from the
Natural Sciences and Engineering Research Council of Canada,
and by a Basic Research Grant (04/BR/M0057) and a Research
Frontiers Grant (2007/RFP/MATF281) from Science Foundation
Ireland. The high—performance computing equipment that was used
was provided by Silicon Graphics Inc. through funding from the
Canada Foundation for Innovation—Leaders Opportunity Fund and
from the Ontario Research Fund—Research Infrastructure Program.

Conﬂict of Interest: none declared.

REFERENCES

Aitken,A.C. (1926) On Bemoulli’s numerical solution of algebraic equations. Proc. R.
Soc Edinb., 46, 289—305.

Akaike,H. (1974) A new look at the statistical model identiﬁcation. IEEE Trans.
Automat. Contr, 19, 716—723.

Alon,U. et al. (1999) Broad patterns of gene expression revealed by clustering analysis
of tumor and normal colon tissues probed by oligonucleotide arrays. Proc. Natl
Acad. Sci. USA, 96, 6745—6750.

Biemacki,C. et al. (2000) Assessing a mixture model for clustering with the integrated
completed likelihood. IEEE Trans. Pattern Anal. Mach. Intell, 22, 719—725.

B6hning,D. et al. (1994) The distribution of the likelihood ratio for mixtures of densities
from the one-parameter exponential family. Ann. Inst. Stat. Math., 46, 373—388.

Dempster,A.P. et al. (1977) Maximum likelihood from incomplete data via the EM
algorithm. J. R. Stat. Soc Ser. B, 39, 1—38.

Dudoit,S. et al. (2002) Comparison of discrimination methods for the classiﬁcation of
tumors using gene expression data. J. Am. Stat. Assoc, 97, 77—87.

Fraley,C. and Raftery,A.E. (1999) MCLUST: software for model-based cluster analysis.
J. Classif., 16, 297—306.

Fraley,C. and Raftery,A.E. (2002) Model-based clustering, discriminant analysis, and
density estimation. J. Am. Stat. Assoc, 97, 611—631.

Frﬁhwirth-Schnatter,S. (2006) Finite Mixture and Markov Switching Models. Springer,
New York.

Getz,G et al. (2000) Coupled two-way clustering analysis of gene microarray data.
Proc. Natl Acad. Sci. USA, 97, 12079—12084.

Ghahramani,Z. and Hinton,GE. (1997) The EM algorithm for factor analyzers.
Technical Report CRG—TR-96—I, University Of Toronto, Toronto.

Golub,T.R. et al. (1999) Molecular classiﬁcation of cancer: class discovery and class
prediction by gene expression monitoring. Science, 286, 531—537.

Hartigan,J.A. and Wong,M.A. (1979)A k-means clustering algorithm. Appl. Stat, 28,
100—108.

Hubert,L. and Arabie,P. (1985) Comparing partitions. J. Classif., 2, 193—218.

Kaufman,L. and Rousseeuw,P.J. (1990) Finding Groups in Data: An Introduction to
Cluster Analysis. Wiley, New York.

Lagrange,J.L. (1788) Me’chanique Analitique. Chez le Veuve Desaint, Paris.

Lindsay,B.G (1995) Mixture models: theory, geometry and applications. In NSF-
CBMS Regional Conference Series in Probability and Statistics, Vol. 5. Institute of
Mathematical Statistics, Hayward, CA.

Lopes,H.F. and West,M. (2004) Bayesian model assessment in factor analysis. Stat.
Sin., 14, 41—67.

McLachlan,GJ. and Krishnan,T. (2008) The EM Algorithm and Extensions, 2nd edn.
Wiley, New York.

McLachlan,GJ. and Peel,D. (2000a) Finite Mixture Models. John Wiley & Sons, New
York.

McLachlan,GJ. and Peel,D. (2000b) Mixtures of factor analyzers. In P. Langley (ed.)
Seventh International Conference on Machine Learning. Morgan Kaufmann, San
Francisco, pp. 599—606, .

McLachlan,GJ. et al. (2002) A mixture model-based approach to the clustering of
microarray expression data. Bioinformatics, 18, 412—422.

McLachlan,GJ. et al. (2004) Analyzing Microarray Gene Expression Data. Wiley,
Hoboken, New Jersey.

McLachlan,GJ. et al. (2006) A simple implementation of a normal mixture approach
to differential gene expression in multiclass microarrays. Bioinformatics, 22,
1608—1615.

McNicholas,P.D. and Murphy,T.B. (2008) Parsimonious Gaussian mixture models. Stat.
Comput., 18, 285—296.

McNicholas,P.D. and Murphy,T.B. (2010) Model-based clustering of longitudinal data.
Can. J. Stat, 38, 153—168.

McNicholas,P.D. et al. (2010) Serial and parallel implementations of model-based
clustering via parsimonious Gaussian mixture models. Comput. Stat. Data Anal,
54, 711—723.

Meng,X.L. and Rubin,D.B. (1993) Maximum likelihood estimation via the ECM
algorithm: a general framework. Biometrika, 80, 267—278.

Meng,X.L. and van Dyk,D.A. (1997) The EM algorithm — an old folk song sung to a
fast new tune (with discussion). J. R. Stat. Soc Ser. B, 59, 511—567.

Rand,W.M. (1971) Objective criteria for the evaluation of clustering methods. J. Am.
Stat. Assoc, 66, 846—850.

R Development Core Team (2010) R: A Language and Environment for Statistical
Computing. R Foundation for Statistical Computing, Vienna, Austria.

Schwarz,G (1978) Estimating the dimension of a model. Ann. Stat, 6, 31—38.

Spearman,C. (1904) The proof and measurement of association between two things.
Am. J. Psychol., 15, 72—101.

Tipping,T.E. and Bishop,C.M. (1999) Mixtures of probabilistic principal component
analysers. Neural Comput., 11, 443—482.

von Luxburg,U. (2009) Clustering stability: an overview. Found. Trends Mach. Leam.,
2, 235—274.

Woodbury,M.A. (1950) Inverting Modiﬁed Matrices. Statistical Research Group,
Memorandum Report no. 42. Princeton University, Princeton, New Jersey.

Yeung,K.Y. et al. (2001) Model-based clustering and data transformations for gene
expression data. Bioinformatics, 17, 977—987.

 

2712

112 Bio's112umofp101xo'sor112u1101u101q”:d11q 111011 p9p1201umoq

9IOZ ‘ISlsnﬁnV uo ::

