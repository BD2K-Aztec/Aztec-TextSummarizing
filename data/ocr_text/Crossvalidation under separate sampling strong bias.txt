ORIGINAL PAPER

Vol. 30 no. 23 2014, pages 3349—3355
doi: 10. 1 093/bioinformatics/btu527

 

Data and text mining

Advance Access publication August 13, 2014

Cross-validation under separate sampling: strong bias and how to

correct it

Ulisses M. Braga-Neto1 ’2, Amin Zollanvaril’3 and Edward R. Dougherty1’2’*

1Department of Electrical and Computer Engineering, 2Center for Bioinformatics and Genomic Systems Engineering and
3Department of Statistics, Texas A&M University, College Station, TX, 77843, USA

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: It is commonly assumed in pattern recognition that cross-
validation error estimation is ‘almost unbiased’ as long as the number
of folds is not too small. While this is true for random sampling, it is not
true with separate sampling, where the populations are independently
sampled, which is a common situation in bioinformatics.

Results: We demonstrate, via analytical and numerical methods, that
classical cross-validation can have strong bias under separate sam-
pling, depending on the difference between the sampling ratios and
the true population probabilities. We propose a new separate-sam-
pling cross-validation error estimator, and prove that it satisﬁes an
‘almost unbiased’ theorem similar to that of random-sampling cross-
validation. We present two case studies with previously published
data, which show that the results can change drastically if the correct
form of cross-validation is used.

Availability and implementation: The source code in C++, along
with the Supplementary Materials, is available at: http://gsp.tamu.
ed u/Publications/su pplementary/zollanvari1 3/.

Contact: ulisses@ece.tamu.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on December 30, 2013; revised on July 23, 2014; accepted
on July 30, 2014

1 INTRODUCTION

The most important property of a classiﬁer is its error rate (prob-
ability of misclassiﬁcation) because the error rate quantiﬁes the
predictive capacity of the classiﬁer. If the feature-label distribu-
tion is known, then the true error can be found exactly; however,
in practice, the feature-label distribution is unknown and the
error must be estimated. If the sample is small, then the estima-
tion must be computed using the same data as that used for
training the classiﬁer. Perhaps the most commonly used training
data-based classiﬁcation error estimator is cross-validation. It
has a long history going back to 1968 (Lachenbruch and
Mickey, 1968). In its most basic form, the k—fold cross-validation
error estimate, 22:“), for a sample of size n (it is assumed that k
divides n) is computed by selecting randomly a partition of the
sample into k data ‘folds’ (subsets), for each fold applying the
classiﬁcation rule on the data not in the fold, computing the
error rate of the designed classiﬁer on the left-out fold, and

 

*To whom correspondence should be addressed.

then averaging the resulting k error rates. When k = 11, one
gets the leave-one-out estimator, 

Cross-validation’s salient good property is that, under random
sampling, it can be proved (see Devroye et al., 1996) that it is

‘almost unbiased’, in the sense that

Eiézv‘k)i=Eisn_n/ki, (1)

where 8,, is the true error (probability of misclassiﬁcation) of a
classiﬁer designed on a sample of size 11. Hence, the bias is not
too great as long as n/k is small. For leave-one-out,
E[§,:]=E[sn_1], and the estimator is essentially unbiased. The
salient point motivating the present article is that (1) depends
on the sampling being random, and that when sampling is not
random, there can be severe bias.

The importance of bias for an arbitrary error estimator s, can
also be gleaned from its role in the estimator root-mean-square
error: RMS[§n] = E[(§,, — 8n)2]1/2 = \/Bias[§n]2 + Vardev[§n],
where Bias[§n] = E[§n — an] and Vardev[§n] =Var[§n — en]
(Braga-Neto and Dougherty, 2004). As mentioned previously,
for classical cross-validation under random sampling, it follows
from (1) that, if n/k is small, then Bias[§n] % 0, in which case
RMS[§n] % Varcliéﬂén]. While the variance of CV is known to be
large in small-sample cases (Braga-Neto and Dougherty, 2004;
Glick, 1973), it will typically reduce to zero as n —> oo (Devroye
et al., 1996). However, the bias introduced by application of the
classical CV estimator under nonrandom sampling will generally
not approach zero as n —> 00. The result is an inconsistent esti-
mator, which is imprecise under arbitrarily large sample sizes.

Under random sampling, an independent and identically dis-
tributed (i.i.d.) sample S is drawn from the mixture of the popu-
lations 1'10 and 1'11. This means that if a sample of size n is drawn
for binary classiﬁcation, then the numbers of sample points no
and n1 drawn from the populations 1'10 and 1'11, respectively, are
random variables no ~Binomial(n, c) and n1~ Binomial(n, l — c),
where c=P(Y= 0) is the a priori probability that the label Y is
zero, i.e. the sample point comes from population 1'10. This
random-sampling assumption is so pervasive that it is usually
assumed without mention and in books is often stated at the
outset and then forgotten. For instance, Duda et al. (2000)
state, ‘In typical supervised pattern classiﬁcation problems, the
estimation of the prior probabilities presents no serious
difﬁculties’. They are referring to the fact that the prior prob-
ability c=Pr (Y: 0) can be consistently estimated by the sam-

pling ratio, 8:  This is simply Bernoulli’s Law of Large

 

 

© The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3349

112 [3.1081120an[plOJXO'SODBIILIOJIITOIQ/[Z(11111 IIIOJJ popeolumoq

910K ‘09 lsnﬁnV no :2

U.M.Braga-Neto et al.

 

Numbers: ’2—0 —> c in probability. However, suppose the sampling
is not random, in the sense that the ratios r = "7° and l — r = "7‘ are
chosen before the sampling procedure. In this separate-sampling
case, S= So U S1, where the sample points in So and S1 are se-
lected randomly from Ho and 1'11, but given n, the individual
class counts no and n1 are not determined by the sampling pro-
cedure. With separate sampling, we have no sensible estimate of
0. Recognition of this particular problem of estimating the prior
probability when sampling is separate and its effect on linear
discriminant analysis (LDA) goes back to 1951 (Anderson,
1951). Often, one says that for separate sampling the ratios r=
"7° and l — r= 1—1 are chosen ‘prior to’ the sampling procedure.
But there is in fact no temporal meaning to this. For instance,
one could simply separately randomly sample no and 1'11 with
no and n1 being randomly selected by a process independent
of the sampling procedure, and the sampling would still be sep-
arate. The point is that r cannot be reasonably used as an esti-
mate of c.

Figure 1 (taken from Esfahani and Dougherty, 2014) illus-
trates the effects of separate sampling on the expected true
classiﬁer error for two classiﬁcation rules and multivariate
Gaussian distributions of equal and unequal covariance struc-
tures and dimensionality d = 3. For a given sample size n,
sampling ratio r, and classiﬁcation rule, the expected true error
rate E[s,,|r] is plotted for different class prior probabilities c, for
LDA and a non-linear radial basis function support vector
machine (RBF-SVM). For each r and n, no is determined as
no = inr'l. We observe that the expected error is close to minimal
when r = c and that it can greatly increase when r 75 c. This kind
of poor performance for separate sampling ratios not close to c is
commonplace (Esfahani and Dougherty, 2014).

In this article, we investigate the effect of separate sampling on
cross-validation error estimation. We will see that for a separate-
sampling ratio r not close to c there can be large bias, optimistic
or pessimistic. A serious consequence of this behavior can be
ascertained by looking at Figure 1. Whereas the expected true
error of the designed classiﬁer grows large when r greatly devi-
ates from c, a large optimistic cross-validation bias when r is far
from c can obscure the large error and leave one with the illusion
of good performance—and this illusion is not mitigated by large
samples! To overcome the bias problem for classical cross-
validation with separate sampling, we introduce a new cross-
validation estimator designed for separate sampling and prove
that it satisﬁes a bias property analogous to (l).

2 SYSTEMS AND METHODS

2.1 Discriminant analysis

We treat classiﬁcation via discriminants to facilitate demarca-
tion of the individual contributions of the class-conditional
distributions to the error analysis. A sample-based discriminant
is deﬁned as a (measurable) function W” : SH ER, where from
the deﬁnition, we see that we actually have a family of discrim-
inants indexed by n. A discriminant Wn deﬁnes a classiﬁcation
rule via

l, W” S, _0
wn<s)(X)=[ ( X>< , (2)

0, otherwise

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

H°9_ “*3-
go go
ﬁq— Te_
0 O
E 5
"UV'._ Poi-
Do 00
‘6’ ‘5
a) a.)
o' o—
o- 9.-
o | | | | | | | o | | | | | | |
02 04 06 08 02 04 06 08
I' 1'
°°._ “3-
§° §°
me. We-
Do 0O
E E
H“. P“.
so" se-
‘9 ‘3
cpl- 9.0!-
X X
mo mo
Q- Q.
o | | | | | | | o | | | | | | |
02 04 06 08 0.2 04 06 08
I' 1'
c=0.001 c=0.1 c=0.3 c=0.4 c=0.5
— c=0.6 — c=0.7 c=0.9 c=0.999

 

 

 

 

 

Fig. 1. Expected true error, E[8,,|r] as a function of r for LDA (left
column) and an RBF-SVM classiﬁer (right column) using synthetic
data. Top row: n=no +n1 =80 and equal covariance matrices; bottom
row: n = no + n1 = 80 and unequal covariance matrices

where X comes from either l'Io or 1'11. Because any classiﬁcation
rule \II,, can be expressed as a discriminant via
Wn(S, X)=Iwn(5)(X)=o — [Wax/(2)4, where I A is the indicator
function, discriminant analysis is completely general. We assume
a common sense property of discriminants, that the order of the
sample points within a sample does not matter.

With separate sampling, there are two separate samples
So={X1, . . . , Xno} and S1={X,,0+1, ...,X,,0+,,1} from popula-
tions Ho and 1'11, respectively. To demarcate the separate-
sampling case from the random-sampling case, we will write the
discriminant and corresponding classiﬁer by Wm”,1 (So, S1, X)
and \Ilno,”1 (So, S1, X), respectively, with the latter deﬁned in
the same manner as (2) with WHO,”1 (So, S1, X) replacing Wn(S, X).

The true classiﬁcation error with random sampling is given by

8,, = 0292 + (l — c)s,11, (3)
where

82=P(Wn(S’ X) S OlXE “0’ S)’ (4)
81=P(Wn(S,X)>OIXE 111. S),

n

are the population-speciﬁc error rates. For separate sampling,
the classiﬁcation error is given by

29an = 02920,”1 + (l — c)s,1,0,n1, (5)
where
82.... =P(Wn.,n.(So, S1, X) s we H0,S0,S1), (6)
8,110,,“ =P(W,,0,,,1(So,S1,X)>0|X e 111, So, S1).

are the population-speciﬁc error rates.

 

3350

112 [3.1081120an[plOJXO'SODBIILIOJIIIOIQ/[Z(11111 IIIOJJ popeolumoq

910K ‘09 lsnﬁnV no :2

Cross-validation under separate sampling

 

2.2 Classical cross-validation error estimation
For U C {1, . . . , n}, let SW) denote the sample S with the points
indexed by U deleted, and deﬁne

 X) = Wn—m(S(U)a X) 9 
where |U| = m is the size of U. Now let k divide it and consider a
(random) partition {U,-; i= 1, . . . , k} of {1, . . . , n}. Then the clas-
sical k—fold cross-validation estimator is given by

1 k
Acv(k) _ E : E :
8" _ Z 1 (IWnUi’(s,Xq)soqu=°+1W2Ui’(S,Xq)>OIYq=1)' (8)
i= qEUi

If k = n, this reduces to the leave-one-out estimator

1 n

A l _

8n — — E :(IrI/(n’)(s,xi)so [Yi=0 +1W9(s,xi)>o 1Y1: 1)’ (9)
n i=1

where we have omitted the braces around the singleton index set {i}.

Using the classical deﬁnition of cross-validation, (1)
does not hold with separate sampling, in general. To
demonstrate this, let No =  1 I yizo be the (random) number
of points from population l'Io in the sample S; the expected
cross-validation error rate under separate sampling is
E[§:V(k)|No =no]. For simplicity, we consider leave-one-out
cross-validation. From (9),

Blame =no] = % P(W§P(S, X1) 5 0| Y1 =0, No =no)

+ % P(W§11)(S,X1)>O|Y1=1,N0=n0)

= 7:70 13(Wno_1,n1 (5(1), S1, X) 5 0|X 6 Ho) (10)
+ % P(Wn.,n._1(So, Sim“), X)>0|X 6 “1)

= % E182._1,..i + g Elsi.,.._li.
On the other hand, it follows from (3) that
E[8n—1|N0 = "0] = CE[82_1|N0 = "0]
+ (1 - C)E[8,1._1|No =no] (11)
= cE[sgo,m_1] + (l — c)E[s,1,0,n1_1].

2.3 Cross-validation for separate sampling

To adapt cross-validation to separate sampling, let U C {1, ... , no},
let V C {no + l, . . . , no + n1}, let SgU) and SE” denote the samples So
and S1, with the points indexed by U and V deleted, respectively, and
deﬁne

W‘U’WSO, $1.20 = W.._m,.._z<s<w, S”), X). (12)

"0,111
where |U| = m and W | = l are the sizes of U and V, respectively.
Now let ko divide no and k1 divide n1, and consider (random) partitions
{U,-; i=1, ...,ko} of {1, ...,no} and {V,-; i=1, ...,kl} of
{no + l, .. . , no + n1}. Separate-sampling (ko, k1)-fold cross-validation
estimators are deﬁned by

(k k)0 1 Zko :1“:

"CV o, 1 , _

8 — I Uiij) a

now] no k1 i=1j=1reUi WELD“ (SOSLXJSO

(13)
(k k)1 1 Zko k1

ACV o, 1 , _

8 — — E E I Ui,VJ-) .

110,111 nlko izljzlrer W20,” (So,S1,X,)>0

These are estimators of the population-speciﬁc true errors 82M] and
81 , respectively. One may use a convex combination of the previous

"Osnl
estimators to yield a separate-sampling cross-validation estimator of

the overall true error rate:

A CV(ko,k1) _ A CV(ko,k1),0 A CV(ko,k1),1
19,10,”1 — c 19,10,”1 + (l — c) 19,10,”1 . (l4)

Ifc is known (or known to a high degree of accuracy), then one can use
it in (14). If e is unknown, then there is no proper cross-validation
estimator of the overall error rate.

If ko =no and k1=n1, then the (ko, k1)-fold cross-validation
estimators deﬁned previously reduce to separate-sampling leave-
one-out estimators:

1 no 111

Al,0 _
8n n _ — :21 WW" (8 S X‘)<0’
0s 1   110an 0! 1! l_

(15)

no 111

él’l — 1 E E I 3
_ — I“, . .
no,n1 nonl i=1j=1 ngoyn1(S0sS1ano+j)>0

A convex combination of these yields a separate-sampling
leave-one-out estimator of the overall true error rate:

g’ =06“) +(l—c)§l’1 . (16)

"0,111 "Osnl "0,111

Again, in the absence of knowledge of e, no proper estimator of
the overall error rate is possible.

We now show that a version of (1) holds for the separate-
sampling cross-validation estimator.

THEOREM. The cross-validation estimator in (14) satisﬁes

A k ,k _
E[8I:(:},(n:) 1)] — E[8no—no/ko,n1—n1/k1] - 

PROOF. First notice that if U C {1, . . . , no} and
VC {no+l,...,no+n1}, with |U| = m and |V| = I, then

ME’PGO, S1, Xi)~
Wno—m,n1—I(S0a S1,  E 1-1an U,
ME’PGO, S1, Xi)~

Wno—m,n1—I(S0a S1,  E 1-11,  E V-

Therefore, from (13),

A CV(ko,k1),0 _ 0
E[Enoﬂl ] _ EIEno—no/ko,n1—n1/k1]’

(k k) 1 (18)
A CV , , _ 1
ED911th0 1 ] _ Eleno—no/kom -nl/k1] '

Finally,

chv(ko,k1)] = CE[€CV(ko,k1),0] +  _  CV(ko,k1),1]

"Osnl "Osnl "Osnl
= 0 _ 1
c Hsno—no/kom1—m1/k1] + (1 C)E18no—no/ko,n1—n1/k1]
= Eleno—no/kom —m1/k1 '

In the case of the separate-sampling leave-one-out estimator
deﬁned in (16), the preceding theorem reduces to

Eiéio,.,i=Eisn._1,n._li. (19)

 

3351

112 [3.1081120an[plOJXO'SODBIILIOJIIIOIQ/[Z(11111 11101; popeolumoq

910K ‘09 lsnﬁnV no :2

U.M.Braga-Neto et al.

 

3 RESULTS AND DISCUSSION
3.1 Simulation study with synthetic and real data

We have performed a set of experiments using both synthetic
models and real data to examine the behavior of classical and
separate-sampling cross-validation under separate sampling.
Throughout we use 5-fold cross-validation. We consider four
well-known classiﬁcation rules: LDA, Quadratic Discriminant
Analysis (QDA), Linear Support Vector Machine (L-SVM)
and RBF-SVM (see the Supplementary Material for deﬁnitions
of these classiﬁcation rules).

To generate synthetic data, we use a model with class-condi-
tional 3-dimensional Gaussian distributions, N(uy, 2,), y = 0, l,
where u0=[0,0, ...,0,0], u1=[0,0, ...,0,9] and 2, has 02 on
the diagonal and p, off the diagonal. The pair (po, p1) can take
on the values (0.8, 0.8) or (0.8, 0.4). We set 9 so that the
Mahalanobis distance between the classes for equal covariance
matrices and the Bhattacharyya distance between the classes for
unequal covariance matrices is 3. We consider it = 80 and
n = 1000, so that we can compare small-sample and large-
sample results.

We consider four public microarray real datasets: pediatric
acute lymphoblastic leukemia (ALL; Yeoh et al., 2002), acute
myeloid leukemia (AML; Valk et al., 2004), multiple myeloma
(Zhan et al., 2006) and breast cancer (Desmedt et al., 2007).
Table 1 provides a summary of these real datasets, including the
total number of features and sample size. For a detailed descrip-
tion of the data preparation, the readers are referred to the
Supplementary Materials. The experiments on real data are essen-
tially similar to those on synthetic data except that in real data
experiments we use t-test feature selection to reduce the dimen-
sionality to d = 3. In real data experiments, we consider only
it = 80, which allows sufﬁcient data for holdout error estimation.

All experiments are performed for a range of
r= "—If e [0.15, 0.85]. We ﬁx it and determine no according to
no = inr'l. At each iteration, So and S1 are randomly picked
from either a synthetic model or real data to train the classiﬁer
and compute the two cross-validation estimates. Finding the bias
requires knowing the true error, which is estimated on 5000 in-
dependent sample points from the synthetic distributions, or held
out points in the case of real data; however, owing to separate
sampling the ordinary holdout method cannot be applied, and
we use separate-sampling holdout as explained by Esfahani and
Dougherty (2014). We consider c = 0.001, 0.1, 0.3,
0.4, 0.5, 0.6, 0.7, 0.9, 0.999. For each classiﬁcation rule, we
repeat the process of obtaining the true error and its estimates
4000 times for each value of r and c to obtain a distribution of
estimates and true errors from which to compute the bias.

Table 1. Microarray studies used in this study

 

 

Dataset Description Features no/n1

(Desmedt et al., 2007) Breast cancer 22 215 98/77
Weoh et al., 2002) Pediatric ALL 5077 149/99
Walk et al., 2004) AML 22215 116/157
(Zhan et al., 2006) Multiple myeloma 54 613 156/78

 

In Figure 2, we provide the results for the synthetic data with
unequal covariance matrices [(po, p1) =(0.8, 0.4)] for n = 80,
1000 and for two of the real datasets (Desmedt et al., 2007;
Valk et al., 2004). The complete set of results is given in the
Supplementary Material. In the ﬁgure, from left to right, the
columns correspond to LDA, QDA, L-SVM and RBF-SVM,
respectively. The top two rows of the ﬁgure correspond to the
real data from Desmedt et al. (2007) and Valk et al. (2004), and
the third and fourth rows correspond to the synthetic data with
n = 80 and n = 1000. The x—axis corresponds to the sampling
ratio r, the y-axis gives the bias, the solid lines are for the pro-
posed separate-sampling cross-validation, the dashed lines are
for classical cross-validation, and the colors code the value of c.

The trends are consistent across all experiments (including
those in the Supplementary Material): (i) for classical cross-
validation with c near 0.5, there is signiﬁcant optimistic bias for
large |r — cl; (ii) for classical cross-validation with small or large 0,
there is optimistic bias for large |r — cl and pessimistic bias for
small |r — cl as long as |r — cl is not very close to 0; (iii) for separate-
sampling cross-validation, estimation is slightly optimistic and
almost unbiased across the range of |r — cl. Combined with the
results of Esfahani and Dougherty (2014), the bias behavior of
classical cross-validation is especially harmful for large |r — c|
because it masks the increase in classiﬁer error that occurs for
large |r — cl, as shown in Figure 1. Furthermore, although the
deviation variance of classical cross-validation can be mitigated
by large samples, the bias issue generally remains just as bad for
large samples.

3.2 Two case studies

To further illustrate the effects of separate sampling on classical
cross-validation bias, we consider two published studies. The ﬁrst
(Ambroise and McLachlan, 2002) uses a colon microarray data-
set containing gene-expression measurements taken from 2000
genes for 62 tissue specimens, 40 tumorous tissues (class 0) and
22 normal tissues (class 1). Using the SVM-RFE classiﬁcation
rule (Guyon et al., 2002), the authors split the data into a training
and a test set, each including 31 specimens, by sampling without
replacement, such that the training data contain 20 tumorous
and 11 normal specimens. They compare the 10-fold cross-val-
idation error using (8) to the standard holdout estimate obtained
by counting the errors on the test set. But the standard holdout
estimate is unbiased under random sampling, not separate sam-
pling. For the latter, holdout estimation must take into account
the value of c to be unbiased (Esfahani and Dougherty, 2014).
Assuming the classiﬁer is applied to the US population, based on
the incidence rate of colorectal cancer among the US population,
which is 40/ 100 000 (Haggar and Boushey, 2009), c = 40 / 100 000.
The black solid and dotted curves in Figure 3 resemble the curves
plotted in Figure l of Ambroise and McLachlan (2002). The gray
solid and dashed curves are obtained by considering the
cross-validation scheme (14) and computing the true error
from (3). The error bars refer to the 95% conﬁdence interval.
All curves show the averaged error and estimated error obtained
on 200 random splits of the data as mentioned above.
These curves show that regardless of the number of genes
considered in the classiﬁer, using the classical cross-validation
(8) induces ~l3% optimistic bias with respect to the true

 

3352

112 [3.10811211an[plOJXO'SODBIILIOJIIIOIQ/[Z(11111 11101; popeolumoq

910K ‘09 lsnﬁnV uo 22

Cross-validation under separate sampling

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

0.6 0.8

0.4

0.2

0.8

0.6

0.4

0.2

0.6 0.8

0.4

0.2

0.4 0.6 0.8

0.2

Uoéaomaom W63 38“\\EowsmoHBmQOm.oxmoH&9:55.on m: n 0: >58” mo. N03

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

. x \ \ x .x x \
\ \ \\\\ \\\\ \\ I8 .\ \ \\\\\\ \\\ I8 - \\\ \\\ \
_ xxxxx I 0 _ xxxxx x 0 :xx x
x x x \\ \\ . xxx x \ I x xx x
_. \\ \\ \ _ \ \ xx x
, xx\x \\ I __ xxxx \ I 2 :xx x
. x _ xxx x _ x x
a .xxx \ x _
’4 ~ \\“\ \ \ lfo. H. — \\\ \\ Ifo. —_ --s~\\
fxxx 0 ...xxx 0 :2:
r x :Ixx s...
I r a I «1 xx
..x s
\ I h w 1 ~\
x..7f 4 4 x
.611 I . I .
x , 4 0 x l 0
.z9 II 71
~ A; III/III I x I’ll/I I xwo
. '1 II [II] x III/III]! x will
_ I II I III x III II [III x aﬂllﬂ
I I I I I 2 I I I III 2 III I
— IIII I I III] | . a IIIIIIII [III | . \ IIIIII/III
I I I
a r I I III/I IIIIO — I III/III IIIIIO . III/III! IIIII
_ _ _ _ _ _ _ _ _ _ _
mﬁﬁm mdmm wdmm
\ x \ \ — s \ xx \ \
x xx \ \\ \\ I00. x xxx \\ \\ I00 I x xxx x
x xx x \\ \ . x x x x
- \ \ \\ \ 0 — \\\\\ \ 0 - \ \\\ \
.. xx xx \ _ xx x 2 xx x x
x \\ \ I _ x x I xx x
a. x\\\ \\ __ :xx x ._ xxx
2 xxx x _ ::x x : :o x
$.xxx \ Irhv. ...xx \ Irnm ..o
a. xxx 0 ... xxx 0 ___. x
I x ,._xxx :2:
I I ~_ x I r s I
x g x k x
I
I”... I4. I I4
x. 6/1; 0 :1 0 a,
x ’51,? x 3! x :4”
I I | I |
. z [I II II x air, [III x a I II
— I II I III \ III [III ~ al/III
I I I I I I 2 I II [III 2 all II
— I IIII I III] | s II IIIIIIIIII | ~ IIIIIIIII
I
a I I IIIII III! 0 _ IIIIIIIIIIIIIII 0 ~ IIIIIIIIIIIII
_ _ _ _ _ _ _ _ _

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

mam mom 35
13
0
I6.
0
| «I.
4 .J
-0
12
0
_ _ _
\ x \ \ .. . xx .. . x.
~ \ \\ \ \\\ \\ IOAN - ssxxxx \ IOO. .~ ssssxx \\
..x xxxxxxxx \\ 0 Zicxx xx 0 I Sxxx x
_— \\\\ \\ | —~¥.\~\\ \ | .~ ss~\\ \
. xxxx \ ,‘xx x .1 x
1. xx x ...: ...:
:. xxxxx I6. :10 x I6. ___.Ix x
I. _. xxx 0 : :3“ xx 0 : ....~x xx
\
0.. | r ——___~\ I r =;_~\
\I e I): .l 1k:
x 4 4 x
. A, | . |
\ arr/II 0 \ 0 \
x _ 01/ x z x I
I I I II It
I .r I ./
~ I’ll III I x ’01! I x [/0
I II I a II ’11”
_ I II . r/ I x II I
III/I II 2 III/II 2 III/II
— I I I I . . [III I . IIII
III/I [III 0 IIIIII” 0 IIIIIINI
. III/I II _ all/I HI _ III]! II
_ _ _ _ _ _ _ _ _ _
oo Nol vol col oo Nol col oo Nol vol
mwmm mﬁmm mmmm

0.6 0.8

0.4

0.2

0.8

0.6

0.4

0.2

0.6 0.8

0.4

0.2

0.6 0.8

0.4

0.2

 

 

c=0.5

 

0:04 ___

c=0.3

c=0.1

c = 0.001

c = 0.999

0:09

0:07

0:06

 

 

Fig. 2. Bias of 5-fold CV as a function of r for LDA, QDA, L-SVM and RBF-SVM classiﬁers using real and synthetic data. Rows from top to bottom:

no +n1=1000

no + n1 = 80 and 20 7E 21; synthetic data n
and So 75 21; Columns from left to right: LDA; QDA; L-SVM; RBF-SVM. Dashed curves: regular cross-validation scheme. Solid curve: new scheme of

cross-validation. For each r and n, no is determined as no

data taken from Desmedt et al. (2007); data taken from Valk et al. (2004); synthetic data n

inrl

 

3353

U.M.Braga-Neto et al.

 

 

0.30 0.40 0.45
| | |

Error rate

0.25
|

 

0.20
|

 

 

 

log2(number of genes)

Fig. 3. The black solid and dotted curves resemble the expected true error
and cross-validation error rates reported in Figure 1 of Ambroise and
McLachlan (2002). The gray solid and dotted curves are the expected true
error and estimated error by using cross-validation scheme (14) when the
prior probability of colon cancer is set to be the incidence rate across the
USA. All curves show the averaged error and estimated error obtained on
200 random splits of the data

error, while the proposed cross-validation scheme is almost
unbiased.

In the second case study, we use the Parkinson’s dataset used
by Kaya et al. (2011). This dataset contains 22 biomedical voice
features and 195 measurements in which 48 belong to individ-
uals with Parkinson (class 0) and 147 measurements are taken
from healthy individuals (class 1), so that r = 0.246. The authors
use this dataset to construct classiﬁers for diagnosis of
Parkinson’s disease based on distorted voice features. Four clas-
siﬁers are constructed: naive Bayes (NB; Friedman et al., 1997),
C45 (Dietterich, 2000), kNN (k = 5) (Devroye et al., 1996) and
RBF-SVM. Although Kaya et al. (2011) have reported the
estimated classical cross-validation error on a single sample of
the data, we repeat the sampling procedure 200 times to get an
estimate of the expected cross-validation error using both the
classical (8) and the corrected cross-validation scheme (14). We
assume the prior probability c of Parkinson’s disease is deter-
mined by the incidence rate of Parkinson’s disease across the
USA, which is 13.4/ 100 000 Wan Den Eeden et al., 2003). In
Figure 4, the white bars are the expected classical cross-valid-
ation error rates; the shaded bars are the estimated error rates
using the separate-sampling cross-validation scheme. The bars
show the averaged estimated error obtained on 200 samplings
of the data. The behavior observed in Figure 2 makes it plaus-
ible that the error estimates for classical cross-validation will
exceed those of separate-sampling cross-validation, which is
nearly unbiased. This is true in all cases except for NB.
However, if we look carefully at Figure 2, we see that the
point at which the bias becomes optimistic (for increasing r)
can be well left of 0.5. This point is affected by the covariance
structure and the classiﬁcation rule. In this case, for NB, it is to
the left of 0.246.

Error rate

 

 

 

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35

NB C4.5 kNN SVM

Classiﬁer

Fig. 4. The white bars are the expected classical cross-validation error
rates on the Parkinson’s dataset used by Kaya et al., (2011) for four
classiﬁers. The shaded bars are the estimated error rates by using cross-
validation scheme (14) when the prior probability of Parkinson’s disease
is set to be the incidence rate across the USA. The bars show the averaged
estimated error obtained on 200 samplings of the data

4 CONCLUDING REMARKS

We show in this article that classical cross-validation may display
substantial bias when it is applied in the separate sampling scen-
ario, which is common in biomedical studies. If one wishes to use
cross-validation with separate sampling, then one should use the
separate-sampling version of cross-validation, which is proposed
here, or else, signiﬁcant bias may result. This means that one
must know the prior probability c (at least a good approximation
of it). A similar requirement was made by Esfahani and
Dougherty (2014) to ensure proper performance of the classiﬁ-
cation rule. Using a sampling ratio signiﬁcantly different from c
will result in poor classiﬁer design and, often, optimistic bias to
obscure the poor design. As concluded by Esfahani and
Dougherty (2014), given the ubiquity of separate sampling in
biomedicine, although it would incur some cost, it would be-
hoove the medical community to gather population statistics so
that accurate estimates of prior class probabilities would be
available. In the absence of such statistics, separate sampling
should not be used.

Funding: This work was supported by NSF award CCF-0845407
(Braga-Neto) and NIH grant 2R25CA090301 (Nutrition,
Biostatistics and Bioinformatics) from the National Cancer
Institute.

Conﬂict of interest: none declared.

REFERENCES

Ambroise,C. and McLachlan,G.J. (2002) Selection bias in gene extraction on the
basis of microarray gene-expression data. Proc. Natl Acad. Sci. USA, 99,
6562—6566.

Anderson,T. (1951) Classiﬁcation by multivariate analysis. Psychometrika, 16,
31—50.

 

3354

112 /810'S112umo[pJOJXO'sot112u1101u101q/ﬁd11q 111011 popcolumoq

910K ‘09 lsnﬁnV uo 22

Cross-validation under separate sampling

 

Braga-Neto,U. and Dougherty,E. (2004) Is cross-validation valid for microarray
classiﬁcation? Bioinformatics, 20, 374—380.

Desmedt,C. et al. (2007) Strong time dependence of the 76-gene prognostic signa-
ture for node-negative breast cancer patients in the transbig multicenter inde-
pendent validation series. Clin. Cancer Res., 13, 3207—3214.

Devroye,L. et al. (1996) A Probabilistic Theory of Pattern Recognition. Springer,
New York.

Dietterich,T.G. (2000) An experimental comparison of three methods for construct-
ing ensembles of decision trees: bagging, boosting, and randomization. Mach.
Leam., 40, 139—157.

Duda,R.O. et al. (2000) Pattern Classiﬁcation. Wiley, New York.

Esfahani,M.S. and Dougherty,E.R. (2014) Effect of separate sampling on classiﬁ-
cation accuracy. Bioinformatics, 30, 242—250.

Friedman,N. et al. (1997) Bayesian network classiﬁers. Mach. Leam., 29,
131—163.

Glick,N. (1973) Sample-based multinomial classiﬁcation. Biometrics, 29,
241—256.

Guyon,I. et al. (2002) Gene selection for cancer classiﬁcation using support vector
machines in machine learning. Mach. Leam., 46, 389—422.

Haggar,F.A. and Boushey,R.P. (2009) Colorectal cancer epidemiology: incidence,
mortality, survival, and risk factors. Clin. Colon. Rectal. Surg., 22, 191—197.
Kaya,E. et al. (2011) Effect of discretization method on the diagnosis of parkinsons

disease. Int. J. Innov. Comput. Inf., 7, 4669—4678.

Lachenbruch,P.A. and Mickey,M.R. (1968) Estimation of error rates in discrimin-
ant analysis. Technometrics, 10, 1—11.

Valk,P.J. et al. (2004) Prognostically useful gene-expression proﬁles in acute myeloid
leukemi. N. Engl. J. Med., 350, 1617—1628.

Van Den Eeden,S.K. et al. (2003) Incidence of parkinson’s disease: variation by age,
gender, and race/ethnicity. Am. J. Epidemiol, 157, 1015—1022.

Yeoh,E.J. et al. (2002) Classiﬁcation, subtype discovery, and prediction of outcome
in pediatric acute lymphoblastic leukemia by gene expression proﬁling. Cancer
Cell, 1, 133—143.

Zhan,F. et al. (2006) The molecular classiﬁcation of multiple myeloma. Blood, 108,
2020—2028.

 

3355

112 /810'S112umo[pJOJXO'sot112u1101u101q/ﬁd11q 111011 popcolumoq

910K ‘09 lsnﬁnV uo 22

