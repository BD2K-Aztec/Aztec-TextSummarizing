ORIGINAL PAPER

Vol. 30 no. 17 2014, pages 2423—2431
doi: 10. 1 093/bioinformatics/btu320

 

Sequence analysis

Advance Access publication May 7, 2014

SEK: sparsity exploiting k-mer-based estimation of bacterial

community composition

Saikat Chatterjee1’*, David Koslicki2, Siyuan Dong3’4, Nicolas Innocenti5, Lu Cheng6,

Yueheng Lan7, Mikko Vehkapera1’8, Mikael Skoglund‘, Lars K. Rasmussen‘, Erik Aurel

and Jukka Corander6

I5,9

1Department of Communication Theory, KTH Royal Institute of Technology, Stockholm, Sweden, 2Department of
Mathematics, Oregon State University, Corvallis, OR, USA, 3Systems Biology program, KTH Royal Institute of
Technology, Sweden, 4Aalto University, Esbo, Finland, 5Department of Computational Biology, KTH Royal Institute of
Technology, Stockholm, Sweden, 6Department of Mathematics and Statistics, University of Helsinki, Helsinki, Finland,
7Department of Physics, Tsinghua University, Beijing, China, 8Department of Signal Processing and 9Department of
Information and Computer Science, Aalto University, Esbo, Finland

Associate Editor: John Hancock

 

ABSTRACT

Motivation: Estimation of bacterial community composition from a
high-throughput sequenced sample is an important task in metage—
nomics applications. As the sample sequence data typically harbors
reads of variable lengths and different levels of biological and technical
noise, accurate statistical analysis of such data is challenging.
Currently popular estimation methods are typically time-consuming
in a desktop computing environment.

Results: Using sparsity enforcing methods from the general sparse
signal processing field (such as compressed sensing), we derive a
solution to the community composition estimation problem by a sim-
ultaneous assignment of all sample reads to a pre—processed refer-
ence database. A general statistical model based on kernel density
estimation techniques is introduced for the assignment task, and the
model solution is obtained using convex optimization tools. Further,
we design a greedy algorithm solution for a fast solution. Our ap-
proach offers a reasonably fast community composition estimation
method, which is shown to be more robust to input data variation
than a recently introduced related method.

Availability and implementation: A platform-independent Matlab im-
plementation of the method is freely available at http://www.ee.kth.se/
ctsoftware; source code that does not require access to Matlab is
currently being tested and will be made available later through the
above Web site.

Contact: sach@kth.se

Received on October 4, 2013; revised on April 11, 2014; accepted on
April 30, 2014

1 INTRODUCTION

High-throughput sequencing technologies have recently enabled
detection of bacterial community composition at an unprece-
dented level of detail. The high-throughput approach focuses
on producing for each sample a large number of reads covering
certain variable part of the 16S rRNA gene, which enables an
identiﬁcation and comparison of the relative frequencies of

 

*To whom correspondence should be addressed.

different taxonomic units present across samples. Depending
on the characteristics of the samples, the bacteria involved and
the quality of the acquired sequences, the taxonomic units may
correspond to species, genera or even higher levels of hierarchical
classiﬁcation of the variation existing in the bacterial kingdom.
However, at the same time, the rapidly increasing sizes of read
sets produced per sample in a typical project call for fast infer-
ence methods to assign meaningful labels to the sequence data, a
problem that has attracted considerable attention (Koslicki et al.,
2013; Meinicke et al., 2011; Ong et al., 2013; Wang et al., 2007).

Many approaches to the bacterial community composition
estimation problem use 16S rRNA amplicon sequencing where
thousands to hundreds of thousands of moderate length (around
250—500 bp) reads are produced from each sample and then
either clustered or classiﬁed to obtain estimates of the prevalence
of any particular taxonomic unit. In the clustering approach, the
reads are grouped into taxonomic units by either distance-based
or probabilistic methods (Cai and Sun, 2011; Cheng et al., 2012;
Edgar, 2010), such that the actual taxonomic labels are assigned
to the clusters afterward by matching their consensus sequences
to a reference database. Recently, the Bayesian BeBAC method
(Cheng et al., 2012) was shown to provide high biological ﬁdelity
in clustering. However, this accuracy comes with a substantial
computational cost such that a running time of several days in a
computing-cluster environment may be required for large read
sets. In contrast to the clustering methods, the classiﬁcation
approach is based on using a reference database directly to
assign reads to meaningful units representing biological vari-
ations. Methods for the classiﬁcation of reads have been based
either on homology using sequence similarity or on genomic
signatures in terms of oligonucleotide composition. Examples
of homology-based methods include MEGAN (Huson et al.,
2007; Mitra et al., 2011) and phylogenetic analysis (von
Mering et al., 2007). A popular approach is the Ribosomal
Database Project’s (RDP) classiﬁer, which is based on a naive
Bayesian classiﬁer (NBC) that assigns a label explicitly to each
read produced for a particular sample WVang et al., 2007).
Despite the computational simplicity of NBC, the RDP classiﬁer

 

© The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 2423

112 ﬁle'slcumo[pJOJXO'sopchOJurorq/ﬁd11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

S. Chatterjee et al.

 

may still require several days to process a dataset in a desktop
environment. Given this challenge, considerably faster methods
based on different convex optimization strategies have been re-
cently proposed (Koslicki et al., 2013; Meinicke et al., 2011). In
particular, sparsity-based techniques, mainly compressive sen-
sing-based algorithms (Candes, and Wakin, 2008), are used for
estimation of bacterial community composition in (Amir and
Zuk, 2011; Koslicki et al., 2013; Zuk et al., 2012). However,
(Amir and Zuk, 2011) used sparsity-promoting algorithms to
analyze mixtures of dye-terminator reads resulting from Sanger
sequencing, with the sparsity assumption that each bacterial
community comprises a small subset of known bacterial species,
the scope of the work thus being different from methods in-
tended for high-throughput sequence data. The Quikr method
of (Koslicki et al., 2013) uses a k—mer-based approach on 16S
rRNA sequence reads and has a considerable similarity to the
method (SEK: Sparsity Exploiting K—mers-based algorithm)
introduced here. Explained brieﬂy, the Quikr setup is based
on the following core theoretical formulation: given a reference
database D = {(11, . . . , dM} of sequences and a set S= {$1, . . . , st}
of sample sequences (the reads to be classiﬁed), it is assumed that
there exists a unique  for each s), such that s; =  In general, all
reference databases and sample sets consist of sequences with
highly variable lengths. In particular, the lengths of reference
sequences and samples reads are often different. Violation of
the assumption leads to sensitivity in Quikr performance accord-
ing to our experiments. Another example of fast estimation is
called Taxy (Meinicke et al., 2011), which addresses the effect of
varying sequence lengths Mommack et al., 2008). Taxy uses a
mixture model for the system setting and convex optimization
for a solution. The method referred to as COMPASS (Amir
et al., 2013) is another convex optimization approach, similar
to the Quikr method, that uses large k—mers and a divide-and-
conquer technique to handle large resulting training matrices.
The currently available version of the Matlab-based
COMPASS software does not allow for training with custom
databases, so a direct comparison with SEK is not yet possible.

To enable fast estimation, we adopt an approach where the
estimation of the bacterial community composition is performed
jointly, in contrast to the read-by—read analysis used in the RDP
classiﬁer. Our model is based on kernel density estimators and
mixture density models (Bishop, 2006), and it leads to solving an
under-determined system of linear equations under a particular
sparsity assumption. In summary, the SEK approach is imple-
mented in three separate steps: off-line computation of k—mers
using a reference database of 16S rRNA genes with known taxo-
nomic classiﬁcation, online computation of k—mers for a
given sample and then ﬁnal online estimation of the relative
frequencies of taxonomic units in the sample by solving an
under-determined system of linear equations.

2 METHODS

2.1 General notation and computational resources used

We denote the non-negative real line by R4,. The 5,, norm is denoted
by ||.||p, and [E[.] denotes the expectation operator. Transpose of a vector/
matrix is denoted by (.)’. We denote cardinality and complement of a set
S by IS I and S, reSpectively. In the computations reported in the remain-
der of the article, we used standard Matlab software with some instances

of C code. For experiments on mock community data, we used a Dell
Latitude E6400 laptop computer with a 3 GHz processor and 8 GB
memory. We also used the cvx (Boyd and Vandenberghe, 2004) convex
optimization toolbox and the Matlab function lsqnonnegO for a least-
squares solution with non-negativity constraint. For experiments on
simulated data, we used standard computers with an Intel Xeon x5650
processor and an Intel i7-4930K processor.

2.2 k-mer training matrix from reference data

The training step of SEK consists of converting an input labeled database
of 16S rRNA sequences into a k—mer training matrix. For a ﬁxed k, we
calculate k—mers feature vectors for a window of ﬁxed length, such that
the window is shifted (or slid) by a ﬁxed number of positions over a
database sequence. This procedure captures variability of localized
k-mer statistics along 16S rRNA sequences. Using bp as the length unit
and denoting the length of a reference database sequence d by Ld, and
further a ﬁxed window length by Lw 5 Ld and the ﬁxed position shift by
LP, the total number of subsequences processed to k—mers is close to
H2419]. The choice of Lw may be decided by the shortest sample sequence
length that is used in the estimation, assuming the reads in a sample set
are always shorter than the reference training sequences. In practice, for
example, we used Lw = 450 bp in experiments using mock communities
data. The choice of L1,, is decided by the trade-off between computational
complexity and estimation performance.

Given a database of reference training sequences D = {d1,. . . , dM}
where dm is the sequence of the mth taxonomic unit, each sequence dm
is treated independently. For dm, the k—mer feature vectors are stored
column-wise in a matrix Xm e RfXN'", where Nm N LMJ. From the

L
training database D, we obtain the following full training matrix

x=[x1x2 ...,xM] eIR‘fXN,
E [X1 X2 ...XN],

M
where m: Nm =N, and X” e Rf“ denotes the nth k—mers feature
vector in the full set of training feature vectors X.

2.3 SEK model

For the mth taxonomic unit, we have the training set
k
Xm =   o o o  E  ,

where we used an alternative indexing to denote the lth k—mer feature
vector by xml. Letting x and Cm denote random k—mer feature vectors and
mth taxonomic unit, respectively, and using Xm, we ﬁrst model the con-
ditional density p(x|Cm) corresponding to mth unit by a mixture density
as follows:

Nm
17(chm) = 2 05ml pml(X|Xml, ®ml)’ (1)
[=1

where am; 2 0,  am; = 1, xml is assumed to be the mean of distribu-
tion pm], and Om, denotes the other parameters/properties apart from the
mean. In general, pm, could be chosen according to any convenient para-
metric or non-parametric family of distributions. In biological terms, am,
reﬂects the ampliﬁcation of a variable sequence region and how probable
that is in a given dataset with a sufﬁcient level of coverage. The approach
of using training data xm, as the mean of pm, stems from a standard
approach of using kernel density estimators [see Section 2.5.1 of
(Bishop, 2006)].

Given a test set of k—mers (computed from reads), the distribution of
the test set is modeled as follows:

M
1900 = 219%) p(XICm),
m=1

 

2424

112 [glO'SIBILInO[p.IOJXO'SODBIIIJOJIIIOIQ/ﬂdllq IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

SEK

 

where we denote prﬂqbability for taxonomic unit m (or class weight) by
p(Cm), satisfying 2m: 1 p(Cm) = 1. {p(C,,,)}nj‘f=1 is the composition of taxo-
nomic units. The inference task is to estimate p(Cm) as accurately and
fast as possible, for which a ﬁrst order moment matching approach is
developed. We ﬁrst evaluate the mean of K under p(x) as follows:

[E[x]

kX
=fxp(x)dxe|Ri1

M
Zp(cm)fxp(xicm)dx
m=1

“Pg:

Nm
 2am] pml(X|Xml, ®ml) dx
1 [=1

m

“Pg:

Nm
mam): amlf x pmrxixml, 9..) dx
l I: l

m
M Nm

2 p(Cm)Zaml Xml-
m= 1 l= 1

. . . —1 .
Introducmg a new 1ndex1ng n én(m, l) =  N]- + l, we can wr1te

N
EIXI=ZVn x. =Xy,

n—l

where

V=IV1V2 oo-mNI‘ERZX“,

A (2)
7’11 2 Vn(m,l) =p(Cm)aml,

with the following properties

"(mst) Nm

2 Vn =p(Cm)Zaml =p(Cm),

n(m, 1) l= 1

N
2%: IIVI|1 =1.
n=1

In our approach, we use the sample mean of the test set. The test set
consists of k—mers feature vectors computed from reads. Each read is
processed individually to generate k—mers in the same manner used for
the reference data. We compute sample mean of the k—mer feature vectors
for test dataset reads. Let us denote the sample mean of the test dataset
by ,u 6 RE“, and assume that the number of reads is reasonably high
such that ,u % [E[x]. Then we can write

,u % Xy.
Considering that model irregularities are absorbed in an additive noise
term 11, we use the following system model
M=xy+nentfx1 (3)
Using the sample mean ,u and knowing X, we estimate y from (3) as
yew, )72 ..., my 6 IRIXX1 followed by estimation of p(Cm) as

n(m,Nm)
13(cm)= Z r...

n(m,1)
The estimation )7 e IRZZX"1 must satisfy the following constraints

)7 z 0,
N M
Iii/Ill: n=Zﬁ(Cm)=1
l m=l

n:

(4)

In (4), )7 Z 0 means Vn, )7” Z 0. We note that the linear setup (3) is under-
determined as 4k<N (in practice 4" << N) and hence, in general, solving

(3) without any constraint will lead to inﬁnitely many solutions. The
constraints (4) result in a feasible set of solutions that is convex and
can be used for ﬁnding a unique and meaningful solution.

We recall that the main interest is to estimate p(Cm), which is achieved
in our approach by ﬁrst estimating y and then p(Cm). Hence, y represents
an auxiliary variable in our system.

2.4 Optimization problem and sparsity aspect

The solution of (3), denoted by )7, must satisfy the constraints in (4).
Hence, for SEK, we pose the optimization problem to solve as follows:

P5221 =9= arg min IIM — Xyllz, y z 0, ||V||1 =1 (5)
y

where ‘ + ’ and ‘1’ notations in P542; refer to the constraints )7 e R1): and
||;‘/||1 =1, respectively. The problem P5121? is a constrained least squares
problem and a quadratic program (QP) solvable by convex optimization
tools, such as cvx (CVX). In our assumption 4" < N, and hence the
required computation complexity is 0(N3) (Boyd and Vandenberghe,
2004).

The form of P54; bears resembance to the widely used LASSO method
from general sparse signal processing, mainly used for solving under-
deterrnined problems in compressive sensing (Candes and Wakin, 2008;
Chatterjee et al., 2012). LASSO deals with the following optimization
problem [see (1.5) of (Effron et al., 2004)]:

LAsso:;‘/1,,,o= arg min IIIL — XV||2,||V||1 S T
y

where ‘L’ e IR+ is a user choice that decides the level of sparsity in ﬁlm);
for example, ‘L’ = 1 will lead to a certain level of sparsity. A decreasing ‘L’
leads to an increasing level of sparsity in LASSO solution. LASSO is
often presented in an unconstrained Lagrangian form that minimizes
{llu — Xy||§ +A||y||1}, where A decides the level of sparsity. P5121? is not
theoretically bound to provide a sparse solution with a similar level of
sparsity achieved by LASSO when a small ‘L’< 1 is used.

For the community composition estimation problem, the auxiliary
variable y deﬁned in (2) is inherently sparse. Two particularly natural
motivations concerning the sparsity can be brought forward. Firstly, con-
sider the conditional densities for taxonomic units as shown in (1).
Regarding the conditional density model for a single unit, a natural hy-
pothesis for the generating model is that the conditional densities for
several other units will induce only few feature vectors, and hence am,
will be negligible or effectively zero for certain patterns in the feature
space, leading to sparsity in the auxiliary variable y (unstructured sparsity
in y). Secondly, in most samples only a small fraction of the possible
taxonomic units is expected to be present, and consequently, many p(Cm)
will turn out to be zero, which again corresponds to sparsity in y (struc-
tured block-wise sparsity in y) (Stojnic, 2010). In practice, for a highly
under-determined system (3) in the community composition estimation
problem with the fact that y is inherently sparse, the solution of P5121;1
turns out to be effectively sparse because of the constraint |y||1 = 1.

2.5 A greedy estimation algorithm

For SEK we solve P5452 using convex optimization tools requiring com-
putational complexity 0(N3). To reduce the complexity without a signiﬁ-
cant loss in estimation performance we also develop a new greedy
algorithm based on orthogonal matching pursuit (0MP) (Tropp and
Gilbert, 2007); for a short discussion of 0MP with pseudo-code, see
also (Chatterjee et al., 2012). In the recent literature, several algorithms
have been designed by extending OMP, such as, for example, the back-
tracking-based OMP (Huang and Makur, 2011), and, by a subset of the
current authors, the look-ahead 0MP (Chatterjee et al., 2011). Because
the standard OMP uses a least-squares approach and does not provide
solutions satisfying constraints in (4), it is necessary to design a new
greedy algorithm for the problem addressed here.

 

2425

112 [glO'SIBILInO[p.IOJXO'SODBIIIJOJIIIOIQ/ﬂ(11111 IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no 22

S. Chatterjee et al.

 

The new algorithm introduced here is referred to as OMP;k’1, and its
pseudo-code is shown in Algorithm 1. In the stopping condition (step 7),
the parameter v is a positive real number that is used as a threshold, and the
parameter I is a positive integer that is used to limit the number of iterations.

The choice of v and I is ad hoc, depending mainly on user experience.

 

Algorithm 1: 0MP;,;1

Input:

1: X, ,u, v, I;

Initialization:

1:r0<—,u,So<—Q, i<—0;

Iterations:

1: repeat

2: i <— i + 1; (Iteration counter)
t, <— index of the highest positive element of Xtr,-_1;

Si <— Si—l U Ti; k _ (ISiI =1)
t.- <— argﬂmin IIM — Kan-“2,18.- 2 0; (X5. E R1“)

 



6: r,- <— u — m, (Residual)
7Illnti1((llli7||1-1IS v) or (i Z 1))
Output:

1: )7 E R1, satisfying ﬁst. =)7,- and )7; =0.

2: )7 <— (Enforcing ||)7||1 =1)

 

||i7||1

 

Compared with the standard 0MP, the new aspects in OMPS‘Q;1 are as

follows:

0 In Step 3 of Iterations, we only search within positive inner product
coefﬁcients.

0 In Step 5 of Iterations, a least-squares solution )7,- with non-negativity
constraint is found for ith iteration via the use of intermediate vari-
able ,8,- e Rifl. In this step, X5, is the sub-matrix formed by columns
of X indexed in 8,. The concerned optimization problem is convex.
We used the Matlab function lsqnonnegO for this purpose.

0 In Step 6 of Iterations, we ﬁnd the least squares residual r,-.

o In Step 7 of Iterations, the stopping condition provides for a solution
that has an £1 norm close to one, with an error decided by the
threshold v. An unconditional stopping condition is provided by
the maximum number of iterations I.

o In Step 2 of Output, the £1 norm of the solution is set to one by a
rescaling.

The computational complexity of the OMPSJ‘QI;1 algorithm is as follows.
The main cost is incurred at Step 5 where we need to solve a linearly
constrained QP using convex optimization tools; here we assume that the
costs of the other steps are negligible. In the ith iteration X51. 6 IRin and
i << 4", and the complexity required to solve Step 5 is 0(4ki2) (Boyd and
Vandenberghe, 2004). As we have a stopping condition is I, the total
complexity of the OMPSE;1 algorithm is within 0(1 x 4‘12) = 0(4kI3). We
know that optimal solution of P54; using convex optimization tools re-
quires a complexity of 0(N3). For a setup with I < 4" << N, we can have
0(4kI3) << 0(N3), and hence the OMPSQ;1 algorithm is typically much
more efﬁcient than using convex optimization tools directly in a high-
dirnensional setting. It is clear that the OMPSJ‘QI;1 algorithm is not allowed
to iterate beyond the limit of I; in practice, this works as a forced con-
vergence. For both OMPSJ‘QI;1 and P3212, we do not have a theoretical
proof on robust reconstruction of solutions. Further, a natural question
remains on how to set the input parameters 1) and I. The choice of par-
ameters is discussed later in Section 3.4.

2.6 Overall system ﬂowchart

Finally, we depict the full SEK system by using a ﬂowchart shown in
Figure 1. The ﬂowchart shows main parts of the overall system and
associated off-line and online computations.

Reads Reference

(test sequences) (training sequences)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Pt“1 or 0MP+‘1

sek sek

 

 

 

 

User choice:

17

Computing composition

 

 

Window length Lw

 

 

 

 

: : : s: -
I I I  I
I k-mers I . k-mers <5 I
: generation : : generation a E
I l I I a -
I I l O I
I I I o :
I I I D I
: Sample mean u : : E I
: g computation I : X matrix E :
I '5 i I creation ‘H '
I S I I O '
I 3 M I I '
I 9-1 I ‘ - - - - - - - u - - - - - - - - p.
I E . . . :
. 8 Fmdmg 7 us1ng I X
I

o
I g I
I Is I
I o I
I I
I I
I I
I I
I I
I I
I I
I I

 

 

 

proportion p(Cm) Window shift L,
i k of k-mers
\ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ¢
Composition proportion

Fig. 1. A ﬂowchart of full SEK system

2.7 Mock communities data

For our experiments on real biological data, we used the mock microbial
communities database developed in (Haas et al., 2011). The database is
called even composition Mock Communities (eMC) for chimeric
sequence detection where the involved bacterial species are known in
advance. Three regions W1—V3, V3—V5, V6—V9) of the 16S rRNA gene
of the composition eMC were sequenced using 454 sequencing technology
in four different sequencing centers. In our experiments, we focused on
the V3—V5 region datasets, because these have been earlier used for evalu-
ation of the BeBAC method [see Experiment 2 of (Cheng et al., 2012)].

2. 7.1 Test dataset (Reads) Our basic test dataset used under a var-
iety of different in silico experimental conditions is the one used in
Experiment 2 of BeBAC (Cheng et al., 2012). The test dataset consists
of 91240 short length reads from 21 different species. The length of reads
has a range between 450 and 550 bp, and the bacterial community com-
position is known at the species level, by the following computation per-
formed in (Cheng et al., 2012). Each individual sequence of the 91240
read sequences was aligned (local alignment) to all the reference se-
quences of reference database Dig; described in the Section 2.7.2 and
then each read sequence is labelled by the species of the highest scoring
reference sequence, followed by computation of the community compos-
ition referred to as ground truth.

2. 7.2 Training datasets (Reference) We used two different data-
bases (known and mixed) generated from the mock microbial community
database (Haas et al., 2011). The ﬁrst database is denoted by Dig; and
it consists of the same M = 21 species present among the reads described
in Section 2.7.1. The details of the Dgﬁflgn database can be found in
Experiment 2 of (Cheng et al., 2012). The database consists of 113 refer-
ence sequences for a total of 21 bacterial species, such that each reference
sequence represents a distinct 16S rRNA gene. Thus, there is a varying
number of reference sequences for each of the considered species. Each
reference sequence has a length of ~1500 bp, and for each species, the
corresponding reference sequences are concatenated to a single sequence.
The ﬁnal reference database Dugoggn then consists of 21 sequences where
each sequence has a length of ~5000 bp.

To evaluate inﬂuence of new species in reference data on the perform-

ance of SEK, we created new databases denoted by DInOCk (E). Here E

mixed

 

2426

112 [3.10811211an[p.IOJXO'SODBIIIJOJIIIOIQ/ﬂdllq IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

SEK

 

represents the number of additional species included to a partial database
created from Dmlgn, by downloading additional reference data from the
RDP database. Each partial database includes only one randomly chosen
reference sequence for each species in Dﬁﬁgﬁn and hence consists of 21
reference sequences of a length of ~1500 bp. For example, with E = 10,
10 additional species were included in the reference database and conse-
quently Dgi‘glfiﬂO) contains 16S rRNA sequences of M = 21 + 10 = 31
species. Several instances of Dgi‘glfiw) were made for each ﬁxed value of
E by choosing a varying set of additional species and we also increased E
from 0 to 100 in steps of 10. Note that, in DEngﬁE), the inclusion of only
single reference sequence results in reduction of biological variability for

each of the original 21 species compared with DIIIOCk

known °

2.8 Simulated data

To evaluate how SEK performs for much larger data than the mock
communities data, we performed experiments for simulated data
described below.

2.8.] Test datasets (Reads) Two sets of simulated data were used to
test the performance of the SEK method. First, the 216 different simu-
lated datasets produced in (Koslicki et al., 2013) were used for a direct
comparison with the Quikr method and the RDP’s NBC. See [(Koslicki
et al., 2013), Section 2.5] for the design of these simulations.

The second set of simulated data consists of 486 different pyrosequen—
cing datasets constituting >179M reads generated using the shotgun/
amplicon read simulator Grinder (Angly et al., 2012). Read-length distri-
butions were set to be one of the following: ﬁxed at 100 bp, normally
distributed at 450 :I: 50 bp, or normally distributed at 800 :I: 100 bp. Read
depth was ﬁxed to be one of 10K, 100K or IM total reads. Primers were
chosen to target either only the V1—V3 regions, only the V6—V9 regions or
else the multiple variable regions V1—V9. Three different diversity values
were chosen (10, 100 and 500) at the species level, and abundance was
modeled by one of the following three distributions: uniform, linear or
power-law with parameter 0.705. Homopolymer errors were modeled
using Balzer’s model (Balzer et al., 2010), and chimera percentages
were set to either 5 or 35%. As only amplicon sequencing is considered,
copy bias was used, but not length bias.

2.8.2 Training datasets (Reference) To analyze the simulated data,
two different training matrices were used corresponding to the databases
Dsmau and Dlarge from (Koslicki et al., 2013). The database Dsmau is iden-
tical to RDP’s NBC training set 7 and consists of 10 046 sequences cover-
ing 1813 genera. Database Dlarge consists of a 275 727 sequence subset of
RDP’s 16S rRNA database covering 2226 genera. Taxonomic informa-
tion was obtained from NCBI.

3 RESULTS

3.1 Performance measure and competing methods

As a quantitative performance measure, we use variational dis-
tance W D) to compare between known proportions of taxo-
nomic units p=[p(Cl), p(Cz), ...,p(CK)]T and the estimated
proportions p=[ﬁ(Cl), 13(62),...,13(CK)]T. The VD is deﬁned
as follows:

VD=0-5 X lip-ﬁlli E [0, 1]

A low VD indicates more satisfactory performance.

We compare performances between SEK, Quikr, Taxy and
RDP’s NBC, for real biological data (mock communities data)
and large-size simulated data.

3.2 Results for mock communities data

Using mock communities data, we carried out experiments where
the community composition problem is addressed at the species
level. Here we investigated how the SEK performs for real bio-
logical data, also vis-a-vis relevant competing methods.

3.2.] k—mers from test dataset In the test dataset, described in
Section 2.7.1, the shortest read is of length 450 bp. We used a
window length LW = 450 bp and refrained from the sliding-the-
window approach in the generation of k-mers feature vectors.
For k = 4 and k = 6, the k-mers generation took 21 and 48 min,
respectively.

3.2.2 Results using small training dataset In this experiment, we
used SEK for estimation of the proportions of species in the test
set described in Section 2.7.1. Here we used the smaller training
reference set Dinnogvlﬁn described in Section 2.2. The experimental
setup is the same as shown in Experiment 2 of BeBAC (Cheng
et al., 2012). Therefore, we can directly compare it with the
BeBAC results reported in (Cheng et al., 2012). SEK estimates
were based on 4-mers computed with the setup Lw = 450 bp and
LP = lbp. The choice of LP = lbp corresponds to the best case
of generating training matrix X, with the highest amount of vari-
ability in reference k—mers. Using Dinnocfgn, the k-mers training
matrix X has the dimension 44 x 121412. For the use of SEK
in such a high dimension, the QP P59? using cvx suffered of
numerical instability, but OMPS‘EI;1 provided results in 3.17s,
leading to a VD = 0.0305. For 0MP;1;1, v and I in algorithm
1 were set to 10‘5 and 100, respectively; the values of these two
parameters remained unchanged for other experiments on mock
communities data presented later. The performance of SEK
using OMPstl;1 is shown in Figure 2, and compared against the
estimates from BeBAC, Quikr and Taxy. The Quikr method
used 6-mers and provided a VD = 0.4044, whereas the Taxy
method used 7-mers and provided a VD = 0.2817. The use of
k = 6 and k = 7 for Quikr and Taxy, respectively, is chosen ac-
cording to the experiments described in Koslicki et al. (2013) and
Meinicke et al. (2011). Here Quikr is found to provide the least
satisfactory performance in terms of VD. BeBAC results are
highly accurate with VD = 0.0038, but come with the require-
ment of a computation time in the order of >30 h. On the other
hand OMPSQI;1 had a total online computation time around
21 min that is mainly dominated by k—mers computation from
sample reads for evaluating ,u; given pre-computed X and ,u, the
central inferenece (or estimation) task of OMPS‘EI;1 took only
3.17s. Considering that Quikr and Taxy also have similar
online complexity requirement to compute k—mers from sample
reads, OMPstl;1 can be concluded to provide a good trade-off
between performance and computational demands.

3.2.3 Results for dimension reduction by higher shifts The
Lp = lbp leads to a relatively high dimension of X, which is
directly related to an increase in computational complexity.
Clearly, the LP = lbp shift produces highly correlated columns
in X, and consequently it might be sufﬁcient to use k-mers fea-
ture vectors with a higher shift without a considerable loss in
variability information. To investigate this, we performed an ex-
periment with a gradual increase in LP. We found that selecting

LP = 15 bp results in an input X e RTXSOSZ, which the cvx-based

 

2427

112 [3.10811211an[p.IOJXO'SODBIIIJOJIIIOIQ/ﬂ(11111 IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

S. Chatterjee et al.

 

 

0-3 I I I I I I I I I I

 

 

- Ground Truth

01. A.baumannii
- BeBAC 02. A.odontolyticus

- Qu'kr 03. B.cereus
- Taxy + 1 O4. B.vulgatus
- SEKIOMPsgk 05. C.beijerinckii
06. D.radiodurans
O7. E.coli

08. E.faecalis

09. H.pylori

10. L.gasseri

02 _ 11. L.monocytogenes _
12. M.smithii

13. N.meningitidis
14. P.acnes

15. P. aeruginosa
16. R.sphaeroides
17. S.agalactiae
18. S.aureus -
19. S.epidermidis
20. S.mutans

21. S.pneumoniae

 

 

 

proportion
O
a:
|

 

 

 

 

 

 

0 2 4 e 8 10 12 14 16 18 20 22
species

Fig. 2. For mock communities data: performance of OMPSJ‘QI;1 using reference training database  Community composition problem is addressed
at the species level. The OMPSJ‘QI;1 performance is shown against the ground truth and performances of BeBAC, Quikr and Taxy. The OMPSQ;1 provides
better match to the ground truth than the competing faster methods Quikr and Taxy. The corresponding VD performances of BeBAC, OMPSE;1 , Taxy
and Quikr are 0.0038, 0.0305, 0.2817 and 0.4044, respectively

P5461;1 was able to process successfully. At Lp = 15bp, the P5463;1 and 0MP;1;1- A130, being Optimal the Performance Of QP PSI}? is
provided a performance of VD = 0.033260, while the execution found to be more cenSiStent than the greedy 0MP;1é1-

time was 25.25 s. The OMPS‘EI;1 took 1.86 s and provided

VD = 0.033551, indicating almost no performance loss compared 3-3 Results for Simulated data

with the optimal PS:I;I. A shift LP > 25 did result in a performance The simulated data experiments deal with community compos-

drop, for example, LP = 30, 50, 100 resulted in VD values 0.0527,
0.0879, 0.1197, respectively. Therefore, shifts around LP = 15bp
appear to be sufﬁcient to reduce the dimension of X, while main-
taining sufﬁcient biological variability. Hence the next experi-
ment (in Section 3.2.4) was conducted using LP = 15 bp.

3.2.4 Results for mixed training dataset In this experiment, we
investigated how the performance of SEK varies with an increase
in the number of additional species in the reference training data-
base, which are not present in the sample test data. We used
reference training datasets Dggggw) described in Section 2.2,
where E = 0,10,20, . . ., 100. For each non-zero E, we created
10 reference datasets to evaluate variability of the performance.
The performance with one-sigma error bars is shown in Figure 3.
The trend in the performance curves conﬁrms that the SEK is
subjected to gradual decrease in performance with the increase in
the number of additional species; the trend holds for both P5463?

ition problem at different taxonomic ranks and also with large
size of X in (3). Owing to the massive size of X, a direct appli-
cation of QP P59? is not feasible, and hence we used only
OMP;1;1. For all results described, I) and I in algorithm 1 were
set to 10‘5 and 409, respectively.

3.3.] Training matrix construction In forming the training
matrix for Dsmau, the lC-mer size was ﬁxed at [C = 6, and the
window length and position shifts were set to Lw = 400 and
LP = 100, respectively. This resulted in a matrix X with dimen-
sions 46 x 109 773. For the database Dlarge, a training matrix X
with dimensions 46 x 500 734 was formed by ﬁxing k = 6,
Lw = 400 and LP = 400. Calculating the matrices took ~2.5
and ~11min, respectively, using an Intel i7-4930K processor
and a custom C program. Slightly varying LP and Lw did not
signiﬁcantly change the results contained in Sections 3.3.2 and
3.3.3 below, but generally decreasing LP and Lw results in lower
reconstruction error at the expense of increased execution time

 

2428

112 ﬁle'sleumo[pJOJXO'sopemJOJqutq/ﬁd11q IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

SEK

 

 

 

 

 

I I I I I I I I I I
o 10 20 30 4o 50 60 7o 80 90 100 110
Number of extra new species in reference (E)

 

 

 

 

 

 

I I I I I I I I I I
o 10 20 30 4o 50 60 7o 80 90 100 110
Number of extra new species in reference (E)

Fig. 3. For mock communities data: VD performance of SEK against increasing reference database DE?)Ck (E), where E = 0,10,20, . . . , 100. The left

1xed

ﬁgure is for Ps‘aél and the right ﬁgure is for OMPSES. The results show that both SEK implementations are subjected to a gradual decrease in

performance with the increase in the number of additional species

and memory usage. The values of LP and Lw were chosen to
provide an acceptable balance between execution time, memory
usage and reconstruction error.

3.3.2 Results for first set of simulated data For test
data, lC-mers were computed in the same manner as described
in Section 3.3.1. On average, 4.0s were required to form the 6-
mer feature vector for each sample. Figure 4 compares the mean
VD error at various taxonomic ranks as well as the algorithm
execution time between SEK (OMP;1;1), Quikr and RDP’s NBC.

As shown in Figure 4, using the database Dlarge, SEK outper-
forms both Quikr and RDP’s NBC in terms of reconstruction
error and has comparable execution time as Quikr. Both Quikr
and SEK have signiﬁcantly lower execution time than RDP’s
NBC. Using the database Dsmau (not shown here), SEK con-
tinues to outperform both Quikr and RDP’s NBC in terms of
reconstruction error, but only RDP’s NBC in terms of execution
time, as SEK had a median execution time of 15.2 min versus
Quikr’s 25s. All three methods have increasing error for lower
taxonomic ranks, but the improvement of SEK over Quikr is
emphasized for lower taxonomic ranks.

3.3.3 Results for second set of simulated data Figure 5 summar-
izes the mean VD and algorithm execution time over the second
set of simulated data described in Section 2.8 for Quikr and SEK
both trained on Dsmau.

Part (a) of Figure 5 demonstrates that SEK shows much lower
VD error in comparison with Quikr at every taxonomic rank.
However, part (b) of Figure 5 shows that this improvement
comes at the expense of moderately increased mean execution
time.

When focusing on the simulated datasets of length 100 bp,
450:I:50 bp and 800:I: 100 bp, SEK had a mean VD of 0.803,
0.410 and 0.436, respectively. As Lw was set to 400, this indicates
the importance of choosing Lw to roughly match the sequence
length of a given sample when forming the k—mer training matrix
if sequence length is reasonably short (~400 bp).

 

 

 

 

(a)
0.5 -
0.4 -
E 0.3 -
0.2 -
0.1 -
0.0 - . . . . .
Phylum Class Order Family Genus
Taxanomic Rank
 RDP's NBC — Quikr(D1arge)
"' SEK (Dlarge)
 I I I I I I I I I I I I I I I I I I I I I I I I I I I I

RDP's NBC

Median=6.12 hours

 (D large) _

Median: 16.5 minutes

SEK (Dlarge) _
Median=20.9 minutes

IOHI2H'4H'6H'8H10 12
Hours

 

 

 

Fig. 4. For simulated data: comparison of SEK (OMPSES) with Quikr
and RDP’s NBC on the ﬁrst set of simulated data. Throughout, RDP’s
NBC version 10.28 with training set 7 was used. (a) VD error averaged
over all 216 simulated datasets versus taxonomic rank for RDP’s NBC,
with SEK and Quikr trained using Dlarge, 0)) Algorithm execution time
for RDP’s NBC, with SEK and Quikr trained using Dlarge, Whiskers
denote range of the data, vertical black bars designate the median and
the boxes demarcate quantiles

 

2429

112 ﬁle'sleumo[pJOJXO'sopemJOJqutq/ﬁd11q IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

S. Chatterjee et al.

 

(a) 0.8-

0.6 -

 

 

 

 

 

0.2 -
0.0 - , , . . .
Phylum Class Order Family Genus
Taxanomic Rank
— Quikr (Dsamll)
"' SEK (Dsmall)
(b)
Quikr (D small) _
Median=3.3 seconds
SEK (Dsmall)
Median=5 .3 minutes

 

 

 

 

Minutes

Fig. 5. For simulated data: comparison of SEK (OMPSEQ) with Quikr on
the second set of simulated data. (a) VD error averaged over all 486
simulated datasets versus taxonomic rank for SEK and Quikr trained
using Dsmau' 0)) Algorithm execution time for SEK and Quikr trained
using Dsmau, Whiskers denote range of the data, vertical black bars des-
ignate the median and the boxes demarcate quantiles

SEK somewhat experienced decreasing performance as a func-
tion of diversity: at the genus level, SEK gave a mean VD of
0.467, 0.579 and 0.603 for the simulated datasets with diversity
10, 100 and 500, respectively.

3.4 Remarks on parameter choice and errors

In SEK, we need to choose several parameters: k, Lw, LP, 1) and I.
Typically an increase in k leads to better performance with the
fact that a higher k always subsumes a lower k in the process of
generating k—mers feature vectors. The trend of improvement in
performance with increase of k was shown for Quikr Koslicki
et al. (2013) and we believe that the same trend will also hold for
SEK. For SEK, the increase in k results in exponential increase
in row dimension of X matrix and hence the complexity and
memory requirement also increase exponentially. There is no
standard approach to ﬁx k, except a brute force search. Let us
now consider choice of Lw and LP. Our experimental results
bring the following heuristic: choose Lw to match the read
length of sample data. On the other hand, choose LP as small
as possible to accommodate a high variability of k—mers infor-
mation in X matrix. A reduction in LP results to a linear increase

in column dimension of X. Overall users should choose k, Lw and
LP such that the dimension of X remains reasonable without
considerable loss in estimation performance. Finally, we consider
I) and I parameters in Algorithm 1 that enforce sparsity, with the
aspect that computational complexity is (9(4kI3). In general,
there is no standard automatic approach to choose these two
parameters, even for any standard algorithm. For example, the
unconstrained Lagrangian form of LASSO mentioned in section
2.4 also needs to set the parameter A by user. For Algorithm 1,
0< v<1 should be chosen as a small positive number and I can
be chosen as a fraction of row dimension of X that is 4k, of
course with the requirement that I is a positive integer. Let us
choose I = In x 4"] where 0<n51. In case of a lower k, the
system is more under-determined and naturally the enforcement
of sparsity needs to be slackened to achieve a reasonable estima-
tion performance. Hence for a lower k, we need to choose a
higher 17 that can provide a good trade-off between complexity
and estimation performance. But, for a higher k, the system is
less under-determined, and to keep the complexity reasonable,
we should choose a lower 17. For mock communities date, we
used k = 4 and I = 100, and hence n: 1% w 0.4, and for simu-
lated data, we used k = 6 and I = 409, and hence 17 = % w 0.1.
Further, it is interesting to ask what are the types of errors
most common in SEK reconstruction. In general, SEK recon-
structs the most abundant taxa with remarkable ﬁdelity. The less
abundant taxa are typically more difﬁcult to reconstruct and at
times each behavior can be observed: low frequency taxa missing,
miss-assigned or their abundances miss-estimated.

4 DISCUSSION AND CONCLUSION

In this article, we have shown that bacterial compositions of
metagenomic samples can be determined quickly and accurately
from what initially appears to be incomplete data. Our method
SEK uses only k—mer statistics of ﬁxed length (here k~4,6) of
reads from high-throughput sequencing data from the bacterial
16S rRNA genes to ﬁnd which set of tens of bacteria are present
out of a library of hundreds of species. For a reasonable size of
reference training data, the computational cost is dominated by
the pre-computing of the k—mer statistics in the data and in the
library; the computational cost of the central inference module is
negligible, and can be performed in seconds/minutes on a stand-
ard laptop computer.

Our approach belongs to the general family of sparse sig-
nal processing where data sparsity is exploited to solve under-
determined systems. In metagenomics, sparsity is present on sev-
eral levels. We have used the fact that k—mer statistics computed
in windows of intermediate size vary substantially along the 16S
rRNA sequences. The number of variables representing the
amount of reads assumed to be present in the data from each
genome and from each window is thus far greater than the
number of observations, which are the k—mer statistics of all
the reads in the data taken together. More generally, although
many bacterial communities are rich and diverse, the number of
species present in, for example, the gut of one patient, will almost
always be only a small fraction of the number of species present
at the same position across a population, which in turn will only
be a small fraction of all known bacteria for which the genomic
sequences are available. We therefore believe that sparsity is a

 

2430

112 ﬁle'sleumo[pJOJXO'sopemJOJqutq/ﬁd11q IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

SEK

 

rather common feature of metagenomic data analysis that could
have many applications beyond the ones pursued here.

The major technical problem solved in the present article stems
from the fact that the columns of the system matrix X linking
feature vectors are highly correlated. This effect arises both from
the construction of the feature vectors, i.e. that the windows are
overlapping, and from biological similarity of DNA sequences
along the 16S rRNA genes across a set of species. An additional
technical complication is that the variables (species abundances)
are non-negative numbers and naturally normalized to unity,
although in most methods of sparse signal processing there are
no such constraints. We were able to overcome these problems
by constructing a new greedy algorithm based on OMP modiﬁed
to handle the positivity constraint. The new algorithm, dubbed
OMP;1;1, integrates ideas borrowed from kernel density estima-
tors, mixture density models and sparsity-exploiting algebraic
solutions.

During the article preparation, we became aware that a similar
methodology (Quikr) has been developed by Koslicki et al.
(2013). Although there is a considerable similarity between
Quikr and SEK, we note that Quikr is based only on sparsity-
exploiting algebraic solutions, while SEK further exploits the
additional sparsity assumption of non-uniform ampliﬁcations
of variable regions in 16S rRNA sequences. We hypothesize
that the improvement of SEK over Quikr is mainly because of
the superior training method of SEK. The comparison between
the two methods reported above in Figures 2, 4 and 5 shows that
SEK performs generally better than Quikr. The development
of two new methodologies independently and roughly simultan-
eously reﬂects the timeliness and general interest of sparse pro-
cessing techniques for bioinformatics applications.

Funding: Erasmus Mundus scholar program of the European
Union (Y.L.), by the Academy of Finland through its Finland
Distinguished Professor program grant project 129024/Aurell (to
E.A.), ERC grant 239784 (to J .C.) and the Academy of Finland
Center of Excellence COIN (to EA. and J .C.), by the Swedish
Research Council Linnaeus Centre ACCESS (to E.A., M.S.,
L.R., SC. and M.V) and by the Ohio Supercomputer Center
and the Mathematical Biosciences Institute at The Ohio State
University (to D.K.).

Conflict of interest: none declared.

REFERENCES

Amir,A. et al. (2013) High-resolution microbial community reconstruction by inte-
grating short reads from multiple l6S rRNA regions. Nucleic Acids Res., 41, e205.

Amir,A. and Zuk,O. (2011) Bacterial community reconstruction using compressed
sensing. J. Comput. Biol, 18, 1723—1741.

Angly,F.E. et al. (2012) Grinder: a versatile amplicon and shotgun sequence simu-
lator. Nucleic Acids Res., 40, 694.

Balzer,S. et al. (2010) Characteristics of 454 pyrosequencing data—enabling realistic
simulation with ﬂowsirn. Bioinformatics, 26, i420—i425.

Bishop,C.M. (2006) Pattern Recognition and Machine Learning. Springer.

Boyd,S. and Vandenberghe,L. (2004) Convex Optimization. Cambridge University
Press.

Cai,Y. and Sun,Y. (2011) Esprit-tree: hierarchical clustering analysis of millions of
16s rRNA pyrosequences in quasilinear computational time. Nucleic Acids Res.,
39, e95.

Candes,E.J. and Wakin,M.B. (2008) An introduction to compressive sampling.
IEEE Signal Proc. Mag, 25, 21—30.

Chatterjee,S. et al. (2011) Look ahead orthogonal matching pursuit. Acoustics,
Speech and Signal Processing (ICASSP), 2011 IEEE International Conference.
pp. 4024—4027.

Chatterjee,S. et al. (2012) Projection-based and look-ahead strategies for atom
selection. IEEE T rans.Signal Process, 60, 634—647.

Cheng,L. et al. (2012) Bayesian estimation of bacterial community composition
from 454 sequencing data. Nucleic Acids Res., 40, 5240—5249.

CVX. A system for disciplined convex programming. http://cvxr.com/cvx/. 2013.

Edgar,R.C. (2010) Search and clustering orders of magnitude faster than blast.
Bioinformatics, 26, 2460—2461.

Effron,B. et al. (2004) Least angle regression. Ann. Statist., 32, 407—499.

Haas,B.J. et al. (2011) Chimeric 16s rRNA sequence formation and
detection in sanger and 454-pyrosequenced pcr amplicons. Genome Res., 21,
494—504.

Huang,H. and Makur,A. (2011) Backtracking-based matching pursuit method for
sparse signal reconstruction. IEEE Signal Process. Lett., 18, 391—394.

Huson,D.H. et al. (2007) Megan analysis of metagenomic data. Genome Res., 17,
377—386.

Koslicki,D. et al. (2013) Quikr: a method for rapid reconstruction of bacterial
communities via compressive sensing. Bioinformatics, 29, 2096—2102.

Meinicke,P. et al. (2011) Mixture models for analysis of the taxonomic composition
of metagenomes. Bioinformatics, 27, 1618—1624.

Mitra,S. et al. (2011) Analysis of 16s rRNA environmental sequences using megan.
BMC Genomics, 12 (Suppl. 3), $17.

Ong,S.H. et al. (2013) Species identiﬁcation and proﬁling of complex microbial
communities using shotgun illumina sequencing of 16s rRNA amplicon se-
quences. PLoS One, 8, e608ll.

Stojnic,M. (2010) lz/ll-optimization in block-sparse compressed sensing and its
strong thresholds. IEEE J. Sel. Top. Signal Process, 4, 350—357.

Tropp,J.A. and Gilbert,A.C. (2007) Signal recovery from random meas-
urements via orthogonal matching pursuit. IEEE Trans. Inf. Theory, 53,
4655—4666.

von Mering,C. et al. (2007) Quantitative phylogenetic assessment of microbial com-
munities in diverse environments. Science, 315, 1126—1130.

Wang,Q. et al. (2007) Naive bayesian classiﬁer for rapid assignment of rrna
sequences into the new bacterial taxonomy. Appl. Environ. Microbiol, 73,
5261—5267.

Wommack,K.E. et al. (2008) Metagenomics: read length matters. Appl Environ
Microbiol, 74, 1453—1463.

Zuk,O. et al. (2012) Accurate Proﬁling of Microbial Communities From Massively
Parallel Sequencing Using Convex Optimization. Vol. LNCS 8214. Cham,
Switzerland, Springer.

 

2431

112 ﬁle'sleumo[pJOJXO'sopemJOJqutq/ﬁd11q IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

