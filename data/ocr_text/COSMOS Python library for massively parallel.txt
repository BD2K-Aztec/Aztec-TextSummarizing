APPLICA TIONS NOTE W 33.3 1’ {3.5392321355322333}???

 

Genome analysis

Advance Access publication June 30, 2014

COSMOS: Python library for massively parallel workflows

Erik Gafni‘“, Lovelace J. Luquette1 ’1, Alex K. Lancaster1 ’2, Jared B. Hawkins‘,
Jae-Yoon Jungl, Yassine Souilmil’s, Dennis P. Walll’2’*’§ and Peter J. Tonellato1 ’2’*

1Center for Biomedical Informatics, Harvard Medical School, 10 Shattuck Street, Boston, MA 02115, 2Department of
Pathology, Beth Israel Deaconess Medical Center, 330 Brookline Avenue, Boston, MA 02215, USA and 3Department of
Biology, Mohammed V University—Agal, 4 lbn Battouta Avenue, Rabat B.P:1014BP, Morocco

Associate Editor: Michael Brudno

 

ABSTRACT

Summary: Efficient workflows to shepherd clinically generated gen-
omic data through the multiple stages of a next-generation sequen-
cing pipeline are of critical importance in translational biomedical
science. Here we present COSMOS, a Python library for workflow
management that allows formal description of pipelines and partition-
ing of jobs. In addition, it includes a user interface for tracking the
progress of jobs, abstraction of the queuing system and fine-grained
control over the workflow. Workflows can be created on traditional
computing clusters as well as cloud-based services.

Availability and implementation: Source code is available for acad-
emic non-commercial research purposes. Links to code and docu-
mentation are provided at http://Ipm.hms.harvard.edu and http://
wall-lab.stanford.edu.

Contact: dpwall@stanford.edu or peter_tonellato@hms.harvard.edu.
Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on February 7, 2014; revised on May 6, 2014; accepted on
June 9, 2014

1 INTRODUCTION

The growing deluge of data from next-generation sequencers
leads to analyses lasting hundreds or thousands of compute
hours per specimen, requiring massive computing clusters or
cloud infrastructure. Existing computational tools like Pegasus
(Deelman et al., 2005) and more recent efforts like Galaxy
(Goecks et al., 2010) and Bpipe (Sadedin et al., 2012) allow the
creation and execution of complex workﬂows. However, few
projects have succeeded in describing complicated workﬂows in
a simple, but powerful, language that generalizes to thousands of
input ﬁles; fewer still are able to deploy workﬂows onto distrib-
uted resource management systems (DRMs) such as Platform
Load Sharing Facility (LSF) or Sun Grid Engine that stitch to-
gether clusters of thousands of compute cores. Here we describe

 

*To whom correspondence should be addressed.

TThe authors wish it to be known that, in their opinion, the ﬁrst two
authors should be regarded as Joint First Authors.

3:Present address: Invitae 458 Brannan St., San Francisco, CA 94107,
USA.

§Present address: Department of Pediatrics, Division of Systems
Medicine, Stanford University, 1265 Welch Road, Stanford, CA, USA.

COSMOS, a Python library developed to address these and other
needs.

2 FEATURES AND METHODS

An essential challenge for a workﬂow deﬁnition language is to
separate the deﬁnition of tools (which represent individual ana-
lyses) from the deﬁnition of the dependencies between them.
Several workﬂow libraries require each tool to expect speciﬁcally
named input ﬁles and produce similarly speciﬁc output ﬁles;
however, in COSMOS, tool I/O is instead controlled by specify-
ing ﬁle types. For example, the BWA alignment tool (Fig. 1a) can
expect FASTQ-typed inputs and produce a SAM-typed output,
but does not depend on any speciﬁc ﬁle names or locations.
Additionally, tool deﬁnitions do not require knowledge of the
controlling DRM.

Once tools have been deﬁned, their dependencies can be for-
malized via a COSMOS workﬂow, which is deﬁned using Python
functions that support the map-reduce paradigm (Dean and
Ghemawat, 2004) (Fig. 1b). Sequential workﬂows are deﬁned
primarily by the sequence_ primitive, which runs tools in
series. The app 1y_ primitive is provided to describe workﬂows
with potentially unrelated branching by executing tools in paral-
lel. To facilitate map-reduce in large and branching workﬂows,
COSMOS introduces a tagging system that associates a set of
key-value tags (e.g. a sample ID, chunk ID, sequencer ID or
other job parameter) with speciﬁc job instances. This tagging
feature enables users to formalize reductions over existing tag
sets or to split by creating new combinations of tags
(Supplementary Fig. S1). To execute a workﬂow, COSMOS
creates a directed acyclic graph (DAG) of tool dependencies at
runtime (Fig. 10) and automatically links the inputs and outputs
between tools by recognizing ﬁle extensions as types. All ﬁle
paths generated by tool connections are managed by
COSMOS, automatically assigning intermediate ﬁle names.

Another major challenge in workﬂow management is execu-
tion on large compute clusters, where transient errors are com-
monplace and must be handled gracefully. If errors cannot be
automatically resolved, the framework should record exactly
which jobs have failed and allow the restart of an analysis
after error resolution. COSMOS uses the DRMAA library
(Troger et al. 2007) to manage job submission, status checking
and error handling. DRMAA supports most DRM platforms,
including Condor, although our efforts used LSF and Sun Grid
Engine. Users may control DRM submission parameters by

 

© The Author 2014. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/Iicenses/
by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial
re-use, please contact journals.permissions@oup.com

112 [3.10811211an[plOJXO'SODBIILIOJIIIOIQ/ﬂ(11111 IIIOJJ popcolumoq

910K ‘09 lsnﬁnV no :2

COSMOS

 

(a) Tool definition
class a1ign(Tool):

inputs = [ 'fastq.gz' ]

outputs = ['sam' ]

mem_req = 4096 # in MB
# cpu_req, time_req, queue and other parameters
# may be defined
name = "BWA MEM paired alignment with interleaved FASTQ"
def cmd(self, i, s, p):
return """bwa mem -M -v 2{s[ref_genome]}
‘P {iIfastq-gzliol} >
 . samn II n

(b) Workflow definition

inputs = [ INPUT(file, tags={'sample':s, 'chunk':c})
for file,s,c in input_data ]
dag = DAG().sequence_(
adq_(inputs, stage_name='Load_FASTQ'),
# Align each chunk
map_(tools.align, stage_name='Align'),
# combine chunks per sample and remove PCR duplicates
reduce_(['sample' ], tools.remdup, stage_name='RemDup'),
map_(tools.index_bam, stage_name='MakeIndex')

)

(C) Visualization of workﬂow

Load_FASTQ
sample: 52 sample: 52 sample: 51 sample: 51
chunk: c2 chunk: c1 chunk: c1 chunk: c2
sample: 52
chunk: c2

sample: 51
chunk: c2

    
  

 
   

    
  

 
 

   

Align

sample: 52 sample: 51
chunk: c1 chunk: c1

RemDup

 

Fig. 1. (21) Tools are deﬁned in COSMOS by specifying input and output types, not ﬁles, and a cmd () function returning a string to be executed in
a shell. cpu_req and other parameters may be inspected by a programmer-deﬁned Python function to set DRM parameters or redirect jobs to queues.
(b) Workﬂows are deﬁned using map-reduce primitives: sequenc e_ , map_ (execute the al ign tool from (a) on each ‘chunk’ in parallel) and reduce_
(group the aligned outputs by sample tag). (c) Directed acyclic graph of jobs generated by the workﬂow in (b) to be executed via the DRM for four input

FASTQ ﬁles (with sample tags $1 and s2, and chunk tags of cl and c2)

overriding a Python function that is called on every job control
event. COSMOS’ internal data structures are stored in an SQL
database using the Django framework (https://djangoproject.
com) and is distributed with a Web application for monitoring
the state of both running and completed workﬂows, querying
individual job states, visualizing DAGs and debugging failed
jobs (Supplementary Figs S2—S5).

Each COSMOS job is continuously monitored for resource
usage, and a summary of these statistics and standard output
and error streams are stored in the database. This allows users
to ﬁne-tune estimated DRM parameters such as CPU and
memory usage for more efﬁcient cluster usage. Pipeline restarts
are also facilitated by the persistent database, as it records both
success and failure using job exit codes.

3 COMPARISON AND DISCUSSION

Projects such as Galaxy and Taverna Molstencroft et al., 2013)
are aimed at users without programming expertize and offer
graphical user interfaces (GUIs) to create workﬂows, but come
at the expense of power. For example, it is straightforward to
describe task dependencies in Galaxy’s drag-and-drop workﬂow
creator; however, to parallelize alignment by breaking the input
FASTQ into several smaller chunks to be aligned independently,
input stages must be manually created for each chunk or the
workﬂow must be applied to each chunk manually. In addition,
the user must ﬁx the number of input chunks a priori. COSMOS
resolves this tedious process for the programmer by dynamically
building its DAG at runtime.

Such limitations may not be a major concern for small-scale
experiments where massive parallelization to reduce runtime is
not critical; however, when regularly analyzing terabytes of raw
data, the logistics of parallelization and job management play a
central role. Snakemake (Koster and Rahmann, 2012) looks to
the proven design of GNU Make to describe DAGs for compli-
cated workﬂows, whereas the Ruffus project (Goodstadt, 2010)

aims to create a DAG by providing a library of Python decor-
ators. However, neither of these projects directly supports inte-
gration with a DRM. The Pegasus system offers excellent
integration with DRMs and even the assembly of several inde-
pendent DRMs using the Globus software; however, the descrip-
tion of some simple workﬂows can require considerably more
code than the equivalent COSMOS code (Supplementary Fig.
S6), and the DAG is not determined at runtime, so cannot
depend on the input. Bpipe offers an elegant syntax for deﬁning
the DAG, but does not include a graphic user interface for moni-
toring and runtime statistics. Additionally, COSMOS’ persistent
database and Web front end allow rapid diagnosis of errors in
data input or workﬂow execution (see Supplementary Table S1
for a detailed feature comparison). COSMOS has been tested on
the Ubuntu, Debian and Fedora Linux distributions. The only
dependency is Python 2.6 or newer and the ability to install
Python packages; we recommend a DRMAA-compatible
DRM for intensive workloads.

Funding: This work was supported by the National Institutes
of Health [1R01MHO90611-01A1 to D.P.W, 1R01LM011566
to P.J.T., and 5T15LMOO7092 to P.J.T. and J.B.H.]; and a
Fulbright Fellowship [to Y.S.].

Conflict of Interest: L.J.L. is also an employee with Claritas
Genomics Inc., a licensee of COSMOS.

REFERENCES

Dean,J. and Ghemawat,S. (2004) MapReduce: simpliﬁed data processing on large
clusters. In: Proceedings of the 6th Conference on Symposium on Operating
Systems Design & Implementation. USENIX Association, Berkeley, CA, p. 10.

Deelman,E. et al. (2005) Pegasus: A framework for mapping complex scientiﬁc
workﬂows onto distributed systems. Sci. Program, 13, 219—237.

Goecks,J. et al. (2010) Galaxy: a comprehensive approach for supporting accessible,
reproducible, and transparent computational research in the life sciences.
Genome Biol, 11, R86.

 

2957

112 ﬁlm'spaumo[pJOJXO'sor1chOJurorw/2d11q wort pepsolumoq

910K ‘09 lsnﬁnV uo ::

E. Gafni et al.

 

Goodstadt,L. (2010) Ruffus: a lightweight Python library for computational pipe-
lines. Bioinformatics, 26, 2778—2779.

K6ster,J. and Rahmann,S. (2012) Snakemake—a scalable bioinforrnatics workﬂow
engine. Bioinformatics, 28, 2520—2522.

Sadedin,S.P. et al. (2012) Bpipe: a tool for running and managing bioinforrnatics
pipelines. Bioinformatics, 28, 1525—1526.

Troger,P. et al. (2007) Standardization of an API for distributed resource manage-
ment systems. In: Seventh IEEE International Symposium on Cluster Computing
and the Grid. IEEE, Rio De J aneiro, Brazil, pp. 619—626.

Wolstencroft,K. et al. (2013) The Taverna workﬂow suite: designing and executing
workﬂows of Web Services on the desktop, Web or in the cloud. Nucleic Acids
Res, 41, W557—W56l.

 

2958

112 ﬁlm'spaumo[pJOJXO'sor1chOJurorw/2d11q wort pepsolumoq

910K ‘09 lsnﬁnV uo 22

