ORIGINAL PAPER

Vol. 28 no. 18 2012, pages 2304-2310
doi: 1 0. 1 093/bioinforma tics/bts360

 

Systems biology

Advance Access publication June 23, 2012

Predicting drug—target interactions from chemical and genomic
kernels using Bayesian matrix factorization

Mehmet G nen*

Helsinki Institute for Information Technology HIIT, Department of Information and Computer Science, Aalto University

School of Science, FI—OOO76 Aalto, Espoo, Finland

Associate Editor: Gunnar Ratsch

 

ABSTRACT

Motivation: Identifying interactions between drug compounds
and target proteins has a great practical importance in the
drug discovery process for known diseases. Existing databases
contain very few experimentally validated drug—target interactions
and formulating successful computational methods for predicting
interactions remains challenging.

Results: In this study, we consider four different drug—target
interaction networks from humans involving enzymes, ion channels,
G-protein-coupled receptors and nuclear receptors. We then
propose a novel Bayesian formulation that combines dimensionality
reduction, matrix factorization and binary classification for predicting
drug—target interaction networks using only chemical similarity
between drug compounds and genomic similarity between target
proteins. The novelty of our approach comes from the joint Bayesian
formulation of projecting drug compounds and target proteins
into a unified subspace using the similarities and estimating the
interaction network in that subspace. We propose using a variational
approximation in order to obtain an efficient inference scheme
and give its detailed derivations. Finally, we demonstrate the
performance of our proposed method in three different scenarios:
(i) exploratory data analysis using low-dimensional projections, (ii)
predicting interactions for the out-of-sample drug compounds and
(iii) predicting unknown interactions of the given network.
Availability: Software and Supplementary Material are available at
http://users.ics.aalto.fi/gonen/kbmf2k.

Contact: mehmet.gonen@aalto.fi

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on May 27, 2012; revised on May 27, 2012; accepted on
June 18, 2012

1 INTRODUCTION

The functions of pharmaceutically useful target protein families such
as enzymes, ion channels, G—protein—coupled receptors (GPCRs)
and nuclear receptors can be modulated by interacting them with
drug compounds. Our knowledge about the genomic space of target
proteins and the chemical space of drug compounds is piling up
as a result of high—throughput experimental projects that analyze
the genome and high—throughput chemical compound screening
with biological assays. Unfortunately, our knowledge about the
relationship between these two spaces remains quite limited

 

*To whom correspondence should be addressed.

due to laborious and costly experimental procedures. Existing
databases such as ChEMBL (Gaulton et al., 2012), DrugBank
(Knox et al., 2011), KEGG DRUG (Kanehisa et al., 2012) and
SuperTarget (Hecker et al., 2012) contain information about a small
number of experimentally validated interactions. Hence, successful
computational methods for identifying interactions between drug
compounds and target proteins make genomic drug discovery
signiﬁcantly efﬁcient and effective. Computational approaches can
be used to guide experimentalists towards new predictions and to
provide supporting evidence for their experimental results.

Traditional computational methods can be grouped into three
categories: (i) docking simulations (Cheng et al., 2007; Rarey
et al., 1996), (ii) ligand—based approaches (Butina et al., 2002;
Byvatov et al., 2003; Keiser et al., 2007) and (iii) literature text
mining (Zhu et al., 2005). Docking simulations require structural
information of target proteins, which is not mostly available for some
protein families such as GPCRs. Ligand—based approaches compare
a candidate ligand with the known ligands of a target protein and may
not perform well for target proteins with a small number of known
ligands. Literature text mining based on keyword search cannot be
used to detect unknown interactions and suffers from the redundancy
due to non—standard naming practice for drug compounds and target
proteins.

Recently, there are many machine learning algorithms proposed
for predicting drug—target interactions using the chemical properties
of drug compounds, the genomic properties of target proteins and
the known interaction network (Bleakley and Yamanishi, 2009;
Jacob and Vert, 2008; van Laarhoven et al., 2011; Wassermann
et al., 2009; Yamanishi et al., 2008, 2010). The main assumption
of these studies is that similar drug compounds are likely to
interact with similar target proteins. These similarities between
drug compounds and target proteins are often encoded using kernel
functions designed speciﬁcally for chemical compounds and protein
sequences, respectively (Schblkopf et al., 2004).

The most basic statistical approach is to formulate the interaction
network inference problem as a binary classiﬁcation task between
drug—target pairs using pairwise kernel functions (Jacob and Vert,
2008; Wassermann et al., 2009). However, this approach can be
computationally quite heavy due to the high number of drug—target
pairs. Supervised bipartite graph inference maps drug compounds
and target points into a uniﬁed space (i.e. pharmacological space)
using the chemical and genomic similarities and tries to estimate
the interaction network using a distance—based procedure in that
subspace (Yamanishi et al., 2008, 2010). Local models are also used
to predict drug—target interaction networks after their successful
applications for protein—protein interaction networks, metabolic

 

2304 © The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /§.IO'S[BU.IHO[p.IOJXO'SOllBIHJOJUTOTQ/ﬁdnq mm; popeommoq

9IOZ ‘091sn8nv uo ::

Kernelized Bayesian matrix factorization with twin kernels

 

networks and regulatory networks (Bleakley and Yamanishi, 2009).
Instead of using the given interaction network just as the output,
integrating a kernel function that considers the given network
topology and the kernels calculated using chemical compounds and
protein sequences can improve the prediction performance (van
Laarhoven 61611., 2011).

In this study, we propose a novel Bayesian formulation
that combines kernel—based nonlinear dimensionality reduction
(Schblkopf and Smola, 2002), matrix factorization (Srebro, 2004)
and binary classiﬁcation for predicting drug—target interaction
networks using only chemical similarity between drug compounds
and genomic similarity between target proteins. Different from
previous studies, our proposed method is the ﬁrst fully probabilistic
formulation for drug—target interaction network inference. We show
its performance on four benchmark datasets using three experimental
scenarios with practical importance: (i) exploratory data analysis
using low—dimensional projections, (ii) predicting interactions for
the out—of—sample drug compounds and (iii) predicting unknown
interactions of the given network.

2 MATERIALS

In this study, we consider four different drug—target interaction networks
from humans, namely, Enzyme, Ion Channel, GPCR and Nuclear
Receptor, provided by Yamanishi etal. (2008). These datasets are publicly
available at http://web.kuicr.kyoto—u.ac.jp/supp/yoshi/drugtarget/. We use
these datasets as they are without adding new interactions from source
databases.

2.1 Drug—target interaction data

Yamanishi et 611. (2008) use KEGG BRITE (Kanehisa e1611., 2006), BRENDA
(Schomburg etal., 2004), SuperTarget (Gunther etal., 2008) and DrugBank
(Wishart etal., 2008) databases to collect information about the interactions
between drug compounds and target proteins. Table 1 summarizes the
datasets in terms of numbers of drug compounds, target proteins and
interactions. The set of known drug—target interactions is regarded as ‘gold
standard’ and used to evaluate the performance of our proposed method in
the cross—validation experiments as in the previous studies (Bleakley and
Yamanishi, 2009; van Laarhoven 61611., 2011; Yamanishi 61611., 2008, 2010).

2.2 Chemical data

Chemical structures of drug compounds are extracted from the DRUG and
COMPOUND sections in the KEGG LIGAND database (Kanehisa et 611.,
2006). Yamanishi et 611. (2008) calculate the structural similarities between
drug compounds using SIMCOMP (Hattori et 611., 2003), which represents
drug compounds as graphs and calculates a similarity score based on the
size of the common substructures between two graphs. Given two drug
compounds 61,- and dk, chemical similarity between them can be found
as sc(d,-,dk)= ldiﬂdkl/ldiUdkl and the similarity matrix between all drug
compound pairs is denoted as SC.

Table 1. The drug—target interaction datasets from Yamanishi etal. (2008)

 

 

Dataset Drugs Targets Interactions
Enzyme 445 664 2926
Ion Channe l 210 204 1476
GPCR 223 95 635
Nuclear Receptor 54 26 90

 

2.3 Genomic data

Aminoacid sequences of target proteins are extracted from the KEGG
GENES database (Kanehisa et 611., 2006). Yamanishi et 611. (2008) calculate
the sequence similarities between target proteins using a normalized version
of Smith—Waterman score (Smith and Waterman, 1981). Given two target
proteins tj and t;, genomic similarity between them can be found as
Sg(tj,tl)=SI/\1(tj,tl)/ SW(tj,tj)SI/\1(tl,tl), where SW(', ') gives the canonical
Smith—Waterman score and the similarity matrix between all target protein
pairs is denoted as Sg.

3 METHODS

We mainly consider the problem of predicting new drug—target interactions
for out—of—sample drug compounds and/or target proteins that are not in the
given interaction network. Our proposed method can also be used to predict
unknown interactions of the given network.

3.1 Problem formulation

We are given Nd drug compounds denoted as Xi = {d1,d2, ...,de} and Nt
target proteins denoted as Xt ={t1,t2, ...,tNt}. We are also given a set of
known interactions between these two sets as the Nd >< Nt adjacency matrix
denoted as Y, where y}: = +1 if drug compound 61,- interacts with target protein
tj and y; = —1 otherwise. We can have three different prediction scenarios:
(i) ﬁnd interacting target proteins from Xt for a new drug compound d,” (ii)
ﬁnd interacting drug compounds from Xd for a new target protein t,. and (iii)
estimate whether a new drug compound 61,. and a new target protein t,. are
interacting with each other or not. In order to attack these three scenarios with
a single uniﬁed approach, we formulate the problem as a binary classiﬁcation
task, which requires to estimate whether there is an interaction between a
drug compound and a target protein using only the similarities between drug
compounds and the similarities between target proteins.

3.2 Kernelized Bayesian matrix factorization with twin
kernels

In order to obtain an efﬁcient Bayesian algorithm, we formulate a fully
conjugate probabilistic model and develop a deterministic variational
approximation mechanism for inference. The main idea is to project drug
compounds and target proteins into a uniﬁed subspace using the kernels
calculated from chemical and genomic data, respectively. These low—
dimensional representations of drug compounds and target proteins can be
used to estimate their interactions.

Figure 1 illustrates the proposed probabilistic model for predicting drug—
target interactions from kernels on drug compounds and target proteins with
a graphical model. The kernel matrix calculated from drug compounds Kd is
used to project them into a low—dimensional space using the projection matrix
Ad. Similarly, the kernel matrix calculated from target proteins Kt is used to
project them into the same subspace, which is called ‘pharmacological space’

@ Kd Kt G

Fig. 1. Graphical model for predicting drug—target interactions from kernels
on drug compounds and target proteins

 

2305

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; popaommoq

9IOZ ‘09 lsnﬁnv uo ::

M.G nen

 

in the previous studies (Yamanishi et a1., 2008, 2010), using the projection
matrix At. The low—dimensional representations of drug compounds and
target proteins in the pharmacological space, namely, Gd and Gt, are used
to calculate the interaction scores between them. Finally, the given interaction
matrix Y is generated from the interaction score matrix F.

The notation we use throughout the rest of the this article is as follows:
Nd and Nt represent the numbers of training drug compounds and target
proteins, respectively. R gives the dimensionality of the projected subspace.
The Nd de kernel matrix for drug compounds is denoted by Kd, where
the columns of Kd by kdai. The Nd ><R matrix of corresponding projection
parameters afi S and their priors Mi, S are denoted by Ad and Ad, respectively,
where the columns of Ad and Ad by ads and his. The R de matrix of
projected instances for drug compounds gsdal. are represented as Gd, where
the columns of Gd as gdal- and the rows of Gd as gfi. The Nt th kernel
matrix for target proteins is denoted by Kt, where the columns of Kt by
ktj. The Nt x R matrix of corresponding projection parameters alt, S and their

priors Alt, S are denoted by At and At, respectively, where the columns of
At and At by at; and km. The R th matrix of projected instances for
target proteins gstaj are represented as Gt, where the columns of Gt as gtaj
and the rows of Gt as gst. The variance for the entries of Gd and Gt is
represented as ogz. The Nd >< Nt matrix of interaction scores J? is represented

as F, where the rows of F as f i and the columns of F as f j. The Nd >< Nt matrix
of associated interaction variables is represented as Y, where each element
y’: e {—1,+1}. As short—hand notations, all priors in the model are denoted
by E 2 {Ad, At}, where the remaining variables by (9 = {Ad,At , Gd, Gt,F}
and the hyper—parameters by g = {01), 6)}. Dependence on g is omitted for
clarity throughout this article.

The distributional assumptions of our proposed model are deﬁned as

  
. . . . _1 S

aaslkas~N(a§is;0,(ka,s) ) V0.5)

T 2 .

gfii Iad,S7kd,i  ad’skdja 0g) V(Sa 1)

AL, ~g()»I;,S§OZAaﬁA) V035)

. . . S . _1 _

altSSIA'JtSS~N(a]t,svoa()\'{;as) ) V075)

T 2 -

gijlatnfaktj  at,skt,ja 0g) V(Sa.])

‘ ‘S T . .

.fJI'lIgdaiagtajNNOSIZagdaigtSjv1) V(la_])

yjl-[JCJ-l~80§’yJ’->v) V(i,j)
where the interaction scores between the matrices of projected instances
and interaction variables are introduced to make the inference procedures
efﬁcient (Albert and Chib, 1993), and the margin parameter I) can be used to
resolve the scaling ambiguity issue and to place a low—density region between
two classes (interacting versus non—interacting), similar to the margin idea
in support vector machines, which is generally used for semi—supervised
learning (Lawrence and Jordan, 2005). N (111,2) represents the normal
distribution with the mean vector 11 and the covariance matrix 2. Q('; a, 6)
denotes the gamma distribution with the shape parameter a and the scale
parameter 6. 8(') represents the Kronecker delta function that returns 1 if its

argument is true and 0 otherwise.

We only use the gamma and normal distributions in our probabilistic
model. The main reason for choosing these speciﬁc distributions is that
they allow us to obtain a very efﬁcient inference mechanism easily due to
conjugacy between them. Their advantage becomes clear when we explain
our inference procedure.

When we consider the random variables as deterministic values, the

interaction score matrix that corresponds to the decision function values
in discriminative methods can be decomposed as

F = GdTGt = (AdTKd)T(AIKt) = KgAdAIKt

where we see that F is factorized using the matrices of projected instances
Gd and Gt. Different from previous Bayesian matrix factorization solutions

such as the probabilistic formulations proposed by Salakhutdinov and Mnih
(200861, 19), our approach parameterizes the matrices of projected instances
in terms of kernel matrices Kd and Kt. This strategy allows us to make
predictions for out—of—sample points using kernel functions.

3.2.1 Eﬁ‘icient inference using variational approximation Exact inference
for our probabilistic model is intractable and using a Gibbs sampling
approach is computationally expensive (Gelfand and Smith, 1990). We
instead formulate a deterministic variational approximation, which is more
efﬁcient in terms of computation time. The variational methods use a lower
bound on the marginal likelihood using an ensemble of factored posteriors to
ﬁnd the joint parameter distribution (Beal, 2003). We can write the factorable
ensemble approximation of the required posterior as

p(®a E IKdaKtaY)%q(®v 
61(Ad)€1(Ad)61(Gd)61(At)€1(At)61(Gt)€1(F)

and deﬁne each factor just like its full conditional distribution:

Nd R
q<Ad)=1_[1_jgog,,;amoral,»

i=1s=l

R
61(Ad) = HN(ad,s; Mada), 201cm»

s=1

Nd
q(Gd)=1_[N(gd,.-;u(gd,,-),zed,»
i=1

Nt R . . .
q(At)=HHg(i’t,.;Mamet.»

j=ls=l

R
q(At)=1_[N(at,.;u(at,.). 2m.»

s=1

Nt
q<Gt)=1_[N(gt,,-;u(gt,-),Eat,»
j=1
Nd Nt
q(F)=HHTNOj-’;MO§’),20?),p03’»

i=1j=1
where a('), ﬁ('), M.) and 2(') denote the shape parameter, the scale
parameter, the mean vector and the covariance matrix for their arguments,
respectively. TN(.; 11,2,p(')) denotes the truncated normal distribution
with the mean vector 11, the covariance matrix )3 and the truncation
rule ,0(') such that TN(-;[L,Z,p(-))OCN(~;[L,Z) if ,0(.) is true and
TN(-; 11, 2,,0(.))=0 otherwise.

We can bound the marginal likelihood using Jensen’s inequality:

10gP(Y|Ka, Kt) Z

Eq(o,s)[10gP(Y, 9, E IKdaKtH —Eq(o,a)[10g61(®, E)] (1)
and optimize this bound by maximizing with respect to each factor separately
until convergence. The approximate posterior distribution of a speciﬁc factor
I can be found as

61(1) OCeXP(Eq({o, E}\r)[10gP(Y, 9, E lKa, Kt)])-

For our proposed model, thanks to the conjugacy between random variables,
the resulting approximate posterior distribution of each factor follows the
same distribution as the corresponding factor.

3.2.2 Inference details The approximate posterior distributions of the
precision priors for drug compounds can be found as
Nd R N
q(Ad)=HHg(ig,.;aA+1/2.(l/a+(ag,,)2/2)‘1) (2)

i=1s=l

 

2306

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; popaommoq

9IOZ ‘09 lsnﬁnv uo ::

Kernelized Bayesian matrix factorization with twin kernels

 

where the tilde notation denotes the posterior expectations as usual, i.e.
h(T)=Eq(T)[h(T)]. The approximate posterior distribution of the projection
parameters for drug compounds can be found as a product of multivariate
normal distributions:

R
61(Ad)= HN(ad,.; 2(ad,.)Kd(ga)T/oz,
s=1
(diagol‘d)+KdeT/o;)—1) (3)
and the approximate posterior distribution of the projected instances for drug
compounds is also a product of multivariate normal distributions:

Nd ~ ~ ~
q(Gd) = HN(gd,.-: Zed,i)(Adid,i/o§ +Gt(f’)T),
i=1
(I/o§+GtGE)—1). (4)
The approximate posterior distributions of the precision priors for target
proteins can be found as

Nt R S (v
q(At)=HHQ(A’t,,;ai +1/2.(1/181 +(a’taS)2/2)_1). (5)
j=1s=1

The approximate posterior distribution of the projection parameters for target
proteins can be found as a product of multivariate normal distributions:

R
q(At)=1_[N(at,.;2(at,.)Kt(g1)T/az,

s=1

(diagomKtKI/ogrl) (6)
and the approximate posterior distribution of the projected instances for target
proteins is also a product of multivariate normal distributions:

Nt ~ ~ ~
q(Gt)= HN(gt,,-; 2(gt,j)(AEkt,j/o§ +Gdfj),
j=1

(I/o§+GdGCT)—1). (7)

The approximate posterior distribution of the interaction scores is a
product of truncated normal distributions given as

Nd Nt ~
q(F>=1_[1‘[TN(;;-‘;g;,g’:,, 1.1;»;- > v) (8)
i=1j=1
where we need to ﬁnd their posterior expectations to update the approximate
posterior distributions of the projected instances for drug compounds and
target proteins. Fortunately, the truncated normal distribution has a closed—
form formula for its expectation.

The inference procedure summarized in Algorithm 1 sequentially updates
the approximate posterior distributions of the model parameters and the latent
variables until convergence, which can be checked by monitoring the lower
bound in ( 1). The ﬁrst term of the lower bound corresponds to the sum of
exponential form expectations of the distributions in the joint likelihood. The
second term is the sum of negative entropies of the approximate posteriors
in the ensemble. The only non—standard distribution in these terms is the
truncated normal distribution used for the interaction scores; nevertheless, the
truncated normal distribution has a closed—form formula also for its entropy.

In our implementation, the chemical similarity function sc(-, -) is used
as the kernel function between drug compounds kd(', ), which means that
the chemical similarity matrix SC is used as the kernel matrix for drug
compounds Kd. Similarly, the genomic similarity function sg(., ') is used
as the kernel function between target proteins kt(', ), which means that the
genomic similarity matrix Sg is used as the kernel matrix for target proteins
Kt. The provided similarity matrices SC and Sg may not be valid kernels (i.e.
positive semideﬁnite), but we use them as they are because our algorithm
does not require them to be positive semideﬁnite. The hyper—parameters
(011, 61) and 0g are set to (1, 1) and 0.1, respectively.

 

Algorithm 1 Kernelized Bayesian matrix factorization with twin kernels
(KBMFZK)
Require: Kd, Kt, Y, R, or)” ,8)“ 0g and v
1. Initialize q(Ad), 61(At), q(Gd), 61(Gt) and 61(F) randomly
repeat
Update 61(Ad), 61(Ad) and 61(Gd) using (2), (3) and (4)
Update 61(At), 61(At) and 61(Gt) using (5), (6) and (7)
Update 61(F) using (8)
until convergence
return 61(Ad) and 61(At)

 

NQP‘PP’N

 

3.3 Prediction scenarios

We consider three different scenarios for drug—target interaction prediction.
For these scenarios, we can get probabilistic estimates from our Bayesian
model but the variances are observed to be very small due to discriminative
nature of the model (i.e. modeling the interaction between drug compounds
and target proteins by introducing the binary classiﬁcation part with a large
margin strategy just after the matrix factorization part). Hence, we only
consider point estimates for simplicity without sacriﬁcing the generalization
performance.

3.3.] Prediction for a new drug compound In the ﬁrst scenario, we assume
that we are given a new drug compound d* and our task is to ﬁnd the set of
target proteins from Xt that interact with d*. We ﬁrst need to calculate the
similarities between d* and Xd:

kd,.=[kd(d.,d1) kd(d.,d2)  I<d(c1...:1N,)]T
and these similarities can be used to ﬁnd the interaction scores for d*:
f* = (X11211)T (X? K) =k1.z$21§1 K
where positive valued entries indicate that the corresponding target proteins
interact with d*.

3.3.2 Prediction for a new target protein In the second scenario, we
assume that we are given a new target protein t... and our task is to ﬁnd
the set of drug compounds from Xd that interact with t*. We ﬁrst need to
calculate the similarities between t... and Xt:

kt,*=[kt(t*,tl) man)  Ict(t...,tN.)]T
and these similarities can be used to ﬁnd the interaction scores for t*:

f... = (QKd)T(§Ekt,*) =K§z§éﬁkt1
where positive valued entries indicate that the corresponding drug
compounds interact with t*.

3.3.3 Joint prediction for a new drug compound and a new target protein
The third scenario is the hybrid of the ﬁrst two scenarios and our task is to
ﬁnd whether a new drug compound d* and a new target protein t... interact
with each other. We can ﬁnd the interaction score for d* and t... as

~ T ~ ~ ~
f: = (Adide (Aikt...) =k;.AdAEkt,*
where a positive value indicates that d* and t... interact with each other.
Note that d* and t... can be a known drug compound and a known target

protein, respectively, from the given interaction network in order to predict
unknown interactions.

4 RESULTS

In order to illustrate the effectiveness of our proposed method,
called ‘kernelized Bayesian matrix factorization with twin kernels’
(KBMFZK), we present the results of three experimental scenarios:
(i) exploratory data analysis using low—dimensional projections, (ii)
predicting interactions for the out—of—sample drug compounds and
(iii) predicting unknown interactions of the given network.

 

2307

112 /§JO'SIBUJnoproSIxosor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

M.G nen

 

  

 

 

Drugs
New Drugs

Targets

 

 

 

    

 

 

 

Targets

Targets
A New Targets A New Targets

 

 

 

 

 

Fig. 2. Two—dimensional projections of drug compounds and target proteins obtained by KBMFZK on Nuclear Receptor dataset with (A) held—out drug
compounds (B) held—out target proteins compounds and (C) held—out drug compounds and target proteins. Provided interactions between drug compounds
and target proteins are shown as dashed lines for training network and thick solid lines for held—out drug compounds and/or target proteins

4.1 Exploratory data analysis using low-dimensional
projections

KBMF2 K can also be used for exploratory data analysis by displaying
low—dimensional projections in addition to predicting interactions.
For three different prediction scenarios described earlier, we provide
visualizations on Nuclear Receptor dataset due to its small
network size. In this set of experiments, we set the subspace
dimensionality R = 2 and the margin parameter 1) :0.

Given a drug—target interaction network, we want to investigate
the interactions of new drug compounds and/or target proteins within
that network. We do not include 10% of drug compounds and/or
target proteins and their interactions to our training network giving
us three different scenarios. Figure 2 displays the two—dimensional
projections of training networks superimposed with the predicted
projections for held—out drug compounds and/or target proteins.
Provided interactions between drug compounds and target proteins
are also shown as dashed lines for training network and thick solid
lines for held—out drug compounds and/or target proteins.

We would like to point out a couple of important observations
from Figure 2. First of all, KBMFZK successfully captures bipartite
nature of the given interaction networks (i.e. two disjoint node
sets) by placing drug compounds and target proteins as clearly
separated node groups. Second, we can easily see that the dashed
lines (i.e. interactions from training network) connect nearby drug
compounds and target proteins. Finally the projections for held—out
drug compounds/target proteins are meaningful because they are
connected to nearby target proteins/drug compounds.

The prediction performance using just two dimensions may not
be enough for a reliable system, but these two—dimensional ﬁgures
can deﬁnitely be used for exploratory data analysis.

4.2 Predicting interactions for the out-of—sample drug
compounds

To show the performance of KBMFZK in predicting interactions
for new drug compounds, we perform experiments on the four
benchmark datasets. We exactly follow the experimental procedure
of Yamanishi et a1. (2010) in order to have comparable results. For
each dataset, drug compounds are split into ﬁve subsets of roughly
equal size. Each subset is then used in turn as the test set and

training is performed on the remaining four subsets. This procedure
is repeated ﬁve times to obtain robust results. The subspace
dimensionality R and the margin parameter 1) of KBMFZK are
selected from {5,10,15,20,25} and {0,1}, respectively, using the
prediction performances on the training sets.

Table 2 gives the average AUC (area under the receiver operating
curve) values for Yamanishi et a1. (2010) and KBMF2 K. Note that
Yamanishi et a1. (2010) also report results with pharmacological
similarity between drug compounds. We compare our results with
the results obtained using the same similarity measures in our
experiments (i.e. chemical similarity for drug compounds and
genomic similarity for target proteins). We see that KBMFZK
achieves higher average AUC values on all datasets. KBMFZK
signiﬁcantly improves the results on Ion Channel and GPCR
datasets by 10.7% and 4.6% respectively.

Figure 3 shows the average AUC values for KBMFZK with
changing subspace dimensionality and v :0. On Nuclear
Receptor dataset, we do not see any effect of the subspace
dimensionality possibly due to small size of the interaction network.
However, there is a clear increasing trend in AUC with increasing
subspace dimensionality for other datasets. On Enzyme and GPCR
datasets, we get the best results with R=25. It is still possible to
improve the results on Enzyme dataset by adding more dimensions
to the common subspace of drug compounds and target proteins.

Instead of using a cross—validation strategy, the intrinsic subspace
dimensionality can be found while learning the model parameters
using, for example, automatic relevance determination (Neal, 1996).
However, we leave this extension as future work.

Table 2. Prediction performances of Yamanishi et a1. (2010) and KBMFZK
on the four benchmark datasets in terms of average AUC values

 

 

Dataset Yamanishi et a1. (2010) KBMFZK
Enzyme 0.821 0.832
Ion Channel 0.692 0.799
GPCR 0.811 0.857
Nuclear Receptor 0.814 0.824

 

 

2308

112 /810'SIBUJnoproSIxosor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Kernelized Bayesian matrix factorization with twin kernels

 

Table 3. The top ﬁve predicted interactions on the four benchmark datasets

 

 

 

Rank Pair Annotation Rank Pair Annotation

1 D00437 Nifedipine (JP16/USPHNN) 1 D02358 Metoprolol (USAN/INN)

CD 1559 cytochrome P450, family 2, subfamily C, polypeptide 9 CD 154 adrenergic, beta-2-, receptor, surface

2 D00542 Halothane (JP16/USP/INN) 2 D04625 Isoetharine (USP)
0) CDK 1571 cytochrome P450, family 2, subfamily E, polypeptide 1 C K 154 adrenergic, beta-2-, receptor, surface
E, 3 D00097 Salicylic acid (JP16/USP) Ed) 3 D00283 Clozapine (JAN/USP/INN)
a CD 5743 prostaglandin-endoperoxide synthase 2 85 CD 1814 dopamine receptor D3
m 4 D00501 Pentoxifylline (JAN/USP/INN) 4 D02354 Thiethylperazine (USAN/INN)

5150 phosphodiesterase 7A 1814 dopamine receptor D3
5 D00139 Methoxsalen (JP16/USP) 5 D00604 Clonidine hydrochloride (JP16/USP)
DK 1543 cytochrome P450, family 1, subfamily A, polypeptide 1 C 148 adrenergic, alpha-1A-, receptor
1 D00438 Nimodipine (USAN/INN) 1 D00182 Norethisterone (JP16)
DK 779 calcium channel, voltage-dependent, L type, alpha 1S subunit ’5 C 2099 estrogen receptor 1

T) 2 D00538 Zonisamide (JAN/USANHNN) ‘5. 2 D00348 Isotretinoin (USP)
21 DK 6331 sodium channel, voltage-gated, type V, alpha subunit 8 C 5915 retinoic acid receptor, beta
g 3 D00552 Ethyl aminobenzoate (JP16) pad) 3 D00348 Isotretinoin (USP)
0 K 6331 sodium channel, voltage-gated, type V, alpha subunit :4 C 5916 retinoic acid receptor, gamma
g 4 D00546 Desﬂurane (JAN/USP/INN) E 4 D00898 Dienestrol (USP/INN)
H 2555 gamma—aminobutyric acid (GABA) A receptor, alpha 2 g C K 2100 estrogen receptor 2

5 D00528 Anhydrous caffeine (JP16) Z 5 D00348 Isotretinoin (USP)

1080 cystic ﬁbrosis transmembrane conductance regulator C 6258 retinoid X receptor, gamma

 

Interactions reported in ChEMBL, DrugBank and KEGG are marked with C, D and K, respectively. Interactions reported in at least one of these databases are shown in bold.

 

   

 

  
   
 

 

 

 

 

 

0.86 - - - .—
0.84-
0.82-
O
:>
<
0.80'
'9-Enzyme
0_78_ -E-Ion Channel
GPCR
O 76 A—Nuclear Receptor
I 5 10 15 20 25

R

Fig. 3. Prediction performance of KBMFZK with changing subspace
dimensionality and v = 0 on the four benchmark datasets in terms of average
AUC values

4.3 Predicting unknown interactions of the given
network

In order to illustrate the performance of KBMFZK in predicting
unknown drug—target interactions of the given network, we perform
a new set of experiments on the four benchmark datasets. Using the
best parameter values for {R, 1)} found in the previous experiments,
we train KBMFZK with the complete interaction network for each
dataset. We rank the non—interacting pairs with respect to their
interaction scores and extract the top 100 predicted interactions.
We report only the top ﬁve predicted interactions for each dataset
and give the full lists of predicted interactions as Supplementary
material.

Table 3 lists the top ﬁve predicted interactions for each dataset. We
check these predicted interactions manually from the latest online
versions of ChEMBL (Gaulton et a1., 2012), DrugBank (Knox et a1.,
2011) and KEGG DRUG (Kanehisa et a1., 2012) databases. We see
that 80% of the predictions (16 out of 20) is reported in at least
one of these databases. This is a strong evidence for the practical
relevance of our method. For example, Enzyme dataset has 2926
interacting and 292554 non—interacting (i.e. not known to interact)
drug—target pairs. We pick only the top ﬁve predicted interactions
and see that four out of these ﬁve drug—target pairs are currently
reported in at least one database. KBMFZK correctly identiﬁes three
and four out of ﬁve predicted interactions on Ion Channel
and GPCR datasets, respectively. The prediction performance of
KBMFZK is even better on Nuclear Receptor dataset. The
top ﬁve predicted interactions are currently reported in ChEMBL
database. We also check the top 10 predicted interactions and see that
all of them are reported in ChEMBL database. Note that the predicted
interactions that are not reported yet may also exist in reality.

5 DISCUSSION

In this study, we consider four different drug—target interaction
networks from humans involving enzymes, ion channels, GPCRs
and nuclear receptors. We then propose a novel Bayesian
formulation that combines kernel—based nonlinear dimensionality
reduction (Schblkopf and Smola, 2002), matrix factorization
(Srebro, 2004) and binary classiﬁcation for predicting drug—target
interaction networks using only chemical similarity between drug
compounds and genomic similarity between target proteins. The
novelty of our approach comes from the joint Bayesian formulation
of projecting drug compounds and target proteins into a uniﬁed
subspace using the similarities and estimating the interaction

 

2309

112 /810'SIBUJnoproSIxosor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

M.G nen

 

network in that subspace. Our proposed method is the ﬁrst
fully probabilistic formulation proposed for drug—target interaction
network inference.

We propose using a variational approximation in order to obtain
an efﬁcient inference scheme and give its detailed derivations. The
most time—consuming steps of the proposed variational inference
mechanism are covariance calculations because we need to perform
matrix inversions. The time complexity of the covariance updates
for the projection matrices in (3) and (6) is (9(RN3) and (9(RNE),
respectively. The time complexity of the covariance updates for
the composite components in (4) and (7) is (9(R3). The other
calculations in these steps can be done very efﬁciently using matrix—
matrix or matrix—vector multiplications. Finding the posterior
expectations of the interaction scores in (8) only requires evaluating
the standardized normal cumulative distribution function and the
standardized normal probability density. In summary, the total
time complexity of each iteration in our variational approximation
scheme is (9(RNg4—RNE —I—R3), which makes our algorithm very
efﬁcient compared to standard pairwise kernel approaches that
require calculating an NdNt X NdNt kernel matrix between object
pairs and training a kernel—based classiﬁer using this kernel matrix.

In order to demonstrate the performance of our proposed
method, called ‘kernelized Bayesian matrix factorization with twin
kernels’ (KBMFZK), we use four benchmark datasets containing
known drug—target interaction networks, chemical kernels between
drug compounds and genomic kernels between target proteins
provided by Yamanishi et a1. (2008). We design three different
experimental scenarios with practical importance as follows (i)
exploratory data analysis using low—dimensional projections, (ii)
predicting interactions for the out—of—sample drug compounds and
(iii) predicting unknown interactions of the given network. In the
ﬁrst set of results, we show that the resulting low—dimensional
projections can be used to predict drug—target interactions and
practitioners can use these projections as two—dimensional ﬁgures for
exploratory data analysis. The remaining sets of results show that
our novel probabilistic interpretation obtains better generalization
performance than earlier optimization—based approaches.

KBMFZK uses one kernel function for chemical similarity and
another kernel function calculated on protein sequences for genomic
similarity. The performance of our approach can be improved by
integrating multiple kernels for both kinds of similarity. In kernel—
based methods, this approach is known as ‘multiple kernel learning’
(G6nen and Alpaydin, 2011) and our method can be extended
towards that direction.

ACKNOWLEDGEMENT

The author thanks Fidan Siimbiil for her very useful comments and
suggestions.

F unding: The Academy of Finland (Finnish Centre of Excellence in
Computational Inference Research COIN, grant no 251170).

Conﬂict of interest: none declared.

REFERENCES

Albert,J.H. and Chib,S. ( 1993) Bayesian analysis of binary and polychotomous response
data. J. Amer. Statist. Assoc, 88, 669—679.

Bea1,M.J. (2003) Variational Algorithms for Approximate Bayesian Inference. PhD
thesis, The Gatsby Computational Neuroscience Unit, University College London.

Bleakley,K. and Yamanishi,Y. (2009) Supervised prediction of drug—target interactions
using bipartite local models. Bioinformatics, 25, 2397—2403.

Butina,D. et a1. (2002) Predicting ADME properties in silico: methods and models.
Drug Discov. Today, 7, S83—S88.

Byvatov,E. et a1. (2003) Comparison of support vector machine and artiﬁcial neural
network systems for drug/nondrug classiﬁcation. J. Chem. Inf Comput. Sci, 43,
1882—1889.

Cheng,A.C. et a1. (2007) Structure-based maximal afﬁnity model predicts small-
molecule druggability. Nat. Biotechnol, 25, 71—75.

Gaulton,A. et a1. (2012) ChEMBL: a large-scale bioactivity database for drug discovery.
Nucleic Acids Res., 40, D1100—D1107.

Gelfand,A.E. and Smith,A.F.M. (1990) Sampling-based approaches to calculating
marginal densities. J. Amer. Statist. Assoc, 85, 398—409.

Gonen,M. and Alpayd1n,E. (2008) Multiple kernel learning algorithms. J. Mach. Learn.
Res., 12, 2211—2268.

Giinther,S. et a1. (2008) SuperTarget and Matador: resources for exploring drug—target
relationships. Nucleic Acids Res., 36, D919—D922.

Hattori,M. et a1. (2003) Development of a chemical structure comparison method for
integrated analysis of chemical and genomic information in the metabolic pathways.
J. Am. Chem. Soc., 125, 11853—11865.

Hecker,N. et a1. (2012) SuperTarget goes quantitative: update on drug—target
interactions. Nucleic Acids Res., 40, Dlll3—Dl 117.

Jacob,L. and Vert,J.-P. (2008) Protein—ligand interaction prediction: an improved
chemogenomics approach. Bioinformatics, 24, 2149—2156.

Kanehisa,M. et a1. (2006) From genomics to chemical genomics: new developments in
KEGG. Nucleic Acids Res., 34, D354—D357.

Kanehisa,M. et a1. (2012) KEGG for integration and interpretation of large-scale
molecular data sets. Nucleic Acids Res., 40, D109—D114.

Keiser,M.J. et a1. (2007) Relating protein pharmacology by ligand chemistry. Nat.
Biotechnol, 25, 197—206.

Knox,C. et a1. (2011) DrugBank 3.0: a comprehensive resource for ‘omics’ research on
drugs. Nucleic Acids Res., 39, D1035—D104l.

Lawrence,N.D. and Jordan,M.I. (2005) Semi-supervised learning via Gaussian
processes. In Advances in Neural Information Processing Systems I 7, pp. 75 3—760.

Neal,R.M. (1996) Bayesian Learning for Neural Networks. Springer, New York, NY.

Rarey,M. et a1. ( 1996) A fast ﬂexible docking method using an incremental construction
algorithm. J. Mol Biol, 261, 470—489.

Salakhutdinov,R. and Mnih,A. (2008a) Bayesian probabilistic matrix factorization using
Markov chain Monte Carlo. In Proceedings of the 25th International Conference
on Machine Learning, pp. 880—887.

Salakhutdinov,R. and Mnih,A. (2008b) Probabilistic matrix factorization. In Advances
in Neural Information Processing Systems 20, pp. 1257—1264.

Scholkopf,B. and Smola,A.J. (2002) Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA.

Scholkopf,B. et al., (eds) (2004) Kernel Methods in Computational Biology. MIT Press,
Cambridge, MA.

Schomburg,I. et a1. (2004) BRENDA, the enzyme database: updates and major new
developments. Nucleic Acids Res., 32, D431—D433.

Smith,T.F. and Waterman,M.S. (1981) Identiﬁcation of common molecular
subsequences. J. Mol Biol, 147, 195—197.

Srebro,N. (2004) Learning with Matrix Factorizations. PhD thesis, Massachusetts
Institute of Technology.

van Laarhoven,T. et a1. (2011) Gaussian interaction proﬁle kernels for predicting drug-
target interaction. Bioinformatics, 27, 3036—3043.

Wassermann,A.M. et a1. (2009) Ligand prediction for orphan targets using support
vector machines and various target—ligand kernels is dominated by nearest neighbor
effects. J. Chem. Inf. Model, 49, 2155—2167.

Wishart,D.S. et a1. (2008) DrugB ank: a knowledgebase for drugs, drug actions and drug
targets. Nucleic Acids Res., 36, D901—D906.

Yamanishi,Y. et a1. (2008) Prediction of drug—target interaction networks from the
integration of chemical and genomic spaces. Bioinformatics, 24, i232—i240.

Yamanishi,Y. et a1. (2010) Drug—target interaction prediction from chemical, genomic
and pharmacological data in an integrated framework. Bioinformatics, 26,
i246—i254.

Zhu,S. et a1. (2005) A probabilistic model for mining implicit ‘chemical compound-
gene’ relations from literature. Bioinformatics, 21 (Suppl 2), ii245—ii251.

 

2310

112 /810'SIBUJnoproSIxosor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

