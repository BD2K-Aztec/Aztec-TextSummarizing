Bioinformatics Advance Access published August 8, 2016

 

Bioinformatics: Application Notes

BiDiFuse: a FIJI plugin for fusing bi-
directionally recorded microscopic image vol-
umes

Detrez JRl, Vanderwinden JMZ, Barbier M13, Verschuuren M1, Nuydens R3,
Langlois X3, Timmerrnans JP1 and De Vos WH1’4’*

1Laboratory of Cell Biology & Histology, Department of Veterinary Sciences, University of Antwerp,
Groenenborgerlaan 171, 2020 Antwerp, Belgium, 2Light Microscopy Facility & Laboratory of Neurophysiolo-
gy, Faculty of Medicine, Free University of Brussels, Route de Lennik 808, 1070 Anderlecht, Belgium,
3Department of Neuroscience, Janssen Research and Development, Tumhoutseweg 30, 2340 Beerse, Belgium,
4Department of Molecular Biotechnology, Ghent University, Coupure Links 653, 9000 Ghent, Belgium.

* To whom correspondence should be addressed.

Associate Editor: Prof. Robert Murphy

Abstract

Summary: Deep tissue imaging is increasingly used for non-destructive interrogation of intact organs
and small model organisms. An intuitive approach to increase the imaging depth by almost a factor of
2 is to record a sample from two sides and fuse both image stacks. However, imperfect three-
dimensional alignment of both stacks presents a computational challenge. We have developed a FIJI
plugin, called BiDiFuse, which merges bi-directionally recorded image stacks via 3D rigid transfor-
mations. The method is broadly applicable, considering its compatibility with all optical sectioning
microscopes and does not rely on fiducial markers for image registration.

Availability and Implementation: The method is freely available as a plugin for FIJI from
https://github.com/JanDetrez/BiDiFuse/

Contact: Winnok.DeVos@UAntwerpen.be

 

1 Introduction

Increasing interest in microscopic visualization of intact biological
specimens has fueled a surge of methodological developments in recent
years, including tissue clearing and optical sectioning microscopy.
Parallel to the continuous improvements of tissue clearing methods
aimed at increasing optical penetration depth (Richardson & Lichtman,
2015), the evolution of optical sectioning techniques, such as confocal,
two-photon and, in particular light-sheet microscopy (Dodt et al., 2007),
have enabled rapid, non-destructive imaging of thick tissue specimen.
Despite the tremendous impact these methods have had in extending
imaging depth, various constraints still exist, including the limited
working distance of microscope objectives, the variable clearing perfor-
mance between different tissue types, and the inevitable loss of signal
quality in deeper tissue layers. A simple method to increase the imaging
depth is to record the sample from opposite sides. As this action involves

ﬂipping and relocation of the sample, a method is required to computa-
tionally fuse both image stacks and generate a single stack with compa-
rable image quality at the top and bottom. We present a method, called
BiDiFuse, which is based on 3D rigid transformations to register corre-
sponding landmark points in both image stacks (Figure 1A). This is the
ﬁrst open-source implementation that is compatible with all ﬂuorescent
labels, and is independent of the optical sectioning method used for
image acquisition.

2 Methods

Microscopic images can be provided as bi-directionally recorded im-
age stacks containing one or more channels, as single or stitched image
volumes. Both image stacks are imported in FIJI open-source freeware
(Schindelin et a1. 2012), after which a reference stack is selected (stack
A) and the other image stack (stack B) is mirrored and the order of the
planes in the stack is reversed.

© The Author (2016). Published by Oxford University Press. All rights reserved. For Permissions, please email:

journals.permissions@oup.com

9mg ‘09 isnﬁnV uo salaﬁuV s01 ‘crulomcg JO AirSJQAru [1 112 [310'811211an[plOJXO'SODBIIIJOJIIIOIQ/[Zdinq IIIOJJ papeolumoq

 

The only user intervention that is required for registration, is the selec-
tion of three corresponding landmark points in both stacks. In stack A,
the ﬁrst two landmark points (Pl and P2) can be chosen freely, after
which the third point (P3) should be indicated on the vector P1-P3,
orthogonal to P1-P2. The same landmark points are subsequently indi-
cated manually on the image stack B.

For the registration process, the ﬁrst landmark point (Pl) is regarded
as the image origin for both image volumes. First, the directed rotation
(0t) about the orthogonal axes, P1-P2 and P1-P3, is determined using an
arctangent function with two arguments, atan2 (eq. 1, 2). The rotation
about the third axis (i.e. z-axis, the axial imaging axis) is calculated as
the angle between the projected vectors P1-P2 in the ﬁrst (A) and second
image stack (B, eq. 3).

180
 ax = (atan2 (Zp3stackB' yp3stackB) _ atan2(Zp3stackA'yp35tackA)) I T

180
 “3’ = (atan2 (szstack 3' xpzstack B) _ atan2 (szstack 14' xpzstackA)) I T
180

 a2 = (atan2 (yplstackA' xplstackA) _ atan2 (yplstack B' xplstackB)) I 11:

Given the angles of rotation in 3D, three rotation matrices are calcu-
lated and applied to the image using the rotate function in TransformJ.
The transition point between image stacks can then be selected on the
basis of the plane intensity and sharpness (derived ﬁom Sobel ﬁltering),
or set manually. A smooth transition between both image stacks can be
achieved by blending a selected even number of planes (SZ) surrounding
the transition point (TP). This can be done by means of various blending
options, including linear weighted sum (eq. 4), maximum, minimum,
mean, median, average, and sum blending.

(4) If 21 < 21' < ZTP—SZ/Z Z: Z {4
IfZTP—SZ/Z S ZiS ZTP+SZ/2 Zi=(1'/SZ)- Zg‘1 +(SZ—1)/SZ - Z15, for 1': 0, ..., SZ
IfZTP+$Z/2<Zi<Zn Zi=Zf

An optional post-hoc automatic registration procedure can be performed
to increase the precision of the fusion process. In this process, registra-
tion is reﬁned using the scale invariant feature transform calculated on
the overlapping plane containing mm,“ and mm,” (Lowe, 2004).

3 Results

Unidirectional recordings of a cleared brain slice with a nuclear counter-
stain lose nearly half their intensity when moving deeper in the tissue,
together with a notable decrease in image contrast (Figure 1B, C). This
loss is compensated for by fusing of bi-directionally recorded image
stacks, resulting in a drop of less than 20% in the middle of the stack.
Fusing can be done with high accuracy, as can be derived from the
alignment of blood vessels in a XZ-view (Figure 1D). This is reﬂected in
an increase in the Pearson correlation of two corresponding images from
both stacks after registration (from 0.12i0.07 to 0.48i0.07, n=3). Fusion
has been performed with multiple samples imaged on different optical
sectioning imaging setups, including two-photon, confocal and Apo-
tome.2 structured illumination (Zeiss) microscopes (Examples are avail-
able in the download package).

4 Discussion

Although a two-view imaging approach has been described earlier
(Susaki et al., 2014), we have developed a convenient image registration
workﬂow that is made available as an open-source FIJI plugin. It is
different from the powerful bead-based multi-view selective plane

3 25:: pm

STACK

 

 

STACK B

SCAN DIRECTION
+—

 

BiDiFuse '-

 

 

— Stack}! — StackE --- BiDiFuse

‘7‘
u

 

I"
a

9
no

BiIZIiFuSQ‘.l

.
l.

D BEFORE FUSING

 

Relative mean intensity“) 0
.0
U‘

.0
a

H

8

300 we
Z~de pth (urn)

 

AFTER FUSING

 

Figure 1: BiDiFuse workﬂow and results. (A) Fusing of bi-directionally recorded image
stacks is done by rigid image transformations (3D translations and rotations). 03) a
cleared brain slice with a nuclear counterstain shows the loss in image intensity in deep
tissue layers, which is compensated for using BiDiFuse, as shown in (C). 0)) An XZ-
view of a cleared brain slice shows the misalignment before and after correct alignment of
blood vessels after ﬁising. Fusion was done without blending to visualise the alignment of
the image stacks. Scale bars: 100 um.

illumination microscopy (SPIM) registration software (Preibisch et al.,
2010) in several aspects. First, BiDiFuse is compatible with all section-
ing microscope setups, and does not require registration of the stage
position. Moreover, registration can be done without additional tissue
preparation measures, such as mounting the sample in the presence of
ﬂuorescent beads or addition of nuclear counterstain. Since our method
only accounts for rigid transformations, the specimen should be ﬁxed
and immobilized for microscopy for optimal results (e. g. mounted be-
tween coverslips, or on a rotatable stage). The dataset size for fusing is
limited by the RAM required for the 3D rotation process, but can be
extended by virtual loading and rotation of the image stacks. The time
for fusing two 1Gb datasets in the order of a few minutes, using the
nearest-neighbor interpolation algorithm for image rotation. The choice
for manual landmark identiﬁcation is driven by the lack of information
on the sample position and the large size of 3D datasets. Increased
precision can be achieved by a post-hoc automatic registration.

Funding

This work was supported by the University of Antwerp (TTBOF/29267) and the
Agency for Innovation by Science and Technology in Flanders (IWT Baekeland
fellowship IWT 140775). JM Vanderwinden is Directeur de Recherche at FRS-
FNRS (Belgium).

Conﬂict of In terest: none declared.

References

Dodt, H.-U., et a1. (2007). Ultramicroscopy: three-dimensional Visualization of
neuronal networks in the whole mouse brain. Nature Methods, 4(4), 331—6.
Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints.
International Journal of Computer Vision, 60(2), 91—110.

Preibisch, S., et a1. (2010). Software for bead-based registration of selective plane
illumination microscopy data. Nature Methods, 7(6), 418—9.

Richardson, D. S., & Lichtman, J. W. (2015). Clarifying Tissue Clearing. Cell,
162(2), 246—257.

9mg ‘09 isnﬁnV uo salaﬁuV s01 ‘BIHJOJHBQ JO AIISJQAIII [1 112 ﬁlm'spaumo[pJOJXO'sonchogurorq/ﬁdnq IIIOJJ papeolumoq

Schindelin, J., et a1. (2012). Fiji: an open-source platform for biological-image
analysis. Nature Methods, 9(7), 676-82.

Susaki, E. a, et a1. (2014). Whole-brain imaging with single-cell resolution using
chemical cocktails and computational analysis. Cell, 157(3), 726—39.

9mg ‘09 1sn8nV uo salaﬁuV s01 ‘121u10111123 10 A1rs19Aruf1 112 /810's112umo[p101x0'sor112u1101urorq/ﬁd11q 111011 papeolumoq

