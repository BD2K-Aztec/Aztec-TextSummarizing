Bioinformatics, 31, 2015, i116—i123
doi: 10.1093/bioinformatics/btv235
ISMB/ECCB 2015

 

Large-scale model quality assessment for
improving protein tertiary structure prediction

Renzhi Cao1, Debswapna Bhattacharya‘, Badri Adhikari‘, Jilong U1, and
Jianlin Cheng1'2'3'*

1Computer Science Department, University of Missouri, Columbia, Missouri, 65211, USA, 2Informatics Institute,
University of Missouri, Columbia, Missouri, 65211, USA and 3C. Bond Life Science Center, University of Missouri,
Columbia, Missouri, 65211, USA

*To whom correspondence should be addressed.

Abstract

Motivation: Sampling structural models and ranking them are the two major challenges of protein
structure prediction. Traditional protein structure prediction methods generally use one or a few
quality assessment (QA) methods to select the best—predicted models, which cannot consistently
select relatively better models and rank a large number of models well.

Results: Here, we develop a novel large—scale model QA method in conjunction with model clus—
tering to rank and select protein structural models. It unprecedentedly applied 14 model QA
methods to generate consensus model rankings, followed by model refinement based on model
combination (i.e. averaging). Our experiment demonstrates that the large—scale model QA ap—
proach is more consistent and robust in selecting models of better quality than any individual QA
method. Our method was blindly tested during the 11th Critical Assessment of Techniques for
Protein Structure Prediction (CASP11) as MULTICOM group. It was officially ranked third out of
all 143 human and server predictors according to the total scores of the first models predicted for
78 CASP11 protein domains and second according to the total scores of the best of the five
models predicted for these domains. MULTICOM’s outstanding performance in the extremely
competitive 2014 CASP11 experiment proves that our large—scale QA approach together with
model clustering is a promising solution to one of the two major problems in protein structure
modeling.

Availability and implementation: The web server is available at: http://sysbio.rnet.missouri.edu/

 

multicom_c|uster/humanl.
Contact: chengji@missouri.edu

 

1 Introduction

Protein tertiary structure prediction has been an important scientific
problem for few decades, especially in bioinformatics and computa—
tional biology (Eisenhaber et al., 1995). Despite more and more na—
tive structures are included in protein data bank (PDB) (Berman
et al., 2000) database, the gap between the sequenced proteins and
the native structures is still enlarging due to the exponential increase
of protein sequences produced by large—scale genome and transcrip—
tome sequencing. It is estimated that <1% of protein sequences
have the native structures in PDB database (Rigden, 2009).
Therefore, accurate computational methods for protein tertiary
structure prediction that are much cheaper and faster than experi-
mental structure determination techniques are needed to reduce this
large sequence-structure gap. Furthermore, computational structure

©The Author 2015. Published by Oxford University Press.

prediction methods are important for obtaining the structures of
membrane proteins whose structures are hard to be determined by
experimental techniques such as X—ray crystallography (Yonath,
2011)

The two major problems of protein structure prediction are
model sampling and model ranking. The former is to generate a
number of structural models (conformations) for a protein target,
and the latter is to rank these models and to select the presumably
best ones as final predictions. The two main ways of generating pro-
tein models are template-based modeling and template-free model-
ing. Template—based modeling methods use the known structures
(templates) of the proteins that are homologous or analogous to a
target protein to construct structural models for the target (Bowie
et al., 1991; Jones, et al., 1992; Zhang, 2008a, b). For instance,

i116

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),
which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact

journals.permissions@oup.com

1e /§JO'S{eu1no [p.IOJXO'SOTlBIIHOJUTOTQ/ﬁdllq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

Large-scale model quality assessment for improving protein tertiary structure prediction i117

 

Table 1. All 14 0A methods with the details

 

Methods Type

Features

 

MULTICOM-NOVEL
OPUS—PSP

ProQ2

RWplus
ModelEvaluator
Modelcheck2
RF_CB_SRS
SELECTpro

Dope

DFIREZ
ModFOLDclustZ Multi
APOLLO M

Pcons M

QApro M+S
MULTICOM (human) Consensus

H“

b
0‘;

('D

mmmmmmmmm

Structural, physical, chemical features

Contact potentials based on side chain functional groups
Structural features

Side—chain orientation dependent potential
Structural features, contacts

Structural features, contacts, disorder, conservation
Distance dependent statistical potential
Energy—based (h—bond, angle, electrostatics, vdw)
Statistical potential

Energy—based potential

Pairwise model similarity (geometry)

Pairwise model similarity

Pairwise model similarity

Weighted pairwise model similarity

Average ranking

 

The highlighted methods are built in house. S: single-model method; M: multi-model method.

during 2014 CASP11 experiment, almost all the structure prediction
servers such as I—TASSER (Zhang, 2008a, b; Zhang, 2014),
MULTICOM (Cheng et al., 2012; Li et al., 2013), MUFOLD
(Zhang et al., 2010) and RaptorX (Kallberg et al., 2012) used the
template—based model technique to predict structures of some
CASP11 targets for which some homologous template structures
could be found. Template—free modeling methods predict the protein
tertiary structure from scratch without using template information.
This is especially important when there are no structural homologs
existing in the database or the template identification techniques
cannot find good templates (Zhang, 2008b). Some CASP11 predic—
tion servers such as ROSETTA (Simons et al., 1997), QUARK (Xu
and Zhang, 2012) and FALCON (Li et al., 2008) used template—free
modeling method to generate structural models for some hard
CASP11 targets.

Once some structural models are generated for a protein, the
remaining challenge is to assess the quality of these models and se—
lect the most accurately predicted models. There are generally two
main kinds of quality assessment (QA) methods: single—model QA
methods (Cao et al., 2014a, b; Randall and Baldi, 2008; Ray et al.,
2012; Shen and Sali, 2006; Wang et al., 2009; Zhang and Zhang,
2010), which evaluate the quality of one single model without
using the information of other models; and multi—model QA meth—
ods (Cao et al., 2014a, b; McGuffin and Roche, 2010; Wallner and
Elofsson, 2006; Wang et al., 2011), which use the structural simi—
larity between one model and other models of the same protein to
assess its quality. The multi—model quality prediction methods gen—
erally perform better than the single—model quality prediction
methods given the pool of models is sampled by independent struc—
ture predictors. However, multi—model QA method is largely influ-
enced by the proportion of good models in the pool or the average
quality of the largest model cluster in the pool, whereas single—
model QA methods may work better in assessing a small number
of models of wide—range quality usually associated with a hard tar—
get or a pool of models with very low proportion of good ones
(Cao et al., 2014a).

Currently, most protein structure prediction methods use one
or at most a few QA methods to rank and select models, generally
leading to the poor performance in selecting models of good qual—
ity due to the extreme difficulty of ranking models and intrinsic
limitations of individual QA methods. Some structure prediction
methods also apply clustering techniques to group models into

different clusters whose center is considered as the best model in
each cluster based on the structural similarities. The hypothesis be—
hind it is that near—native structures are more likely clustered in a
large free-energy basin in the free—energy landscape (Dobson et al.,
1998; Shortle et al., 1998). The clustering based approaches gener—
ally select an average model rather than the best model and cannot
work well if the quality of the largest cluster is not good.
Therefore, although numerous methods have been developed to as—
sess, rank and select models, protein model ranking is still largely
an unsolved problem.

In order to address this challenge, we developed a large—scale
consensus QA method (MULTICOM) to combine 14 complemen-
tary model QA methods to improve the reliability and robustness
of protein model ranking. The general model ranking is also syner-
gistically integrated with model clustering techniques to increase
the diversity and quality of the final selected models. On the very
competitive 2014 CASP11 benchmark, this new method substan—
tially outperform any single QA method, suggesting its unique
value in addressing one major problem of protein structure
prediction.

2 Materials and methods

2.1 Large—scale protein model 0A for protein tertiary
structure prediction

Given a pool of structural models generated for a target protein
(e.g. hundreds of models generated for a CASP11 target), the
MULTICOM method used unprecedentedly 14 complementary
model QA methods to predict the quality score of each model first
(Table 1). These QA methods include both single—model and
multi-model QA methods. The single—model methods include our
new single—model global QA method MULTICOM—NOVEL based
on the difference between secondary structure and solvent accessi—
bility predicted by Spine X (Faraggi et al., 2012) and SSpro4
(Cheng et al., 2005) from the protein sequence and those of a
model parsed by DSSP (Kabsch and Sander, 1983), physical—chem—
ical features (i.e. surface polar score, weighted exposed score, and
etc.) (Mishra et al., 2013), the normalized quality score generated
by ModelEvaluator (Wang et al., 2009), RWplus score (Zhang
and Zhang, 2010), dope score (Shen and Sali, 2006)
and RF_CB_SRS_OD score (Rykunov and Fiser, 2007); ProQ2

1e /§JO'S{eu1no [p.IOJXO'SSUBUHOJUTOTQ/ﬁdllq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

R.Cao et al.

 

 

 

 

 

 

I1 18
mull-mm
WEIEHWEHEHWENEHWEIEHI
ﬁnalizing structure prediction
_ Model pool _.,,-.~.-:_L 
at“ . girl-
.1. - '
'39:,
MT] ' I—I'h‘ILL .
‘L  :11: 1'
~. _.
airlock-l ranking wlusmrinﬁ
.-' "IT-f .- -. _ HIE-u. IN
E 
f or. .
..... .. .  L . as“ d.
r ' I. '— a.  *1: $9.:
I ' I-_.-
I .- l I)" i I?
'-.I _.' tttt w. .
1' 2'
TH / r
'H. “an. "If
Large scale ' 
consensus ranking t' '
Integration
'IL.-'_  ‘T,-_'-t I “up
If.-  'I a"
lntegratlan
_. 3.9 .-
Refinemant :2;

Fig. 1. The workflow of the MULTICOM method comprised of six steps. (1)
A pool of tertiary structure models is predicted for a target protein. (2)
Models are scored and ranked by different OA methods. (3) Models are
clustered into groups based on structural similarity. (4) The consensus of
individual OA rankings and other information are synthesized to generate
the final ranking of all the models. (5) The final ranking and the clustering
results are integrated to select top five diverse models for submission. (6)
The top five models are combined to generate five refined models to be
submitted to CASP11

(Ray et al., 2012); Model check2 method produced by an im—
proved version of ModelEvaluator (Wang et al., 2009); a recali—
brated SELECTpro energy (Randall and Baldi, 2008); Dope (Shen
and Sali, 2006); DFIRE2 (Yang and Zhou, 2008); OPUS_PSP
(Lu et al., 2008); prlus (Zhang and Zhang, 2010);
ModelEvaluator (Wang et al., 2009) and RF_CB_SRS_OD
(Rykunov and Fiser, 2007). The multi—model QA methods include
ModFOLDclust2 (McGuffin and Roche, 2010); Pcons (Wallner
and Elofsson, 2006); APPOLLO (Wang et al., 2011); QApro—
a weighted combination of ModelEvaluator and APOLLO
(Cao et al., 2014a). The details of each method are described in
Table 1.

During the 2014 CASP11 experiment, MULTICOM used two
different combinations of the QA scores produced by 14 QA meth—
ods to generate consensus rankings to rank all models of each target.
The first one is the complete combination, in which each of 14 QA
methods was applied to all the models of a target and generated a
ranking for them based on their QA scores, and the average rank of
14 ranks of each model assigned by the 14 QA methods was used as
its final rank. The second one is the consensus rankings based on the
same average ranks produced by only six QA methods including
(MULTICOM—NOVEL QA score, QApro score, Pcons score,

Modelcheck2 score, Dope score, OPUS_PSP score). These six meth—
ods were selected because their combination performed best on all
the models of 46 CASP10 when all possible combinations were
benchmarked before CASP11 experiment started. On these CASP10
models, the average loss score of top one model based on 6 QA
methods is 0.037, lower than 0.057 of all 14 QA methods.
However, considering that the optimization process in benchmark-
ing could over fit the data, we let MULTICOM use the consensus
rankings of both the 6 selected QA methods and all 14 QA methods.

During the modeling ranking process, if the same top one model
was selected by the two consensus rankings, which happened in
>50% cases, the consensus ranking of the six QA methods were
used as the final ranking of all the models. But if they disagreed with
each other, the score of top one model selected by the pairwise QA
method APOLLO was used to break the tie as follows. On one
hand, if the score of APOLLO’s top one model was >03, which
generally meant quite some models in the model pool were of good
quality due to relatively high pairwise similarity between them, the
final ranking was set as the consensus ranking of the 6 QA methods
or all 14 QA methods depending on whose top one model was more
similar to the top one model of APOLLO than the other.
Furthermore, the top one models of the two consensus rankings and
of the top predictors (e.g. MULTICOM—CLUSTER and Zhang-
Server) were compared with the top one model of APOLLO, and the
model most similar to the top one model of APOLLO was used the
top one model in the final ranking without changing the ranking of
all other models. On the other hand, if the score of the top one
model selected by APOLLO was <2 0.3, which only occasionally
happened and suggested that the target was hard and most models
were of bad quality, MULTICOM calculated the percent of match—
ing between the secondary structures extracted from the top one
model selected by either 6— or 14—QA consensus ranking with the
secondary structure of the target predicted from its sequence. The
final ranking was one of 6 or 14 consensus ranking whose top one
model had the higher percentage of matching of secondary
structures.

Since the top five models selected by the final ranking above
sometime could be very similar to each other, the risk for all of
them to fail altogether was high for hard targets. To reduce this
risk, MULTICOM only kept the top two models of the final rank-
ing as the two predicted structures. And then, in order to increase
the diversity of top five models selected as final predictions for
each target, MULTICOM used MUFOLD_CL (Zhang and Xu,
2013) to cluster models, and then selected the other three models
according to the final ranking in separate clusters different from
those of the top two models. MUFOLD—CL (Zhang and Xu, 2013)
is a model clustering method based on the comparison of the pro—
tein distance matrices. Comparing with other clustering techniques
based on structural distance such as root—mean—square deviation
(RMSD) (Kabsch, 1976), it is much faster, but yields similar accur—
acy, which is desirable for clustering a large number of protein
models. During the selection of the other three models from differ-
ent clusters, MULTICOM also skipped the models ranked at bot—
tom 10% according to our newly developed MULTICOM—
NOVEL QA method. This guaranteed that the top five selected
structures were largely different, which indeed improved the score
of the best of top five models.

Finally, MULTICOM used a model combination approach
(Wang et al., 2010) to integrate each selected model with other
similar models in the model pool to generate its refined model.
The workflow of our MULTICOM method described earlier is illus—
trated in Figure 1.

1e /§JO'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTCI/ﬁdllq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

Large-scale model quality assessment for improving protein tertiary structure prediction i119

 

2.2 Evaluation of top—ranked models

We downloaded publically available native structures for
42 CASP11 human targets from the CASP’s website (http://www.
predictioncenter.org/casp11/index.cgi). During CASP11, our
MULTICOM method was blindly benchmarked on these targets to—
gether with 142 human and server predictors. The predicted struc—
tural models were assessed on 55 domains of the 42 targets. For
comparison, we downloaded both the other predictors’ predictions
and our submitted predictions from the CASP11’s website. During
CASP11, each predictor submitted up to five predicted model with
the first one (TS1) designated as the best model. We evaluated the
performance of each predictor’s first model by calculating the GDT—
TS score between it and its native structure. The TM—score (Zhang
and Skolnick, 2004) was used for calculating the global distance
test — total score (GDT—TS). The Z—score of a model was calculated
as the model’s GDT—TS score minus the average GDT—TS score of
all the models in the model pool of a target divided by the standard
deviation of all GDT—TS scores. The negative Z—score was converted
to 0 during summation of Z—scores. The sum of the Z—scores of the
first models predicted by a predictor for the 42 targets was used to
measure its overall performance. Similarly, the sum of the Z—scores
of the best of the five submitted models predicted by a predictor for
the 42 targets was used to measure its performance if the best of all
five submitted models was considered.

3 Results and discussion

We evaluated the performance of MULTICOM human predictor
along with 44 CASP11 server predictors on 42 CASP11 human tar-
gets. The sum of Z—scores of all first (i.e. T51) models or the best of

Table 2. The top 10 tertiary structure predictors ranked based on
the summation of the Z—scores of the first models, and their sum-
mation of the Z-scores of best of the five submitted models

 

 

Server name Sum of Sum of Z of best
Z/rank of five/rank
MULTICOM (human) 5 7.49/1 78.42/1
Zhang—Server 5 3.62/2 70.5 7/ 3
QUARK 5190/3 7193/2
an 35.07/4 51.79/6
Myprotein—rne 34.1 1/5 5 2.73/5
MULTICOM—CLUSTER 3 1.39/6 39.03/10
MULTICOM—CONSTRUCT 3 1.33/7 38.65/1 1
RBO_A1eph 3077/8 4065/9
BAKER—ROSETTASERVER 28.80/ 9 6 3.64/4
MULTICOM—NOVEL 25.71/10 43.43/ 7

 

five submitted models predicted by these predictors was reported in
Table 2. Other human server predictions were not considered in the
analysis here since they were not publicly available. It is shown that
MULTICOM performs better than all server predictors. Its total
Z—score of first models is around 4 points higher than the best server
predictor Zhang—Server, and its total Z—score of the best of five mod—
els is >6 points higher than the best server predictor QUARK. These
results demonstrate MULTICOM’s ability to rank a large pool of
models for selecting top one or five models. According to CASP11’s
official evaluation of all 143 human and server predictors,
MULTICOM was ranked third based on the sum of Z—score of the
first model and second based on the sum of Z—score of best of the
five submitted models. The MULTICOM’s outstanding perform—
ance in the extremely competitive CASP11 experiment demonstrates
that our large—scale model QA is powerful for ranking and selecting
good models from a pool of models of different quality.

In order to investigate types of the models selected by
MULTICOM and the contribution of individual structure pre—
dictors, we calculated the number of times that the models predicted
by each predictor were ranked within top five by MULTICOM.
Table 3 shows that the contribution of top 10 server predictors
whose models were selected by MULTICOM to refine to generate
the final predictions. It shows that a diverse set of server predictors
including Zhang-Server made significant contributions to the final
prediction, suggesting the large—scale QA used by MULTICOM can
reliably assess a very diverse set of models generated by different ter—
tiary structure predictors in the field.

To study how our large—scale model QA method improves model
ranking, we compared its performance with that of each individual
QA method and the two other simple consensus methods (one based
on the sum of 14 original QA scores and another based on the sum
of 14 Z—scores calculated from original scores). The first two col—
umns in Table 4 reports the average GDT—TS score of the first mod—
els selected by these QA methods for all 42 human targets and a
subset of 30 template—based human targets, respectively. The results
show that MULTICOM performs better than every individual QA
method, and sometime the improvement is substantial. And not sur—
prisingly, the multi-model QA methods outperformed single—model
QA methods on template—based human targets whose model pool
was often of good quality. For instance, a multi—model QA method
APOLLO ranks sixth on all human targets, but third on template—
based human targets. The third columns in Table 4 shows the aver—
age Z—score of the first models selected by different QA methods. It
is interesting to notice that the single—model QA methods tend to
have higher Z—score than the multiple—model QA methods. For ex—
ample, the multiple QA method APOLLO has a relatively high aver—
age GDT—TS score (0.338) of the first selected models, however, its

Table 3. The top 10 predictors ranked based on the total number times their models were selected by our MULTICOM
predictor on all the human targets or template-based (TBM) human targets only

 

 

Rank Servers on all human targets Num. on all Servers on TBM Num. on TBM
1 Zhang—Server 5 8 Zhang—Server 43
2 BAKER—ROSETTASERVER 36 BAKER—ROSETTASERVER 27
3 QUARK 29 QUARK 22
4 RBO_A1eph 29 myprotein—me 20
5 myprotein—me 28 an 19
6 an 21 Seok—server 14
7 Seok—server 17 RBO_A1eph 13
8 MULTICOM—REFINE 1 0 MULTICOM—REFINE 8
9 FUSION 7 RaptorX 4
10 RaptorX 5 FUSION 4

 

1e /§JO'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTCI/ﬁdllq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

i120

R.Cao et al.

 

average Z—score of the first selected models is lower than most single
QA methods. The reason is probably because the multiple—model
QA methods tend to work well on easy targets whose models have
similarly good quality and thus low Z—scores, whereas single—model
QA methods may select some good models for some hard targets
whose models are mostly bad, resulting in a high Z-score.

Considering average ranking is just one way of combining differ-
ent QA scores, we tested another two ways to combine QA scores
for comparison. The first one simply calculated the average of ori—
ginal 14 QA scores to rank models. The second one first converted
all original QA scores of each method into Z—scores, and then used
the average of 14 Z—scores to rank models. Table 4 shows that con—
sensus of 14—QA Z—scores performed best in terms of the average Z—
score of the top one models, whereas MULTICOM performed best
in terms of the average GDT—TS score of the top one models. The re—
sults demonstrate that the way of integrating different QA scores in-
ﬂuences the quality of the final ranking.

Moreover, we compared MULTICOM with a simple combin-
ation approach that used a good single—model QA method (i.e.
ProQ2) to rank models of very hard targets and a good clustering
method (APOLLO) to rank the models of other targets. If the max—
imum APOLLO pairwise score of the models of a target is <0.2, it is
considered a hard target, otherwise an easy target. The average
Z—score and GDT score of the top one model selected by this simple
combination method is 0.980 and 0.350, respectively, which is
higher than that (0.584 and 0.338) of APOLLO, but substantially
lower than that (1.364 and 0.374) of MULTICOM.

Furthermore, compared with the two other top—ranked consen—
sus methods participating in CASP11 experiment—TASSER (ranked
ninth in CASP11) and keasar (ranked 27th) that used several QA
methods according to the official CASP11 experiment,
MUTLICOM was rank third, demonstrating its effectiveness and
robustness.

We also used Wilcoxon signed ranked sum test to assess the sig—
nificance of the difference between MULTICOM and each individ—
ual QA method. The fifth column of Table 4 shows P—value of the
top one model’s Z—score difference between MULTICOM and each

QA method. According to 0.05 threshold, MULTICOM performed
significantly better than any individual QA method.

In addition, in order to test the impact of each single—model QA
method on the performance of the consensus approach, we tested
how removing each QA method may change the average Z—score of
top one model selected by the consensus ranking of the remaining
13 QA methods. The results were in column 6 in Table 4. According
to the results, the removal of MULTICOM—NOVEL caused the big—
gest decrease in the average Z—score of top one models selected by
the consensus method.

Moreover, we counted the total number of times one QA method
selected better models than all other QA methods. In the cases where
more than one QA method selected the same better model, all of
them were counted as better than others methods once. Table 5
shows that MULTICOM consistently selected better top models
more frequently than any other QA method. Interestingly,
SELECTpro only selected better model once (Table 5), yet it had the
higher average GDT—TS scores for all the top one models than the
other 13 individual QA methods (Table 4), suggesting that
SELECTpro selected top models with relatively higher GDT-TS
score for most targets, but not necessarily the best models compared
with other individual QA methods.

In addition to assessing the overall performance, we specifically
investigated two examples to illustrate how MULTICOM assessed
the quality of the models of the following two targets. The first case
is T0783—D2 (domain 2 of Target T0783). Figure 2A illustrates the
distribution of the GDT—TS scores of the models of this domain,
where most of the models actually have the true GDT—TS score less
than 0.2 (i.e. very low quality), some models have the GDT—TS score
around 0.4 (medium quality), and a few models have GDT—TS score
0.6 (relatively good quality). Figure 2B is the plot of true GDT-TS
scores of these models against their ranking predicted by
MULTICOM. It is shown that MULTICOM ranked the best model
with the highest GDT—TS score (e.g. nns_TSl) as no. 1. In this case,
all the individual single QA methods ranked this model within top
five, but a pairwise method ranked it at no. 19. Combining these in—
dividual rankings, the consensus ranking predicted by MULTICOM

Table 4. Comparison of MULTICOM with each OA method and the two different consensus methods (one based on 6 0A methods and an-
other one based on 14 0A methods) on the average GDT-TS score and Z-score ofthe top models selected, and the significance of difference

between each OA method and MULTICOM

 

 

QA method Ave. GDT—TS Ave. GDT—TS Ave. Z—score P—Value of Ave. Z—score
score on all score on TBM on all Z—score diff. removed
MULTICOM 0.374 0.425 1.364 — —
Consensus of 14 QA scores 0.369 0.420 1.217 — —
Consensus of 14 Z—scores 0.357 0.402 1.406 — —
SELECTpro 0.351 0.407 0.893 1.831e—05 1.338
ProQ2 0.343 0.387 0.887 1.19e—02 1.365
MULTICOM-NOVEL 0.340 0.383 0.861 5.612e—03 1.321
ModFOLDclust2 0.339 0.399 0.734 2.074e—04 1.356
APOLLO 0.338 0.403 0.584 9.331e—05 1.379
Dope 0.334 0.382 0.819 1.861e—03 1.360
Pcons 0.333 0.397 0.565 1.831e—05 1.325
ModelEz/a 0.333 0.378 0.870 9.840e—03 1.334
Dﬁre2 0.329 0.367 0.826 1.662e—03 1.360
QApro 0.328 0.371 0.783 2.889e—02 1.430
RWplus 0.327 0.373 0.752 5.193e—04 1.365
OPUS-PSP 0.326 0.366 0.793 5.784e—03 1.356
RF_CB_SRS 0.300 0.343 0.372 7.13e—05 1.365
Modelcheck2 0.297 0.347 0.559 1.192e—02 1.340

 

Italic font denotes single-model methods.

12 /§JO'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTCI/ﬁdllq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

Large-scale model quality assessment for improving protein tertiary structure prediction i121

 

Table 5. The total number times that each OA method performed
better than other OA methods on all human targets or all template-
based (TBM) human targets only

 

 

 

QA methods Frequency QA methods Frequency
on all on TBM
targets

MULTICOM 1 7 MULTICOM 1 1

QApro 12 QApro 8

ProQ2 1 1 ModelEz/a 7

ModelEz/a 9 ProQ2 7

Dﬁre2 9 Dope 7

Dope 9 RWplus 6

RWplus 8 D ﬁre2 6

MULTICOM- 8 MULTICOM- 6

N O VEL N O VEL

OP US-PSP 8 OP US-PSP 6

Modelcheck2 4 APOLLO 4

RF_CB_SRS 4 M odel check2 3

APOLLO 4 RF_CB_SRS 3

ModFOLDclust2 3 ModFOLDclust2 3

Pcons 2 Pcons 2

SELE CTpro 1 SELECTpro 1

Italic denotes single-model methods.

I!
III

.IIHHun-[J
:unrr-r: EL "15 .1an
_. E I_ _ r .3.

..;. n: . .-- . . . -- ..  -. .. .1 .. .-.-.
trlrt! 5111' TS HEIJFE ranking by IIII.T|[IIIII

 

Fig. 2. Tertiary structure prediction of domain 2 of T0783 (T0783-D2). (A)
The superposition of the MULTICOM human TS1 model on domain 2 with
the native structure. (B) The distribution of 191 models in the model pool.
(C). The plot of the true GDT-TS scores of models against their predicted
ranking

was able to select this model to combine with other three similar
models (nns_TS3, nns_TS2, and FFAS—3D_TS1) to generate a
refined model as final prediction. Figure 2C is the superposition of
this model with the native structure, which is an alpha—best—alpha
protein. Our final model has a well—predicted four—strand beta—sheet
in the middle and two well—positioned alpha helices in periphery.
The final GDT-TS score of this model is 0.625.

The second case is T0767—D1 (domain 1 of Target T0767).
Figure 3A shows the distribution of the true GDT—TS score for the
whole model pool. Most models are of low quality (i.e. the true

A .~ B

l; “Hun-III
I I

_|.
I
E
9:.
El
—|.
—-.
'44
n.
a
a:
.‘I

rrrtrltilIH. in} HUIII'JLHIII

 

Fig. 3. Tertiary structure prediction of domain 1 of T0767 (T0767-D1). (A)
The superposition of the MULTICOM human TS1 model on domain 1 with
the native structure. (B) The distribution of 195 models in the model pool.
(C) The plot of the true GDT-TS scores of models against their predicted
ranking

 

    
       
 

initial and ﬁnal model
.0 .0 .0

Ce between
.0

GDT-TS snore differen
.5 .c‘: .5

 

 

 

074 075 of
Initial GDT-TS score

Fig. 4. The plot of the difference between the initial GDT-TS scores before
model combination and the GDT-TS scores after model combination against
the initial GDT-TS scores of top one models of 42 targets

GDT—TS score around 0.25), which makes model QA difficult.
Therefore, three pairwise QA methods (APOLLO, Pcons and
ModFOLDclust2) failed to rank the models of good quality at or
near the top, whereas some single—model QA methods ranked them
higher. Figure 3B is the plot of the true GDT—TS scores of these mod—
els against their ranking predicted by MULTICOM. It is shown that
our large—scale model QA combining both single— and multi-model
QA methods was able to rank the third best model at the top, even
though it missed the best model BAKER-ROSETTASERVER_TS2
in the model pool. The initial model selected by MULTICOM was
Zhang—Server_TS5 with GDT—TS score 0.5658. Figure 3C visualizes
the superposition of the predicted model and the native structure. It
is shown that the beta sheet was predicted rather accurately,
whereas the alpha helices were only partly correctly predicted.
Finally, we investigated if the model combination could refine and
improve the quality of the selected models. Figure 4 shows the differ—
ence between the initial GDT—TS scores of the models before refine-
ment and the GDT—TS scores of the final models after the refinement
process on 42 CASP11 human targets. The GDT—TS scores of the

112 /§JO'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTCI/ﬁdllq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

i122

R.Cao et al.

 

models of 19 targets were increased by the model combination, those
of another 19 targets were decreased, and those of the remaining four
targets stayed the same. The average change of GDT—TS scores of all
42 targets was 0, suggesting the refinement process did not improve
the global quality of the models on average, which is consistent with
the observation on the performance of most current model refinement
protocols (Bhattacharya and Cheng, 2013).

4 Conclusion

We developed a large—scale model QA technique in conjunction with
model clustering and refinement to improve protein tertiary struc—
ture prediction. Inspired by the previous work (Pawlowski et al.,
2008) that integrated several primary QA methods, our method that
combined a large number of protein model QA methods reliably
and consistently improved protein model ranking—one of the major
challenges of protein structure prediction. For the first time, we
demonstrate that this large—scale consensus QA approach is more ro—
bust and accurate than any individual quality method by integrating
their strength together. Our tertiary structure prediction based on
this method outperformed all the server predictors during the very
competitive CASP11 experiment in 2014. The CASP11 official as-
sessment also ranked our method as one of the top three best tertiary
structure prediction methods on all the CASP11 human targets. This
outstanding performance demonstrates our large—scale model QA
approach is a promising direction to advance the state of the art of
protein model ranking and selection. Moreover, our approach
adopts an open QA system, into which, adding more complimentary
methods may potentially improve the ranking, but incorporating re-
dundant methods does not necessarily lead to an improvement.
However, our general combination approach demonstrates the im—
portance of developing more individual QA methods and the possi—
bility of optimally combining them together to advance the field of
protein structure prediction.

Funding

The research was partially supported by an NIH grant (R01GM093123) to
].C. We thank anonymous reviewers for valuable comments.

Conﬂict of Interest: none declared.

References

Berman,H.M. et al. (2000) The protein data bank. Nucleic Acids Res., 28,
235—242.

Bhattacharya,D. and Cheng,]. (2013) 3Dreﬁne: consistent protein structure re-
ﬁnement by optimizing hydrogen bonding network and atomic-level energy
minimization. Proteins Struct. Funct. Bioinform., 81, 119—131.

Bowie,].U. et al. (1991) A method to identify protein sequences that fold into a
known three-dimensional structure. Science, 253, 164—170.

Cao,R. et al. (2014a) Designing and evaluating the MULTICOM protein local
and global model quality prediction methods in the CASP10 experiment.
BMC Struct. Biol., 14, 13.

Cao,R. et al. (2014b) SMOQ: a tool for predicting the absolute residue-speciﬁc
quality of a single protein model with support vector machines. BMC
Bioinformatics, 15:120.

Cheng,]. et al. (2005) SCRATCH: a protein structure and structural feature
prediction server. Nucleic Acids Res., 33, W72—W76.

Cheng,]. et al. (2012) The MULTICOM toolbox for protein structure predic-
tion. BMC Bioinformatics, 13, 65.

Dobson,C.M. et al. (1998) Protein folding: a perspective from theory and ex-
periment. Angewandte Claemie International Edition, 37, 868—893.

Eisenhaber,F. et al. (1995) Protein structure prediction: recognition of pri-
mary, secondary, and tertiary structural features from amino acid sequence.
Crit. Rev. Biochem. Mol. Biol., 30, 1—94.

Faraggi,E. et al. (2012) SPINE X: improving protein secondary structure pre-
diction by multistep learning coupled with prediction of solvent accessible
surface area and backbone torsion angles. ]. Comput. Claem., 33, 259—26 7.

Jones,D.T. et al. (1992) A new approach to protein fold recognition. Nature,
358, 86—89.

Kabsch,W. (1976) A solution for the best rotation to relate two sets of vectors.
Acta Crystallogr. Sec. A, 32, 922—923.

Kabsch,W. and Sander,C. (1983) Dictionary of protein secondary structure:
pattern recognition of hydrogen-bonded and geometrical features.
Biopolymers, 22, 25 77—2637.

Kallberg,M. et al. (2012) Template-based protein structure modeling using the
RaptorX web server. Nat. Protoc., 7, 151 1—1522.

Li,]. et al. (2013) Designing and benchmarking the MULTICOM protein
structure prediction system. BMC Struct. Biol., 13, 2.

Li,S.C. et al. (2008) Fragment-HMM: a new approach to protein structure
prediction. Protein Sci., 17, 1925—1934.

Lu,M. et al. (2008) OPUS-PSP: an orientation-dependent statistical all-
atom potential derived from side-chain packing. ]. Mol. Biol., 376,
28 8—301.

McGufﬁn,L.]. and Roche,D.B. (2010) Rapid model quality assessment for
protein structure predictions using the comparison of multiple models With-
out structural alignments. Bioinformatics, 26, 182—188.

Mishra,A. et al. (2013) Capturing native/native like structures with a physico-
chemical metric (pcSM) in protein folding. Biochim. Biophys. Acta (BBA)
Proteins Proteomics, 1834, 1520—1531.

Pawlowski,M. et al. (2008) MetaMQAP: a meta-server for the quality assess-
ment of protein models. BM C Bioinformatics, 9, 403.

Randall,A. and Baldi,P. (2008) SELECTpro: effective protein model selection
using a structure-based energy function resistant to BLUNDERs. BMC
Struct. Biol, 8, 52.

Ray,A. et al. (2012) Improved model quality assessment using ProQ2. BMC
Bioinformatics, 13, 224.

Rigden,D.]. (2009) From Protein Structure to Function wit/7 Bioinformatics.
Springer, Dordrecht.

Rykunov,D. and Fiser,A. (2007) Effects of amino acid composition, ﬁnite size
of proteins, and sparse statistics on distance-dependent statistical pair po-
tentials. Proteins Struct. Funct. Bioinform., 67, 559—568.

Shen,M.Y. and Sali,A. (2006) Statistical potential for assessment and predic-
tion of protein structures. Protein Sci., 15, 25 07—2524.

Shortle,D. et al. (1998) Clustering of low-energy conformations near the na-
tive structures of small proteins. Proc. Natl. Acad. Sci., 95, 1 1 15 8—1 1 162.
Simons,K.T. et al. (1997) Assembly of protein tertiary structures from frag-
ments with similar local sequences using simulated annealing and Bayesian

scoring functions. ]. Mol. Biol., 268, 209—225.

Wallner,B. and Elofsson,A. (2006) Identiﬁcation of correct regions in protein
models using structural, alignment, and consensus information. Protein Sci.,
15 , 900—913.

Wang,Z. et al. (2009) Evaluating the absolute quality of a single protein model
using structural features and support vector machines. Proteins Struct.
Funct. Bioinform., 75, 63 8—647.

Wang,Z. et al. (2010) MULTICOM: a multi-level combination approach to
protein structure prediction and its assessments in CASP8. Bioinformatics,
26, 882—888.

Wang,Z. et al. (2011) APOLLO: a quality assessment service for single and
multiple protein models. B ioinformatics, 27, 1715—1716.

Xu,D. and Zhang,Y. (2012) Ab initio protein structure assembly using con-
tinuous structure fragments and optimized knowledge-based force ﬁeld.
Proteins Struct. Funct. Bioinform., 80, 1715—1735.

Yang,Y. and Zhou,Y. (2008) Ab initio folding of terminal segments with sec-
ondary structures reveals the ﬁne difference between two closely related all-
atom statistical energy functions. Protein Sci., 17, 1212—1219.

Yonath,A. (2011) X-ray crystallography at the heart of life science. Curr.
Opin. Struct. Biol, 21, 622—626.

Zhang,]. et al. (2010) MUFOLD: a new solution for protein 3D structure pre-
diction. Proteins Struct. Funct. Bioinform., 78, 1137—1152.

112 /§JO'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTCI/ﬁdllq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

Large-scale model quality assessment for improving protein tertiary structure prediction i123

 

Zhang,]. and Xu,D. (2013) Fast algorithm for population-based protein struc-
tural model analysis. Proteomics, 13, 221—229.

Zhang,]. and Zhang,Y. (2010) A novel side-chain orientation
dependent potential derived from random-walk reference
state for protein fold selection and structure prediction. PloS One, 5,
e15386.

Zhang,Y. (2008a) I-TASSER server for protein 3D structure prediction. BMC
Bioinformatics, 9, 40.

Zhang,Y. (2008b) Progress and challenges in protein structure prediction.
Curr. Opin. Struct. Biol., 18, 342—348.

Zhang,Y. (2014) Interplay of I-TASSER and QUARK for template-based and
ab initio protein structure prediction in CASP10. Proteins Struct. Funct.
Bioinform., 82, 175—187.

Zhang,Y. and Skolnick,]. (2004) Scoring function for automated assessment
of protein structure template quality. Proteins Struct. Funct. B ioinform., 57,
702—710.

112 /§JO'S{eu1no [p.IOJXO'SOTlBIHJOJUTOTCI/ﬁdllq 11101; popeommoq

9IOZ ‘09 lsnﬁnv uo ::

