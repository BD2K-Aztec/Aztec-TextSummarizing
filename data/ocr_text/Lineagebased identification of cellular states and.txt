Vol. 28 ISMB 2012, pages i250-i25 7
doi: 1 0. 1 093/bioinforma tics/b ts204

 

Lineage-based identification of cellular states

and expression programs

Tatsunori Hashimoto 1, Tommi Jaakkolal, Richard Sherwood2, Esteban O. Mazzoni3,

Hynek Wichterle3 and David Gifford 1’*

1Department of Computer Science and Electrical Engineering, Massachusetts Institute of Technology, Cambridge,

MA 02139

2Brigham Women’s Hospital and Harvard Medical School, Boston, MA 02115 and
3Departments of Pathology, Neurology and Neuroscience, Center for Motor Neuron Biology and Disease and
Columbia Stem Cell Initiative, Columbia University Medical Center, New York, NY 10032, USA

 

ABSTRACT

Summary: We present a method, LineageProgram, that uses the
developmental lineage relationship of observed gene expression
measurements to improve the learning of developmentally relevant
cellular states and expression programs. We find that incorporating
lineage information allows us to significantly improve both the
predictive power and interpretability of expression programs that are
derived from expression measurements from in vitro differentiation
experiments. The lineage tree of a differentiation experiment is a
tree graph whose nodes describe all of the unique expression
states in the input expression measurements, and edges describe
the experimental perturbations applied to cells. Our method,
LineageProgram, is based on a log-linear model with parameters
that reflect changes along the lineage tree. Regularization with
L1 that based methods controls the parameters in three distinct
ways: the number of genes change between two cellular states,
the number of unique cellular states, and the number of underlying
factors responsible for changes in cell state. The model is estimated
with proximal operators to quickly discover a small number of key
cell states and gene sets. Comparisons with existing factorization,
techniques, such as singular value decomposition and non-negative
matrix factorization show that our method provides higher predictive
power in held, out tests while inducing sparse and biologically
relevant gene sets.

Contact: gifford@mit.edu

1 INTRODUCTION

The directed differentiation of embryonic stem (ES) cells into
therapeutically important cell types holds great potential for
regenerative medicine. Identifying stage— speciﬁc transcription factor
candidates for the directed differentiation of ES cells has been
difﬁcult, and the computational identiﬁcation of lineage—associated
transcription factor programs would signiﬁcantly beneﬁt this
process.

LineageProgram is a new method that identiﬁes the experimental
protocols that result in the same cellular states (Figure 1).
It further decomposes these states into interpretable expression
programs, which we deﬁne to be sets of co—varying genes. In
contrast to analyzing the correlation between genes, we deﬁne a

 

*To whom correspondence should be addressed.

‘13
m
ES (embryonic) {$1, $4}

ES (embryonic) {$1}

l”\

{$2} {$4} DE(deﬁnitive endoderm){$2} ES + 30$2{$3}
a1 1 \ (11,113 1
‘13
{$6} {$5} GTE(gut tube endoderm){$6, $5}

Fig. 1. Example experimental tree (left) and lineage tree (right).
LineageProgram attempts to identify salient cellular states and covarying
gene sets. Different treatments may result in the same cell state (a1 and a3
after deﬁnitive endoderm), while some treatments may have no effect (a3 at
stem cell). Our goal is to merge and prune these types of treatments

program to be sets of genes that co—vary during a differentiation
event. Analysis of developmental expression data has revealed the
existence of expression programs regulating pluripotency across
many cell types [9] as well as lineage—speciﬁc programs. We
provide a principled method that discovers both types of programs.
LineageProgram is a log—linear model that uses latent factors
and L1 regularization to obtain sparse parameters, structures and
spectra.

Our primary goal is to estimate expression programs that
are informed and improved by lineage information. To our
knowledge, LineageProgram is the ﬁrst method to approach
this problem. Prior work in the estimation of developmental
expression programs has used biclustering, factor or topic
decompositions, mixture models and self organizing maps,
all of which simultaneously identify expression programs and
sample clusters, but without incorporating information from the
experimental lineage. As shown in studies of expression time series,
treating dependent expression data as independent observations
can lead to signiﬁcant loss of information [3]. Separate prior
work has focused on estimating lineage trees and lineage states
in the absence of expression programs. Differentiation has been
modeled both as a time series without branching and de novo tree
estimation.

The remainder of this article presents the LineageProgram
model (Section 2), a comparative analysis of LineageProgram with
other methods on lineage—associated expression data from motor
neuron and pancreatic development (Section 3) and a conclusion
about what we have learned about using lineage information
(Section 4).

 

© The Author(s) 2012. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non—Commercial License (http://creativecommons.org/licenses/
by—nc/3.0), which permits unrestricted non—commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.

112 /§JO'SIBUJn0[pJOJXO'SOllBIHJOJUTOTQ/ﬁdnq 11101; prBOIIIAAOG

9IOZ ‘09 lsnﬁnv uo ::

Cellular states and expression programs

 

2 THE LINEAGEPROGRAM METHOD
Model

LineageProgram operates on N expression measurements of P
genes made on an experimental tree with M nodes, where the
root corresponds to the ES cell state and edges correspond to
experimental interventions. We represent the cell state at a node
i with the probability vector 67, whose kth component is the
probability that gene k is transcribed. The expression measurement
is modeled as proportional to a multinomial draw from 6. Methods
such as GeneProgram [10, 16] have successfully used this discrete—
count model for expression. Our objective function is the continuous
extension of the multinomial likelihood function. We will show
later that this natural continuous extension exists as a discretization
limit and allows us to handle continuous data such as microarray
measurements directly.

A differentiation event is a change in 6, which we represent by
a log—odds change 77. The change of a gene k from a parent state i
with vector 6,- to child state j is written as

6 9(i,k)eXP(77(j,k))
-’k : —.
(I ) 216(i,l)exp(77(j,l))

This formulation of a log—odds count model has been shown to
outperform analogous Latent Dirichlet Allocation type models [8].

We represent the root stem cell state in the experimental tree as
a log probability vector gb of size P, and the remaining states are
represented as log—odds changes from their parent. For each node
on the cell state tree, we write the expression probability as the sum
of log changes along its path from the ES state and the ES cell
expression qb. Let  be the set of nodes along the path from node j
to the ES state. Then the probability of observing gene k at node j
is given as follows:

6 eXP(Zie7Dj 77(i,k) +61%)
',k = .
(I ) ZleXP(Zie73j 77(i,1)+¢1)

We represent the experimental structure as two matrix
multiplications: an M ><M path sum matrix T with 7:1 if
i6 73]- and zero otherwise and an N X M observation matrix D with
DOJ) = 1, if the tth expression measurement was made at node j and
zero otherwise.

The parameters are represented as two matrices, M X P parameter
matrix 77 (which we will regularize to be sparse and low—rank) and
a l X P ES expression vector qb.

Given the N X P data matrix ka), which we interpret to be
proportional to counts of an expression event, our log likelihood
takes the form

 

 

llh(77) oc ZXmmlog

( exp ((DT77)(t,k) +¢k) )
t,k

Zz eX10((DT77)(t,1) +051)

We design our regularization with three objectives: there should
be few genes changing per differentiation event (L1 penalty on 77),
few unique differentiation events (L2 penalty on the rows of 77) and
few programs needed to explain the lineage (trace norm penalty on
77). The L1 penalty is the sum of absolute values of 77, which induces
77 to have entries with zeroes. The L2 penalty is the sum of the row—
norms of 77, which induces 77 to have rows that are all zeroes. Finally,

the trace norm penalty is the sum of the singular values of 77, which
induces 77 to have low rank.

With the regularization and deﬁning the shorthand notation |77 ll =
Z”- |77(,-,j)| and ||77i | lg = (Zj 77(2i,j))(1/2), the overall objectivef takes
the form

f(77)=—llh(77)+)»1|77|1+kzzllmllz+k3llnllm

I

We show that discretizing the data, X, to some precision ,8, and
taking the limit as ,8 —> 00 has the equivalent minima (up to scaling
and the zero set) by using X without discretization. Note that the
gradient of the log—likelihood function takes the form

. LXt k/Bl
1 X ’—
781310;“ t’k/IBJ)(Zk/LXt,k’/IBJ

_ exp((DT77)(t,k)+¢k) )
:15 exp ((DTﬂ)(7,k’) +¢k’)
Xt,k

1
_ X —
9 73:7; “(2

k/Xt’k/

 

 

_ exp((DT77)(t,k)+¢k) )
Zk7exp((DT77)(t,k’)+¢/c’) ,

which is the continuous extension of llh up to a constant  The
regularization terms |77|1, ||77||2 and ||77| lTR are convex, but not
strictly convex, so the optima of the continuous extension and the
limit can differ up to elements of the zero set. Testing both small
discretization and the continuous extension, we ﬁnd no difference
in results, but for completeness we use a threshold of le—5 to set a
small neighborhood near zero to be part of the zero set.

Finally, we deﬁne the concept of an expression program as a set
of basis vectors spanning 77. The trace norm regularization implicitly
penalizes the rank of matrix 77, and for large X3, 77 will have small
rank and can be represented as the linear combination of a few
‘basis’ programs. We choose the singular value decomposition of T77
as our program decomposition. The ﬁrst k programs have a natural
interpretation as the best rank—k approximation of the unnormalized
log expression parameters.

Inference

The advantage of our method over topic model formulations is the
convexity of our objective f, which guarantees fast convergence to
the global maxima. Our overall inference strategy is to use gradient
descent on the likelihood combined with proximal steps on each
of the regularization terms. To speed convergence, we also use the
accelerated proximal gradient method by.

The proximal gradient method allows us to efﬁciently optimize
convex functions of the form f = f * —i— g, where f * is convex
differentiable and g is convex and continuous. f is optimized with
a gradient step on f * followed by a proximal operator, which uses
a quadratic approximation of f * to optimize f * —i— g. Given x7, we
generate the next iterate xt+1 with the following update

xt+1=Proxg,€(xt— 6Vf*(xt))

Proxg,6(xt)=argminy | Ix; — Y||/2—l—6g(Y).

 

i251

112 /§JO'smurnofproatxosor1au110jurorq//zd11q 11101; prBOIIIAAOG

9IOZ ‘09 lsnﬁnv uo ::

'lZHashimoto et aI.

 

(a) Edge sparsity ([3) Program sparsity

I _$.
wwaa—e_ ww a

7231641- 7231641-
'0 'O
O O
O O
:E :E
-—231642— ——231642-
2 2
T T
3 5’
—231643 — —231643 —
. ﬁ 0 $
0
—231644 - —231644 -
l l l l l l l l l l l l l l l l l l l l l l l l
8 10 11 12 13 18 19 21 22 27 29 30 31 7 9 10 11 12 13 14 15 20 24 25
number of edges number of programs

Fig. 2. The goodness of ﬁt as measured by held—out likelihood shows that the optimal model has few of edges and programs

 

 

 

 

 

 

 

 

 

 

 

(a) Held-out errors on the full lineage tree (b) Held-out error trained only on pancreatic lineage
pancreas motor neuron
1600 — l
4000 -
1400 -
3000 -
h 1200—
'é a
'6 a:
g “3
g . E 1000-
5 2000— a,
.C

 

 

 

 

800 -

waeeia. m

I I I I I I I I
LineageProgram SVD LDA NMF LineageProgram SVD LDA NMF

 

 

 

 

LineageProgram svD LDA NMF
method method

Fig. 3. Cross—validated mean square errors on the full cell tree shows good predictive performance by LineageProgram

In our case, our log likelihood is concave differentiable, and there
are three convex continuous functions: g1,g2 and g3, corresponding
to each regularization term.

f* 81 g2
Ha

f(77)=—llh(77)+)»1l77l1+kzzllmll

l

83
K—“h‘x
+A3H77HTR-

Vf*(77j)k = TT 2 (X(t,k)

t

 

_ 2X eXp((DT77)(t,k)+¢k) )
N 0’0 ZICXP((DT77)(t,l)+¢1) '

The proximal operators for g1 and g2 are the soft—threshold operators

nan—6M =77<i,j)>6?~1
PI‘OXgl,€(77(i’J-)): 77(i’j)-|-6A1 277(i’j) <—6)\.1
0

The gradient for f * is given by the difference between predicted and
I|77(i,j)| < 6k1

observed counts

 

i252

112 /§JO'smurnofproatxosor1au110jurorq//zd11q 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

Cellular states and expression programs

 

 

. - Pancreatic cell
o Motor neuron

oooooooooo oo oo - ES cell

 

 

 

Fig. 4. Murine differentiation tree representing the derivation of 88
pancreatic and 17 motor neuron expression measurements. Each edge is
an experimental treatment and each vertex a experimental state

)Illmllz >€}~2

Illmllz <6l2'
For g3, the proximal operator can be written in closed form in terms
of its singular value decomposition (SVD) [14]. Let 77 = U DVT and
max(D — A36, 0) be the SVD and entrywise subtraction followed by
thresholding, then the proximal operator takes the form

||2

PI‘OXgZ,€(77(i’k))= { 3(l’k)( 2 “777'

Proxg3,€(77)= Umax(D—)t3e,0)VT.

At each step of the optimizer, we take some x; and step size 6 and
produce the next iterate with

xt+1 = Proxg3 ,6 (Prong (Proxg1 ,6 (x; — 6Vf*(xt)))).

The sequential proximal gradient converges for our objective due
to separability. We also make use of the accelerated gradient method
by, which ﬁnds a sequence of x; which converges toward the optima,
using an internal variable y; and a magniﬁcation of the gradient, at
to increase convergence rates near the mode.

Xt+1 = PrOXg3,6(Pr0Xg2,e(Pr0Xg1,e(yr — 6 Vf* 07))»

(1+ 4a§+1)
dbl—12+

at—l

 

Yt+1=xt+ (X7 _xt—l)-

at—l—l

In the context of a single proximal operator, this produces
the optimal quadratic ﬁrst—order convergence rate. In our case,
the multiple proximal operators do not provide a guaranteed
convergence rate, but in practice, we ﬁnd the accelerated gradient
makes convergence signiﬁcantly faster. An implementation of this
inference method as well as the results of our analysis are available
from our website at http://psrg.csail.mit.edu/resources.html

For the remainder of the article we use a convergence tolerance of
10‘5 and hot starts, which allows us to quickly ﬁnd solutions over
a list of candidate 21 values by using the optima of one problem to
initialize a new problem with similar regularization penalties. This
allows us to obtain the regularization path over 21 for the 105 array
experiments below within 10 min on a computer with a Core 2 Duo
e6300 CPU and 2 GB of memory.

Frequency
3
I

 

 

 

 

 

| | | | | |
0 10 20 30 40 50

change in held out error

Fig. 5. Measureing held—out error in models with and without trace norm
penalty shows that removing trace norm penalty causes large increases in
held—out error for 7 out of 16 held—out sets

Inference with this method is fast enough that we are able to ﬁt
the model across a 50 X 50 X 50 grid of all valid 21 ,22 and 23, which
we use to set the regularization parameters as described in the next
section.

3 RESULTS

The algorithm was tested on directed differentiation experiments
for murine pancreatic progenitors and motor neurons, as shown in
Figure 4. Both lines were produced using known differentiation
protocols. The pancreatic line has a large number of states, but
relatively few replicates, while the motor neuron line has multiple
replicates per state. The 88 microarray measurements of the
pancreatic line were performed with Illumina bead arrays, while the
17 in the motor neuron line were performed with Affymetrix 430a2
microarrays. We rank—match the Illumina data to the Affymetrix data
in order to reduce the data to the same scale.

The quantile normalization technique is described in further detail
by Irizarry et al (2003).

Cross-validation procedure

We use cross—validation to estimate the regularization parameters 21 ,
22 and 23 over the 50 X 50 X 50 grid of all nondegenerate values. For
every experimental state with more than one observation, we include
one observation in the training set and include the rest in the held—out
set. We choose to leave out observations per—node rather than over all
observations, since we must leave at least one observation at each
node in order to make a prediction at the node. The performance
of the model is measured in terms of held—out likelihood, which
indicates goodness of ﬁt, and squared error, measures predictive
performance.

Cross—validation results in Figure 2 show the existence of a sharp
drop in predictive power around a dozen edges and programs;
this sharp transition suggests that there exists a necessary level of
regularization for our algorithm to generalize well.

 

i253

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 11101; prBOIIIAAOG

9IOZ ‘09 lsnﬁnv uo ::

'lZHashimoto et aI.

 

 

(a)

1.0

LineageProgram

—SV

 
 
    

NMF
LDA

 

 

0.6
I

fraction of GO terms enriched
0.4

0.2

 

 

0.0
I

 

| | | |
0.00 0.02 0.04 0.06

p-value

 

O-

V

0 7
|

LineageProgram
SVD

NMF
LDA

 

 

0.5 0.6
|

fraction of program explained by largest category
0.4

0.3
|

 

 

 

 

program rank by specificity

Fig. 6. Gene set enrichment in Gene Ontology (GO) and manually curated developmental gene sets show programs recovered by LineageProgram are

biologically relevant

The optimal regularization parameters discovered by cross—
validation signiﬁcantly penalize both the number of edges and
rank of the parameter 77. At this optimal value, we ﬁnd that the
estimated cell state tree in Figure 8 is signiﬁcantly sparser than the
experimental tree. We also found that a strong L1 penalty dominated
the group L2 penalties, resulting in 21 controlling both structural and
parameter sparsity.

Necessity of trace norm regularization

The trace—norm penalty is a critical part of modeling large branching
factors. The L1 and L2 penalties shrink the changes between nodes
to zero and bring the probabilities at each child—node toward their
common parent. Therefore, the two edge penalties, 21 and 22,
control whether leaves differ at all from their parent, rather than how
they differ. In contrast, the trace norm penalty restricts directions in
which the leaves can differ by forcing the leaves to lie within a small
subspace near their parent.

To test the necessity of trace norm regularization, we compared
our model with and without trace—norm regularization. For both
models, we ﬁt the regularization parameters through cross—validation
and compared the mean squared error on a common held—out set
of 16 arrays selected by removing one array from each node with
replicates. Results in Figure 5 indicate that without the trace norm
penalty, 7 of the 16 held—out observations show signiﬁcant increases
in squared error. All seven of these observations are children of
the highest degree node in Figure 8 suggesting that the trace norm
penalty plays a key role preventing overﬁtting on highly branching
data.

Methods compared

Our algorithm was compared against three existing classes of
approaches: SVD, non—negative matrix factorization (NMF) and
latent Dirichlet allocation (LDA) on both held—out prediction error
and quantitative program metrics. We were restricted to considering
non—lineage—informed methods since we did not ﬁnd any lineage
informed methods in the literature.

For each competing method, we tested major variants of the
algorithms and chose to compare only to the best variant. For
SVD, we tried direct decomposition of the data, mean subtraction
[resulting in Principal Component Analysis (PCA)] and mean
subtraction on pancreatic and motor neuron branches; simple mean
subtraction outperformed the others and is shown here. For NMF,
we tested Kullback—Leibler (KL) divergence and square distance
minimization objective proposed by as well as sparse NMF; we use
KL divergence minimization. For LDA, we used the collapsed Gibbs
sampler as well as variational Bayes updates and found the Gibbs
sampler with 20 000 samples to perform best.

Low cross-validation error on branching data

The held—out mean squared prediction error measures the
generalization performance of each of the algorithms. We split the
expression measurements into cross—validation and held—out sets
selecting 16 replicate experiments one from each node with more
than one measurement. The cross—validation set is split into training
and test sets using the procedure described in the the cross—validation
section. The models are ﬁt on the training data and we calculate the
squared error between the predicted values from the model and the
test data.

Training on the full set of motor neuron and pancreatic data
(Figure 3a), we ﬁnd that overall, LineageProgram has the lowest
mean squared prediction error. On lineages with no branching, we
would expect to see SVD perform best due to its direct minimization
of squared error. We ﬁnd that on the motor neuron lineage with no
branching, our algorithm performs like SVD. In the worst case of
non—branching data, our algorithm compares favorably to current
factorization methods. Interestingly, LDA degenerates on the motor
neuron held—out sets with behavior consistent on both training
methods and across multiple replications. We ﬁnd that LDA overﬁts
on the later stage pancreatic states at the expense of the motor neuron
states, which is consistent with behavior observed in prior work [8].

To rule out the possibility that the low performance of the
competing algorithms was due to the inclusion of two differing array

 

i254

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 11101; prBOIIIAAOG

9IOZ ‘09 lsnﬁnv uo ::

Cellular states and expression programs

 

methods
I LineageProgram SVD NMF LDA

600 -

500 -

400 -

300 -

number of genes

200 -

100-

 

l I l I
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.55
average change in odds

Fig. 7. The distribution of average odds change over all programs per gene
shows that LineageProgram produces sparse programs with many genes at
zero and few dominant genes

types in the pancreatic and motor lineages, we re—ran the test, ﬁtting
and evaluating only on the pancreatic lineage (Figure 3b).

The tests on the pancreatic lineage show nearly the same results
for all methods, with all methods, particularly LDA, performing
slightly better. However, the general ordering of the methods remains
the same, and the performance results are due to the inability
of current methods to handle the large branching factors in the
pancreatic lineage rather than the combination of multiple lineages.

Comparison of program quality

Although methods such as SVD can reconstruct expression values
well, these program decompositions are often difﬁcult to interpret
biologically. Therefore, we quantify the quality of the decomposition
in terms of sparsity and biological relevance. First, we would hope
for each program to have several dominant genes in order to produce
candidate genes for further analysis. We measure this objective
by the coefﬁcient distribution of the programs. Second, we want
each program to encode for biologically meaningful sets of genes.
As with prior work [10], we measure Gene Ontology (GO) term
enrichment as a proxy for biological relevance. Finally, we want each
program to map to a unique developmental gene set. We measure
this objective with the proportion of the program explained by the
dominant developmental gene set.

Fewer genes are used to model the lineage

To measure sparsity, we normalize the programs from all methods
to have unit norm and measure the L2 norm of each gene over all
programs. The L2 norm is measure of average change in odds across
programs. If there are a few genes dominating each program, we
would expect to see only a few genes with high L2 norm. Our goal
is to recover interpretable programs, where each program contains
a few key genes with large and unique activations. Our results in
Figure 7 show that LDA and LineageProgram both produce a few
large coefﬁcients and a large number of coefﬁcients within machine
precision of zero. The dominance of few genes in each program

allows us to label each program with a set of representative genes
in Table 8.

Higher biological relevance of extracted programs

To measure the biological relevance of the discovered programs, we
performed GO enrichment analysis using the weighted Kolmogorov
statistic from gene set enrichment analysis. Plotting GO enrichment
across all programs as a function of Benjamini Hochberg corrected
P—values (Figure 6a), we ﬁnd that LineageProgram achieves
signiﬁcantly higher GO enrichments when compared to the other
methods. The higher GO enrichment across all P—values suggests
that the programs recovered by LineagePro gram more closely match
subtrees of the GO annotation than those recovered by competing
methods. If this were purely a result of chance, we would expect
LineageProgram to outperform the others on a small subset of
P—value cutoffs, rather than across all cutoffs. In combination with
our sparsity results, this suggests that LineageProgram discovers
small sets of biologically relevant genes in each program.

To ensure that GO enrichment is in developmentally relevant
categories, we also test the uniqueness of each program. In an
ideal decomposition, each program would encode a different aspect
of the developmental process. We use six developmental gene
sets found in the literature, including four GO categories (‘stem
cell maintenance’,‘endocrine pancreas differentiation’,‘embryonic
skeletal development’, and ‘anterior posterior development’), as
well as marker genes for motor neurons (Pax6, Mnx], Isl], thl
and th3) [13] and pancreas (Proxl, de1, H179 and ka6—1) [15].
For each program, we measure the proportion of the program’s L2
norm that is accounted by genes in the most activated category.
The results in Figure 6 show that LineageProgram as well as NMF
have programs closesly matching known differentiation programs.
These correspond to the loss of pluripotency; LDA and SVD
were unable to discover programs that could be mapped solely to
pluripotency. Importantly, LineageProgram maintains a relatively
high correspondence to known gene sets across most of its programs,
indicating its ability to encode small, highly speciﬁc gene modules
corresponding to both the pancreatic and motor neuron lineages.

Analysis of full lineage data

Finally, we train LineageProgram on the full dataset to estimate
lineage programs and use cross—validation on replicates to estimate
regularization parameters. The resulting tree is sparse, with only two
branching points (Figure 8). The ﬁrst branching point corresponds
to the split between motor neuron and pancreatic lineages. The other
branching point differentiates a successful Day 6 pre—endoderm
differentiation from a late—stage Sox17 overexpression experiment.

We compared the lineage tree from the model with a manual
annotation produced separately from this project. Taking only
the vertices corresponding to known cellular states, we match
the manual curation almost exactly, successfully reducing all the
branches before the Day 4 preendoderm and only mis—merging the
Day 5 endoderm with the Day 6 posterior foregut. The regularization
and cross—validation have successfully pruned nearly all spurious
branches of the linage tree.

The Sox17 branch in Figure 8 represents a late—stage experiment
aimed at recovering competency in Day 8 endoderm using
Sox17 overexpression. Although the experiment shows signiﬁcant
expression level differences from the Day 4 pre—endoderm, we have

 

i255

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 1110131 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

'lZHashimoto et aI.

 

ES Cell

[- Day 3
Mesoendoderm
1 7
Day 4

68 12

Day 7 MN

x /k

 
  
 

 

Pancreatm

Progenitor

   
 
     
       

 
 
  
 

    
  
  

Day 5
Endoderm
+Sox17

Day 5 Endoderm
and

 
 

Day 6 Posterior
Foregut

kas-1(+)
Cdx1(+)
ch2(+)

 

 

 

 

J

Model estimate of lineage tree

(b)

ES Cell

Day 3
Mesoendoderm

    
          
      

  

  

Day 4
Pre-endoderm

Day 5
Endoderm
. Day 6 Posterior
Gay 6 Negativa C Foregut >

Manually annotated pancreatic subtree

 
       
   

Day 6
Intenstinal
Endoderm

 

 

 

 

No.| Type | Description I GO terms | Genes
1 Global Loss of pluripotency (—)Stem cell maintenance* (—)Nanog ( — )Pou5f1
2 Pancreatic General endoderm speciﬁcation (+)Endocrine pancreas speciﬁcation* (—)Gbx2 (—)Sox2
3 Neural Neural positioning program (+)Embryonic skeletal system morphogenesis** (+)Hoxb5,Hoxc4,Hoxa3

(+)anterior/posterior formation*

 

Pancreatic Suppression of mesodermal differentiation

(—)axial mesoderrn development* (—)th1

 

Pancreatic Endodermal speciﬁcation

(+)endodermal cell fate speciﬁcation*

(+)ka6—1 (+)an1b

 

(+)Embryonic skeletal system morphogenesis* (+)Is11 (+)MnX1 (—)Pax6

 

Global Suppression of alternative paths

None below 0.05 (—)Hand1 (—)Hoxb8

 

 

 

 

4
5
6 Neural Postmitotic motor neuron speciﬁcation
7
8

Neural Postmitotic motor neuron speciﬁcation

 

None below 0.05 (+)My11 ( +)Is11

Fig. 8. (Top) Estimated lineage tree for motor neuron and pancreatic progenitor; nodes are grouped by cell type and edges are labeled by program involved; color
indicates program type (see table below) (Bottom) Major GO terms and Genes corresponding to developmental programs stars denote Benjamini—Hochberg

corrected P—value of enrichment t—test; *P > 0.05; ** P > 0.01

not positively identiﬁed this state as a biologically signiﬁcant cell
state.

Identiﬁcation of enriched GO terms and genes also show close
correspondence with known biological facts. We correctly identify
all major stages of the motor neuron differentiation process, from
loss of pluripotency and development of positional identity to spinal
motor neuron speciﬁcation. On the pancreatic branch, we identify
loss of pluripotency, suppression of mesodermal identity and ﬁnally
speciﬁcation of endodermal and pancreatic identity. We include full
summary output of the enriched genes and GOs for each program
in the appendix.

CONCLUSION

We have presented LineageProgram, a log—linear sparse regularized
model and inference algorithm for lineage—associated expression
data that provide strong interpretability with no loss in predictive
power. Existing ﬂat modeling methods were unable to cope with the
large number of leaves that occur at the ends of the differentiation
experiment, and signiﬁcant statistical power is lost re—estimating the
background cellular expression levels. Our biological metrics also
suggest that ﬂat factorization methods do not extract biologically
meaningful expression programs, and modeling the differences

between each differentiation state is necessary to estimating ﬁne
behaviors occuring during differentiation.

Our analysis of the combined pancreas and motor neuron data
provides a computational analysis of the pancreatic and motor
neuron differentiation pathways that recapitulate known biological
markers and states. The ability of the sparse regularization to
remove spurious branches in the lineage tree suggests the use of this
model to estimate novel cellular states in later—stage differentiation
experiments, where explicit cellular states are less characterized.

Although we have restricted our model to expression—based data,
we hope to extend this framework of Ll—regularized branching
models to sequencing data with similar structure. The penalty model
presented here would allow a wide variety of log—convex generative
models to incorporate lineage—associated structure.

Funding: National Institutes of Health [P01—NS055923 and 1—UL1—
RR024920 to D.K.G].

Conﬂict of Interest: none declared.

REFERENCES

Akashi,K. er al. (2003) Transcriptional accessibility for genes of multiple tissues and
hematopoietic lineages is hierarchically controlled during early hematopoiesis.
Blood, 101, 383—389.

 

i256

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 1110131 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Cellular states and expression programs

 

Alter,O. et al. (2000) Singular value decomposition for genome-wide expression data
processing and modeling. Proc. Na. Acad. Sci. USA, 97, 10101—10106.

Bar-Joseph,Z. et al. (2004) Analyzing time series gene expression data. Bioinformatics,
20, 2493—2503.

Carmona—Saez,P. et al. (2000) Biclustering of gene expression data by non-smooth
non-negative matrix factorization. BMC Bioinformatics, 7, 78.

Cheng,Y and Church,G.M. (2000) Biclustering of expression data. In Proceedings
/  International Conference on Intelligent Systems for Molecular Biology;
ISMB. International Conference on Intelligent Systems for Molecular Biology, 8:
pp. 93—103.

Costa,1. et al. (2007) Gene expression trees in lymphoid development. BMC
Immunology, 8, 25.

Ivan Costa,G. et al. (2008) Inferring differentiation pathways from gene expression.
Bioinformatics, 24, i156—i164.

Eisenstein,J. et al. (2011) Sparse additive generative models of text.

Ferrari,F. et al. (2007) Genomic expression during human myelopoiesis. BMC
Genomics, 8, 264—264.

Georg,K. et al. (2007) Automated discovery of functional generality of human gene
expression programs. PLoS Computational Biology, 3, e148.

Hoyer,P.O. (2004) Non-negative matrix factorization with sparseness constraints. J.
Machine Learn. Res, 5, 1457—1469.

Irizarry,R.A. et al. (2003) Exploration, normalization, and summaries of high density
oligonucleotide array probe level data. Biostatistics, 4, 249.

Jessell,T.M. (2000) Neuronal speciﬁcation in the spinal cord: inductive signals and
transcriptional codes. Nature Reviews Genetics, 1, 20—29.

Ji,S. and Ye,J. (2009) An accelerated gradient method for trace norm minimization. In
Proceedings of the 26th Annual International Conference on Machine Learning,
ACM, pp. 457—464.

J0rgensen,M.C. et al. (2007) An illustrated review of early pancreas development in
the mouse. Endocrine reviews, 28, 685.

Joung,J.-G. et al. (2006) Identiﬁcation of regulatory modules by co-clustering latent
variable models: stem cell differentiation. Bioinformatics, 22, 2005—2011.

Lee,D.D. and Seung,H.S. (2001) Algorithms for non-negative matrix factorization.
Advances in neural information processing systems, 13, 788—791.

Martins,A.F.T. et al. (2011) Online learning of structured predictors with multiple
kernels. In Proceedings of the I 4th International Conference on Artiﬁcial
Intelligence and Statistics.

Mazzoni,E.O. et al. (2011) Embryonic stem cell-based mapping of developmental
transcriptional programs. Nature methods.

Medvedovic,M. et al. (2004) Bayesian mixture model based clustering of replicated
microarray data. Bioinformatics (Oxford, England ), 20, 1222—1232.

N esterov,Y. and N esterov,I.U.E. (2004) Introductory Lectures on Convex Optimization:
A Basic Course. Applied optimization. Kluwer Academic Publishers, MA, USA.

N iakan,K.K. et al. (2010) Sox17 promotes differentiation in mouse embryonic stem cells
by directly regulating extraembryonic gene expression and indirectly antagonizing
self-renewal. Genes & Development, 24, 312.

Subramanian,A. et al. (2005) Gene set enrichment analysis: a knowledge-based
approach for interpreting genome-wide expression proﬁles. Proc. Nat. Acad. Sci.
USA, 102, 15545.

Wirth,H. et al. (2011) Expression cartography of human tissues using self organizing
maps. BMC Bioinformatics, 12, 306.

Zagar,L. et al. (2011) Stage prediction of embryonic stem cell differentiation from
genome-wide expression data. Bioinformatics, 27, 2546 —2553.

Zhang,B. et al. (2011) Estimating developmental states of tumors and normal tissues
using a linear time-ordered model. BMC Bioinformatics, 12, 53.

 

i257

112 /810's12urnofp101x0'soi1au1101uioiq//zd11q 1110131 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

