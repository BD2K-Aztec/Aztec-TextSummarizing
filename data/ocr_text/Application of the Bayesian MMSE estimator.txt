ORIGINAL PAPER

Vol. 27 no. 13 2011, pages 1822-1831
doi: 10. 1093/bioinformatics/btr2 72

 

Gene expression

Advance Access publication May 5, 2011

Application of the Bayesian MMSE estimator for classification
error to gene expression microarray data

Lori A. Dalton” and Edward R. Doughertym’3

1Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX 77843,
2Computational Biology Division,Trans|ationa| Genomics Research Institute, Phoenix, AZ 85004 and 8Department of
Bioinformatics and Computational Biology, University of Texas M. D. Anderson Cancer Center, Houston, TX 77030

USA

Associate Editor: Joaquin Dopazo

 

ABSTRACT

Motivation: With the development of high-throughput genomic and
proteomic technologies, coupled with the inherent difficulties in
obtaining large samples, biomedicine faces difficult small-sample
classification issues, in particular, error estimation. Most popular
error estimation methods are motivated by intuition rather than
mathematical inference. A recently proposed error estimator based
on Bayesian minimum mean square error estimation places error
estimation in an optimal filtering framework. In this work, we examine
the application of this error estimator to gene expression microarray
data, including the suitability of the Gaussian model with normal—
inverse-Wishart priors and how to find prior probabilities.

Results: We provide an implementation for non-linear classification,
where closed form solutions are not available. We propose a
methodology for calibrating normal-inverse-Wishart priors based
on discarded microarray data and examine the performance on
synthetic high-dimensional data and a real dataset from a breast
cancer study. The calibrated Bayesian error estimator has superior
root mean square performance, especially with moderate to high
expected true errors and small feature sizes.

Availability: We have implemented in C code the Bayesian
error estimator for Gaussian distributions and normal—inverse-
Wishart priors for both linear classifiers, with exact closed-
form representations, and arbitrary classifiers, where we use a
Monte Carlo approximation. Our code for the Bayesian error
estimator and a toolbox of related utilities are available at
http://gsp.tamu.edu/Publications/supplementary/dalton1 1a. Several
supporting simulations are also included.

Contact: ldalton@tamu.edu

Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on January 25, 2011; revised on April 8, 2011; accepted
on April 24, 2011

1 INTRODUCTION

Classiﬁcation is a major constituent of bioinformatics, in particular,
phenotypic discrimination, which can be accomplished via many
different data types, such as gene expression, protein expression or
sequence data. The misclassiﬁcation error of a classiﬁer quantiﬁes
its predictive capacity, the key aspect of any scientiﬁc model.

 

*To whom correspondence should be addressed.

Thus, accuracy of the error estimation represents the salient
epistemological issue in classiﬁcation, model validity (Dougherty
and Braga—Neto, 2006). The main measure of error estimation
accuracy is the root mean square (RMS) error of the estimator,

RMS = E [(Stru — Sest)2] ,

where am and eest are the true and estimated errors of the classiﬁer
and E is expectation with respect to the random sampling procedure.
Given a large data sample, the data can be split between training and
test data, the classiﬁer designed on the training data and classiﬁer
error estimated on the test data. In this scenario, there is a satisfactory
distribution—free bound, RMS 5 1/2ﬂ, where m is the size of the
test sample (Devroye et al., 1996). However, when the sample is
small, splitting the data is unacceptable because the classiﬁer will
be trained on too small a set, thereby resulting in poor classiﬁer
design. Thus, in small sample settings (the concern of this article),
a classiﬁer is trained and its error estimated on the same data.

A number of training data—based error estimators have been
proposed in the past and we will consider several in this article.
Perhaps the one most commonly employed in bioinformatics is
cross—validation. In this method, the data are partitioned into k
folds (subsets); at each state of the procedure, one fold is held
out, a surrogate classiﬁer trained on the remaining folds and its
error estimated on the held—out fold. The error of the classiﬁer
(originally trained on the full sample) is estimated by the average
surrogate errors on the left—out folds. In the special case k=n,
the sample size, each held—out fold consists of one point and the
error method is termed ‘leave—one—out’. For leave—one—out, there
is only one partition of folds; however, when k <n evaluating all
combinations of partitions is computationally prohibitive. Hence,
in this case partitions are randomly chosen to make the estimation.
Although cross—validation is close to being unbiased if k is not too
small, it tends to have a large variance for small samples (Braga—
Neto and Dougherty, 2004b; Devroye et al., 1996) and also to be
poorly correlated with the true error (Hanczar et al., 2007), the two
combining to create a large RMS for small samples [for a review of
error estimation performance, see Dougherty et al. (2010)].

A natural question arises: can cross—validation be used for small
samples or, equivalently, are there small—sample cases in which
the RMS of cross—validation is sufﬁciently small so that it can be
considered a valid error estimator? To answer this question, one
might ﬁrst ask if it is possible to use distribution—free bounds. Not

 

1822 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 1110.1} pepeommoq

9IOZ ‘091sn8nv uo ::

Application of the Bayesian MMSE error estimator to microarray data

 

:- ||Z|

01D
.._|__.—_I_

0 1n

{I U!
\3 -_
.I 5 I-
I1 3- I-
r .. ..
II It
{I I35
\ -,_I_
‘6 .III III
II! ID- -
I- I-
I- r-
:I III:

[IUD

um:
I__

am:

 

 

Fig. 1. Leave—one—out RMS versus Bayes error for LDA. (plus) is 20
samples, (triangle) 40 samples and (circle) 60 samples.

only are there very few cases in which such bounds are known,
but also when they are known they are so loose as to be useless in
practice. For instance, consider the following leave—one—out RMS
bound for the k—nearest neighbor classiﬁcation rule with random tie
breaking (Devroye and Wagner, 1979):

 

If k=3 and the sample size is n: 100, then the bound is
approximately 0.353, which is useless.

Let us now consider bounds when there are distributional
assumptions. We consider a feature—label distribution having two
equally probable Gaussian class—conditional densities sharing a
known covariance matrix and the linear discriminant analysis (LDA)
classiﬁcation rule. For this model, we possess analytic representation
of the joint distribution of the true error with the leave—one—out
estimator (Zollanvari et al., 2010). Figure 1 shows the RMS to be
a one—to—one increasing function of the Bayes error for dimensions
p=5,10,25, and sample sizes n=20,40,60, the RMS and Bayes
errors being on the y and x axes, respectively. In this model, where
the Bayes error is a function of the distance between the class—
conditional means, the maximum RMS is bounded and does not
exceed 0.15, even with only 20 sample points. Moreover, if one
wishes to bound the RMS below some tolerance, ‘13, one need to
only make an assumption on the minimum distance between the
means, which corresponds to a maximum Bayes error. This kind
of behavior, where the RMS of leave—one—out is tolerable when
the Bayes error is small, is often observed—indeed, we see this
in Figure 5 of this article—but it has only been quantiﬁed in a small
number of cases (Braga—Neto and Dougherty, 2010; Zollanvari et al.,
2010).

The upshot of these considerations is that if cross—validation
is going to be used when the sample size is small, there must
be modeling assumptions to make the RMS acceptable. Hence,
why not take a Bayesian minimum mean square error (MMSE)
approach and thereby guarantee that the average RMS across the
model family for the resulting error estimator is minimal among
all possible error estimators? That is what is done in Dalton and
Dougherty (2011a, b), where a parameterized family of class—
conditional feature distributions is assumed, a prior distribution is
applied to the parameters of the model, and this prior along with
observed data are used to compute an unbiased, MMSE estimate of
classiﬁcation error. An advantage of this approach, besides achieving
average minimum RMS across the model family, is that it depends
only on the form of the designed classiﬁer, not the classiﬁcation rule

used to design the classiﬁer. In particular, it is independent of the
feature selection method, which is part of the classiﬁcation rule.

Two problems naturally arise. First, how does one arrive at a
prior distribution governing the model? This issue arises in any
Bayesian method and, as previously explained, would arise in
the context of small—sample error estimation even if one were
to use a classical error estimator. The current paper proposes a
method to determine a prior distribution when using microarray
data. The second issue is the difﬁculty of deriving an analytic
expression for the Bayesian MMSE estimator. This is done for
discrete classiﬁcation under a family of generalized beta prior
distributions in Dalton and Dougherty (2011a) and for linear
classiﬁers applied to Gaussian distributions under normal—inverse—
Wishart prior distributions in Dalton and Dougherty (2011b). While
we are not advocating the abandonment of analytic methods,
it is practically useful to have software that can arrive at the
Bayesian MMSE estimator via Monte Carlo methods. Currently,
approximation is necessary when using a non—linear classiﬁer, where
a closed form solution for the model is not known. This article
develops and provides publicly available software.

2 SYSTEMS AND METHODS

2.1 Modeling microarray data

We assume two classes and require the training sample to consist of
normalized log ratios. Thus, use of normalization schemes such as
total intensity normalization or the LOESS method, which are popular
transformations before high—level analysis is applied, are required. Log—
transformed gene expression values have nearly Gaussian class—conditional
distributions (with unknown parameters) (Autio et al., 2009; Hoyle et al.,
2002). To further validate a Gaussian modeling assumption, during feature
selection we will permit only features that pass a Shapiro—Wilk Gaussianity
test. Note that Bayesian error estimators designed under the Gaussian
model are robust in the sense that performance is still good when the true
distributions are Johnson distributions (Dalton and Dougherty, 2011b), which
are a class of non—Gaussian distributions with four free parameters to control
mean, variance, skewness and kurtosis.

Normal—inverse—Wishart priors compose a ﬂexible class of distributions
with many degrees of freedom to facilitate calibration of the priors to gene
expression microarrays. Further, this family of priors possesses a fast closed—
form solution when used with linear classiﬁcation. In problems where the
Gaussian model applies and one wishes to use a linear classiﬁer, the beneﬁt
one might gain by having more control over the prior is not worth the much
greater amount of time required to run an integral approximation code and
the effort of designing a specialized model, especially for small samples
where one cannot afford a very complex model anyway. Hence, we focus on
calibrating normal—inverse—Wishart priors.

Assuming the parameters between classes are fairly independent, we
have justiﬁed the assumptions posed by Dalton and Dougherty (2011a), the
others being that the class—conditional distributions are relatively Gaussian
and that normal—inverse—Wishart priors are adequate for representing prior
knowledge. We are left to devise a method of generating priors for the mean
and covariance of each class.

2.2 The Bayesian error estimator for Gaussian
distributions with normal—inverse-Wishart priors

This section summarizes essentials from Dalton and Dougherty (2011b),
using similar notation. Consider the class y e {0, 1} in a binary classiﬁcation
problem. D multivariate Gaussian features are used for classiﬁcation and we
deﬁne the distribution parameters for these D features to be the mean and
covariance, 6 = m, 2}. We assume 2 is invertible with probability 1, and for

 

1 823

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdllq 11101; pepraommoq

9IOZ ‘09 lsnﬁnv uo ::

L.A.Dalton and E.R.Dougherty

 

invertible 2 our priors are of the form:
71(9)=JT(M|E)JT(E),
where
ﬂ(u|2)~N(m, 23/12),

I
71(2)OCIZ|_(K+D+1)/26Xp (— Etrace (SE—1)).

That is, the mean conditioned on the covariance is Gaussian with mean m
and covariance 23/1), and the marginal distribution of the covariance is an
inverse—Wishart distribution. The hyperparameters of 71(6) are a real number
1) Z 0, a length D real vector m, a real number K and a non—negative deﬁnite
D ><D matrix S. For linear classiﬁcation, we also restrict K to be an integer
to guarantee a closed form solution. The hyperparameters m and S can be
viewed as targets for the mean and the shape of the covariance, respectively.
The larger 1) is the more localized the prior is about m, and the larger K is
the less the shape of E is allowed to wiggle.

Given my observed sample points, we update the prior for class y to a
posterior, 31*. This posterior has the same form as the prior, with updated
hyperparameters given by

K* = IC-I-I’ly,
nyv
ny + v

s*=(ny—1)E+S+ (ﬁ—m)(ﬁ—m)T,

 

v*=v+ny,
n ﬁ+vm
111*: y
ny+v

where II and E are the sample mean and sample covariance of points
from class y, respectively. To ensure a proper prior, we require K>D— 1,
S positive deﬁnite, and v> 0. These restrictions are not mandatory as long
as the posterior is proper with K* >D— 1, S* positive deﬁnite and v* > 0.

Assuming the a priori probability of class 0 is uniform between 0 and
1 and assuming prior (and posterior) independence between this and the
distribution parameters in each class, the Bayesian MMSE error estimator
can be expressed as

n0+1
n+2

n1+1
n+2

’5:

 

 

Ea [821+ Ea [8.1.],

where n =n0 +n1 is the total number of sample points. E7148“ may be
viewed as the posterior expectation of the error contributed by class y. With
a ﬁxed classiﬁer and given 6, the true error, 8% (6), is deterministic and

Ea [821:] smearxeme, (1)

y

where ®y is the parameter space of class y. For non—linear classiﬁers, this
integral must be approximated with Monte Carlo methods.
For a linear classiﬁer, i.e. a classiﬁer of the form

_ Oifg(x)§0
w"(x)_{1ifg(x)>0’

where g(x) =aTx+b with some constant vector 3 and constant scalar b,

E [y] 1 1+ (AH A2 1 IC*—D+I (2)
* 8 =— S n ;_7 1
” n 2 g A2+aTS*a 2 2

v*
A=(—1)y8(m*)‘i v*—+1,

and I (x; or, ) is the regularized incomplete beta function. For positive integer

 

where

13
N, I (x; %,  has a closed form solution, in particular, I (1; %,  =1 and for

0§x< 1,
( 1 N)
I x; _7 _
2 2
(2/71) sin—1  ifN=1
(2/71)sin_1 
(N_1)/2 _ H
= 4.? Z %(1—x)k_% ifN>lis Odd (3)
k=1 ”
(N—2)/2 _ H
ﬂ 2 %(1—x)k ifN>1 is even,

k=0

where I! is the double factorial.

2.3 Implementation of exact and approximate Bayesian
error estimators

Assuming a Gaussian model with normal—inverse—Wishart priors for the
Gaussian distribution parameters, with ﬁxed hyperparameters for the priors
of each class, we use the observed sample to update the hyperparameters of
the posteriors. We also check that these posteriors are valid density functions,
and if they are not, by default the code reports the error contributed by that
class to be 0.5. Note that the Bayesian error estimator is most useful in
a small sample setting, but the sample size must not be so small that the
posterior is not a valid density function. This may happen, for instance,
if we use a ﬂat prior with K+D+2=0 and the sample size for class y is
my §2D+1, so that K* =K+ny SD—l. In such cases, the Bayesian error
estimator is meaningless because the available information is not sufﬁcient
for estimation, but generally there are also too few sample points for any
error estimator to provide meaningful results.

Given valid normal—inverse—Wishart posteriors, the closed form Bayesian
error estimator in Equation (2) for linear classiﬁcation is easily evaluated.
For arbitrary classiﬁers, we approximate the Bayesian error estimator in
Equation (1) with a Monte Carlo approach. For each class, we generate
a random mean and covariance pair according to the speciﬁed posterior
normal—inverse—Wishart distribution. Several algorithms for generating
normal—inverse—Wishart distributed multivariate sample points are available,
see Johnson (1987). For each mean and covariance pair, the true error
contributed by the class for the designed classiﬁer is approximated by
generating 10000 sample points from the Gaussian distribution having the
speciﬁed mean and covariance, and ﬁnding the error of these sample points
on the classiﬁer. The Bayesian error estimator is computed by averaging
these true errors over 2500 random sets of mean and covariance pairs.

A toolbox of C code for Bayesian error estimation is publicly available.
This includes the exact Bayesian error estimator for linear classiﬁers,
the approximation code described above for arbitrary classiﬁers, a three—
stage feature selection algorithm discussed in Section 2.4, as well as code
implementing the method of generating priors described in Section 2.5.
Simulations demonstrating the accuracy of this approximation with synthetic
data and LDA classiﬁcation are available in the Supplementary Material.

2.4 Feature selection

We use a three—stage feature selection method based on the t—test and a
Gaussianity test to reduce the original feature set to D features. Since this
article is not focused on optimizing a classiﬁcation scheme, but rather on
investigating the performance of error estimators, this feature selection
scheme is intended to be a simple possible scheme to produce highly
differentially expressed Gaussian features.

In the ﬁrst stage, only highly differentially expressed features or features
with a high likelihood of biological signiﬁcance are selected. These may be
selected by a t—test or based on biological knowledge. This stage reduces the
number of features from tens of thousands to a few hundred. The second
stage applies a Shapiro—Wilk hypothesis test (Shapiro and Wilk, 1965) on
each feature of each class. Only features passing the Shapiro—Wilk test with

 

1 824

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 1110131 pepraommoq

9IOZ ‘09 lsnﬁnv uo ::

Application of the Bayesian MMSE error estimator to microarray data

 

95% conﬁdence in both classes are used, unless there are not enough features
passing the test, in which case we select a ﬁxed number of features with the
highest sum of the Shapiro—Wilk test statistics in each class. In the ﬁnal
stage of feature selection, we reduce the feature set to D features. This is
done either by applying a t—test if it has not already been applied in the ﬁrst
stage or by using the same t—test statistics from the ﬁrst stage to pick the D
most differentially expressed Gaussian features.

This implementation employs classiﬁer—independent feature selection
schemes, such as the t—test and Shapiro—Wilk test. However, even for
classiﬁer—dependent schemes, once the feature selection and classiﬁcation
schemes have been implemented, the Bayesian error estimator (BEE) may
be calculated as a deterministic function of the ﬁxed classiﬁer. This is in
contrast to cross—validation, which uses surrogate classiﬁers to estimate the
error of the designed classiﬁer.

2.5 Estimating prior hyperparameters

When calibrating priors for microarrays, what data should be used and how?
With the explosion of microarray experimentation over the last decade, the
genomics community has amassed an enormous database of gene expression
data, and trends in the entire history of microarray experimentation could
be used to ﬁnd a prior, perhaps conditioned on a particular organism, tissue,
gene and/or type of abnormality, depending on the nature of the experiment at
hand. However, different microarray experiments are currently very difﬁcult
to compare, although there have been some recent efforts to normalize and
integrate different datasets (Autio et al., 2009).

The method employed here uses discarded gene expression data,
consisting of a subset of the features from the microarray data that are not
used for classiﬁcation, to calibrate the priors of the Bayesian error estimator.
Though these features are not used in the actual classiﬁer, they may implicitly
contain useful calibration information such as the varying concentrations of
DNA material used in each microarray, background intensities and other
characteristics of the digitized images of a microarray slide. And although
calibration requires a large amount of data and in microarray gene expression
analysis we typically expect a very small sample setting, the huge number
of discarded features ensures that there is enough data for a successful
calibration of the hyperparameters.

It is possible to deﬁne a prior on the entire feature set and to compute the
Bayesian error estimator over the reduced feature set based on the marginal
distribution of this prior on only the selected features. However, the following
approach directly deﬁnes a prior on only the selected features.

We essentially use a method of moments approach to calibrate the
hyperparameters; however, estimating a vector m and matrix S may be
problematic for a small number of sample points, so to simplify the analysis
we assume the following structure on these hyperparameters:

m=m[1,1,...,1]T,

5:02  . ,

where m is a real number, 02 Z 0, and —1 5 p 5 1. This structure is justiﬁed
because prior to observing the data, there is no reason to think that any feature,
or pair of features, should have unique properties. With this simpliﬁcation,
our problem is now reduced to estimating ﬁve scalers for each class: 1), m,
K, 02 and p.

In the ﬁrst stage of a method of moments approach, we ﬁnd the theoretical
ﬁrst and second moments of the random variables y. and 2 (random because
of the prior distribution applied to them) in terms of the hyperparameters
we wish to estimate. Throughout the remainder of this section, a subscript i
represents the i—th element of a vector, and a subscript jk represents the j—th
row, k—th column element of a matrix.

First consider the parameter E, with a marginal prior having an inverse—
Wishart distribution with hyperparameters K and S. The mean of this

distribution is well known (Rowe, 2003),

 

S
E 2 = —,
[ ] K—D—1
and given the previously deﬁned structure on S, we obtain

02=(K—D—1)E[2111 (4)

E1212]
p: . (5)

E1211]

Due to our imposed structure, only E[211] and E[212] are needed.
The variance of the j—th diagonal element in inverse—Wishart distributed
2 may be expressed as
_ 261-192 _ 2(E12111)2
(IC—D—I)2(IC—D—3) K—D—3 ’
where we have applied Equation (4) in the second equality. Solving for K,
2(E[211])2
K = —
Var (211)

 

Var (Zr)

+D+3. (6)

We next consider the mean, it, which is parameterized by the
hyperparameters v and m. The marginal distribution of the mean is a
multivariate Student’s t—distribution given by Rowe (2003):

PCT“) VD |S|—1

7T(M)=_— — K .
Nié“) 77” (1+v(rL—m)TS‘1(rL—m)) +1

 

 

The mean and covariance of this distribution are well known:
EUL] = m,

_ S _E[2]
_ (K—D—1)v _ 1)

With the assumed structure on m, we obtain

 

VaFUL)

m=E[rL1], (7)
E1211]
2 —. 8

Finally, our objective is to approximate the expectations in Equations (4)
through (8) using calibration features left out of the classiﬁcation scheme.
Suppose the calibration data for the current class consists of n sample points
with E >>D features. Let TIE be the sample mean and EB be the sample
covariance matrix of the complete set of E features in the calibration data.
From these we wish to ﬁnd several sample moments of p. and E in our original
D feature problem, that is, to ﬁnd E[p.1], V5011), E[211], E[212] and
@(211), where the hats indicate the sample moment of the corresponding
quantity. All these are scaler quantities.

To compress the set of E features in the calibration data to solve an
estimation problem on just D features, and ultimately to ﬁnd these scaler
sample moments in a balanced way, we emulate the feature selection process
by assuming that the selected features are drawn uniformly. Since any of the
E features is equally likely to be selected as the i—th feature, the sample
mean of the mean of the i—th feature, Emi], is computed as the average of
the sample means of all E features in the calibration data. This result is the
same for all i, and we use E[y.1] to represent all features. In particular,

E
Emu: E21)... (9)
1:

Thanks to uniform feature selection, all other moments are balanced over
all features or any pair of distinct features. The remaining sample moments
are obtained in a similar manner:

E

A 1 A
Var(m)= HEW—E11111)? (10)
i=1
A 1 E A
E1211]: E2257, (M)
i=1

 

1 825

112 /§JO'S[BUJHOIpJO}XO'SOIlBIIIJOJUIOIQ/ﬁdllq 1110131 pepraommoq

9IOZ ‘09 lsnﬁnv uo ::

L.A.Dalton and E.R.Dougherty

 

E i—l

A 2 A
EMM=EKBZXE5 am
i=2j=1
A 1 E A A 2
Var(211)=ﬁZ(ZE—E[le]) . (13)

i=1

Here/Yr; (11.1) represents the variance of each feature in the mean. We also
have E[211] and E[212] representing the sample mean of diagonal elements
and off—diagonal elements in 2, respectively. Finally, V2; (211) is the sample
variance of the diagonal elements in E.

Plugging our sample moments into Equations (4) through (8), we obtain

 

A (32mm
222E 2 A— 1 14
0 [ 111<Var(211) + )7 ( )
Elzrz]
=1 , 15
p HEM] ()
A 2
ZEEEEB_+D+a co
Var(211)
m=Emu, an
E1211]
v Vmwn ()

Note Equation (6) for K was plugged into Equation (4) to obtain the ﬁnal 02.

In sum, calibration for the prior hyperparameters is deﬁned by
Equations (14) through (18), the sample moments being given in
Equations (9) through (13). The estimates of K and 1) can be unstable, since
they rely on second moments, @(211) and V5011), in a denominator.
These parameters can be made more stable by discarding outliers when
computing the sample moments. Herein, we discard the 10% of the II? with
largest magnitude and the 10% of the  with largest value.

This method is one of many possible approaches; for simplicity and
to avoid an over—deﬁned system of equations, we do not incorporate the
covariance between distinct features in it [that is, Cov (1012], the variance
of off—diagonal elements in E [that is, Var (212)], or the correlation between
distinct elements in 2, though it may be possible to use these to improve
the estimates of the hyperparameters. It may also be feasible to use other
estimation methods, such as maximum likelihood. Furthermore, the method
proposed here to calibrate the priors is a purely data—driven technique for
easy and general application to microarray experiments. Ideally, the best way
to calibrate priors would be to incorporate data and biological knowledge
speciﬁc to the particular features selected for classiﬁcation.

3 RESULTS AND DISCUSSION

We present two sets of results demonstrating good performance of
Bayesian error estimators, one on synthetic high—dimensional data
with three—stage feature selection and a second based on breast
cancer data with two stages of feature selection.

3.1 High-dimensional synthetic data

In this section, we apply our Bayesian prior estimation method
to synthetic high—dimensional microarray data. We use the same
synthetic data model provided in Hua et al. (2009), which
models many observations made in microarray expression—based
studies, including blocked covariance matrices to model groups of
interacting variables with negligible interactions between groups.
Our model emulates a full feature—label distribution with 20 000
total features. Features are categorized as either ‘markers’ or
‘non—markers’. Markers represent features that have different class—
conditional distributions in the two classes and are further divided

...>

Class 1
...><...........

Subclass O Subclass 1

x . - . . ' .

<————SAMPLES ————>
ClassO
<

' - ’ a . . - x - ' . x u u ' .'

Global Heterogeneous High-variance LOW-variance
markers markers non-markers non-markers
4 _ _ _ _ _ _ _ _ _ _ FEATURES _ _ _ _ _ _ _ _ _ _ _ ,

Fig. 2. Different feature types in constructing the hi gh—dimensional synthetic
data model (Hua et 01., 2009).

into two subtypes: global markers and heterogeneous markers. Non—
markers have the same distributions for both classes and thus have no
discriminatory power, and are also divided into two subtypes: high—
variance non—markers and low—variance non—markers. A summary
of the feature types is shown in Figure 2.

Twenty features are global markers, which are homogeneous in
each class. In particular, the set of all global markers in class i has a
Gaussian distribution with mean aim and covariance matrix Efm.

Within class 1, we assume each sample point belongs to one of
two equally likely subclasses named 0 and 1, representing different
stages or subtypes of cancer. Each subclass is associated with
50 heterogeneous markers, which are jointly Gaussian with mean
[trim and covariance 211m“. Sample points associated with the other
subclass have the same distribution as class 0, which is Gaussian
with mean Mgm and covariance 28”“. Each heterogeneous marker
may only be associated with one subclass, thus there are 100 total
heterogeneous markers in the model.

We simplify the model by assuming that aim and 11.?“ have the

form m,- X (1,1,...,1) for ﬁxed scalers m. We assume 2ng and

2%“ have the form oizE, where a? are constants and 2, not to be

confused with the deﬁnition in Section 2.2, has a block covariance
structure, i.e.

2p  0
E:    ,
0  23/)

with 2,) being a 5 X 5 matrix with 1 on the diagonal and ,0:0.8 off
the diagonal. That is, we group markers into blocks of ﬁve features,
where the blocks are independent from each other, and the markers
within each block are correlated with a relatively high correlation
coefﬁcient to emulate a pathway.

We generate 2000 hi gh—variance non—marker features, which have
independent mixed Gaussian distributions given by pN(m0,o(2))—l—

(1 —p)N(m1,012), where m,- and at? are the same scalers deﬁned for
markers. The random variable 19 is selected independently for each
feature with a uniform distribution over [0, 1] and is applied to all
sample points of both classes. These features can be viewed as genes
regulated by mechanisms unrelated to those that regulate the class
0 and class 1 phenotypes. The remaining features are low—variance
non—marker features, each having independent univariate Gaussian
distributions with mean m0 and variance 0%.

 

1 826

112 /§.IO'SIBUJHOIpJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Application of the Bayesian MMSE error estimator to microarray data

 

Table 1. Synthetic high—dimensional data model parameters

 

 

Parameters Values/description

Total features 20 000

Global markers 20

Subclasses in class 1 2

Heterogeneous markers 50 per subclass (100 total)
High—variance features 2000

Low—variance features 17 880

Mean m0 =0, m1 =1

Variances 02 = 3:012 (controls Bayes error)
Block size 5

Block correlation 0.8

a priori probability of class 0 0.5

 

In this model, heterogeneous markers are Gaussian within each
subclass, but the class—conditional distribution for class 1 is a mixed
Gaussian distribution (mixing the distributions of the subclasses),
and is thus not Gaussian. Further, the high—variance features are
also mixed Gaussian distributions, so this model incorporates both
Gaussian and non—Gaussian features to challenge the Shapiro—Wilk
Gaussianity test in the feature selection scheme.

To simplify our simulations, we set the a priori probability of
both classes to 0.5 and ﬁx the parameters m0 :0 and m1 = 1. We
also deﬁne a single parameter 0220320? which speciﬁes the
difﬁculty of the classiﬁcation problem. A summary of our synthetic
high—dimensional data model parameters is given in Table 1. In all
simulations, the values for 02 are chosen so that a single global
feature (note that all global features are identical) has a speciﬁc
Bayes error. We call this the ‘Bayes error’ in the remainder of this
section, and it is given by 8* = <1>(—1/(2o)), where CD is the unit
normal Gaussian cumulative distribution function, so for instance,
we use 0209537 for a Bayes error of 0.3.

Under this high—dimensional model, we run several Monte Carlo
simulations. In each experiment, we ﬁx the training sample size,
n, the number of selected features, D, and the difﬁculty of the
classiﬁcation problem via a. The synthetically generated samples
are non—stratiﬁed, meaning that in each iteration the sample size
of each class is not ﬁxed but determined by a binomial (0.5,n)
experiment, and the corresponding sample points are randomly
generated according to the distributions deﬁned for each class.

Once the sample has been generated, we apply the three—stage
feature selection scheme outlined in Section 2.4. In the ﬁrst stage, we
apply a t—test to obtain 1000 highly differentially expressed features
by removing most non—informative features. In the second stage, we
apply a Shapiro—Wilk Gaussianity test and eliminate features that
do not pass the test with 95% conﬁdence. The number of features
output in this stage is variable. If there are not at least 30 features
that pass the test, then we return the 30 features with the highest sum
of the Shapiro—Wilk test statistics for both classes. In the ﬁnal stage,
we use the same t—test values computed before to obtain the ﬁnal set
of D highly differentially expressed Gaussian features, which will
be used to design our classiﬁer. The 1000—D features that pass the
ﬁrst stage of feature selection but are not used for classiﬁcation are
saved as calibration data.

The feature selected training data are then used to train an
LDA classiﬁer. With the classiﬁer ﬁxed, 5000 testing points are

’5?

’6"
_. V
O

 

8
Q

0
Q

 

'9'n=60, D=7
101n=120, D=7

0 0.1 0.2 0.3 0.4 0.5
expected true error

 

 

 

 

 

-e-n=60
+n=120

2 4 6 8 10
Number of selected features (D)

 

 

Selected feat that are global (%)
i
D
n
O)
o -

_ - U -

n |
(D

Selected feat that are global (%)
(D
0

vs. expected true error vs. feature size, Bayes error = .3

Fig. 3. Percentage of three—stage selected features that are global features in
the synthetic high—dimensional data model. (3) versus expected true error;
(b) versus feature size, Bayes error = 0.3.

drawn from exactly the same distribution as the training data and
used expressly to approximate the true error. Subsequently, several
training data error estimators are computed, including leave—one—
out (100), 5—fold cross—validation (cv), 0.632 bootstrap (boot) and
bolstered resubstitution (bol) (see the Supplementary Material for
details). Two Bayesian error estimators are also applied, one with ﬂat
non—informative priors deﬁned by JT (6) = 1 (ﬂat BEE), and the other
with priors calibrated as described in Section 2.5 (calibrated BEE).
Since the classiﬁer is linear, these BEEs are computed exactly. This
entire process is repeated 120 000 times to approximate the RMS
deviations from the true error for each error estimator.

We ﬁrst analyze the quality of features selected by the three—
stage feature selection algorithm. Figure 3a shows the percentage
of selected features that are global features with respect to the
expected true error of the designed classiﬁer. We would like to
graph performance with respect to Bayes error, which is a more pure
measure of the difﬁculty of a classiﬁcation problem, but evaluating
Bayes error on our high—dimensional model is difﬁcult and it may
not be close to the true error of the designed classiﬁer. Hence, in our
graphs we focus on performance with respect to expected true error.
Similarly, Figure 3b graphs against feature size with a ﬁxed Bayes
error of 0.3. Recall that this model uses 20 000 features, of which
only 20 are global features that most effectively discriminate the
classes. As long as the feature size is reasonable given the difﬁculty
of the problem (expected true error and sample size), this percentage
is quite large. However, in Figure 3b for sample size 60 we see that a
feature size larger than 7 will result in <80% of the selected features
being global features. This illustrates the necessity of restricting
feature size in a small sample setting, and is consistent with earlier
studies showing the difﬁculty of ﬁnding good feature sets when the
number of features is large and the sample is small (Dougherty et al.,
2009; Sima and Dougherty, 2006).

The graphs in Figure 4 show the percentage of selected feature
sets that are not rejected by a multivariate Shapiro—Wilk test
on either class at a 95% signiﬁcance level. There are several
multivariate Gaussianity tests based on the Shapiro—Wilk statistic.
We used Villasenor Alvaa and Estradaa (2009), which generalizes
the classical univariate Shapiro—Wilk test to the multivariate case
by transforming the data into a set of approximately independent
standard normal random variables, and essentially summing up the
standard Shapiro—Wilk statistic on each dimension. The results show
that even though the three—stage feature selection algorithm only
uses a univariate Gaussianity test, and univariate normality does not

 

1 827

112 /§.IO'SIBUJHOIpJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

L.A.Dalton and E.R.Dougherty

 

imply multivariate normality, the resulting feature set still tends to
have a high probability of passing the multivariate Gaussianity test.

We next turn our attention to the RMS performance of error
estimators under our synthetic high—dimensional model, where a
summary of all simulation settings are available in Table 2. Our ﬁrst
battery of simulations in Figure 5 shows RMS deviation from true
error for all error estimators with respect to expected true error for
LDA classiﬁcation with 1, 3, 5 or 7 selected features and either 60
or 120 sample points. Given the sample sizes, it is prudent to keep
the number of selected features small to have satisfactory feature
selection (Sima and Dougherty, 2006) and to avoid the peaking
phenomena (Hua et al., 2005, 2009). Lines marked with ‘0’ represent
the Bayesian error estimator with ﬂat priors, and lines marked
with ‘x’ represent the Bayesian error estimator with the calibrated
priors. The key point in these graphs is that the calibrated BEE
has best performance in the mid and high range. For an expected
true error of about 0.25 and 11:60, the RMS for the calibrated
BEE outperforms 5—fold cross—validation for D: 1,3,5 and 7 by
0.0507, 0.0300, 0.0335 and 0.0379, respectively, representing 64,
32, 30 and 29% decrease in RMS, respectively. For 11: 120, the
decrease in RMS for D:1,3,5 and 7 is 0.0366, 0.0175, 0.0192
and 0.0198, respectively, for 67, 34, 35, and 33 percent decrease
in RMS, respectively. All other error estimators typically have best
performance for low expected true errors, with the ﬂat BEE having
even better performance than the classical error estimation schemes.
Indeed, all graphs except Figure 5g demonstrate that either the ﬂat

A
m

V

A

Pass multivariate Shapiro—Wilk (%) 0'

V

 

 

<0
01

 

<0
0

v

 

00
U1

'9-n=60, D=7
101n=120, D=7

0 0.1 0.2 0.3 0.4 0.5
expected true error

-e-n=60
+n=120

2 4 6 8 10
Number of selected features (D)

Pass multivariate Shapiro—Wilk (%)

 

 

 

 

 

 

 

(I
C'

Fig. 4. Percentage of three—stage selected features that are not rejected by
a multivariate Shapiro—Wilk test on either class at a 95% signiﬁcance level
with the synthetic high—dimensional data model. (a) versus expected true
error; (b) versus feature size, Bayes error = 0.3.

or calibrated Bayesian error estimator is the best scheme over the
whole range of expected true error.

Our next set of graphs in Figure 6 show simulation results with
respect to feature size. For reference, graphs of the expected true
error for these simulations are shown in Figure 7. Calibrated priors
provide the best performance, except when combining large feature
and small sample sizes, in which case a ﬂat prior performs best. In
fact, performance of the calibrated BEE in Figure 6 tends to be best
precisely in the rage of feature sizes with the highest percentage of
global features and the lowest true errors. For example, the calibrated
BEE in Figure 6a for sample of size 60 has the best performance
up to 7 features, where in Figure 3b the percentage of selected
features being global is greater than about 80% and in Figure 7
the true error has started to level off. Note, also, the consistently
superior performance of the calibrated BEE over the non—Bayesian
estimators for 11 : 60; indeed, throughout the range of feature sizes,
the calibrated BEE has an RMS of at least 0.0263 smaller than the
best performing non—Bayesian error estimator, which represents an
improvement of at least 14%.

Note the upward RMS trend in Figure 6a and the downward trend
in Figure 6b for the non—Bayesian error estimators. Although it can
be dangerous to generalize about the behavior of error estimators, let
us at least conjecture. We see in Figure 7 that the true error is large
for 11:60, with little improvement as we increase the number of
features and, in fact, increasing true error as the number of features
passes 7, which is a clear sign of the peaking phenomenon. Thus, for
11:60, adding features creates a more difﬁcult estimation problem
that is not offset by easing error estimation on account of small true
errors. On the other hand, in Figure 7 we see a fast reduction of
true error for 11: 120 as more features are added, thereby greatly
easing the error estimation problem and resulting in the declining
RMS trend in Figure 6b. While these comments apply directly to the
non—Bayesian error estimators, they apply to the Bayesian estimators
relative to their change of slope. The ﬂat BEE is relatively constant
in Figure 6a but falls along with the non—Bayesian error estimators in
Figure 6b, whereas the calibrated BEE consistently rises in Figure 6a
but remains relatively ﬂat in Figure 6b.

3.2 Empirical breast cancer data

We applied the Bayesian error estimator to normalized
gene expression measurements from a breast cancer study

Table 2. Data model and classiﬁcation settings for simulation with synthetic high—dimensional data

 

 

 

 

Data model Classiﬁer Sample size Feature selection BEE calibration Iteration
Bayes error Training Test Original 1st t—test Shapiro—Wilk test 2nd t—test

0.05—0.45 LDA n : 60 5000 20 000 1000 95% conﬁdence D : 1 1000 — D 120 000
005—045 LDA n : 60 5000 20 000 1000 95% conﬁdence D : 3 1000 — D 120 000
005—045 LDA n : 60 5000 20 000 1000 95% conﬁdence D : 5 1000 — D 120 000
005—045 LDA n : 60 5000 20 000 1000 95% conﬁdence D : 7 1000 — D 120 000
005—045 LDA n : 120 5000 20 000 1000 95% conﬁdence D : 1 1000 — D 120 000
005—045 LDA n : 120 5000 20 000 1000 95% conﬁdence D : 3 1000 — D 120 000
005—045 LDA n : 120 5000 20 000 1000 95% conﬁdence D : 5 1000 — D 120 000
005—045 LDA n : 120 5000 20 000 1000 95% conﬁdence D : 7 1000 — D 120 000
0.3 LDA n : 60 5000 20 000 1000 95% conﬁdence 1 to 10 1000 — D 120 000
0.3 LDA n : 120 5000 20 000 1000 95% conﬁdence 1 to 10 1000 — D 120 000

 

 

1 828

112 /§.IO'SIBUJHOIpJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Application of the Bayesian MMSE error estimator to microarray data

 

m
V

O'
V

0.1

0.1

 

FlMS deviation from true error

 

RMS deviation from true error

0.1 0.2 0.3 0.4 0.1 0.2 0.3 0.4
expected true error expected true error

’6
E
V

 

---boot
—bo|stering
015 -e-BEE, flat
+BEE, calib.

9

  
 

.0
—L

 

 

 

RMS devratlon from true error
. _o . I .
RMS devratlon from true error

 

 

0 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
expected true error expected true error

A
(D
V
A
-h
0V
in
cm

 

1-1-‘cv
---boot
—bo|stering
-e—BEE, flat
+BEE, calib.

O.

N

0.1

_O
.5
01

 

 

0.1

.0
o
01

 

RMS deviation from true error
RMS deviation from true error
0

 

 

 

.0
01
on

0.1 0.2 0.3 0.4 0.1 0.2 0.3 0.4
expected true error expected true error

to

V
3'

V

0.1

  
 

.0
FIMS deviation from true error

 

RMS deviation from true error

0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
expected true error expected true error

Fig. 5. RMS deviation from true error for the synthetic high—dimensional
data model with LDA classiﬁcation versus expected true error. (a) 11:60,
D:1; (b) 11:120, D:1; (c) 11:60, D:3; (d) 11:120, D: 1; (e) 11:60,
D:5; (f) n: 120, D:5; (g) 11:60, D:7; (h) n: 120, D:7.

 

 

 
 
  

 

 

 

 

 

  

(a) (b)

w-wloo 11111 “loo
g 0-2 “CV gooetg r r r r 1 , , - - r -1cv
a, 1 ___boot 1» < . [1‘  ---boot
3  8 —bo|stering g \‘s‘ \“x: """""""""" ,, —b°|5tering
E -9-BEE, flat 0.3;.» , E 0.05 ‘ ' """"""""""""""""" .. -9-BEE, flat
9 0'16 —x—BEE, calib.,w" ' 9 ~~~~~~ '~ +BEE’ calib'
C  2 ‘ ~ ~ ~ ~ ‘ ‘ ‘ - ‘ - ‘ - - -‘ 
2 014  ‘ .9
g >  0.04 _ _ - _ _ _ h
g 0.12 3 >
U) U)
E E 0.03
[I 0.1 a;

 

 

 

 

 

 

10 10

4 6 8 4 6 8
Number of selected features (D) Number of selected features (D)

Fig. 6. RMS deviation from true error for the synthetic high—dimensional
data model with LDA classiﬁcation versus feature size. See Figure 7 for
the expected true errors in these graphs. (a) 11:60, Bayes error = 0.3; (b)
n: 120, Bayes error = 0.3.

 

 

 

Lu 0.22 Tnzm
' +n=120

2 4 6 8
Number of selected features (D)

 

Fig. 7. Expected true error for the synthetic high—dimensional data model
with LDA classiﬁcation versus feature size, Bayes error = 0.3.

(van de Vijver et al., 2002). This study used 295 sample points,
with 180 assigned to class 0 (good prognosis) and 115 in class 1
(bad prognosis), and provides a 70 feature prognosis proﬁle. From
the original 295 points, we randomly draw a non—stratiﬁed training
sample of size 11. Since the number of features in the dataset is
relatively small, we apply only the last two stages of our feature
selection scheme in Section 2.4. The ﬁrst stage selects features
passing a Shapiro—Wilk Gaussianity test with 95% conﬁdence and
must report at least D features, while the second stage selects D
features with the highest t—test statistic. The 70—D features not
used for classiﬁcation are retained as calibration data for Bayesian
error estimation. After feature selection, we train an LDA, QDA or
3NN classiﬁer.

The remaining sample points are used as holdout data to
approximate the true error of the designed classiﬁer. The previously
considered error estimators are also evaluated from the training
samples [except in the case of 3NN where semibolstering is
used instead of bolstering owing to its superior performance
for 3NN (Braga—Neto and Dougherty, 2004a)], along with exact
Bayesian error estimators (for LDA) or approximate Bayesian error
estimators (for QDA and 3NN). Both ﬂat and calibrated priors are
applied. This process is repeated either 100 000 times (for LDA)
or 10000 times (for QDA and 3NN) to estimate the average RMS
deviation of each error estimator from the true error.

The Bayesian error estimator priors are calibrated as discussed in
Section 2.5. A typical prior with 2 features and 40 sample points
is 12:16.80, m:—0.004, K:12, 0'2/(K—D—1)=0.042 and ,0:
0.020 for class 0, and v:2.78, m:—0.068, K:10, 0'2/(K—D—
1):0.024 and ,0:0.073 for class 1. These indicate that the good
prognosis class (0) has a distribution with a more concentrated mean
(since 1) is much larger) and the mean is close to 0, which is expected
since the data have been normalized. On the other hand, K is fairly
large for both classes, suggesting that the variance of each feature in
either class is probably close to the prior expected variance, 02 / (K —
D— 1). Interestingly, the variance is a bit larger for class 0 and ,0 is
usually small but positive.

Figures 8, 9 and 10 provide simulation results for LDA, QDA
and 3NN, respectively. Each ﬁgure contains subplots representing
ﬁxed feature sizes between one and ﬁve, and one ﬁgure showing
the expected true error for all simulations with the corresponding
classiﬁer. A summary of the simulation settings is shown in Table 3.
The uniform prior performs well over a wide range of sample and
feature sizes, and generally shows signiﬁcant improvement over
the classical error estimators. Prior calibration can have even more
pronounced improvement, especially for small feature sets. Also,
although the uniform prior often performs better than the calibrated

 

1 829

112 /§.IO'SIBUJDOIpJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

L.A.Dalton and E.R.Dougherty

 

 

 

    

 
   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

     

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    

' 0.055

.0
o
01

(a) (b)
g 0.13 """ " '00 g  “““ “ '00
3 --‘-‘cv g 0-1  ‘---‘cv
a 0-12 ---boot a ‘ "  ---boot
E 011 —bol E 0.09 —bol
g ( -e-BEE, flat g -e-BEE, flat
: 0.1 +BEE, calib. : 0.08
.9 , .9
E: 0.09 '  T5
5  ,,,,,, I. 5 
a 0.08 ~  a
2 2 0.06
D: 0.07 E

20 3 40_ 25 30 35 40 45_ 50 55

Number of sample pornts (n) Number of sample pornts (n)
(c) (d) I
g 0'09  “““ -‘ loo g 0_og  ----- ~loo
CD ~¢ __,_,cv g  ._._.cv
E 0.08 g x  ---boot 3 0-075  ---boot
E ‘~':';.:'I';'~~.. —b0| E   —b0|
9 ‘4  -9-BEE, flat 2 ’-..§"'1-6-BEE, flat
“E 0.07  g 0.065 "ax—BEE, ca|ib_
.9 -— “‘1..- vvvvvv ..
E a   -N‘
> 0.06
g g 0.055
co cm
0.05

E 0.05 E .5

30 50 _ 60

Number of sample pornts (n)

(e (f)
.5  0.4
t 0.075  “““ ” '00 —D=1
g  1-1-‘cv ,_ «20:2
2 0.07  ---boot g o 35\ -EI—D=3
“  . —bol “3 +D=4
E ~ 
g 0065  -e-BEE, flat 3 +D=5
.6 0'06 +BEE’  8 0.3 M
a—r ..‘.‘_ 46
g a
'o 115
U)
E

 

 

 

R
.0
O
1;
J1

 

 

 

70

.1;
O

Numbe?%f sample ggints (n) Nuretber 61 sampl5e points ?n)
Fig. 8. RMS deviation from true error and expected true error with
LDA classiﬁcation of empirical measurements from a breast cancer study.
(a) 1 feature; (b) 2 features; (c) 3 features; ((1) 4 features; (e) 5 features;
(1') expected true error.

prior for high feature sizes, see for example Figure 8e for 5 features,
we observe in Figure 8f that true error does not improve much, and
may actually get worse, for as little as 5 features. This may indicate
that when there is not enough calibration data for good prior design,
there is also probably insufﬁcient data for good classiﬁer design.

3.3 Concluding remarks

Our synthetic data simulations demonstrate the power of prior
knowledge in two ways: we may assume a low Bayes error by using
a ﬂat prior and outperform the classical error estimators where they
perform best, or we may calibrate a prior, even using purely data—
driven methods, and obtain superior performance in the midrange
of Bayes errors. Also note that for moderately difﬁcult classiﬁcation
problems which are typical in a small sample biological setting,
the midrange is precisely where training data error estimation is
needed. One might argue that there is a risk with postulating
a low—Bayes—error prior since, although it will show excellent
performance if the Bayes error is truly low, it will suffer for large
Bayes errors. In Figure 5, not only does performance deteriorate
with increasing Bayes error for the Bayesian MMSE estimator,
but also the performance of cross—validation. This should not be
surprising because the use of cross—validation presupposes that the
Bayes error is small because its performance seriously degrades
for increasing Bayes error. This behavior, noted more than 30
years ago in a simple 1D Gaussian model (Glick, 1978), has been

 

 

   
   

   

 

 

 

 

(a) (b)

g   loo 0.1  11111 H loo

3 0.12   ---boot 0.09 ---boot

E 0.11   —bol —bol

9 -9-BEE, flat 0.08   -9-BEE, flat
“g  "" ~+BEE,calib. ’

....... .,

  

9
o
on

 

RMS deviation from true error

 

9
o
01

 

 

 

 

 

 

20 30 4O 25 30 35 40 45 50 55
Number of sample points (11) Number of sample points (n)
(c (d)
0_09   loo — D=1
 1-‘--cv -e- D=2
 - - -boot \ —a— D=3
0-08  —bol 0.3 —x- D=4
 + BEE, flat + D=5

 

    

 

 

+BEE, calib.

 

Expected true error
0
ix)
U1

 

 

 

RMS deviation from true error
0
o
\‘

 

 

 

40 50 4 0 0
Number of sample points (n) Number of sample points (n)

Fig. 9. RMS deviation from true error and expected true error with
QDA classiﬁcation of empirical measurements from a breast cancer
study.(a) 1 feature; (b) 2 features; (c) 3 features; ((1) expected true error.

V
A
V

RMS deviation from true error ’5
RMS deviation from true error 0'

   

25 30 35 40
Number of sample points (n)

20

30 40
Number of sample points (n)

O
V

O.
V

  
  

RMS deviation from true error
RMS deviation from true error

   

35 40 45 50 _
Number of sample pomts (n)

30 40 50
Number of sample points (n)

 

 

 

 

 

 

(e) (f)
 \_ _D=.
g  L -e-D=2
a """""" ~ E ED=3
g g 0.3 +D=4
e a +D=5
‘5 g We
e *5
g 80.25 W
'o .31 W
m _
E

40 0'20

 

50 60 30 4O 50 60
Number of sample points (n) Number of sample points (n)

Fig. 10. RMS deviation from true error and expected true error with
3NN classiﬁcation of empirical measurements from a breast cancer
study.(a) 1 feature; (b) 2 features; (c) 3 features; ((1) 4 features; (e) 5 features;
(1') expected true error.

 

1 830

112 /§.IO'SIBUJDOIpJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pep1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Application of the Bayesian MMSE error estimator to microarray data

 

Table 3. Classiﬁcation schemes and settings for simulation with real breast cancer data

 

 

 

 

Classiﬁer Sample size Feature selection BEE calibration Iteration
Training Test Original Shapiro—Wilk test 2nd t—test

LDA 20—50 295 — n 70 95% conﬁdence D = 1 70 — D 100 000
LDA 25—55 295 — n 70 95% conﬁdence D = 2 70 — D 100 000
LDA 30—60 295 — n 70 95% conﬁdence D = 3 70 — D 100 000
LDA 35—65 295 — n 70 95% conﬁdence D = 4 70 — D 100 000
LDA 40—70 295 — n 70 95% conﬁdence D = 5 70 — D 100 000
QDA 20—50 295 — n 70 95% conﬁdence D = 1 70 — D 10 000
QDA 25—55 295 — n 70 95% conﬁdence D = 2 70 — D 10 000
QDA 30—60 295 — n 70 95% conﬁdence D = 3 70 — D 10 000
3NN 20—50 295 — n 70 95% conﬁdence D = 1 70 — D 10 000
3NN 25—55 295 — n 70 95% conﬁdence D = 2 70 — D 10 000
3NN 30—60 295 — n 70 95% conﬁdence D = 3 70 — D 10 000
3NN 35—65 295 — n 70 95% conﬁdence D = 4 70 — D 10 000
3NN 40—70 295 — n 70 95% conﬁdence D = 5 70 — D 10 000

 

demonstrated via large simulations for both discrete and Gaussian
models (Dalton and Dougherty, 2011a, b), and has been analytically
proven in the Gaussian model (Zollanvari et al., 2010). In other
words, unless one is not interested in error estimator performance,
use of cross—validation carries with it implicitly assumed prior
knowledge. If one knows that the Bayes error is low, then why not
deﬁne a prior model based on this assumption to design a Bayesian
error estimator with even better performance?

ACKNOWLEDGMENTS

The authors would like to thank Yidong Chen for his helpful
discussions on modeling microarray data.

Funding: Philanthropic Educational Organization (PE.O.); National
Science Foundation (CCF—0634794) in part.

Conﬂict of Interest: none declared.

REFERENCES

Autio,R. et al. (2009) Comparison of Affymetrix data normalization methods using
6,926 experiments across ﬁve array generations. BMC Bioinformatics, 10(Suppl.
1), Article S24.

Braga—Neto,U. and Dougherty,E.R. (2004a) Bolstered error estimation. Pattern Recogn,
37, 1267—1281.

Braga—Neto,U.M. and Dougherty,E.R. (2004b) Is cross-validation valid for small-
sample microarray classiﬁcation? Bioinformatics, 20, 374—380.

Braga—Neto,U. and Dougherty,E.R. (2010) Exact correlation between actual and
estimated errors in discrete classiﬁcation. Pattern Recogn. Lett., 31, 407—412.
Dalton,L.A. and Dougherty,E.R. (2011a) Bayesian minimum mean-square error

estimation for classiﬁcation error—part 1: deﬁnition and the Bayesian MMSE error

estimator for discrete classiﬁcation. IEEE Trans. Signal Process, 59, 115—129.
Dalton,L.A. and Dougherty,E.R. (2011b) Bayesian minimum mean-square error

estimation for classiﬁcation error—part II: the Bayesian MMSE error estimator for

linear classiﬁcation of Gaussian distributions. IEEE Trans. Signal Process, 59,
130—144.

Devroye,L. and Wagner,T. (1979) Distribution-free inequalities for the deleted and hold-
out error estimates. IEEE Trans. Inf. Theory, 25, 202—207.

Devroye,L. et al. (1996) A Probabilistic Theory of Pattern Recognition. Springer,
New York.

Dougherty,E.R. and Braga-Neto,U. (2006) Epistemology of computational biology:
mathematical models and experimental prediction as the basis of their validity.
J. Biol. Syst., 14, 65—90.

Dougherty,E.R. et al. (2009) Performance of feature selection methods. Curr. Genomics,
10, 365—374.

Dougherty,E.R. et al. (2010) Performance of error estimators for classiﬁcation.
Curr. Bioinformatics, 5, 53—67.

Glick,N. (1978) Additive estimators for probabilites of correct classiﬁcation. Pattern
Recogn, 10, 211—222.

Hanczar,B. et al. (2007) Decorrelation of the true and estimated classiﬁer errors in
high-dimensional settings. EURASIP J. Bioinform. Syst. Biol., 38473.

Hoyle,D.C. et al. (2002) Making sense of microarray data distributions. Bioinformatics,
18, 576—584.

Hua,J. et al. (2005) Optimal number of features as a function of sample size for various
classiﬁcation rules. Bioinformatics, 21, 1509—1515.

Hua,J. et al. (2009) Performance of feature-selection methods in the classiﬁcation of
high-dimension data. Pattern Recogn, 42, 409424.

Johnson,M.E. (1987) Multivariate Statistical Simulation. John Wiley and Sons,
New York.

Rowe,D.B. (2003) Multivariate Bayesian Statistics: Models for Source Separation and
Signal Unmixing. Chapman & Hall/CRC, Boca Raton, FL.

Shapiro,S.S. and Wilk,M.B. (1965) An analysis of variance test for normality (complete
samples). Biometrika, 3, 591—611.

Sima,C. and Dougherty,E.R. (2006) What should be expected from feature selection in
small-sample settings. Bioinformatics, 22, 2430—2436.

van de Vijver,M.J. et al. (2002) A gene-expression signature as a predictor of survival
in breast cancer. N. Engl. J. of Med., 347, 1999—2009.

Villasenor Alvaa,J.A. and Estradaa,E.G (2009) A generalization of Shapiro-Wilk’s test
for multivariate normality. Comm. Statist. Theory Methods, 38, 1870—1883.

Zollanvari,A. et al. (2010) On the joint sampling distribution between the actual
classiﬁcation error and the resubstitution and leave-one-out error estimators for
linear classiﬁers. IEEE Trans. Inf. Theory, 56, 784—804.

 

1831

112 /§.IO'SIBU.IHO[p.IOJXO'SOllBIIIJOJUIOIQ/ﬁdllq 11101; popnommoq

9IOZ ‘09 lsnﬁnv uo ::

