Bioinformatics, 32(15), 2016, 2306—231 2

doi: 10.1093/bioinformatics/btw097

Advance Access Publication Date: 9 March 2016
Original Paper

 

 

Genetics and population analysis

Rapid genotype refinement for whole-genome
sequencing data using multi-variate normal

distributions

Rudy Arthur*, Jared O'Connell, Ole Schulz-Trieglaff and Anthony J. Cox

lllumina Cambridge Ltd, Chesterford Research Park, Little Chesterford, Essex CB10 1XL, UK

*To whom correspondence should be addressed.
Associate Editor: Oliver Stegle

Received on November 11, 2015; revised on January 25, 2016; accepted on February 14, 2016

Abstract

Motivation: Whole-genome low-coverage sequencing has been combined with linkage-disequilibrium
(LD)—based genotype refinement to accurately and cost-effectively infer genotypes in large cohorts of
individuals. Most genotype refinement methods are based on hidden Markov models, which are accur-
ate but computationally expensive. We introduce an algorithm that models LD using a simple multivari-
ate Gaussian distribution. The key feature of our algorithm is its speed.

Results: Our method is hundreds of times faster than other methods on the same data set and its
scaling behaviour is linear in the number of samples. We demonstrate the performance of the

method on both low— and high-coverage samples.

Availability and implementation: The source code is available at https://github.com/illumina/marvin

Contact: rarthur@illumina.com

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

The 1000 Genomes Project (1000GP) has pioneered the approach of
combining low-coverage whole-genome sequencing (LCWGS) with
linkage disequilibrium (LD)-based genotype refinement to success-
fully build large panels of accurately genotyped individuals (The
1000 Genomes Project Consortium, 2010, 2012, 2015). This has
provided a cost-effective alternative to sequencing many individuals
at high-coverage. However, genotype refinement has a large compu-
tational burden. For example, Delaneau et al. (2014) quote around
32 compute years to perform haplotype estimation on 1092
LCWGS individuals using the 1000GP haplotype estimation pipe-
line. This figure measures the cost of haplotype phasing (which our
method does not address) as well as genotype refinement. Given
increasing sample sizes, decreasing sequencing costs and the typic-
ally super-linear scaling of refinement algorithms, we are fast ap-
proaching a point where computation will account for a substantial
proportion of the cost of such analyses.

Low-coverage genotyping typically proceeds by calculating
genotype likelihoods (GLs) at a fixed set of variants (SNPs and small
indels) from read alignments, the variant list being created at an

earlier variant discovery step. These GLs reflect the likelihood of the
read data conditional on each of the three possible genotypes
(assuming a bi-allelic site). These uncertain GLs are then refined
into genotypes by exploiting LD, the correlation between physically
close variants across individuals. This final step is often referred to
as genotype refinement and involves one (or more) phasing and im-
putation algorithms. The most accurate phasing and imputation
techniques typically employ hidden Markov models (HMMs) which
are computationally demanding, examples include Beagle
(Browning and Browning, 2007), Thunder (Li et al., 2011) and
SHAPEIT (Delaneau et al., 2012, 2013). The final genotypes of
1000GP were created using a combination of SHAPEIT and Beagle;
starting haplotypes were generated with the faster Beagle method
and then were further refined using the slower, and more accurate,
SHAPEIT (Delaneau et al., 2014).

A closely related problem is the imputation of variants into study
samples assayed on DNA microarrays from reference panels of
sequenced individuals (Marchini et al., 2007). Several very fast
methods have recently emerged for this scenario (Durbin, 2014;
Fuchsberger et al., 2015; Howie et al., 2012). These rely on the

©The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 2306

9mg ‘09 isnﬁnV uo seleﬁuV socl ‘erulomeg JO AirSJQAru [1 112 [3.10811211an[plOJXO'SODBIILIOJIIlOlQ/ﬂ(11111 won pepeolumoq

Rapid genotype refinement for whole-genome sequencing data

2307

 

availability of phased haplotypes for both study and reference data
and it is not clear such algorithms will generalize to the LCWGS use
case.

An alternative to HMM-based imputation is simply to predict
genotypes as linear combinations of other genotypes at physically
close ﬂanking markers, modelling the correlation between variants
as a multivariate normal (MVN) distribution. This idea was first
introduced by Wen and Stephens (2010), where it was used in the
more traditional setting of imputing genotypes into DNA micro-
array samples from a reference panel. Menelaou and Marchini
(2013) introduced a related approach, MVNcall, that performs im-
putation on LCWGS data for which the individual has also been
assayed on a DNA microarray, exploiting the ‘backbone’ of confi-
dent microarray genotypes to improve genotypes at non-microarray
sites.

We introduce a new technique based on MVN representations of
LD that extends these ideas to the LCWGS-only imputation scen-
ario. The method exploits various efficient linear algebra operations,
making it hundreds of times faster than the fastest HMM method.
This speed comes with a decrease in accuracy compared with
HMMs, but is still substantially more accurate than genotype calls
made using no LD information.

In the ‘Methods’ section, we outline the model and its implemen-
tation. In our ‘Results’ section, we contrast the speed and accuracy
of our technique with Beagle on 2535 samples from 1000GP Phase
3 (LCWGS) and 3781 samples taken from the UK10K project
(UK10K Consortium et al., 2015; Huang et al., 2015). Finally, we
demonstrate the applicability of LD-based genotype refinement in
the high-coverage WGS setting, something that has not been investi-
gated to date.

The method is implemented in a software package called
MarViN (MultiVariate Normal imputation) and is freely available
under the GPLv3 license.

2 Materials and methods

We assume that N diploid individuals have been sequenced and used
to detect M bi-allelic polymorphisms. We record the number of cop-
ies of the non-reference (alternate) allele in a matrix

G,,- 6 {0,1,2}, (1)

where the indexes i and j label polymorphic sites and individuals re-
spectively. We assume that we have been given GLs

P(R,—,~|G,—,— = k), (2)

where k E {0, 1, 2} and R,,- denotes the reads aligning to site iin indi-
vidual i.

2.1 Single-site model

We now describe a simple Expectation-Maximization (EM) algo-
rithm that we use to initialize our model. We apply Bayes’ theorem
to obtain posterior probabilities of genotypes:

P(G,‘i = klRiI') OC P(G,‘i = k)P(Rii|Gi/' =  

where P(G,~,~ = k) is the prior probability of seeing genotype k and is
initialized as  Dosages (expected genotype values) at each site can
be calculated from

G,,- = Z kP(G,—,— = k|R,—,~) (4)
k

This constitutes the E-step of our routine. The M-step involves re-
estimating our prior, P(G,~,~ = k). First, we estimate site allele fre-
quencies as

A 1
u,- =  Gii- (5)

Assuming Hardy-Weinberg equilibrium, our updated prior is then

(1 — aai)2 k = 0)
P(Gii = k) = 201-(1 — in) k = 1, (6)
p} k = 2.

The E-step and M-step are iterated and generally converge rapidly.

2.2 Multi-site model

This EM algorithm gives an estimate of G,,- that takes into account
the population allele frequency at site i but ignores any correlation
with flanking sites (i.e. LD). We now describe how to improve
the estimate of G using LD. A simple way to encode LD is with the
M X M covariance matrix 2, where

1
21'1" = m;(Gi/' — Hi)(Gz"/' — W)- (7)

Following Wen and Stephens (2010), we make the assumption that
the probability density for the vector of dosages gm for individual i,
the jth column of the genotype matrix ) = G,,-), is MVN:

Hg“) = g) oc exp (g — M)TE‘1(g — in) (8)

We can then ask ‘what is the distribution for the dosage at site i of
individual i conditional on the dosages at all other sites?’ For the
MVN, a closed form expression for this conditional probability exists:

P(G._kIG.._ (i)  9
1/— I’I—gi')o<eXP 20. l )

where gig) refers to all genotypes excluding site i and

M M ~ 0)
0i = 217+ 2 Z Eilglmzmia (10)
179i maéi
” (')
Vii = Hi + Z Z 21191”ij — Hm), (11)
179i maéi

and the matrix (2‘ is the inverse of the matrix formed by deleting the
ith row and column from 2.

In words, what we are doing is using the genotype matrix to esti-
mate allele frequencies and LD. Fixing these, we re-estimate the
genotype matrix using the MVN assumption. The approach is simi-
lar to our single site EM algorithm, but with the simple population
frequency prior in Equation (6) replaced with the more sophisticated
population LD prior in Equation (9).

Examining the terms closely, we see that o,- is independent of the
individual, as is the quantity

t,,,, = 2 2,953,. (12)
[an

Thus we need only calculate it once. Rewriting Equation (11), we
see that updating the mean of individual i is achieved by evaluating

Vii = Mi + ZmﬁtiMij — Hm), (13)

at a cost of one dot product per site per individual.

9mg ‘09 isnﬁnV uo sejeﬁuV SO’I ‘etulomeg JO KitSJQAtu [1 112 ﬁlO'Sjeumo[pJOJXO'sopeuuogutotq/ﬁdnq wort pepeolumoq

2308

R.Arthur et al.

 

2.3 Algorithm description
We initialize G using the single-site model described in Section 2.1.
We then repeat the following steps for a default of five iterations:

Calculate u and 2 from G.

Calculate tim and o,- for all sites i and m.

For all sites i and individuals I, calculate Vii.
Update P(G,-,- = k|R,-,-) using Equations (3) and (9).
Recalculate G.

9:“?!“1‘

We take the final estimate of G as our imputed genotypes. We could
iterate steps 2 and 3, reusing the covariance matrix obtained at the
beginning of the iteration but we found this to be unhelpful in
practice.

2.4 Calculating Q
Computationally, step 1 is dominated by the calculation of 2,
which takes O(NM2) operations. Step 2 requires a matrix vector
product for every individual and so is also O(NM2). However, a
straightforward implementation of step 3 would be O(M4), since a
matrix must be inverted at each site at a cost of O(M3) per
inversion.

To see how step 3 can be sped up, consider the case where we
want to update the marker 1 while fixing the M — 1 markers to the
right. We write the covariance matrix in the following form:

211 212
2 = (14)
221 222
where 211 is 1 X 1 and 222 is (M — 1) X (M — 1).
To calculate 01, we require

01 = 211 + 21222—21221 (15)
[compare Equation (10)]. The big overhead here is calculating 22—21.
We define
_1 Q11 912
9:2 2 , um
Q21 922

where the blocks are sized to match the corresponding submatrices
of 2. By making an LDU decomposition, we can show that

22—21 = 922 — Q2191—11912, (17)

which is known as the Schur complement of £211 in (2. This gives us
52(1) 2 22—21 which we can use in Equation (12).

Consider the variant at site i. The matrix we need to invert in
order to evaluate the conditional expectation is the inverse of a sub-
matrix of 2 formed by deleting the ith row and column of 2.
Swapping rows 2' and 1 and columns 2' and 1 of 2 puts the matrix we
need the inverse of in the position of 222 in Equation (14). A row
and column can be swapped by pre- and post-multiplying with a

permutation matrix P.
2 —> P2PT. (18)
Because permutation matrices are orthogonal we have that
(PEPT)_1 = P(2)_1PT = PQPT. (19)

The required inverse for variant 2' can be obtained by applying
Equation (17) again on the permuted matrix. In practice we just
swap rows and columns of the matrix the usual way, which is
equivalent to the multiplication. This trades M matrix inverses for a

single matrix inverse plus M matrix operations of complexity O(M2)
each (matrix-vector products), giving an O(M3) overall cost.

2.5 Using a reference panel

If we have a small number of individuals to impute and a reference
panel formed from a large number of individuals with hard geno-
types assigned, we can impute individuals using the panel by follow-
ing the procedure below:

1. Calculate allele frequencies )1 and the covariance matrix 2 from
the panel.

2. Use the panel allele frequencies to obtain an initial estimate of G
from the GLs.

3. Calculate tim and o,- for all sites i.

4. For each individual with genotype gm to be imputed, the follow-
ing steps are performed K times:

5. For all sites i, calculate vii

Update Hg?) 2 k|Rij) using Bayes’ theorem, Equations (3) and (9).

9“

7. Recalculate g0).

Calculating tim is O(M3) and 2 is O(NM2), both of which must
be done once per panel. To impute each new individual then re-
quires performing O(M2) operations for each of K iterations, where
K was be around 5 in practice, we found that performing more than
five iterations did not improve the quality of the imputation in al-
most every case.

2.6 Regularizing the covariance matrix

To guard against degeneracy due to perfect correlation and force the
variance to be non-zero, we performed Tikhonov regularization on
the covariance matrix, i.e. applied the transformation

2 _> 2 + 21. (20)

By scanning a range of possible values of 2 we found 2 = 0.06 to be
an effective value for the regularization parameter, the same value
as found in Menelaou and Marchini (2013). Alternative regulariza-
tion methods (such as adding a matrix proportional to the diagonal
of the covariance matrix, as done in the Levenberg-Marquardt algo-
rithm) were evaluated but were not found to confer a significant

improvement.
After Wen and Stephens (2010), we also modify the mean as
follows:
0
Hi —’ (1 — 9).“i ‘1'? (21)
where

. _ (2?“ r1) 4
_ 2N + (21.20” r1)_1 '

This correction is relevant in the case of small cohorts where

 

the empirical mean may be a bad estimate of the true mean, the
specific form above is derived in Wen and Stephens (2010) using
the model of Li and Stephens (2003). In our case, with cohorts of
2500 or more, the difference between this and the sample mean is
very small.

2.7 Implementation
We implemented our method in C++ using the the Eigen matrix
library (Gaél Guennebaud, Benoit Jacob and others, Eigen v3,

9mg ‘09 isnﬁnV uo sejeﬁuV SO’I ‘etulomeg JO AnswAtu [1 112 ﬁlO'Sjeumo[pJOJXO'sopeuuogutotq/ﬁdnq wort pepeolumoq

Rapid genotype refinement for whole-genome sequencing data

2309

 

http://eigen.tuxfamily.org) for matrix manipulations and HTSlib (Li
et al., 2009) for streaming the input VCF/BCF files.

2.8 Data

2.8.1 Low-coverage data

We make use of two different publicly available large cohorts to
evaluate our method in the low-coverage scenario. First, the
1000GP Phase 3 samples which consist of 25 35 samples from a het-
erogeneous mix of 26 populations, each sample sequenced to an
average of 7.4><. Second, data from the UK10K control group, a
more homogeneous cohort than the 1000GP samples comprising
3781 samples, each sequenced to around 7><. We only evaluated
SNPs with minor allele count >1 in these comparisons.

As validation data, we used freely available high-coverage
(>80><) data from Complete Genomics (CG). A subset of the
1000GP samples (287) were also sequenced by CG. To create valid-
ation data for the UK10K samples, we took 63 of the European CG
samples and calculated GLs at the UK10K sites for these samples
from their respective low-coverage BAM files using bcftools (Li
et al., 2009). MarViN imputation was performed in 200 kbp win-
dows with an overlap of 100kbp between windows. We performed
a number of small timing experiments on a 2 Mbp region of chr20,
and a more rigorous accuracy experiment using the entire chr20 for
both cohorts. A summary of the samples and number of variants is
in Table 1.

On both these cohorts, we compared MarViN with two alterna-
tive genotype refinement schemes: Beagle 4.0 (r1399) (Browning
and Browning, 2007) and the ‘no-LD’ method we described in
Section 2.1, which does not use LD information. We chose Beagle as
a comparison due its popularity, ease-of—use and relative speed com-
pared with other HMM routines. Notably the SHAPEIT pipeline
(used to produce 1000GP Phase 3 haplotypes) requires running
Beagle as a first step, and hence is more accurate but slower than
Beagle. Given we expect MarViN to be substantially faster, but also
less accurate, than Beagle, it is reasonable to conclude that MarViN
will be faster (and less accurate) than other more computationally
demanding HMM based routines.

2.8.2 High-coverage data

We took 5 0X coverage of 100 bp-paired reads sequenced from the
widely studied NA12878 sample (ENA AC:ERR194147). These
were aligned with BWA-MEM 0.7.12 (Li, 2013) and small variants
were called according to GATK3.3-0 best practices (Auwera et al.,
2013; DePristo et al., 2011), the associated GLs were supplied to
MarViN. If an alternate allele for a variant in the 1000GP reference
panel was not detected in a given sample then we used the GL taken
from the homozygous-to-reference interval in the gvcf file that over-
lapped the variant site.

MarViN can only improve genotyping at variants seen in the ref-
erence panel (variants with LD and frequency information). Any
variant called in an individual that has been seen in a curated panel
such as 1000GP is likely to be real given sufficient coverage (some
amount of false discovery in 1000GP notwithstanding), since these
variants have already been carefully filtered. Variants called in an in-
dividual that are not present in 1000GP require more scrutiny, al-
though we still expect tens to hundreds of thousands of novel
(mostly rare) variants in a given sample.

Hence we apply the hard filters described in Li (2015) to non-
1000GP variants using hapdip (http://bit.ly/HapDip). For variants
called by GATK that intersect with 1000GP, we are less stringent,
only filtering on the genotype quality (GQ) field, the phred-scaled

probability that a genotype is incorrect. The GQ field is produced
both by GATK and MarViN.

When setting up the reference panel, we excluded NA12878 and
all other CEPH1463 pedigree members from the 1000GP Phase 3
panel so as not to bias results. We only considered bi-allelic SNPs
with an alternate allele count of at least five, reasoning that very
rare variants were unlikely to benefit greatly from LD-based refine-
ment. We ran MarViN for five iterations with a window size of 210
kbp with overlap of 5 kbp at each end (so each window overlaps by
10 kbp).

As truth data, we used the highly accurate NA12878 call
set from Platinum Genomes v7.0.0 (http://www.illumina.com/plati
numgenomes). This consists of variants and confident homozygous-
reference intervals generated from multiple aligners/callers on the
17-member CEPH1463 pedigree. The reliability of the variant calls
is enhanced by retaining only those calls whose inheritance pattern
across the pedigree is consistent with Mendelian inheritance. GATK/
MarViN callsets were compared to this truth data using hap.py
(https://github.com/Illumina/hap.py), a tool which compares vari-
ants via alignment and exact matching.

3 Results

3.1 Low-coverage genotype refinement
We first evaluated each method’s speed and accuracy as a function
of sample size by sampling subsets of the UK10K cohort of sizes
N: {100, 200, 500, 1000, 2000, 3844} and performing genotyping
on a 2 Mbp window of chromosome 20 (35—37 Mbp) containing 14
416 SNPs. We measured the non-reference discordance (NRD) of
each method, which is defined as (FP + FN)/(FP + TP + FN), where
TP, FP and EN count the number of true positive, false positive and
false negative genotypes involving an alternate allele call. The ad-
vantage of NRD over discordance is that genotypes that are homo-
zygous-reference (in both the imputed and truth set) are ignored,
these counts are typically large and represent easy genotypes to call,
causing a simple discordance metric to be overly optimistic. Timings
were performed on a an Intel Xeon E5-2670v2 CPU with no other
compute intensive processes running. We do not report compute
times for no-LD as this process is dominated by I/O operations.
Figure 1 plots NRD (left) and compute time in hours (right)
against sample size. When N = 100; no-LD, MarViN and Beagle
had NRD of 5.74, 5.20 and 1.26% meaning MarViN was substan-
tially less accurate than Beagle. However, MarViNr, accuracy dra-
matically increases with sample size. MarViN had 0.71% NRD at
N = 1000 and 0.63% at N = 3844 versus 0.59 and 0.38% for
Beagle. Although still less accurate than Beagle, MarViN,e speed ad-
vantage widens with increasing N, it being 104>< and 1445 >< faster
than Beagle for N = 1000 and 3844, respectively. Notably MarViN

Table 1. Summary of the number of samples and SNPs for each
LCWGS data set

 

 

Cohort Samples SNPs CG sample CG SNPs
1000GP 2535 1 628 533 287 565 991
UK10K 3844 489 278 63 223 528

 

Sample size is the number of samples present in the input GLs, for UK10K
this includes the UK10K control cohort (3781 samples) plus an additional 63
CG validation samples with GLs calculated from low coverage alignments.
Number of SNPs is the number of non-singleton bi-allelic SNPs in each re-
spective cohort on chromosome 20. The rightmost two columns count the
number of samples and SNPs that are also in the CG validation data.

9mg ‘09 1sn8nV uo sejeﬁuV SO’I ‘121u10111123 10 A11s19Atu [1 112 /810'S{12umo[p101x0'831112u1101u101q/ﬁd11q 111011 pepeolumoq

2310

R.Arthur et al.

 

had around 9-fold fewer errors than no-LD at N = 3844 and
required minimal compute resources (@14CPU 14 CPU minutes for
N = 3844) .

We then evaluated each method for both the 1000GP Phase 3 co-
hort (N = 25 35 ) and the UK10K (N = 3844) for the entire chromo-
some 20 (1.63 and 0.49 million SNPs, respectively). To achieve a
fair comparison of compute requirements, we gave each method ex-
clusive use of a 20-core compute node (2 X 10-core E5-2670v2
CPUs), running Beagle with 20 threads and running 20 simultaneous
MarViN processes (concatenation time is included in the results).

Table 2 summarizes the accuracy and compute times. MarViN
was 360>< faster than Beagle on UK10K and 46x faster on 1000GP.
MarViN.y speed advantage on 1000GP is decreased relative to
UK10K due to a much larger number of SNPs. MarViN had higher
NRD than Beagle with 1.66 versus 0.90% on 1000GP and 0.64 ver-
sus 0.41% on UK10K. Although these accuracy differences may
seem small, the error rates are concentrated at low-frequency geno-
types. For example, NRD for UK10K on MAF <5% SNPs was
6.52, 2.66 and 1.20% for no-LD, MarViN and Beagle, a larger dif-
ference than when common variants are also considered.
Nevertheless, MarViN has 4.64>< and 9.82>< fewer errors than the
naive no-LD routine.

We then investigated accuracy at different allele frequencies by
binning genotypes by allele frequency and calculating Pearsonti cor-
relation coefficient (rz) between the imputed genotypes and the
high-coverage validation genotypes within each bin. Figure 2 plots
r2 against allele frequency (loglo scale) for 1000GP (left) and

 

 

 
    
   
   

 

 

 

 

 

 

 

 

 

6 _
°\° -0- Beagle
(D
g -0- MarViN
g 10 - -0- no—LD
84' a»
.12 5‘
'0 'U _
8 D All
c 0- - - MAF<5%
e 0 5 _
a) 2 '
q—
‘f’
C
O
z
o - o _
I I I I I I I I I
0 1000 2000 3000 4000 0 1000 2000 3000 4000
Sample size Sample size

Fig. 1. Performance of each method on UK10K cohort for increasing samples
sizes (chr20:35M-37M). Left: NRD versus sample size for three different meth-
ods. Right: Total compute time in hours versus sample size (Color version of
this figure is available at Bioinformatics online.)

Table 2. Performance for each method on the 1000GP (N = 2535)
and UK10K (N: 3844) chr20 data

 

 

All SNPs MAF < 5 %
Cohort Method Time Discordance NRD DIS NRD
(DIS)
1000GP no-LD — 0.62 7.71 0.1 8 1 1.28
1000GP MarViN 19.03 0.1 1 1.41 0.08 4.74
1000GP Beagle 878.31 0.07 0.90 0.04 2.77
UK10K no-LD — 1.16 6.35 0.21 6.52
UK10K MarViN 2.26 0.1 1 0.61 0.09 2.66
UK10K Beagle 812.52 0.08 0.44 0.04 1.20

 

Time is the compute time in hours on a 20-core (2xE5-2670v2 CPUs) ser-
ver with 132GB RAM when using 20 threads. DIS is the percentage of dis-
cordant genotypes between the imputed genotypes and high-coverage
genotypes on the CG validation samples. NRD is the discordance when not
counting genotypes that were homozygous reference in both the imputed and
high-coverage genotypes.

UK10K (right). We see for common variants (AF 2 2%) Beagle and
MarViN are roughly equal (and substantially better than no-LD).
Beagle outperforms MarViN at lower allele frequencies. Figure 2
and Table 2 also suggest that MarViN performs less well on hetero-
geneous cohorts such as 1000GP, compared with relatively homoge-
neous cohorts like UK10K.

3.2 High-coverage genotype refinement

Figure 3 plots recall (proportion of PG SNPs detected and correctly
genotyped) against precision (the proportion of called SNPs that are
concordant with PG) for GATK before and after refinement with
MarViN for increasingly liberal filters on the GQ field. MarViN re-
finement yields a modest, but consistent, improvement in SNP recall
for a given level of precision (and vice versa).

Table 3 further breaks down these results. First, there were 243
381 SNPs called by GATK that were not in 1000GP (with minor al-
lele count >4), these were filtered using the hard filters in hapdip.
Such SNPs cannot be further refined but we report them for

 

 

- - - - -
- ~ ~
N
98.0-
97.5-
o\°
8910-
G)
or
96.5-
— GATK
96.0-
- GATK+MarViN

 

 

 

I I I I I I
99.700 99.725 99.750 99.775 99.800 99.825
Precision %

Fig. 2. Pearson.t correlation coefficient between imputed and true genotypes
for different cohorts as a function of allele frequency for different data sets.
Left: 1000GP Right: UK10K

 

1000G UK10K

 

1.00-

0.95 -

0.90 -

Aggregate R2

0.85 -

 

 

 

 

 

0.80 '
I I I I I

I I I I I
0.01 0.1 1 10 1000.01 0.1 1 10 100
Non—reference allele frequency (%)

 

Fig. 3. Recall versus precision for GATK SNP genotype calls before and after re-
finement with MarViN for increasingly aggressive GO filters. Recall is defined
as the proportion of PG alternate genotypes correctly identified. Precision is the
proportion of alternate genotypes called that are concordant with the PG data
set (Color version of this figure is available at Bioinformatics online.)

9mg ‘09 1sn8nV uo sejeﬁuV SO’I ‘121u10111123 10 A11s19Atu [1 112 /810'S{12umo[p101x0'831112u1101u101q/ﬁd11q 111011 pepeolumoq

Rapid genotype refinement for whole-genome sequencing data

2311

 

Table 3. Summary of high coverage SNP calls by GATK that were
not present in 1000G with MAC >4 (first column), calls made by
GATK that were present in 1000G (middle column) and GATK calls
after applying MarViN genotype refinement (third column)

 

 

non-1000G 1000G
GATK MarVN
Total 243 381 3 387 126 3 408 128
Unevaluated 97 748 74 813 86 845
TP 143 247 3 309 226 3 317 953
FN 3 377 480 211 373 202 888
FP1 (GT wrong) 1251 1577 1318
FP2 (allele wrong) 98 81 96
FP3 (homref) 1037 1429 1916
FP (total) 2386 3087 3330

 

completeness. Of these, 143 247 SNPs were validated in the
Platinum Genomes dataset and a total of 2386 were classified as
false positives due to having either incorrect genotypes, incorrect al-
leles or being called in a known homozygous-reference region. This
yields a precision of 98.36%, which as one might expect, is lower
than calls that intersect with 1000GP variants.

For SNP calls that intersect 1000GP, we only applied a GO 2 fil-
ter. The GATK callset contained 3 387 126 SNPs, rising to 3 408
128 after refinement with MarViN. Of these, 3 309 226 and 3 317
95 3 were validated as correct in Platinum Genomes, meaning
MarViN refinement yielded an additional 8727 correctly genotyped
SNPs. In terms of effect on false positive rate, MarViN reduced the
number of incorrect genotypes (with correct allele) from 1577 to
1318, as one might expect genotype refinement to do. However,
MarViN also imputed a greater number of variants with incorrect
alternate allele (96 versus 81) and SNPs in homozygous reference
regions (1916 versus 1 037). This means MarViN had a slightly
higher number of false positives than the raw GATK callset, 3330
versus 3087, bringing its precision to 99.902 versus 99.909% for
GATK 99.902%. Given the gains in SNP recall, this seems a minor
cost to pay.

4 Discussion

The algorithm presented in this article is at least two orders of mag-
nitude faster than Beagle on the UK10K cohort. Although this speed
does come with a decrease in accuracy (particularly for rare vari-
ants), our method still makes nearly 10-fold fewer errors than a gen-
otyping routine that does not take LD into account.

The rapidly growing size of reference panels may soon preclude
the use of super-linear complexity techniques such as Beagle, since
computation will become too expensive. For example, the
Haplotype Reference Consortium (McCarthy et al., 2015) has col-
lected 32 488 LCWGS samples to create a reference panel for imput-
ation. Extrapolating from Figure 1, it seems unlikely it would be
tractable to run Beagle on a cohort of this size. One possible use of
MarViN would be to quickly generate an initial estimate of geno-
types, which could then be supplied as starting values to a more
sophisticated routine, reducing the number of iterations the latter
needs to perform. MarViN might also be an ideal routine for inter-
mediate coverage (z15 ><) projects.

The reduced accuracy of MarViN compared to Beagle at lower
frequency variation is likely due to the limitations of modelling the
population using one vector of allele frequencies and one covariance
matrix. This simplistic model may not capture more subtle

population substructure. Notably MarViN performs better on the
more homogeneous UK10K cohort than on the 1000GP cohort
which has far more population structure (although also has a
smaller sample size). One possible way to improve this situation
would be to add more flexibility to the MarViN model by using an
MVN mixture distribution, but we leave this for future work.

We have also demonstrated the efficacy of genotype refinement
in the high-coverage scenario, the first such investigation to our
knowledge. A modest gain in recall for SNPs was achieved at a cost
of a negligible decrease in precision. We also attempted refining
indels with this approach, gains in recall were indeed observed but
were accompanied by unacceptable increases in the false-discovery
rate (FDR). This may be due to a higher FDR in the 1000GP indels
and could perhaps be solved via aggressive filtering.

Although the improvements seen on high-coverage data are mod-
est, we nevertheless believe it noteworthy that results achieved from
high-coverage data can be improved at all by this method. Moreover
the efficiency of our method means it adds little additional overhead
to processing pipelines for WGS data, whereas genotype refinement
using existing HMM-based methods would be a considerable com-
putational undertaking.

Acknowledgement
We thank Stathis Kanterakis for providing GATK results.

Conﬂict of Interest: All authors are employees of Illumina Cambridge Ltd., a
public company that develops and markets systems for genetic analysis, and
receive shares as part of their compensation.

References

Auwera,G.A. et al. (2013) From FastQ data to high-conﬁdence variant calls:
the Genome Analysis Toolkit best practices pipeline. Curr. Protoc.
Bioinformatics, 11, 11.10.1—11.10.33.

Browning,S.R. and Browning,B.L. (2007) Rapid and accurate haplotype
phasing and missing-data inference for whole-genome association studies
by use of localized haplotype clustering. Am. ]. Hum. Genet., 81,
1084—1097.

Delaneau,O. et al. (2012) A linear complexity phasing method for thousands
of genomes. Nat. Methods, 9, 179—181.

Delaneau,O. et al. (2013) Improved whole-chromosome phasing for disease
and population genetic studies. Nat. Methods, 10, 5—6.

Delaneau,O., Marchini,]., 1000 Genomes Project Consortium. et al. (2014)
Integrating sequence and array data to create an improved 1000 Genomes
Project haplotype reference panel. Nat. Commun. 5, 3934.

DePristo,M.A. et al. (2011) A framework for variation discovery and
genotyping using next-generation DNA sequencing data. Nat. Genet., 43,
491—498.

Durbin,R. (2014) Efﬁcient haplotype matching and storage using the positional
Burrows Wheeler transform (PBWT). B ioinformatics, 30, 1266—1272.

Fuchsberger,C. et al. (2015 ) minimac2: faster genotype imputation.
Bioinformatics, 31, 782—784.

Howie,B. et al. (2012) Fast and accurate genotype imputation in genome-wide
association studies through pre-phasing. Nat. Genet., 44, 95 5—95 9.

Huang,]. et al. (2015 ) Improved imputation of low-frequency and rare vari-
ants using the UK10K haplotype reference panel. Nat. Commun., 6,.

Li,H. (2013). Aligning sequence reads, clone sequences and assembly contigs
with BWA-MEM. arXiU preprint arXiv:13 03 .3 99 7. https://sourceforge.net/
p/bio-bwa/mailman/message/30894287/.

Li,H. (2015). Fermikit: assembly-based variant calling for Illumina resequenc-
ing data. Bioinformatics, 31, 3694—3696.

Li,H. et al. (2009) The sequence alignment/map format and SAMtools.
Bioinformatics, 25 , 2078—2079.

9mg ‘09 1sn8nV uo sejeﬁuV SO’I ‘121u10111123 10 A11s19Atu [1 112 /810'S{12umo[p101x0'831112u1101u101q/ﬁd11q 111011 pepeolumoq

2312

R.Arthur et al.

 

Li,N. and Stephens,M. (2003) Modelling linkage disequilibrium and identify-
ing recombination hotspots using SNP data. Genetics, 165, 2213.

Li,Y. et al. (2011) Low-coverage sequencing: implications for design of com-
plex trait association studies. Genome Res., 21, 940—951.

Marchini,]. et al. (2007) A new multipoint method for genome-wide associ-
ation studies by imputation of genotypes. Nat. Genet., 39, 906—913.

McCarthy,S. et al. (2015) A reference panel of 64,976 haplotypes for genotype
imputation. doi: 10.1101/035170.

Menelaou,A. and Marchini,]. (2013) Genotype calling and phasing using
next-generation sequencing reads and a haplotype scaffold. Bioinformatics,
29, 84—91.

The 1000 Genomes Project Consortium. (2010) A map of human genome vari-
ation from population-scale sequencing. Nature, 467, 1061—1073.

The 1000 Genomes Project Consortium. (2012) An integrated map of genetic
variation from 1,092 human genomes. Nature, 491, 5 6—65 .

The 1000 Genomes Project Consortium. (2015 ) A global reference for human
genetic variation. Nature, 526, 6 8—74.

UK10K Consortium. et al. (2015 ) The UK10K project identiﬁes rare variants
in health and disease. Nature, 526, 82—90.

Wen,X. and Stephens,M. (2010) Using linear predictors to impute allele
frequencies from summary or pooled genotype data. Ann. Appl. Stat., 4,
1 1 5 8.

91m ‘09 1sn8nV uo sejeﬁuV SO’I ‘121u10111123 10 A11s19Atu [1 112 ﬁhO'smumo[p101x0'831112u1101u101q/ﬁd11q 111011 pepeolumoq

