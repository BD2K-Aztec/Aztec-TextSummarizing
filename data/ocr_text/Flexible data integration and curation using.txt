Bioinformatics, 32(6), 2016, 918—925

doi: 10.1093/bioinformatics/btv644

Advance Access Publication Date: 10 November 2015
Original Paper

 

Databases and ontologies

Flexible data integration and curation using a
graph-based approach

Samuel Croset*, Joachim Rupp and Martin Romacker

Roche Innovation Center Basel, F. Hoffmann-La Roche AG, CH-4070 Basel, Switzerland

*To whom correspondence should be addressed.
Associate Editor: John Hancock

Received on June 5, 2015; revised on October 20, 2015; accepted on October 21, 2015

Abstract

Motivation: The increasing diversity of data available to the biomedical scientist holds promise for
better understanding of diseases and discovery of new treatments for patients. In order to provide a
complete picture of a biomedical question, data from many different origins needs to be combined
into a unified representation. During this data integration process, inevitable errors and ambiguities
present in the initial sources compromise the quality of the resulting data warehouse, and greatly di-
minish the scientific value of the content. Expensive and time-consuming manual curation is then
required to improve the quality of the information. However, it becomes increasingly difficult to dedi-
cate and optimize the resources for data integration projects as available repositories are growing
both in size and in number everyday.

Results: We present a new generic methodology to identify problematic records, causing what we
describe as ’data hairball’ structures. The approach is graph-based and relies on two metrics trad-
itionally used in social sciences: the graph density and the betvveenness centrality. We evaluate
and discuss these measures and show their relevance for flexible, optimized and automated
data curation and linkage. The methodology focuses on information coherence and correctness to
improve the scientific meaningfulness of data integration endeavors, such as knowledge bases
and large data warehouses.

Contact: samuel.croset@roche.com

Supplementary information: Supplementary data are available at Bioinformatics online.

 

1 Introduction

 

With the current quantity and diversity of data available in the bio-
medical domain (Lipinski et al., 2014; Marx, 2013), it becomes in-
creasingly necessary to combine the information coming from
multiple sources, in a variety of formats, into a unified representation.
This practice goes by the name of data integration or record linkage
(Winkler, 1995). Different reasons motivate the exercise: it can be for
instance the appealing possibility to query and analyze information
about complementary thematics (e.g. gene—disease relationship), con-
solidation of some knowledge existing about one topic, or the absorp-
tion of a source into another for maintenance needs (e.g. dataset
coming from company acquisitions). Recently, data integration ef-
forts have been particularly active in the drug discovery domain, with

platforms such as transMART (Szalma et al., 2010) or Open
PHACTS (Williams et al., 2012a), focused on building a pharmaco-
logical space from public repositories. Other examples include
ChemSpider (Pence and Williams, 2010), a resource providing a cen-
tral hub related to chemical names and structures from various
sources or Identifiers.org (Juty et al., 2012), a cross-reference platform
for biomedical identifiers and data connections.

Fundamentally, data integration can be seen as a problem
of creating the correct links between equivalent, yet disconnected,
database records. This process is sometimes called ‘stitching’, or ‘rec-
onciliation’ (Bollacker et al., 2008; Dong et al., 2014). Once records
are rightfully associated, it becomes possible to query across or
merge them if necessary. As links are created between entries, it is

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 918

9mg ‘09 isnﬁnV uo seleﬁuV socl ‘etulomeg JO KitSJQAtu [1 112 ﬂJO'sleumo[pJOJXO'sopemJogutotq/ﬁdnq IIIOJJ pepeolumoq

Flexible data integration and curation

919

 

intuitive to rely on an abstract graph structure to represent the prob-
lem faced. Vertices or nodes are records or entities of interest; edges
represent the associations between them. An illustration of the graph
abstraction is the Semantic Web: this series of standards relies on the
Resource Description Framework (RDF) and a graph structure to fa-
cilitate the interoperability and integration of independent pieces of
information on the World Wide Web (Berners-Lee et al., 2001). The
Semantic Web also provides means to establish equivalence between
segregated records (sameAs or exactMatch relations), with the use
of rules or reasoners for instance. Moreover, the biomedical domain
is rich in unique identifiers (e.g. chemical structure identifiers like
InChIKeys) and cross-references which can be used to automatically
assess equivalence between database entries: if one record references
another, it might be possible to deduce that the two entities are the
same, for instance.

All these strategies are automatable and reliable in theory, how-
ever complications can arise quickly if cross-references are absent,
errors present in the original sources or if the data is fuzzy and am-
biguous by nature, such as with drug names and chemical structures
for instance (Tiikkainen et al., 2013; Williams et al., 2012b). The ef-
fect of erroneous information is dramatic for data integration: records
can get incorrectly associated with other entries, themselves recur-
sively linked to other records. This cascade of events leads to the cre-
ation of unwanted ‘hairballs’, which we define as groups of records
not equivalents and not supposed to he linked, but yet connected be-
cause of the deficient state of the data. In such a scenario, manual
intervention from expert curators is required in order to improve the
quality and scientific validity of the dataset. Unfortunately, with the
increasing size of biomedical databases and repositories, it becomes
more likely that such errors will arise by chance, and more expensive
and time consuming to correct them by hand.

In this document, we use an in-house data integration project
related to the creation of a drug product terminology in order to il-
lustrate a generic and ﬂexible approach to identify and handle prob-
lematic and erroneous records during the integration process. The
drug terminology is built from millions of database entries present
in eight heterogeneous sources, including four from third-party ven-
dors (Integrity, Cortellis, PharmaProject and AdisInsight), one de-
veloped internally and three public drug databases (DrugBank, part
of ChEMBL and ChEBI). Our motivation is to build and maintain
an integrated database (also called data warehouse) from these vari-
ous sources, each of which contains a part of the desired information
(see Fig. 1). The graph-based method presented allows researchers
to isolate and prioritize problematic entries and flexibly adjust the

A Diazepam Lipitor é
Valium diazepam
diazepam
Ro-5-2807
B Diazepam C
Valium

Ro-5-2807 diazepam

Fig. 1. The data integration challenge. (A) Records coming from multiple
sources, represented by different colours. All these records refer to the same
drug product, in this case diazepam, but contain only partial information
about it, the synonyms. (B) The entries have to be integrated and merged into
consolidated records. (C) In the process, erroneous entries have to be
excluded, to improve the quality and consistency of the resulting data
warehouse

curation work required over an automatic integration to maximize
the quality and scientific usefulness of the data.

1.1 Limitations of previous work

The methodology presented in this manuscript differentiates itself
from previous record linkage approaches by its ﬂexibility to handle
many different sources and exclude erroneous records, as well as
with its capability to deal with redundant identifier types. We brieﬂy
summarize in this section the state of the art and the current
limitations.

Record linkage depends on the presence of one or more common
identifiers in the available datasets in order to assert equality between
database entries (Hernandez and Stolfo, 1998, Winkler, 2014). It can
for instance be the name of a person, or in our case one of the various
synonyms used to describe drug products (e.g. regulatory name, brand
name). Previously published techniques can then be divided in two cat-
egories: deterministic and probabilistic approaches (R005 and Wajda,
1991). Deterministic methodologies rely on a series of custom rules to
create the equivalence between the records, using techniques close to
text-mining. Records are merged based on exact or similarity-based
matching of the common identifiers. Such an approach is straightfor-
ward and can deal with data coming from many sources, yet it is very
vulnerable to erroneous information (Wajda et al., 1991).
Deterministic approaches do not provide means to detect and remove
mistakes in the original data and are therefore not suitable for our
needs: the presence of an incorrect drug product synonym would for in-
stance result in the mixing of two or more drug products in the same
category, situation to be avoided at all costs. Probabilistic-based
approaches try to link pairs of records by assigning weights to a range
of identifiers and by determining a likelihood for a match to be correct
(Fellegi and Sunter, 1969). These techniques have the clear limitation
of handling only pairs of records, whereas we needed n data sources to
be considered at once for the integration process. Moreover, these
methodologies usually assumed that identifiers are independents
(Wilson, 2011), which is not the case with drug names: a brand name
appears for instance late in the life cycle of a product, and is always
preceded by a regulatory name. Finally, none of the previous
approaches are designed to exclude erroneous records, they limit them-
selves to determine whether there is a match or non-match between re-
cords. This feature is critical for our use-case and for data quality.

To conclude, the approach we introduce is graph-based, i.e. re-
cords are abstracted as vertices, further linked by edges, and is there-
fore compatible with any graph representation such as RDF or
graph database for implementation. The identification of erroneous
records relies on two indicators from graph theory and traditionally
used in social science analysis: the graph density and the between-
ness centrality. We demonstrate and evaluate their relevance in
the data integration and cleaning tasks. The work presented finds its
application for the automatic combination of large amounts of ex-
pensive data coming from different sources, where curation needs to
be prioritized and optimized in order to generate high quality scien-
tific content. Future use cases also include the management of data-
sets resulting from mergers and acquisitions, in order to integrate
the new valuable information with existing internal databases and
reduce the amount of maintenance work necessary.

2 Methodology

A total of eight different sources were used to build the data ware-
house (i.e. the drug product terminology). The methodology for the
integration of the data is decomposed in three steps, respectively

9mg ‘09 isnﬁnV uo seleﬁuV socl ‘etulomeg JO KitSJeAtu [1 112 [glO'SIBILInO[p.IOJXO'SODBIIIJOJIIIOIQ/ﬂdllq won pepeolumoq

920

S.Croset et al.

 

called Link, Evaluate and Clean. Depending on the desired outcome,
it is also possible to add an extra final step called Merge in which re-
cords are merged together. Briefly, in the Link step the graph struc-
tures are formed by flexibly creating edges between the initial
entries from the sources. The presence of particular label types was
used to connect the records; for instance, if two records have a
brand name in common, an edge between the two entries was cre-
ated. This step results in the creation of many graphs, reﬂecting the
associations between records. In the Evaluate step, metrics are com-
puted over the graphs (i.e. betweenness centrality and density).
From these measures, it is possible to isolate the problematic graphs
and entries, which can be cleaned manually or automatically given a
certain threshold (Clean step). Each of these steps is described in de-
tail in this section.

2.1 Data sources and implementation

The creation of a comprehensive terminology about drug products
requires the integration of multiple starting data sources, each pro-
viding a subset of the wanted information (see Fig. 1). A total of
373 823 drug records are to be integrated, related to 1 881 760
labels representing the possible synonyms a drug can have: labora-
tory code, generic name, brand name, identifier, cross-reference, etc.
(see Supplementary Fig. 1 for full scope). Three of these sources are
public databases: DrugBank (11 584 drug records), ChEBI (53 185
drug records) and ChEMBL (84 901 drug records, entries with syno-
nyms only). Alongside these, we also used four private feeds from
various vendors: Thomson Reuters Integrity (87561 drug records,
including only entries with a generic or brand name), Thomson
Reuters Cortellis (5 7410 drug records), PharmaProjects (64 206
drug records) and AdisInsight (32 633 drug records). The last data
source is an internal repository containing the drugs developed
within the company alongside their synonyms and laboratory codes
(5 312 drug entries). This starting set of sources illustrates one of the
data integration challenges for pharmaceutical companies: informa-
tion comes with different level of privacy and confidentiality and
often requires a custom in-house solution. All sources were updated
the 17th of September 2015 in different formats (XML, RDF, SQL)
and loaded in a Java web application built using the Play! frame-
work 1.2.7 using MongoDB as back-end store.

For our use-case, each drug entry is abstracted as a vertex (also
called node) and has a list of one or more labels related to it. These
labels will in turn be used to create the links between the records. As
a result, a graph structure is created, over which it is possible to de-
rive various measures.

2.2 Graph measures

Connected data can be abstracted as graph structures, and a data in-
tegration problem can be seen as correctly creating undirected links
or edges between pairs of records or vertices. The links are created
following a certain condition, for instance given the presence of a
cross-reference, or using labels shared by records. One of the main
benefits of the graph abstraction is the possibility to re-use the math-
ematical tools and descriptors of graph theory. We wanted to iden-
tify problematic records coming from S complementary yet different
sources. To perform this task, we used the graph density as a theor-
etical measure of deviation against the perfect case (i.e. a complete
graph, discussed later). Once problematic graphs, or ‘hairballs’, are
identified, it is possible to isolate and clean ambiguous records using
the betweenness centrality, or measure of the relative influence of a
vertex within the graph based on its connectivity. This section

introduces these various graph descriptors and serves as a theoretical
reference.

2.2.1 Complete graphs and data sources

A complete graph is an undirected graph in which every pair of dis-
tinct vertices are connected by an edge (Wikipedia, 2014a). Given a
number V of vertices, the associated completed graph is denoted KN.
Its number Ek of edges can be calculated as follows:

:Vx(V—1)

E
k 2

(1)

Complete graphs are interesting for data integration purposes;
they represent a perfect agreement between records coming from dif-
ferent sources. For example, consider the integration of three data-
bases, each containing some complementary and partially
overlapping information, such as three resources describing drugs;
one expects the entries present in one of these databases to be also
equivalent to some of the records in the other resources. For in-
stance, the entry related to the diazepam in ChEBI (CHEBI:495 75 )
is equivalent to the entry describing the diazepam in DrugBank
(DB00829) and so forth, based on the synonyms they have in com-
mon. If links were drawn between equivalent records, one would
therefore expect to find triangle-shaped graphs where the equivalent
records from each three databases were connected to each other.
The presence of the triangle-shaped graphs gives confidence in the
integration: one node from each resource is reciprocally linked to a
corresponding node from each other resource, forming a densely
connected network, representing an ideal data integration situation.
In the case of three databases being integrated, the triangle shape
corresponds to the complete graph K3 of three vertices (see Fig. 2A).

From this specific example, it is possible to generalize the ap-
proach considering the data integration of S sources; for such cases
it is expected to find complete graphs KS, where one record from
each of the S original sources is linked to every other equivalent re-
cord from the other sources. In summary, complete graphs represent
the perfect cases for data integration purposes, where records repre-
sented as vertices are tightly connected together by the edges repre-
senting an equivalence relationship, with a size depending on the
number of starting sources S.

2.2.2 Density

In practice, linked records often do not form complete graph struc-
tures, but have a geometry drifting away from it, due to the only
partial overlap of information (e.g. non-existing cross-references or
shared labels). It is possible to quantify this deviation using a metric
called density. The density of a graph is a number ranging from 0 to
1 and reflecting how connected a graph is in regards to its maximum
potential connectivity. Given an undirected graph with E edges and
V vertices, the density d is defined as:

2><E

d=V><(V—1)

(2)

The density can also be formulated as the ratio between actual
connections, i.e. the number of existing edges in the graph (E), and
the number of maximum potential connections between vertices,

called PC:

E

dzﬁ

(3)

PC is the same as the number of edges Ek in the complete graph,
as defined in Equation 1 and leading to the equality:

9mg ‘09 1sn8nV uo seleﬁuV socl ‘121u10111123 10 A1tSJeAtuf1 112 /810'S{12umo[p101x0'831112u1101utotq/ﬁd11q 111011 pepeolumoq

Flexible data integration and curation

921

 

.\ 0-.
'2' 95;

 

K3 = 3 vertices, 3 edges K4 = 4 vertices, 6 edges K7 = 7 vertices, 21 edges

 

Fig. 2. Graph types and measures for data integration. (A) Examples of com-
plete graph for various number of vertices. Vertices are equivalent records

from different databases, linked on the basis of having synonyms in com-
mon. (B) Illustration of the graph density behavior. (C) Betweenness centrality
computed for each vertex of an example graph. Vertices with low BC are in
green, and high BC in red. The vertex in red connects communities of records
that should be separated, and can then be excluded to improve the resolution
of the information

E E 2><E

=p—c=E—k=m (4)

This property is of primary interest for data integration, as the
density directly reflects how far away a graph is from the ideal case
of a complete graph, as outlined in the previous section. Intuitively,
the graphs with the lowest densities are the ones diverging the most
from an ideal scenario (i.e. perfect graph) and are therefore more
suspicious. The evolution of the density for different graph struc-
tures is illustrated in Figure 2B. This indicator is used in our context
to prioritize and rank the graphs from the Link step, in order to
identify which entries are not densely linked and therefore problem-
atic (i.e. the hairballs).

2.2.3 Betweenness centrality

The graph density helps to identify groups of problematic records,
diverging from the perfect case; however, it does not provide any
help to further clean problematic vertices. The betweenness central-
ity (BC) fulfils this task. The BC is an indicator of a vertex’s central-
ity in a graph. Given all possible shortest paths between every pair
of vertices in a graph, the betweenness centrality of a vertex, written
BC(v), is the fraction of the number of times a given vertex is on the
shortest path:

BC(v) = Z —“(S’t'”) (5)

s,t€V 0(5’ t)

Where V is the set of vertices in a graph, o(s,t) is the number of
shortest (s,t) paths and o(s,t|v) is the number of those paths passing
through the vertex v other than (s,t). The measure ranges from 0 to
1, with high values reﬂecting a relatively high centrality of a vertex
in the graph, in regards to other vertices. For data integration pur-
poses, BC helps to identify the vertices with a prominent inﬂuence
i.e. the records connecting communities or groups of records within
a graph (Fig. 2C). As this measure can be potentially calculated for a
very large number of vertices, likely millions, it is important to con-
sider its computational complexity. In this regard, the computation
of the BC can take 0(EV) time for undirected graphs, where E is the
number of edges and V the number of vertices (Brandes, 2001). This
time is translatable as 0(V3) from Equation 1, assuming the worst
case where all the graphs are completes. This value is below polyno-
mial time and therefore considered as a tractable problem, which
makes it in principle computationally affordable for large datasets.
The BC used in combination with the graph density helps to first
identify the suspicious groups and secondly to retrieve the problem-
atic records in these hairballs. For both measures, a threshold value
can be set in order to optimize the number of data points to be in-
spected or cleaned.

2.3 Integration process

The integration process is based on a series of steps, summarized in
Figure 3. First the graph structures are created, then the graph de-
scriptors introduced previously are computed. Then follows an itera-
tive cleaning, based on the graph metrics and resources available;
the cleaning can be automatic or manual. Each step is presented in
the coming sections; the actual implementation is flexible and can
vary depending on the task addressed.

2.3.1 Link

In this step, individual records or vertices are connected by edges
representing an equivalence relation, i.e. a record from one source is
supposedly equivalent to a record from another source. The reason
behind the creation of the edge is case-specific and flexible; it can be
based on a shared identifier between vertices, for instance a cross-
reference, or on a series of special rules (also referred to as determin-
istic approach in the literature).

Our starting dataset did not include many cross-references; we
therefore had to rely on a series of custom rules to connect the
equivalent records. Our aim is to build a terminology about experi-
mental and approved drug products; examples of use-cases involve
the retrieval of all existing synonyms given a specific product or in-
ternal code. The terminology should be also suitable to search and
monitor the scientific literature, among other things.

We decided to connect records with an edge if they have at least
a chemical name, a generic name, a brand name, a laboratory code
or a chemical structure in common. The presence of only one identi-
cal label was enough to create a link between vertices. Fuzzy match-
ing and other string similarity-based equivalences over the label
values were purposefully avoided: We are primarily interested in
gathering real and unambiguous synonyms, malformed labels
should be excluded by the methodology. For instance, a record con-
taining the erroneous label ‘Valuim’ should be excluded and not
merged with the records containing the correct spelling ‘Valium’.
We considered this series of rules to be specific enough for our

9mg ‘09 1sn8nV uo seleﬁuV socl ‘121u10111123 10 A1tSJeAtuf1 112 /810'S{12umo[p101x0'831112u1101utotq/ﬁd11q 111011 pepeolumoq

922

S.Croset et al.

 

 

A Prepare

373 823 records

 

 

 

  
    

B Link

4'\.

182 389 graphs

C Evaluate

.5»

 

E Merge

.5}.
x}.

252 488 records

 

 

 

D Clean

.2?»

9 651 excluded

  

 

 

Fig. 3. Data integration process. (A) The records from the source databases are downloaded locally. (B) The graph structure is formed on top of the source re-
cords, and equivalence edges created. (C) The graph metrics are computed (density and betweenness centrality). (D) Hairballs are identified (graphs with low
density) and the problematic records are excluded from the build (vertices with high betweenness centrality). (E) Finally, the entries present in the same graph
are merged, in order to create consolidated records containing the originally segregated information (optional step)

purpose, yet it could be easily adapted for different use-cases or
datasets.

Links between records are undirected in order to capture as sim-
ply as possible the fact that a pair of records are semantically
identical. This relation could also be represented by a bi-directed
edge, or using two directed edges starting from each vertex of the
pair. However, such a representation would add unnecessary com-
plexity to the problem as the edge directionality is not used during
the computation of the graph density and BC. We outlook at the
end of this document the potential use and impact of other graph
metrics and representation, such as weighted edges.

2.3.2 Evaluate

The graphs created in the Link step vary widely in size and shape
(see Section 3). Because of the fuzziness and ambiguity of the ori-
ginal data, some records aggregate in ‘hairball-like’ structures,
which must be cleaned and disambiguated. The betweenness central-
ity and density are both helpful metrics in this regard, and they are
computed in this step. We used the GraphStream library (Dutot et
al., 2007) to calculate the betweenness centrality (Equation (5)), the
computation of the density was implemented directly in the pro-
gram. We also created a series of methods to export and visualize
the metrics, as illustrated in the Section 3. This step is generic and
can be applied on the top of any type of graph structure.

2.3.3 Clean

Problematic and ambiguous graphs can be identified from the met-
rics computed in the Evaluate step. Depending on the time, available
resources and dataset size, a threshold can be applied to ﬂexibly iso-
late and classify the graphs to be further inspected. The density and
the betweenness centrality are however not ﬂawless measures: for
instance, sometimes graphs with low density can be correct (false
positives) or erroneous nodes can have a low betweenness centrality
(false negatives). In order to find the optimal threshold values yield-
ing the best results, we performed a receiver operating characteristic
(ROC) analysis. For this purpose, both graph measures were con-
sidered as independent binary classifiers, and we manually inspected
a series of cases to generate the curves. The optimal threshold values
were determined using Matthews correlation coefficient (MCC). In
order to find the optimal density value separating the problematic
graphs from the correct ones, experts looked at 200 randomly se-
lected graphs created in the Link step (see Supplementary Fig. 2).
Based on this training set, the density threshold of 0.59 gave the best

results at separating graphs to be inspected versus correct ones
(MCC= 0.651). The optimal betweenness centrality threshold was
derived from inspecting 100 random vertices coming from true
problematic graphs. The betweenness centrality threshold of 0.33
gave the best results and was used to separate vertices to be excluded
from vertices to be kept (MCC= 0.837, see Supplementary Fig. 3).
This approach was used as we wanted most of the process to be
automated; note that it is also possible to define the thresholds based
on the number of cases to inspect manually or any other external
parameters. The Clean and Evaluate steps can be repeated at will
until a satisfactory state of the data is reached. We repeated them
four times before merging the records (see Supplementary Fig. 4 for
effect of cleaning steps on the dataset).

2.3.4 Merge

The final step is optional and relevant only when the records need to
be merged or consolidated. The graph structure is no longer required
for such scenarios, as only merged individual entries are necessary in
the final data warehouse. As our work required the output to be de-
livered in this fashion, we merged the cleaned graphs and simplified
the duplicate and redundant labels.

2.3.5 Warehouse content evaluation and availability

The suitability of the data warehouse content for scientific tasks was
evaluated against a list of largest selling pharmaceutical products
Q4 2013, obtained from Wikipedia the 10th of December 2014
(Wikipedia, 2014b). Given a brand name on the list, the task was to
check if an entry was present in the data warehouse, and if this entry
was correct or not: only one drug product should be described in the
entry, and all the information about the product should be located on
this entry and not spread across multiple records. We chose this list of
best selling products, as corresponding entries in the warehouse are more
likely to contain ambiguities and errors due to an often larger synonym
set for the given products. These drug products are also more likely to be
searched and referred to by the users of the warehouse, given their im-
portance on the market, and are therefore solid evaluation points. The
Wikipedia list contains a few errors and imprecisions on its own, high-
lighted on the Supplementary Tables 1 and 2 alongside the results. More
evaluation work was performed internally, in particular a successful
demonstration of superiority against existing solution (not presented).
A subset of the final database containing the public sources is openly
available at https://github.com/loopasam/ﬂexible-data-integration. The

9mg ‘09 1sn8nV uo seleﬁuV socl ‘121u10111123 10 A1tSJeAtuf1 112 /810'S{12umo[p101x0'831112u1101utotq/ﬁd11q 111011 pepeolumoq

Flexible data integration and curation

923

 

repository also contains the excluded records by the methodology, and a
discussion based on some illustrative cases.

3 Results

The goal of the methodology is to integrate large amounts of heteroge-
neous data coming from different sources while detecting and handling
errors to optimize the required curation. Equivalent records from differ-
ent databases can indeed be linked based on the common labels they
share (e.g. a brand name), as presented in the methodology section (Link
step); the approach is very straightforward and assumes that entities
with the same name are identical. However this strategy fails when a
label is misassigned in an original record or when a name is ambiguously
used to describe different entities. As a result, links between records are
incorrectly created and propagated which creates hairballs, or large sets
of records erroneously grouped together. For such cases, the source
entries containing the misleading information, at the origin of the hair-
ball, must be excluded. We present in this section the results and evalu-
ation of the methodology, and characterize with examples the hairballs
and their resolution towards better quality scientific content. We started
with eight different sources describing 373 823 drug records and contain-
ing a total of 1 814 281 unique label entries. After the Link step,
182 389 graphs were created. The final database of integrated records
contains 252 488 entries (i.e. the drug products) and 1 814 281 labels. In
the process, 9 651 problematic records were identiﬁed and excluded
from the build (2.58% of total starting number of records).

3.1 Hairballs

The methodology presented emphasizes the identification of problematic
records, as they are the ones subject to manual curation, an expensive
and tedious process. Data hairballs are an expected phenomenon using
biomedical data (Williams et al., 2012b), and vary in sizes and shapes.
They are intuitively identifiable as graphs of low densities, when

1'

 

Fig. 4. Illustration of data 'hairball’. The graph contains numerous records
(n= 503) and has a low density (d= 0.027). The hairball contains many different
unrelated entries, which would be erroneously merged together if ambiguous
records were not identified and excluded. Data hairballs compromise the value
and scientific quality ofthe data warehouse

compared against the ideal case of a complete graph (see Section 2). In
our work, we determined using an optimal density threshold of 0.59 to
identify erroneous graphs using a ROC analysis (sensitivity 2 0.828, spe-
ciﬁcity = 0.846). In line with this observation, the lower the density, the
more likely the graph is to be problematic: We did not find for instance
any non-problematic graph with a density below 0.33 (sensitiv-
ity: 0.234, specificity: 1.0). Figure 4 presents one of these hairballs,
containing no less than 503 records, themselves describing dozens of dif-
ferent drug products. If kept in the integrated data, hairballs create fur-
ther ambiguities and greatly diminish the value of the data integration
exercise. For instance, it would not be possible to separately identify
the different drug products contained in the hairball, with potentially im-
portant consequences if the integrated data were used for pharmacovigi—
lance purposes. It should be noted that the older a drug is, the more
likely it is to be part of a hairball, as more synonyms are used to
describe it.

Fortunately, hairballs are not the most prevalent structures
found in the integrated dataset. Figure 5 presents the distribution of
size and density for the graphs formed during the equivalence link-
ing; the large majority of graphs are complete or of high density
(>0.95) and of small size, as expected (i.e. below the number of
starting sources). The same figure also reveals the series of graphs
with lower densities, namely the hairballs to be corrected. Given the
large number of original records and labels, it would have been very
challenging to manually identify these entries.

3.2 Error resolution

In order to increase the quality of the integrated data, hairballs have
to be identified and the records causing them removed. We used the
graph density to detect hairballs and the betweenness centrality to
identify and exclude hairball-causing records (see Section 2). Figure 6
illustrates this process for the atorvastatin molecule (also commonly
referred to/sold as Lipitor). Following a succession of cleaning steps,
the records related to the molecule of interest are progressively iso-
lated from the other drugs, until they form a stable graph with a high

 

 

1.0 —A -
B
0.8 — — Counts
97511
a 0.6 - — 14373
2
8 2119
46
0.2 — _ 7
c 1
0.0 — _

 

 

 

| | | | | |
0 100 200 300 400 500

Size

Fig. 5. Density and size of the 182389 graphs formed in the Link step, before
cleaning. The red line represents the density threshold under which graphs
are inspected for cleaning (d=0.59). (A) Optimal data integration, graphs
have a very high density and an expected size, based on the number of start-
ing sources. (B) Dense graphs, with a large size, appearing due to a large
number of similar entries in the source databases, for instance related to
'gene therapy’. (C) Graphs of low density and large size, the hairballs, as
shown on Figure 4

9mg ‘09 1sn8nV uo sejeﬁuV socl ‘121u10111123 10 A1tSJeAtuf1 112 /810'sp2umo[p101x0'831112u1101u101q/ﬁd11q 111011 pepeolumoq

924

S.Croset et al.

 

 

 

(D)

 

 

 

Fig. 6. Cleaning and resolution of a data hairball for the atorvastatin (Lipitor)
entry. (A) Initial hairball of erroneously linked records, describing 12 different
drug products. (B) Exclusion of the records with high BC after the first clean-
ing step, the Lipitor-related entries are still mixed with other products (blue
graph, low density). Some graphs are now dense and do not need further
cleaning, for instance (B1) refers to gemfibrozil and (BZ) to adenosine. (C) The
hairball is subject to a second iteration step, where more records are
excluded (arrows on the graph). (D) The entries related to the atorvastatin are
eventually well isolated (blue graph) and ready to be merged as a high quality
consolidated record. The other drug product (D1) is fenofibrate. See
Supplementary Material for bigger version of this figure

density. Records causing the hairballs are excluded during the pro-
cess: it therefore results in a loss of information, but an improvement
of the resolution and data quality. Drug product categories are well
separated and isolated from one another and it becomes possible to
uniquely and unambiguously refer to the atorvastatin molecule after
the cleaning steps. Furthermore, given that the Lipitor is one of the
best selling drugs of all time (Kidd, 2006), we expect that it will be
searched and referred to by many users, stressing the importance of
providing a quality entry for it in the integrated dataset.

3.3 Evaluation

The validity of the graph-based methodology presented in this docu-
ment was evaluated with a ROC analysis, performed to determine
the optimal density and betweenness centrality thresholds (see
Section 2). For the optimal density threshold (d = 0.59), the method
obtained a sensitivity of 0.828 and specificity of 0.846. These high
values provide confidence in the approach and in the suitability of
the density as a measure to identify problematic groups of linked re-
cords. We obtained a specificity of 0.951 and a sensitivity of 0.879
for the optimal betweenness centrality threshold (BC 2 0.33).
These numbers reﬂect here as well the capability of the betweenness
centrality to be a useful metric to separate correct records from
problematic ones.

We decided to further evaluate the quality of the final resource
after the integration and the multiple cleaning steps by looking at
the list of the hundred best-selling drugs in 2013. Given a drug
brand name as found in the list, we looked at whether the drug
was present in the integrated data or not, and whether the entry was
correct (see Section 2 for details). We were able to find entries for all

of these drugs, with an overall correctness of 94%. These numbers
show the suitability of the approach for data integration and quality
control, in order to produce a resource suitable to address real-
world problems.

3.3.1 Error analysis

As a primary motivation for this work, we wanted to integrate data
in order to generate a resource describing drug products and their
various synonyms in an unambiguous way: one entry should eventu-
ally correspond to one drug product only. However, drugs are com-
plex entities to represent and define; multiple abstractions or views
can be used to categorize them, either based on the chemical struc-
ture or on the clinical usage for instance (Batchelor et al., 2014). We
adopted a flexible representation, reﬂected in the Link step: original
entries are asserted as equivalent if they share either a chemical
name or structure (e.g. InChI), but also a brand name or a labora-
tory code among other properties. As a result the final categories
can contain multiple chemical entities, representing for instance the
active ingredient and the corresponding salts, prodrug forms or iso-
topic variants. It should be noted that we did not consider such cases
as problematic during the evaluation, they were desired features.
Graphs of integrated records can also have a low density and still be
correct; this is often the case with drugs sold in a variety of different
forms and formulations, such as glucagon or insulin for instance.
For such cases, the important number of entries in source databases
referring to the drug creates an unexpectedly large graph with a low
density value, appearing as a false positive to be cleaned.

A common type of excluded records is the ones referring to pro-
jects or compound series, themselves describing multiple molecules.
Such records are prone to generate hairballs, by connecting
the entries describing the single molecules only. For such cases, the
assigned labels were not necessarily wrong, but ambiguous and un-
desirable for the wanted representation. We also identified errors in
the source records, for instance the same laboratory codes being in-
correctly used to refer to different molecules. Whenever possible,
we feed this information back to the original provider for inspection
and correction; then the improved version of the record can be re-
considered in a future release of the resource. Another source of
error comes from some drug combinations and mixtures. Depending
on the representation in the original record, combinations can be
linked to entries referring to an active ingredient alone. For instance
the integrated entry about the paracetamol molecule erroneously
contains an original record related to the drug combination para-
cetamol/methionine. This record was linked to the other entries be-
cause both the chemical structure of methionine and paracetamol
molecules appear independently in the source entry describing the
drug combination. In conclusion, some errors still remain unre-
vealed by the methodology (false negatives), but based on the evalu-
ation, the overall results confirmed the suitability of the approach to
prioritize and automatically resolve ambiguous entries, in particular
in the absence of cross-references or when unstructured text has
to be linked to connect records. Moreover, the method helps to
evaluate and control the quality of the resulting data warehouse.

4 Discussion

We present in this manuscript a generic methodology to integrate
and merge data coming from different repositories into a central
warehouse containing the consolidated information. As opposite to
previous approaches (see Section 1), the method emphasizes error
detection and is qualified as ﬂexible, for multiple reasons: First, the

9mg ‘09 1sn8nV uo sejeﬁuV socl ‘121u10111123 10 A1tSJeAtuf1 112 /810'S{12umo[p101x0'831112u1101u101q/ﬁd11q 111011 pepeolumoq

Flexible data integration and curation

925

 

assertion of equivalence between records can be implemented as a
series of steps including for instance, data cleaning and transform-
ation, in order to best match the desired outcome (e.g. chemical
structures, cross-references, free-text label). Secondly, the approach
can handle n numbers of data sources, and is not limited to handle
only pairs of records. Thirdly, the density measure helps to extract
numbers regarding how confident one can be in regards to the qual-
ity of the integrated information and to determine how much cur-
ation is required on the top of the data. Finally, the betweenness
centrality identifies the erroneous records, and can be used to resolve
issues either automatically or manually. Thresholds for graph meas-
ures can be ﬂexibly adjusted too, in order to optimise the work
based on the time and resources available.

The abstraction of the data integration problem as a graph repre-
sentation has been already introduced by previous work (Berners-
Lee et al., 2001; Bollacker et al., 2008; Singhal, 2012), yet the use of
graph measures on the top of the data to assess the quality of the in-
tegration and to detect anomalies is novel as far as our knowledge
goes. From a usability perspective, we believe that the quality of the
integrated data is more important than the technology or standards
used; in this regard, the methodology can be implemented in a var-
iety of frameworks, from RDF graphs to traditional relational data-
bases, as needed by the user. Other graph indicators could be used
in the future for similar tasks and to quantify different problem
types coming from data integration: As an example, we decided not
to assign weights to the edges of the graphs. This choice was moti-
vated by the sparsity of the data; we estimated that records sharing
one label were just as likely to be equivalent to records sharing mul-
tiple labels. Considering weights could result in the computation of
different graph metrics, useful to characterize a particular type of re-
cords, assuming a rationale could be defined for the alternative indi-
cators the same way it was done here with the density and BC.

A particularly challenging task resulting from data integration
initiatives is the maintenance of the created resource, the update of
records and the incorporation of new information. This can be im-
plemented alongside the methodology proposed, in different ways:
First it is possible to create incremental versions of the warehouse.
With this approach, the new data sources are downloaded following
a certain time period, and the integrated dataset is rebuilt from
scratch each time, following the same graph-based methodology.
New and corrected records are tested again, and excluded if neces-
sary. The main drawback of this option comes from the difficulty of
maintaining unique identifiers, as equivalences can vary from one re-
lease to the other. The second possibility for data update considers
the current data warehouse just as any standard starting source, and
tries to match and disambiguate new records following the same
methodology as presented in this article. Identifiers are easier to
maintain as the new version derives from the old one, but it might
become challenging to keep track of the origin of the data. Removed
information from starting sources, such as synonyms, can also be
trickier to detect and correct.

In conclusion, the goal of data integration is to provide a complete
picture of a scientific problem; we introduced here a methodology
emphasizing data coherence and correctness to fulfill this greater
task, where ambiguities, redundancies and errors are identified and
removed. In summary, the methodology presented addresses practical
concerns faced by large scale data integration exercises, prevalent
nowadays in the drug discovery domain. In an ideal world, database
entries and cross-references would be perfectly maintained and errors
non-existent, which would enable the straightforward creation of a
semantic network between entities. In practice, a costly and tedious
curation step is often necessary in order to control the quality of the

resource and the scientific value of its content. The presented method-
ology focuses on this aspect, in order to best prepare the integrated
data for the derivation of scientific knowledge.

Funding
F. Hoffmann-La Roche AG.

Conﬂict of Interest: none declared.

References

Batchelor,C. et al. (2014) Scientiﬁc lenses to support multiple views over
linked chemistry data. In: The Semantic Web—IS WC 2014. Springer.
pp. 98—113.

Berners-Lee,T. et al. (2001) The semantic web. Scientiﬁc American, 284, 28—37.

Bollacker,K. et al. (2008) Freebase: a collaboratively created graph database
for structuring human knowledge. In: Proceedings of the 2008 ACM
SI GMOD international conference on Management of data. ACM,
pp. 1247—1250.

Brandes,U. (2001) A faster algorithm for betweenness centrality*. journal of
Mathematical Sociology, 25, 163—177.

Dong,X. et al. (2014) Knowledge vault: A web-scale approach to probabilistic
knowledge fusion. In Proceedings of the 20th ACM SI GKDD international
conference on Knowledge discovery and data mining. ACM, pp. 601—610.

Dutot,A. et al. (2007) GraphStream: A tool for bridging the gap between com-
plex systems and dynamic graphs. In Emergent Properties in Natural and
Artiﬁcial Complex Systems. Satellite Conference within the 4th European
Conference on Complex Systems (ECCS’2007).

Fellegi,I.P. et al. (1969) A theory for record linkage. ]. Am. Stat. Assoc., 64,
1183—1210.

Hernandez,M.A. et al. (1998) Real-world data is dirty: Data cleansing and the
merge/purge problem. Data Mining Knowled. Discov., 2, 9—37.

Juty,N.et al. (2012) Identiﬁers. org and miriam registry: community resources
to provide persistent identiﬁcation. Nucleic Acids Res., 40, D5 80—D5 86.

Kide. (2006) Life after statin patent expiries. Nature Reviews Drug
Discovery, 5, 813—814.

Lipinski,C.A. et al. (2014) Parallel worlds of public and commercial bioactive
chemistry data. ]. Med. Chem.

Marx,V. (2013) Biology: The big challenges of big data. Nature, 498, 255—260.

Pence,H.E. and Williams,A. (2010) ChemSpider: an online chemical informa-
tion resource. ]. Chem. Educ., 87, 1123—1124.

Roos,L. and Wajda,A. (1991) Record linkage strategies. part i: Estimating in-
formation and evaluating approaches. Methods Inform. Med., 30, 1 17—123.

Singhal,A. (2012) Introducing the knowledge graph: things, not strings.
Oﬁqcial Google Blog.

Szalma,S. et al. (2010) Effective knowledge management in translational medi-
cine. ]. Trans. Med., 8, 68.

Tiikkainen,P. et al. (2013) Estimating error rates in bioactivity databases. ].
Chem. Inform. Model., 53, 2499—25 05 .

Wajda,A. et al. (1991) Record linkage strategies: Part ii. portable software and
deterministic matching. Methods Inform. Med., 30, 210—214.

Wikipedia (2014a) Complete graph.

Wikipedia (2014b) List of largest selling pharmaceutical products.

Williams,A.]. et al. (2012a) Open phacts: semantic interoperability for drug
discovery. Drug Discov. Today, 17, 1188—1198.

Williams,A.]. et al. (2012b) Towards a gold standard: regarding quality in
public domain chemistry databases and approaches to improving the situ-
ation. Drug Discov. Today, 17, 685—701.

Wilson,D.R. (2011) Beyond probabilistic record linkage: Using neural net-
works and complex features to improve genealogical record linkage. In
Neural Networks (I ]CNN), The 2011 International joint Conference on,
pages 9—14. IEEE.

Winkler,W.E. (1995) Matching and record linkage. Business Survey Methods,
1, 35 5—3 84.

Winkler,W.E. (2014) Matching and record linkage. Wiley Interdisciplinary
Reviews: Computational Statistics, 6, 313—325.

9mg ‘09 1sn8nV uo sejeﬁuV socl ‘121u10111123 10 A1tSJeAtuf1 112 /810'S{12umo[p101x0'831112u1101u101q/ﬁd11q 111011 pepeolumoq

