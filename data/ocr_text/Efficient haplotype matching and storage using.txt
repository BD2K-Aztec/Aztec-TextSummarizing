ORIGINAL PAPER

Vol. 30 no. 9 2014, pages 1266—1272
doi: 1 0. 1093/bioinformatics/btu0 14

 

Genetics and population analysis

Advance Access publication January 9, 2014

Efficient haplotype matching and storage using the positional
Burrows—Wheeler transform (PBWT)

Richard Durbin

Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Cambridge C81 0 18A, UK

Associate Editor: Jeffrey Barrett

 

ABSTRACT

Motivation: Over the last few years, methods based on suffix arrays
using the Burrows—Wheeler Transform have been widely used for DNA
sequence read matching and assembly. These provide very fast
search algorithms, linear in the search pattern size, on a highly com-
pressible representation of the dataset being searched. Meanwhile,
algorithmic development for genotype data has concentrated on stat-
istical methods for phasing and imputation, based on probabilistic
matching to hidden Markov model representations of the reference
data, which while powerful are much less computationally efficient.
Here a theory of haplotype matching using suffix array ideas is
developed, which should scale too much larger datasets than those
currently handled by genotype algorithms.

Results: Given M sequences with N bi—allelic variable sites, an O(NM)
algorithm to derive a representation of the data based on positional
prefix arrays is given, which is termed the positional Burrows—Wheeler
transform (PBWT). On large datasets this compresses with run-length
encoding by more than a factor of a hundred smaller than using gzip
on the raw data. Using this representation a method is given to find all
maximal haplotype matches within the set in O(NM) time rather than
O(NMZ) as expected from naive painNise comparison, and also a fast
algorithm, empirically independent of M given sufficient memory for
indexes, to find maximal matches between a new sequence and the
set. The discussion includes some proposals about how these
approaches could be used for imputation and phasing.

Availability: http://github.com/richarddurbin/pbwt

Contact: richard.durbin@sanger.ac.uk

Received on September 14, 2013; revised on December 9, 2013;
accepted on January 4, 2014

1 INTRODUCTION

Given a large collection of aligned genetic sequences, or haplo-
types, it is often of interest to ﬁnd long matches between
sequences within the collection, or between a new test sequence
and sequences from the collection. For example, sufﬁciently
long identical substrings are candidates to be regions that are
identical by descent (IBD) from a common ancestor. (I will use
the word ‘substring’ to denote contiguous subsequences, as is
standard in the computer science text matching literature.)
When using imputation approaches to infer missing values one
wants to identify sequences that are as close as possible to the test
sequence around the location being imputed, such as those that
are IBD, or at least share long matches with the test sequence.
Maximizing the number of such long matches could also form
the basis of genotype phasing.

Naive substring match testing would take 0(N2M) time for
each test sequence, where there are N variable sites and M
sequences, and hence 0(N2M2) time for complete all-pairs
comparison within a set of sequences. By keeping a running
match score to ﬁnd maximal matches as in BLAST, it is straight-
forwardly possible to reduce this to O(NM) per single test, and so
0(NM2) across the whole collection, but this is still large for
large M. Recently sufﬁx-array-based methods have proved
powerful in standard sequence matching, as exempliﬁed by
Bowtie (Langmead et al., 2009), BWA (Li and Durbin, 2009)
and SOAP2 (Li et al., 2009). Here an approach based on sufﬁx
arrays is described that can ﬁnd best matches within a set of
sequences in O(NM) time, following preprocessing of the dataset
also in O(NM) time, and empirically best single haplotype
matches in O(N) time.

The differences between the algorithms described here and
standard sufﬁx array based sequence matching are derived
from the fact that there are many sequences that are all of the
same length and already aligned. So on the one hand there is no
need to consider offsets of the test sequence with respect to the
sequences in the collection, but on the other hand the test
sequence is long and we are looking for maximal matches of
an arbitrary substring of the test sequence, not of the whole
test sequence.

2 APPROACH

When looking at genetic data from humans or other diploid
organisms, there are two underlying genome sequences per
person, one from their father and one from their mother.
These are known as ‘haplotype’ sequences. Here I consider the
case where we are given these two sequences separately, rather
than unphased diploid ‘genotype’ sequences, where the two
haplotype sequences have been observed together.

Consider a set X of M haplotype sequences xi, 1': 0, . . .,
(M — 1) over N variable sites indexed by k, numbered from 0
to (N — 1). We can take all the sites to be bi—allelic with values 0
or 1, so a typical site xi[k] e {0, 1}. For any sequence 5, let us
write s[k1, k2) to represent the semi-open substring of 5 starting at
k1 and ﬁnishing at (k2 — 1). We will say that there is a ‘match’
between s and t from k1 to k2 if s[k1,k2) = t[k1,k2), and this
match is ‘locally maximal’ if there is no extension that is also a
match, i.e. if k1 = 0 or s[k1 — l] 75 t[k1 — l], and k2 = N or
s[k2] 7E t[k2]. When comparing 5 to the set of sequences X
we say that s has a set-maximal match to x,- from k1 to k2
if the match is locally maximal and there is no longer match
from s to any other xj that includes the interval [k1, k2).

 

© The Author 2014. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which
permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

112 ﬁhO'smumo[pJOJXO'sor1BmJOJurorW/2d11q won popcolumoq

910K ‘09 lsnﬁnV no :2

Positional Burrows-Wheeler transform

 

For some applications we will be interested in the set-maximal
matches within X, i.e. the set-maximal matches of each x,- to
X\{Xi}-

Fundamental to our approach will be to consider a particular
form of ordering on substrings of the set X of sequences. Here is
an explanation for this ordering, with some motivation about
why it is important. We will consider a separate ordering for
each position k between 0 and N. For given k, let us order the
sequences x in X so that their reversed preﬁxes x[0, k) are
ordered, by which I mean that the reversed sequences of the
preﬁxes running back from (k — l) to 0 are ordered in the natural
fashion, with them being ordered according to their index i in X
if the preﬁxes are the same.

Let us consider a set-maximal match between two sequences in
X from k’ to k. If we sort in this reversed order at k, then the
maximally matching sequences must be adjacent, because, if
there were another preﬁx sorting in between them, then it
would have to match both from k’ to k because of sort order,
and it would have to match one of the two at position (k’ — 1),
because the maximally matching sequences must take different
values there and there are only two possible values. The new
sequence would thus form a longer match, which contradicts
the presumption that the match between the original pair was
set-maximal. Strictly, this argument requires that the original
match was set-maximal in both directions. We will see this is
important below. However, this is just a motivating paragraph
so there is no need to consider yet the case where it is set-
maximal in only one direction.

Those with prior exposure to sufﬁx array algorithms will
notice that I talk here about preﬁxes and reversed ordering
rather than sufﬁxes and lexicographic ordering. In this text the
direction of the standard theory is reversed so that algorithms
process forwards naturally through the sequences from 0 to
(N — 1), rather than backwards; this has no substantive effect
on any of the algorithms or results, but will enable us to process
very large datasets in the order in which they naturally come, and
also makes some of the notation more natural.

2.1 Derivation of preﬁx array representation

The argument in the preceding paragraph suggests that having
the sequences sorted in order of reversed preﬁxes at position k
would help ﬁnd maximal matches. It might seem that ﬁnding this
sort order for all the preﬁxes at every position k would be com-
putationally costly, but that is not the case. If we know the sort
order at position k Algorithm 1 shows how to derive the sort
order at position (k+ l) by a simple process looking only at the
k—th value of each sequence. We can therefore calculate the entire
set of orderings for all k in a single pass through all the se-
quences, in time proportional to NM. Let ak[z'_| be the index
m<M of the sequence xm from which the i-th preﬁx in the re-
versed ordering at k is derived. The array ak is a permutation of
the numbers 0, . . ., (M — 1). Because we are often going to want
to discuss the sequences sorted in the order of their preﬁxes,
I deﬁne y? to be the i-th sequence in this sorted ordering
y? = xakm. The key observation is that, conditional on the
value of yﬂk], the order of the elements of ak+1 is the same as
their order in ak. An illustration is given in Figure l.

 

Algorithm 1 BuildPreﬁxArray—build the positional preﬁx array ak+1
from ak

 

u <— 0, v <— 0, create empty arrays a[], b[]
fori=0—>M—1do
if [k] = 0 then
a[u] <— ak[z'_|, u <— u + 1
else
b[v] <— ak[z'_|, v <— v +1
ak+1 <— the concatenation of a followed by b

 

To identify where maximal matches start, we need to keep
track of the start position of matches between neighboring pre-
ﬁxes. Formally, for 1' >0 deﬁne dk[z'_| to be the smallest value j
such that yi[j, k) matches yi_1[j, k) (note that I have dropped the
k sufﬁx of the y’s here and in the following for ease of notation,
since its value is implicitly k for the time being). If
yi[k — l] 75 yi_1[k — 1] then set dk[z'_| = k. It can then be shown
that the start of any maximal match ending at k between any
32,-, yJ-(i< j) is given by maxi<m§j dk[m]. Using this we can efﬁ-
ciently extend Algorithm 1 to update dk in parallel with ak as
we sweep through the data, as shown in Algorithm 2.

 

Algorithm 2 BuildPreﬁxAndDivergenceArrays—build the divergence
array dk+1 along with ak+1 from dk and ak

 

u<—0,v<—0,p<—k+1,q<—k+l
create empty arrays a[], b[], dl], el]
fori=0—>M—1do
if dk[z'_| >p then 1) <— dk[z'_|
if dk[z'_| >q then q <— dk[z'_|
if y,[k] = 0 then
a[u] <— ak[z'_|,d[u] <—p,u <— u+ 1,]; <— 0
else
b[v] <— ak[z'_|,e[v] <— q,v <— v+1,q <— 0
ak+1 <— the concatenation of a followed by b
dk+1 <— the concatenation of d followed by e

 

Because we are dealing with bi—allelic data, so long as dk[z'_| > 0,
the values of yi_1[dk[z'_| — l] and yi[dk[z] — 1] must be 0 and 1,
respectively, since they differ by deﬁnition and are in sorted
order. This means as a corollary that it is not possible for dk[z'_|
to be equal to dk[i + l], as long as they are greater than 0, be-
cause otherwise yi[dk[z'_| — 1] would need to be both 1 and 0,
which is impossible.

I will call the collection of arrays ak for all k the ‘positional
preﬁx arrays’ of X. These are related to standard sufﬁx arrays,
but apart from being preﬁx rather than sufﬁx arrays because the
sorting is in the reverse direction, they differ because they form a
set of N arrays each of size M rather than a single array of size
NM. The dk contain the equivalent of ‘longest common preﬁx’
values in standard sufﬁx array algorithms.

2.2 Finding all matches within X longer than a
minimum length L

Now we can use the ak and dk arrays to efﬁciently ﬁnd matches.
In order to count matches only once, we will sweep through the
sequences with increasing k and only report matches at each k
that end at k, i.e. for which yi[k] 7E yJ-[k]. In order for the match
to be longer than L, we must by deﬁnition have dk[m] 5 k — L
for all i<m 5 j, so pairs of indices (1', j) with this property will
occur in blocks in the sorted list, separated by positions at which

 

1267

112 [3.10811211an[plOJXO'SODBIIIJOJIIIOIQ/[Z(11111 wort popcolumoq

910K ‘09 lsnﬁnV no :2

R. Durbin

 

0d.

yklkl

l10ll100100100000000000100010000011000000100100010101000100110000100110000010000100
00011111000101010000100100011000000110000010100011110001000101999199119999919999199
10011100100101010000100100011000000110000010100010001000000111010000010000010100190
10011100010 0 0 a... to 000
10011100100110001101000100011010011010000001110010000000100101010100110000100000019
10011111010101010000000100010101101001000000000100010001100100000000000111199999919
00100000010101010000000100010101100000100100100100100111199JQQQQQQQQQQQJJJJQQQQQQJQ
00011111000101010000100100011000000001000000000011110001000101999999999111199999919
10011000010001010100100100011000100001000100100010000000000101010000110000019999919
10100011010100010000000100010101100000100100100001100001001100000001000000010199919
10101111010101001100000110101000100001000100100100100000100101010100110000119199919
10011100100110100000000110101000100001000100100011110000000101011000010000101199919
10011000010101010000100100011000000000000010100100100000100101010199919009101190919
100111110001010100001001000110001QQQQQQQQQ1Q1QQlQQ1QQQQn199Inlﬂlﬂlﬂﬂnlnnﬂﬂlﬂllﬂﬂﬂlﬂ
10011111000101011110000000010000000000000100100001100001000100000100010000010119919
10000000010101001010001100101000000110000010000001100001000110100000000110919119919
10011l11100100010011000100011000000000100100100001100001000100010000010000100101919
10011100100100000000000100011010011010000001110010010001000111010000110000010010119
10011100100110001101000100011010011010000001110010000000100101010100110000100110011
00011100100101010000100100011000000110000010100100100111000101011100110000019119911

<

 

 

Reverse sorted preﬁxes at k

0 o 0 H10 0 o o o H H o H H H H H H H H

00000001101

0; 0000011
108 ~001001
100 ~00 001
OH... O.

0 .0100 {‘3>"

I I Z I :99"; Q 0

1.9&¢3«g.:.:o

0100 thh"s".

°  £32 ”¢’§oo
0 54.?» Q' ‘ 
o'o>‘zvépuo
on. 01(Q§"

0000110219
0400 our 01
02209001101

0 2020- 91
10900001001
0100000101

’0

yk+1[k+ 

OOOOOOOHHHOI—‘I—‘OOOI—‘I—‘I—‘I—l

Fig. 1. A set of haplotype sequences sorted in order of reversed preﬁxes at position k, showing the set of values at k isolated from those before and after,
and on the right hand side how the order at position (k + 1) is derived from that at k as in Algorithm 1. Maximal substrings shared with the preceding
sequence ending at k are shown bold underlined; these start at position dk[z] as calculated in Algorithm 2

dk[z]>k — L. We therefore proceed through the sorted list at
position k, keeping track of the last time that dk[z] was greater
than (k— L). Given this, our algorithm looks remarkably like
that for generating the ak arrays.

 

Algorithm 3 ReportLongMatches—report matches within X ending at k
longer than L

 

u <— 0, v <— 0, create empty arrays a[], b[]
fori=0—>M—1 do
if dk[z] >k — L then
if u>0 and v>0 then
for all 0 5 iu<u and 0 5 iv<v do
report match from a[iu] to b[iv] ending at k
u <— 0, v <— 0
if yi[k] = 0 then
a[u] <— ak[z], u <— u + 1
else
b[v] <— ak[z], v <— v + 1

 

This algorithm is 0(max(NM, number of matches)); reporting
the pairs of subsets a[] and b[] of sizes u and v would be O(NM).
Note that ReportLongMatches can be run in the same sweep
through the data as used to calculate the a and d arrays, so if
we are happy to discard previous values of 61,- and  for j<k as
we go, it can be carried out in O(M) space.

A variation of this method can deliver all matches that extend
in both directions from a location by at least a minimum length
L/2. In this case one considers blocks within which
dk(i, j) 5 (k — (2L + 1)) to ﬁnd sets of such matches centered on
position k — (L — 1), and does not separate into subsets for which
yk[z] = 0 or 1. This approach may be relevant when looking for
similar sequences at a position, perhaps for the purpose of imput-
ation. Long matches will recur many times in this formulation, so
it is best if possible to use the similar subsets as they are identiﬁed
during the sweep, rather than to store them for future use.

2.3 Finding all set-maximal matches within X in
linear time

Consider a sequence y,- in the sorted list at k. Under what
conditions will it have a set-maximal match ending at k?
Clearly the match must be to one or more sequences directly
preceding or following it in the sort order. First we ﬁnd
the candidate interval [m,n] such that for all j 75 i with
m<j 5 n, dk[]] 5 min(dk[z], dk[i + 1]). If yJ-[k] 7E yi[k] for all
these j then y,- has a set-maximal match to them all, but (unless
K: N) if any have yJ-[k] = yi[k] then the match between y,- and y]-
can be extended forwards, and there is no set-maximal match
ending at k. Iterating over all k and i we get algorithm 4.

 

Algorithm 4 ReportSetMaximalMatches—report set maximal matches
in X
for k = 0 —> N do
dk[0] <—k+1,dk[M] <—k+1
fori=0—>M—1do
m = i — 1, n = i+ 1
if dk[i] S dk[i + 1] then
while dk[m + 1] 5 dk[z] do
if ym[k] = y,[k] and K 75 N next i
m <— m — 1
if dk[i] Z dk[i + 1] then
while dk[n] 5 dk[i + 1] do
if yn[k] = yi[k] and K 75 N then next i
n <— n + 1
forj=m+1—>i—1do
report match of ak[z] to ak[]] from dk[z] to k
forj=i+1—>n—1do
report match of ak[z] to ak[]] from dk[i + 1] to k

 

1> sentinels at boundaries

1> scan down the array

1> scan up the array

 

Despite the inner loops this algorithm only has time complex-
ity O(NM), because the requirement that yJ-[k] 7E yi[k] limits the
search so that each position is compared at most once from each

 

1268

112 ﬁhO'smumo[pJOJXO'sor112u1101urorq/ﬁd11q 111011 popcolumoq

910K ‘09 lsnﬁnV no 22

Positional Burrows-Wheeler transform

 

direction. To be completely precise, because matches have to
terminate at the start and end of the sequence, this last statement
relies on there not being arbitrarily large groups of sequences
identical from 0 to N. Under these conditions, this also proves
that the total number of set-maximal matches within X is
bounded by a ﬁxed multiple of NM.

As with ReportLongMatches, ReportSetMaximalMatches can
be run in the same sweep through the data as used to calculate
the a and d arrays, so if we are happy to discard previous values
of 61,- and  as we go, it also can be carried out in O(M) space.

2.4 Finding all set-maximal matches from a new
sequence z to X

Next let us consider the case where we have a new sequence 2,
and want to ﬁnd the set-maximal matches between it and set X.
We will again sweep forward through the sequence, and in this
case keep track of the start ek of the longest match of z to some y,-
ending at position k, and the interval [fk,gk) g [0, . . .,M) of
indices in ak with that match. So for all i such that fk 5 i<gk
we have z[ek, k) = yi[ek, k), but z[ek — 1] 7E yi[ek — 1]. We allow
gk to be M if y M_1 is included in the set of longest matches.

We want an efﬁcient procedure for updating e, f and g as we
move from k to (k+ 1). First let us imagine that we have a pro-
cedure for updating f and g to f ’ and g’ given a ﬁxed starting
position ek. If f ’ < g“ then at least some of the original matches
starting at ek and ending at k can be extended to (k+1), so
e; +1 2 ek by deﬁnition and we are done. If on the other hand
f ’ = g’ then none of the matches can be extended, and so
the matches ending at k to sequences between fk and gk are
set-maximal and can be reported. Then we need to ﬁnd a new
ek+1 >ek and corresponding new fk+1 <gk+1.

To efﬁciently update f and g we need the values it and v from
Algorithm 1. We did not store them at the time, but let us
now assume that we did so, in arrays uk and vk, and also kept
track of the total number of zero values at position k in the
sequences as value ck, which is equivalently the length of
array a[] in Algorithm 1. Now if we deﬁne wk(i, 0) = uk[z]
and wk(i, 1) = ck + vk[z] then Algorithm 1 tells us that
ak+1[wk(i, yi[k])] = ak[z]. Furthermore, if ark is the inverse of the
permutation ak, then ak+1[z] = wk(ork[z],x,-[k]). This last state-
ment gives us a clue about how to update f and g. If we deﬁne
f’ = w(f, z[k]) then this will be the index in ak+1 of the ﬁrst se-
quence yj with j 2 f for which yJ-[k] = z[k], which is what want.
Similarly g“ = w(g, z[k]) is what we want for updating g. So we
can now update f and g by simple lookup from stored values.

Now, as we saw above, if f ’ < g’ then we are done. On the
other hand, if f ’ = g“ then there are no extensions of the match
starting at ek. At position k, we know that 2 sorted either just
before the block [f,g) in the natural preﬁx ordering, or just after
it. So it either sorted just before f or just before g. From this we
can infer that yk: 11 <z<yjli+1 in the natural ordering of reversed
preﬁxes. So ek+1 5 dk+1[f’]. Let us consider z[dk+1[f’] — 1]. If this
is 0 then 2 matches yk: 11 better than y?“ 1, and we will set
gk+1 =f’ = g“ and look for fk+1 <f’. If it is 1 then 2 matches
yjlfffl better than yk: 11, and we will set fk+1 = f ’ = g“ and look for
gk+1 >g’. In either case we need to search back from dk+1[f’] to
ﬁnd ek+1. We need to take a little care at the boundaries 0
and M.

 

Algorithm 5 UpdateZmatches—report any set-maximal matches of z to X
ending at k and update to (k+ 1)

f, (— W(fk9  (— W(gk9 

 

iff’<g’ then
e’ <— ek
else
for i = f; i < g; i <— i+ 1 do report match to ak[z] from e to k
3’ <— dk+1[f’] — 1
if z[e’] = 0 andf’>0 then
ﬂea—1

while z[e’ — 1] = yk,+1[e’ — 1] do e’ <— e’ — 1
while dk+1[f’] 5 e’ dof’ <—f’ —1
else
a+f+1
while z[e’ — 1] = yffr1[e’— 1] do e’ <— e’ — 1
while g’<M and dk+1[g’] 5 e’ do g’ <— g’ +1
fk+1 <—f’,gk+1 <—g’,ek+1 <— 8’

 

It is not immediately obvious that the algorithm is O(N). The
while loop in f ’ or g“ is inevitable because it only takes as many
iterations as there are matches to report the next time that
f ’ = g“. The total number of set-maximal matches is bounded
by NA, as required. More complicated are the while loops that
decrement e’. The sum of times these are used is bounded by a
constant multiple of N.

2.5 Compact representation of X

The algorithms described above all use the ak and dk arrays to
ﬁnd matches. However, these are arrays of integers with a total
number of elements equal to the number of binary values in the
original dataset, so storing them would take more space than a
bit representation of the starting data. Some algorithms can be
applied on the ﬂy, in the same sweep through the data as used to
generate the values of a and d, but for other purposes, in par-
ticular for analyzing new sequences, we would like to store the
relevant information more efﬁciently. Here is a description of
how to do that.

First we notice that in the matching processes we do not ac-
tually use directly the ak[z] indexes, but rather the yi[k] = Xakm [k]
values. These are a permutation of the xi[k] values determined by
the ak permutation indicating the sort order at k of preﬁxes up
to position (k— 1), and are therefore a positional analogue of
the Burrows—Wheeler Transform (BWT) of X (Burrows and
Wheeler, 1994; see Li and Durbin (2009) for an explanation
closer to that given here if this is not familiar). Let us call the
set of ordered y sequences the Positional Burrows—Wheeler
Transform (PBWT). As with the BWT, the PBWT is composed
of bit values not integer values, so we can store it in the same
space as the original data. Furthermore we can also expect the y
arrays to be strongly run-length compressible. This is because
population genetic structure means that there is local correlation
in values due to linkage disequilibrium, which means that haplo-
types with similar preﬁxes in the sort order will tend to have the
same allele values at the next position, giving rise to long runs of
identical values in the y array. So the PBWT can easily be stored
in smaller space than the original data. This will be true even if
the original data is run-length encoded, since the left-to-right
orientation of the data in X will not reﬂect shared haplotype
structure due to linkage disequilibrium.

 

112 ﬁhO'smumo[pJOJXO'sor112u1101urorq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV no 22

R. Durbin

 

To ﬁnd matches to a new sequence, as in Algorithm 5, we need
also the stored arrays uk and vk, in order to evaluate the exten-
sion function wk(). These arrays correspond to the information
stored in the index described by Ferragina and Manzini (2000),
commonly known as the F M-index. Given the PBWT we can
store the information needed to generate them efﬁciently in an
exactly analogous fashion to that for normal strings.

We do need values of ak[] for reporting, but given the y values
in the PBWT and the position FM-index, we can do this efﬁ-
ciently by only storing ak for a subset of values of k, for example
every 32 or 64 positions. Reported matches will be longer than
this, so extending to the next stored value of a by use of the
extension function wk() is relatively inexpensive.

Finally, we need a compact representation of the dk arrays.
For now, it is proposed to Huffman encode the differences be-
tween adjacent dk[z], and perhaps only store them at a subset of k
as for ak. There is probably scope for further improvement here.

3 RESULTS

Here I present initial results on simulated data. A dataset of
100 000 haplotype sequences covering a 20Mb section of
genome sequence was simulated using the sequentially
Markovian coalescent simulator MaCS Chen (2009) using essen-
tially the command macs 100000 2e7 —t O .001 —r O .001
(in fact a larger simulation was undertaken, which crashed a little
beyond 20Mb, and the remaining material was trimmed down to
this set). There are 370 264 segregating sites in this dataset. The
raw MaCS output contains essentially the haplotype sequences
written in 0’s and 1’s, and so is approximately 37GB in size. This
compresses with gzip to 1.02GB.

An initial implementation pbwt of the key algorithms was
produced. This uses single byte run length encoding for the
PBWT, with the top bit encoding the value, the next two bits
selecting whether the length is in units of 1, 64 or 2048, and the
remaining 5 bits giving the number of units. For runs >64 but
<2048 this typically requires 2 bytes, and for runs >2048 but
<64k this typically requires 3 bytes. All experiments were carried
out on an Apple Mac Air laptop with a 2.13GHz Intel Core 2
Duo processor using a single core. Encoding the dataset of
100 000 sequences described above took 1070s (user plus
system), generating a PBWT representation that is 7.7MB in
size, over 130 times smaller than the gzip compression of the
raw data. Further results including application to subsets of
the data are given in Table 1, showing that the relative gain
increases with the number of sequences, indicating clearly the
non-linear beneﬁts of the algorithm. This can be clearly seen
by the observation that for each increase of a factor of ten in
the number of sequences, the average number of bytes used by
the PBWT to store the haplotype values at a site only approxi-
mately doubles. As a test on real data, similar measures were
applied to the chromosome 1 data from the 1000 Genomes
Project phase 1 data release 1000 Genomes Project (2012), con-
sisting of 2184 haplotypes at 3 007 196 sites. The gzip ﬁle of this
data took 303MB, whereas the PBWT used 51.1MB, nearly a
factor of six smaller, not far from the factor expected based on
the simulated data.

Next Algorithm 4 was implemented to ﬁnd all set-maximal
matches within the simulated datasets. As expected the time

Table 1. Compression performance of pbwt on datasets of increasing
size

 

 

Number of sequences 1000 10 000 100 000
Sequences .gz size (KB) 10 515 105 559 1 024 614
PBWT size (KB) 1686 3372 7698
Ratio .gz/PBWT 6.2 31.3 133.1
PBWT bytes/site 4.6 9.1 20.8

 

Table 2. Set-maximal match performance of pbwt on datasets of increas-

 

 

ing size

Number of sequences 1000 10 000 100 000
Set-maximal time (s) 12.1 120.3 1213.7
Set-maximal average length (Mb) 0.27 1.48 3.98

 

taken was linear in the number of sequences (Table 2), taking
only 20 min to ﬁnd all maximal shared substrings within 100 000
sequences.

Finally the comparative performance of three different
approaches to matching new sequences to a pre-indexed refer-
ence panel was evaluated, ﬁnding all set-maximal matches of
each new sequence to the reference set. For this evaluation, I
subsampled the simulated sequence dataset to approximate typ-
ical data from a genotype array experiment, by only retaining a
fraction (10%) of sites with allele frequency >5%. This reduced
the number of sites to 5940, approximately one pre 3.4 kb, cor-
responding to 850k in the human genome, comparable to the
content of a standard a genotyping array. Table 3 shows results
for matching 1000 of the sequences from the simulation, compar-
ing them to non-overlapping subsets between 1000 and 50 000 in
size.

First a ‘naive’ algorithm was implemented, in which each
sequence was compared to all sequences in the panel in a
single pass, keeping the best matching segment covering each
base in the test sequence. As expected, this approach takes
time linear in the size of the panel, taking ~0.05 s per sequence
in the panel. Second Algorithm 5 was implemented, which is
termed ‘indexed’ in Table 3. This takes constant user time of 0.9
user seconds to match 1000 sequences to reference panels up to
size 10000, but for 50 000 reference sequences the size of the
stored u, v and d arrays (which were not compressed in this im-
plementation) became larger than the available memory, result-
ing in an increase in system time from 0.2 to 15 s, and a smaller
increase in user time to 1.7 s. I therefore conclude that the
PBWT-based approach can be hundreds of times faster than a
direct search approach and ﬁnd matches in time independent of
the reference panel size, as conjectured above, so long as the
associated index arrays ﬁt in memory.

For situations where the indexes do not ﬁt in memory, we can
still use the PBWT data structures to provide a third ‘batch’
matching process that is still much faster than the naive

 

1 270

112 ﬁhO'smumo[pJOJXO'sor112u1101urorq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV no 22

Positional Burrows-Wheeler transform

 

Table 3. Time to match 1000 new sequences in seconds, split into user (u)
and system (s) contributions for the indexed and batch approaches

 

 

Number of 1000 5000 10 000 50 000
sequences

Naive 52.1 258.9 519.2 2582.6
Indexed 0.9u + 0.1s 0.9u + 0.1s 0.9u + 0.2s 1.7u + 15s
Batch 2.3u+0.1s 3.5u+0.1s 4.8u+0.1s 12.1u+0.1s

 

approach. This uses a modiﬁed version of the within-set
Algorithm 4 that passes jointly through the panel and combined
set of new sequences together, just considering matches between
new and old sequences. As shown in Table 3, the time taken by
this batch approach, whose memory requirements are low and
independent of the number of sites, increases with reference
panel size, but is still many times more efﬁcient than a direct
search as in the naive approach. Asymptotically the time taken
by current implementation will depend linearly M, but it may be
possible to reduce this by careful avoidance of PBWT compres-
sion where it is not needed.

4 DISCUSSION

I have presented here a series of algorithms to generate positional
preﬁx array data structures from haplotype sequences and to
use them for very strong compression of haplotype data, and
time and space efﬁcient haplotype matching. In particular, the
matching algorithms remove a factor of M, the size of the set of
haplotype sequences being matched to, from the search time
taken by direct pair-wise comparison methods. This makes it
possible to ﬁnd all best matches within tens of thousands of
sequences in minutes, and generates the potential for practical
software that scales to millions of sequences. Although the
algorithms are presented for binary data, they can be extended
to multi-allelic data with a little care.

These algorithms share aspects of their design with analogous
algorithms based on sufﬁx arrays for general string matching,
but are structured by position along the string resulting in sub-
stantial differences. One consequence is that, unlike with suf-
ﬁx array methods where linear time sorting algorithms are
non-trivial, building the sorted positional preﬁx arrays in linear
time using Algorithm 1 is straightforward. The approach used
here is reminiscent of that used by Bauer et al. (2011) to generate
a string BWT from very large sets of short strings.

With respect to efﬁcient representation, it is interesting to note
that the original BWT was introduced by Burrows and Wheeler
(1994) for string data compression, not search, and it in fact
forms the basis of the bzip compression algorithm. Valimaki
et al. (2007) have previously explored the use of BWT com-
pressed self-index methods for efﬁcient compression and search
of genetic sequence data from many individuals, but this does
not require a ﬁxed alignment of variable sites as in the work
presented here, and is substantially different.

All the algorithms described here require exact matching with-
out errors or missing data. As for sequence matching, if a more

sensitive search is required that permits errors, it is still possible
to use the exact match algorithms to ﬁnd seed matches, and then
join or extend these by direct testing. This would typically be the
approach taken by production software, but having powerful
methods to identify seeds is key to performance.

An alternative to using sufﬁx/preﬁx array methods in sequence
matching is to build a hash table to identify exact seed matches.
Analogous to the creation of position preﬁx arrays described
here, it would be possible to build a set of positional hash
tables for each position in the haplotype sequences. Hash
based methods when well tuned can be faster than sufﬁx array
based methods, because the basic operations are simpler, but
they typically require greater memory, particularly in cases
where the sufﬁx representation can be compressed as it can be
here. A problem with genotype data not present in standard
sequence matching is that the information content of positions
varies widely, with a preponderance of rare sites with very little
information, which would mean that the length of hash word
would need to change depending on position in the sequence. An
alternative would be to build hashes based on a subset of sites
with allele frequency greater than some value such as 10%, or in
some frequency range, but this would lose information leading to
false seed matches.

Most research into algorithms for analyzing large sets of
haplotype or genotype data has focused on statistical methods
that are powerful for inference, but only scale up to a few thou-
sand sites and sequences; e.g. see Marchini and Howie (2010).
Recently accelerated methods have been developed that can han-
dle data up to tens of thousands of sequences, e. g. Williams et al.
(2012) or Delaneau et al. (2012). However, these methods pro-
vide approximations to the statistical matching approaches, and
are still much heavier than the algorithms presented here. Over a
million people have been genotyped, and although there are
logistical issues in bringing together datasets on that scale, geno-
type data on sets of >100 000 people are becoming available
(Hoffman et al., 2011). One approach to more efﬁcient
phasing and imputation may be to use computationally efﬁcient
approaches such as the positional preﬁx array methods to seed
matches for statistical genotype algorithms, or at other compu-
tational bottlenecks. For example, in their BEAGLE software
Browning and Browning (2007) build a probabilistic hidden
Markov Model from a variable length Markov model of the
local haplotype sequences which is essentially derived from a
dynamic truncation of the positional preﬁx array. Although
their algorithms using this probabilistic model take them in a
different direction, I suggest that the methods described here
could be used to signiﬁcantly speed up the model building
phase of BEAGLE.

Alternatively, a more direct approach may also be possible.
Most phasing and imputation algorithms build a model from the
entire dataset, then thread each sequence in turn against it to
provide a new phasing based effectively on a series of matches.
Instead, the positional preﬁx algorithms progress jointly along all
sequences. If we start at both ends of the data, then at some
position k we have information about matches in both directions
based on the current phasing, and can propose an assignment of
alleles for all sequences at k in a single step, before incrementing
k. Approaches based on this idea may be fast and complemen-
tary to current methods.

 

1271

112 ﬁhO'smumo[pJOJXO'sor112u1101urorq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV no 22

R. Durbin

 

ACKNOWLEDGEMENT

I thank Heng Li and Jared Simpson for the insights they have
given me into the Burrows—Wheeler Transform and sufﬁx-array-
based data structures and algorithms, and Tomislav Ilicic for
development of an earlier implementation of Algorithm 5. This
work developed from thoughts sown during a sabbatical Visit to
NCBI in 2010.

Funding: Wellcome Trust (grant 098051); ORAU.

Conflict of Interest: none declared.

REFERENCES

Bauer,M.J. et al. (2011) Lightweight BWT Construction for Very Large String
Collections. In: Giancarlo,R. and Manzini,G. (eds) Combinatorial Pattern
Matching. Springer, Berlin, pp. 219—231.

Browning,S.R. and Browning,B.L. (2007) Rapid and accurate haplotype phasing
and missing-data inference for whole-genome association studies by use of loca-
lized haplotype clustering. Am. J. Hum. Genet, 81, 1084—1097.

Burrows,M. and Wheeler,D.J. (1994) A block-sorting lossless data compression al-
gorithm. Technical report 124, Digital Equipment Corporation, Palo Alto, CA.

Chen,G.K. et al. (2009) Fast and ﬂexible simulation of DNA sequence data.
Genome Res., 19, 136—42.

Delaneau,O. et al. (2012) A linear complexity phasing method for thousands of
genomes. Nat. Methods, 9, 179—181.

Ferragina,P. and Manzini,G. (2000) Opportunistic data structures with applica-
tions. In: Proceedings of the 41st Symposium on Foundations of Computer
Science (FOCS 2000 ), Redondo Beach, CA (California). IEEE Computer
Society, pp. 390—398.

1000 Genomes Project. (2012) An integrated map of genetic variation from 1,092
human genomes. Nature, 491, 56—65.

Hoffman,T.J. et al. (2011) Design and coverage of high throughput genotyping
arrays optimized for individuals of East Asian, African American, and Latino
race/ethnicity using imputation and a novel hybrid SNP selection algorithm.
Genomics, 98, 422—430.

Langmead,B. et al. (2009) Ultrafast and memory-efﬁcient alignment of short DNA 3
sequences to the human genome. Genome Biol., 10, R25.

Li,H and Durbin,R. (2009) Fast and accurate short read alignment with Burrows—
Wheeler transform. Bioinformatics, 25, 1754—1760.

Li,R. et al. (2009) SOAP2: an improved ultrafast tool for short read alignment.
Bioinformatics, 25, 1966—1967.

Marchini,J. and Howie,B. (2010) Genotype imputation for genome-wide association
studies. Nat. Rev. Genet, 11, 499—511.

Williams,A.L. et al. (2012) Phasing of many thousands of genotyped samples. Am.
J. Hum. Genet, 91, 238—251.

Valimaki,N. et al. (2007) Compressed sufﬁx tree—a basis for genome-scale sequence
analysis. Bioinformatics, 23, 629—630.

 

1 272

112 ﬁhO'smumo[pJOJXO'sot112u1101utotq//2d11q IIIOJJ popcolumoq

910K ‘09 lsnﬁnV no 22

