ORIGINAL PAPER

Vol. 27 no. 9 2011, pages 1269—1276
doi: 10. 1093/bioinformatics/btr1 12

 

Gene expression

Advance Access publication March 3, 2011

Mixtures of common t-factor analyzers for clustering

high-dimensional microarray data
Jangsun Baekl’i‘ and Geoffrey J. McLachlan2

1Department of Statistics, Chonnam National University, Gwangju 500—757, South Korea and 2Department of
Mathematics and Institute for Molecular Bioscience, University of Queensland, Brisbane QLD 4072, Australia

Associate Editor: Trey Ideker

 

ABSTRACT

Motivation: Mixtures of factor analyzers enable model-based
clustering to be undertaken for high-dimensional microarray data,
where the number of observations n is small relative to the
number of genes p. Moreover, when the number of clusters is
not small, for example, where there are several different types
of cancer, there may be the need to reduce further the number
of parameters in the specification of the component-covariance
matrices. A further reduction can be achieved by using mixtures of
factor analyzers with common component-factor loadings (MCFA),
which is a more parsimonious model. However, this approach is
sensitive to both non-normality and outliers, which are commonly
observed in microarray experiments. This sensitivity of the MCFA
approach is due to its being based on a mixture model in which
the multivariate normal family of distributions is assumed for the
component-error and factor distributions.

Results: An extension to mixtures of t-factor analyzers with common
component-factor loadings is considered, whereby the multivariate
t-family is adopted for the component-error and factor distributions.
An EM algorithm is developed for the fitting of mixtures of common
t-factor analyzers. The model can handle data with tails longer than
that of the normal distribution, is robust against outliers and allows
the data to be displayed in low-dimensional plots. It is applied here
to both synthetic data and some microarray gene expression data
for clustering and shows its better performance over several existing
methods.

Availability: The algorithms were implemented in Matlab. The Matlab
code is available at http://blog.naver.com/aggie100.

Contact: jbaek@jnu.ac.kr

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on August 20, 2010; revised on February 10, 2011;
accepted on February 27, 2011

1 INTRODUCTION

Model-based methods have been widely used for both clustering
and classifying high-dimensional microarray data (McLachlan
et al., 2002; Yeung et al., 2001). Thalamuthu et al. (2006)
compared various clustering techniques and showed that model-
based method performed well for microarray gene clustering.

 

*To whom correspondence should be addressed.

The ﬁnite normal mixture model with unrestricted component-
covariance matrices is a highly parameterized model (McLachlan
and Basford, 1988; McLachlan and Feel, 2000a). Banﬁeld and
Raftery (1993) introduced a parameterization of the component—
covariance matrix based on a variant of the standard spectral
decomposition, and its program MCLUST (Fraley and Raftery,
2003) has been often used. But if the number of genes p is large
relative to the sample size n, it may not be possible to use this
decomposition to infer an appropriate model for the component-
covariance matrices. Even if it were possible, the results may not be
reliable due to potential problems with near— singular estimates of the
component-covariance matrices when p is large relative to n. In this
case, mixtures of factor analyzers (MFA) is a useful model to reduce
the number of parameters by allowing factor-analytic representation
of the component-covariance matrices. Hinton et al. (1997) proposed
the MFA adopting a ﬁnite mixture of factor analysis models, which
was considered for the purposes of clustering by McLachlan and
Feel (2000a, 2000b) and McLachlan et al. (2003, 2007). McLachlan
et al. (2002, 2003) applied MFA to tissue samples with microarray
gene expression data for clustering. Martella (2006) used MFA to
classify microarray data successfully. Recently, Xie et al. (2010)
proposed a penalized MFA to allow both selection of effective
genes and clustering of hi gh—dimensional data simultaneously. Zhou
et al. (2009) has proposed another penalized model-based clustering
method with unconstrained covariance matrices.

In practice, for example, where there are several different
types of cancer, there is often the need to reduce further the
number of parameters in the speciﬁcation of the component-
covariance matrices by factor-analytic representations. McNicholas
and Murphy (2008) introduced some parsimonious MFA models,
which include various MFA models with fewer parameters.
Galimberti et al. (2009) proposed another parsimonious factor
mixture model to allow both dimension reduction and variable
selection. Baek and McLachlan (2008) and Baek et al. (2010)
proposed the use of mixtures of factor analyzers with common
component-factor loadings (MCFA) and applied it to a microarray
dataset for clustering. The method considerably reduces further the
number of parameters, and allows the data to be displayed in low-
dimensional plots in a straightforward manner in contrast to MFA.
Several analyses of many real datasets, however, have suggested that
the empirical distribution of gene expression levels is approximately
log—normal or sometimes with a slightly heavier tailed t-distribution
depending on the biological samples under investigation (Li, 2002).
In particular, Giles and Kipling (2003) applied the Shapiro—Wilks
test to Affymetrix microarray expression data and concluded that

 

© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 1269

112 [3.10'8112(1an[plOJXO'SODBIIIJOJIIIOIQ/[Z(11111 IIIOJJ popcolumoq

9IOZ ‘09 lsnﬁnV uo ::

J.Baek and G.J.McLachlan

 

non—normal distributions are common (up to 46% of probe sets).
Lonnstedt and Speed (2002) also noted that outliers occur frequently
in microarray experiments. Therefore, the above approaches are
sensitive to both non-normality of the data and extreme expression
levels as they are based on a mixture model in which the multivariate
normal family of distributions is assumed for the component-error
and factor distributions. McLachlan et al. (2007) extended MFA to
incorporate the multivariate t-distribution for the component-error
and factor distributions. In this article, we propose an extension of
MCFA to incorporate the multivariate t-distribution to handle the
data with tails longer than that of the normal distribution.

In the next section, we review brieﬂy the MCFA approach as
proposed by Baek and McLachlan (2008) and considered further
in Baek et al. (2010). We then describe the mixtures of t-
factor analyzers model with common factor loadings (MCtFA)
and develop its implementation Via the expectation—maximization
(EM) algorithm. In Section 3, its application is demonstrated in the
clustering of two microarray gene expression datasets. The results
so obtained illustrate the improved performance of MCtFA over
MCLUST, MFA and MCFA for these two datasets. A short discussion
is given in Section 4.

2 METHODS
2.1 Mixtures of common t-factor analyzers and its EM
algorithm

Finite mixture models are being increasingly used to model the distributions
of a wide variety of random phenomena and to cluster datasets; see,
for example, McLachlan and Peel (2000a). Let Y=(Y1,..., Yp)T be a
p-dimensional vector of feature variables. For continuous features Yj, the
density of Y can be modelled by a mixture of a sufﬁciently large enough
number g of multivariate normal component distributions,

g
f@; ‘11)=Zm¢(v;u.-,2i), (1)
i=1

where ¢(y; u, 2) denotes the p-Variate normal density function with mean u,
and covariance matrix 2. Here, the vector ‘11 of unknown parameters consists
of the mixing proportions m, the elements of the component means it,- and
the distinct elements of the component-covariance matrices 2,-(i=1, ...,g).
We focus on the use of mixtures of factor analyzers to reduce the number
of parameters in the speciﬁcation of the component-covariance matrices,
as discussed in Hinton et al. (1997), McLachlan and Peel (2000a) and
McLachlan et al. (2003). With the factor-analytic representation of the

component-covariance matrices, we have that

2i=AiAiT+Di (i=1,---,g), (2)

where A,- is a p x q matrix and D,- is a diagonal matrix. To see this, we ﬁrst
note that the MFA approach with the factor-analytic representation (2) on 2,-
is equivalent to assuming that the distribution of the difference YJ- — u,- can
be modelled as

Yj—uizAiUij+e,-j with prob. 7r,- (i=1,...,g)

for j = 1, ...,n, where the (unobservable) factors U ,1, ..., U in are distributed

independently N (0, I q), independent of the eij, which are distributed

independently N (0,D,-), where D,- is a diagonal matrix (i = 1, ...,g).
However, this model may not lead to a sufﬁciently large enough reduction

in the number of parameters, particularly if g is not small. Hence for this

case, Baek and McLachlan (2008) and Baek et al. (2010) proposed the MCFA

approach whereby the distribution of YJ- is modelled as
YJ- =AU,-j+e,-j with prob. 7r,- (i=1,...,g) (3)

for j = 1, ...,n, where the (unobservable) factors U ,1, ..., U in are distributed
independently N (‘g‘ i, 9,), independent of the eij, which are distributed

independently N (0, D), where D is a diagonal matrix (i=1, ..., g). Here
A is a p x q matrix of factor loadings, which satisﬁes ATA =Iq. Then MCFA
is considered as the normal mixture model (1) with the restrictions

and
2,:As2,AT+D (i=1,...,g), (5)

where A is a p x q matrix, ‘6,- is a q-dimensional vector, 52,- is a q x q
positive deﬁnite symmetric matrix, and D is a diagonal p x p matrix. With
the restrictions (4) and (5) on the component mean u,- and covariance matrix
22,-, respectively, the MCFA approach provides a greater reduction in the
number of parameters compared with MFA (see Table 1 in Baek et al. 2010).
The implementation of the EM algorithm to ﬁt this model is described in the
Appendix of Baek et al. (2010). Another useful feature of the MCFA approach
is its ability to portray the results of a clustering in low-dimensional space.
We can plot the estimated posterior means fij of the factors [as deﬁned by
Equation (34) of Baek et al. (2010)] in 2D or 3D space, using the implied
cluster labels. In contrast, MFA does not have the ability to project the high-
dimensional objects in low-dimensional space since the mean vector of the
factor is assumed to be 0 for each cluster. This is illustrated in McLachlan
and Peel (2000a, Chapter 8).

Now we assume that y1,...,yn are an observed random sample from the
t-mixture density

g
f0; ‘1')=met(v; nix-.12.), (6)
where u,- =A§i and l 1
2,:Asz,AT+D(i=1,...,g), (7)
and where the multivariate t-probability density function
ft(y; u, 22,12) is deﬁned as
I‘((v+p)/2)IZ I‘l/2

fro” u’z’v)_ (Irv)I’/21‘(v/2){1+6010;>:)/v}("+f’)/2 ’
where 6(y, u; 2)=(y—M,)TZT1(y—u), and the vector of unknown
parameters ‘11 consists of the degrees of freedom v,- in addition to the mixing
proportions 7t,- and the elements of the ‘g‘i, Sli,A andD (i = 1, . . . , g). As in the
mixture of common factor analyzers model, A is a p x q matrix and D is a
diagonal matrix.

In order to ﬁt the model (6) with the restriction (7), it is computationally
convenient to exploit its link with factor analysis. Therefore, we assume
that the distribution of YJ- of MCtFA is modelled as (3), where the joint
distribution of the factor U ,7- and the error eij needs to be speciﬁed so that it
is consistent with the t-mixture formulation (6) for the marginal distribution
of Yj. In the EM framework, the component label zj associated with the
observation yj is introduced as missing data, where zij = (zj),- is one or
zero according as yj belongs or does not belong to the i-th component of
the mixture (i=1, ..., g; j: 1, ...,n). The unobservable factors uij are also
introduced as missing data in the EM framework.

Now we postulate that conditional on membership of the i-th component
of the mixture the joint distribution of YJ- and its associated factor (vector)
U i]- is multivariate t-distribution. That is,

 

Y- _
 (1:19-'-9g)9 
I]

Where M1? = (Mi, gi) = (AT,Iq)T§i and
K. _ ASliAT+D A9,-

" 9,-AT s2,- '
This speciﬁcation of the joint distribution of YJ- and its associated factors in
(3) will imply the t-mixture model (6) for the marginal distribution of YJ- with
the restriction (7). Using the characterization of the t-distribution related to
the normal distribution, it follows that we can express (8) alternatively as

Y.
(0’. ) IWj,Zij= 1 ~Np+q(u;-“,Ki/w,-), (9)
l]

where wj is a value of the weight variable W]- taken
to have the gamma distribution fg(wj;v,-/2,v,-/2), where

 

1 270

112 Bio'SIBulnoprOJxo'soi1cu110juioiq//:d11q IIIOJJ popcommoq

9IOZ ‘09 lsnﬁnV uo ::

Mixtures of common t-factor analyzers

 

fg(w; or, ,8): {ﬂawo‘Tl/ I‘(oc)}exp(—,Bw)I[0,oo)(w)(a, ,8 > 0). Therefore,

it can be established from (9) that
Uijle,z,-j =1~Nq(§i, Qi/Wj)

and
eijle,z,-j =1~Np(0,D/Wj)

and hence that

Uijlzij= 1 “421093, 913W)
and

eijlzij =1~tp(0,D,v,-).

Thus, with this formulation, the error terms eij and the factors U i]- are
distributed according to the t-distribution with the same degrees of freedom.
However, the factors and error terms are no longer independently distributed
as in the normal-based model for common factor analysis, but they are
uncorrelated. To see this, we have from (9) that conditional on wj, U i]- and
eij are uncorrelated, and hence, unconditionally uncorrelated.

By adopting a common factor loading matrix and the t-distribution for the
factors and error terms, the MCtF model has fewer parameters and is more
robust against extreme observations, thus providing a better ﬁt to data with
skewed heavy tails.

We can obtain the maximum likelihood estimator of the vector of unknown
parameters in the mixture of common t-factor analyzers model speciﬁed by
(6) and (7) as follows. We use a modiﬁed version of the AECM algorithm
outlined in McLachlan et al. (2007) for mixtures of t-factor analyzers.
We assume that the component-indicators zij, the factors U i]- in (3) and
the weights wj in the characterization (9) of the t-distribution for the i-th
component distribution of YJ- and U i]- are all missing. We have from (9) that

leuij,wj,z,-j =1~Np(Au,-j,D/wj) (i=1, ..., g).

Thus in the EM framework for this problem, the complete data consist,
in addition to the observed data yj, of the component-indicators zij, the
unobservable weights wj, and the latent factors  The complete-data log
likelihood for \11 formed on the basis of the complete data is given by

g n
loch(\Il) = ZZzij-logaij,

i=1 j=1
where

aij = NifG(WJ-; 15/2, Vi/2)¢(uij§ 35,-, Qi/Wj)¢(yj;Auij’D/Wj)-

2.1.1 E-step In order to carry out the E-step, we need to be able to
calculate the conditional expectation of the terms ZijWJ-(Uij —‘;',-) and
ZiJ-VVJ-(Uij —‘;‘,-)(U,-j —‘;',-)T. From (9), we have that conditional on yj and wj,
the i-th component-conditional distribution of U ,7- — ‘6,- is multivariate normal
with mean yiT(yj —A‘;',-) and covariance matrix (I q — yiTA)Sl,-/wj, where
7;- =(ASliAT +D)_1A9i- Thus, E{Uij_§i|(}’j’wj’zij =1)}=r,-T(vj-A§i),
and E{<Ui,-—§.-)<Uij—§.)Tlo,,w,,zy=1)}=y$oj—A£i)oj—A§.)Tm+
(I q — yiTA)Sl,-/wj. The conditional expectation of W]- given yj and zij :1 is
given by
W +P

v,+6(yj,A§,;Asz,-AT+D)’

 

wxvj; ‘11): (10)
where 6(yj,A§,-;ASl,-AT +D) =(yj —A‘;',-)T(ASl,-AT+D)_1(yj —A§,). The
conditional expectation of Z; given yj is given by the posterior probability
ti(yj; \II) that yj belongs to the i-th component of the mixture;

niﬁGj;A§,-,ASZ,-AT +1), 1),)

T' -; ‘1’ =

(11)

 

(i=1,...,g;j=1,...,n).

2.1.2 CM—step We use two CM steps in the AECM algorithm, which
correspond to the partition of ‘11 into the two subvectors \Ill and \112, where
\111 consists of the mixing proportions, the elements of ‘6,- and the degrees
of freedom vi (i = 1, . . ., g). The subvector ‘15 consists of the elements of the
common factor loadings matrix A, the $2,- and the diagonal matrix D.

On the ﬁrst cycle, we specify the missing data to be the component-
indicator variables Z; and the weights wj in the characterization (9) of the
t-distribution for the component distribution of y j. On the (k + 1)-th iteration
of the algorithm, we update the estimators of the mixing proportions using

’1
n§k+1)=Zn(v,-;‘Il(k))/n,

J=1

where the posterior probabilities are calculated using (11). The updated
estimate of the i-th component factor mean is given by

n n
k+1 k T k
25- )=ZTi(VJ-;‘1’(k))w,(-j W) yj/Znoj;w<k>)w§,’,
j=1 j=1

where the current weight wgc)

11:00 for 11: in (10).
In the case where the degrees of freedom v,- in the component
t-distributions are not speciﬁed but are to be estimated from the data, we

have to update the estimate of v,- on the ﬁrst cycle. The updated estimate
(k+1)
1).
l

= wi(yj; \II) is formed using the current value

of v,- does not exist in closed form, but is given as a solution of the
equation

W W 1 n (k) (k) (k)
—10(§)+log(3)+1+ E ti]. (logwij —wij )
i=1

n§k>
(k) (k)
v. +p v. +p
+¢(1T)—10g(lT)=0,

where rig-k)=t,-(vj;\ll(k)),n§k) =Z;=lti§k) (i=1,...,g), and 10(-) is the
digamma function.

The estimate of \II is updated so that its current value after the ﬁrst cycle
is given by \Il(k+1/2) =(\1:(1k+1)T,\11(2k)T)T.

On the second cycle of this iteration, the complete data are expanded
to include the unobservable factors U ,3- associated with the yj. An
E-step is performed to calculate Q(\II; \Il(k+1/2)), which is the conditional
expectation of the complete-data log likelihood given the observed data,
using ‘11 = \Il(k+1/2). Then the new posterior probability, ti(yj; \Il(k+1/2)), is
estimated by

k 1 k 1 k T 1
7;; + )ftoj;A(k)§l(, + ),A(k)§zl(, )A(k) +D(k),vl(,k+ ))

 

Zi=1n2k+1)ﬁ(yj;A(k)§§1k+1),A(k)9§1k)A(k)T +D(k),vl(1k+1))'

The CM-step on this second cycle is implemented by the maximization of
Q(\II; \Il(k+1/2)) over \11 with \111 set equal to \II(1k+1). This yields the updated
estimates A(k+1), 9,116+” and DOC“). Set

7516) = (A(k)9l(,k>A(k)T +D<k))—1A(k)9§k>,

 

k v(k+1)+
WE.- +1/2) = i p ,
J v§k+1)+5(yj,A(k)§§k+1);A(k)52§k)A(k)T +D(k))
n§k+1/2)

n
= ZwﬁquH/m),
j=1

0,1,; .1,(k+1/2))w;k+1/2)SE§k+1/2)

 

S(k+1/2) _ 23:1”
" _ 23:1 no); WWI/2)) ’

where SEEkH/Z) =07 —A(k)§§k+1))(yj —A(k)§l(.k+1))T. Then 95.16“) is given by

k1k12Tk12k12 k Tk12
9§+)=yl(+/) S§+/)y§+/)+9§)(Iq_A(k) yl(+/)),

 

1271

112 Bio's112umofp101xo'soi112u1101uioiq/ﬁd11q 111011 p9p1201umoq

9IOZ ‘09 lsnﬁnV uo ::

J.Baek and G.J.McLachlan

 

and the updated estimate DOC“) is given by

g
k+1 1 (k+1/2) k (k+1/2)T (k+1/2)
D( )ZWZW {W )7;- 4105,-
Zi=1ni i=1
1 2 T T 1 2 T
_(A(k)yl§k+ / ) _Ip)T+A(k)9§k)(lq _A(k) y§k+ / ))A(k) }_
k 1 2 k 1 2 T
Let "5+ / >=,g + /)

A(k+1) is obtained by

(yj —A(k)§§k+1))+£§k+l). Then the updated estimate

8 n

k+1 _ . k 12 (k+1/2) (k+1/2)T

A< >—(;zlee,,w<+/>>w, m, )
1:]:
g n k 12 (k+1/2) (k+1/2) (k+1/2)T
-{Z(Znoj;w( + / ))w,-,- 21,-,- 21--

1]
i=1 j=1

T
+nl(k+1/2)(Iq _ y§k+1/2) A(k))szl('k))}_1.

We have to specify an initial value for the vector ‘11 of unknown parameters
in the application of the EM algorithm. A random start is obtained by
ﬁrst randomly assigning the data into g groups. Using the sample mean
and sample covariance matrix of each randomly partitioned data, the initial
parameter estimates are obtained as described in the Appendix of Baek et al.
(2010).

We can portray the observed data yj in q-dimensional space by plotting
the corresponding value of the 12,-]- , which are the estimated conditional
expectations of the factors U ,7, corresponding to the observed data points yj.
Note that

E(Uij|yj,zij=1) = E{E(Ujj|yj,Wj,Zij=1)}
= £.-+ri(v,-—A£,-) (12)

We let 12,-]- denote the value of the right-hand side of (12) evaluated at the
maximum likelihood estimates of ‘g‘i, y,- and A. We can deﬁne the estimated
value ﬁj of the j-th factor corresponding to yj as

g
aj=Zr,(y,-;\It)a,-j (i=1,...,n). (13)
i=1
An alternative estimate of the posterior expectation of the factor
corresponding to the j-th observation yj is deﬁned by replacing ti(yj;\ll)
by 2,-1- in (13).

3 RESULTS

Souto et al. (2008) compared different clustering methods for the
analysis of 35 cancer gene expression datasets. For our experiment,
we considered 2 of these 35 datasets. We applied MCtFA to cluster
each of these two datasets, and compared its performance with
other methods. We compare our method with MCLUST, MFA and
MCFA. The performance is measured by the Adjusted Rand Index
(ARI; Hubert and Arabie, 1985) and the error rate since the true
membership of each observation is known.

MCLUST is a software package that implements Gaussian
mixture models Via EM algorithm and the Bayesian Information
Criterion (BIC, Schwarz, 1978) for model-based clustering (Fraley
and Raftery, 2003). In MCLUST, the component-covariance matrix
22,- is parameterized by eigenvalue decomposition in the form
22,- =piEiAiEiT, where p,- is a constant, E is the matrix of
eigenvectors, Ai is a diagonal matrix with elements proportional
to the eigenvalues of 22,-. Different conditions on pi, Ai and E,-
characterize the volume, shape and orientation of each component
distribution in MCLUST. We deal with the 10 submodels of
MCLUST: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV, VVV

Table 1. Comparison of MCLUST, MFA, MCFA and MCtFA models for
implied clustering versus the true membership of Chowdary’s 104 cancer
tissues

 

 

 

Model Factors BIC AWE ARI Error rate
MCLUST VVI 242 872 0.0657 0.3462
MFA 250 841 257 805 0.0296 0.3750

241 124 250 855 0.0296 0.3750
234 888 247 371 0.5858 0.1154
232917 248137 0.0386 0.3750
230 485 248 426 0.0657 0.3462
230 326 250 974 0.4726 0.1538
230 461 253 800 0.2790 0.2308
229 825 255 839 0.1431 0.2981
229 433 258 107 0.1696 0.2885

MCFA 261 337 264 164 0.6800 0.0865
253 301 257 532 0.0464 0.3654
246 291 251 934 0.1282 0.3077
243 084 250 139 0.0657 0.3462
240 297 248 767 0.1587 0.2885
236 654 246 539 0.0657 0.3462
233 076 244 374 0.0657 0.3462
232 083 244 795 0.2128 0.2596
231 226 245 354 0.1240 0.3077
MCtFA 234 450 237 278 0 0.4038

229 677 233 920 0.8505 0.0385
228 106 233 764 0.8505 0.0385
226 891 233 976 0.0675 0.3558
225 387 233 876 0.5564 0.1250
223 518 233 419 0.8867 0.0288
222 455 233 772 0.6808 0.0865
222 156 234 884 0.7465 0.0673
221 134 235 276 0.4742 0.1538

\OOO\10\M4>WNH\OOO\10\M4>WNH\OOO\IO\UIJ>WNH

 

The bold numbers are the optimal values of BIC, AWE, ARI and Error rate for each
model.

(Fraley and Raftery, 2003, Table 1). For MFA, we assumed the
covariance matrix of errors is equal for each component. We took
advantage of the mclust software for R (Team RDC 2004) and the
EMMIX program (McLachlan et al., 1999) for MFA, and developed
programs for the MCFA and the MCtFA approaches, using the
MATLAB language.

The ﬁrst set concerns both breast and colon cancer data
(Chowdary et al., 2006), which consists of 104 gene expressions
for 52 matched tissue pairs of two different cancer types (32 pairs
of breast tumour and 20 pairs of colon tumour). There are 22 283
genes in the original data, but Souto et al. (2008) selected 182
genes by ﬁltering uninformative genes. It has been reported in many
analyses of real datasets that the empirical distribution of gene
expression levels is approximately log-normal or sometimes (on
the log scale) with a slightly heavier tailed t-distribution depending
on the biological samples under investigation (Li, 2002). Thus,
these data may also have many extreme expressions for each gene.
Figure 1 shows boxplots of the expression levels for the ﬁrst 10
genes. The distribution of each gene is skewed and has a very long
tail. The rest of the genes also have similar shaped distributions. In
particular, gene 6 has very high expression levels for six particular
tissues.

 

1 272

112 /810's112umofp101xo's31112u1101u101q”:d11q 111011 p9p1201umoq

9IOZ ‘09 lsnﬁnV uo ::

Mixtures of common t-factor analyzers

 

 

++-

V
.n
O
O
O
0
+1-

expression le el
+

+

1:. . :21

ii;

8 9 10

 

 

m-+ll-Hl-
u-h- ++

 

4
gene

2

Fig. 1. The boxplots for the ﬁrst 10 genes in the cancer data of Chowdary
et al. (2006).

We implemented the MCLUST, MFA, MCFA and MCtFA
procedures with g=2 components with the number of factors q
ranging from 1 to 9, using 50 starting values for the parameters.
For each value of q, we computed the ARI and the associated error
rate. The results are presented in Table 1. We have also listed in this
table the values of the BIC and the Approximate Weight of Evidence
(AWE: Banﬁeld and Raftery, 1993) for each model with different
q. AWE is a model selection criterion based on an approximation to
the classiﬁcation log-likelihood. AWE is deﬁned as

—2logL(\II)+2EN(1)+2m(3/2+log(n)), (14)

where EN (1:) = — 217:1 22:1 ti(yj; \Il)log(ti(yj; ‘11)) is the entropy
of the classiﬁcation matrix with (i , j)—th element equal to ti(yj; \II)
and m is the number of (free) parameters. It penalizes complex
models more severely than BIC, and thus selects more parsimonious
models than BIC. It would appear that BIC works well at a practical
level; see, for example, Fraley and Raftery (1998). Further, Keribin
(2000) proved that BIC provides a consistent estimator of g (Celeux,
2007). But BIC can lead to too few or too many clusters in practice,
depending on the problem at hand. For the present problem of
choosing the number of factors q, it would appear from Table 1
that it leads to too many factors being ﬁtted in the mixtures of factor
analyzers model. An apparent explanation for this is that for the
present dataset (and the other one to follow), the data are not well
represented by the true models of clusters and/or the true clusters are
not well separated. As a consequence, BIC leads to too many factors
in the mixture model being ﬁtted to the data. The AWE criterion is
preferable to BIC here as it leads to fewer factors since it places a
higher penalty on more complex model due to the presence of twice
of the entropy and the extra constant term (2EN(‘r) +3m+mlog(n))
in (14). We also considered the ICL criterion which chooses q to
minimize —2logL(\II)+2EN('r)+mlog(n), which is similar to the
AWE criterion. They are the same apart from the additional penalty
of 3m +mlog(n) imposed by AWE, which for our present problem
leads to a better choice of q.

It can be seen that the lowest error rate (0.0288) and highest
value (0.8867) of the ARI is obtained by using q=6 factors with
the MCtFA model, which coincides with the choice on the basis of
AWE. The lowest error rate (0.1154) of the MFA model is obtained
for q = 3 factors. The best result of MCFA model is obtained with its
lowest error rate 0.0865 for q = 1 factor (AWE suggests using q = 7).
MCLUST chose VVI model as its best and its error rate is 0.3462.
It can be seen that the error rate and ARI for MCtFA are better than
those for MCLUST, MFA and MCFA. We have also calculated BIC
for all models. It can be seen that it failed to select the best model

 

 

—0.5

Fig. 2. Plot of the (estimated) posterior mean factor scores Via the MCtFA
approach based on the implied clustering for the cancer data of Chowdary
et al. (2006).

 

16000 - +
14000 '

12000 -

+
+

10000 - +

++

8000 -

expression level

+

6000 - $

4000 - +

+

2000- + i
.2 1 e

9

6 7 8

 

1|]:l—l-IH—H-l-H +-l+-Hl- + + -

 

lﬂj—l+—lH—ll+ +l-+

10 11 12 13 14 15 16 17
ﬁssue

ﬂj—HHH+++

I-llI-l-+

 

Fig. 3. The boxplots of expression levels of all genes for the 6th—17th colon
tumor tissues.

for each method. BIC reached its minimum for largest q(q=9) of
each method, so it selected a more complex model than the one with
highest ARI and lowest error rate. In the case where the distribution
from which the data arose is not in the collection of considered
mixture models, BIC criterion tends to overestimate the correct size
regardless of the separation of the clusters (Celeux, 2007).

To illustrate the usefulness of the MCtFA approach for portraying
the results of a clustering in low-dimensional space, we have plotted
in Figure 2 the estimated factor scores fij as deﬁned by (13) with
the implied cluster labels shown. In this plot, we used the third and
sixth factors in the ﬁt of the MCtFA model with q=6 factors. It
can be seen that the clusters are represented in this plot with very
little overlap. The estimated factor scores were plotted according to
the implied clustering labels. The degrees of freedom of the factor
t-distributions for both groups were estimated as 1.0 and 1.0, which
means their tails of the distributions are very thick and long. We can
easily detect 6 distinct extreme tissues as shown in Figure 2, which
are known to be the 9th—14th colon tumor tissue. Figure 3 shows the
expression levels of all genes for the 6th—17th colon tumor tissues.
In Figure 3, we observe that all of these 6 outliers are very different
from others since they have extremely large expression levels not
only of the 6th gene shown in Figure 1, but also of other genes.

The second dataset to which we applied our method is a lung
cancer data (Bhattacherjee et al., 2001), for which the number of
classes is not small (g=5). It consists of 203 gene expressions
partitioned into ﬁve subpopulations: four lung cancer types and
normal tissues. Souto et al. (2008) selected 1543 informative
genes from the original 12 600 genes. There are big differences
among the class sizes of the data. The number of tissues for each
class is 139, 17, 6, 21 and 20, respectively. Figure 4 shows the

 

1 273

112 /810's112umofp101xo's01112u1101u101q”:d11q 111011 p9p1201umoq

9IOZ ‘09 lsnﬁnV uo ::

J.Baek and G.J.McLachlan

 

 

1500 '

1000 -

1
i
f

500- S

I .
I
| _
I .
El : D '
f 1:! 4
2 3 4 5
sub—population

expression level

 

 

 

Fig. 4. The boxplots for the expressions of a gene by subpopulation: the
lung cancer data of Bhattacherjee et al. (2001).

boxplots of the expressions of a gene plotted for each subpopulation.
It can be seen that there exist skewed distributions mixed with
symmetric distribution with or without extreme observations for
ﬁve components in the plot. Since the selected (1543) genes are
still too many for the mixture model, we grouped the genes into 50
clusters and selected the centroid from each cluster of genes. That
is, we applied the k-means algorithm to the 1543 gene expressions
and clustered them into 50 groups of similar characteristics. Then
we extracted the centroid from each group to make 50 new features
for the mixture models.

We implemented the MCLUST, MFA, MCFA, and MCtFA
approaches with g=5 components for the number of factors q
ranging from 1 to 10, using 50 starting values for the parameters.
For each value of q, we computed the ARI and the error rate. The
results are presented in Table 2. We have also listed in this table
the values of BIC and AWE for each model with different levels
of q. MCtFA attains its largest ARI (0.7322) and lowest error rate
(0.1133) for q=6, although AWE suggested the model with q=7.
We notice that there is little difference between the AWE values
for q=6 and for q=7. The lowest error rate for MCFA is 0.2611
for q=2 and the largest ARI is 0.4570 for q=9. The error rate
(NA) of MCFA for q=1 was not able to be calculated since the
estimated number of clusters was less than the true value 5. Neither
BIC nor AWE indicated the best model for MCFA. MFA reached its
best ARI (0.3487) and error rate (0.3498) for q=7. The minimum
BIC and AWE were obtained at q=6, and at q: 1, respectively.
The best model for MCLUST showed similar performance (ARI:
0.3021, error rate: 0.3350) to MFA. MCtFA again performed better
than the other methods for this dataset.

We display the data using the estimated factor scores of our model
in 3D space (Figure 5). In the latter, we used the second, the fourth
and the ﬁfth factors in the ﬁt of the MCtFA model with q = 6 factors.
The estimated factor scores were plotted according to the implied
clustering labels. It can be seen that the ﬁve clusters are represented
in this plot with very little overlap. The degrees of freedom of the
factor t-distributions for the components were estimated as 1.1, 1.3,
7.8, 4.0 and 4.1. There are two distributions with long tails [v1=
1.1(tricmgle), v2 =1.3(circle)] in the plot. We have also given in
Figure 6 the plot corresponding to that in Figure 5 with the true
cluster labels shown. There are 23 misallocated tissues which can
be seen in other’s clusters, but as a whole there is a good agreement
between the two plots.

4 DISCUSSION

For clustering high-dimensional data such as microarray gene
expressions, MFA is a useful technique since it can reduce the
number of parameters through its factor-analytic representation
of the component-covariance matrices. However, this approach is

Table 2. Comparison of MCLUST, MFA, MCFA and MCtFA models for
implied clustering versus the true membership of Bhattacharjee’s 203 lung
cancer tissues

 

 

 

Model Factors BIC AWE ARI Error rate
MCLUST VVI 135 023 0.3021 0.3350
MFA 140 710 145 324 0.3100 0.4286

139479 146125 0.3219 0.3941
139 313 147 955 0.3156 0.4089
139016 149611 0.3013 0.4039
139068 151573 0.3368 0.3645
138 870 153 244 0.2445 0.4236
139 358 155 561 0.3487 0.3498
139 747 157 738 0.2616 0.4335
140 207 159 943 0.2571 0.4433
0 140 414 161 854 0.2368 0.4680

MCFA 148 255 149 674 0.0721 NA
144 994 146 585 0.3348 0.2611
142 978 145 042 0.3090 0.3300
141 826 144 453 0.4376 0.2759
140943 144139 0.3703 0.3153
140 123 143 943 0.3269 0.3251
139 362 143 800 0.3692 0.3153
138921 144015 0.3775 0.3153
138 420 144 194 0.4570 0.2709
0 138 134 144 633 0.2418 0.4335
MCtFA 144 424 145 607 —0.1269 0.4384

142 708 144 294 0.3619 0.2413
141 155 143 236 0.4735 0.2266
139 683 142 334 0.6179 0.1527
138 937 142154 0.6657 0.1379
138 194 142 025 0.7322 0.1133
137 538 142 009 0.5875 0.1675
136 973 142105 0.6417 0.1773
136 660 142474 0.4408 0.2365
0 136 431 142 967 0.3215 0.3153

HOOOQO'NUl-RUJNHHOOOQQUI-bWNHHOOOQQUl-RWNH

 

The bold numbers are the optimal values of BIC, AWE, ARI and Error rate for each
model. NA means Not Available.

10000

 

—5000 —5000

Fig. 5. Plot of the (estimated) posterior mean factor scores Via the MCtFA
approach based on the implied clustering for the lung cancer data of
Bhattacherjee et al. (2001).

sensitive to outliers as it is based on a mixture model in which
the multivariate normal family of distributions is assumed for the
component factor and error distributions. McLachlan et al. (2007)
extended MFA to incorporate t-distributions for the component
factor and error in the mixture model for dealing with unusual
extreme observations (MtFA). These methods, however, may

 

1 274

112 /810's112umofp101xo's01112u1101u101q”:d11q 111011 pepcolumoq

9IOZ ‘09 lsnﬁnV uo ::

Mixtures of common t-factor analyzers

 

5000

—5000 -‘--"""T

—10000 .--"""'~" .
5000 a:
10000

 

—5000 —5000

Fig. 6. Plot of the (estimated) posterior mean factor scores via the MCtFA
approach with the true labels shown for the lung cancer data of Bhattachte ee
et al. (2001).

not provide a sufﬁcient reduction in the number of parameters,
particularly when the number of clusters (subpopulations) is not
small. In this article, we proposed a new mixture model which can
reduce the number of parameters further in such instances and cluster
the data containing outliers simultaneously by introducing a mixture
of t-distributions with both component-mean and component
covariance represented by common factor loadings. We call this
approach mixtures of common t-factor analyzers (MCtFA). We
describe the implementation of an EM algorithm for ﬁtting the
MCtFA. This approach also has the ability to portray the results of
a clustering in low-dimensional space. We can plot the estimated
posterior means of the factors a,- as deﬁned by (13) with the
implied cluster labels. On the other hand, the approaches MCLUST,
MFA and MtFA cannot project high-dimensional objects in low-
dimensional space. The applications of MCtFA to two cancer
microarray datasets have demonstrated the usefulness and its relative
superiority in clustering performance over MCLUST, MFA and
MCFA. It has shown that our method works well for clustering
data containing outliers. Moreover, it provides information on
the distribution structure of each subpopulation by displaying the
estimated factor scores in low-dimensional space. We observed also
that the proposed approach ﬁtted the experimental datasets better
than the other approaches, and the performance difference between
MCtFA and the others becomes even greater when the number of
clusters is not small, such as in the case of second dataset (Section 1.1
of Supplementary Material).

Often BIC is used to provide a guide to the choice of the number of
factors q and the number of components g to be used. However, it did
not always lead to the correct choice of the best model. That is, BIC
can lead to too simple or too complex model in practice, depending
on the problem at hand. Simulation studies reported in Biernacki
and Govaert (1997), Biernacki et al. (2000) and McLachlan and
Peel (2000a) show that BIC will overrate the number of clusters
under misspeciﬁcation of the component density, whereas several
alternative criteria such as the AWE and ICL criterion are able to
identify the correct number of clusters even when the component
densities are misspeciﬁed (Friihwirth—Schnatter and Pyne, 2010). In
both of our real data applications, we observed that BIC did not
choose the best q. An apparent explanation for this is that BIC tries
to choose more complex model since some of the subpopulations
of the datasets have skewed distributions and have several extreme
outliers. On the other hand, AWE leads to the best or almost the
best model with smallest error rate since it is more robust against
misspeciﬁcation of the component densities for the experimental
datasets. Recently, Frﬁhwirth—Schnatter and Pyne (2010) reported
that AWE picked the correct model for both skew-t and skew-normal

mixture distributions. Also a small simulation study conﬁrms the
better performance of the AWE over the BIC when the distribution
of the data has skewed heavy tails due to some extreme observations
(Section 1.2 of Supplementary Material). In future work, we wish to
investigate the use of various model selection criteria on choosing
the number of factors q and the number of components g in mixtures
of t or skewed distributions.

Funding: Korea Research Foundation Grant funded by the Korean
Government (MOEHRD, Basic Research Promotion Fund, KRF—
2007-521-C00048 to J .B.). Australian Research Council (to G.J.M.).

Conﬂict of Interest: none declared.

REFERENCES

Baek,J. and McLachlan,GJ. (2008) Mixtures of factor analyzers with common
factor loadings for the clustering and visualisation of high-dimensional data.
Technical Report NIO8018—SCH. Preprint Series of the Isaac Newton Institute for
Mathematical Sciences, Cambridge.

Baek,J. et al. (2010) Mixtures of factor analyzers with common factor loadings:
applications to the clustering and visualisation of high-dimensional data. IEEE
Trans. Pattern Anal. Mach. Intel., 32, 1298—1309.

Banﬁeld,J.D. and Raftery,A.E. (1993) Model-based Gaussian and non-Gaussian
clustering. Biometrics, 49, 803—821.

Bhattacherjee,A. et al. (2001) Classiﬁcation of human lung carcinomas by mRNA
expression proﬁling reveals distinct adenocarcinoma subclasses. Proc. Natl Acad.
Sci LHLA,98,13790—13795.

Biemacki,C. and Govaert,G (1997) Using the classiﬁcation likelihood to choose the
number of clusters. C0mput. Sci. Stat, 29, 451—457.

Biemacki,C. et al. (2000) Assessing a mixture model for clustering with the integrated
completed likelihood. IEEE Trans. Pattern Anal. Mach. Intel, 22, 719—725.

Celeux,G (2007) Mixture models for classiﬁcation. In Decker,R. et al. (ed.) Advances
in Data Analysis. Springer, Berlin.

Chowdary,D. et al. (2006) Prognostic gene expression signatures can be
measured in tissues collected in RNAlater preservative. J. Mol. Diagn., 8,
31—39.

Fraley,C. and Raftery,A.E. (1998) How many clusters? Which clustering methods?
Answers via model-based cluster analysis. C0mput.J., 41, 578—588.

Fraley,C. and Raftery,A.E. (2003) Enhanced model-based clustering, density estimation,
and discriminant analysis software: MCLUST. J. Classiﬁc., 20, 263—286.

Friihwirth-Schnatter,S. and Pyne,S. (2010) Bayesian inference for ﬁnite mixtures of
univariate and multivariate skew-normal and skew-t distributions. Biostatistics, 11,
317—336.

Galimberti,G et al. (2009) Penalized factor mixture analysis for variable selection in
Clustered Data, C0mput. Stat. Data Anal., 53, 4301—4310.

Giles,P.J. and Kipling,D. (2003) Normality of oligonucleotide microarray data and
implications for parametric statistical analyses. Bioinformatics, 19, 2254—2262.
Hinton,GE. et al. (1997) Modeling the manifolds of images of handwritten digits. IEEE

Trans. Neural Netw., 8, 65—73.

Hubert,L. and Arabie,P. (1985) Comparing partitions. J. Classiﬁc, 2, 193—218.

Keribin,C. (2000) Consistent estimation of the order of mixture models. Sankhya Ser
11, (i2, 41£)-(3(3.

Li,K.C. (2002) Genome-wide coexpression dynamics: theory and application. Proc.
Natl Acad. Sci. USA, 99, 16875—16880.

L6nnstedt,I. and Speed,T. (2002) Replicated microarray data. Stat. Sinica, 12, 31—46.

Martella,F. (2006) Classiﬁcation of microarray data with factor mixture models.
Bioinformatics, 22, 202—208.

McLachlan,GJ. and Basford,K.E. (1988) Mixture Models: Inference and Applications
to Clustering. Marcel Dekker Inc, New York.

McLachlan,GJ. and Peel,D. (2000a) Finite Mixture Models. Wiley, New York.

McLachlan,GJ. and Peel,D. (2000b) Mixtures of factor analyzers. In Langley, P. (ed.).
Proceedings of the Seventeenth International Conference on Machine Learning.
Morgan Kaufmann, San Francisco, pp. 599—606.

Mclachlan,GJ. et al. (1999) The EMMIX software for the ﬁtting of mixtures of normal
and t-components. J. Stat. S0ftw., 4, 2.

McLachlan,GJ. et al. (2002) Mixture model-based approach to the clustering of
microarray expression data. Bioinformatics, 18, 413—422.

 

1 275

112 /810's112umofp101xo's01112u1101u101q”:d11q 111011 pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

J.Baek and G.J.McLachlan

 

McLachlan,G.J. et al.(2003) Modelling high-dimensional data by mixtures of factor
analyzers. C0mput. Stat. Data Anal., 41, 379—388.

McLachlan,G.J. et al. (2007) Extension of the mixture of factor analyzers model
to incorporate the multivariate t distribution. C0mput. Stat. Data Anal., 51,
5327—5338.

McNicholas,P.D. and Murphy,T.B. (2008) Parsimonious Gaussian mixture models. Stat.
C0mput., 18, 285—296.

Schwarz,G (1978) Estimating the dimension of a model. Ann. Stat, 6,
461—464.

Souto,M. et al. (2008) Clustering cancer gene expression data: a comparative study.
BMC Bioinformatics, 9, 497.

Team RDC (2004) R: A language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria.

Thalamuthu,A. et al. (2006) Evaluation and comparison of gene clustering methods in
microarray analysis. Bioinformatics, 22, 2405—2412.

Xie,B. et al. (2010) Penalized mixtures of factor analyzers with application to clustering
high dimensional microarray data. Bioinformatics, 26, 501—508.

Yeung,K.Y. et al. (2001) Model-based clustering and data transformations for gene
expression data. Bioinformatics, 17, 977—987.

Zhou,H. et al. (2009) Penalized model-based clustering with unconstrained covariance
matrices. Electron. J. Stat, 3, 1473—1496.

 

1 276

112 /810's112umofp101xo's01112u1101u101q”:d11q 111011 pepeolumoq

9IOZ ‘09 lsnﬁnV uo ::

