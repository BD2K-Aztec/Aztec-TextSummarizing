Bioinformatics, 31(11), 2015, 1754—1761

doi: 10.1093/bioinformatics/btv037

Advance Access Publication Date: 24 January 2015
Original Paper

 

Gene expression

Bayesian feature selection for high-dimensional
linear regression via the lsing approximation
with applications to genomics

Charles K. Fisher* and Pankaj Mehta*

Department of Physics, Boston University, Boston, MA 02215, USA

*To whom correspondence should be addressed.
Associate Editor: lnanc Birol

Received on August 11,2014; revised on November 13, 2014; accepted on January 16,2015

Abstract

Motivation: Feature selection, identifying a subset of variables that are relevant for predicting a re—
sponse, is an important and challenging component of many methods in statistics and machine
learning. Feature selection is especially difficult and computationally intensive when the number of
variables approaches or exceeds the number of samples, as is often the case for many genomic
datasets.

Results: Here, we introduce a new approach—the Bayesian lsing Approximation (BIA)—to rapidly
calculate posterior probabilities for feature relevance in L2 penalized linear regression. In the re—
gime where the regression problem is strongly regularized by the prior, we show that computing
the marginal posterior probabilities for features is equivalent to computing the magnetizations of
an Ising model with weak couplings. Using a mean field approximation, we show it is possible to
rapidly compute the feature selection path described by the posterior probabilities as a function of
the L2 penalty. We present simulations and analytical results illustrating the accuracy of the BIA on
some simple regression problems. Finally, we demonstrate the applicability of the BIA to high—di—
mensional regression by analyzing a gene expression dataset with nearly 30 000 features. These re—
sults also highlight the impact of correlations between features on Bayesian feature selection.
Availability and implementation: An implementation of the BIA in C—l——l—, along with data for repro—
ducing our gene expression analyses, are freely available at http://physics.bu.edu/~pankajm/
BIACode.

Contact: charleskennethfisher@gmail.com or ckfisher@bu.edu or pankajm@bu.edu
Supplementary information: Supplementary data are available at Bioinformatics online.

 

 

1 Introduction

Linear regression is one of the most broadly and frequently used
statistical tools. Despite hundreds of years of research on the subject
(Legendre, 1805), modern applications of linear regression to large
datasets present a number of new challenges. Modern applications
of linear regression, such as Genome Wide Association Studies
(GWAS), often consider datasets that have at least as many potential
variables (or features) as there are data points (McCarthy et 61].,
2008). Applying linear regression to high—dimensional datasets often

involves selecting a subset of relevant features, a problem known as
feature selection in the literature on statistics and machine learning
(Guyon and Elisseeff, 2003). Even for classical least—squares linear
regression, it turns out that the associated feature selection problem
is quite difficult (Huo and Ni, 2007).

The difficulties associated with feature selection are especially
pronounced in genomics and GWAS. In general, the goal of many
genomics studies is to identify a relationship between a small num-
ber of genes and a phenotype of interest, such as height or body

©The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please email: journaIs.permissions@oup.com 1754

112 /§JO'S{eumo [p.IOJXO'SSUBUHOJUIOIQ/ﬁdllq 11101; popeoIII/vxoq

9IOZ ‘09 lsnﬁnv uo ::

High-dimensional linear regression via the BIA

1755

 

mass index (Burton et 61]., 2007; McCarthy et 61]., 2008; Peng et 61].,
2010; Subramanian et 61]., 2005; Wu et 61]., 2009). For example,
many GWAS seek to identify specific genetic mutations (called sin—
gle nucleotide polymorphisms—SNPs) that best explain the vari—
ation of a quantitative trait, such as height or body mass index, in a
population (Yang et 61]., 2012). Using various techniques, the trait is
regressed against binary variables representing the presence or ab—
sence of the SNPs in order to find a subset of SNPs that are highly
explanatory for the trait (Peng et 61]., 2010; Wu et 61]., 2009).
Although the number of individuals genotyped in such a study may
be in the thousands or even tens of thousands, this pales in compar—
ison to the number of potential SNPs which can be in the millions
(McCarthy et 61]., 2008). Moreover, the presence or absence of vari—
ous SNPs tends to be correlated due to chromosome structure and
genetic processes that induce the so—called linkage disequilibrium
(Yang et 61]., 2012). As a result, selecting the best subset of SNPs for
the regression involves a search for the global minimum of a land-
scape that is both high dimensional (due to the large number of
SNPs) and rugged (due to correlations between SNPs).

The obstacles that make feature selection difficult in GWAS also
occur in many other applications of linear regression to big datasets.
In fact, the task of finding the optimal subset of features is proven,
in general, to be NP—hard (Huo and Ni, 2007). Therefore, it is usu—
ally computationally prohibitive to search over all possible subsets
of features and one has to resort to other methods of feature selec—
tion. For example, forward (or backward) selection adds (or elimin—
ates) one feature at a time to the regression in a greedy manner
(Guyon and Elisseeff, 2003). Alternatively, one may use heuristic
methods such as Sure Independence Screening (SIS) (Fan and Lv,
2008), which selects features independently based on their correl—
ation with the response, or Minimum Redundancy Maximum
Relevance (Ding and Peng, 2005), which penalizes features that are
correlated with each other. The most popular approaches to feature
selection for linear regression, however, are penalized least—squares
methods (Candes and Tao, 2007; Hoerl and Kennard, 1970;
Tibshirani, 1996; Zou and Hastie, 2005) that introduce a function
that penalizes large regression coefficients. Common choices for the
penalty function include an L2 penalty, called ‘Ridge’ regression
(Hoerl and Kennard, 1970), and an L1 penalty, commonly referred
to as LASSO regression (Tibshirani, 1996).

Penalized methods for linear regression typically have natural in—
terpretations as Bayesian approaches with appropriately chosen prior
distributions. For example, L2 penalized regression can be derived by
maximizing the posterior distribution obtained with a Gaussian prior
on the regression coefficients. Similarly, L1 penalized regression can
be derived by maximizing the posterior distribution obtained with a
Laplace (i.e. double—exponential) prior on the regression coefficients.
While penalized regression methods essentially aim to find the features
that maximize a posterior distribution they do not allow one to actu—
ally compute posterior probabilities, which provide information about
confidence in a Bayesian framework. Calculating these posterior prob—
abilities generally requires Monte Carlo methods, which can be very
computationally demanding in high dimensions (George and
McCulloch, 1993; Guan et 61]., 2011; Li and Zhang, 2010). Thus, in
order to apply Bayesian approaches to feature selection to high—
dimensional problems it is necessary to develop approximate methods
for computing posterior probabilities that bypass the need for exten—
sive sampling from the posterior distribution.

Inspired by the success of statistical physics approaches to hard
problems in computer science (Mézard et 61]., 2002; Monasson et 61].,
1999) and statistics (Balasubramanian, 1997; Malzahn and Opper,
2005; Nemenman and Bialek, 2002), we study high-dimensional

regression with ‘strongly regularizing’ prior distributions. A strongly
regularizing prior distribution is one that exerts a significant influ-
ence on the posterior distribution even when the sample size goes to
infinity. The definition will be made more precise later. In this
strongly regularized regime, we show that the marginal posterior
probabilities of feature relevance for L2 penalized regression are
well—approximated by the magnetizations of an appropriately
chosen Ising model—a widely studied model from physics used to
describe magnetic materials (Opper and Winther, 2001). For this
reason, we call our approach the Bayesian Ising Approximation
(BIA) of the posterior distribution. Using the BIA, the posterior
probabilities can be computed without resorting to Monte Carlo
simulation using an efficient mean field approximation that facili-
tates the analysis of very high—dimensional datasets. We envision the
BIA as part of a two—stage procedure where the BIA is applied to
rapidly screen irrelevant variables, i.e. those that have low rank in
posterior probability, before applying a more computationally inten-
sive cross—validation procedure to infer the regression coefficients
for the reduced feature set. This study is especially well suited to
modern feature selection problems where the number of features, p,
is often larger than the sample size, 11.

Our approach differs significantly from previous methods for
feature selection. Traditionally, penalized regression and related
Bayesian approaches have focused on the ‘weakly regularized re—
gime’ where the effect of the prior is assumed to be negligible as the
sample size tends to infinity. The underlying intuition for consider-
ing the weak-regularization regime is that as long as the prior (i.e.
the penalty parameter) is strong enough to regularize the inference
problem, a less inﬂuential prior distribution should be better suited
for feature selection and prediction tasks because it ‘allows the data
to speak for themselves’ (Gelman et 61]., 2013). In the machine learn-
ing literature, the penalty parameter is usually chosen using cross
validation to maximize out—of—sample predictive ability (Tibshirani,
1996; Zou and Hastie, 2005). A similar esthetic is also reflected in
the abundant literature on ‘objective’ priors for Bayesian inference
(Ghosh et 61]., 2011). As expected, these weakly regularizing
approaches perform well when the sample size exceeds the number
of features (71 >> p). However, very strong priors may be required
for high-dimensional inference where the number of features can
greatly exceed the sample size ([2 >>  Our BIA approach exploits
the large penalty parameter in this strongly regularized regime to ef—
ficiently calculate marginal posterior probabilities using methods
from statistical physics.

The article is organized as follows: in Section 2.1, we review
Bayesian linear regression; in Section 2.2, we derive the BIA using a
series expansion of the posterior distribution and describe the associ—
ated algorithm for variable selection; and in Section 3.1, we present
analytical results and simulations on the performance of the BIA
using features with a constant correlation, in Section 3.2 we analyze
a real dataset for predicting bodyfat percentage from 12 different
body measurements and in Section 3.3 we analyze a real dataset for
predicting a quantitative phenotypic trait from data on the expres—
sion of 28 395 genes in soybeans.

2 Methods

2.1 Bayesian linear regression

In this section, we brieﬂy review the necessary aspects of Bayesian
linear regression. This entire section follows standard arguments,
the details of which can be found in many textbooks on Bayesian
statistics (see e.g. O’Hagan et 61]., 2004). The goal of linear regres-
sion is to infer the set of coefficients ﬂl- for j = 1, . . . , p that describe

112 /810'spaumo [p.IOJXO'SSUBUHOJUIOIQ/ﬁdllq 11101; popeoIII/vxoq

9IOZ ‘09 lsnﬁnv uo ::

1756

C. K.Fisher and P. Mehta

 

the relationship 32 : XTﬂ + 17 from n observations (32,-, X1) for
i: 1, ... ,n. Here, X is a (p X 1) vector of features and 17 N N(0, 02)
is a Gaussian distributed random variable with unknown variance
02. Without loss of generality, we will assume throughout this art—
icle that the data are standardized with
21y,- : 0, Zia/£2 : n, Zi(X,‘)l~ : 0 and  = n so that it is
not necessary to include an intercept term in the regression.
Penalized least—squares methods estimate the regression coefficients
by minimizing a convex objective function in the form of:

UW) = 2 (3’1

1'

—x.-T/3)2+2f(/3), (1)

where f (ﬂ) is a function that penalizes large regression coefficients
and 2 is the strength of the penalty. Common choices for the penalty
function include f (B) =  for L2 penalized or ‘Ridge’ regression
(Hoerl and Kennard, 1970), and f (B) : ﬂi| for L1 penalized or
LASSO regression (Tibshirani, 1996). The standard least—squares
(and maximum likelihood) estimate B : (XTX)_1XTy is recovered
by setting 2 = 0, where X is the (n X p) design matrix with columns
xi. Adding a penalty to the least—squares objective function mitigates
instability that results from computing the inverse of the XTX ma—
trix. In the case of the L1 penalty, many of the regression coefficients
end up being shrunk exactly to 0 resulting in a type of automatic fea—
ture selection (Candes and Tao, 2007; Tibshirani, 1996; Zou and
Hastie, 2005).

Bayesian methods combine the information from the data,
described by the likelihood function, with a priori knowledge,
described by a prior distribution, to construct a posterior distribu—
tion that describes one’s knowledge about the parameters after
observing the data. In the case of linear regression, the likelihood

1 >"CX ((yXﬂ)T(yX/3))
m p 202 '

In this work, we will use standard conjugate prior distributions for B
and 0'2 given byP(ﬂ, 02|s) : P(02)P(ﬂ|02, s)where

function is a Gaussian

 

Pol/3,02) = (

P102) o< (ca—(“0+”exp1—bo/02)
,1 .2
P113102, s) oc 11((1 — $0503» + (1 + s) .
I

These distributions were chosen because they ensure that the poster—
ior distribution can be obtained in closed—form (O’Hagan et 61].,
2004). Here, we have introduced a vector (s) of indicator variables
so that ﬂl- : 0 if s,- = —1 and ﬂl- 31E 0 if s,- = +1. We also have to spe—
cify a prior for the indicator variables, which we will set to a ﬂat
prior P(s) QC 1 for simplicity. In principle, do, b0 and the penalty par—
ameter on the regression coefficients, 2, are free parameters that
must be specified ahead of time to reflect our prior knowledge. We
will discuss these parameters in the following section.

We have set up the problem so that identifying which features
are relevant is equivalent to identifying those features for which
5,- = +1. Therefore, we need to compute the posterior distribution
for s, which can be determined from Bayes’ theorem:

log szly) + c = log jdﬂdaZPoI/s. 0an, azls)P(s)

1 1 n 1 (2)
_ _ _ _ T _ _ _

_2mun ﬂmu+xgxi @mrﬂm(m+2Edp)
where C is a constant and Es(2) is the sum of the squared residual
errors. In this expression, q : Ziﬂ + 5)) / 2 is the number

of variables with s,- = +1, I is the (q x q) identity matrix and X, is a
(n x q) restricted design matrix which only contains columns corres—
ponding to features where s,- = +1. The sum of the squared residual
errors is given by Es(2) : yTy — yTXSBS(2), where

35(2) : (21 + XSTXS)_1XSTy is the Bayesian estimate for the regres—

sion coefficients corresponding to those variables for which 5,- = +1.

2.2 The Ising approximation
2.2.1 Strongly regularized expansion
In principle, one can directly use Equation (2) to estimate the rele—
vance of each feature using two different approaches. First, we could
find the s that maximizes the posterior probability distribution.
Alternatively, we could compute the marginal probabilities of fea—
ture relevance, P),(s,- = +1|y) : (1 + (si))/2, where (5;) is the expect—
ation value of 57- with respect to the posterior distribution, and select
the features with the largest P),(s,- : +1|y). In the Bayesian setting,
these two point estimates result from the use of different utility func-
tions (Berger, 1985). Here, we will focus on computing the latter,
i.e., the expected value of s. The expectation values cannot be eval—
uated analytically due to the cumbersome restriction of the design
matrix to those variables for which 5,- = +1. Moreover, although the
computation of the expectation values can be performed using
Monte Carlo methods (George and McCulloch, 1993; Li and
Zhang, 2010), the numerical calculations often take a long time to
converge for high-dimensional inference problems.

Our main result—which we call the BIA of the posterior distribu—
tion for feature selection—is that a second—order series expansion of
Equation (2) in 2_1 corresponds to an Ising model described by

n2 1
10gP2(S|Y) 2 — 2 [911/051 + — Z lid/2255i (3)
4’2 i 2 ii-z'aéi
with an error that is O 2‘3Tr[(XEXS)3]> where Tr[-] is the matrix
trace operator and the external fields and couplings are defined as

lot-<2) = Hm —  + 212(2) <4)
11,-<2) = Franc-mi) I

n 1 2 (5)
7 (maxim, we, xi) — Er u. mm, x»).
Here, r(z1,z2) is the Pearson correlation coefficient between vari—
ables z1 and zz. In writing this expression, we have assumed that the
hyperparameters do and b0 are small enough to neglect, though this
assumption is not necessary. A detailed derivation of this result is
presented in the Supporting Information.

The series expansion converges as long as 2" > Tr[(XSTXS)k] for
all s and integer powers 1221, which defines the regime that we call
‘strongly regularized’. Since X, is the restricted design matrix for
standardized data, we can relate Tr[(XSTXS)k] to the covariances be—
tween xj’s. In particular, Gershgorin’s Circle Theorem (Varga, 2010)
implies that the series will converge as long as 2 > n(1 + pf) where 17
: %inf ,- Zi#i|r(X,-, Xi)| (see Supporting Information). For large p, we

can re lace 17 b the root—mean—squared correlation between features,
r = (p—1(p — D4: r2(X,-, X,). This defines a natural scale
(75].

rznu+pn. (@

for the penalty parameter at which the BIA is expected to break—
down. We expect the BIA to be accurate when 2 >> 2* and to break—
down when 2 << 2*.

Because higher—order terms in the series can be neglected, the
strongly regularized expansion allows us to remove any references

112 /810's112umo [p.IOJXO'SOleIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

High-dimensional linear regression via the BIA

1757

 

to the restricted design matrix, and maps the posterior distribution
to the Ising model, which has been studied extensively in the physics
literature. Moreover, the magnitude of the couplings (],~,~) scales as
2_1, ensuring that the couplings are weak, which will allow us to
compute posterior probabilities analytically. To perform feature se—
lection, we are interested in computing marginal probabilities
P),(s,- : 1 |y) 2 (1 + mi(2)) / 2, where we have defined the magnetiza—
tions mi(2) : (5)). While there are many techniques for calculating
the magnetizations of an Ising model, we focus on the mean field ap—
proximation which leads to a self—consistent equation (Opper and
Winther, 2001):

712
mm) = tanh [47 (Ia-(2) + ],-,-(2)m,-(2)>] . (7)

This mean field approximation provides a computationally efficient
tool that approximates Bayesian feature selection for linear regres-
sion, requiring only the calculation of the Pearson correlations and
solution of Equation (7).

2.2.2 Computing the feature selection path

As with other approaches to penalized regression, our expressions
depend on a free parameter (2) that determines the strength of the
prior distribution. As it is usually difficult, in practice, to choose a
specific value of 2 ahead of time it is often helpful to compute the
feature selection path; i.e. to compute mi(2) over a wide range of
2’s. Indeed, computing the variable selection path is a common prac—
tice when applying other feature selection techniques such as
LASSO regression. To obtain the mean field variable selection path
as a function of e = 1 / 2, we notice that lim E_,0 mi(e) : 0 and so de—
fine the recursive formula

(6 + 56ml <

mi(e + (36) % tanh 4

 

19,-(6 + 56) + % ZL‘AE + (Sam/(0)]
(751'

with a small step size 56 << 1/2* : n_1(1 +pr)_1. We have set 56
: 0.05 / 2* in all of the examples presented below. We note that our
implementation of the BIA is an example of homotopy algorithm
and could potentially be improved by applying more advanced
methods (Allgower and Georg, 2003).

2.2.3 Remarks
The BIA provides a computationally efficient framework to calcu—
late posterior probabilities of feature relevance as a function of 2
without Monte Carlo simulations. We have used a simple, unopti—
mized C++ implementation of the BIA method. Using this code,
computing the entire feature selection path for a genomics dataset
with almost 30000 features took ~15 min on a desktop computer
with 24 GB of RAM and two 2.4 GHZ 6—core Intel Xeon processors.
The bulk of the computational effort—in terms of both processing
power and memory usage—is expended computing the (p X p) cor—
relation matrix. For example, computing the feature selection path
for the genomics dataset with our naive implementation required
~15 GB of RAM. However, any method designed for efficiently
computing large correlation matrices could be applied to improve
the computational performance of the BIA. For example, adaptive
thresholding estimators could be used to obtain a sparse correlation
matrix that requires less memory (Cai and Liu, 2011). In any case,
we have left the optimization of the code for future research.

To first order in e : 2_1, the posterior distribution corresponds
to an Ising model with fields and couplings given by Io,- : 72(y,x,-)

—1/n and ],~,~ : 0. That is, the indicator variables representing feature
relevance are independent, and the probability that a feature is rele—
vant is only a function of its squared correlation with the response.
Specifically, mi(2)20 if |r(y,x,-)| > 1/\/ﬁ and mi(2)g0 if
|r(y,  < 1/ Therefore, the BIA demonstrates that methods
that rank features by their squared correlation with the response,
such as SIS (Fan and LV, 2008), are actually performing a first-order
approximation to Bayesian feature selection in the strongly regular—
ized limit.

The couplings between the spin variables representing feature
relevance enter into the BIA with the second—order term in e : 2_1.
A positive coupling between spins i and j favors models that include
both features 1' and 1', whereas a negative coupling favors models that
include one feature or the other, but not both. In general, the cou—
pling terms are negative for highly correlated variables which min—
imizes the redundancy of the feature set.

3 Examples

We have chosen three examples to illustrate different characteristics
of the BIA for Bayesian feature selection. (A) First, we consider re—
gression problems with p features that have a constant correlation r.
We present some simple analytic expressions in the large p limit that
illustrates how different aspects of the problem affect feature selec—
tion performance, and study some simulated data. (B) Next, we ana—
lyze a dataset on the prediction of bodyfat percentage from various
body measurements. The number of features ([9 : 12) is small
enough that we can compute the exact posterior probabilities and,
therefore, directly assess the accuracy of the BIA for these data. (C)
Finally, we demonstrate the applicability of the BIA for feature se—
lection on high-dimensional regression problems by examining a
dataset relating the expression of p = 28 395 genes to the susceptibil—
ity of soybean plants to a pathogen.

3.1 Features with a constant correlation

Correlations between features are detrimental to feature selection. For
example, suppose that we observe a response variable 3) given by
y : ﬂx1 + 17. A second feature x2 that is strongly correlated with x1
will also be correlated with 3). Thus, identifying which feature, x1 or
x2, is the relevant one is not an easy task. Of course, the reasoning be—
comes more complicated in high dimensions, but similar effects are
observed in high—dimensional regression with the LASSO (Tibshirani,
1996; Zou and Hastie, 2005 ). Given these observations, we use this
section to analyze a simple model of BIA feature selection that allows
us to examine many of the characteristics that inﬂuence feature selec—
tion performance. Specifically, we consider a simple, analytically
tractable, model in which we are given p features that are correlated
with each other with a constant Pearson correlation coefficient, r. The
response, 37, is a linear function of the first [fgp variables, which have

equal true regression coefficients ﬂl- : B for jgp”. That is, 37: ﬂ

 x,-+17~ where 17~~ N(0, 0:2) is a Gaussian noise. We are inter-

ested in studying the behavior of this model when the number of fea—
tures is large ([9 >> 1). To simplify analytic expressions, it is helpful to
define the number of samples as n : 0p, and the number of relevant
features as [f : ¢p. Furthermore, we assume that the correlation be—
tween features scales as r : tarp—1 so that the correlation between 3)
and x,- stays constant in the large p limit.

Figure 1a presents an example feature selection path computed
using the BIA for a simulation of this model. This variable selection
path was generated for data simulated from a linear model using

112 /810's112umo [p.IOJXO'SOleIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

1758

C. K.Fisher and P. Mehta

 

p :200 features with a constant correlation r = 2 / p, n = 100,
if: 10 and (02 : 0:2/ﬂ2 : 1. Figure 1a demonstrates that all but
one of the relevant features (red) have higher posterior probabilities
than the irrelevant features (black) as long as 2 > 2*. In fact, there is
a clear gap in posterior probability separating the relevant and ir—
relevant features, and the correct features can be easily selected by
visible inspection of the feature selection path in Fig. 1a. The BIA
breaks down beyond the threshold of the penalty parameter and the
feature selection performance of the BIA deteriorates, as demon—
strated by the mixing of the probabilities for the relevant (red lines)
and irrelevant (black lines) features in Fig. 1a.

The indicator variables characterizing the feature selection prob—
lem can be divided into two groups: relevant features with jgp” and
magnetization mm, and irrelevant features with j > [f and magnet—
ization m(_). Note that an algorithm that performs perfect variable
selection will have mm = +1 and m(_) : —1. The Pearson correl-
ation coefficient of a relevant feature (7:1)”) with the standardized re—

sponse y = 37/ ( /VAR(y~) is given by
1 + r(p”— 1)

r(y,x,'=1...p*) E 1’(Jr) 2 602 + p”(rp”+ 1 — 7’) a

where (02 262/62 N 0(1) is an inverse signal—to—noise ratio.

 

Similarly, the Pearson correlation coefficient of an irrelevant vari—
able (j > if) with the standardized response is

17f

7(3), xi=p~+1,...,p) E 71—) Z 602 + p”(rp”+ 1 — 1’) ’

Note that correlations make this problem incredibly difficult when

 

the number of true features is large, i.e. r(_) /r(+) —> 1 as p~—> 00 for
r> 0. If we choose 2 : sz to ensure that the problem is always in
the strongly regularized regime, the magnetizations can be computed
explicitly to order 1 /p giving

 

 

   

 

 

 

 

{ﬂim—
E 03—
ll GET.
3'" 04 -
If Relevant
cl"*‘Irrtlmrelant ;
oo-r I i I
11121 0.5 1.0 1.5 2 I} 2 5 3.1]
ma
2.0
{b}
1.5
we 1.0

[LE

 

 

 

“or: 0.5 to 1.5 an

dB

Fig. 1. Performance of BIA feature selection. (a) An example variable selection
path as a function of decreasing regularization. The relevant variables are
red, and the irrelevant variables are black. The dashed vertical line is at
2 = 2* = n(1+ rp), which is the estimated breakdown point of the approxima-
tion. Simulations were performed with p=200, n: 100, ~ pN =10, r = 2/p
and (02 = 1. (b) A phase diagram illustrating the regions of parameter space
where m(_) < 0 < mm computed with 2 2 6p2

8— 1— 01 1

4¢> p 17
N 1+ocd)—oc2d>01 1
 we)

In general, we say that feature selection performance is good, on
average, as long as m(_) < 0 < mm, because relevant features have
P(s,- : +1|y) > 1 / 2 and irrelevant features have
P(s,- : +1|y) < 1 / 2. Figure 1b shows that the average feature selec—
tion performance is good in this sense within a large volume of the
phase space. Specifically, m(_) < 0 < mm when

1 <9<1+a¢
1+ocd> d) (wjz‘

 

 

However, m(_) < mm even if the stronger statement m(_) < 0
< mm is not satisfied. As a result, there is always a gap between the
posterior probabilities of the relevant and irrelevant features.
Nevertheless, the gap between the relevant and irrelevant features
shrinks with increasing correlations, suggesting that feature selection
performance will be strongly affected by sample—to—sample ﬂuctu—
ations, which we have neglected here.

3.2 Bodyfat percentage

Bodyfat percentage is an important indicator of health, but obtain—
ing accurate estimates of bodyfat percentage is challenging. For ex-
ample, underwater weighing is one of the most accurate methods for
measuring bodyfat percentage but it requires special equipment, e.g.
a pool. Here, we analyze a well—known dataset obtained from
StatLib (http://lib.stat.cmu.edu/datasets/) on the relationship be—
tween bodyfat percentage and various body measurements from
112252 men (Penrose et 61]., 1985). The p = 12 features included in
our regression are age and body mass index (height/massz), as well
as circumference measurements of the neck, chest, waist, hip, thigh,
knee, ankle, upper arm, forearm and wrist. All of the data were
standardized to have mean 0 and variance 1. Therefore, there are
212 = 4096 potential combinations of features.

For our purposes, the most interesting part about the bodyfat
dataset is that the number of features is small enough to compute
the posterior probabilities exactly using Equation (2) by enumerat—
ing all of the 4096 feature combinations. The exact posterior proba—
bilities as a function of 2_1 are shown in Fig. 2a.The posterior
probabilities computed from recursive solution of the BIA are shown
in Fig. 2b. Comparing Fig. 2a with Fig. 2b demonstrates that the
posterior probabilities computed from the BIA are very accurate for
2 >> 2*, with 2* : n(1 + pr) and r the root—mean—squared correl—
ation between features. However, the approximation breaks down
for 2 << 2* as expected. Figure 2c provides another representation of
the breakdown of the BIA upon approaching the breakdown
point of the penalty (2*). The root—mean—squared error given by

RMSE<2>—\/p12<Pract<si—1Iy>PEIAci—llynz is sig-
i

 

moidal, with an inflection point close to 2*.

In the strongly regularized regime with 2 >> 2*, the exact
Bayesian probabilities and those computed using the BIA both rank
waist and chest circumference as the most relevant features. Below
the breakdown point of the penalty parameter, however, the BIA
suggests solutions that are too sparse. That is, it underestimates
many of the posterior probabilities describing whether or not the
features are relevant. Far below the breakdown point of the penalty
parameter (beyond the range of the graph in Fig. 2), the BIA ranks
age and body mass index as the most relevant variables even though

112 /810's112umo [p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

High-dimensional linear regression via the BIA

1759

 

these have some of the smallest correlations with the response.
Age and body mass index also become increasingly important for
small 2’s in the exact calculation; though, they are never ranked as
the most relevant variables. The change in the rankings of the fea-
tures as a function of 2 highlights the importance of the coupling
terms (Ly-(2)) that punish correlated features.

3.3 Gene expression

In 2010, the Dialogue for Reverse Engineering Assessments and
Methods (DREAM) (Prill et al., 2010) initiative issued a challenge
to predict the response of soybean plants to a pathogen from data
on gene expression (Zhou et al., 2009). These DREAM5 training
data consist of a response of 112200 different soybean plants to a
pathogen (specifically, the response is a measure of the amount of
pathogen in an infected tissue sample) along with the expressions of
p228 395 genes. The team (Loh et al., 2011) that achieved the
highest rank correlation on a blind test set of 30 other soybean
plants trained their model using elastic net regression to predict the
ranks of the responses in the training set. The ranks were used rather
than the actual values of the responses to mitigate the effects of out—
liers, and the value of the penalty parameter was chosen using cross
validation. Loh et al. (2011) found that their cross—validation pro—
cedure for elastic net regression favored sparse models with only a
few features, and they highlighted 12 of these features that were fre—
quently chosen by their procedure. However, even the best teams
achieved only modest performance on the test data (Loh et al.,
2011). Nevertheless, the soybean gene expression dataset presents a
good benchmark to compare Bayesian feature selection with the BIA
to feature selection using cross—validated penalized regression for a
very high-dimensional inference problem.

(a) [In HR] a-‘-lt:e. (JP'I. age. W'et, ferea'ln. nest. ups-er arm. tree. Hg“ 11 [1 real use 5'. {red}

 

 

 

 

 

 

 

 

 

 

 

1.5 2.0 2 5 3.0
II'M

 

FIJI

 

FEMS-E

    

I
(ll) 11.5- 1.!) 1.5 2.1} 2 5- 3.0

II'IJI

 

 

 

Fig.2. Comparison of exact Bayesian marginal probabilities to the BIA for the
bodyfat data. (a) Exact Bayesian marginal probabilities for decreasing regu-
larization. (b) BIA approximations of the marginal probabilities for decreasing
regularization. (c) RMSE between the exact and BIA probabilities as a function
of decreasing regularization. The dashed vertical line is at 2 = 2* = n(1 + rp),
which is the estimated breakdown point of the approximation. The variables
have been color coded (blue to red) by increasing squared Pearson correl-
ation coefficient with the response (bodyfat percentage)

We used the BIA to compute the posterior probabilities for all
p228 395 features as a function of 2_1 using the ranks of the re-
sponses of the soybean plants to the pathogen as our 3) variable. As
before, all of the data were standardized to have mean 0 and vari—
ance 1. Figure 3a compares the posterior probabilities of the 12 fea—
tures highlighted by Loh et al. (2011) (red lines) to the distribution
of posterior probabilities for all of the features (gray area). Visual
inspection of Fig. 3a suggests that the 12 features identified by Loh
et al. (2011) have some of the highest posterior probabilities among
all 28 395 features. Similarly, Fig. 3b shows that only a small per—
centage of features have higher posterior probabilities than those
identified by Loh et al. (2011), demonstrating that there is generally
a pretty good agreement between features that are predictive (i.e.
those that perform well in cross validation) and those with high pos—
terior probabilities computed with the BIA.

Although our analyses of the soybean gene expression data iden—
tify similar features as cross—validated elastic net regression, the pos—
terior probabilities all fall in the range PMs/(y) : 1/2i0.001. The
small range of posterior probabilities around the value representing
random chance (Pg(s,-|y) : 1/2) is consistent with the highly vari-
able out—of—sample performance discussed by Loh et al. (2011). One
reason for the generally poor performance of feature selection on
these data, aside from the underdetermined nature of the problem, is
that the expressions of the genes are significantly correlated
(r m 0.29). To demonstrate this, we constructed synthetic datasets
with varying numbers of relevant and irrelevant genes and computed
the rate at which true features were identified by the BIA (Fig. 4 and
Supporting Information). Like the original data, these synthetic
datasets each contained 11 :200 distinct samples. The true positive
rate (or sensitivity) was defined as the fraction of true features
among the q features with the highest BIA posterior probabilities at
2 = 0.5 2*. Comparing the true positive rates of BIA feature selection
on synthetic data using genes with a strong correlation (r m 0.28,
Fig. 4a) and synthetic data with a weak correlation obtained by ran—
domly shufﬂing the genes (r m 0.07, Fig. 4b) clearly demonstrates
the dramatic effect that interfeature correlations have on feature se—
lection performance. This highlights the importance of strong

 

0.5006

g

0.500?

 

Pitta, - IIIrI
‘5?

 

 

 

o ease I i I I I I
U {I G 5 'I [I 1 El 2.” 2.5 3J1

 

ET
:3:-
3;?

 

1g 13— tﬁean
_ Median

Ln
«:1

in :5
I

ngher Rani-Ilng
Fee-tures [1%)
a. .-

 

III
-I-
I

 

 

 

l I l I I 1
I10 [II 5 1 El- 1 5 2.1!} 2.5 3 I3

Eli.

 

Fig. 3. Feature selection path for the gene expression data. The problem is se-
verely under-determined, involving the prediction of a quantitative pheno-
type from the expressions of p: 28 395 genes given a sample size of n: 200
and, therefore, the posterior probabilities remain close to P;_(s,- = 1|y) = 1/2.
(a) Features selected in a previous study (red lines) by cross validation with
the elastic net have high ranking posterior probabilities. Gray area represents
the 1—99% quantiles, and the black area represents the 25—75% quantiles. (b)
The median (solid black line) and mean (dashed red line) percentage of fea-
tures with higher posterior probabilities than those identified by Loh et al.
(2011). The vertical axis is a logarithmic scale. The dashed vertical line is at
the breakdown point2 = 2* = n(1+ rp)

112 /810's112umo [p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

1760

C. K.Fisher and P. Mehta

 

regularization procedures that specifically account for correlation
between genes in high—dimensional genomic studies.

4 Discussion

To summarize, we have shown that Bayesian feature selection for
L2 penalized regression, in the strongly regularized regime, corres—
ponds to an Ising model, which we call the BIA. Mapping the poster—
ior distribution to an Ising model that has simple expressions for the
local fields and couplings using a controlled approximation opens
the door to analytical studies of Bayesian feature selection using the
vast number of techniques developed in physics for studying the
Ising model. It will be interesting to see if our analyses can be gener—
alized to study Bayesian feature selection for many statistical tech—
niques other than linear regression, as well as other prior
distributions. From a practical standpoint, the BIA provides an algo—
rithm to efficiently compute Bayesian feature selection paths for L2
penalized regression. Using our approach, it is possible to compute
posterior probabilities of feature relevance for very high-dimen-
sional datasets such as those typically found in genomic studies.

Unlike most previous work of feature selection, the BIA is ideally
suited for large genomic datasets where the number of features can
be much greater than the sample size, p >> n. The underlying reason
for this is that we work in strongly regularized regime where the
prior always has a large inﬂuence on the posterior probabilities.
This is in contrast to previous works on penalized regression and
related Bayesian approaches that have focused on the ‘weakly regu—
larized regime’ where the effect of the prior is assumed to be small.
Moreover, we have identified a sharp threshold for the regulariza-
tion parameter 2* : n(1 + pr) where the BIA is expected to break
down. This threshold depends on the sample size, n, number of fea—
tures, p, and root—mean—squared correlation between features, r. The
threshold at which the BIA breaks down occurs precisely at the tran—
sition from the strongly regularized to the weakly regularized re—
gimes where the prior and the likelihood have a comparable
inﬂuence on the posterior distribution.

This study also highlights the importance of accounting for cor—
relations between features when assessing statistical significance in
large datasets. When the number of features is large, even small cor—
relations can cause a huge reduction in the posterior probabilities of
features. For example, our analysis of a dataset including the expres—
sion of 28 395 genes demonstrates that the resulting posterior proba—
bilities of gene relevance may be very close to value representing
random chance PMs/(y) : 1 / 2 when p >> n and the genes are moder—
ately correlated, e.g. r m 0.29. This is likely to have important

.--.
E'.

EDITEHDEIIIGBME _ UFGEITE'EHEIII EH13!

       

Innue-

E E wool
a 11 m . I:
i a man. M g
... ... 50121121.. :1? II:
E E some 3:: E
E E quu._ m a
5 E :Iooo.  E
3 anon. on

In men. '19

Ibihjuq'nshmmtineumo'
Hunter a! True Features. [q]

me. an In an en rt: Eiu some
manner a“ True Fealures [ﬁll

Fig. 4. True positive rates for feature selection with (a) correlated and (b)
uncorrelated genes. Features were selected by taking the q genes with high-
est posterior probability at 2 = 0.52*. The uncorrelated genes were created by
randomly shuffling the correlated genes. The root mean squared correlation
among the correlated genes was r: 0.28 compared with r: 0.7 for the uncor-
related genes

implications for assessing the results of GWAS studies where such
correlations are often ignored.

Moreover, we suggest that it is generally not reasonable to
choose a posterior probability threshold for judging significance on
very high-dimensional problems. Instead, the BIA can be used as
part of a two—stage procedure, where the BIA is applied to rapidly
screen irrelevant variables, i.e. those that have low rank in posterior
probability, before applying a more computationally intensive cross—
validation procedure to infer the regression coefficients. The compu—
tational efficiency of the BIA and the existence of a natural threshold
for the penalty parameter where the BIA works make this procedure
ideally suited for such two—stage procedures.

Acknowledgements
We thank Alex Lang, Javad Noorbakhsh and David Schwab for comments on

this manuscript.

Funding

RM. and C.K.F. were supported by a Sloan Research Fellowship and a
Simons Investigator Award [to P.M.], and internal funding from Boston
University.

Conﬂict of Interest: none declared.

References

Allgower,E.L. and Georg,K. (2003) Introduction to Numerical Continuation
Methods, Vol. 45. SIAM, Philadelphia.

Balasubramanian,V. (1997) Statistical inference, Occam’s razor, and statis-
tical mechanics on the space of probability distributions. Neural Comput.,
9, 349—36 8.

Berger,].O. (1985) Statistical Decision Theory and Bayesian Analysis.
Springer, New York.

Burton,P.R. et al. (2007) Genome-wide association study of 14,000 cases of
seven common diseases and 3,000 shared controls. Nature, 447, 661—678.
Cai,T. and Liu,W. (2011) Adaptive thresholding for sparse covariance matrix

estimation. ]. Am. Stat. Assoc., 106, 672—684.

Candes,E. and Tao,T. (2007) The dantzig selector: statistical estimation when
p is much larger than n. Ann. Stat., 35, 2313—2351.

Ding,C. and Peng,H. (2005) Minimum redundancy feature selection from
microarray gene expression data. ]. Bioinform. Comput. Biol., 3, 185—205.
Fan,]. and LV,]. (2008) Sure independence screening for ultrahigh dimensional

feature space]. R. Stat. Soc. B, 70, 849—911.

Gelman,A. et al. (2013) Bayesian Data Analysis. CRC Press, London.

George,E.I. and McCulloch,R.E. (1993) Variable selection Via Gibbs sam-
pling. ]. Am. Stat. Assoc., 88, 881—8 89.

Ghosh,M. et al. (2011) Objective priors: an introduction for frequentists. Stat.
Sci., 26, 187—202.

Guan,Y. et al. (2011) Bayesian variable selection regression for genome-wide
association studies and other large-scale problems. Ann. Appl. Stat., 5,
1 78 0—1 8 1 5.

Guyon,I. and Elisseeff,A. (2003) An introduction to variable and feature selec-
tion.]. Mac/7. Learning Res., 3, 1157—1182.

Hoerl,A.E. and Kennard,R.W. (1970) Ridge regression: biased estimation for
nonorthogonal problems. Technometrics, 12, 55—67.

Huo,X. and Ni,X. (2007) When do stepwise algorithms meet subset selection
criteria? Ann. Stat., 35, 870—887.

Legendre,A.M. (1805) Nouvelles Méthodes Pour la Determination des
Orbites des Cometes. F. Didot, Paris.

Li,F. and Zhang,N.R. (2010) Bayesian variable selection in structured high-di-
mensional covariate spaces with applications in genomics. ]. Am. Stat.
Assoc., 105, 1202—1214.

112 /810'S[12umo IpJOJXO'SOTlBIIlJOJUTOTQ/ﬂClllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

High-dimensional linear regression via the BIA

1761

 

Loh,P.-R. et al. (2011) Phenotype prediction using regularized regression on
genetic data in the dream5 systems genetics b challenge. PLoS ONE, 6,
e29095.

Malzahn,D. and Opper,M. (2005 ) A statistical physics approach for the ana-
lysis of machine learning algorithms on real data. ]. Stat. Mech.: Theory
Exp, 2005, P11001.

McCarthy,M.I. et al. (2008) Genome-wide association studies for complex traits:
consensus, uncertainty and challenges. Nat. Rev. Genet., 9, 356—369.

Mézard,M. et al. (2002) Analytic and algorithmic solution of random satisﬁ-
ability problems. Science, 297, 812—815.

Monasson,R. et al. (1999) Determining computational complexity from char-
acteristic phase transitions. Nature, 400, 133—137.

Nemenman,I. and Bialek,W. (2002) Occam factors and model independent
Bayesian learning of continuous distributions. Phys. Rev. E, 65, 026137.

O’Hagan,A. et al. (2004) Bayesian Inference. Arnold, London.

Opper,M. and Winther,O. (2001) 2 from naive mean ﬁeld theory to the tap
equations. In: Advanced Mean Field Methods: Theory and Practice,
Opper,M. and Saad,D. (eds), MIT Press, Cambridge, MA, pp. 7—20.

Peng,]. et al. (2010) Regularized multivariate regression for identifying master
predictors with application to integrative genomics study of breast cancer.
Ann. App]. Stat., 4, 53.

Penrose,K.W. et al. (1985 ) Generalized body composition prediction equa-
tion for men using simple measurement techniques. Med. Sci. Sports Exerc., 17,
189.

Prill, R.]. et al. (2010) Towards a rigorous assessment of systems biology mod-
els: the dream3 challenges. PLoS ONE, 5, e9202.

Subramanian,A. et al. (2005) Gene set enrichment analysis: a knowledge-
based approach for interpreting genome-wide expression proﬁles. Proc.
Natl Acad. Sci. USA, 102, 15545—15550.

Tibshirani,R. (1996) Regression shrinkage and selection Via the lasso. ]. R.
Stat. Soc. B, 58, 267—288.

Varga,R.S. (2010) Gersgorin and His Circles. Springer Science 85 Business,
Heidelberg.

Wu,T.T. et al. (2009) Genome-wide association analysis by lasso penalized lo-
gistic regression. B ioinformatics, 25, 714—721.

Yang,]. et al. (2012) Conditional and joint multiple-SNP analysis of GWAS
summary statistics identiﬁes additional variants inﬂuencing complex traits.
Nat. Genet., 44, 369—375.

Zhou,L. et al. (2009) Infection and genotype remodel the entire soybean tran-
scriptome. BMC Genomics, 10, 49.

Zou,H. and Hastie,T. (2005) Regularization and variable selection Via the
elastic net. ]. R. Stat. Soc. B, 67, 301—320.

112 /810's112umo [p.IOJXO'SOTlBIIIJOJUTOTQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

