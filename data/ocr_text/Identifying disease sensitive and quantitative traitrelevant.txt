Vol. 28 ISMB 2012, pages i127—i136
doi: 10. 1093/bioinformatics/bts228

 

Identifying disease sensitive and quantitative trait-relevant
biomarkers from multidimensional heterogeneous imaging
genetics data via sparse multimodal multitask learning

Hua Wang‘, Feiping Nie‘, Heng Huang”: Shannon L. Risacherz, Andrew J. Saykin2 and
Li Shen2’*; For the Alzheimer’s Disease Neuroimaging InitiativeT

1Department of Computer Science and Engineering, University of Texas at Arlington, TX 76019 and 2Department
of Radiology and Imaging Sciences, Indiana University School of Medicine, Indianapolis, IN 46202, USA

 

ABSTRACT

Motivation: Recent advances in brain imaging and high-throughput
genotyping techniques enable new approaches to study the
influence of genetic and anatomical variations on brain functions
and disorders. Traditional association studies typically perform
independent and pairwise analysis among neuroimaging measures,
cognitive scores and disease status, and ignore the important
underlying interacting relationships between these units.

Results: To overcome this limitation, in this article, we propose
a new sparse multimodal multitask learning method to reveal
complex relationships from gene to brain to symptom. Our main
contributions are three-fold: (i) introducing combined structured
sparsity regularizations into multimodal multitask learning to integrate
multidimensional heterogeneous imaging genetics data and identify
multimodal biomarkers; (ii) utilizing a joint classification and
regression learning model to identify disease-sensitive and cognition-
relevant biomarkers; (iii) deriving a new efficient optimization
algorithm to solve our non-smooth objective function and providing
rigorous theoretical analysis on the global optimum convergency.
Using the imaging genetics data from the Alzheimer’s Disease
Neuroimaging Initiative database, the effectiveness of the proposed
method is demonstrated by clearly improved performance on
predicting both cognitive scores and disease status. The identified
multimodal biomarkers could predict not only disease status but also
cognitive function to help elucidate the biological pathway from gene
to brain structure and function, and to cognition and disease.
Availability: Software is publicly available at: http://ranger.uta.edu/
%7eheng/multimodal/

Contact: heng@uta.edu; shen|i@iupui.edu

1 INTRODUCTION

Recent advances in acquiring multimodal brain imaging and
genome—wide array data provide exciting new opportunities to study
the inﬂuence of genetic variation on brain structure and function.
Research in this emerging ﬁeld, known as imaging genetics, holds
great promise for a system biology of the brain to better understand
complex neurobiological systems, from genetic determinants to
cellular processes to the complex interplay of brain structure,
function, behavior and cognition. Analysis of these multimodal

 

*To whom correspondence should be addressed.
TData for the Alzheimer’s Disease Neuroimaging Initiative are provided in
the Acknowledgement section.

datasets will facilitate early diagnosis, deepen mechanistic
understanding and improved treatment of brain disorders.

Machine learning methods have been widely employed to predict
Alzheimer’s disease (AD) status using imaging genetics measures
(Batmanghelich et al., 2009; Fan et al., 2008; Hinrichs et al.,
2009b; Shen et al., 2010a). Since AD is a neurodegenerative
disorder characterized by progressive impairment of memory
and other cognitive functions, regression models have also been
investigated to predict clinical scores from structural, such as
magnetic resonance imaging (MRI), and/or molecular, such as
ﬂuorodeoxyglucose positron emission tomography (FDG—PET),
neuroimaging data (Stonnington et al., 2010; Walhovd et al., 2010).
For example, Walhovd et al. (2010) performed stepwise regression
in a pairwise fashion to relate each of MRI and FDG—PET measures
of eight candidate regions to each of four Rey’s Auditory Verbal
Learning Test (RAVLT) memory scores. This univariate approach,
however, did not consider either interrelated structures within
imaging data or those within cognitive data. Using relevance vector
regression, Stonnington et al. (2010) jointly analyzed the voxel—
based morphometry (VBM) features extracted from the entire brain
to predict each selected clinical score, while the investigations of
different clinical scores are independent from each other.

One goal of imaging genetics is to identify genetic risk factors
and/or imaging biomarkers via intermediate quantitative traits (QTs,
e.g. cognitive memory scores used in this article) on the chain
from gene to brain to symptom. Thus, both disease classiﬁcation
and QT prediction are important machine learning tasks. Prior
imaging genetics research typically employs a two—step procedure
for identifying risk factors and biomarkers: one ﬁrst determines
disease—relevant QTs, and then detects the biomarkers associated
with these QTs. Since a QT could be related to many genetic or
imaging markers on different pathways that are not all disease
speciﬁc (e.g. QT 2 and Gene 3 in Fig. 1), an ideal scenario would
be to discover only those markers associated with both QT and
disease status for a better understanding of the underlying biological
pathway speciﬁc to the disease.

On the other hand, identifying genetic and phenotypic biomarkers
from large—scale multidimensional heterogeneous data is an
important biomedical and biological research topic. Unlike
simple feature selection working on a single data source,
multimodal learning describes the setting of learning from
data where observations are represented by multiple types of
feature sets. Many multimodal methods have been developed
for classiﬁcation and clustering purposes, such as co—training
(Abney, 2002; Brefeld and Scheffer, 2004; Ghani, 2002; Nigam

 

© The Author(s) 2012. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution Non—Commercial License (http://creativecommons.org/Iicenses/
by—nc/3.0), which permits unrestricted non—commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.

112 /§JO'SIBUJHO[p.IOJXO'SOTlBIHJOJUTOTQ/ﬁdnq mm; pop1201um0q

9IOZ ‘09 lsnﬁ'nv uo ::

H. Wang et aI.

 

r _ _‘_ _ __'

I I "-l

 

 

 

Details about CITE

 

 

Sane 1| SE” 2 fl gel"? 3 Gene 3 encodes normal variation
i  -::" _ I .: l
gm 1 '1‘ GTE; 1' {1T 3: Norma-l Dis-Ease
,ZF'“  -1 , II . 1
. - . I. I . l.‘ I I . T .

I IDIsease Symptom I | Nurmalhi‘arlatlun Gene 2 enmdﬁ normal m disease

I l

-. _ .u' -_ _ _ —.-

Fig. 1. A simpliﬁed schematic example of two pathways from gene to QTs
to phenotypic endpoints: the red one is disease relevant while the blue one
yields only normal variation. Traditional two—stage imaging genetic strategy
identiﬁes QT 1 and QT 2 ﬁrst and then Genes 1, 2, 3. Our new method will
identify only disease relevant genes (i.e. Gene 1 and Gene 2); and Gene 3
would not be identiﬁed because it cannot be used to classify disease status

et al., 2000) and multiview clustering (Bickel and Scheffer, 2004;
Dhillon et al., 2003). However, they typically assume that the
multimodal feature sets are conditionally independent, which does
not hold in many real—world applications such as imaging genetics.
Considering different representations give rise to different kernel
functions, several Multiple Kernel Learning (MKL) approaches
(Bach et al., 2004; Hinrichs et al., 2009a; Kloft et al., 2008;
Lanckriet et al., 2004; Rakotomamonjy et al., 2007; Sonnenburg
et al., 2006; Suykens et al., 2002; Ye et al., 2008; Yu et al., 2010; Zien
and Ong, 2007) have been recently studied and employed to integrate
heterogeneous data and select multitype features. However, such
models train a single weight for all features from the same modality,
i.e. all features from the same data source are weighted equally,
when they are combined with the features from other sources. This
limitation often yields inadequate performance.

To address the above challenges, we propose a new
sparse multimodal multitask learning algorithm that integrates
heterogeneous genetic and phenotypic data effectively and
efﬁciently to identify disease—sensitive and cognition—relevant
biomarkers from multiple data sources. Different to LASSO
(Tibshirani, 1996), group LASSO (Yuan and Lin, 2006) and other
related methods that mainly ﬁnd the biomarkers correlated to each
individual QT (memory score), we consider predicting each memory
score as a regression task and select biomarkers that tend to play an
important role in inﬂuencing multiple tasks. A joint classiﬁcation
and regression multitask learning model is utilized to select the
biomarkers correlated to memory scores and disease categories
simultaneously.

Sparsity regularizations have recently been widely investigated
and applied to multitask learning models (Argyriou et al., 2007;
Kim and Xing, 2010; Micchelli et al., 2010; Obozinski et al., 2006,
2010; Sun et al., 2009). Sparse representations are typically achieved
by imposing non—smooth norms as regularizers in the optimization
problems. From the view of sparsity organization, we have two
types: (i) The ﬂat sparsity is often achieved by EO—norm or £1—norm
regularizer or trace norm in matrix/tensor completion. Optimization
techniques include LARS (Efron et al., 2004), linear gradient search
(Liu et al., 2009), proximal methods (Beck and Teboulle, 2009). (ii)
The structured sparsity is usually obtained through different sparse
regularizers such as £2,1—norm (Kim and Xing, 2010; Obozinski
et al., 2010; Sun et al., 2009), ﬁlo—norm (Luo et al., 2010),
£00,1—norm (Quattoni et al., 2009) (also denoted as £1,2—norm, £1,00—
norm in different papers) and group £1—norm (Yuan and Lin, 2006)
which can be solved by methods in Micchelli et al. (2010) and
Argyriou et al. (2008). We propose a new combined structured sparse

    

I - . . . I'D |: In:
I Genetlc'u'arlatlons: 4—- D :1 . .
'1' u— H
 AlzGeneCandidatL-s CI 4; E DIEEHDSIS
I I ’- 3 WI
-| ‘Q g I: U
'I- 31 III-
“:  
E a h .e
if: ' x. [a
Structurall'u'lm: E —I*/ I:
1: I: '1
UBMMeasures I: D ,1,
E 'G .3:
[IV-=3: I,— " r:
t E E:
:I be
u a
E n: u

 

     

Fig. 2. The proposed sparse multimodal multitask feature selection method
will identify biomarkers from multimodal heterogeneous data resources. The
identiﬁed biomarkers could predict not only disease status, but also cognitive
functions to help researchers better understand the underlying mechanism
from gene to brain structure and function, and to cognition and disease

regularization to integrate features from different modalities and to
learn a weight for each feature leading to a more ﬂexible scheme for
feature selection in data integration, which is illustrated in Figure 3.
In our combined structured sparse regularization, the group £1—
norm regularization (blue circles in Fig. 3) learns the feature global
importance, i.e. the modal—wise feature importance of every data
modality on each class (task), and the £2,1—norm regularization (red
circles in Fig. 3) explores the feature local importance, i.e. the
importance of each feature for multiple classes/tasks. The proposed
method is applied to identify AD—sensitive biomarkers associated
with the cognitive scores by integrating heterogeneous genetic and
phenotypic data (as shown in Fig. 2). Our empirical results yield
clearly improved performance on predicting both cognitive scores
and disease status.

2 IDENTIFYING DISEASE SENSITIVE AND
QT—RELEVANT BIOMARKERS FROM
HETEROGENEOUS IMAGING GENETICS DATA

Pairwise univariate correlation analysis can quickly provide
important association information between genetic/phenotypic data
and QTs. However, it treats the features and the QTs as
independent and isolated units, therefore the underlying interacting
relationships between the units might be lost. We propose a new
sparse multimodal multitask learning model to reveal genetic and
phenotypic biomarkers, which are disease sensitive and QT—relevant,
by simultaneously and systematically taking into account an
ensemble of SNPs (single nucleotide polymorphism) and phenotypic
signatures and jointly performing two heterogeneous tasks, i.e.
biomarker—to—QT regression and biomarker—to—disease classiﬁcation.
The QTs studied in this article are the cognitive scores.

In multitask learning, given a set of input variables (i.e. features
such as SNPs and MRI/PET measures), we are interested in learning
a set of related models (e.g. relations between genetic/imaging
markers and cognitive scores) to predict multiple outcomes (i.e.
tasks such as predicting cognitive scores and disease status). Because
these tasks are relevant, they share a common input space. As a
result, it is desirable to learn all the models jointly rather than
treating each task as independent and ﬁtting each model separately,
such as Lasso (Tibshirani, 1996) and group Lasso (Yuan and Lin,
2006). Such multitask learning can discover robust patterns (because
signiﬁcant patterns in a single task could be outliers for other tasks)
and potentially increase the predictive power.

 

i128

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

Multidimensional imaging genetics data integration

 

In this article, we write matrices as uppercase letters and vectors
as boldface lowercase letters. Given a matrix W = [wlj], its i—th row

and j—th column are denoted as wi and wj, respectively. The £2,1—

norm of the matrix W is deﬁned as ||W||2,1=Zi=1||wi||2 (also
denoted as £1,2—norm by other researchers).

2.1 Heterogeneous data integration via combined
structured sparse regularizations

First, we will systematically propose our new multimodal learning
method to integrate and select the genetic and phenotypic biomarkers
from large—scale heterogeneous data. In the supervised learning
setting, we are given n training samples {(xi,yi)};‘=1, where X):
(xi1 ,  ,X§)T 6 Std is the input vector including all features from a
total of k different modalities and each modality j has dj features
(d = ELI dj). yi 6 SEC is the class label vector of data point X,- (only
one element in yi is 1, and others are zeros), where c is the number
of classes (tasks). LetX = [x1,--- ,Xn] EERdX” and Y: [y1 ,  ,yc] 6
SEC X”. Different to MKL, we directly learn a d X c parameter matrix
as:

1 1
w1 . WC d
W: . . . . . . . .. em X6, (1)
k k
w1 .. WC

where wg 65qu indicates the weights of all features in the q—th
modality with respect to the p—th task (class). Typically, we can use
a convex loss function £(X, W) to measure the loss incurred by W
on the training samples. Compared with MKL approaches that learn
one weight for one kernel matrix representing one modality, our
method will learn the weight for each feature to capture the local
feature importance. Since the features come from heterogeneous
data sources, we impose the regularizer R(W) to capture the
interrelationships of modalities and features as:

n‘1}[I/D£(X,W)+)/R(W), (2)

where y is a trade—off parameter. In heterogeneous data fusion,
from multiview perspective of view, the features of a speciﬁc view
(modality) can be more or less discriminative for different tasks
(classes). Thus, we propose a new group £1—norm (G1—norm) as a
regularization term in Equation (2), which is deﬁned over W as
following:

6 k _
IIWIIG,=ZZII“{.II2, (3)

i=1j=1

which is illustrated by the blue circles in Figure 3. Then the
Equation (2) becomes:

néii/n£(X,W)+i/1IIWIIG,- (4)

Since the group £1—norm uses Ez—norm within each modality and E 1 —
norm between modalities, it enforces the sparsity between different
modalities, i.e. if one modality of features are not discriminative
for certain tasks, the objective in Equation (4) will assign zeros
(in ideal case, usually they are very small values) to them for
corresponding tasks; otherwise, their weights are large. This new
group £1—norm regularizer captures the global relationships between
data modalities.

 

 

k types of d features

 

4—c tasks—>

 

Fig. 3. Illustration of the feature weight matrix WT. The elements in matrix
with deep blue color have large values. The group £1—n0rm (G1—norm)
emphasizes the learning of the group-wise weights for a type of features
(e.g. all the SNPs features, or all the MRI imaging features, or all the
FDG—PET imaging features) corresponding to each task (e. g. the prediction
for a disease status or a memory score) and the Egg—norm accentuates the
individual weight learning cross multiple tasks

However, in certain cases, even if most features in one modality
are not discriminative for the classiﬁcation or regression tasks, a
small number of features in the same modality can still be highly
discriminative. From the multitask learning point of view, such
important features should be shared by all/most tasks. Thus, we
add an additional £2,1—norm regularizer into Equation (4) as:

11%{i/n£(X,W)+)/1 IIWIIG1+V2IIWII2,1- (5)

The £2,1—norm was popularly used in multitask feature selection
(Argyriou et al., 2008; Obozinski et al., 2010). Since the £2,1—norm
regularizer impose the sparsity between all features and non— sparsity
between tasks, the features that are discriminative for all tasks will
get large weights.

Our regularization items consider the heterogeneous features from
both group—wise and individual viewpoints. Figure 3 visualizes the
matrix WT as a demonstration. In Figure 3, the elements with deep
blue color have large values. The group £1—norm emphasizes the
group—wise weights learning corresponding to each task and the
£2,1—norm accentuates the individual weight learning cross multiple
tasks. Through the combined regularizations, for each task (class),
many features (not all of them) in the discriminative modalities
and a small number of features (may not be none) in the non—
discriminative modalities will learn large weights as the important
and discriminative features.

The multidimensional data integration has been increasingly
important to many biological and biomedical studies. So far, the
MKL methods are most widely used. Due to the learning model
deﬁciency, the MKL methods cannot explore both modality—wise
importance and individual importance of features simultaneously.
Our new structured sparse multimodal learning method integrates
the multidimensional data in a more efﬁcient and effective way. The
loss function £(X , W) in Equation (8) can be replace by either least
square loss function or logistic regression loss function to perform
regression/classiﬁcation tasks.

2.2 Joint disease classiﬁcation and QT regression

Since we are interested in identifying the disease—sensitive and
QT—relevant biomarkers, we consider performing both logistic
regression for classifying disease status and multivariate regression
for predicting cognitive memory scores simultaneously (Wang
et al., 2011). A similar model was used in Yang et al. (2009) for

 

i129

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOICI/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

H. Wang et aI.

 

heterogeneous multitask learning. Regular multitask learning only
considers homogeneous tasks such as regression or classiﬁcation
individually. Joint classiﬁcation and regression can be regarded as a
learning paradigm for handling heterogeneous tasks.

First, logistic regression is used for disease classiﬁcation, which
minimizes the following loss function:

n 61 Cr
T .
£1(W)=E E yik10gE 6W1 X‘ —yi'kW/{Xi - (6)
i=1k=1 [:1

Here, we perform three binary classiﬁcation tasks for the following
three diagnostic groups respectively (c1=3): AD, mild cognitive
impairment (MCI), and health control (HC).

Second, we use the traditional multivariate least squares
regression model to predict memory scores. Under the regression
matrix P E Eﬁdxcz, the least squares loss is deﬁned by

£200): HXTP—ZHi, (7)

where X is the data points matrix, P is the coefﬁcient
matrix of regression with c2 tasks, the label matrix Z:

[(Zl>T, (22>T,,,.,(zn)T:|T EmnXCZ.

We perform the joint classiﬁcation and regression tasks, the
disease—sensitive and QT—relevant biomarker identiﬁcation task can
be formulated as the following objective:

n 61 Cr
C T I
mvn  : yiklogE 6W1 X'—yikW/{ Xi
T 2
—I—HX P—ZHF1—y1 ||VIIGI+y2IIVII2,1,

where V = [W P] 6 Std X(Cl +62). As a result, the identiﬁed biomarkers
will be correlated to memory scores and also be discriminative to
disease categories.

Since the objective in Equation (8) is a non—smooth problem
and cannot be easily solved in general, we derive a new efﬁcient
algorithm to solve this problem in the next subsection.

2.3 Optimization algorithm

We take the derivatives of Equation (8) with respect to W and P
respectively, and set them to zeros, we have

 

841(W) Cl
8W +2y1;Diwi+2)/2DW=0, (9)
C2
2XXTP—2XZ—I—2y1 Z Dipi+2y2DP=0, (10)
i=Cj-I—I

where Di(15i5c1—I—c2) is a block diagonal matrix with the k—

th diagonal block as WI;C (1k is a dk by dk identity matrix),
i 2

D is a diagonal matrix with the k—th diagonal element as 
Since Di(15i5c1—I—c2) and D depend on V=[W P], they are
also unknown variables to be optimized. In this article, we provide
an iterative algorithm to solve Equation (8). First, we guess a
random solution VEERdX(Cl+C2), then we calculate the matrices
Di(1 5 i 5 c1 —I—c2) and D according to the current solution V. After

obtaining the Di(1 5 i 5 c1 —I—c2) and D, we can update the solution
V = [ W P] based on Equation (9). Speciﬁcally, the i—th column of
P is updated by p,- = (XX T —I— y1Dl- —I— yzD)_1Xz,-. We cannot update
W with a closed form solution based on Equation (9), but we can
obtained the updated W by the Newton’s method. According to
Equation (9), we need to solve the following problem:

Ci
nvnvn £1(W)—I—)/1waDiwi—I—WTKWTDW). (11)
i=1
Similar to the traditional method in the logistic regression
(Krishnapuram et al., 2005; Lee et al., 2006), we can use the
Newton’s method to obtain the solution W.

For the ﬁrst term, the traditional logistic regression derivatives
can be applied to get the ﬁrst—and second—order derivatives (Lee
et al., 2006).

For the second term, the ﬁrst—and second—order derivatives are

Ci
3 Z Wl-TDl'Wi
i=1
BWMP = 2Dp(“a “)Wup 7
(12)
Cr
3 Z Wl-TDl'Wi
i=1
— =2D , 6 6 ,
BWMPBWW] pm 14) av pq
where Dp(a, a) is the a—th diagonal element of Dp.
For the third term, the ﬁrst—and second—order derivatives are
8Tr(WTDW)
— =2D(u, bowl...
T (13)
BT W DW

After obtaining the updated solution V = [ W P], we can calculate
the new matrices Di(1 5i 5 c1 —I—c2) and D. This procedure is
repeated until the algorithm converges. The detailed algorithm is
listed in Algorithm 1. We will prove that the above algorithm will
converge to the global optimum.

2.4 Algorithm analysis

To prove the convergence of the proposed algorithm, we need a
lemma as follows.

LEMMA 1. For any vectors V and V0, we have the following

. l. _ _ “V”; < _ IIVoII§
inequa 11y- IIVIIZ '2||v0||2—“V0“2 2||Vo||2'

 

PROOF. Obviously, —(||v||2 — llvo “2)2 5 0, so we have

2 2 2
—(||V||2— ||V0||2) £0=>2||V||2 ||V0||2— ||V||2 E ||V0||2

 

 

“vii Ivong

=>||V||2— E ||V0|| — , (14)
2IIvoII2 2 2IIVoII2

which completes the proof. D

Then we prove the convergence of the algorithm, which is
described in the following theorem.

THEOREM 1. The algorithm decreases the objective value of problem
( 8) in each iteration.

 

i130

112 /§.IO'SIBUJHOIPJOJXO'SOllBIHJOJUIOlCI/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

Multidimensional imaging genetics data integration

 

PROOF. In each iteration, suppose the updated W is W, and the
updated P is P, then the updated V is V = [ W P]. From Step 3 in
the Algorithm 1, we know that:

Ci
£1(W)—I—)/1ZWKDiwi—I—yZTKWTDW)

i=1

C (15)
l
5 £1(W)+y1waDin—WTKWTDW).
i=1
According to Step 4, we have:
2 C2
HXTP — Y HF +y1;piTDipi+y2Tr(PTDP)
" (16)

2 C2
5 HXTP— Y HF +y1Zpl-TDipi—I—y2Tr(PTDP).
i=1

Based on the deﬁnitions of Di(1 5 i 5 c1 —I—c2) and D, and Lemma 1,
we have two following inequalities:

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

2 2
K K v]? K K V];
~k 2 k 2
Z Vi z-Z k 52 Vi z-Z k
_ _ 2 v. _ _ 2 V-
k—l k—l l 2 k—l k—l l 2
K K
~k ~T ~ k T
=> 2 Vl- 2—Vl Dlvl5Z Vl- ViDiVl
Cj‘I—Cz K Cl‘I—C2
~k ~T ~
=> V1 2 2 Vi _l/l Vi DiVi
i=1 [(21 1:1
Cj‘I—Cz K Cl‘I—C2
k T . .
5 vi  2 v,- 2—1/1  v,D.v.. (17)
1:1 [(21 1:1
and
2 2
d d If] d d M
~"II 2 II "II 2
v — < v —
2‘ 2  —Z 2 zzlvk”
k=1 k=1 2 k=l k=l 2
d
=> )9th —)/2Tr(VTDV)
k=l
d
5 yZZHVkHZ—szr(VTDV). (18)

Note that the following two equalities:

Cj‘I—Cz C1

C2
T T T
Z V,- DiVi = 2W,- Diwi +21),- Dipi,
i=1 i=1 i=1

Tr(VTDV)= Tr(WTDW)+Tr(PTDP),

(19)

 

Algorithm 1 An efﬁcient iterative algorithm to solve the optimization
problem in Equation (8).

 

Input: X=[x1,x2,---,xn]eiiidx”, Y:

[(Y1>T,(y2>T,-~,(y”)T]T€{0,1}”XCI and Z:
[(Zl>T, (22>T,,.. , (Zn)T:|T Emnxcz.

Output: W e Eﬁdxcl and P e Eﬁdxcz.
1. Initialize WEERdXCI, Peiitdxcz. Let V=[W P]e
ERG, X(C 1 +62).
repeat
2. Calculate the block diagonal matrices Di(1 5 i 5 c1 —I—c2),
where the k—th diagonal block of Di is ﬁn? Calculate the
1

I2
2||Vk||2'

 

 

diagonal matrix D, where the k—th diagonal element is

 

3. Update w by w—BTIa, where the d>I<(p—1)—I—
a(15a5d,15p5c1)—th element of aeii‘tdclx1 is

8<£1(W)+)’1inDiwi—I—szKWTDWO
i=1 8W , the (d>I<(p—1)—I—

up
a,d>I<(q—1)—I—v)(15a,v5d,15p,q5c1)—th

Cl
8<£1(W)+y1ZWfDiwi+y2Tr(WTDW))

dc Xdc - i=1
BEERI 1 1s aWupaWVq

Construct the updated W E Eﬁdxcl by the updated vector
w E 31"“, where the (a,p)—th element of W is the
(d >I<(p— 1)—I—a)—th element of w.
4. Update the i—th column of P by pi =(XXT —I—y1D,-—I—
VZD)_1XZi-
5. Update the V by V=[W P].

until Converges

 

element of

 

 

then by adding Equations (15—18) in the both sides, we arrive at

Cj‘I—Cz K d

£1(W)+£2(P)+yi Z 2 if 2H2:
k:

 

 

 

 

 

It]
2
i=1 k=1 1
Cj‘I—Cz K

s £1(W)+£2(P)+yi Z 2
i=1 k=1

 

 

k
Vi

 

 

d
2 "II
V .
WHII 2

Therefore, the algorithm decreases the objective value of problem
(8) in each iteration. E!

In the convergence, W, P, Di(15i5c1—I—c2) and D satisfy the
Equation (9). As the Equation (8) is a convex problem, satisfying the
Equation (9) indicates that V =[ W P] is a global optimum solution
to the Equation (8). Therefore, the Algorithm 1 will converge
to the global optimum of the Equation (8). Since our algorithm
has the closed form solution in each iteration, the convergency is
very fast.

3 EMPIRICAL STUDIES AND DISCUSSIONS

Data used in the preparation of this article were obtained from
the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database
(adni.loni.ucla.edu). One goal of ADNI has been to
test whether serial magnetic resonance imaging (MRI), positron
emission tomography (PET), other biological markers, and clinical

 

i131

112 /§.IO'SIBUJHOIPJOJXO'SOllBIHJOJUIOlCI/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

H. Wang et al.

 

and neuropsychological assessment can be combined to measure
the progression of MCI and early AD. For up—to—date information,
see WWW . adni— info . org. Following a prior imaging genetics
study (Shen et al., 2010b), 733 non—Hispanic Caucasian participants
were included in this study. We empirically evaluate the proposed
method by applying it to the ADNI cohort, where a wide range
of multimodal biomarkers are examined and selected to predict
memory performance measured by ﬁve RAVLT scores and classify
participants into HC, MCI and AD.

3.1 Experimental design

Overall setting: our primary goal is to identify relevant genetic
and imaging biomarkers that can classify disease status and predict
memory scores (Fig. 2). We describe our genotyping, imaging and
memory data in Section 3.1; present the identiﬁed biomarkers in
Section 3.2; discuss the disease classiﬁcation in Section 3.3; and
demonstrate the memory score prediction in Section 3.4.

Genotyping data: the single—nucleotide polymorphism (SNP) data
(Saykin et al., 2010) were genotyped using the Human 610—Quad
BeadChip (Illumina, Inc., San Diego, CA, USA). Among all SNPs,
only SNPs, belonging to the top 40 AD candidate genes listed
on the AlzGene database (www.alzgene.org) as of June 10, 2010,
were selected after the standard quality control (QC) and imputation
steps. The QC criteria for the SNP data include (i) call rate check
per subject and per SNP marker, (ii) gender check, (iii) sibling
pair identiﬁcation, (iv) the Hardy—Weinberg equilibrium test, (v)
marker removal by the minor allele frequency and (vi) population
stratiﬁcation. The quality—controlled SNPs were then imputed using
the MaCH software to estimate the missing genotypes. After that,
the Illumina annotation information based on the Genome build 36.2
was used to select a subset of SNPs, belonging or proximal to the top
40 AD candidate genes. This procedure yielded 1224 SNPs, which
were annotated with 37 genes (Wang et al., 2012). For the remaining
3 genes, no SNPs were available on the genotyping chip.

Imaging biomarkers: in this study, we use the baseline structural
MRI and molecular FDG—PET scans, from which we extract
imaging biomarkers. Two widely employed automated MRI analysis
techniques were used to process and extract imaging genotypes
across the brain from all baseline scans of ADNI participants
as previously described (Shen et al., 2010b). First, voxel—based
morphometry (VBM) (Ashburner and Friston, 2000) was performed
to deﬁne global gray matter (GM) density maps and extract local GM
density values for 86 target regions (Fig. 4a). Second, automated
parcellation via freeSurfer V4 (Fischl et al., 2002) was conducted to
deﬁne 56 volumetric and cortical thickness values (Fig. 4b) and to
extract total intracranial volume (ICV). Further information about
these measures is available in Shen et al. (2010b). All these measures
were adjusted for the baseline age, gender, education, handedness
and baseline ICV using the regression weights derived from the
healthy control participants. For PET images, following Landau
et al. (2009), mean glucose metabolism (CMglu) measures of 26
regions of interest (ROIs) in the Montreal Neurological Institute
(MNI) brain atlas space were employed in this study (Fig. 40).

Memory data: The cognitive measures we use to test the proposed
method are the baseline RAVLT memory scores from all ADNI
participants. The standard RAVLT format starts with a list of 15
unrelated words (List A) repeated over ﬁve different trials and

  
  
 
 
 
 
 
 
 
  
     
   
  
   
  
  
  
  
  
   
  
  
   
 

[a] VBM  FreeSurfer {c} FDG-F‘ET
Am dale .
AIIqular“ . _ hwy“ Angu'ar
Anmngulate CHEW” Fu5IforrrI
F'ﬁﬂm CE'FEUWM Hippo-campus
I gppncalrapue Heat-“0| Ianmntlﬂ
n rente_ per
In'iFrenlal Tnang _ Inn—awﬁm Inmarlﬁlal
InfCIrEantal LHEVEHE IntTer'npeIal
Ian'arIEIal
Ini'l'emperel Emu MEIme

F usuferm Mi L'l Tem peral '

 

MedOermntel '“Ipai'ﬂﬂ' PueICIng
MﬁﬁEmeFlE-l- 1 _ In'l'TIen‘Ip-m'al Precuneus
lhiltlilllgigrlaitefl- . . M'dTEmPﬂ'H' SupF’arieIal-
wigrbﬁgrglﬂl Farah-pp Supremarg
hindTee-mmpperaii ‘ PDEICW SupTempnral
DITEEIDFII Posteenirel :— “. 5“ A?" -
P h - - - 3‘
PDE'CI:EUIg:-ltg Precemral U 5  r23“
usicenl'ral _. p-
Prmmlal  recuneue  SNP
Free-mans Sumea' renames-Ame x
Rulandlﬁﬂﬁ SupPatIELal rEj?335425_DAPK1
Supﬁgnm| 1 Sup-Temperal rEE-Eﬂ-dﬂﬂi-ENTF’DT
SupOerrontel 5” WEI-nary r511 IBBEED-EURC31
Euppii't'a' maze-:1 mam
SLIDTE'I'HDPCIIE' TemporalF'nle 7'5 '
SupTempnral Meanclng r512999411-BIN1
SuppMGIDr-‘WH rEEDEETTTvSORCSi
Stﬁfmmnﬁgg “WWW r511 ldiﬁﬁT-DAPM
MeanCIn “Hm-anemia 521331521 51:91:51
H [alti#nFtoI-Ita: MaanMeuTemp rsﬁiUEEE-PICALM
earn at am re
MeanMIedT-emggral- _ Mae’an Eggdﬂrmggnﬂggl‘
Hﬂﬂggrlﬁlig: MeenSensMoI-er Trigaﬂa ILC‘EEEHEA
Tl IE - r5. .
. M T
meantemporal. 9”" “m” 4 I: 4 a: regiment-Prime
—.' II. -.' II, ' |— [512dEQEiDD-ILIE
. I 3 it"; '3 I I E” g :3'  -’_—- .
i} 5 =3 15 q: 3 El 5 '“i i a 5 G‘ at.

Fig. 4. Weight maps for multimodal data: (a) VBM measures from MRI,
(b) FreeSurfer measures from MRI, (c) glucose metabolism from FDG—PET,
and ((1) top SNP ﬁndings. Weights for disease classiﬁcation were labeled
as Diag—L (left side), Diag—R (right side) or Diag; and weights for RAVLT
regression were labeled as AVLT—L, AVLT—R or AVLT. In (a—c), weights
were normalized by dividing the corresponding threshold used for feature
selection, and thus all selected features had normalized weights :1 and were
marked with ‘x’. In (d), only top SNPs were shown, weights were normalized
by dividing the weight of the 10th top SNP, and the top 10 SNPs for either
classiﬁcation or regression task had normalized weights :1 and were marked
with ‘x’

Table 1. RAVLT cognitive measures as responses in multitask learning

 

 

Task ID Description of RAVLT scores

TOTAL Total score of the ﬁrst 5 learning trials

TOT6 Trial 6 total number of words recalled

TOTB List B total number of words recalled

T30 30 minute delay total number of words recalled
RECOG 30 minute delay recognition score

 

participants are asked to repeat. Then the examiner presents a second
list of 15 words (List B), and the participant is asked to remember
as many words as possible from List A. Trial 6, termed as 5 min
recall, requests the participant again to recall as many words as
possible from List A, without reading it again. Trial 7, termed as
30 min recall, is administrated in the same way as Trial 6, but after
a 30 min delay. Finally, a recognition test with 30 words read aloud,
requesting the participant to indicate whether or not each word is on
List A. The RAVLT has proven useful in evaluating verbal learning
and memory. Table 1 summarizes ﬁve RAVLT scores used in our
experiments.

 

i132

112 /§.IO'SIBUJHOIPJOJXO'SOllBIHJOJUIOlCI/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

Multidimensional imaging genetics data integration

 

Table 2. Multimodal feature sets as predictors in multiview learning

 

 

View ID (feature set ID) Modality No. of features
VBM MRI 86
FreeSurfer MRI 56
FDG—PET FDG—PET 26
SNP Genetics 1244

 

Participant selection: In this study, we included only participants
with no missing data for all above four types (views) of features and
cognitive scores, which resulted in a set of 345 subjects (83 HC,
174 MCI and 88 AD). The feature sets extracted from baseline
multimodal data of these subjects are summarized in Table 2.

3.2 Biomarker identiﬁcations

The proposed heterogeneous multitask learning scheme aims to
identify genetic and phenotypic biomarkers that are associated
with both cognition (e.g. RAVLT in this study) and disease
status in a joint regression and classiﬁcation framework. Here we
ﬁrst examine the identiﬁed biomarkers. Shown in Figure 4 is a
summarization of selected features for all four data types, where the
regression/classiﬁcation weights are color—mapped for each feature
and each task.

In Figure 4a, many VBM measures are selected to be associated
with disease status, which is in accordance with known global brain
atrophy pattern in AD. The VBM measures associated with RAVLT
scores seem to be a subset of those disease—sensitive markers,
showing a speciﬁc memory circuitry contributing to the disease,
as well as suggesting that the disease is implicated by not only this
memory function but also other complicated factors. Evidently, the
proposed method could have a potential to offer deep mechanistic
understandings. Shown in Figure 5 is a comparison between RAVLT—
relevant markers and AD—relevant markers and their associated
weights mapped onto a standard brain space.

Figure 4b shows the identiﬁed markers from the FreeSurfer data.
In this case, a small set of markers are discovered. These markers,
such as hippocampal volume, amygdala volume and entorhinal
cortex thickness, are all well—known AD—relevant markers, showing
the effectiveness of the proposed method. These markers are also
shown to be associated with both AD and RAVLT. The FDG—PET
ﬁndings (Fig. 4c) are also interesting and promising. The AD—
relevant biomarkers include angular, hippocampus, middle temporal
and post cingulate regions, which agrees with prior ﬁndings e.g.
Landau et al. (2009). Again, a subset of these markers are also
relevant to RAVTL scores.

As to the genetics, only top ﬁndings are shown in Figure 4d. The
APOE E4 SNP (rs429358), the best known AD risk factor, shows the
strongest link to both disease status and RAVLT scores. A few other
important AD genes, including recently discovered and replicated
PICALM and BIN1, are also included in the results. For those newly
identiﬁed SNPs, further investigation in independent cohorts should
be warranted.

3.3 Improved disease classiﬁcation

We classify the selected participants of ADNI cohort using the
proposed methods by integrating the four different types of data.

 

Fig. 5. VBM weights of joint regression of AVLT scores and classiﬁcation
of disease status were mapped onto brain (3) Overall weights for disease
classiﬁcation; (b) Overall weights for AVLT regression

We report the classiﬁcation performances of our method. We
compare our methods against several most recent MKL methods that
are able to make use of multiple types of data including SVM Koo
MKL method (Sonnenburg et al., 2006), SVM £1 MKL (Lanckriet
et al., 2004), SVM £2 MKL method (Kloft et al., 2008), least
square (LSSVM) (Zoo MKL method (Ye et al., 2008), LSSVM
£1 MKL method (Suykens et al., 2002) and LSSVM £2 MKL
method (Yu et al., 2010). We also compare a related method,
Heterogeneous Multitask Learning (HML) method (Yang et al.,
2009), which simultaneously conducts classiﬁcation and regression
like our method. However, because this method is designed for
homogenous input data and is not able to deal with multiple types
of data at the same time, we concatenate the four types of features
as its input. In addition, we report the classiﬁcation performances
by our method and SVM on each individual types of data as
baselines. SVM on a simple concatenation of all four types of
features are also reported. In our experiments, we conduct three—
class classiﬁcation, which is more desirable and more challenging
than binary classiﬁcations using each pair of three categories.

We conduct standard 5—fold cross—validation and report the
average results. For each of the ﬁve trials, within the training
data, an internal 5—fold cross—validation is performed to ﬁne
tune the parameters. The parameters of our methods [y1
and )/2 in Equation (8)] are optimized in the range of
{lo—5, 10—4,  104, 105}. For SVM method and MKL methods,
one Gaussian kernel is constructed for each type of features
[i.e.IC(X,-,Xj) =exp (—y  —Xj  where the parameters y are
ﬁne tuned in the same range used as our method. We implement
the MKL methods using the codes published by Yu et al. (2010).
Following Yu et al. (2010), in LSSVM Zoo and £2 methods,
the regularization parameter )1 is estimated jointly as the kernel
coefﬁcient of an identity matrix; in LSSVM £1 method, )1 is set to 1;
in all other SVM approaches, the C parameter of the box constraint is
set to 1. We use LIB SVM (http://www.csie.ntu.edu.tw/ cjlin/libsvml)
software package to implement SVM. We implement HML method
following the details in its original work, and set the parameters to be
optimal. The classiﬁcation performances measured by classiﬁcation
accuracy of all compared methods in AD detection are reported in
Table 3.

A ﬁrst glance at the results shows that our methods consistently
outperform all other compared methods, which demonstrates the
effectiveness of our methods in early AD detection. In addition,

 

i133

112 /810's112u1nofp101x0's31112u1101u101q//:d11q 111011 pop1201um0q

9IOZ ‘09 lsnﬁnv uo ::

H. Wang et al.

 

Table 3. Classiﬁcation performance comparison between the proposed

method and related methods for distinguishing HC, MCI and AD

Table 4. Comparison of memory prediction performance measured by
average RMSEs (smaller is better)

 

 

 

Methods Accuracy (mean + SD) Test case TOTAL TOT6 TOTB T30 RECOG
SVM (SNP) 0.561 :I: 0.026 MRV (SNP) 6.153 2.476 2.168 2.201 3.483
SVM (FreeSurfer) 0.573 :I: 0.012 MRV (FreeSurfer) 5.928 2.235 2.039 2.088 3.339
SVM (VBM) 0.541 :I: 0.032 MRV (VBM) 6.093 2.289 2.142 2.137 3.394
SVM (PET) 0.535 :I: 0.026 MRV (PET) 6.246 2.514 2.237 2.215 3.615
SVM (all) 0.575 :I: 0.019 MRV (all) 5.909 2.232 1.992 2.032 3.306
HML (all) 0.638 :I: 0.019 1

Ridge (SNP) 6.076 2.416 2.147 2.117 3.368
SVM (ZOO MKL method 0.624 :I: 0.031 Ridge (FreeSurfer) 5.757 2.203 2.004 2.017 3.237
SVM £1 MKL method 0.593 :I: 0.042 Ridge (VBM) 5.976 2.147 2.038 2.129 3.249
SVM £2 MKL method 0.561 :I: 0.037 Ridge (PET) 6.153 2.443 2.186 2.107 3.515
LSSVM (ZOO MKL method 0.614 :I: 0.031 Ridge (all) 5.704 2.143 1.989 1.994 3.193
LSSVM £1 MKL method 0.585 :I: 0.018
LSSVM £2 MKL method 0577 :I: 0033 Our method (SNP) 5.991 2.201 2.008 2.001 3.107

Our method (FreeSurfer) 5.601 2.106 1.947 1.886 3.015
Our method (SNP) 0.673 :I: 0.021 Our method (VBM) 5.715 2.011 1.899 1.974 3.041
Our method (FreeSurfer) 0.689 :I: 0.029 Our method (PET) 6.013 2.241 2.017 2.017 3.331
Our method (VBM) 0.669 :I: 0.031 Our method (all) 5.506 1.984 1.886 1.841 2.989
Our method (PET) 0.621 :I: 0.028
Our method 0.726 :I: 0.032

 

the methods using multiple data sources are generally better than
their counterparts using one single type of data. This conﬁrms
the usefulness of data integration in AD diagnosis. Moreover, our
methods always outperform the MKL methods in these experiments,
although both take advantage of multiple data sources. This
observation is consistent with our theoretical analysis. That is, our
methods not only assign proper weight to each type of data, but also
consider the relevance of the features inside each individual type of
data. In contrast, the MKL methods address the former while not
taking into account the latter.

3.4 Improved memory performance prediction

Now we evaluate the memory performance prediction capability of
the proposed method. Since the cognitive scores are continuous,
we evaluate the proposed method via regression and compare it
to two baseline methods, i.e. multivariate linear regression (MRV)
and ridge regression. Since both MRV and ridge regression are for
single—type input data, we conduct regression on each of the four
types of features and a simple concatenation of them. Similarly, we
also predict memory performance by our method on the same test
conditions. When multiple—type input data are used, as demonstrated
in Section 3.2, our method automatically and adaptively select the
prominent biomarkers for regression. For each test case, we conduct
standard 5—fold cross—validation and report the average results. For
each of the ﬁve trials, within the training data, an internal 5—fold
cross—validation is performed to ﬁne tune the parameters in the range

of {10_5,10_4,...,104,105 for both ridge regression and our

method. For our method, in each trial, from the learned coefﬁcient
matrix we sum the absolute values of the coefﬁcients of a single
feature over all the tasks as the overall weight, from which we pick
up the features with non—zero weights (i.e. w >10T3) to predict
regression responses for test data. The performance assessed by
root mean square error (RMSE), a widely used measurement for
statistical regression analysis, are reported in Table 4.

From Table 4 we can see that the proposed method always has
better memory prediction performance. Among the test cases, the
FreeSurfer imaging measures and VBM imaging measure have
similar predictive power, which are better than those of PET imaging
measures and SNP features. In general, combining the four types of
features are better than only using one type of data. Since our method
adaptively weight each type of data and each feature inside a type
of data, it has the least regression error when using all available
input data. These results, again, demonstrated the usefulness of our
method and data integration in early AD diagnosis.

4 CONCLUSIONS

We proposed a novel sparse multimodal multitask learning
method to identify the disease—sensitive biomarkers via integrating
heterogeneous imaging genetics data. We utilized the joint
classiﬁcation and regression learning model to identify the
disease—sensitive and QT—relevant biomarkers. We introduced a
novel combined structured sparsity regularization to integrate
heterogeneous imaging genetics data, and derived a new efﬁcient
optimization algorithm to solve our non—smooth objective function
and followed with the rigorous theoretical analysis on the
global convergency. The empirical results showed our method
improved both memory scores prediction and disease classiﬁcation
accuracy.

ACKNOWLEDGEMENT

Data used in preparation of this article were obtained from
the Alzheimer’s Disease Neuroimaging Initiative (ADNI)
database (adni.loni.ucla.edu). As such, the investigators within
the ADNI contributed to the design and implementation
of ADNI and/or provided data but did not participate in
analysis or writing of this report. A complete listing of ADNI
investigators can be found at: http://adni.loni.ucla.edu/wp—
content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.

 

i134

112 /810's112u1nofp101x0's31112u1101u101q//:d11q 111011 pop1201um0q

9IOZ ‘09 lsnﬁnv uo ::

Multidimensional imaging genetics data integration

 

Funding: [This research was supported by National Science
Foundation Grants CCF—0830780, CCF—0917274, DMS—0915228,
and IIS—1117965] at UTA; and by [National Science Foundation
Grant IIS—1117335, National Institutes of Health Grants UL1
RR025761, U01 AG024904, NIA RC2 AG036535, NIA R01
AG19771, and NIA P30 AG10133—18S1] at IU.

Data collection and sharing for this project was funded
by the Alzheimer’s Disease Neuroimaging Initiative (ADNI)
(National Institutes of Health Grant U01 AG024904). ADNI
is funded by the National Institute on Aging, the National
Institute of Biomedical Imaging and Bioengineering, and through
generous contributions from the following: Abbott; Alzheimer’s
Association; Alzheimer’s Drug Discovery Foundation; Amorﬁx
Life Sciences Ltd.; AstraZeneca; Bayer HealthCare; BioClinica,
Inc.; Biogen Idec Inc.; Bristol—Myers Squibb Company; Eisai
Inc.; Elan Pharmaceuticals Inc.; Eli Lilly and Company; F.
Hoffmann—La Roche Ltd and its afﬁliated company Genentech,
Inc.; GE Healthcare; Innogenetics, N.V.; Janssen Alzheimer
Immunotherapy Research & Development, LLC.; Johnson &
Johnson Pharmaceutical Research & Development LLC.; Medpace,
Inc.; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; Novartis
Pharmaceuticals Corporation; Pﬁzer Inc.; Servier; Synarc Inc.;
and Takeda Pharmaceutical Company. The Canadian Institutes
of Health Research is providing funds to support ADNI clinical
sites in Canada. Private sector contributions are facilitated by the
Foundation for the National Institutes of Health (www.fnih.org). The
grantee organization is the Northern California Institute for Research
and Education, and the study is coordinated by the Alzheimer’s
Disease Cooperative Study at the University of California, San
Diego. ADNI data are disseminated by the Laboratory for Neuro
Imaging at the University of California, Los Angeles. This research
was also supported by NIH [P30 AG010129, K01 AG030514] and
the Dana Foundation.

Conﬂict of Interest: none declared.

REFERENCES

Abney,S. (2002) Bootstrapping. Annual Meeting of the Association for Computational
Linguistics, pp. 360—367.

Argyriou,A. et al. (2007) Multi-task feature learning. In Advances in Neural Information
Processing System (NIPS), The MIT Press, pp. 41—48.

Argyriou,A. et al. (2008) Convex multitask feature learning. Machine Learning, 73,
243—272.

Ashburner,J. and Friston,K. (2000) Voxel-based morphometry—the methods.
Neuroimage, 11, 805—821.

Bach,F. et al. (2004) Multiple Kernel Learning, Conic Duality, and the SMOAlgorithm.
In International Conference on Machine Learning ( I CML), ACM, pp. 6.

Batmanghelich,N. et al. (2009) A general and unifying framework for feature
construction, in image-based pattern classiﬁcation. Inf Process Med Imaging, 21,
4234134.

Beck,A. and Teboulle.,M. (2009) A fast iterative shrinkage-thresholding algorithm for
linear inverse problems. SIAM J. Imaging Sci., 2, 183—202.

Bickel,S. and Scheffer,T. (2004) Multi-view clustering. In IEEE International
Conference on Data Mining (ICDM), pp. 36.

Brefeld,U. and Scheffer,T. (2004) Co-em support vector learning. In International
Conference on Machine Learning (ICML), ACM, pp. 16.

Dhillon,I.S. et al. (2003) Information-theoretic co-clustering. In ACM SIGKDD
(Special Interest Group on Knowledge Discovery and Data Mining) International
Conference on Knowledge Discovery and Data Mining, pp. 89—98.

Efron,B. et al. (2004) Least angle regression. Ann. Stat, 32, 407—499.

Fan,Y. et al. (2008) Spatial patterns of brain atrophy in MCI patients, identiﬁed
via high-dimensional pattern classiﬁcation, predict subsequent cognitive decline.
Neuroimage, 39, 1731—1743.

Fischl,B. et al. (2002) Whole brain segmentation: automated labeling of
neuroanatomical structures in the human brain. Neuron, 33, 341—355.

Ghani,R. (2002) Combining labeled and unlabeled data for multi-class text
categorization. In International Conference on Machine Learning, ACM,
pp. 187—194.

Hinrichs,C. et al. (2009a) MKL for robust multi-modality ad classiﬁcation. In
Proceedings of the 12th International Conference on Medical Image Computing
and Computer-Assisted Intervention: Part II, pp. 786—794.

Hinrichs,C. et al. (2009b) Spatially augmented LPboosting for AD classiﬁcation with
evaluations 0n the ADNI dataset. Neuroimage, 48, 138—149.

Kim,S. and Xing,E. (2010) Tree-Guided Group Lasso for Multi-Task Regression with
Structured Sparsity. In International Conference on Machine Learning (ICML).
pp. 352—359.

Kloft,M. et al. (2008) N on-sparse multiple kernel learning. In Proceedings of the NIPS
Workshop on Kernel Learning: Automatic Selection of Optimal Kernels.

Krishnapuram,B. et al. (2005) Sparse multinomial logistic regression: fast algorithms
and generalization bounds. In IEEE Trans. Pattern Anal. Mach. Intell, 27,
957—968.

Lanckriet,G et al. (2004) Learning the kernel matrix with semideﬁnite programming.
In JMLR, 5, 27—72.

Landau,S. et al. (2009) Associations between cognitive, functional, and FDG-PET
measures of decline in AD and MCI. Neurobiol. Aging, 32, 1207—1218.

Lee,S.-I. et al. (2006) Efﬁcient ll regularized logistic regression. In The 21st National
Conference on Artiﬁcial Intelligence (AAAI), pp. 401.

Liu,J. et al. (2009) Large-scale sparse logistic regression. In ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 547—556.

Lu0,D. et al. (2010) Towards structural sparsity: an explicit [2/10 approach. In IEEE
International Conference on Data Mining (ICDM), pp. 344—353.

Micchelli,C. et al. (2010) A family of penalty functions for structured sparsity.
In Advances in Neural Information Processing System (NIPS), The MIT Press,
pp. 1612—1623.

Nigam,K. et al. (2000) Text classiﬁcation from labeled and unlabeled documents using
em. Machine Learning, 39, 103—134.

Obozinski,G. et al. (2006) Multi-task feature selection. Technical report, Department
of Statistics, University of California, Berkeley.

Obozinski,G. et al. (2010) Joint covariate selection and joint subspace selection for
multiple classiﬁcation problems. Stat. Comput., 20, 231—252.

Quattoni,A. et al. (2009) An efﬁcient projection for [1, 00 regularization. In International
Conference on Machine Learning (ICML), ACM, pp. 857—864.

Rakotomamonjy,A. et al. (2007) More efﬁciency in multiple kernel learning. In
International Conference on Machine Learning (ICML), ACM, pp. 775—782.

Saykin,A.J. et al. (2010) Alzheimer’s disease neuroimaging initiative biomarkers
as quantitative phenotypes: genetics core aims, progress, and plans. Alzheimers
Dement, 6, 265—273.

Shen,L. et al. (2010a) Sparse bayesian learning for identifying imaging biomarkers in
AD prediction. Med. Image Comput. Comput. Assist. Interv, 13(Pt 3), 611—618.
Shen,L. et al. (2010b) Whole genome association study of brain-wide imaging
phenotypes for identifying quantitative trait loci in MCI and AD: A study of the

ADNI cohort. Neuroimage, 53, 1051—1063.

S0nnenburg,S. et al. (2006) Large scale multiple kernel learning. In JMLR, 7,
1531—1565.

Stonnington,C.M. et al. (2010) Predicting clinical scores from magnetic resonance scans
in alzheimer’s disease. Neuroimage, 51, 1405—1413.

Sun,L. et al. (2009) Efﬁcient recovery of jointly sparse vectors. In Advances in Neural
Information Processing Systems (NIPS) 22, The MIT Press, pp. 1812—1820.

Suykens,J. et al. (2002) Least Squares Support Vector Machines, World Scientiﬁc,
Singapore. (ISBN 981-238-151-1)

Tibshirani,R. (1996) Regression shrinkage and selection via the LASSO. J. R. Statist.
Soc B., 58, 267—288.

Walhovd,K. et al. (2010) Multi-modal imaging predicts memory performance in normal
aging and cognitive decline. Neurobiol. Aging, 31, 1107—1 121.

Wang,H. et al. (2011) Identifying AD-Sensitive and Cognition-Relevant Imaging
Biomarkers via Joint Classiﬁcation and Regression. In The Proceedings of The I 4th
International Conference on Medical Image Computing and Computer Assisted
Intervention (MICCAI 2011), Lecture Notes in Computer Science (LNCS) 6893,
Springer, pp. 115—123.

Wang,H. et al. (2012) Identifying quantitative trait loci via group-sparse multitask
regression and feature selection: an imaging genetics study of the ADNI cohort.
Bioinformatics, 28, 229—237.

Yang,X. et al. (2009) Heterogeneous multitask learning with joint sparsity constraints.
In Advances in Neural Information Processing System (NIPS), The MIT Press,
pp. 2151—2159.

 

i135

112 /810's112u1nofp101x0's31112u1101u101q//:d11q 111011 pop1201um0q

9IOZ ‘09 lsnﬁnv uo ::

H. Wang et al.

 

Ye,J. et al. (2008) Multi-class discriminant kernel learning via convex programming.
In JMLR, 9, 719—758.

Yu,S. et al. (2010). L 2-n0rm multiple kernel learning and its application to biomedical
data fusion. BMC Bioinformatics, 11, 309.

Yuan,M. and Lin,Y. (2006) Model selection and estimation in regression with grouped
variables. J. R. Stat. Soc. Ser. B, 68, 49C—67.

Zien,A. and Ong,C. (2007) Multiclass multiple kernel learning. In International
Conference on Machine Learning (ICML), ACM, pp. 1191—1198.

 

i136

112 /810's112u1nofp101x0's31112u1101u101q//:d11q 111011 pop1201um0q

9IOZ ‘09 lsnﬁnv uo ::

