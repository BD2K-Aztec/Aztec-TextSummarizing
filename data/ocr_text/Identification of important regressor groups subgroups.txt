ORIGINAL PAPER

Vol. 30 no. 6 2014, pages 831—837
doi:10. 1093/bioinformatics/btt608

 

Systems biology

Advance Access publication October 24, 2013

Identification of important regressor groups, subgroups
and individuals via regularization methods: application to

gut microbiome data

Tanya P. Garcia”, Samuel MUIIerZ, Raymond J. Carroll3 and Rosemary L. Walzem4
1Department of Epidemiology & Biostatistics, School of Rural Public Health, Texas A&M Health Science Center,
College Station, TX 77843—1266, USA, 2School of Mathematics and Statistics, University of Sydney,

NSW 2006 Australia, 3Department of Statistics, Texas A&M University, College Station, TX 77843—3143, USA and
4Department of Poultry Science, Intercollegiate Faculty of Nutrition, Texas A&M University, College Station,

TX 77840, USA

Associate Editor: Ziv Bar-Joseph

 

ABSTRACT

Motivation: Gut microbiota can be classified at multiple taxonomy
levels. Strategies to use changes in microbiota composition to effect
health improvements require knowing at which taxonomy level inter-
ventions should be aimed. Identifying these important levels is difficult,
however, because most statistical methods only consider when the
microbiota are classified at one taxonomy level, not multiple.
Results: Using L1 and L2 regularizations, we developed a new variable
selection method that identiﬁes important features at multiple
taxonomy levels. The regularization parameters are chosen by a new,
data-adaptive, repeated cross-validation approach, which performed
well. In simulation studies, our method outperformed competing
methods: it more often selected significant variables, and had small
false discovery rates and acceptable false-positive rates. Applying
our method to gut microbiota data, we found which taxonomic levels
were most altered by specific interventions or physiological status.
Availability: The new approach is implemented in an R package,
which is freely available from the corresponding author.

Contact: tpgarcia@srph.tamhsc.edu

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on February 20, 2013; revised on September 19, 2013;
accepted on October 18, 2013

1 INTRODUCTION

With improved culture-independent techniques, a typical study
of gut microbiota now involves data from numerous microbes.
The microbes are classiﬁed at multiple taxonomy levels, namely,
phylum, class, order, family, genus and species. Each taxonomy
level has many subdivisions, and the number of subdivisions
increase on progression from phylum to species level.
Strategies to use changes in microbiota composition to effect
health improvements require knowing at which taxonomy level
interventions should be aimed. Levels to target are those with
subdivisions identiﬁed as having an impact on the target health
outcome. From a biological perspective, only a few subdivisions
at each level are believed to play a role in certain health

 

*To whom correspondence should be addressed.

outcomes. Identifying the few important subdivisions at each
level is difﬁcult, however, because of the increasing number of
subdivisions on progression from phylum to species level and
because the microbial data are typically based on small sample
sizes. Thus, a method that overcomes these difﬁculties and
identiﬁes important subdivisions at multiple taxonomy levels
is needed.

This biological problem corresponds to a variable selection
problem where the variables are grouped at multiple levels,
and the number of variables (p) far exceeds the sample size (n).
We suppose that each level has sparse effects. In the microbiota
data, sparse effects mean that only a few subdivisions within a
particular taxonomy level actually impact the health phenotypes
of interest. For our purposes, we consider the case where
variables are divided into groups and subgroups within the
groups. Our interest, thus, is developing a method that selects
important groups (e.g. phyla), subgroups (e.g. families) and
individual predictors (e. g. genera).

Selecting variables clustered into groups and subgroups is
challenging. When the variables are divided only into groups
(without subgroups), a popular technique is the group Lasso
Wuan and Lin, 2006), which selects an entire group of variables
to be included or excluded from the model. The group Lasso,
however, has substantial drawbacks. First, the method assumes
that the model submatrices for each group are orthonormal.
When orthonormality is not satisﬁed, the group Lasso may
select an incorrect model (Friedman et al., 2010). Second, the
group Lasso does not achieve sparsity within each group,
which can be useful. For the microbial data, we could design
more speciﬁc strategies for changing microbiota composition if
we knew which particular families (i.e. subgroups) in phyla (i.e.
group) impacted health phenotypes of interest.

To overcome the deﬁciencies of the group Lasso, Simon et al.
(2012) recently proposed the sparse-group Lasso (SGL). The
method imposes no orthonormality requirements on the
group model submatrices and achieves sparsity between and
within groups through a clever use of the Nesterov (2007)
method for generalized gradient descent. The SGL works well
when variables are clustered into groups, but not when they are
clustered at more than one level—a feature inherent to gut
microbiota data.

 

© The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 831

112 /810's112umo[pJOJXO'sopchOJurorq/ﬁd11q IIIOJJ popcolumoq

910K ‘09 lsnﬁnV no :2

T.P. Garcia et al.

 

To accommodate selecting important groups, subgroups
and individual predictors, we propose three new algorithms.
The ﬁrst algorithm, the sparse group-subgroup Lasso
(SGSL), generalizes the work of Simon et al. (2012). It is
based on using L1 and L2 regularizations in a linear regression
model; convex non-linear regression models are discussed in
the Supplementary Material. Our two other proposed algo-
rithms use appropriate combinations of already existing vari-
able selection procedures. First, we propose applying the group
Lasso to the groups followed by SGL applied to the sub-
groups. Second, we propose applying the group Lasso to
both the groups and subgroups followed by applying the
Lasso (Tibshirani, 1996) to select among the individual pre-
dictors. We demonstrate in a simulation study that our ﬁrst
algorithm outperforms the other two.

SGSL is a special case of the tree-structured group Lasso
(Jenatton et al., 2011; Liu and Ye, 2010; Zhao et al., 2009),
where nodes on the tree represent groups or subgroups of
features and ‘leaf nodes represent individual features. The
tree-structured group Lasso, however, uses a smoothing prox-
imal gradient method (Kim and Xing, 2012) to ‘prune’ the
entire tree collectively, whereas our method uses an accelerated
generalized gradient descent approach to determine sparsity
among groups, then subgroups and then individual features.
Moreover, we consider a tree without cycles, meaning there is
no overlap between groups/subgroups of features; i.e. each in-
dividual feature only belongs to one subgroup, and each sub-
group only belongs to one group. Hence, our problem differs
from the overlapping group Lasso as in the analysis of breast
cancer gene expression data (Van de Vijver et al., 2002) where
the interest is ﬁnding important pathways among overlapping
genes. Our problem also differs from a hierarchical variable
selection (Zhao et al., 2009) where a feature is subject to selec-
tion only after another feature is selected ﬁrst. We do not
impose this requirement.

Like other Lasso-based procedures, SGSL also requires se-
lecting tuning parameters, for which we propose a data-adap-
tive approach. Our approach involves multiple applications of
10-fold cross-validation that we show performs well in selecting
the tuning parameters through various simulation studies.
Therefore, the main contributions from our work include (i) a
new variable selection procedure (SGSL), which identiﬁes
important groups, subgroups and individual predictors through
combined L1 and L2 regularizations. (ii) We show that achiev-
ing sparsity at multiple levels cannot be achieved through
simple combinations of existing Lasso approaches. We show
that such combinations will select relevant features less
often than SGSL or never (Section 3). (iii) We provide a
data-adaptive cross-validation approach that improves over
the traditional cross-validation to select the tuning parameters.
(iv) In microbiome data, our method identiﬁes which taxonomic
levels were most altered by speciﬁc interventions or physio-
logical status.

The rest of the article is as follows. Section 2 describes SGSL
and Section 3 evaluates its performance compared with
competing methods. In Section 4, we describe the microbiota
data that motivated this methodology and analyze the data.
Section 5 concludes the article.

2 METHODS

2.1 Data structure

We consider a linear regression model with sample size n, a response
variable y = (yl, ..., yn)T across the samples, and an n x 1) matrix of
predictors X. For the microbial data, y corresponds to measurements
of health features, and X contains information about the p microbes.
We have p>n, and without loss of generality, all variables are standar-
dized to have mean zero and sample variance one, so that the intercept
is excluded from the model.

Because the predictors have subgroup and group memberships, we
suppose there are L disjoint groups, and each group k has Mk disjoint
subgroups, k = 1, ...,L. By the disjointedness assumption, there is no
overlap between groups, or overlap between subgroups.

We assume that group k contains pk predictors denoted by the n x pk
matrix XU‘) C X. We also assume that subgroup m in group k contains
phm predictors denoted by the n x phm matrix X(k’m) C X0“). The nota-
tion is such that XU‘) refers to the predictors in group k; whereas X(k’m)
refers to the predictors in subgroup m of group k. The total number of
predictors across all subgroups in group k is pk (i.e. 23:1 12km, 2 pk), and
the total number of predictors across all groups is p (i.e.
p = 21:11)], 2 2,le 23:1 phm). Finally, [30“) denotes the coefﬁcient
vector associated with group k, and [10" m) is associated with subgroup
m in group k.

2.2 New criterion for achieving sparsity among groups,
subgroups and individual predictors

2.2.1 SGSL: extension of the SGL Our primary objective is iden-
tifying the relevant groups, subgroups and individual predictors in rela-
tion to y. Doing so involves ﬁnding a sparse solution for the coefﬁcient
values; i.e. some coefﬁcient values will be zero and some will be non-zero.
If a group’s (subgroup’s) coefﬁcient vector is all non-zero, then that
group (subgroup) is relevant. Otherwise, if there is a mix of zero and
non-zero coefﬁcients in a subgroup, then those predictors with non-
zero coefﬁcient values are relevant and those predictors with zero
coefﬁcient values are not.

To determine which coefﬁcient values are zero and non-zero, we
propose solving I]? = argmin,,Q([i) where

L L
Q(B) = (1/2)ny — Zx<k>p<k>n§ + on?» 2 m.an
k=1 k=1

L Mk
+ 0‘21 ZZy—pm Ilﬁ(k’m)||2 +(1— on — azmmnl.

k=1m=1

Here, || - “2 denotes the Lz-norm and II - “1 denotes the Ll-norm. The
regularization parameters A, 051, and 052 control the level of sparsity
among the groups, subgroups and individual predictors, and satisfy
two criteria: A,a1,a2 Z 0 and a1 +052 5 1. Sparsity among groups and
subgroups results from the non-differentiability of the Lz-norm at zero.

For example, because Ilﬁ(k)||2 = ﬁ(k)Tﬁ(k) is non-differentiable at

[30“) = 0, the group coefﬁcient [30“) can be exactly zero. Likewise, the
subgroup coefﬁcient [10" m) can be exactly zero because || [10" m)||2 is
non-differentiable at Bow") 2 0. Though we deﬁne Q([f) for a linear
model, our method also extends to convex non-linear regression
models; see the Supplementary Material.

The criterion Q([f) also encompasses different versions of the Lasso.
We have the Lasso (Tibshirani, 1996) at on = 0, a2 = 0; the group Lasso
Wuan and Lin, 2006) at on = 1, a2 = 0; the group Lasso at the subgroup
level at 041 = 0, a2 = 1; SGL (Simon et al., 2012) among groups at
052 = 0; SGL among subgroups at on = 0; and we have sparsity only
among groups and subgroups when 051 >0, a2 >0 and a1 + a2 = 1.

 

832

112 /810's112umo[pJOJXO'sor1chOJurorw/2d11q IIIOJJ popcommoq

910K ‘09 lsnﬁnV no :2

Application to gut microbiome data

 

To ﬁnd the minimizer 1]? of Q([i), we take advantage of the criterion’s
convexity and separability between groups and subgroups. Through a
careful analytical derivation involving properties of subgradients
and the Karush—Kuhn—Tucker conditions, we derive the conditions
for when the group coefﬁcient [30“) and the subgroup coefﬁcient ﬂow")
are exactly zero; see Supplementary Material. These results motivate us to
use a blockwise descent algorithm at the group and subgroup levels.
When a subgroup’s coefﬁcient vector ﬂaw") is non-zero, we estimate the
subgroup coefﬁcients using the accelerated generalized gradient descent
method (Nesterov, 2007) and a step-size optimization as in Simon et al.
(2012).

In the algorithm below, let r(_k) = y — Zegk X093“) denote the partial
residual after removing group k, and r(_k,m) = r(_k) — 2&2”, X(k’s)’ﬁ\(k’s)
denote the partial residual after removing subgroup m from group k.
Let S(-) be the coordinate-wise soft thresholding operator (Donoho
and Johnston, 1994): [S{Z, (1 — a1 — a2)A}]j = sign(zj){|zj| — (1 — a1—
a2)A}+ where 2+ = max(z,0). Let £{r(_k,m), [i(k’m)} = ||r(_k,m) — X(k’m)

r‘k’mmé/z. and deﬁne Karim} = SM”) — rve{r(_k,m).p<kem>}.

(1 — a1 — a2)tA] as well asf,{[i(k’m)} = [1 _ L  Rt{ﬁ(k’m)}.
IIRtW’ }||2 +

Our proposed algorithm is then:

1. Group component: Iterate through each group k = 1, ...,L. If for

group k,
Mk
2 ([1151x<k’m>Tr(_k), (1 — e1 — 052)}~}||2
“=1 , (1)
- 052Aw/pk,m:| ) S wilzpk,
+

set Ilia“) = 0; otherwise, 30‘) 75 0 and do step 2 for group k.

2. Subgroup component: Iterate through the subgroups m = 1, . . . , M k
of group k and do the following.

a. If IIS{X<k’m>Tr(_k,m).(1—a1 — ammz s any—pm. set
30‘”) = 0. Otherwise, 30‘”) 75 0, and do step (b).
b. Set the step-size t=1 and counter s: 1. Deﬁne At{[i(k’1), ...,

WW} = [ﬂute 1>}. ...,f,T{"(k’M")}]T and

U{p(k,m), t} = [1 _ tatle/17k ]
IIAt{B(k’1), ~~~ap(k’Mk)}||2 +
x[1_M:| R {p(k,m)}.
IIRt{B(k’m)}II2 + ‘
Let B(k’m)’s = 0(k’m)’s = m), where 30””) is the current value.
Iterate through the following steps until convergence:

 

(1) Compute the gradient g = V£{l'(_k,m), [10" "0’3 },

(2) Compute A0“) = U{p(ksm)as, t} _ p(k,m),s.

(3) If £[r(_k,m), U{[i(k’m)’s, 1}] >g{1-(_k,m), p(k,m),S} + gr A0“) +
2;, ||A(s,t)||%, update the step size t to 0.8t. Repeat until the
inequality no longer holds to optimize t.

(4) Set 0(k’m)’s+1 as U{p(k,m),s, t}.

(5) Set 3(k’m)’s+1 as 0(k’m)’s + {S/ (S + 3)}{0(k’m’s+1) — 0(k’m)’s}; i.e.
a Nesterov step.

(6) Update .9 to s+ 1.

The algorithm above, known as SGSL, generalizes the SGL algorithm.
When the predictors are divided only into groups (i.e. a2 = 0), the above
algorithm is actually distinctly different from SGL. This is because of
the deﬁnition of U{[i(k’m), t} in Step 2(b), which uses information from

subgroups and groups, not just groups. When a2 = 0, the condition
in Equation (1) is equivalent to when an entire group is excluded from
the model in SGL (Simon et al., 2012).

2.2.2 Selection of regularization parameters Different choices of
A,a1,a2 yield different solutions  To select these tuning parameters,
we proceed as follows.

First, for a ﬁxed a1, 052, we choose the optimal A by varying it over
the range [tA*, A*], where A* is the smallest A such that Q([i) is minimized
at B: 0, and t is a small fraction, such as 0.05. To ﬁnd A*, note that

from condition (1), ﬂ = 0 minimizes Q(ﬁ) when

Mk 2
Z ([1 IS{X(k’m)Ty, (1 — a1 — ammz — any—AW] )

m=1 + 
S OliAZPke

for all groups k = 1, ...,L. Thus, A* is the smallest A value where the
above inequality holds for all groups. A practical approach for approx-
imating A* is taking A = 21 for j = 0, 1, 2, . . . , and stopping at the ﬁrst j
for which condition (2) holds for all groups. At this ﬁrst j, we have that
A* E(Zj_1,2j). To further improve the estimate of A*, one may then
bisect the interval (2‘1, 21) repeatedly until A* 6 (A1, A2), where
|A2 — A1|<0.0001. Here, when A = A2, condition (2) holds for all
groups, and when A = A1, condition (2) fails to hold for at least one
group. Finally, take A* = A2.

Performing the algorithm in Section 2.2.1 at ﬁxed 051,052 and over
the range of A values yields different model ﬁts. Among all ﬁts, we
choose the best descriptive model as the one that minimizes Mallows’
Cp criterion: M n(p*) = SSEI,* /82 — n + 217*, where [2* denotes the number
of predictors in the selected model, SSEI,* denotes the residual sum of
squares and 82 is an appropriate estimator of the model error variance.
For example, when n>p, 82 can be the residual mean square when
using all available variables, or when n <1), 82 can be the variance of y
(Hirose et al., 2013). Mallows’ Cp criterion balances the residual sum
of squares of a ﬁtted model with the number of non-zero parameter
estimates. Other model selection criteria may also be used (Muller and
Welsh, 2010).

The above procedure selects A well for a ﬁxed 051,052, and now we
describe how to optimally select an and 052. We propose selecting the
optimal 051,052 based on repeated 10-fold cross-validation as advocated
by Garcia et al. (2013) and Martinez et al. (2011). For a ﬁxed a1 2 am
and a2 = 0:20, a single application of 10-fold cross-validation works
as follows: (i) randomly partition the data into 10 non-overlapping
equal-sized subsets; (ii) remove data subset d, and apply the algorithm
in Section 2.2.1 at 0410,0520 and over the range of A, and select the
model that minimizes Mallow’s Cp criterion. The minimizing model has

associated solution denoted by 3H,) (the subscript (_d) emphasizes the

notion that data subset d was removed); (iii) repeat step (ii) for each
data subset d = 1, ..., 10 and compute the cross-validation score
CV(a10,a2o) = 2:11 ||y(d) — X(d;[i(_d)||§ where y(d) and X(d) denote the
response and explanatory variables for the data subset d that was
removed and 3H,) is the solution from step (ii). In our applications, we
repeated this three-step procedure for 0410,0520 taking values 0.01, 0.04,
0.07, 0.10, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95 such that
am + 0520 <1. The optimal 051,052 is the pair that minimizes the cross-
validation score.

Analogous to the work done in Garcia et al. (2013) and Martinez et al.
(2011), we did two additional steps to the above 10-fold cross-validation.
First, when the minirnizer of the cross-validation score was not unique,
we took a1, 052 as the average of the minimizers. Second, because Step (i)
yields a different random partition on each application, repeated
applications of the 10-fold cross-validation may yield different optimal
a1, a2 and thus different selected variables, especially when the signals are
sparse and small. Martinez et al. (2011) also noted this and suggested

 

833

112 /810's112umo[pJOJXO'sor112u1101urorq//2d11q 111011 pepeolumoq

910K ‘09 isnﬁnV no 22

T.P. Garcia et al.

 

performing the 10-fold cross-validation repeatedly, e.g. 100 times, to
develop a complete understanding of the variables selected. The idea,
thus, is to repeat the 10-fold cross-validation multiple times and retain
those variables that were selected at least 60% of the time, say.

2.3 Repeated application of current Lasso methods

Other possible approaches for obtaining sparsity among groups, sub-
groups and individual predictors are through appropriate combinations
of the Lasso, group Lasso and SGL.

2.3.] Group Lasso and SGL To achieve sparsity among the groups,
one may ﬁrst apply the group Lasso after orthonormalizing the group
model matrices. The group Lasso criterion is when a1 = 1 and a2 = 0 in
Q([f), and hence depends only on the regularization parameter A. To
optimally select A, we evaluate the criterion Q([f) with a1 = 1,012 = 0 at
a range of A values as in Section 2.2.2. The optimal A corresponds to the
model that minimizes Mallows’ Cp criterion. Because the group Lasso
selects an entire group of predictors to be included/excluded from the
model, the chosen model will have some groups with all non-zero coef-
ﬁcients (i.e. groups retained by the group Lasso), and some groups with
all zero coefﬁcients (i.e. groups dismissed by the group Lasso).

After achieving sparsity among groups, we then proceed to achieve
sparsity among the subgroups and individual predictors via SGL. For
those groups selected by the group Lasso, we apply SGL to all subgroups
within these groups. When applying SGL, we do not orthonormalize the
subgroup model matrices as done for the group Lasso. The criterion for
SGL among subgroups is when a1 = 0 in Q([i) and thus depends
on 012, A. We choose the optimal A and 012 via a repeated 10-fold cross-
validation as in Section 2.2.2.

2.3 .2 Repeated group Lasso and Lasso Another way to achieve the
desired sparsity is as follows. First, apply the group Lasso to the ortho-
normalized group model matrices to select relevant groups. Second, using
the selected groups, select relevant subgroups within by applying the
group Lasso to the orthonormalized subgroup model matrices. Lastly,
select relevant individual predictors by applying the Lasso to all
predictors in the selected subgroups. In the last step, the Lasso is applied
to the original predictors, not the orthonormalized versions. In each
application of group Lasso and Lasso, criterion Q([i) only depends on
A, which is chosen as in Section 2.2.2.

3 SIMULATION STUDY
3.1 Simulation design

We evaluated the performance of the proposed methods in
Section 2 on simulated data where predictors have group and
subgroup memberships. We considered L = 10 groups such that
each group had 2 subgroups. We divided p280 predictors
so that each subgroup had 4 predictors, and each group had
8 predictors.

Covariates in each group were generated from a Normal(0, 2)
distribution where 2 = diag(2*, 2*) and 2* = 0.7J4 + 0.314.
Here, J4 corresponds to a 4 x 4 matrix of ones, and I4 is the
4 x 4 identity matrix. This data generation procedure implies
that predictors within the same subgroup have a correlation of
0.7, but predictors in different subgroups/groups are
independent.

We set sample size n = 30 and generated the response variable

y: 211:1 311:1X(k’m)ﬁ(k’m) +6, where e is Normal(0,021,,).
The parameter 02 and the coefﬁcient vectors for each subgroup
were chosen according to two settings. In Setting 1, B“: 1) =

[2M = [#2: 1) = (6,6,4, 6.6, 8)T, 113:1)=(12.5,12.5,0,0)T and
02 = 1. All remaining subgroup coefﬁcients were zero. In
Setting 2, [$091) = [$092) = [#2: 1) = (2, 4, 6, 8)T, [13’ 1) = (10,10,
0, 0)T and 02 = 1. Again, all remaining subgroup coefﬁcients
were zero.

For each parameter setting, we generated 500 datasets and
applied seven methods: SGSL, the two variable selection proced-
ures in Section 2.2.2 and the following four other competing
methods.

(1) Lasso: We applied the Least Angle Regression algorithm
of Efron et al. (2004), which provides the entire sequence
of model ﬁts in the Lasso path. The best ﬁtting model was
that which minimized Mallows’ CI, criterion. This method
ignores the grouped nature of the predictors.

(2) Group Lasso: We applied the group Lasso after orthonor-
malizing the group model matrices. To ﬁnd the best ﬁtting
model, we minimized Q(B), with a1 = 1,012 = 0, over a
range of A values as in Section 2.2.2, and chose the
model that minimized Mallows’ CI, criterion. This

method yields sparsity among groups, but not among

subgroups, nor individual predictors.

(3) Repeated group Lasso: We applied the group Lasso at the
group and subgroup levels. In each application of the
group Lasso, the best ﬁtting model was that which mini-
mized Mallows’ CI, criterion. This method yields sparsity
among groups and subgroups, but not among individual
predictors.

(4) Sparse-group Lasso: We applied SGL among the groups;
that is, we minimized Q(B) where 052 = 0. To select the
tuning parameter a1 and A, we applied the repeated
10-fold cross-validation in Section 2.2.2. This method
ignores subgroup memberships and may not select signiﬁ-
cant subgroups.

For all methods requiring a selection of a1 and/or 012, we
repeated the 10-fold cross-validation 100 times to select the
optimal a1 and/or 012. Ultimately, this led to 100 possibly differ-
ent 011,012 values, and thus 100 possibly different ways variables
were selected. Ultimately, we retained variables that were
chosen at least 60% of the time in the 100 repeated applications.
We did not use the average of the 011,012 values to select the
variables.

To evaluate the seven methods, we computed the average
percentage of time predictors were selected, the observed false
discovery rate (Benjamini and Hochberg, 1995, FDR) and
geometric mean of speciﬁcity and sensitivity (deﬁned later in
the text). To compute these quantities, we divided the predictors
in each subgroup into those whose true parameter values are
non-zero (i.e. relevant predictors), and those whose true param-
eter values are zero (i.e. irrelevant predictors). We then reported
the average percentage of time relevant and irrelevant predictors
were selected in each subgroup. The observed FDR is the ratio
of the average number of irrelevant predictors selected (i.e.
false selections) over the average number of predictors selected.
The geometric mean of sensitivity and speciﬁcity is
G E (speciﬁcity x sensitivity)”2 (Kubat et al., 1998). Speciﬁcity
is the proportion of irrelevant predictors that were not selected

 

834

112 /810's112umo[pJOJXO's31112u1101u101q/ﬁd11q 111011 pepeolumoq

910K ‘09 isnﬁnV no 22

Application to gut microbiome data

 

among irrelevant predictors, and sensitivity is the proportion of
relevant predictors that were selected among relevant predictors.
The range of G is [0,1], and large G—Values indicate that most
predictors are classiﬁed correctly. We prefer G over speciﬁcity
and sensitivity alone, as it counteracts the imbalance between the
number of relevant and irrelevant predictors (Kubat et al., 1998).
Observed FDR and G—Values were computed using all groups,
and using only Groups 2 and 3 so as to demonstrate how the
methods perform for these two groups, which have sparsity
within their subgroups.

Among all methods, the reliable one will routinely select rele-
vant predictors, and rarely or never select irrelevant predictors.
Thus, the ideal method will have low F DRs and high G—values.

3.2 Simulation results

Results for the two simulation settings are given in Table 1. In
general, our procedure based on the new criterion Q(B) provided
the most reliable results: it largely selected the relevant predictors
and ignored irrelevant predictors (irrelevant predictors were in-
correctly chosen <5 % of the time). This performance resulted in
small F DRs, often smaller than the F DRs from other methods.
In comparison to SGL, we expected our method to perform
equally well when determining relevant groups (both methods
have essentially similar criterion for determining if a group is
relevant or not), but we expected our method to outperform
SGL in detecting sparsity between and within subgroups. SGL
is not designed to detect relevant subgroups within a group, nor
is it designed to detect relevant individual predictors within a
subgroup. Our method, in contrast, can do this. The results
from our simulation study conﬁrmed these expectations.

Our proposed procedure performed as well as SGL in
selecting Group 1, which had all non-zero coefﬁcients. But, our
method better detected the true sparsity in Groups 2 and 3.
Compared with SGL, our method correctly selected the relevant
subgroups and relevant individual predictors at least 4% more

Table 1. Simulation results for Setting 1 and 2 based on 500 simulations

often, and had nearly the same or fewer incorrect decisions in
selecting irrelevant predictors. This correct classiﬁcation is
evident by the larger G—value for Groups 2 and 3 (see G1 in
Table 1). For these two groups, our proposed method has a
G—value at least 1.14 times bigger than the G-value for SGL.
When considering all groups together (see G* in Table 1), the
G—Values for our proposed method and SGL are similar because
of the similar performance in Groups 1 and Groups 4—10. This is
no surprise given that for Group 1 and Groups 4—10, our pro-
posed method and SGL have similar selection criteria, and thus,
should behave equally well as they do. However, when there is
sparsity between and within subgroups (as is common in micro-
biome data; see Section 4), SGL fails to detect such a structure.
Thus, when there is sparsity between and within groups and
subgroups, our method has higher sensitivity and more power
than SGL.

Our proposed method also yielded better results than the other
ﬁve methods in terms of capturing the true clustering and achiev-
ing higher G—values. The Lasso, designed to select individual
predictors but not entire subsets, largely ignored the relevant
cluster of predictors in Group 1 and in Group 2, subgroup 1.
For Group 3, which only had 3 of 10 relevant predictors, the
Lasso did successfully select these variables as often as our pro-
posed procedure did. Thus, when necessary, our method can
behave similarly to the Lasso, which is an attractive feature
when individual features need to be selected. Still, because our
simulated data has a speciﬁc grouping structure which the Lasso
cannot capture, our proposed method has larger G—Values than
the Lasso both when computed across all groups (0.64 for our
method compared with 0.56 for the Lasso) and when computed
for Groups 2 and 3 (0.40 for our method compared with 0.36 for
the Lasso). Hence, because our interest goes beyond selecting
individual predictors, we prefer our proposed method.

Lastly, the proposed iterative procedures all fared poorly,
with G—Values nearly half that of our proposed method. These

 

 

 

Group Subgroup SGSL SGL Lasso GpL, GpL X 2, GpL X 2 GpL New SGL Lasso GpL, GpL X 2, GpL X 2 GpL
SGL Lasso method SGL Lasso
Setting 1 Setting 2
1 1 (Non-zero) 53.20 56.70 40.80 14.10 11.05 20.80 25.20 50.80 52.45 38.85 11.00 9.25 16.80 18.00
2 (Non-zero) 54.05 56.35 41.50 14.10 12.65 23.00 25.20 52.50 52.95 39.90 11.10 8.20 15.20 18.00
2 1 (Non-zero) 16.55 12.55 13.40 0.00 0.00 0.00 0.00 17.65 12.75 15.45 0.00 0.00 0.00 0.00
2 (Zero) 4.85 5.45 5.70 0.00 0.00 0.00 0.00 4.90 5.30 5.70 0.00 0.00 0.00 0.00
3 1 (Non-Zero) 19.20 13.60 21.30 0.00 0.00 0.00 0.00 26.50 19.70 25.80 0.00 0.00 0.00 0.00
1 and 2 (Zero) 3.50 3.37 3.77 0.00 0.00 0.00 0.00 4.40 4.60 4.77 0.00 0.00 0.00 0.00
4—10 (Zero) 0.03 0.01 0.04 0.00 0.00 0.00 0.00 0.04 0.02 0.04 0.00 0.00 0.00 0.00
FDRa 0.07 0.07 0.10 0.00 0.00 0.00 0.00 0.08 0.09 0.11 0.00 0.00 0.00 0.00
Ga 0.64 0.64 0.56 0.30 0.28 0.38 0.41 0.63 0.63 0.56 0.27 0.24 0.33 0.35
FDRb 0.28 0.35 0.32 NA NA NA NA 0.27 0.35 0.31 NA NA NA NA
Gb 0.40 0.35 0.36 0.00 0.00 0.00 0.00 0.41 0.35 0.38 0.00 0.00 0.00 0.00

 

Note: Average percentages of time each set of variables is selected with our proposed sparse group—subgroup Lasso (‘SGSL’); sparse-group Lasso (‘SGL’); Lasso; group Lasso
at group level and sparse-group Lasso at subgroup level (‘GpL, SGL’); group Lasso at group and subgroup levels, and Lasso at individual features level (‘GpL X 2, Lasso’);
group Lasso at group and subgroup levels (‘GpL X 2’); and group Lasso (‘GpL’). A ‘non-zero’ subgroup means variables are relevant; a ‘Zero’ subgroup means variables are

irrelevant.

aWe also report observed false discovery rate (FDR) and G-values using all groups.
bWe also report observed false discovery rate (FDR) and G—values using Groups 2 and 3.

‘NA’ denotes values were incomputable because of division by 0.

 

835

112 /810's112umo[pJOJXO's31112u1101u101q/ﬁd11q 111011 pepeolumoq

910K ‘09 isnﬁnV no 22

T.P. Garcia et al.

 

iterative methods selected relevant predictors in Group I nearly
three times less often than did our proposed method and never
selected the relevant predictors in Groups 2 and 3. The inability
to detect the relevant clusters in Groups 2 and 3 most likely
resulted from the initial application of the group Lasso. As the
group Lasso is designed to detect relevant groups, it has difﬁculty
determining if an entire group is relevant when that group is
sparse. Thus, the sparsity in Groups 2 and 3 prevented the
group Lasso, and all forthcoming Lasso-based methods, from
selecting these groups or the relevant clusters within.

4 EMPIRICAL EXAMPLE
4.1 Microbial data

Our motivating example is from a dietary treatment study in
mice (Thomas et al., 2013) for which we measured fecal micro-
bial diversity. The study used an obesity reversal paradigm and
consisted of n = 30 obese male mice equally and randomly
assigned to one of three diets: (i) a control soy-based diet with
0.5% (by weight) inorganic calcium; (ii) a high calcium soy-based
diet with 1.5% (by weight) inorganic calcium; and (iii) a non-fat
dry milk (NF DM) diet with 1.5% (by weight) calcium as
NF DM-intrinsic and inorganic calcium. After 10 weeks of feed-
ing, feces from all mice were analyzed for microbial communities
via pyrosequencing. Mice on the NF DM diet had enhanced
bodyfat loss (Thomas et al., 2013).

For each mouse, data consists of relative messenger RNA
(mRNA) expression of CD68 in adipose and microbial percent-
ages (X) from p: 51 microbes classiﬁed at the phylum, family
and genus levels. The mRNA expression of CD68 is used to
judge the extent to which macrophages have inﬁltrated adipose,
an event that occurs with bodyweight gain and is associated with
systemic inﬂammation (Thomas et al., 2013). The microbes were
classiﬁed into two phyla: Bacteriodetes and F irmicutes. Each
phylum had at least ﬁve families, with each family having at
least two bacterial genera. The key interest is to ﬁnd those
microbial phyla, families and genera associated with CD68
mRNA expression in this p>n setting.

A prior analysis in Garcia et al. (2013) demonstrated that diet
has a signiﬁcant impact on expression of mRNA for CD68. To
accommodate this diet effect, we took the response variable (y)
as the residuals from regressing expression of mRNA for CD68
on diet. See Garcia et al. (2013) for other approaches.

4.2 Results

We applied the same seven variable selection techniques from
the simulation study to the microbial data. We found that our
proposed procedure selected the entire family Streptococcaceae
in the Firmicutes phyla to have an effect on expression of
mRNA for CD68. The family consisted of Lactococcus and
Streptococcus genera. In comparison, SGL and Lasso were
only able to pick one member from this family (Streptococcus),
which indicates the inﬂexibility of these latter two methods in
selecting important families (i.e. subgroups).

Having our method select the Streptococcaceae family makes
sense, as members of Streptococcaceae ﬂourish in nutrient-rich
environments, such as an overfed subject’s gut (e. g. obese mice).
Moreover, mice in this study experienced chronic inﬂammation

secondary to obesity and hyperglycemia as evidenced by elevated
adipose CD68 arising from macrophage inﬁltration of adipose
tissue (Thomas et al., 2013). At present, it seems unlikely that
simple chronic caloric excess promoted Streptococcaceae abun-
dance in the obese mice, as this relationship was not seen in
newly obese mice [see Thomas et al. (2012) and Supplementary
Material]. Secondary effects appear to play a role as changes in
host inﬂammatory state were previously associated with
Streptococcaceae family members in hosts with either strongly
positive or negative energy balance. Intestinal infusion of fecal
microbiota from lean donors improved glucose metabolism in
obese humans with metabolic syndrome in conjunction with a
30% reduction in the Streptococcaceae family member
Streptococcus bovis in the small intestine Wrieze et al., 2012).
Obesity driven type II diabetes and metabolic syndrome are con-
sidered chronic inﬂammatory states (Dandona et al., 2005).

In recent studies, the Streptococcaceae family has been shown
to be associated with inﬂammation of various origins and in an
energy independent fashion. First, host physiology can inﬂuence
the composition of the microbiota. For example, poor glucose
control in a cohort of European women was associated with
Streptococcus sp. C150 (Karlsson et al., 2013). Second, micro-
biota composition can modulate host physiology in a variable
way. For example, formula feeding is associated with increased
frequency of pediatric intestinal inﬂammation including necrotiz-
ing enterocolitis; in a mouse model formula feeding increased
Lactococcus at the expense of Lactobacillus and altered host
gene expression to indicate increased oxidative stress, inﬂamma-
tion and impaired defense capacity (Carlisle et al., 2013). Third,
Smith et al. (2013) showed that microbiota composition deter-
mines host health outcome in response to identical dietary shifts;
in this case, microbiota from twin pairs discordant for the disease
Kwashiorkor variably provoke disease depending on the com-
plexity and adequacy of the diet.

Stability and resilience of intestinal microbial diversity is an
active area of research, and it is now recognized that chronic
alterations in host environmental exposure (e.g. diet) and physio-
logical state (e.g. obese) can inﬂuence intestinal microbiota
(Lozupone et al., 2012) and likewise microbiota can respond to
diet to increase/decrease susceptibility of the host to disease
(Carlisle et al., 2013; Smith et al., 2013). Our results further high-
light the need to understand temporal dimension of interventions
to improve efﬁcacy.

5 DISCUSSION

We developed SGSL, a new variable selection procedure that
yields sparsity among predictor groups, subgroups and individ-
uals. For simulated data that had a rich clustering structure, our
method outperformed competing methods with small F DRs and
high geometric means of sensitivity and speciﬁcity. Our method
was capable of capturing features detectable by SGL and Lasso,
but went a step further: it correctly identiﬁed sparsity between
and within subgroups, a feature common to microbiome data.
We applied our method to a gut microbiota dataset to select
important phyla, families and genera that show an association
with CD68 mRNA expression in adipose. After controlling
for diet effects, our preferred method revealed a family level
relationship in which members of the Streptococcaceae family

 

836

112 /810's112umo[pJOJXO's31112u1101u101q/ﬁd11q 111011 popcolumoq

910K ‘09 isnﬁnV no 22

Application to gut microbiome data

 

were linked to CD68 expression in adipose tissue of mice with
chronic obesity. All other methods could not detect this relation-
ship. In the Supplementary Material, we analyze a second micro-
biome dataset, in which only our method and Lasso detects an
important individual bacterial genus.

The data we analyzed were classiﬁed into different taxonomies
following pyrosequencing, which can result in some genera being
more diverse. If consistently sized operational taxa are needed,
one could use operational taxonomic unit clustering (The
Human Microbiome Project Consortium, 2012). Still, regardless
of the classiﬁcation, our method is applicable. Thus, one could
apply our method using the different classiﬁcations to gain
insight into how microbes impact health-related features.
Of course, which classiﬁcation to use depends on the project’s
overall goal and available resources.

ACKNOWLEDGEMENTS

The authors thank Sean H. Adams for the metabolic data, as
well as the editor, associate editor and two referees for their
insightful comments that greatly improved the article.

Funding: Post-doctoral training grant from the National Cancer
Institute (R25T-CA090301) (to T.P.G.). Australian Research
Council (DP110101998) (to S.M.). National Cancer Institute
(R37-CA057030) (to R.J.C.). Texas AgriLife Research (Project
No. 8738) and National Dairy Council administered by the
Dairy Research Institute (to R.L.W.).

Conflict of Interest: none declared.

REFERENCES

Benjamini,Y. and Hochberg,Y. (1995) Controlling the false discovery rate: a
practical and powerful approach to multiple testing. JRSSB, 57, 289—300.
Carlisle,E.M. et al. (2013) Murine gut microbiota and transcriptome are diet

dependent. Ann. Surg., 257, 287—294.

Dandona,P. et al. (2005) Metabolic syndrome: a comprehensive perspective based
on interactions between obesity, diabetes, and inﬂammation. Circulation, 111,
1448—1454.

Donoho,D.L. and J ohnston,J .M. (1994) Ideal spatial adaptation by wavelet shrink-
age. Biometrika, 81, 425—455.

Efron,B. et al. (2004) Least angle regression. Ann. Stat., 32, 407—499.

Friedman,J. et al. (2010) A note on the group Lasso and a sparse-group Lasso.
Technical Report, Preprint arXiv:1001.0736.

Garcia,T.P. et al. (2013) Structured variable selection with q-values. Biostatistics, 14,
695—707.

Hirose,K. et al. (2013) Tuning parameter selection in sparse regression modeling.
Computational Statistics and Data Analysis, 59, 28—40.

Jenatton,R. et al. (2011) Proximal methods for hierarchical sparse coding. Journal
of Machine Learning Research, 12, 2297—2334.

Karlsson,F.H. et al. (2013) Gut metagenome in European women with normal,
impaired and diabetic glucose control. Nature, 498, 99—103.

Kim,S. and Xing,E.P. (2012) Tree-guided group lasso for multi-response regression
with structured sparsity with an applicaton to eQTL mapping. Ann. Stat., 6,
1095—1 1 17.

Kubat,M. et al. (1998) Machine learning for the detection of oil spills in satellite
radar images. Mach. Learn, 30, 195—215.

Liu,J. and Ye,J. (2010) Moreau-Yosida Regularization for Grouped Tree Structure
Learning. In: Advances in Neural Information Processing Systems.

Lozupone,C.A. et al. (2013) Diversity, stability and resilience of the human gut
microbiota. Nature, 489, 220—230.

Martinez,J.G. et al. (2011) Empirical performance of cross validation with oracle
methods in a genomics context. The American Statistician, 65, 223—228.

Miiller,S. and Welsh,A.H. (2010) On model selection curves. International Statistical
Review, 78, 240—256.

Nesterov,Y. (2007) Gradient methods for minimizing composite objective function.
CORE report.

Simon,N. et al. (2012) A sparse-group Lasso. J. Comput. Graph. Stat., 22, 231—245.

Smith,M.I. et al. (2013) Gut microbiomes of Malawian twin pairs discordant for
kwashiorkor. Science, 339, 548—554.

The Human Microbiome Project Consortium. (2012) A framework for human
microbiome research. Nature, 486, 215—221.

Thomas,A.P. et al. (2012) A high calcium diet containing nonfat dry milk reduces
weight gain and associated adipose tissue inﬂammation in diet-induced obese
mice when compared to high calcium alone. Nutr. Metabol., 9, 3.

Thomas,A.P. et al. (2013) A dairy-based high calcium diet improves glucose
homeostatis and reduces steatosis in the context of preexisting obesity.
Obesity, 21, E229—E235.

Tibshirani,R. (1996) Regression shrinkage and selection via the Lasso. JRSSB, 58,
267—288.

Van de Vijver,M.J. et al. (2002) A gene-expression signature as a predictor of
survival in breast cancer. N. EnglJ. Med, 347, 1999—2009.

Vrieze,A. et al. (2012) Transfer of intestinal microbiota from lean donors increases
insulin sensitivity in subjects with metabolic syndrome. Gastroenterology, 143,
913—916.

Yuan,M. and Lin,Y. (2006) Model selection and estimation in regression with
grouped variables. JRSSB, 68, 49—67.

Zhao,P. et al. (2009) The composite absolute penalties family for grouped and
hierarchical variable selection. Ann. Stat., 37, 3468—3497.

 

837

112 /810's112umo[p101x0's31112u1101u101qﬂ2d11q 111011 popcolumoq

910K ‘09 isnﬁnV no 22

