Vol. 29 no. 12 2013, pages 1583—1585
LER   E doi:10.1093/bioinformatics/btt176

 

Data and text mining

Advance Access publication April 14, 2013

Novel algorithms and the benefits of comparative validation

Robert Smith”, Dan Ventura1 and John T. Prince2
1Department of Computer Science and 2Department of Chemistry, Brigham Young University, Provo, UT 84602, USA

Associate Editor: Martin Bishop

 

Contact: 2robsmith@gmail.com

Received on February 22, 2013; revised on April 9, 2013; accepted on
April 10, 2013

Bioinformatic research has produced a large volume of proposed
algorithmic solutions to a host of problems. Whether presented
as a processing step in a clinical experiment or treated in a stand-
alone publication, novel bioinformatic algorithms are often not
subjected to the thorough comparative evaluation endured by
their counterparts in other closely related ﬁelds—such as com-
puter science—where an algorithm unevaluated against extant
methods is considered unpublishable. Two audiences are inter-
ested in algorithmic publications: the practitioner, who may use
the algorithm, and the researcher, who will work to develop
solutions superior to those extant. We argue that failure during
the review/publication process to require comparative evaluation
for novel algorithms is detrimental to both parties.

To demonstrate the dilemma, we conducted a case study of
novel LC—MS alignment algorithms. Of the 48 publications from
2001 to 2012 that present alignment algorithms of which we are
aware, 60% include no comparison with other methods. Another
20% compare their method with one or two others (Fig. 1). Only
two articles compare performance against the state-of-the-art
methods available at the time of publication. Interestingly,
both of these, with six and seven comparisons, respectively,
reuse comparative evaluation performance data and datasets
from a stand-alone review article of six methods (Lange et al.,
2008).

It is natural to wonder whether publication year correlates to
the number of comparisons made. After all, earlier articles would
have fewer methods to compare against. We found no correl-
ation (r = 0.397) between year of publication and number of
comparisons (Table 1). Again, the correlation number would
be even lower if it was not for the fact that someone published
a comparative evaluation of at least some of the extant alignment
methods. Without the reuse of that survey article data, the cor-
relation coefﬁcient would drop to 0.313. These data reinforce the
prevailing paradigm that comparative performance of a new al-
gorithm to existing ones is too time consuming for the author
and reviewers and ought to be the subject of dedicated research
(Ballardini et al., 2011). At least for alignment, such dedicated
comparison studies are few and far between—we are aware of
only one such comparative survey article, even though almost 50
new algorithm articles have been published during the past 11
years (see Lange et al., 2008). Even if these evaluative review
articles were more numerous, there are many reasons why

 

*To whom correspondence should be addressed.

these evaluations ought to be primarily provided in the novel
algorithm publications themselves.

A practitioner relies on the peer review process to ensure that
the methods they are choosing have met a minimum standard of
quality. Though a new method’s description or performance may
be convincing, these qualities alone are insufﬁcient to weigh the
usefulness of an algorithm. Without comparative evaluation,
algorithms that underperform against existing ones can easily
ﬂood a domain, making the practitioner’s task of selecting an
algorithm more difﬁcult with every additional publication.
Besides an extensive literature review caused by the inundation
of articles on the subject, the practitioner must also perform a
comparative evaluation of the existing algorithms, as they have
no mechanism for quantifying the comparative strengths or
weaknesses of the methods from the publications themselves.
As pointed out by a recent article, this process is as time con-
suming as it is difﬁcult, given the oft-encountered difﬁculties of
obtaining and then successfully running someone else’s software
(Ballardini et al., 2011). Extensive comparative analysis reduces
the practitioner’s overall time commitment by reducing the
number of algorithms under consideration as well as by provid-
ing a realistic expectation of performance, hopefully justifying
the inevitable inconvenience of obtaining and operating new
software. Often, evaluation is made much more difﬁcult (if not
impossible) when open source code is omitted in submission.
Although English descriptions and pseudocode assist in building
intuition about an algorithm, they are lossy deﬁnitions that leave
out essential details needed for code implementation. Besides
time savings, requiring source code facilitates more expansive
comparison through automation as well as providing the
reviewers an easy metric to determine whether the method is
suitably formally deﬁned to be distributed and replicated or
whether it is an ad hoc agglomeration.

There are also secondary consequences to consider.
Publication is an incentive that can drive innovation. If novel
algorithms are not required to outperform extant ones, then in-
novation—true forward progress not necessarily achieved by
mere invention—is less likely to occur. Finding the best choice
in an expanding sea of mediocre choices then becomes a
Herculean task sure to exhaust any practitioner. The practical
result is that practitioners stop short of exhaustively evaluating
all the possible options and choose based on some other criteria
(e. g. popularity, ease of use or familiarity). The inevitable out-
come of the algorithm selection crapshoot are results poorer than
what may otherwise have been.

Researchers (the algorithm makers) also suffer when compara-
tive evaluation is neglected. In the face of burgeoning publication
numbers, they encounter the same exhaustive search problem
faced by the practitioner, but they also face a moral

 

© The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 1583

112 ﬁle'slcumo[pJOJXO'sot1chOJutotw/2d11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

R. Smith et al.

 

 

29

28-

 

 

21-

Number
of
Papers

14-

 

  
 
      

 

5

4
-.-.-.

0 1 4 6 7
Number of Comparisons

    

 

 

 

 

 

 

 

 

Fig. 1. A comparison of the number of articles presenting MS alignment
algorithms and the number of competing algorithms against with they
compare. The majority of novel alignment method articles fail to com-
pare against even one extant method

dilemma—the current environment makes it easy to generate
many publications, yet difﬁcult to perform the sort of due dili-
gence comparison advocated in this letter. A good comparison
requires choosing among the several existing evaluation methods,
each of which highlights only speciﬁc behavior. The choice is
non-trivial—in alignment, metrics include metrics that evaluate
the alignment in isolation (Christin et al., 2008, 2010; Van
Nederkassel et al., 2006), in combination with other data-pro-
cessing steps (Ballardini et al., 2011; Lange et al., 2008), globally,
and locally. One must also ﬁnd datasets, which should include
sufﬁcient data representative of the different typical perform-
ance-affecting real-world characteristics (e. g. complexity of the
data, variability of peptide concentration, number of unique and
common peptides, extent and form of retention time shift in the
data, etc.). What’s more, there is no disincentive provided for
publishing work untested against existing methods. Thus, left to
their own devices, will the researcher ever behave in a manner
that is not in his best interest, though it is in the best interest of
the ﬁeld? Apparently, not very often. Our experience suggests
that the pattern we found in alignment algorithms applies to
algorithmic approaches in proteomics and metabolomics gener-
ally, and it may extend to other bioinformatics subﬁelds where
we have less experience.

So what is the solution? The problem, we have found, does not
lie in the lack of venue requirements for performance demonstra-
tion against state-of-the-art algorithms. Interestingly, many of
the articles with zero comparisons came from journals that
explicitly require authors to provide quantitative comparison
with state-of-the-art methods. Similarly, though an openly avail-
able group of standard datasets and metrics as described here
would greatly facilitate the evaluations petitioned for, authors in
other ﬁelds manage to provide comparisons even without stan-
dardized metrics or open frameworks for evaluation.

We suggest that greater care be taken by editors and reviewers
to require novel algorithmic contributions to contain a reason-
able comparative quantitative evaluation with existing methods.
New contributions should also include necessary elements to
facilitate future comparisons with other algorithms such as
source code and parameter setting guidance. Such an effort
will inevitably maximize the outcome of practitioner results,

Table 1. A list of articles presenting novel -omics alignment algorithms

 

 

Publication #Comp Year Venue

Fraga et al. 0 2001 Anal Chem
Hastings et al. 0 2002 Rapid Com in MS
Bylund et al. 1 2002 J Chrom A

Torgrip et al. 2 2003 J Chemometrics
Aberg et al. 0 2004 J Chemometrics
Lee et al. 0 2004 Anal Chim Acta
Tomasi et al. 0 2004 J Chemometrics
Eilers 0 2004 Anal Chem

Vorst et al. 0 2005 Metabolomics
Pierce et al. 0 2005 Anal Chem
Walczak et al. 4 2005 Chem Intel Lab Sys
Baran et al. 0 2006 BM C Bioinformatics
Smith et al. 0 2006 Anal Chem
Sadygov et al. 0 2006 Anal Chem

Fischer et al. 0 2006 Bioinformatics
Jaitly et al. 0 2006 Anal Chem

Prince et al. 1 2006 Anal Chem

Skov et al. 0 2007 J Chemometrics
Yao et al. 0 2007 J Chrom A
Kirchner et al. 0 2007 J Stat Software
Palmblad et al. 0 2007 ASMS

Lange et al. 0 2007 Bioinformatics
Wang et al. 0 2007 Biostatistics
Mueller et al. 0 2007 Proteomics
Listgarten et al. 0 2007 Bioinformatics
Fischer et al. 2 2007 BM C Bioinformatics
Csenki et al. 3 2007 Anal Bioanal Chem
Aberg et al. 0 2008 J Chrom A

De Groot et al. 0 2008 Proteomics

Suits et al. 0 2008 Anal Chem
Shinoda et al. 0 2008 Bioinformatics
Christin et al. 1 2008 Anal Chem
Podwojski et al. 2 2009 Bioinformatics
Befekadu et al. 3 2009 IEE EMBS
Christin et al. 3 2010 JPR

Daszykowski et al. 0 2010 J Chrom A

Tomasi et al. 1 2010 J Chrom A
Bloemberg et al. 1 2010 Chem Intel Lab Sys
Eliasson et al. 0 2011 Curr Pharm Biotech
Sinkov et al. 0 2011 Anal Chim Acta
Befekadu et al. 3 2011 IEE ACM T CBB
Tang et al. 3 2011 Prot Science
Ballardini et al. 6 2011 J Chrom A

Voss et al. 7 2011 Bioinformatics
Zhang 0 2012 ASMS

Struck et al. 1 2012 J Chrom A
Hoekman et al. 2 2012 ASBMB

Kaya et al. 3 2012 Inform Sciences

 

Note: The data have a correlation coefﬁcient of 0.397, suggesting there is no trend
toward comparison against extant algorithms.

encourage the widespread use of the highest-quality tools and
provide researchers an incentive to truly innovate.

Funding: National Science Foundation Graduate Research
Fellowship [DGE-0750759].

Conflict of Interest: none declared.

 

1584

112 ﬁle'sleumo[pJOJXO'sot1emJOJutotw/2d11q IIIOJJ pepeolumoq

910K ‘09 isnﬁnV no :2

Novel algorithm comparative evaluation

 

REFERENCES

Ballardini,R. et al. (2011) MassUntangler: a novel alignment tool for label-free
liquid chromatography—mass spectrometry proteomic data. J. Chromatogr. A,
1218, 8859—8868.

Christin,C. et al. (2008) Optimized time alignment algorithm for LC-MS data:
correlation optimized warping using component detection algorithm-selected
mass chromatograms. Anal. Chem, 80, 7012—7021.

Christin,C. et al. (2010) Time alignment algorithms based on selected mass traces for
complex LC—MS data. J. Proteome Res., 9, 1483—1495.

Lange,E. et al. (2008) Critical assessment of alignment procedures for LC-MS
proteomics and metabolomics measurements. BM C Bioinformatics, 9, 375.
Van Nederkasse1,A. et al. (2006) A comparison of three algorithms for

chromatograms alignment. J. Chromatogr. A, 1118, 199—210.

 

1e ﬁle'sleumo[pJOJXO'sot1emJOJutotw/2d11q IIIOJJ pepeolumoq

910K ‘09 isnﬁnV no 22

