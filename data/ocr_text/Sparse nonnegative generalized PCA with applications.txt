ORIGINAL PAPER

Vol. 27 no. 21 2011, pages 3029-3035
doi: 10. 1093/bioinformatics/btr522

 

Systems biology

Advance Access publication September 19, 2011

Sparse non-negative generalized PCA with applications to

metabolomics

Genevera |. Allen1’2’* and Mirjana Maletic-Savatic1

1Department of Pediatrics—Neurology, Baylor College of Medicine, Jan and Dan Duncan Neurological Research
Institute at Texas Children’s Hospital, 1250 Moursund St. Suite 1365, Houston, TX 77030 and 2Department of
Statistics, Rice University, 6100 Main St. MS—138, Houston, TX 77005, USA

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: Nuclear magnetic resonance (NMR) spectroscopy has
been used to study mixtures of metabolites in biological samples.
This technology produces a spectrum for each sample depicting the
chemical shifts at which an unknown number of latent metabolites
resonate. The interpretation of this data with common multivariate
exploratory methods such as principal components analysis (PCA) is
limited due to high-dimensionality, non-negativity of the underlying
spectra and dependencies at adjacent chemical shifts.

Results: We develop a novel modification of PCA that is appropriate
for analysis of NMR data, entitled Sparse Non-Negative Generalized
PCA. This method yields interpretable principal components and
loading vectors that select important features and directly account for
both the non-negativity of the underlying spectra and dependencies
at adjacent chemical shifts. Through the reanalysis of experimental
NMR data on five purified neural cell types, we demonstrate the
utility of our methods for dimension reduction, pattern recognition,
sample exploration and feature selection. Our methods lead to the
identification of novel metabolites that reflect the differences between
these cell types.

Availability: www.stat.rice.edu/~gallen/software.html

Contact: gallen@rice.edu

Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on June 27, 2011; revised on August 23, 2011; accepted
on September 10, 2011

1 INTRODUCTION

Metabolomics, one of the newest ﬁelds within systems biology
approaches to biomarker discovery in medicine, investigates an
abundant pool of small molecules present in cells and tissues
(Bollard et al., 2005; Hollywood at al., 2006; Holmes et al.,
2008). One of the commonly used technologies for acquisition
of this data is nuclear magnetic resonance (NMR) spectroscopy.
It is a high—throughput technology for acquiring reproducible and
resolved spectra that can be used to study the complete metabolic
proﬁle of a biological sample (Nicholson and Lindon, 2008). The
spectra contain thousands of chemical resonances, which may
belong to hundreds of metabolites (De Graaf, 2007). However, many
metabolites resonate at multiple resonances and thus, unlike the
typical DNA microarray data, different metabolite spectra overlap

 

*To whom correspondence should be addressed.

and introduce complexities that need to be addressed by signal
processing and careful statistical analysis (Ebbels and Cavill, 2009;
Weljie et al., 2006).

As understanding relationships between the set of biological
samples and the underlying spectra is a challenge, principal
components analysis (PCA) is commonly used for both dimension
reduction and pattern recognition with NMR data (Coen et al.,
2008; Dunn et al., 2005; Goodacre et al., 2004; MaletiC—Savatic
et al., 2008; Weckwerth and Morgenthal, 2005). In high—dimensional
settings, however, it is well known that PCA can perform poorly
due to the large number of irrelevant variables (J ohnstone and Lu,
2009). Hence, many have proposed to incorporate sparsity into the
principal component directions, thus selecting important features
(Johnstone and Lu, 2009; Jolliffe et al., 2003; Shen and Huang,
2008; Zou et al., 2006). Non—negativity of the matrix factors, or
principal component directions, has also been proposed in a number
of settings to improve interpretability of the factors (Lee and Seung,
1999; Sajda et al., 2004). Several recent papers have combined
these concepts to encourage both sparsity and non—negativity into
the model (Hoyer, 2004; Kim and Park, 2007; Zass and Shashua,
2007).

In this article, we make the following statistical contributions:
(i) propose a framework for incorporating sparsity, known structural
dependencies and non—negativity into the principal component
(PC) loadings and (ii) develop a fast, computationally efﬁcient
algorithm to compute these in high—dimensional settings. This
work is presented in Section 2. Then, in Section 3, we evaluate
the performance of our methods on real NMR data. We also
demonstrate how to interpret the PC loadings to understand
important biological patterns and identify candidate metabolites. In
Section 4, we conclude with a summary of the implications of our
work and future areas of research.

2 METHODS

We introduce a framework for PCA that incorporates structural dependencies,
sparsity and non—negativity to better understand relationships between the
samples and recognize patterns among the variables.

2.1 Review: generalized PCA

Recently, Allen et al. (2011) introduced a new matrix decomposition, the
Generalized Least Squares Matrix Decomposition (GMD), and showed
how this decomposition can be used to generalized PCA by directly
incorporating known structural information or dependencies. Here, we

 

© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com 3029

112 /§JO'SIBUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdnq mm; popeoIIJ/noq

9IOZ ‘091sn8nv uo ::

G.I.AIIen and M.MaIetié-Savatié

 

review the Generalized PCA (GPCA) problem and speciﬁcally discuss its
utility in the context of spectroscopy data.

We observe data, Xeél‘tnxp, for n samples and p variables that has
previously been normalized. (With NMR data, this includes baseline
correction, normalizing by the integral of the spectrum and standardizing the
variables at each ppm.) Let R e W” W be a positive semi—deﬁnite matrix called
the quadratic operator that captures the noise structure in the data. Then,
GPCA seeks the linear combination of variables maximizing the sample
variance in the inner product space induced by R:

maximize vlfRXTXRv/C
Vk
subject to ngvk =1 & ngvk/ :0 v k/ <k. (1)

The k—th GPC is zk=XRvk. If R=I, then we have the standard PCA
optimization problem. Additionally, Allen et al. (2011) have shown that an
extension of the power method for computing eigenvectors can be used to
calculate these GPCs.

GPCA can be used to directly account for dependencies between adjacent
variables in the spectra. The quadratic operator, R, behaves like an inverse
covariance matrix of multivariate normal data (Allen et al., 2011). We can
let R encode the inverse covariance of dependencies or structure in the
data that do not contribute, and are independent of the signal of interest.
The resulting GPCA solution can be interpreted as a decomposition of the
covariance given by: Cov(X)=VD2VT+R_1, where D2 is diagonal with
entries, d]? =v£RXTXRvk. With NMR spectroscopy, variables at adjacent
chemical shifts are strongly positively correlated. These dependencies,
however, do not contribute to the biological signal, or the peaks and
groups of peaks that vary across the samples. Thus, letting R encode these
dependencies between adjacent chemical shifts, allows GPCA to ignore
the biologically irrelevant structure and estimate more of the biologically
relevant variability.

2.1.1 Kernel smoothers as quadratic operators To this end, we employ
kernel smoothers that are a function of the distance between the variables.
Take the p>< p distance matrix, D, where Di]- is the pair—wise distance
between variables i and j. Then, the quadratic operator R can be taken
as le=k(Dij, y) where k() is a kernel and y is the smoothing parameter.
Standard kernels used in local linear regression, such as the Gaussian

D3.
kernel, k(D,-j, y) = W exp(— # ), can be employed. If y = 10, for example,

then elements in the kernel smoother are weighted according to a normal
distribution with a SD of 10 distance units apart. For NMR data, the GPCA
loading vectors multiply the data through a range of adjacent chemical shifts
weighted by the kernel smoother. Thus, we directly account for dependencies
between neighboring variables.

2.2 Sparse non-negative GPCA

While GPCA directly accounts for biologically irrelevant structure in NMR
data, the problems of high dimensionality and the non—negativity of the
spectra are left unsolved. To this end, we introduce Sparse Non—Negative
GPCA, which gives interpretable PCA direction vectors by incorporating
feature selection through sparsity and by constraining the loadings to be
non—negative.

2.2.] Problem and solution We introduce the single—factor sparse non—
negative GPCA optimization problem. Let ueéﬁ”, A :0, and R and v as
deﬁned previously, and consider the following:

maximize uTXRv—Allvlll
V,ll

subjectto uTugl, vTRv51,&v30. (2)

The PCA loading vectors, vk are constrained to be non—negative, and sparsity
is encouraged via the ill—norm or lasso penalty on the loadings (Tibshirani,
1996). Here, A is a penalty parameter controlling the amount of sparsity.

This simple criterion for the single—factor sparse non—negative GPCA is
related to many existing approaches to sparse PCA and non—negative PCA.
First, if A :0, the non—negativity constraint is removed, and the remaining
inequalities hold with equality, Equation (2) is equivalent to the GPCA
or GMD optimization problem (Allen et al., 2011). This is related to the
Lagrangian form of the sparse PCA approach in Witten et al. (2009), and
is also a constrained version of the regression—based sparse PCA approach
of Shen and Huang (2008). This single factor problem, however, differs
from the multicomponent problem for sparse non—negative PCA of Zass and
Shashua (2007). Also, notice that we do not require subsequent direction
vectors to be orthogonal. Many have noted that orthogonality of sparse PCA
factors is unwarranted and hence is often not imposed (Joume’e et al., 2010;
Shen and Huang, 2008; Zou et al., 2006).

Our single—factor approach has many advantages. Notice that the problem
is biconcave, meaning that it is concave in v with u ﬁxed and in u with v ﬁxed.
This leads to a simple maximization strategy that is guaranteed to increase
the objective and converge to a local maximum: alternate maximizing with
respect to u and v. These coordinate—wise maximization problems turn out
to have a simple solution:

PROPOSITION 1. Let v be the minimizer of the following:
1
minimize EllXTu—vllﬁ—Mlvlll subject to v30. (3)
V

Then, the coordinate updates, u* and v*, maximizing the single-factor sparse
non-negative GPCA problem, (2), are given by:

v.2 {Wilvlln ifllvlln >0 * XRv

0 otherwise, — IIXRVI l2.

(All proofs are given in the Supplementary Materials).

The solution to the single—factor sparse non—negative GPCA problem, (2),
can be obtained by solving a simple lasso penalized non—negative regression
problem. This non—negative regression problem in turn can be solved via a
fast coordinate descent algorithm:

PROPOSITION 2. The solution to (3) can be obtained via coordinate descent
' . A . _ L . T _ . .A ._ .
wzth updates. v] — Rjj (RUX u Rméj v75] A)+, where RU denotes the row

elements of column j of R and ()+ denotes the positive part.

This coordinate descent approach is related to the fast shooting algorithms
of Friedman et al. (2010), and the speed can be further improved
by employing active set learning and warm starts. We note that this
algorithmic approach is a major improvement in terms of computational
efﬁciency over the least angle—based approach to the non—negative lasso of
Renard et al. (2008).

2.2.2 Algorithm We have presented an optimization problem and solution
to the single—factor sparse non—negative GPCA problem, and we are also
interested in extracting multiple components. Then, we employ a greedy
approach to estimating multiple components that is closely related to the
power method algorithm for computing eigenvectors. This algorithm is
summarized in Algorithm 1.

The sparse non—negative GPCA algorithm begins with the standardized
data and computes the ﬁrst component by solving the single—factor problem
via coordinate descent. Subsequent components are calculated by solving the
single—factor problem for the residual where the previously computed outer
product has been removed. Each component is calculated in a greedy manner
and is hence conditional on the previously estimated components. Thus, the
components are not necessarily ordered in terms of the amount of variance
they explain. This approach is common among existing methods for sparse
PCA (Allen et al. , 2011; Lee et al. , 2010; Shen and Huang, 2008; Witten et al. ,
2009; Zou et al., 2006). As the dominant operation in our algorithm is solving
a non—negative lasso problem, the computational complexity is 0(n3). While
traditional PCA methods may be faster to compute, our algorithm requires
comparable computational time to existing sparse and/0r non—negative PCA
methods (Shen and Huang, 2008; Zass and Shashua, 2007).

 

3030

112 /§JO'S[BUJHOIPJOJXO'SOIlBIHJOJUIOIQ/ﬁdllq 11101; popeoIII/vxoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse non-negative GPCA

 

 

Algorithm 1 Sparse Non—Negative GPCA Algorithm
(1)

 

1. Standardize the columns of X and set X =X

2. Fork=1...K:

(a) Initialize uk and V k to the ﬁrst left and right GMD factor
A (k)
of X

(b) Repeat until convergence:

, respectively.

Mk)

X RV

' SetuszW—k.
IIX RVkHZ

- Forj=1,...,p,1,...,p,1,...
— Setv-=L(R -XTu’—R- v -—x)
J R},- r.I 1,791 #1 +-

9/lleIR if H‘A’HR >0

0 Set vk = _
0 otherw1se.

(c) Set dk =u£ X(k)Rvk.

A(k+1):X(k)

(d) Set x —ukdk vg.

3. Return principal components, Z = [XRV1 , . . .XRVK], loading
vectors V =[V1, ...VK], sample principal components U:
[u1,...,uK] and scaling factors D=diag(d1,...dK).

 

2.2.3 Selecting regularization parameters The amount of sparsity in the
GPCA loading vectors, v, is controlled by the regularization parameter, A.
We seek a data—driven mechanism for selecting the amount of sparsity in
each of the components. To this end, we employ the A value that minimizes
the following Bayesian Information Criterion (BIC) for each factor, vk:
BIC(x)=1og(||x—dkukv,{nﬁ/np)+wcﬁd). Here, @(x) denotes the
degrees of freedom associated with the value of A. For the non—negative
lasso, d7“(A)= |{v(A)}|, that is the number of non—zero elements of v. This
follows from a result of Tibshirani and Taylor (2011). The criterion can
be derived from considering each update in the power method algorithm
as a generalized least squares problem with unknown variance (Allen et al.,
2011; Lee et al., 2010). While other methods such as cross—validation may be
employed to ﬁnd the optimal regularization parameter, minimizing the BIC
is computationally more efﬁcient and leads to greater ﬂexibility to select
differing penalty parameters for each component.

2.2.4 Amount of variance explained When using PCA methods for
dimension reduction and exploratory analysis, the amount of variance
explained by each principal component is an important measure to consider.
As our GPCA and sparse non—negative GPCA methods incorporate structural
information through the quadratic operator, R, the formulas for calculating
the variance explained by each component are altered.

PROPOSITION 3.

(i) The proportion of variance explained by the k-th GPC is
ngxTXRvk /tr(XRXT).

(ii) Deﬁne Vk=[v1,...vk] and xk=XRvk(v,{Rvk)‘1v,{. Then, the
cumulative proportion of variance explained by the k-th sparse
non-negative GPC is tr(XkRX,€)/tr(XRXT).

Note that the proportion of variance explained by individual sparse non—
negative GPCs can be found by taking the differences of the cumulative
proportion explained. Thus, the proportion of variance explained by our
methods can be interpreted as the ratio of the R—norm projected sample
variance of the k—th linear projection relative to the total variance of the data
in the R—norm. Notice that as the sparse non—negative GPCA factors are not

constrained to be orthogonal, the sample variance explained must be adjusted
for possible correlations among the factors as discussed in Shen and Huang
(2008). Given these results, we can compare our methods to traditional PCA
and sparse PCA methods in terms of the variance explained and dimension
reduction.

3 RESULTS

We evaluate the utility of GPCA and Sparse Non—Negative GPCA for
metabolomics through comparisons on real NMR data. (Simulation
studies are given in the Supplementary Materials.) We use a
dataset with 27 samples acquired by in vitro 1D H—NMR on ﬁve
neural cell types: neurons, neural stem cells, microglia, astrocytes
and oligodendrocytes (Manganas et al., 2007). [For methodology
used on cell culturing, see Manganas et al. (2007)] The data are
preprocessed in the traditional manner (Dunn et al., 2005): after
acquisition, functional spectra is discretized by binning variables
into bins of size 0.04 ppms yielding a total of 2394 variables. For
each sample, the spectra are baseline corrected and normalized to
their integral. Before applying multivariate techniques, the variables
are standardized to have mean zero and variance one. While typically
PCA is applied to unsupervised or unlabeled data, we apply our
methods to this labeled data so that we may test their performance
in terms of sample exploration, dimension reduction, pattern
recognition and feature selection when the biological relationships
between samples clear.

We compare our GPCA method to traditional PCA and our sparse
non—negative GPCA method to sparse non—negative PCA. The later
is implemented via Algorithm 1 by setting R :1. The BIC method
is used to select penalty parameters for both sparse PCA methods
and the ﬁrst 15 PCs are calculated for all methods. For the GPCA
methods, the quadratic operator, R, was taken to be a Gaussian
kernel smoother with smoothing parameter, y=20. Five possible
values of y were considered, y=5, 10, 15,20,25, with y chosen to
explain the most sample variance.

In Figure 1, we compare scatter plots of the normalized sample
PCs for the four methods. Notice that the scatterplots of all methods
exhibit clustering of the neuron and neural stem cell samples,
while the other cell types are more scattered. Sparse methods
and especially sparse non—negative GPCA, however, cluster the
remaining cell types better, illustrating the utility of incorporating
sparsity in high—dimensional data analysis.

Next, we compare the methods in terms of dimension reduction
in Figure 2. As sparse PCA methods naturally explain less
sample variance than PCA methods, we compare the two sets of
methods separately. Also note that as sparse PCA methods calculate
components in a greedy manner, they are not necessarily ordered in
terms of how much variance they explain. Overall, by incorporating
the known structure of spectroscopy data into the PCA problem,
the GPCA methods explain a larger portion of the sample variance.
Thus, the reduction of dimensions for GPCA methods is greater.
This behavior is especially pronounced for the sparse non—negative
methods where seven PCs explain over 90% of the variance for
sparse non—negative GPCA, while 15 PCs are needed to explain the
same amount of variance for sparse non—negative PCA. Thus, sparse
non—negative GPCA provides over 50% more dimension reduction
than sparse non—negative PCA. GPCA methods demonstrate a clear
advantage over traditional PCA methods in terms of variance
explained and dimension reduction.

 

3031

112 /§.IO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

G.I.AIIen and M.MaIetié-Savatié

 

2 2 2
or- i :1. * uI-rl'
1— W W
U” I: H" GU 1' r :10
D. ' El. ‘ |'.'L

 

2 $ 2 'l- 2_
1,. +' m e e:
. 'I
go ‘ go. . go
‘5' LII-il- w

 

2 2 2
E a
1— *.4- N I m
E D -.. . E D ..I e E D
m ﬁo- m "' _ 0'21
. : I.
-z -z ' -2
2 ' 2 2
I ll .
1— " F N -|" m
E o " _ E o .3 E o
re <3 :5
no ‘. . to I e:
l

 

 

.* 2  " Hiomglra
. 1 1’ 1T 1' + ‘ fairer:er
o- E D * I'll: ‘ ﬁllgodend'ooﬂe
" " Neural Stem Gall
* Neuron
: 2 e I. * Hieroglra
‘ i 1- *' * earn e
I II I: U  I a . or!
x "I E + * ‘ Oligodenclrooﬂe
' Neural Elem Gel!
* Neuron
2 :i - Mioroglia
- 1- t * Aetrooyrle
. r s g D ‘i _ J. Oligodenol'ooﬁe
i. - Neural Stem our
'2 * Neuron
E l. * Mioroglia.
a , "H * Fatrooyte
. g 0 1 + ﬂligodenrkowle
*“ T f” ' -..- - Neural Stem our
" Neuron

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Fig. 1. Scatter plots of normalized sample PCs for the neural cell types data. Results from PCA, GPCA, Sparse Non—Negative PCA (SPCA) and Sparse
Non—Negative GPCA (SGPCA) are compared for the ﬁve neural cell types. Sparse methods (bottom rows) demonstrate clearer separation of samples from

different cell types.

     
 

 

 

 

 

PIS-ﬂ. Sparse Non-Negative PM
0.5 . D '
E - Tracinlonal b - Traduonel
g :I Generalized Generalized
ljj I] 4
Lu
U
 I12
g
g

 

 

 

CI
123456.739“)

12345673910

 

 

 

 

 

 

 

 

 

 

 

 

 

 

ﬂ Poe Pﬂe

III

E

T1351 1  fl II":II-l-ﬁ_.ltl_ﬂn_."

a 9:5' ..a--“ J.n-"""°'“

d 2" F .r'

e r err

a oe 0.5 

ELI -C

g "'9" Traicﬁ1imal '1 '1'" Tradfiu'al

3. ‘13" Generalized ‘13" Generalized

E o o

g z 4 e 3 1o 2 4 e 3 1o
F‘D's F'Ce

Fig. 2. Amount of variance explained by the PCs for the ﬁve neural cell
type data. Comparison of the percentage of variance explained by individual
PCs (top panel) and cumulative percentage of variance explained (bottom)
between PCA and GPCA (left), and sparse non—negative PCA and sparse
non—negative GPCA (right). GPCA methods explain larger proportions of
the sample variance.

A major motivation of our work is to incorporate feature selection
into the traditional PCA framework and assess its utility for NMR
data. We compare the degree of sparsity seen in the PCs for the sparse
non—negative PCA and GPCA methods in Figure 3. By directly
accounting for the dependencies at adjacent chemical shifts, sparse

Degree ofsrzerenv- Eperee Non-Negau-re PCA Loadmge

 

 

6:] r 1 I I r r I 1 I I

E -Taadilional

% 40 EGeneralrzed _
ED

LI'.I

III

a

E 9'3 '
it

e2

 

 

 

 

 

 

 

 

 

1'33 A _ t
- —. I-I-I l- -
E I..- l— - -'*I--_
e *' .e
z ,-ee—--*
III-I'a --
E EU a- i- "' '- _
E
3.3 --¢-- Traditional
e3 "‘9" Generalized
D L l L l

T5 EDD ﬁ 93
as variance Explained

Fig. 3. Proportion of features selected on the ﬁve neural cell types data by
sparse non—negative PCA and GPCA for individual PCs (top) and by the
cumulative PCs (bottom). Sparse non—negative GPCA explains more of the
sample variance with fewer features selected.

non—negative GPCA gives a greater degree of sparsity, yielding a
more parsimonious model. The GPCA method also explains more
of the variance in the data with fewer features selected, an important
attribute. Greater sparsity means that one needs to consider fewer
peaks when explaining the patterns in the data. Also, a parsimonious

 

3032

112 /3.IO'SIBUJHOIpJOJXO'SOIlBIIlJOJUIOIQ/ﬂdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse non-negative GPCA

 

Sprase Non-Negative GPCA Loadings PC1

(’1 (f
_\
N
N
01

 

05- gk l
_ 7‘ ~, _7 J».  ﬂisyﬂ‘: - V _e ‘ivj‘ti - / \

(J'I
_\
N
N
01

U1

_\
A.
01.
N
N.
01

 

 

(11

A

_\ i
01 .
N

N

U1

01
A
—\ .
01.
N
N
01

 

Scaled Intensity Scaled Intensity Scaled Intensity Scaled Intensity Scaled Intensity Scaled Intensity Scaled Intensity
O
01
|

 

 

Mean Spectra Neural Stem Cell
Mean Spectra Neuron
Sparse Non—Negative GPCA Loadings

 

 

 

 

 

 

 

 

 

 

 

 

0.5 1 1.5 2 2.5
Chemical Shift (ppm)

Microglia
Oligodendrocyte

Astrocyte
Neuron

Neural Stem Cell

 

 

 

 

3 3.5 4 P02

Microglia
Oligodendrocyte

Astrocyte
Neuron

_, _ w. , \..  Neural Stem Cell
3 3.5 4 PC3

Microglia
Oligodendrocyte

Astrocyte
Neuron

47 _ t V, s .  Neural Stem Cell
3 3.5 4 PC4

Microglia
Oligodendrocyte

Astrocyte
Neuron

 , Neural Stem Cell
3.5 4 PC5

Microglia
Oligodendrocyte

Astrocyte
Neuron

_ ._ s H ., Neural Stem Cell
3 3.5 4 P06

Microglia
Oligodendrocyte

Astrocyte
Neuron

' .  2 .2 A _ Neural Stem Cell
3 3.5 4 P07

Microglia
Oligodendrocyte

Astrocyte
Neuron

Neural Stem Cell

 

 

 

 

 

 

 

 

0.5
Mean Spectra Glia PC 0
Mean Spectra Microglia Heatmaps

-0.5

Fig. 4. Sparse non—negative GPCA loadings and sample PC heatmaps for the ﬁrst seven PCs, which explain over 90% of the sample variance. Scaled PC
loadings are superimposed on the average scaled spectra of neural stem cells, neurons, microglia and ‘Glia’, which includes oligodendrocytes and astrocytes.
Sparse non—negative GPCA loadings reveal important patterns across the samples and spikes in the loadings denote the location of peaks that vary greatly across
the samples. For example, PC3 exhibits peaks that have higher intensities in neural stem cells, while the peaks selected by PC5 have higher concentrations

in microglia.

PC loading vector indicates that more irrelevant variables have been
discarded from the model. As sparse non—negative PCA does not
incorporate structural information, many more variables are selected
as the method tries to explain both the dependencies between
neighboring chemical shifts and the biological variation. By directly
accounting for these spatial dependencies, however, sparse non—
negative GPCA is free to select features that explain the biological
variation in the samples. Overall, these results indicate that sparse
non—negative GPCA outperforms PCA, GPCA and sparse non—
negative PCA in terms of sample exploration, dimension reduction
and feature selection.

Sparse non—negative GPCA can be used to understand important
biological patterns in the NMR data. Figure 4 gives the sparse non—
negative GPCA loadings for the ﬁrst seven sparse non—negative
GPCs which explain over 90% of the variance in the data. Along
with these loadings, we give heatmaps of the sample PCs to show
how each of the samples contribute to the patterns seen in the
loading vectors. The loading vectors are scaled and superimposed
on the mean spectra from neurons, neural stem cells, microglia and
‘glia’, which includes astrocytes and oligodendrocytes. (Plots of the
loading vectors for PCA, GPCA and sparse non—negative PCA are
given in the Supplementary Materials.)

 

3033

112 /3.IO'SIBUJHOIpJOJXO'SOIlBIIlJOJUIOIQ/ﬂdllq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

G.I.AIIen and M.MaIetié-Savatié

 

Table 1. Locations in parts per million (ppm) of the most important peaks
identiﬁed by the ﬁrst seven sparse non—negative GPCA loadings

 

 

Peak location (ppm) Cell types Metabolites
0.96 Neuron, microglia

1.19 Microglia, neuron

1.28 Neural stem cell, ogliodendrocyte Lipid moiety
1.48 Ogliodendrocyte

2.02 Neuron NAA

2.65 Astrocyte

3.01 Neuron

3.04 Ogliodendrocyte Creatine
3.23 Ogliodendrocyte, neuron, astrocyte Choline
3.43 Ogliodendrocyte

3.66 Microglia

 

Boldfaced locations denote peaks with especially strong signals as indicated by the
loading vectors. Information on which cell types exhibited the highest intensity as well
as metabolites that have previously been identiﬁed at the locations is also given.

By constraining the PC loading vectors to be non—negative,
interpretation of the relationships between the features selected and
the samples is made simpler. Spikes selected in the loading vectors
indicate peaks that vary greatly across the samples. The positive
sample PCs or scores (shown in the heatmaps of Fig. 4) have higher
intensities at the peaks selected by the associated loading vector.
The groups of spikes selected by each loading vector then indicate
an important metabolic pattern that is up— or downregulated in each
sample as revealed by the sample PCs. These metabolic patterns will
consist of both metabolites that resonate at multiple peaks and also
metabolites belonging to the same pathway. Thus, further testing of
the peaks selected by our methods should be done to resolve the
speciﬁc metabolites responsible for the metabolic pattern identiﬁed.

Considering the ﬁrst loading vector, the features selected are at
chemical shifts where there are few peaks. This occurs as the ﬁrst
direction vector accounts for the baseline height difference between
the samples due to normalization to the integral. This behavior is
observed also in the ﬁrst loading vector for the three competing
methods (shown in the Supplementary Materials). Loading vectors
two and three denote peaks that have higher concentrations in neural
stem cells. Loading vector four exhibits a pattern of peaks that are
upregulated in neurons and microglia, while the peaks selected in
loading vectors ﬁve have higher intensities in microglia. Peaks in
loading vectors six and seven denote metabolites that are upregulated
in astrocytes and both oligodendrocytes and astrocytes, respectively.

In Table 1, we give the locations of important selected peaks
in parts per million, the cell types in which these peaks exhibited
the highest intensities, as well as metabolites that have previously
been identiﬁed at these peak locations. A previous analysis of
this data using traditional PCA methods identiﬁed the peaks at
1.28, 2.02 and 3.23 ppm as higher in neural stem cells, neurons,
and astrocytes, respectively (Manganas et al., 2007). Our methods
however, identify several other novel biomarkers, especially for
microglia. In future work, we will identify candidate metabolites
for the novel biomarkers in Table 1 via public databases such
as BioMagResBank (BMRB) (Ulrich et al., 2008), metabolite
identiﬁcation models (Crockford et al., 2005; Zheng et al., 2011)
and spike—in experiments. Thus, our results are consistent with

the existing literature, andt also identify novel biomarkers for
consideration.

These results demonstrate the many advantages of using sparse
non—negative GPCA for NMR spectroscopy. Not only does our
method exhibit greater dimension reduction, better clustering
of samples according to biological relationships and provide
more feature selection than competing methods, but also yields
easily interpretable results that lead to understanding of important
biological patterns in the spectra.

4 DISCUSSION

We have presented a framework for incorporating structural
dependencies, sparsity and non—negativity into PCA. By comparing
our techniques to traditional PCA methods on real NMR data, we
have demonstrated the many advantages of our methods. Future
areas of research are to extend our framework to supervised
multivariate analysis techniques such as partial least squares and
linear discriminant to better classify NMR samples.

While we have demonstrated our methods on 1D H—NMR
spectroscopy, our approach can be applied to many other
high—throughput metabolomics technologies. Mass spectrometry
and other spectroscopy techniques also produce a spectrum of
non—negative variables. Additionally, many researchers employ
multidimensional spectroscopy to further identify metabolites in
a sample (De Graaf, 2007). In this data, each sample consists of
a matrix of spectroscopy variables. Sparse non—negative GPCA
can be applied to this multidimensional data in a straightforward
manner by vectorizing the matrix of variables and employing a
2D kernel smoother over the lattice of variables. As a future area
of research, one can also extend our methods to tensors or higher
order PCA to ﬁnd patterns and achieve dimension reduction for this
multidimensional metabolomics data.

In addition to metabolomics data, our methods are general and
hence applicable to a variety of other structured biomedical data.
As the dependencies of the noise must be known to construct the
quadratic operator, our methods can be used to ﬁnd patterns in
data where these noise dependencies are well established. Possible
further applications of our methods then include copy number
variation and methylation data in which variables strongly depend
on known chromosomal location, and microscopy, neuroimaging
and other bio—medical imaging data in which pixels are spatially
correlated with adjacent pixels.

In conclusion, we have developed a novel modiﬁcation of PCA
particularly suited to the challenges associated with analyzing NMR
data. While our methods show numerous advantages in the analysis
of metabolomics data, there are still many open research problems
and potential extensions related to our work.

ACKNOWLEDGEMENTS

The authors would like to thank Han Xu, Yanli Chen, Dr Li—Hua
Ma, Dr Marina Vannucci and Dr Juan Botas for helpful discussions
related to this work.

F unding: National Institute of Neurological Disorders and Stroke
(R21N805875—1 and K08NSOO44276); McKnight Endowment
Fund; DANA Foundation; Lisa and Robert Lourie Foundation and

 

3034

112 /810's112u1nofp101x0'sor112u1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

Sparse non-negative GPCA

 

the NIH Intellectual and Developmental Disabilities Research Grant
(P30HD024064) (to M.M.—S.).

Conﬂict of Interest: none declared

REFERENCES

Allen,G.l. et al. (2011) A generalized least squares matrix decomposition. Technical
Report No. TR2011-03. Rice University, USA.

Bollard,M. et al. (2005) NMR-based metabonornic approaches for evaluating
physiological inﬂuences on bioﬂuid composition. NMR Biomed, 18, 143—162.
Coen,M. et al. (2008) NMR-based metabolic proﬁling and metabonornic approaches to

problems in molecular toxicology. Chem. Res. Toxicol, 21, 9—27.

Crockford,D. et al. (2005) Curve-ﬁtting method for direct quantitation of compounds in
complex biological mixtures using 1h NMR: application in metabonornic toxicology
studies. Anal. Chem, 77, 4556—4562.

De Graaf,R.A. (2007) In Vzvo NMR Spectroscopy: Principles and Techniques. John
Wiley & Sons, West Sussex, England.

Dunn,W. et al. (2005) Measuring the metabolome: current analytical technologies.
Analyst, 130, 606—625.

Ebbels,T. and Cavill,R. (2009) Bioinformatic methods in NMR-based metabolic
proﬁling. Progress in Nuclear Magnetic Resonance Spectroscopy, 55, 361—374.
Friedman,J. et al. (2010) Regularization paths for generalized linear models via

coordinate descent. J. Stat. Softw, 33, 1.

G00dacre,R. et al. (2004) Metabolomics by numbers: acquiring and understanding
global metabolite data. Trends Biotechnol, 22, 245—252.

Hollywood,K. et al. (2006) Metabolomics: current technologies and future trends.
Proteomics, 6, 4716—4723.

Holmes,E. et al. (2008) Metabolic phenotyping in health and disease. Cell, 134,
714—717.

Hoyer,P. (2004) N on-negative matrix factorization with sparseness constraints. J. Mach.
Learn. Res., 5, 1457—1469.

J0hnst0ne,l. and Lu,A. (2009) On consistency and sparsity for principal components
analysis in high dimensions. J. Am. Stat. Assoc, 104, 682—693.

Jolliffe,l. et al. (2003) A modiﬁed principal component technique based on the LASSO.
J. Comput. Graph. Stat., 12, 531—547.

J0urne’e,M. et al. (2010) Generalized power method for sparse principal component
analysis. J. Mach. Learn. Res., 11, 517—553.

Kim,H. and Park,H. (2007) Sparse non-negative matrix factorizations via
alternating non-negativity-constrained least squares for microarray data analysis.
Bioinformatics, 23, 1495.

Lee,D. and Seung,H. (1999) Learning the parts of objects by non-negative matrix
factorization. Nature, 401, 788—791.

Lee,M. et al. (2010) Biclustering via sparse singular value decomposition. Biometrics,
66, 1087—1095.

Maletié-Savatic,M. et al. (2008) Metabolomics of neural progenitor cells: a novel
approach to biomarker discovery. Cold Spring Harb. Symp. Quant. Biol, 73,
389—401.

Manganas,L. et al. (2007) Magnetic resonance spectroscopy identiﬁes neural progenitor
cells in the live human brain. Science, 318, 980.

Nicholson,J. and Lindon,J. (2008) Systems biology: metabonornics. Nature, 455,
1054—1056.

Renard,B. et al. (2008) NITPICK: peak identiﬁcation for mass spectrometry data. BMC
Bioinformatics, 9, 355.

Sajda,P. et al. (2004) N onnegative matrix factorization for rapid recovery of constituent
spectra in magnetic resonance chemical shift imaging of the brain. Med. Imag. IEEE
Trans. 23, 1453—1465.

Shen,H. and Huang,J. (2008) Sparse principal component analysis via regularized low
rank matrix approximation. J. Multivar. Anal, 99, 1015—1034.

Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. R. Stat. Soc.
Ser. B , 58, 267—288.

Tibshirani,R. and Taylor,J. (2011) The solution path of the generalized lasso. Ann. Stat,
39, 1335—1371.

Ulrich,E. et al. (2008) Biomagresbank. Nucleic Acids Res., 36 (Suppl. 1), D402.

Weckwerth,W. and Morgenthal,K. (2005) Metabolomics: from pattern recognition to
biological interpretation. Drug Discov. Today, 10, 1551—1558.

Weljie,A. et al. (2006) Targeted proﬁling: quantitative analysis of 1h NMR
metabolomics data. Anal. Chem, 78, 4430—4442.

Witten,D.M. et al. (2009) A penalized matrix decomposition, with applications to sparse
principal components and canonical correlation analysis. Biostatistics, 10, 515—534.

Zass,R. and Shashua,A. (2007) N onnegative sparse PCA. Adv. Neural Informat. Process.
Syst., 19, 1561.

Zheng,C. et al. (2011) Identiﬁcation and quantiﬁcation of metabolites in 1H NMR
spectra by Bayesian model selection. Bioinformatics, 27, 1637.

Zou,H. et al. (2006) Sparse principal component analysis. J. Comput. Graph. Stat, 15,
265—286.

 

3035

112 /§.IO'SIBUJHOprOJXO'SOIlBIHJOJUIOICI/ﬁdnq 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

