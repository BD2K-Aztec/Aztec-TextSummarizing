ORIGINAL PAPER

Vol. 29 no. 16 2013, pages 1946—1952
doi:10. 1093/bioinformatics/btt331

 

Sequence analysis

Advance Access publication June 21, 2013

Multilabel classification for exploiting cross-resistance
information in HIV-1 drug resistance prediction

Dominik Heider1 ’*, Robin Senge2, Weiwei Cheng2 and Eyke Hiillermeier2

1Department of Bioinformatics, University of Duisburg—Essen, 45141 Essen, Germany and 2Department of Mathematics
and Computer Science, University of Marburg, 35032 Marburg, Germany

Associate Editor: Alfonso Valencia

 

ABSTRACT

Motivation: Antiretroviral treatment regimens can sufficiently sup-
press viral replication in human immunodeficiency virus (HlV)-infected
patients and prevent the progression of the disease. However, one of
the factors contributing to the progression of the disease despite on-
going antiretroviral treatment is the emergence of drug resistance. The
high mutation rate of HIV can lead to a fast adaptation of the virus
under drug pressure, thus to failure of antiretroviral treatment due to
the evolution of drug-resistant variants. Moreover, cross-resistance
phenomena have been frequently found in HIV-1, leading to resistance
not only against a drug from the current treatment, but also to other
not yet applied drugs. Automatic classification and prediction of drug
resistance is increasingly important in HIV research as well as in clin-
ical settings, and to this end, machine learning techniques have been
widely applied. Nevertheless, cross-resistance information was not
taken explicitly into account, yet.

Results: In our study, we demonstrated the use of cross-resistance
information to predict drug resistance in HIV-1. We tested a set of
more than 600 reverse transcriptase sequences and corresponding
resistance information for six nucleoside analogues. Based on multi-
label classification models and cross-resistance information, we were
able to significantly improve overall prediction accuracy for all drugs,
compared with single binary classiﬁers without any additional informa-
tion. Moreover, we identiﬁed drug-speciﬁc patterns within the reverse
transcriptase sequences that can be used to determine an optimal
order of the classiﬁers within the classifier chains. These patterns
are in good agreement with known resistance mutations and support
the use of cross-resistance information in such prediction models.
Contact: dominik.heider@uni-due.de

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on February 8, 2013; revised on April 25, 2013; accepted on
June 4, 2013

1 INTRODUCTION

The human immunodeﬁciency virus (HIW is one of the major
human diseases leading to about 2 million deaths yearly.
Although antiretroviral treatment is working well in principle,
the high mutation rate of HIV frequently leads to a fast adapta-
tion of the virus and thus to the development of drug-resistant
viral strains. Tripathi et al. (2012) performed stochastic

 

*To whom correspondence should be addressed.

simulations of the within-host evolution of HIV-1. By estimating
the structure of the HIV-1 quasispecies, they were able to calcu-
late an error threshold of HIV-1. They discovered that HIV-l
has a mutation rate that is close to error catastrophe and that the
error threshold depends heavily on the recombination rate of
HIV-1 (Tripathi et al., 2012). Pennings (2012) analyzed the evo-
lution of resistance in HIV-1 due to standing genetic variation.
She ascertained that, depending on the treatment, probabilities
of evolution of drug resistance due to standing genetic variation
vary between 0 and 39%.

The most important parameters for the evolution of drug re-
sistance are the effective population size of the virus before treat-
ment and the ﬁtness of the resistant virus during treatment.
Evolution of drug resistance ﬁnally leads to a failure of antiretro-
viral treatment and thus to the progression of disease.
Experimental testing of viral resistance in patients has been
widely used in research as well as in clinical settings to gain in-
formation about the way drug resistance evolves. In contrast to
these experimental phenotypic assays, computational approaches
offer the possibility to predict drug resistance in HIV-1 in an easy
and fast way based on short sequence information of the viral
genotype, e. g. the sequence of the viral reverse transcriptase
(RT). Such computational models for predicting drug resistance
in HIV-1 have been developed and widely applied. These com-
putational models are mainly based on statistical or machine
learning methods that try to ﬁnd a mapping from the sequence
information to a ‘resistance factor’. Usually, the I C50 ratios [The
concentration of a speciﬁc drug inhibiting 50% of viral replica-
tion compared with cell culture experiments without the drug is
deﬁned as IC50 (50% inhibitor concentration).] are used in these
models to deﬁne resistance. In general, drug resistance means
reduced inhibition of viral replication by antiretroviral drugs,
resulting in increased IC50 values. The IC50 values of the drug
resistant isolates and HIV wild type are used to calculate resist-
ance factors

IC50(drug concentration for resistant strain)
I C50(drug concentration for wild type)

 

as a standardized measure of HIV drug resistance. The data are
separated into the classes ‘susceptible’ and ‘resistant’ based on a
drug-speciﬁc cutoff of the resistance factor. A computational
model is then trained based on these data pairs (sequence and
corresponding class), which can then be used to predict the re-
sistance factor or the resistance class for new unseen sequences.
For instance, Rhee et al. (2006) used ﬁve different statistical and

 

1946 © The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

112 ﬂJO'smumo[pJOJXO'sopchogurorq/ﬁd11q IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

Multilabel classification

 

machine learning methods (decision trees, artiﬁcial neural net-
works, support-vector machines, least-squares regression and
least angle regression) to predict drug resistance in HIV-1 for
16 drugs, including protease- and RT inhibitors. Kierczak
et al. (2009) developed a rough set-based model considering phy-
sico-chemical changes of mutated sequences compared with the
wild-type strain to predict RT inhibitor resistance in HIV-1.
Moreover, the same group developed the ﬁrst systems biology
approaches to HIV-1 drug resistance, showing networks of inter-
acting positions (Kierczak et al., 2010).

One important ﬁnding, however, has not been exploited in
these models so far, namely the occurrence of cross-resistance.
In the context of HIV-1, cross-resistance means that mutations
leading to a resistance against a speciﬁc drug, which is currently
part of the antiretroviral treatment of a speciﬁc patient, also
leads to resistance (In some cases, albeit less frequently, it
could also lead to a re—sensitization for other drugs.) against
other drugs (that may or may not be part of the same treatment).
In the current study, we analyzed cross-resistance in HIV-1 for a
dataset of more than 600 RT sequences and six nucleoside ana-
logues (NAs), namely Lamivudine (3TC), Abacavir (ABC),
Zidovudine (AZT), Stavudine (d4T), Didanosine (ddI) and
Tenofovir (TDF). Moreover, we developed a model that exploits
knowledge about cross-resistance to improve the overall predic-
tion accuracy for the whole repertoire of drugs used in this study.
To this end, we made use of novel methods for so-called multi-
label classification (MLC), a generalization of conventional
(polychotomous) classiﬁcation that has recently gained increas-
ing attention in machine learning (Tsoumakas and Katakis,
2007).

To the best of our knowledge, this is the ﬁrst time information
about RT inhibitors cross-resistance has been explicitly inte-
grated in HIV-1 drug resistance prediction models. In contrast
to protease inhibitors (PIs) as well as non-nucleoside reverse
transcriptase inhibitors (NNRTIs), NAs have less side effects
(Stiirmer et al., 2007). Moreover, the combination of different
drug classes during therapy may lead to unpredictable inter-
actions of HS and NNRTIs with the cytochrome P450 system
and thus may complicate the therapy. Therefore, one option
might be the use of quadruple nucleoside therapy (Stiirmer
et al., 2007).

2 METHODS
2.1 Data

For our analyses, we used a publicly available dataset consisting of RT
sequences of HIV-1 with corresponding resistance factors (IC50 ratios) for
six NAs, namely 3TC, ABC, AZT, d4T, ddI and TDF (Rhee et al., 2006).
A summary of the dataset is shown in Table 1. For our method, we
needed RT sequences for which IC50 ratios for all mentioned drugs are
available, so we discarded the information about TDF (owing to the low
number of sequences) and merged the other sequences and IC50 ratios of
the remaining drugs. Strains lacking phenotypic results for any drug
analyzed in the current study were discarded before analysis.
Eventually, this yields 614 RT sequences with IC50 ratios for 3TC,
ABC, AZT, d4T and ddI. The class ratio (positive samples / negative
samples) for all drugs ranges between 0.83 and 2.39. All sequences origi-
nated from subtype B strains.

Table 1. Data overview

 

 

Drugs Number of IC50 ratio Class
sequences cutoff ratioa
3TC 629 z 3 2.18
ABC 624 z 2 2.39
AZT 626 z 3 0.91
ddI 628 z 1.5 1.03
d4T 626 z 1.5 0.83

 

aRatio of resistant versus susceptible sequences.

2.2 Predictive modeling

The goal of our study was to build models that can be used to predict
whether there are resistance mutations within an RT sequence of a spe-
ciﬁc virus for different NAs. Thus, we used drug-speciﬁc cutoffs for the
IC50 ratios to separate the sequences into the classes ‘resistant’ and ‘sus-
ceptible’ for the different drugs. For 3TC and AZT, the cutoff was set to
3, for d4T and ddI it was set to 1.5 and for ABC it was set to 2. This
means that sequences having a corresponding I C50 ratio above or equal to
the cutoff are deﬁned as ‘resistant’ (Table 1), whereas sequences having an
IC50 ratio below the cutoff are deﬁned as ‘susceptible’. For instance, an
RT sequence with a ratio of 10.3 for 3TC is deﬁned as ‘resistant’, whereas
another RT sequence with a ratio of 2.5 is deﬁned as ‘susceptible’.

In some studies, e.g. Rhee et al. (2006), a third class was deﬁned as
‘intermediate resistant’. Nevertheless, since ‘intermediate resistant’ strains
are somehow resistant, too, we simply subsumed those sequences under
the ‘resistant’ category. The resulting classes are rather balanced: for each
drug (except for 3TC and ABC), ~50% of the RT sequences belong to the
class ‘resistant’ and 50% to the class ‘susceptible’. Concretely, the fraction
of RT sequences categorized as ‘resistant’ are, respectively, 50.81%,
47.72%, 45.44%, 68.57% and 70.52% for ddI, AZT, d4T, 3TC and ABC.

As already demonstrated in several protein classiﬁcation studies, e.g.
(Chowriappa et al., 2008; Dybowski et al., 2010; Heider et al., 2009,
2010), the use of physico-chemical properties and especially the use of
hydrophobicity characteristics (Kyte and Doolittle, 1982), lead to good
prediction results. Thus, we encoded the RT protein sequences into
hydrophobicity vectors and normalized them to length 240, which repre-
sents the average sequence length in the dataset, using Interpol (Heider
and Hoffmann, 2011). Thus, we eventually end up with a dataset consist-
ing of 614 instances (RT sequences), each of which is characterized in
terms of a normalized hydrophobicity vector of length 240. Moreover,
each instance is associated with ﬁve binary class labels, namely resistance
for ddI, AZT, d4T, 3TC and ABC. Our goal, now, is to train a model
that generalizes beyond these examples, i.e., that allows for accurately
predicting each of the ﬁve outputs on the basis of any normalized hydro-
phobicity vector given as input information.

2.3 Multilabel classiﬁcation

The above problem obviously falls in the realm of (binary) classification,
a well-established and thoroughly explored subﬁeld of statistics and ma-
chine learning. In fact, the arguably most simple way to solve it is to train
one binary classiﬁer for each of the ﬁve outputs, thereby splitting the
original maltioatpat problem into ﬁve single-output problems. Each of
these problems can then be solved individually, using the large repertoire
of existing methods for binary classiﬁcation.

This approach has an important disadvantage, however. It cannot take
any advantage of possible dependencies between the different outputs.
Modeling and exploiting such dependencies to improve prediction accur-
acy is one of the key goals of MLC. Intuitively, if the value of one output
may (statistically) depend on the value of others, then predicting all

 

1947

112 [glO'SIBILInO[plOJXO'SODBIILIOJHIOIQ/[l(11111 IIIOJJ pepcolumoq

910K ‘09 lsnﬁnV no :2

D.Heider et al.

 

outputs simultaneously should indeed be better than predicting them
separately. This is the main argument against simple decomposition tech-
niques like the one proposed above, called binary relevance (BR) learning
in the context of MLC.

More formally, let £ 2 {A1, ...,Am} be a ﬁnite set of class labels (in
our case the resistance for the ﬁve drugs), and let X be an instance space
(in our case the 240-dimensional hydrophobicity vectors). An MLC task
assumes a training set S = {(x1,y1), ..., (xn, yn)}, generated independ-
ently and identically according to a probability distribution P(X, Y) on
X x y. Here, 32 is the set of possible label combinations, i.e., the power
set of £. To ease notation, we deﬁne a label combination y as a binary
vector y = (yl, yz, ..., ym), in which yj = 1 indicates the presence (rele-
vance) and yj = 0 the absence (irrelevance) of M. Under this convention,
the output space is given by y = {0, 1}”‘. The goal in MLC is to induce
from S a model h : X —> 32 that correctly predicts the subset of relevant
labels for unlabeled query instances x e X. Recall that, in our context,
‘relevance of a label’ stands for ‘resistance against a drug’; thus, a pre-
diction y = (1, 0, 1, 0, 0) would suggest that the instance (RT sequence) at
hand is resistant against the ﬁrst and the third drug while being suscep-
tible to the others.

2.3.] Performance metrics The prediction of label subsets (vectors)
instead of single labels suggests different types of performance metrics for
MLC. Commonly used examples of such metrics, which also seem to be
meaningful in the context of our application, include the Hamming loss,
the subset 0/1 loss and the F-measure. Let X W = {x1, . . . , x N} be a set of
test instances. Moreover, let y,-, = (yi,1, ..., ytm) e y be the labeling of
test instance x,-, where yid- is the value of label M for x,-, and denote by
37,-, = (jg-,1, .. . , am) the corresponding prediction produced by the clas-
siﬁer. The Hamming loss of the prediction y,, is then deﬁned as the
fraction of labels whose relevance is incorrectly predicted (For a predicate
P, the expression |[P]] evaluates to 1 if P is true and to 0 if P is false.)

LHO’z-nyi.) = "—1: [DU 75 yi,j]] E [021] (1)
j=1

The subset 0/1 loss simply checks whether the complete label subset is
predicted correctly or not:

LSCVioﬁjyio) : [[yio  E {09  

The F-measure essentially corresponds to the harmonic mean of the
precision and recall of the prediction; it is deﬁned as follows:

m A
2 ii)”; jyi, j
J:
—m E [0, 1], (3)

yi,j + Z JAM
1 j=1

LFCVivaio) :

S

J

where 0/0 = 1 by deﬁnition. The above metrics are used to evaluate the
prediction 37,-. for an individual instance x,-, i.e., they are computed in-
stance-wise. Correspondingly, in an experimental study, the average ac-
curacy would be reported as the average over all instances x,- in the test
data X test.

Apart from that, another option is of course to report accuracy in a
label-wise manner, namely to compute standard performance metrics
such as classiﬁcation rate (percentage of correct predictions) or AUC
(area under the receiver—operating characteristic curve) separately for
each label A,- 6 £. Note that some metrics can be computed instance-
wise as well as label-wise. For example, the label-wise version of the
F-measure is given by

N

2 ii)”; jﬁi, j
LFCVojayoj) : Nl——N E [09 1]: 
Zyu + JAM
i=1 i=1

where y.j = (ij, .. . , yNd) is the vector of values for the label M and 37.]-
the corresponding vector of predictions.

2.4 Classiﬁer chains

Until now, several methods for MLC have been proposed in the litera-
ture. Here, we shall focus on a method called classifier chains (CCs; Read
et al., 2011), which, despite having been introduced only lately, already
enjoys great popularity. This is arguably due to the fact that it is based on
a simple and elegant yet effective idea for capturing label dependencies.
The CCs method learns m binary classiﬁers (each one dealing with the
BR problem associated with one label) linked along a chain, each time
extending the feature space by all previous labels in the chain. For in-
stance, if the chain follows the order A1 —> A2 —>  —> Am, then the
classiﬁer hj responsible for predicting the relevance of M is of the form

12,-: X x {0,1}i-1 —> {0,1}. (5)

The training data for this classiﬁer consists of (expanded) instances
(xi,y,-,1, .. . , yi, j_1) labeled with ya], that is, original training instances x,-
supplemented by the relevance of the labels A1, . . . , Aj_1 preceding M in
the chain. Thus, the classiﬁer hj supposed to predict the label of class A]-
makes use of the preceding labels as additional input information, thereby
capturing possible dependencies between the labels. Theoretically, the CC
approach can be motivated by the product rule of probability
(Dembczyr'iski et al., 2010):

P(ylx) = HPoklx.y1. ....yk_1) (6)
k=1

Note that, for training the classiﬁer (5), any standard method for
binary classiﬁcation can be used (logistic regression, decision trees, sup-
port vector machines, etc.).

At prediction time, when a new instance x needs to be labeled, a label
vector y = (y1, .. . , 9m) is produced by successively querying each classi-
ﬁer hj. Note, however, that the inputs of these classiﬁers are not well-
deﬁned, because the supplementary attributes yi,1, .. . , yi,j_1 are not
available. These missing values are therefore replaced by their respective
predictions: yl used by hz as an additional input is replaced by J71 = In (x),
yz used by h3 as an additional input is replaced by 92 = h2(x, 91) and so
forth. Thus, the prediction y is of the form

y = 011(35): (DOB/11(30): h3(x, (11(35): h2(x,h1(x)))a ---)

The process of training a CC and using it for prediction is illustrated in
Figure 1.

2.5 Ensembles 0f CCs

Realizing that the order of labels in the chain may inﬂuence the perform-
ance of the classiﬁer, and that an optimal order is hard to anticipate,
Read et al. (2011) propose the use of an ensemble of CC classiﬁers. This
approach combines the predictions of different random orders and,
moreover, uses a different sample of the training data to train each
member of the ensemble. Ensembles of classifier chains (ECC) have
been shown to increase prediction performance over CC by effectively
using a simple voting scheme to aggregate predicted relevance sets of the
individual chains. For each label A], relevance is predicted by thresholding
the proportion wj of classiﬁers predicting yj = 1 at a level t, i.e.,

J71 = [W 2 ill-

3 RESULTS AND DISCUSSION

The major goal of our experimental study was to provide empir-
ical evidence for the conjecture that capturing statistical depen-
dencies between HIV-1 drugs is instrumental in learning
classiﬁers for resistance prediction. Dependencies of that type

 

112 /810's112umofpinXO'soiiemJOJuioiw/2dnq IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

Multilabel classification

 

output 9‘ E J?

claasiﬁcr 1 lIIIIIIllEDDDDl
{original and

Classiﬁer 2 I I I I I I I El ‘:|  aupplturientary)
classiﬁer 3 I I I I I I I I IE DD E response to

d be predicted
mmwlllllmmllmm

IIIIIIHIIIIElm

clmiﬁerlllllllllnnnnnl  
 III-n mammal 
classiﬁer-3  

IIIIINMIIIH

Fig. 1. Illustration of the CCs approach: training phase (above) and pre-
diction phase (below)

input :I: E X

 

attributes used
as predict are

 

 

 

 

 

 

classiﬁer 5

 

 

 

 

 

 

 

 

 

 

 

 

 

I supplementary
attributes

 

 

response to

classiﬁer 5 .
ht: predicted

 

 

are biologically plausible and suggested by the observation of
cross-resistance; besides, they are also conﬁrmed by our data:
Table 2 shows the pair-wise associations between the binary
class labels (drugs), expressed in the form of the phi coefﬁcient
(This coefﬁcient is equal to the Pearson correlation for binary
variables and is also closely connected to the x2 statistics.). As
can be seen, the dependency between the resistance for different
drugs is positive throughout and speciﬁcally high for the two
pairs 3TC/ABC and AZT/d4T. This observation is in perfect
agreement with attribute importance analyses on the basis of
random forest classiﬁers that were trained for each class indi-
vidually, which are in partial agreement with recent expert-
deﬁned resistance mutations (Johnson et al., 2011) and other
computational approaches, e.g. Kierczak et al. (2010). As can
be seen in Figure 2, for 3TC as well as ABC, the most important
sequence positions (often selected as discriminative attributes by
the classiﬁers) are found in the C-terminal part with the highest
peak at position 184. For the other drugs, namely AZT, d4T and
ddI, the highest peaks are spread at the C-terminal as well as at
the N—terminal part, e.g. 41, 70, 210 and 215. Some NAs resist-
ance patterns are well known (Stiirmer et al., 2007), e.g., the
so-called thymidine analogue mutations at position 41, 65, 67,
70, 210, 215 and 219, leading to varying levels of AZT and d4T
resistance (Antinori et al., 2006; Garcia-Lerma et al., 2003;
Lafeuillade and Tardy, 2003). Another important mutation at
position 184 is also reﬂected in the importance analyses. The
mutation M184V is associated with high-level 3TC resistance
as well as with ABC resistance. For ABC resistance also muta-
tions at positions 65, 74 and 115 could be found during ABC
therapy. Moreover, mutation patterns at position 151 in combin-
ation with mutations at position 62, 69, 75, 77 and 116 are also
associated with high-level resistance against AZT, 3TC and ABC
(Sirivichayakul et al., 2003). Interestingly, position 65, which is
associated with a broad cross-resistance in almost all NAs except
for AZT, has also a high importance for the AZT classiﬁcation.
Nevertheless, random forest importance analyses have some limi-
tations, as they only estimate the importance of a sequence pos-
ition for the classiﬁcation, but do not provide information

Table 2. Values of the phi coefﬁcient, a measure of association that
ranges between —1 (perfect negative dependency) and +1 (perfect positive
dependency)

 

 

Drugs 3TC ABC AZT D4T DDI
3TC 1.0 0.824 0.274 0.364 0.618
ABC 0.824 1.0 0.381 0.489 0.614
AZT 0.276 0.381 1.0 0.804 0.392
d4T 0.364 0.489 0.804 1.0 0.538
ddI 0.618 0.614 0.392 0.538 1.0

 

whether a speciﬁc sequence position is positively or negatively
associated with resistance; moreover, they do not provide infor-
mation about interacting sequence positions that contribute to
resistance. For a comprehensive structural analysis and interpret-
ation of resistance mutations in RT see (Kierczak et al., 2010).
Interestingly, phylogenetic analyses of the sequences using a
neighbor-joining approach (Gouy et al., 2010) as well as princi-
pal component analysis on the distance matrix showed that the
sequences cannot be easily separated into the different resistance
classes based only on the sequence information (Supplementary
Figures S1 and S2).

It is important to note, however, that a positive correlation
between labels does not necessarily imply a beneﬁt for prediction.
In particular, while the above correlation is an unconditional
measure of dependence between class labels, a multilabel classi-
ﬁer such as CC seeks to capture conditional dependencies, namely
the dependence between class labels given the input information
x. Table 3 shows the average misclassiﬁcation rates of classiﬁers
(logistic regression) that have been trained for the individual
class labels A,- on different input information, namely (i) the ori-
ginal predictor variables x and (ii) this feature vector
supplemented by the resistance information of one of the other
drugs A]; thus, we simply assumed that A,- was already known
when A,- needs to be predicted. As can be seen, 3TC beneﬁts
more from knowing ddI than from knowing ABC, and d4T
beneﬁts more from ABC than from AZT. Another possible
effect that cannot be excluded and could have an inﬂuence on
our ﬁndings is treatment history. Unfortunately, the treat-
ment histories in the dataset are highly diverse. However, most
of the patients have an unknown treatment history or have not
been treated yet (see http://hivdb.stanford.edu). Thus we as-
sume that treatment history might play only a minor role in
our model.

The current study was related to the idea of CCs in so far as
class labels A,- are used as additional predictor variables for other
labels Ai. Here, however, we assumed the true values yj of the
additional predictor to be known, not only for training but also
for prediction. In chaining, on the other hand, the true values y,-
are only known in the training step, whereas for prediction, they
have to be replaced by their estimates 37,-. Thus, although the past
study conﬁrms the potential beneﬁt of label information for pre-
diction purposes, it is not clear that label dependencies can
indeed be exploited in a practically realistic setting where other
labels are not known at prediction time.

 

112 /810's112umofpinXO'soiiemJOJuroiw/2dnq IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

D.Heider et al.

 

 

3TC

20 25
|

15
I

 

 

 

 

0 50 1 00 1 50 200

 

ABC

15
I

10
I

 

 

 

 

0 50 1 00 1 50 200

 

30
I

AZT

10
I

 

 

 

0 50 1 00 1 50 200

 

30
I

D4T

25
|

20
I

 

 

 

0 50 1 00 1 50 200

 

DDI

20 25
I I

15
I

 

 

 

| I | I |
0 50 1 00 1 50 200

Fig. 2. Importance analyses from single classiﬁers. On the x—axis the sequence positions are shown, whereas the y-axis represents the sum of all decreases
in Gini impurity. Feature importance for ﬁve single random forests was assessed using the sum of all decreases in Gini impurity, which has been shown to
be more robust compared with the mean decrease in accuracy (Calle and Urrea, 2010)

 

1 950

112 /810's112umofpinXO'soiiemJOJuroiw/2dnq IIIOJJ pepeolumoq

910K ‘09 lsnﬁnV no 22

Multilabel classification

 

Table 3. Average classiﬁcation rate of logistic regression models trained
on different input information (original and supplemented)

 

 

Input 3TC ABC AZT d4T ddI
x 0.821 0.764 0.689 0.702 0.667
x + 3TC — 0.766 0.696 0.701 0.689
x + ABC 0.833 — 0.698 0.758 0.675
x + AZT 0.816 0.769 — 0.725 0.667
x + d4T 0.815 0.797 0.742 — 0.694
x + ddI 0.852 0.776 0.711 0.735 —

Table 6. Performance of BR, CC and ECC in terms of instance-wise
metrics (mean :I: standard deviation), in brackets the rank

 

 

Types Hamming loss Subset 0/1 F-measure
BR 0.2159 :I: 0.0298 (3) 0.5775 :I: 0.0476 (3) 0.7455 :I: 0.0366 (3)
CC 0.2129 :I: 0.0459 (2) 0.5098 :I: 0.0514 (2) 0.7631 :I: 0.0459 (2)

ECC 0.1947 :I: 0.0255 (1) 0.4961 :I: 0.0476 (1) 0.7787 :I: 0.0344 (1)

 

Note: The numbers are determined through lO-fold cross-validation repeated ﬁve
times. The best result per label is highlighted in bold font.

Table 4. Performance of BR, CC and ECC in terms of instance-wise
metrics (mean :I: standard deviation), in brackets the rank

 

Types Hamming loss Subset 0/1 F-measure

 

BR 0.2695 3: 0.0235 (2)
CC 0.2697 3: 0.0266 (3)
ECC 0.2384 3: 0.0538 (1)

0.6905 3: 0.0519 (3)
0.6904 3: 0.0528 (2)
0.6312 3: 0.0538 (1)

0.6741 3: 0.0420 (3)
0.6788 3: 0.0417 (2)
0.7166 3: 0.0366 (1)

 

Note: Logistic regression was used as base learner.

Table 5. Performance of BR (top), CC (middle) and ECC (bottom) in
terms of label-wise metrics (mean :I: standard deviation), in brackets the
rank

 

 

Note: Random forests (of size 16) were used as base learner.

Table 7. Performance of BR (top), CC (middle) and ECC (bottom) in
terms of label-wise metrics (mean :I: standard deviation), in brackets the

 

 

rank

Drugs Classiﬁcation rate AUC F-measure

3TC 0.8289 :I: 0.0515 (2) 0.8910 :I: 0.0520 (2) 0.8815 :I: 0.0403 (2)
ABC 0.8235 :I: 0.0473 (3) 0.8575 :I: 0.0530 (3) 0.8828 :I: 0.0348 (3)
AZT 0.7852 :I: 0.0582 (2) 0.8800 :I: 0.0446 (3) 0.7827 :I: 0.0589 (3)
d4T 0.7655 :I: 0.0451 (3) 0.8603 :I: 0.0351 (3) 0.7417 :I: 0.0582 (3)
ddI 0.7177 :I: 0.0634 (2) 0.7962 :I: 0.0483 (3) 0.7292 :I: 0.0618 (2)
3TC 0.8289 :I: 0.0515 (2) 0.8910 :I: 0.0520 (2) 0.8815 :I: 0.0403 (2)
ABC 0.8295 :I: 0.0473 (2) 0.8705 :I: 0.0484 (2) 0.8861 :I: 0.0348 (2)
AZT 0.7965 :I: 0.0630 (2) 0.8726 :I: 0.0527 (3) 0.7928 :I: 0.0626 (2)
d4T 0.7721 :I: 0.0598 (2) 0.8668 :I: 0.0454 (2) 0.7603 :I: 0.0652 (2)
ddI 0.7085 :I: 0.0497 (3) 0.7922 :I: 0.0525 (3) 0.7373 :I: 0.0464 (2)
3TC 0.8392 :I: 0.0493 (1) 0.8942 :I: 0.0439 (1) 0.8905 :I: 0.0386 (1)
ABC 0.8404 :I: 0.0375 (1) 0.8801 :I: 0.0462 (1) 0.8943 :I: 0.0281 (1)
AZT 0.8144 :I: 0.0420 (1) 0.8976 :I: 0.0361 (1) 0.8125 :I: 0.0447 (1)
d4T 0.7970 :I: 0.0503 (1) 0.8901 :I: 0.0341 (1) 0.7831 :I: 0.0590 (1)
ddI 0.7357 :I: 0.0392 (1) 0.8215 :I: 0.0394 (1) 0.7573 :I: 0.0420 (1)

 

Drugs Classiﬁcation rate AUC F-measure

3TC 0.8192 :I: 0.0512 (2) 0.8394 :I: 0.0638 (2) 0.8623 :I: 0.0441 (2)
ABC 0.7524 :I: 0.0543 (3) 0.7551 :I: 0.0613 (3) 0.8176 :I: 0.0478 (3)
AZT 0.6960 :I: 0.0590 (3) 0.6963 :I: 0.0631 (3) 0.6819 :I: 0.0534 (3)
d4T 0.7004 :I: 0.0483 (3) 0.7119 :I: 0.0621 (2) 0.6613 :I: 0.0748 (3)
ddI 0.6846 :I: 0.0657 (2) 0.6779 :I: 0.0888 (3) 0.6820 :I: 0.0769 (2)
3TC 0.8192 :I: 0.0512 (2) 0.8394 :I: 0.0638 (2) 0.8623 :I: 0.0441 (2)
ABC 0.7584 :I: 0.0538 (2) 0.7602 :I: 0.0592 (2) 0.8211 :I: 0.0469 (2)
AZT 0.7004 :I: 0.0578 (2) 0.7025 :I: 0.0612 (2) 0.6837 :I: 0.0574 (2)
d4T 0.7021 :I: 0.0616 (2) 0.7107 :I: 0.0650 (3) 0.6665 :I: 0.0790 (2)
ddI 0.6716 :I: 0.0450 (3) 0.6819 :I: 0.0620 (2) 0.6701 :I: 0.0525 (3)
3TC 0.8403 :I: 0.0548 (1) 0.9119 :I: 0.0472(1) 0.8814 :I: 0.0425 (1)
ABC 0.7980 :I: 0.0440 (1) 0.8566 :I: 0.0390 (1) 0.8541 :I: 0.0375 (1)
AZT 0.7488 :I: 0.0565 (1) 0.8282 :I: 0.0543 (1) 0.7378 :I: 0.0549 (1)
d4T 0.7211 :I: 0.0515 (1) 0.8115 :I: 0.0506 (1) 0.6874 :I: 0.0683 (1)
ddI 0.6999 :I: 0.0569 (1) 0.7761 :I: 0.0576 (1) 0.7058 :I: 0.0514 (1)

 

Note: Logistic regression was used as base learner.

3.1 The effect of chaining

To analyze the practical usefulness of classiﬁer chaining, we
compared the prediction accuracy of the following methods:

0 BR: A single binary classiﬁer is trained independently for

each of the ﬁve labels.

0 CCs: The ﬁve classiﬁers are trained according to the CC
approach outlined in Section 2.4. The chain was constructed

 

Note: Random forests (of size 16) were used as base learner.

by sorting the labels in decreasing order according to their
individual (BR) prediction accuracy (This is a commonly
used rule of thumb, which is motivated by the observation
that mistakes of a single classiﬁer tend to be propagated
along the rest of the chain (Senge et al., 2013); consequently,
strong classiﬁers should be placed at the top and poor ones
more toward the end of the chain.):

3TC —> ABC —> AZT —> d4T —> ddI

o ECCs: The ECC method described in Section 2.5 was im-
plemented with 10 chains, each time choosing the order of
labels at random. The threshold t was taken as 1/2.

All methods were implemented with standard logistic regres-
sion as a base learner. Prediction performance was measured in
terms of the Hamming loss (i), the subset 0/1 loss (ii) and the
F -measure (iii) as instance-wise metrics, and the classiﬁcation
rate, the AUC and the F -measure (iv) as label-wise metrics.
Each of these metrics was estimated by means of a 10-fold
cross-validation repeated ﬁve times, and results are reported in
terms of the mean values and the standard deviations. Moreover,
we also indicate the ranking of the three methods, with the best
performing method on rank 1 and the worst performing method
on rank 3.

 

1951

112 /810's112umo[p101x0'soi112u1101u101qﬂ2d11q 111011 pepeolumoq

910K ‘09 lsnﬁnV no 22

D.Heider et al.

 

The results for the instance-wise metrics are summarized in
Table 4, those for the label-wise metrics in Table 5. Although
the differences are not always statistically signiﬁcant, as can be
seen from the standard deviations, the overall picture is clear and
obviously in favor of the chaining methods. In fact, chaining
achieves systematic (albeit sometimes small) gains in comparison
with standard BR learning. Among the two chaining methods,
ECC performs even stronger than CC and typically yields the
best results.

To make sure that the results are not too much inﬂuenced by
the underlying base learner used by all methods, we repeated the
same experiments with random forests (Breiman, 2001) instead
of logistic regression. These two learners exhibit different proper-
ties. In particular, while logistic regression ﬁts a linear decision
boundary in the instance space, decision trees are much more
ﬂexible and able to model highly non-linear concepts; this ﬂexi-
bility is even increased by the aggregation of different trees in the
random forest approach. Thus, it comes at no surprise that the
performance of all methods is in general improved. Nevertheless,
in terms of relative comparison, the picture is more or less iden-
tical to the ﬁrst experiment with logistic regression. Both chain-
ing methods improve on BR, with ECC being even better than
CC (Tables 6 and 7).

4 CONCLUSION

We conclude with an afﬁrmative answer to one of the main
questions of our study, namely whether or not cross-resistance
information can be used to improve overall accuracy in drug
resistance prediction. By using MLC methods, a relatively
recent development in machine learning, we were able to exploit
cross-resistance information for RT inhibitors. More concretely,
our results are based on a speciﬁc MLC method called CCs.

We consider these results as promising and, therefore, intend
to further explore this direction in future work. On the methodo-
logical side, we would like to try alternative MLC methods,
including the probabilistic variant of CCs proposed by
Dembczynski et al. (2010) but also approaches that are not
based on the idea of chaining. As an interesting property of
the former, let us mention that it does not only produce binary
predictions, but proper probability estimates of single labels or
label combinations. Predictions of that kind are interesting, not
only for the minimization of various loss functions, but also for
the purpose of representing uncertainty. Moreover, we want to
include multiclass and regression models to be able to predict
more classes, e. g. intermediate resistance, and even the resistance
factors.

On the application side, our study has focused on NAs so far,
although a typical clinical treatment includes drugs from several
classes. It might of course be interesting to test our approach for
other types of antiretroviral drugs, for example, non-nucleoside
RT inhibitors, and for other target proteins, such as HIV-1 pro-
tease and corresponding PIs. By now, our approach is limited to
specialized treatment cases and thus is currently not well applic-
able in clinical settings. However, in the future we plan to adapt
our approach for NNRTIs as well as for PIs. Moreover, all se-
quences used in the current study originated from subtype B

strains, thus the results of our model might be misleading if it
is applied to other subtypes.

Funding: This work was partially supported by the Center for
Synthetic Microbiology (SYNMIKRO), Marburg, Germany.

Conflict of Interest: none declared.

REFERENCES

Antinori,A. et al. (2006) Antiviral efﬁcacy and genotypic resistance patterns of
combination therapy with stavudine/tenofovir in highly active antiretroviral
therapy experienced patients. Antivir. T her., 11, 233—243.

Breiman,L. (2001) Random forests. Mach. Learn, 45, 5—32.

Calle,M.L. and Urrea,V. (2010) Letter to the editor: stability of random forest
importance measures. Brief. Bioinform., 12, 86—89.

Chowriappa,P. et al. (2008) Protein structure classiﬁcation based on
conserved hydrophobic residues. IEEE/ACM Trans. Comput. Biol. Bioinform.,
6, 639—51.

Dembczynski,K. et al. (2010) Bayes optimal multilabel classiﬁcation via probabil-
istic classiﬁer chains. In ICML, pages 279—286.

Dybowski,J.N. et al. (2010) Prediction of co-receptor usage of HIV-1 from geno-
type. PLoS Comput. Biol., 6, e1000743.

Garcia-Lenna,J.G. et al. (2003) A novel genetic pathway of human immunodeﬁ-
ciency virus type 1 resistance to stavudine mediated by the K65R mutation.
J. Virol., 77, 5685—5693.

Gouy,M. et al. (2010) SeaView version 4: A multiplatforrn graphical user interface
for sequence alignment and phylogenetic tree building. Mol. Biol. Evol., 27,
221—224.

Heider,D. and Hoffmann,D. (2011) Interpol: an R package for preprocessing of
protein sequences. BioData Min., 4, 16.

Heider,D. et al. (2009) A computational approach for the identiﬁcation of small
GTPases based on preprocessed amino acid sequences. Technol. Cancer Res.
T reat., 8, 333—342.

Heider,D. et al. (2010) Predicting Bevirirnat resistance of HIV-1 from genotype.
BMC Bioinformatics, 11, 37.

J ohnson,V.A. et al. (2011) 2011 update of the drug resistance mutations in HIV-1.
Top. Antivir. Med, 19, 156—164.

Kierczak,M. et al. (2009) A rough set-based model of HIV-1 reverse transcriptase
resistome. Bioinform. Biol. Insights, 3, 109—127.

Kierczak,M. et al. (2010) Computational analysis of molecular interaction networks
underlying change of HIV-1 resistance to selected reverse transcriptase inhibi-
tors. Bioinform. Biol. Insights, 4, 137—146.

Kyte,J. and Doolittle,R. (1982) A simple method for displaying the hydropathic
character of a protein. J. Mol. Biol., 157, 105—132.

Lafeuillade,A. and Tardy,J.C. (2003) Stavudine in the face of cross-resistance be-
tween HIV-l nucleoside reverse transcriptase inhibitors: a review. AIDS Rev., 5,
80—86.

Pennings,P.S. (2012) Standing genetic variation and the evolution of drug resistance
in HIV. PLoS Comput. Biol., 8, e1002527.

Read,J. et al. (2011) Classifier chains for multi-label classification. Mach. Learn, 85,
333—359.

Rhee,S.Y. et al. (2006) Genotypic predictors of human immunodeﬁciency virus type
1 drug resistance. Proc. Natl. Acad. Sci. USA, 103, 17355—17360.

Senge,R. et al. (2013) On the problem of error propagation in classier chains for
multi-label classiﬁcation. In: Schmidt-Thieme,L. and Spiliopoulou,M. (eds)
Data Analysis, Machine Learning and Knowledge Discovery. Proceedings of the
36th Annual Conference of the German Classification Society. Springer,
Hildesheim, Germany.

Sirivichayakul,S. et al. (2003) Nucleoside analogue mutations and Q151M in HIV-1
subtype A/E infection treated with nucleoside reverse transcriptase inhibitors.
AIDS, 17, 1889—1896.

Stiirmer,M. et al. (2007) Quadruple nucleoside therapy with Zidovudine, lamivudine,
abacavir and tenofovir in the treatment of HIV. Antivir. T her., 12, 695—703.

Tripathi,K. et al. (2012) Stochastic simulations suggest that HIV-1 survives close to
its error threshold. PLoS Comput. Biol., 8, e1002684.

Tsoumakas,G. and Katakis,I. (2007) Multi label classiﬁcation: an overview. Int. J.
Data Warehouse Min., 3, 1—13.

 

1 952

112 /810's112umo[p101x0'soi112u1101u101qﬂ2d11q 111011 pepeolumoq

910K ‘09 lsnﬁnV no 22

