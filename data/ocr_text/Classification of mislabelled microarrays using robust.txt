ORIGINAL PAPER

Vol. 29 no. 7 2013, pages 870-877
doi: 1 0. 1093/bioinformatics/btt0 78

 

Gene expression

Advance Access publication February 15, 2013

Classification of mislabelled microarrays using robust sparse

logistic regression
Jakramate Bootkrajang* and Ata Kaban

School of Computer Science, University of Birmingham, Edgbaston, Birmingham B15 2W, UK

Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: Previous studies reported that labelling errors are not un-
common in microarray datasets. In such cases, the training set may
become misleading, and the ability of classifiers to make reliable in-
ferences from the data is compromised. Yet, few methods are cur-
rently available in the bioinformatics literature to deal with this
problem. The few existing methods focus on data cleansing alone,
without reference to classification, and their performance crucially de-
pends on some tuning parameters.

Results: In this article, we develop a new method to detect
mislabelled arrays simultaneously with learning a sparse logistic re-
gression classifier. Our method may be seen as a label-noise robust
extension of the well-known and successful Bayesian logistic
regression classifier. To account for possible mislabelling, we formu-
late a label-flipping process as part of the classifier. The regularization
parameter is automatically set using Bayesian regularization, which
not only saves the computation time that cross-validation would
take, but also eliminates any unwanted effects of label noise when
setting the regularization parameter. Extensive experiments with
both synthetic data and real microarray datasets demonstrate that
our approach is able to counter the bad effects of labelling errors in
terms of predictive performance, it is effective at identifying marker
genes and simultaneously it detects mislabelled arrays to high
accuracy.

Availability: The code is available from http://cs.bham.ac.uk/
~ij008.

Contact: J.Bootkrajang@cs.bham.ac.uk

Supplementary information: Supplementary data are available at
Bioinformatics online.

Received on December 21, 2012; revised on February 6, 2013;
accepted on February 9, 2013

1 INTRODUCTION

High-throughput microarray technologies make it possible to
measure the expression levels of thousands of genes. Our ability
to use these data to reliably predict the presence of a certain
disease and to better understand the biological mechanisms
underlying the development of disease is of fundamental import-
ance from the perspective of treatment and prevention. Statistical
machine learning methods have already shown a lot of promise
towards these goals, and methods that can deal with high dimen-
sional and low sample size settings have been the subject of con-
siderable research efforts over the last decade.

 

*To whom correspondence should be addressed.

However, the classical machinery of learning a classiﬁer relies
on a set of labelled examples, and the quality of a classiﬁer
depends crucially on the accurate labelling of these data.
Unfortunately, the task of labelling is complex and not without
ambiguities. As a result, there is no guarantee that the class labels
are all correct; in fact, there is an increasing realization that
labelling errors are not uncommon in microarray data—see
Malossini et al. (2006) and Zhang et a]. (2009).

The presence of class label noise in training sets has been re-
ported to deteriorate the performance of the existing classiﬁers in
a broad range of classiﬁcation problems (Krishnan and Nandy,
1990; Lawrence and Scholkopf, 2001; Malossini et al., 2006;
Yang et al., 2012; Yasui et al., 2004). Although, the problem
posed by the presence of class label noise is acknowledged,
often it is naively ignored in practice. Part of the reason may
be that symmetric label noise can be relatively harmless—how-
ever, asymmetric noise inevitably deteriorates the performance,
as it changes the decision boundary between the true classes
(Chhikara and McKeon, 1984; Lachenbruch, 1974; Lugosi,
1992).

Various approaches have been devised in the machine learning
literature to address the issue of learning from samples with label
noise. The seemingly straightforward approach is by means of
data preprocessing where any suspect samples are removed or
relabelled (Barandela and Gasca, 2000; Brodley and Friedl, 1999;
Jiang and Zhou, 2004; Maletic and Marcus, 2000; Muhlenbach
et al., 2004; sanchez et al., 2003). However, these approaches
hold the risk of removing useful data too, which is unsuitable
in microarray classiﬁcation, as the number of training examples
is limited.

In sharp contrast with the multitude of methods for micro-
array classiﬁcation, there are few attempts to address the prob-
lem of label noise in the bioinformatics literature. Malossini et a].
(2006) pointed out the difference between mislabelled arrays and
outliers, and proposed two methods to detect mislabellings based
on data perturbation. Zhang et al. (2009) developed this work
further and obtained improved precision and recall in both syn-
thetic and real data settings. Both of these works are based on
data perturbation, and their main focus is to detect suspects that
are potentially mislabelled. These methods can help repairing the
labels, so we can imagine a two-stage procedure of creating a
repaired training set ﬁrst and feed this to existing classiﬁers in a
second stage. However, one must be aware that any errors made
in separate stages of analysis will necessarily accumulate.

In this article, we address the above problems by developing
an integrated approach where the ambiguity of the given label
assignments is modelled explicitly during the training of a

 

870 © The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /310's113umo [p.IOJXO'SSUBUHOJUTOTQ/ﬁdllq 11101; prBOIIIAAOG

910Z ‘091sn3nv uo ::

Robust sparse logistic regression

 

classiﬁer. This allows us to build on classiﬁers that have been
successful for microarray classiﬁcation by developing an exten-
sion to account for possible label noise. Speciﬁcally, here we will
harness the sparse Bayesian logistic regression (BLogReg) model
proposed by Cawley and Talbot (2006) with a robustness against
label noise. From our model formulation, we then derive a new
algorithm that alternates between training the classiﬁer and
estimating the label noise probabilities. Straightforward calcula-
tions further provide the posterior probability of mislabelling for
each of the training points. This enables us to detect the suspect
samples for possible follow-up study. In addition, our experi-
mental validation results, using both synthetic and real micro-
array datasets, demonstrate that the proposed method improves
on traditional algorithms and achieves a reduced classiﬁcation
error rate. A variant of our approach appears in Bootkrajang
and Kaban (2012).

2 METHODS

2.1 A model for label-noise robust logistic regression

We now describe our label-noise robust Logistic Regression (RLogReg)
model. We will use the term ‘robust’ to differentiate this from
traditional logistic regression. Consider a set of training data
S: {(x1,)71), ...,(xD,)7D)}, where x, e W" and )7, 6 {0,1}, where )7,
denotes the observed label of x,. As in the classical scenario for binary
classiﬁcation, we start with deﬁning the log likelihood:

D
£(w) = 2371' 10g (P02 = 1|Xi9W)) + (1 —)7i)10g(l7()7i = 1|Xi9WD (1)
i=1

where w is the weight vector orthogonal to the decision boundary and it
determines the orientation of the separating hyperplane. If the labels were
presumed to be correct, then for a point x,- we would take

1

1 + e(_wTXi) 

p07,- : l|x,-,w) = 0(wa,-) =
and whenever this is above 0.5 we would decide that x, belongs to class 1.
However, when there is label noise present, making predictions in this
way is no longer valid. Instead, we will introduce a latent variable y to
represent the true label, and we rewrite 1707,- : klxi, w) as the following:

1
2 2 . . de
po. = lei,W) = 2pm- : kly =J)p<y =Jlx.,w) =1" Si (3)
1:0

In Equation (3), p07 2 kly = j) défyjk represents the probability that the
label has ﬂipped from the true label j to the observed label k. These
parameters form a transition table, which we will call the ‘gamma
table’, I“, and these label ﬂipping probabilities may be estimated. Using
this model, instead of Equation (2) we will have:

1

1 + e(_WTXi) 

P0} = 1|Xi,W) = 0(WTX1') =

We decide that x belongs to class 1 whenever p(y = 1|x,w) 2 0.5.

2.2 Sparsity prior

Microarray data are high dimensional with more features than observa-
tions while only a subset of the features is relevant to the target. A vast
literature demonstrates that sparsity-inducing regularization approaches
are effective in such cases (Cawley and Talbot, 2006; MacKay, 1995;
Shevade and Keerthi, 2003). Hence, we now incorporate sparsity in our
model described in the previous section. Following Shevade and Keerthi

(2003) and Cawley and Talbot (2006), we will use an L1 regularization
term, which results in the following objective function:

D

mgx Z logpoilxs w) — i ll w “1 (5)
i=1

where )t is the Lagrange multiplier (or regularization parameter) that
balances between ﬁtting the data well and having small parameter
values. The Ll-norm in the regularization term is deﬁned as,

M
II Wll1=Zlel (6)
d=1

Now, the regularization parameter A needs be determined. We cannot
use cross-validation, not only for its computational demand, but primar-
ily because it would need a validation set with trusted correct labels,
which may be not available. Hence, we adopt the Bayesian regularization
approach of Cawley and Talbot (2006), which bypasses the need for
cross-validation and determines A automatically by putting a Jeffrey’s
prior on )t and integrating it out from the model. This yields the following
(see Cawley and Talbot, 2006, for details):

x — i <7)

_ N
Z lel
d=0

where N denotes the number of non-zero parameters, i.e. those with
wd 75 0 —so N 5 M.

2.3 Parameter estimation

It now remains to estimate w and I‘. Notice that Equation (5) is not dif-
ferentiable at the origin. Shevade and Keerthi (2003) proposed a simple,
yet effective, algorithm to optimize the non-smooth but convex objective
function of sparse logistic regression (SLogReg) using the Gauss-Seidel
method and using coordinate-wise descent. We will create a modiﬁcation
of this approach to make it applicable to our non-convex objective.
Deﬁne Fd = 333:), where wdzo is the bias term that is usually left unre-
gularized. The optimality conditions for Equation (5), which are the same
as in Shevade and Keerthi (2003) and Cawley and Talbot (2006) can be

stated algebraically as the following:

 

Fd=0 ifd=0
Fdz). ifwd>0,d>0
Fd= —x ifwd<0,d>0
—x nggx ifwd=0,d>0

Accordingly, the violation from optimality of wd may be summarized
as:

ViOld = IFdI ifd = 0
=|x—Fd| ifwd>0,d>0
= |).+Fd| ifwd<0,d>0

= max(Fd—)t, —)t —Fd,0) ifwd = 0,d>0

We start optimizing the component wd that makes the largest violation to
an optimality condition. At this point, if the objective function was
convex then it would be possible to use gradient information to bracket
the region where the optimal wd lies by specifying upper and lower limits
(H and L). For example, Shevade and Keerthi (2003) identify 10 different
cases for their sparse logistic regression model. However, since our like-
lihood term is non-convex, the cases identiﬁed there are not applicable
because the sign of gradients give no information about the interval
where the optimal solution resides. Therefore we introduce a simple
modiﬁcation by performing two searches: one in the range [R+ U {0}
and another in the range [R— U {0}. We then choose the solution that
returns a higher value of the objective function. This modiﬁed searching

 

871

112 /310's113umo [p.IOJXO'SSUBUHOJUIOIQ/ﬁdllq 11101; prBOIIIAAOG

9IOZ ‘091sn3nv uo ::

J.Bootkrajang and A.Kabén

 

approach is more general and will work on any locally differentiable
function at the expense of a slight increase in computation time. In prac-
tice, L and H are ﬁnite—provided that the design matrix is standardized
and appropriate regularization is imposed on the solution, it is sufﬁcient
to search in the (0, 1000) and (—1000, 0) intervals.

Finally, having completed the optimization of w, it remains to derive
the update rule for the label-ﬂipping probabilities. Conveniently, these can
be estimated via ﬁxed point update equations. By introducing a Lagrange
multiplier to ensure that the probabilities in each row of the I‘ table sum
to 1 and solving the stationary equations, we obtain the following update
equations (for details see Bootkrajang and Kaban, 2012):

.901

1/00 = —g00 y01 = — (8)
300 + .901 9 300 + .901
310 gii
1/10=—,)/11=— 9)
gio +g11 gio +g11 (

where

_ D(1—yz-)_ T, _ 0g T,
goo — V00: SO (1 “(W X0) sgll — V11: SIU(W X1)
i=1 i i=1 i

_ D&_ T, _ 00—11) T,
gel—WE 5,0 a<w Xz))sg10—1/10: SO o<w x.)
i=1 1' i=1

i

Derivation details are given in the Supplementary Material.

The optimization of the log-likelihood is then to alternate between
optimizing w along with updating A according to Equation (7) until con-
vergence is reached, and we alternate this with the ﬁxed point update
equations of the label-ﬂipping probabilities. The entire optimization
procedure is summarized in Algorithms 1—2.

 

Algorithm 1 Main loop

 

Input: Training examples.
Initialize w <— 0, A <— 0, [m <— {wo}, [Z <— {wd,de{1,n}}, F.
while Optimality violator exists in [Z d0
Find the greatest optimality violator, v, in 12
repeat
Optimize wv using Algorithm 2

[Z <— Iz\{wv}

II’IZ e Inz U 

Find the maximum optimality violator, v, in ["2
until No violator exists in ["2
Update the entries of I‘ by Equations (8) and (9)
Update regularization parameter, A by Equation (7)
end while
Output: Optimized weight vector, w. Optimized I‘.

 

Algorithm 2 Optimization of w

 

Input: Violating component, w,,, 12, [m
w, <— 0
if w, satisﬁes optimality conditions then

12 <— 12 U {Wu}

Inz <— Inz\{wv}

break
else
Restore previous value of WV
11 <— Optimize Equation (1) w.r.t w, in the range (—lim, 0) range
[2 <— Optimize Equation (1) w.r.t w, in the range (0,1im) range
end if
w, <— t, that maximize Equation (1), where i e {1, 2}.
Output: Optimized w,

 

2.4 Detecting mislabelled points

For an observation (x,, 17,), the probability of it being mislabelled can be
computed as the following:

1
po 2 fix.) = 2 pg =lei) (10)

J=0,J7Eyi

This may be thought of as the models ‘degree of belief’ that xi’s label is
incorrect. We may use it either in this form, or in a hard-thresholded form
(i.e. predict that the point x,- is mislabelled if p(y 75 filxi) 2 0.5).

2.5 A note on low sample size, high dimensional data
Since additional parameters I‘ are being estimated from the data,
we expect that RLogReg will require more training examples to deliver
its full potential. In microarray datasets, the training set size is often of
the order of tens only. A possible workaround in such cases is to guide
the algorithm by presetting the gamma table from domain know-
ledge about the likely proportion of mislabelled data. When such
knowledge exists, the values of gamma may either be ﬁxed throughout
the optimization process or they may be seeded initially and then
optimized.

3 RESULTS

3.1 Experiment setting

We will compare the classiﬁcation performance of RLogReg,
RLogReg with ﬁxed gamma table (denoted RLogReg-F) and
its traditional counterpart, i.e. BLogReg of Cawley and Talbot
(2006). The reader is referred to Cawley and Talbot (2006) for a
comparison between BLogReg against the Relevance Vector
Machine (RVM) and SLogReg (Shevade and Keerthi, 2003)
where BLogReg was shown to be superior. We shall demonstrate
that our proposed robust extension of BLogReg performs better
than the original BLogReg in terms of classiﬁcation performance
when there is label noise present in the training set. Moreover,
our model can be used to identify mislabelled arrays for potential
follow-on study.

Before proceeding, we should comment that symmetric and
asymmetric label ﬂipping have very different consequences in
classiﬁcation. Symmetric or uniform ﬂipping means that each
class is affected by label ﬂipping in the same proportion. In con-
trast, asymmetric or non-uniform ﬂipping is when the label ﬂips
from one class to another more often than vice-versa. The latter
type of label ﬂipping has been theoretically shown (Lugosi, 1992)
to degrade the performance of an algorithm to a much larger
degree, as it modiﬁes the decision boundary between the true
classes. Our empirical study (Bootkrajang and Kaban, 2012)
also demonstrated this. Therefore, we will mainly focus our at-
tention on datasets with asymmetric label noise and indeed
expect the advantages of our approach to be most apparent in
that setting.

To demonstrate the beneﬁt of having a label noise model
embedded in the classiﬁer, we start with experiments on synthetic
data where labels were asymmetrically ﬂipped at the rate of 30%.
The use of synthetic data for controlled experiments is standard
in bioinformatics (see e.g. Zhang et al., 2009), as it allows us
assess the performance of a new approach against a ground
truth. We shall then move on to analysing real microarray

 

872

112 /310's112umo [p.IOJXO'SOIlBIHJOJUIOICI/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn3nv uo ::

Robust sparse logistic regression

 

Table 1. Characteristics of the datasets used in the reported experiments

 

 

 

 

Dataset No. of samples No. of genes No. of wrong labels
Class 1 Class 2 Class 1 Class 2
Synth—500 250 250 100—1000 0 75
Synth—IOO 50 50 100—1000 0 15
Colon 40 (T) 22 (N) 2000 5 4
Breast 25 (ER+) 24 (ER—) 7129 4 5

 

datasets where label noises have not been injected artiﬁcially.
These datasets have been previously reported to contain wrongly
labelled samples. Finally, we shall assess the ability of our pro-
posed approach to identifying mislabelled arrays using Receiver
Operating Characteristics (ROC) analysis.

3.2 Datasets

We generate synthetic data by sampling points from a standard
Gaussian distribution where the class label associated with each
point is assigned by a logistic function with a predeﬁned
weight vector W having only three relevant features,
W1 2 W2 2 W3 2 10/3,w,- = 0, Vi> 3, following Ng (2004). We
create sets with 500 training points and sets with 100 training
points together with independent test sets of 100 points each
time, and call these datasets Synth-500 and Synth-IOO, respect-
ively. The dimensionality of the synthetic datasets ranges from
100 up to 1000. Asymmetric label noise was artiﬁcially injected
into each synthetic dataset at the 30% rate.

Further, we use two real microarray datasets: Colon cancer
(Alon et al., 1999) and Breast cancer (West et al., 2001)—both of
which are known to contain some mislabelled arrays. No artiﬁ-
cial label ﬂipping is injected in these data. We standardize these
datasets so the rows of the D x M design matrix (where D is the
number of observations and M is the dimensionality) of the input
sample will have zero mean and unit variance. Table l summar-
izes the characteristics of all of these datasets used. Additional
datasets and results are given in the Supplementary Material.

3.2.] Error measures While in the case of synthetic data the
true labels can be used to validate the predictive accuracy of
our algorithm, in the real microarray data there is no absolute
ground truth. Since the labels given in the datasets may be in-
correct, the issue of what should count as a misclassiﬁcation
must be deﬁned. We deﬁne two variants for measuring
out-of-sample error rates:

0 Corrected (CRT): Count misclassiﬁcation errors against the
‘corrected’ labels where corrections are made cf. the misla-
bellings reported in the literature.

0 Cleansed (CLN): Exclude any mislabelled suspects (known
in the literature) from the test sets for the purpose of evalu-
ation, so these are always placed into the training set instead;
then count the misclassiﬁcation errors on test sets in the
usual way.

 

 

  

 

 

 

 

(3),, (b ..
0.. . .

g 'tl': 9T0; _ WK

III 2: Lu ‘vmowo

5:) 7" +ElL-ngFlEg 

8 RLDgFl-eg 2 .

§ 'I.2- % _ —)lt-EangFlEg

8 :.  . .  .2 RLogReg

3 “,1 E I. l-O-‘HLDQHEQ F
I. I I I I I If I I I I I
f 2 :n: 4.1:: r: m". 9. m: -:I-.'. .1 5 3'3 "3'7- E 7”"- 9- 3'"- '-'"-'- 71

Number of Features Number of Features
Synth-500 Synth-100

Fig. 1. Misclassiﬁcation on test set, as obtained by RLogReg and
BLogReg, respectively, on synthetic datasets with 30% asymmetric
label noise. Left: Training sets of size 500; Right: Training sets of size
100. RLogReg-F denotes the version of RLogReg with the gamma
matrix pre—set to its correct value

3.3 Results and analysis

3.3.1 Results on synthetic data The average misclassiﬁcation
error rates on the Synth-500 and Synth-100 datasets are shown
in Figure l as the data dimension is varied. Each point on these
plots represents the average misclassiﬁcation rate on the test sets,
where the average is taken over 500 independent repetitions of
the experiment. The error bars are too small to be visible. We see
that RLogReg achieves signiﬁcantly lower error rates than
BLogReg on the datasets that contain more training examples
(Synth-500). This clearly demonstrates the advantage of model-
ling the label noise process. On the smaller size dataset
(Synth-100), however, the performance gain becomes mar-
ginal—this is because the accurate estimation of the additional
parameters (label-ﬂipping probabilities) requires sufﬁcient train-
ing data for our approach to achieve its full potential.
Nevertheless, it is should be noticed that even in the small
sample setting, RLogreg performs no worse than BLogReg on
all the datasets tested (additional results are given in the
Supplementary Material). More importantly, the rightmost
plot shows that we can counter the problem of small sample
sizes by using prior knowledge about the extent of label noise,
e.g. by pre-deﬁning the gamma table. We denote this version as
RLogReg-F in the ﬁgure, and we see this signiﬁcantly improves
the classiﬁcation accuracy in the small sample setting.

Beyond classiﬁcation performance, it is of interest to evaluate
the methods’ ability to identify the relevant predictive genes.
Figure 2 shows the estimated weight vectors as obtained by
BLogReg and RLogReg respectively from 100-dimensional syn-
thetic data with only the ﬁrst three features being relevant. The
classiﬁers were trained on 250 training examples per class that
were subjected to 30% asymmetric label ﬂipping. We see that
RLogReg achieved a more accurate estimation of the weight
vector, while BLogReg became confused by the noisy labels
and selected too many false non-zero weights. This is an import-
ant advantage of RLogReg over BLogReg when it comes to
ﬁnding a small set of predictive marker genes.

3.3.2 Results on colon cancer The colon cancer classiﬁcation
task aims to distinguish between normal tissue and tumour.
According to Alon et al. (1999), there is biological evidence
that the samples T2, T30, T33, T36, T37, N8, N12, N34, N36
may be mislabelled. The proportion of mislabelling in the two
classes is unequal; hence, this is a case of asymmetric label

 

873

112 /310's112umo [p.IOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn3nv uo ::

J.Bootkrajang and A.Kabén

 

 

 

 

 

 

 

 

 1 5 . . . . . , , , , , , , . . . . . . . . . . , , , , , , , . . . . . . . . , , , , , , , , . . . . . . . . , , , , H   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 w
(I) (I)
"O
E) .43 2o 111111111111111111111111111111111111111111111111111111 m
g CC»
0 C
E E 10-1 111111111111111111111111111111111111111111111111111111 m
‘D 8
8’ 9 G -
3% —o.5 1111111111111111111111111111111111111111 w 3%
_1 . . —1G . .
o 50 100 0 50 100
Feature FeOTUFe
BLogReg RLogReg

Fig. 2. Comparison of the magnitude of weights for the 100 features as
obtained in one run of BLogReg and RLogReg, respectively, on synthetic
data that contains only three relevant features (250 training examples in
each class, 30% asymmetric label noise). We see that BLogReg selects too
many features whereas RLogReg has a better ability to turn off the
irrelevant ones

Table 2. L00 misclassiﬁcation (%) on Colon Cancer dataset

 

 

Algorithm LOO-CRT LOO-CLN No. of genes
BLogReg 8.06 :l: 0.44 7.55 :l: 0.64 11.94 :l: 0.41
RLogReg 9.68 :l: 0.48 9.43 :l: 0.66 11.85 :l: 0.41
RLogReg-F 4.83 :l: 0.35 1.88 :l: 0.54 9.21 :l: 0.45

 

The average number of selected genes (:tstandard deviation) was computed from
the CLN runs.

ﬂipping that can distort the correct decision boundary of the
classes. The limited number of training observations implies
that a good estimate of the gamma table may be difﬁcult to
obtain from the data alone (as we have seen in the previous
section), nevertheless prior knowledge of the noise proportions
may still allow us to exploit the advantages of having a noise
model as integral part of our classiﬁer. Therefore, we include
RLogReg-F in our experiments, with the gamma table set to
the true label-ﬂipping proportions. Table 2 reports the lea-
ve-one—out (LOO) errors in terms of the error measures deﬁned
in Section 3.2.1, and we also give the average number of genes
selected by the three methods considered.

The results conﬁrm the expectations. RLogReg that attempts
to estimate the gamma table along with all other parameters is
marginally worse than BLogReg (although not statistically sig-
niﬁcantly so, according to the unpaired t-test), while RLogReg-F
improves over BLogReg in all validation criteria used, and it also
selects a smaller fraction of relevant features.

Figure 3 shows the average magnitude of each gene according
to BLogReg and RLogReg-F, respectively. These are averages of
W estimates across 1000 bootstrap repetitions to inspect possible
systematic differences. These average weights turned out to be
quite similar for BLogReg and RLogReg-F, with the exception
of a few genes that had been ranked differently by the two meth-
ods. To see this, a summary of top ten selected genes and their
estimated weights are given in Tables 3 and 4.

3.3.3 Results on breast cancer We further apply the proposed
model on the Breast Cancer dataset from West et al. (2001).
The aim is to discriminate between oestrogen-positive and

 

 

 

 

a b

a) o

E 0.4- 3 0.4-

E E

g 0.2 g 02

s o- s o-

9 9

a; —0.2 9 —0-2-

< <

 I I I I  I I I I
0 500 1000 1500 2000 0 500 1000 1500 2000

Feature Gene
BLogReg RLOQRGQ'F

Fig. 3. Comparison of the average weights of features selected by
BLogReg and RLogReg-F on the Colon cancer dataset over 1000 boot-
strap repeats (50 train/ 12 test)

oestrogen-negative observations. According to West et al.
(2001), there is biological evidence that the arrays ll, l4, 16,
31, 33, 40, 43, 45, 46 are mislabelled. However, unlike the
Colon dataset, we observe the nature of label ﬂipping in the
Breast cancer dataset is rather close to symmetric. As a conse-
quence, mislabelling might do less harm to traditional classiﬁers
in terms of class prediction on future arrays. Table 5 summarizes
LOO error rates together with the numbers of genes selected by
the classiﬁers. The picture is quite similar to what we have seen in
the case of Colon, although the differences tend to be smaller, as
the label noise here is more symmetric.

We also see that RLogReg did pretty well with a limited
amount of training data, but of course the difﬁculty of accurate
estimation of the gamma table from such few points remains an
issue. In fact, the estimated gamma table of RLogReg may con-
verge to identity in such conditions, which statistically will result
in a weight vector that is identical to that of BLogReg. As pre-
viously, knowledge of the extent of noise can be used here, re-
sulting in a slight improvement for RLogReg-F. Finally, as
somewhat expected, the average magnitude of gene weights
from BLogreg and RLogreg-F look similar, as shown in
Figure 4, which was expected by the symmetric nature of the
label noise in this dataset.

3.4 Computation time

We should give an indication of the added computation over-
head required by our noise modelling relative to the existing
BLogReg. One LOO loop on all datasets considered took on
average 4 s for RLogReg, while BLogReg required roughly 0.2
s on an Intel’s Core-i5 3.2GHz machine. We believe this extra
computation time is most worthwhile especially when the train-
ing set size is sufﬁciently large to exploit the full potential of the
presented approach.

3.5 Detecting mislabelled instances

One of the most appealing features of our proposed algorithm is
the possibility to detect mislabelled examples from the data, in
addition to classiﬁcation and gene selection.

There are two types of possible errors: (i) a false positive is
when a sample is believed to be mislabelled despite it is in fact
labelled correctly; and (ii) a false negative is when a sample is
believed to be labelled correctly despite its label is in fact incor-
rect. A good way to summarize both, while also making use of
the probabilistic outputs given by the sigmoid function, is by

 

874

112 /810's112umo [p.IOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

Robust sparse logistic regression

 

Table 3. Relative importance of top 10 genes selected by the BLogReg algorithm

 

Gene number Gene annotation

Average magnitude

 

765 Human cysteine-rich protein (CRP) gene, exons 5 and 6 0.4289
377 H.sapiens mRNA for GCAP-II/uroguanylin precursor 0.4132
1644 C4-DICARBOXYLATE TRANSPORT SENSOR PROTEIN DCTB (Rhizobium leguminosarum) 0.2618
1870 PEPTIDYL-PROLYL CIS-TRANS ISOMERASE, MITOCHONDRIAL PRECURSOR (HUMAN) —0.2435
249 Human desmin gene, complete cds 0.2416
1346 60S RIBOSOMAL PROTEIN L24 (Arabidopsis thaliana) —0.2376
1772 COLLAGEN ALPHA 2(XI) CHAIN (Homo sapiens) —0.2292
1024 ATP SYNTHASE A CHAIN (T rypanosoma brucei) —0.1356
1482 Human spermidine synthase gene, complete cds 0.1290
1641 Human enkephalin B (enkB) gene, exon 4 and 3’ ﬂank and complete cds —0.1090

 

Table 4. Relative importance of the top 10 genes selected by RLogReg-F algorithm

 

Gene number Gene annotation

Average magnitude

 

765 Human cysteine-rich protein (CRP) gene, exons 5 and 6 0.3988
377 H.sapiens mRNA for GCAP-II/uroguanylin precursor 0.3273
249 Human desmin gene, complete cds. 0.3201
1644 C4-DICARBOXYLATE TRANSPORT SENSOR PROTEIN DCTB (R.leguminosarum) 0.2883
1870 PEPTIDYL-PROLYL CIS-TRANS ISOMERASE, MITOCHONDRIAL PRECURSOR (HUMAN) —0.2852
1346 60S RIBOSOMAL PROTEIN L24 (A.thaliana) —0.2420
1024 ATP SYNTHASE A CHAIN (T .brucei) —0.2220
1993 Human hormone-sensitive lipase (LIPE) gene, complete cds —0.21 14
493 MYOSIN HEAVY CHAIN, NONMUSCLE (Gallus gallus) 0.1325
1772 COLLAGEN ALPHA 2(XI) CHAIN (H.sapiens) —0.1224

 

Table 5. L00 misclassiﬁcation (%) on Breast Cancer dataset

 

 

Algorithm LOO-CRT LOO-CLN No. of genes
BLogReg 18.37 :l: 0.79 2.50 :l: 0.40 9.22 :l: 0.58
RLogReg 18.37 :l: 0.79 2.50 :l: 0.40 9.10 :l: 0.63
RLogReg-F 16.33 :l: 0.76 0.00 :l: 0.29 7.58 :l: 0.50

 

The average number of selected genes (:tstandard deviation) was computed from
the CLN runs.

E
E

 

 

 

 

 

 

2-11 2 ................................................................. .
<1) (I)
3 s
8) 1- “““““““““““““““““““““““““““““““““““ “ E, 1-
o o
E E
o
8 O I II I I 08)) 0 II I I
o 5
i E
— I I l _'1 I I I
0 2000 4000 6000 0 2000 4000 6000
Gene
BLogReg RLogReg-F

Fig. 4. Comparison of the average weights of features selected by
BLogReg and RLogReg-F on the Breast Cancer dataset over 1000 boot-
strap repeats (39 train/ 10 test)

E
E

 

   

   

 

 

 

 

9 [1 ti ,3

g a

a; -:I I: g '3' '3

§ {I 4 DO_ fl 4

<1) 3

2 . .. —ElLI;-gFleg ,: ,3. —E“—0tIFlE*Q
F 'J ‘ --- RLngF-leg ‘ 1-1-1 “Laurie;-

- - - RLogReg—F I: ' ' ' RLDQHCQ—F

 

 

 

 

 

 

n

0 3'? :1 a u a '13
False Positive Rote
Synth-500
Fig. 5. Average ROC curves for BLogReg, RLogReg and RLogReg-F
on Synth-500 and colon cancer benchmarks. For consistency with classi-
ﬁcation result bootstrap is performed on Synth-500 while LOO is used to
obtain the result for colon cancer. The prediction is based on

hard-thresholded rule

.13 3 5- ':J 5
False Positive Rdte

Colon cancer

constructing the ROC curves. The area under the ROC curve
signiﬁes the probability that a randomly drawn and mislabelled
example would be ﬂagged by the proposed algorithms. Figure 5
shows the ROC curves for Synth-500 and Colon cancer datasets.
Superimposed for reference, we also plotted the ROC curves that
correspond to BLogReg. BLogReg considers that all points have
the correct labels, and it has not been designed to spot misla-
belled points. The best we can do is to take that mistakes
made on the training points are mislabelling predictions. From

 

875

112 /810's112umo [p.IOJXO'SOllBIIIJOJUIOIQ/ﬁ(1111] 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

J.Bootkrajang and A.Kabén

 

Table 6. Identifying mislabelled samples in colon cancer dataset

 

 

Source Suspects identiﬁed Extra samples identiﬁed
Alon et al. (1999) T2 T30 T33 T36 T37 N8 N12 N34 N36

Furey et al. (2000) — O O O — O — O 0

Li et al. (2001) — O O O — — — O O

Kadota et al. (2003) O — — — O O — O 0 T6, N2

Malossini et al. (2006) (RAPIV) — O O O O O — O 0 N28, N29, N40
Malossini et al. (2006) (PRAPIV) — O O O O O — O 0 N2, N28

BLogReg O O O O — O O O 0 T3, T32, N35, N40
BLogReg (%) 1 9 14 63 0 9 18 32 37

RLogReg-F O O O O — O O O 0 N2, T32, N40
RLogReg-F (%) 4 22 55 79 0 15 15 37 68

 

The detections for RLogReg-F are based on the hard threshold rule (p07 75 ylx, w) z 0.5). The ﬁrst line is the ‘gold standard’ that is backed up by biological evidence in the

literature.

Figure 5, we see the gap between the two curves is signiﬁcant and
well apparent in the experiment on Synth-500. This quantiﬁes the
gain that our modelling approach is able to obtain. The gain for
Colon is smaller but still signiﬁcant, despite the dataset size is so
limited, provided that RLogReg incorporates knowledge about
the proportion of mislabelling (i.e. RLogReg-F).

3.6 Comparison with previous findings

In addition to comparisons that quantify the beneﬁts of having a
noise model, we compare our results with previously identiﬁed
mislabelling in the Colon cancer samples. We conduct 100 boot-
strap repetitions drawing subsets of size 50 from the total of 62
points randomly while imposing that none of the suspects from
the literature are left out. In Table 6, after quoting the previous
detections from the literature, we report the mislabelling detec-
tions obtained by BLogReg-F and BLogReg respectively, in two
forms: (i) from the run that returned the largest number of de-
tections, and (ii) the percentage that a particular array was
ﬂagged up as a mislabelling during the 100 repetitions.

It is interesting to note that RLogReg-F was able to identify
up to seven mislabelled points, and these also agree with the
majority of previously reported detections using other algorithms
(i.e. for T30, T33, T36, N34 and N36). BLogReg is also able to
ﬁnd up to seven mislabelled samples but with fewer true positives
and more false positives.

From both ﬁgures, we see that RLogReg-F is able to identify
mislabelled arrays more often than BLogReg can.

4 CONCLUSIONS

We proposed a robust extension of sparse Bayesian logistic
regression for classiﬁcation in the presence of labelling errors.
The numerical experiments suggest that our approach is superior
to its traditional counterpart when the training data contains
labelling errors, and more signiﬁcantly so when the label-ﬂipping
distribution is asymmetric. Simultaneously, our methods are
effective in identifying marker genes and detecting mislabelled
data. Since our robust model needs to estimate the label-ﬂipping
probabilities together with the parameters of the classiﬁer, it does
require more training data to achieve its full potential. However,

in our experience, RLogReg performs statistically no worse than
BLogReg even when the training set sizes are small. The need for
more data can also be relaxed by incorporating knowledge about
the extent of label noise.

Funding: J .B. is supported by the Royal Thai Government. A.K.
acknowledges the MRC Discipline Hopping Award G0701858
(ID no. 85545).

Conﬂict of Interest: none declared.

REFERENCES

Alon,U. et al. (1999) Broad patterns of gene expression revealed by clustering ana-
lysis of tumor and normal colon tissues probed by oligonucleotide arrays.
Proc. Natl Acad. Sci. USA, 96, 6745—6750.

Barandela,R. and Gasca,E. (2000) Decontamination of training samples for super-
vised pattern recognition methods. In: Advances in Pattern Recognition, Lecture
Notes in Computer Science, Vol. 1876. Springer, Berlin Heidelberg, pp. 621—630.

Bootkrajang,J. and Kaban,A. (2012) Label-noise robust logistic regression and its ap-
plications. In: Proceeding of Machine Learning and Knowledge Discovery in
Databases - European Conference, ECML—PKDD 2012, Part I, pp. 143—158.

Brodley,C.E. and Friedl,M.A. (1999) Identifying mislabeled training data. J. Artif.
Intell. Res., 11, 131—167.

Cawley,G.C. and Talbot,N.L. (2006) Gene selection in cancer classiﬁcation using
sparse logistic regression with Bayesian regularization. Bioinformatics, 22,
2348—2355.

Chhikara,R.S. and McKeon,J. (1984) Linear discriminant analysis with misalloca-
tion in training samples. J. Am. Stat. Assoc., 79, 899—906.

Furey,T.S. et al. (2000) Support vector machine classiﬁcation and validation of
cancer tissue samples using microarray expression data. Bioinformatics, 16,
906—914.

Jiang,Y. and Zhou,Z.H. (2004) Editing training data for k-NN classiﬁers with
neural network ensemble. In: Advances in Neural Networks, Lecture Notes in
Computer Science, Vol. 3173. Springer, pp. 356—361.

Kadota,K. et al. (2003) Detecting outlying samples in microarray data: a critical
assessment of the effect of outliers on sample classiﬁcation. Chem. Bio. Inform.
J., 3, 30—45.

Krishnan,T. and Nandy,S.C. (1990) Efﬁciency of discriminant analysis when initial
samples are classiﬁed stochastically. Pattern Recognit., 23, 529—537.

Lachenbruch,P.A. (1974) Discriminant analysis when the initial samples are
misclassiﬁed II: non-random misclassiﬁcation models. T echnometrics, 16,
419—424.

Lawrence,N.D. and Scholkopf,B. (2001) Estimating a kernel ﬁsher discriminant in
the presence of label noise. In: Proceedings of the 18th International Conference
on Machine Learning, pp. 306—313.

 

876

112 /810's112umo [p.IOJXO'SOUBIHJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

Robust sparse logistic regression

 

Li,L. et al. (2001) Gene assessment and sample classiﬁcation for gene expression
data using a genetic algorithm/k-nearest neighbor method. Comb. Chem. High
Throughput Screen., 4, 727—739.

Lugosi,G. (1992) Learning with an unreliable teacher. Pattern Recognit, 25, 79—87.

MacKay,D.J. (1995) Probable networks and plausible predictions - a review of
practical Bayesian methods for supervised neural networks. Network, 6,
469—505.

Maletic,J.I. and Marcus,A. (2000) Data cleansing: beyond integrity analysis. In:
Proceedings of the Conference on Information Quality, pp. 200—209.

Malossini,A. et al. (2006) Detecting potential labeling errors in microarrays by data
perturbation. Bioinformatics, 22, 2114—2121.

Muhlenbach,F. et al. (2004) Identifying and handling mislabelled instances. J. Intell.
Inf. Syst., 22, 89—109.

Ng,A.Y. (2004) Feature selection, L1 vs. L2 regularization, and rotational invari-
ance. In: Proceedings of the 21st International Conference on Machine Learning,
ICML ’2004, pp. 78—85.

Sétnchez,J.S. et al. (2003) Analysis of new techniques to obtain quality training sets.
Pattern Recognit. Lett., 24, 1015—1022.

Shevade,S.K. and Keerthi,S.S. (2003) A simple and efﬁcient algorithm for gene
selection using sparse logistic regression. Bioinformatics, 19, 2246—2253.

West,M. et al. (2001) Predicting the clinical status of human breast
cancer by using gene expression proﬁles. Proc. Natl Acad. Sci. USA, 98,
11462—11467.

Yang,T. et al. (2012) Multiple kernel learning from noisy labels by stochastic pro-
gramming. In: Proceedings of the 29th International Conference on Machine
Learning, ICML ’1202, pp. 233—240.

Yasui,Y. et al. (2004) Partially supervised learning using an EM-boosting algorithm.
Biometrics, 60, 199—206.

Zhang,C. et al. (2009) Methods for labeling error detection in microarrays based on
the effect of data perturbation on the regression model. Bioinformatics, 25,
2708—2714.

 

877

112 /810's112umo [p.IOJXO'SOUBIHJOJUIOIQ/ﬁdllq 111011 pop1201umoq

9IOZ ‘091sn8nv uo ::

