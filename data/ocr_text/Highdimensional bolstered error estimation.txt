ORIGINAL PAPER

Vol. 27 no. 21 2011, pages 3056-3064
doi: 1 0. 1 093/bioinformatics/btr5 1 8

 

Data and text mining

Advance Access publication September 13, 2011

High-dimensional bolstered error estimation
Chao Simal, Ulisses M. Braga—Neto2 and Edward R. Dougherty1’2’3’*

1Computational Biology Division, Translational Genomics Research Institute, Phoenix, AZ, 2Department of Electrical
and Computer Engineering, Texas A&M University, College Station, TX and 3Department of Pathology, University of

Texas M. D. Anderson Cancer Center, Houston, TX, USA

Associate Editor: Jonathan Wren

 

ABSTRACT

Motivation: In small-sample settings, bolstered error estimation
has been shown to perform better than cross-validation and
competitively with bootstrap with regard to various criteria. The
key issue for bolstering performance is the variance setting for the
bolstering kernel. Heretofore, this variance has been determined in
a non-parametric manner from the data. Although bolstering based
on this variance setting works well for small feature sets, results can
deteriorate for high-dimensional feature spaces.

Results: This article computes an optimal kernel variance depending
on the classification rule, sample size, model and feature space, both
the original number and the number remaining after feature selection.
A key point is that the optimal variance is robust relative to the model.
This allows us to develop a method for selecting a suitable variance
to use in real-world applications where the model is not known, but
the other factors in determining the optimal kernel are known.
Availability: Companion website at
http://compbio.tgen.org/paper_supp/high_dim_bolstering

Contact: edward@mail.ece.tamu.edu

Received on February 10, 2011; revised on September 2, 2011;
accepted on September 5, 2011

1 INTRODUCTION

Throughout most of the history of pattern recognition, the number
of features was much smaller than the numbers currently being
generated in high—throughput biology. Less than 15 years ago, in
two studies on feature selection most cases considered involved
<30 features and the maximum number considered was 65 (Jain
and Zongker, 1997; Kudo and Sklansky, 2000). The advent of
high—throughput technologies has radically altered the landscape.
In conjunction with large numbers of features, bioinformatics is
confronted by small sample sizes, often <100, which forces one
to train and test on the same data, where bias, variance (Braga—Neto
and Dougherty, 2004b) and lack of correlation with the true error
(Hanczar et al., 2007, 2010) can severely degrade error estimation.
Performance can degrade even further in the presence of feature
selection (Molinaro et al., 2005). Recent articles have pointed out
the difﬁculty in establishing performance advantages for proposed
classiﬁcation rules (Boulesteix, 2010; Jelizarow et al., 2010; Rocke
et al., 2009). Two statistically grounded sources of overoptimism
have been highlighted: (i) applying a classiﬁcation rule to numerous
datasets and then reporting only the results on the dataset for
which the designed classiﬁer possesses the lowest estimated error

 

*To whom correspondence should be addressed.

(Youseﬁ et al., 2010); and (ii) applying multiple classiﬁcation rules
to a dataset and comparing the classiﬁcation rules according to
the estimated errors of the designed classiﬁers (Boulesteix and
Strobl, 2009). In both cases, optimism is a result of inaccurate error
estimation.

A good error estimator ideally would have small bias and small
variance. This is a difﬁcult trade—off in small—sample settings. In
small—sample cases, resubstitution generally has small variance
but tends to be quite optimistically biased. Cross—validation has
small bias, but tends to display high variance. Bolstered error
estimation (Braga—Neto and Dougherty, 2004a) attempts to achieve a
compromise to this bias—variance dilemma in small—sample settings.
It is based on the idea of modifying (‘bolstering’) the empirical
distribution of the data by placing kernels at each data point
and then estimating classiﬁer error by the error on this bolstered
empirical distribution in such a way that it reduces bias, while at the
same time reducing variance. Bolstered error estimation has shown
good performance when compared with popular error estimators
in small—sample settings, in particular, for feature—set ranking and
when used internally within a feature—selection algorithm (Sima
et al., 2005a) and for ranking feature sets (Sima et al., 2005b).
Its good performance, including the latter applications, has been
demonstrated in the context a small number of features, including
feature selection via sequential forward selection (SFS), where it
is applied to small potential feature sets in the SFS algorithm.
A critical aspect of the method is selecting the right amount of
bolstering, which is given by the variance of the bolstering kernels.
The original bolstering paper (Braga—Neto and Dougherty, 2004a)
proposed a non—parametric estimator for the kernel variance, which
was found empirically to perform well in low—dimensional spaces;
however, estimation was found to degrade in high dimensions, so
that a correction factor can be required (Vu and Braga—Neto, 2008).
In fact, it was demonstrated in a preliminary study that a correction
factor can also be beneﬁcial for low—dimensional bolstering (Huynh
et al., 2007).

This leads us to consider optimal bolstering, speciﬁcally, ﬁnding
an optimal variance for the bolstering kernels. Error estimators like
resubstitution and cross—validation (assuming the number of folds is
preset) are non—parametric. They contain no free parameters. This
is not the case for bootstrap. In general, bootstrap has the form of a
convex error estimator, namely,

ébloot :(1_ aﬁresub +a<§zero, (1)

where éresub and Seer are the resubstitution and zero—bootstrap
estimators and 0 fa 5 1. The zero—bootstrap utilizes the empirical

distribution F*, which puts mass % on each of the n available

 

3056 © The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com

112 /§.IO'SIBUJHOprOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 11101; prBOIUAAOG

9IOZ ‘09 lsnﬁnv uo ::

High-dimensional bolstered error estimation

 

data points. A bootstrap sample 5;; from F* consists of n equally—
likely draws with replacement from the original data Sn. The basic
bootstrap zero estimator (Efron, 1983) is written in terms of the
empirical distribution as

602%. (|Y—g(s;:,X)|:(x,Y)es,,\s;:). (2)

In practice, the expectation EF* has to be approximated by a
Monte—Carlo estimate based on independent replicates 52kb, for
b = 1, . . . ,B, in which case the classiﬁer is designed on the bootstrap
sample and tested on the original data points left out. An optimal
bootstrap estimator results from a value of a that minimizes the
mean—square error between 5% and the true error for a given
feature—label distribution (Sima and Dougherty, 2006b). Setting
a=0.632, as is commonly done (Efron, 1983), can lead to a far
from optimal estimator (optimal weights).

The present article considers optimal bolstering relative to its
one free parameter, kernel variance and the manner in which
optimal bolstering can be used to arrive at practical implementation
of bolstering in high—dimensional feature space. The end product
is an implementation protocol in which optimal kernel variances
across different models are combined to produce a suitable kernel
variance for the problem at hand. Throughout, we will assume
feature selection because that would be the standard way to approach
classiﬁcation in the high—dimensional setting we are considering,
although this is not a mandatory requirement of the approach.

2 SYSTEMS AND METHODS

This section will be broken into subsections, with the aim of arriving at
the implementation protocol for real—world data. Section 2.1 brieﬂy reviews
the necessary essentials of error estimation, mainly bolstering. Section 2.2
deﬁnes the scaling factor by which to adjust the bolstering kernel to high
dimensions. Section 2.3 discusses optimization of the scaling factor and
illustrates the construction of a set of optimal scaling factors across a family
of models varying in both structure and classiﬁcation difﬁculty. Section 2.4
provides the implementation of high—dimensional bolstered resubstitution
based on a family of optimal scaling factors.

2.1 Error estimation

In two—group statistical pattern recognition, there is a feature vector
XeRp and a label Y e{0,1}. The pair (X, Y) has a joint probability
distribution F, which is unknown in practice. Hence, a classiﬁer is designed
from training data, which is a set of n independent observations, Sn:
{(X1,Y1),...,(Xn,Yn)}, drawn from F. A classiﬁcation rule is a mapping
\Ifn : {RP x {0, 1}}” x RP —> .73, where .7: is the set of mappings from RP into
{0,1}. It maps S, into a classiﬁer run :Rp—> {0, 1}. The classiﬁcation error
8,, is the probability of an erroneous classiﬁcation:

8n=P(wn(X)7éY|Sn)=EF(|Y_Wn(X)l)a (3)

where EF denotes expectation with respect to F. Were F known, then the
error could be found via Equation (3). In practice, one must use an error
estimator an. An error estimator can suffer from bias, Bias: E [en — 8n], and
deviation variance, VardeV =Var[§n — an]. These combine to contribute to the
most common measure (used herein) for evaluating the accuracy of an error
estimator, the root-mean-square (RMS):

RMS = «Egan—8,42 =j/Vardev-l—Blasz. (4)

2.1.1 Classical error estimation The simplest way to estimate the error
in the absence of independent test data is to compute its error directly on the

sample data itself. This resubstitution estimator, émub, is usually optimistic
(i.e. biased low), sometimes very much so.

In k-fold cross-validation, the dataset S, is partitioned into k folds
SS), for i=1,...,k (for simplicity, we assume that k divides n). Each
fold is left out of the design process and used as a test set, and the
estimate, écv, is the overall proportion of error on all folds. A k—fold cross—
validation estimator is unbiased as an estimator of 8n_n/k. Cross—validation
estimators are pessimistic, since they use smaller training sets to design
the classiﬁer; however, their bias tends to be small. Their main drawback
is their large variance (Braga—Neto and Dougherty, 2004b; Devroye et al.,
1996). Sometimes cross—validation is repeated some number of times with
different fold partitions and the results averaged. In this article, we use
10—fold cross—validation without repetition.

A recently developed estimation method, called adjusted bootstrap (éabs),
which carries out further bootstrap resampling in each fold, has been found
to have good RMS performances (J iang and Simon, 2007). Speciﬁcally, Sn is
partitioned into n folds and, for each sample left out for testing, B bootstrap
sample sets of size ln are drawn from the remaining n — 1 points, l = 1, 2, ,L.
For each l, the error e; is the proportion of misclassiﬁed samples across n
folds and B bootstrap sample sets. Finally, the adjusted bootstrap error éabs
is computed in the form

éabs = an—C +177
where a, b and a are least squares estimates for the function

e; =a(n-ul)_c+b,

and u; is the proportion of the expected number of non—repeated samples in
a size ln bootstrap sample set.

2.1.2 Bolstered error estimation The empirical feature-label distribution
F * is a discrete distribution that puts mass % on each of the n available data
points. The resubstitution estimator can be written in terms of the empirical
feature—label distribution as

éresubZEF*[lY_Wn(X)|]- (5)

Relative to F *, no distinction is made between points near or far from
the decision boundary. If one spreads the probability mass of the empirical
distribution at each point, then variation is reduced because points near the
decision boundary will have more mass on the other side of the boundary
than will points far from the decision boundary. Consider a probability
density function ff, for i: 1, ...,n, called a bolstering kernel, and deﬁne
the bolstered empirical distribution F O, with probability density function
given by

<> _1 ” <> _
f (X)—;Zi:1f,- (X—X.). (6)

The bolstered resubstitution estimator (Braga—Neto and Dougherty,
2004a) is obtained by replacing F * by F O in Equation (5) to obtain

éborst=EF<>[lY—1ﬁ(X)I]. (7)

Bolstering can be applied to other error estimators; however, we only use
bolstered resubstitution, the bolstering method used the most to date.
The bolstered resubstitution estimator is given by

. 1 ”

8bolst=; E (1y,=0/ fi<>(x—x,-)dx
- A
l=1 1

+1i=1f fi<>(x—x,-)dx),
A0

where Aj = {xl ib(x) = j}. The integrals are the error contributions made by
the data points, according to whether y,- = 0 or y,- = 1. If the classiﬁer is linear,
then the decision boundary is a hyperplane and it is usually possible to ﬁnd

 

3057

112 /§JO'S[BUJn0[pJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 11101; popeoprmoq

9IOZ ‘09 lsnﬁnv uo ::

C.Sima et aI.

 

analytical expressions for the integrals; otherwise, Monte—Carlo integration
can be employed.

The amount of bolstering determines the variance and bias properties
(hence, RMS also) of the bolstered estimator. As a general rule, wider
bolstering kernels lead to lower variance estimators, but after a certain point
this advantage becomes offset by increasing pessimistic bias. In the other
direction, insufﬁciently wide kernels tend to result in optimistic bias. A zero—
mean, spherical Gaussian bolstering kernel ff with covariance matrix of the
form K121, where I is the identity matrix, has been proposed (Braga—Neto and
Dougherty, 2004a), and has been shown to work well in low—dimensional
feature spaces. Since bolstered estimators spread the test points, the task is
to ﬁnd the amount of spreading that makes the test points to be as close as
possible to the true mean distance to the training data points. The true mean
distance can be estimated by its sample—based estimate:

 

Zl=1minj¢i{llxi—lel}11,=y
Z" I '
i=1 yl:y

The estimate dy is the mean minimum distance between points belonging

21, = (9)

to class y. Next, let fl.“ 1 be a unit—variance bolstering kernel, R,- be the random

variable equal to the distance of a point randomly selected from ff’1 to the

origin and F Rt. (x) be the cumulative distribution function of Ri. In the case

of the bolstering kernel ff with covariance matrix K121, all distances get

multiplied by Ki. In Braga—Neto and Dougherty (2004a), a single variance
2

Ky is estimated for all points from class y, such that the median distance of a

test point to the origin is equal to the estimated true mean distance dy. This
implies that half of the ‘ mass’ (i.e. the ‘ test points’ ) of the bolstering kernel
will be farther from the center than dy and the other half will be nearer. Hence,
K, is the solution of the equation KyFIQl(1/2)=£1y. Letting Otp =F§il(1/2),
and recognizing that the R,- are identically distributed, the estimated SDs for
the bolstering kernels are given by

Ki: —i, (10)

fori=1,2,...,n.

2.2 High-dimensional bolstered resubstitution

In high—dimensional settings, it is commonplace to perform feature selection
and, when performed, feature selection is part of the classiﬁcation rule, with
the entire set of potential features constituting the feature set relative to the
classiﬁcation rule. Feature selection constrains the space of functions from
which a classiﬁer might be chosen, but it does not reduce the number of
features in the design process. This is why when using cross—validation error
estimation, feature selection has to be carried out in each partitioned fold.

If we perform feature selection on a D—dimensional dataset 5,1,) and arrive
at a d —dimensional set Sf,l (d < D), then the bolstered error estimator can use
the previously deﬁned kernel size Ki, computed on 5,1,) , not Sf,l . Speciﬁcally,
the mean minimum distance dy is estimated on 5,1,) and up 2051). For high
dimensions, we replace K,- by

A

D
K7 =kD ><  (11)
where kD is an additional scaling factor determined by the dimension and
where we have indicated the dimension in the mean minimum distance
estimate. The idea is to adjust the kernel size by choosing kD so the bolstered
error estimator will be optimal (minimum RMS). kD = 1 yields the previously
proposed kernel variance. In essence, K,- is a parameter for the bolstered
estimator and Equation (11) sets it free, thereby allowing for optimization.
The situation is akin to 0.632 bootstrap as opposed to optimal bootstrap.
Given the kernel sizes, the bolstered resubstitution error estimate is given
by Equation (8) in D dimensions. For Gaussian kernels with independent
variables, this integral reduces. Let fi<>’d(x—x,-) and ﬁO’D_d(x—xi) denote
the Gaussian kernels in d— and (D—d)—dimensional spaces, respectively,

so that the D—dimensional Gaussian kernel decomposes as
fi<>(X—xi) =ﬁ0’dtx—xi)ﬁ<>:0‘d(x—xi). (12)

Denoting x —x,- as Axi, then Equation (8) can be rewritten as

A 1 n <>d <>D—d
Sgolst:;Z(IYi=0[41ﬁ ’ (Axuf, ’ (Axi)dx

i=1

+5.21] ﬁo’d(Axi)ﬁ<>’D_d(Axl-)dx)
A0
1 ” oo _
=;Z(Iyi=ofdﬁ<>’d(AXz-)dxf ff”) d(Axi-)dx+ (13)
i=1 A1 —00

OO
[i=1] ﬁ’dmxadxf ﬁO’D‘d(Axl->dx)
Ag —00

1 n
2.} :(IinOf fi<>’d(Ax,-)dx+lyi:1 f in’d(Ax,-)dx),
” i=1 A? A8

where Afl,j=0, 1, is the projection of the classiﬁer decision region Aj into
d—dimensional space, and we added a superscript ‘D’ to the bolstered error
estimator to indicate it refers to the error in D—dimensional space. The
previous result indicates that the integrals necessary to ﬁnd the bolstered
error estimate in D—dimensional space can be equivalently carried out in d—
dimensional space. This is akin to resubstitution, where the error count is
the same whether it is done in D— or d—dimensional space. For performance
comparison purposes, we will also estimate the kernel size using only the
low—dimensional data Sf,l , resulting in a bolstered error estimator égolst, which
uses the originally proposed kernel variance (no correction, or kD =1). For
feature selection, we will use sequential forward ﬂoating search (SFFS)
(Pudil et al., 1994).

2.3 Optimization method

To ﬁnd the optimal kernel scaling factor kD, we utilize the following
procedure:
Protocol 1

(1) Generate a sample set 5,? of size n and a total of D features from a
speciﬁed synthetic model.

(2) Select a size—d feature set A using a feature—selection method F on
5,1,), resulting in a reduced dimension sample set Sf,l for the feature
setA.

(3) Design a classiﬁer run for 5,21 according to the given classiﬁcation
rule ‘11”.

(4) Compute the true error 8,, using the underlying distribution of
the model.

(5) Compute the 10—fold cross—validation error écv (keeping in mind that
feature selection must be repeated for each fold).

Ad
(6) Compute the bolstered error 8b 015,.

(7) Compute the bolstered errors €555, for a list of kernel scaling factors
k k

DJ, D2, 
(8) Calculate RMS for each error estimator by repeating Steps 1 through
7 a number N of times.

(9) Repeat Steps 1 through 8 for different models M, different levels of
model complexities and different classiﬁcation rules ‘11”.

We consider four data models, each a two—class Gaussian model with
equally likely classes and class—conditional densities having covariance
matrices 21 and 22. One class mean is located at —;7. and the other at 17., with
the location of i7. = 8* [a1 a2  aD] depending on the model. The parameter
8 is chosen to achieve prescribed values for the expected classiﬁcation error
E [8”]; different values of E [an] represent different levels of difﬁculty at
sample size n.

 

3058

112 /§JO'S[BUJn0[pJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 11101; popeoprmoq

9IOZ ‘09 lsnﬁnv uo ::

High-dimensional bolstered error estimation

 

Table 1. Summary of simulation experiments

 

Data models M M1, M2, M3, M4

Model difﬁculty E[8,,] 0.05,0.10,0.15

Classiﬁcation rules ‘11,, LDA,3NN, LSVM
NNet,CART

Feature—selection methods .7: SFFS

No. of repetitions N 500

No. of sample size n 50,100, 150

No. of selected features d 5, 10

No. of total features D 200, 500

Kernel scaling factors kD 0.2 to 2.0 in 0.2 increment

 

d0 = 10, G = 20, c = 2.25,,0 = 0.25

 

LDA, linear discriminant analysis; 3NN, 3-nearest—neighbor; LSVM, linear support
vector machine; NNet, neural net; CART, classiﬁcation and regression tree.

M1: A simple linear model in which 21 = 22 = I, the identity matrix, so that
all features are uncorrelated. a,- =1 for i = 1,2, ...do and uniformly
distributed over [0,1] for i: d0+1,d0+2, ...D before all ai’s are
randomly permuted.

M2: Similar to M l but with 21 =1 and 22 2 c1, where c is a constant and
c 751. The Bayesian decision boundary is quadratic.

M3: This is a Block Covariance Model where all features are equally divided
into G groups. The features from different groups are uncorrelated
and the features from the same group possess the same correlation, ,0,
among each other. The structure of the covariance matrix is

[2p 0

i 0 2

zlzzzzszl .
i .
l

'b
CO

|________l

M...
b

0 0
ﬁf—J
G blocks

where 2,, has 1 on the diagonal and ,0 off the diagonal. Here a1: 1 for
i = 1, 2, ...D.
M4: Similar to M3, but with 21: 2c; and 22 =cZg.

Table 1 gives a summary of the simulation experiments. Two limiting
factors should be noted. First, the maximum total number of features, 500,
is smaller than those often considered in microarray studies and, second, the
number of selected features is kept to 5 or 10. There are three reasons for this,
one pragmatic to our set of simulations and the others having to do with the
nature of feature selection. The pragmatic reason is computational: we wish
to do a large simulation study and therefore want to limit the computational
burden. As for feature selection, given the sample sizes, it is prudent to
keep the numbers of total and selected features small to have satisfactory
feature selection (Sima and Dougherty, 2006a) and the number of selected
features small to avoid the peaking phenomena (Hua et al., 2005, 2009).
Regarding the total number of features, limiting the total number of features
via prior biological knowledge or requirements on data quality raises the
likelihood of ﬁnding good feature sets via feature selection (Zhao et al.,
2010). Regarding the efﬁcacy of selecting small feature sets, studies have
shown that good classiﬁcation can be achieved with two or three genes when
re—examining data from studies that had originally used much larger feature
sets (Braga—Neto, 2007; Grate, 2005).

We plot the RMS versus kernel scaling factor kD for @501“, using all
combinations of simulation parameters displayed in Table 1. Additionally,
we compute the RMS for LDA with D: 200, d =3 and n=50 for E [8”] =
0.20, 0.25, 0.30, 0.35, 0.40 and 0.45. For comparison, RMS values for égolst,
écv and éabs are also plotted, which appear as horizontal lines as they are

not related to kD. Here, we present some typical results, the complete set
of plots appearing on the companion website. Note that due to the intensive
computing in éabs we only compute it for LDA with D2200, d=3 and
n = 50.

Figure 1 shows the result for LDA, n=50, and selecting d=3 out of
D = 200 features for nine values of E [8”]. Letting kg“ denote the value of kD
achieving minimum RMS, we see that k““ increases for increasing expected
error, the increase being slight for small expected errors but becoming
signiﬁcant for large expected errors (for E [8”] = 0.40 and 0.45, kg“ is to
the right of where we have stopped the plots at k0 = 2.0; see extended plots
to kD 23.0 for these cases on the companion website). We observe that écv
and éabs typically perform better than 5% 015,, sometimes by a large margin.
However, for an appropriate kernel scaling factor, éfolst outperforms éabs
and often outperforms écv by a wide margin. This improvement is achieved
by a range of scaling factors and is robust across different models and
complexities. Regarding model robustness, for a ﬁxed value of E [8”] the
RMS curves are remarkably similar; in particular, the value, kg“, of kD
achieving minimum RMS is consistent. In three cases, kg“ remains ﬁxed
across the models and in the others it changes by not > 0.2. Moreover, in
the latter cases, the RMS at the different values of kg“ is approximately the
same. The overall robustness has important practical implications, because in
real—world problems we do not know the data model or its level of difﬁculty,
but we do know the sample size n, the total and selected numbers of features,
D and d, and the classiﬁcation rule. As we will subsequently see, the fact
that kg“ is robust relative to the data model means that, in practice, we can
derive a value of kD, albeit not optimal, that can be used in éfolst for a better
error estimator.

There is also robustness with respect to the classiﬁcation rule and number
of features. Figure 2a and b show robustness curves for 3NN and CART,
respectively, for complexity E [8,,] 20.10 (more on the companion website)
for n=50, d=3 and D2200. The curves bear a strong resemblance to
the corresponding curves for LDA in Figure 1 and for all models kg“ =
0.8, as with LDA. We again have LDA in Figure 2c for E[8,,]=0.10
(more on the companion website), but now with D2500. Again there is
resemblance to the corresponding case in Figure 1 and again kg“ 20.8 for
all models.

The preceding observations are mostly constrained to small samples.
When n is large, the beneﬁts of using @553? tend to diminish. Figure 3a and b
show RMS curves for LDA for n: 100 and n: 150, respectively, E [8,,] =
0.10 (more on the companion website), d=3 and D2200. If the model
is known, an optimal @5015, is achievable, but robustness diminishes. For
n = 100, there is still some robustness, but for n = 150, even a small deviation

from kg“ can result a worse performance than écv. Hence, for n: 150,

. . AD,opt . . . . .
choosmg an appropriate abolst is not feas1ble in practice; however, smce our

interest is using bolstered error estimation for very small samples, this is not
a signiﬁcant drawback.

2.4 Implementation for real data

For practical application, based on the sample size, the total and selected
numbers of features, and the classiﬁcation rule, we will perform a model—
based analysis like the ones we have performed, thereby resulting in a look—up
table of pairs (E [8”], kg“) as in Figure 1. To illustrate, by averaging across
the four models, we obtain the following table for (E [8”], kg“): (0.05, 0.8),
(010,08), (015,09), (0,20,1.0), (0.25, 1.2), (0.30, 1.3), (0,35, 1.6). Upon
designing a classiﬁer from the data, we will obtain the 10—fold cross—
validation error estimate, 80, and then, in the fashion of the method of
moments, set E [8n]=80 and choose the corresponding value of kg“ to
serve as the scaling factor for bolstering. Since the look—up table is discrete,
E [8n]=80 must be solved approximately by interpolation. Corresponding
to the seven values of kg“ in the look—up table for Figure 1, we have:
kgin=0.8 for 80<0.125, kgﬁn=0.9 for 0.125580<0.175, kglin= 1.0 for
0.175580 <0.225, k131i“: 1.2 for 0.225580<0.275, kgﬁn=r3 for 0.275:
80 <0.325 and kgin=1.6 for 0.325580. If one so desires, then a ﬁner

 

3059

112 /§JO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdnq 11101; popeoprmoq

9IOZ ‘09 lsnﬁnv uo ::

C.Sima et aI.

 

E[e,,] = 0.05

E[€n] = 0.10

E[an] = 0.15

E[e,,] = 0.20

E[e,,] = 0.25

E[e,,] = 0.30

E[s,,] = 0.35

E[e,,] = 0.40

E[e,,] = 0.45

M1

M2

M3

M4

 

 

0.2 0.2 0.2 0.2
0.15 0.15 0.15 0.15
0 1 0.1 0.1 0.1
0.05 0 05 0.05 I 0.05 l

 

 

v

 

0.2 0.4 0.6 0.8 1 1.21.41.61.8 2

0.2 0.4 0.6 0.8 1 1.21.41.61.6 2

 

v

 

   

0.20.4 0.60.8 1 1.21.41.61.8 2

\/

 

\/

 

 

 

 

v 0.2 0.40.6 0.8 1 1.21.41.61.8 2

0.20.4 0.60.8 1 1.21.41.61.8 2

 

 

 

 

 

 

v 0.2 0.40.6 0.8 1 1.21.41.61.8 2

0.2 0.4 0.6 0.8 1 1.2 1.4 1.61.8

 

   

 

 

 

 

 

 

 

 

 

02 0.2 0.2 0.2
0.15 0.15 \ 0.15 \
0.1 0.1
0.05 ' 0.05 v

c 0.2 0.4 0.6 0.8 1 1.21.41.61.8 2 0.2 0.4 0.6 0.8 1 1.21.41.61.63 2 c 0.2 0.4 0.6 0.8 1 1.21.41.61.8 2 0.2 0.4 0.6 0.8 1 1.21.41.61.8 2
0.2
0.15
0.1
0.05

“ 0.2 0.4 0.6 0.8 1 1.21.41.61.8 2 0.2 0.4 0.5 0.3 1 1.21.41.61.13 2 “ 0.2 0.4 0.6 0.8 1 1.21.41.61.13 2 0.2 0.4 0.6 0.8 1 1.21.41.61.8 2
025 025

\

\

\

 

 

 

\/

 

 

0.2 0.4 0.6 0.8 1 1.21.41.61.8 2

 

 

0.20.4 0.60.8 1 1.21.41.61.8 2

 

 

\__/

 

0.2 0.4 0.6 0.8 1 1.21.41.61.13 2

0.25 0.25
\

0.2 0.4 0.6 0.8 1 1.21.41.61.8 2

\

 

02 0.2
015 0.15
0.1

 

o..-..¢...g.. .. . ... .. ..

 

0.20.4 0.60.8 1 1.21.41.61.53 2

 

 

0.2 0.4 0.6 0.8 1 1.21.41.61.13 2

V 0.2 0.40.6 0.8 1 1.21.41.61.8 2

0.20.4 0.60.8 1 1.21.41.61.8 2

 

 

 

 

 

 

 

0.25 \ 0.25 \ \ \
0.2 0.2

0.15 0.15
0.1 1 0.1

0.05 \ 0.05

 

 

 

 

 

 

0.2 0.4 0.6 0.8 1 1.21.41.61.8 2

AD
5blst

0.2 0.4 0.6 0.8 1 1.21.41.61.8 2

A
EC’U

u 0.2 0.40.6 0.8 1 1.21.41.61.8 2

Ad

Eblst Eabs

0.20.4 0.60.8 1 1.21.41.61.8 2

o--e--o

Fig. 1. RMS versus scaling factor kD for LDA with sample size n = 50, total feature size D = 200 and selected feature size d = 3.

selection of expected errors and interpolation can be obtained. One might
also use a coarser interpolation for computational purposes, with some loss

kgﬁn=12 for 0.225: 8050.275; kgﬁn=r4 for 0.275: 8050.325; and
kgi“ :15 for 80 > 0.325.

of performance. In fact, that is precisely what we do here because we
will subsequently perform a computationally intensive robustness analysis.
Here we use: kg“=0.8 for 80<0.125; kg“=1 for 0.125580<0.225;

The ﬁnal bolstered error estimate is computed from the data using this
scaling factor. The success of the procedure depends on robustness in
choosing a scaling factor because (i) the estimated model will be inaccurate

 

3060

112 /§JO'SIBUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 11101; popeoprmoq

9IOZ ‘09 lsnﬁnv uo ::

High-dimensional bolstered error estimation

 

(a) M1 M2 M3 M4

 

  
    
 
  
 

 

u 0.2 0.4 0.6 0.8 1 1.21.41.61.8 2
0.2

” O.20.40.60.8 1 1.21.41.61.8 2 “

   

 

0.2 0.4 0.6 0.8 1 1.21.41.61.8 2 0.20.4 0.6 0.8 1 1.21.41.61.8 2

 

 

 

 

 

 

V O.20.40.60.8 1 1.21.41.61.8 2 v 0.20.40.60.8 1 1.21.41.61.8 2 v 0.20.40.60.8 1 1.21.41.61.8 2 V 0.20.40.60.8 1 1.21.41.61.8 2

AD x Ad
Eblst 5811 5513i

Fig. 2. RMS versus scaling factor kD for sample size n = 50 and selected feature size d = 3, for (a) 3NN with total feature size D = 200, (b) CART with D = 200
and (c) LDA with D = 500.

 

 

 

 

 

 

   

 

(a)

0.2

0.15

0.1

0.05

c 0.2 0.4 0.6 0.8 1 1.21.41.61.8 2 3 G 0.2 0.4 0.6 0.8 1 1.21.41.61.8 2

 0 4 0 2 0.2

0 3 0.15 0.15

0.2 0.1 0.1

0.1 0.05

0.20.40.60.8 1 1.21.41.61.8 2 u 0.20.40.60.8 1 1.21.41.61.8 2 u 0.20.40.60.8 1 1.21.41.61.8 2 u 0.20.40.60.8 1 1.21.41.61.8 2

AD A Ad
5515i 56v 551st

Fig. 3. RMS versus scaling factor kD for LDA with total feature size D = 200 and selected feature size d = 3, for (a) sample size n = 100 and (b) n = 150.

3 RESULTS AND DISCUSSION

To illustrate application, we have applied the method to two gene
expression datasets:

owing to small sample size, (ii) cross—validation has signiﬁcant variance
for small samples, (iii) the estimated model will differ to some extent from
the models involved in creating the look—up table and (iv) the method of
moments is not optimal. The following protocol is used to obtain the bolstered
resubstitution error estimate:

0 Myeloma dataset: data are downloaded from the NIH
Protocol 2

Gene Expression Omnibus (GEO) under accession numbers
GSE5900 and GSE2658, which contain 54613 probe sets
and 559 multiple myeloma (MM) samples, as well as 3
other subtypes [monoclonal gammopathy of undetermined
signiﬁcance (MGUS)], 44 samples; smoldering MM (SMM),
12 samples; healthy donors with normal plasma cell (NPC),
22 samples (Zhan et al., 2006). Samples are labeled into two
classes, one for MGUS/SMM/NPC and the other for MM.

Due to the signiﬁcant unbalance of the samples between the
AD,data

(4) Compute the bolstered error estimate abolst using the selected scaling two Classes’ only 156 samples are randomly seleCted from
factor. the 559 MM samples. The number 156 has been chosen as

(1) Given a sample set S? with size n and dimension D, select a size—d
feature set A using a feature—selection method F on S? , resulting in a
reduced dimension sample set S21 for the feature set A.

(2) Design a classiﬁer run for S21 according to the given classiﬁcation rule
‘11”, and compute the 10—fold cross—validation error estimate 80.

(3) From the look—up table (E [8,,],kgnin) choose the kernel scaling factor
kg“ by setting E [8,,] =80.

 

3061

112 /§JO'S[BUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq 11101; popeoprmoq

9IOZ ‘09 lsnﬁnv uo ::

C.Sima et al.

 

a compromise to take as many samples as possible from
MM without signiﬁcant unbalance between the two classes.
Furthermore, only D: 200 features with the largest variances
across samples are selected from the total 54 613 probe sets. It
is advantageous to limit ourselves to the 200 features with the
largest variances, because these are more likely to reveal class
discrimination and feature selection tends to perform poorly
for very large numbers of features when samples are small
(Sima and Dougherty, 2006a). Here we must put in a word
of caution concerning the methodology. We are using feature
variance to produce a set of 200 features to be taken as the
full feature set for our performance analysis and will apply
feature selection, classiﬁer design and error estimation based
on this set, including cross—validation. In practice, this approach
would be unacceptable, because the actual dataset to which we
are applying data—dependent feature selection is the full 54 613
probe sets. For instance, cross—validation would have to use the
variance—based feature reduction from the full 54 613 on each
fold, else it would be optimistically biased. But that is not our
goal here. We are a priori assuming that there are only 200
features to which we will apply data analysis. In practice, such
a scenario would occur if the reduction to 200 were based on
prior biological knowledge.

° Breast cancer data set: data are from a microarray—based
classiﬁcation study that analyzes breast tumor samples from
295 patients (van de Vijver et al., 2002). Using a previously
established D=70—gene prognosis proﬁle (van’t Veer et al.,
2002), a prognosis signature based on gene expression is
proposed in van de Vijver et al. (2002) that correlates well
with patient survival data and other clinical measures. Of the
295 microarrays, 115 belong to the ‘good—prognosis’ class and
180 belong to the ‘poor—prognosis’ class. Referring to our
cautionary comment regarding the multiple myeloma data, we
note here that feature selection was used originally to obtain the
70 genes, but, again, from our performance perspective, that is
not important for our analysis.

We consider sample size n :50 and d :3 features selected from
the D: 200 and D270 features in the myeloma and breast cancer
datasets, respectively, and LDA for classiﬁcation. We repeatedly
draw (stratiﬁed) n=50—point samples with replacement from the
empirical distribution (full dataset) as training data with the
remaining sample points held out for true error estimation in
computing the RMS (s0 is still computed from the training data).
The total number of repetitions is 200. The average true error and
SD for the myeloma dataset are 0.2170 and 0.0309, respectively.
For the breast cancer dataset, the average true error and SD are
0.2340 and 0.0362, respectively. Figure 4 shows the RMS for the

two patient datasets. In both cases, égéﬁita performs signiﬁcantly
better. Owing to robustness of the optimal scaling factor, a coarse
selection of expected errors and interpolation has proven sufﬁcient.

To further demonstrate the effectiveness of Protocol 2, we
have applied it to four models in Figure 1: models M1 and
M2 with expected errors 0.20 and 0.35. The performance graphs
corresponding to Figure 4 are provided in Figure 5. Of particular
interest are the scaling factors produced by the protocol. The average
scaling factors for the four models are given by: M1, E [an] =0.20
— average scaling factor 1.10; M1, E [an] 20.35 — average scaling
factor 1.39; M2 E [an] =0.20 — average scaling factor 1.09; and M2,

 

 

 

 

 

         

EC” 8bolst 8bolst Subs ECU abolst Ebolst subs

Fig. 4. RMS using LDA for (a) myeloma dataset, total feature size D: 200
and (b) breast cancer dataset, total feature size D270. For both datasets:
sample size n = 50 and selected feature size d = 3.

 

 

 

 

 

 

 

50v Ebolst eboist Eabs Ehoist Ebolst

 

 

 

 

 

 

 

 

80'” Ebolst abolst 8017-3 Ebolst Ebolst aabs

Fig. 5. RMS using LDA and protocol 2 for (3) M1, E[8,,] 20.20, (b) Ml,
E[8,,]=0.35, (c) M2, E[8,,]=0.20, ((1) M2, E[8,,]=0.35. All with sample
size n = 50 and selected feature size d = 3.

E [8n] = 0.35 — average scaling factor 1.43. Referring to Figure 1, we
see that all these averages are centered within the range of scaling
factors where optimal bolstering outperforms éabs.

3.1 Robustness to non-Gaussian data

Although k?“ is derived with Gaussian models, it is robust enough
for models where this assumption is violated, as with the patient data,
where the underlying distribution is almost certainly not Gaussian.
To further investigate this issue, we take the model M2 in Section
2.3, but perturb the skewness and kurtosis of the class at the origin
to obtain a Pearson system (Elderton and Johnson, 1969). Figure 6
shows the eight different distributions in the Pearson system with
varying skewness and kurtosis. For the resulting model Mp and
each skewness and kurtosis combination, where valid, we do the
following:

(1) Generate a sample set 5,? of size n = 50 and a total of D = 200
features from the model Mp .

(2) Feature select a size—d = 3 feature set A, resulting in a reduced
dimension sample set 53'.

(3) Design a classiﬁer run for 531 using LDA.

 

3062

112 /§JO'S[BUJHOIPJOJXO'SOIlBIIIJOJUIOIQ/ﬁdllq morj popeoprmoq

9IOZ ‘091snﬁ'nv uo ::

High-dimensional bolstered error estimation

 

Type 7
Type 6
Type 5
Type 4

' Type 3
\Point of Type 0

Type 2

 

Skewness

Type 1

Type 0

 

 

Kurtosis

Fig. 6. The plane of (skewness, kurtosis) pairs and their corresponding
probability distributions in a Pearson system. In particular, Type 0 (Gaussian
distribution) has a skewness of 0 and kurtosis of 3, which is represented by
a single point on the plane. There are eight different types of distributions in
a Pearson system.

kurtosis
012 3 4 5 6 7 8 9101112131415161718192021222324

 

-0.049
-0.039
-0.029
-0.019
-0.009
N/A

skewness
N

 

 

 

Fig. 7. Heatmap for RMS of égo’gita minus RMS of 3C, for different skewness

and kurtosis. All values are negative.

(4) Compute the true error 8,, using the underlying distribution
of the model Mp .

(5) Compute the 10—fold cross—validation error 30V.

(6) Compute §€6ﬁita using kg‘in from the previous section.
AD,data

(7) Calculate RMS for 50V and abolst
through 6 a number N = 400 of times.

by repeating Steps 1

Figure 7 shows the values of RMS for 556%? minus the RMS

for écv for different skewness and kurtosis in a heatmap. Due to
symmetry, only positive skewness is shown. In all cases, égéﬁita is

superior to écv.

3.2 Concluding remarks

We have derived an optimal kernel scaling factor that can be used for
bolstered error estimation in high feature dimensions. This bolstered
error estimator achieves a signiﬁcant RMS improvement over cross—
validation when samples are small, with continued, albeit smaller,
performance improvement over the adjusted bootstrap. This superior
performance is robust over a wide range of models. Hence, we
have been able to incorporate optimality criteria from across a
collection of families to arrive at suitable bolstering kernels for
practical situations, thereby facilitating its use in applications like
classiﬁcation of genomic data when samples are small.

ACKNOWLEDGEMENTS

We would also like to thank the High—Performance Biocomputing
Center of TGen for providing the clustered computing resources
used in this study; this includes the Saguaro—2 cluster supercomputer,
a collaborative effort between TGen and the ASU Fulton High
Performance Computing Initiative.

Funding: National Science Foundation (CCF—0634794 and CCF—
0845407); National Institutes of Health grant (1S10RR025056—01)
to Saguaro—2 cluster, in part.

Conﬂict of Interest: none declared.

REFERENCES

Boulesteix,A.—L. (2010) Over-optimism in bioinformatics research. Bioinformatics, 26,
437—439.

Boulesteix,A.—L. and Strobl,C. (2009) Optimal classiﬁer selection and negative bias
in error rate estimation: an empirical study on high-dimensional prediction. BMC
Med. Rese. Methodol, 9, 85.

Braga—Neto,U. (2007) Fads and fallacies in the name of small-sample microarray
classiﬁcation. IEEE Sig. Proc. Mag, 24, 91—99.

Braga—Neto,U. and Dougherty,E. (2004a) Bolstered error estimation. Pattern Recognit,
37, 1267—1281.

Braga—Neto,U.M. and Dougherty,E.R. (2004b) Is cross-validation valid for small-
sample microarray classiﬁcation? Bioinformatics, 20, 374—380.

Devroye,L. et al. (1996) A Probabilistic Theory of Pattern Recognition. Springer, New
York.

Efron,B. (1983) Estimating the error rate of a prediction rule: Improvement on cross-
validation. J. Am. Stat. Assoc., 78, 316—331.

Elderton,S.W. and Johnson,N. ( 1969) Systems of Frequency Curves. Cambridge
University Press, Cambridge.

Grate,L. (2005) Many accurate small-discriminatory feature subsets exist in microarray
transcript data: biomarker discovery. BMC Bioinformatics, 6, 97.

Hanczar,B. et al. (2007) Decorrelation of the true and estimated classiﬁer errors in
high-dimensional settings. EURASIP J. Bioinformatics Syst. Biol, 2007, 1—12.
Hanczar,B. et al. (2010) Small-sample precision of roc-related estimates.

Bioinformatics, 26, 822—830.

Hua,J. et al. (2005) Optimal number of features as a function of sample size for various
classiﬁcation rules. Bioinformatics, 21, 1509—1515.

Hua,J. et al. (2009) Performance of feature-selection methods in the classiﬁcation of
high-dimension data. Pattern Recognit, 42, 4094124.

Huynh,K. et al. (2007) Improved bolstering error estimation for gene ranking. In
Proceedings of the IEEE EMBS. Lyon, France.

Jain,A. and Zongker,D. (1997) Feature selection: evaluation, application, and small
sample performance. IEEE Trans. Pattern Anal. Mach. Intell., 19, 153—158.

Jelizarow,M. et al. (2010) Over-optimism in bioinformatics: an illustration.
Bioinformatics, 26, 1990—1998.

Jiang,W. and Simon,R. (2007) A comparison of bootstrap methods and an adjusted
bootstrap approach for estimating the prediction error in microarray classiﬁcation.
Stat. Med, 26, 5320—5334.

Kudo,M. and Sklansky,J. (2000) Comparison of algorithms that select features for
pattern classiﬁers. Pattern Recognit, 33, 25—41.

Molinaro,A.M. et al. (2005) Prediction error estimation: a comparison of resampling
methods. Bioinformatics, 21, 3301—3307.

Pudil,P. et al. ( 1994) Floating search methods in feature-selection. Pattern Recognit.
Lett, 15, 1119—1125.

Rocke,D.M. et al. (2009) Papers on normalization, variable selection, classiﬁcation or
clustering of microarray data. Bioinformatics, 25, 701—702.

Sima,C. and Dougherty,E. (2006a) What should be expected from feature selection in
small-sample settings. Bioinformatics, 22, 2430—2436.

Sima,C. and Dougherty,E.R. (2006b) Optimal convex error estimators for classiﬁcation.
Pattern Recognit, 39, 1763—1780.

Sima,C. et al. (2005a) Impact of error estimation on feature selection. Pattern Recognit,
38, 2472—2482.

Sima,C. et al. (2005b) Superior feature-set ranking for small samples using bolstered
error estimation. Bioinformatics, 21, 1046—1054.

van de Vijver,M.J. et al. (2002) A gene-expression signature as a predictor of survival
in breast cancer. N. Engl. J. Med, 347, 1999—2009.

 

3063

112 /810's112urnofproarx0'sor1eu1101urorq//:d11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

C.Sima et al.

 

van’t Veer,L.J. et al. (2002) Gene expression proﬁling predicts clinical outcome of
breast cancer. Nature, 415, 530—536.

Vu,T. and Braga—Neto,U. (2008) Preliminary study on bolstered error estimation in
high-dimensional spaces. In Proceedings of the IEEE GENSIPS. Phoenix, AZ.
Youseﬁ,M.R. et al. (2010) Reporting bias when using real data sets to analyze

classiﬁcation performance. Bioinformatics, 26, 68—76.

Zhan,F. et al. (2006) The molecular classiﬁcation of multiple myeloma. Blood, 108,
2020—2028.

Zhao,C. et al. (2010) Characterization of the effectiveness of reporting lists of small
feature sets relative to the accuracy of the prior biological knowledge. Cancer
Inform, 9, 49—60.

 

3064

112 /810'sreurnofproarxosor1eu1101urorq//zd11q 111011 pop1201umoq

9IOZ ‘09 lsnﬁnv uo ::

