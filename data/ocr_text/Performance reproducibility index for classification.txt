ORIGINAL PAPER

Vol. 28 no. 21 2012, pages 2824—2833
doi: 10. 1 093/bioinformatics/bts5 09

 

Data and text mining

Advance Access publication September 6, 2012

Performance reproducibility index for classification

Mohammadmahdi R. Yousefi‘ and Edward R. Dougherty1’2’*

1Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX 77843 and
2Computational Biology Division, Translational Genomics Research Institute, Phoenix, AZ 85004, USA

Associate Editor: Martin Bishop

 

ABSTRACT

Motivation: A common practice in biomarker discovery is to decide
whether a large laboratory experiment should be carried out based on
the results of a preliminary study on a small set of specimens.
Consideration of the efficacy of this approach motivates the introduc-
tion of a probabilistic measure, for whether a classifier showing pro-
mising results in a small-sample preliminary study will perform similarly
on a large independent sample. Given the error estimate from the
preliminary study, if the probability of reproducible error is low, then
there is really no purpose in substantially allocating more resources to
a large follow-on study. Indeed, if the probability of the preliminary
study providing likely reproducible results is small, then why even per-
form the preliminary study?

Results: This article introduces a reproducibility index for classiﬁca-
tion, measuring the probability that a sufficiently small error estimate
on a small sample will motivate a large follow-on study. We provide a
simulation study based on synthetic distribution models that possess
known intrinsic classification difficulties and emulate real-world scen-
arios. We also set up similar simulations on four real datasets to show
the consistency of results. The reproducibility indices for different dis-
tributional models, real datasets and classification schemes are em-
pirically calculated. The effects of reporting and multiple-rule biases on
the reproducibility index are also analyzed.

Availability: We have implemented in C code the synthetic data dis-
tribution model, classification rules, feature selection routine and error
estimation methods. The source code is available at http://gsp.tamu
.edu/Publications/supplementary/yousefi12a/. Supplementary simula-
tion results are also included.

Contact: edward@ece.tamu.edu

Supplementary Information: Supplementary data are available at
Bioinformatics online.

Received on July 3, 2012; revised on August 6, 2012; accepted on
August 10, 2012

1 INTRODUCTION

Perhaps no problem in translational genomics has received more
attention than the discovery of biomarkers for phenotypic dis-
crimination. To date, there has been little success in developing
clinically useful biomarkers and much has been said concerning
the lack of reproducibility in biomarker discovery (Boulesteix
and Slawski, 2009; Ioannidis, 2005; Sabel et al., 2011; Zhang
et al., 2008). In particular, recently a report concerning

 

*To whom correspondence should be addressed.

comments made by US Food and Drug Administration (FDA)
drug division head Janet Woodcock stated:

Janet Woodcock, drug division head at the FDA, this week
expressed cautious optimism for the future of personalized
drug development, noting that ‘We may be out of the general
skepticism phase, but we’re in the long slog phase. . .’. The
‘major barrier’ to personalized medicine, as Woodcock sees
it is ‘coming up with the right diagnostics’. The reason for
this problem is the dearth of valid biomarkers linked to
disease prognosis and drug response. Based on conversa-
tions Woodcock has had with genomics researchers, she
estimated that as much as 75% of published biomarker as-
sociations are not replicable. ‘This poses a huge challenge
for industry in biomarker identiﬁcation and diagnostics de-
velopment’, she said (Ray, 2011).

Evaluating the consistency of biomarker discoveries across
different platforms, experiments and datasets has attracted the
attention of researchers. The studies addressing this issue mainly
revolve around the reproducibility of signals (for example, lists of
differentially expressed genes), their signiﬁcance scores and rank-
ings in a prepared list. They try to answer the following question:
Do the same genes appear differentially expressed when the ex-
periment is re-run? Boulesteix and Slawski (2009), Li et al.
(2011), Zhang et al. (2009) and the references therein suggest
several solutions to this and related questions. Our interest is
different.

A prototypical reproducibility paradigm arises when a classi-
f1er is designed on a preliminary study based on a small sample,
and, based on promising reported results, a follow-on study is
performed using a large independent data sample to check
whether the classiﬁer performs well as reported in the prelimin-
ary study. Many issues affect reproducibility, including the meas-
urement platform, specimen handling, data normalization and
sample compatibility between the original and subsequent stu-
dies. These may be categorized as laboratory issues; note that
here we are not talking about the issue of providing access to
data and software for follow-up studies on published results
(Hothorn and Leisch, 2011). One can conjecture mitigation of
these issues as laboratory technique improves; however, there is a
more fundamental methodological issue, namely, error estima-
tion. In particular, inaccurate error estimation can lead to ‘over-
optimism’ in reported results (Boulesteix, 2010; Castaldi et al.,
201 1).

Consider a protocol in which the expressions of 30 000 genes
are measured from 50 patients, each suffering from a different
stage of breast cancer—30,000 features and a sample size of 50.

 

2824 © The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com

112 [3.10811211an[plOJXO'SODBIILIOJIIiOiQ/ﬂ(11111 IIIOJJ papeolumoq

910K ‘09 lsnﬁnV no :2

Performance reproducibility index

 

The typical analysis proceeds in the following fashion: (i) based
on the data, a feature set is chosen from the 30 000; (ii) a classiﬁer
is designed, with feature selection perhaps being performed in
conjunction with classiﬁer design and (iii) the classiﬁcation
error is measured by some procedure using the same sample
data upon which feature selection and classiﬁer design have
been performed. Given no lack of reproducibility owing to la-
boratory issues, if the error estimate is sufﬁciently deemed small
and a follow-on study with 1000 independent data specimens is
carried out, can we expect the preliminary error estimate on a
sample of 50 to be reproduced on a test sample of size 1000?
Since the root-mean—square (RMS) error between the true and
estimated errors for independent-test—data error estimation is
bounded by (AM—i)“, where m is the size of the test sample
(Devroye et al., 1996), a test sample of 1000 insures
RMS 5 0.016, so that the test-sample estimate can be taken as
the true error.

There are two fundamental related questions (Dougherty,
2012): (i) Given the reported estimate from the preliminary
study, is it prudent to commit large resources to the follow-on
study in the hope that a new biomarker diagnostic will result? (ii)
Prior to that, is it possible that the preliminary study can obtain
an error estimate that would warrant a decision to perform a
follow-on study? A large follow-on study requires substantially
more resources than those required for a preliminary study. If the
preliminary study has a very low probability of producing repro-
ducible results, then there is really no purpose in doing it. We
propose a reproducibility index that simultaneously addresses
both questions posed earlier. Our focal point is not that inde-
pendent validation data should be used—this has been well
argued, for instance, in the context of bioinformatics to avoid
overoptimism (Jelizarow et al., 2010); rather, the issue addressed
by the reproducibility index is the efﬁcacy of small-sample pre-
liminary studies to determine whether a large validating study
should be performed. We set up a simulation study on synthetic
models that emulate real-world scenarios and on some real data-
sets. We calculate the reproducibility index for different distribu-
tional models (and real datasets) and classiﬁcation schemes.

We consider two other scenarios: (i) multiple independent pre-
liminary studies with small samples are carried out and only the
best results (minimum errors) reported and (ii) multiple classiﬁ-
cation schemes are applied to the preliminary study with small
samples and only the results (minimum errors) of the best class-
ﬁer are reported. A decision is made for a large follow-on study
because the reported errors show very good performance.
Youseﬁ et al. (2010, 2011) show that there is a poor statistical
relationship between the reported results and true classiﬁer per-
formance in these scenarios, namely, there is a potential for sig-
niﬁcant optimistic ‘reporting bias’ or ‘multiple-rule bias’. These
two biases can substantially impact the reproducibility index.

2 SYSTEMS AND METHODS

We deﬁne a classiﬁer rule model as a pair (\11, E), where \11 is a
classiﬁcation rule, possibly including feature selection, and E is a
training-data error estimation rule on a feature-label distribution
F. Given a random sample 8,, of size n drawn from F, the de-
signed classiﬁer is w, = \Il(8,,). The true error of 10,, is given by
8,, = P(ip,,(X) 75 Y), where (X, Y) is a feature-label pair. The

error estimation rule E provides an error estimate, 5,, = E(8,,),
for 0,. To characterize reproducibility, we postulate a prelimin-
ary study in which a classiﬁer, 0,, is designed from a sample of
size n and its error is estimated. We say that the original study is
reproducible with accuracy ,0 3 0 if 8,, 5 5,, + ,0. One could re-
quire that the true error lies in an interval about the estimated
error, but our interest is in whether the proposed classiﬁer is as
good as claimed in the original study, which means that we only
care whether its true performance is below some tolerable bound
of the small-sample estimated error.

Given a preliminary study, not any error estimate will lead to a
follow-on study: the estimate has to be sufﬁciently small to mo-
tivate the follow-on. This means there is a threshold, 1', such that
the second study occurs if and only if 5,, 5 ‘L'. We deﬁne the
reproducibility index by

R,,(,0, t) = P(8,, < 5,, + ,0|§,, 5 1').

R,,(,0, 1') depends on the classiﬁcation rule, \11, the error estima-
tion rule, E, and the feature-label distribution F. Clearly,
:01 f :02 implies Rn(:019 T) f Rn(:029 T)- If “En _  f :0 almOSt
surely, meaning P(||e,, — 5,,“ 5 ,0) = 1, then R,,(,0, T) = 1 for all
T. This means that, if the true and estimated errors are sufﬁ-
ciently close, then, no matter the decision threshold, the repro-
ducibility index is 1. In practice, this ideal situation does not
occur. Often, the true error greatly exceeds the estimated
error when the latter is small, thereby driving down R,,(,0, T)
for small 1'.

We are interested in the relationship between reproducibility
and classiﬁcation difﬁculty. Fixing \II and E makes R,,(,0, I) de-
pendent on F. If we parameterize F, and call it F (0), then we can
write the reproducibility index as R,,(,0, ‘L'; 0). If we select 0 so
there is a 1-1 monotonic relation between 0 and the Bayes
error, shay, then there is a direct relationship between reprodu-
cibility and intrinsic classiﬁcation difﬁculty, R,,(,0, ‘L'; shay).

If we know the joint distribution between the true and esti-
mated errors, then the entire analysis can be done analytically,
for instance, in the case of the 1D Gaussian model with linear
discriminant analysis (LDA) classiﬁcation and leave-one—out
(LOO) error estimation (Zollanvari et al., 2010). Fixing the vari-
ances and letting the means be zero and 0 gives the desired scen-
ario. We can now analytically derive the reproducibility index
R,,(,0, ‘L'; 0). Unfortunately, there are very few distributions for
which the joint distribution between the true and estimated
errors is known. If not, then we use simulation to compute

Rn(p, r; 9)-

2.1 Analysis

To analyze the reproducibility index as a function of ‘L' and tie
this to the joint distribution of the true and estimated errors, we
expand R,,(,0, T) to obtain

1

P(§n <8n_pfr) '

Rn(:09 1) 5

Consider the special case in which ,0 = 0, and let r(t) denote
the probability fraction in the denominator. Then we have the
upper bound R,,(0, T) 5  To gain insight into this bound,
we postulate a geometrically simple model whose assumptions

are not unrealistic. First, we assume that the linear regression of

 

2825

112 ﬁhO'smumoprOJXO'soi1chOJuioiw/2d11q IIIOJJ papeolumoq

910K ‘09 lsnﬁnV no :2

M.R.Yousefi and E.R.Dougherty

 

 

 

 

P(én <8" <1") 4‘”
‘be
# ___ ___
T |
<-
| Hg<g<ﬂ
|
|
' :
7 .11 5

n

Fig. 1. A single-level cut of the joint distribution and corresponding
probabilities

8,, on 5,, is a horizontal line, which has been observed approxi-
mately in many situations (Hanczar et al., 2007, 2010). This
means that 8,, and 5,, are uncorrelated, which have also been
observed for many cases. Then, let us approximate the resulting
joint distribution by a Gaussian distribution with common mean
,u (unbiased estimation) and ,0 = 0 (according to our assump-
tions). Finally, let us assume that the standard deviation of 8,,
is less than the standard deviation of 5,, as is typically the case
for cross-validation. Letting f(e,,, 5,) denote the joint Gaussian
density,

 

r05) =  (inf(8m gn)d§nd8n '
f0r 57(8):, 5n)d8nd§n

Figure 1 shows a pictorial of a single-level cut of the joint
distribution, along with the horizontal regression line and the
8,, = 53,, axis. Relative to the level cut, the dark gray and light
gray regions correspond to the regions of integration for the
numerator and denominator, respectively, of r(t). It is clear
that for small I, r(t) becomes large, thereby making R,,(0, 1')
small.

2.2 Synthetic model

A model is adopted for generating synthetic data, which is built
upon parameterized multivariate distributions, each representing
a class of observations (phenotype, prognosis condition, etc.).
The model is designed to reﬂect a scenario in which there are
subnetworks (pathways) for which genes within a given subnet-
work are correlated but there is no (negligible) correlation be-
tween genes in different subnetworks. The situation is modeled
by assuming a block covariance matrix (Hanczar et al., 2010;
Hua et al., 2005; Youseﬁ et al., 2010, 2011).

Sample points are generated from two equally likely classes,
Y=0 and Y=1 with d features. Therefore, each sample point
is speciﬁed by a feature vector X 6 Rd and a label Ye {0,1}.
The class conditional densities are multivariate Gaussian
with f(x| Y: y) ~ Nd(,uy,022y), for 31:0, 1, where
[L0 = [0,0,0, ...,0]T and u1=[0,0,0, ...,9]T are dx 1
column vectors (for d = 1, we have ,uo = 0 and ,ul 2 0), and

Table 1. Four microarray real datasets used in this study

 

Dataset Dataset type Feature—sample size

 

Yeoh et al. (2002) Pediatric ALL
Zhan et a]. (2006) Multiple myeloma 54613—156/78
Chen et al. (2004) HCC 10237—75/82
Natsoulis et al. (2005) Drugs response on rats 8491—120/61

5077—149/99

 

2,, is a d x d block matrix with off-diagonal block matrices
equal to 0 and l x l on-diagonal block matrices Epy being 1 on
the diagonal and ,0, off the diagonal.

Three classes of Bayes optimal classiﬁers can be deﬁned de-
pending on ,00 and ,01. If the features are uncorrelated, i.e.
,00 = ,01 = 0, the Bayes classiﬁer takes its simplest form: a
future point is assigned to the class to which it has the closest
Euclidian distance. When ,00 = ,01 75 0, the Bayes classiﬁer is a
hyperplane in Rd, which must pass through the midpoint be-
tween the means of two classes. If ,00 7E ,01, the Bayes classiﬁer
takes a quadratic form, and decision surfaces are hyperquadrics.

2.3 Real data

We consider four microarray real datasets, each having more
than 150 arrays: pediatric acute lymphoblastic leukemia (ALL)
Weoh et al., 2002), multiple myeloma (Zhan et al., 2006), hepa-
tocellular carcinoma (HCC) (Chen et al., 2004), and a dataset for
drugs and toxicant response on rats (Natsoulis et al., 2005). We
follow the data preparation instructions reported in the cited
articles. Table 1 provides a summary of the four real datasets.
A detailed description can be found in the Supplementary
Materials.

2.4 Classiﬁer rule models

Three classiﬁcation rules, two linear and one non-linear, are con-
sidered: LDA, linear support vector machine (L-SVM) and
radial basis function SVM (RBF-SVM). Three error estimation
methods are considered: 0.632 bootstrap, LOO and 5-fold
cross-validation (SF-CW. In total, we have nine classiﬁer rule
models.

LDA is a plug-in rule for the optimal classiﬁer in a Gaussian
model with common covariance matrix. The sample means and
pooled sample covariance matrix obtained from the data are
plugged into the discriminant. Assuming equally likely classes,
LDA assigns a sample point x to class 1 if and only if
(x — 12072-10 — 121) s (x — ﬂora-xx — no), where 12, is
the sample mean for class y 6 {0,1}, and E is the pooled
sample covariance matrix. LDA usually provides good results
even when the assumptions of Gaussianity with common covari-
ances are mildly violated.

Given a set of training sample points, the goal of support
vector machine classiﬁer is to ﬁnd a maximal margin hyperplane.
When the data are not linearly separable, one can introduce
some slack variables in the optimization procedure allowing for
mislabeled sample points and solve the dual problem. This clas-
siﬁer is called L-SVM, which is essentially a hyperplane in the
feature space. Alternatively, using a transformation the data can

 

2826

112 ﬁhO'slcumoprOJXO'soi112u1101uioiq//2d11q 111011 pepcolumoq

910K ‘09 lsnﬁnV no 2:

Performance reproducibility index

 

be projected into a higher-dimensional space, where it becomes
linearly separable. One can avoid using the transformation and
work with a kernel function that is expressible as an inner prod-
uct in a feature space. The equivalent classiﬁer back in the ori-
ginal feature space will generally be non-linear (Boser et al., 1992;
Cortes and Vapnik, 1995). When the kernel function is a
Gaussian radial basis function, the corresponding classiﬁer is
referred to as RBF-SVM.

In general, the 0.632 bootstrap error estimator can be
written as,

gboot = O368§resub + O-632t‘9\zer09

where éresub and ézero are the resubstitution and bootstrap zero
estimators. The resubstitution uses the empirical distribution by
putting mass l/n on each of the 11 sample points in the original
data. A bootstrap sample is made by drawing 11 equally likely
points with replacement from the original data. A classiﬁer is
designed on the bootstrap sample, and its error is calculated by
counting the misclassiﬁed original sample points not in the boot-
strap sample. The basic bootstrap zero estimator is the expect-
ation of this error with respect to the bootstrap sampling
distribution. This expectation is usually approximated by a
Monte-Carlo estimate based on a number of independent boot-
strap samples (between 25 and 200 is typical, we use 100).

In 5F-CV, the sample 8,, is randomly partitioned into ﬁve
folds 82‘), for i = 1, 2,. . ., 5. Each fold is held out of the classiﬁer
design process in turn as the test set, a (surrogate) classiﬁer 10$? is
designed on the remaining sets 8,, \ 8S), and the error of 10$? is
estimated by counting the misclassiﬁed sample points in 82‘). The
5F-CV estimate is the average error counted on all folds. Beside
the variance arising from the sampling process, there is ‘internal
variance’ resulting from the random selection of the partitions.
To reduce this variance, we consider 5F-CV with 10 repetitions,
meaning that we also average the cross-validation estimates of
10 randomly generated partitions over 8,,. L00 error estimation
is a special case of cross-validation with n folds, where each fold
consists of a single point. Therefore, LOO has no internal vari-
ance since there is only a single way to partition the data into n
folds. With small samples, cross-validation tends to be inaccurate
owing to high overall variance (Braga-Neto and Dougherty,
2004) and poor correlation with the true error (Hanczar et al.,
2007).

2.5 Simulation design

For the synthetic data, we assume that the features have multi-
variate Gaussian distributions as described in Section 2.2. We
choose de {1,2, 5, 10, 15}, l=d if d<5 and [:5 ifdz 5. We
also assume that a = 0.6 and the pair {,00, ,01} takes three differ-
ent values: {0, 0}, {0.8, 0.8} and {0.4, 0.8}. For ﬁxed a,{,00,,01}
and d, we choose 0 so that the Bayes error equals some desired
values; speciﬁcally, from 0.025 to 0.4 (or the maximum possible
value depending on {,00, ,01}) with increments of 0.025. This will
deﬁne a large class of different distribution models in our simu-
lation. From each distribution model, we also generate random
samples of size 30, 60 and 120 (half from each class) to emulate
real-world problems, where only a small number of sample
points are available. Due to the large number of simulations in
this study, we have limited the dimension of the cases studied;

however, the reproducibility index is not limited by dimension
and, owing to the increased estimation variance, one can expect
that, with larger dimensions, reproducibility can be expected to
be even more problematic in such circumstances.

For each model, we generate 10000 random samples. For
each sample, the true and estimated error pairs of all classiﬁer
rule models are calculated. The true errors of the designed
classiﬁers are found exactly if analytical expressions are avail-
able. Otherwise, they are calculated via a very large inde-
pendent sample (10000 points) generated from the same
distribution model. For each ,0 e {0.0005,0.01,0.05,0.1},
‘L' e {0, 1 /60, 2/ 60, ...,0.5} and classiﬁer rule model, we empir-
ically calculate R,, (,0, ‘L'; shay) from 10 000 true and estimated error
pairs.

The real-data simulation is essentially the same as for the syn-
thetic data, except that each real dataset now serves as a
high-dimensional distribution model. Thus, there is a need for
feature selection, which is part of the classiﬁcation rule. Another
difference is in calculating the true error: at each iteration, n = 60
sample points are randomly picked for training, and a
feature-selection step is carried out where d = 5 features with
highest t—scores are selected. Then a classiﬁer is designed and
its error estimated. The remaining held-out sample points are
used to calculate the true error.

3 RESULTS AND DISCUSSION

The complete set of simulation results can be found in the com-
panion website of this article, including graphs for the joint dis-
tributions and reproducibility indices for different distribution
models, real datasets and classiﬁer rule models. Here, we provide
some results that represent the general trends observed in the
simulations.

3.1 Joint distribution

The joint distributions between 5,, and 8,, are estimated with a
density estimation method that uses bivariate Gaussian kernels.
Here we present the results for only two synthetic distribution
models with d = 5 features and different sample sizes. For the
ﬁrst model, the class-conditional covariance matrices are equal
and the features are uncorrelated. The target Bayes error is set to
0.2, being equivalent to 0 = 1.0. The results are shown for LDA
and 5F-CV in Figure 2(a—c). For the second model, the
class-conditional covariance matrices are assumed to be unequal
and the features are correlated ({,00, ,01} = {0.4, 0.8}). The target
Bayes error is 0.1, which results in 0 = 0.82. Figure 2(d—f) shows
the corresponding graphs when RBF-SVM and LOO are used.
Each plot also includes the regression line (dotted) and a small
circle, indicating the sample mean of the joint distribution. Lack
of regression and correlation, slightly high-bias and very
high-variance for the estimated error are evident for small
sample sizes. These graphs, which are consistent with ones in
previous studies (Dougherty et al., 2010; Hanczar et al., 2007),
show a resemblance to Figure 1, indicating that our analysis in
Section 2.1 is suitable for the synthetic model.

The expected true errors for different classiﬁcation rules
applied to different real datasets are listed in Table 2. Similar
to the synthetic data, the joint distributions for the real data are

 

2827

112 ﬁhO'slcumoprOJXO'soi112u1101uioiq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV no 2:

M.R.Yousefi and E.R.Dougherty

 

(a) 0.5

  

Fig. 2. Joint distribution of the true and estimated errors for n : 30, 60,
120, d: 5, two classiﬁcation rules (LDA and RBF-SVM) and two error
estimation methods (SF-CV and LOO). The covariance matrices are
equal with features uncorrelated for LDA and unequal with features
correlated for RBF-SVM: (a) n : 30, LDA, 5F-CV, shay : 0.2;
0)) n : 60, LDA, 5F-CV, shay : 0.2; (c) n : 120, LDA, 5F-CV,
shay : 0.2; (d) n: 30, RBF-SVM, LOO, shay : 0.1; (e) n: 60,
RBF-SVM, LOO, abay : 0.1; (f) n: 120, RBF-SVM, LOO, shay : 0.1.
The white line shows the 8,, : 5,, axis, the dotted line shows the regression
line and the circle indicates the sample mean of the joint distribution

Table 2. Expected true errors of three classiﬁcation rules used on the real
datasets

 

 

Dataset LDA L-SVM RBF-SVM
Yeoh et al. (2002) 0.080 0.083 0.080
Zhan et al. (2006) 0.186 0.193 0.188
Chen et al. (2004) 0.154 0.151 0.140
Natsoulis et al. (2005) 0.247 0.258 0.301

 

estimated using a bivariate Gaussian-kernel density estimation
method. Here we only present the joint distribution results for
the multiple myeloma dataset (Zhan et al., 2006) with LDA as
the classiﬁcation rule in Figure 3(a—c), and for the HCC dataset
(Chen et al., 2004), when RBF-SVM is used, in Figure 3(d—f). In
all cases, correlation between the true and estimated errors is
small in absolute value and negative, the regression line having
negative slope, which means that the conditional expectation of
the true error decreases as the estimate increases. This behavior is
not anomalous (Dougherty et al., 2010); indeed, a negative cor-
relation has been shown analytically for LOO with discrete clas-
siﬁcation (Braga-Neto and Dougherty, 2010).

3.2 Reproducibility index

Figure 4 shows the reproducibility index as a function of (t, shay)
for different sample sizes and ,0. We assume d: 5 uncorrelated
features with equal class-conditional covariance matrices. The
classiﬁcation rule is LDA and error estimation is 5F-CV. Note
that LDA is a consistent classiﬁcation rule for this distribution
model. As the Bayes error increases, a higher reproducibility

 

Fig. 3. Joint distribution of the true and estimated errors for two real
datasets and different classiﬁer rule models. The training size is 60, and
d: 5 features are selected using t-test feature selection method: (a) mul-
tiple myeloma, LDA and 5F-CV; 0)) multiple myeloma, LDA and LOO;
(c) multiple myeloma, LDA and 0.632 bootstrap; (d) HCC, RBF-SVM
and 5F-CV; (e) HCC, RBF-SVM and L00; (1) HCC, RBF-SVM and
0.632 bootstrap. The white line shows the 8,, : 8,, axis, the dotted line
shows the regression line and the circle indicates the sample mean of the
joint distribution

 
   
  

 
       
    
 
  
   
 
  

           
   

         

      
 

 
  
   

   
  
     

 
  

      

    
    
   

 

. ‘ 53:: 2:: . «\‘\ 999998929389“!
R”:  8.2.: 1'1‘1111‘118‘88988
 v :vws‘swv  111W " "'w “

     

“
‘tozss‘ozto‘ 0-4

‘6
ss‘
s

‘
s

  
  

(c) (d)

 
      

‘
'

g“

‘s \\

       
   
 
 

   
 
  

  
 

  

‘
‘6‘
\s‘

\

 
  

    
     

  
       
     

          
          
     

 

 .s.-;;§::i"  o.  as,
. v~33 3: - “\‘V‘V’W’Vﬁﬁ‘ “
Rngikv‘vVoWotwvvsstzvs R..3.  ‘ “‘“‘““‘v“\'§ ‘
.  A 'v"\““\"‘\“ - ‘  t A. v  ‘
o (20  5 O 2 1   o 5
0.1 ‘ ~=~<z-zz~sz~§zz~32~§-~ . - - “ “5:555:11: -
EbaY 03 040 0.27? 04 81):: 03 040

Fig. 4. Reproducibility index for n : 60, 120, LDA classiﬁcation rule
and 5F-CV error estimation. d: 5 and the covariance matrices are
equal with features uncorrelated. (a) 12:60, 0 : 0.0005; (b) 12:60,
p : 0.05; (c) n : 120, p : 0.0005; ((1) n : 120, p : 0.05

index is achieved only for larger 1'; however, for ,0 : 0.0005,
which is close to zero, the upper bound for the reproducibility
index is about 0.5, which is consistent with our analysis in
Section 2.1. It is also notable that the rate of change in the re-
producibility index gets slower for higher Bayes error and smaller
sample size. Even though the rate of change in the reproducibility
index is faster for sample size 120, the maximum is almost iden-
tical to what we have for sample size 60. This phenomenon can
be attributed to the difﬁculty of classiﬁcation (for higher Bayes
error), high-variance of the error estimate, ﬁat regression and
lack of correlation between the true and estimated errors due
to the small-sample nature of the problem.

An irony appears in Figure 4 when one tries to be prudent by
only doing a large-scale experiment if the estimated error is small

 

2828

112 ﬁlos112umo[pJOJXO'soi112u1101uioiq/ﬁd11q 111011 popcolumoq

910K ‘09 lsnﬁnV uo 2:

Performance reproducibility index

 

 
   
      

  
    
  
    
 
 

      
  
 
  
 

   

1 1

= \
\\.«\ \‘ \\ \ 7‘
[11.880 W “ A ‘
05

V‘\
\x \ x \1\\\\ \
0'05 w o_4 0'5 0. \‘\\\\<
' ‘ 0.3 -
0.15
0.2 0.15
shay 0-2 0.1 T shay 0.2

0.25 O 0.25 0

.
 0.3

 

 

    
      

      
 
 

     
 

M

W11“:

’

  

/
9

Fig. 5. Reproducibility index for p : 0.01, 12 : 60, 120, two classiﬁcation
rules (LDA and RBF-SVM) and 5F-CV error estimation. d: 5 and the
covariance matrices are unequal with features correlated (Po : 0.4 and
p1 : 0.8). (a) 12:60, LDA; a1) 12: 120, LDA; (c) 12:60, RBF-SVM;
(d) 12 : 120, RBF-SVM

in the preliminary study, that is, only proceeding to a follow-on
study if 13,, 5 ‘L' and ‘L' is small. From the ridges in the ﬁgure, we
see that the reproducibility index drops off once 1' is chosen
below a certain point. While small 1' decreases the likelihood of
a follow-on study, it increases the likelihood that the preliminary
results are not reproducible. Seeming prudence is undermined by
poor error estimation. For larger p, reproducibility improves;
however, for shay : 0.2 and 60 sample points, which is very typ-
ical in real-world classiﬁcation problems, even for very large I,
say 1' : 0.3, we have R60(0.05, 0.3) : 0.832.

Figure 5 shows the results for the case of a distribution model
with correlated features, unequal covariance matrices and
p : 0.01. The classiﬁcation rules are LDA and RBF-SVM.
5F-CV serves as the error estimation rule. LDA is no longer a
consistent rule for this model, and we expect RBF-SVM to pro-
duce classiﬁers that, on average, perform better. If so, we would
then expect the reproducibility index to be better for RBF-SVM
because lower Bayes error usually means more accurate
cross-validation error estimation, at least for the Gaussian
model (Dougherty et al., 2011). The graphs conﬁrm this: as the
reproducibility index for higher Bayes error is uniformly
(slightly) better for RBF-SVM. The improvement is notable
for RBF-SVM, compared with LDA, for larger sample size:
for shay : 0.1725, we have R120(0.01, 0.3) : 0.575 for RBF-
SVM while R120(0.01, 0.3) : 0.244 for LDA.

Figure 6 presents the reproducibility index results for the real
data when LDA and RBF-SVM are used, and their errors are
estimated with 5F-CV and LOO. The trends are very similar to
those in the synthetic data. The reproducibility index for 5F-CV
is highly variable among datasets, speciﬁcally the datasets with
higher expected true error have lower reproducibility index for
small- to mid-range values of (p, t). The situation is worse for
LOO due to its high variance.

3.3 Reporting bias effect

Suppose that a study has tested a proposed classiﬁcation rule on
several datasets and reported only the best results, i.e. the ones
on the datasets with the lowest estimated errors. Youseﬁ et al.
(2010) have shown that, for a very large class of problems, this

 

 

(a) I — Chen et al. (2004) uuuuuuu n Yeoh et al. (2002) - - - Natsoulis et al. (2005)  Zhan et al. (2006)|

 

4"

 

1 0.1 0.2 0.3 0.4 0.5
T

 

 

 

T

 

 

(b) I — Chen et al. (2004) mmeeoh et al. (2002) - -- Natsoulis et al. (2005) m-IZhan et al. (2006)|
1 

 

 

  

= 0.0005

 

 

 

i , [7:01
J '0.1 0.2 0.3 0.4 0.5
T

 

2 2'
(c) l—Chen etal. (2004) iiiiiii "Yeoh etal. (2002) ---Natsoulis etal. (2005)  etal. (2006)|
1

 

 

1

     
 

    

‘_._-

 

T T T
I — Chen et al. (2004) uuuuuuu n Yeoh et al. (2002) - - - Natsoulis et al. (2005)  Zhan et al. (2006)
1

 

(d)

 

1

___-—

 

T T T

Fig. 6. Reproducibility index for the real datasets, two classiﬁcation rules
(LDA and RBF-SVM) and two error estimation methods (5F-CV and
LOO). The training sample size is 60, and d : 5 features are selected using
t-test feature selection method: (a) LDA, 5F-CV; a1) RBF-SVM, 5F-CV;
(c) LDA, L00; ((1) RBF-SVM, LOO

practice introduces a major source of (reporting) bias to the
results.

Let {8,1,,8,2,, . . . ,82‘} be a family of m i.i.d. samples of size 12,
randomly drawn from a single distribution. Given a ﬁxed clas-
siﬁer rule model, for each S2, a classiﬁer is designed, its error
estimated and the true error of the designed classiﬁer is also
calculated. Assume that instead of reporting all the estimated
errors, only the minimum estimated error is reported:
52m“ : min{§,1,,§,2,, ...,énm}. Letting 82min denote the sample on
which the minimum error estimate occurs, the corresponding
true error is then 82m“. In this case, the reported estimated error
is €311“ for the dataset 82min. Hence, the reproducibility index is
computed for the pair 82m and 8?“, and the reported study is
reproducible with accuracy p Z 0 from the m performed studies if
8211“ 5 13?“ + p. The reproducibility index for 112 independent
datasets takes the form

Rn’"(p, T) = P09;min 5 53”“ + pléinin 5 1:).

Rﬂp, t) depends on the number of datasets, the classiﬁcation
rule, the error estimation rule and the feature-label distribution,

 

2829

112 /810's112umoprOJXO'sot112u1101utotq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV uo 2:

M.R.Yousefi and E.R.Dougherty

 

these being m, \11, E, and F, respectively. Quantities such as
RZ‘(p, ‘L'; 0) and Rn’"(p, ‘L'; shay) are deﬁned before.

To illustrate the effect of reporting bias, for a ﬁxed
m e {1, 2, . . . , 5}, we randomly draw 112 pairs from the previously
generated 10 000 error pairs. The minimum estimated error and
its corresponding true error are found and recorded. This process
is repeated to generate 10000 new error pairs. Now similar to
R,,(,0,‘L'; shay), we calculate Rn’"(p,t; shay) for each m, ,0,‘L' and
classiﬁer rule model. Figures 7 and 8 show the effect of reporting
bias on the reproducibility index for m:2, 5, d: 5, LDA and
5F-CV when the covariance matrices are equal and the features
are uncorrelated. Compare Figure 8 with Figure 4. Strikingly,
but not surprisingly, we do not need more than 112 : 5 samples to
observe a rapid drop (almost half) of reproducibility for p : 0.05
as the Bayes error and 1' increase. Moreover, for p : 0.0005, the
reproducibility index is almost zero independent of the sample
size. As m increases, the reporting bias, E5n[ef;nin — 52mm], also
increases. Pictorially, the wide ﬁat distribution in Figure 1 be-
comes more circular with smaller variance and gets shifted to the
left side of the 8,, : 5,, axis. Thus, the probability that aim is
smaller than 1331i“ + p diminishes to 0 even though 13?“ 5 ‘L' for
all 1:.

3.4 Multiple-rule bias effect

Suppose r classiﬁcation rules are considered in the preliminary
study and only the results of the best one are considered. In this
case, a random small sample is drawn from the feature-label
distribution F, and r classiﬁers are designed. Assuming F is un-
known, the errors of the designed classiﬁers are estimated from
sample data using 8 different error estimation methods, and the
classiﬁcation rule leading to the classiﬁer with minimum esti-
mated error is chosen as ‘best’. This practice has been shown
to introduce substantial optimistic bias Wouseﬁ et al., 2011).

Denote r classiﬁcation rules by 111,112, ...,\II", and 8 error
estimation rules by El, E2, ...,ES. In total, there are m:rs
classiﬁer rule models: (\111, E1), (\111, E2 , ...,(\111, ES),
(\112, E1), (\112, E2), . . . , (\II", ES). Given a random sample 8,,
drawn from F, the classiﬁcation rules yield r designed classiﬁers:
1V : \Ili(8,,) for i: 1, 2,. . ., r. The true error of W is denoted by
8;. Let  denote the jth estimated error for W, where j: 1,
2,. . ., s. The minimum estimated error is

1,s
n

Amin - A1,1A1,2 A A2,1 "r,s
an :m1n{en ,8” ,...,e ,8” ,...,en }.

Letting imin and jmin denote the classiﬁer number and error esti-
mator number, respectively, for which the error estimate is min-
imum, we have 1331i“ : ézmn’jmin. The corresponding true error is
then aim.

The reproducibility index is now computed for the pair aim
and 52m“ and the reported study is reproducible with accuracy
p Z 0 from the m performed studies if 82m“ 5 13?“ + p. The re-

producibility index for m classiﬁer rule models is deﬁned by
117(2), 1:) = P(8;mi“ 5 33“ + piégﬁn 5 1:).

Quantities such as Rﬂp, ‘L'; 0) and Rﬂp, ‘L'; shay) are deﬁned as
before.

We use the original true and estimated error pairs described in
Section 2.5 and consider three classiﬁcation rules (LDA, L-SVM
and RBF-SVM) and three error estimation methods (0.632

   
       
    
 

I
’1

.
\
\\\“\\
\\\‘\\\\‘\\
\\“‘\\\\‘\\\‘\s‘
\\\“ \\\ \\\‘ \\‘
WW

\\\ \s‘

\\\\‘\\ WW \\\1‘
\ \s s
\‘ W‘szz‘ﬁzxx‘):

: ‘1‘“ “ ‘ *9 ‘C‘
11111.11 ‘1

  
 

   

 v,
1.111 90 133‘

        
  
 

     

     

“ s“‘ ‘\“s““. 1,
§ “ ‘ *

o-2  ~~~~  99~ 8 . , 9
o 3

  
  

  

“\“9 - \
s x}: s2:

,,

s
¢¢§9 “&
‘ s

     

         
 
   

é °‘§°‘
oxymo‘

‘zs‘ ¢:‘° 0.4 I 02
8 0.2 V» o. 8 .
bay 0.3 m 0-2 2' bay

Q4 0 0.4 0

     
 

   

   
  
      
 

“
s‘teﬁ
0’ ‘ ‘

   
         
   

    

    

            
    

  

   

:9s‘$°: ‘\“
Rng' ”  V  R": ‘ 9W1“
' A V ‘ “l ‘ ‘99¢\‘9\‘s‘ - “
- ,5 , h” 05
¢Q®§§§z§‘¢ 04 I sztsz‘é 0.4 .
03

.2

Fig. 7. Reporting bias effect on the reproducibility index for m :2,
12: 60, 120, LDA classiﬁcation rule and 5F-CV error estimation. d: 5
and the covariance matrices are equal with features uncorrelated:
(a) 12: 60, p : 0.0005; 0)) 12: 60, p : 0.05; (c) 12: 120, p : 0.0005;
((1) 12: 120, p : 0.05

\\\\\“‘§\
\\\\\\\\\\\\\\\ \\
3 1 W?“ s‘

s ‘ \ ‘
s ‘\ s ‘ ‘
ss 4“ s s s s
- "‘ “‘ ‘ ‘

   
  
  

      
   
    

5
\‘°‘{\‘
‘°\““{‘\\\\“\‘\\
ss“\\\\ \“‘\\\\“

 
  
   

    

       
       
 
  

 
 
 
 

   
 

     

0.2

     

Mew
.‘gg
I ‘1
s ‘zor‘ss“£“‘$°
3
Fig. 8. Reporting bias effect on the reproducibility index for m : 5,
12: 60, 120, LDA classiﬁcation rule and 5F-CV error estimation. d: 5

WW ,
and the covariance matrices are equal with features uncorrelated:

v 1‘
1.111111111111111
V
(a) 12 : 60, p : 0.0005; (b) 12 : 60, p : 0.05; (c) 12 : 120, p : 0.0005;
((1) 12 : 120, p : 0.05

bootstrap, LOO and 5F-CV). Therefore, we can have r: 1, 2,
3. We generate all  possible collections of classiﬁcation rules of
size r, each associated with three error estimation rules, resulting
in  collections of classiﬁer rule models of size 112 : 3r. For each
collection of size 112, we ﬁnd the true and estimated error pairs
from the original error pairs and record the minimum estimated
error and its corresponding true error. We repeat this process
10 000 times. Now, similar to R,,(p, ‘L'; shay), we calculate
Rﬂp, ‘L'; shay) for each m, p and T. Figure 9 shows the effect of
multiple-rule bias on the reproducibility index for m: 3, d: 5,
LDA and 5F-CV when the covariance matrices are equal and the
features are uncorrelated. The cases for m : 6, 9 are given on the
companion website. Similar observations to those of reporting
bias can be made here. The reproducibility index decreases for
increasing m.

3.5 Application methodology

Application of the reproducibility index in practice requires that
the deﬁning probability be computed, or at least approximated,

 

2830

112 /810's112umoprOJXO'sot112u1101utotq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV uo 2:

Performance reproducibility index

 

        

 

         
         

      
    
 
 

   

                         
 

        
 
  

        
    
    
     
  

  
   

  

  

  

        

         
 

      

     
   
  

   
  
 

   
  
   

     
   
   

   
 

   

    

. \\ \\ \ s
0.8    °-3 ““YWWQX‘sga
m_~ ‘ ‘§V®$ﬁﬂ~ﬁﬁ8§$§ 02 Q‘QNHVWVV‘ ‘l
J <32 ‘$§§’04 ' ' meﬁ§ﬁgggggé$
M °03 M “ ‘9
0.4 O T
c) 0) ’°
1 1 s
“’1;  °-8 ‘W  , .
R2" jji. Rig; ‘ 0 a“
ngggégéggg§g&&&&§ 05 % 1‘ s‘u&k&ﬁs“.>o5
. ~s~¢~s-s~$~§~3§2~“‘ o. ' - “‘ ‘~:‘~:-* o '
M ‘ m ,N*C “w° .
gbay 03 04 o 01 1' 8b; 03 0.4 o 2.01 02 1‘33

 

Fig. 9. Multiple-rule bias effect on the reproducibility index for m : 3,
12 : 60, 120, LDA classiﬁcation rule, and 5F-CV error estimation. d: 5
and the covariance matrices are equal with features uncorrelated:
(a) 12 : 60, p : 0.0005; 0)) 12 : 60, p : 0.05; (c) 12 : 120, p : 0.0005;
((1) 12 : 120, p : 0.05

beforehand. This requires prior knowledge regarding the
feature-label distribution. If the feature-label distribution was
known and the corresponding theory regarding the joint distri-
bution of the true and estimated errors developed, then
R,,(p, ‘L'; 0) could be directly computed for different values of n,
p and ‘L'. For instance, in the case of LDA in the Gaussian model
with known covariance matrix, the joint distribution is known
exactly in the univariate case and can be approximated in the
multivariate case (Zollanvari et al., 2010). Of course, if the
feature-label distribution was known, then there would be no
reason to collect any data; just derive the Bayes classiﬁer from
the model. Thus, when we speak of prior knowledge, we mean
the assumption that the feature-label distribution belongs to an
uncertainty class of feature-label distributions. Considering our
earlier remarks about parameterizing the feature-label distribu-
tion by 0, thereby treating it as F (0), the uncertainty class can be
denoted by (9, with each 0 6 ® determining a possible
feature-label distribution. Furthermore, taking a Bayesian per-
spective, we can put a prior distribution, 7t(0), perhaps
non-informative, on (9.

Assuming an uncertainty class in the case of reproducibility
is pragmatic because reproducibility concerns error-estimation
accuracy and virtually nothing practical can be said concerning
error-estimation accuracy in the absence of prior knowledge
(Dougherty et al., 2011). For instance, the most common meas-
ure of error-estimation accuracy is the RMS between the true
and estimated errors, and, without distributional assumptions,
the RMS cannot be usefully bounded in the case of training-
data-based error estimators unless the sample size is very large,
well beyond practical biological circumstances and beyond what
is needed to split the data into training and testing data. As noted
by Fisher in 1925, ‘Only by systematically tackling small sample
problems on their merits does it seem possible to apply accurate
tests to practical data’ (Fisher, 1925). Large-sample bounds do
not help. Owing to this limitation, optimal error estimation rela-
tive to a prior distribution on an uncertainty class of feature-label
distributions has been developed (Dalton and Dougherty, 2011a)
and applied in gene-expression classiﬁcation (Dalton and
Dougherty, 2011b).

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(a ___,202 ___,=05 (w mmmr=02 ---r=0%
1:01 ,,,,,,,,, “7:015 1:01 --------- “7:015
1 1
§ 0.8 S 0-8
-« u .
E 0.6- ’ __ __ ::_-,_7,.-_.-_.—_.-= =-'=-=- D: 0.6 ’ _ _ ._I_.‘.___-_‘-_,—_:-_.-_.=.=.=._-..
V ~ : ------------------------------------------- " v ,  .................  ------------- 
m= 0.4-x  -------- -- - ET 0-4 ,2  ------------ 
L—é 0.2M mm 
0
S |

10 20 so 40 so 60 7'0 80 90100110120 010 2'0 50 40 50 50 7‘0 9'0 91015011101120

 

 

 

 

Sample size amp 9 Size
(c) .-.--- 7:0,2 - - - 1:025 (d) "-'-'- 2'=0.2 - - - 2:0.25 ‘
7:0.1 I I I I I I I I I II 

T=0_1 1 1 1 I I 1 1 I I II 

 

 

 

 

   

 

 

 

 

 

 

010 20 30 40 50 60 70 _80 90100110120

010 20 30 40 50 60 70 80 90100110120
' Sample Size

Sample Size
Fig. 10. Expected reproducibility index for LDA and RBF-SVM classi-
ﬁcation rules, and 0.632 bootstrap error estimation as a function of 12:
(a) p : 0.01, LDA; (b) p : 0.01, RBF-SVM; (c) p : 0.05, LDA;
(d) p : 0.05, RBF-SVM

Given an uncertainty class and prior distribution, the problem
is to ensure a desired level of reproducibility before experimental
design; that is, determine n, p and 1' so that the desired level is
achieved. A conservative approach would be to ensure
111111969 R,,(p, ‘L'; 0)>r, where r is the desired level of reproduci-
bility. If we assume that p and ‘L' are given, satisfaction of the
inequality would yield a required sample size n. The weakness of
this approach is that the minimization requirement is determined
by worst-case values of 0. A less conservative approach, and
the one we take here, is to require that E9[R,,(p,1:;0)]>r. One
could apply other (more conservative) criteria, such as
E9[R,,(p, ‘L'; 0)] — 2SD9[R,,(p, ‘L'; 0)] >r, where SDg denotes stand-
ard deviation with respect to 0. As noted, for demonstration
purposes, we stay with E9[R,,(p, ‘L'; 0)] >r.

Rarely can this inequality be evaluated analytically. We dem-
onstrate a Monte-Carlo approach to ﬁnd the minimum sample
size yielding a desired reproducibility index for classiﬁcation rule
\11 with error estimation rule E. Assume, from our prior know-
ledge, that the feature-label distribution generating the experi-
mental samples, after processing the data, can be approximated
with the synthetic model introduced in Section 2.2, with d: 2
features, a : 0.6, {p0, p1} : {0.4, 0.8} and 0 being normally dis-
tributed with mean 1.167 and variance 02 / 5d : 0.036 (0 % 1.167
corresponding to shay % 0.1). For given p and ‘L', and for ﬁxed 12,
generate random 01 ~ N(1.167, 0.036), i: 1, ...,1000. For each
0‘, draw random samples 8;, j : 1, . . . , 5000, from the distribu-
tion model F(0‘). For each sample 8;, design a classiﬁer
1b; : 11(82), calculate its true error using an independent large
sample drawn from the same distribution F(0‘) and estimate its
error by E(8{,). Now calculate R,,(p, ‘L'; 0‘) empirically from these
5000 pairs of true and estimated errors and approximate
E9[R,,(p, ‘L'; 0)] by averaging over R,,(p, ‘L'; 0‘). Repeat the proced-
ure for different 12 until E9[R,,(p,1:; 0)] > r for a given r. Figure 10
shows the expected reproducibility index for LDA, RBF-SVM
and 0.632 bootstrap error estimation with respect to different
sample size, p and ‘L'. If r:0.6, p : 0.01, ‘L' : 0.2 and the

 

2831

112 /810's112umofpinXO'sot112u1101utotq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV uo 2:

M.R.Yousefi and E.R.Dougherty

 

classiﬁcation rule is LDA, Figure 10a shows that 12 must exceed
82. As another example, the graph in Figure 10d shows that, for
r:0.8, p : 0.05,1: : 0.15 and RBF-SVM, n>60.

3.6 Concluding remarks

Performance reproducibility is an epistemological issue: What
knowledge is provided by a study? Ultimately, we are led back
to the core epistemological issue in biomarker prediction, accur-
acy of the error estimate. To the extent that the estimated clas-
siﬁer error differs from the true error on the feature-label
distribution, there is lack of knowledge at the conclusion of the
ﬁrst study. If there is virtually no reproducibility, then there is
virtually no knowledge. Thus, there is no justiﬁcation for a large
study based on the preliminary study. Indeed, why proceed with
the preliminary study if there is no reason to believe that its
results will be reproducible? The issue of reproducibility should
be settled before any study, small or large. The proposed repro-
ducibility index provides the needed determination.

Ultimately, the reproducibility index depends on the accuracy
of the error estimator, and if we judge accuracy by the RMS,
then the deviation variance of the estimator plays a crucial rule
since RMS : \/Vardev[§,,] + Bias2[§,,], where the bias and devi-
ation variance are deﬁned by Bias[§,,] : E[§,, — e,,] and
Vardev[§,,] : Var[§,, — 8,], respectively. When the bias is small,
as in the case of LOO,

RMS % x/Vardev[§,,]
= \/Vaf[§n] + V31'[8n] —  V3r[§n]Var[8n]

where p is the correlation coefﬁcient between the true and esti-
mated errors. As we see in Figures 2 and 3, Var[§,,] tends to be
large and p tends to be very small or even negative (Braga-Neto
and Dougherty, 2010; Hanczar et al., 2007). This large variance
and lack of positive correlation results in lack of reproducibility
for small samples.

Let us conclude with some remarks concerning validation,
which, in our particular circumstance, means validation of the
classiﬁer error from the original small-sample study. For com-
plex models, such as stochastic dynamical networks, validation
of the full network is typically beyond hope, and one must be
content with validating some characteristic of the network, such
as its steady-state solution, by comparing it to empirical obser-
vations (Dougherty, 2011). As for how close the theoretical and
the corresponding empirical characteristic must be to warrant
acceptance, closeness must be deﬁned by some quantitative cri-
terion understood by all. The intersubjectivity of validation res-
ides in the fact that some group has agreed on the measure of
closeness (and the requisite experimental protocol), although
they might disagree on the degree of closeness required for ac-
ceptance (Dougherty and Bittner, 2011). In the case of classiﬁ-
cation (as noted in the Introduction), when applying a classiﬁer
on an independent test set, the RMS possesses a distribution-free
bound of (2,/n_2)_1. Agreeing to using the RMS as the closeness
criterion and using this bound, one can determine a test sample
size to achieve a desired degree of accuracy, thereby validating
(or not validating) the performance claims made in the original
experiment.

 

 

The situation is much more subtle when using the RMS on the
training data. In very few cases are any distribution-free bounds
known and, when known, they are useless for small samples. To
obtain useful RMS bounds, one must apply prior distributional
knowledge. There is no option. Given prior (partial) distribu-
tional knowledge, one can determine a sample size to achieve a
desired RMS (Zollanvari et al., 2012). Furthermore, given a prior
distribution on the uncertainty class, one can ﬁnd an exact ex-
pression for the RMS given the sample, meaning that one can
use a censored sampling approach to sample just long enough to
achieve the desired RMS (Dalton and Dougherty, 2012a). Prior
knowledge can also be used to calibrate ad hoc error estimators
such as resubstitution and LOO to gain improved estimation
accuracy (Dalton and Dougherty, 2012b). One might argue
that assuming prior knowledge carries risk because the know-
ledge could be erroneous. But if one does not bring sufﬁcient
knowledge to an experiment to achieve meaningful results, then
he or she is not ready to do the experiment. Pragmatism requires
prior knowledge. The prior knowledge is uncertain, and our for-
mulation of it must include a measure of that uncertainty. The
more uncertain we are, the less impact the knowledge will have
on our conclusions. In the case of the reproducibility index, we
have introduced a few criteria by which one can decide whether,
in the framework of this uncertainty, a desired level is achieved.
A key point regarding uncertainty in the context of reproduci-
bility is that, should the prior distribution on the uncertainty
class be optimistic, it may result in carrying out a second study
without sufﬁcient justiﬁcation but it will not lead to an over-
optimistic conclusion because the conclusion will be based on
the independent larger follow-on study in which the prior know-
ledge is not employed. This is far better than basing the decision
to proceed with a large independent study on a meaningless error
estimate.

ACKNOWLEDGEMENT

The authors thank the High-Performance Biocomputing Center
of TGen for providing the clustered computing resources used in
this study; this includes the Saguaro-2 cluster supercomputer,
partially funded by NIH grant 1S10RR025056-01.

Conflict of Interest: none declared.

REFERENCES

Boser,B.E. et al. (1992) A training algorithm for optimal margin classiﬁers. In
COLT ’92: Proceedings of the Fifth Annual Workshop on Computational
Learning Theory. ACM, New York, pp. 144—152.

Boulesteix,A.—L. (2010) Over-optimism in bioinformatics research. Bioinformatics,
26, 437—439.

Boulesteix,A.-L. and Slawski,M. (2009) Stability and aggregation of ranked gene
lists. Brief. Bioinform, 10, 556—568.

Braga-Neto,U.M. and Dougherty,E.R. (2004) Is cross-validation valid for
small-sample microarray classiﬁcation? Bioinformatics, 20, 374—380.

Braga-Neto,U.M. and Dougherty,E.R. (2010) Exact correlation between actual and
estimated errors in discrete classiﬁcation. Pattern Recognit. Lett., 31, 407—413.

Castaldi,P.J. et al. (2011) An empirical assessment of validation practices for mo-
lecular classiﬁers. Brief. Bioinform, 12, 189—202.

Chen,X. et al. (2004) Novel endothelial cell markers in hepatocellular carcinoma.
Modern Pathol., 17, 1198—1210.

Cortes,C. and Vapnik,V.N. (1995) Support-vector networks. Mach. Learn, 20,
273—297.

 

2832

112 /810's112umofpinXO'sot112u1101utotq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV uo 2:

Performance reproducibility index

 

Dalton,L.A. and Dougherty,E.R. (2011a) Bayesian minimum mean-square error
estimation for classiﬁcation error—Part 1: Deﬁnition and the Bayesian MMSE
error estimator for discrete classiﬁcation. IEEE Trans. Signal Process., 59,
115—129.

Dalton,L.A. and Dougherty,E.R. (2011b) Application of the Bayesian MMSE error
estimator for classiﬁcation error to gene-expression microarray data.
Bioinformatics, 27, 1822—1831.

Dalton,L.A. and Dougherty,E.R. (2012a) Exact MSE performance of the Bayesian
MMSE estimator for classiﬁcation error—Part II: Consistency and performance
analysis. IEEE Trans. Signal Process., 60, 2588—2603.

Dalton,L.A. and Dougherty,E.R. (2012b) Optimal MSE calibration of error esti-
mators under Bayesian models. Pattern Recognit., 45, 2308—2320.

Devroye,L. et al. (1996) A Probabilistic Theory of Pattern Recognition.
Springer-Verlag, New York.

Dougherty,E.R. (2011) Validation of gene regulatory networks: scientiﬁc and infer-
ential. Brief. Bioinform., 12, 245—252.

Dougherty,E.R. (2012) Prudence, risk, and reproducibility in biomarker discovery.
BioEssays, 34, 277—279.

Dougherty,E.R. and Bittner,M.L. (2011) Epistemology 0f the Cell: A Systems
Perspective on Biological Knowledge. John Wiley, New York.

Dougherty,E.R. et al. (2010) Performance of error estimators for classiﬁcation.
Curr. Bioinform., 5, 53—67.

Dougherty,E.R. et al. (2011) The illusion of distribution-free small-sample classiﬁ-
cation in genomics. Curr. Genomics, 12, 333—341.

Fisher,R.A. (1925) Statistical Methods for Research Workers. Oliver and Boyd,
Edinburg.

Hanczar,B. et al. (2007) Decorrelation of the true and estimated classiﬁer errors in
high-dimensional settings. EURASIP J. Bioinform. Syst. Biol., 2007, 12.

Hanczar,B. et al. (2010) Small-sample precision of ROG-related estimates.
Bioinformatics, 26, 822—830.

Hua,J. et al. (2005) Optimal number of features as a function of sample size for
various classiﬁcation rules. Bioinformatics, 21, 1509—1515.

Hothorn,T. and Leisch,F. (2011) Case studies in reproducibility. Brief Bioinform.,
12, 288—300.

Ioannidis,J.P.A. (2005) Why most published research ﬁndings are false. PLoS Med,
2, 0124.

Jelizarow,M. et al. (2010) Over-optimism in bioinformatics: an illustration.
Bioinformatics, 26, 1990—1998.

Li,Q. et al. (2011) Measuring reproducibility of high-throughput experiments. Ann.
Appl. Stat., 5, 1752—1779.

Natsoulis,G. et al. (2005) Classification of a large microarray data set: algorithm
comparison and analysis of drug signatures. Genome Res., 15, 724—736.

Ray,T. (2011) FDA’s Woodcock says personalized drug development entering
‘long slog’ phase. Pharmacogen. Rep., http://www.genomeweb.com/mdx/fdas-
woodcock-says-personalized-drug—development-entering-long—slog—phase
(26 October 2011, date last accessed).

Sabel,M.S. et al. (2011) Proteomics in melanoma biomarker discovery: great poten-
tial, many obstacles. Int. J. Proteom., 2011, 8.

Yeoh,E.J. et al. (2002) Classiﬁcation, subtype discovery, and prediction of outcome
in pediatric acute lymphoblastic leukemia by gene expression proﬁling. Cancer
Cell, 1, 133—143.

Youseﬁ,M.R. et al. (2010) Reporting bias when using real data sets to analyze
classiﬁcation performance. Bioinformatics, 26, 68—76.

Youseﬁ,M.R. et al. (2011) Multiple-rule bias in the comparison of classiﬁcation
rules. Bioinformatics, 27, 1675—1683.

Zhan,F. et al. (2006) The molecular classiﬁcation of multiple myeloma. Blood, 108,
2020—2028.

Zhang,M. et al. (2008) Apparently low reproducibility of true differential expression
discoveries in microarray studies. Bioinformatics, 24, 2057—2063.

Zhang,M. et al. (2009) Evaluating reproducibility of differential expression discov-
eries in microarray studies by considering correlated molecular changes.
Bioinformatics, 25, 1662—1668.

Zollanvari,A. et al. (2010) Joint sampling distribution between actual and estimated
classiﬁcation errors for linear discriminant analysis. IEEE Trans. Inform.
Theory, 56, 784—804.

Zollanvari,A. et al. (2012) Exact representation of the second-order moments for
resubstitution and leave-one-out error estimation for linear discriminant analysis
in the univariate heteroskedastic Gaussian model. Pattern Recognit., 45,
908—917.

 

2833

112 /810's112umofpinXO'sot112u1101utotq//2d11q 111011 popcolumoq

910K ‘09 lsnﬁnV uo 2:

