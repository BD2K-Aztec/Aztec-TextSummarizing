
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Genetics and population analysis The Bayesian lasso for genome-wide association studies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Jiahan</forename>
								<surname>Li</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<addrLine>State College</addrLine>
									<postCode>16802</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Kiranmoy</forename>
								<surname>Das</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<addrLine>State College</addrLine>
									<postCode>16802</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Guifang</forename>
								<surname>Fu</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<addrLine>State College</addrLine>
									<postCode>16802</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Runze</forename>
								<surname>Li</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<addrLine>State College</addrLine>
									<postCode>16802</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Rongling</forename>
								<surname>Wu</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<addrLine>State College</addrLine>
									<postCode>16802</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jeffrey</forename>
								<surname>Barrett</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Statistical Genetics</orgName>
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<postCode>17033</postCode>
									<settlement>Hershey</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Genetics and population analysis The Bayesian lasso for genome-wide association studies</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">4</biblScope>
							<biblScope unit="page" from="516" to="523"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq688</idno>
					<note type="submission">Received on August 16, 2010; revised on December 3, 2010; accepted on December 9, 2010</note>
					<note>[09:55 2/2/2011 Bioinformatics-btq688.tex] Page: 516 516–523 Associate Editor: for the approach developed is available at Penn State Center for Statistical Genetics web site, http://statgen.psu.edu. Contact: rwu@hes.hmc.psu.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Despite their success in identifying genes that affect complex disease or traits, current genome-wide association studies (GWASs) based on a single SNP analysis are too simple to elucidate a comprehensive picture of the genetic architecture of phenotypes. A simultaneous analysis of a large number of SNPs, although statistically challenging, especially with a small number of samples, is crucial for genetic modeling. Method: We propose a two-stage procedure for multi-SNP modeling and analysis in GWASs, by first producing a &apos;preconditioned&apos; response variable using a supervised principle component analysis and then formulating Bayesian lasso to select a subset of significant SNPs. The Bayesian lasso is implemented with a hierarchical model, in which scale mixtures of normal are used as prior distributions for the genetic effects and exponential priors are considered for their variances, and then solved by using the Markov chain Monte Carlo (MCMC) algorithm. Our approach obviates the choice of the lasso parameter by imposing a diffuse hyperprior on it and estimating it along with other parameters and is particularly powerful for selecting the most relevant SNPs for GWASs, where the number of predictors exceeds the number of observations. Results: The new approach was examined through a simulation study. By using the approach to analyze a real dataset from the Framingham Heart Study, we detected several significant genes that are associated with body mass index (BMI). Our findings support the previous results about BMI-related SNPs and, meanwhile, gain new insights into the genetic control of this trait.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent genotyping technologies allow the fast and accurate collection of genotype data throughout the entire genome for many subjects. By genome-wide association studies (GWASs), the genetic variants associated with a complex disease or trait, their chromosomal distribution and individual effects, can be identified. GWASs are based on either case–control cohorts to test the * To whom correspondence should be addressed. associations between SNPs and diseases or population cohorts to estimate genetic effects of SNPs on traits. In both cases, there are hundreds of thousands of SNPs genotyped on samples involving thousands of subjects. This typical problem, having the number of predictors far exceeding the number of observations, makes it impossible to analyze the data using traditional multivariate regression. In current GWASs, simple univariate linear regression that analyzes one SNP at a time is usually used and, by adjusting for multiple comparisons, the significance levels of the detected genes are then calculated (<ref type="bibr" target="#b16">McCarthy et al., 2008</ref>). These single SNP-based GWASs have been instrumental for reproducibly detecting significants genes for various complex diseases or traits (<ref type="bibr" target="#b5">Donnelly, 2008</ref>). However, such strategies have three major disadvantages, limiting the future applications of GWAS. First, because most complex traits are polygenic, a single SNP analysis can only detect a very small portion of genetic variation and, also, may not be powerful for identifying weaker associations (<ref type="bibr" target="#b12">Hoggart et al., 2008</ref>). Second, different genes may interact with each other to form a complex network of genetic interactions, which cannot be characterized from a single SNP analysis. Third, many GWASs analyze genetic associations separately for different environments, such as males and females, and then make an acrossenvironment comparison in genetic effects. This analysis is neither powerful nor precise for the identification of gene–environment interactions. Because of these limitations, many authors have developed various approaches for simultaneously analyzing multiple SNPs for GWASs (<ref type="bibr" target="#b15">Logsdon et al., 2010;</ref><ref type="bibr" target="#b21">Wu et al., 2009;</ref><ref type="bibr" target="#b22">Yang et al., 2010</ref>), although most approaches focus on case–control cohorts. There is a daunting need on the development of a variable selection model to identify SNPs with significant effects on quantitative traits in population cohorts and estimate all selected predictors simultaneously. Traditionally, a subset of predictors in a regression model is obtained by forward selection, backward elimination and stepwise selection, but these approaches are computationally expensive and unstable even when the number of predictors is not large. Recently, alternative approaches have been developed, including ridge regression, bridge regression (<ref type="bibr" target="#b9">Frank and Friedman, 1993</ref>), least absolute shrinkage and selection operator (LASSO) (<ref type="bibr" target="#b19">Tibshirani, 1996</ref>), elastic net (<ref type="bibr" target="#b24">Zou and Hastie, 2005</ref>) and the smoothly clipped absolute deviation (SCAD) penalty (<ref type="bibr" target="#b7">Fan and Li, 2001</ref>). For the number of variables much larger than that of subjects, as commonly seen in GWASs,<ref type="bibr" target="#b8">Fan and Lv (2008)</ref>proposed a two-stage procedure for variable selection by first suppressing the high dimensionality of response into its low-dimensional representation and then finding a subset of predictors that can predict the suppressed response. A similar two-stage approach was also developed by<ref type="bibr" target="#b18">Paul et al. (2008</ref>).Page: 517 516–523</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lasso for GWAS</head><p>In this article, we for the first time integrate Paul et al.'s preconditioning procedure into LASSO to develop a two-stage strategy for identifying important SNPs in GWASs. In step one, we find a linear combination of predictors that are strongly correlated with the response by a supervised principle component analysis and get a consistent 'preconditioned' estimate of response variable. In step two, we implement the Bayesian lasso (<ref type="bibr" target="#b17">Park and Casella, 2008</ref>) for variable selection based on the 'preconditioned' response that mitigates the observational noise. The Markov chain Monte Carlo (MCMC) algorithm is used to estimate all the parameters. The Bayesian hierarchical model is implemented to control an issue of over-fitting that arises when too many associations are included. Our model shows a great flexibility to fit many SNPs and many covariates at the same time. The statistical properties of the model were tested through simulation studies. We used a real GWAS dataset from the Framingham Heart Study (FHS) to validate the usefulness and utilization of the new model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BAYESIAN GWAS MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preconditioning</head><p>When the number of predictors far exceeds the number of observations, preconditioning via a supervised principal component analysis is recommended to reduce the effect of observational noise on model selection (<ref type="bibr" target="#b18">Paul et al., 2008</ref>). In a GWAS of n subjects, we express a response variable y (assumed to be normally distributed) as a function of p SNPs genotyped throughout the entire genome using a linear model y = W b+,</p><formula>(1)</formula><p>where W = (w 1 ,...,w n ) T is a (n×p) design matrix, b = (b 1 ,...,b p ) T is the vector of regression coefficients and ∼ N n (0,σ 2 I n ) is the residual error. The design matrix is reduced to one that consists of only those predictors whose estimated regression coefficients exceed a threshold θ in the absolute value. Thus, the reduced design matrix W reduced consists of the j-th column of W , where j ∈{j :| ˆ b j | &gt;θ}. The principal components of W reduced , called supervised principal components, are computed. The first m supervised principal components can serve as independent variables in a linear regression model, from which a consistent predictor˜ypredictor˜ predictor˜y of the true response is obtained. In practice, we select θ by 5-fold cross-validation. Since only the first few components are useful for prediction, in the following examples we consider the first three principal components. Next, a standard variable selection procedure will be conducted for the preconditioned response variable˜yvariable˜ variable˜y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lasso penalized regression</head><p>Given phenotypical measurements and genotype information, we could obtain the preconditioned response˜yresponse˜ response˜y based on the generic form of linear regression (1). However in GWASs, a number of covariates, which are either discrete or continuous, may be measured for each subject. In order to estimate genetic effects precisely by adjusting for these covariates, a GWAS model that takes into account the effects of important covariates would be more appropriate. Therefore, we describe the preconditioned value˜yvalue˜ value˜y i of a quantitative trait for subject i as˜y as˜ as˜y i = µ+X T i α +Z T i β +ξ T i a +ζ T i d+ i , i = 1,...,n,</p><formula>(2)</formula><p>where µ is the overall mean, X i is the d 1-dimensional vector of discrete covariates for subject i, α = (α 1 ,...,α d 1 ) T is the vector of regression coefficients for discrete covariates, Z i is the d 2-dimensional vector of continuous covariates for subject i, β = (β 1 ,...,β d 2 ) T is the vector of regression coefficients for continuous covariates, a = (a 1 ,...,a p ) T and d = (d 1 ,...,d p ) T are the p-dimensional vectors of the additive and dominant effects of SNPs, respectively, ξ i and ζ i are the indicator vectors of the additive and dominant effects of SNPs for subject i, and i is the residual error assumed to follow a N(0,σ 2 ) distribution. The j-th elements of ξ i and ζ i are defined as</p><formula>ξ ij = ⎧ ⎨ ⎩ 1</formula><p>, if the genotype of SNP j is AA 0, if the genotype of SNP j is Aa −1, if the genotype of SNP j is aa,</p><formula>ζ ij = 1</formula><p>, if the genotype of SNP j is Aa 0, if the genotype of SNP jis AA or aa.</p><p>Despite p n in the GWAS, most of the regression coefficients in</p><p>(2) are expected to have no or only weak effects on the phenotype. To identify a few SNPs that may have notable effects and enhance prediction performance, we put L 1 lasso penalties on the sizes of additive effects and the dominant effects and encourage sparse solutions using</p><formula>p j=1 |a j |≤t, p j=1 |d j |≤t * , fort ≥ 0,t * ≥ 0, (3)</formula><p>where t and t * are a certain value chosen to penalize the additive and dominant effects, respectively. Thus, parameters in Equation (2) are estimated by the penalized least squares 1 2 ||˜y−µ−Xα||˜y−µ−Xα −Zβ −ξa −ζd|| 2 +λ</p><formula>p j=1 |a j |+λ * p j=1 |d j |,</formula><formula>(4)</formula><formula>where˜ywhere˜ where˜y = (˜ y 1 ,..., ˜ y n ) T , µ = (µ,...,µ) T , X = (X 1 ,...,X n ) T , Z = (Z 1 ,...,Z n ) T , ξ = (ξ 1 ,...,ξ n ) T , ζ = (ζ 1 ,...,ζ n )</formula><p>T , and λ and λ * are tuning parameters or lasso parameters that control the degrees of shrinkage in the estimate of the genetic effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bayesian hierarchical model and prior distributions</head><p>Noting the form of the L 1-penalty term in (4),<ref type="bibr" target="#b19">Tibshirani (1996)</ref>suggested that lasso estimates can be interpreted as posterior mode estimates when the regression parameters have independent and identical Laplace (i.e. double-exponential) priors. Therefore, when lasso penalties are imposed on the additive and dominant effects of SNPs, the conditional prior for a j is a Laplace distribution with the scale parameter σ/λ:</p><formula>π(a|σ 2 ) = p j=1 λ 2 √ σ 2 e −λ|a j |/ √ σ 2 ,</formula><formula>(5)</formula><p>Similarly, the conditional Laplace prior for d j is</p><formula>π(d|σ 2 ) = p j=1 λ * 2 √ σ 2 e −λ * |d j |/ √ σ 2 .</formula><formula>(6)</formula><p>Since the Laplace distribution can be represented as a scale mixture of a normal distribution with an exponential distribution Page: 518 516–523</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.Li et al.</head><p>(<ref type="bibr">Andrews and Mallows, 1974</ref>), we have the following hierarchical representation of the penalized regression model:</p><formula>α ∼ N d 1 (0,, α ), β ∼ N d 2 (0,, β ),</formula><formula>a|σ 2 ,τ 2 1 ,...,τ 2 p ∼ N p (0,σ 2 diag(τ 2 1 ,...,τ 2 p )),</formula><formula>τ 2 1 ,...,τ 2 p |λ ∼ p j=1 exp( λ 2 2 ), d|σ 2 ,τ * 2 1 ,...,τ * 2 p ∼ N p (0,σ 2 diag(τ * 2 1 ,...,τ * 2 p )), τ * 2 1 ,...,τ * 2 p |λ * ∼ p j=1 exp( λ * 2 2 ), σ 2 ∼ π(σ 2 ), σ 2 ,τ 2 1 ,...,τ 2 p ,τ * 2 1 ,...,τ * 2 p &gt; 0.</formula><p>After integrating out τ 2 1 ,...,τ 2 p and τ * 2 1 ,...,τ * 2 p , the conditional priors on a and d have the desired forms (5) and (6), respectively. We assign conjugate normal priors to α and β because they are low dimensional and not the parameters of interest. Finally, since the data are usually sufficient to estimate µ and σ, we can use an independent, flat prior π(µ) = 1 for µ and a non-informative scale-invariant prior π(σ 2 ) = 1/σ 2 for σ 2. The tuning parameters of the ordinary lasso can be prespecified by cross-validation, generalized cross-validation or the idea based on Stein's unbiased risk estimate. However, in the Bayesian lasso, λ and λ * can be estimated along with other parameters by assigning appropriate hyperpriors to them. This procedure avoids the choice of lasso parameters and allows us to determine the amount of shrinkage from the data. In particular, we consider the conjugate gamma priors on λ 2 /2 and λ * 2 /2,</p><formula>π λ 2 2 ∼ Gamma(a,b), π λ * 2 2 ∼ Gamma(a * ,b * ).</formula><p>where a, b, a * and b * are small values so that the priors are essentially non-informative. With this specification, lasso parameters can be treated similar to the other parameters and estimated by the Gibbs sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">POSTERIOR COMPUTATION AND INTERPRETATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MCMC algorithm</head><p>We estimate the parameters by sampling from their conditional posterior distributions through the MCMC algorithm. The joint posterior distribution can be expressed as:</p><formula>π(µ,α,β,a,τ 2 1 ,...,τ 2 p ,λ,d,τ * 2 1 ,...,τ * 2 p ,λ * ,σ 2 |˜y|˜y) ∝ n i=1 π(˜ y i |·)π(µ)π(σ 2 )π ( α)π ( β) p j=1 π(a j |τ 2 j )π(τ 2 j |λ)π(λ)π(d j |τ * 2 j )π(τ * 2 j |λ * )π(λ * )</formula><p>Two-level hierarchical modeling allows us to easily derive the conditional posterior distributions of parameters and hyperparameters, from which the Gibbs sampler draws posterior samples. Conditional on the parameters</p><formula>(a,d,τ 2 1 ,...,τ 2 p ,τ * 2 1 ,...,τ * 2 p )</formula><p>, the model is the standard linear regression and, thus, the conditional posterior distributions of (α,β,σ 2 ) are</p><formula>α|· ∼ N d 1 n i=1 X i (˜ y i −µ−Z T i β −ξ T i a −ζ T i d) σ 2 ,, , with = n i=1 X i X T i σ 2 + −1 α −1 , β|· ∼ N d 2 n i=1 Z i (˜ y i −µ−X T i α −ξ T i a −ζ T i d) σ 2 ,, , with = n i=1 Z i Z T i σ 2 + −1 β −1 , σ 2 |· ∼ Inv−χ 2 n, 1 n n i=1 (˜ y i −µ−X T i α −Z T i β −ξ T i a −ζ T i d) 2 . Conditional on the parameters (τ 2 1 ,...,τ 2 p ,τ * 2 1 ,...,τ * 2 p ,α,β</formula><p>), the model becomes the weighted linear regression, and thus the conditional posterior distributions of (a,d) are</p><formula>a|· ∼ N A −1 a ξ(˜ y i −µ−X T i α −Z T i β −ζ T i d),σ 2 A −1 a , with A −1 a = ξξ T +diag(τ 2 1 ,...,τ 2 p ) −1 −1 , d|· ∼ N A −1 d ζ(˜ y i −µ−X T i α −Z T i β −ξ T i a),σ 2 A −1 d , with A −1 d = ζζ T +diag(τ * 2 1 ,...,τ * 2 p ) −1 −1 .</formula><p>Moreover, the full conditional for</p><formula>τ 2 1 ,...,τ 2 p ,τ * 2 1 ,...,τ * 2 p are conditionally independent, with 1 τ 2 j |· ∼ Inverse-Gaussian ⎛ ⎝ λ 2 σ 2 β 2 j ,λ 2 ⎞ ⎠ , j = 1,...,p, and 1 τ * 2 j |... ∼ Inverse-Gaussian ⎛ ⎝ λ * 2 σ 2 β 2 j ,λ * 2 ⎞ ⎠ , j = 1,...,p.</formula><p>Finally, with the conjugate priors Gamma(a,b) and Gamma(a * ,b * ), the conditional posterior distributions of thePage: 519 516–523</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lasso for GWAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>hyperparameters are gammas</head><formula>λ 2 |· ∼ Gamma ⎛ ⎝ p+a, p j=1 τ 2 j 2 +b ⎞ ⎠ , and λ * 2 |· ∼ Gamma ⎛ ⎝ p+a * , p j=1 τ * 2 j 2 +b * ⎞ ⎠ .</formula><p>An efficient Gibbs sampler based on these full conditionals proceeds to draw posterior samples from each full conditional posterior distribution, given the current values of all other parameters and the observed data. This process continues until all chains converge. We use the potential scale reduction factorˆRfactorˆ factorˆR to access the convergence (<ref type="bibr" target="#b10">Gelman and Rubin, 1992</ref>). Oncê R &lt; 1.1 for all scalar estimands of interest, we continue to draw 15 000 iterations to obtain samples from the joint posterior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Posterior interpretation</head><p>The proposed MCMC algorithm for our Bayesian lasso model can provide posterior median estimates of the additive effects and dominant effects of individual SNPs, while adjusting for the effects of all other SNPs and covariates. Furthermore, using the posterior samples of a, d, and the observed genotypes, we can calculate the proportion of the phenotypic variance explained by a particular SNP, i.e. heritability, by</p><formula>h 2 j = 2ˆp2ˆp 1 ˆ p 0 (ˆ a j +(ˆ p 1 − ˆ p 0 ) ˆ d j ) 2 +4ˆp+4ˆp 2 1 ˆ p 2 0 ˆ d 2 j var(˜ y) , j = 1,...,p,</formula><p>wherê p 1 is the estimated allele frequency for A, andˆpandˆ andˆp 0 is the estimated allele frequency for a, ˆ a j is the median estimate of the additive effect for SNP j andˆdandˆ andˆd j is the median estimate of the dominant effect for SNP j. Since heritability estimates are unitless, they could guide variable selection and identify SNPs that have relatively large effects on the phenotype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Worked example</head><p>We used the newly developed model to analyze a real GWAS dataset from the FHS, a cardiovascular study based in Framingham, Massachusetts, supported by the National Heart, Lung and Blood Institute, in collaboration with Boston University (<ref type="bibr" target="#b4">Dawber et al., 1951</ref>). Recently, 550 000 SNPs have been genotyped for the entire Framingham cohort (<ref type="bibr" target="#b14">Jaquish, 2007</ref>), from which 418 males and 559 females were chosen for our data analysis. These subjects were measured for body mass index (BMI) at different ages from 29 to 61 years. As is standard practice, SNPs with minor allele frequency &lt;10% were excluded from data analysis. The numbers and percentages of non-rare allele SNPs vary among different chromosomes and ranges from 4417 to 28 771 and from 64% to 72%, respectively. In principle, our approach can handle an extremely large number of SNPs at the same time. To save our computing time, however, we use those SNPs that cannot be neglected according to a simple single SNP analysis. We chose the phenotypic data of BMI in a middle measure age of each subject for a single SNPanalysis, separately for males and females. Supplementary<ref type="figure" target="#fig_3">Figure S1</ref>gives −log 10 P-values for each SNP in the two sexes from which 1837 SNPs with a −log 10 P-value of &gt; 3.5 in at least one sex was selected for Bayesian lassso analysis. Before this analysis, we imputed missing genotypes for a small proportion of SNPs (5.16%) according to the distribution of genotypes in the population. A preconditional analysis with m = 3 and θ = 0.426 was used to mitigate observational noise, leading to the preconditioned phenotypes. Like original measures, the preconditioned BMI also displays a normal distribution (<ref type="figure" target="#fig_3">Fig. 1</ref>), which meets the normality assumption required by the new approach. By treating the sex as a discrete covariate and age as a continuous covariate, we imposed lasso penalties on the additive effects a 1 ,...,a p and dominant effects d 1 ,...,d p to identify those SNPs with notable effects on BMI. We employ the proposed MCMC algorithms to estimate all parameters and implement variable selection, where α = 1, β = 1 and all parameters in the conjugate gamma hyperpriors are 0.1. In unreported tests, we find that the posteriors are not sensitive to these prior specifications, as long as a and b are small values so that the priors are relatively flat (<ref type="bibr" target="#b17">Park and Casella, 2008;</ref><ref type="bibr" target="#b23">Yi and Xu, 2008</ref>).<ref type="figure" target="#fig_5">Figure 2</ref>plots the estimated additive and dominant effects of each SNP after adjusting for the effects of other SNPs and covariates. The heritability explained by each SNP is shown in<ref type="figure" target="#fig_6">Figure 3</ref>. The Bayesian hierarchical model automatically shrinks small coefficients to zero, and hence the posterior estimates of a, d and h 2 j can guide variable selection. We claim that a genetic effect is significant if its 95% posterior credible interval does not contain zero. Alternatively, Hoti and Sillanpaa (2006) suggested to preset a threshold value, c, such that one SNP is included into the final model if the heritability explained by this SNP is greater than c. We usually report the SNPs with high heritabilities and, thus, this threshold can be chosen on more subjective grounds.<ref type="figure" target="#tab_1">Table 1</ref>tabulates the names and positions of SNPs with the heritability (h 2 j ) greater than 0.5, as well as the estimated additive effects and heritabilities. We do not report the estimated dominant effects since they are relatively low in this example. The Bayesian lasso tends to shrink small effects of genes into zero. Assuming that a = d = 0.4 for a SNP with allele frequencies of 0.5 in a population, the additive and dominant variances explained by this SNP isthat the dominant effects are shrunk to a greater extent than the additive effects if they are of similar size. This may partly explain why the dominant effects estimated for significant SNPs are much smaller than the additive effects. The amount of shrinkage in the estimates of additive and dominant effects are quantified by two hyperparameters λ and λ * determined from the data. The posterior medians for λ and λ * are 54.474 and 54.523, respectively, with the 95% posterior intervals being<ref type="bibr">[53.325, 55.626]</ref>and<ref type="bibr">[53.359, 55.678]</ref>, respectively. These suggest that the tuning parameters for the additive and dominant effects can be estimated precisely. Since five significant SNPs are selected from chromosome 1, and four from chromosome 10, we will further examine the correlations among the significant SNPs from the same chromosome, as suggested by a referee. The correlation matrix of five significant SNPs from chromosome 1 is given by ⎛</p><formula>⎞ ⎟ ⎟ ⎟ ⎟ ⎠ ,</formula><p>where star denotes significant correlations at the significance level 1%. Clearly, these SNPs can be classified into two groups, and within each group, SNPs are highly correlated. The correlation matrix of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computer simulation</head><p>The new approach is investigated through simulation studies. We generate data according to the model (2) with µ = 0, σ 2 = 10 and n = 500. For ease of simulation, ξ ij is derived from u ij , where each u ij has a standard normal distribution marginally, and ρ = cov(u ij ,u ik ) = 0.1. Then, to mimic a SNP with equal allele frequencies, we set</p><formula>ξ ij = ⎧ ⎨ ⎩ 1, u ij &gt; c 0, −c ≤ u ij ≤ c −1, u ij &lt; −c,</formula><p>where −c is the first quartile of a standard normal distribution. Finally, ζ ij is derived from ξ ij. We assume that there are 1000 SNPs from which 20 are significant for a phenotypic trait. The positions and additive and dominant effects of individuals are given in<ref type="figure" target="#tab_2">Table 2</ref>. It is assumed that the trait is measured at a subject-specific age, following the data structure of the FHS.<ref type="figure" target="#fig_6">Figure 3</ref>gives the estimated additive and dominant genetic effects of different SNPs over 50 simulations, and<ref type="figure">Figure 4</ref>plots the heritability explained by each SNP. It is clear that lasso penalties shrink small genetic effects to zeros, resulting in sparse solutions of the regression coefficients. In general, the 20 assumed SNPs can be well identified and their additive and dominant effects well estimated. Also, two hyperparameters λ and λ * whose influence the degree of shrinkage can be well estimated. In Supplementary<ref type="figure" target="#fig_5">Figure 2</ref>, the histograms of these two hyperparameters are shown. Then, we carry out another simulation study to compare the performance of preconditioned Bayesian lasso, Bayesian lasso without preconditioning and the traditional single SNP analysis. Without loss of generality, only the additive model is considered. Specifically, we generate data on n = 200 and p = 500 or 1000 according to the model (2), with µ = 0, σ 2 = 10, ρ = 0.1, a j = 1 for 1 ≤ j ≤ 20 and a j = 0 for j &gt; 20. We apply three methods to the 100 simulated datasets: single SNP analysis (SSA), standard Bayesian lasso (B-lasso) and the Bayesian lasso applied to the preconditioned response from supervised principal components (PB-lasso). In single SNP analysis, we reject the null hypothesis that the genetic effect of an individual SNP equals to zero at the significance level of 5% with the FDR adjustment. For the Bayesian lasso and preconditioned Bayesian lasso, we reject the null hypothesis based on 95% Bayesian credible intervals. To Page: 522 516–523</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 521 516–523</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lasso for GWAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.Li et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>When the number of predictors p is much larger than the number of observations n, highly regularized approaches, such as penalized regression models, are needed to identify non-zero coefficients, enhance model predictability and avoid over-fitting (<ref type="bibr" target="#b11">Hastie et al., 2009</ref>). The L 1 penalized regression or lasso is such one of the most popular techniques. In this article, we presented a Bayesian hierarchical model with lasso penalties to simultaneously fit and estimate all possible genetic effects associated with all SNPs in a GWAS, adjusting for both discrete and continuous covariates. Lasso penalties are imposed on the additive and dominant effects, and implemented by assigning double-exponential priors to their regression coefficients. It shrinks small effects toward zero and produces sparse solutions. In this framework, SNPs with significant genetic effects can be identified more accurately. We fit the model in a fully Bayesian approach, employing the MCMC algorithm to generate posterior samples from the joint posterior distribution, which can be used to make various posterior inferences. Although computationally intensive, it is easy to implement and provides not only point estimates but also interval estimates of all parameters. The Bayesian lasso treats tuning parameters as unknown hyperparameters and generates their posterior samples when estimating other parameters. This technique avoids the choice of tuning parameters, and automatically accounts for the uncertainty in its selection that affects the estimation of the final model. In contrast, standard lasso algorithms usually select tuning parameters by K-fold cross-validation, which involves partitioning the whole dataset and refitting the model many times. This process may result in unstable tuning parameter estimates. In order to improve the performance of lasso when p is greater than n, preconditioning is considered before variable selection. Preconditioning encourages the principal components of a reduced design matrix to be highly correlated with the response, and thus in most cases only the first or first few components tend to be useful for prediction. It denoises the response variable so that variable selection becomes more efficient. Our simulation demonstrated that when p greatly exceeds n, preconditioned Bayesian lasso could successfully identify almost all the SNPs with true genetic effects. By analyzing real data, our approach is shown to produce biologically relevant results. For example, the approach detected a significant SNP ss66171460 at position 22580931 of chromosome 20 associated with BMI. It is interesting to note that this SNP is within 500 kb of the FOXA2 (Forkhead Box A2) gene, an important genetic variant that regulates obesity (<ref type="bibr" target="#b20">Wolfrum et al., 2003</ref>). One simulation example of<ref type="bibr" target="#b18">Paul et al. (2008)</ref>implies that, in the context of GWASs, SNPs that are marginally independent of the phenotype could be screened out by preconditioning, but can be identified by standard variable selection techniques such as lasso or Bayesian lasso. In theory, if SNPs are correlated with the phenotype through marginal correlations, we believe the preconditioning step is worthwhile to identify more important SNPs. However, in reality, since different SNPs may display interactions, this approach may not work perfectly. In any case, this two-step variable selection procedure should always be advantageous over a single SNP analysis, because we are always testing the marginal correlation between the predictor and response when one SNP is analyzed at a time. Motivated by<ref type="bibr" target="#b19">Tibshirani (1996)</ref>, Park and<ref type="bibr" target="#b17">Casella (2008)</ref>developed the Bayesian lasso and demonstrated the diabetes data (<ref type="bibr" target="#b6">Efron et al., 2004</ref>) with p = 10 and n = 442. We applied the Bayesian lasso to the high-dimensional regression problem, and improved it by preconditioning. We have concentrated on the preconditioned Bayesian lasso method for continuous trait in GWASs. The proposed preconditioning procedure and MCMC algorithm can be readily extended to survival data analysis and lasso penalized logistic regression in case–control disease gene mapping. Also, we may look for gene–gene interaction effects after identifying main effects, as suggested by<ref type="bibr" target="#b21">Wu et al. (2009)</ref>. The model with a capacity to identify epistatic interactions will enables geneticists to decipher a detailed picture of the genetic architecture of a complex trait. Funding: NSF/NIH Mathematical Biology grant (No. 0540745); NIDA; NIH grants (R21 DA024260 and R21 DA024266). The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIDA or the NIH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest: none declared.</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>[09:55 2/2/2011 Bioinformatics-btq688.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>[09:55 2/2/2011 Bioinformatics-btq688.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.1.</head><figDesc>Fig. 1. The histograms of original and preconditioned BMI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.2.</head><figDesc>Fig. 2. Estimated additive (A) and dominant effects (B) of 1837 SNPs from the Framingham heart study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.3.</head><figDesc>Fig. 3. Estimated additive (A) and dominant effects (B) based on 50 simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 1. Information about significant SNPs</figDesc><table>Chromosome Name 
Position 
Additive Heritability (%) 

1 
ss66185476 
8445140 
0.15 
0.74 
1 
ss66374301 
8451728 
0.19 
1.36 
1 
ss66295856 
8578082 
0.20 
1.35 
1 
ss66516012 198313489 
−0.13 
0.89 
1 
ss66364251 198321700 
0.12 
0.66 
10 
ss66311679 
32719838 
0.33 
4.65 
10 
ss66293192 
32903593 
0.27 
2.08 
10 
ss66303064 
32995111 
0.36 
5.93 
10 
ss66128868 
33407810 
0.28 
2.75 
20 
ss66171460 
22580931 
−0.13 
0.78 
22 
ss66055592 
23420006 
0.33 
5.13 
22 
ss66164329 
23420370 
0.37 
6.70 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Genetic effects of 20 assumed SNPs for data simulation</figDesc><table>Position 
Additive 
Position 
Dominant 

100 
1.2 
50 
1.2 
200 
1.2 
150 
1.2 
300 
1.2 
250 
1.2 
400 
0.8 
350 
0.8 
500 
0.8 
450 
0.8 
600 
0.8 
550 
0.8 
700 
0.4 
650 
0.4 
800 
0.4 
750 
0.4 
850 
1.2 
850 
1.2 
900 
0.8 
900 
1.2 
950 
1.2 
950 
0.8 
1000 
0.8 
1000 
0.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 3. Simulation results for three methods based on 100 simulations</figDesc><table>Method 
Bias 
Empirical 
Aver. 
Proportion of 
Standard Error 
non-zeros 
correct-fit 

n = 200,p = 500, β 1 – β 20 
SSA 
4.17 
1.99 
16.62 
0.08 
(0.21) 
(0.19) 
(1.51) 
B-lasso 
0.07 
0.34 
18.28 
0.18 
(0.04) 
(0.02) 
(1.36) 
PB-lasso 
0.00 
0.35 
19.68 
0.63 
(0.03) 
(0.03) 
(0.65) 

n = 200,p = 500, β 21 – β 500 
SSA 
0.44 
0.43 
0.78 
0.46 
(0.05) 
(0.07) 
(1.09) 
B-lasso 
0.00 
0.04 
0.95 
0.42 
(0.01) 
(0.01) 
(0.94) 
PB-lasso 
0.00 
0.03 
1.25 
0.30 
(0.01) 
(0.04) 
(0.73) 

n = 200,p = 1000, β 1 – β 20 
SSA 
4.13 
1.96 
15.71 
0.04 
(0.18) 
(0.17) 
(2.73) 
B-lasso 
0.36 
0.38 
17.11 
0.08 
(0.06) 
(0.07) 
(2.69) 
PB-lasso 
0.00 
0.36 
19.24 
0.51 
(0.04) 
(0.03) 
(1.81) 

n = 200,p = 1000, β 21 – β 1000 
SSA 
0.43 
0.43 
0.42 
0.69 
(0.04) 
(0.06) 
(0.84) 
B-lasso 
0.00 
0.02 
0.33 
0.76 
(0.01) 
(0.01) 
(0.18) 
PB-lasso 
0.00 
0.02 
1.17 
0.56 
(0.00) 
(0.01) 
(1.38) 

</table></figure>

			<note place="foot">© The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Fig. 4. Estimated heritability explained by each SNP based on 50 simulations. ameliorate the bias of the parameter estimates introduced by lasso penalties, we always refit the linear regression model without the penalty term using only those SNPs selected by the model selection procedure. For each estimated genetic effect obtained from each method, we calculate the average bias and empirical standard error over 100 simulations. Since the first 20 genetic effects are non-zeros with the same true value, in Table 3 we report the average values over the first 20 SNPs and over the rest of the SNPs separately. The standard error of each average is in parentheses. In the column labeled &apos;Aver. Nonzeros&apos;, we present the average number of non-zero coefficients correctly identified to be non-zero, or the average number of zero coefficients incorrectly estimated to be non-zero in 100 replications. In the column &apos;Proportion of Correct-fit&apos;, we present the proportion of replications that the exact true model was identified. As can be seen from Table 3, the single SNP analysis tend to overestimate the genetic effect, since when we test a SNP for the association with the phenotype, we assume the genetic variation is solely due to this particular SNP, and ignore the effects from all other SNPs. Therefore, in terms of parameter estimates, model selection methods that simultaneously estimate the genetic effects associated with all SNPs outperform the traditional single SNP analysis. In terms of variable selection, although preconditioned Bayesian lasso has a slightly higher false positive rate due to the preconditioning step, it greatly improves the probability of correctly identifying regression coefficients with non-zero effects. Moreover, as the number of SNPs gets larger, single SNP analysis detected fewer important SNPs, since this method subjects to severe multiple comparison adjustment. However, preconditioned Bayesian lasso is still able to identify non-zero coefficients and zero coefficients correctly in almost every simulation. Supplementary Table 1 displays the simulation results when ρ = 0.5, which are consistent with our findings. Since our method is from the Bayesian perspective and is based on the Gibbs sampler, the computational time is relatively high. For example, for each replicate in this simulation study, averagely it takes 439 s when p = 1000 and 109 seconds when p = 500 on a 2.0 GHz PC.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Scale mixture of normal distributions</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">F</forename>
				<surname>Andrews</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="99" to="102" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="issue">22</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="516" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Gwas</forename>
				<surname>Lasso For</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Epidemiological approaches to heart disease: the Framingham study</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Dawber</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ame. J. Public Health</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="279" to="286" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Progress and challenges in genome-wide association studies in humans</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Donnelly</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="page" from="728" to="731" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Least angle regression (with discussion)</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Stat</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Variable selection via nonconcave penalized likelihood and its oracle properties</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1348" to="1360" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Sure independence screening for ultrahigh dimensional feature space (with discussion)</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lv</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="849" to="911" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">A statistical view of some chemometrics regression tools</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">E</forename>
				<surname>Frank</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">H</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="109" to="148" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Inference from iterative simulation using multiple sequences</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gelman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B</forename>
				<surname>Rubin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="457" to="511" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">High-dimensional problems: p &gt; N. The Elements of Statistical Learning</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous analysis of all SNPs in genome-wide and resequencing association studies</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hoggart</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Genet</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian mapping of genotype × expression interactions in quantitative and qualitative traits</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hoti</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Sillanpaa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heredity</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4" to="18" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">The Framingham heart study, on its way to becoming the gold standard for cardiovascular genetic epidemiology</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Jaquish</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Genet</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">A variational Bayes algorithm for fast and accurate multiple locus genome-wide association analysis</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">A</forename>
				<surname>Logsdon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="11" to="58" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Genome-wide association studies for complex traits: consensus, uncertainty and challenges</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Mccarthy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="356" to="369" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">The Bayesian lasso</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Park</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Casella</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="681" to="686" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Preconditioning for feature selection and regression in highdimensional problems</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Paul</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Stat</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1595" to="1618" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selction via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Role of Foxa-2 in adipocyte metabolism and differentiation</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wolfrum</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Invest</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="345" to="356" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Genome-wide association analysis by lasso penalized logistic regression</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">T</forename>
				<surname>Wu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="714" to="721" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Common SNPs explain a large proportion of the heritability for human height</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="565" to="569" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian lasso for quantitative trait loci mapping</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Yi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Xu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="1045" to="1055" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>