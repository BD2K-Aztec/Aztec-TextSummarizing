
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">System biology OKVAR-Boost: a novel boosting algorithm to infer nonlinear dynamics and interactions in gene regulatory networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Né</forename>
								<surname>Hé My Lim</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Yasin</forename>
								<surname>S° Enbabaog ˘ Lu</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computational Medicine and Bioinformatics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109-2218</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">George</forename>
								<surname>Michailidis</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109-1107</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">AMIB/TAO</orgName>
								<orgName type="institution" key="instit2">INRIA-Saclay</orgName>
								<orgName type="institution" key="instit3">LRI umr CNRS 8623</orgName>
								<orgName type="institution" key="instit4">Université Paris Sud</orgName>
								<address>
									<postCode>91400</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Associate Editor: Alfonso Valencia</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Florence</forename>
								<surname>D &apos;alché-Buc</surname>
							</persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBISC EA 4526</orgName>
								<orgName type="institution" key="instit2">Université d&apos;E ´ vry-Val d&apos;Essonne</orgName>
								<address>
									<addrLine>91000 E ´ vry</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">System biology OKVAR-Boost: a novel boosting algorithm to infer nonlinear dynamics and interactions in gene regulatory networks</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="issue">11</biblScope>
							<biblScope unit="page" from="1416" to="1423"/>
							<date type="published" when="2013">2013</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt167</idno>
					<note type="submission">Received on November 15, 2012; revised on March 24, 2013; accepted on April 4, 2013</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Reverse engineering of gene regulatory networks remains a central challenge in computational systems biology, despite recent advances facilitated by benchmark in silico challenges that have aided in calibrating their performance. A number of approaches using either perturbation (knock-out) or wild-type time-series data have appeared in the literature addressing this problem, with the latter using linear temporal models. Nonlinear dynamical models are particularly appropriate for this inference task, given the generation mechanism of the time-series data. In this study, we introduce a novel nonlinear autoregressive model based on operator-valued kernels that simultaneously learns the model parameters, as well as the network structure. Results: A flexible boosting algorithm (OKVAR-Boost) that shares features from L 2-boosting and randomization-based algorithms is developed to perform the tasks of parameter learning and network inference for the proposed model. Specifically, at each boosting iteration, a regularized Operator-valued Kernel-based Vector AutoRegressive model (OKVAR) is trained on a random subnetwork. The final model consists of an ensemble of such models. The empirical estimation of the ensemble model&apos;s Jacobian matrix provides an estimation of the network structure. The performance of the proposed algorithm is first evaluated on a number of benchmark datasets from the DREAM3 challenge and then on real datasets related to the In vivo Reverse-Engineering and Modeling Assessment (IRMA) and T-cell networks. The high-quality results obtained strongly indicate that it outperforms existing approaches. Availability: The OKVAR-Boost Matlab code is available as the archive: http://amis-group.fr/sourcecode-okvar-boost/OKVARBoost</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The ability to reconstruct cellular networks plays an important role in our understanding of how genes interact with each other and how this information flow coordinates gene regulation and expression in the cell. Gene regulatory networks (GRN) have the potential to provide us with the cellular context of all genes of interest, as well as with a means to identify specific subnetworks that are malfunctioning in a given disease state (<ref type="bibr" target="#b5">Cam et al., 2004;</ref><ref type="bibr" target="#b15">Jesmin et al., 2010</ref>). A diverse suite of mathematical tools has been developed and used to infer gene regulatory interactions from spatial and temporal high-throughput gene expression data (see<ref type="bibr" target="#b2">Bansal et al., 2007;</ref><ref type="bibr" target="#b21">Markowetz and Spang, 2007</ref>and references therein). A fair comparison for the relative merits of these methods requires their evaluation on benchmark datasets, which the DREAM (Dialogue for Reverse Engineering Assessments and Methods) project (<ref type="bibr" target="#b19">Marbach et al., 2009</ref>) provided. It aims to understand the strengths and the limitations of various algorithms to reconstruct cellular networks from high-throughput data (<ref type="bibr" target="#b37">Stolovitzky et al., 2007</ref>). In addition to the choice of the algorithm, network reconstruction heavily depends on the input data type used. Data that measure the response of the cell to perturbations—either by knocking out or silencing genes—are particularly useful for such reconstructions because they offer the potential to obtain a detailed view of cellular functions. The downside is that obtaining large-scale perturbation data is expensive and relatively few methods have been proposed in the literature to infer regulatory networks from such data due to computational challenges (<ref type="bibr" target="#b13">Gupta et al., 2011;</ref><ref type="bibr" target="#b38">Yip et al., 2010</ref>). Data from time-course gene expression experiments have the potential to reveal regulatory interactions as they are induced over time. A number of methods have been used for this task, including dynamic Bayesian networks (<ref type="bibr" target="#b24">Morrissey et al., 2010;</ref><ref type="bibr" target="#b39">Yu et al., 2004</ref>), Granger causality models (see Shojaie and Michailidis, 2010b and references therein) and state-space models (<ref type="bibr" target="#b26">Perrin et al., 2003;</ref><ref type="bibr" target="#b30">Rangel et al., 2004</ref>). The first set of methods is computationally demanding, while the latter two use linear dynamics, hence limiting their appeal. Other approaches are based on assumptions about the parametric nature of the dynamical model and resort to time-consuming evolutionary algorithms to learn the network (<ref type="bibr">SıˆrbuSıˆrbu et al., 2010</ref>). Moreover, in spite of the rich collection of methods used to solve the topology and dynamics of GRNs, certain types of errors continue to *To whom correspondence should be addressed. y The authors wish it to be known that, in their opinion, the first two authors should be regarded as jointThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com challenge the modeling efforts, implying that there is still significant room for improvement (<ref type="bibr" target="#b20">Marbach et al., 2010;</ref><ref type="bibr" target="#b36">Smet and Marchal, 2010</ref>). This study makes a number of key contributions to the challenging problem of network inference based solely on timecourse data. It introduces a powerful network inference framework based on nonlinear autoregressive modeling and Jacobian estimation. The proposed framework is rich and flexible, using penalized regression models that coupled with randomized search algorithms, and features of L 2-boosting prove particularly effective as the extensive simulation results attest. The models used require tuning of a number of parameters, and we introduce a novel and generally applicable strategy that combines bootstrapping with stability selection to achieve this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Nonlinear autoregressive models and network inference</head><p>Let x t 2 R p denote the observed state of a GRN comprising p genes, with S ¼ f1, Á Á Á , pg. We assume that a first-order stationary model is adequate to capture the temporal evolution of the network state, which can exhibit nonlinear dynamics captured by a function H : R p ! R p ; i.e. x tþ1 ¼ Hðx t Þ þ u t , where u t is a noise term. The regulatory interactions among the genes are captured by an adjacency matrix A, which is the target of our inference procedure. Note that for a linearly evolving network, A can be directly estimated from the data. However, in our setting, it can be obtained by averaging the values of the empirical Jacobian matrix J of the function H, over the whole set of time points. Specifically, denote by x 0 ,. .. , x NÀ1 the observed time series of the network state. Then, 8ði, jÞ 2 S Â S, the empirical estimate of the Jacobian matrix of model H is given by</p><formula>JðHÞ ij ¼ X NÀ2 t¼0 @Hðx t Þ i @ðx t Þ j ð1Þ</formula><p>and an estimate of the adjacency matrix A of the network is given by ^ A ij ¼ gðJðHÞ ij Þ where g is a thresholding function. Note that in the presence of sufficient number of time points (N44p) one can use the above posited model directly to obtain an estimate of A, provided that a good functional form of H is selected. However, the presence of more genes than time points makes the problem more challenging, which together with the absence of an obvious candidate functional form for H make a nonparametric approach an attractive option. Such an approach is greatly facilitated by adopting an ensemble methodology, where H is built as a linear combination of nonlinear vector autoregressive base models defined over overlapping subsets of genes (e.g. subnetworks). Let M be the number of subnetworks and S m &amp; S (m ¼ 1,. .. , M) be the subset of genes that constitutes the m th subnetwork. Each subnetwork has the same size k. We assume that H can be written as a linear combination of M autoregressive functions of the form h : R p ! R p such that</p><formula>b x tþ1 ¼ Hðx t Þ ¼ X M m¼1 m hðx t ; S m Þ ð 2Þ</formula><p>The paramater set S m defines the subspace of R p where h operates. This component-wise subnetwork approach is intended to overcome the intractability of searching in high-dimensional spaces and to facilitate model estimation. In our framework, subnetworks do not have any specific biological meaning and are allowed to overlap. Efficient ways to build an ensemble of models include bagging, boosting and randomization-based methods such as random forests (<ref type="bibr" target="#b8">Dietterich, 2000;</ref><ref type="bibr" target="#b9">Friedman et al., 2001</ref>). The latter two approaches have been empirically shown to perform well in classification and regression problems. In this study, we use an L 2boosting type algorithm suitable for regression problems (Bu¨hlmann<ref type="bibr" target="#b4">Bu¨hlmann and Yu, 2003;</ref><ref type="bibr" target="#b9">Friedman et al., 2001</ref>) enhanced with a randomization component where we select a subnetwork at each iteration. The algorithm sequentially builds a set of predictive models by fitting at each iteration the residuals of the previous predictive model. Early-stopping rules developed to avoid overfitting improve the performance of this algorithm. Next, we discuss a novel class of base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A new base model</head><p>The ensemble learner is a linear combination of M base models denoted by h [Equation</p><p>(2)]. Even though h works on a subspace of R p defined by S m , for the sake of simplicity we present here a base model h : R p ! R p that works with the whole set of genes, e.g. in the whole space R p. Here, we introduce a novel family of nonparametric vector autoregressive models called OKVAR (Operator-valued Kernel-based Vector AutoRegressive) (<ref type="bibr" target="#b17">Lim et al., 2012</ref>) within the framework of Reproducing Kernel Hilbert Space (RKHS) theory for vector-valued functions. Operator-valued kernel-based models have been previously used for multitask learning problems (<ref type="bibr" target="#b23">Micchelli and Pontil, 2005</ref>), functional regression (<ref type="bibr" target="#b16">Kadri et al., 2010</ref>) and link prediction (<ref type="bibr" target="#b3">Brouard et al., 2011</ref>). OKVAR models generalize kernel-based methods initially designed for scalar-valued outputs, such as kernel ridge regression, elastic net and support vector machines, to vector-valued outputs. An operator (matrix)-valued kernel (as output space is R p , the operator is a linear application on vectors of R p and thus a matrix), whose properties can be found in Senkene and Tempel'man (1973), takes into account the similarity between two vectors of R p in a much richer way than a scalar-valued kernel, as shown next. Let x 0 ,. .. , x NÀ1 be the observed network states. Model h is built on the observation pairs ðx 0 , x 1 Þ,. .. , ðx NÀ2 , x NÀ1 Þ and defined as</p><formula>hðx t ; SÞ ¼ X NÀ2 k¼0 Kðx k , x t Þ:c k ð3Þ</formula><p>where KðÁ, ÁÞ is an operator-valued kernel and each c k (k 2 f0,. .. , N À 2g) is a vector of dimension p. In the following, we will denote by C ¼ ðc k, i Þ k, i 2 M NÀ1, p , the matrix composed of the N – 1 row vectors c T k of dimension p.</p><p>In this work, we define a novel matrix-valued kernel built on the Hadamard product of a decomposable kernel and a transformable kernel previously introduced in<ref type="bibr" target="#b7">Caponnetto et al., 2008</ref>(see details in the Supplementary Material): 8ðx, zÞ 2 R 2p ,</p><formula>Kðx, zÞ ij ¼ b ij exp À 0 jjx À zjj 2 À Á : exp À 1 ðx i À z j Þ 2 À Á ð4Þ</formula><p>K depends on a matrix hyperparameter B that must be a positive semi-definite matrix. The term exp À 0 jjx À zjj 2 À Á is a classical Gaussian kernel that measures how a pair of states ðx, zÞ are close. More interestingly, the term exp À 1 ðx i À z j Þ 2 À Á measures how close coordinate i of state x and coordinate j of state z are, for any given pair of states ðx, zÞ.One great advantage of such a kernel is that it includes a term that reflects the comparison of all coordinate pairs of the two network states and does not reduce them to a single number. The matrix B serves as a mask, imposing the zeros. When b ij is zero, the i-th coordinate of x and the j-th coordinate of z do not interact and do not play a role in the output of the model. In other words, for a given gene i 2 S, the output of the model writes as follows:</p><formula>hðx t ; SÞ i ¼ P NÀ2 k¼0 ðKðx k , x t Þ:c k Þ i ¼ P p j¼1 b ij P NÀ2 k¼0 exp À 0 jjx k À x t jj 2 À Á exp À 1 ðx ki À x tj Þ 2 À Á c kj hðx t ; SÞ i ¼ X p j¼1 b ij f ij ðx t Þ ð 5Þ Equation (5)</formula><p>shows that the expression level of gene i at time t þ 1 is modeled by a linear combination of nonlinear terms f ij ðx t Þ that share parameter C. The function f ij itself is a nonparametric function built from training data. f ij ðyÞ ¼</p><formula>P NÀ2 k¼0 exp À 0 jjx k À yjj 2 À Á exp À 1 ðx ki À y j Þ 2 À Á c kj . The function</formula><p>f ij expresses the role of the regulator j on gene i. If b ij equals 0, then gene j does not regulate gene i, according to the model. Matrices B and C need to be learned from the available training data. If B is fixed, C can be estimated using penalized least squares minimization as in (<ref type="bibr" target="#b3">Brouard et al., 2011</ref>). However, learning B and C simultaneously is more challenging, as it involves a nonconvex optimization problem. We propose here to define B as the Laplacian of an undirected graph represented by an adjacency matrix W to ensure the positive semi-definiteness of B. Then, learning B reduces to learn W. In this work, we decouple the learning of W and C by first estimating W and then C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">OKVAR-Boost</head><p>The proposed algorithm is called OKVAR-Boost, as H models the temporal evolution between network states x t with an L 2-boosting approach. As seen in Algorithm 1 and illustrated in<ref type="figure" target="#fig_0">Figure 1</ref>, it generates H m ðx t Þ, an estimate of x tþ1 at iteration m, and updates this estimate in a while-loop until an earlystopping criterion is met, or until the prespecified maximum number of iterations M is reached. In the OKVAR-Boost loop, H 0 ðx t Þ is initialized with the mean values of the genes across the time points. The steps for estimating H in a subsequent iteration m are as follows: Step 1 computes the residuals u ðmÞ tþ1 for time points t 2 f0,. .. , N À 2g. Computing the residuals in this step confers OKVAR-Boost its L 2-boosting nature. In Step 2, an early-stopping decision is made based on the comparison between the norms of the residuals and a prespecified stopping criterion ". If the norms for all dimensions (genes) are less than ", the algorithm exits the loop. In Step 3, a random subset S m of size k is chosen among the genes in S, whose norm exceeds ". This step constitutes the 'randomization component' of the algorithm. Step 4 uses the current residuals in the subspace to estimate the interaction matrix W m and parameters C ðmÞ .</p><p>Subsequently, m is optimized through a line search. The m th boosting model H m ðx t Þ is updated in Step 5 with the current W m , C ðmÞ and m estimates. If the prespecified number of iterations M has not been reached, the algorithm loops back to Step 1. Otherwise, it exits the loop and estimates the adjacency matrix ^ A by computing and thresholding the Jacobian matrix. We next delineate how the interaction matrix W m and model parameters C ðmÞ and m are estimated from residuals in Step 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 OKVAR-Boost</head><p>Inputs: Network states: x 0 ,. .. , x NÀ1 2 R p Early-stopping threshold " Initialization: 8t 2 f0,. .. , N À 1g, H 0 ðx t Þ :¼ ð x 1 ,. .. , x p Þ T Iteration m ¼ 0, STOP ¼ false while m5M and STOP ¼ false do Step 0: Update m m þ 1</p><p>Step 1: Compute the residuals u ðmÞ tþ1 :¼ x tþ1 À H mÀ1 ðx t Þ Step 2: STOP: ¼ true if 8j 2 f1,. .. , pg, jju j ðmÞ jj if STOP ¼ false then Step 3: Select S m , a random subset of genes of size k p Step 4: (a) Estimate the interaction matrix W m 2 f0, 1g kÂk from u ðmÞ 1 ,. .. , u ðmÞ N and compute B m as the Laplacian of W m , (b) estimate the parameters C m and (c) estimate m by a line search. Step 5: Update the m th boosting model:</p><formula>H m ðx t Þ :¼ H mÀ1 ðx t Þþ m hðx t ; fS m , W m , C m gÞ</formula><p>end if end while m stop :¼ m Compute the Jacobian matrix J mstop of H mstop across time points, and threshold to get the final adjacency matrix ^ A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Randomization and estimation of the interaction matrix</head><p>Combining features of random forests and boosting algorithms gave robust results in a previous study (<ref type="bibr" target="#b11">Geurts et al., 2007</ref>). We use this approach and select, at each iteration m (Step 3) a random subset of genes denoted S m &amp; S. Then, in (Step 4), we use partial correlation estimation, as a weak graph learner, on S m to increase the robustness of the algorithm and reinforce its ability to focus on subspaces. The details of the statistical test for conditional independence based on partial correlations can be found in the Supplementary Material. Based on the matrix W m resulting from this test, we define B m as the Laplacian of W m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Autoregression using OKVAR</head><p>At each iteration m, an OKVAR model such as previously described in Equation (3) is defined to work in the k dimensional subspace associated with the subset S m. Denoted by P ðmÞ the p Â p diagonal matrix is defined as follows: p ðmÞ ii ¼ 1 if gene i belongs to S m , and p ðmÞ ii ¼ 0 otherwise. Formally, h m ¼ hðÁ; fS m , W m , C ðmÞ gÞ has to be learnt from e u ðmÞ t ¼ P ðmÞ u ðmÞ t instead of residuals u ðmÞ t. Then, we only need to complete Step 4(b) by learning parameters C ðmÞ. This estimation can be realized via the functional estimation of h m within the framework of regularization theory, e.g. the minimization of a cost function comprising the empirical square loss and the square ' 2 norm of the function h m , which imposes smoothness to the model. Moreover, our aim is 2-fold: we do not only want to get a final model H that fits the data well and predicts successfully future time points, but we also want to extract the underlying regulatory matrix from the model; therefore, the cost function to be minimized must also reflect this goal. Following Subsection 2.1, the adjacency matrix of the network A is estimated by the empirical Jacobian J(H), expressed in terms of the empirical Jacobian J ðmÞ of the base models h m (m ¼ 1,. .. , m stop ) using the observed data (not residuals):</p><formula>8ði, jÞ 2 S Â S, J ij ¼ P mstop m¼1 m J ðmÞ ij ¼ 1 NÀ1 P mstop m¼1 m P NÀ2 t¼0 J ðmÞ</formula><p>ij ðtÞ where for a given time point t, the coefficients of the Jacobian, J ðmÞ ij ðtÞ, are given as follows:</p><formula>J ðmÞ ij ðtÞ ¼ @h m ðx t Þ i @ðx t Þ j ¼ X NÀ2 k¼0 X p '¼1 c ðmÞ k, ' @K ðmÞ ðx k , x t Þ i' @ðx t Þ j</formula><p>The full expression of the instantaneous Jacobian when K ðmÞ is chosen as the Gaussian matrix-valued kernel defined in<ref type="bibr">Equation (4)</ref>is given in the Supplementary Material. Whatever is K ðmÞ , when it is fixed, controlling the sparsity of the coefficients of C ðmÞ will impact the sparsity of J ðmÞ and will avoid too many false-positive edges. Therefore, we add to the cost function previously discussed, an ' 1 term to ensure the sparsity of C ðmÞ :</p><formula>LðC ðmÞ Þ ¼ X NÀ2 t¼0 e u ðmÞ tþ1 À h m ðe u ðmÞ t Þ 2 þ 2 jjh m jj 2 H þ 1 jjC ðmÞ jj 1 ð6Þ</formula><p>The respective norms can be computed as follows:</p><formula>jjh m jj 2 H ¼ X NÀ2 i, j¼1 c ðmÞ T i K ðmÞ ðe u ðmÞ j ,e u ðmÞ i Þc ðmÞ j and jjC ðmÞ jj 1 ¼ P NÀ2 t¼0</formula><p>P j2Sm jc ðmÞ tj j. This regularization model combining ' 1 and ' 2 penalties is known as the elastic net model (<ref type="bibr" target="#b9">Friedman et al., 2001</ref>) and it has been shown that not only does it achieve sparsity like lasso penalized models, but also encourages grouping effects, which might be relevant in our case to highlight possible joint regulation among network variables (genes). We used a projected scaled subgradient method (<ref type="bibr" target="#b32">Schmidt et al., 2009</ref>) to minimize the cost function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data description</head><p>The performance of OKVAR-Boost was evaluated on a number of GRNs obtained from DREAM3 in silico challenges. Specifically, 4 and 46 time series consisting 21 time points corresponding, respectively, to size-10 and size-100 networks for Escherichia coli (2) and Yeast (3) were selected. The data were generated by simulating from a thermodynamic model for gene expression to which Gaussian noise was added. The multiple time series correspond to different random initial conditions for the thermodynamic model (<ref type="bibr" target="#b29">Prill et al., 2010</ref>). The topology of the networks is extracted from the currently accepted E.coli and Saccharomyces cerevisiae GRNs, and exhibits varying patterns of sparsity and topological structure. Some summary statistics for the networks are presented in Supplementary Table S1. Yeast2 and Yeast3 have markedly higher average-degree, density and lower modularity for both size-10 and size-100 networks. Ecoli2 is seen to be different from Ecoli1 in that for size 10 is denser, less modular, has higher average-degree, whereas for size 100, these relations are reversed. Yeast1 is observed to be closer to the Ecoli networks for all three statistics. In addition to these synthetic datasets, we applied OKVARBoost to two other datasets. The first deals with activation of T-cells (<ref type="bibr" target="#b30">Rangel et al., 2004</ref>) and comprises 44 times series (replicates) for 58 genes. The second dataset comes from the In vivo Reverse-Engineering and Modeling Assessment (IRMA) experiment (<ref type="bibr" target="#b6">Cantone et al., 2009</ref>), where a size-5 network was synthesized, with each gene controlling the transcription of at least another gene. Further, galactose and glucose are, respectively, used to switch on or off the network. In this study, we focus on time-series measurements (four switch-off series and five switch-on series) comprising 9 up to 20 time points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyperparameters and model selection</head><p>Because the OKVAR-Boost algorithm depends on a number of tuning parameters, some of them were a priori fixed, with the remaining ones selected automatically with a new variant of stability criterion, appropriate for time series, called Block Stability. Let us first summarize the hyperparameters that we fixed a priori. They include a stopping criterion for the norm of the residual vector, set to ¼ 10 À2 , the size of random subnetworks k in Step 1 set to eight genes for size-10 networks, to 17 for size100 networks, to six for T-cell and to four for IRMA (parameters based on a grid search) and in Step 4 the level of the partial correlation test is set to a conservative ¼ 5%. If the algorithm fails to find any significant interactions with the partial correlation test, the subnetwork is discarded and a new k Â k subnetwork is randomly chosen. This procedure is repeated for a maximum of 100 iterations. In Step 5, the parameter of the Gaussian matrix-valued kernel 1 Equation (4) is fixed to 0.2. As the role of the scalar Gaussian kernel of Equation (4) is not central in the network inference, 0 is fixed to 0 in those experiments. For the other hyperparameters, we consider stability, which is a finite sample criterion that has been applied to select hyperparameters in various settings, such as clustering or feature selection in regression (<ref type="bibr" target="#b22">Meinshausen and Bu¨hlmannBu¨hlmann, 2010</ref>). The idea underlying stability-driven selection is to choose the hyperparameters that provide the most stable results when randomly subsampling the data. We propose a new selection criterion, called Block stability based on the block bootstrap. Block bootstrap resamples time series by consecutive blocks ensuring that each block of observations in a stationary time series can be treated as exchangeable (<ref type="bibr" target="#b27">Politis et al., 1999</ref>). For the DREAM data, we chose a length of 12 and 15 time points for size 10 and size 100, respectively, and 7 for both the T-cell and IRMA datasets, while the number of pairs of block-bootstrapped subsamples was set to B ¼ 20. We define the Block instability noted BIS for a pair of hyperparameters ð 1 , 2 Þ by measuring how the two Jacobian matrices built from two models learnt from two different subsamples differ in average. The reader will find the expression of the BIS criterion in the Supplementary Material. When L time series are available, the criterion becomes</p><formula>BISð 1 , 2 ; x NÀ1, 1 0 ,. .. , x NÀ1, L 0 Þ ¼ 1 L P L '¼1 BISð 1 , 2 ; x NÀ1, ' 0 Þ.</formula><p>In the experiments, hyperparameters 1 and 2 were chosen as the minimizers of the block-instability criterion BIS when only a single time series was available and BIS when multiple ones were provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OKVAR-Boost with multiple runs</head><p>As OKVAR-Boost residuals diminish rapidly, there is a risk that the potential regulators and their targets may not be fully explored by the random subnetwork procedure of the algorithm. To address this issue, the algorithm was run nRun ¼ 10 times and a consensus network was built by combining the predictions from each run. Specifically, for each pair of nodes, the frequency with which the edge appears over multiple runs was calculated, thus yielding the final network prediction. If the frequency was above a preset threshold, the edge was kept, otherwise discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Consensus network from multiple time series</head><p>In many instances, multiple (L) time series may be available, either because of multiple related initial conditions or due to biological and/or technical replicates. In this case, the procedure just needs to be repeated accordingly and the L Á nRun obtained networks are combined as described above to provide a final consensus network. We set ^ A ij ¼ 1 if and only if P</p><formula>LÁnRun r¼1 j ^ A ðrÞ ij j ! f cons Á L Á</formula><p>nRun, where ^ A ðrÞ is the estimated adjacency matrix for run number r and f cons 2 ½0, 1 is the consensus threshold level for edge acceptance. When doing multiple runs, f cons should be adjusted if prior knowledge about the size, density and modularity of the underlying network is available. In general, the larger the size of a biological network, the bigger are the combinatorial challenges for discovering true edges and avoiding false ones. Therefore, the consensus threshold should be set to smaller values for larger networks. For a fixed size, the threshold will depend on the density and modularity of the network. Denser and more modular networks have greater instances of co-regulation for certain genes, which lowers prediction accuracy for network inference algorithms (<ref type="bibr" target="#b20">Marbach et al., 2010</ref>) and leads to a greater number of false positives. In our experience, lower consensus thresholds are also recommended for denser and more modular networks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network inference and evaluation</head><p>When ground truth is available for the network inference task, namely for simulated data from DREAM3 challenges and real data from the synthetic network IRMA, we evaluated the results according to the DREAM3 challenge assessment. In DREAM3 challenges, the target graph is directed but not labeled with inhibitions or inductions. The performance of the algorithm is assessed using the following standard metrics: the receiver operating characteristic curve (ROC), the area under ROC (AUROC) and the area under the precision-recall curve (AUPR). To extract the adjacency matrix from the Jacobian (see subsection 2.1), the hyperbolic tangent transformation applied to the normalized coefficients of the Jacobian was used</p><formula>(for a matrix Q, jjQjj F ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi P i, j Q 2 ij q is the Frobenius norm of Q): ^ A ij ¼ s tanh Jij jjJjj F À , with sðxÞ ¼ 1</formula><p>if x40 and 0, otherwise and varying to get ROC and PR curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Numerical results for DREAM3 networks</head><p>Overall, the OKVAR-Boost algorithm succeeds in fitting the observed data and exhibits fast convergence. In<ref type="figure" target="#fig_1">Figure 2</ref>, results from the Ecoli2 networks (size 10 and size 100) are presented. Note that the algorithm is rich and flexible enough to have the mean squared error for genes diminishing fast toward zero in only 5–10 iterations. The performance of the OKVAR-Boost algorithm for prediction of the network structure is given in Tables 1 and 2 and Supplementary<ref type="figure">Table S3</ref>. The results show a comparison between the base learner alone when the true B is provided for DREAM3 size-10 networks (<ref type="figure" target="#tab_1">Table 1</ref>), boosting with multiple runs using a single time series and all the available time series. The base learner is an elastic-net OKVAR model learnt given the Laplacian of the true undirected graph and applied on the whole set S of genes. The LASSO row corresponds to a classical linear least squares regression: x tþ1, i ¼ x T t i , realized on each dimension (gene) i ¼ 1. .. p subject to an ' 1 penalty on the i parameters. An edge ði, jÞ is assigned for each nonzero ij coefficient. The LASSO was run on all the available time series and a final consensus network is built in the same fashion as delineated in section 3.4. The AUROC and AUPR values obtained strongly indicate that OKVAR-Boost outperforms the LASSO and the teams that exclusively used the same set of time-series data in the DREAM3 competition. The multiple-run consensus strategy achieved superior AUROC and AUPR results for all networks except for size-10 Yeast2. We particularly note that the OKVAR-Boost consensus runs exhibited excellent AUPR values compared with those obtained by teams 236 and 190. The consensus thresholds for multiplerun and bootstrap experiments were chosen taking into account network properties such as size, density, modularity, averagedegree and topology. For size-10 networks, Yeast2 and Yeast3 have substantially higher density and average-degree suggestinglower consensus thresholds. In light of this information, we used a threshold of 50% for Ecoli1, Ecoli2, Yeast1, and 40% for Yeast2 and Yeast3 for multiple-run experiments. For size-100 networks, we made use of the prior information that Ecoli2 has a star topology composed of few central hubs that regulate many genes. Because it is more difficult to reconstruct such special modularities, one should expect to observe lower edge frequencies. Thus, a smaller consensus threshold would be appropriate. For the multiple-run experiments, we used 20% for Ecoli2 and 40% for all other networks. A comparison between algorithms for size-100 networks (<ref type="figure" target="#tab_2">Table 2</ref>) shows that OKVAR-Boost again clearly outperforms Team 236, the only team that exclusively used time-series data for the size-100 challenge. It is noticeable that AUROC values for size-100 networks still remain high and look similar to their size-10 counterparts, while AUPR values in all rows have stayed lower than 10% except for size-100 Ecoli2. A similar decline is also observed in the results of Team 236. It can be seen that AUPR values can be impacted more strongly by the lower density of the size-100 networks, where the non-edges class severely outnumbers the edges class, rather than the choice of algorithm.</p><p>Additionally, for such difficult tasks, the number of available time series may be too small to get better AUROC and AUPR. Although there is no information on the structure of team 236's algorithm, its authors responded to the postcompetition DREAM3 survey stating that their method uses Bayesian models with an in-degree constraint (<ref type="bibr" target="#b29">Prill et al., 2010</ref>). This in-degree constraint may explain their particularly poor AUROC and AUPR performance for the high averagedegree networks Yeast2 and Yeast3 (average-degree values in Supplementary<ref type="figure" target="#tab_1">Table S1</ref>). Team 190 (<ref type="figure" target="#tab_1">Table 1</ref>) reported in the same survey that their method is also Bayesian with a focus on nonlinear dynamics and local optimization. This team did not submit predictions for the size-100 challenge. Interestingly, Supplementary<ref type="figure" target="#tab_2">Table S2</ref>highlights that as expected, performance depends on the number of the training time series and that the use of all the provided time series allows to reach significant gains. This illustrates that the number of observations required to get good performance is related to the complexity of the dynamics in the state space. The optimal condition to use this nonparametric approach is to visit as many different initial conditions as possible. In practice, the user will also pay attention that the number of time points in a single time series is larger than the number of considered genes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on IRMA datasets</head><p>OKVAR-Boost exhibits outstanding performance for the IRMA network (Supplementary<ref type="figure">Table S3</ref>). Specifically, for the switch-off series both AUROC and AUPR performance metrics exceed 80% (the inferred network is shown in Supplementary<ref type="figure" target="#fig_0">Fig. S1</ref>), while for the switch-on series they get a perfect score. The method clearly outperforms a Bayesian method using ordinary differential equations coupled with Gaussian processes (A ¨ ijoänd La¨hdesma¨kiLa¨hdesma¨La¨hdesma¨ki,Note: All the results are obtained using the 46 available time series. The numbers in boldface are the maximum values of each column. a</p><p>Ecoli2 has a strong star topology, which suggests a different consensus threshold for this network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boosting for nonlinear gene regulatory dynamics and interactions</head><p>2009) for the switch-on series and lags by a small margin for the switch-off series. The LASSO method gave fairly poor results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on T-cell activation dataset</head><p>The reconstructed regulatory network using OKVAR-Boost is given in Supplementary<ref type="figure" target="#fig_1">Figure S2</ref>. The following hyperparameters were used: 1 ¼ 1, 2 ¼ 1 and a threshold for the consensus network of 0.01. The resulting network contains 144 edges. As discussed in<ref type="bibr" target="#b30">Rangel et al., 2004</ref>, the main functional categories involved in T-cell response are cytokines, apoptosis and cell cycle. Some important regulating and regulated genes include FYB, GATA3 and CD 9 (inflammation), CASP 7 and 8 (apoptosis) and CDC2 (cell cycle). All of them appeared in the reconstructed network. In addition, the algorithm identified CCNA2 involved in the cell cycle (<ref type="bibr" target="#b25">Ody et al., 2000</ref>), SIVA involved in apoptosis (<ref type="bibr" target="#b12">Gudi et al., 2006</ref>) and MKBNIA, which has been associated with T-cell immunodeficiency (Lopez<ref type="bibr" target="#b18">Granados et al., 2008</ref>), as key hub genes. Overall, the algorithm identifies previously known ones in T-cell activation, but also emphasizes the role of some new ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. General scheme of OKVAR-Boost. The m th learner is run on the residuals of the global model on a random subset of time series, denoted S m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Mean squared error of OKVAR-Boost model for each gene using Ecoli2 datasets. (a) Size-10 Ecoli2 (b) Size-100 Ecoli2. The algorithm terminated after 14 and 4 iterations, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Boost (1 TS) 0.665 AE 0.088 0.272 AE 0.081 0.629 AE 0.095 0.466 AE 0.065 0.663 AE 0.037 0.256 AE 0.022 0.607 AE 0.049 0.312 AE 0.056 0.594 AE 0.072 0.358 AE 0.099 OKVAR-Boost (4 TS)OKVAR-Boost results using one time series [OKVAR-Boost (1 TS)] (average AE standard deviations) and the four available time series [OKVAR-Boost (4 TS)] are from consensus networks. The numbers in boldface are the maximum values of each column. a Consensus thresholds for Yeast2 and Yeast3 are different due to their higher density and average-degree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>First Authors. z Present address: CEA, LIST, 91191 Gif-sur-Yvette CEDEX, France ß The Author 2013. Published by Oxford University Press.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 1. AUROC and AUPR for OKVAR-Boost ( 1 ¼ 1, 2 ¼ 10 selected by Block Stability), LASSO, Team 236 and Team 190 (DREAM3 challenge) run on DREAM3 size-10 networks</figDesc><table>Size-10 
Ecoli1 
Ecoli2 
Yeast1 
Yeast2 a 
Yeast3 a 

AUROC 
AUPR 
AUROC 
AUPR 
AUROC 
AUPR 
AUROC 
AUPR 
AUROC 
AUPR 

OKVAR þ True B 
0.932 
0.712 
0.814 
0.754 
0.856 
0.494 
0.753 
0.363 
0.762 
0.450 
OKVAR-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 2.</figDesc><table>AUROC and AUPR for OKVAR-Boost ( 1 ¼ 0:001, 2 ¼ 0:1 selected by Block Stability), LASSO and Team 236 (DREAM3 challenge) run 
on DREAM3 size-100 networks 

Size-100 
Ecoli1 
Ecoli2 a 
Yeast1 
Yeast2 
Yeast3 

AUROC 
AUPR 
AUROC 
AUPR 
AUROC 
AUPR 
AUROC 
AUPR 
AUROC 
AUPR 

OKVAR-Boost 
0.718 
0.036 
0.772 
0.107 
0.729 
0.042 
0.650 
0.073 
0.643 
0.069 
LASSO 
0.519 
0.016 
0.512 
0.057 
0.507 
0.016 
0.530 
0.044 
0.506 
0.044 
Team 236 
0.527 
0.019 
0.546 
0.042 
0.532 
0.035 
0.508 
0.046 
0.508 
0.065 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Boosting for nonlinear gene regulatory dynamics and interactions at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">N.Lim et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="5"> DISCUSSION Gene regulatory inference has been cast as a feature selection problem in numerous works. For linear models, lasso penalized regression models have been effectively used for the task (Fujita et al., 2007; Perrin et al., 2003; Shojaie and Michailidis, 2010a). As an alternative to lasso regularization, an L 2 boosting algorithm was proposed in Anjum et al., 2009 to build a combination of linear autoregressive models that work for large networks. In nonlinear nonparametric modeling, random forests and their variants, extra-trees (Huynh-Thu et al., 2010), have recently won the DREAM5 challenge devoted to static data by solving p regression problems. Importance measures computed on the explanatory variables (genes) provide potential regulators for each of the candidate target gene. Compared with these approaches, OKVARBoost shares features with boosting and selected features of randomization-based methods, such as the use of a random subnetwork at each iteration. It exhibits fast convergence in terms of mean squared error due to the flexibilty of the OKVAR to capture nonlinear dynamics. Further, it uses an original and general way to extract the regulatory network through the Jacobian matrix of the estimated nonlinear model. The control of sparsity on the Jacobian matrix is converted into a constraint of the parameters of each base model h m , for which the independence matrix W m has been obtained by a conditional independence test. It should also be emphasized that prior information about the regulatory network can be easily incorporated into the algorithm by fixing known coefficients of the independence matrices used at each iteration. OKVAR-Boost also directly extends to additional observed time series from different initial conditions. Although we only showed one specific OKVAR model that is of special interest for network inference, other kernels can be defined and be more appropriate depending on the focus of the study.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning gene regulatory networks from gene expression measurements using non-parametric molecular kinetics</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>¨ Ijo¨, Ijo¨</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>La¨hdesma¨kila¨hdesma¨la¨hdesma¨ki</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2937" to="2944" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">A boosting approach to structure learning of graphs with and without prior knowledge</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Anjum</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2929" to="2936" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">How to infer gene networks from expression profiles</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Bansal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised Penalized Output Kernel Regression for Link Prediction</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Brouard</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Boosting with the L 2 loss</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bu¨hlmannbu¨hlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="324" to="339" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">A common set of gene regulatory networks links metabolism and growth inhibition</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Cam</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Cell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="399" to="411" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">A yeast synthetic network for in vivo assessment of reverseengineering and modeling approaches</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Cantone</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="172" to="181" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal multitask kernels</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Caponnetto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1615" to="1646" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">G</forename>
				<surname>Dietterich</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Multiple Classifier Systems. Springer</publisher>
			<biblScope unit="page" from="1" to="15" />
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">H</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling gene expression regulatory networks with the sparse vector autoregressive model</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Fujita</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient boosting for kernelized output spaces</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Geurts</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="289" to="296" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Siva-1 negatively regulates NF-kappaB activity: effect on T-cell receptor-mediated activation-induced cell death (AICD)</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Gudi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oncogene</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3458" to="3462" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">A computational framework for gene regulatory network inference that combines multiple methods and datasets</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Gupta</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring regulatory networks from expression data using tree-based methods</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">A</forename>
				<surname>Huynh-Thu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12776</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Gene regulatory network reveals oxidative stress as the underlying molecular mechanism of type 2 diabetes and hypertension</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Jesmin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Genomics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear functional regression: a functional RKHS approach</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Kadri</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Network discovery using nonlinear nonparametric modeling with operator-valued kernels. Online proceedings of Object, functional and structured data: towards next generation kernel-based methods</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Lim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2012 Workshop</title>
		<meeting><address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel mutation in NFKBIA/IKBA results in a degradation-resistant N-truncated protein and is associated with ectodermal dysplasia with immunodeficiency</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Lopez-Granados</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Mutat</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="861" to="868" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating realistic in silico gene networks for performance assessment of reverse engineering</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Marbach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="229" to="239" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Revealing strengths and weaknesses of methods for gene network inference</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Marbach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="6286" to="6291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Inferring cellular networks-a review</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Markowetz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Spang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Suppl. . 6</note>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Stability selection (with discussion)</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Meinshausen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bu¨hlmannbu¨hlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Series B</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">On learning vector-valued functions</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">A</forename>
				<surname>Micchelli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Pontil</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="177" to="204" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">On reverse engineering of gene interaction networks using time course data with repeated measurements</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Morrissey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2305" to="2312" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">MHC class II and c-kit expression allows rapid enrichment of T-cell progenitors from total bone marrow cells</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ody</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="3988" to="3990" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Gene networks inference using dynamic Bayesian networks</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Perrin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="138" to="148" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Suppl. . 2</note>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">N</forename>
				<surname>Politis</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Lim</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards a rigorous assessment of systems biology models: the DREAM3 challenges</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Prill</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9202</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling T-cell activation using gene expression profiling and state-space models</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rangel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1361" to="1372" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Hilbert spaces of operator-valued functions</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Senkene</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Tempel &apos;man</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Trans. Acad. Sci. Lith. SSR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="665" to="670" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<monogr>
		<title level="m" type="main">Optimization methods for l1-regularization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schmidt</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Penalized likelihood methods for estimation of sparse high dimensional directed acyclic graphs</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Shojaie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Michailidis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="519" to="538" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Discovering graphical granger causality using a truncating lasso penalty</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Shojaie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Michailidis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="517" to="523" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Comparison of evolutionary algorithms in gene regulatory network model inference</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Sıˆrbusıˆrbu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Advantages and limitations of current network inference methods</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">D</forename>
				<surname>Smet</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Marchal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Microbiol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="717" to="729" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Dialogue on reverse-engineering assessment and methods: the dream of high-throughput pathway inference</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Stolovitzky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. N Y Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">1115</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved reconstruction of in silico gene regulatory networks by integrating knockout and perturbation data</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">Y</forename>
				<surname>Yip</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8121</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Advances to Bayesian network inference for generating causal networks from observational biological data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3594" to="3603" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<monogr>
		<title level="m" type="main">Boosting for nonlinear gene regulatory dynamics and interactions</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>