
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ultra-fast FFT protein docking on graphics processors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">19 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">David</forename>
								<forename type="middle">W</forename>
								<surname>Ritchie</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Nancy—Grand Est</orgName>
								<orgName type="institution" key="instit2">LORIA</orgName>
								<address>
									<addrLine>615 Rue du Jardin Botanique</addrLine>
									<postCode>54506</postCode>
									<settlement>Vandoeuvre-lès-Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Vishwesh</forename>
								<surname>Venkatraman</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Nancy—Grand Est</orgName>
								<orgName type="institution" key="instit2">LORIA</orgName>
								<address>
									<addrLine>615 Rue du Jardin Botanique</addrLine>
									<postCode>54506</postCode>
									<settlement>Vandoeuvre-lès-Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ultra-fast FFT protein docking on graphics processors</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="page" from="2398" to="2405"/>
							<date type="published" when="2010">19 2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq444</idno>
					<note type="submission">Structural bioinformatics Advance Access publication August 4, 2010 Received on April 29, 2010; revised on July 19, 2010; accepted on July 31, 2010</note>
					<note>[13:41 28/8/2010 Bioinformatics-btq444.tex] Page: 2398 2398–2405 Associate Editor: Burkhard Rost Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Modelling protein–protein interactions (PPIs) is an increasingly important aspect of structural bioinformatics. However, predicting PPIs using in silico docking techniques is computationally very expensive. Developing very fast protein docking tools will be useful for studying large-scale PPI networks, and could contribute to the rational design of new drugs. Results: The Hex spherical polar Fourier protein docking algorithm has been implemented on Nvidia graphics processor units (GPUs). On a GTX 285 GPU, an exhaustive and densely sampled 6D docking search can be calculated in just 15 s using multiple 1D fast Fourier transforms (FFTs). This represents a 45-fold speed-up over the corresponding calculation on a single CPU, being at least two orders of magnitude times faster than a similar CPU calculation using ZDOCK 3.0.1, and estimated to be at least three orders of magnitude faster than the GPU-accelerated version of PIPER on comparable hardware. Hence, for the first time, exhaustive FFT-based protein docking calculations may now be performed in a matter of seconds on a contemporary GPU. Three-dimensional Hex FFT correlations are also accelerated by the GPU, but the speed-up factor of only 2.5 is much less than that obtained with 1D FFTs. Thus, the Hex algorithm appears to be especially well suited to exploit GPUs compared to conventional 3D FFT docking approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Protein docking is the task of calculating the 3D structure of a protein complex starting from unbound or model-built protein structures (<ref type="bibr" target="#b10">Halperin et al., 2002</ref>). As well as providing a useful technique to help study fundamental biomolecular mechanisms, using docking tools to predict protein–protein interactions (PPIs) is emerging as a promising complementary approach to rational drug design (<ref type="bibr" target="#b8">Grosdidier et al., 2009</ref>). Although proteins are intrinsically flexible, many protein docking algorithms begin by assuming the proteins to be docked are rigid, and they employ geometric hashing (<ref type="bibr" target="#b0">Bachar et al., 1993</ref>) or fast Fourier transform (FFT) correlation techniques (<ref type="bibr" target="#b12">Katchalski-Katzir et al., 1992</ref>) to find putative initial docking poses, which are then re-scored and refined using more sophisticated but more computationally * To whom correspondence should be addressed. expensive techniques (<ref type="bibr" target="#b35">Vajda and Kozakov, 2009</ref>). This article focuses on using graphics processor units (GPUs) to accelerate FFT-based approaches to the initial rigid body stage of a docking calculation. The FFT approach was first used to as a rapid way to calculate shape complementarity within a 3D Cartesian grid (<ref type="bibr">KatchalskiKatzir et al., 1992</ref>). It was later extended to include electrostatic interactions, e.g. FTDOCK (<ref type="bibr" target="#b5">Gabb et al., 1997</ref>) and DOT (<ref type="bibr" target="#b16">Mandell et al., 2001</ref>), or both electrostatic and desolvation contributions, e.g. ZDOCK (<ref type="bibr" target="#b3">Chen et al., 2003</ref>). However, because most FFT-based approaches use 3D Cartesian grid representations of proteins, they can only compute translational correlations, and these must be repeated over multiple rotational samples in order to cover the 6D search space. Recently, FFT techniques have been used to calculate correlations of multi-term knowledge-based potentials (<ref type="bibr" target="#b13">Kozakov et al., 2006;</ref><ref type="bibr" target="#b33">Sumikoshi et al., 2005</ref>). However, each cross-term in the potential requires a corresponding FFT to be calculated, and this adds to the overall computational expense. Furthermore, protein docking algorithms provide a useful way to study the nature of encounter complexes (<ref type="bibr" target="#b9">Grünberg et al., 2004</ref>), and they are beginning to be used as an in silico technique to help predict PPI networks (<ref type="bibr" target="#b19">Mosca et al., 2009;</ref><ref type="bibr" target="#b37">Yoshikawa et al., 2009</ref>). Both of these approaches are computationally intensive because they involve performing many cross-docking calculations. There is, therefore, a need to develop more efficient techniques to calculate PPIs. To address the main limitations of the Cartesian FFT approaches, we developed the spherical polar Fourier (SPF) technique, which uses rotational correlations (<ref type="bibr" target="#b22">Ritchie and Kemp, 2000</ref>) to accelerate the calculation. This reduces execution times to a matter of minutes on an ordinary workstation (). The related FRODOCK (fast rotational docking) approach has also recently demonstrated considerable performance gains compared to Cartesian gridbased FFT approaches (<ref type="bibr" target="#b6">Garzon et al., 2009</ref>). Nonetheless, further computational improvements are always desirable because greater speed may be traded for greater accuracy. In recent years, many scientific calculations have benefited from the very high arithmetic capabilities of modern GPUs (<ref type="bibr" target="#b21">Owens et al., 2007</ref>). Initially, it required considerable skill and knowledge of graphics programming techniques to transform a scientific calculation into a form that could be executed by dedicated pixel processing hardware on a GPU. However, with the advent of programmable GPUs and software development tools such as Brook (<ref type="bibr" target="#b2">Buck et al., 2004</ref>) and the CUDA (Common Unified Device Architecture) toolkit (http://www.nvidia.com/), it is now much easier to deploy scientific software on GPUs. For example, using GPUs to calculate protein and DNA sequence alignments can give speed-ups from at least a factor of 10 (<ref type="bibr" target="#b15">Manavski and Valle, 2008;</ref><ref type="bibr" target="#b26">Schatz et al., 2007</ref>) to over a 100 (<ref type="bibr" target="#b31">Suchard and Rambaut, 2009</ref>) compared to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein docking using GPUs</head><p>the same calculation on conventional CPUs. Similarly, GPUs can give speed-ups from around 10 to 100 for molecular dynamics simulations (<ref type="bibr" target="#b30">Stone et al., 2007;</ref><ref type="bibr" target="#b36">van Meel et al., 2008</ref>), up to a factor of 130 for quantum chemistry calculations (<ref type="bibr" target="#b34">Ufimtsev and Martínez, 2008</ref>), and more than a 200-fold increase for wavelet analyses of mass spectrometry data (<ref type="bibr" target="#b11">Hussong et al., 2009</ref>). GPUs have also been used to accelerate the calculation of protein accessible surface areas, for example, giving speed-up factors from around 100 to over 300, depending on the size of the protein (<ref type="bibr" target="#b4">Dynerman et al., 2009</ref>). In the context of protein docking, Sukhwani and Herbordt (2009) have implemented the PIPER program (<ref type="bibr" target="#b13">Kozakov et al., 2006</ref>) on a C1060 GPU, to achieve a speed-up of about a factor of 18 compared to a 2 GHz CPU. As far as we know, PIPER is so far the only docking program for which GPU performance results have been published. However, because PIPER uses 3D Cartesian FFT grids, and because processing each grid takes over 0.5 s on the GPU, a 6D docking calculation still takes several GPU-hours. Although Hex is already much faster on one CPU than the GPU version of PIPER, we were nonetheless encouraged by these earlier successes to explore whether similar speed-ups could be achieved by implementing SPF docking correlations on a GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GPUs</head><p>Although the details may vary, modern GPUs consist of several programmable multi-processors (MPs) each of which comprises multiple scalar processors (SPs), or 'cores', and a modest amount of fast on-chip, or 'local', memory. This local memory is often divided into read-only 'constant' memory for storing constant parameters, and re-writable 'shared memory' which may be accessed simultaneously by multiple SPs. Additionally, GPUs often have a large amount of external 'global' memory (in the order of hundreds megabytes or several gigabytes), which is accessible by each SP. Communication between the CPU and the GPU mainly involves copying data between the CPU and global GPU memory. The main GPU used here is an Nvidia GeForce GTX 285. This relatively high-end device has 30 MPs, 240 SPs, 1 Gb of global memory, and a clock speed of 1.48 GHz. Each SP can calculate at least one single precision floating point arithmetic operation (or 'flop') per clock cycle, and in favourable circumstances fused multiply–add instructions (three flops per cycle) can be used. Hence, this GPU is capable of at least 355 Gflops and has a theoretical maximum of 1065 Gflops. This is at least 100 times greater than that of a single core of a conventional CPU. GPUs manufactured by ATI, for example, have a somewhat different architecture, but have broadly similar computational performance. We chose to use Nvidia hardware in order to exploit the associated CUDA programming development tools and run-time libraries. The CUDA device architecture promotes a very fine-grained 'SIMT' (Simultaneous Instructions Multiple Threads) programming model in which individual threads of execution are responsible for manipulating a small number of closely related data elements. The SIMT model is implemented using small 'kernel' functions, which have a similar syntax to the C and C++ programming languages, and which are executed in parallel by the MPs. This model is well suited for performing simple and repetitive arithmetic operations such as those found in matrix multiplications and FFTs. For such calculations, it is natural to let one SIMT thread be responsible for calculating one element of an array, for example. Although the overall aim of CUDA is to provide an abstract way to program Nvidia GPUs, it is still necessary to understand the characteristics of these devices in order to achieve the best possible performance. For example, in contrast to conventional CPUs that often have several megabytes of fast cache memory, a considerable drawback of current GPUs is that they do not have any cache memory at all. This is significant because access to global memory is about 80 times slower than access to a register or shared memory. Hence, it is important to ensure that data traffic between the MPs and global memory is kept to a minimum, and that as much work as possible is done with data in the fast on-chip registers and shared memory. In the CUDA SIMT model, threads are given numerical index identifiers and are grouped into 'blocks' with consecutive indices. Blocks of threads may further be grouped into 'grids' of thread blocks. Thus, threads may be indexed using 1D, 2D or 3D indexing schemes, respectively. CUDA devices schedule and execute blocks of threads by dividing them into 'warps' of 32 threads. Each warp executes one instruction at a time, so maximum efficiency is achieved when all of the threads of a warp execute the same sequence of instructions. On the other hand, access to global memory is most efficient when the data is aligned in memory on even word boundaries, and when all threads in a half-warp (i.e. either the first or second group of 16 threads in a warp) access consecutive memory elements simultaneously, for example. When this occurs, the MP can coalesce multiple memory accesses into a single transaction (the precise conditions necessary for coalescing memory accesses are described in the CUDA Programming Guide: developer.nvidia.com/object/cuda_downloads.html). By running several warps concurrently, MPs can hide the latency of global memory provided that at least one warp always has sufficient data in registers or shared memory to operate on. To take into account the above characteristics and to optimize overall performance with a minimum of effort, we compiled from the CUDA@BULLET copy data between global and shared GPU memory using 16×16 tiles following the 'transpose' example in the Nvidia developers' toolkit.</p><p>All of these techniques were used here, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SPF correlations</head><p>The SPF docking approach has been described previously (<ref type="bibr" target="#b22">Ritchie and Kemp, 2000;</ref><ref type="bibr" target="#b24">Ritchie, 2005;</ref>). Nonetheless, a brief summary is given here in order to describe how it has been implemented on GPUs. The SPF approach begins with a voxel-based representation of protein shape similar to that originally described by<ref type="bibr" target="#b12">Katchalski-Katzir et al. (1992)</ref>. However, instead of directly calculating conventional 3D Cartesian FFTs, we use the voxel samples to encode the shapes of proteins as 3D polynomial expansions of orthonormal spherical polar basis functions. For example, theinterior volume of protein A (the 'receptor'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.W.Ritchie and V.Venkatraman</head><p>) is encoded as an expansion to order N using</p><formula>τ A (r) = nlm a τ nlm R nl (r)y lm (θ,φ),</formula><formula>(1)</formula><p>where r = (r,θ,φ) are 3D spherical polar coordinates, y lm (θ,φ) are normalized real spherical harmonic functions (<ref type="bibr" target="#b1">Biedenharn and Louck, 1981</ref>), R nl (r) are orthonormal Gauss-Laguerre radial basis functions (<ref type="bibr" target="#b24">Ritchie, 2005</ref>) and a τ nlm</p><p>are the expansion coefficients. The summation ranges over all subscript values that satisfy |m|≤l &lt; N. The default expansion order is N = 25. The volumes of a surface skin region around the receptor σ A (r) and the corresponding volumes on protein B (the 'ligand'), τ B (r) and σ B (r), are expressed in a similar way. Other properties such as electrostatic potential and charge density may also be encoded similarly. Thanks to the orthogonality of the basis functions, the expansion coefficients are easily determined by numerical integration on a 0.6 Å 3 grid (<ref type="bibr" target="#b22">Ritchie and Kemp, 2000</ref>). This correspond to performing a forward Fourier transform in conventional Cartesian grid-based FFT approaches. Although the SPF expansion coefficients have three subscripts, it is often convenient to store and manipulate them as compact 1D vectors, as illustrated in<ref type="figure" target="#fig_1">Figure 1</ref>. With this representation, shape complementarity may be written as a two-term overlap expression:</p><formula>C = σ A (r)τ B (r)+σ B (r)τ A (r) dV ,</formula><formula>(2)</formula><formula>where σ B (r) = σ B (r)−Qτ B (r)</formula><p>, and where Q is a penalty factor that penalizes interior–interior overlaps. We use Q = 11. Now, it can be shown that the SPF coefficients transform among themselves under rotation according to</p><formula>a nlm (α,β,γ) = m R (l) mm (α,β,γ)a nlm ,</formula><formula>(3)</formula><p>where (α,β,γ) are Euler rotation angles and R</p><formula>(l)</formula><p>mm (α,β,γ) are matrix elements of the real Wigner rotation matrices (<ref type="bibr" target="#b1">Biedenharn and Louck, 1981</ref>). In other words, the effect of rotating a protein may be simulated by transforming only its expansion coefficients according to Equation (3). Similarly, it can be shown that the effect of translating a protein by a distance R along the z-axis may be simulated by transforming its expansion coefficients according to:</p><formula>a nlm (R) = kj T (|m|) nl,kj (R)a kjm ,</formula><formula>(4)</formula><p>where the T</p><formula>(|m|)</formula><p>nl,kj (R) are the matrix elements for translations of the Gauss-Laguerre basis functions (<ref type="bibr" target="#b24">Ritchie, 2005</ref>). Our overall strategy for calculating docking correlations, therefore, is to calculate lists of rotated and translated coefficient vectors for the receptor and ligand proteins, and to evaluate Equation (2) for all possible pairs of such vectors. However, in order to accelerate the calculation using FFT techniques, it is convenient to use both real and complex expansion coefficients. For example, Equation (1) can equally be written as</p><formula>τ A (r) = nlm A τ nlm R nl (r)Y lm (θ,φ),</formula><formula>(5)</formula><p>where Y lm (θ,φ) are the complex spherical harmonics and A τ nlm are the corresponding complex expansion coefficients. It can be shown that the real and complex coefficients are related according to</p><formula>A τ nlm = m a τ nlm U (l) m m ,</formula><formula>(6)</formula><p>where U (l) is a unitary transformation matrix (<ref type="bibr" target="#b1">Biedenharn and Louck, 1981</ref>). Hence, by taking complex linear combinations of pairs of property vectors</p><formula>A nlm = m (a τ nlm +ia σ nlm )U (l) m m B nlm = m (b σ nlm +ib τ nlm )U (l) m m ,</formula><formula>(7)</formula><p>the overlap Equation (2) may be calculated as</p><formula>C = Re nlm A * nlm B nlm ,</formula><formula>(8)</formula><p>where i = √ −1 and the asterisk denotes complex conjugation. The in vacuo electrostatic interaction energy may be calculated in a similar way (<ref type="bibr" target="#b22">Ritchie and Kemp, 2000</ref>). Furthermore, if A nlm (R,β A ,γ A ) and B nlm (β B ,γ B ) represent translated and rotated complex coefficients of the receptor and ligand, respectively, the overlap score as a function of the remaining twist angle degree of freedom, α B , may be calculated as</p><formula>C(α B ) = m e −imα B S m (R,β A ,γ A ,β B ,γ B ), (9)</formula><p>where the coefficients S m are given by</p><formula>S m (R,β A ,γ A ,β B ,γ B ) = nl A * nlm (R,β A ,γ A )B nlm (β B ,γ B ).</formula><formula>(10)</formula><p>Because Equation (9) has the form of a 1D Fourier series in the m index, the calculation over multiple rotational samples for α B may be performed efficiently using a 1D FFT. We normally use an FFT length of 64, which gives angular increments of 360 @BULLET /64 = 5.625 @BULLET. For the remaining rotation angles, near-regular patterns of (β,γ) angles are generated from icosahedral tessellations of the sphere (<ref type="bibr" target="#b22">Ritchie and Kemp, 2000</ref>). For example, the default icosahedral tessellation of 812 vertices gives angular samples with an average separation of ∼ 7.5 @BULLET. To complete a docking search, Equation (9) is evaluated over multiple pairs (812 × 812) of (β,γ) molecular rotations, and the entire calculation is repeated over a range of intermolecular separations, R. We normally use 50 translational steps of 0.8 Å. This generates in the order of two billion (2×10 9 ) trial docking poses. By rewriting Equations (3) and (9) to expose different Euler rotational angles as complex exponentials, it is possible to express the overall calculation as a list of 3D or even 5D FFTs. For example, a 3D rotational FFT can be calculated using</p><formula>C(α B ,β B ,γ B ) = muv e −i(mα B +2uβ B +vγ B ) S m (R,β A ,γ A ), (11)</formula><p>where the S m coefficients are now given by</p><formula>S m (R,β A ,γ A ) = nl A * nlm (R,β A ,γ A )B nlv um lv ,</formula><formula>(12)</formula><p>and where um lv is a rotational scaling factor (). Equation (11) is normally evaluated using a 3D FFT grid of 48×24×48 elements, which gives rotational steps of 7.5 @BULLET for each of the three ligand rotation angles (α B ,β B ,γ B ). In each case, outer iterations over the remaining degree(s) of freedom must be performed to cover the 6D search space. Fivedimensional rotational FFTs may be calculated in a similar way. It might be expected that 5D FFTs would be faster than 3D and 1D FFTs, but we find that this is often not the case in practice due to the large memory requirement Page: 2401 2398–2405</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein docking using GPUs</head><formula>, S(R,β A ,γ A ,β B ,γ B</formula><p>). The S coefficients are then transformed into an array of docking scores using a batch of 1D FFTs<ref type="bibr">[Equation (9)]</ref>, and the result is scanned to find the best score for each twist angle, α B. of the 5D FFT grid. Hence, only the above 1D and 3D FFT schemes are considered further here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementing SPF correlations on the GPU</head><p>From previous experience, we knew a priori that the rate-limiting steps in the CPU-based Hex docking calculations are the receptor translation [Equation (4)] and pair-wise coefficient multiplication steps [Equations (10) and</p><p>(12)], all of which involve double summations over the n and l subscripts. Although calculating the initial expansion coefficients and applying the (β,γ) rotations to the receptor and ligand coefficients costs several seconds of CPU time, these are not rate-limiting steps because they can be performed before the main iteration over pairs of receptor and ligand poses. We, therefore, wrote GPU kernels only to implement Equations (4), (10) and (12), and we used the Nvidia cuFFT library for the 1D and 3D FFTs to calculate Equations (9) and (11), respectively. Using the GPU programming strategies listed in Section 2.1, each pair of nl subscripts are first mapped to single index, p, in which successive data elements are arranged in order of the l and then n subscripts, and where the total number of elements, P, is rounded up to a multiple of 16. This scatters the elements of a compact coefficient vector into a sparse 2D array of dimension P ×(2L +1) elements, where L = N −1. This is illustrated in the upper part of<ref type="figure" target="#fig_2">Figure 2</ref>. Re-indexing M rotated coefficient vectors in this way gives a 3D block of P ×M ×(2L +1) coefficients in sparse format (<ref type="figure" target="#fig_2">Fig. 2</ref>). Similarly, the SPF translation matrix elements are re-indexed to give a sparse 3D array of P ×P ×(L +1) elements. This allows all M coefficient vectors to be translated together by performing 2L +1 matrix–matrix multiplications, as illustrated in the lower part of<ref type="figure" target="#fig_2">Figure 2</ref>. This can be done very efficiently in a GPU kernel by using one 16×16 block of threads to calculate each tile of the result matrix. Furthermore, it is straightforward to make this kernel skip completely any tile consisting entirely of zeros. To complete the 1D FFT docking scheme, two further GPU kernels were implemented. The first of these cross-multiplies and zero-pads pairs of receptor and ligand coefficient according to Equation (10). This gives a list of N ×M data vectors of length 64 which can be evaluated as a batch of 1D FFTs using the cuFFT library. The output from the FFT is an array of docking scores, or pseudo-energies, as a function of the intermolecular twist angle α B. This array is scanned by the second kernel to identify the lowest energy for each twist angle. Finally, the resulting list of poses and energies is copied back to main CPU memory. These steps are illustrated in<ref type="figure" target="#fig_3">Figure 3</ref>. The above sequence of operations is repeated for each translational step of the 6D search. The 3D FFT scheme generally follows the same execution path, but only a single unrotated ligand coefficient vector has to be copied to the GPU to calculate the array of 3D FFT coefficients<ref type="bibr">[Equation (12)]</ref>. This array is then passed to the 3D cuFFT function, and the best poses and energies are Page: 2402 2398–2405copied back to CPU main memory, as before. Faster implementations of the 3D FFT have been proposed for GPUs (<ref type="bibr" target="#b7">Govindraju et al., 2008;</ref><ref type="bibr" target="#b20">Nukada et al., 2008</ref>), but we have not explored these here because significantly better docking performance is obtained using 1D FFTs, as shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.W.Ritchie and V.Venkatraman</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-threaded implementations</head><p>All calculations have been implemented using 'thread-safe' programming techniques using the Posix 'pthreads' and Windows thread libraries for Linux and Windows systems, respectively. Thus, multiple GPUs and CPU cores may be used simultaneously on most current workstations. The results presented here refer mainly to a contemporary workstation with a quad-core 2.3 GHz Xeon CPU and one GTX 285 GPU. Some overall timing results are also given for several other Nvidia devices for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Forward SPF transforms are not rate-limiting</head><p>In the SPF approach, protein shapes are sampled just once by an initial forward transform using numerical integration. Thereafter, assuming that different complexes are docked with the same search parameters, as is normally the case in Hex, all subsequent SPF calculations are independent of the size and nature of the proteins because they manipulate and transform SPF coefficient vectors of the same length and in the same way. Hence, calculation times for SPF docking correlations are practically constant for all complexes for a given FFT sampling scheme (1D or 3D).<ref type="figure" target="#tab_1">Table 1</ref>shows the extent to which the initial sampling times (and, therefore, also overall execution times) vary according to the sizes of the proteins. Because this initialization step is currently not rate-limiting for docking, it has not been implemented on the GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GPUs give almost identical numerical results</head><p>All FFT calculations were performed in double precision using the Intel Math Kernel Library (MKL) on the CPU and in single precision using the cuFFT library on the GPU. Hence, some small numerical differences between the CPU and GPU results are to be expected. From visual inspection of the results for docking the Kallikrein A / BPTI example, we find that the calculated GPU and CPU docking energies agree to within at least four decimal digits, or equivalently to within at least 0.05 kJ/mol for each pose (data not shown). However, as expected, some small differences between the 1D and 3D FFT schemes were observed, because the two schemes use fundamentally different rotational sampling techniques. To quantify the effect of these differences in more detail, blind docking was performed on the 63 'rigid body' complexes of the Protein Docking Benchmark (<ref type="bibr" target="#b17">Mintseris et al., 2005</ref>). Full details of the results are presented in Supplementary Materials.<ref type="figure" target="#tab_2">Table 2</ref><ref type="figure" target="#tab_1">Table 1</ref>. This shows that the GPU and CPU calculations often give identical or very similar ranks to the first pose found within 10 Å RMS of the complex, although there are sometimes some fluctuations in the ranks and poses of less highly ranked predictions.<ref type="figure" target="#tab_2">Table 2</ref>also shows that the 3D FFT scheme gives marginally better results than the 1D scheme. We believe this is because the 3D FFT scheme tends to over-sample rotation space near the poles, and hence has a slightly better chance of sampling a near-native poses than the more regular icosahedral sampling pattern used in the 1D FFT scheme. Clearly, using different rotational and translational sampling densities for either FFT scheme would cause comparable fluctuations in the results. Overall, these tables show that the GPU and CPU calculations give almost identical numerical results, and that the effect of any arithmetic differences is very small compared to using different orientational sampling patterns.<ref type="figure" target="#fig_4">Figure 4</ref>shows the overall docking correlation rates at different polynomial expansion orders for both the 1D and 3D FFT calculation schemes described above. This shows that a GPU can calculate the 1D FFT docking scheme significantly faster than the same calculation on the CPU, and indeed also significantly faster than the 3D FFT scheme on both the GPU and CPU. As might be expected, 3D FFT calculation rates are less sensitive to the polynomial order than the 1D FFTs because they require less explicit matrix arithmetic and they can benefit more from the O(N logN) nature of the FFT. However, for the relatively low expansion orders used here, 3D FFTs on the GPU are not substantially faster than using a single CPU core. On the other hand, because Hex performs an initial scan of the search space using N = 18, and because only a small fraction of the remaining poses are re-scored using N = 25, the overall benefit of using a GPU to calculate the 1D docking scheme is dramatic. As shown in<ref type="figure" target="#fig_4">Figure 4</ref>, the 1D FFT scheme can score 236 million poses per second using N = 18 on the GPU, and 104 million poses per Page: 2403 2398–2405<ref type="figure">5</ref>. Total execution times for exhaustive Hex docking runs with default search parameters on a variety of GPU devices compared to using from one to four CPU cores simultaneously. The GPU devices have the following specifications: GTX 285: 240 cores, 1.48 GHz, 1 GB memory (workstation); GTX 9800: 128 cores, 1.7 GHz, 512 MB memory (workstation); GTS 260M: 112 cores, 1.37 GHz, 1 GB memory (laptop); 9400M: 16 cores, 1.4 GHz, 256 MB memory (laptop). The 9400M device has insufficient memory for shape plus electrostatic correlations. second using N = 25. These rates correspond to speed-up factors of 100 and 130, respectively, compared to a single CPU core. Although the above speed-ups are impressive, overall execution times are reduced by smaller factors than these because it is not feasible to implement all of the steps of a docking calculation on the GPU. Furthermore, because current CPUs typically have up to four identical cores, it is perhaps fairer to compare one GPU with four CPU cores. Although other protein docking programs such as DOT (<ref type="bibr" target="#b16">Mandell et al., 2001</ref>), PIPER (<ref type="bibr" target="#b13">Kozakov et al., 2006</ref>) and ZDOCK 3.0.1 (<ref type="bibr" target="#b18">Mintseris et al., 2007</ref>) can distribute the workload over multiple nodes in a compute cluster using, for example, the MPI message passing library (http://www.open-mpi.org/), to our knowledge Hex is currently the only protein docking program that uses multi-threading techniques to exploit multi-core processors.<ref type="figure">Figure 5</ref>compares the overall execution times of a typical Hex docking run on a variety of GPU devices with using from one to four CPU cores simultaneously. In all cases, adding electrostatics to the scoring function costs little because it is only calculated for the best 25 000 poses in the final re-scoring step.Figure 5 also shows that using two CPU cores nearly doubles the overall speed, but using four CPU cores gives only about a 3-fold speed-up. On the other hand, for the 1D FFT scheme, using one GPU core is still at least 10 times faster than using four CPU cores together, which is clearly a significant improvement. Furthermore, on our GPU-based server (<ref type="bibr" target="#b14">Macindoe et al., 2010</ref>), we find that using two GPUs together is twice as fast as one GPU. However, because the 1D FFT scheme is so fast, much of this gain is masked by file transfer and network overheads on the web server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Over 100-fold GPU speed-up for 1D correlations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein docking using GPUs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Speed comparison with ZDOCK and PIPER</head><p>To allow a more direct comparison between the performance of the SPF representation and conventional Cartesian FFT grid-based docking approaches,<ref type="figure" target="#tab_3">Table 3</ref>compares overall docking times for Hex with the execution time for ZDOCK 3.0.1 measured on the same CPU along with CPU and GPU execution times for PIPER, which have been estimated from the timings given by<ref type="bibr" target="#b32">Sukhwani and Herbordt (2009)</ref>using the same 'dense' rotational sampling as ZDOCK (54 000 ligand rotational steps of ∼6 @BULLET ). The ZDOCK dense sampling mode is similar to the default Hex sampling scheme (812×64 = 51 968 ligand rotations). However, it should be noted that the ZDOCK and PIPER grid sizes of (92×1.2Å) 3 and (128× 1.0Å) 3 , respectively, are both larger than the default translational step size used in Hex (0.8 Å). Furthermore, ZDOCK 3.0.1 and PIPER both employ multi-term potentials derived from 12 residue types in ZDOCK (<ref type="bibr" target="#b18">Mintseris et al., 2007</ref>) and using up to 22 cross-terms in PIPER (<ref type="bibr" target="#b13">Kozakov et al., 2006</ref>), whereas Hex uses a much simpler two-term shape complementarity model with an optional two-term in vacuo electrostatic contribution. Bearing these similarities and differences in mind,<ref type="figure" target="#tab_3">Table 3</ref>shows that using the Hex 1D FFT scheme on a high-end GPU is about 475 times faster (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.W.Ritchie and V.Venkatraman</head><p>1D FFT scheme gives the best overall performance, being over 45 times faster (676/15) than the corresponding calculation on a single CPU core, whereas the GPU accelerates the 3D FFT scheme by only a very modest factor of about 2.5 (224/84). Given that one complex FFT calculation can correlate two shape terms in Hex or two potentials in ZDOCK or PIPER, the underlying difference in speed between the 1D SPF (GPU) and 3D Cartesian (CPU) approaches may be estimated to be about a factor of 80 for ZDOCK (i.e. 7172/15/6) and 2840 for PIPER (i.e. 468 625/15/11) per pair-wise property correlation. This clearly indicates that PIPER contains some very expensive steps compared to ZDOCK. Comparing the Hex 3D SPF CPU time with the 3D FFT Cartesian calculation in ZDOCK in a similar way gives a SPF/Cartesian speed-up factor of about 14 (i.e. 7172/84/6). Hence, the relatively low GPU/CPU factor of 2.5 for the Hex 3D SPF correlations indicates that the Hex 3D CPU implementation is in fact very well optimized. Overall,<ref type="figure" target="#tab_3">Table 3</ref>shows that the fastest GPU scheme (1D SPF) is about 15 times faster (224/15) than the fastest CPU scheme (3D SPF) which is itself about 32 times faster (7172/224) than the 3D Cartesian-based FFT in ZDOCK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">1D SPF correlations are well suited for GPUs</head><p>Taking into account that Sukhwani and Herbordt (2009) compared PIPER using cuFFT on the GPU with FFTW (http://www.fftw.org) on the CPU (which is known to be slower for FFTs than MKL), our 3D SPF results also show that GPUs do not give substantial speedups for 3D FFT calculations. This is because 3D FFT algorithms require multiple passes through the data volume, and on the GPU this exposes the latency of repeatedly accessing global memory without the benefit of fast cache memory. On the other hand, because multiple 1D FFTs can be processed in a single pass over global GPU memory, it follows that the SPF 1D FFT scheme is especially well suited to exploit current GPU architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>The Hex 1D and 3D FFT docking schemes have been implemented on CUDA GPUs. Although the CPU version of Hex is already much faster than conventional Cartesian grid-based FFT docking algorithms, GPU-based correlations using the 1D FFT scheme are accelerated by at least a factor of 100 compared to a single CPU core, and a very satisfactory 45-fold overall speed-up is achieved for the 1D FFT scheme. This corresponds to an 15-fold speed-up compared to the 3D FFT scheme on the CPU. However, only a very modest GPU/CPU speed-up factor of about 2.5 is obtained for the 3D FFT scheme. This shows that the Hex 1D FFT docking scheme is especially well suited to exploit current GPU architectures. On a contemporary high-end GPU, the 1D FFT scheme allows an exhaustive protein docking calculation to be completed in just 15 s, which is at least two orders of magnitude faster than leading conventional Cartesian-based docking algorithms such as ZDOCK and PIPER. Thus, for the first time, exhaustive FFT-based protein docking may now be carried out in interactive time-scales using a modern GPU. This algorithmic improvement will facilitate the use of docking techniques to help study PPIs and PPI networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Schematic illustration of the calculation and storage of the SPF expansion coefficients as compact 1D coefficent vectors indexed by three subscripts, nlm. Overlines represent negative subscript values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Re-ordering the expansion coefficient vectors into sparse arrays suitable for tiled matrix multiplication on the GPU. Here, M rotated receptor coefficient vectors are re-indexed to give a 3D array of 2L +1 planes of dimension M ×P. Similarly, the translation matrix elements are re-indexed to give L +1 planes of dimension P ×P. This allows multiple coefficient vectors to be translated efficient using tile-based matrix multiplications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. Calculating multiple 1D FFTs on the GPU for multiple pairs of transformed receptor and ligand coefficient vectors. For each translation step R, M translated and rotated receptor coefficient vectors and N rotated ligand coefficient vectors are combined using Equation (10) to give M ×N vectors of twist angle coefficients, S(R,β A ,γ A ,β B ,γ B ). The S coefficients are then transformed into an array of docking scores using a batch of 1D FFTs [Equation (9)], and the result is scanned to find the best score for each twist angle, α B .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.4.</head><figDesc>Fig. 4. Docking correlation rates (millions of poses per second) using the 1D and 3D correlation schemes at various polynomial expansion orders. The GPU rates include all data transfers between the GPU and CPU, the time required to re-index and translate the coefficients, and the time spent in the cuFFT library. All rates exclude the costs of calculating the initial forward transform and reading the translation matrices from disc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.</head><figDesc>Fig. 5. Total execution times for exhaustive Hex docking runs with default search parameters on a variety of GPU devices compared to using from one to four CPU cores simultaneously. The GPU devices have the following specifications: GTX 285: 240 cores, 1.48 GHz, 1 GB memory (workstation); GTX 9800: 128 cores, 1.7 GHz, 512 MB memory (workstation); GTS 260M: 112 cores, 1.37 GHz, 1 GB memory (laptop); 9400M: 16 cores, 1.4 GHz, 256 MB memory (laptop). The 9400M device has insufficient memory for shape plus electrostatic correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Programming Guide and associated code examples a list of simple strategies for porting code to the GPU: @BULLET only implement rate-limiting calculations on the GPU; @BULLET perform non-trivial initializations on the CPU and copy the values to the GPU; @BULLET store commonly used constants in the 'constant' memory area of the GPU;</figDesc><table>@BULLET store complex numbers as consecutive pairs of single precision data 
elements; 

@BULLET use the CUDA '__align__' macro to force data structures to begin on 
8-byte boundaries; 

@BULLET re-structure complicated data structures as regular arrays; 

@BULLET round up array dimensions to multiples of 16; 

@BULLET re-write a group of matrix–vector multiplications as one matrix–matrix 
multiplication; 
@BULLET avoid using conditional statements inside loops; 

@BULLET associate one array subscript with one thread index; 

@BULLET access multi-dimensional arrays in natural subscript order; 

@BULLET copy data between CPU and GPU memory in large chunks; 

@BULLET perform matrix operations using 16×16 tiles of data following the 
'matrixMul' example in the Nvidia developers' toolkit; 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 1. Typical SPF initialization times in seconds for some example complexes using expansions to order N = 25 on a 2.3 GHz workstation</figDesc><table>Receptor 
#residues Ligand 
#residues 1×CPU 4×CPU 

Kallikrein A 233 
BPTI 
58 
7.5 
2.0 
HyHel-5 Fv 
215 
Lysozyme 130 
8.5 
2.3 
TGF-β 
331 
FKB12 
108 
11.7 
3.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Mean rank, RMS deviation, and number of hits obtained for exhaustive unbound–unbound docking of the 63 'rigid body' complexes of the Docking Benchmark (version 2)</figDesc><table>3D CPU 
3D GPU 
1D CPU 
1D GPU 
Rank (RMS) Hits Rank (RMS) Hits Rank (RMS) Hits Rank (RMS) Hits 

147 (7.8) 6.4 
147 (7.9) 6.2 
166 (7.9) 5.8 
165 (7.9) 5.6 

This table summarizes the results presented in Supplementary Table 1. For each FFT 
docking scheme, the overall results are listed as the mean rank, the average ligand C α 
RMS deviation with respect to the crystal structure of the complex, and the average 
number of 'hits' found within the top 1000 docking poses. Any pose for which the 
ligand RMS is within 10 Å of the complex is considered to be a 'hit'. Means of ranks 
were calculated using the mean log rank formula of Ritchie et al. (2008). All docking 
runs used default search parameters with a steric scan using N = 18 followed by shape 
plus electrostatic re-scoring using N = 25. 

summarizes these results in terms of the mean rank and average 
ligand C α root-mean-squared (RMS) deviation from the complex 
of the first 'hit' (here defined as a pose within 10 Å RMS of the 
complex) found within the first 1000 solutions. These values show 
that there is very little overall difference between the GPU and CPU 
calculations. This may be confirmed by closer examination of the 
individual docking results in Supplementary </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 3. Total execution times in seconds for ZDOCK, PIPER and Hex ZDOCK PIPER a PIPER</figDesc><table>a 
Hex 
Hex 
Hex b 
1×CPU 
1×CPU 
1×GPU 1×CPU 4×CPU 1×GPU 

3D 
7172 
468 625 
26 372 
224 
60 
84 
(3D) c (1195) 
(42 602) (2398) 
224 
60 
84 
1D 
– 
– 
– 
676 
243 
15 

In this table, the ZDOCK and Hex values are measured execution times for 
unconstrained exhaustive docking of the Kallikrein A / BPTI complex, and are tabulated 
according to the number of CPUs and GPUs used. Dense rotational sampling was used 
in ZDOCK with a Cartesian grid size of (92Å) 3 . A comparable rotational sampling 
density was used in Hex, as described in the main text. 
a The PIPER times are estimated for 54 000 rotational sample steps (as in ZDOCK 

dense sampling) using the per-rotation times quoted in Table 1 of Sukhwani and 
Herbordt (2009), i.e. 2.0 GHz CPU: 9.98 s/rotation and C1060 GPU: 0.556 s/rotation, 
for a receptor grid size of 128 3 and a ligand grid of 32 3 . The PIPER CPU time given 
here has been scaled to that of a 2.3 GHz processor, and the GPU time (C1060: 240 
cores, 1.3 GHz) has been scaled to that of a GTX 285 (240 cores, 1.48 GHz). 
b Here, only the GPU is used in the docking search, although the initialization step uses 

four CPU cores (see Table 1). 
c Times given in brackets for ZDOCK and PIPER have been corrected from the 

times measured for 12-term (ZDOCK) and estimated for 22-term (PIPER) runs to a 
hypothetical two-term potential like the two-term pseudo-energy used in Hex. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>7172/15) than ZDOCK 3.0.1, about 31 200 times faster (468 625/15) than PIPER on a single 2.3 GHz CPU core, and about 1750 times faster (26 372/15) than PIPER on a comparable GPU. Table 3 also shows the overall Hex execution times for different numbers of CPU cores. This shows that using one GPU to calculate Page: 2404 2398–2405</figDesc><table></table></figure>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the Nvidia Professor Partner Programme for the gift of a graphics card.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">A computer vision based technique for 3D sequence-independent structural comparison of proteins</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Bachar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Eng</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="279" to="288" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title level="m" type="main">Angular Momentum in Quantum Physics</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">C</forename>
				<surname>Biedenharn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">C</forename>
				<surname>Louck</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Brook for GPUs: stream computing for graphics hardware</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Buck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="777" to="786" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">ZDOCK: an initial-stage protein-docking algorithm</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct. Funct. Bioinform</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="80" to="87" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">CUSA and CUDE: GPU-accelerated methods for estimating solvent accessible surface area and desolvation</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Dynerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="523" to="537" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Modelling protein docking using shape complementarity, electrostatics and biochemical information</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">A</forename>
				<surname>Gabb</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">272</biblScope>
			<biblScope unit="page" from="106" to="120" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">FRODOCK: a new approach for fast rotational protein-protein docking</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">I</forename>
				<surname>Garzon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2544" to="2551" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">High performance discrete Fourier transforms on graphics processors</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">K</forename>
				<surname>Govindraju</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM/IEEE conference on Supercomputing</title>
		<meeting>the 2008 ACM/IEEE conference on Supercomputing<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Computer applications for prediction of protein-protein interactions and reational drug design</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Grosdidier</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. App. Bioinf. Chem</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="101" to="123" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Complementarity of structure ensembles in protein-protein docking</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Grünberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Principles of docking: An overview of search algorithms and a guide to scoring functions</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Halperin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct. Funct. Genet</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="409" to="443" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Highly accelerated feature detection in proteomics data sets using modern graphics processing units</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Hussong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1937" to="1943" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Molecular surface recognition: determination of geometric fit between proteins and their ligands by correlation techniques</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Katchalski-Katzir</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci</title>
		<meeting>. Natl Acad. Sci</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="2195" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">PIPER: an FFT-based protein docking program with pairwise potentials</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Kozakov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct. Funct. Bioinform</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="392" to="406" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">HexServer: an FFT-based protein docking server powered by graphics processors</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Macindoe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="445" to="449" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">CUDA-compatible GPU cards as efficient hardware accelerators for Smith-Waterman sequence alignment</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">A</forename>
				<surname>Manavski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Valle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Protein docking using continuum electrostatics and geometric fit</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">G</forename>
				<surname>Mandell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="105" to="113" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Protein-protein docking benchmark 2.0: an update</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mintseris</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct. Funct. Bioinform</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="214" to="216" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrating statistical pair potentials into protein complex prediction</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mintseris</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct. Funct. Bioinform</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="511" to="520" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Pushing structural information into the yeast interactome by high-throughput protein docking experiments</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Mosca</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1000490</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Bandwidth intensive 3-D FFT kernel for GPUs using CUDA</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Nukada</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM/IEEE conference on Supercomputing</title>
		<meeting>the 2008 ACM/IEEE conference on Supercomputing<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of general-purpose computation on graphics hardware</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">D</forename>
				<surname>Owens</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="80" to="113" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Protein docking using spherical polar Fourier correlations</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Ritchie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">J L</forename>
				<surname>Kemp</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct. Funct. Genet</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="178" to="194" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Accelerating and focusing protein-protein docking correlations using multi-dimensional rotational FFT generating functions</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Ritchie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1865" to="1873" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">High-order analytic translation matrix elements for real-space sixdimensional polar Fourier correlations</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Ritchie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Cryst</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="808" to="818" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Recent progress and future directions in protein-protein docking</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Ritchie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Protein Pept. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">High-throughput sequence alignment using graphics processors</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">C</forename>
				<surname>Schatz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">474</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="41" to="69" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>btq444. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2405" to="2398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<monogr>
		<title level="m" type="main">Protein docking using GPUs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Accelerating molecular modeling applications with graphics processors</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Stone</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Chem</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2618" to="2640" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Many-core algorithms for statistical phylogenetics</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Suchard</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rambaut</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1370" to="1376" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">GPU acceleration of a production molecular docking code</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Sukhwani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">C</forename>
				<surname>Herbordt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPGPU-2: Proceedings of 2nd Workshop on General Purpose Processing on Graphics Processing Units</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">A fast protein-protein docking algorithm using series expansions in terms of spherical basis functions</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Sumikoshi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Inform</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantum chemistry on graphical processor units. 1. Strategies for two-electron integral evaluation</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">S</forename>
				<surname>Ufimtsev</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">J</forename>
				<surname>Martínez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="222" to="231" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Convergence and combination of methods in proteinprotein docking</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Vajda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Kozakov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Struct. Biol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="164" to="170" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Harvesting graphics power for MD simulations</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Van Meel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Simul</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="259" to="266" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving the accuracy of an affinity prediction method by using statistics on shape complementarity between proteins</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Yoshikawa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="693" to="703" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>