
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining AUC-based biomarker ensemble with an application on gene scores predicting low bone mineral density</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">. 21 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">X</forename>
								<forename type="middle">G</forename>
								<surname>Zhao</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Bone and Joint Surgery</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xi&apos;an Medical University</orgName>
								<address>
									<addrLine>Xi&apos;an 710077</addrLine>
									<settlement>Shaanxi Province</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">W</forename>
								<surname>Dai</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biostatistics</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Y</forename>
								<surname>Li</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biostatistics</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">L</forename>
								<surname>Tian</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Health Research and Policy</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94301</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining AUC-based biomarker ensemble with an application on gene scores predicting low bone mineral density</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="page" from="3050" to="3055"/>
							<date type="published" when="2011">. 21 2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr516</idno>
					<note type="submission">Received on May 10, 2011; revised on August 29, 2011; accepted on September 1, 2011</note>
					<note>[10:59 3/10/2011 Bioinformatics-btr516.tex] Page: 3050 3050–3055 Associate editor: John Quackenbush Contact: lutian@stanford.edu Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: The area under the receiver operating characteristic (ROC) curve (AUC), long regarded as a &apos;golden&apos; measure for the predictiveness of a continuous score, has propelled the need to develop AUC-based predictors. However, the AUC-based ensemble methods are rather scant, largely due to the fact that the associated objective function is neither continuous nor concave. Indeed, there is no reliable numerical algorithm identifying optimal combination of a set of biomarkers to maximize the AUC, especially when the number of biomarkers is large. Results: We have proposed a novel AUC-based statistical ensemble methods for combining multiple biomarkers to differentiate a binary response of interest. Specifically, we propose to replace the non-continuous and non-convex AUC objective function by a convex surrogate loss function, whose minimizer can be efficiently identified. With the established framework, the lasso and other regularization techniques enable feature selections. Extensive simulations have demonstrated the superiority of the new methods to the existing methods. The proposal has been applied to a gene expression dataset to construct gene expression scores to differentiate elderly women with low bone mineral density (BMD) and those with normal BMD. The AUCs of the resulting scores in the independent test dataset has been satisfactory. Conclusion: Aiming for directly maximizing AUC, the proposed AUC-based ensemble method provides an efficient means of generating a stable combination of multiple biomarkers, which is especially useful under the high-dimensional settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Given that there are multiple biomarkers and a binary response of interest (e.g. case and control), it is often of substantial interest to combine the biomarkers to form a 'strong' scoring system for the differentiation of cases from controls. While the choice of the predictive measure is not unique, the most appealing choice is the area under the receiver operating characteristic (ROC) curve (AUC) in the case–control study (<ref type="bibr" target="#b11">Pepe, 2003;</ref><ref type="bibr" target="#b17">Zhou et al., 2002</ref>). * To whom correspondence should be addressed. For finite samples, AUC is simply the non-parametric two-sample Mann–Whitney U test statistics. Unlike the measures such as misclassification rate, the AUC reflects the intrinsic predictive value of a score in that it does not depend on the prevalence of the cases and thus is invariant under the case–control sampling. Therefore, it is natural to combine biomarkers by maximizing the AUC under ROC curve (<ref type="bibr">Huang, 2005, 2007;</ref><ref type="bibr" target="#b12">Pepe et al., 2006;</ref><ref type="bibr" target="#b16">Ye et al., 2007;</ref><ref type="bibr" target="#b18">Zhou et al., 2011</ref>). However, it is notoriously difficult to maximize the AUC numerically since the objective function is neither continuous nor convex. Ad hoc methods have been proposed to tackle the numerical problem. For example, sigmoid function has been used to approximate the indicator function used in calculating AUC (<ref type="bibr" target="#b10">Ma and Huang, 2007</ref>). However, the smoothed objective function may still have multiple local maximums, with no guarantee of locating the global maximizer by using the commonly used numerical algorithms. In view of these challenges, we propose a class of ensemble methods aiming for maximizing AUC with multiple biomarkers. Specifically, we introduce a class of convex surrogate loss functions to approximate the non-convex AUC, greatly facilitating computation and optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Surrogate loss functions</head><p>Assume that X 1 ,...,X n are n independently identically distributed (i.i.d) copies of p-dimensional random vector X, representing, for example, p biomarkers for cases, and Y 1 ,...,Y m are m i.i.d copies of p-dimensional random vector Y for controls. Suppose that we want to construct a score as a linear combination of the p biomarkers with the aim of maximizing the AUC under ROC curves. Specifically, we want to find a vector β to maximize the objective function</p><formula>AUC(β) = (nm) −1 n i=1 m j=1 I(β X i &gt;β Y j ).</formula><p>Ideally, we would want the score for cases to be higher than that for controls, which yields 1 for the AUC and completely differentiates cases and controls. However, several challenges are prominent. First, since the objective is invariant in β → c 0 β for arbitrary c 0 &gt; 0, there is no unique maximizer for the objective function. One often needs to subjectively select an anchor biomarker with its weight in the linear combination being one and maximize the remaining p−1 components in β. The performance of the score heavily depends on the selection of the anchor biomarker. Second, even with a given anchor biomarker the objective function is still neither continuous nor concave and therefore it is very likely that conventionally used optimization iterations have been trapped around local maximum points depending on the subjectively selected initial point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC-based biomarker ensemble</head><p>We are now in a position to propose a method addressing these two challenges. By noting that 1−AUC(β) can be interpreted as the misclassification rate of using the binary rule β (X i −Y j ) &gt; 0 to classify a binary response always taking value 1, we may borrow the popular classification approaches aimed for minimizing the misclassification rate in the data mining literature (<ref type="bibr" target="#b5">Friendman et al., 2000;</ref><ref type="bibr" target="#b6">Hastie and Zhu, 2006</ref>). Specifically, instead of directly maximizing AUC(β) or equivalently minimizing 1−AUC(β), it is sensible to minimize a surrogate loss function. We propose the following two surrogate functions</p><formula>M 1 (β) = (nm) −1 n i=1 m j=1 log[1+exp{−β (X i −Y j )}], and M 2 (β) = (nm) −1 n i=1 m j=1 {1−β (X i −Y j )} + ,</formula><p>which correspond to the negative log likelihood function raised in the conditional logistic regression and hinge loss function used in support vector machine, respectively, where x + = xI(x &gt; 0). Since these two functions are continuous convex function, their minimizers are well defined and can be reliably located. Numerically, it amounts to replace the indicator function I(x &lt; 0) by log{1+exp(−x)} and (1−x) + , respectively. Previous data mining literature reveals that algorithms minimizing such surrogate loss functions often result in models with good performance in minimizing the misclassification rate. Analogously, the estimated scores tend to render satisfactory AUCs. With moderate dimension p, while M 1 (β) can be minimized via the scoring algorithm in fitting a generalized linear model, we can use linear programming technique to minimize M 2 (β). Specifically, it is equivalent to</p><formula>minimizing n i=1 m j=1 ξ ij+ subject to ⎧ ⎪ ⎨ ⎪ ⎩ ξ ij = ξ ij+ −ξ ij− ξ ij = 1−β (X i −X j ) ξ ij+ ≥ 0,ξ ij− ≥ 0 1 ≤ i ≤ n,1 ≤ j ≤ m.</formula><p>Several advantages of the proposal are obvious. First, minimizing either M 1 (β) or M 2 (β) does not require selecting an anchor biomarker a priori, which is especially appealing for high-dimensional case. Second, in the limiting case, the maximizer of the AUC under the ROC curve</p><formula>E{AUC(g)}=pr{g(X) &gt; g(Y )}, is g(·) = m{f 1 (·)/f 0 (·)}</formula><p>for any strictly monotone increasing function m(·), where f 1 (·) and f 0 (·) are the underlying density functions of X and Y , respectively (<ref type="bibr" target="#b7">Jin and Lu, 2008</ref>). The minimizer of</p><formula>E{M 1 (g)}=E log[1+e −{g(X)−g(Y )} ], is log{f 1 (·)/f 0 (·)}</formula><p>and thus minimizing E{M 1 (g)} is equivalent to maximizing the AUC under the ROC curve. The minimizer of E{M 2 (g)}=E{1−g(X)+g(Y )} + , is more complicated. Numerical studies point that the minimizer may also be a monotone transformation of f 1 (·)/f 0 (·), as opposed to the conventional support vector machine whose solution approximates the optimal decision boundary.<ref type="figure">Figure 1</ref>(1) Set the initial</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Adaptive generalizations</head><formula>β ← argmin β M 2 (β)</formula><p>(2) Update β as</p><formula>β ← argmin γ n i=1 m j=1 {1−γ (X i −Y j )} + 1+|1−ˆβ1+|1−ˆ 1+|1−ˆβ k−1 (X i −Y j )| ,</formula><p>where the minimization can be solved via linear programming technique.</p><p>(3) Repeat Step (2) until convergence or the number of iteration reaches a pre-specified number.</p><p>Our numerical results (reported in the Supplementary Material) show that the adaptive iteration may increase the resulting AUC especially when there are potential outliers. The robustness of the method is not a surprise, because the influence from individual observations on the objective function via m c (x) is always bounded. Furthermore, we find that one or two iterations often suffice to harvest most of the gain in maximizing AUC(β) and thus, in general, there is no need to continue the iteration until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Extension to survival outcomes</head><p>When the outcome is survival time subject to potential right censoring, the c− index as the generalized AUC is often computed as</p><formula>c(β) = n i=1 n j=1 I(β Z i &gt;β Z j )I( T i &lt; T j ) i n i=1 n j=1 I( T i &lt; T j ) i , where ( T i ,, i ,Z i ),i = 1</formula><p>,...n are n i.i.d copies of ( T ,,,Z), T is the minimum of the censoring and failure times, is the binary censoring indicator and Z is the covariate vector (<ref type="bibr" target="#b0">Cai and Cheng, 2008</ref>). Similarly, one may find β by minimizing</p><formula>˜ M 1 (β) = n i=1 n j=1 log[1+exp{−β (Z i −Z j )}]I( T i &lt; T j ) i , and ˜ M 2 (β) = n i=1 n j=1 {1−β (Z i −Z j )} + I( T i &lt; T j ) i ,</formula><formula>the counterparts of M 1 (β) and M 2 (β)</formula><p>, respectively. Indeed, the log-partial likelihood function also can be viewed as surrogate to the c−index in that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X.G.Zhao et al.</head><formula>both log 1+ K k=1 e −x k and K k=1 log(1+e −x k ),</formula><p>may serve as a surrogate to</p><formula>K j=1 I(x k &lt; 0).</formula><p>This may explain that the Cox model-based c−index is often high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Regularization for high-dimensional covariates</head><p>When p is high, the proposed surrogate loss function can be conveniently regularized for feature selection. While many regularization methods can be used, we hereafter pursue the popular lasso regularization for illustration (<ref type="bibr" target="#b15">Tibshirani, 1996</ref>). Specifically, we propose to minimize</p><formula>M j (β)+λ|β|,j = 1,2 where β = (β 1 ,...,β p ) and |β|= p j=1 |β j |. For M 1 (β)</formula><p>, one may use the following iterative coordinate descending algorithm to minimize the objective function with a given λ</p><p>(1) Set an initial estimator β.</p><formula>(2) Update β β ← argmin β n i=1 m j=1 W ij { ˜ Z ij −β (X i −Y j )} 2 ,</formula><formula>where W ij = p ij (1−p ij ), ˜ Z ij = β (X i −Y j )+p ij (1−p ij ), and p ij = 1 1+exp{β (X i −Y j )} ,</formula><p>for i = 1,...,n and j = 1,...,m. The standard coordinate descending algorithm can be used to minimize the weighted L 2 loss function in this step (<ref type="bibr" target="#b4">Friedman et al., 2010</ref>).</p><p>(3) Repeat Step 2 until convergence.</p><p>The coordinate descending algorithm is not directly applicable for the nondifferentiable surrogate loss function M 2 (β). However, since the objective function M 2 (β) is convex and piecewise linear, the exact solution path with λ varying from ∞ to 0 is also piecewise linear and can be computed using the generalized LARS algorithm (<ref type="bibr" target="#b1">Cai et al., 2009;</ref><ref type="bibr" target="#b14">Rosset and Zhu, 2007</ref>). When the dimension p or sample size n is high, the computation is demanding due to dense joints in the solution path. As a remedy, we propose a forward stagewise algorithm that generates an approximate solution path of β (<ref type="bibr" target="#b3">Friedman and Popescu, 2004</ref>).</p><p>(1) Set an initial estimator β = 0 and small positive number &gt;0.</p><p>(2) At step k = 1,..., identify the coordinate j with the largest decrease</p><formula>max{M 2 (β)−M 2 (β +e j ),M 2 (β)−M 2 (β −e j )},</formula><formula>and update β ← β +s j e j ,</formula><p>where &gt;0 is an small constant selected a priori, e j is a p-dimensional vector with all the components being zero except the j-th component, which is 1 and s j = 2I{M 2 (β +e j ) &lt; M 2 (β −e j )}−1.When the exact lasso solution is desirable, one may employ an ad hoc twostage approach. Specifically, one may first implement the aforementioned forward stagewise algorithm to screen informative features. The forward stagewise algorithm stops when the number of selected features reaches a prespecified maximum number of biomarkers to be used for constructing the score in practice, say 30. At the second step, the exact lasso solution path can be computed with only the selected features. In either case, the penalty parameter can be selected via cross-validation. The objective function used in the cross-validation can be either M j (β) itself or the AUC under ROC curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SIMULATION</head><p>Extensive simulations are conducted to examine the finite sample performance of the proposed method. We generate the covariates</p><formula>{X 1 ,...,X n } and {Y 1 ,...,Y m }</formula><p>from the following models:</p><formula>(1) (multivariate normal) X i iid ∼ N(µ 1 ,, 1 ) and Y j iid ∼ N(µ 2 ,, 2 ), where X i and Y j are p-dimensional random vectors.</formula><formula>(2) (normal mixture) X i iid ∼ 0.8N(µ 1 ,, 1 )+0.2N(µ 3 ,, 3 ) and Y j iid ∼ 0.8N(µ 2 ,, 2 )+0.2N(µ 3 ,, 3 ), i.e. 20%</formula><p>of the markers values in both cases and controls are contaminated by a common error distribution.</p><formula>(3) (multivariate log-normal) log(X i ) iid ∼ N(µ 1 ,, 1 ) and log(Y j ) iid ∼ N(µ 2 ,, 2 ).</formula><formula>(4) (log-normal mixture) log(X i ) iid ∼ 0.8N(µ 1 ,, 1 )+ 0.2N(µ 3 ,, 3 ) and log(Y j ) iid ∼ 0.8N(µ 2 ,, 2 )+0.2N(µ 3 ,, 3 ).</formula><p>In the above settings, we let µ 1 = (1,0,0,...,0) ,</p><formula>µ 2 = (0,1,0,...,0) , µ 3 = (1,1</formula><p>,1,0,...,0) ,</p><formula>1 = 2 = ⎛ ⎜ ⎜ ⎝ 1 1/3 ··· 1/3 1/3 1 ··· 1/3 ··· ··· ··· ··· 1/3 1/3 ··· 1 ⎞ ⎟ ⎟ ⎠ , 3 = 5I p , I p</formula><p>is the identity matrix. We considered several configurations of n, m and p to investigate the operational characteristics of the proposed method. First, we examine the scenario where the number of covariates is low relative to the sample size. To this end, we let p = 3 and n = m = 50. For each generated dataset, we construct a linear combination of the covariates as a score differentiating cases from controls, where the weights of the linear combinations are estimated by minimizing</p><formula>(i) M 1 (β) (ii) M 2 (β) (iii) the loss function S(β) =− n i=1 m j=1 1 1+e −β (X i −Y j )/σ ,</formula><formula>(1)</formula><p>proposed in Ma and Huang (2007) and (iv) by fitting a regular logistic regression. We also implement the popular ada-boosting with 300 iterations using the simple stump as the base classifier (<ref type="bibr" target="#b5">Friendman et al., 2000</ref>). The continuous class probability based on ada-boosting trained ensemble is used to generate the ROC curve. In minimizing S(β), we first identify the 'anchor covariate' with the most significant p value from t-test comparing the covariate distribution between cases and controls and set its regression coefficient at +1 or −1 depending on the sign of the<ref type="figure">Figure 2</ref>. The AUCs in the training sets are higher than their counterparts in the validation sets as expected. In most cases including the first setting, where the logistic regression estimates the optimal combination in terms of maximizing the AUC, the scores based on M 1 (β), M 2 (β), S(β) and logistic regression perform similarly in terms of AUC in the validation sets. In general, the score based on ada-boosting has the lowest AUC, which could be due to overfitting indicated by the high AUCs in the training set. Furthermore, the score based on the one-step adaptive hinge loss function performs similar or slightly superior to that based on hinge loss function itself. Second, we have examined the performance of the proposed method for covariates with moderate dimension. In this case, we let p = 200 and n = m = 50 and the lasso regularization is used for selecting the important features in logistic regression. The forward stagewise algorithm similar to that presented in Section 2.2 for minimizing M 2 (β) is also used to minimize S(β). We choose the popular lasso penalty mainly for the purpose of fair comparison, i.e. evaluating the relative performance of various methods under similar regularization schemes. The boxplots of AUCs in independent test sets over 250 replications are plotted in<ref type="figure" target="#fig_4">Figure 3</ref>. In general, the scores based on M j (β) perform better than that based on the alternatives in terms of average AUC in the test sets. Furthermore, the AUCs from scores constructed via M j (β) also tend to have smaller variability than their counterparts. In the most challenging fourth setting, the empirical average AUC in the test sets is 0.66 for score minimizing M 1 (β), 0.66 for score minimizing the hinge loss, M 2 (β), 0.60 for scores minimizing S(β), 0.60 for score from the logistic regression fitting and 0.63 for score from ada-boosting using three markers. An increase from 0.60–63 to 0.66 in the AUC is often considered non-trivial in clinical practice.Lastly, we have examined the cases for high-dimensional covariate. Here, we let p = 20 000 and n = m = 50. To save computational time, the ada-boosting is only applied to top 500 features selected based on significance levels of t-test comparing cases and controls in the training set. The simulation results are presented in<ref type="figure">Figure 4</ref>. For the high-dimensional covariates, the relative performance of the proposed methods is even better than that in the previous case where p = 200. For example, in the third setting, the empirical average AUC in the test sets is 0.85 for score minimizing M 1 (β), 0.85 for score minimizing the hinge loss, 0.76 for scores minimizing S(β), 0.74 for score from the logistic regression fitting and 0.64 for score from ada-boosting using three markers. Similarly, in the fourth setting, the empirical average AUC in the test sets are 0.56, 0.56, 0.53, 0.53 and 0.53 for aforementioned five methods.</p><formula>A B C D E A B C D E A B C D E A B C D E 0.5 0.6 0.7 0.8 0.9 0.5 0.6 0.7 0.8 AUC AUC AUC AUC (a) ( b) (c) ( d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS OF THE BONE MINERAL DENSITY STUDY</head><p>We apply the proposed method to a dataset (<ref type="bibr" target="#b13">Reppe et al., 2010</ref>) arising from a study that recruited 301 non-related post-menopausal ethnic Norwegian women at the Lovisenberg Deacon Hospital. Among them, bone mineral density (BMD) and gene expression levels (Affymetric array) were measured for 84 women. Since low BMD is associated with higher fracture rates (<ref type="bibr" target="#b2">Cooper, 1997</ref>), it is of interest to identify a linear combination of gene expression levels to differentiate the osteopenia or osteoporosis (low BMD) from normal among post-menopausal women. Bone biopsies show that there are 39 from 84 women having osteopenia or osteoporosis. All the normalized gene expression level are log-transformed. After screening out ∼25% probesets with lowest variation, we have 40 411 probesets for each patient. We randomly split the data into training and validation sets and apply the proposed method to the trainingare determined using the same method as that presented in the simulation study. To save computational time, adaboosting is only applied to the top 2000 genes according to their significance level in t-test comparing average gene expression levels between cases and controls. With the estimated scores based on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X.G.Zhao et al.</head><formula>(a) ( b) (c) ( d) A B C D E A B C D E A B C D E A B C D E 0.5 0.6 0.7 0.8 0.9 AUC 0.5 0.6 0.7 0.8 0.9 AUC AUC AUC</formula><formula>M 1 (β), M 2 (β), S(β)</formula><p>, the regularized logistic regression and adaboosting, we examine their corresponding AUC in the validation set. The results are shown in<ref type="figure" target="#fig_8">Figure 5</ref>. It can be seen that in general scores based on M 1 (β) and M 2 (β) yield higher AUC than that based on S(β), the commonly used logistic regression and adaboosting with the same number of covariates in the validation set. In<ref type="figure" target="#fig_8">Figure 5</ref>, we also plot the AUC in the training set. Since the number of covariates is much higher than the sample size, the maximum AUC (AUC = 1) corresponding to complete separation between case and control in the training set is reached with 20– 30 covariates for all the methods. The highest AUCs for scores based on S 1 (β), regularized logistic regression and ada-boosting are 0.764, 0.687 and 0.728, respectively, while the highest AUC is 0.708 for score based on M 1 (β) and 0.764 for score based on M 2 (β). As a reference, the AUC for age is only 0.669 in this cohort. Furthermore, while the optimal scores with M 1 (β) and M 2 (β) use 9 and 13 genes, respectively, their counterparts based S(β), regularized logistic regression and ada-boosting use 37, 12 and 45 genes, respectively. These comparisons suggest that the genes score based on M 2 (β) possesses the best combination of sparsity and prediction performance: it attains the highest AUC in the validation set with only 13 genes. The gene lists selected by these methods are heavily overlapping. For example, there are seven common genes shared by at least three out of four linear combinations constructed based</p><formula>M 1 (β), M 2 (β), S(β)</formula><p>and logistic regression. The probe set AFFXM27830_M_at, which is shared by all four scores, is a member of the eight core genes reported by (<ref type="bibr" target="#b13">Reppe et al., 2010</ref>). Furthermore, gene SOST (Affymetrix ID 223869_at) shared by scores based on M 1 (β), M 2 (β) and logistic regression is also a member of the eight core genes explaining the variation of BMD and sits in the 'center' of the constructed intermolecular network sharing significant associations reported in the original paper (<ref type="bibr" target="#b13">Reppe et al., 2010</ref>). The selected genes and their corresponding weights are summarized in<ref type="figure" target="#tab_1">Table 1</ref>, where the weights are standardized such that the probe set AFFXM27830_M_at has the unit weight for comparison purpose. One interesting and reassuring observation is that signs of all non-zero weights were consistent across methods. We also repeat analysis based on other random training test splitting and obtain similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Motivated by recent advances in data mining, we have proposed a class of methods combining biomarkers to construct a scoring system, boosting the resulting AUC under the ROC curve, a prevalence-free summarization of intrinsic predictive values of a continuous score. The method is easily adapted to high-dimensional cases, wherein one may need to identify informative features from thousands of candidate biomarkers. In high-dimensional case, we propose to apply lasso regularization to yield a parsimonious combination maximizing the AUC. On the other hand, lasso is neither the unique nor the universally optimal regularization method for analyzing high-dimensional data. Due to the convexity of the proposed loss function, it is straightforward to couple M j (β) with other penalty functions such as elastic net, adaptive lasso and SCAD, which may have superior performance to simple lasso in specific settings (<ref type="bibr" target="#b19">Zou, 2006;</ref><ref type="bibr" target="#b20">Zou and Hastie, 2005;</ref><ref type="bibr" target="#b21">Zou and Li, 2008</ref>). The key proposal is to target a convex surrogate loss function instead of a discontinuous Mann–Whitney rank statistic. While in this article, we have focused on the hinge loss function (corresponding to the 1− norm support vector machine), our results can be extended to accommodate other versions of SVM loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC-based biomarker ensemble</head><p>functions, such asfor any given α ≥ 1. Another alternative is the exponential function used in boosting algorithm</p><formula>(nm) −1 n i=1 m j=1 {1−β (X i −Y j )} α + ,</formula><formula>(nm) −1 n i=1 m j=1 e −β (X i −Y j ) .</formula><p>Finally, while the AUC under the entire ROC curve is a useful global measure, the AUC under partial ROC curve has recently emerged as a useful problem-specific measure in practice (<ref type="bibr" target="#b8">Komori and Equchi, 2010</ref>). Therefore, an numerically efficient algorithm combining multiple biomarkers to maximize the AUC under partial ROC curves or sensitivity for given specificity level is worth further investigations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>[10:59 3/10/2011 Bioinformatics-btr516.tex] Page: 3051 3050–3055</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig</head><figDesc>Fig. 1. m c (x) against the 0–1 loss function. The 0–1 loss function I(x &lt; 0) (black); hinging loss function (red); green: m 1 (x); m 0.5 (x) (blue); m 0.25 (x) (cyan); log{1+exp(−x)} (purple).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(3)</head><figDesc>Repeat Step 2 until the number of non-zero components of β reaches a prespecified number or the AUC(β) becomes 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.3.</head><figDesc>Fig. 3. Empirical AUC in the validation sets for different methods [A: M 1 (β); B: M 2 (β); C: S(β); D: lasso-regularized logistic regression; E: ada-boosting using stumps with 300 iterations] with moderate dimensional covariates. Empty box: AUC with one selected covariate; gray box: AUC with three selected covariates; dark gray box, AUC with 10 selected covariates. (a) Simulation setting I (normal); (b) simulation setting II (normal mixture); (c) simulation setting III (log-normal); (d) simulation setting IV (log-normal mixture).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.5.</head><figDesc>Fig. 5. AUC of scores for differentiating the women having low and normal BMD. Thick lines: AUC in validation set; thin lines: AUC in training set; red: score based on M 1 (β); green: score based on M 2 (β); blue: score based on S(β); black: score based on logistic regression; cyan: score based on ada-boosting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><figDesc>Funding: R01 HL089778-04 (to L. Tian) and R01 CA95747 (to Y. Li) from National Institute of Health. Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>As neither M 1 (β) nor M 2 (β) provides a good approximation to 1−AUC(β) (indeed no convex functions accurately approximate the indicator function), we employ an iterative algorithm to approximately minimize 1−AUC(β). More specifically, given that m c (x) = (c−x) + c+|c−x| → I(x &lt; 0) as c → 0+, we expect that the minimizer of n i=1 m j=1 m c {β (X i −Y j )} approximates that of 1−AUC(β) for small c &gt; 0.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>(·) with different cs. Noting that m c (β x) = m 1 {(β/c) x}, the scoring system minimizing m c (β x) is equivalent to that minimizing m 1 (β x). Thus, we may employ the following adaptive algorithm</figDesc><table>. 1. m c (x) against the 0–1 loss function. The 0–1 loss function I(x &lt; 0) 
(black); hinging loss function (red); green: m 1 (x); m 0.5 (x) (blue); m 0.25 (x) 
(cyan); log{1+exp(−x)} (purple). 

against log{1+exp(−x)}, the hinge loss function and m c </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>t-statistics. The σ in S(β) is then selected as 20% of the mean group difference of the anchor covariate as suggested in Ma and Huang (2007). We then calculate the AUCs in an independent test set consisting of 2000 cases and 2000 controls for all the five constructed scores. The boxplots of AUCs over 250 replications in each setting are</figDesc><table>Page: 3053 3050–3055 

AUC-based biomarker ensemble 

A 
B 1 
B 2 
C 
D 
E 
A 
B 1 
B 2 
C 
D 
E 

A 
B 1 
B 2 
C 
D 
E 
A 
B 1 
B 2 
C 
D 
E 
0.75 0.80 

0.80 

0.85 

0.85 

0.90 

0.90 

0.95 

0.95 

1.00 

0.5 0.6 0.7 0.8 0.9 1.0 

1.00 

AUC 
AUC 

0.7 

0.8 

0.9 

1.0 

AUC 
AUC 

(a) 
( b) 

(c) 
( d) 

Fig. 2. Empirical AUC in both training and validation sets for different 
methods [A: M 1 (β); B1: M 2 (β); B2: one-step adaptive M 2 (β); C: S(β); D: 
lasso-regularized logistic regression; E: ada-boosting using stumps with 300 
iterations] with low-dimensional covariates. Gray box: training set; empty 
box: validation set. (a) Simulation setting I (normal); (b) simulation setting 
II (normal mixture); (c) simulation setting III (log-normal); (d) simulation 
setting IV (log-normal mixture). 

plotted in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 1.</figDesc><table>The estimated scores for differentiating low and normal BMD 

Affymatrix ID 
Weights 

M 1 (β) 
M 2 (β) 
S(β) 
L R 

AFFX-M27830_M_at 
1 
1 
1 
1 
211769_x_at 
0 
0 
0 
0.946 
215887_at 
0 
0 
0 
1.915 
217761_at 
0 
0 
0 
−0.447 

220900_at 
0 
0 
0 
1.073 
223869_at 
0.587 
0.159 
0 
0.112 
227405_s_at 
0.055 
0 
0 
0.024 
231599_x_at 
0 
0 
0 
−0.175 

235102_x_at 
0.983 
0.113 
0 
2.822 
237739_at 
0.779 
0.093 
0 
0.896 
238020_at 
0 
0 
0 
−1.625 

239498_at 
1.161 
0.185 
0 
1.146 
206742_at 
−0.068 
−0.06 
−0.026 
0 
222735_at 
−0.639 
−0.185 
−0.082 
0 
244035_at 
0.692 
0.013 
0 
0 
219747_at 
0 
0.007 
0 
0 
235439_at 
0 
0.033 
0 
0 
238705_at 
0 
0.212 
0 
0 
238946_at 
0 
0.06 
0.02 
0 
1552477_a_at 
0 
−0.02 
0 
0 
206273_at 
0 
0 
0.008 
0 
206307_s_at 
0 
0 
0.006 
0 
206326_at 
0 
0 
0.038 
0 
207369_at 
0 
0 
−0.036 
0 
210045_at 
0 
0 
−0.002 
0 
210174_at 
0 
0 
−0.036 
0 
214412_at 
0 
0 
−0.004 
0 
215196_at 
0 
0 
−0.008 
0 
215431_at 
0 
0 
0.002 
0 
219566_at 
0 
0 
−0.01 
0 
220554_at 
0 
0 
0.09 
0 
220584_at 
0 
0 
−0.012 
0 
221631_at 
0 
0 
−0.01 
0 
227440_at 
0 
0 
−0.078 
0 
229201_at 
0 
0 
−0.108 
0 
230349_at 
0 
0 
−0.01 
0 
230839_at 
0 
0 
−0.024 
0 
231231_at 
0 
0 
0.01 
0 
231468_at 
0 
0 
−0.112 
0 
231759_at 
0 
0 
−0.106 
0 
231828_at 
0 
0 
−0.004 
0 
232114_at 
0 
0 
0.09 
0 
234259_at 
0 
0 
0.012 
0 
234421_s_at 
0 
0 
−0.036 
0 
234604_at 
0 
0 
0.058 
0 
241736_at 
0 
0 
0.048 
0 
243673_at 
0 
0 
−0.016 
0 
243889_at 
0 
0 
−0.068 
0 
244338_at 
0 
0 
0.022 
0 
1553027_a_at 
0 
0 
−0.008 
0 
1556803_at 
0 
0 
0.044 
0 
1556938_a_at 
0 
0 
0.048 
0 
1560779_a_at 
0 
0 
0.032 
0 

LR, logistic regression. 

</table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust combination of multiple diagnostic tests for classifying censored event times</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Cai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cheng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="216" to="233" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Regularized estimation for the accelerated failure time model</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Cai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="394" to="404" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">The crippling consequences of fractures and their impact on quality of life</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Cooper</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Med</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="12" to="17" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">Gradient directed regularization for linear regression and classification</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Popescu</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softwr</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friendman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="337" to="407" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Discussion of &quot; support vector machines with applications</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Zhu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<editor>Javier Moguerza and Alberto Munoz</editor>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="352" to="357" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">A procedure for determining whether a simple combination of diagnostic tests may be noninferior to the theoretical optimum combination</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Lu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Decis Making</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="909" to="916" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">A boosting method for maximizing the partial area under the ROC curve</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Komori</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Equchi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="314" to="330" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Regularized roc method for disease classification and biomarker selection with microarray data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4356" to="4362" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining multiple markers for classification using ROC</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="751" to="757" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">The Statistical Evaluation of Medical Tests for Classification and Prediction</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Pepe</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining predictors for classification using the area under the receiver operating characteristic curve</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Pepe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Eight genes are highly associated with bmd variation in postmenopausal caucasian women</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Reppe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bone</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="604" to="612" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Piecewise linear regularized solution paths</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Rosset</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Zhu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1012" to="1030" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc., Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">On the analysis of glycomics mass spectrometry data via the regularized area under the ROC curve</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ye</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="477" to="488" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<monogr>
		<title level="m" type="main">Statistical Methods in Diagnostic Medicine</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Variable selection using the optimal roc curve: An application to a traditional chinese medicine study on osteoporosis disease</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med. [Epub ahead of print</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">The adaptive lasso and its oracle properties</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">One-step sparse estimates in nonconcave penalized likelihood models</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1509" to="1533" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>