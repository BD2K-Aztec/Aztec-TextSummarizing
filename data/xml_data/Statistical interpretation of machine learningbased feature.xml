
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Statistical interpretation of machine learning-based feature importance scores for biomarker discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">. 13 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Anh</forename>
								<surname>Huynh-Thu</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Systems and Modeling</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">GIGA-Research, Bioinformatics and Modeling</orgName>
								<orgName type="institution">University of Liège</orgName>
								<address>
									<postCode>4000</postCode>
									<settlement>Liège</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Yvan</forename>
								<surname>Saeys</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Plant Biotechnology and Bioinformatics</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<postCode>9052</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Louis</forename>
								<surname>Wehenkel</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Systems and Modeling</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">GIGA-Research, Bioinformatics and Modeling</orgName>
								<orgName type="institution">University of Liège</orgName>
								<address>
									<postCode>4000</postCode>
									<settlement>Liège</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Pierre</forename>
								<surname>Geurts</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Systems and Modeling</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">GIGA-Research, Bioinformatics and Modeling</orgName>
								<orgName type="institution">University of Liège</orgName>
								<address>
									<postCode>4000</postCode>
									<settlement>Liège</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Statistical interpretation of machine learning-based feature importance scores for biomarker discovery</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="page" from="1766" to="1774"/>
							<date type="published" when="2012">. 13 2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts238</idno>
					<note type="submission">Advance Access publication April 25, 2012 Received on December 9, 2011; revised on April 2, 2012; accepted on April 18, 2012</note>
					<note>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:27 13/6/2012 Bioinformatics-bts238.tex] Page: 1766 1766–1774 Associate Editor: Jonarthan Wren Contact: vahuynh@ulg.ac.be, or p.geurts@ulg.ac.be Supplementary information: Supplementary data are available at Bioinformatics online. * To whom correspondence should be addressed.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Univariate statistical tests are widely used for biomarker discovery in bioinformatics. These procedures are simple, fast and their output is easily interpretable by biologists but they can only identify variables that provide a significant amount of information in isolation from the other variables. As biological processes are expected to involve complex interactions between variables, univariate methods thus potentially miss some informative biomarkers. Variable relevance scores provided by machine learning techniques, however, are potentially able to highlight multivariate interacting effects, but unlike the p-values returned by univariate tests, these relevance scores are usually not statistically interpretable. This lack of interpretability hampers the determination of a relevance threshold for extracting a feature subset from the rankings and also prevents the wide adoption of these methods by practicians. Results: We evaluated several, existing and novel, procedures that extract relevant features from rankings derived from machine learning approaches. These procedures replace the relevance scores with measures that can be interpreted in a statistical way, such as p-values, false discovery rates, or family wise error rates, for which it is easier to determine a significance level. Experiments were performed on several artificial problems as well as on real microarray datasets. Although the methods differ in terms of computing times and the tradeoff, they achieve in terms of false positives and false negatives, some of them greatly help in the extraction of truly relevant biomarkers and should thus be of great practical interest for biologists and physicians. As a side conclusion, our experiments also clearly highlight that using model performance as a criterion for feature selection is often counter-productive. Availability and implementation: Python source codes of all tested methods, as well as the MATLAB scripts used for data simulation, can be found in the Supplementary Material.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Univariate hypothesis testing is widely used in the context of biomarker discovery in bioinformatics, where one seeks to identify biological variables (e.g. genes or genetic polymorphisms) that truly provide information about some phenotype of interest (e.g. disease status or treatment response). A classic procedure consists in applying a statistical test to compute a p-value for each variable of the considered problem and selecting variables that have a p-value lower than a chosen threshold. To cope with multiple hypothesis problems, p-values are typically replaced with an estimation of the false discovery rate (FDR) or the family wise error rate (FWER), (<ref type="bibr" target="#b16">Ge et al., 2003</ref>). Univariate tests can only identify variables that provide a significant amount of information about the output variable in isolation from the other inputs. Since biological processes are expected to involve complex interactions between variables, these procedures potentially miss some informative biomarkers. Nowadays, when one seeks multivariate interacting effects between features, one can resort to relevance scores provided by machine learning techniques. Among these, the most popular methods include importance scores derived from a tree-based ensemble method (<ref type="bibr" target="#b22">Hastie et al., 2003</ref>) or feature weights computed for example from a linear support vector machine (SVM) (<ref type="bibr" target="#b26">Rakotomamonjy, 2003</ref>). However, unlike the p-values returned by univariate tests, these relevance scores are usually not statistically interpretable. This lack of interpretability prevents the wide adoption of these methods by practicians, biologists or physicians and also makes the identification of the truly relevant variables among the top-ranked ones, i.e. the determination of a relevance threshold, a very difficult task in practice. In this article, we evaluate several, existing and novel, procedures that extract relevant features from a ranking returned by a multivariate algorithm. These procedures replace the original relevance score with a measure that can be interpreted in a statistical way and hence allow the user to determine a significance threshold in a more informed way. Most of these methods exploit a resampling procedure to estimate the FDR or FWER among the k top-ranked features, for increasing values of k. Just like for standard univariate tests, the user can then choose a threshold on this new measure depending on the risk he/she is ready to take when deeming that all features above this threshold are relevant. Experiments on several artificial problems, as well as on real microarray datasets, show that some of these measures greatly help in the extraction of truly relevant features from a ranking derived from a multivariate approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical interpretation of importance scores</head><p>We also highlight that the common approach to this problem, i.e. selecting the top k features minimizing some cross-validated error, is not a good practice in general, as it typically leads to the selection of several irrelevant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>In this article, we focus on the problem of selecting relevant features from a ranking. We assume that we have at our disposal a learning sample LS of n instances of input–output pairs drawn from some unknown probability distribution. There are m input variables denoted X i ,i = 1,...,m. We further assume that we have a machine learning algorithm A(LS) that outputs from the learning sample LS a feature ranking, typically derived from a relevance score s i for each input variable X i. These scores are not supposed to be independent and no further assumption is made about A. The goal is then to determine a value k such that the subset composed of the k top-ranked variables contains the highest possible number of relevant features, i.e. variables that convey information about the target output variable, in isolation or in conjunction with other relevant variables. Different sensitivity/specificity compromises are possible and depend on the considered application. In this article, we aim at high specificity, i.e. at identifying subsets of relevant variables while maintaining the rate of false positives as small as possible. This type of compromise is typically sought in the context of biomarker discovery because of high costs of subsequent experiments (<ref type="bibr" target="#b28">Saeys et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEATURE SELECTION METHODS</head><p>We describe below several methods that have been developed for the selection of relevant variables from a ranking. We assume that we have an algorithm A (LS) that returns, from a learning sample LS, a relevance score s i for each input variable X i ,i = 1,...,m, and we further assume, without loss of generality, that the features are numbered according to their relevance score, i.e. s 1 ≥ s 2 ≥ ··· ≥ s m. Most of the presented methods then reuse A on a modified LS (obtained from a subsampling, a permutation, etc.) to replace each original relevance score s i with a statistically interpretable measure. The intuition behind each method is given below and their detailed pseudo-code descriptions can be found in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Estimation of the generalization error of a model (err-A and err-TRT)</head><p>We include in our comparison, as a baseline method, the procedure based on the computation of the generalization error of a predictive model<ref type="bibr">[see Geurts et al. (2005)</ref>for an example<ref type="bibr">]</ref>. This method consists in estimating the error rate (resp. quadratic error) e i of a classification (resp. regression) model that uses only the first i variables of the ranking, ∀i = 1,...,m, and selecting the k top-ranked variables such that</p><formula>k = arg min i=1,...,m e i .</formula><formula>(1)</formula><p>The m predictive models can be learned using the algorithm A that was used to compute the ranking of variables and the generalization error of one model can be estimated using a cross-validation procedure (10-fold in all our experiments). We call this method err-A to denote the fact that the same algorithm A is used both to rank the features and to estimate the error associated with each feature subset. A sharper threshold can be obtained by estimating the generalization error with an algorithm that is not robust to irrelevant variables, such as k-NN (<ref type="bibr" target="#b15">Fukunaga and Hostetler, 1975</ref>) or totally randomized trees (TRT, i.e. an ensemble of trees with completely random split choices;<ref type="bibr" target="#b19">Geurts et al., 2006</ref>). Compared with a robust algorithm, we expect the error of such a procedure to increase in a more abrupt way when irrelevant variables are introduced in the predictive model and therefore to yield a smaller number of selected variables. We used TRT in our experiments as this method is computationally less expensive than k-NN and we call the resulting feature selection method err-TRT. One potential drawback of this approach is the fact that it is prone to selection bias (<ref type="bibr" target="#b9">Ambroise and McLachlan, 2002;</ref><ref type="bibr" target="#b31">Smialowski et al., 2010</ref>), as the same instances of LS are used to rank the variables and to estimate the generalization error. This results in a too optimistic estimation of the errors e i , and in particular of the minimal error min i e i , whose effect on the number of selected features is difficult to appraise. One could get better error estimates by ranking the features inside the cross-validation loop (<ref type="bibr" target="#b9">Ambroise and McLachlan, 2002</ref>) but this would leave open the question of the selection of the final feature subset among the subsets generated within each fold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiple testing with random permutations (nFDR, eFDR and conditional error rate)</head><p>The FDR (Storey and<ref type="bibr" target="#b33">Tibshirani, 2003</ref>) is the expected rate of truly irrelevant features among the variables that are deemed relevant. Hence, given a selection threshold s i , the FDR is defined as</p><formula>FDR i = E V i R i |R i &gt; 0 P(R i &gt; 0),</formula><formula>(2)</formula><p>where R i is the number of variables considered relevant at score s i and V i is the number of those variables that are truly irrelevant. V i /R i is thus set to zero if R i = 0. To select a subset of variables, some methods estimate the FDR for increasing values of i and choose the maximum value of i such that FDR i &lt; α, where α is typically small and reflects the risk one is ready to accept in terms of false positives when selecting the variables. In the context of univariate variable scoring procedures, a classic approach to estimate the FDR is based on permutation tests (<ref type="bibr" target="#b16">Ge et al., 2003</ref>). The FDR is approximated by</p><formula>FDR i = E[V i |H 1→m I ] R i ,</formula><formula>(3)</formula><p>where H 1→m I is the hypothesis that all the variables are irrelevant. R i is considered equal to i and E[V i |H 1→m i</p><p>] is taken as the expected number of variables that get a score greater than s i when the output values are randomly permuted in the learning sample, making all the variables irrelevant. We call the FDR estimated using this approach the nFDR.<ref type="bibr" target="#b8">Altmann et al. (2010)</ref>proposed a very similar permutation scheme to associate a p-value to Random Forests (RFs) importance scores. Huynh<ref type="bibr" target="#b24">Thu et al. (2008)</ref>applied the nFDR approach when the relevance scores are derived from tree-based importance measures instead of univariate scores. They showed empirically that this procedure overestimates in an unpredictable way the real FDR and thus can lead to unreliable selections of relevant subsets. This overestimation of the FDR can be explained, at least partially, by the fact that this procedure does not take into account the dependence that exists between the tree–based importance scores for different variables. To overcome this limitation, they proposed an alternative measure to be associated with each threshold s i and that takes into account the scores of the variables that are ranked above X i. For each subset of i top-ranked variables, the procedure consists in computing the following conditional probability, called the conditional error rate (CER):</p><formula>CER i = P( max k=i,...,m S k ≥ s i |H 1→i−1 R ,H i→m I ), (4)</formula><p>where H 1→i−1 R denotes the hypothesis that features X 1 ,...,X i−1 are relevant H i→m I is the hypothesis that features X i ,...,X m are irrelevant and S k is the random variable denoting the relevance score of X k under these two hypotheses. CER i is thus the probability that at least one irrelevant variable among m−i +1 gets a relevance score greater or equal to s i , when these</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.A.Huynh-Thu et al.</head><p>scores are computed under the assumption that variables X 1 ,...,X i−1 are all relevant. The CER is hence an estimation of the FWER, that is defined as the probability to include at least one irrelevant variable among those selected. H 1→i−1 R is approximated by keeping the values of the output variable and of the first i −1 variables unchanged, whereas hypothesis H i→m I is simulated by randomly permuting the values of X i ,...,X m. To adhere as much as possible to the original joint distribution of the variables, they are furthermore permuted jointly, i.e. using the same permutation vector. Note that when the scores s i are univariate statistics, expression (4) corresponds precisely to the definition of Westfall and Young's (1993) step-down maxT adjusted p-values (<ref type="bibr" target="#b16">Ge et al., 2003</ref>). Another permutation-based approach was proposed by<ref type="bibr" target="#b17">Ge et al. (2008)</ref>to estimate the FDR in the context of univariate rankings. This approach also makes the assumption that the first i −1 variables are relevant and the FDR at threshold s i is defined by</p><formula>FDR i = E V i V i +i −1 H 1→i−1 R ,H i→m I .</formula><formula>(5)</formula><p>The number V i of false positives is estimated by the following way. Let s p k be the relevance score of X k (∀k = 1,...,m), calculated from a random permutation of the data that simulates</p><formula>H 1→i−1 R and H i→m I , and let s p</formula><p>(k) be the k-th largest member of {s</p><formula>p i ,...,s p m }. V i is then computed as V i = max k=1,...,m−i+1 {k : s p (1) ≥ s i ,s p (2) ≥ s i+1 ,...,s p (k) ≥ s i+k−1 }.</formula><formula>(6)</formula><p>The FDR estimated using Equations (5) and</p><p>(6) is called eFDR. When applying this approach to rankings derived from a multivariate approach, we propose to use the same permutation scheme as in the CER approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Empirical estimation of the null rank distribution (mr-test)</head><p>The mr-test (<ref type="bibr" target="#b37">Zhang et al., 2006</ref>) estimates an empirical distribution of the rank of an irrelevant feature, to derive a p-value p i to be associated with each variable X i , defined as the probability for an irrelevant variable to be ranked above or at the same position as X i. To estimate the distribution of the rank of an irrelevant variable,<ref type="bibr" target="#b37">Zhang et al. (2006)</ref>proposed to proceed as follows. P feature rankings are obtained by applying the algorithm A on P resamplings of the original learning sample. Given a user-defined number k, the k variables that have on average the largest ranks among all the variables are considered putative irrelevant variables and the null rank distribution is estimated from their k ×P ranks over the P rankings. The p-value p i is then estimated as the proportion of these k ×P ranks that are lower than the average rank ¯ r i of X i over the P rankings. As the p-values calculated using this procedure are raw p-values, the so-called multiple-testing problem occurs, where the higher the number of variables in the considered problem, the higher the number of expected variables with a p-value lower than some threshold α, even if these variables are irrelevant. We therefore propose to apply a multiple-testing correction procedure and to select the variables based on the corrected p-values. In our experiments, we used the Benjamini Hochberg correction (<ref type="bibr" target="#b10">Benjamini and Hochberg, 1995</ref>), which was shown to control the FDR in the context of univariate statistical tests. The mr-test procedure has two parameters. The first one is the number k of putative irrelevant variables from which the empirical null rank distribution is estimated. A small value of k would result in overoptimistic selections of variables whereas a high value of k would be too conservative. In our experiments, k was fixed to m/2. The second parameter is the number of resampled instances at each iteration that we fixed to half of the number of instances in the original learning sample, as proposed by<ref type="bibr" target="#b37">Zhang et al. (2006).</ref><ref type="bibr" target="#b32">Stoppiglia et al. (2003)</ref>suggested to introduce one random feature to compute the probability p i for this random feature to be ranked above or at the same position as X i. They applied this idea in the context of linear models where each variable X i is ranked according to the squared cosine of the angle between X i and the output variable, and where therefore the distribution of the rank of the random feature can be computed analytically. However, to be able to apply this approach with any ranking procedure, the authors suggested in their conclusions to compute the null rank distribution empirically by artificially introducing random probes. We therefore propose the following procedure, that we call 1Probe. In each of P iterations, we introduce in the original learning sample an additional variable X rand whose values are randomly sampled from N (0,1). We then estimate the p-value p i by the rate of iterations where X rand is ranked above X i. As the p-values calculated using this procedure are prone to the multiple-testing problem, we propose to correct them using the Benjamini Hochberg procedure, such as in the mr-test procedure. Note that the 1Probe method is parametric, as the choice of the distribution of the random probe can have an impact on its rank. The level of impact, however, depends on the ranking method used. Along a similar line, the ACE (for " Artificial Contrasts with Ensembles " ) method (<ref type="bibr" target="#b34">Tuv et al., 2009</ref>) introduces as many random features as there are input variables in the original problem. Each random feature is generated by permuting the values of one original variable. The method then assumes that an original variable is irrelevant if it has a relevance score not statistically higher than that of a random feature. In the original approach, a t-test is applied to determine the significance of each variable and the procedure is actually wrapped into a gradient boosting type algorithm that iteratively selects subsets of important variables. We propose to use a variant of ACE, that we call mProbes, where instead of applying a t-test, we simply compute the proportion of simulations where at least one random feature is ranked above X i. We also drop the gradient boosting procedure and apply the approach in one single run. The value associated with X i that is returned by mProbes thus estimates the FWER when selecting X i and all the variables ranked above X i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Introduction of random probes (1Probe and mProbes)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Computational complexity</head><p>Although computing time is not a real issue in most applications,<ref type="figure" target="#tab_1">Table 1</ref>shows the computational complexity of each method. Except err-A and err-TRT, all the methods have a common parameter P, which is the number of iterations or permutations. The higher the value of P, the better the (Monte Carlo) estimate of the FDR/FWER/p-value. In all our experiments, P was fixed to a typical value of 1000, that gives a good compromise between accuracy and computing times. Obviously, in the context of a specific study, P could be increased to improve precision if needed. Among all methods, the mr-test has the lowest complexity as A is run on only half of the instances of the learning sample in each iteration. On the other hand, the eFDR and CER have the higher complexities if one wants to compute these measures for all m variables. However, as suggested by<ref type="bibr" target="#b24">Huynh-Thu et al. (2008)</ref>, the computing times of these procedures can be</p><formula>CER M ×P ×C A (n,m) nFDR P ×C A (n,m) eFDR M ×P ×C A (n,m) mr-test P ×C A ( n 2 ,m) 1Probe P ×C A (n,m+1) mProbes P ×C A (n,2m) err-A C A (n,1)+C A (n,2)+···+C A (n,m) err-TRT m×n×logn</formula><p>P is the number of iterations, M is the number of variables for which one wants to compute the eFDR/CER, C A (n,m) is the computational complexity of algorithm A when applied on a learning sample with n instances and m variables. For RFs,</p><formula>C RF (n,m) = O( √ m.n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical interpretation of importance scores</head><p>reduced by stopping them as soon as the eFDR/CER is greater than some significance level. It is also worth mentioning that all the methods can be easily parallelized on a computing grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATASETS AND PROTOCOL</head><p>We describe in this section the artificial and real datasets that we used for our experiments, the performance metrics and the compared ranking methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Artificial datasets</head><p>We generated two families of artificial problems to validate the feature selection methods in a context where relevant variables are perfectly known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear</head><p>This is a linear two-class classification problem. All input variables are continuous and their values are sampled from N (0,1). The output Y is given by</p><formula>Y = sgn p i=1 w i X i ,</formula><formula>(7)</formula><p>where the values of w i are uniformly distributed random numbers between 0 and 1. In addition, 1% of the output labels are randomly flipped. Irrelevant variables, in the form of pure Gaussian noise, are added to the p relevant variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypercube</head><p>This two-class classification problem was generated by adapting the MATLAB ® (http://www.mathworks.com/) code originally used to produce the Madelon dataset for the NIPS feature selection challenge. (http://www.clopinet.com/isabelle/Projects/NIPS2003/) All input variables are continuous and their values are sampled from N (0,1). Each class is composed of a number of Gaussian clusters that are placed at random on the vertices of a hypercube in a p-dimensional space, where p is the number of relevant variables. Unlike the previous problem, the decision boundary is thus potentially nonlinear. Irrelevant variables, which are pure Gaussian noise, are added to these p variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Microarray datasets</head><p>We performed experiments on six real gene expression datasets (<ref type="figure" target="#tab_2">Table 2</ref>). For each dataset, the goal was to find a subset of genes that helps to discriminate between two groups of patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance metrics</head><p>Each method returns a subset of features that it considers relevant. In the context of the artificial datasets where all relevant features are perfectly known, we used the following metrics to evaluate such a subset:</p><formula>@BULLET precision = TP S , @BULLET recall = TP P ,</formula><p>where S is the number of selected features, TP is the number of these features that are truly relevant and P is the total number of truly relevant variablesin the considered problem. We also compared the precision and recall levels with the following values: @BULLET p max : the precision of a method (called rec-1) that would select the first k variables of the ranking, where k is the smallest integer such that {X 1 ,X 2 ,.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>..,X k } contains all</head><p>the truly relevant variables (the recall is equal to one). @BULLET r max : the recall of a method (called prec-1) that would select the first k variables of the ranking, where k is the largest integer such that {X 1 ,X 2 ,...,X k } contains only truly relevant variables (the precision is equal to one). Finally, to evaluate a ranking of variables independently of the choice of a specific threshold, we used the area under the precision-recall (AUPR) curve which plots the precision versus the recall for varying thresholds. The higher the AUPR, the better the ranking.<ref type="bibr" target="#b11">Boser et al., 1992</ref>). The goal of a linear SVM is to find a hyperplane in the form</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Compared ranking methods</head><formula>f (X) = m i=1 w i .X i +b, (w i ,b ∈ R)</formula><p>that separates the instances of different classes in the input space with the largest margin, while softly penalizing the instances that are on the wrong side of the hyperplane. The score s i of feature X i is simply taken as the absolute value of the coefficient w i (<ref type="bibr" target="#b21">Guyon et al., 2002</ref>). We used this procedure rather than the wellknown RFE procedure (<ref type="bibr" target="#b21">Guyon et al., 2002</ref>) because it is much less computationally expensive. For our experiments, we used the LIBSVM library (<ref type="bibr" target="#b13">Chang and Lin, 2011</ref>), with the regularization parameter C of the SVM set to 1 (default value), except in<ref type="figure" target="#tab_3">Table 3</ref>where C was set by 10-fold cross validation. Before training, we rescaled the data so that the values of each variable were comprised between −1 and 1 in the learning sample.Even though our methods target multivariate ranking techniques, they can all be applied to univariate techniques as well. We thus included the t-test as a representative of univariate ranking methods, to check the behaviour of the feature selection methods in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Results on artificial and real microarray datasets, obtained using the described methods, are presented in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Artificial datasets</head><p>Comparison of the ranking methods<ref type="figure" target="#fig_5">Figure 1</ref>shows the AUPRs of the three ranking procedures (RFs, linear SVM and t-test) onlinear and hypercube datasets, as well as the AUPRs of a method that returns a random ranking for comparison. The AUPR values were averaged over 50 randomly generated datasets. The three ranking methods perform better than the random procedure. The linear SVM yields the highest AUPRs on the linear datasets, although the t-test performs equally well for a high number of irrelevant features. On the (nonlinear) hypercube datasets, the RFs procedure is the best performer. Figures S1–S3 of Supplementary material show the AUPRs corresponding to the final rankings returned by mr-test, 1Probe and mProbes. Indeed, these procedures each compute a statistical measure (p-value or FWER) associated with each variable X i. These three methods thus potentially modify the original variable ranking by reordering the features according to the corresponding statistical measure. Nevertheless, the new rankings do not change much with respect to the original ranking, their corresponding AUPRs hardly varying. On the other hand, the CER, nFDR and eFDR procedures each estimate a statistic that corresponds to an importance score s i rather than to a variable in itself. Therefore, the variables cannot be reordered according to this statistic, and the monotonicity of the estimated measures is enforced instead (see pseudo-codes in Supplementary material). The enforced monotonicity ensures that a variable X i can be selected only if all the variables ranked above X i are also selected.<ref type="figure" target="#fig_7">Figure 2a</ref>plots the curves of the different methods on a linear dataset with 20 relevant features. The RFs method was used as ranking procedure. At each rank i, we show the relevance score derived from the RFs, as well as the observed FDR, i.e. the proportion of truly irrelevant features among the i topranked variables. Nearly identical observed FDR curves are obtained when the variables are ranked using mr-test, 1Probe and mProbes (Supplementary<ref type="figure" target="#fig_9">Fig. S4</ref>). Therefore, only the observed FDR related to the original ranking is plotted in<ref type="figure" target="#fig_7">Figure 2</ref>, for the sake of clarity. We can see that selecting the variables based solely on the original relevance score is difficult as this score does not suggest any clear threshold (dashed curve in the top of<ref type="figure" target="#fig_7">Fig. 2</ref>). On the other hand, almost all studied methods successfully help to select variables. CER and mProbes provide a good estimation of the FWER as the transition between low and high CER/mProbes values is quite well centred at the point where irrelevant variables start appearing in the ranking (indicated by the observed FDR that becomes &gt;0). The nFDR overestimates the real FDR, as already observed by Huynh<ref type="bibr" target="#b24">Thu et al. (2008)</ref>, whereas the eFDR is closer to it. CER, nFDR, mr-test and mProbes tend to be highly conservative, as their curves increase whereas the observed FDR is still equal to 0. In contrast, the values returned by eFDR and 1Probe become high only when larger subsets of top-ranked variables are considered. err-RF and err-TRT both select a high number of false positives. The minimal error rate of err-RF is obtained when the observed FDR is ∼0.6, meaning that 60% of the selected variables are false positives, while, as expected, err-TRT selects fewer variables. Unlike the other methods, err-RF and err-TRT do not clearly highlight a threshold on the ranking. The error rate does not seem to be affected much by the introduction of irrelevant variables, resulting in rather flat curves. The different methods generate similar curves when applied on a hypercube dataset (Supplementary<ref type="figure" target="#fig_10">Fig. S5</ref>) and when linear SVM is used as ranking procedure instead of RFs (Supplementary Figs. S6 and S7).<ref type="figure" target="#fig_8">Figure 3</ref>shows the precision and the recall of the methods on linear datasets, for different numbers of irrelevant variables. The RFs algorithm was used as ranking procedure and a significance level α = 0.05 was chosen (we used this significance level in all our experiments). Precision and recall values are averaged over 50 datasets. When the number of irrelevant variables increases, the recall of each method decreases. As already observed from the curves, CER, nFDR, mr-test and mProbes are rather conservative. The precision of these methods remains always almost at its highest value and their recall never reaches the recall r max of the prec-1 method. On the other side, eFDR and 1Probe trade some precision, which remains nevertheless high, for a recall that is higher and close to r max. err-RF and err-TRT obtain the highest recall values but also the lowest precision levels. Moreover, these precision levels clearly decrease when the number of irrelevant variables increases. err-TRT tends to select fewer variables than err-RF and has therefore a higher precision. Similar results are observed on hypercube datasets (Supplementary<ref type="figure">Fig. S8</ref>) and when SVM is used as ranking procedure (Supplementary Figs. S9 and S10). When we increase the number of instances in the learning samples, the recall of all the methods increases, as well as the precision of errRF/SVM and err-TRT (Figs. 4, Supplementary S11–S13). We again observe three families of methods: those having a high precision and a recall lower than r max (CER, nFDR, mr-test and mProbes), those having a high precision and a recall close to r max (eFDR and 1Probe), and those with a lower precision and a recall higher than r max (err-A and err-TRT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.A.Huynh-Thu et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretability of the curves</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision and recall of the methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Univariate rankings Figures 2b and 5 show</head><p>, respectively, the score curves and precision/recall values of each method, when the relevance score of a variable is the absolute value of the statistic t computed by a t-test. All the results are similar to those obtained with a multivariate ranking method except for the nFDR which, when used with a t-test, provides a much better estimation of the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical interpretation of importance scores</head><p>(a) (b)and the t-test (right) as ranking method. Score is the relevance score derived from the RFs (left) or the absolute value of the statistic t derived from the t-test (right). Obs. FDR is the observed FDR. The dashed blue (resp. plain red) vertical line indicates the position of the lowest error rate for err-RF (resp. err-TRT)real FDR (<ref type="figure" target="#fig_7">Fig. 2b</ref>) and has a recall equal to r max while having a high precision (<ref type="figure" target="#fig_10">Fig. 5</ref>) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.A.Huynh-Thu et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Microarray datasets</head><p>The evaluation of feature selection techniques on real datasets is difficult as the truly relevant features are unknown on these problems, precluding the computation of any precision-recall values as we did on artificial problems. The purpose of our experiments in this section is thus only to illustrate the behaviour of the different methods on real microarray datasets and to spot any difference with respect to artificial problems.<ref type="figure" target="#tab_3">Table 3</ref>shows the number of genes selected by each method on the six microarray datasets (see Section 4.2), using RFs as the feature ranking method. To highlight the behaviour of the methods on a problem where none of the variables is truly relevant, the last row was obtained by averaging the number of genes selected by each method over 50 new datasets, each one obtained by randomly permuting the output labels of the Prostate1 dataset, while leaving the input features unchanged. Similar experiments with SVM and t-test are reported in Tables S1 and S2 of Supplementary material. A first interesting observation is that the number of selected features is very problem-dependent. There are almost no features selected on Breast and Lymphoma2, few on Lymphoma1 and a significant number on the other datasets. There seems to be no correlation between the number of selected features and problem size. For example, the Leukemia and Lymphoma2 datasets contain about the same number of features and samples, while there are no feature selected on Lymphoma2 and several on Leukemia.As expected, the number of selected genes is close to 0 on the permuted Prostate1 dataset with all methods except err-RF and errTRT. These observations thus suggest that all the evaluated feature selection methods, except those based on prediction performance, can adapt to the problem and ranking quality. To further illustrate the interpretability of the proposed measures on microarray datasets,<ref type="figure" target="#fig_11">Figure 6</ref>shows RFs importance scores and mProbes FWER estimates for increasing rank on three datasets: Prostate1, Lymphoma2 and Prostate1-permuted. As was observed in<ref type="figure" target="#fig_7">Figure 2a</ref>on artificial problems, RFs importance scores, which are very similar on the three datasets, do not suggest any clear threshold. The mProbes method on the other hand shows that about 100 features are relevant on the Prostate1 dataset and that there is no significant feature found by the RF method on the Lymphoma2 and Prostate1-permuted datasets. Comparing the different feature selection methods, we observe that the eFDR method leads to selections of large subsets of genes, whereas the other methods are more conservative. Compared with the artificial problems, the largest subsets are no longer obtained by err-A and err-TRT. These two methods select relatively small numbers of genes because an error rate close to zero is typically achieved by many subsets of genes (see Supplementary<ref type="figure" target="#fig_5">Fig. S14</ref>). This can be explained by the low number of instances compared with the number of genes and the fact that the error rate was estimated on instances that were also used to compute the gene ranking (the so-called selection bias,<ref type="bibr" target="#b9">Ambroise and McLachlan, 2002</ref>). Another difference compared with the results obtained on the artificial data is the fact that the 1Probe procedure appears to be more stringent here. One potential explanation for this difference is that this method corrects for multiple tests by using the Benjamini Hochberg procedure, which might be more conservative than the permutation-based correction embedded in the other methods. For each problem, the number of genes selected by one method is also very much dependent on the chosen ranking procedure. For most methods, using linear SVM leads to the selection of very small subsets of genes (see Supplementary<ref type="figure" target="#tab_1">Table S1</ref>) whereas much larger subsets are selected with the t-test (see Supplementary<ref type="figure" target="#tab_2">Table S2</ref>). Given a feature selection method and a ranking algorithm A, the number of selected genes can also vary depending on the tuning of the parameters of A. As an example, one parameter of RFs is the number T of trees that are grown in an ensemble. Increasing T from 1000 to 10 000 results in larger subsets of selected genes for all the methods except err-RF and err-TRT (<ref type="figure" target="#tab_4">Table 4</ref>). Huynh<ref type="bibr" target="#b24">Thu et al. (2008)</ref>already observed this phenomenon for the nFDR and the CER. Due to the very small sample to dimension ratio, the random trees, and thus the corresponding rankings, are highly unstable.Averaging a very large number of trees results in a stabilization and an improvement of the feature ranking, and thus the possibility to select more variables without including any false positive. However, in spite of this improvement, err-RF and err-TRT do not select more genes, again suggesting that the error rate is not a relevant criterion to assess the quality of subsets of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical interpretation of importance scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>In this article, we evaluated several procedures that aim to identify a maximal subset of variables that truly provide some information about an output variable. These procedures assume that a (multivariate) ranking method A was first used to compute a relevance score for each variable of the considered problem and then extract relevant features from this ranking, by replacing the original relevance score with a measure that can be interpreted in a statistical way. Depending on the procedure, this measure is either the generalization error of a predictive model (err-A and err-TRT), the FDR (nFDR, eFDR), the FWER (CER, mProbes) or a p-value (mrtest, 1Probe). Although there is still a need to determine a threshold on this new measure, the determination of this threshold is clearly easier due to its interpretability. This threshold is also not dependent anymore on the problem at hand and on the ranking method A. Among the feature selection methods that we evaluated, err-A and err-TRT are the only ones that do not require to choose a significance level a priori. However, on artificial problems, they always have the lowest precision among all methods and they wrongly select a non-negligible number of variables on the permuted Prostate1 datasets. Moreover, they are subject to the selection bias problem, preventing the selection of an adequate number of variables. Prediction performance is thus clearly not an appropriate measure for the identification of relevant features. Although they clearly highlight a threshold on the feature ranking, several of the remaining methods have also some disadvantages. The nFDR method is the simplest one but was shown to overestimate the real FDR in the case of dependent scores (<ref type="bibr" target="#b24">Huynh-Thu et al., 2008</ref>). The drawback of the 1Probe procedure is that the selected variables depend on the chosen distribution of the random probe, which makes it a parametric method. The mr-test method estimates the rank distribution of an irrelevant variable from the k variables with the highest observed ranks. The determination of k thus introduces some dependency on the problem and ranking method used. Although our default choice seems to be robust, an inappropriate value of k can lead to a dramatic over-or underestimation of the p-values. The mrtest also includes as a second parameter the number of resampled instances at each iteration. Among the three remaining methods, CER and mProbes are highly selective methods that avoid the inclusion of any irrelevant feature as much as possible. mProbes has a computational advantage</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:27 13/6/2012 Bioinformatics-bts238.tex] Page: 1767 1766–1774</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:27 13/6/2012 Bioinformatics-bts238.tex] Page: 1768 1766–1774</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>We validated the feature selection methods in the context of three popular ranking algorithms; two representatives of multivariate techniques and one standard univariate method: @BULLET Importance measures derived from a specific tree-based ensemble method called Random Forests (Breiman, 2001). In a Random Forests (RFs) ensemble, each tree is built from a bootstrap copy of the original learning sample and at each test node, K variables are selected at random among all candidate ones before determining the best split, i.e. the split that reduces the most the class entropy in the resulting subsets of instances. As variable importance measures, we used the importance scores that result from the sum of the total reduction of class entropy brought by each variable over all trees in the ensemble. K was fixed to its default value √ m and ensembles of 1000 trees were grown. @BULLET Importance measures derived from a linear SVM (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>@BULLET</head><figDesc>The absolute values of the t statistics derived from a t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:27 13/6/2012 Bioinformatics-bts238.tex] Page: 1770 1766–1774</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.1.</head><figDesc>Fig. 1. AUPRs of each ranking method, for different numbers of irrelevant features. Random is a method that randomly ranks the variables. Top on linear datasets, bottom on hypercube datasets. The AUPR values were averaged over 50 datasets in each case</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:27 13/6/2012 Bioinformatics-bts238.tex] Page: 1771 1766–1774</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.2.</head><figDesc>Fig. 2. Curves of the different methods on a linear dataset, with the RFs (left) and the t-test (right) as ranking method. Score is the relevance score derived from the RFs (left) or the absolute value of the statistic t derived from the t-test (right). Obs. FDR is the observed FDR. The dashed blue (resp. plain red) vertical line indicates the position of the lowest error rate for err-RF (resp. err-TRT)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.3.</head><figDesc>Fig. 3. Precision and recall on linear datasets, for different numbers of irrelevant features. We used the RFs algorithm as ranking method and α = 0.05. The precision and recall values were averaged over 50 datasets in each case</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig.4.</head><figDesc>Fig. 4. Precision and recall on linear datasets, for different numbers of instances. We used the RFs algorithm as ranking method and α = 0.05. The precision and recall values were averaged over 50 datasets in each case</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig.5.</head><figDesc>Fig. 5. Precision and recall on linear datasets when the t-test is the ranking method and α = 0.05. The precision and recall values were averaged over 50 datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig.6.</head><figDesc>Fig. 6. RFs importances scores and mProbes FWER estimates on the Prostate1, Lymphoma2 and Prostate1-permuted datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:27 13/6/2012 Bioinformatics-bts238.tex] Page: 1773 1766–1774</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Table 1. Computational complexity Method Complexity</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>.logn). For SVMs, C SVM lies between O(m.n 2 ) and O(m.n 3 ).</figDesc><table>Copyedited by: TRJ 

MANUSCRIPT CATEGORY: ORIGINAL PAPER 

[14:27 13/6/2012 Bioinformatics-bts238.tex] 
Page: 1769 1766–1774 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Characteristics of the microarray datasets</figDesc><table>Name 
No. of 
No. of 
No. of 
Reference 
Class 1 Class 2 Features 

Breast 
107 
179 
22 283 
Wang et al. (2005) 
Leukemia 
47 
25 
7129 
Golub et al. (1999) 
Lymphoma1 
22 
23 
4026 
Alizadeh et al. (2000) 
Lymphoma2 
32 
26 
7129 
Shipp et al. (2002) 
Prostate1 
34 
19 
4344 
Dhanasekaran et al. (2001) 
Prostate2 
52 
50 
12 600 
Singh et al. (2002) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 3.</figDesc><table>Number of selected genes for the microarray datasets (α = 0.05), 
using RFs as ranking procedure 

CER nFDR eFDR mr-test 1Probe mProbes err-RF err-TRT 

Breast 
0 
0 
0 
0 
0 
0 
3 0 
1 1 0 
Leukemia 
36 
82 
368 
16 
62 
63 
51 
4 
Lymphoma1 
6 
33 
94 
0 
49 
18 
208 
42 
Lymphoma2 
0 
0 
0 
0 
3 
0 
3 3 
1 0 4 
Prostate1 
58 
73 
391 
18 
54 
91 
5 
3 
Prostate2 
63 
131 
456 
14 
62 
53 
67 
28 

Prostate1-perm. 0.0 
0.1 
0.1 
0.0 
1.4 
0.0 
29.3 
20.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 4.</figDesc><table>Number of selected genes for the Prostate1 dataset (α = 0.05), using 
the RFs as ranking procedure 

T 
CER nFDR eFDR mr-test 1Probe mProbes err-RF err-TRT 

1000 
58 73 
391 
18 
54 
91 
5 
3 
10 000 136 88 
668 
193 
444 
215 
4 
3 

T is the number of grown trees in a RF ensemble. 

</table></figure>

			<note place="foot">© The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">over the CER method while this latter has a nice interpretation when the scores are derived from univariate scores. Finally, the eFDR method is less stringent and trades some precision for a higher recall. The choice between a more or less conservative method clearly depends on the application and, as these three methods all have a very high precision on the artificial data, our advice is thus to use mProbes or CER when a very stringent method is needed (i.e. a very low false positive rate), and eFDR otherwise. Obvious future works include the application of the feature selection methods to other popular machine learning-based ranking methods such as for example Relief (Robnik-Sikonja and Kononenko, 2003). In this article, we performed an empirical evaluation of the different methods that showed their practical utility. As future work, it would be interesting also to better characterize the different methods from a theoretical point of view. This is however not an easy task given the limited theory that exists about some machine learning-based feature ranking methods. Our experiments on the microarray datasets highlighted that the number of selected variables depends strongly on the ranking method and the precise values of its parameters (e.g. the number of trees in RFs). We believe that this number could provide a valid criterion along which to assess and compare different ranking algorithms, which could be used as a replacement for predictive performance. Indeed, a higher number of variables with a low FDR or FWER indicates that it is more unlikely that an irrelevant variable reaches the very top of the feature ranking, and hence that the ranking is more reliable. In the future, we plan to explore further the use of the number of selected variables to tune the parameters (such as, e.g. the number T of ensemble terms in the RF models) of existing methods or even to design new ranking algorithms that would explicitly try to optimize the feature scores defined by the different feature selection methods. Recently, a great interest has raised for the analysis of the stability of feature ranking and selection methods (Abeel et al., 2010; He and Yu, 2010). The rationale behind this analysis is that a good method should lead to the selection of (nearly) identical features when small changes are made to the dataset. We believe that it would be of interest to confront this kind of stability analysis with the approaches presented in the present article. Indeed, unstable feature importance scores are very likely to lead to high FDR/FWER estimates because of the increased chance of an irrelevant feature to get a high importance. On the other hand, since stability is not a sufficient condition for relevant feature selection, the FDR/FWER measures are intrinsically complementary to the stability criterion. Finally, in this article, we focused on the problem of finding all the relevant variables from a ranking, i.e. potentially including features that contain redundant information about the output. In some applications, it would be more interesting to identify a minimal subset of relevant variables, such that no other variable conveys complementary information about the target conditionally to these variables (the so-called Markov boundary; Pearl, 1988). The adaptation of our procedures to solve this problem would be an interesting direction of future research.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors thank the GIGA Bioinformatics platform and the SEGI for providing computing resources, as well as the anonymous reviewers for their valuable suggestions and remarks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>bts238. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1774" to="1766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Huynh-Thu</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Funding: Belgian Federal Science Policy Office (IAP P6/25 BIOMAGNET)</title>
	</analytic>
	<monogr>
		<title level="j">ARC Biomod</title>
		<imprint>
			<publisher>French Community of Belgium</publisher>
			<publisher>French Community of Belgium</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title level="m" type="main">is recipient of a F.R.I.A. fellowship, Y.S. is a Postdoctoral Fellow of the Research Foundation-Flanders (FWO), and P.G. is a research associate of the F</title>
		<author>
			<persName>
				<forename type="first">European</forename>
				<surname>Network Of Excellence</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Pascal2</forename>
				<forename type="middle">V A H</forename>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">REFERENCES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust biomarker identification for cancer diagnosis with ensemble feature selection methods</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Abeel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="392" to="398" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Distinct types of di?use large B-cell lymphoma identiþed by gene expression proþling</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Alizadeh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page" from="503" to="511" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Permutation importance: a corrected feature importance measure</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Altmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1340" to="1347" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Selection bias in gene extraction on the basis of microarray gene-expression data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ambroise</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">J</forename>
				<surname>Mclachlan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nati. Acad. Sci</title>
		<meeting>. Nati. Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="6562" to="6566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: A practical and powerful approach to multiple testing</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Benjamini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Hochberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soci., Ser. B (Methodol</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">E</forename>
				<surname>Boser</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory</title>
		<editor>Haussler, D.</editor>
		<meeting>the 5th Annual ACM Workshop on Computational Learning Theory<address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName>
				<forename type="first">C.-C</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C.-J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="1" to="2727" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Delineation of prognostic biomarkers in prostate cancer</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Dhanasekaran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">412</biblScope>
			<biblScope unit="page" from="822" to="826" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">) k-Nearest-neighbor Bayes-risk estimation</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Fukunaga</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Hostetler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="285" to="293" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<monogr>
		<title level="m" type="main">Resampling-based multiple testing for microarray data analysis</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Ge</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Some step-down procedures controlling the false discovery rate under dependence</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Ge</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stati. Sin</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="881" to="904" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Proteomic mass spectra classification using decision tree based ensemble methods</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Geurts</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3138" to="3145" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Extremely randomized trees</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Geurts</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">R</forename>
				<surname>Golub</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Guyon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Stable feature selection for biomarker discovery</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>He</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Biology and Chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="215" to="225" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting tree-based variable importances to selectively identify relevant variables</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">A</forename>
				<surname>Huynh-Thu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference proceedings</title>
		<meeting><address><addrLine>Antwerp</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="60" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pearl</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Variable selection using svm based criteria</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rakotomamonjy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1357" to="1370" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Theoretical and empirical analysis of ReliefF and RReliefF</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Robnik-Sikonja</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Kononenko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. J</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="23" to="69" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">A review of feature selection techniques in bioinformatics</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Saeys</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2507" to="2517" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Diffuse large b-cell lymphoma outcome prediction by geneexpression profiling and supervised machine learning</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Shipp</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Gene expression correlates of clinical prostate cancer behavior</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Singh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Pitfalls of supervised feature selection</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Smialowski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="440" to="443" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Ranking a random feature for variable and feature selection</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Stoppiglia</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1399" to="1414" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Statistical significance for genomewide studies</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">D</forename>
				<surname>Storey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
		<meeting>. Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="9440" to="9445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature selection with ensembles, artificial variables, and redundancy elimination</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Tuv</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1341" to="1366" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Gene-expression profiles to predict distant metastasis of lymphnode-negative primary breast cancer</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page" from="671" to="679" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<monogr>
		<title level="m" type="main">Resampling-Based Multiple Testing: Examples and Methods for p-Value Adjustment</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">H</forename>
				<surname>Westfall</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">S</forename>
				<surname>Young</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Significance of gene ranking for classification of microarray samples</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinform</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>