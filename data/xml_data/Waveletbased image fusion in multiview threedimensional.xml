
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wavelet-based image fusion in multi-view three-dimensional microscopy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Jose</forename>
								<forename type="middle">L</forename>
								<surname>Rubio-Guivernau</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Biomedical Image Technologies</orgName>
								<orgName type="institution" key="instit2">ETSI Telecomunicación</orgName>
								<orgName type="institution" key="instit3">Universidad Politénica de Madrid</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centro de Investigación Biomédica en Red en Bioingeniería</orgName>
								<orgName type="institution">Biomateriales y Nanomedicina (CIBER-BBN)</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Vasily</forename>
								<surname>Gurchenkov</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institut des Systèmes complexes &amp; NeD</orgName>
								<orgName type="department" key="dep2">Institut de Neurobiologie Alfred Fessard</orgName>
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Gif-sur-Yvette</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Miguel</forename>
								<forename type="middle">A</forename>
								<surname>Luengo-Oroz</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Biomedical Image Technologies</orgName>
								<orgName type="institution" key="instit2">ETSI Telecomunicación</orgName>
								<orgName type="institution" key="instit3">Universidad Politénica de Madrid</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centro de Investigación Biomédica en Red en Bioingeniería</orgName>
								<orgName type="institution">Biomateriales y Nanomedicina (CIBER-BBN)</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Louise</forename>
								<surname>Duloquin</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institut des Systèmes complexes &amp; NeD</orgName>
								<orgName type="department" key="dep2">Institut de Neurobiologie Alfred Fessard</orgName>
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Gif-sur-Yvette</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Paul</forename>
								<surname>Bourgine</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="department">Institut des Systèmes complexes &amp; CREA</orgName>
								<orgName type="institution" key="instit1">École Polytechnique</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Andres</forename>
								<surname>Santos</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Biomedical Image Technologies</orgName>
								<orgName type="institution" key="instit2">ETSI Telecomunicación</orgName>
								<orgName type="institution" key="instit3">Universidad Politénica de Madrid</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centro de Investigación Biomédica en Red en Bioingeniería</orgName>
								<orgName type="institution">Biomateriales y Nanomedicina (CIBER-BBN)</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Nadine</forename>
								<surname>Peyrieras</surname>
							</persName>
							<email>nadine.peyrieras@inaf.cnrs-gif.fr</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institut des Systèmes complexes &amp; NeD</orgName>
								<orgName type="department" key="dep2">Institut de Neurobiologie Alfred Fessard</orgName>
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Gif-sur-Yvette</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Maria</forename>
								<forename type="middle">J</forename>
								<surname>Ledesma-Carbayo</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Biomedical Image Technologies</orgName>
								<orgName type="institution" key="instit2">ETSI Telecomunicación</orgName>
								<orgName type="institution" key="instit3">Universidad Politénica de Madrid</orgName>
								<address>
									<settlement>Madrid</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centro de Investigación Biomédica en Red en Bioingeniería</orgName>
								<orgName type="institution">Biomateriales y Nanomedicina (CIBER-BBN)</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wavelet-based image fusion in multi-view three-dimensional microscopy</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="issue">2</biblScope>
							<biblScope unit="page" from="238" to="245"/>
							<date type="published" when="2012">2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr609</idno>
					<note type="submission">Systems biology Advance Access publication November 9, 2011 Received on June 9, 2011; revised on October 11, 2011; accepted on October 29, 2011</note>
					<note>[17:43 20/12/2011 Bioinformatics-btr609.tex] Page: 238 238–245 Associate Editor: Jonathan Wren A public release, free of charge for non-commercial use, is planned after the publication of this article. Contact: Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Multi-view microscopy techniques such as Light-Sheet Fluorescence Microscopy (LSFM) are powerful tools for 3D + time studies of live embryos in developmental biology. The sample is imaged from several points of view, acquiring a set of 3D views that are then combined or fused in order to overcome their individual limitations. Views fusion is still an open problem despite recent contributions in the field. Results: We developed a wavelet-based multi-view fusion method that, due to wavelet decomposition properties, is able to combine the complementary directional information from all available views into a single volume. Our method is demonstrated on LSFM acquisitions from live sea urchin and zebrafish embryos. The fusion results show improved overall contrast and details when compared with any of the acquired volumes. The proposed method does not need knowledge of the system&apos;s point spread function (PSF) and performs better than other existing PSF independent fusion methods. Availability and Implementation: The described method was implemented in Matlab (The Mathworks, Inc., USA) and a graphic user interface was developed in Java. The software, together with two sample datasets, is available at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Multi-view 3D microscopy on live embryos</head><p>3D + time images of fluorescently labeled cells in live model organisms are essential to developmental biology (<ref type="bibr" target="#b4">Dzyubachyk et al., 2010;</ref><ref type="bibr" target="#b14">Muzzey and van Oudenaarden 2009;</ref><ref type="bibr" target="#b27">Truong and Supatto 2011</ref>). When this kind of images is acquired with enough * To whom correspondence should be addressed. temporal and spatial resolution, tracking of every single cell and reconstructing the cell lineage tree becomes possible (<ref type="bibr" target="#b15">Olivier et al., 2010;</ref><ref type="bibr" target="#b24">Swoger et al., 2010</ref>). Typical optical microscopy techniques (like confocal or two-photon laser scanning microscopy) provide images with resolution along the optical axis considerably worse than lateral resolution. Moreover, image quality gets progressively worse as light travels deeper inside the specimen, meaning that for relatively large specimens (such as zebrafish embryos or larvae) it is impossible to get good images of the whole embryo. To overcome this kind of limitations other microscopy techniques have been developed. Spinning-disk microscopy (<ref type="bibr" target="#b6">Graf et al., 2005</ref>), for instance, provides much higher imaging speed compared to scanned techniques. In theta-microscopy (<ref type="bibr" target="#b22">Stelzer, 1994</ref>), separate illumination and detection optical paths along orthogonal directions are used in order to improve the axial resolution. In a more recent approach called Light-Sheet-based Fluorescence Microscopy (LSFM) (<ref type="bibr" target="#b8">Huisken et al., 2004;</ref><ref type="bibr" target="#b7">Huisken and Stainier, 2009</ref>) widefield detection is combined with a lateral light-sheet illumination along the focal plane of the detection objective. LSFM is claimed to offer several advantages over conventional confocal laser scanning microscopy (CLSM) technique: @BULLET Unlike CLSM where a large part of the specimen is illuminated when recording a single point or plane, LSFM illuminates only the imaged plane, greatly reducing the overall photo-damage of the specimen. @BULLET The system's point spread function (PSF) is the combination of the light-sheet shape and the detection objective's PSF. For this reason, when low numerical aperture detection lenses are used, LSFM achieves significantly better axial resolution than other techniques such as confocal or two-photon fluorescence microscopy. @BULLET In LSFM wide-field detection is used, meaning that a full plane of the volume is acquired at once by an array of detectors (typically a CCD camera). In contrast, laser scanning microscopy (either confocal or two-photon) acquires one pixel at a time. This means that, for identical frame rates, LSFM has much longer per-pixel measurement time, which translates in a higher number of collected photons per-pixel, higher signal-to-noise ratio and dynamic range.Page: 239 238–245of the fusion of the three different views of a sea urchin embryo, acquired at ∼20 h post-fertilization (hpf) is shown at the center. Renders of the individual views are shown in a smaller scale around the fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wavelet-based image fusion in multi-view 3D microscopy</head><p>Another important feature of typical LSFM implementations is their ability to perform multi-view imaging, i.e. to obtain several volumes of the sample with different orientations. This is usually achieved by mounting the specimen under study on a rotating stage, and it takes advantage of the large working distance of water dipping lens objectives or of relatively low NA objectives, which makes the implementation of the rotation stage easier. Each view is a 3D volume formed as a stack of planes acquired sequentially. General schemes of LSFM can be found in the literature (<ref type="bibr" target="#b7">Huisken and Stainier, 2009</ref>) and in Supplementary<ref type="figure" target="#fig_1">Figure S1</ref>. Such multi-view acquisition capability is desirable because, despite the previously mentioned advantages of LSFM over other fluorescence microscopy techniques, single views still show a series of problems (see Supplementary<ref type="figure">Fig. S2</ref>), like an axial resolution still worse than in plane resolution, slices increasingly dim and blurred with depth, etc., which will be further described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Multi-view fusion techniques</head><p>The multi-view imaging capability of LSFM provides extra information with respect to a single-view approach, as those regions of the specimen that are acquired with lower quality in one view will appear sharper and brighter in a different view. To be fully useful, the information of the sample distributed among several volumes by multi-view imaging should be combined into a single volume (<ref type="figure" target="#fig_1">Fig. 1</ref>). Several techniques have been recently proposed for this task (<ref type="bibr" target="#b11">Krzic, 2009;</ref><ref type="bibr" target="#b18">Preibisch et al., 2008 ;</ref><ref type="bibr" target="#b23">Swoger et al., 2007;</ref><ref type="bibr" target="#b25">Temerinac-Ott et al., 2011</ref>). One approach is to pose the problem as a multi-view deconvolution, which can be solved using extensions of classic iterative deconvolution algorithms like Richardson–Lucy (<ref type="bibr" target="#b11">Krzic, 2009</ref>) or Maximum A Posteriori (<ref type="bibr" target="#b23">Swoger et al., 2007</ref>) to the multi-view situation, by updating the estimate using one view at a time. This kind of algorithms performs fine when the actual PSF is well approximated and used in the deconvolution process. Its main drawback is precisely the need for a good estimate of the PSF, which is particularly difficult to obtain when the PSF is spatially variant as observed in the LSFM imaged volume due to the shape of light sheet and thickness of specimens. In (<ref type="bibr" target="#b25">Temerinac-Ott et al., 2011</ref>) a spatially variant Richardson–Lucy is proposed, where the PSF is dynamically estimated based on fluorescent beads embedded in the sample. Finally, it is worth mentioning that PSF-based algorithms require an extremely precise registration of the views. A different approach called content-based fusion (<ref type="bibr" target="#b18">Preibisch et al., 2008</ref>) was recently proposed in which the fused volume is the weighted average of all the available views, using the local entropy of each view as weights. This method is very fast and the results provided show clear improvement with respect to each of the acquired views. We propose here an alternative method for multi-view fusion on the Discrete Wavelet Transform (DWT) (<ref type="bibr" target="#b13">Mallat, 2008</ref>) space. An advantage of such approach when compared to multi-view deconvolution is the fact that it does not require a PSF estimate. Moreover, due to the use of multi-band transforms that decompose each acquired view onto several scales and orientations, our method is able to select the best view independently along each band and can thus be considered as a multi-scale and multi-orientation contentbased fusion. This enhanced orientation discrimination feature is very valuable for LSFM data, as in a given region of the sample two or more views often provide useful information, each one along a different orientation (an example of such situation is provided in Supplementary<ref type="figure" target="#fig_2">Figure S3</ref>). In this kind of situation, the image-space weighted average performed by the content-based fusion method is unable to independently discriminate directional information components on each view, while the proposed method can do so by working in the wavelet-space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem description</head><p>The proposed method is designed to deal with the multi-view volume fusion problem in LSFM. There are mainly two ways in which different 3D fluorescence microscopy acquisitions can be complementary in a multi-view setting (<ref type="bibr" target="#b23">Swoger et al., 2007</ref>): @BULLET Even though LSFM offers a relatively small axial PSF, due to the use of orthogonal illumination and detection axes, axial resolution remains usually worse than in-plane resolution. The in-plane and axial resolution throughout the specimen's volume change roles while it rotates around an axis orthogonal to the detection axis. Two orthogonal views offer the most complementary information in terms of PSF.</p><p>For highly diffusive samples (which applies to living tissues even when relatively transparent) excitation and emission light far from the excitation and detection objective respectively gets severely attenuated/absorbed as it travels through the sample, which results in increasingly degraded image quality deeper into the sample. For large specimens such as the zebrafish embryo or larva, each view will contain useful information for less than half of the sample. The complementarity of views taken from different angles to compensate for this effect is quite clear. The goal of our method is to take a set of volumes, each one corresponding to a different view of the same sample (e.g. live embryo or larva of chosen animal models), and combine the complementary information available in all of them to create a new fused volume of a better overall quality than any original view alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 240 238–245</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.L.Rubio-Guivernau et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Overview</head><p>of the pre-processing workflow. On the left (a) and on the right (b) we find the two possible registration methods, while the final step is depicted in the middle (c). For gray-level registration (a) two additional steps are performed: automatic cropping and pre-alignment, while bead-based registration (b) is directly performed on the acquired volumes. With either registration, the output of this step is a total transformation matrix for each of the input volumes, which is then used (c) to create the registered volumes and masks that will be passed to the fusion method. Both the registered volumes and the masks are padded as needed so that all of them have the same dimensions and all their information is preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-processing</head><p>Several pre-processing steps might be performed before applying the fusion method to the data, e.g. cropping the volumes and, for very large datasets, down-sampling the in-plane resolution to lower the computation time. However, the only necessary step that must be performed before the fusion is the alignment or registration of all the acquired views, because any miss-registration will lead to artifacts in the final fused volume. The rotation angle of the sample mount is known from the acquisition setup. But it only provides a coarse estimation of the actual transformation between views. Several artifacts including the lack of precision and accuracy in the rotation calibration and operation (<ref type="bibr" target="#b11">Krzic, 2009</ref>) make a fine registration necessary. Fine registration of LSFM images is quite challenging, as the depthdependent blurring means that the registration procedure is complicated by information differences between the views. Furthermore, for large specimens, some regions can be totally obscured because of light scattering as described above. To face these problems, more views might be required to ensure sufficient overlap of usable information and external cues such as fluorescent beads in the mounting medium might be required (<ref type="bibr" target="#b19">Preibisch et al., 2010</ref>). Two different approaches have been used during this work, depending on the characteristics of the dataset: @BULLET A bead-based affine registration algorithm (<ref type="bibr" target="#b19">Preibisch et al., 2010</ref>) was used for datasets with fluorescent beads embedded in the mounting medium around the specimen. @BULLET A gray-level affine registration algorithm (<ref type="bibr" target="#b26">Thevenaz et al., 1998</ref>) for datasets with no fluorescent beads, which uses just the information from the image intensity values.</p><p>For small embryos such as sea urchin, bead-less sample preparation and thus gray-level registration is preferred. However, 3D volumes acquired from large embryos like zebrafish show extensive blurring and eventually little overlap between views, and we confirm that the bead-based registration proposed previously (<ref type="bibr" target="#b19">Preibisch et al., 2010</ref>) is a useful strategy. The detailed pre-processing scheme used for each of the registration approaches is depicted in<ref type="figure">Figure 2</ref>. For the gray-level registration, each of the acquired volumes is first automatically cropped in order to reduce as much as possible the data size and thus the time and memory consumption of the registration. Due to the low noise level of LSFM images, their background is quite uniform and such automatic cropping works well. Each cropped volume is then prealigned according to the rotation step configured during the acquisition, and the pre-aligned volumes are then passed to the gray-level affine registration algorithm. The transformation matrices corresponding to pre-alignment and fine registration are then composed to produce the total transformation matrix of each input volume. On the other hand, when bead-based registration is selected no initial cropping is used, mainly because it would not make sense to remove the beads needed for the registration algorithm to perform its task. No prealignment step is needed either for this kind of registration, as it performs well directly on the acquired volumes. Once the total transformation matrix has been computed for each of the views (by composing pre-alignment and fine registration matrices in the gray-level case, or directly obtained from the bead-based registration otherwise) they are applied to the original views in order to obtain the registered volumes, which are padded as needed in order to make all of them have the same dimensions. In parallel to this transformation, binary masks are also computed that indicate which voxels of each registered volume contain actual acquired data. These masks will be later used to prevent border artifacts, as it will be further explained later. In both approaches, the first view is arbitrarily chosen as a reference, and either registration method is used to compute the affine transform matching it with the other views. When all the views have been brought to the reference frame coordinate system, the fusion process can start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">3D wavelet fusion</head><p>There is a family of techniques named image fusion (<ref type="bibr" target="#b17">Piella, 2003</ref>), typically used on 2D images, which can effectively combine information from different sources into a single composite image. Among these techniques, wavelet-based image fusion (<ref type="bibr" target="#b12">Li, 1995</ref>) is one of the most widely used, and has already found some applications in biomedical imaging (<ref type="bibr" target="#b20">Rajpoot et al., 2009</ref>). The main idea behind this approach is to take advantage of the properties of multi-band image decomposition schemes like the discrete wavelet transform. In this kind of decompositions, an input image (or volume) X 0 is decomposed onto several bands (each one corresponding to a specific scale and orientation, for instance)</p><formula>W X 0 = Y 1 ,...,Y K ,X K , where Y k ,k = 1</formula><p>...K are the different bands in which the image is decomposed and X K is the residual low-pass approximation of X 0. Salient features in the input image X 0 become high-energy coefficients in at least one of the decomposition bands Y k , while smooth regions become low-or zero-energy coefficients. So, roughly speaking, by applying a common multi-band decomposition to all the input images available, and then choosing the highest energy coefficients, a fused image combining salient features from all input images is obtained. The detailed process is outlined in<ref type="figure" target="#fig_2">Figure 3a</ref>and the different steps are further described below. Before explaining the different steps, we recall the Page: 241 238–245the registered views X 0 i (i = 1...N) and a corresponding set of masks K 0 i (i = 1...N). Each of the registered views is then passed to the wavelet transform of choice, generating the wavelet decomposition bands Y n,m i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wavelet-based image fusion in multi-view 3D microscopy</head><p>(where n = 1...N is the scale index and m = 1...7 the orientation index) and the low-pass approximation residuals X N i. In parallel the masks are also processed to get the multi-level masks K n i (n = 1...N) needed later. An activity measure a n,m i is computed for each of the wavelet coefficients, and by looking at those activity measures together with the multi-level masks the decision maps M n,m i are computed. Finally, the wavelet coefficients are combined according to the decision maps, giving place to the fusion decomposition</p><formula>Y 1,1 F ,...,Y N,7 F ,X N F</formula><p>, and by applying the inverse transform we get the fused volume X 0 F as output.</p><p>(b) Wavelet decomposition example of a 2D slice, using three scales and three orientations, resulting in nine bands (N = 3 and K = 9). Although in this work we use undecimated wavelet transform, for illustration purposes this figure represents the classical decimated wavelet transform. inputs of the fusion method: @BULLET A set of I volumes X 0 i (i = 1...I), each of them containing a different 3D view of the specimen, which have already been registered, using the first volume X 0 1 as reference. @BULLET The corresponding set of binary masks K 0 i , each of them indicating which voxels on each of the volumes X 0 i contain relevant information. These masks are used, for instance, to tell which voxels of a registered volume come from the actual transformed volume, and which ones are just padding, allowing us to avoid border artifacts due to incomplete specimen coverage in some of the views.</p><p>Transform input volumes: we first apply the wavelet transform of choice to each of the input volumes:</p><formula>W X 0 i = Y 1 i ,...,Y K i ,X K i</formula><p>, where i = 1...I is the view index. Depending on the specific wavelet transform chosen, there will be a different number and arrangement of the bands. If 3D DWT is selected, we get 7 different orientations on each scale, so when N scales are used during the decomposition, we end up with K = 7·N bands. In this case it is more practical to index the bands by scale (n = 1...N) and orientation (m = 1...7):<ref type="figure" target="#fig_2">Figure 3b</ref>shows an example of the decomposition of a single slice of a LSFM Volume, using a 2D DWT with three scales (N = 3) and three orientations on each scale, resulting in K = 3·N = 9 bands. The size of the bands will depend on the precise wavelet transform used. For instance, when using decimated DWT, the bands' size is halved on each level, so that the total number of elements in the whole decomposition W X 0 i is always equal to that of X 0 i. On the other hand, undecimated implementations of DWT, keep the size of each Y n,m i and X N i equal to X 0 i 's size, so each additional level increases the total number of elements in the decomposition. Undecimated implementations are usually preferred for fusion applications due to its shift-invariance property (<ref type="bibr" target="#b0">Amolins et al., 2007;</ref><ref type="bibr" target="#b5">Gonzalez-Audicana et al., 2004;</ref><ref type="bibr" target="#b21">Redondo-Tejedor, 2007</ref>). Generate multi-scale masks: parallel to the transformation of the input volumes, the corresponding masks are processed in order to prepare adequate multi-scale masks: K n i (n = 1...N). These multi-scale masks are designed to ensure that only valid voxels from the input volumes are taken into account when generating the fused volume. For this purpose we define the support of any given wavelet coefficient</p><formula>W X 0 i = Y 1,1 i ,...,Y n,m i ,Y N,7 i ,X N i</formula><formula>(Y n,m i (·) or X n i (·)</formula><p>), as the set of voxels in the corresponding input volume which get involved in the computation of said coefficient. The size of the support will depend on the scale n and on the wavelet family used. When using haar wavelets, for instance, the support will be a 2×2×2 sub-volume for coefficients in scale 1, a 4×4×4 sub-volume for scale 2, and so on. If we denote by S X n i (·) the support of the coefficient X n i (· ), the multiscale masks are built so that K n i (·) takes the value 1 if the original mask K 0 i is equal to 1 at all the voxels v belonging to S</p><formula>X n i (·)</formula><p>, and otherwise it takes the value 0, that is:</p><formula>K n i (· ) = 1 if K 0 i (v ) = 1,∀v ∈ S (X n i (· )</formula><p>) 0 otherwise Supplementary<ref type="figure" target="#fig_3">Figure S4</ref>shows an example of how the use of these multilevel masks prevents the appearance of artifacts when some of the acquired views do not fully cover the specimen. Measure activity: the next step is to compute an activity or saliency measure at every location of each band from all the wavelet decompositions. This will later be used to decide which of the wavelet coefficients are related to salient features in the input volumes, so that they should be included in the fused volume. A common and simple choice, providing very good results, Page: 242 238–245</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.L.Rubio-Guivernau et al.</head><p>is to use the energy or the module of wavelet coefficients. In this context, the activity measure for any given wavelet coefficient Y n,m i</p><formula>(· ) is: a n,m i (· ) = Y n,m i (· )</formula><p>Computation of decision maps: this is the most important step in the process depicted in<ref type="figure" target="#fig_2">Figure 3a</ref>, and it might be considered the core of the fusion method. We mean by decision maps the weights that define how, for each location in every band of the wavelet decompositions (including the residual low-pass approximation), the coefficients from all the input volumes are combined to create the fused volume. As already mentioned, these kind of methods aim at keeping the most salient features, at different scales and orientations, among the input volumes. Having the activity measure from the previous step, a straightforward way to implement such behavior is to create decision maps M n,m i that, for each location, scale and orientation, select the maximum activity measure from the different input volumes. The multi-level masks are taken into account in order to avoid selecting coefficients whose support includes non-valid voxels from the input volumes. This can be achieved with the following decision map equation:</p><formula>M n,m i (· ) = ⎧ ⎨ ⎩ 1 if a n,m i (· )·K n i (· ) = max j=1...I a n,m j (· )·K n j (· ) 0 otherwise</formula><p>However, this definition is not adequate for using on every decomposition band. For instance, depending on the size of the objects present in the input volumes, some scales of their wavelet decomposition might be dominated by noise, so selecting the maximum coefficients on those bands would lead to increased noise in the fused volume. As an alternative for bands featuring noisy components, selecting the minimum activity coefficients instead of the maximum leads to better results. Supplementary<ref type="figure" target="#fig_5">Figure S5</ref>shows a comparison demonstrating how using minimum activity selection on some bands can improve the results. In order to take full advantage of the orientation and scale discrimination capability of the wavelet transform, it is necessary that the decision maps are able to select, in a given region of the image space, information from different views for different decomposition bands. In this sense, the local entropy maps used in content-based fusion, as seen in figure 6 from (<ref type="bibr" target="#b18">Preibisch et al., 2008</ref>), are too smooth. For this reason, in this work more local decision maps like the ones we just defined are preferred. In order to support this statement, in Supplementary<ref type="figure">Figure S6</ref>we show the result of combining wavelet decomposition with local entropy maps, which gives results almost identical to content-based fusion itself. With respect to the low-pass approximations corresponding to each input volume, they should contain similar information after extracting the relevant features into the different bands. For this reason it is usually preferred to average all the low-pass approximations, taking into account only those coefficients whose associate multi-level mask equals 1, which can be implemented by defining a different decision map equation, which is simply:</p><formula>M n,m i (· ) = K n i (· ) I j=1 K n j (· )</formula><p>Other possible strategies are easy to incorporate into the framework by just creating alternative definitions of the decision maps. Combination of wavelet coefficients: once the decision maps are ready, the next step is to combine the coefficients from all the input volume decompositions, generating the wavelet decomposition of the fused volume:</p><formula>Y n,m F (· ) = I j=1 (M n,m j (· )·Y n,m j (· ) ) I j=1 M n,m j (· )</formula><p>As for the residual low-pass approximation of the fused volume X N F , it is computed in the same way by averaging the low-pass approximations of all the available volumes X N i ,i = 1...I. Inverse transform: finally, the wavelet decomposition generated in the previous step is inverted to get the final fused volume.</p><formula>W −1 Y 1,1 F ,...,Y n,m F ,Y N,7 F ,X N F = X 0 F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Algorithm implementation and performance</head><p>The whole algorithm has been implemented in MATLAB , using a custom 3D extension of its undecimated discrete wavelet transform implementation (swt and iswt functions).<ref type="figure" target="#tab_1">Table 1</ref>summarizes the time consumed by the two main parts of the overall process: registration and fusion. Times are provided for the two registrations used and for both the proposed fusion method and the content-based fusion included in the 'SPIM Registration' plug-in of Fiji (http://www.fiji.sc). The results are averages based on several timesteps from each dataset, and to allow comparison despite different volume sizes, times have been normalized to a common volume size of 600×600×600. For wavelet-based fusion, the final inverse transform step is independent of the number of views, so the time has been split in a per-view time plus a fixed time. The current implementation of the proposed method is quite more timeconsuming than content-based fusion. We expect a 5-to 10-fold reduction in computation time when the method is recoded in C. Besides all the process is based on filters implemented as convolutions with rather small kernels, so the code should greatly benefit from parallelization schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Dataset description</head><p>Datasets used in this work were acquired using a Digital Scanned Light-Sheet (DSLM) version of the LSFM microscope (<ref type="bibr" target="#b9">Keller et al., 2008</ref>) where the light sheet is generated by scanning a light beam instead of using a cylindrical lens as in the Selective Plane Illumination Microscope (SPIM) (<ref type="bibr" target="#b8">Huisken et al., 2004</ref>). Our method was applied to various kinds of datasets. The results presented here correspond to two datasets obtained from developing sea urchin (Paracentrotus lividus) and zebrafish (Danio rerio) embryos. Acquisition parameters are summarized in<ref type="figure">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Live sea urchin embryo</head><p>A live sea urchin embryo was imaged for 20 h, from 5 h post fertilization (hpf) until 25 hpf. The whole acquisition time was divided in 180 s intervals, leading to over 400 time steps. At each time step 3 volumes (i.e. views) were acquired, using a 120 @BULLET rotation of the sample between consecutive views. On all the acquired volumes, signal intensity relates to the local concentration of fluorescent protein accumulated in the cell nucleus.The sample did not contain fluorescent beads in this study, so the image registration method used before the fusion is the aforementioned gray-level affine registration (<ref type="bibr" target="#b26">Thevenaz et al., 1998</ref>). After views registration, we applied the wavelet fusion method as described in Section 2.3. A 3D undecimated DWT was used as W X 0 i , with haar wavelet filters and three scales (N = 3). Regarding the different possibilities for computation of decision maps (as discussed in Section 2.3), the following configuration parameters were used for this dataset: @BULLET Minimum activity selection for bands in scale 1 (the finest) which is dominated by noise. @BULLET Maximum activity selection for bands in scales 2 and 3, which contain the most relevant detail information. @BULLET Averaging of the low-pass approximations.<ref type="figure" target="#fig_3">Figure 4</ref>shows an example of the fusion performance, by comparing slices and profiles of the fused volume and the acquired views corresponding to a time step around 20 hpf. Each of the three views provides an incomplete picture of the embryo, with some regions showing sharp and bright nuclei, while others exhibit heavy blurring and even some missing nucleus. Nevertheless, slices from the fused volume present overall sharp nuclei with good contrast. At the bottom of the figure line profiles show that the fusion provides the best representation of cell populations, because even though each view might show increased brightness and contrast for some cells, they might completely miss other ones that are indeed recovered in the fused volume. The whole dataset (406 time steps) was processed similarly. A complete view of the results for this dataset is showed in the video displaying the evolution of the embryo during the 20 h of the imaging procedure (Supplementary Movie S11) 1. Supplementary Movie S11 shows the improvement of individual cell identification in the fused volume that becomes even more obvious as the embryo grows and the individual views are unable to properly capture the details of the whole embryo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 243 238–245</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wavelet-based image fusion in multi-view 3D microscopy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Live zebrafish embryo</head><p>The second example is a live zebrafish embryo imaged for 12 h, starting at 5 hpf. The whole acquisition time was divided in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.L.Rubio-Guivernau et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>We propose a novel methodology to fuse multi-view images, such as those obtained from light-sheet-based fluorescence microscopy, but not limited to that particular technique. Almost any imaging technique in which several volumes of the same object are acquired, each of one providing incomplete and complementary information, could benefit from the proposed fusion scheme, including recent light-sheet techniques like DSLM using structured illumination (<ref type="bibr" target="#b10">Keller et al., 2010</ref>). Our method is based on wavelet multi-scale and multi-orientation decomposition of the input volumes, and does not rely on a priori knowledge of the system PSF. While PSF-aware methods have more potential for reducing PSF-related artifacts, the proposed method can be used on those situations in which such knowledge is not available as is the case with LSFM acquisition where the PSF varies over the volume due to light sheet shape and to distorsion caused by the specimen itself. With respect to the other PSF-independent method mentioned in this work, content-based weighted averaging, the new method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 245 238–245</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wavelet-based image fusion in multi-view 3D microscopy</head><p>has improved orientation and scale discrimination as a direct consequence of performing the fusion in the wavelet domain instead of the intensity space. This improved discrimination is useful for instance in regions of the sample where two views contribute sharp details in different orientations. This work focuses on the fusion method itself, and does not consider the effect of other complementary methods like denoising techniques which could be used in a pre-processing step. For instance wavelet denoising (<ref type="bibr" target="#b2">Chang et al., 2000;</ref><ref type="bibr" target="#b3">Donoho and Johnstone 1994</ref>) and bilateral filtering (<ref type="bibr" target="#b16">Paris et al., 2009;</ref><ref type="bibr" target="#b28">Yang et al., 2009</ref>) seem good choices for future tests. Some variations (cross/joint and dual bilateral filtering) have been used for image fusion in certain situations (<ref type="bibr" target="#b1">Bennett et al., 2007</ref>), but they are not adequate in situations where different views do not cover the full specimen, as uniform regions in the image being filtered will stay uniform regardless of the information provided by the other views. A possible way to further improve the usefulness of the proposed method would be to automatically adapt the wavelet fusion parameters, e.g. number of scales and fusion scheme, to the content of the input volumes. Moreover, the general scheme can be accommodated to any kind of wavelet transform (different base wavelet basis, non-separable decompositions, etc.), making the proposed method highly flexible and thus adaptable to any kind of input volumes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[17:</head><figDesc>43 20/12/2011 Bioinformatics-btr609.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. A rendering of the fusion of the three different views of a sea urchin embryo, acquired at ∼20 h post-fertilization (hpf) is shown at the center. Renders of the individual views are shown in a smaller scale around the fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. (a) Overview of the proposed fusion method. The inputs are a set of volumes containing the registered views X 0 i (i = 1...N) and a corresponding set of masks K 0 i (i = 1...N). Each of the registered views is then passed to the wavelet transform of choice, generating the wavelet decomposition bands Y n,m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.4.</head><figDesc>Fig. 4. Results of the proposed method on a sea urchin embryo, imaged at 20 h post-fertilization (hpf). Each column shows three orthogonal slices from a single volume. Columns 1–3 contain slices from each of the three acquired views, while column 4 contains slices from the volume obtained by our fusion method. Rows 1–3 show slices along XY, XZ and YZ planes, respectively. At the bottom of the figure, three line profiles compare each of the individual views with the fusion. On each profile, the blue plot represents intensity values on the corresponding view, while the yellow line follows intensity on the fused volume. Line segments have been overlaid to the XZ slices (middle row), in parallel to the group of five cells represented by the profiles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.5.</head><figDesc>Fig. 5. Comparison of content-based fusion (left column) and our fusion method (right column) for the zebrafish embryo, same slices as in Supplementary Figure S7. At the bottom, a line profile comparison of both (content-based fusion in yellow and our fusion in blue) across a group of three cells in a blurred region, surrounded by insets showing more detail of the areas where the line profile was obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Table 1.</figDesc><table>Performance evaluation of both registrations and both fusion 
methods 

Step 
Method 
Time (min/view) 
Fixed Time (min) 

Registration 
Gray-level 
24.5 
0 
Bead-based 
1.8 
0 
Fusion 
Wavelet 
64.7 
19.2 
Content-based 
1.5 
0 

All tests were performed on a Intel® Xeon® E5506@2.13 GHz with 48 GB RAM. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. Acquisition parameters for the two datasets used for this work</figDesc><table>Animal 
[Dataset ID] 

Objective 
Volume size 
(voxels) 

Voxel size (μm) 
No. of 
views 
[angle] 

No. of 
time 
steps 

Sea urchin 
[090916eS] 

Zeiss 
40×/1.0NA W 

972×972×82 0.185×0.185×1.85 3 [120 @BULLET ] 406 

Zebrafish 
[100728aS] 

Zeiss 
10×/0.3NA W 

600×600×111 1.48×1.48×5.55 
5 [72 @BULLET ] 281 

</table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> Direct download available at http://www.die.upm.es/im/videos/SPIM/ Movie_S11.avi (last accessed date November 27, 2011).</note>

			<note place="foot" n="155"> s intervals, leading to over 280 time steps. At each time step five volumes (i.e. views) were acquired, using a 72 • rotation of the sample between consecutive views. In all the acquired volumes, signal intensity correlates with the fluorescent protein concentration in the cell nucleus. Fluorescent beads were added to the mounting medium, so that the bead-based registration method could be used. This means that the five acquired views show little overlapping information as shown in Supplementary Figure S7. For this reason, standard gray-based registration would be very challenging for this dataset, and beadbased registration is preferred. The last column in Supplementary Figure S7 shows the result of fusing the five available views (columns 1–5). The configuration parameters used for this dataset, including wavelet family, number of scales and decision maps computation scheme, were identical to the ones described in Section 3.1 for the sea urchin dataset. The fusion represents the full embryo better than any of the individual volumes, and, while some regions of the embryo are brighter in one specific view, the fusion captures all the relevant information, keeping good contrast whenever such contrast exists in any of the views (Supplementary Movie S12) 2 It is worth mentioning that, due to the size of this embryo, each view appears highly blurred in some regions, but the fusion method is able to deal with this situation and gives preference to non-blurred information from other views. Supplementary Figure S8 shows snapshots of the renders available in the video, with the five original views around the resulting fused volume. For both Supplementary Figure S8 and Movie S12, a hard threshold was applied to the original and fused volumes in order to reduce the number of visible beads in the renders. 3.3 Comparison with existing methods For both datasets, we have also compared our results with those of the previously mentioned content-based weighted averaging (Preibisch et al., 2008), which is also PSF independent and freely available as part of Fiji open-source image processing package. The comparison between our method and Preibisch et al.&apos;s method is shown in Supplementary Figure S9 for the sea urchin embryo, and in Figure 5 for the zebrafish. The results of the wavelet-based fusion method look less blurred, and more individual nuclei can be distinguished. The latter is supported by the line profile comparison through a group of three nuclei in a blurred region (Fig. 5). For the sea urchin, the difference is less striking, but the line profiles in Supplementary Figure S9, reveals slightly better peak contrast with our wavelet fusion method (in blue) compared to the content-based fusion (in yellow). Additionally, a quantitative comparison was carried out on two different timesteps for both datasets, by measuring the contrast of several line profiles, each one passing through two adjacent nuclei. An overall 31% improvement was measured for the proposed method in comparison to content-based fusion. The details are provided in Supplementary Figure S10. 2 Direct download available at http://www.die.upm.es/im/videos/SPIM/ Movie_S12.avi (last accessed date November 27, 2011).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Wavelet based image fusion techniques</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Amolins</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J Photogramm</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="249" to="263" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Multispectral bilateral video fusion</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">P</forename>
				<surname>Bennett</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1185" to="1194" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive wavelet thresholding for image denoising and compression</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">G</forename>
				<surname>Chang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1532" to="1546" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Threshold selection for wavelet shrinkage of noisy data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">L</forename>
				<surname>Donoho</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">M</forename>
				<surname>Johnstone</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 16th Annual International Conference of the IEEE EMBS. IEEE</title>
		<meeting>16th Annual International Conference of the IEEE EMBS. IEEE<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="24" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated analysis of time-lapse fluorescence microscopy images: from live cell images to intracellular foci</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Dzyubachyk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2424" to="2430" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusion of multispectral and panchromatic images using improved IHS and PCA mergers based on wavelet decomposition</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gonzalez-Audicana</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Geosci. Remote</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1291" to="1299" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Live cell spinning disk microscopy</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Graf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Biochem. Eng. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="57" to="75" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Selective plane illumination microscopy techniques in developmental biology</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huisken</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">Y R</forename>
				<surname>Stainier</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Development</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="1963" to="1975" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Optical sectioning deep inside live embryos by selective plane illumination microscopy</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huisken</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="page" from="1007" to="1009" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Reconstruction of zebrafish early embryonic development by scanned light sheet microscopy</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Keller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page" from="1065" to="1069" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast, high-contrast imaging of animal development with scanned light sheet-based structured-illumination microscopy</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Keller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="637" to="642" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiple-View Microscopy with Light-Sheet based Fluorescence Microscope</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Krzic</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Multisensor Image Fusion Using the Wavelet Transform</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph. Model Im. Proc</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">A Wavelet Tour of Signal Processing</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mallat</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Academic Press</publisher>
			<pubPlace>Burlington, MA</pubPlace>
		</imprint>
	</monogr>
	<note>3rd. edn</note>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantitative time-lapse fluorescence microscopy in single cells</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Muzzey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Oudenaarden</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Van</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Cell Dev. Biol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="301" to="327" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Cell lineage reconstruction of early zebrafish embryos using label-free nonlinear microscopy</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Olivier</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">329</biblScope>
			<biblScope unit="page" from="967" to="971" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<monogr>
		<title level="m" type="main">Bilateral Filtering: Theory and Applications. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Paris</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">A general framework for multiresolution image fusion: from pixels to regions</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Piella</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Fusion</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="280" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Mosaicing of single plane illumination microscopy images using groupwise registration and fast content-based image fusion</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Preibisch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>. SPIE</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="69140" to="69141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Software for bead-based registration of selective plane illumination microscopy data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Preibisch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="418" to="419" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<monogr>
		<title level="m" type="main">Multiview RT3D Echocardiography Image Fusion</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Rajpoot</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="134" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">New Contributions on Image Fusion and Compression Based on Space-Frequency Representations</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Redondo-Tejedor</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Madrid</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Fundamental reduction of the observation volume in far-field light microscopy by detection orthogonal to the illumination axis: confocal theta microscopy</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Stelzer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Commun</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="536" to="547" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-view image fusion improves resolution in threedimensional microscopy</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Swoger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Express</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">8029</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">4D retrospective lineage tracing using SPIM for zebrafish organogenesis studies</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Swoger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biophotonics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="122" to="134" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatially-variant Lucy-Richardson deconvolution for multiview fusion of microscopical 3D images</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Temerinac-Ott</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<meeting><address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="899" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">A pyramid approach to subpixel registration based on intensity</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Thevenaz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="27" to="41" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward high-content/high-throughput imaging and analysis of embryonic morphogenesis</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">V</forename>
				<surname>Truong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Supatto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genesis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="555" to="569" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time O(1) bilateral filtering</title>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>