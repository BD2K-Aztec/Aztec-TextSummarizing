
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Positive and negative forms of replicability in gene network analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">W</forename>
								<surname>Verleyen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Cold Spring Harbor Laboratory</orgName>
								<orgName type="institution">Stanley Institute for Cognitive Genomics</orgName>
								<address>
									<addrLine>500 Sunnyside Boulevard Woodbury</addrLine>
									<postCode>11797</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">S</forename>
								<surname>Ballouz</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Cold Spring Harbor Laboratory</orgName>
								<orgName type="institution">Stanley Institute for Cognitive Genomics</orgName>
								<address>
									<addrLine>500 Sunnyside Boulevard Woodbury</addrLine>
									<postCode>11797</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">J</forename>
								<surname>Gillis</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Cold Spring Harbor Laboratory</orgName>
								<orgName type="institution">Stanley Institute for Cognitive Genomics</orgName>
								<address>
									<addrLine>500 Sunnyside Boulevard Woodbury</addrLine>
									<postCode>11797</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Positive and negative forms of replicability in gene network analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btv734</idno>
					<note type="submission">Received on 13 July 2015; revised on 7 December 2015; accepted on 9 December 2015</note>
					<note>*To whom correspondence should be addressed. Associate Editor: Igor Jurisica Availability and implementation: Algorithms, network data and a guide to the code available at: https://github.com/wimverleyen/AggregateGeneFunctionPrediction. Contact: jgillis@cshl.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Gene networks have become a central tool in the analysis of genomic data but are widely regarded as hard to interpret. This has motivated a great deal of comparative evaluation and research into best practices. We explore the possibility that this may lead to overfitting in the field as a whole. Results: We construct a model of &apos;research communities&apos; sampling from real gene network data and machine learning methods to characterize performance trends. Our analysis reveals an important principle limiting the value of replication, namely that targeting it directly causes &apos;easy&apos; or unin-formative replication to dominate analyses. We find that when sampling across network data and algorithms with similar variability, the relationship between replicability and accuracy is positive (Spearman&apos;s correlation, r s $0.33) but where no such constraint is imposed, the relationship becomes negative for a given gene function (r s $ À0.13). We predict factors driving replicability in some prior analyses of gene networks and show that they are unconnected with the correctness of the original result, instead reflecting replicable biases. Without these biases, the original results also vanish replicably. We show these effects can occur quite far upstream in network data and that there is a strong tendency within protein–protein interaction data for highly replicable interactions to be associated with poor quality control.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Increasingly, biologists have turned to computational methods to sift through the vast array of pre-existing genomics data for validation that a gene has a molecular role in the phenotype of interest or to prioritize a candidate as disease causal (<ref type="bibr" target="#b31">Moreau and Tranchevent, 2012;</ref><ref type="bibr" target="#b48">Wang and Marcotte, 2010</ref>). These computational methods usually fit under the rubric of 'machine learning' and use network data that represent the interaction of genes or their products. Many of these computational methods depend on a form of 'guilt by association', in which a gene is inferred to possess a particular function based on its similarity to other genes with that function (<ref type="bibr" target="#b35">Oliver, 2000</ref>). The most common form of similarity used in these tasks is that of genomic sequence similarity which is easily implemented through supervised use of BLAST (<ref type="bibr" target="#b0">Altschul et al., 1990</ref>) and comparatively straightforward to interpret. While sequence-based analysis is essentially routine within biology, one of the promises of systems biology has been to extend the form of 'association' used to relate genes to potentially subtler relationships, such as protein–protein interaction (PPI), co-expression, genetic interaction or phylogenetic profiles. Systems-based prediction of gene function has found particular application in the interpretation of disease-causal variants due to the difficulty of finding overlaps in V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com known functions among candidate genes (<ref type="bibr" target="#b15">Geschwind, 2008;</ref><ref type="bibr" target="#b19">Greene and Troyanskaya, 2012;</ref><ref type="bibr" target="#b33">Oellrich et al., 2012</ref>). However, progress in the context of both data and methodology has been surprisingly uncertain (<ref type="bibr" target="#b38">Pavlidis and Gillis, 2013</ref>). The need for better assessment of methods in function inference and network analysis is widely recognized and has led to numerous field-wide evaluations, often called critical assessments (<ref type="bibr" target="#b7">Bornigen et al., 2012;</ref><ref type="bibr" target="#b25">Kryshtafovych et al., 2014;</ref><ref type="bibr" target="#b40">Pena-Castillo et al., 2008;</ref><ref type="bibr" target="#b43">Radivojac et al., 2013</ref>). The two principal goals of critical assessments are (i) to make the performances of individual methods less prone to overfitting and (ii) for comparisons between methods to be within the same framework. Overfitting is minimized since participants are truly blind to the success of their method prior to assessment and thus cannot 'tailor' their solutions to the benchmarking metric. Gene networks possess unusually prominent consensus resources [e.g. the Gene Ontology (GO) (<ref type="bibr" target="#b2">Ashburner et al., 2000</ref>), BioGRID (<ref type="bibr" target="#b46">Stark et al., 2006)]</ref>, making evaluation within a welldefined framework possible. By reducing overfitting and making methods directly comparable, critical assessments endeavor to make science more replicable; their outputs and comparative evaluations can be trusted to generalize. The difficulty of characterizing the features in gene networks that drive successful uses has contributed to making replicability in their output, which can be more easily measured, particularly important to evaluation within their critical assessments [e.g. the DREAM challenges (<ref type="bibr" target="#b26">Marbach et al., 2012</ref>) and the Critical Assessment of protein Function Annotation algorithms, CAFA challenge (<ref type="bibr" target="#b43">Radivojac et al., 2013)]</ref>. In performing this evaluation, critical assessments are simply performing a more top–down version of the usual scientific process of refinement through replicability (<ref type="bibr" target="#b14">Fisher, 1935</ref>). While this may be desirable in some ways, it creates a new potential for overfitting for the field in its entirety. We decided to explore this possibility by simulating multiple gene function prediction tasks and outcomes and hence the field of gene network analysis as a whole. In our model of research in gene network analysis, each separate researcher is represented by an individually developed machine learning algorithm with access to particular data. The algorithms are both diverse and in common use for diverse bioinformatics problems and thus reasonably reflecting ordinary practice. The data resources (or 'library') given to these algorithms are similarly diverse and frequently used sources of human gene network information, varying from individual expression profiles to consensus pathway information. We refer to a specific combination of algorithm and data as a 'researcher' (<ref type="figure">Fig. 1</ref>). For example, a researcher may consist of the algorithm 'random walk with restarts' using specific co-expression data. The individual sampled resources do not represent partial data sets but rather ones which are at least as comprehensive as is typical of any given study. Because it is a central characteristic by which we judge results, our focus is on using these model researchers to understand replicability in gene network analysis. After deriving general principles through our simulations, we focus on two important applications affecting the interpretation of disease genes and PPI data in current research, with a focus on psychiatric genetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Our analysis occurs in two parts. In the first, we build a model of researchers assessing gene networks data, and in the second, we work through an application using PPI network (PPIN) data to characterize genes linked to autism and schizophrenia (SCZ). To build models of researchers, each using a network analysis method and data, we need to assemble these resources. In general, each of our network analysis methods is operating as a gene function prediction method. These methods consist of three components: functional annotations, biological data or network and a machine learning algorithm. We describe the functional annotations in Section 2.1, data resources in Section 2.2 and algorithms in Section 2.3. Because our model is concerned with the behavior of these methods in comparison to one another, we then describe methods for evaluating their aggregate accuracy (Section 2.4) and replicability (Section 2.5). We close our model analysis by evaluating variation at two different time points (corresponding to the start and close of the project, Section 2.6). We then move to an application of the principals derived from the model in the characterization of network properties of genes linked to autism and SCZ (Section 2.7). This suggests replicable interactions in PPI data may be problematic, which we directly evaluate using quality control data, with methods described in Section 2.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ontology and annotations (GO)</head><p>The GO (revision 1.1363) (<ref type="bibr" target="#b2">Ashburner et al., 2000</ref>) and GO annotations (date:<ref type="bibr">April 23, 2014</ref>) were used as gene annotations for gene function prediction. We first propagated the genes through the GO hierarchy and then filtered for GO terms with associated gene sizes ranging between 20 and 1000 genes and excluded associations with evidence codes from IEA (inferred from electronic annotation). Filtering GO terms within this range shows stable performance (<ref type="bibr" target="#b16">Gillis and Pavlidis, 2011</ref>). A total of 2930 GO terms fit this criterion. To minimize selection biases, we considered the fixed set of genes with at least one GO annotation and which were present in our microarray expression data; this totaled 12 529 annotated genes. To make the GO analysis more tractable, we used a filtered subset of GO terms from GO slim (date:<ref type="bibr">April 24, 2014</ref>), totaling 109 GO terms developed by the GO consortium. It shows similar performance when compared to the filtered subset of complete GO and was therefore appropriate for our analyses (<ref type="bibr" target="#b47">Verleyen et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data resources and gene sets for network construction</head><p>We collected three different types of gene association/interaction data for our networks: (i) PPI, (ii) semantic similarity and (iii) co-expression. Each data resource was parsed into a network, described in more detail in the following sections. We converted all protein IDs and gene symbols into gene Entrez IDs using HUGO (<ref type="bibr" target="#b49">White et al., 1997</ref>). As above (Section 2.1), all the genes that had a gene association in the GO and that overlapped with the co-expression data, totaling 12 529 genes, were used as our basis gene set. We also restricted our genes to this same set once constructing networks based upon PPI and semantic similarity data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Protein–protein interaction networks</head><p>We constructed PPINs from five different databases: (i) BioGRID (<ref type="bibr" target="#b10">Chatr-aryamontri et al., 2013</ref>), (ii) HIPPIE (<ref type="bibr" target="#b44">Schaefer et al., 2012</ref>), (iii) IntAct (<ref type="bibr" target="#b36">Orchard et al., 2014</ref>), (iv) I2D (<ref type="bibr" target="#b9">Brown and Jurisica, 2007</ref>) and (v) GeneMANIA (<ref type="bibr" target="#b50">Zuberi et al., 2013</ref>). Each database has gene–gene or PPIs listed, which were used to create a binary network or a weighted network, depending on the available information. Data from the BioGRID database (version 3.2.111) were used to construct a binary network from all physical interactions and no further filtering on experimental type was applied. The HIPPIE database (version 1.6) was used to construct a weighted network, with no filtering on the data. The IntAct database (version 2.0; downloaded on April 22, 2014) was used to construct a weighted network, filtered on interactions from Homo sapiens (taxonomic ID 9606). The I2D database (version 2.3) was used to construct a binary network. The interactions are filtered from the following original databases in I2D: BioGRID, IntAct, HPRD, BIND, MINT and INNATEDB. The GeneMANIA data (version date October 24, 2013) was used to construct a binary network. For all networks, only the physical interactions from the databases were incorporated. Summary network properties of the networks are shown in Supplementary Table S1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Semantic similarity networks</head><p>We constructed semantic similarity profiles based upon five databases: (i) KEGG (<ref type="bibr" target="#b34">Ogata et al., 1999</ref>), (ii) Reactome (Joshi<ref type="bibr" target="#b24">Tope et al., 2005</ref>), (iii) Phenocarta (<ref type="bibr" target="#b42">Portales-Casamar et al., 2013</ref>), (iv) InterPro (<ref type="bibr" target="#b22">Hunter et al., 2012</ref>) and (v) Pfam (<ref type="bibr" target="#b13">Finn et al., 2014</ref>). The semantic similarity is defined as equal to the Jaccard index between two genes over the set membership they exhibit in each of the five listed databases (<ref type="bibr" target="#b30">Mistry and Pavlidis, 2008</ref>). Weighted semantic similarity networks were constructed from the semantic similarity of all gene pairs, so that the weight of each gene–gene edge in the network is equal to the Jaccard index. The KEGG database was used to construct a similarity profile based upon 274 biological pathways. The Reactome database (version date May 7, 2014) was used to construct a similarity profile based upon 128 biological pathways. The Phenocarta database (version date May 1, 2014) was used to construct a similarity profile based upon 2923 terms. The InterPro database (version 46.0) was used to construct a similarity profile based upon 7137 shared protein domains. The Pfam database (version 27.0) was used to construct a similarity profile based upon 6142 shared protein domains. Summary network properties of the networks are shown in Supplementary Table S2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Co-expression networks</head><p>We generated aggregate co-expression networks constructed from publically available expression data sets (Gene Expression Omnibus and Sequence Read Archive), using either RNA-seq or microarraybased studies as previously described (<ref type="bibr" target="#b3">Ballouz et al., 2015</ref>). For each aggregate network, 20 individual co-expression networks were selected. Each co-expression network was constructed by taking the Spearman correlation coefficient of the gene pairs as an edge weight. Each value was then ranked, and the aggregate was the sum of these ranked weights. The aggregate network was then thresholded for the top 1% connections. Four such aggregate networks were constructed, two from microarray datasets and two from RNA-seq data sets. The individual experiments for each aggregate are listed in Supplementary Table S3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Machine learning algorithms</head><p>A set of six machine learning algorithms were chosen in our analyses. We selected algorithms that are well established for gene function prediction and those that belong to different machine learning categories. We picked three network inference algorithms based upon different mathematical formalizations: (i) neighbor voting, (ii) GeneMANIA and (iii) random walk with random restart (implemented for this project specifically, see our supporting information and its earlier use for general properties). These algorithms typically exploit topological characteristics of the network. We also implemented less specialized algorithms which interpret network data as sets of features (i.e. the feature data for a given gene is its connectivity profile with other genes). We selected (iv) logistic regression as it is perhaps the most fundamental machine learning algorithm for building classifiers as well as two online or lazy setting algorithms, (v) support vector machine with a stochastic gradient descent solver and (vi) the passive aggressive approach. All three of these more general methods were implemented using sckit-learn (<ref type="bibr" target="#b39">Pedregosa et al., 2011</ref>). The output of each gene function prediction task is, for a given function, a vector of ranked values (across all the genes)<ref type="figure">Fig. 1</ref>. Modelling replicability in gene network analysis. A library of algorithms and data is sampled from the complete library to be made available to a given research community. The set of researchers for that community is constructed by sampling from their available resources. Each instantiated researcher tries to predict gene function held back from that community based on prior knowledge. Replicability is assessed by comparing each researcher to the consensus of the remainder while performance of the total research community (consensus) is assessed with reference to the held back knowledge (cross-validation) indicating the probability of the gene belonging to the function. Performance is calculated using 3-fold cross-validation. We describe each algorithm in more detail in the Supplementary Information and have previously benchmarked them in yeast (<ref type="bibr" target="#b47">Verleyen et al., 2015</ref>). We repeat the benchmarking task on human data (see Supplementary Figs S1–S3). We calculate Spearman correlation coefficients (r s ) throughout using the scipy.stats.spearmanr function, which also calculates a p value from a two-sided test of noncorrelation (http://www.scipy.org/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Aggregation of methods</head><p>Aggregation is a fundamental approach to create more robust computational models and improve overall performance of these models under their given task. We have performed aggregation at the level of the output scores of a predictor. We define a predictor as the output from a gene function prediction method containing an algorithm and a parsed network. For algorithm aggregation, we used the same network and aggregated the output scores of each algorithm. For data aggregation, we used the same algorithm on different networks and combined their scores. Note that our results are robust to more sophisticated aggregation strategies, such as weighting (e.g. correlation-based feature selection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Replicability</head><p>In our application of replicability in gene function prediction, we have variability stemming from the choice of machine learning algorithms and data resources. We calculated the replicability in our gene function prediction task by measuring the degree to which a given held-out predictor was 'validated' by the consensus (treated as a gold standard). More precisely, we held back knowledge of genefunction from all methods and ran all the possible predictors (i.e. algorithm and data resource combination). Leaving out the vector of scores of one predictor, we created a consensus solution by averaging the scores for each gene from the other predictors. Using this consensus solution, we created a new label vector with the top 10 genes labeled as positives and the other genes labeled as negatives. The area under the receiver operating characteristic curve (AUROC) is computed based upon these new labels and scores from the prediction of the held-out algorithm. In other words, the held-out algorithm is validated in its predictions of held out gene-function data not by reality, but by the consensus among other algorithms. This was iterated over all possible combination of predictors. The final measure of replicability was the averaged AUROC over all the iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Temporal variation</head><p>To examine the degree to which the trends we observed might be changing with time or reflect a temporary snapshot of the data, we re-ran analyses after freezing all data on April 24, 2014 and then updating any relevant data to that available on<ref type="bibr">August 20, 2015</ref>. This duration covered roughly the beginning of final analyses for the project (first freezing) to midway through review (updating). The updated resources are listed in Supplementary Table S4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Analysis of psychiatric disorder studies</head><p>In our application, we analyze topological network characteristics from disease-associated genes and compare to that of randomly constructed distributions from null networks. The random distributions from Monte Carlo-based methods are typically based upon node permutation (i.e. shuffling the nodes of the network). An alternative is to shuffle across edges or links (e.g. in the list of gene pairs giving connections in the network, permute among all of the second of the pair to create random connectivities that preserve node degree). Using link permutations instead of node permutations allows us to test for a selection bias related to the node degree of the genes in the gene list. To demonstrate that these biases influence results and interpretations in real research problems, we selected gene lists from major studies on autism spectrum disorder (ASD) (O'<ref type="bibr">Roak et al., 2012</ref>) and SCZ (<ref type="bibr" target="#b20">Gulsuner et al., 2013</ref>) and performed the Monte Carlo experiment based upon link permutation alongside the original analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Quality control for PPIN data</head><p>To further study the paradoxical effects of replicability, we looked at the relationship between recurrence, a common metric of replicability and quality control in the PPI databases previously described. We used data on protein contaminants from the CRAPome (<ref type="bibr" target="#b28">Mellacheruvu et al., 2013</ref>) (version 1.1, Homo sapiens, date: January 1, 2014). This database contains the spectral counts of 8473 proteins identified in controls across a collection of 411 affinity capture mass spectrometry (AC-MS) experiments. For each protein in the CRAPome, a quality control 'reliability' score was calculated as the average of spectral counts across all the experiments. For proteins missing from the CRAPome, they were given a score of 0. However, we also assessed missing proteins by giving them a score of NA; all results are robust to this choice. Then, for each PPI in the given PPI database, we calculated the quality of the interaction score as the rank of the sum of the reliability scores of the bait and the prey proteins. We then measured the recurrence of a PPI in the individual PPI databases as the count of how many individual studies the protein pair appeared in and compared that to the reliability score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Our approach to formalizing replicability is to treat it exactly parallel to how performance is conventionally assessed. In general, performance is measured by determining if a researcher can correctly predict some unknown (or held back) result; likewise, we measure replicability by measuring how well a researcher correctly predicts the consensus across other researchers. That is, there are conventional metrics for assessing whether a given researcher's answer is similar to the truth; in measuring replicability, we perform the identical assessment but treat the consensus output among other researchers as the truth against which a given researcher is evaluated. We assess this using the area under the receiver operating characteristic curve (AUROC) in both cases (see Section 2 for further details). Note that all of our analysis is readily reproducible, by which we simply mean that analyses can be re-done, which we differentiate from replicability involving independent analysis, the phenomena we are modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modelling replicability</head><p>3.1.1 Concurrence among either algorithms or data predicts improved accuracy In our first set of model experiments, the algorithms are using various data to predict gene functions as annotated in GO. We first consider researchers sampling from different machine learning algorithms using a single aggregated co-expression network resource (summated across 80 independent transcriptomic experiments and totaling 5672 separate expression samples; see<ref type="figure" target="#fig_0">Fig. 2A</ref>). In using this data, the more replicable the researcher output, the likelier the joint output is to be correct in predicting which genes possess a given function (<ref type="figure" target="#fig_0">Fig. 2A</ref>, gray line, Spearman's r s $ 0.66, P $ 4.7 E-15). Alternatively, we can consider using a common algorithm for all researchers, with each researcher using a co-expression network derived from different data (<ref type="figure" target="#fig_0">Fig. 2a</ref>, black line). In this case, the correlation between replicability and truth is also quite high (Spearman's r s $ 0.85, P $ 1.77 E-31). This positive relationship between replicability and performance is maintained in virtually every case, with researchers sampling from diverse data and algorithms (see Supplementary Figs S4–S7). Note, however, that the line describing researchers which vary by algorithm sits below the line in which researchers vary by data. That is, for a given replicability, alignment with truth is higher if the researchers used different data. 3.1.2 Concurrence within a fixed set of algorithms and data predicts improved accuracy We now generalize from our previous case and construct communities of researchers sampling randomly from combinations of algorithms and data. Researchers may sample from co-expression, PPI and pathway data and use a variety of pre-existing algorithms to 'guess' gene function. Because we are modeling the behavior of these researchers, we vary the scenarios under which they operate. In our case, that takes the form of varying the degree of independence among the resources these researchers sample. So, for example, we simulate all the cases in which the field as a whole uses only a single algorithm on (a variety of) PPI data, or similarly, the researchers use either co-expression or PPI data across five algorithms, etc. We call each of these sets of simulations, which draw upon particular data or algorithms, 'research communities' (<ref type="figure">Fig. 1</ref>) and we can assess replicability and performance for any given research community. The research communities can be regarded as describing the state of the field as a whole given the parameters describing what algorithms and data resources are available (and how variable they are). In total, we analyzed 266 research communities, each running 10 researchers, repeated 10 times for a given parameter set, with each researcher making predictions for 109 GO functions across 12 529 genes (across 3-folds). Our first analysis of these research communities is to determine whether the relationship between replicability and accuracy varies based on independence, i.e. the underlying variability of data resources in the research communities. We characterize the degree of researcher independence by the probability of the researchers within a research community of having sampled from data of the same modality (PPI data or semantic data). For example, when each researcher within the research community is sampling from the same data modality, it is considered to be a case of low methodological variability or low independence. At this stage, we divide the data into four groups (quartiles) depending on the fraction of data a research community uses which is of the same modality (see Supplementary<ref type="figure">Fig. S8</ref>). While we describe these as 'more dependent' researchers, they are all independent analyses as the term is typically used. Their variation in dependence is more like that between, for example, research groups interested in similar data types. The trend seen in the expression data remains true in this broader model: researchers always show a positive relationship between replicability and performance, but as they become more dependent, they sit further to the bottom-right of the performance-replicability space. Averaging the research communities into quartiles by independence, we can plot the average relationship between replicability and performance (<ref type="figure" target="#fig_0">Fig. 2B</ref>). In these quartile groupings, the relationship is strongly positive (Spearman's r s &gt; 0.34); however, the more variability in the 'library' for the research community, the better the performance for a given replicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Sampling</head><p>from algorithms and data with improved joint replicability yields lower accuracy Because each set of model researchers was considering the same set of scientific questions (i.e. which genes have a particular function), we can determine the correlation between replicability and scientific truth for each such scientific question across our quartiles. This is plotted in<ref type="figure" target="#fig_0">Figure 2C</ref>and is negative for any given scientific question. In other words, if we are asked a given scientific question, the more replicable the answer for a research community, the less likely it is to be true (across research communities). This is similar to asking what types of practices in science are 'good' ones which lead research communities to converge on accurate information. As a commonplace example of this effect in practice, we might suppose that removing genetic variation in model organisms through inbreeding makes replicability easier to achieve but should make results less meaningful for a given degree of replication (artifactual properties can now dominate replication). Generalization outside of the modelAssessing replicability and performance in coexpression data for GO groups (points) and showing two research communities, one where the researchers vary only by algorithm (gray) used on consensus data and one where the researchers vary only by data using a consensus method (black). (B) Research communities are constructed by sampling from across data and algorithms with research communities drawing on resources of different degrees of variability (or independence). These research communities are grouped into quartiles by this variability and aggregate performance and replicability are plotted for each set of research communities. The mean (line) is plotted along with the standard deviation (shadow); window size 35. (C) The variability in performance and replicability for each gene function across the different research community quartiles. Smoothed with a window size of 2, so the principal independent observation is that the slopes are uniformly negative. (D) The relationship between performance and replicability within each research community is positive (black; r s ¼ 0.333), but for a given gene function, the performance is negative across research communities (gray; mean r s ¼ À0.125) Positive and negative forms of replicability in gene network analysisorganism would be expected to become harder. This is true after grouping communities by variability; we next assess whether it is true across all communities without such grouping. Our quartile plots show a very strong average trend, but the approximate effect is visible within virtually every research community. The correlation between replicability and truth is positive for a given research community (<ref type="figure" target="#fig_0">Fig. 2D</ref>, Spearman's r s $ 0.33), but the distribution of correlations is negative across research communities for a given scientific question (<ref type="figure" target="#fig_0">Fig. 2D</ref>, Spearman's r s $ À0.13). This result arises through a generalized version of the Yule-Simpson effect (<ref type="bibr" target="#b6">Bickel et al., 1975</ref>): it is possible for replication to be useful in assessing truth for every fixed level of dependence in experimental design but have negative value in assessing the truth of a given scientific question overall. In essence, some results will replicate more easily than others not because they are correct, but because methods or data have been more tightly controlled. The less diversity in methods and data, the less likely we are to converge on the truth in aggregate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Temporal variation in replicability trends</head><p>We initially froze data on April 24, 2014, in our analyses and updated available resources to<ref type="bibr">August 20, 2015</ref>, to test for variation in any of our reported results (Supplementary Tables S4–S6). This only affected our semantic and PPI network data, since the coexpression networks reflect particular experimental data and are not updated meta-analytic resources themselves. For this analysis, all aspects other than the updated data were held constant; i.e. set partitioning in cross-validation and the exact combination of data and methods each simulated researcher sampled in each case is identical. That is, we are not just holding the sampling distributions constant, but the actual 'random' selection. We assessed each algorithm in each of the seven network resources which underwent updates in this interval. We characterize each combination by its average performance in cross-validation on the GO slim prediction used throughout, at the two time points. The correlation of performances between the two time points is quite high (Spearman's r s ¼ 0.868) and nearly follows the identity line (Supplementary<ref type="figure">Fig. S9</ref>). While this correlation in performance trends is high enough for our own modeling results to hold (see below), it is interesting to note that it implies that comparison between methods or data are potentially fragile with respect to subtle variations in time. We next updated the results from panels B, C and D shown in<ref type="figure" target="#fig_0">Figure 2</ref>to that using the newer data (Supplementary<ref type="figure">Fig. S9</ref>). The change in reported results is extremely modest, with the Spearman correlation between performance and replicability across gene functions within a research community falling from r s ¼ 0.333 to r s ¼ 0.328. The correlation between performance and replicability for a given GO functions across research communities falls from r s ¼ À0.125 to r s ¼ À0.135. These modest changes leave it an open question as to whether the non-independence of data is varying with time. For an analyses of the effect of temporal variation in GO and its annotations, readers are referred to our previous work (<ref type="bibr" target="#b17">Gillis and Pavlidis, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">assessing and predicting replicability</head><p>3.2.1. Autism de novo variant network convergence exhibits artifacts To the extent our model is accurate, we should predict that false results will be likely to replicate precisely because they are false. That is, if replication is dominated by artifactual overlaps, then results which are purely due to those artifacts will replicate very well across data. We turn to an interesting natural experiment to demonstrate this effect. An important result in the analysis of candidate psychiatric genetic variants is that the disease genes cluster within network data (<ref type="bibr" target="#b37">Parikshak et al., 2013</ref>). In general, this helps us to believe that we are finding some point of functional convergence defining the disease. Among the most influential of such reports is provided in the analysis of autism de novo variants in PPI data by O'<ref type="bibr">Roak et al.</ref>(2012) (<ref type="figure">Fig. 3A</ref>). However, the data used in this case were problematic. The authors report '1.5 million physical interactions' which is far too many, even after halving this number (to make it unique interactions). In fact, due to an interpretation error, their interaction set includes many tested pairs which were not actually positive results. For example, data derived from a study of the human autophagy system (<ref type="bibr" target="#b5">Behrends et al., 2010</ref>) adds nearly 200 000 interactions, which is approximately its list of tested pairs, rather than the $700 interactions actually reported as positive results in that study. The erroneous parsing specifically contributes 49 interactions to the excess (out of $200) observed by O'Roak among their disease set. An ordinary response to this issue would be to look to replicate the result in other datasets where these problems do not exist and for that replication to validate the original finding. We hypothesized that the result would, in fact, replicate but that this would be because most PPI data has overlapping artifacts. Furthermore, we supposed that if we could then determine what these artifacts were and control for them, the replicated result would vanish in data. In other words, decreased variance among potential replicating data has destroyed its value as an indicator of truth.</p><p>with each gene, a proxy for selection bias predictive of functional properties (<ref type="bibr" target="#b16">Gillis and Pavlidis, 2011</ref>). Aside from controlling for selection bias, significance should generally be easier to attain then the node permutation case since the null now has no structure. In this analysis, the significance vanishes from all of the real data (<ref type="figure">Fig. 3D</ref>and E), including the updated and corrected version of the original PPI data. Crucially, functional sets of genes as defined by GO retain their significant connectivity (<ref type="figure">Fig. 3F</ref>). Altogether, this indicates that replicability in the disease gene analysis indicated replicability of bias, a factor which attaches no more to this study than any other, except for some irrelevant bad luck in choosing data for which it would be difficult to obtain meaningful results. We perform a qualitatively identical analysis in another case (<ref type="bibr" target="#b20">Gulsuner et al., 2013</ref>) with similar results in the Supplementary Material (see Supplementary Figs S10–S14). While these cases involve methods accidentally exploiting selection bias, the problem they identify is essentially orthogonal. The meta-analytic confound we have identified is likely to be dominated by other biases in other data modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Replicability indicates poor data quality in PPIs</head><p>The evidence of high bias in the underlying PPI data suggested to us that replicability within the networks themselves might be dominated by artifacts. We also see some evidence for this within the model analysis, where research communities dominated by PPI data had replicability to performance correlations significantly lower than those seen in the co-expression data, which is less prone to selection bias since genome wide (Spearman's r s $ 0.31 versus 0.53). To assess these upstream effects in the underlying data, we focus on among the most commonly used network resource, BioGRID (<ref type="bibr" target="#b46">Stark et al., 2006</ref>). Awareness of the potential for confounds in aggregated PPI data has made some degree of quality control in using BioGRID commonplace. The most common approach is to threshold for replicability by requiring interactions to have been reported multiple times (<ref type="bibr" target="#b1">Anastassiadis et al., 2011</ref>). That is, replicability is the method by which correction for bias is generally attempted. The individual reports determining this replicability should have quite strong variation in their degree of dependence (in our terms) since practices underlying data collection can vary enormously across what is, in essence, almost the entire field of proteomics. We would therefore hypothesize that there should be little value to replication. A recent comprehensive analysis of the quality of AC-MS data (<ref type="bibr" target="#b28">Mellacheruvu et al., 2013</ref>) allows us to evaluate this quantitatively, by determining whether replicable interactions are more likely to involve proteins for which results cannot be considered reliable. We find a strikingly strong relationship between the degree of replicability and the mean 'unreliability' score of the interactions (Supplementary<ref type="figure">Fig. S15</ref>, Spearman's r s $ 0.99, P $ 9.24 E-6), suggesting that replicability has negative value in PPI data. It is not just that the PPI data is noisy but that how we most easily fix such problems is now incorrect. becoming more complex, our practices less precise or whether our evaluation of problems is simply clearer. Our analysis of gene network methods demonstrates an alternate possibility: that as methods and data are optimized to improve replicability, the independent value of replicability diminishes. Goodhart's law, that '<ref type="bibr">[a]</ref>ny observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes' (<ref type="bibr" target="#b18">Goodhart, 1975</ref>), is true even for scientific replication. While dependencies underlying replicability have been touched on previously both in the context of human judgment and machine learning algorithms, this has principally been seen as a major factor to exploit in improving performance (<ref type="bibr" target="#b8">Breiman, 1996;</ref><ref type="bibr" target="#b29">Mellers et al., 2014</ref>). Our demonstration of a potentially negative value for replicability may sound incompatible with these findings or past scientific practice in general, but we suggest that this is not really the case. Normally, assessment of the value of a result depends on an understanding of the factors underlying it. It is only where we target replicability and are indifferent to how it is achieved that our report raises a central concern. Unfortunately, the increased focus and attention on overlapping reference data and consensus evaluation makes field-wide overfitting ('bad' replicability) a real possibility for gene network methods. The problem of overfitting is central to machine learning, and approaches for avoiding it even when comparing methods, have been addressed within that literature (<ref type="bibr" target="#b11">Demsar et al., 2006</ref>). These mostly resemble comparisons of the sort we cover in sections 3.1.1 and so also exploit replicability. One standard approach, is 5 x 2 cross-validation (<ref type="bibr" target="#b12">Dietterich, 1998</ref>), where 2-fold cross-validation is repeated multiple times to ensure comparative performances are robust/replicable. It might be tempting for us to target the problem of data orthogonality directly within this framework, but this merely raises Goodhart's law anew. Specifically targeting orthogonal data will make us sensitive to overfitting on, however, we characterize 'orthogonality'. However, heuristics for feature selection do already incorporate orthogonality as a desirable property (<ref type="bibr" target="#b21">Hall, 2000</ref>), and it is natural to wonder whether refinement of this analysis or filtering at the data collection stage would minimize this problem. Our data collection was intended to be reflective of general use (e.g. Mousefunc included both Pfam and InterPro as separate resources) rather than whatever we might consider ideal and we certainly leave open the possibility that better assessment of data orthogonality could make the field insensitive to the problems we have identified, remembering that for any given field of research modeled, the relationship between replicability and accuracy was a positive one. Indeed, it is important to recognize that replicability and meaningful comparative evaluation really are desirable properties. Just as high performance in an algorithm is a good property but not one we should enforce by fiat, the same can be said of replicability. We particularly note that the effect we describe is a problem with the way we attempt to fix problems in scientific practice or data and not just a scientific problem in itself. For example, it is typical to threshold by replicability in PPI data to avoid methodological or data issues. Likewise, consensus resources, data sets, methodological practices, animal models, etc., are often specified precisely to allow researchers to better obtain replicable results. While it may seem intuitive that replication should be a test of robustness and that it will lose value where this is not true, our observation is that most explicit focus on replication strongly diminishes its utility through the use of very tightly constrained systems, data and methods. Our suggestion is that these difficulties arise particularly in genomics because our real problems are often poorly defined. Predicting 'gene function' is hard and so we swap in the better defined problem of predicting GO or otherwise alter evaluation to make results 'sensible'; however, closing this feedback loop so directly removes independence between method and assessment. We suggest thisFunding JG, WV, and SB were supported by a grant from T. and V. Stanley. Conflict of Interest: none declared.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.2.</head><figDesc>Fig. 2. Meta-analytic properties of predicted gene functions. Performance is assessed by conventional cross-validation against held-out true positives. Replicability is assessed by cross-validation against consensus predictions from held-out researchers. (A) Assessing replicability and performance in coexpression data for GO groups (points) and showing two research communities, one where the researchers vary only by algorithm (gray) used on consensus data and one where the researchers vary only by data using a consensus method (black). (B) Research communities are constructed by sampling from across data and algorithms with research communities drawing on resources of different degrees of variability (or independence). These research communities are grouped into quartiles by this variability and aggregate performance and replicability are plotted for each set of research communities. The mean (line) is plotted along with the standard deviation (shadow); window size 35. (C) The variability in performance and replicability for each gene function across the different research community quartiles. Smoothed with a window size of 2, so the principal independent observation is that the slopes are uniformly negative. (D) The relationship between performance and replicability within each research community is positive (black; r s ¼ 0.333), but for a given gene function, the performance is negative across research communities (gray; mean r s ¼ À0.125)</figDesc></figure>

			<note place="foot">W.Verleyen et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="3">.2.2. Network results can replicate despite artifacts driving the original report We find that the result replicates drawing on data from other PPI collections which do not ostensibly suffer from the described problem (Fig. 3B). While these resources draw on similar data, they exhibit substantial differences depending on curation practices and assessment, the very factors at issue for this particular analysis and therefore in need of replication; the new resources have only 33 661 in their intersect out of 200 499 pairs in their union. Because the results hold in all of this other data, the quality control issues in the original analysis are pure happenstance not affecting the result. This might reflect the strength of the finding and that even substantial noise added to the datasets does not affect the result. Alternatively, it may support the view that the dominant signal across resources is heavily influenced by some bias, even where accurately collected and that replicability is no guarantor of correctness. In this case, selection bias is a natural candidate for the shared confound precisely because of the contamination of &apos;tested&apos; data as positives within the original analysis. One point suggesting the disease clustering is less significant than raw P values indicate is that performing the same analysis across GO groups reveals that the disease set, while very significant, is far less clustered than normal &apos;functional&apos; sets of genes (Fig. 3C). 3.2.3. Network results replicate because of artifacts In this case, assessing the mechanism underlying replicability is straightforward because we know the results replicate even in data heavily influenced by selection bias. We re-analyzed all of the data with an alternate control, permuting through interactions rather than nodes to calculate the null (Maslov and Sneppen, 2002). This entirely randomizes the network connectivity rather than just the labelling but retains the same number of connections associated</note>

			<note place="foot" n="4"> Discussion Concerns about replicability in science have been much discussed recently (Begley and Ellis, 2012; Ioannidis, 2005). It has not been clear whether this has emerged as a recent focus because our systems are Fig. 3. Replicating network properties of de novo mutations in autism and edge permutation as a null removes all significance from the autism-derived gene list. (A) Replication of the Monte Carlo experiment performed in O&apos;Roak et al. (2012) (see Supplementary Fig. S10). The number of edges between the genes in the autism (ASD) gene list in a network based upon GeneMANIA (version date August 3, 2011) physical interaction data is statistically different (P &lt; 0.0001). The parsing of this network in the original analysis conflates &apos;tested&apos; pairs with &apos;validated&apos; pairs of interacting genes across much of the data. (B) Networks drawing on different network resources also show a statistically significant number of edges between the genes in the ASD gene list: BioGRID (P $ 0.007), HIPPIE (P $ 0.0006), IntAct (P $ 0.015) and I2D (P $ 0.0004). Because we ran 10 000 iterations to calculate P values, the maximum value on the graph is 4; the original GeneMANIAbased result is some point past this, indicated by its placement. (C) However, conducting the same analysis on gene lists derived from GO terms reveals that they are much more likely to exhibit significant linkage than the ASD gene list (GO: mean z-score ¼ 21.96; ASD gene list ¼ 4.41). (D) Using the corrected GeneMANIA data as well as holding node degree per each gene fixed in the null simulations removes all significant association between ASD genes. Note the null distribution of number of edges differs between this and (A); axis scale has also changed. (E) This property replicates across network data derived from multiple sources. (F) Functional sets of genes defined by GO remain learnable even after accounting for node degree in this way Positive and negative forms of replicability in gene network analysis</note>

			<note place="foot">meta-analytic difficulty can be solved by targeting a real biological problem with diverse and well-powered data. This will differ from field to field and should be regarded as an important research effort in itself. Within transcriptomics, predicting the sex of the organism from which all public data were collected for some recent interval trained on the past would be a worthwhile and achievable task, before moving on to tissue, cell-type, etc. The critical point is to pick a problem in which the question is perfectly defined and the answer is perfectly knowable. In the meantime, we do not recommend deviating from the current dry-lab cross-validation practice on standardized data and instead advocate in favor of control experiments which reveal what factors affect performance, as in our examples. Because the model we have constructed is quite general, we might expect to see this effect outside science and we suggest that this is, in fact, the case. The heuristic our model suggests is that high concurrence is a reason to disbelieve a claim if the methods whereby that concurrence arose are unknown. A careful reading of the substantial literature in the social sciences on persuasiveness suggest that this effect, while not previously recognized, may be responsible for some otherwise puzzling results. For example, people in diverse environments are more interested in opinion information (as opposed to factual) (Scheufele, 2014). This is explained as their preparing for argument, but our analysis suggests that people may simply be drawing the rational inference that opinion information is more valuable where it is diverse (and where concurrence will imply correctness). Similarly, experts become more convincing when their views are more divergent from pre-existing beliefs (Pornpitakpan, 2004), which may be puzzling in a naively Bayesian sense but is intuitive if the divergence of belief within the populace as a whole is being estimated and used to weight the value of opinion. We call this effect the &apos;talking points&apos; heuristic, since accusations that concurrence must be artifactual simply because it is high are sometimes described in this way. Our findings also have clear implications for public science funding, which has increasingly focused on generating reference data as a matter of deliberate policy, sometimes specifically to target replicability. In this article, we have emphasized some subtleties around evaluating meta-analytical properties of results. Results independently derived from different data resources do not replicate &apos;easily&apos; and so are more meaningful where it occurs; similarly, they profit the most from aggregation or comparison. However, folding such aggregation directly into methodological construction makes replicable results easier to achieve and less meaningful. In these cases, we must seek more orthogonal validation or more carefully calibrated control experiments. While our work is the most comprehensive quantification of this problem, these ideas have already found purchase within machine learning based on more specific analyses: the perils of overfitting are often discussed. Our perception is that replicability within genomics, and gene network analysis particularly, is usually seen as somehow more fundamental than these concerns and so a universal good to be strived for. As our analysis quantifies, this is far from true and becomes ever less so the more replicability is enforced from the top down. Replicability as a form of validation is a finite resource even if data generation is not and more thoughtful stewardship by scientific organizers (of all types) is necessary.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Paul Pavlidis and Shane McCarthy for helpful comments on a draft of the manuscript. We thank Quaid Morris for the GeneMANIA code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">F</forename>
				<surname>Altschul</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Comprehensive assay of kinase catalytic activity reveals features of kinase inhibitor selectivity</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Anastassiadis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1039" to="1045" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the unification of biology. The Gene Ontology Consortium</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ashburner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Guidance for RNA-seq co-expression network construction and analysis: safety in numbers</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ballouz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="31" to="2123" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Drug development: raise standards for preclinical cancer research</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G</forename>
				<surname>Begley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">M</forename>
				<surname>Ellis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="page" from="531" to="533" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Network organization of the human autophagy system</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Behrends</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">466</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Sex bias in graduate admissions: data from Berkeley</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Bickel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="398" to="404" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">An unbiased evaluation of gene prioritization tools</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Bornigen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3081" to="3088" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Unequal evolutionary conservation of human protein interactions in interologous networks</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">V</forename>
				<surname>Brown</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Jurisica</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">The BioGRID interaction database: 2013 update</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Chatr-Aryamontri</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="816" to="823" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Demsar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">G</forename>
				<surname>Dietterich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Pfam: the protein families database</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">D</forename>
				<surname>Finn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<monogr>
		<title level="m" type="main">The Design of Experiments</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
				<surname>Fisher</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1935" />
			<publisher>Oliver and Boyde</publisher>
			<pubPlace>Edinburgh, London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<monogr>
		<title level="m" type="main">Autism: many genes, common pathways? Cell</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">H</forename>
				<surname>Geschwind</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="391" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">The impact of multifunctional genes on &quot; guilt by association &quot; analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gillis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pavlidis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17258</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Assessing identity, redundancy and confounds in Gene Ontology annotations over time</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gillis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pavlidis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="476" to="482" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title level="m" type="main">Problems of Monetary Management: The UK Experience. Reserve Bank of Australia</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">A E</forename>
				<surname>Goodhart</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
	<note>Papers. in Monetary Economics</note>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate evaluation and analysis of functional genomics data and methods</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">S</forename>
				<surname>Greene</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<forename type="middle">G</forename>
				<surname>Troyanskaya</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. N. Y. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">1260</biblScope>
			<biblScope unit="page" from="95" to="100" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial and temporal mapping of de novo mutations in schizophrenia to a fetal prefrontal cortical network</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Gulsuner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="518" to="529" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Correlation-based feature selection for discrete and numeric class machine learning</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Hall</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<editor>Langley,P.</editor>
		<meeting>the Seventeenth International Conference on Machine Learning<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">InterPro in 2011: new developments in the family and domain prediction database</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Hunter</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="306" to="312" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Contradicted and initially stronger effects in highly cited clinical research</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Ioannidis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">294</biblScope>
			<biblScope unit="page" from="218" to="228" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Reactome: a knowledgebase of biological pathways</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Joshi-Tope</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="428" to="432" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">CASP10 results compared to those of previous CASP experiments</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kryshtafovych</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="164" to="174" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Wisdom of crowds for robust gene network inference</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Marbach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="796" to="804" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Specificity and stability in topology of protein networks</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Maslov</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Sneppen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="page" from="910" to="913" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">The CRAPome: a contaminant repository for affinity purification-mass spectrometry data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Mellacheruvu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="730" to="736" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Psychological strategies for winning a geopolitical forecasting tournament</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Mellers</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1115" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Gene Ontology term overlap as a measure of gene functional similarity</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Mistry</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pavlidis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">327</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Computational tools for prioritizing candidate genes: boosting disease gene discovery</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Moreau</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">C</forename>
				<surname>Tranchevent</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Sporadic autism exomes reveal a highly interconnected protein network of de novo mutations</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>&apos;roak</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">J</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">485</biblScope>
			<biblScope unit="page" from="246" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving disease gene prioritization by comparing the semantic similarity of phenotypes in mice with those of human diseases</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Oellrich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">38937</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">KEGG: Kyoto Encyclopedia of Genes and Genomes</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Ogata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="29" to="34" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Guilt-by-association goes global</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Oliver</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page" from="601" to="603" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">The MIntAct project-IntAct as a common curation platform for 11 molecular interaction databases</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Orchard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="358" to="363" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Integrative functional genomic analyses implicate specific molecular pathways and circuits in autism</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">N</forename>
				<surname>Parikshak</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1008" to="1021" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<monogr>
		<title level="m" type="main">Progress and challenges in the computational prediction of gene function using networks: 2012-2013 update</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pavlidis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gillis</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Scikit-learn: machine learning in Python</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Pedregosa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">A critical assessment of Mus musculus gene function prediction using integrated genomic evidence</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Pena-Castillo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">The persuasiveness of source credibility: a critical review of five decades&apos; evidence</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Pornpitakpan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">Neurocarta: aggregating and sharing disease-gene relations for the neurosciences</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Portales-Casamar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Genomics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title level="a" type="main">A large-scale evaluation of computational protein function prediction</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Radivojac</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="221" to="227" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b44">
	<analytic>
		<title level="a" type="main">HIPPIE: integrating protein interaction networks with experiment based quality scores</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">H</forename>
				<surname>Schaefer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">31826</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<analytic>
		<title level="a" type="main">Science communication as political communication</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Scheufele</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
		<meeting>. Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13585" to="13592" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct   xml:id="b46">
	<analytic>
		<title level="a" type="main">BioGRID: a general repository for interaction datasets</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Stark</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="535" to="539" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b47">
	<analytic>
		<title level="a" type="main">Measuring the wisdom of the crowds in networkbased gene function inference</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Verleyen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="31" to="745" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b48">
	<analytic>
		<title level="a" type="main">It&apos;s the machine that matters: predicting gene function and phenotype from protein networks</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">I</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">M</forename>
				<surname>Marcotte</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Proteomics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="2277" to="2289" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b49">
	<analytic>
		<title level="a" type="main">Guidelines for human gene nomenclature</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>White</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genomics</title>
		<imprint>
			<publisher>HUGO Nomenclature Committee</publisher>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="468" to="471" />
			<date type="published" when="1997" />
			<publisher>HUGO Nomenclature Committee</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b50">
	<analytic>
		<title level="a" type="main">GeneMANIA prediction server 2013 update</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Zuberi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="115" to="122" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>