
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Systems biology Identification of important regressor groups, subgroups and individuals via regularization methods: application to gut microbiome data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Tanya</forename>
								<forename type="middle">P</forename>
								<surname>Garcia</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Epidemiology &amp; Biostatistics</orgName>
								<orgName type="institution" key="instit1">School of Rural Public Health</orgName>
								<orgName type="institution" key="instit2">Texas A&amp;M Health Science Center</orgName>
								<address>
									<postCode>77843-1266</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Samuel</forename>
								<surname>Mü Ller</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<settlement>Australia</settlement>
									<region>NSW</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Raymond</forename>
								<forename type="middle">J</forename>
								<surname>Carroll</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843-3143</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Rosemary</forename>
								<forename type="middle">L</forename>
								<surname>Walzem</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Poultry Science</orgName>
								<orgName type="department" key="dep2">Intercollegiate Faculty of Nutrition</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77840</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Systems biology Identification of important regressor groups, subgroups and individuals via regularization methods: application to gut microbiome data</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="issue">6</biblScope>
							<biblScope unit="page" from="831" to="837"/>
							<date type="published" when="2014">2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt608</idno>
					<note type="submission">Received on February 20, 2013; revised on September 19, 2013; accepted on October 18, 2013</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Ziv Bar-Joseph Availability: The new approach is implemented in an R package, which is freely available from the corresponding author. Contact: tpgarcia@srph.tamhsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Gut microbiota can be classified at multiple taxonomy levels. Strategies to use changes in microbiota composition to effect health improvements require knowing at which taxonomy level interventions should be aimed. Identifying these important levels is difficult, however, because most statistical methods only consider when the microbiota are classified at one taxonomy level, not multiple. Results: Using L 1 and L 2 regularizations, we developed a new variable selection method that identifies important features at multiple taxonomy levels. The regularization parameters are chosen by a new, data-adaptive, repeated cross-validation approach, which performed well. In simulation studies, our method outperformed competing methods: it more often selected significant variables, and had small false discovery rates and acceptable false-positive rates. Applying our method to gut microbiota data, we found which taxonomic levels were most altered by specific interventions or physiological status.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With improved culture-independent techniques, a typical study of gut microbiota now involves data from numerous microbes. The microbes are classified at multiple taxonomy levels, namely, phylum, class, order, family, genus and species. Each taxonomy level has many subdivisions, and the number of subdivisions increase on progression from phylum to species level. Strategies to use changes in microbiota composition to effect health improvements require knowing at which taxonomy level interventions should be aimed. Levels to target are those with subdivisions identified as having an impact on the target health outcome. From a biological perspective, only a few subdivisions at each level are believed to play a role in certain health outcomes. Identifying the few important subdivisions at each level is difficult, however, because of the increasing number of subdivisions on progression from phylum to species level and because the microbial data are typically based on small sample sizes. Thus, a method that overcomes these difficulties and identifies important subdivisions at multiple taxonomy levels is needed. This biological problem corresponds to a variable selection problem where the variables are grouped at multiple levels, and the number of variables (p) far exceeds the sample size (n). We suppose that each level has sparse effects. In the microbiota data, sparse effects mean that only a few subdivisions within a particular taxonomy level actually impact the health phenotypes of interest. For our purposes, we consider the case where variables are divided into groups and subgroups within the groups. Our interest, thus, is developing a method that selects important groups (e.g. phyla), subgroups (e.g. families) and individual predictors (e.g. genera). Selecting variables clustered into groups and subgroups is challenging. When the variables are divided only into groups (without subgroups), a popular technique is the group Lasso (<ref type="bibr" target="#b25">Yuan and Lin, 2006</ref>), which selects an entire group of variables to be included or excluded from the model. The group Lasso, however, has substantial drawbacks. First, the method assumes that the model submatrices for each group are orthonormal. When orthonormality is not satisfied, the group Lasso may select an incorrect model (<ref type="bibr" target="#b5">Friedman et al., 2010</ref>). Second, the group Lasso does not achieve sparsity within each group, which can be useful. For the microbial data, we could design more specific strategies for changing microbiota composition if we knew which particular families (i.e. subgroups) in phyla (i.e. group) impacted health phenotypes of interest. To overcome the deficiencies of the group Lasso,<ref type="bibr" target="#b17">Simon et al. (2012)</ref>recently proposed the sparse-group Lasso (SGL). The method imposes no orthonormality requirements on the group model submatrices and achieves sparsity between and within groups through a clever use of the<ref type="bibr" target="#b16">Nesterov (2007)</ref>method for generalized gradient descent. The SGL works well when variables are clustered into groups, but not when they are clustered at more than one level—a feature inherent to gut microbiota data. *To whom correspondence should be addressed.To accommodate selecting important groups, subgroups and individual predictors, we propose three new algorithms. The first algorithm, the sparse group-subgroup Lasso (SGSL), generalizes the work of<ref type="bibr" target="#b17">Simon et al. (2012)</ref>. It is based on using L 1 and L 2 regularizations in a linear regression model; convex non-linear regression models are discussed in the Supplementary Material. Our two other proposed algorithms use appropriate combinations of already existing variable selection procedures. First, we propose applying the group Lasso to the groups followed by SGL applied to the subgroups. Second, we propose applying the group Lasso to both the groups and subgroups followed by applying the Lasso (<ref type="bibr" target="#b22">Tibshirani, 1996</ref>) to select among the individual predictors. We demonstrate in a simulation study that our first algorithm outperforms the other two. SGSL is a special case of the tree-structured group Lasso (<ref type="bibr" target="#b8">Jenatton et al., 2011;</ref><ref type="bibr" target="#b12">Liu and Ye, 2010;</ref><ref type="bibr" target="#b26">Zhao et al., 2009</ref>), where nodes on the tree represent groups or subgroups of features and 'leaf' nodes represent individual features. The tree-structured group Lasso, however, uses a smoothing proximal gradient method (<ref type="bibr" target="#b10">Kim and Xing, 2012</ref>) to 'prune' the entire tree collectively, whereas our method uses an accelerated generalized gradient descent approach to determine sparsity among groups, then subgroups and then individual features. Moreover, we consider a tree without cycles, meaning there is no overlap between groups/subgroups of features; i.e. each individual feature only belongs to one subgroup, and each subgroup only belongs to one group. Hence, our problem differs from the overlapping group Lasso as in the analysis of breast cancer gene expression data (<ref type="bibr" target="#b23">Van de Vijver et al., 2002</ref>) where the interest is finding important pathways among overlapping genes. Our problem also differs from a hierarchical variable selection (<ref type="bibr" target="#b26">Zhao et al., 2009</ref>) where a feature is subject to selection only after another feature is selected first. We do not impose this requirement. Like other Lasso-based procedures, SGSL also requires selecting tuning parameters, for which we propose a data-adaptive approach. Our approach involves multiple applications of 10-fold cross-validation that we show performs well in selecting the tuning parameters through various simulation studies. Therefore, the main contributions from our work include (i) a new variable selection procedure (SGSL), which identifies important groups, subgroups and individual predictors through combined L 1 and L 2 regularizations. (ii) We show that achieving sparsity at multiple levels cannot be achieved through simple combinations of existing Lasso approaches. We show that such combinations will select relevant features less often than SGSL or never (Section 3). (iii) We provide a data-adaptive cross-validation approach that improves over the traditional cross-validation to select the tuning parameters. (iv) In microbiome data, our method identifies which taxonomic levels were most altered by specific interventions or physiological status. The rest of the article is as follows. Section 2 describes SGSL and Section 3 evaluates its performance compared with competing methods. In Section 4, we describe the microbiota data that motivated this methodology and analyze the data. Section 5 concludes the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ß</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data structure</head><p>We consider a linear regression model with sample size n, a response variable y ¼ ðy 1 ,. .. , y n Þ T across the samples, and an n Â p matrix of predictors X. For the microbial data, y corresponds to measurements of health features, and X contains information about the p microbes. We have p4n, and without loss of generality, all variables are standardized to have mean zero and sample variance one, so that the intercept is excluded from the model. Because the predictors have subgroup and group memberships, we suppose there are L disjoint groups, and each group k has M k disjoint subgroups, k ¼ 1,. .. , L. By the disjointedness assumption, there is no overlap between groups, or overlap between subgroups. We assume that group k contains p k predictors denoted by the n Â p k matrix X ðkÞ &amp; X. We also assume that subgroup m in group k contains p k, m predictors denoted by the n Â p k, m matrix X ðk, mÞ &amp; X ðkÞ. The notation is such that X ðkÞ refers to the predictors in group k; whereas X ðk, mÞ refers to the predictors in subgroup m of group k. The total number of predictors across all subgroups in group k is p k (i.e. P Mk m¼1 p k, m ¼ p k ), and the total number of predictors across all groups is p (i.e.</p><formula>p ¼ P L k¼1 p k ¼ P L k¼1 P Mk m¼1 p k, m</formula><p>). Finally, b ðkÞ denotes the coefficient vector associated with group k, and b ðk, mÞ is associated with subgroup m in group k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">New criterion for achieving sparsity among groups, subgroups and individual predictors</head><p>2.2.1 SGSL: extension of the SGL Our primary objective is identifying the relevant groups, subgroups and individual predictors in relation to y. Doing so involves finding a sparse solution for the coefficient values; i.e. some coefficient values will be zero and some will be non-zero. If a group's (subgroup's) coefficient vector is all non-zero, then that group (subgroup) is relevant. Otherwise, if there is a mix of zero and non-zero coefficients in a subgroup, then those predictors with nonzero coefficient values are relevant and those predictors with zero coefficient values are not. To determine which coefficient values are zero and non-zero, we propose solving b b ¼ argmin b QðbÞ where</p><formula>QðbÞ ¼ ð1=2Þjjy À X L k¼1 X ðkÞ b ðkÞ jj 2 2 þ 1 X L k¼1 ffiffiffiffiffi p k p jjb ðkÞ jj 2 þ 2 X L k¼1 X Mk m¼1 ffiffiffiffiffiffiffiffiffi p k, m p jjb ðk, mÞ jj 2 þ ð1 À 1 À 2 Þjjbjj 1 :</formula><p>Here, jj Á jj 2 denotes the L 2-norm and jj Á jj 1 denotes the L 1-norm. The regularization parameters , 1 , and 2 control the level of sparsity among the groups, subgroups and individual predictors, and satisfy two criteria: , 1 , 2 ! 0 and 1 þ 2 1. Sparsity among groups and subgroups results from the non-differentiability of the L 2-norm at zero.</p><p>For example, because jjb ðkÞ jj 2 ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi b ðkÞ T b ðkÞ q is non-differentiable at b ðkÞ ¼ 0, the group coefficient b ðkÞ can be exactly zero. Likewise, the subgroup coefficient b ðk, mÞ can be exactly zero because jjb ðk, mÞ jj 2 is non-differentiable at b ðk, mÞ ¼ 0. Though we define QðbÞ for a linear model, our method also extends to convex non-linear regression models; see the Supplementary Material. The criterion QðbÞ also encompasses different versions of the Lasso. We have the Lasso (<ref type="bibr" target="#b22">Tibshirani, 1996</ref>) at 1 ¼ 0, 2 ¼ 0; the group Lasso (<ref type="bibr" target="#b25">Yuan and Lin, 2006</ref>) at 1 ¼ 1, 2 ¼ 0; the group Lasso at the subgroup level at 1 ¼ 0, 2 ¼ 1; SGL (<ref type="bibr" target="#b17">Simon et al., 2012</ref>) among groups at 2 ¼ 0; SGL among subgroups at 1 ¼ 0; and we have sparsity only among groups and subgroups when 1 40, 2 40 and 1 þ 2 ¼ 1.</p><p>To find the minimizer b b of QðbÞ, we take advantage of the criterion's convexity and separability between groups and subgroups. Through a careful analytical derivation involving properties of subgradients and the Karush–Kuhn–Tucker conditions, we derive the conditions for when the group coefficient b ðkÞ and the subgroup coefficient b ðk, mÞ are exactly zero; see Supplementary Material. These results motivate us to use a blockwise descent algorithm at the group and subgroup levels. When a subgroup's coefficient vector b ðk, mÞ is non-zero, we estimate the subgroup coefficients using the accelerated generalized gradient descent method (<ref type="bibr" target="#b16">Nesterov, 2007</ref>) and a step-size optimization as in<ref type="bibr" target="#b17">Simon et al. (2012)</ref>. In the algorithm below, let r ðÀkÞ ¼ y À P '6 ¼k X ð'Þ b b ð'Þ denote the partial residual after removing group k, and r ðÀk, mÞ ¼ r ðÀkÞ À P s6 ¼m X ðk, sÞ b b ðk, sÞ denote the partial residual after removing subgroup m from group k. Let SðÁÞ be the coordinate-wise soft thresholding operator (<ref type="bibr" target="#b3">Donoho and Johnston, 1994</ref>): ½Sfz, ð1 À 1 À 2 Þg j ¼ signðz j Þfjz j j À ð1 À 1 À b ðk, mÞ jj 2 2 =2, and define R t fb ðk, mÞ g ¼ S h b ðk, mÞ À tr'fr ðÀk, mÞ , b ðk, mÞ g,</p><formula>ð1 À 1 À 2 Þt i as well as f t fb ðk, mÞ g ¼ 1 À t2 ffiffiffiffiffiffi ffi pk, m p jjRtfb ðk, mÞ gjj 2 h i þ R t fb ðk, mÞ g.</formula><p>Our proposed algorithm is then:</p><p>1. Group component: Iterate through each group k ¼ 1,. .. , L. If for group k,b. Set the step-size t ¼ 1 and counter s ¼ 1. Define A t fb ðk, 1Þ ,. .. ,</p><formula>X Mk m¼1 jjSfX ðk, mÞ T r ðÀkÞ , ð1 À 1 À 2 Þgjj 2 À 2 ffiffiffiffiffiffiffiffiffi p k, m p þ 2 2 1 2 p k ,</formula><formula>b ðk, MkÞ g ¼ h f T t fb ðk, 1Þ g,. .. , f Tfb ðk, M k Þ g t i T and Ufb ðk, mÞ , tg ¼ 1 À t 1 ffiffiffiffiffi p k p jjA t fb ðk, 1Þ ,. .. , b ðk, MkÞ gjj 2 þ Â 1 À t 2 ffiffiffiffiffiffiffiffiffi p k, m p jjR t fb ðk, mÞ gjj 2 þ R t fb ðk, mÞ g:</formula><p>Let b ðk, mÞ, s ¼ h ðk, mÞ, s ¼ b b ðk, mÞ , where b b ðk, mÞ is the current value. Iterate through the following steps until convergence:</p><p>(1) Compute the gradient g ¼ r'fr ðÀk, mÞ , b ðk, mÞ, s g.</p><formula>(2) Compute Á ðs, tÞ ¼ Ufb ðk, mÞ, s , tg À b ðk, mÞ, s .</formula><p>(3) If ' h r ðÀk, mÞ , Ufb ðk, mÞ, s , tg i 4'fr ðÀk, mÞ , b ðk, mÞ, s g þ g T Á ðs, tÞ þ 1 2t jjÁ ðs, tÞ jj 2 2 , update the step size t to 0:8t. Repeat until the inequality no longer holds to optimize t.</p><p>(4) Set h ðk, mÞ, sþ1 as Ufb ðk, mÞ, s , tg.</p><p>(5) Set b ðk, mÞ, sþ1 as h ðk, mÞ, s þ fs=ðs þ 3Þgfh ðk, m, sþ1Þ À h ðk, mÞ, s g; i.e. a Nesterov step.</p><p>(6) Update s to s þ 1. The algorithm above, known as SGSL, generalizes the SGL algorithm. When the predictors are divided only into groups (i.e. 2 ¼ 0), the above algorithm is actually distinctly different from SGL. This is because of the definition of Ufb ðk, mÞ , tg in Step 2(b), which uses information from subgroups and groups, not just groups. When 2 ¼ 0, the condition in Equation (1) is equivalent to when an entire group is excluded from the model in SGL (<ref type="bibr" target="#b17">Simon et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Selection of regularization parameters Different choices of</head><p>, 1 , 2 yield different solutions b b. To select these tuning parameters, we proceed as follows. First, for a fixed 1 , 2 , we choose the optimal by varying it over the range ½ Ã , Ã , where Ã is the smallest such that QðbÞ is minimized at b b ¼ 0, and is a small fraction, such as 0.05. To find Ã , note that from condition (1), b b ¼ 0 minimizes QðbÞ when</p><formula>X Mk m¼1 jjSfX ðk, mÞ T y, ð1 À 1 À 2 Þgjj 2 À 2 ffiffiffiffiffiffiffiffiffi p k, m p h i þ 2 2 1 2 p k , ð2Þ</formula><p>for all groups k ¼ 1,. .. , L. Thus, Ã is the smallest value where the above inequality holds for all groups. A practical approach for approximating Ã is taking ¼ 2 j for j ¼ 0, 1, 2,. .. , and stopping at the first j for which condition (2) holds for all groups. At this first j, we have that Ã 2 ð2 jÀ1 , 2 j Þ. To further improve the estimate of Ã , one may then bisect the interval ð2 jÀ1 , 2 j Þ repeatedly until Ã 2 ð 1 , 2 Þ, where j 2 À 1 j50:0001. Here, when ¼ 2 , condition (2) holds for all groups, and when ¼ 1 , condition (2) fails to hold for at least one group. Finally, take Ã ¼ 2. Performing the algorithm in Section 2.2.1 at fixed 1 , 2 and over the range of values yields different model fits. Among all fits, we choose the best descriptive model as the one that minimizes Mallows'</p><formula>C p criterion: M n ðp Ã Þ ¼ SSE p Ã = ^ 2 À n þ 2p Ã ,</formula><p>where p Ã denotes the number of predictors in the selected model, SSE p Ã denotes the residual sum of squares and ^ 2 is an appropriate estimator of the model error variance. For example, when n4p, ^ 2 can be the residual mean square when using all available variables, or when n5p, ^ 2 can be the variance of y (<ref type="bibr" target="#b7">Hirose et al., 2013</ref>). Mallows' C p criterion balances the residual sum of squares of a fitted model with the number of non-zero parameter estimates. Other model selection criteria may also be used (<ref type="bibr" target="#b15">Muïler and Welsh, 2010</ref>). The above procedure selects well for a fixed 1 , 2 , and now we describe how to optimally select 1 and 2. We propose selecting the optimal 1 , 2 based on repeated 10-fold cross-validation as advocated by<ref type="bibr" target="#b6">Garcia et al. (2013) and</ref><ref type="bibr" target="#b14">Martinez et al. (2011)</ref>. For a fixed 1 ¼ 10 and 2 ¼ 20 , a single application of 10-fold cross-validation works as follows:</p><p>(i) randomly partition the data into 10 non-overlapping equal-sized subsets; (ii) remove data subset d, and apply the algorithm in Section 2.2.1 at 10 , 20 and over the range of , and select the model that minimizes Mallow's C p criterion. The minimizing model has associated solution denoted by b b ðÀdÞ (the subscript ðÀdÞ emphasizes the notion that data subset d was removed); (iii) repeat step (ii) for each data subset d ¼ 1,. .. , 10 and compute the cross-validation score(2011), we did two additional steps to the above 10-fold cross-validation. First, when the minimizer of the cross-validation score was not unique, we took 1 , 2 as the average of the minimizers. Second, because Step (i) yields a different random partition on each application, repeated applications of the 10-fold cross-validation may yield different optimal 1 , 2 and thus different selected variables, especially when the signals are sparse and small.<ref type="bibr" target="#b14">Martinez et al. (2011)</ref>also noted this and suggested performing the 10-fold cross-validation repeatedly, e.g. 100 times, to develop a complete understanding of the variables selected. The idea, thus, is to repeat the 10-fold cross-validation multiple times and retain those variables that were selected at least 60% of the time, say.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Repeated application of current Lasso methods</head><p>Other possible approaches for obtaining sparsity among groups, subgroups and individual predictors are through appropriate combinations of the Lasso, group Lasso and SGL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Group Lasso and SGL</head><p>To achieve sparsity among the groups, one may first apply the group Lasso after orthonormalizing the group model matrices. The group Lasso criterion is when 1 ¼ 1 and 2 ¼ 0 in QðbÞ, and hence depends only on the regularization parameter. To optimally select , we evaluate the criterion QðbÞ with 1 ¼ 1, 2 ¼ 0 at a range of values as in Section 2.2.2. The optimal corresponds to the model that minimizes Mallows' Cp criterion. Because the group Lasso selects an entire group of predictors to be included/excluded from the model, the chosen model will have some groups with all non-zero coefficients (i.e. groups retained by the group Lasso), and some groups with all zero coefficients (i.e. groups dismissed by the group Lasso). After achieving sparsity among groups, we then proceed to achieve sparsity among the subgroups and individual predictors via SGL. For those groups selected by the group Lasso, we apply SGL to all subgroups within these groups. When applying SGL, we do not orthonormalize the subgroup model matrices as done for the group Lasso. The criterion for SGL among subgroups is when 1 ¼ 0 in QðbÞ and thus depends on 2 ,. We choose the optimal and 2 via a repeated 10-fold crossvalidation as in Section 2.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Repeated group Lasso and Lasso</head><p>Another way to achieve the desired sparsity is as follows. First, apply the group Lasso to the orthonormalized group model matrices to select relevant groups. Second, using the selected groups, select relevant subgroups within by applying the group Lasso to the orthonormalized subgroup model matrices. Lastly, select relevant individual predictors by applying the Lasso to all predictors in the selected subgroups. In the last step, the Lasso is applied to the original predictors, not the orthonormalized versions. In each application of group Lasso and Lasso, criterion QðbÞ only depends on , which is chosen as in Section 2.2.2.The parameter 2 and the coefficient vectors for each subgroup were chosen according to two settings. In Setting 1, b ð1, 1Þ ¼ b ð1, 2Þ ¼ b ð2, 1Þ ¼ ð6, 6:4, 6:6, 8Þ T , b ð3, 1Þ ¼ ð12:5, 12:5, 0, 0Þ T and 2 ¼ 1. All remaining subgroup coefficients were zero. In Setting 2, b ð1, 1Þ ¼ b ð1, 2Þ ¼ b ð2, 1Þ ¼ ð2, 4, 6, 8Þ T , b ð3, 1Þ ¼ ð10, 10, 0, 0Þ T and 2 ¼ 1. Again, all remaining subgroup coefficients were zero. For each parameter setting, we generated 500 datasets and applied seven methods: SGSL, the two variable selection procedures in Section 2.2.2 and the following four other competing methods.(2) Group Lasso: We applied the group Lasso after orthonormalizing the group model matrices. To find the best fitting model, we minimized QðbÞ, with 1 ¼ 1, 2 ¼ 0, over a range of values as in Section 2.2.2, and chose the model that minimized Mallows' C p criterion. This method yields sparsity among groups, but not among subgroups, nor individual predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SIMULATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simulation design</head><p>(3) Repeated group Lasso: We applied the group Lasso at the group and subgroup levels. In each application of the group Lasso, the best fitting model was that which minimized Mallows' C p criterion. This method yields sparsity among groups and subgroups, but not among individual predictors.</p><p>(4) Sparse-group Lasso: We applied SGL among the groups; that is, we minimized QðbÞ where 2 ¼ 0. To select the tuning parameter 1 and , we applied the repeated 10-fold cross-validation in Section 2.2.2. This method ignores subgroup memberships and may not select significant subgroups.</p><p>For all methods requiring a selection of 1 and/or 2 , we repeated the 10-fold cross-validation 100 times to select the optimal 1 and/or 2. Ultimately, this led to 100 possibly different 1 , 2 values, and thus 100 possibly different ways variables were selected. Ultimately, we retained variables that were chosen at least 60% of the time in the 100 repeated applications. We did not use the average of the 1 , 2 values to select the variables. To evaluate the seven methods, we computed the average percentage of time predictors were selected, the observed false discovery rate (<ref type="bibr" target="#b0">Benjamini and Hochberg, 1995</ref>, FDR) and geometric mean of specificity and sensitivity (defined later in the text). To compute these quantities, we divided the predictors in each subgroup into those whose true parameter values are non-zero (i.e. relevant predictors), and those whose true parameter values are zero (i.e. irrelevant predictors). We then reported the average percentage of time relevant and irrelevant predictors were selected in each subgroup. The observed FDR is the ratio of the average number of irrelevant predictors selected (i.e. false selections) over the average number of predictors selected. The geometric mean of sensitivity and specificity is G ðspecificity Â sensitivityÞ 1=2 (<ref type="bibr" target="#b11">Kubat et al., 1998</ref>). Specificity is the proportion of irrelevant predictors that were not selected among irrelevant predictors, and sensitivity is the proportion of relevant predictors that were selected among relevant predictors. The range of G is<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, and large G-values indicate that most predictors are classified correctly. We prefer G over specificity and sensitivity alone, as it counteracts the imbalance between the number of relevant and irrelevant predictors (<ref type="bibr" target="#b11">Kubat et al., 1998</ref>). Observed FDR and G-values were computed using all groups, and using only Groups 2 and 3 so as to demonstrate how the methods perform for these two groups, which have sparsity within their subgroups. Among all methods, the reliable one will routinely select relevant predictors, and rarely or never select irrelevant predictors. Thus, the ideal method will have low FDRs and high G-values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simulation results</head><p>Results for the two simulation settings are given in<ref type="figure" target="#tab_1">Table 1</ref>. In general, our procedure based on the new criterion QðbÞ provided the most reliable results: it largely selected the relevant predictors and ignored irrelevant predictors (irrelevant predictors were incorrectly chosen 55% of the time). This performance resulted in small FDRs, often smaller than the FDRs from other methods. In comparison to SGL, we expected our method to perform equally well when determining relevant groups (both methods have essentially similar criterion for determining if a group is relevant or not), but we expected our method to outperform SGL in detecting sparsity between and within subgroups. SGL is not designed to detect relevant subgroups within a group, nor is it designed to detect relevant individual predictors within a subgroup. Our method, in contrast, can do this. The results from our simulation study confirmed these expectations. Our proposed procedure performed as well as SGL in selecting Group 1, which had all non-zero coefficients. But, our method better detected the true sparsity in Groups 2 and 3. Compared with SGL, our method correctly selected the relevant subgroups and relevant individual predictors at least 4% more often, and had nearly the same or fewer incorrect decisions in selecting irrelevant predictors. This correct classification is evident by the larger G-value for Groups 2 and 3 (see G z in<ref type="figure" target="#tab_1">Table 1</ref>). For these two groups, our proposed method has a G-value at least 1.14 times bigger than the G-value for SGL. When considering all groups together (see G Ã in<ref type="figure" target="#tab_1">Table 1</ref>), the G-values for our proposed method and SGL are similar because of the similar performance in Groups 1 and Groups 4–10. This is no surprise given that for Group 1 and Groups 4–10, our proposed method and SGL have similar selection criteria, and thus, should behave equally well as they do. However, when there is sparsity between and within subgroups (as is common in microbiome data; see Section 4), SGL fails to detect such a structure. Thus, when there is sparsity between and within groups and subgroups, our method has higher sensitivity and more power than SGL. Our proposed method also yielded better results than the other five methods in terms of capturing the true clustering and achieving higher G-values. The Lasso, designed to select individual predictors but not entire subsets, largely ignored the relevant cluster of predictors in Group 1 and in Group 2, subgroup 1. For Group 3, which only had 3 of 10 relevant predictors, the Lasso did successfully select these variables as often as our proposed procedure did. Thus, when necessary, our method can behave similarly to the Lasso, which is an attractive feature when individual features need to be selected. Still, because our simulated data has a specific grouping structure which the Lasso cannot capture, our proposed method has larger G-values than the Lasso both when computed across all groups (0.64 for our method compared with 0.56 for the Lasso) and when computed for Groups 2 and 3 (0.40 for our method compared with 0.36 for the Lasso). Hence, because our interest goes beyond selecting individual predictors, we prefer our proposed method. Lastly, the proposed iterative procedures all fared poorly, with G-values nearly half that of our proposed method. Thesethe initial application of the group Lasso. As the group Lasso is designed to detect relevant groups, it has difficulty determining if an entire group is relevant when that group is sparse. Thus, the sparsity in Groups 2 and 3 prevented the group Lasso, and all forthcoming Lasso-based methods, from selecting these groups or the relevant clusters within.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL EXAMPLE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Microbial data</head><p>Our motivating example is from a dietary treatment study in mice (<ref type="bibr" target="#b21">Thomas et al., 2013</ref>) for which we measured fecal microbial diversity. The study used an obesity reversal paradigm and consisted of n ¼ 30 obese male mice equally and randomly assigned to one of three diets: (i) a control soy-based diet with 0.5% (by weight) inorganic calcium; (ii) a high calcium soy-based diet with 1.5% (by weight) inorganic calcium; and (iii) a non-fat dry milk (NFDM) diet with 1.5% (by weight) calcium as NFDM-intrinsic and inorganic calcium. After 10 weeks of feeding, feces from all mice were analyzed for microbial communities via pyrosequencing. Mice on the NFDM diet had enhanced bodyfat loss (<ref type="bibr" target="#b21">Thomas et al., 2013</ref>). For each mouse, data consists of relative messenger RNA (mRNA) expression of CD68 in adipose and microbial percentages (X) from p ¼ 51 microbes classified at the phylum, family and genus levels. The mRNA expression of CD68 is used to judge the extent to which macrophages have infiltrated adipose, an event that occurs with bodyweight gain and is associated with systemic inflammation (<ref type="bibr" target="#b21">Thomas et al., 2013</ref>). The microbes were classified into two phyla: Bacteriodetes and Firmicutes. Each phylum had at least five families, with each family having at least two bacterial genera. The key interest is to find those microbial phyla, families and genera associated with CD68 mRNA expression in this p4n setting. A prior analysis in<ref type="bibr" target="#b6">Garcia et al. (2013)</ref>demonstrated that diet has a significant impact on expression of mRNA for CD68. To accommodate this diet effect, we took the response variable (y) as the residuals from regressing expression of mRNA for CD68 on diet. See<ref type="bibr" target="#b6">Garcia et al. (2013)</ref>for other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We applied the same seven variable selection techniques from the simulation study to the microbial data. We found that our proposed procedure selected the entire family Streptococcaceae in the Firmicutes phyla to have an effect on expression of mRNA for CD68. The family consisted of Lactococcus and Streptococcus genera. In comparison, SGL and Lasso were only able to pick one member from this family (Streptococcus), which indicates the inflexibility of these latter two methods in selecting important families (i.e. subgroups). Having our method select the Streptococcaceae family makes sense, as members of Streptococcaceae flourish in nutrient-rich environments, such as an overfed subject's gut (e.g. obese mice). Moreover, mice in this study experienced chronic inflammation secondary to obesity and hyperglycemia as evidenced by elevated adipose CD68 arising from macrophage infiltration of adipose tissue (<ref type="bibr" target="#b21">Thomas et al., 2013</ref>). At present, it seems unlikely that simple chronic caloric excess promoted Streptococcaceae abundance in the obese mice, as this relationship was not seen in newly obese mice<ref type="bibr">[see Thomas et al. (2012)</ref>and Supplementary<ref type="bibr">Material]</ref>. Secondary effects appear to play a role as changes in host inflammatory state were previously associated with Streptococcaceae family members in hosts with either strongly positive or negative energy balance. Intestinal infusion of fecal microbiota from lean donors improved glucose metabolism in obese humans with metabolic syndrome in conjunction with a 30% reduction in the Streptococcaceae family member Streptococcus bovis in the small intestine (<ref type="bibr" target="#b24">Vrieze et al., 2012</ref>). Obesity driven type II diabetes and metabolic syndrome are considered chronic inflammatory states (<ref type="bibr" target="#b2">Dandona et al., 2005</ref>). In recent studies, the Streptococcaceae family has been shown to be associated with inflammation of various origins and in an energy independent fashion. First, host physiology can influence the composition of the microbiota. For example, poor glucose control in a cohort of European women was associated with Streptococcus sp. C150 (<ref type="bibr" target="#b9">Karlsson et al., 2013</ref>). Second, microbiota composition can modulate host physiology in a variable way. For example, formula feeding is associated with increased frequency of pediatric intestinal inflammation including necrotizing enterocolitis; in a mouse model formula feeding increased Lactococcus at the expense of Lactobacillus and altered host gene expression to indicate increased oxidative stress, inflammation and impaired defense capacity (<ref type="bibr" target="#b1">Carlisle et al., 2013</ref>). Third,<ref type="bibr" target="#b18">Smith et al. (2013)</ref>showed that microbiota composition determines host health outcome in response to identical dietary shifts; in this case, microbiota from twin pairs discordant for the disease Kwashiorkor variably provoke disease depending on the complexity and adequacy of the diet. Stability and resilience of intestinal microbial diversity is an active area of research, and it is now recognized that chronic alterations in host environmental exposure (e.g. diet) and physiological state (e.g. obese) can influence intestinal microbiota (<ref type="bibr">Lozupone et al., 2012</ref>) and likewise microbiota can respond to diet to increase/decrease susceptibility of the host to disease (<ref type="bibr" target="#b1">Carlisle et al., 2013;</ref><ref type="bibr" target="#b18">Smith et al., 2013</ref>). Our results further highlight the need to understand temporal dimension of interventions to improve efficacy. were linked to CD68 expression in adipose tissue of mice with chronic obesity. All other methods could not detect this relationship. In the Supplementary Material, we analyze a second microbiome dataset, in which only our method and Lasso detects an important individual bacterial genus. The data we analyzed were classified into different taxonomies following pyrosequencing, which can result in some genera being more diverse. If consistently sized operational taxa are needed, one could use operational taxonomic unit clustering (The Human Microbiome Project Consortium, 2012). Still, regardless of the classification, our method is applicable. Thus, one could apply our method using the different classifications to gain insight into how microbes impact health-related features. Of course, which classification to use depends on the project's overall goal and available resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>CVð 10 , 20 Þ ¼ P 10 r¼1 jjy ðdÞ À X ðdÞ b b ðÀdÞ jj 2 2 where y ðdÞ and X ðdÞ denote the response and explanatory variables for the data subset d that was removed and b b ðÀdÞ is the solution from step (ii). In our applications, we repeated this three-step procedure for 10 , 20 taking values 0.01, 0.04, 0.07, 0.10, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95 such that 10 þ 20 51. The optimal 1 , 2 is the pair that minimizes the crossvalidation score. Analogous to the work done in Garcia et al. (2013) and Martinez et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>We evaluated the performance of the proposed methods in Section 2 on simulated data where predictors have group and subgroup memberships. We considered L ¼ 10 groups such that each group had 2 subgroups. We divided p ¼ 80 predictors so that each subgroup had 4 predictors, and each group had 8 predictors. Covariates in each group were generated from a Normalð0, DÞ distribution where D ¼ diagðD Ã , D Ã Þ and D Ã ¼ 0:7J 4 þ 0:3I 4. Here, J 4 corresponds to a 4 Â 4 matrix of ones, and I 4 is the 4 Â 4 identity matrix. This data generation procedure implies that predictors within the same subgroup have a correlation of 0.7, but predictors in different subgroups/groups are independent. We set sample size n ¼ 30 and generated the response variable y ¼ P 10 k¼1 P 2 m¼1 X ðk, mÞ b ðk, mÞ þ , where is Normalð0, 2 I n Þ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(1)</head><figDesc>Lasso: We applied the Least Angle Regression algorithm of Efron et al. (2004), which provides the entire sequence of model fits in the Lasso path. The best fitting model was that which minimized Mallows' C p criterion. This method ignores the grouped nature of the predictors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 1. Simulation results for Setting 1 and 2 based on 500 simulations</figDesc><table>Group 
Subgroup 
SGSL 
SGL 
Lasso 
GpL, 
SGL 

GpL Â 2, 
Lasso 

GpL Â 2 
GpL 
New 
method 

SGL 
Lasso 
GpL, 
SGL 

GpL Â 2, 
Lasso 

GpL Â 2 
GpL 

Setting 1 
Setting 2 

1 
1 (Non-zero) 
53.20 
56.70 
40.80 
14.10 
11.05 
20.80 
25.20 
50.80 
52.45 
38.85 
11.00 
9.25 
16.80 
18.00 
2 (Non-zero) 
54.05 
56.35 
41.50 
14.10 
12.65 
23.00 
25.20 
52.50 
52.95 
39.90 
11.10 
8.20 
15.20 
18.00 
2 
1 (Non-zero) 
16.55 
12.55 
13.40 
0.00 
0.00 
0.00 
0.00 
17.65 
12.75 
15.45 
0.00 
0.00 
0.00 
0.00 
2 (Zero) 
4.85 
5.45 
5.70 
0.00 
0.00 
0.00 
0.00 
4.90 
5.30 
5.70 
0.00 
0.00 
0.00 
0.00 
3 
1 (Non-Zero) 
19.20 
13.60 
21.30 
0.00 
0.00 
0.00 
0.00 
26.50 
19.70 
25.80 
0.00 
0.00 
0.00 
0.00 
1 and 2 (Zero) 
3.50 
3.37 
3.77 
0.00 
0.00 
0.00 
0.00 
4.40 
4.60 
4.77 
0.00 
0.00 
0.00 
0.00 
4–10 
(Zero) 
0.03 
0.01 
0.04 
0.00 
0.00 
0.00 
0.00 
0.04 
0.02 
0.04 
0.00 
0.00 
0.00 
0.00 
FDR a 
0.07 
0.07 
0.10 
0.00 
0.00 
0.00 
0.00 
0.08 
0.09 
0.11 
0.00 
0.00 
0.00 
0.00 
G a 
0.64 
0.64 
0.56 
0.30 
0.28 
0.38 
0.41 
0.63 
0.63 
0.56 
0.27 
0.24 
0.33 
0.35 
FDR b 
0.28 
0.35 
0.32 
NA 
NA 
NA 
NA 
0.27 
0.35 
0.31 
NA 
NA 
NA 
NA 
G b 
0.40 
0.35 
0.36 
0.00 
0.00 
0.00 
0.00 
0.41 
0.35 
0.38 
0.00 
0.00 
0.00 
0.00 

Note: Average percentages of time each set of variables is selected with our proposed sparse group–subgroup Lasso ('SGSL'); sparse-group Lasso ('SGL'); Lasso; group Lasso 
at group level and sparse-group Lasso at subgroup level ('GpL, SGL'); group Lasso at group and subgroup levels, and Lasso at individual features level ('GpL Â 2, Lasso'); 
group Lasso at group and subgroup levels ('GpL Â 2'); and group Lasso ('GpL'). A 'non-zero' subgroup means variables are relevant; a 'Zero' subgroup means variables are 
irrelevant. 

a 

We also report observed false discovery rate (FDR) and G-values using all groups. 

b 

We also report observed false discovery rate (FDR) and G-values using Groups 2 and 3. 
'NA' denotes values were incomputable because of division by 0. 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">T.P.Garcia et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="2"> Þg þ where z þ ¼ maxðz, 0Þ. Let &apos;fr ðÀk, mÞ , b ðk, mÞ g ¼ jjr ðÀk, mÞ À X ðk, mÞ</note>

			<note place="foot">Application to gut microbiome data at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="5"> DISCUSSION We developed SGSL, a new variable selection procedure that yields sparsity among predictor groups, subgroups and individuals. For simulated data that had a rich clustering structure, our method outperformed competing methods with small FDRs and high geometric means of sensitivity and specificity. Our method was capable of capturing features detectable by SGL and Lasso, but went a step further: it correctly identified sparsity between and within subgroups, a feature common to microbiome data. We applied our method to a gut microbiota dataset to select important phyla, families and genera that show an association with CD68 mRNA expression in adipose. After controlling for diet effects, our preferred method revealed a family level relationship in which members of the Streptococcaceae family T.P.Garcia et al.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors thank Sean H. Adams for the metabolic data, as well as the editor, associate editor and two referees for their insightful comments that greatly improved the article.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: a practical and powerful approach to multiple testing</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Benjamini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Hochberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JRSSB</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Murine gut microbiota and transcriptome are diet dependent</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">M</forename>
				<surname>Carlisle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="287" to="294" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Metabolic syndrome: a comprehensive perspective based on interactions between obesity, diabetes, and inflammation</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Dandona</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="1448" to="1454" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Ideal spatial adaptation by wavelet shrinkage</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">L</forename>
				<surname>Donoho</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">M</forename>
				<surname>Johnston</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<monogr>
		<title level="m" type="main">A note on the group Lasso and a sparse-group Lasso</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Structured variable selection with q-values</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">P</forename>
				<surname>Garcia</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="695" to="707" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Tuning parameter selection in sparse regression modeling</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Hirose</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Proximal methods for hierarchical sparse coding</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Jenatton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2297" to="2334" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Gut metagenome in European women with normal, impaired and diabetic glucose control</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">H</forename>
				<surname>Karlsson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">498</biblScope>
			<biblScope unit="page" from="99" to="103" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Tree-guided group lasso for multi-response regression with structured sparsity with an applicaton to eQTL mapping</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">P</forename>
				<surname>Xing</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1095" to="1117" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine learning for the detection of oil spills in satellite radar images</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kubat</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="195" to="215" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Moreau-Yosida Regularization for Grouped Tree Structure Learning</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ye</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Diversity, stability and resilience of the human gut microbiota</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">A</forename>
				<surname>Lozupone</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">489</biblScope>
			<biblScope unit="page" from="220" to="230" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<monogr>
		<title level="m" type="main">Empirical performance of cross validation with oracle methods in a genomics context. The American Statistician</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">G</forename>
				<surname>Martinez</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">On model selection curves</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Muïler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">H</forename>
				<surname>Welsh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="240" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<monogr>
		<title level="m" type="main">Gradient methods for minimizing composite objective function</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Nesterov</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">A sparse-group Lasso</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="231" to="245" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Gut microbiomes of Malawian twin pairs discordant for kwashiorkor</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">I</forename>
				<surname>Smith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">339</biblScope>
			<biblScope unit="page" from="548" to="554" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">The Human Microbiome Project Consortium. (2012) A framework for human microbiome research</title>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="page" from="215" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">A high calcium diet containing nonfat dry milk reduces weight gain and associated adipose tissue inflammation in diet-induced obese mice when compared to high calcium alone</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nutr. Metabol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">A dairy-based high calcium diet improves glucose homeostatis and reduces steatosis in the context of preexisting obesity</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Obesity</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="229" to="235" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the Lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JRSSB</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">A gene-expression signature as a predictor of survival in breast cancer</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Van De Vijver</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl.J. Med</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Transfer of intestinal microbiota from lean donors increases insulin sensitivity in subjects with metabolic syndrome</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Vrieze</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="913" to="916" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Yuan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JRSSB</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">The composite absolute penalties family for grouped and hierarchical variable selection</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3468" to="3497" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title level="m" type="main">Application to gut microbiome data</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>