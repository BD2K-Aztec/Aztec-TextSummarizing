
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence analysis SCALCE: boosting sequence compression algorithms using locally consistent encoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName>
								<forename type="first">Faraz</forename>
								<surname>Hach</surname>
							</persName>
							<email>fhach@cs.sfu.ca or cenk@cs.sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<postCode>V5A 1S6</postCode>
									<settlement>Burnaby</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ibrahim</forename>
								<surname>Numanagic´1numanagic´numanagic´1</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<postCode>V5A 1S6</postCode>
									<settlement>Burnaby</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Can</forename>
								<surname>Alkan</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<postCode>V5A 1S6</postCode>
									<settlement>Burnaby</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">S</forename>
								<forename type="middle">Cenk</forename>
								<surname>Sahinalp</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<postCode>V5A 1S6</postCode>
									<settlement>Burnaby</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Alfonso</forename>
								<surname>Valencia</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bilkent University</orgName>
								<address>
									<postCode>06800</postCode>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence analysis SCALCE: boosting sequence compression algorithms using locally consistent encoding</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="issue">23</biblScope>
							<biblScope unit="page" from="3051" to="3057"/>
							<date type="published" when="2012">2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts593</idno>
					<note type="submission">Received on June 6, 2012; revised on September 11, 2012; accepted on September 27, 2012</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: The high throughput sequencing (HTS) platforms generate unprecedented amounts of data that introduce challenges for the computational infrastructure. Data management, storage and analysis have become major logistical obstacles for those adopting the new platforms. The requirement for large investment for this purpose almost signalled the end of the Sequence Read Archive hosted at the National Center for Biotechnology Information (NCBI), which holds most of the sequence data generated world wide. Currently, most HTS data are compressed through general purpose algorithms such as gzip. These algorithms are not designed for compressing data generated by the HTS platforms; for example, they do not take advantage of the specific nature of genomic sequence data, that is, limited alphabet size and high similarity among reads. Fast and efficient compression algorithms designed specifically for HTS data should be able to address some of the issues in data management, storage and communication. Such algorithms would also help with analysis provided they offer additional capabilities such as random access to any read and indexing for efficient sequence similarity search. Here we present SCALCE, a &apos;boosting&apos; scheme based on Locally Consistent Parsing technique, which reorganizes the reads in a way that results in a higher compression speed and compression rate, independent of the compression algorithm in use and without using a reference genome. Results: Our tests indicate that SCALCE can improve the compression rate achieved through gzip by a factor of 4.19—when the goal is to compress the reads alone. In fact, on SCALCE reordered reads, gzip running time can improve by a factor of 15.06 on a standard PC with a single core and 6 GB memory. Interestingly even the running time of SCALCE þ gzip improves that of gzip alone by a factor of 2.09. When compared with the recently published BEETL, which aims to sort the (inverted) reads in lexicographic order for improving bzip2, SCALCE þ gzip provides up to 2.01 times better compression while improving the running time by a factor of 5.17. SCALCE also provides the option to compress the quality scores as well as the read names, in addition to the reads themselves. This is achieved by compressing the quality scores through order-3 Arithmetic Coding (AC) and the read names through gzip through the reordering SCALCE provides on the reads. This way, in comparison with gzip compression of the unordered FASTQ files (including reads, read names and quality scores), SCALCE (together with gzip and arithmetic encoding) can provide up to 3.34 improvement in the compression rate and 1.26 improvement in running time. Availability: Our algorithm, SCALCE (Sequence Compression Algorithm using Locally Consistent Encoding), is implemented in Cþþ with both gzip and bzip2 compression options. It also supports multithreading when gzip option is selected, and the pigz binary is available. It is available at http://scalce.sourceforge.net. Contact:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Although the vast majority of high throughput sequencing (HTS) data are compressed through general purpose methods, in particular gzip and its variants, the need for improved performance has recently lead to the development of a number of techniques specifically for HTS data. Available compression techniques for HTS data either exploit (i) the similarity between the reads and a reference genome or (ii) the similarity between the reads themselves. Once such similarities are established, each read is encoded by the use of techniques derived from classical lossless compression algorithms such as Lempel-Ziv-77 (<ref type="bibr" target="#b30">Ziv and Lempel, 1977</ref>) (which is the basis of gzip and all other zip formats) or Lempel-Ziv-78 (<ref type="bibr" target="#b31">Ziv and Lempel, 1978</ref>). Compression methods that exploit the similarity between individual reads and the reference genome use the reference genome as a 'dictionary' and represent individual reads with a pointer to one mapping position in the reference genome, together with additional information about whether the read has some differences with the mapping loci. As a result, these methods (<ref type="bibr" target="#b17">Hsi-Yang Fritz et al., 2011;</ref><ref type="bibr" target="#b20">Kozanitis et al., 2010</ref>) require (i) the availability of a reference genome and (ii) mapping of the reads to the reference genome. Unfortunately, genome mapping is a time-wise costly step, especially when compared with the actual execution of compression (i.e. encoding the reads) itself. Furthermore, these methods necessitate the availability of a reference genome both for compression and decompression. Finally, many large-scale sequencing projects such as the Genome 10K Project (<ref type="bibr" target="#b16">Haussler et al., 2009</ref>) focus on species without reference genomes. Compression methods that exploit the similarity between the reads themselves simply concatenate the reads to obtain a single sequence:<ref type="bibr" target="#b6">Bhola et al., 2011</ref>apply modification of<ref type="bibr">Lempel-Ziv algorithm, Tembe et al., 2010;</ref><ref type="bibr" target="#b10">Deorowicz and</ref><ref type="bibr">Grabowski, 2011 use Huffman Coding (Huffman, 1952</ref>) and *To whom correspondence should be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>y</head><p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.<ref type="bibr" target="#b9">Cox et al., 2012</ref>use Burrows Wheeler transformation (<ref type="bibr" target="#b7">Burrows and Wheeler, 1994</ref>). In particular, the Lempel-Ziv methods (e.g. gzip and derivatives) iteratively go over the concatenated sequence and encode a prefix of the uncompressed portion by a 'pointer' to an identical substring in the compressed portion. This general methodology has three major benefits:</p><p>(i) Lempel-Ziv–based methods (e.g. gzip and derivatives) have been optimized through many years and are typically fast; in fact, the more 'compressible' the input sequence is, the faster they work, both in compression and decompression; (ii) these methods do not need a reference genome and (iii) because these techniques are almost universally available, there is no need to distribute a newly developed compression algorithm. Interestingly, the availability of a reference genome can improve the compression rate achieved by standard Lempel-Ziv techniques. If the reads are first mapped to a reference genome and then reordered with respect to the genomic coordinates they map to before they are concatenated, they are not only compressed more because of increased locality, but also in less time. This, mapping-first compressing-later approach, combines some of the advantages of the two distinct sets of methods above:</p><p>(i) it does not necessitate the availability of a reference genome during decompression (compression is typically applied once to a dataset, but decompression can be applied many times), and (ii) it only uses the reordering idea as a front end booster [Burrows Wheeler transform—BWT—is a classical example for a compression booster. It rearranges input symbols to improve the compression achieved by Run Length Encoding and Arithmetic Coding. Further boosting for BWT is also possible: see (<ref type="bibr" target="#b13">Ferragina and Manzini, 2004;</ref><ref type="bibr" target="#b14">Ferragina et al., 2005</ref><ref type="bibr" target="#b15">Ferragina et al., , 2006</ref>. Any well-known, well-distributed compression software can be applied to the reordered reads. Unfortunately, this strategy still suffers from the need for a reference genome during compression. In this article, we introduce a novel HTS genome (or transcriptome, exome, etc.) sequence compression approach that will combine the advantages of the two types of algorithms above. It is based on reorganization of the reads so as to 'boost' the locality of reference. The reorganization is achieved by observing sufficiently long 'core' substrings that are shared between the reads, and clustering such reads to be compressed together. This reorganization acts as a fast substitute for mapping-based reordering (see above); in fact, the first step of all standard seed and extend-type mapping methods identify blocks of identity between the reads and the references genome. The core substrings of our boosting method are derived from the Locally Consistent Parsing (LCP) method devised by Sahinalp and colleagues (<ref type="bibr" target="#b24">Sahinalp and Vishkin, 1996;</ref><ref type="bibr" target="#b8">Cormode et al., 2000;</ref><ref type="bibr" target="#b5">Batu et al., 2006</ref>). For any user-specified integer c and with any alphabet (in our case, the DNA alphabet), the LCP identifies 'core' substrings of length between c and 2c such that (i) any string from the alphabet of length 3c or more includes at least one such core string, (ii) there are no more than three such core strings in any string of length 4c or less and (iii) if two long substrings of a string are identical, then their core substrings must be identical. LCP is a combinatorial pattern matching technique that aims to identify 'building blocks' of strings. It has been devised for pattern matching, and provides faster solutions in comparison with the quadratic running time offered by the classical dynamic programming schemes. As a novel application, we introduce LCP to genome compression, where it aims to act as a front end (i.e. booster) to commonly available data compression programs. For each read, LCP simply identifies the longest core substring (there could be one or more cores in each read). The reads are 'bucketed' based on such representative core strings and within the bucket, ordered lexicographically with respect to the position of the representative core. We compress reads in each bucket using Lempel-Ziv variants or any other related method without the need for a reference genome. As can be seen, LCP mimics the mapping step of the mappingbased strategy described above in an intelligent manner: on any pair of reads with significant (suffix-prefix) overlaps, LCP identifies the same core substring and subsequently buckets the two reads together. For a given read, the recognition of the core strings and bucketing can be done in time linear with the read length. Note that the 'dictionary' of core substrings is devised once for a given read length as a pre-processing step. Thus, the LCP-based booster we are proposing is efficient. LCP provides mathematical guarantees that enable highly efficient and reliable bucketing that captures substring similarities. We have applied the LCP-based reordering scheme for (i) short reads of length 51 bp obtained from bacterial genomes and (ii) short reads of length 100 bp from one human genome, and obtained significant improvements in both compression rate and running time over alternative methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A theoretical exposition to the LCP technique</head><p>The simplest form of the LCP technique works only on reads that involve no tandemly repeated blocks (i.e. the reads can not include a substring of the form XX where X is a string of any length ! 1; note that a more general version of LCP that does not require this restriction is described in<ref type="bibr">Vishkin, 1994, 1996;</ref><ref type="bibr" target="#b5">Batu et al., 2006</ref>so that LCP works on any string of any length). Under this restriction, given the alphabet f0, 1, 2,. .. , k À 1g, LCP partitions a given string S into non-overlapping blocks of size at least 2 and at most k such that two identical substrings R 1 and R 2 of S are partitioned identically—except for a constant number of symbols on the margins. LCP achieves this by simply marking all local maxima (i.e. symbols whose value is greater than its both neighbours) and all local minima, which do not have a neighbour already marked as a local maxima—note that beginning of S and the ending of S are considered to be special symbols lexicographically smaller than any other symbol. LCP puts a block divider after each marked symbol and the implied blocks will be of desirable length and will satisfy the identical partitioning property mentioned above. Then, LCP extends each block residing between two neighbouring block dividers by one symbol to the right and one symbol to the left to obtain core blocks of S. Note that two neighbouring core blocks overlap by two symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Example</head><p>Let S ¼ 21312032102021312032102; in other words, S ¼ X0X, where X ¼ 21312032102. The string S satisfies the above condition; i.e. it contains no identically and tandemly repeated substrings. When the above simple version of LCP is applied to S, it will be partitioned as j213j12j03j2102j02j13j12j03j2102j. Clearly, with the exception of the leftmost blocks, the two occurrences of X are partitioned identically. Now LCP identifies the core blocks as 2131,<ref type="bibr">3120,</ref><ref type="bibr">2032,</ref><ref type="bibr">321020,</ref><ref type="bibr">2021,</ref><ref type="bibr">2131,</ref><ref type="bibr">3120,</ref><ref type="bibr">2032,</ref><ref type="bibr">32102</ref>. Observe that the (i) two occurrences of string X are partitioned by LCP the same way except in the margins. Further observe that (ii) if a string is identified as a core block in a particular location, it must be identified as a core block elsewhere because of the fact that all symbols that lead LCP to identify that block as a core block are included in the core block. As a result, (iii) all core blocks that entirely reside in one occurrence of X should be identical to those that reside in another occurrence of X. Finally observe that (iv) the number of cores that reside in any substring X is at most 1/2 of its length and at least 1/k of its length. The above version of LCP can return core blocks with length as small as 4; a length 4 substring is clearly not specific enough for clustering an HTS read; we have to ensure that the minimum core block length c is a substantial fraction of the read length. LCP as described in<ref type="bibr">Vishkin, 1994, 1996;</ref><ref type="bibr" target="#b5">Batu et al., 2006</ref>enables to partition S into non-overlapping blocks of size at least c and at most 2c – 1 for any user defined c. These blocks can be extended by a constant number of symbols to the right and to the left to obtain the 'core' blocks of S. (Please see the Supplementary Data to get a flavour of how this is done.) In the context of compressing HTS reads, if c is picked to be a significantly long fraction of the read size, LCP applied on the HTS reads will guarantee that each read will include at least one and at most three of these core blocks. Unfortunately, this general version of LCP is too complex to be of practical interest. As a result, we have developed a practical variant of LCP described below to obtain core blocks of each HTS read with minimum length 8 and maximum length 20. Interestingly, we observed that in practice 499% of all HTS reads of length 50 or more include at least one core of length 14 or less. As a result, we are interested in identifying only those core blocks of lengths in the range of 8–14. Still there could be multiple such core blocks in each HTS read; SCALCE will pick the longest one as the representative core block of the read (if there are more than one such block, SCALCE may break the tie in any consistent way). SCALCE will then cluster this read with other reads that have the same representative core block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A practical implementation of LCP for reordering reads</head><p>The purpose of reordering reads is to group highly related reads, in fact those reads that ideally come from the same region and have large overlaps together so as to boost gzip and other Lempel-Ziv-77–based compression methods. If one concatenates reads from a donor genome in an arbitrary order, highly similar reads will be scattered over the resulting string. Because Lempel-Ziv-77–based techniques compress the input string iteratively, from left to right, replacing the longest possible prefix of the uncompressed portion of the input string with a pointer to its earlier (already compressed) occurrence, as the distance between the two occurrences of this substring to be compressed increases, the binary representation of the pointer also increases. As a result, gzip and other variants only search for occurrences of strings within a relatively small window. Thus reordering reads so as to bring together those with large (suffix–prefix) overlaps is highly beneficial to gzip and other similar compression methods. For this purpose, it is possible to reorder the reads by sorting them based on their mapping loci on the reference genome. Alternatively, it may be possible to find similarities between the reads through pairwise comparisons (<ref type="bibr" target="#b29">Yanovsky, 2011</ref>). However each one of these approaches are time-wise costly. In contrast, our goal here is to obtain a few core blocks for each read so that two highly overlapping reads will have common core blocks. The reads will be reordered based on their common core blocks, which satisfy the following properties: (i) Each HTS read includes at least one core block. (ii) Each HTS read includes at most a small number of core blocks. This would be achieved if any sufficiently 'long' prefix of a core block can not be a suffix of another core block (this assures that two subsequent core blocks can not be too close to each other). We first extend the simple variant of LCP described above so as to handle strings from the alphabet AE ¼ f0, 1, 2, 3g (0 ¼ A, 1 ¼ C, 2 ¼ G, 3 ¼ T) that can include tandemly repeated blocks. In this variant, we define a core block as any 4-mer that satisfies one of the following rules:</p><p>(Local Maxima) xyzw where x5y and z5y;</p><p>(Low Periodicity) xyyz where x 6 ¼ y and z 6 ¼ y;</p><p>(Lack of Maxima) xyzw where x 6 ¼ y and y5z5w;</p><p>(Periodic Substrings) yyyx where x 6 ¼ y.</p><p>We computed all possible 4-mers (there are 256 of them) from the 4 letter alphabet AE and obtained 116 core blocks that satisfy the rules above. The reader can observe that the minimum distance between any two neighbouring cores will be 2 and the maximum possible distance will be 6 (note that this implementation of LCP is not aimed to satisfy any theoretical guarantee; rather, it is developed to work well in practice). This ensures that any read of length at least 9 includes one such core block. To capture longer regions of similarity between reads, we need to increase the lengths of core blocks. For that purpose, we first identify the so-called marker symbols in the read processed as follows. Let x, y, z, w, x, v 2 AE, then y is a marker for xyz, when x5y and z5y; y is a marker for xyyz, when x5y and z5y; y is a marker for xyyyz, when x 6 ¼ y and z 6 ¼ y; yy is a marker for xyyyyz, when x 6 ¼ y and z 6 ¼ y; y is a marker for xwyzv, when y5w x and y5z v.</p><p>Now on a given read, we first identify all marker symbols. We apply LCP to the sequence obtained by concatenating these marker symbols to obtain the core blocks of the marker symbols. We then map these core blocks of the marker symbols to the original symbols to obtain the core blocks of the original read. Given read R ¼ 0230000300, we identify its marker symbols as follows: 3 is the marker for 230, 00 is the marker for 300003 and 3 is the marker for 030 as per the marker identification rules above. The sequence obtained by concatenating these markers is 3003, which is itself (4-mer) core block according to the LCP description above. The projection of this core block on R is 23000030, which is thus identified as a core block (the only core block) of the read. For the 4 letter alphabet AE, we computed all ($5 million) possible core blocks of length f8,. .. , 14g according to the above rules (this is about 1% of all blocks in this length range). These rules assure that the minimum distance between two subsequent core blocks is 4 and thus the maximum number of core blocks per read is at most 11 per each HTS read of length 50. Furthermore, we observed that more than 99.5% of all reads have at least one core block (the other reads have all cores of length 15–20). Although this guarantee is weaker than the theoretical guarantee provided by the most general version of LCP, it serves our purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">A data structure for identifying core substrings of reads</head><p>We build a trie data structure representing each possible core substring by a path to efficiently place reads into 'buckets'. We find 'all' core substrings of each read and place the read in the bucket (associated with the core substring) that contains the maximum number of reads (if there are two or more such buckets, we pick one arbitrarily). If one simply uses the trie data structure, finding all core substrings within a read would require OðcrÞ time where r is the read length, and c is the length of all core substrings in that read. To improve the running time, we build an automaton implementing the Aho-Corasick dictionary matching algorithm (<ref type="bibr" target="#b3">Aho and Corasick, 1975</ref>). This improves the running time to Oðr þ kÞ, where k is the number of core substring occurrences in each read. Because the size of the alphabet AE is small (4 symbols), and the number of the core substrings is fixed, we can further improve the running time by pre-processing the automaton such that, for a given state of the automaton we calculate the associated bucket in Oð1Þ time, reducing the total search time to OðrÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Compressing the quality scores</head><p>Note that the HTS platforms generate additional information for each read that is not confined to the 4-letter alphabet AE. Each read is associated with a secondary string that contains the base calling phred (<ref type="bibr" target="#b12">Ewing and Green, 1998</ref>) quality score. Quality score of a base defines the probability that the base call is incorrect, and it is formulated as Q ¼ À10 Â log 10 ðPðerrorÞÞ (<ref type="bibr" target="#b12">Ewing and Green, 1998</ref>). The size of the alphabet for the quality scores is typically jAEj ¼ 40 for the Illumina platform, thus the compression rate for quality scores is lower than the actual reads. As mentioned in previous studies (<ref type="bibr" target="#b28">Wan et al., 2012</ref>), lossy compression can improve the quality scores compression rate. We provide an optional controlled lossy transformation approach based on the following observation. In most cases, for any basepair b, the quality scores of its 'neighbouring' basepairs would be either the same or within some small range of b's score (see<ref type="figure">Fig. 1</ref>). Based on this observation, we provide a lossy transformation scheme to reduce the alphabet size. We calculate the frequency table for the alphabet of quality scores from a reasonable subset of the qualities (1 million quality scores). We first use a simple greedy algorithm to find the local maxima within this table. We then reduce the variability among the quality scores in the vicinity of local maxima up to some error threshold e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>We evaluated the performance of the SCALCE algorithm for boosting gzip on a single core 2.4 GHz Intel Xeon X5690 PC (with network storage and 6 GB of memory). We used four different datasets in our tests: (i) Pseudomonas aeruginosa RNA-Seq library (51 bp, single lane), (ii) P.aeruginosa genomic sequence library (51 bp, single lane), (iii) whole genome shotgun sequencing (WGS) library generated from the genome of the HapMap individual NA18507 (100 bp reads at 40Â genome coverage) and (iv) a single lane from the same human WGS dataset corresponding to $1.22Â genome coverage (Sequence Read Archive ID: SRR034940). We removed any comments from name section (any string that appears after the first space). Also the third row should contain a single character (þ/À) separator character. The reads from each dataset were reordered through SCALCE and three separate files were obtained for (i) the reads themselves, (ii) the quality scores and (iii) the read names (each maintaining the same order). Note that LCP reordering is useful primarily for compressing the reads themselves through gzip. The quality scores were compressed via the scheme described above. Finally the read names were compressed through gzip as well. The compression rate and run time achieved by gzip software alone, only on the reads from the P.aeruginosa RNA-Seq library (dataset 1) is compared against those achieved by SCALCE followed by gzip in<ref type="figure" target="#tab_1">Table 1</ref>. The compression rates achieved by the gzip software alone in comparison with gzip following SCALCE on the combination of reads, quality scores and read names are presented in<ref type="figure" target="#tab_2">Table 2</ref>. The run times for the two schemes (again on reads, quality scores and read names all together) are presented in<ref type="figure" target="#tab_3">Table 3</ref>. When SCALCE is used with arithmetical coding of order 3 with lossless qualities, it boosts the compression rate of gzip between 1.42-and 2.13-fold (when applied to reads, quality scores and read names), significantly reducing the storage requirements for HTS data. When arithmetical coding of order 3 is used with 30% loss— without reducing the mapping accuracy—improvements in compression rate are between 1.86 and 3.34. In fact, the boosting factor can go up to 4.19 when compressing the reads only. Moreover, the speed of the gzip compression step can be improved by a factor of 15.06. Interestingly, the total run time for SCALCE þ gzip is less than the run time of gzip by a factor of 2.09. Furthermore, users can tune the memory available to SCALCE through a parameter to improve the run time when a large main memory is available. In our tests, we limited the memory usage to 6 GB. Note that our goal here is to devise a fast boosting method, SCALCE, which in combination with gzip gives compression<ref type="figure">Fig. 1</ref>. Original (left) and transformed (right) quality scores for two random reads that are chosen from NA18507 individual. The original scores show much variance, where the transformed quality scores are smoothened except for the peaks at local maxima, that help to improve the compression ratiorates much better than gzip alone. It is possible to get better compression rates through mapping-based strategies, but these methods are several orders of magnitude slower than SCALCE þ gzip. We tested the effects of the lossy compression schemes for the quality scores, used by SCALCE as well as CRAM tools, to single nucleotide polymorphism (SNP) discovery. For that, we first mapped the NA18507 WGS dataset with the original quality values to the human reference genome (GRCh37) using the BWA aligner (<ref type="bibr" target="#b21">Li and Durbin, 2009</ref>), and called SNPs using the GATK software (<ref type="bibr" target="#b11">DePristo et al., 2011</ref>). We repeated the same exercise with the reads after 30% lossy transformation of the base pair qualities with SCALCE. Note that the parameters for BWA and GATK we used in these experiments were exactly the same. We observed almost perfect correspondence between two experiments. In fact, 499:95% of the discovered SNPs were the same (<ref type="figure" target="#tab_4">Table 4</ref>); not surprisingly, most of the difference was due to SNPs in mapping to common repeats or segmental duplications. We then compared the differences of both SNP callsets with dbSNP Release 132 (<ref type="bibr" target="#b26">Sherry et al., 2001</ref>) in<ref type="figure" target="#tab_4">Table 4</ref>. In addition, we carried out the same experiment with compressing/decompressing of the alignments with CRAM tools. As shown in<ref type="figure" target="#tab_4">Table 4</ref>, quality transformation of the CRAM tools introduced about 2.5% errors in SNP calling (97.5% accuracy) with respect to the calls made for the original data (set as the gold standard). One interesting observation is that 70.7% of the new calls after SCALCE processing matched to entries in dbSNP where this ratio was only 62.75% for the new calls after CRAM tools quality transformation. Moreover, 57.95% of the SNPs that SCALCE 'lost' are found in dbSNP, and CRAM tools processing caused removal of 18.4 times more potentially real SNPs than SCALCE.</p><p>As a final benchmark, we compared the performance of SCALCE with mapping-based reordering before gzip compression. We first mapped one lane of sequence data from the genome of NA18507 (same as above) to human reference genome (GRCh37) using BWA (<ref type="bibr" target="#b21">Li and Durbin, 2009</ref>), and sorted the mapped reads using samtools (<ref type="bibr" target="#b21">Li et al., 2009</ref>), and reconverted the map-sorted BAM file back to FASTQ using<ref type="bibr" target="#b9">Cox et al., 2012</ref>) combined with bzip2 by a factor between 1.09 and 2.07, where running time is improved by a factor between 3.60 and 5.17 (see<ref type="figure" target="#tab_5">Table 5</ref>). SCALCE (on full FASTQ files) also outperforms DSRC (<ref type="bibr" target="#b10">Deorowicz and Grabowski, 2011</ref>) compression ratio on complete FASTQ files by a factor between 1.09 and 1.18 (see<ref type="figure">Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION AND DISCUSSION</head><p>The rate of increase in the amount of data produced by the HTS technologies is now faster than the Moore's Law (<ref type="bibr" target="#b4">Alkan et al., 2011</ref>). This causes problems related to both data storage and transfer of data over a network. Traditional compression tools such as gzip and bzip2 are not optimized for efficiently reducing the files to manageable sizes in short amount of time. To address this issue, several compression techniques have been developed with different strengths and limitations. For example, pairwise comparison of sequences can be used to increase similarity within 'chunks' of data, thus increasing compression ratio (<ref type="bibr" target="#b29">Yanovsky, 2011)</ref>, but this approach is also time consuming. Alternatively, reference-based methods can be used such as SlimGene (<ref type="bibr" target="#b20">Kozanitis et al., 2010</ref>) and CRAM tools (Hsi-Yang<ref type="bibr" target="#b17">Fritz et al., 2011</ref>). Although these algorithms achieve high compression rates, they have three major shortcomings. First, they require pre-mapped (and sorted) reads along with a reference genome, and this mapping stage can take a long time depending on the size of the reference genome. Second, speed and compression ratio are highly dependent on the mapping ratio because the unmapped reads are handled in a more costly manner (or completely discarded), which reduces the efficiency for genomes with high novel sequence insertions and organisms with incomplete reference genomes. Finally, the requirement of a reference sequence makes them unusable for de novo sequencing projects of the genomes of organisms where no such reference is available, for example, the Genome 10K Project (<ref type="bibr" target="#b16">Haussler et al., 2009</ref>). The SCALCE algorithm provides a new and efficient way of reordering reads generated by the HTS platform to improve not only compression rate but also compression run time. Although it is not explored here, SCALCE can also be built into specialized alignment algorithms to improve mapping speed. We note that the names associated with each read do not have any specific information and they can be discarded during compression. The only consideration here is that during decompression, new read names will need to be generated. These names need to be unique identifiers within a sequencing experiment, and the paired-end information must be easy to track. In fact, the Sequence Read Archive developed by the International Nucleotide Sequence Database Collaboration adopts this approach to minimize the stored metadata, together with a lossy transformation of the base pair quality values similar to our approach (<ref type="bibr" target="#b19">Kodama et al., 2011</ref>). However, in this article, we demonstrated that lossy compression of quality affects the analysis result, and although the difference is small for SCALCE, this is an optional parameter in our implementation, and we leave the decision to the user. Additional improvements in compression efficiency and speed may help ameliorate the data storage and management problems associated with HTS (<ref type="bibr" target="#b25">Schadt et al., 2010</ref>).Here, the datasets contained only reads from the FASTQ file, as BEETL supports only FASTA file format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.Hach et al.</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>20 30 40 50 60 70 80 90 100 Error probability basepair R2 Lossy 30% 0.0001 0.001 0 10 20 30 40 50 60 70 80 90 100 Error probability basepair R2 Original</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F.</head><figDesc>Hach et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. Input data statistics and compression rates achieved by gzip only and SCALCE þ gzip on reads from the P.aeruginosa RNA-Seq library (dataset 1)</figDesc><table>Dataset 
gzip 
SCALCE þ gzip 

Number of reads 
Size 
Size 
Rate 
Time 
Size 
Rate 
Boosting factor 
gzip only time 
SCALCE þ gzip time 

89M 
4327 
1071 
4.04 
13 min 18 s 
256 
16.92 
4.19Â 
53 s 
6 min 21 s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. Input data statistics and compression rates achieved by gzip only and SCALCE þ gzip þ AC on complete FASTQ files</figDesc><table>Dataset 
gzip 
SCALCE (lossless) 
SCALCE (lossy 30%) 

Name 
Number of reads 
Size 
Size 
Rate 
Size 
Rate 
Boosting factor 
Size 
Rate 
Boosting factor 

P.aeruginosa RNAseq 
89M 
10 076 
3183 
3.17 
1496 
6.74 
2.13Â 
953 
10.58 
3.34Â 
P.aeruginosa genomic 
81M 
9163 
3211 
2.85 
1655 
5.54 
1.94Â 
1126 
8.14 
2.85Â 
NA18507 WGS 
1.4B 
300 337 
113 132 
2.65 
76 890 
3.91 
1.47Â 
58 031 
5.18 
1.95Â 
NA18507 single lane 
36M 
7708 
3058 
2.52 
2146 
3.59 
1.42Â 
1639 
4.70 
1.86Â 

File sizes are reported in megabytes. M, million; B, billion. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 4.</figDesc><table>Number of SNPs found in the NA18507 genome using original 
and transformed qualities with 30% noise reduction and qualities recon-
structed by CRAM tools 

Experiment 
settings 

Number of 
SNP count 

dbSNP 
v132 (%) 

Novel 

Total 
In SD þ CR (%) 

Original 
qualities 

4 296 152 
4 092 923 (95.26) 203 229 192 114 (94.53) 

Qualities 
using 
SCALCE 

4 303 140 
4 098 875 (95.25) 204 265 192 976 (94.47) 

Lost 
7931 
4596 (57.95) 
3335 
2963 (88.84) 
New 
14 919 
10 548 (70.70) 
4371 
3825 (87.51) 
Qualities 
using 
CRAM 
tools 

4 202 298 
4 013 401 (95.50) 188 897 179 875 (95.22) 

Lost 
101 957 
84 607 (82.98) 17 350 15 036 (86.66) 
New 
8103 
5085 (62.75) 
3018 
2797 (92.67) 

Also reported are the number and percentage of novel SNPs in regions of segmental 
duplication or common repeats (SD þ CR). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 3.</figDesc><table>Run time for running gzip alone and SCALCE þ gzip þ AC on complete FASTQ files 

Name 
gzip 
SCALCE þ gzip þ AC, single thread 
SCALCE þ gzip þ AC, 3 threads 

Time 
Reordering 
gzip þ AC 
Total compression 
Total compression 

P.aeruginosa RNAseq (min) 
20 
7 
6 
13 
9 
P.aeruginosa genomic (min) 
20 
6 
5 
11 
9 
NA18507 WGS 
10 h 52 min 
3 h 
3 h 1 min 
6 h 1 min 
4 h 28 min 
NA18507 single lane 
18 min 
5 min 
5 min 
10 min 
7 min 32 s 

SCALCE </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 6. Comparison of single-threaded SCALCE with DSRC</figDesc><table>Name 
DSRC 
time 

DSRC 
size 

SCALCE 
time 

SCALCE 
size 

P.aeruginosa RNAseq 12 min 
1767 
13 min 
1496 
P.aeruginosa genomic 
6 min 
1846 
11 min 
1655 
NA18507 WGS a 
3 h 16 min 94 707 
6 h 1 min 
76 890 
NA18507 single lane 
4 min 
2341 
10 min 
2146 

DSRC was tested using the -l option. This option provides better compression ratio 
but it is slower. 

a 

DSRC with -l option crashed on WGS dataset. Instead we used a faster but less 
powerful settings for this dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 5.</figDesc><table>Comparison of single-threaded SCALCE with BEETL 

Name 
BEETL 
time (min) 

BEETL 
size 

SCALCE 
time (min) 

SCALCE 
size 

P.aeruginosa RNAseq 29 
197 
8 
95 
P.aeruginosa Genomic 31 
257 
6 
137 
NA18507 single lane 
51 
448 
10 
412 

</table></figure>

			<note place="foot">ß The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">F.Hach et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Combating Infectious Diseases Project (BCID to S.C.S. in parts</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title level="m" type="main">Michael Smith Foundation for Health Research grants (to S.C.S. in parts); Canadian Research Chairs Program; and an NIH grant HG006004 to C.A. Conflict of Interest</title>
		<imprint/>
	</monogr>
	<note>none. declared</note>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">REFERENCES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient string matching: an aid to bibliographic search</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">V</forename>
				<surname>Aho</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Corasick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="333" to="340" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Genome structural variation discovery and genotyping</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Alkan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Oblivious string embeddings and edit distance approximations</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Batu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA. pp</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="792" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">No-reference compression of genomic data stored in fastq format</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Bhola</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIBM. pp</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="147" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<monogr>
		<title level="m" type="main">A block-sorting lossless data compression algorithm</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Burrows</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J</forename>
				<surname>Wheeler</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Palo Alto, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Communication complexity of document exchange</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Cormode</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA. pp</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale compression of genomic sequence databases with the burrows-wheeler transform</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Cox</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1415" to="1419" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Compression of DNA sequence reads in fastq format</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Grabowski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="860" to="862" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">A framework for variation discovery and genotyping using next-generation DNA sequencing data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Depristo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="491" to="498" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Base-calling of automated sequencer traces using phred. II. Error probabilities</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Ewing</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Green</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="186" to="194" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Compression boosting in optimal linear time using the burrows-wheeler transform</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ferragina</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Manzini</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA. pp</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="655" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Boosting textual compression in optimal linear time</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ferragina</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="688" to="713" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">The engineering of a compression boosting library: theory vs practice in bwt compression</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ferragina</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESA</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="756" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Genome 10K: a proposal to obtain whole-genome sequence for 10,000 vertebrate species</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Haussler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Hered</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="659" to="674" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient storage of high throughput DNA sequencing data using reference-based compression</title>
		<author>
			<persName>
				<forename type="first">Hsi-Yang</forename>
				<surname>Fritz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="734" to="740" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">A Method for the Construction of Minimum-Redundancy Codes</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Huffman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<publisher>IEEE Journals</publisher>
			<date type="published" when="1952" />
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">The sequence read archive: explosive growth of sequencing data</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kodama</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="54" to="56" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<monogr>
		<title level="m" type="main">Compressing genomic sequence fragments using SlimGene</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Kozanitis</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="310" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and accurate short read alignment with Burrows-Wheeler transform</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Durbin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1754" to="1760" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">The sequence alignment/map format and SAMtools</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2078" to="2079" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Symmetry breaking for suffix tree construction</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C</forename>
				<surname>Sahinalp</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Vishkin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC. pp</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient approximate and dynamic matching of patterns using a labeling paradigm</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C</forename>
				<surname>Sahinalp</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Vishkin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="320" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Computational solutions to large-scale data management and analysis</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">E</forename>
				<surname>Schadt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="647" to="657" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">dbSNP: the NCBI database of genetic variation</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">T</forename>
				<surname>Sherry</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="308" to="311" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">G-sqz: compact encoding of genomic sequence and quality data</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Tembe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2192" to="2194" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Transformations for the compression of fastq quality scores of next-generation sequencing data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Wan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">ReCoil—an Algorithm for compression of extremely large datasets of DNA data</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Yanovsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">A universal algorithm for sequential data compression</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ziv</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Lempel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="337" to="343" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Compression of individual sequences via variable-rate coding</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ziv</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Lempel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Inf Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="530" to="536" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">SCALCE</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>