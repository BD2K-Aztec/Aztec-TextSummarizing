
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Event trigger identification for biomedical events extraction using domain knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Deyu</forename>
								<surname>Zhou</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country>China,</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Dayou</forename>
								<surname>Zhong</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country>China,</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Yulan</forename>
								<surname>He</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">Aston University</orgName>
								<address>
									<postCode>B4 7ET</postCode>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Event trigger identification for biomedical events extraction using domain knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="issue">11</biblScope>
							<biblScope unit="page" from="1587" to="1594"/>
							<date type="published" when="2014">2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu061</idno>
					<note type="submission">Received on October 1, 2013; revised on January 23, 2014; accepted on January 24, 2014</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Jonathan Wren Contact: d.zhou@seu.edu.cn Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: In molecular biology, molecular events describe observable alterations of biomolecules, such as binding of proteins or RNA production. These events might be responsible for drug reactions or development of certain diseases. As such, biomedical event extraction , the process of automatically detecting description of molecular interactions in research articles, attracted substantial research interest recently. Event trigger identification, detecting the words describing the event types, is a crucial and prerequisite step in the pipeline process of biomedical event extraction. Taking the event types as classes, event trigger identification can be viewed as a classification task. For each word in a sentence, a trained classifier predicts whether the word corresponds to an event type and which event type based on the context features. Therefore, a well-designed feature set with a good level of discrimination and generalization is crucial for the performance of event trigger identification. Results: In this article, we propose a novel framework for event trigger identification. In particular, we learn biomedical domain knowledge from a large text corpus built from Medline and embed it into word features using neural language modeling. The embedded features are then combined with the syntactic and semantic context features using the multiple kernel learning method. The combined feature set is used for training the event trigger classifier. Experimental results on the golden standard corpus show that 42.5% improvement on F-score is achieved by the proposed framework when compared with the state-of-the-art approach, demonstrating the effectiveness of the proposed framework. Availability and implementation: The source code for the proposed framework is freely available and can be downloaded at https://github.com/dphansti/mango.
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In molecular biology, molecular events describe observable alterations of biomolecules, such as binding of proteins or RNA production. These molecular events influence the formation of a phenotype, which may be responsible for drug reactions or development of certain diseases. However, knowledge about these events is scattered in the scientific literature with continuing fast growth. Tremendous systematic and automated efforts are required to use the underlying information. As such, biomedical event extraction attracted much research interest recently. Several evaluation tasks, such as BioNLP'09 (<ref type="bibr" target="#b6">Kim et al., 2009</ref>), BioNLP'11 (<ref type="bibr" target="#b7">Kim et al., 2012</ref>) and BioNLP'13 (<ref type="bibr" target="#b9">Ne´dellecNe´dellec et al., 2013</ref>) shared tasks, have been held in recent years to allow researchers to develop and compare their methods for biomedical events extraction. In general, each biomedical event consists of a trigger and one or more arguments. For example, '.. . inhibiting tyrosine phosphorylation of STAT6. . .' describes two events, one is the phosphorylation event and the other is the negative regulation event, which is signaled by the word 'inhibiting' and takes the first phosphorylation event as its argument. In a typical biomedical event annotation, these two events are represented as follows: E1 (Event Type:Phosphorylation, Theme:STAT6, ToLoc: tyrosine) E2 (Event Type: Negative_regulation:inhibiting Theme:E1)</p><p>Biomedical event extraction aims to extract such event information from biomedical literature and reformats this extracted information in structures as represented by the two annotations presented above. By extracting detailed behaviors of biomolecules, biomedical event extraction can be used to support the development of biomedical-related databases. To extract events from texts, most systems rely on a pipeline procedure, which usually consists of three cascaded modules including biomedical term identification, event trigger identification and event argument detection (<ref type="bibr" target="#b13">Zhou and He, 2011</ref>). In such pipeline-based approaches, it is crucial to identify event triggers reliably, as errors in an early stage will be propagated and hurt the performance of the subsequent module. Analysis on the event extraction results show that460% of extraction errors are attributed to the errors of event trigger identification (<ref type="bibr" target="#b10">Pyysalo et al., 2012</ref>). To achieve a better performance, existing approaches to event trigger identification are mostly based on learning classifiers from annotated data instead of using manually constructed dictionaries containing a list of trigger words or manually defined linguistic rules. In such approaches, event types are treated as classes, and the aim is to classify words in sentences as indicating a particular event type or not by taking the context features including the syntactic and semantic features into account. *To whom correspondence should be addressed.However, such approaches rely on abundant annotated training data and may not work well when certain event instances are rare in the training data. For example, the word 'proteolysis' does not occur as an event trigger for catabolism type in the training data of the multi-level event extraction (MLEE) corpus (<ref type="bibr" target="#b10">Pyysalo et al., 2012</ref>). Therefore, it is difficult to recognize it as an event trigger in the sentence 'The effects of IGF-1 are mediated principally through the IGF-1R but are modulated by complex interactions with multiple IGF binding proteins that themselves are regulated by phosphorylation, proteolysis, polymerization, and cell or matrix association', which appears in the test set. Nevertheless, we notice that another word 'hydrolysis' was annotated as an event trigger in the training data as shown in<ref type="figure" target="#tab_1">Table 1</ref>. If we search through Medline (http://www.ncbi.nlm.nih. gov/entrez/query/static/overview.html), we can find that the two words 'proteolysis' and 'hydrolysis' occur in similar context thus tend to have similar meanings following the distributional hypothesis (<ref type="bibr" target="#b4">Harris, 1970</ref>). Examples of the similar context where 'proteolysis' and 'hydrolysis' occur in Medline are presented in<ref type="figure" target="#tab_2">Table 2</ref>. If we can learn such domain knowledge and incorporate it into trigger word identification, then 'proteolysis' might be correctly identified as an event trigger even if it did not appear in the training data at all. In this article, we argue that biomedical domain knowledge, such as words, tends to occur in similar context, is highly related and this can be incorporated into the learning process of the event trigger classifier to improve the performance of trigger word identification. In specific, we propose a novel framework to learn biomedical knowledge from a large text corpus built from Medline and embed it into word features using neural language modeling. The embedded features are further combined with the well-designed syntactic and semantic context features using the multiple kernel learning (MKL) method for classifier training. We conducted extensive experiments on the MLEE corpus (<ref type="bibr" target="#b10">Pyysalo et al., 2012</ref>), and the results show that 42.5% improvement on F-score is achieved using the proposed framework when compared with the state-of-the-art approach, demonstrating the effectiveness of the proposed framework. The rest of the article is organized as follows. Section 2 presents the proposed framework, which consists of three steps, domain knowledge embedding, local context features extraction and MKL. Experimental setup and results are discussed in Section 3. Finally, Section 4 concludes the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ß</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OUR APPROACH</head><p>Our proposed framework for event trigger identification works as follows, which is illustrated in<ref type="figure" target="#fig_0">Figure 1</ref>. First, scientific publications from Medline are crawled to form a corpus where domain knowledge can be obtained. Then a neural language model is built from such a corpus using unsupervised learning. The distributional representation for each word is induced as the feature of the word (word embedding). Then, for sentences in the training and testing datasets, protein name identification, syntactic parsing and dependency parsing are performed and local context features are extracted from the parsing results. After that, features induced by neural language model and features extracted from syntactic and dependency parsing results are combined through MKL. Finally, training and testing are conducted on the combined feature set. In what follows, we first describe how to formulate the task of event trigger identification as a classification problem, in which two sets of features, domain knowledge embedding and local context features, are used. Then, we present how to learn the parameters of our proposed unified classification framework using MKL. Finally, we discuss how the two feature sets can be constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem definition</head><p>Event trigger identification in the biomedical domain can be seen as the task of assigning labels to words. Existing approaches for event trigger identification typically rely on annotated training data where those event trigger words are labeled with their corresponding event types. A rich set of manually designed features are then extracted from annotated sentences and fed into a classification algorithm such as support vector machines (SVMs) for training. In our approach here, we adopt a similar procedure of training a classifier from annotated data for trigger word identification. However, apart from the annotated training data, we additionally crawled articles from Medline to form a corpus where domain knowledge can be extracted. Given sentences S ¼ fs i : w i1 w i2 :::w ini , i ¼ 1:::Lg, their corresponding trigger annotations T ¼ ft i : a i1 a i2 :::a ini , i ¼ 1:::Lg and an additional unannotated corpus where domain knowledge can be extracted, S u ¼ fs i : w i1 w i2 :::w ini , i ¼ ðL þ 1Þ, ðL þ 2Þ::: ðL þ ULÞg where L and UL are the numbers of sentences in the training data and the domain corpus, respectively, the objective is to estimate a hypothesis f : S°T minimizing the prediction error on unseen data. For traditional machine learningWith the ease of resistance to proteolysis, the development of sequence-specific AApeptides. . .<ref type="figure" target="#tab_1">Table 1</ref>. The sentences in the MLEE Corpus in which 'hydrolysis' was annotated as an event trigger Sentences approaches, f is determined by minimizing the loss between the prediction fðÈðwÞÞ for the training instance w and its actual label a w based on some loss function Loss. Here ÈðwÞ is the feature set related to w. As will be shown in Section 2.4, local context around w is used for constructing ÈðwÞ. Moreover, to make sure that words occurring in similar contexts share the same class label, the loss between the prediction f 2 ðÉðw 0 ÞÞ of w 0 and a w is also minimized, where w 0 is the word that is found to have highest contextual similarity with w from in the domain corpus and Éðw 0 Þ is another type of feature set related to w 0. As will be shown in Section 2.3, contextually similar words can be modeled using neural language modeling. Because w 0 is the word with the highest contextual similarity with w, Éðw 0 Þ can be approximated as ÉðwÞ. Our final objective function is</p><formula>^ f ¼ argmin f¼ð f1, f2Þ2Â X w ðLossð f 1 ðÈðwÞÞ, a w Þ þ rLossð f 2 ðÉðwÞÞ, a w ÞÞ ð1Þ</formula><p>where r is a parameter controlling the trade-off between two losses. When r ¼ 0, Equation (1) reduces to the object function for classification based on local context features only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parameter learning</head><p>In our framework, we use the local context features derived from the annotated training data for È and use word embeddings induced from the domain corpus using neural language modeling for É. We use SVM for both f 1 and f 2 and f 1 ¼ hw 1 , ÈðwÞi þ b 1 and f 2 ¼ hw 2 , ÉðwÞi þ b 2. Therefore, the above problem can then be solved by optimizing the parameters of w 1 , w 2 , b 1 , b 2 , r. However, these parameters cannot be optimized directly using the general learning approach for SVM. By considering parameters optimization as learning the optimal weights of different types of features from the data automatically, the problem is converted into to feature combination. Under kernel learning, feature combination is translated into kernel combination by defining two kernels K 1 , K 2 based on ÈðwÞ, ÉðwÞ. There are many possible ways for kernel combination. A simplest one is to average several kernels by setting r ¼ 1. In our work here, we use MKL (<ref type="bibr" target="#b0">Bach et al., 2004</ref>), which has been shown to produce good results in object classification in computer vision (<ref type="bibr" target="#b3">Gehler and Nowozin, 2009</ref>). The aim of MKL is to learn a kernel combination during the training phase of the algorithm by optimizing jointly over a linear combination of kernels P m i¼1 i K i ðw, w 0 Þ and the parameters of an SVM, where m is the number of kernels to be combined. Under MKL, the object function described in Equation (1) is changed to</p><formula>min , , b 1 2 X 2 i¼1 i T K i þ C X N j¼1 Lða wj , b þ X 2</formula><p>Here, 1 and 2 are the weighting of two features set. The problem can be solved using the SimpleMKL (<ref type="bibr" target="#b11">Rakotomamonjy et al., 2008</ref>) Toolbox (http://asi.insa-rouen.fr/enseignants/*arakotom/ code/mklindex.html). The decision function is of the following form,</p><formula>signð X 2 i¼1 i ðK i ðxÞ T þ bÞÞ ð4Þ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word embeddings learned by neural language modeling</head><p>We use neural language modeling (<ref type="bibr" target="#b5">Huang et al., 2012</ref>) to learn word representations by discriminating the next word given its local context and global context. Given a word sequence s i ¼ ðw i1 , w i2 , :::, w in Þ and a document d j ¼ ðw j1 , w j2 , :::, w jm Þ, which contain s i , the goal of the model is to discriminate the w in (the correct one) from a random word w. Thus, the object function of the model is to minimize the ranking loss for each</p><formula>ðs i , d j Þ: X i X j X w2Vnwin maxð0, 1 À fðs i , d j Þ þ fðs w i , d j Þ, ð5Þ</formula><p>where s w i ¼ w i1 , w i2 , :::, w i, nÀ1 , w is the sequence by changing the last word w in into w. The dataset for learning the language model can be constructed by considering all the word sequences in the Medline corpus. Positive examples are the word sequences fromMedline, whereas negative examples are the same word sequence with the last word replaced by a random one. Instead of using only local context for language model learning, document context (or global context) is also considered. Thus, the score function fðs i , d j Þ is replaced by two functions, score l ðs i , d j Þ and score g ðs i , d j Þ, which are defined to capture local context and global context, respectively. The score function of local context score l ðs i , d j Þ is calculated by a neural network with one hidden layer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><formula>a l 1 ¼ gðW l 1 X l þ b l 1 Þ ð 6Þ score l ¼ W l 2 a l 1 þ b l 2 ð7Þ where X l ¼ ½x 1 , x 2</formula><p>, :::, x n  is the concatenation of the n word embeddings representing sequence s i , g is an element-wise activation function such as tanh, a l 1 is the activation of the hidden layer with h l hidden nodes, W l 1 and W l 2 are the first and second layer weights of the neural network, respectively, and b l 1 , b l 2 are the biases of each layer. The score function of global context score g ðs i , d j Þ is calculated by a two-layer neural network:</p><formula>a g 1 ¼ hðW g 1 X g þ b g 1 Þ ð 8Þ score g ¼ W g 2 a g 1 þ b g 2 ð9Þ where X g ¼ ½ P m t¼1 ðw jt ÞX t P m t¼1 ðw jt Þ , x n </formula><p>which is the weighted average of all word vectors in the document d j , is a weighting function describing the importance of word w jt in the document d j , h is an element-wise activation function such as tanh, a g 1 2 R h g Â1 is the activation of the hidden layer with h g hidden nodes, W g 1 and W g 2 are the first and second layer weights of the neural network, respectively, and b g 1 , b g 2 are the biases of each layer. The local score preserves word order and syntactic information, whereas the global score uses a weighted average that is similar to bag-of-words features, capturing more of the semantics and topics of the document. The gradient of the objective is sampled by randomly choosing a word from the vocabulary as a corrupted example for each sequence–document pair ðs i , d j Þ. The derivative of the ranking loss is taken with respect to the parameters and these weights are updated via backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Local contexts features</head><p>The syntactic and semantic features used in the framework are generated from the outputs of GDep (a dependency parser) (<ref type="bibr" target="#b12">Sagae and Tsujii, 2007</ref>) and Enju parser (a syntactic parser) (<ref type="bibr" target="#b8">Miyao and Tsujii, 2008</ref>). All the features used in the framework are extracted based on (<ref type="bibr" target="#b10">Pyysalo et al., 2012</ref>), described as follows:</p><p>Lexical and syntactic features of the word itself. The features such as whether the word has a capital letter, whether it is at the beginning of the sentences, whether it has a number, whether it has a symbol, whether it is in a trigger word dictionary, whether it is in a protein base form, its POS tag and n-grams of characters (n ¼ 2, 3, 4) are extracted. For features like whether it has certain property, boolean value is used for the feature value. In addition, to check whether a word is in the trigger word dictionary, we constructed a dictionary by collecting all the trigger words from the training set. Triggers that contain more than one word are filtered. Also, hyphenated compound words are added into the dictionary if one of its words already appears in the trigger word dictionary. Local context features. For the sequence of three words before or after the candidate word, n-grams (n ¼ 1, 2, 3, 4) are used. For example, for the word 'retarget' in the sentence 'The binding of I kappa B/MAD-3 to NF-kappa B p65 is sufficient to retarget NF-kappa B p65 from the nucleus to the cytoplasm', the word sequence 'is sufficient to retarget protein from the' is used to generate the relevant n-grams. Also, each word is represented by its base form, the POS tag and the relative position (before or after) to the target word. Local dependency features. The two-depth path started from the candidate word in the dependency tree generated from the GDep parser is identified first. Features are then extracted from the path such as n-grams (n ¼ 2) of dependencies, n-grams (n ¼ 2, 3) of words represented by their base forms and the POS tags and n-grams (n ¼ 2, 3, 4) of dependencies and words. For word tokens not having two-depth paths, such as the root node or the direct children of the root node, these types of features are ignored. N-grams (n ¼ 2) of dependencies are represented as dependency1–dependency2. Similarly, n-grams (n ¼ 2, 3) of words or n-grams (n ¼ 2, 3, 4) of dependencies and words are represented as word1–word2–word3 or word1–dependency1–word2 and so on. For example, for the word 'retarget' in the sentence 'the binding of I kappa B/MAD-3 to NF-kappa B p65 is sufficient to retarget NF-kappa B p65 from the nucleus to the cytoplasm.', its two-depth path 'retarget ! AMOD ! sufficient ! PRD ! is' can be retrieved from the GDep parsing results. Its n-grams (n ¼ 2) of dependencies are given as 'AMOD PRD'. Shortest path features. The shortest path, a directed path between the candidate and the closest protein, is also retrieved from the dependency parse generated from GDep parser. The vertex walks, edge walks, n-grams (n ¼ 2, 3, 4) of dependencies, n-grams (n ¼ 2, 3, 4) of words represented as base forms plus POS tags and the length of path are extracted as the path features. For example, for the word 'retarget' in the sentence 'The binding of I kappa B/MAD3 to NF-kappa B p65 is sufficient to retarget NF-kappa B p65 from the nucleus to the cytoplasm.', its shortest path is 'retarget OBJ protein'. The length of path that is 1, edge walks as retarget OBJ protein, vertex walks as OBJ can be extracted. The reason of using shortest path is that a candidate and its closest proteins are much more likely to be involved in a biomedical event. Thus, features extracted from the shortest path should be useful for detecting triggers in biomedical event extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we present our experiments to evaluate the effectiveness of the proposed framework. We will first discuss results obtained on event trigger identification in comparison with the best performance obtained so far. We will then present performance comparison results with or without using the MKL method for comparison, followed by the results of using neural language models trained under different corpora. Finally, we compare our results with those obtained using the latent Dirichlet allocation (LDA) model as another distributional semantics approach instead of neural language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>We used MLEE corpus for our experiments on trigger words identification. Instead of focusing exclusively on molecularlevel entities and process, MLEE corpus is extended to encompass all levels of biological organization from the molecular to the whole organism. The corpus is generated from 262 PubMed abstracts on angiogenesis, which involves a tissue/organ-level process closely associated with cancer and other organism-level pathologies. Texts in that domain represent a good test case for event extraction across multiple levels of biological organization. The annotation follows the guideline formalized in the BioNLP 2009 Shared Task on event extraction. In this guideline, events are n-ary associations of participants (entities or other events) with specific role such as theme and cause. Each event is assigned a type from a fixed set defined for the task (e.g. binding and phosphorylation) and is associated with a specific span of text stating the event, termed the event trigger. The events are categorized as four groups such as 'ANATOMICAL', 'MOLECULAR', 'GENERAL' and 'PLANNED', which are further classified into 19 classes. These 19 classes are the target classes of our trigger word classifier. It is worth noting that we used a combination of training and development datasets of the MLEE corpus for training, and the test set for testing. To train a neural language model, we additionally built a corpus from Medline because of its wide coverage of topics in the biomedical domain. Abstracts of biomedical literature published in 2011 and 2012 were retrieved to build the corpus. All the sentences in the Medline corpus were preprocessed such as lowercasing and stemming. We chose the most frequent words in the corpus to construct vocabularies with different size D ¼ f15, 000, 30, 000, 60, 000, 90, 000g. Words starting with a digital number are mapped to the 'NUMBER' token. Words starting with a special character are mapped to the 'UNUSUAL' token. Other rare words not in the dictionary are replaced with the 'UNKNOWN' token. For neural language model training, we used 50 dimensional embeddings and set the number of hidden units to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental results</head><p>This section presents the evaluation results in details. In our framework, the one-versus-rest SVMs are used for trigger word classification. To alleviate the unbalanced classification problem, we boosted the positive examples by placing more weights on them during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Event trigger identification results</head><p>We implemented a baseline following the approach proposed in (<ref type="bibr" target="#b10">Pyysalo et al., 2012</ref>), which achieved the state-of-the-art performance on trigger word identification using the features extracted from the syntactic and semantic parsing results as described in Section 2.4. We conducted experiments on the MLEE corpus and compared our framework with the baseline approach.<ref type="figure" target="#tab_3">Table 3</ref>lists the recall, precision and F-score obtained on the test set of the MLEE corpus. In the results reported here, we trained a neural language model on the Medline corpus with the vocabulary size of 30 000. Using the features induced from the learned neural language model, the performance of event trigger identification is improved significantly with $5% on precision. The overall improvement on F-score is $2.5%. To further investigate how the improvement is achieved, we analyzed the experimental results of the baseline approach and the proposed framework. We found that positive instances identified correctly by the baseline approach are still identified correctly by the proposed framework in 97.8% of cases. Out of the false-negative instances identified by the baseline, 7.8% were correctly identified as positive instances by our framework. To further study the difference of our proposed framework against the existing state-of-the-art approach in different event categories, we list the detailed results in each event category in<ref type="figure" target="#tab_4">Table 4</ref>. It can be observed from the table that of 19 event types, our proposed framework outperforms the baseline approach on 13 event types and gives almost identical results on another 5 event types. To investigate the performance improvement under different event types, we analyze the relationship between performance improvement and the size of the training data in each event category. The results are illustrated in<ref type="figure" target="#fig_1">Figure 2</ref>. It can be observed that the performance improvement decreases when the size of the training data increases. The largest improvement (100%) is achieved in the 'dephosphorylation' event type when there are only five training instances. Our approach successfully identified the 'dephosphorylation' event triggers in all three instances in the test set, while the baseline approach failed to identify any of them. When the training data are relatively abundant, our approach appears to have less improvement compared with the baseline. From the above observations, we can speculate that our proposed framework with domain knowledge incorporated is particularly effective when facing with scarce training data. The only exception is the 'transcription' event type with 30 training instances for which the baseline identified one event trigger correctly from the test set, while our approach failed to recognize any. It is shown as negative performance improvement in<ref type="figure" target="#fig_1">Figure 2</ref>. One possible reason is that words contextually similar to 'transcription' are not annotated in the training set either.Event trigger identification using domain knowledge 3.2.2 Comparison of feature combination methods To investigate the effectiveness of the feature combination method based on MKL, experiments were conducted where MKL is replaced with a simple averaging method. In the average method, features induced by neural language model and features extracted from syntactic and semantic parsing results are combined with equal weights.<ref type="figure" target="#tab_5">Table 5</ref>shows the comparison result of the two methods. It can be observed that the precision of event trigger identification is improved by 43% when using MKL for feature combination. Nevertheless, its recall value slightly dropped. The overall improvement on F-score is $1.2%. Although the improvement appears to be marginal, the MKL method should still be favored for feature combination when precision value could well be regarded as much more important than recall in the open biomedical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Impact of dictionary size on neural language modeling</head><p>The vocabulary size D in the neural language model is set in advance. If D is too small, some semantically important words might be omitted. On the contrary, if D is too big, some noisy words might be included and it becomes expensive to train the neural language model. To explore whether and how vocabulary size in the neural language model impacts the trigger word identification performance of the proposed framework, four different vocabularies were used in neural language model learning. We first list in<ref type="figure" target="#tab_6">Table 6</ref>the coverage of all the distinct words and the coverage of all the words in our crawled Medline corpus for each vocabulary. It shows that the top most frequent words occur most of the time. For all the vocabularies we experimentedNote: 'B' denotes the baseline approach, 'P' denotes our proposed method in the 'Method' column and the better performance is shown in boldface.here, they cover at least 95% of word occurrences in the whole corpus.<ref type="figure" target="#tab_7">Table 7</ref>lists the results obtained on the test set of the MLEE corpus with different vocabulary size. It can be observed that the final performance of the proposed framework outperforms the baseline approach regardless which vocabulary was used. The relative improvement on F-score ranges between 1.7 and 2.5%. We also observe that increasing the vocabulary size improves the performance with the peak reached at 30 000. Based on the above observation, we can conclude that the choice of the vocabulary can be made by considering its coverage of the words in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Learning neural language model</head><p>from difference source To explore the effectiveness of embedding domain knowledge into language model, we compare the event trigger identification results with neural language model trained on Wikipedia (<ref type="bibr" target="#b2">Collobert et al., 2011</ref>). The Wikipedia corpus contains a wide range of topics in general domains. The results are shown in<ref type="figure" target="#tab_8">Table 8</ref>. Compared with the baseline approach, using the Wikipedia corpus did not appear to improve the performance of event trigger identification. Nevertheless, learning the neural language model from the Medline corpus gives superior performance on event trigger identification than the baseline. Only domain-specific knowledge can be used to improve the performance of event trigger identification.Event trigger identification using domain knowledge</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. The system architecture of our proposed framework for event trigger identification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Performance improvement versus size of training data in each event category</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. Examples of the similar contexts where the two words 'proteolysis' and 'hydrolysis' occur in Medline</figDesc><table>Hydrolysis 
Proteolysis 

An increase of the products of 
casein hydrolysis, the proteose– 
peptone (p–p) fraction and 
minor (m) caseins 

Use of indices of proteolysis of 
caseins such as the proteose– 
peptone, m-casein and PI 

AApeptides are resistant to 
enzymatic hydrolysis 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 3. Comparison of the performance of event trigger identification</figDesc><table>Method 
Recall (%) 
Precision (%) 
F-score (%) 

Baseline 
81.69 
70.79 
75.84 
Proposed 
81.29 
75.56 
78.32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 4.</figDesc><table>Performance comparison of event trigger identification in 
different event types 

Event 
category 

Event type 
Method Recall 
(%) 

Precision 
(%) 

F-score 
(%) 

Anatomical Cell proliferation 
B 
69.77 63.83 
66.67 
P 
67.44 78.38 
72.5 
Development 
B 
83.51 68.07 
75 
P 
81.44 69.30 
74.88 
Blood vessel develop B 
96.33 95.70 
96.01 
P 
97.33 98.65 
97.99 
Growth 
B 
83.93 69.12 
75.81 
P 
83.92 77.05 
80.34 
Death 
B 
94.29 56.90 
70.97 
P 
88.57 72.09 
79.49 
Breakdown 
B 
34.78 80 
48.48 
P 
34.78 80 
48.48 
Remodeling 
B 
60 
85.71 
70.59 
P 
60 
85.71 
70.59 
Molecular Synthesis 
B 
50 
33.33 
40 
P 
5 0 
4 0 
44.44 
Gene expression 
B 
93.94 83.78 
88.57 
P 
92.42 84.72 
88.41 
Transcription 
B 
14.28 25 
18.18 
P 
0 
0 
0 
Catabolism 
B 
0 
0 
0 
P 
33.33 16.67 
22.22 
Phosphorylation 
B 
100 
50 
66.66 
P 
100 
75 
85.71 
Dephosphorylation B 
0 
0 
0 
P 
100 
100 
100 
General 
Localization 
B 
83.46 79.86 
81.62 
P 
85.71 80.85 
83.21 
Binding 
B 
76.36 84 
80 
P 
78.18 81.13 
79.63 
Regulation 
B 
60.37 46.48 
52.52 
P 
53.05 56.49 
54.72 
Positive regulation 
B 
86.73 67.85 
76.14 
P 
86.41 71.58 
78.30 
Negative regulation B 
77.03 74.35 
75.66 
P 
78.83 77.09 
77.95 
Planned 
Planned process 
B 
75 
53.92 
62.73 
P 
75.64 56.46 
64.66 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 5. Event trigger identification results with or without MKL</figDesc><table>Method 
Recall (%) 
Precision (%) 
F-score (%) 

Averaging 
82.89 
72.14 
77.14 
MKL 
81.29 
75.56 
78.32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><figDesc>Table 7. Event trigger identification performance with neural language model with different vocabularies</figDesc><table>Size of vocabulary 
Recall (%) 
Precision (%) 
F-score (%) 

15 000 
82.03 
73.90 
77.75 
30 000 
81.29 
75.56 
78.32 
60 000 
81.29 
75 
78.02 
90 000 
80.60 
74.68 
77.53 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 9. Event trigger identification performance using neural language modeling versus LDA</figDesc><table>Method 
Recall (%) 
Precision (%) 
F-score (%) 

Baseline 
81.69 
70.79 
75.84 
LDA 
81.12 
71.64 
76.08 
NLM 
81.29 
75.56 
78.32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><figDesc>Table 8.</figDesc><table>Event trigger identification performance with neural language 
model trained from difference sources 

Method 
Recall (%) 
Precision (%) 
F-score (%) 

Wikipedia 
82.60 
70.50 
76.07 
Medline 
81.29 
75.56 
78.32 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> Angiostatin inhibits both ATP synthesis and ATP hydrolysis (Moser et al., 2001) and interferes with intracellular pH regulation (Wahl and Grant, 2002; Wahl et al., 2002). 2 Our data suggest that VEGFR2-mediated regulation of endothelial function is dependent on different, but specific, Rab-mediated GTP hydrolysis activity required for endosomal trafficking.</note>

			<note place="foot">D.Zhou et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">i¼1 i K i ðwÞ T Þ ð2Þ where N is the number of training instances, C is a predefined positive trade-off parameter between model simplicity and classification error, typically used in SVMs, ¼ ð 1 , :::, N Þ T is the vector of dual variables corresponding to each separation constraint, K 1 ðwÞ ¼ ðhÈðw 1 Þ, ÈðwÞi, :::, hÈðw N Þ, ÈðwÞiÞ T , K 2 ðwÞ ¼ ðhÉðw 1 Þ, ÉðwÞi, :::, hÉðw N Þ, ÉðwÞiÞ T , K 1 ¼ ðhÈðw i Þ, Èðw j ÞiÞ NÂN , K 2 ¼ ðhÉðw i Þ, Éðw j ÞiÞ NÂN and Lða wj , tÞ ¼ maxð0, 1 À a wj tÞ is the hinge loss. For efficiency and interpretability, the objection function subjects to 1 þ 2 ¼ 1, 1 ! 0, 2 ! 0 ð3Þ</note>

			<note place="foot" n="3">.2.5 Neural language model versus topic model To further investigate the effectiveness of neural language model, we compare the event trigger identification results with word classes induced by the LDA model, which is a generative graphical model originally proposed for topic discovery (Blei et al., 2003). Assuming that each document is represented as an unordered collection of words and characterized by a particular set of topics, disregarding grammar and word order, the LDA model can be used for grouping the words in similar topics in an unsupervised way. Each word in the LDA model is represented as probability distribution over topics, and then combined with the features described in Section 3.2 for training SVM classifiers for event trigger identification. In our experiments, the LDA model by varying the number of topics {50, 100, 150, 200, 250} using the Stanford topic modeling toolbox (http://nlp.stanford.edu/downloads/tmt/tmt-0.4/). The optimal topic number is chosen using the perplexity measure on the 10% held-out set from our Medline corpus. The final event trigger identification results using LDA are reported in Table 9 by setting the topic number to 200. It can be observed that LDA only gives an almost negligible improvement of 0.24% in F-score compared with the baseline and it performs worse than our proposed framework using neural language modeling. Two possible reasons are (i) LDA ignores word ordering in documents, which is important when comparing words occurring in similar semantic context and (ii) it is difficult to choose the proper number of topics (or word classes) that group words into well-separated semantic clusters. On the contrary, our proposed framework is based on neural language modeling, which learns the distributional representation of words without the need of specifying the number of induced word classes. 4 CONCLUSIONS AND FUTURE WORK In this article, we have proposed a novel framework to construct a feature set for learning classifiers for event trigger identification. In particular, biomedical domain knowledge is learned from a large text corpus built from Medline and embedded into word features using neural language modeling. The embedded features are combined with the well-designed syntactic and semantic context features, which is further used for event trigger classifier learning. Experimental results on the MLEE corpus show that 42.5% improvement on F-score is achieved by the proposed framework when compared with the state-ofthe-art feature-based approach, demonstrating the effectiveness of our proposed framework. In future work, we will further investigate the feasibility of our proposed framework on other corpora. Another possible future direction is to incorporate domain-specific prior knowledge into neural language model learning using semi-supervised learning to further improve the performance of event trigger identification.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors thank the anonymous reviewers for their insightful comments. They also thank Dr Makoto Miwa for his suggestions on constructing the baseline system.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple kernel learning, conic duality, and the SMO algorithm</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">R</forename>
				<surname>Bach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Blei</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Collobert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Gehler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Nowozin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title level="m" type="main">Distributional structure In: Papers in Structural and Transformational Linguistics</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Harris</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>Reidel Publishing Company</publisher>
			<biblScope unit="page" from="775" to="794" />
			<pubPlace>Dordrecht, Holland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">H</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of bionlp&apos;09 shared task on event extraction</title>
		<author>
			<persName>
				<forename type="first">J.-D</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP. NJ</title>
		<meeting>the Workshop on BioNLP. NJ</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">The genia event and protein coreference tasks of the bionlp shared task 2011</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">D</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature forest models for probabilistic hpsg parsing</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Miyao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Tsujii</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="35" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of bionlp shared task 2013</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ne´dellecne´dellec</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2013 Workshop. Bulgaria</title>
		<meeting>the BioNLP Shared Task 2013 Workshop. Bulgaria<address><addrLine>Sofia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Event extraction across multiple levels of biological organization</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Pyysalo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="575" to="581" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rakotomamonjy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SimpleMLK. J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2491" to="2521" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<monogr>
		<title level="m" type="main">Dependency parsing and domain adaptation with LR models and parser ensembles</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Sagae</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Tsujii</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>EMNLP-CoNLL</publisher>
			<biblScope unit="page" from="1044" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Biomedical events extraction using the hidden vector state model</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>He</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="205" to="213" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>