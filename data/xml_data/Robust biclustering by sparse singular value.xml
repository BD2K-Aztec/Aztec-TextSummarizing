
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gene expression Robust biclustering by sparse singular value decomposition incorporating stability selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Martin</forename>
								<surname>Sill</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Biostatistics</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Sebastian</forename>
								<surname>Kaiser</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Working Group Computational Statistics</orgName>
								<orgName type="laboratory">LMU</orgName>
								<address>
									<postCode>80539</postCode>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Axel</forename>
								<surname>Benner</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Biostatistics</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Annette</forename>
								<surname>Kopp-Schneider</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Biostatistics</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gene expression Robust biclustering by sparse singular value decomposition incorporating stability selection</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">15</biblScope>
							<biblScope unit="page" from="2089" to="2097"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr322</idno>
					<note type="submission">Received on March 1, 2011; revised on April 11, 2011; accepted on May 10, 2011</note>
					<note>[12:14 5/7/2011 Bioinformatics-btr322.tex] BIOINFORMATICS ORIGINAL PAPER Associate Editor: Martin Bishop Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Over the past decade, several biclustering approaches have been published in the field of gene expression data analysis. Despite of huge diversity regarding the mathematical concepts of the different biclustering methods, many of them can be related to the singular value decomposition (SVD). Recently, a sparse SVD approach (SSVD) has been proposed to reveal biclusters in gene expression data. In this article, we propose to incorporate stability selection to improve this method. Stability selection is a subsampling-based variable selection that allows to control Type I error rates. The here proposed S4VD algorithm incorporates this subsampling approach to find stable biclusters, and to estimate the selection probabilities of genes and samples to belong to the biclusters. Results: So far, the S4VD method is the first biclustering approach that takes the cluster stability regarding perturbations of the data into account. Application of the S4VD algorithm to a lung cancer microarray dataset revealed biclusters that correspond to coregulated genes associated with cancer subtypes. Marker genes for different lung cancer subtypes showed high selection probabilities to belong to the corresponding biclusters. Moreover, the genes associated with the biclusters belong to significantly enriched cancer-related Gene Ontology categories. In a simulation study, the S4VD algorithm outperformed the SSVD algorithm and two other SVD-related biclustering methods in recovering artificial biclusters and in being robust to noisy data. Availability: R-Code of the S4VD algorithm as well as a documentation can be found at http://s4vd.r-forge.r-project.org/. Contact: m.sill@dkfz.de</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Clustering methods belong to the most commonly used statistical tools in the analysis of high-dimensional datasets. If additional information about the sample class labels is lacking, other types of analysis like supervised classification methods or testing for differentially expressed genes cannot be performed. In this case, unsupervised clustering allows to reveal unknown structures that are * To whom correspondence should be addressed. possibly hidden in the gene expression data matrix. These structures may be characterized by groups of genes that are coregulated by a common transcription factor and thus belong to the same pathway or samples that share a similar gene expression pattern. One disadvantage of commonly used clustering algorithms like hierarchical clustering or k-means clustering is that the cluster assignment of objects are based on the complete feature space, e.g. in case of clustering the samples, the resulting clusters are derived with respect to all genes. But groups of genes may only be coregulated within a subset of the samples and samples may share a common gene expression pattern only for a subset of genes. Such clusters that exist only in a subspace of the feature space can hardly be detected by these classical one-way clustering algorithms. To find such clusters, other clustering concepts are needed. In the past decade, the concept of biclustering has emerged in the field of gene expression analysis. Biclustering which is also known as coclustering or two-way clustering describes the simultaneous clustering of the rows and the columns of a data matrix. The first biclustering algorithm, the so-called Block Clustering, has been developed by<ref type="bibr" target="#b12">Hartigan (1972)</ref>. Cheng and<ref type="bibr" target="#b7">Church (2000)</ref>proposed the first biclustering algorithm for the analysis of high-dimensional gene expression data. Since then, many different biclustering algorithms have been developed. Currently, there exists a diverse spectrum of biclustering tools that follow different strategies and algorithmic concepts. Among others, popular algorithms are the Coupled Two-Way Clustering (CTWC) by<ref type="bibr" target="#b11">Getz et al. (2000)</ref>, Order Preserving Sub Matrix (OPSM) algorithm by<ref type="bibr" target="#b1">Ben-Dor et al. (2003)</ref>, the Iterative Signature Algorithm (ISA) by<ref type="bibr" target="#b2">Bergmann et al. (2003)</ref>, the Plaid Model by<ref type="bibr">Lazzeroni and Owen (2002)</ref>and the improved Plaid Model (<ref type="bibr" target="#b27">Turner et al., 2005</ref>), SAMBA by<ref type="bibr" target="#b25">Tanay et al. (2004)</ref>, biclustering by non-smooth non-negative matrix factorization by<ref type="bibr" target="#b6">Carmona-Saez et al. (2006)</ref>, the Bi-correlation clustering algorithm (BCCA) by Bhattacharya and De (2009) and factor analysis for bicluster acquisition (FABIA;<ref type="bibr" target="#b13">Hochreiter et al., 2010</ref>).<ref type="bibr" target="#b19">Prelic et al. (2006)</ref>developed a fast divide-and-conquer algorithm (Bimax) and conducted a systematic comparison of different biclustering algorithms.<ref type="bibr">Santamaria et al. (2007)</ref>published an article on validation indices for the evaluation of biclustering results and the comparison for biclustering algorithms. Comprehensive reviews about the concept of biclustering and the different biclustering approaches have been written by<ref type="bibr" target="#b16">Madeira and Oliveira (2004)</ref>and Van<ref type="bibr" target="#b28">Mechelen et al. (2004)</ref>. In a more theoretical review,<ref type="bibr" target="#b5">Busygin et al. (2008)</ref>emphasized the mathematical concepts behind several biclustering algorithms and pointed out that the SVD represents a capable tool for finding biclusters. Furthermore, most existing biclustering algorithms use the SVD directly or have a strong association with it. To keep track of the huge diversity, regarding the mathematical properties of the existing biclustering algorithms,<ref type="bibr" target="#b5">Busygin et al. (2008)</ref>suggest to relate new and existing biclustering algorithms to the SVD. A major drawback of many biclustering methods is that they rely on random starting seeds and thus are inconsistent and results may vary even when the algorithm is applied to the same dataset. As often in unsupervised clustering it is difficult to judge the biclustering results regarding their stability. For one-way clustering, several resampling approaches to validate the stability of the clustering results are known, e.g. multiscale bootstrap hierarchical clustering (<ref type="bibr">Suzuki and Shimodeira, 2006</ref>) and consensus clustering (<ref type="bibr">Monti, 2005</ref>). In case of biclustering, similar methods that take the stability of the results into account are not yet available. Recently,<ref type="bibr" target="#b15">Lee et al. (2010)</ref>proposed a sparse SVD (SSVD) method to find biclusters in gene expression data. Singular vectors of an SVD are interpreted as regression coefficients of a linear regression model. The SSVD algorithm alternately fits penalized regression models to the singular vector pair to obtain a sparse matrix decomposition. The sparseness of the resulting singular vectors strongly depends on the choice of the penalization parameter. In this article, we propose to choose the penalization parameters by stability selection (<ref type="bibr" target="#b17">Meinshausen and Bühlmann, 2010</ref>), which is a subsampling procedure that can be applied to penalized regression models to select stable variables. In addition, stability selection offers the possibility to control Type I error rates (<ref type="bibr" target="#b9">Dudoit et al., 2003</ref>), e.g. the per-family error rate (PFER) or the per-comparison wise error rate (PCER). Applying the new combined algorithm, the sparse SVD algorithm with nested stability selection (S4VD) to a lung cancer gene expression dataset reveals biclusters that represent lung cancer subtypes characterized by relevant sets of coregulated genes. In a simulation study, we compare the S4VD with the SSVD algorithm as well as the improved Plaid Model (<ref type="bibr" target="#b27">Turner et al., 2005</ref>) and the ISA (<ref type="bibr" target="#b2">Bergmann et al., 2003</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SVD and biclustering</head><p>Let X = (x ij ) ∈ R p×n be the gene expression matrix with indices i = 1,...,p and j = 1,...,n. The number of genes p is usually by multiple greater than the number of samples n. The SVD of X can be written as:</p><formula>X ≈ UDV T = r k=1 d k u k v T k ,</formula><formula>(1)</formula><p>where r is the rank of X and the columns of the matrix U = (u 1 ,...,u r ) are the orthonormal left-singular vectors and the columns of V = (v 1 ,...,v r ) are the orthonormal right-singular vectors. The elements of the diagonal matrix D are the corresponding positive singular values d 1 ≥ d 2 ≥ ... ≥ d r &gt; 0. Thus, the SVD is the sum of rank of one matrices d k u k v T k , herein after also called SVD-layers. According to<ref type="bibr" target="#b5">Busygin et al. (2008)</ref>, biclustering can be related to the SVD by considering an idealized data matrix. This matrix has a block diagonal structure where each block represents a bicluster and the elements outside these blocks are equal to zero:</p><formula>X = ⎛ ⎜ ⎜ ⎜ ⎝ X 1 0 ··· 0 0 X 2 0 ··· . . . . . . .. . 0 0 0 ··· X r ⎞ ⎟ ⎟ ⎟ ⎠ ,</formula><formula>(2)</formula><p>where X k , k = 1,...,r are submatrices of X. If we decompose X by the SVD, then each submatrix X k will be associated with a singular vector pair (u k ,v k ) such that the non-zero coefficients in u k represent the rows that belong to X k and the non-zero coefficients v k represent the columns that belong to X k. In the presence of noise and if the data matrix has no block diagonal structure, the SVD will still be able to detect the rows and columns of the submatrices as the prominent coefficients in the singular vector pair. These properties make the SVD a practical method for biclustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The SSVD algorithm</head><p>A sparse SVD method for biclustering high-dimensional gene expression data has been proposed by<ref type="bibr" target="#b15">Lee et al. (2010)</ref>. The idea is to interpret the singular vectors of a regular SVD as regression coefficients of a linear regression and use sparsity-inducing penalties to obtain sparse singular vector pairs. According to<ref type="bibr" target="#b10">Eckart and Young (1936)</ref>, the first SVD-layer gives us the best rank-one approximation of X with respect to the squared Frobenius norm, i.e.</p><formula>(d 1 ,u 1 ,v 1 ) = arg min d,u,v X −duv T 2 F ,</formula><formula>(3)</formula><p>where · 2 F indicates the squared Frobenius norm, which is the sum of squared elements of the matrix.<ref type="bibr" target="#b15">Lee et al. (2010)</ref>showed how this rank-one approximation can be related to linear regression. Suppose u 1 is fixed, then the minimization of (3) with respect to (d 1 ,v 1 ) is equivalent to a minimization with respect tõ v 1 = (d 1 v 1 ). Accordingly, the loss function can be written as:</p><formula>X −u 1 ˜ v T 1 2 F = y −(I n ⊗u 1 )˜ v 1 ,</formula><formula>(4)</formula><p>where y = (x T 1 ,...,x T n ) T ∈ R pn with x j being the j-th column of X. Then the minimization of (4) can be interpreted as least squares problem with y as the response vector, I n ⊗u 1 as the design matrix and the˜vthe˜ the˜v 1 as vector of regression coefficients. The least squares estimator of˜vof˜ of˜v 1 is:</p><formula>ˆ ˜ v 1 = (I n ⊗u 1 ) T (I n ⊗u 1 ) −1 (I n ⊗u 1 ) T y (5) = (u T 1 x 1 ,...,u T 1 x n ) T = X T u 1 .</formula><p>In the same way, we can derive the least squares estimator for the product of the first left singular vector multiplied with the first singular value˜uvalue˜ value˜u 1. So without loss of generality with v 1 fixed, the minimization of (3) with respect tõ u 1 = (d 1 u 1 ) is given by the minimization of:</p><formula>X − ˜ u 1 v T 1 2 F = z−(I n ⊗v 1 ) ˜ u 1 ,</formula><formula>(6)</formula><p>where z = (x 1 ,...,x p ) T ∈ R pn with x T i being the i-th row of X. Here z is the response vector and (I n ⊗v 1 ) is the design matrix. Finally, the least squares estimator of˜uof˜ of˜u 1 is given by:</p><formula>ˆ ˜ u 1 = (I n ⊗v 1 ) T (I n ⊗v 1 ) −1 (I n ⊗v 1 ) T z (7) = (x T 1 v 1 ,...,x T p v 1 ) = Xv 1 .</formula><p>In order to obtain sparse singular vector pairs,<ref type="bibr" target="#b15">Lee et al. (2010)</ref>suggest to find the first SVD-layer that minimizes the Frobenius norm subject to sparsity-inducing penalty terms</p><formula>P 1 (d 1 u 1 ) and P 2 (d 1 v 1 ): X −d 1 u 1 v T 1 2 F +λ u 1 P 1 (d 1 u 1 )+λ v 1 P 2 (d 1 v 1 ),</formula><formula>(8)</formula><p>where λ u 1 and λ v 1 are tuning parameters. Possible penalty functions are the adaptive lasso penalties (<ref type="bibr" target="#b29">Zou, 2006</ref>). The corresponding penalized function is given by:</p><formula>P 1 (d 1 u 1 ) = d 1 p i=1 w 1,i |u 1,i |, P 2 (d 1 v 1 ) = d 1 n j=1 w 2,j |v 1,j |,</formula><formula>(9)</formula><p>where w 1,i and w 2,j are weights that can be chosen according to<ref type="bibr" target="#b29">Zou (2006)</ref>, e.g. for w 1,i = w 2,j = 1 we obtain the lasso penalty. Thus, the penalty functions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse SVD incorporating stability selection</head><p>are weighted sums of the absolute values of the elements of the first singular vector pair. Fixing u 1 and using the adaptive lasso penalty, the minimization of (8) becomes:</p><formula>X −d 1 u 1 v T 1 2 F +λ v 1 n j=1 w 2,j |v 1,j | (10) = X 2 F + n j=1˜v j=1˜ j=1˜v 2 1,j −2˜v−2˜v 1,j (X T u 1 ) j +λ v 1 w 2,j |˜v|˜v 1,j | .</formula><p>To solve this penalized regression and estimate the sparse right singular vector,<ref type="bibr" target="#b15">Lee et al. (2010)</ref>proposed an algorithm that incorporates a simple component-wise thresholding rule. The component-wise minimizer of (10) is:</p><formula>ˆ ˜ v 1,j = sign (X T u 1 ) j (|(X T u 1 ) j |−λ v 1 w 2,j /2) + .</formula><formula>(11)</formula><p>This is the well-known soft threshold estimator proposed by<ref type="bibr" target="#b26">Tibshirani (1996)</ref>. Thenˆ˜ThenˆThenˆ˜</p><formula>v 1 = ( ˆ ˜ v 1,1 ,..., ˆ ˜ v 1,n ) T</formula><p>, is an estimate for the product of the first right singular vector multiplied with the first singular value. In order to get an estimate for the first sparse right singular vector, we have to update the first singular value. The first update of d 1 is d 1,v 1 ==ˆ˜==ˆ==ˆ˜ v 1 and accordingly the estimated sparse singular vector becomesˆvbecomesˆ becomesˆv</p><formula>1 = ˆ ˜ v 1 /d 1,v 1 .</formula><p>The penalized regression for the left singular vector can be solved in the same way. For fixed v 1 and with the adaptive lasso penalty, the loss function of (8) becomes:</p><formula>X −d 1 u 1 v T 1 2 F +λ u 1 p i=1 w 1,i |u 1,i | (12) = X 2 F + p i=1˜u i=1˜ i=1˜u 2 1,i −2˜u−2˜u 1,i (Xv 1 ) i +λ u 1 w 1,i |˜u|˜u 1,i | .</formula><p>The component-wise minimizer of (12) is:</p><formula>ˆ ˜ u 1,i = sign{(Xv 1 ) i }(|(Xv 1 ) i |−λ u 1 w 1,i /2) + .</formula><formula>(13)</formula><p>The updated singular value is d 1,u 1 ==ˆ˜==ˆ==ˆ˜ u 1 , withˆ˜withˆwithˆ˜</p><formula>u 1 = ( ˆ ˜ u 1,1 ,..., ˆ ˜ u 1,p ) T .</formula><p>Finally, the estimated sparse left singular vector isûisˆisû</p><formula>1 = ˆ ˜ u 1 /d 1,u 1 .</formula><p>The degree of sparsity, which is defined as the number of non-zero coefficients in the singular vector pair, depends on the choice of the penalty parameters.<ref type="bibr" target="#b15">Lee et al. (2010)</ref>proposed to choose the optimal degree of sparsity by computing the complete penalization path and apply the penalty parameter that minimizes the Bayesian information criterion (BIC). In the SSVD algorithm, the two regressions with the corresponding parameter tuning are alternated until convergence is reached, which is if either</p><formula>v 1 − ˆ v 1 &lt;&lt; or u 1 − ˆ u 1</formula><p>&lt;&lt;, where &gt;0 is an arbitrary convergence threshold. After convergence the final estimate of the first singular value of the sparse SVD-layer isˆdisˆ</p><formula>isˆd 1 = ˆ u T 1 X ˆ v 1 .</formula><p>The next sparse rankone approximation can be obtained by subtracting the sparse SVD-layer and applying the SSVD method to the residual matrix</p><formula>X − ˆ d 1 ˆ u 1 ˆ v T 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The SSVD algorithm</head><p>1. Apply the standard SVD to X. Let {d 1 ,u 1 ,v 1 } denote the first SVD triplet.</p><p>2. Update:</p><formula>(a) Setˆ˜SetˆSetˆ˜ u 1,i = sign{(Xv 1 ) i }(|(Xv 1 ) i |−λ u 1 w 1,i /2) + , where λ u 1</formula><p>minimizes the BIC. Letˆ˜LetˆLetˆ˜</p><formula>u 1 = ( ˆ ˜ u 1,1 ,..., ˆ ˜ u 1,p ) T , d 1,u 1 ==ˆ˜==ˆ==ˆ˜ u 1 , andû andˆandû 1 = ˆ ˜ u 1 /d 1,u 1 .</formula><p>(b) Setˆ˜SetˆSetˆ˜</p><formula>v 1,j = sign (X T ˆ u 1 ) j (|(X T ˆ u 1 ) j |−λ v 1 w 2,j /2) + , where λ v 1 minimizes the BIC. Letˆ˜LetˆLetˆ˜ v 1 = ( ˆ ˜ v 1,1 ,..., ˆ ˜ v 1,n ) T , d 1,v 1 ==ˆ˜==ˆ==ˆ˜ v 1 , andˆv andˆ andˆv 1 = ˆ ˜ v/d 1,v 1 .</formula><formula>(c) Set v 1 = ˆ v 1 , u 1 = ˆ</formula><formula>− ˆ d 1 ˆ u 1 ˆ v T 1 .</formula><p>In practice, we observed that choosing the regularization parameters according to the BIC results in singular vector pairs with a relative low degree of sparsity. In addition, the SSVD algorithm does not offer a stopping criterion and so the choice of the number of SVD-layers is arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stability selection</head><p>In this article, we propose to choose the penalization parameters and to control the degree of sparsity of the resulting SVD-layers using stability selection (<ref type="bibr" target="#b17">Meinshausen and Bühlmann, 2010</ref>). The idea of stability selection is to combine resampling with variable selection methods, e.g. penalized regression models. For each variable, its probability of being selected is estimated by resampling the data and calculating relative frequencies of being selected. Meinshausen and Bühlmann (2010) provide a theoretical framework for controlling Type I error rates of falsely selecting variables based on the maximum of these selection probabilities over the range of regularization parameters. Suppose we want to infer the true set of non-zero coefficients in the left singular vector S u 1 = i : u 1,i = 0 . The set of possible penalization parameters that can be applied within the adaptive lasso regression isGiven any λ u 1 , the estimated setˆSsetˆ setˆS λu 1 u 1 can be written as a function of the samples J = {1,...,n}, e.g.</p><formula>ˆ S λu 1 u 1 = ˆ S λu 1</formula><p>u 1 (J). If J * ⊂ J is a subsample drawn without replacement, then the estimated selection probability is: ˆ</p><formula>λu 1 i = P(i ∈ ˆ S λu 1 u 1 (J * )).</formula><formula>(14)</formula><p>The selection probability can be estimated by calculating the relative selection frequencies of i with regard to all subsamples. Given an arbitrary threshold π thr ∈ (0.5,1) and the set of penalization parameters u 1 , the set of non-zero coefficients estimated with the stability selection is:</p><formula>ˆ S stable u 1 = i : max λu 1 ∈u 1 ˆ λu 1 i ≥ π thr .</formula><formula>(15)</formula><p>According to<ref type="bibr" target="#b17">Meinshausen and Bühlmann (2010)</ref>, the value of π thr has a negligible influence and they recommend to choose values in the range of</p><formula>[0.6,0.9]. LetˆSLetˆ LetˆS u 1 =∪ λu 1 ∈u 1 ˆ S λu 1</formula><p>be the union of the estimated sets of selected coefficients with regard to all λ u 1 ∈ u 1. Then the average number of selected coefficients is q</p><formula>u 1 = E(| ˆ S u 1 (J * )|). Let N u 1</formula><p>denote the set of zero coefficients, then the number of falsely selected coefficients with stability selection is given by</p><formula>V u 1 =|N u 1 ∩ ˆ S stable u 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|. Following Theorem 1 in</head><p>Meinshausen and Bühlmann (2010), the expected number of falsely selected coefficients is bounded by:</p><formula>E(V u 1 ) ≤ 1 (2π thr −1) q 2 u 1 p .</formula><formula>(16)</formula><p>Interpreting Equation (16), the expected number of falsely selected coefficients decreases by either reducing the average number of selected coefficients q u 1 or by increasing the threshold π thr. Supposing that π thr is fixed, the stability selection controls the desired error level of E(V u 1 ) as long as the average number of selected coefficients is less then e u 1 , where</p><formula>e u 1 = E(V u 1 )p(2π thr −1</formula><p>) is an upper bound for the average number of selected coefficients that can be controlled by reducing the length of the regularization path u 1. In multiple testing, the expected number of falsely selected variables is also known as PFER and if divided by the total number of the variables, it will become the PCER (<ref type="bibr" target="#b9">Dudoit et al., 2003</ref>). The stability selection allows to control these Type I error rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 2092 2089–2097</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Sill et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The SSVD algorithm with nested stability selection</head><p>Here, we propose to replace the BIC-based penalty parameter selection of the SSVD algorithm by the stability selection. This combined approach allows to control the expected number of falsely selected non-zero coefficients in the singular vector pair and therefore the degree of sparsity of the resulting SVD-layers. Furthermore, the error control also serves as stopping criterion for the improved SSVD algorithm and determines the number of reasonable layers. We aim to estimate the left singular vectorûvectorˆvectorû 1 and at the same time infer the true set of non-zero coefficients S u 1. For each possible λ u 1 , we draw subsamples and estimate the selection probabilitiesˆλuprobabilitiesˆ probabilitiesˆλu 1 i. Given a threshold π thr and the desired Type I error E(V u 1 ), the regularization region u 1 is defined so that q u 1 ≤ e u 1. Then the estimated set of non-zero coefficients is:</p><formula>ˆ S stable u 1 = i : max λu 1 ∈u 1 ˆ λu 1 i ≥ π thr</formula><formula>(17)</formula><p>To estimatêestimatê˜ estimatê˜u 1 , we apply the component-wise minimizer of<ref type="bibr" target="#b15">Lee et al. (2010)</ref>with the smallest penalization value of the regularization path λ min</p><formula>u 1 . ˆ ˜ u 1,i = sign{(Xv 1 ) i }(|(Xv 1 ) i |−λ min u 1 w 1,i /2) + (18)</formula><p>Like in the original SSVD approach, the first update of the singular value is</p><formula>d 1,u 1 ==ˆ˜==ˆ==ˆ˜ u 1 , withˆ˜withˆwithˆ˜ u 1 = ( ˆ ˜ u 1,1 ,..., ˆ ˜ u 1,n ) T .</formula><p>The estimated sparse singular vector isûisˆisû 1 = ˆ ˜ u 1 /d 1,u 1. Without loss of generality, we estimate the sparse right singular vectorˆvvectorˆ vectorˆv 1 and infer the respective set of non-zero coefficients</p><formula>q v 1 ≤ e v 1 , where e v 1 = E(V v 1 )n(2π thr −1</formula><p>). Consequently, the estimated set of non-zero coefficients in the right singular vector is:</p><formula>ˆ S stable v 1 = j : max λv 1 ∈v 1 ˆ λv 1 j ≥ π thr</formula><formula>(19)</formula><p>Given the smallest parameter of the penalization path λ min</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v 1</head><p>, the components of˜vof˜ of˜v 1 are:</p><formula>ˆ ˜ v 1,j = sign (X T u 1 ) j (|(X T u 1 ) j |−λ min v 1 w 2,j /2) + (20)</formula><p>Finally letˆ˜letˆletˆ˜</p><formula>v 1 = ( ˆ ˜ v 1,1 ,..., ˆ ˜ v 1,n ) T ,</formula><p>the updated first singular value is d 1,v 1 = ˜ v 1 and estimated sparse singular vector isˆvisˆ isˆv</p><formula>= ˆ ˜ v 1 /d 1,v 1 .</formula><p>These two penalized regression models with the nested stability selection are alternated until convergence, e.g. that is if either</p><formula>v 1 − ˆ v 1 &lt;&lt; or u 1 − ˆ u 1</formula><p>&lt;&lt;, where &gt;0. After convergence, the estimated singular value isˆdisˆ isˆd</p><formula>1 = ˆ u T 1 X ˆ</formula><formula>1 becomê u 1,i = 1(i ∈ ˆ S stable u 1</formula><p>)ˆ u 1,i and the components ofˆvofˆ ofˆv 1 becomêbecomê</p><formula>v 1,j = 1(j ∈ ˆ S stable v 1 )ˆ v 1,j , where 1(·)</formula><p>is an indicator function. The high degree of sparsity of the resulting SVD-layers may lead to a poor matrix factorization that might induce noise to the residual matrix when subtracted from the data matrix. Like for multivariate regression models, this can be seen as a trade-off between a high degree of sparsity and hence interpretability for the cost of losing prediction power. Regarding the sequential fitting procedure of the S4VD algorithm, the acceptance of a poor matrix approximation might induce noise into the residual matrix. This induced noise may perturb the fitting process for subsequent biclusters. In order to avoid a propagation of errors induced by a poor matrix approximation, we propose to apply the regular SVD to the submatrix defined by the stable subsets of rows and columns identified with the S4VD algorithm. According to<ref type="bibr" target="#b10">Eckart and Young (1936)</ref>, the rank-one SVD approximation of this submatrix is the best rank-one approximation of the submatrix with respect to the Frobenius norm. The next bicluster can be detected by subtracting this rank-one approximation of the submatrix from the corresponding submatrix of the input data matrix and applying the S4VD algorithm to the resulting residual matrix.</p><p>Alternatively non-overlapping biclusters can be detected by excluding either the rows or the columns (or both) that correspond to the non-zero coefficients in the singular vector pair and apply the S4VD method to the submatrix. By incorporating the stability selection, a stopping criterion can be defined. If in any iteration an estimated set of non-zero coefficients is an empty set, the sequential fitting of sparse rank-one layers will be interrupted. Due to the element of resampling the S4VD algorithm will not necessarily converge to the exact same result when applied to the same dataset. However, the element of resampling also allows to take the bicluster stability into account by controlling the Type I error levels of falsely assigning rows and columns. As demonstrated by the simulations presented in Section 3, the S4VD algorithm shows a better performance in revealing the true bicluster structure and is more robust to noise.)ˆ v 1,j. 4. To obtain the next layer, apply steps 1–3 to the residual matrix after subtracting the rank-one approximation derived by applying a regular SVD to the submatrix defined byˆSbyˆ byˆS stable</p><formula>u 1 w 1,i /2) + Letˆ˜LetˆLetˆ˜ u 1 = ( ˆ ˜ u 1,1 ,..., ˆ ˜ u 1,p ) T , d 1,u 1 ==ˆ˜==ˆ==ˆ˜ u 1 , andûandˆandû 1 = ˆ ˜ u/d 1,u 1</formula><formula>(</formula><formula>v 1 . Setˆ˜SetˆSetˆ˜ v 1,j = sign (X T ˆ u 1 ) j (|(X T ˆ u 1 ) j |−λ min v 1 w 2,j /2) + Letˆ˜LetˆLetˆ˜ v 1 = ( ˆ ˜ v 1,1 ,..., ˆ ˜ v 1,n ) T , d 1</formula><formula>u 1 andˆSandˆ andˆS stable v 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Stop steps 1–4 if eitherˆSeitherˆ eitherˆS stable</head><formula>v 1 =∅ orˆSorˆ orˆS stable u 1 =∅.</formula><p>The subsampling steps of the stability selection makes the S4VD algorithm computationally very demanding. To reduce the computation time, we implemented the pointwise error control suggested by Meinshausen and Bühlmann (2010). To examine how the pointwise error control reduces the computation time, the runtimes of the SSVD algorithm, S4VD algorithm and S4VD algorithm with the pointwise error control have been compared. In the first part of the simulation study described in the Section 3 at a noise level of 0.5, the mean runtime of the SSVD algorithm was 33.9 s, for the S4VD it was 181.7 s and for the S4VD with the pointwise error control it was 5.8 s. The simulations have been carried out using a notebook with an Intel®Core™2 Duo Processor T7700 2.4 GHz and 4 GB DDR2 SDRAM. Details about the pointwise error control and boxplots of the runtimes are shown in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>In order to demonstrate that the here proposed S4VD algorithm is able to find biologically relevant biclusters, we applied it to a knownthat the heatmap shows only those genes that have been selected in at least one bicluster. The rectangles indicate the genes and samples that correspond to the three biclusters (the rectangle on the left side corresponds to Bicluster 1, the two rectangles in the middle correspond to Bicluster 2 and the two rectangles on the right side to Bicluster 3). lung cancer gene expression dataset (<ref type="bibr">Bhattacharyee et al., 2001</ref>). Furthermore, to examine the influence of increasing levels of noise regarding the performance of the S4VD algorithm, we performed a simulation study. The S4VD algorithm was compared with the SSVD method, the improved Plaid Model (<ref type="bibr" target="#b27">Turner et al., 2005</ref>) and the ISA (<ref type="bibr" target="#b2">Bergmann et al., 2003</ref>). The ISA and the Plaid Model are known to be closely related to the SVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 2093 2089–2097</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse SVD incorporating stability selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation of the lung cancer dataset</head><p>Here we analyzed the same subset of the lung cancer gene expression data set (<ref type="bibr">Bhattacharyee et al., 2001</ref>) that was used by<ref type="bibr" target="#b15">Lee et al. (2010)</ref>to illustrate the SSVD algorithm. This dataset contain 56 samples and gene expression values of 12 625 genes measured using the Affymetrix 95av2 GeneChip. The samples comprise 20 pulmonary carcinoid samples (Carcinoid), 13 colon cancer metastasis samples (Colon), 17 normal lung samples (Normal) and 6 small cell lung carcinoma samples (SmallCell).<ref type="bibr" target="#b15">Lee et al. (2010)</ref>applied the SSVD method to this gene expression matrix and decomposed it into the first three sparse SVD-layers. For each of the resulting SVD-layers, the degree of sparsity was relatively low, e.g. for the three singular vectors that correspond to the samples, the number of non-zero coefficients were 55 for the first two and 47 for the third. The singular vectors that correspond to the genes contained 3205, 2511 and 1221 non-zero coefficients. Scatterplots of the sample singular vectors showed a clear grouping of the samples into the different cancer subtypes. In addition,<ref type="bibr" target="#b15">Lee et al. (2010)</ref>formed 27 gene sets according to the sign of the coefficients in the three gene singular vectors. The mean expression profiles of these gene sets showed clear differences between the cancer subtypes. However, despite these results a direct interpretation of each singular vector pair is not possible. To obtain SVD-layers with a higher degree of sparsity that can be interpreted as single biclusters, we applied the S4VD algorithm controlling a PCER of 0.5 for falsely selecting coefficients in the sample singular vector and a PCER of 0.01 for falsely selecting coefficients in the gene singular vector. Furthermore, we did not allow the samples to overlap, e.g. each sample is assigned to only one bicluster. Therefore, after a sparse SVD-layer is fitted, we exclude the corresponding columns from the data matrix and applied the S4VD algorithm to the resulting submatrix. According to the stopping criterion of the S4VD algorithm, three biclusters have been obtained and are shown in the heatmap in<ref type="figure" target="#fig_1">Figure 1</ref>. The first bicluster corresponds to a subset of 550 genes and a subset of 28 samples including 14 Normal samples and 14 Carcinoid samples. The second bicluster comprises 12 Colon samples and one falsely assigned Carcinoid sample together with a subset of 506 genes. The third bicluster consists of 6 SmallCell samples and 344 genes. All other samples and genes have not been assigned to any bicluster. To illustrate that the selected genes represent genes that are associated with the cancer subtypes, we performed a geneset enrichment analysis (<ref type="bibr" target="#b0">Alexa et al., 2006</ref>). Tables of all significantly enriched Gene Ontology (GO) terms (p &lt; 0.01) as well as a description of the analysis can be found in the Supplementary Material.<ref type="bibr" target="#b4">Bhattacharjee et al. (2001)</ref>identified several possible marker genes for the different cancer subtypes. A list of eight of these genes together with the corresponding selection probabilities with respect to the three biclusters are shown intissue and thus have high selection probabilities for the first bicluster. This coincides with the GO analysis, e.g. 2 of the 62 GO terms that are significantly enriched by the genes corresponding to the first bicluster are TGFβ receptor signaling pathway (GO:0007179) and response to retinoic acid (GO:0032526). Integrin,α6 as well as v-myc (c-myc) are usually overexpressed in colon cancer. These genes have high selection probabilities with respect to the second bicluster. In addition, among the 61 significantly enriched GO terms corresponding to the second bicluster is the term endothelial cell migration (GO:0043542), which coincides with the fact that the associated samples correspond to colon cancer metastases. The cell-cycle inhibitor protein p18 and thymosin-β are markers for small cell carcinomas and show high selection probabilities in the third bicluster. Among the 97 GO terms significantly enriched in the third bicluster are many cell cycle-associated terms, e.g. cell division (GO:0051301), mitotic spindle organization (GO:0007052) and cell cycle checkpoint (GO:0000075). Furthermore, for the first bicluster as well as for the third bicluster the GO term positive regulation of Notch signaling pathway (GO:0045747) is significantly enriched. Alterations of the Notch signaling cascade are known to be associated with several human cancer types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simulation study</head><p>In the first part of the simulations, we generated 100 artificial data matrices comprising p = 1000 genes and n = 100 samples, where each entry of the data matrix is set to 0. In each dataset, we randomly assigned 100 genes and 10 samples to a bicluster that shows constant upregulated gene expression represented by a value of 1 in the data matrix. Normally distributed noise N(0,σ 2 ) was added to each entry of the data matrix. We examined different noise levels in the range of σ = (0,0.1,...,1). In the second part of the simulation study, 100 data matrices of the same dimension were generated. This time four biclusters were included where each consists of 100 genes and 10 samples. Constant up-and downregulation was represented by values of 1,−1,0.5 and −0.5. For both scenarios, the performance of the S4VD algorithm was examined in comparison to the original SSVD algorithm, the improved Plaid Model (PM;<ref type="bibr" target="#b27">Turner et al., 2005</ref>) and the ISA (<ref type="bibr" target="#b2">Bergmann et al., 2003</ref>). Since the SSVD algorithm does not include a stopping criterion, we considered only the first SVD-layer as result in the first scenario and the first four SVD-layers as the biclustering result in the second scenario. The clustering results were validated by application of an external validation index based on the Jaccard coefficient. In addition, the stability of the clustering results was assessed through the average proportion of falsely selected rows and columns. Details on the validation indices, the remaining biclustering algorithms and their relation to the SVD are provided in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Scenario 1</head><p>The simulation results of the first scenario are shown in<ref type="figure">Figure 2</ref>. For low noise levels up to σ = 0.3, all biclustering algorithms except the SSVD show an almost perfect performance with relevance and recovery scores mostly equal to one and no falsely selected rows and columns. For noise levels of 0.1 to 0.7, all biclusters proposed by the SSVD algorithm are too large and on average a proportion around 0.015 of the rows and 0.012 of the columns are falsely assigned. This results in relevance and recovery scores around 0.8. In case of larger noise levels, the SSVD algorithm often fails to converge and thus the relevance scores and the number of falsely assigned rows and columns approach zero. For noise levels above 0.3, the first bicluster detected by the Plaid Model usually consists of a strict subset of those rows and columns that belong to the true artificial bicluster in the data. Thus, the performance of the Plaid Model regarding the relevance and the recovery decreases with noise. Furthermore, the algorithm starts to fit the noise and proposes a number of further small biclusters. This explains why the relevance score is inferior compared with the recovery score. Most of these small biclusters correspond to parts of the true artificial bicluster and hence the proportions of falsely assigned rows and columns are close to zero. Beginning with a noise level of 0.5, the ISA proposes an increasing number of biclusters of which only one shows a strong agreement with the true bicluster. Even after applying the additional filtering functions available in the isa2 R-package (<ref type="bibr" target="#b8">Csardi et al., 2010</ref>), some nonsense biclusters remain. Thus, both scores start to decrease with noise but are superior to the Plaid Model. The number of falsely assigned rows and columns increases with the noise level indicating that some of the detected biclusters correspond to fitted noise. Regardless of the noise level, the S4VD algorithm always detects a single bicluster that agrees with the true bicluster. For noise levels above 0.6, the proposed bicluster becomes smaller and represents only a part of the true bicluster. Therefore, both scores start to decrease with noise but are superior to that of all other biclustering methods considered in the simulation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Scenario 2</head><p>The results of the second part of the simulation study are shown in<ref type="figure">Figure 3</ref>.</p><formula>(a) ( b) (c) ( d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Sill et al.</head><p>The Plaid Model algorithm in some cases perfectly revealed the hidden structure, but in other situations depending on the randomly chosen starting values and the noise level, the algorithm falsely assigns rows and columns to the biclusters. The stopping criterion of the algorithm depends on a permutation test that can fail to reject unimportant biclusters that correspond to noise. On the other hand for higher noise levels, the permutation test also tends to reject biclusters early in the fitting process so that only three or less biclusters are detected. Thus, the resulting relevance and recovery scores are highly variable and decrease with noise. Regarding low noise levels, the SSVD algorithm mostly identifies the correct biclusters but usually falsely assigns some additional rows and columns. This behavior maintains for higher noise levels, but additionally the number of correctly identified biclusters becomes less. For noise levels above 0.7, both the SSVD algorithm and the Plaid Model mostly do not detect any of the artificial biclusters and hence the average proportions of falsely assigned rows and columns approach zero. The performance of the ISA decreases due to an increasing number of identified irrelevant biclusters, starting with noise levels above 0.2. For noise level 0.5, the medians of both similarity scores are around 0.5, and the relevance scores show a high variability. For noise levels above 0.5, the two embedded biclusters generated to have a constant up-and downregulation of 0.5 and −0.5 are masked by noise, and hence, the ISA as well as the S4VD algorithm tend to miss these clusters. This results in a slight increase of their relevance scores while the recovery scores decrease. Moreover, the relevance scores for both algorithms show a high variability at noise level 0.6. In summary, the S4VD algorithm outperforms all other biclustering algorithms considered in the simulation study regarding the relevance and the recovery of the artificial biclusters for all simulation scenarios. Furthermore, due to the stability selection the S4VD algorithm rarely assigns false rows and columns to the proposed bicluster and does not detect any additional nonsense clusters. Thus for all simulation scenarios, the average proportions of falsely assigned rows and columns are close to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION AND CONCLUSION</head><p>In this article, we propose a new biclustering algorithm that combines the SSVD algorithm suggested by<ref type="bibr" target="#b15">Lee et al. (2010)</ref>with the stability selection of Meinshausen and Bühlmann (2010). In brief, the model selection-based parameter tuning of the penalized regression models of the SSVD algorithm is replaced by a subsampling-based variable selection that controls Type I error rates. The S4VD approach here presented allows to control the degree of sparsity of the resulting SVD-layers by choosing desired Type I error levels. The stability selection estimates the selection probabilities of the rows and columns to belong to a bicluster. Depending on the chosen Type I error levels, the results are robust biclusters represented by rows and columns that have high selection probabilities. If the noise level is getting too high, the stopping criterion leads to an interruption of the S4VD algorithm preventing from fitting additional SVD-layers that correspond to noise. So far, the S4VD method is the only biclustering approach that takes the cluster stability regarding perturbations of the data into account. We applied the S4VD algorithm to evaluate a lung cancer microarray dataset and showed that the resulting biclusters represent tumor subclasses together with coregulated genes. Marker genes for the different tumor subclasses showed high selection probabilities in the respective biclusters. In addition, a gene set enrichment analysis revealed that the genes associated with identified biclusters belong to significantly enriched cancerrelated GO terms. In a simulation study, the S4VD algorithm was compared with the SSVD algorithm, the improved Plaid Model (<ref type="bibr" target="#b27">Turner et al., 2005</ref>) and the ISA (<ref type="bibr" target="#b2">Bergmann, 2003</ref>). The S4VD algorithm showed the best performance regarding the recovery of biclusters and was more robust to noisy data compared with the other methods. The subsampling steps of the stability selection make the S4VD algorithm computationally very demanding. However, an improvement that strongly reduces the computation time is presented in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest:</head><p>none declared.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>u1.</head><figDesc>Each λ u 1 ∈ u 1 leads to a different estimated subset of indices of non-zero coefficientsˆScoefficientsˆ coefficientsˆS λu 1 u 1 ⊆ {1,...,p}. Meinshausen and Bühlmann (2010) illustrate the stability selection with the so-called stability paths that show the selection probabilities of each coefficient along the range of penalization parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>S v1.</head><figDesc>The selection probabilitiesˆλvprobabilitiesˆ probabilitiesˆλv 1 j for each λ v 1 are estimated by drawing subsets of the genes I * ⊂ I, where I = {1,...,p}. Again, given the desired Type I error E(V v 1 ) and the threshold π thr the regularization region is delimited such that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>v 1 and finally those coefficients that are not in the two sets of stable coefficientsˆScoefficientsˆ coefficientsˆS stable u 1 andˆSandˆ andˆS stable v 1 are set to zero. So the components ofûofˆofû</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.1.</head><figDesc>Fig. 1. Heatmap showing the biclusters identified in the lung cancer dataset. Note that the heatmap shows only those genes that have been selected in at least one bicluster. The rectangles indicate the genes and samples that correspond to the three biclusters (the rectangle on the left side corresponds to Bicluster 1, the two rectangles in the middle correspond to Bicluster 2 and the two rectangles on the right side to Bicluster 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig</head><figDesc>Fig. 3. Simulation results of the second scenario. The relevance score M(G,F), recovery score M(F,G) and the average proportions of falsely assigned rows V I (G,F) and columns V J (G,F) are described in the Supplementary Material. The boxplots show the distribution of these validation indices with respect to the 100 simulated datasets. σ indicates the considered noise level. (a) Relevance; (b) recovery; (c) falsely selected rows; (d) falsely selected columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>The S4VD algorithm 1. Apply the standard SVD to X. Let {d 1 ,u 1 ,v 1 } denote the first SVD triplet. Choose the desired Type I errors E(V v 1 ) and E(V u 1 ) and the threshold π thr. 2. Update: (a) For each λ u 1 draw subsamples J * and estimatê λu 1 i. Define u 1 such that q u 1 ≤ e u 1 and estimate the set of non-zero coefficientsˆScoefficientsˆ coefficientsˆS stable u 1 . Setˆ˜SetˆSetˆ˜ u 1,i = sign{(Xv 1 ) i }(|(Xv 1 ) i |−λ min</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>b) For each λ v 1 draw subsamples I * and estimatê λv 1 j. Define v 1 such that q v 1 ≤ e v 1 and estimate the set of non-zero coefficientsˆScoefficientsˆ coefficientsˆS stable</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 1. TGF-β receptor II, tetranectin, retinoic acid receptor responder 3 and ficolin 3 are known to be highly expressed in normal lung tissue compared with carcinoid</figDesc><table>Page: 2094 2089–2097 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 1.</figDesc><table>Selection probabilities of lung cancer subclass marker genes 

Gene 
Bicluster 1 
Bicluster 2 
Bicluster 3 

Retinoic acid receptor responder 3 
0.99 
0.00 
0.00 
Transforming growth factor, β receptor II (70/80kDa) 
1.00 
0.00 
0.86 
C-type lectin domain family 3, member B (tetranectin) 
1.00 
0.74 
0.68 
Ficolin (collagen/fibrinogen domain containing) 3 (Hakata antigen) 
1.00 
0.71 
0.98 
v-myc myelocytomatosis viral oncogene homolog 
0.00 
1.00 
0.20 
Integrin, α 6 
0.00 
0.93 
0.00 
Cyclin-dependent kinase inhibitor 2C (p18) 
0.00 
0.00 
0.91 
Thymosin β 
0.00 
0.15 
0.98 

</table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved scoring of functional groups from gene expression data by decorrelating GO graph structure</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Alexa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1600" to="1607" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Discovering local structure in gene expression data: the orderpreserving submatrix problem</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ben-Dor</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="373" to="384" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Iterative signature algorithm for the analysis of large-scale gene expression data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bergmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E. Stat. Nonlin. Soft. Matter Phys</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31902</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Pt. 1</note>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Bi-correlation clustering algorithm for determining a set of co-regulated genes</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Bhattacharya</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>De</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2795" to="2801" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Classification of human lung carcinomas by mRNA expression profiling reveals distinct adenocarcinoma subclasses</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Bhattacharjee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="13790" to="13795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Biclustering in data mining</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Busygin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2964" to="2987" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Biclustering of gene expression data by non-smooth non-negative matrix factorization</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Carmona-Saez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Biclustering of expression data</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Cheng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">M</forename>
				<surname>Church</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Intell. Syst. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Modular analysis of gene expression data with r</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Csardi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1376" to="1377" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple hypothesis testing in microarray experiments</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Dudoit</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="71" to="103" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Eckart</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Young</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Coupled two-way clustering analysis of gene microarray data</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Getz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="12079" to="12084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Direct clustering of a data matrix</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Hartigan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="123" to="129" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Fabia: factor analysis for bicluster acquisition</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Hochreiter</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1520" to="1527" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Plaid models for gene expression data</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Lazzeroni</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Owen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sin</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="61" to="86" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Biclustering via sparse singular value decomposition</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Biclustering algorithms for biological data analysis: a survey</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C</forename>
				<surname>Madeira</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L</forename>
				<surname>Oliveira</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="24" to="45" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Stability selection</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Meinshausen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bühlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Monti</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="91" to="118" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">A systematic comparison and evaluation of biclustering methods for gene expression data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Prelic</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1122" to="1129" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Methods to bicluster validation and comparison in microarray data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Santamaría</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Intelligent Data Engineering and Automated Learning</title>
		<meeting>the 8th International Conference on Intelligent Data Engineering and Automated Learning<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="780" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Pvclust: an r package for assessing the uncertainty in hierarchical clustering</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Suzuki</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Shimodaira</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1540" to="1542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12147</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2097" to="2089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">Sparse SVD incorporating stability selection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Revealing modularity and organization in the yeast molecular network by integrated analysis of highly heterogeneous genomewide data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Tanay</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2981" to="2986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B Methodol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Biclustering models for structured microarray data</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">L</forename>
				<surname>Turner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinform</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="316" to="329" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-mode clustering methods: a structured overview</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Van Mechelen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Methods Med. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="363" to="394" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">The adaptive lasso and its oracle properties</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>