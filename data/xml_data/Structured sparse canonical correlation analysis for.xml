
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured sparse canonical correlation analysis for brain imaging genetics: an improved GraphNet method Downloaded from</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Lei</forename>
								<surname>Du</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Heng</forename>
								<surname>Huang</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">The University of Texas at Arlington</orgName>
								<address>
									<settlement>Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jingwen</forename>
								<surname>Yan</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Sungeun</forename>
								<surname>Kim</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Shannon</forename>
								<forename type="middle">L</forename>
								<surname>Risacher</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Mark</forename>
								<surname>Inlow</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution" key="instit1">Rose-Hulman Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Terre Haute</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jason</forename>
								<forename type="middle">H</forename>
								<surname>Moore</surname>
							</persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Institute for Biomedical Informatics</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Andrew</forename>
								<forename type="middle">J</forename>
								<surname>Saykin</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Li</forename>
								<surname>Shen</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">for the Alzheimer&apos;s Disease Neuroimaging Initiative</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structured sparse canonical correlation analysis for brain imaging genetics: an improved GraphNet method Downloaded from</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Bioinformatics</title>
						<imprint>
							<biblScope unit="volume">32</biblScope>
							<biblScope unit="issue">2016</biblScope>
							<biblScope unit="page" from="1544" to="1551"/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw033</idno>
					<note type="submission">Received on 21 September 2015; revised on 29 December 2015; accepted on 16 January 2016 Advance Access Publication Date: 21 January 2016</note>
					<note>Bioimage informatics *To whom correspondence should be addressed. Associate Editor: Robert Murphy † Data used in preparation of this article were obtained from the Alzheimer&apos;s Disease Neuroimaging Initiative (ADNI) data-base (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investi-gators can be found at: Availability and implementation: The Matlab code and sample data are freely available at http:// www.iu.edu/$shenlab/tools/angscca/. Contact: shenli@iu.edu Supplementary information: Supplementary data are available at Bioinformatics online. 1544 Original Paper at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Structured sparse canonical correlation analysis (SCCA) models have been used to identify imaging genetic associations. These models either use group lasso or graph-guided fused lasso to conduct feature selection and feature grouping simultaneously. The group lasso based methods require prior knowledge to define the groups, which limits the capability when prior knowledge is incomplete or unavailable. The graph-guided methods overcome this drawback by using the sample correlation to define the constraint. However, they are sensitive to the sign of the sample correlation, which could introduce undesirable bias if the sign is wrongly estimated. Results: We introduce a novel SCCA model with a new penalty, and develop an efficient optimization algorithm. Our method has a strong upper bound for the grouping effect for both positively and negatively correlated features. We show that our method performs better than or equally to three competing SCCA models on both synthetic and real data. In particular, our method identifies stronger canonical correlations and better canonical loading patterns, showing its promise for revealing interesting imaging genetic associations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sparse canonical correlation analysis (SCCA) (<ref type="bibr" target="#b8">Chen and Liu, 2012;</ref><ref type="bibr" target="#b8">Chen et al., 2012;</ref><ref type="bibr" target="#b10">Du et al., 2014;</ref><ref type="bibr" target="#b14">Lin et al., 2013;</ref><ref type="bibr" target="#b15">Parkhomenko et al., 2009;</ref><ref type="bibr" target="#b18">Witten et al., 2009</ref>), is a powerful bi-multivariate analysis technique (<ref type="bibr" target="#b17">Vounou et al., 2010</ref>). It has recently become a popular method in brain imaging genetics studies to identify bi-multivariate associations between single nucleotide polymorphisms (SNPs) and imaging quantitative traits (QTs). SCCA was initially proposed by<ref type="bibr" target="#b18">Witten et al. (2009)</ref>and Witten and<ref type="bibr" target="#b18">Tibshirani (2009)</ref>in the analysis of gene expression data. This first SCCA model introduced the ' 1-norm (lasso) term into the traditional CCA model to make both canonical loadings sparse. The penalized matrix decomposition (PMD) technique was used to solve this sparse learning problem. For a group of correlated features, lasso tends to randomly select only one feature from the group, and often cannot recover all the relevant and correlated features.<ref type="bibr" target="#b18">Witten et al. (2009)</ref>and Witten and Tibshirani (2009) also proposed the fused lasso based SCCA, which takes into account the spatial correlation among features. Thus, neighboring features tend to be selected together to help discover regional structures. In order to accommodate other types of structures in the data, several structured SCCA methods (<ref type="bibr" target="#b7">Chen et al., 2013;</ref><ref type="bibr" target="#b8">Chen and Liu, 2012;</ref><ref type="bibr" target="#b8">Chen et al., 2012;</ref><ref type="bibr" target="#b10">Du et al., 2014</ref><ref type="bibr" target="#b11">Du et al., , 2015</ref><ref type="bibr" target="#b14">Lin et al., 2013;</ref><ref type="bibr" target="#b18">Witten et al., 2009;</ref><ref type="bibr" target="#b18">Witten and Tibshirani, 2009;</ref><ref type="bibr" target="#b20">Yan et al., 2014</ref>) arise recently. We group these SCCA methods into two kinds according to their distinct regularization terms. One kind used the group lasso penalty, and the other kind used the graph/networkguided fused lasso penalty to conduct feature selection and feature grouping. The first kind, i.e. the group lasso based SCCA, required prior knowledge to define the group structure.<ref type="bibr" target="#b14">Lin et al. (2013)</ref>incorporated the priori knowledge into the SCCA model with a group lasso regularizer, where the same PMD technique was used to identify non-overlapping group structure.<ref type="bibr" target="#b10">Du et al. (2014)</ref>proposed S2CCA using group lasso, and incorporated both the covariance matrix information and the priori knowledge information to discover group-level bi-multivariate associations. The KG-SCCA (<ref type="bibr" target="#b20">Yan et al., 2014</ref>) was an extension of S2CCA (<ref type="bibr" target="#b10">Du et al., 2014</ref>), which also employed the group lasso to constrain one canonical loading. This type of SCCA methods may not be useful when the biological priori knowledge is incomplete or unavailable. Of note, it is a hard task to provide precise prior knowledge in real biomedical studies. The second kind of structured SCCA methods use graph/ network-guided fused lasso penalties. These methods can perform well on any given priori knowledge. In case the prior knowledge is not available, these methods can also work via using the sample correlation to define the graph/network constraint.<ref type="bibr" target="#b7">Chen et al. (2013)</ref>proposed ssCCA using a graph-guided fused ' 2-norm penalty for one canonical loading of the taxa based on their relationship on a phylogenetic tree.<ref type="bibr" target="#b8">Chen et al. (2012)</ref>proposed a network-guided fused lasso based SCCA which penalized every pair of features by the ' 1-norm of ðu i À u j Þ. It could be viewed as an extension to the fused lasso based SCCA without demanding the features being ordered.<ref type="bibr" target="#b11">Du et al. (2015)</ref>proposed GN-SCCA which penalizes the ' 2-norm of ðu i À u j Þ. These two SCCA methods could only handle the positively correlated features.<ref type="bibr" target="#b8">Chen and Liu (2012)</ref>proposed an improved network-structured SCCA (NS-SCCA) by incorporating the sign of the sample correlation within features. NS-SCCA penalized the ' 1-norm of ðu i À signðq ij Þu j Þ to tune a similar weight value for u i and u j if q ij &gt; 0, or dissimilar if q ij &lt; 0. The aforementioned KG-SCCA (<ref type="bibr" target="#b20">Yan et al., 2014</ref>) employed ' 2-norm of ðu i À signðq ij Þu j Þ on one canonical loading. Most of these SCCA methods used the data-driven correlation as the network constraint, while some incorporated prior knowledge to define the network constraint (<ref type="bibr" target="#b8">Chen and Liu, 2012;</ref><ref type="bibr" target="#b20">Yan et al., 2014</ref>). In the data-driven mode, they were dependent on the sign of the pairwise sample correlation to identify the hidden structure pattern. Unfortunately, this can introduce additional estimation bias since the sign of the correlations can be wrongly estimated due to possible graph/network misspecification caused by noise (<ref type="bibr" target="#b21">Yang et al., 2012</ref>). We focus on the data-driven mode in this paper. We first propose a novel structured penalty using the pairwise difference of absolute values between features, which is an improved GraphNet penalty (<ref type="bibr" target="#b12">Grosenick et al., 2013</ref>). Then we introduce our novel structured SCCA model coupled with an effective SCCA algorithm, i.e. SCCA using the absolute value based GraphNet (AGN-SCCA). Our contributions are summarized as follows.</p><p>(i) The new regularizer penalizes the difference between the absolute values of the coefficients no matter whether their correlations are positive or negative. Thus it could tune both positively and negatively correlated features to have similar weights despite the correlation signs. (ii) AGN-SCCA could reduce estimation bias due to its independence to the signs of sample correlation, and thus has better performance and generalization ability than those methods dependent on sample correlation signs. (iii) We provide a quantitative upper bound for the grouping effect of AGN-SCCA and prove that the algorithm is guaranteed to converge fast. (iv) On both synthetic and real imaging genetic data, AGN-SCCA yields higher or comparable correlation coefficients, and generates more accurate and cleaner patterns than three competing methods, i.e. L1-SCCA (CCA with lasso) (<ref type="bibr" target="#b18">Witten et al., 2009;</ref><ref type="bibr" target="#b18">Witten and Tibshirani, 2009</ref>) FLSCCA (CCA with fused lasso) (<ref type="bibr" target="#b18">Witten et al., 2009;</ref><ref type="bibr" target="#b18">Witten and Tibshirani, 2009</ref>) and NS-SCCA (<ref type="bibr" target="#b8">Chen and Liu, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this paper, we use the boldface lowercase letter to denote a vector, and use the boldface uppercase one to denote a matrix. m i represents the ith row of matrix M. We use X ¼ fx 1 ;. .. ; x n g R p and Y ¼ fy 1 ;. .. ; y n g R q to denote the SNP data and the QT data originating from the same population. The SCCA model proposed in (<ref type="bibr" target="#b18">Witten et al., 2009;</ref><ref type="bibr" target="#b18">Witten and Tibshirani, 2009</ref>) can be defined as follows:</p><formula>min u;v Àu T X T Yv (1)</formula><formula>st jjujj 2 2 1; jjvjj 2 2 1; jjujj 1 c 1 ; jjvjj 1 c 2 ;</formula><p>where jjujj 1 c 1 and jjvjj 1 c 2 are constraints for controlling the model sparsity, and typical constraints include lasso (<ref type="bibr" target="#b8">Chen et al., 2012;</ref><ref type="bibr" target="#b15">Parkhomenko et al., 2009;</ref><ref type="bibr" target="#b18">Witten et al., 2009;</ref><ref type="bibr" target="#b18">Witten and Tibshirani, 2009</ref>) and fused lasso (<ref type="bibr" target="#b18">Witten et al., 2009;</ref><ref type="bibr" target="#b18">Witten and Tibshirani, 2009</ref>).<ref type="bibr" target="#b12">Grosenick et al. (2013)</ref>have extended the traditional elastic net regularizer to a more general form, which is named GraphNet, i.e. jjujj GN ¼ k 1 u T Mu þ b 1 jjujj 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The new penalty</head><formula>(2)</formula><p>where M is a matrix, and ðk 1 ; b 1 Þ are tuning parameters. Note that GraphNet becomes the elastic net if M ¼ I (<ref type="bibr" target="#b12">Grosenick et al., 2013</ref>). Typical GraphNet studies (<ref type="bibr" target="#b11">Du et al., 2015;</ref><ref type="bibr" target="#b12">Grosenick et al., 2013</ref>) have M ¼ L, where L is the Laplacian matrix of a graph. Let G be the graph formed by our sample correlation matrix A. Let D be a diagonal degree matrix with the following diagonal entries: Dði; iÞ ¼ P j Aði; jÞ. The Laplacian matrix L is defined as L ¼ D À A (<ref type="bibr" target="#b12">Grosenick et al., 2013</ref>). When M ¼ L, the GraphNet term can be transferred and written as: jjujj GN ¼ k 1 X ði;jÞ2G w i;j ðu i À u j Þ 2 þ b 1 jjujj 1 :</p><formula>(3)</formula><p>It is easy to see that this penalty only puts emphasis on the positively correlated features, and does not take into consideration the negatively correlated features. To address this issue, we introduce a novel penalty which uses the pairwise difference between absolute values instead, i.e. P ðju i j À ju j jÞ 2. SCCA requires two penalties, one for each canonical loading. Thus, we propose the following new penalties: jjujj AGN ¼ k 1 X w i;j ðju i j À ju j jÞ 2 þ b 1 jjujj 1 ; jjvjj AGN ¼ k 2 X w 0 i;j ðjv i j À jv j jÞ 2 þ b 2 jjvjj 1 :</p><formula>(4)</formula><p>where w i;j and w 0 i;j depend on the pairwise sample correlation of X and Y respectively. b 1 jjujj 1 and b 2 jjvjj 1 are used to control the model sparsity. In accordance to the form of GraphNet, we rewrite the penalty and call it absolute value based GraphNet penalty, jjujj AGN ¼ k 1 juj T L 1 juj þ b 1 jjujj 1 ; jjvjj AGN ¼ k 2 jvj T L 2 jvj þ b 2 jjvjj 1 :</p><formula>(5)</formula><p>where L 1 and L 2 are Laplacian matrices of the correlation matrices of X and Y respectively. The main motivations for proposing jjujj AGN are as follows. First, if we have some priori knowledge, e.g. the pathway information about genetic markers, each pairwise penalty encourages ju i j and ju j j to be similar. This makes sure that the two markers have a high probability to be selected together if they are connected on the graph. Second, if the priori knowledge is unavailable, every pairwise term will be imposed to encourage ju i j % ju j j for both positively and negatively correlated features based on the strength of their sample correlations, which will be supported by Theorem 1. Third, genetic (or imaging) markers in the same pathway (or brain circuitry) could play different roles for a specific disease. That is, some markers could be significant, while others could be irrelevant. Therefore, we impose lasso to assure additional sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AGN-SCCA model and proposed algorithm</head><p>By imposing the novel GraphNet penalty into a CCA model, we obtain our AGN-SCCA model.</p><formula>min u;v Àu T X T Yv (6)</formula><formula>st jjXujj 2 2 1; jjYvjj 2 2 1; jjujj AGN c 1 ; jjvjj AGN c 2 :</formula><p>Note that we utilize jjXujj 2 2<ref type="figure" target="#tab_2">2   2</ref>1, which embraces the covariance structure of the data in our model. The strength of this strategy has been demonstrated by our prior S2CCA work (<ref type="bibr" target="#b10">Du et al., 2014</ref>). Using Lagrange multiplier method, we define the Lagrangian L below,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">and jjYvjj</head><formula>Lðu; v; CÞ ¼ Àu T X T Yv þ k 1 2 juj T L 1 juj þ b 1 2 jjujj 1 þ k 2 2 jvj T L 2 jvj þ b 2 2 jjvjj 1 þ c 1 2 jjXujj 2 2 þ c 2 2 jjYvjj 2 2 (7)</formula><p>where C ¼ fk; b; cg f 0 are the Lagrange multipliers, which are also called dual variables.</p><p>According the Lagrange duality, the Lagrangian can represent problem Eq. (6) as the following unconstrained one,</p><formula>p Ã ¼ min u;v max Cf0 Lðu; v; CÞ (8)</formula><p>Now that there is no constraint term in Lagrangian L, it is easy to solve Eq. (8) than Eq. (6). Given the optimal dual variables C Ã , we could obtain the solution by taking derivative regarding u and v respectively, and let both of them be zero. @L @u ¼ 0; @L @v ¼ 0;</p><formula>(9)</formula><p>However, the new proposed penalty is non-differentiable at zero value owning to the ' 1-norm term and the absolute value based GraphNet term. Thus we use the sub-gradient in Eq. (9) instead, and obtain the following (if ju i j ¼ 0, the ith element of diagonal matrix D 1 is not available. So we regularize the element in</p><formula>D 1 as 1 2 ffiffiffiffiffiffiffiffi u 2 i þf p (f is</formula><p>a very small positive number) when ju i j ¼ 0. Then the objective function regarding u is L Ã ðuÞ ¼</p><formula>P p i¼1 ð À u i x T i Yv þ k1 2 ffiffiffiffiffiffiffiffiffiffiffiffiffi u 2 i þ f q L 1 ffiffiffiffiffiffiffiffiffiffiffiffiffi u 2 i þ f q þ b1 2 ffiffiffiffiffiffiffiffiffiffiffiffiffi u 2 i þ f q þ c 1 2 jjx i u i jj 2 2 Þ.</formula><p>It is easy to prove that L Ã ðuÞ will reduce to problem (7) regarding u when f ! 0. Those jv i j ¼ 0 can also be regularized by the same strategy),</p><formula>ðk 1 b D 1 þ b 1 D 1 þ c 1 X T XÞu ¼ X T Yv; (10)</formula><formula>ðk 2 b D 2 þ b 2 D 2 þ c 2 Y T YÞv ¼ Y T Xu; (11)</formula><p>where D 1 is a diagonal matrix with the ith element as 1 2juij ði 2 ½1; pÞ, and D 2 is a diagonal matrix with the jth element as 1 2jvjj ðj 2 ½1; qÞ. b D 1 is a diagonal matrix with the k 1 th element asBoth D 1 and b D 1 depend on u; and both D 2 and b D 2 depend on v. Since u and v are unknown, we propose an effective iterative algorithm called AGN-SCCA to solve this problem. Algorithm 1 shows the pseudocode. In each iteration, the algorithm first fixes v to calculate u and then fixes u to calculate v. This procedure repeats until it converges. Computational analysis. Step 4 and Step 7 are the key steps of Algorithm 1. To assure the efficiency, we solve a system of linear equations with quadratic complexity to update u and v other than computing the matrix inverse with cubic complexity. Step 10 is a simple operation to rescale the results. So, the whole procedure is efficient and runs fast. Moreover, the algorithm is guaranteed to converge, as shown in Theorems 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The grouping effect analysis</head><p>It is important to investigate the grouping effect of the a structured learning method in handling high-dimensional data (<ref type="bibr" target="#b22">Zou and Hastie, 2005</ref>). Although many structured SCCA methods have been proposed and could recover structure pattern practically. None of them provides a theoretical bound for the grouping effect. In this work, we have the following theorem which provides a qualitative theoretical bound in grouping correlated features. THEOREM 1 Given two datasets X and Y, and the pre-tuned parameters ðk; b; cÞ. Let ~ u be the solution to our SCCA problem of Eqs.</p><p>(10) and (11). Without loss of generality, we consider the u i th and u j th feature are only linked to each other on the graph, i.e. e i;j ¼ 1. Let q ij is the sample correlation between them, w i;j is their edge weight. Then the estimated canonical loading u satisfies,</p><formula>j~ u i À ~ u j j 1 c 1 þ 2k 1 w i;j ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 2ð1 À q ij Þ q ; if q ij ! 0; j~ u i þ ~ u j j 1 c 1 þ 2k 1 w i;j ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 2ð1 þ q ij Þ q ; if q ij &lt; 0;</formula><formula>(12)</formula><p>and the estimated canonical loading v satisfies,</p><formula>j~ v i À ~ v j j 1 ðc 2 þ 2k 2 w 0 i;j Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 2ð1 À q 0 ij Þ q ; if q 0 ij ! 0; j~ v i þ ~ v j j 1 ðc 2 þ 2k 2 w 0 i;j Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 2ð1 þ q 0 ij Þ q</formula><p>; if q 0 ij &lt; 0:</p><formula>(13)</formula><p>where w 0 i;j is the weight between the ith and jth feature of v, and q 0 ij is their sample correlation coefficient. The proof of this theorem can be found in Appendix A (See Supplementary File). Theorem 1 not only provides an upper bound for the difference between the canonical loading paths of the ith and jth features when they are positively correlated, but also provides a quantitative description when they are negatively correlated. If q ij ! 0, the higher correlation two features have, the smaller difference there is between their coefficients. While if q ij &lt; 0, a smaller value will generate a closer-to zero value for the sum of their coefficients. This is desirable because AGN-SCCA can estimate the coefficients with equal amplitude except signs for two negatively correlated features. This quantitative description for the grouping effect demonstrates that our novel structured SCCA is suitable for sparse structure learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The convergence analysis</head><p>We have the following theorems regarding the Algorithm 1. THEOREM 2 The problem Eq. (8) is lower bounded by –1. THEOREM 3 In each iteration, the AGN-SCCA algorithm monotonously decreases the objective value till it converges. The proofs are provided in Appendix B and C (See Supplementary File) due to space limitation. Since the objective value keeps deceasing during the iteration, and the problem has the lower bound, the proposed algorithm is guaranteed to converge to a local optimum.</p><p>In our implementation, we set the stopping criterion of Algorithm 1 as maxfjdjjd 2 ðu tþ1 À u t Þg s and maxfjdjjd 2 ðv tþ1 À v t Þg s, where s is a predefined estimation error. In this paper, s ¼ 10 À5 is empirically set based on experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Benchmarks</head><p>We chose three existing SCCA methods for comparison in this study, one is the state-of-the-art method NS-SCCA (network-structured CCA) (<ref type="bibr" target="#b7">Chen et al., 2013</ref>), and the other two methods are the L1SCCA (CCA with lasso) and FL-SCCA (CCA with fused lasso). The latter two can be found in package PMA (the PMA software package implements both L1-SCCA and FL-SCCA, and they are widely used as benchmark algorithms. See http://cran.r-project.org/web/packages/ PMA/for details), which is widely used for SCCA studies. We do not compare our method with KG-SCCA (<ref type="bibr" target="#b20">Yan et al., 2014</ref>) due to two reasons: (i) KG-SCCA uses ' 2;1-norm on one canonical loading (similar to S2CCA), uses ' 2-norm of ðu i À signðq ij Þu j Þ on the other (similar to NS-SCCA), and requires predefined group and network structures.</p><p>(ii) NS-SCCA uses the ' 1-norm of ðu i À signðq ij Þu j Þ, which is supposed to be more reasonable in sparse learning than KG-SCCA since ' 1-norm is a sparse constraint but ' 2-norm is not. Therefore we include NS-SCCA instead of KG-SCCA as a competing method. We also do not compare our method with GN-SCCA (<ref type="bibr" target="#b11">Du et al., 2015</ref>) because it only focuses on the positively correlated features. In addition, ssCCA (<ref type="bibr" target="#b7">Chen et al., 2013</ref>), CCA-SG (CCA-sparse group) (<ref type="bibr" target="#b14">Lin et al., 2013</ref>) and S2CCA (<ref type="bibr" target="#b10">Du et al., 2014</ref>) are opted out, since they are knowledge-guided methods and applicable only when priori knowledge is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Parameter tuning According to Eqs. (10) and</head><p>(11), we have six parameters to be tuned. Obviously, this is very time consuming by blind grid search. Thus we here employ some tricks to speed up the tuning procedure. The major difference between the traditional CCA and SCCA is the penalty terms. On one hand, SCCA and CCA will yield similar results if the parameters are too small. On the other hand, SCCA will overpenalize the result when the parameters are too large. Thus a neither too large nor too small parameter is more reasonable. As a result, we tune these parameters from<ref type="bibr">[</ref>P 5 j¼1 CorrðX j u Àj ; Y j v Àj Þ where X j and Y j denote the jth subset of the input data (testing set), and u Àj and v Àj mean the canonical loadings estimated from the training set. We choose the arg max CVðk; b; cÞ as the tuned optimal parameters. For efficiency purpose, these parameters are only tuned from the first run of the crossvalidation strategy. That is, the parameters are tuned when the first four folds are used as the training set. Then we directly use the tuned parameters for all the remaining experiments. Though this could limit the performance for the rest of the experiments, we find that it will not affect the performance significantly from the results which will be shown later. All these methods utilize the same partition during cross-validation to make a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on synthetic data</head><p>We simulate four different datasets with different properties in this study, and we expect the diversity could make sure a thorough comparison. The true signals and the strengths of correlation coefficients within these data are distinct. As a simulation of a large p small n Algorithm 1. The AGN-SCCA Algorithm Require: X ¼ fx 1 ;. .. ; x n g T ; Y ¼ fy 1 ;. .. ; y n g T Ensure: Canonical loadings u and v. 1: Initialize u 2 R pÂ1 ; v 2 R qÂ1 ; L 1 ¼ D u À A u and L 2 ¼ D v À A v only on the training set; 2: while not converged do 3: while not converged regarding u do 4: Solve u according to Eq. (10) 5: end while 6: while not converged regarding v do 7: Solve v according to Eq. (11) 8: end while 9: end while 10: Scale u so that jjXujj 2 2 ¼ 1, and v so that jjYvjj 2 2 ¼ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An improved GraphNet method</head><p>problem, we here set the number of observations be smaller than the number of features, i.e. n ¼ 80, p ¼ 100 and q ¼ 120. The generation procedure is similar to that in (<ref type="bibr" target="#b8">Chen and Liu, 2012</ref>) except for the last step: (i) We create u and v separately according the predefined structure. (ii) We generate a latent variable z $ Nð0; I nÂn Þ. (iii) We generate X with the entry:</p><formula>x i $ Nðz i u; P x Þ, where ð P x Þ jk ¼ exp ÀjujÀu k j</formula><p>, and Y with the entry:</p><formula>y i $ Nðz i v; P y Þ, where ð P y Þ jk ¼ exp ÀjvjÀv k j .</formula><p>(iv) For the first group of nonzero coefficients in u, we change the first half of their signs. At the same time, we also change the signs of the corresponding data. As a result, we still have X 0 u 0 ¼ Xu where X 0 and u 0 are the data matrix and coefficients after the sign swap. Note that these synthetic data are order-independent, and thus this setup is equivalent to randomly change a portion of signs for coefficients u (<ref type="bibr" target="#b21">Yang et al., 2012</ref>). For the Y side, we do the same. The details of the four datasets are as follow.</p><p>(i) The first two datasets have the same signal structure, i.e. the same group structure regarding u and v. But their correlation coefficients are different. The correlation coefficient of the first dataset is 0.52, while that of the second dataset is 0.17. (ii) The third dataset is different from the first two datasets in its group structure regarding u and v. Its correlation coefficient is 0.58. (iii) The fourth dataset is different from all the above three datasets in its group structure regarding u and v. Its correlation coefficient is 0.51. The true signal of each dataset can be observed from the first row in<ref type="figure">Figure 1</ref>. In<ref type="figure" target="#tab_1">Table 1</ref>, we present the estimated correlation coefficients from both training and testing data, and their differences from the true correlation coefficients (i.e. the numbers in parentheses). We use the boldface to highlight the highest value as well as those that are not significantly smaller than the highest value. For the training set, we observe that our method obtains the best correlation coefficients on Dataset 2 and Dataset 3, and it is only slightly smaller than the best method on the rest of the two datasets. Though AGNSCCA does not obtain the highest for every dataset, it is not statistically different from the best method. If we consider the true correlation coefficients, we observe that AGN-SCCA and L1-SCCA are two methods which have smaller estimation errors. That is, both AGN-SCCA and L1-SCCA identify more accurate correlation coefficients than FL-SCCA and NS-SCCA regarding the training results. For the testing set, AGN-SCCA outperformed the competing methods on Dataset 3, and was not significantly different from the best method on the remaining datasets. Besides, AGN-SCCA's estimation error is the smallest for Datasets 2–4, which means it performs better than the competing methods regarding the prediction performance. Generally, this is more interesting since the testing performance is more important than the training results. These results show that AGN-SCCA either outperforms or performs similarly to those competing methods in terms of estimated correlation coefficients. We show the estimated canonical loadings of four SCCA methods in<ref type="figure">Figure 1</ref>. As we can see, none of these methods could generate stable results for the small-n-large-p problem when using crossvalidation strategy. They still exhibit group structures for the estimated canonical loadings. However, neither L1-SCCA nor FL-SCCA can accurately recover the true signals. They cannot identify those coefficients with signs swapped. Thus they treat the positively and negatively correlated features with no difference. NSSCCA and AGN-SCCA successfully recognize the coefficients whose signs are changed. The reason is that AGN-SCCA uses the absolute difference between the coefficients, and NS-SCCA takes advantage of the sign of sample correlation. Note the sign of sample correlation depends on the signal-to-noise ratio (SNR), and it is likely to be wrong due to a high proportion of noise. Therefore, for the three datasets with high correlations (Dataset 1, Dataset 3 and Dataset 4), NS-SCCA could exhibit a similar pattern to AGN-SCCA with respect to the canonical loadings. While for the second dataset whose correlation is small, AGN-SCCA outperforms NS-SCCA in terms of the structure pattern, especially for the canonical loading v. In order to make this clear, we also calculate the AUC (area under ROC curve) to present the performance regarding the canonical loadings pattern in<ref type="figure" target="#tab_2">Table 2</ref>with those best values marked in bold. We observe that both structured SCCA, i.e. AGN-SCCA and NS-SCCA, perform consistently better than L1-SCCA and FL-SCCA. Our AGN-SCCA obtains the best scores in most runs except on few folds, especially for the canonical loadings v. In summary, the AGN-SCCA not only estimates the most accurate correlation coefficients in most cases, but also identifies the signal locations with the best accuracy in all the cases. These promising results reveal that our method outperforms the competing methods, showing that it can handle a range of synthetic datasets with distinct structures and correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on real neuroimaging genetics data</head><p>Apart from the synthetic data, it is essential to evaluate our method on real neuroimaging genetics data. Real imaging genetics data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). For up-to-date information, see www.adni-info.org. The genotyping and baseline amyloid imaging data (preprocessed<ref type="bibr">[11C]</ref>Florbetapir PET scans) of 283 non-Hispanic Caucasian participants were downloaded from the ADNI website (adni.loni.usc.edu). The amyloid imaging data were preprocessed according to the steps in (<ref type="bibr" target="#b20">Yan et al., 2014</ref>), and then pre-adjusted by regressing out the effects of the baseline age, gender, education and handedness. Using the voxel-based imaging data, we extracted 191 ROI level mean amyloid measurements, where the ROIs were defined by MarsBaR AAL atlas. For the genotyping data, we included 58 SNP markers within the APOE gene, including the APOE e4 SNP rs429358 (i.e. the best-known AD genetic risk factor)<ref type="figure">Fig. 1</ref>. Canonical loadings estimated on four synthetic datasets. The first column is for Dataset 1, and the second column is for Dataset2, and so forth. For each dataset, the estimated weight of u is shown on the left figure, and v is on the right. The first row is the ground truth, and each remaining one corresponds to a method: (1) Ground Truth.</p><formula>ANG-SCCA u u u u v v v v-5 0 5x10-3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An improved GraphNet method</head><p>(<ref type="bibr" target="#b16">Ramanan et al., 2014</ref>). We aim to evaluate the associations between the amyloid data and the APOE SNP data using the proposed method. In<ref type="figure">Table 3</ref>, we present the correlation coefficients estimated by the four different SCCA methods via 5-fold cross-validation strategy. As we can see, AGN-SCCA can not only identify the strongest correlation on the training set, but also outperform those competing methods on the testing set. Although all methods yield acceptable correlation coefficients, AGN-SCCA significantly and consistently outperforms other SCCA methods, demonstrating its capability in identifying strong imaging genetic associations. We also show the canonical loadings estimated from the training set in<ref type="figure" target="#fig_5">Figure 2</ref>using the heat maps. In<ref type="figure" target="#fig_5">Figure 2</ref>, each row refers to a method. The estimated u, containing weights for genetic markers, is shown on the left panel and the estimated v, containing weights for the imaging markers, is shown on the right. For the canonical loading u, AGN-SCCA only identified the APOE e4 SNP rs429358, i.e. the best-known AD genetic risk factor. L1-SCCA and FL-SCCA also discovered the APOE e4 SNP, but reported much more additional SNPs than AGN-SCCA. Thus their results are not as sparse as AGN-SCCA. NS-SCCA also identified many SNPs which is hard to interpret. For the v side, we can observe that FL-SCCA fused the results of L1-SCCA because of its pairwise smoothness penalty. However, their results consists of too many signals, making them hard to interpret. NS-SCCA identified even more signals than FL-SCCA and L1-SCCA due to its pairwise smoothness imposed on the whole graph, which is suboptimal for biomarker discovery. As a result, we could see that AGN-SCCA exhibits a very clean pattern and reports very few relevant imaging signals, including frontal and caudate regions that are known to be related to AD (<ref type="bibr" target="#b13">Jiji et al., 2013</ref>). In short, the proposed AGN-SCCA algorithm successfully discovered a biologically meaningful associations between APOE SNP rs429358 and the amyloid accumulations at the AD related brain regions. This demonstrates that AGN-SCCA can not only reveal strong imaging genetic associations, but also identify meaningful and relevant genetic and imaging markers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have proposed a novel structured regularization term using the pairwise difference between absolute values of two weights, and incorporated it into a SCCA framework. This proposed structured SCCA model, named as AGN-SCCA, aims to discover any group or network structure laying behind the data. We have demonstrated that AGN-SCCA has strong upper bound of grouping effect, and have developed an iterative procedure with proven convergence. The existing structured SCCA methods either use the group lasso (<ref type="bibr" target="#b10">Du et al., 2014;</ref><ref type="bibr" target="#b14">Lin et al., 2013;</ref><ref type="bibr" target="#b20">Yan et al., 2014</ref>) or the graph/netwrok-guided fused lasso (<ref type="bibr" target="#b7">Chen et al., 2013;</ref><ref type="bibr" target="#b8">Chen and Liu, 2012;</ref><ref type="bibr" target="#b8">Chen et al., 2012;</ref><ref type="bibr" target="#b11">Du et al., 2015;</ref><ref type="bibr" target="#b20">Yan et al., 2014</ref>) to model the structure information. The first type of methods rely on prior knowledge to define the group structure, and the prior knowledge is sometimes unavailable in real applications. The latter type of methods can perform well on any given priori knowledge. In case the prior knowledge is not available, these methods can also work via using the sample correlation to define the graph/network constraint. However, they depend on the sign of sample correlation being defined in advance, which could be wrongly estimated due to possible graph/network misspecification caused by noise (<ref type="bibr" target="#b21">Yang et al., 2012</ref>). Our proposed SCCA is different from those previously published ones in the following aspects: (i) AGN-SCCA employs a novel absolute value based GraphNet penalty, and it does not require to estimate the sign of sample correlation. (ii) The AGN-SCCA could tune positively correlated features as well as negatively correlated ones to have similar weights despite the correlation signs. (iii) AGN-SCCA has a strong theoretical upper bound for the grouping effect, and the corresponding algorithm is guaranteed to converge fast. We have compared AGN-SCCA with three competing SCCA methods with different penalty functions, including L1-SCCA, FLSCCA and NS-SCCA, using both synthetic data and real imaging genetics data. The experimental results demonstrate the following:</p><p>(i) For the estimated correlation coefficients, AGN-SCCA obtained the best or comparable results on the synthetic data, and significantly outperformed the competing methods on the real data. (ii) For the estimated canonical loadings, AGN-SCCA yielded better canonical loading pattern on both synthetic data and real data, especially on the real data where it produced much cleaner patterns than the competing methods. By discovering a strong association between the APOE SNP data and the amyloid accumulation data in an AD study, AGN-SCCA demonstrated itself as a promising structured SCCA method. The theoretical convergence and upper bound of the grouping effect further reveal that AGN-SCCA is of efficiency and effectiveness in identifying meaningful bi-multivariate associations in brain imaging genetics studies. In this work, we only tested AGNThe best value and those that are NOT significantly worse (t-test with p-value smaller than 0.05) are shown in boldface.SCCA while using data-driven covariance structure as the graph/network constraint. In the future, we will apply the AGN-SCCA model to more general cases and test its performance when priori knowledge is available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>the k 2 th row of the Laplacian matrix L 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>10 À2 ; 10 À1 , 10 , 10 1 , 10 2 ]. All the parameters are tuned through the nested 5-fold cross-validation CVðk; b; cÞ ¼ 1 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Fig. 1. Canonical loadings estimated on four synthetic datasets. The first column is for Dataset 1, and the second column is for Dataset2, and so forth. For each dataset, the estimated weight of u is shown on the left figure, and v is on the right. The first row is the ground truth, and each remaining one corresponds to a method: (1) Ground Truth. (2) L1-SCCA. (3) FL-SCCA. (4) NS-SCCA. (5) AGN-SCCA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.2.</head><figDesc>Fig. 2. Canonical loadings estimated on real imaging genetics dataset. Each row corresponds to an SCCA method: (1) L1-SCCA. (2) FL-SCCA. (3) NSSCCA. (4) AGN-SCCA. For each method, the estimated weights of u are shown on the left panel, and those of v are on the right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Table 2.</figDesc><table>The AUC (area under the curve) of estimated canonical loadings on synthetic data: the AUC of each individual fold and their MEAN are shown 

Methods 

Dataset 1 (cc 

¼ 0.52) 

MEAN Dataset 2 (cc 

¼ 0.17) 

MEAN Dataset 3 (cc 

¼ 0.58) 

MEAN Dataset 4 (cc 

¼ 0.51) 

MEAN 

Estimated Canonical Loading u 
L1-SCCA 
1.00 1.00 1.00 1.00 1.00 

1.00 

1.00 

0.00 

1.00 

1.00 

1.00 

0.80 
0.83 0.83 0.83 0.83 0.83 

0.83 
0.83 0.83 0.83 0.83 0.83 

0.83 

FL-SCCA 
1.00 1.00 1.00 1.00 1.00 

1.00 
NaN NaN NaN NaN NaN 

NaN 
1.00 1.00 1.00 1.00 1.00 

1.00 
1.00 0.83 1.00 1.00 1.00 

0.97 

NS-SCCA 
1.00 1.00 1.00 1.00 1.00 

1.00 

1.00 

0.00 

1.00 

1.00 

1.00 

0.80 
1.00 1.00 1.00 1.00 1.00 

1.00 
1.00 0.98 1.00 1.00 1.00 

1.00 

AGN-SCCA 1.00 1.00 1.00 1.00 1.00 

1.00 

1.00 

1.00 

1.00 

1.00 

1.00 

1.00 
1.00 1.00 1.00 1.00 1.00 

1.00 
0.67 1.00 1.00 1.00 1.00 

0.93 

Estimated Canonical Loading v 
L1-SCCA 
0.83 0.33 0.83 0.83 0.83 

0.73 

0.33 

0.00 

0.00 

0.33 

0.00 

0.13 
0.83 0.83 0.83 0.83 0.83 

0.83 
1.00 1.00 1.00 1.00 1.00 

1.00 

FL-SCCA 
0.67 0.63 0.67 0.67 0.67 

0.66 
NaN NaN NaN NaN NaN 

NaN 
0.67 0.67 0.67 0.83 0.67 

0.70 
1.00 1.00 1.00 1.00 1.00 

1.00 

NS-SCCA 
0.67 0.67 0.33 0.67 0.67 

0.60 

0.33 

0.00 

0.00 

0.28 

0.00 

0.12 
0.96 1.00 1.00 1.00 1.00 

0.99 
1.00 1.00 1.00 1.00 1.00 

1.00 

AGN-SCCA 1.00 0.72 0.59 0.67 1.00 

0.80 

0.33 

0.00 

0.33 

0.67 

0.00 

0.27 
0.67 1.00 1.00 1.00 1.00 

0.93 
1.00 1.00 1.00 1.00 1.00 

1.00 

'NaN' means the AUC is not available. The best MEAN values of each fold are shown in boldface. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>5-fold cross-validation results on synthetic data: the estimated correlation coefficients of each individual fold and their MEAN are shown 
Methods 
Dataset 1 (cc 

¼ 0.52) 

MEAN 

Dataset 2 (cc 

¼ 0.17) 

MEAN 

Dataset 3 (cc 

¼ 0.58) 

MEAN 

Dataset 4 (cc 

¼ 0.51) 

MEAN 

AVG. Error 

Training results 
L1-SCCA 
0.51 0.56 0.50 0.47 0.59 0.53 (þ0.01) 0.25 0.12 0.26 0.18 0.25 0.21 (0.04) 0.51 0.51 0.53 0.51 0.56 0.52 (-0.06) 0.44 0.49 0.46 0.49 0.51 0.48 (-0.03) 

0.03 

FL-SCCA 
0.50 0.56 0.49 0.47 0.59 0.52 (0.00) 

NaN NaN NaN NaN NaN 

NaN 
0.51 0.53 0.56 0.59 0.56 0.55 (-0.03) 0.50 0.54 0.47 0.55 0.54 0.52 (0.05) 

0.05 

NS-SCCA 
0.38 0.45 0.33 0.37 0.48 0.40 (-0.12) 0.23 0.14 0.22 0.16 0.21 0.19 (0.02) 0.32 0.36 0.39 0.47 0.40 0.39 (-0.19) 0.28 0.48 0.45 0.38 0.41 0.40 (-0.11) 

0.11 

AGN-SCCA 0.61 0.63 0.13 0.55 0.64 0.51 (-0.01) 0.27 0.16 0.33 0.21 0.26 0.25 (0.08) 0.47 0.58 0.60 0.56 0.60 0.56 (-0.02) 0.45 0.54 0.46 0.55 0.53 0.51 (0.00) 

0.03 

Testing Results 
L1-SCCA 
0.57 0.16 0.65 0.71 0.12 0.44 (-0.08) 0.39 0.40 0.13 0.26 0.02 0.24 (0.07) 0.57 0.57 0.55 0.60 0.40 0.54 (-0.04) 0.58 0.40 0.71 0.32 0.14 0.43 (-0.08) 

0.07 

FL-SCCA 
0.57 0.16 0.66 0.70 0.16 0.24 (-0.28) NaN NaN NaN NaN NaN 

NaN 
0.58 0.58 0.50 0.31 0.45 0.48 (-0.10) 0.63 0.42 0.79 0.40 0.30 0.51 (0.00) 

0.14 

NS-SCCA 
0.49 0.15 0.16 0.70 0.39 0.48 (-0.04) 0.01 0.32 0.57 0.22 0.02 0.23 (0.06) 0.42 0.15 0.32 0.48 0.30 0.33 (-0.25) 0.01 0.14 0.78 0.38 0.19 0.30 (-0.21) 

0.14 

AGN-SCCA 0.60 0.24 0.31 0.74 0.32 0.44 (-0.08) 0.25 0.07 0.18 0.28 0.01 0.16 (-0.01) 0.73 0.60 0.45 0.50 0.56 0.57 (-0.01) 0.58 0.26 0.77 0.39 0.19 0.44 (-0.07) 

0.04 

The difference (numbers in parentheses) between the estimated correlation coefficients and the true ones, and their average estimated error area also shown. 'NaN' means a method fails to estimate a pair of canonical load-
ings. '0.00' means a very small value. The highest values and those that are NOT significantly worse (t-test with p-value smaller than 0.05) are shown in boldface. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 3. 5-fold cross-validation results on real data: the estimated correlation coefficients of each individual fold and their MEAN are shown</figDesc><table>Methods 
Training results 
MEAN 
Testing results 
MEAN 

L1-SCCA 
0.57 
0.56 
0.56 
0.56 
0.54 
0.56 
0.46 
0.54 
0.53 
0.49 
0.63 
0.53 
FL-SCCA 
0.51 
0.48 
0.50 
0.50 
0.48 
0.49 
0.38 
0.51 
0.45 
0.44 
0.56 
0.47 
NS-SCCA 
0.53 
0.50 
0.53 
0.51 
0.50 
0.52 
0.41 
0.42 
0.37 
0.42 
0.62 
0.45 
AGN-SCCA 
0.61 
0.59 
0.59 
0.59 
0.58 
0.59 
0.48 
0.59 
0.57 
0.52 
0.65 
0.56 

</table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">L.Du et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">Department of Defense award number W81XWH-12-2-0012) ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer&apos;s Association; Alzheimer&apos;s Drug Discovery Foundation</title>
		<author>
			<persName>
				<forename type="first">U01</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Dod</forename>
				<surname>Adni</surname>
			</persName>
		</author>
		<imprint>
			<publisher>Araclon Biotech</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company</title>
		<imprint>
			<publisher>CereSpir, Inc</publisher>
			<publisher>CereSpir, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech</title>
		<author>
			<persName>
				<forename type="first">Eisai</forename>
				<surname>Inc</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">Johnson &amp; Johnson Pharmaceutical Research &amp; Development LLC.; Lumosity; Lundbeck</title>
		<author>
			<persName>
				<surname>Fujirebio</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Healthcare</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Ixico Ltd</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">&amp;</forename>
				<surname>Janssen Alzheimer Immunotherapy Research</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Llc</forename>
				<surname>Development</surname>
			</persName>
		</author>
		<imprint>
			<publisher>Merck &amp; Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title level="m" type="main">NeuroRx Research Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www. fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer&apos;s Disease Cooperative Study at the University of California</title>
		<author>
			<persName>
				<forename type="first">Llc</forename>
				<surname>Meso Scale Diagnostics</surname>
			</persName>
		</author>
		<imprint>
			<pubPlace>San Diego</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Funding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">R01 AG 042437 to P.C., and R01 AG046171 to R.K.], the National Science Foundation [IIS-1117335 to L.S.], the United States Department of Defense [W81XWH-14-2-0151 to T At University of Texas at Arlington, this work was supported by the National Science Foundation</title>
		<author>
			<persName>
				<forename type="first">R01</forename>
				<surname>Lm011360</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">S</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J S</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">W</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">W</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J S</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J S</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">S</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">this work was supported by the National Institutes of Health to M.W., and W81XWH-12-2-0012 to M.W.], and the National Collegiate Athletic AssociationCCF-0830780 to H.H., CCF-0917274 to H.H., DMS-0915228 to H.H., and IIS-1117965 to H.H.]. At University of Pennsylvania, the work was supported by the National Institutes of Health [R01 LM011360 to J.M., R01 LM009012 to J.M., and R01 LM010098 to J.M.]. Conflict of Interest: none declared</title>
		<imprint>
			<biblScope unit="page" from="81" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Structure-constrained sparse canonical correlation analysis with an application to microbiome data analysis</title>
		<author>
			<persName>
				<forename type="first">References</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="244" to="258" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">An efficient optimization algorithm for structured sparse cca, with applications to eqtl mapping</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Biosci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">Structured sparse canonical correlation analysis</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">A Novel structure-Aware Sparse Learning Algorithm for</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Du</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Imaging Genetics. MICCAI</title>
		<imprint>
			<biblScope unit="page" from="329" to="336" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Gn-scca: Graphnet based sparse canonical correlation analysis for brain imaging genetics</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Du</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Informatics and Health</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="275" to="284" />
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpretable whole-brain prediction analysis with graphnet</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Grosenick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="304" to="321" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation and volumetric analysis of the caudate nucleus in alzheimer&apos;s disease</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Jiji</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1525" to="1530" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Correspondence between fMRI and SNP data by group sparse canonical correlation analysis</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="891" to="902" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse canonical correlation analysis with application to genomic data integration</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Parkhomenko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">APOE and BCHE as modulators of cerebral amyloid deposition: a florbetapir pet genome-wide association study</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">K</forename>
				<surname>Ramanan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="351" to="357" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering genetic associations with high-dimensional neuroimaging phenotypes: a sparse reduced-rank regression approach</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Vounou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1147" to="1159" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Extensions of sparse canonical correlation analysis with applications to genomic data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Witten</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Witten</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="515" to="534" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Transcriptome-guided amyloid imaging genetic analysis via a novel structured sparse learning algorithm</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="564" to="571" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature grouping and selection over an undirected graph</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="922" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc.: Ser. B (Stat. Methodol.)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<monogr>
		<title level="m" type="main">An improved GraphNet method</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>