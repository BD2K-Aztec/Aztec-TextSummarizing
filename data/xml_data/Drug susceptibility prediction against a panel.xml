
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drug susceptibility prediction against a panel of drugs using kernelized Bayesian multitask learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">G</forename>
								<forename type="middle">€</forename>
								<surname>Mehmet</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution">Sage Bionetworks</orgName>
								<address>
									<postCode>98109</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<surname>Onen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution">Sage Bionetworks</orgName>
								<address>
									<postCode>98109</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Adam</forename>
								<forename type="middle">A</forename>
								<surname>Margolin</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution">Sage Bionetworks</orgName>
								<address>
									<postCode>98109</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Drug susceptibility prediction against a panel of drugs using kernelized Bayesian multitask learning</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="page" from="556" to="563"/>
							<date type="published" when="2014">2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu464</idno>
					<note>BIOINFORMATICS Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Human immunodeficiency virus (HIV) and cancer require personalized therapies owing to their inherent heterogeneous nature. For both diseases, large-scale pharmacogenomic screens of molecularly characterized samples have been generated with the hope of identifying genetic predictors of drug susceptibility. Thus, computational algorithms capable of inferring robust predictors of drug responses from genomic information are of great practical importance. Most of the existing computational studies that consider drug susceptibility prediction against a panel of drugs formulate a separate learning problem for each drug, which cannot make use of commonalities between subsets of drugs. Results: In this study, we propose to solve the problem of drug susceptibility prediction against a panel of drugs in a multitask learning framework by formulating a novel Bayesian algorithm that combines kernel-based non-linear dimensionality reduction and binary classification (or regression). The main novelty of our method is the joint Bayesian formulation of projecting data points into a shared subspace and learning predictive models for all drugs in this subspace, which helps us to eliminate off-target effects and drug-specific experimental noise. Another novelty of our method is the ability of handling missing phenotype values owing to experimental conditions and quality control reasons. We demonstrate the performance of our algorithm via cross-validation experiments on two benchmark drug susceptibility datasets of HIV and cancer. Our method obtains statistically significantly better predictive performance on most of the drugs compared with baseline single-task algorithms that learn drug-specific models. These results show that predicting drug susceptibility against a panel of drugs simultaneously within a multitask learning framework improves overall predictive performance over single-task learning approaches. Availability and implementation: Our Matlab implementations for binary classification and regression are available at https://github. com/</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human immunodeficiency virus (HIV) and cancer, which are two major human diseases causing millions of deaths yearly, require 'personalized therapies' owing to their inherent heterogenous nature. For both diseases, large-scale pharmacogenomic screens have been performed with the hope of discovering associations between genetic subtypes of each disease and drug susceptibility (<ref type="bibr" target="#b1">Barretina et al., 2012;</ref><ref type="bibr" target="#b6">Garnett et al., 2012;</ref><ref type="bibr" target="#b16">Rhee et al., 2003</ref>). HIV is usually treated with antiretroviral therapies, which have demonstrated high efficacy. However, the high mutation rate of HIV helps the virus adapt fast, leading to drug-resistant viral strains. Thus, selecting the optimal therapeutic regimen for a given HIV strain requires the ability to predict drug resistance based on its genomic sequence. To enable this type of discovery,<ref type="bibr" target="#b16">Rhee et al. (2003)</ref>characterize the susceptibility of 41000 genomically sequenced HIV strains to subsets of multiple HIV therapeutic agents. Cancer is a collection of genetically diverse diseases, and many modern cancer therapeutics have demonstrated selective efficacy in specific matched genetic subtypes (<ref type="bibr" target="#b5">Druker et al., 2001</ref>). Thus, patient selection strategies for personalized cancer therapeutics require the ability to predict drug sensitivity based on molecular information about a patient's tumor. For this purpose,<ref type="bibr" target="#b1">Barretina et al. (2012) and</ref><ref type="bibr" target="#b6">Garnett et al. (2012)</ref>characterize the sensitivity of 4500 molecularly profiled cancer cell lines to 24 and 138 anticancer compounds, respectively. For both HIV and cancer, researchers have developed genomic predictors of drug susceptibility using modern machine learning techniques for high-dimensional classification or regression. For example,<ref type="bibr" target="#b17">Rhee et al. (2006)</ref>use machine learning algorithms such as decision trees, artificial neural networks, support vector machines, least squares regression and least angle regression to predict drug resistance in HIV type 1 (HIV-1) using the sequence of the viral reverse transcriptase.<ref type="bibr" target="#b1">Barretina et al. (2012) and</ref><ref type="bibr" target="#b6">Garnett et al. (2012)</ref>use a regularized regression method (elastic net) to predict drug sensitivities based on cancer cell line molecular profiles, and<ref type="bibr" target="#b13">Neto et al. (2014)</ref>formulate a Bayesian extension of this approach in a recent study.<ref type="bibr" target="#b12">Menden et al. (2013)</ref>combine genomic features of cell lines and chemical features of drugs for sensitivity prediction using a neural network approach.<ref type="bibr" target="#b10">Jang et al. (2014) and</ref><ref type="bibr" target="#b14">Papillon-Cavanagh et al. (2013)</ref>compare the performance of various machine learning methods applied to the cancer cell line datasets. One potential limitation of these approaches is the formulation of a separate learning task for each drug. In particular, because each pharmacogenomic screen profiles multiple drugs with similar mechanisms of action, leveraging information across multiple related drugs may yield improved model robustness by reducing the impact of 'off-target effects' and drug-specific experimental noise. Moreover, methods that jointly model sensitivity profiles across multiple drugs may yield insights into groups of drugs effecting similar biological processes or infer mechanisms of action for uncharacterized compounds. For example,<ref type="bibr" target="#b22">Wei et al. (2012)</ref>combine elastic net regression with an expectation *To whom correspondence should be addressed. y Present address: Department of Biomedical Engineering, Oregon Health &amp; Science University, 3303 SW Bond Avenue, Portland, OR 97239, USA ß The Author 2014. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com maximization algorithm to simultaneously cluster groups of similarly behaving compounds and infer a predictive model for each cluster.<ref type="bibr" target="#b8">Heider et al. (2013)</ref>formulate predicting drug resistance against a panel of HIV-1 drugs as a 'multilabel learning' problem (<ref type="bibr" target="#b21">Tsoumakas et al., 2010</ref>), which aims to use all available information by learning models for all drugs simultaneously. They show that this joint modeling approach is better than independent modeling in terms of predictive performance. However, their algorithm has some limitations: (i) It is based on the classifier chains formulation (i.e. training separate predictors for all drugs successively linked along a chain) (<ref type="bibr" target="#b15">Read et al., 2011</ref>), which is not sufficient to capture more complex dependencies between drugs. (ii) It assumes that each data point have the corresponding drug resistance score for all of the drugs considered (i.e. no missing output), which limits the applicability of the proposed method because, in large-scale pharmacogenomic assays, there may be many missing values owing to experimental conditions, quality control reasons, etc. For predicting drug susceptibility against a panel of drugs, we propose a novel Bayesian formulation that combines kernelbased non-linear dimensionality reduction (Sch € olkopf and<ref type="bibr" target="#b18">Smola, 2002</ref>) and binary classification (or regression) in a 'multitask learning' framework (<ref type="bibr" target="#b4">Caruana, 1997</ref>), which tries to solve distinct but related tasks jointly to improve overall generalization performance. Our proposed method, called 'kernelized Bayesian multitask learning' (KBMTL), has two key properties: (i) It maps all data points into a shared subspace and learns predictive models for all drugs simultaneously in this subspace to capture commonalities between the drugs. Joint modeling of drugs enables us to eliminate off-target effects and drug-specific experimental noise, leading to a better predictive performance. (ii) It can handle missing values of drug susceptibility measurements, which enables us not to discard data points with missing outputs, leading to larger data collections. As a result, the obtained predictions become more robust especially for drugs with a large number of missing phenotype values. To show the performance gain of our method over standard modeling approaches, we perform cross-validation experiments on two benchmark drug susceptibility datasets of HIV and cancer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MATERIALS</head><p>In this study, we use two different drug susceptibility datasets, which we extract from the following sources: (i) HIV Drug Resistance Database (HIVDB) (<ref type="bibr" target="#b16">Rhee et al., 2003</ref>), (ii) Genomics of Drug Sensitivity in Cancer (GDSC) (<ref type="bibr" target="#b23">Yang et al., 2013</ref>). These two data sources are publicly available at http://hivdb.stanford.edu and http://www.cancerrxgene.org, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">HIV drug resistance database</head><p>HIVDB contains phenotype and genotype information about HIV-1 (i.e. viral reverse transcriptase sequences with corresponding susceptibility results and amino acid sequences). We extract all reverse transcriptase sequences originated from subtype B strains, which gives us 970 reverse transcriptase sequences in total. We use drug susceptibility results measured using the PhenoSense method for eight nucleoside analogs, namely, Lamivudine (3TC), Abacavir (ABC), Zidovudine (AZT), Stavudine (d4T), Zalcitabine (ddC), Didanosine (ddI), Tenofovir (TDF) and</p><p>Emtricitabine (FTC). Drug susceptibility results are given as fold change in susceptibility (i.e. standardized measure of HIV drug resistance), which is defined as IC 50 ratio = IC 50 of an isolate IC 50 of a standard wild-type control isolate where IC 50 of a resistant or wild-type control isolate gives its half maximal inhibitory concentration. We label reverse transcriptase sequences as 'resistant' or 'susceptible' using drug-specific cutoff values as done similarly in the earlier studies (<ref type="bibr" target="#b8">Heider et al., 2013;</ref><ref type="bibr" target="#b17">Rhee et al., 2006</ref>). The cutoff is set to 1.5 for d4T, ddC, ddI and TDF, and to 3.0 for 3TC, ABC, AZT and FTC. Supplementary<ref type="figure">Figure S1</ref>shows the drug resistance labels and the histogram of available IC 50 ratios for 970 reverse transcriptase sequences. We remove the sequences with no phenotype information (i.e. 48 reverse transcriptase sequences with no IC 50 ratios), leading to a final dataset with 922 reverse transcriptase sequences.<ref type="figure" target="#tab_1">Table 1</ref>summarizes the final dataset by listing the drug name, the corresponding analog, the number of reverse transcriptase sequences with measured IC 50 ratio, the IC 50 ratio cutoff and the ratio between resistant and susceptible classes for each drug. For each reverse transcriptase, genotype information is extracted from the amino acid sequence of positions 1–240. Amino acid differences from the subtype B consensus wild-type sequence are considered as mutations.</p><p>There are 1474 unique mutations for 922 reverse transcriptase sequences in our dataset, which means each reverse transcriptase sequence can be represented as a 1474-dimensional binary vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Genomics of drug sensitivity in cancer</head><p>GDSC contains phenotype and genotype information about cancer (i.e. cancer cell lines with corresponding sensitivity results and genomic profiles). We use drug sensitivity results measured against 138 anticancer drugs, which are given in terms of half maximal inhibitory concentration (IC 50 ) and area under the dose–response curve (AUC) values. We choose to perform our analysis on AUC values because IC 50 values are not observed before the maximum screening concentration for a significant proportion of the drug and cell line pairs (i.e. most of the cell lines are resistant to a given drug within the range of experimental screening concentrations). Supplementary<ref type="figure" target="#fig_1">Figure S2</ref>shows the AUC values and the histogram of available dose–response curves for 790 cancer cell lines. GDSC contains genomic profiles in the forms of copy number variation, gene expression and mutation profiles. We choose to use only gene expression, as it is shown to be the most informative data source in earlier studies (<ref type="bibr" target="#b10">Jang et al., 2014</ref>). Gene expression profile is extracted from hybridized RNA in HT-HGU133A Affymetrix whole genome array.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i557</head><p>Drug susceptibility prediction against a panel of drugs</p><p>There are 12 024 normalized gene expression intensities generated using the MAS5 algorithm (<ref type="bibr" target="#b9">Hubbell et al., 2002</ref>), which means each cell line can be represented as a 12 024-dimensional real-valued vector. We remove the cell lines with no phenotype or genotype information, leading to a final dataset with 664 cell lines and 138 drugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>We consider the problem of predicting susceptibility against a panel of drugs simultaneously for each data point, which is a viral reverse transcriptase for the HIV dataset and a cell line for the cancer dataset. Instead of training drug-specific models separately, we choose to solve this problem with a multitask learning formulation by considering each drug as a distinct task and learning a unified model for all tasks conjointly. We first discuss our proposed method for binary classification (i.e. classifying a data point as resistant or susceptible) in detail and then briefly mention how we extend our method to regression (i.e. predicting real-valued sensitivity measures such as IC 50 or AUC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem formulation</head><p>We assume that there are T related binary classification tasks defined on the domain X. We are given an independent and identically distributed sample X = fx i 2 Xg N i=1. For each task, we are given a label vector y t =fy t;i 2 fÀ1; +1gg i2I t , where I t gives the indices of data points with given class labels in task t. There is a kernel function to define similarities between the data points, i.e. k : X Â X ! R, which is used to calculate the kernel matrix K=fkðx i ; x j Þg N;N i=1;j=1 .<ref type="figure">Figure 1</ref>illustrates the method we propose to learn a conjoint model across the tasks; it is composed of two main parts: (i) projecting data points into a shared subspace using a 'kernel-based dimensionality reduction' model and (ii) performing 'binary classification' in this subspace using the task-specific classification parameters. We first briefly explain these two parts and introduce the notation used. We first perform feature extraction using the input kernel matrix K 2 R NÂN and the projection matrix A 2 R NÂR , where N is the number of data points and R is the subspace dimensionality. When we map the data points into a low dimensional latent subspace using the projection matrix A, we obtain their hidden representations in this shared subspace, i.e. H=A &gt; K. Using a kernel-based formulation has three main implications: (i) We can apply our method to tasks with high dimensional representations such as genomic information and small sample size (i.e. large p, small n). (ii) We can learn better subspaces using non-linear kernels such as the Gaussian kernel (i.e. kernel trick). (iii) We can use domain-specific kernels (e.g. graph and tree kernels for structured objects) to better capture the underlying biological processes (Sch € olkopf et al., 2004). The task-specific classification parts calculate the predicted outputs ff t =H &gt; t w t g T t=1 in the shared subspace using the hidden representations and the task-specific parameters fw t 2 R R g T t=1 , where H t contains only the data points in I t. These predicted outputs are mapped to class labels by looking at their signs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Kernelized Bayesian multitask learning</head><p>We formulate a probabilistic model, called KBMTL, for the method described earlier. We can derive an efficient inference algorithm using variational approximation because our method combines the kernelbased dimensionality reduction and task-specific classification parts with a fully conjugate probabilistic model.<ref type="figure" target="#fig_1">Figure 2</ref>gives the graphical model of KBMTL with hyper-parameters, priors, latent variables and model parameters. As described earlier, the main idea can be summarized as (i) finding hidden representations for the data points by mapping them into a subspace with the help of kernel and projection matrices and (ii) performing binary classification in this shared subspace using the task-specific classification parameters. There are some additions to the notation described earlier: the N Â R matrix of priors for the entries of the projection matrix A is denoted by L. For these priors, f ; g are used as hyper-parameters. The standard deviations for the hidden representations and classification parameters are given as h and w , respectively. As short-hand notations, the hyper-parameters of the model are denoted by =f ; ; h ; w ; g, the priors, latent variables and model parameters by Â=fL; A; H; fw t ; f t g T t=1 g. Dependence on is omitted for clarity throughout the manuscript. The distributional assumptions of the kernel-based dimensionality reduction part are defined as i s $ Gð i s ; ; Þ 8ði; sÞ a i s j i s $ N ða i s ; 0; ð i s Þ À1 Þ 8ði; sÞ h s i ja s ; k i $ N ðh s i ; a &gt; s k i ; 2 h Þ 8ðs; iÞ; where the superscript indexes the rows, and the subscript indexes the columns. N ðÁ; m; SÞ represents the normal distribution with the mean vector m and the covariance matrix S. GðÁ; ; Þ denotes the gamma distribution with the shape parameter and the scale parameter. The binary classification part has the following distributional assumptions:</p><formula>w t;s $ N ðw t;s ; 0; 2 w Þ 8ðt; sÞ f t;i jh i ; w t $ N ðf t;i ; w &gt; t h i ; 1Þ 8ðt; i 2 I t Þ y t;i jf t;i $ ðf t;i y t;i 4Þ 8ðt; i 2 I t Þ;</formula><p>where the predicted outputs ff t g T t=1 , similar to the discriminant outputs in support vector machines, are introduced to make the inference procedures efficient (<ref type="bibr" target="#b0">Albert and Chib, 1993</ref>). The non-negative margin parameter is introduced to resolve the scaling ambiguity and to place a lowdensity region between two classes, similar to the margin idea in support vector machines, which is generally used for semi-supervised learning (<ref type="bibr" target="#b11">Lawrence and Jordan, 2005</ref>). ðÁÞ represents the Kronecker delta function that returns 1 if its argument is true and 0 otherwise. Note that the dimensionality reduction part considers all data points, whereas the binary classification part considers only the data points with given labels in each task, leading to the ability of handling missing values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Inference using variational Bayes</head><p>To obtain an efficient inference mechanism, we formulate a deterministic variational approximation instead of using a Gibbs sampling approach, which is computationally expensive (<ref type="bibr" target="#b7">Gelfand and Smith, 1990</ref>). The variational methods use a lower bound on the marginal likelihood using an ensemble of<ref type="figure">Fig. 1</ref>. Flowchart of KBMTL for binary classification. In the kernelbased dimensionality reduction part, we first calculate the kernel matrix K using the original data matrix X and then find the hidden representation matrix H by projecting the kernel matrix into a subspace using the projection matrix A. In the binary classification part, we first calculate the predicted outputs ff t g T t=1 over the hidden representations using the taskspecific classification parameters fw t g T t=1 and then map these outputs into the class labels fy t g T t=1 i558</p><formula>pðQjK; fy t g T t=1 Þ % qðQÞ=qðLÞqðAÞqðHÞ Y T t=1 ½qðw t Þqðf t Þ</formula><p>and define each factor in the ensemble just like its full conditional distribution:</p><formula>qðLÞ= Y N i=1 Y R s=1 Gð i s ; ð i s Þ; ð i s ÞÞ qðAÞ= Y R s=1 N ða s ; ða s Þ; Sða s ÞÞ qðHÞ= Y N i=1 N ðh i ; ðh i Þ; Sðh i ÞÞ qðw t Þ=N ðw t ; ðw t Þ; Sðw t ÞÞ qðf t Þ= Y i2I t</formula><p>T N ðf t;i ; ðf t;i Þ; Sðf t;i Þ; ðf t;i ÞÞ;</p><p>where ðÁÞ; ðÁÞ; ðÁÞ and SðÁÞ denote the shape parameter, the scale parameter, the mean vector and the covariance matrix for their arguments, respectively. T N ðÁ; m; S; ðÁÞÞ denotes the truncated normal distribution with the mean vector m, the covariance matrix S and the truncation rule ðÁÞ such that T N ðÁ; m; S; ðÁÞÞ / N ðÁ; m; SÞ if ðÁÞ is true and T N ðÁ; m; S; ðÁÞÞ=0 otherwise. We can bound the marginal likelihood using Jensen's inequality: log pðfy t g T t=1 jKÞ ! E qðQÞ ½log pðfy t g T t=1 ; QjKÞ À E qðQÞ ½log qðQÞ and optimize this bound by maximizing with respect to each factor separately until convergence. The approximate posterior distribution of a specific factor can be found as qðÞ / exp ðE qðQnÞ ½log pðfy t g T t=1 ; QjfK t g T t=1 ÞÞ:</p><p>For our proposed model, thanks to the conjugacy, the resulting approximate posterior distribution of each factor follows the same distribution as the corresponding factor. The technical details of our inference mechanism can be found in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prediction scenario</head><p>We can replace pðAjK; fy u g T u=1 Þ with its approximate posterior distribution qðAÞ and obtain the predictive distribution of the latent representation h ! for a new data point x ! as</p><formula>pðh ! jk ! ; K; fy u g T u=1 Þ= Y R s=1 N ðh s ! ; ða s Þ &gt; k ! ; 2 h +k &gt; ! Sða s Þk ! Þ:</formula><p>The predictive distribution of the predicted output f t;! can also be found by replacing pðw t jK; fy u g T u=1 Þ with its approximate posterior distribution qðw t Þ:</p><formula>pðf t;! jh ! ; K; fy u g T u=1 Þ= N f t;! ; ðw t Þ &gt; 1 h ! 2 4 3 5 ; 1+ 1 h ! Â Ã Sðw t Þ 1 h ! 2 4 3 5 0 @ 1 A ;</formula><p>and the predictive distribution of the class label y t;! can be formulated using the predicted output distribution:</p><formula>pðy t;! =+1jf t;! ; K; fy u g T u=1 Þ=Z À1 t;! È ðf t;! Þ Sðf t;! Þ ;</formula><p>where Z t;! is the normalization coefficient calculated for the test data point, and ÈðÁÞ is the standardized normal cumulative distribution function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline algorithms</head><p>To show the practical importance of multitask learning, we compare our method to two baseline algorithms: (i) Bayesian single-task learning and (ii) kernelized Bayesian single-task learning. The technical details for the baseline algorithms can be found in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Bayesian single-task learning Instead of learning a unified</head><p>model for all tasks conjointly, we can train a separate model for each task. For this purpose, we use a Bayesian linear classification algorithm, which is known as 'probit classifier' (<ref type="bibr" target="#b0">Albert and Chib, 1993</ref>). We call this algorithm 'Bayesian probit classifier' (BPROBIT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Kernelized Bayesian single-task learning Instead of training</head><p>a linear model, we can also use a kernelized algorithm to obtain nonlinear models. For this purpose, we use a kernelized Bayesian classification algorithm, which is known as 'relevance vector machine' (<ref type="bibr" target="#b3">Bishop and Tipping, 2000;</ref><ref type="bibr" target="#b20">Tipping, 2001</ref>). We call this algorithm 'Bayesian relevance vector machine' (BRVM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extension to regression problems</head><p>Our method and two baseline algorithms are defined for the binary classification scenario but they can easily be extended to regression problems. The technical details for the regression variant of our method can be found in the Supplementary Material. We explain the regression variant of our method in detail, and the regression variants of baseline algorithms can also be derived similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION</head><p>To illustrate the effectiveness of our proposed KBMTL method, we report its results on two datasets and compare it with two baseline algorithms, namely, BPROBIT and BRVM. We have three main reasons for these particular choices: (i) Both BPROBIT and BRVM use same type of inference mechanism with our method. (ii) BPROBIT is from the family of linear and regularized algorithms, which are considered as the standard approach for drug susceptibility prediction. (iii) We can see the effect of multitask formulation by comparing our method toBRVM, which can also make use of kernel functions for drugspecific models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting and performance metrics</head><p>For each dataset, data points are split into five subsets of roughly equal size. Each subset is then used in turn as the test set, and training is performed on the remaining four subsets. This procedure is repeated 10 times (i.e. 10 replications of 5-fold crossvalidation) to obtain robust results. We use 'area under the receiver operating characteristic curve' (AUROC) to compare classification results. AUROC is used to summarize the receiver operating characteristic curve, which is a curve of true positives as a function of false positives while the threshold to predict labels changes. Larger AUROC values correspond to better performance. We use 'normalized root mean square error' (NRMSE) to compare regression results. NRMSE of drug t can be calculated as</p><formula>NRMSE t = ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ðy t À ^ y t Þ &gt; ðy t À ^ y t Þ ðy t À 11 &gt; y t =N t Þ &gt; ðy t À 11 &gt; y t =N t Þ s ;</formula><p>where y t and ^ y t denote the measured and predicted output vectors, respectively. Smaller NRMSE values correspond to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance comparison on HIVDB</head><p>On HIVDB, we compare three algorithms, namely, BPROBIT, BRVM and KBMTL, in terms of their classification performances. For BPROBIT, the hyper-parameter values are selected as ð ; Þ=ð1; 1Þ; ð ; Þ=ð1; 1Þ and = 1. For BRVM, the hyper-parameter values are selected as ð ; Þ=ð1; 1Þ; ð ; Þ=ð1; 1Þ and = 1. For KBMTL, the hyper-parameter values are selected as ð ; Þ=ð1; 1Þ; h =0:1; w =1 and = 1. The shape and scale hyper-parameters of gamma distributed priors are set to non-informative values not to impose sparsity on the model parameters. The number of components in the hidden representation space is selected as R = 10. For all algorithms, we perform 200 iterations during variational inference. To calculate similarity between reverse transcriptase sequences for BRVM and KBMTL, we use the Gaussian kernel defined as</p><formula>k G ðx i ; x j Þ=exp Àjjx i À x j jj 2 2 =s 2 À Á ,</formula><p>where the kernel width s is chosen among ffiffiffiffiffi</p><formula>15 p ; ffiffiffiffiffi 20 p ; ffiffiffiffiffi 25 p ; ffiffiffiffiffi 30 p and</formula><p>ffiffiffiffiffi 35 p using an internal 5-fold cross-validation scheme on the training set. We decide to make a selection from these particular values because the mean of pairwise Euclidean distances between data points, which is frequently used as the default value for s, is approximately ffiffiffiffiffi 25 p .<ref type="figure" target="#tab_2">Table 2</ref>gives the mean and standard deviation of AUROC values obtained by BPROBIT, BRVM and KBMTL for each drug over 50 replications as their performance measures. We see that KBMTL obtains the highest mean AUROC values for seven of eight HIV-1 drugs by improving the results from 0.5 (3TC) to 2.3% (TDF) compared with the second highest. For FTC, BPROBIT obtains the highest mean AUROC value, whereas KBMTL falls behind by 0.3%. We also report the average AUROC values over drugs in the last row of<ref type="figure" target="#tab_2">Table 2</ref>. We see that KBMTL outperforms BPROBIT and BRVM by 2.1 and 1.7%, respectively.<ref type="figure" target="#fig_11">Figure 3</ref>compares the performance of BPROBIT, BRVM and KBMTL for each drug using box-andwhisker plots. It also compares KBMTL and the best baseline algorithm for each drug using scatterplots. We clearly see that KBMTL is superior to BPROBIT and BRVM on all drugs except FTC. The performance differences obtained by KBMTL over BPROBIT and BRVM on these seven drugs are statistically significant according to the paired t-test with P50.01. The increased performance of KBMTL cannot be explained by the non-linearity introduced owing to the Gaussian kernel alone because BRVM also uses the Gaussian kernel and is able to outperform BPROBIT by only 0.4%. The main reason of this increased performance is the joint modeling of drugs with multitask learning. To illustrate the biological relevance of our method, we analyze the ability to identify drugs with similar mechanisms of action based on hierarchical clustering of drugs based on the task-specific classification parameters inferred by KBMTL. Supplementary<ref type="figure" target="#fig_11">Figure S3</ref>compares the clustering results obtained using KBMTL parameters versus clustering based on similarity of IC 50 ratios. We see that the analogs of Cytidine (3FC and FTC) are clustered together at the bottom level of the dendogram using both IC 50 ratios and KBMTL parameters for correlation calculation. However, the other drugs with the same analog are not clustered together at the bottom level based on IC 50 ratios. If we use the task-specific classification parameters fw t g 8 t=1 found by KBMTL for correlation calculation, hierarchical clustering is able to find three clusters: (i) analogs of Cytidine (3TC and FTC), (ii) analogs of Guanosine (ABC and ddI) and (iii) analogs of Thymidine (AZT and d4T). These results show that KBMTL is able to reveal underlying biological similarities between drugs and to make use of this information to improve predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance comparison on GDSC</head><p>On GDSC, we compare four algorithms, namely, BRVM with the linear kernel (BRVM<ref type="bibr">[L]</ref>), BRVM with the Gaussian kernel (BRVM<ref type="bibr">[G]</ref>), KBMTL with the linear kernel (KBMTL<ref type="bibr">[L]</ref>) and KBMTL with the Gaussian kernel (KBMTL<ref type="bibr">[G]</ref>), in terms ofNote: The best result for each row is marked in bold face if it is statistically significantly better than the others according to the paired t-test with P50.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i560</head><p>M.G € onen and A.A.Margolin their regression performances. For BRVM, the hyper-parameter values are selected as ð ; Þ=ð1; 1Þ; ð ; Þ=ð1; 1Þ and ð ; Þ=ð1; 1Þ. For KBMTL, the hyper-parameter values are selected as ð ; Þ=ð1; 1Þ; ð ; Þ=ð1; 1Þ; h =0:1 and w =1. The number of components in the hidden representation space is selected as R = 100. For all algorithms, we perform 200 iterations during variational inference. The training set is normalized to have zero mean and unit standard deviation, and the test set is then normalized using the mean and the standard deviation of the original training set. To calculate similarity between cell lines for BRVM and KBMTL, we use (i) the linear kernel defined asdrugs) using box-and-whisker and scatterplots. We see that KBMTL<ref type="bibr">[L]</ref>obtains statistically significantly better results than BRVM<ref type="bibr">[L]</ref>in terms of both per-drug and average performances according to the paired t-test with P50.01. This result is also valid when we compare KBMTL<ref type="bibr">[G]</ref>and BRVM<ref type="bibr">[G]</ref>.<ref type="figure" target="#tab_3">Table 3</ref>gives the pairwise comparison results between the four algorithms over 138 per-drug performance values. For example, KBMTL<ref type="bibr">[L]</ref>obtains better predictive performance than BRVM<ref type="bibr">[L]</ref>on 126 of 138 drugs. On 102 of these 126 drugs, KBMTL<ref type="bibr">[L]</ref>is statistically significantly better than BRVM<ref type="bibr">[L]</ref>according to the paired t-test with P50.01. If we sort the algorithms in terms of their predictive performances, we find the following ordering: KBMTL</p><formula>k L ðx i ; x j Þ=x &gt; i x j ,</formula><formula>[G]4KBMTL[L]4BRVM[G]4BRVM[L].</formula><p>These results show that predicting drug sensitivities with a joint model obtains superior predictive performance than using drugspecific models irrespective of the kernel function used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this study, we consider the problem of drug susceptibility prediction based on pharmacogenomic screens against a panel of drugs. In contrast to earlier studies, we choose to solve this problem with a multitask learning formulation by considering each drug as a distinct task and learning a unified model forThe scatterplots give the AUROC values of the best baseline algorithm and KBMTL for 50 replications on the x-and y-axes, respectively. For comparison, blue: KBMTL is better; red: KBMTL is worse i561</p><formula>C B A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 0</formula><formula>C B A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●</formula><formula>C B A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●</formula><formula>C B A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●</formula><formula>C B A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●</formula><formula>C B A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●</formula><formula>C B A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 0</formula><formula>C B A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 0</formula><p>Drug susceptibility prediction against a panel of drugs all tasks conjointly. For this purpose, we propose a novel Bayesian multitask learning algorithm that combines kernelbased non-linear dimensionality reduction and binary classification to classify data points as resistant or susceptible. We formulate a deterministic variational approximation inference scheme, which is more efficient than using a Gibbs sampling approach in terms of computation time. We then extend our algorithm to regression to predict real-valued outputs such as half maximal inhibitory concentration and AUC. The main novelty of our approach comes from the joint Bayesian formulation of projecting data points into a shared subspace and learning predictive models for all drugs in this subspace, which enables us to capture commonalities between subsets of drugs to improve predictive performance. The increased performance is due to elimination of off-target effects and drug-specific experimental noise that may be present in drug susceptibility values. Another novelty of our approach comes from the ability to handle missing drug susceptibility values owing to experimental conditions and quality control reasons, which increases the effective sample size, leading to more robust predictions especially for drugs with a large number of missing phenotype values. To demonstrate the performance of our algorithm, called KBMTL, we perform cross-validation experiments on drug susceptibility datasets of two major human diseases, namely, HIV and cancer. For the HIV dataset, we classify viral reverse transcriptase sequences as resistant or susceptible against eight nucleoside analogs using mutation profiles extracted from sequence information of the viral genotype. Our multitask learning method obtains statistically significantly better results on seven of eight drugs compared with two baseline single-task learning methods that consider each drug separately. For the cancer dataset, we predict AUC within the range of experimental screening concentrations for each cell line against 138 anticancer drugs using gene expression profiles. Our method with the linear or Gaussian kernel obtains statistically significantly better results on 102 or 98 of 138 drugs, respectively, compared to a singletask learning method with the same kernel function. These results show that predicting drug susceptibility against a panel of drugs simultaneously within a multitask learning framework improves overall predictive performance over single-task learning approaches that learn drug-specific models. We implement both single-task and multitask learning methods using efficient variational approximation schemes, where covariance calculations are the most time-consuming steps because of matrix inversions. BRVM has OðN 3 Þ complexity per iteration, but we need to train a separate model for each drug, leading to OðTN 3 Þ overall complexity. KBMTL learns a unified model for all drugs conjointly and has OðRN 3 +NR 3 +TR 3 Þ complexity per iteration, which shows that our algorithm has comparable computational complexity with single-task learning methods up to moderate values of R. We envision several possible extensions of our work in future pharmacogenomic applications. Based on an analysis over KBMTL model parameters, we are able to identify groups of compounds with similar mechanisms of action. As functional screens are being performed on increasingly large numbers of compounds or genetic perturbations, often with poorly characterized mechanisms or strong off-target effects, jointly modeling<ref type="figure" target="#fig_1">.92 0.96 1.00 1.</ref><ref type="figure" target="#fig_1">.8 1.0 1.2 1.</ref><ref type="figure">Fig. 4</ref>. Performance comparison between (A) BRVM with the linear kernel, (B) BRVM with the Gaussian kernel, (C) KBMTL with the linear kernel and (D) KBMTL with the Gaussian kernel in terms of NRMSE values on cancer drug sensitivity dataset. The per-drug and average performance results compare the algorithms using 138 mean NRMSE values calculated over 50 replications and 50 mean NRMSE values calculated over 138 drugs, respectively. The box-and-whisker and scatterplots compare the NRMSE values of BRVM (on the x-axis of the scatterplots) and KBMTL (on the y-axis of the scatterplots) with the same kernel. For comparison, blue: KBMTL is better; red: KBMTL is worseNote: The numbers in each comparison give statistically significant wins according to the paired t-test with P50.01 and wins according to the direct comparison, respectively, for the method of the corresponding row.</p><formula>● ● ● ● ● ●● Per−drug performance C A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 0.8 1.0 1.2 1.4 0.8 1.0 1.2 1.4 A C p−value &lt; 1e−5 ● ● ● Average performance C A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per−drug performance</head><formula>D B ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average performance</head><formula>D B ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i562</head><p>M.G € onen and A.A.Margolin each compound in the context of the full screening collection should yield novel insights into compound mechanisms. Moreover, the ability to identify groups of related compounds with a shared robust molecular predictor should aid drug discovery efforts by improving the interpretability of large screens and providing multiple lead compounds effecting similar biological processes. From an algorithmic perspective, the kernelized Bayesian framework provides an extensible template for incorporating prior knowledge. For example, prior information may be incorporated to encourage similar predictors to be inferred for compounds known to target proteins in the same pathway. Importantly, extensions of more complex prior information are computationally tractable owing to the highly efficient inference performed by the variational Bayes algorithm. In summary, we believe that the method presented in this work contributes to the field of pharmacogenomic analysis by improving the robustness of drug susceptibility predictions by leveraging information shared across multiple compounds in a screen, and it provides an efficient Bayesian inference framework that may be applied and extended by the community in future applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>M.G €</head><figDesc>onen and A.A.Margolin factored posteriors to find the joint parameter distribution (Beal, 2003). We can write the factorable ensemble approximation of the required posterior as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Graphical model of KBMTL for binary classification. Small filled circles: hyper-parameters; large shaded circles: observed variables; other large circles: random variables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>where we normalize the kernel matrix to unit maximum value (i.e. dividing the kernel matrix by its maximum value) to eliminate scaling issues and (ii) the Gaussian kernel whose width parameter s is chosen amonginternal 5-fold crossvalidation scheme on the training set. We decide to make a selection from these particular values because the mean of pairwise Euclidean distances between data points is $ ffiffiffiffiffiffiffiffiffiffiffiffi 25000 p. Figure 4 compares the performance of BRVM and KBMTL with the same kernel in terms of per-drug performance (i.e. 138 mean NRMSE values calculated over 50 replications) and average performance (i.e. 50 mean NRMSE values calculated over 138</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig.3.</head><figDesc>Fig. 3. Performance comparison between (A) BPROBIT, (B) BRVM and (C) KBMTL in terms of AUROC values on HIV-1 drug resistance dataset for each drug. The box-and-whisker plots compare the AUROC values of the algorithms over 50 replications. The scatterplots give the AUROC values of the best baseline algorithm and KBMTL for 50 replications on the x-and y-axes, respectively. For comparison, blue: KBMTL is better; red: KBMTL is worse</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. Summary of HIV-1 dataset</figDesc><table>Drug name 
Analog 
Number of 
sequences 

IC 50 ratio 
cutoff 

Class ratio 

3TC 
Cytidine 
910 
3.0 
2.487 
ABC 
Guanosine 
743 
3.0 
1.444 
AZT 
Thymidine 
905 
3.0 
1.257 
d4T 
Thymidine 
908 
1.5 
1.147 
ddC 
Pyrimidine 
472 
1.5 
1.713 
ddI 
Guanosine 
908 
1.5 
1.253 
TDF 
Adenosine 
545 
1.5 
0.622 
FTC 
Cytidine 
165 
3.0 
2.587 

Note: Class ratio denotes the ratio between numbers of resistant and susceptible 
sequences. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. Mean and standard deviations of AUROC values for BPROBIT, BRVM and KBMTL on HIV-1 drug resistance dataset together with ranks in parentheses</figDesc><table>Drug 
BPROBIT 
BRVM 
KBMTL 

3TC 
0.942 AE 0.013 (2) 
0.933 AE 0.018 (3) 
0.947 AE 0.014 (1) 
ABC 
0.881 AE 0.027 (3) 
0.908 AE 0.026 (2) 
0.917 AE 0.024 (1) 
AZT 
0.940 AE 0.015 (3) 
0.952 AE 0.015 (2) 
0.958 AE 0.013 (1) 
d4T 
0.904 AE 0.026 (3) 
0.927 AE 0.021 (2) 
0.936 AE 0.020 (1) 
ddC 
0.880 AE 0.038 (3) 
0.886 AE 0.047 (2) 
0.897 AE 0.039 (1) 
ddI 
0.827 AE 0.025 (3) 
0.859 AE 0.023 (2) 
0.869 AE 0.021 (1) 
TDF 
0.884 AE 0.030 (2) 
0.876 AE 0.031 (3) 
0.907 AE 0.025 (1) 
FTC 
0.971 AE 0.030 (1) 
0.920 AE 0.053 (3) 
0.968 AE 0.034 (2) 
Average 
0.904 AE 0.011 (3) 
0.908 AE 0.013 (2) 
0.925 AE 0.012 (1) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 3.</figDesc><table>Pairwise comparison of four algorithms in terms of per-drug 
performances on cancer drug sensitivity dataset 

Algorithm 
BRVM[L] 
BRVM[G] 
KBMTL[L] 
KBMTL[G] 

BRVM[L] 
25/45 
5/12 
2/11 
BRVM[G] 
70/93 
26/43 
5/17 
KBMTL[L] 
102/126 
64/95 
4/25 
KBMTL[G] 
114/127 
98/121 
84/113 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian analysis of binary and polychotomous response data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">H</forename>
				<surname>Albert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Chib</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="669" to="679" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Barretina</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="page" from="603" to="607" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">Variational algorithms for approximate Bayesian inference</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Beal</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational relevance vector machines</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">M</forename>
				<surname>Bishop</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Tipping</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 16th Conference on Uncertainty in Artificial Intelligence<address><addrLine>Stanford, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Caruana</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficacy and safety of a specific inhibitor of the BCR-ABL tyrosine kinase in chronic myeloid leukemia</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">J</forename>
				<surname>Druker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="page" from="1031" to="1037" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Systematic identification of genomic markers of drug sensitivity in cancer cells</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Garnett</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="page" from="570" to="577" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Sampling-based approaches to calculating marginal densities</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Gelfand</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">F M</forename>
				<surname>Smith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="398" to="409" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilabel classification for exploiting cross-resistance information in HIV-1 drug resistance prediction</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Heider</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1946" to="1952" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust estimators for expression analysis</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Hubbell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1585" to="1592" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Systematic assessment of analytical methods for drug sensitivity prediction from cancer cell line data</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">S</forename>
				<surname>Jang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pac. Symp. Biocomput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="63" to="74" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning via Gaussian processes</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">D</forename>
				<surname>Lawrence</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">I</forename>
				<surname>Jordan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="753" to="760" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Machine learning prediction of cancer cell sensitivity to drugs based on genomic and chemical properties</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">P</forename>
				<surname>Menden</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">The stream algorithm: computationally efficient ridgeregression via Bayesian model averaging, and applications to pharmacogenomic prediction of cancer cell line sensitivity</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">C</forename>
				<surname>Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pac. Symp. Biocomput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparison and validation of genomic predictors for anticancer drug sensitivity</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Papillon-Cavanagh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="597" to="602" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Read</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="333" to="359" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Human immunodeficiency virus reverse transcriptase and protease sequence database</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">Y</forename>
				<surname>Rhee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="298" to="303" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Genotypic predictors of human immunodeficiency virus type 1 drug resistance</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">Y</forename>
				<surname>Rhee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="17355" to="17360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName>
				<forename type="first">€</forename>
				<surname>Sch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Olkopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Smola</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<monogr>
		<title level="m" type="main">Kernel Methods in Computational Biology</title>
		<author>
			<persName>
				<forename type="first">€</forename>
				<surname>Sch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Olkopf</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning and the relevance vector machine</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Tipping</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">Mining multi-label data (eds) Data Mining and Knowledge Discovery Handbook</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Tsoumakas</surname>
			</persName>
		</author>
		<editor>Maimon,O. and Rokach,L.</editor>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="667" to="685" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Chemical genomics identifies small-molecule MCL1 repressors and BCL-xL as a predictor of MCL1 dependency</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Wei</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="547" to="562" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Genomics of Drug Sensitivity in Cancer (GDSC): a resource for therapeutic biomarker discovery in cancer cells</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="955" to="961" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">i563 Drug susceptibility prediction against a panel of drugs</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>