
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-11T00:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying disease sensitive and quantitative trait-relevant biomarkers from multidimensional heterogeneous imaging genetics data via sparse multimodal multitask learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Hua</forename>
								<surname>Wang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<postCode>76019</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Feiping</forename>
								<surname>Nie</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<postCode>76019</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Heng</forename>
								<surname>Huang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<postCode>76019</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Shannon</forename>
								<forename type="middle">L</forename>
								<surname>Risacher</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University School of Medicine</orgName>
								<address>
									<postCode>46202</postCode>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Andrew</forename>
								<forename type="middle">J</forename>
								<surname>Saykin</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University School of Medicine</orgName>
								<address>
									<postCode>46202</postCode>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Li</forename>
								<surname>Shen</surname>
							</persName>
							<email>shenli@iupui.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="institution">Indiana University School of Medicine</orgName>
								<address>
									<postCode>46202</postCode>
									<settlement>Indianapolis</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying disease sensitive and quantitative trait-relevant biomarkers from multidimensional heterogeneous imaging genetics data via sparse multimodal multitask learning</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="page" from="127" to="136"/>
							<date type="published" when="2012">2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts228</idno>
					<note>Copyedited by: TRJ MANUSCRIPT CATEGORY: [16:31 29/5/2012 Bioinformatics-bts228.tex] Page: i127 i127–i136 BIOINFORMATICS For the Alzheimer&apos;s Disease Neuroimaging Initiative † Availability: Software is publicly available at: http://ranger.uta.edu/ %7eheng/multimodal/ Contact:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Recent advances in brain imaging and high-throughput genotyping techniques enable new approaches to study the influence of genetic and anatomical variations on brain functions and disorders. Traditional association studies typically perform independent and pairwise analysis among neuroimaging measures, cognitive scores and disease status, and ignore the important underlying interacting relationships between these units. Results: To overcome this limitation, in this article, we propose a new sparse multimodal multitask learning method to reveal complex relationships from gene to brain to symptom. Our main contributions are threefold: (i) introducing combined structured sparsity regularizations into multimodal multitask learning to integrate multidimensional heterogeneous imaging genetics data and identify multimodal biomarkers; (ii) utilizing a joint classification and regression learning model to identify disease-sensitive and cognition-relevant biomarkers; (iii) deriving a new efficient optimization algorithm to solve our non-smooth objective function and providing rigorous theoretical analysis on the global optimum convergency. Using the imaging genetics data from the Alzheimer&apos;s Disease Neuroimaging Initiative database, the effectiveness of the proposed method is demonstrated by clearly improved performance on predicting both cognitive scores and disease status. The identified multimodal biomarkers could predict not only disease status but also cognitive function to help elucidate the biological pathway from gene to brain structure and function, and to cognition and disease.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent advances in acquiring multimodal brain imaging and genome-wide array data provide exciting new opportunities to study the influence of genetic variation on brain structure and function. Research in this emerging field, known as imaging genetics, holds great promise for a system biology of the brain to better understand complex neurobiological systems, from genetic determinants to cellular processes to the complex interplay of brain structure, function, behavior and cognition. Analysis of these multimodal datasets will facilitate early diagnosis, deepen mechanistic understanding and improved treatment of brain disorders. Machine learning methods have been widely employed to predict Alzheimer's disease (AD) status using imaging genetics measures (<ref type="bibr" target="#b5">Batmanghelich et al., 2009;</ref><ref type="bibr" target="#b11">Fan et al., 2008;</ref><ref type="bibr" target="#b15">Hinrichs et al., 2009b;</ref><ref type="bibr" target="#b31">Shen et al., 2010a</ref>). Since AD is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions, regression models have also been investigated to predict clinical scores from structural, such as magnetic resonance imaging (MRI), and/or molecular, such as fluorodeoxyglucose positron emission tomography (FDG-PET), neuroimaging data (<ref type="bibr" target="#b34">Stonnington et al., 2010;</ref><ref type="bibr" target="#b38">Walhovd et al., 2010</ref>). For example,<ref type="bibr" target="#b38">Walhovd et al. (2010)</ref>performed stepwise regression in a pairwise fashion to relate each of MRI and FDG-PET measures of eight candidate regions to each of four Rey's Auditory Verbal Learning Test (RAVLT) memory scores. This univariate approach, however, did not consider either interrelated structures within imaging data or those within cognitive data. Using relevance vector regression,<ref type="bibr" target="#b34">Stonnington et al. (2010)</ref>jointly analyzed the voxelbased morphometry (VBM) features extracted from the entire brain to predict each selected clinical score, while the investigations of different clinical scores are independent from each other. One goal of imaging genetics is to identify genetic risk factors and/or imaging biomarkers via intermediate quantitative traits (QTs, e.g. cognitive memory scores used in this article) on the chain from gene to brain to symptom. Thus, both disease classification and QT prediction are important machine learning tasks. Prior imaging genetics research typically employs a two-step procedure for identifying risk factors and biomarkers: one first determines disease-relevant QTs, and then detects the biomarkers associated with these QTs. Since a QT could be related to many genetic or imaging markers on different pathways that are not all disease specific (e.g. QT 2 and Gene 3 in<ref type="figure">Fig. 1</ref>), an ideal scenario would be to discover only those markers associated with both QT and disease status for a better understanding of the underlying biological pathway specific to the disease. On the other hand, identifying genetic and phenotypic biomarkers from large-scale multidimensional heterogeneous data is an important biomedical and biological research topic. Unlike simple feature selection working on a single data source, multimodal learning describes the setting of learning from data where observations are represented by multiple types of feature sets. Many multimodal methods have been developed for classification and clustering purposes, such as co-training (<ref type="bibr" target="#b0">Abney, 2002;</ref><ref type="bibr" target="#b8">Brefeld and Scheffer, 2004;</ref><ref type="bibr" target="#b13">Ghani, 2002;</ref><ref type="bibr">Nigam H.Wang et al.</ref><ref type="figure">Fig. 1</ref>. A simplified schematic example of two pathways from gene to QTs to phenotypic endpoints: the red one is disease relevant while the blue one yields only normal variation. Traditional two-stage imaging genetic strategy identifies QT 1 and QT 2 first and then Genes 1, 2, 3. Our new method will identify only disease relevant genes (i.e. Gene 1 and Gene 2); and Gene 3 would not be identified because it cannot be used to classify disease<ref type="bibr">status et al., 2000</ref>) and multiview clustering (<ref type="bibr" target="#b7">Bickel and Scheffer, 2004;</ref><ref type="bibr" target="#b9">Dhillon et al., 2003</ref>). However, they typically assume that the multimodal feature sets are conditionally independent, which does not hold in many real-world applications such as imaging genetics. Considering different representations give rise to different kernel functions, several Multiple Kernel Learning (MKL) approaches (<ref type="bibr" target="#b4">Bach et al., 2004;</ref><ref type="bibr" target="#b14">Hinrichs et al., 2009a;</ref><ref type="bibr" target="#b17">Kloft et al., 2008;</ref><ref type="bibr" target="#b19">Lanckriet et al., 2004;</ref><ref type="bibr" target="#b29">Rakotomamonjy et al., 2007;</ref><ref type="bibr" target="#b33">Sonnenburg et al., 2006;</ref><ref type="bibr" target="#b36">Suykens et al., 2002;</ref><ref type="bibr">Ye et al., 2008;</ref><ref type="bibr">Yu et al., 2010;</ref><ref type="bibr">Zien and Ong, 2007</ref>) have been recently studied and employed to integrate heterogeneous data and select multitype features. However, such models train a single weight for all features from the same modality, i.e. all features from the same data source are weighted equally, when they are combined with the features from other sources. This limitation often yields inadequate performance. To address the above challenges, we propose a new sparse multimodal multitask learning algorithm that integrates heterogeneous genetic and phenotypic data effectively and efficiently to identify disease-sensitive and cognition-relevant biomarkers from multiple data sources. Different to LASSO (<ref type="bibr" target="#b37">Tibshirani, 1996</ref>), group LASSO (<ref type="bibr">Yuan and Lin, 2006</ref>) and other related methods that mainly find the biomarkers correlated to each individual QT (memory score), we consider predicting each memory score as a regression task and select biomarkers that tend to play an important role in influencing multiple tasks. A joint classification and regression multitask learning model is utilized to select the biomarkers correlated to memory scores and disease categories simultaneously. Sparsity regularizations have recently been widely investigated and applied to multitask learning models (<ref type="bibr" target="#b1">Argyriou et al., 2007;</ref><ref type="bibr" target="#b16">Kim and Xing, 2010;</ref><ref type="bibr" target="#b24">Micchelli et al., 2010;</ref><ref type="bibr" target="#b26">Obozinski et al., 2006</ref><ref type="bibr" target="#b27">Obozinski et al., , 2010</ref><ref type="bibr" target="#b35">Sun et al., 2009</ref>). Sparse representations are typically achieved by imposing non-smooth norms as regularizers in the optimization problems. From the view of sparsity organization, we have two types: (i) The flat sparsity is often achieved by 0-norm or 1-norm regularizer or trace norm in matrix/tensor completion. Optimization techniques include LARS (<ref type="bibr" target="#b10">Efron et al., 2004</ref>), linear gradient search (<ref type="bibr" target="#b22">Liu et al., 2009</ref>), proximal methods (<ref type="bibr" target="#b6">Beck and Teboulle, 2009</ref>). (ii) The structured sparsity is usually obtained through different sparse regularizers such as 2,1-norm (<ref type="bibr" target="#b16">Kim and Xing, 2010;</ref><ref type="bibr" target="#b27">Obozinski et al., 2010;</ref><ref type="bibr" target="#b35">Sun et al., 2009</ref>), 2,0-norm (<ref type="bibr" target="#b23">Luo et al., 2010</ref>), ∞,1-norm (<ref type="bibr" target="#b28">Quattoni et al., 2009</ref>) (also denoted as 1,2-norm, 1,∞norm in different papers) and group 1-norm (<ref type="bibr">Yuan and Lin, 2006</ref>) which can be solved by methods in<ref type="bibr" target="#b24">Micchelli et al. (2010) and</ref><ref type="bibr" target="#b2">Argyriou et al. (2008)</ref>. We propose a new combined structured sparse<ref type="figure">2</ref>. The proposed sparse multimodal multitask feature selection method will identify biomarkers from multimodal heterogeneous data resources. The identified biomarkers could predict not only disease status, but also cognitive functions to help researchers better understand the underlying mechanism from gene to brain structure and function, and to cognition and disease regularization to integrate features from different modalities and to learn a weight for each feature leading to a more flexible scheme for feature selection in data integration, which is illustrated in<ref type="figure" target="#fig_1">Figure 3</ref>. In our combined structured sparse regularization, the group 1norm regularization (blue circles in<ref type="figure" target="#fig_1">Fig. 3</ref>) learns the feature global importance, i.e. the modal-wise feature importance of every data modality on each class (task), and the 2,1-norm regularization (red circles in<ref type="figure" target="#fig_1">Fig. 3</ref>) explores the feature local importance, i.e. the importance of each feature for multiple classes/tasks. The proposed method is applied to identify AD-sensitive biomarkers associated with the cognitive scores by integrating heterogeneous genetic and phenotypic data (as shown in<ref type="figure">Fig. 2</ref>). Our empirical results yield clearly improved performance on predicting both cognitive scores and disease status.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IDENTIFYING DISEASE SENSITIVE AND QT-RELEVANT BIOMARKERS FROM HETEROGENEOUS IMAGING GENETICS DATA</head><p>Pairwise univariate correlation analysis can quickly provide important association information between genetic/phenotypic data and QTs. However, it treats the features and the QTs as independent and isolated units, therefore the underlying interacting relationships between the units might be lost. We propose a new sparse multimodal multitask learning model to reveal genetic and phenotypic biomarkers, which are disease sensitive and QT-relevant, by simultaneously and systematically taking into account an ensemble of SNPs (single nucleotide polymorphism) and phenotypic signatures and jointly performing two heterogeneous tasks, i.e. biomarker-to-QT regression and biomarker-to-disease classification. The QTs studied in this article are the cognitive scores. In multitask learning, given a set of input variables (i.e. features such as SNPs and MRI/PET measures), we are interested in learning a set of related models (e.g. relations between genetic/imaging markers and cognitive scores) to predict multiple outcomes (i.e. tasks such as predicting cognitive scores and disease status). Because these tasks are relevant, they share a common input space. As a result, it is desirable to learn all the models jointly rather than treating each task as independent and fitting each model separately, such as Lasso (<ref type="bibr" target="#b37">Tibshirani, 1996</ref>) and group Lasso (<ref type="bibr">Yuan and Lin, 2006</ref>). Such multitask learning can discover robust patterns (because significant patterns in a single task could be outliers for other tasks) and potentially increase the predictive power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i128</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multidimensional imaging genetics data integration</head><p>In this article, we write matrices as uppercase letters and vectors as boldface lowercase letters. Given a matrix W = w ij , its i-th row and j-th column are denoted as w i and w j , respectively. The 2,1norm of the matrix W is defined as ||W || 2,1 = i=1 ||w i || 2 (also denoted as 1,2-norm by other researchers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Heterogeneous data integration via combined structured sparse regularizations</head><p>First, we will systematically propose our new multimodal learning method to integrate and select the genetic and phenotypic biomarkers from large-scale heterogeneous data. In the supervised learning setting, we are given n training samples {(</p><formula>x i ,y i )} n i=1 , where x i = (x 1 i ,··· ,x k i ) T ∈∈ d is</formula><p>the input vector including all features from a total of k different modalities and each modality j has d j features</p><formula>(d = k j=1 d j ). y i ∈∈ c</formula><p>is the class label vector of data point x i (only one element in y i is 1, and others are zeros), where c is the number of classes (tasks).</p><formula>Let X =[x 1 ,··· ,x n ]∈∈ d ×n and Y =[y 1 ,··· ,y c ]∈ c×n</formula><p>. Different to MKL, we directly learn a d ×c parameter matrix as:</p><formula>W = ⎡ ⎣ w 1 1 ... w 1 c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>... ... ...</head><formula>w k 1 ... w k c ⎤ ⎦ ∈∈ d ×c ,</formula><formula>(1)</formula><p>where w q p ∈∈ d q indicates the weights of all features in the q-th modality with respect to the p-th task (class). Typically, we can use a convex loss function L(X ,W ) to measure the loss incurred by W on the training samples. Compared with MKL approaches that learn one weight for one kernel matrix representing one modality, our method will learn the weight for each feature to capture the local feature importance. Since the features come from heterogeneous data sources, we impose the regularizer R W to capture the interrelationships of modalities and features as:</p><formula>min W L(X ,W )+γ R W ,</formula><formula>(2)</formula><p>where γ is a trade-off parameter. In heterogeneous data fusion, from multiview perspective of view, the features of a specific view (modality) can be more or less discriminative for different tasks (classes). Thus, we propose a new group 1-norm (G 1-norm) as a regularization term in Equation (2), which is defined over W as following:</p><formula>W G 1 = c i=1 k j=1 ||w j i || 2 ,</formula><formula>(3)</formula><p>which is illustrated by the blue circles in<ref type="figure" target="#fig_1">Figure 3</ref>. Then the Equation (2) becomes:</p><formula>min W L(X ,W )+γ 1 W G 1 .</formula><formula>(4)</formula><p>Since the group 1-norm uses 2-norm within each modality and 1norm between modalities, it enforces the sparsity between different modalities, i.e. if one modality of features are not discriminative for certain tasks, the objective in Equation (4) will assign zeros (in ideal case, usually they are very small values) to them for corresponding tasks; otherwise, their weights are large. This new group 1-norm regularizer captures the global relationships between data modalities.emphasizes the learning of the group-wise weights for a type of features (e.g. all the SNPs features, or all the MRI imaging features, or all the FDG-PET imaging features) corresponding to each task (e.g. the prediction for a disease status or a memory score) and the 2,1-norm accentuates the individual weight learning cross multiple tasks</p><p>However, in certain cases, even if most features in one modality are not discriminative for the classification or regression tasks, a small number of features in the same modality can still be highly discriminative. From the multitask learning point of view, such important features should be shared by all/most tasks. Thus, we add an additional 2,1-norm regularizer into Equation (4) as:</p><formula>min W L(X ,W )+γ 1 W G 1 +γ 2 W 2,1 .</formula><formula>(5)</formula><p>The 2,1-norm was popularly used in multitask feature selection (<ref type="bibr" target="#b2">Argyriou et al., 2008;</ref><ref type="bibr" target="#b27">Obozinski et al., 2010</ref>). Since the 2,1-norm regularizer impose the sparsity between all features and non-sparsity between tasks, the features that are discriminative for all tasks will get large weights. Our regularization items consider the heterogeneous features from both group-wise and individual viewpoints.<ref type="figure" target="#fig_1">Figure 3</ref>visualizes the matrix W T as a demonstration. In<ref type="figure" target="#fig_1">Figure 3</ref>, the elements with deep blue color have large values. The group 1-norm emphasizes the group-wise weights learning corresponding to each task and the 2,1-norm accentuates the individual weight learning cross multiple tasks. Through the combined regularizations, for each task (class), many features (not all of them) in the discriminative modalities and a small number of features (may not be none) in the nondiscriminative modalities will learn large weights as the important and discriminative features. The multidimensional data integration has been increasingly important to many biological and biomedical studies. So far, the MKL methods are most widely used. Due to the learning model deficiency, the MKL methods cannot explore both modality-wise importance and individual importance of features simultaneously. Our new structured sparse multimodal learning method integrates the multidimensional data in a more efficient and effective way. The loss function L(X ,W ) in Equation (8) can be replace by either least square loss function or logistic regression loss function to perform regression/classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint disease classification and QT regression</head><p>Since we are interested in identifying the disease-sensitive and QT-relevant biomarkers, we consider performing both logistic regression for classifying disease status and multivariate regression for predicting cognitive memory scores simultaneously (<ref type="bibr" target="#b39">Wang et al., 2011</ref>). A similar model was used in<ref type="bibr" target="#b41">Yang et al. (2009)</ref>for i129</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.Wang et al.</head><p>heterogeneous multitask learning. Regular multitask learning only considers homogeneous tasks such as regression or classification individually. Joint classification and regression can be regarded as a learning paradigm for handling heterogeneous tasks. First, logistic regression is used for disease classification, which minimizes the following loss function:</p><formula>L 1 (W ) = n i=1 c 1 k=1 ⎛ ⎝ y ik log c 1 l=1 e w T l x i −y ik w T k x i ⎞ ⎠ .</formula><formula>(6)</formula><p>Here, we perform three binary classification tasks for the following three diagnostic groups respectively (c 1 = 3): AD, mild cognitive impairment (MCI), and health control (HC). Second, we use the traditional multivariate least squares regression model to predict memory scores. Under the regression matrix P ∈∈ d ×c 2 , the least squares loss is defined by</p><formula>L 2 (P) = X T P −Z 2 F ,</formula><formula>(7)</formula><p>where X is the data points matrix, P is the coefficient matrix of regression with c 2 tasks, the label matrix Z =</p><formula>z 1 T , z 2 T ,··· , z n T T ∈∈ n×c 2 .</formula><p>We perform the joint classification and regression tasks, the disease-sensitive and QT-relevant biomarker identification task can be formulated as the following objective:</p><formula>min V n i=1 c 1 k=1 ⎛ ⎝ y ik log c 1 l=1 e w T l x i −y ik w T k x i ⎞ ⎠ + X T P −Z 2 F +γ 1 V G 1 +γ 2 V 2,1 ,</formula><formula>(8)</formula><p>where V =<ref type="bibr">[W P]</ref>∈∈ d ×(c 1 +c 2 ). As a result, the identified biomarkers will be correlated to memory scores and also be discriminative to disease categories. Since the objective in Equation (8) is a non-smooth problem and cannot be easily solved in general, we derive a new efficient algorithm to solve this problem in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization algorithm</head><p>We take the derivatives of Equation (8) with respect to W and P respectively, and set them to zeros, we have</p><formula>∂L 1 (W ) ∂W +2γ 1 c 1 i=1 D i w i +2γ 2 DW = 0, (9) 2XX T P −2XZ +2γ 1 c 2 i=c 1 +1 D i p i +2γ 2 DP = 0,</formula><formula>(10)</formula><p>where D i (1 ≤ i ≤ c 1 +c 2 ) is a block diagonal matrix with the kth diagonal block as</p><formula>1 2 v k i 2 I k (I k is a d k by d k identity matrix),</formula><p>D is a diagonal matrix with the k-th diagonal element as</p><formula>1 2v k 2 . Since D i (1 ≤ i ≤ c 1 +c 2 ) and D depend on V =[ W P ]</formula><p>, they are also unknown variables to be optimized. In this article, we provide an iterative algorithm to solve Equation (8). First, we guess a random solution V ∈∈ d ×(c 1 +c 2 ) , then we calculate the matrices D i (1 ≤ i ≤ c 1 +c 2 ) and D according to the current solution V. After obtaining the D i (1 ≤ i ≤ c 1 +c 2 ) and D, we can update the solution V =<ref type="bibr">[ W P ]</ref>based on Equation (9). Specifically, the i-th column of P is updated by</p><formula>p i = (XX T +γ 1 D i +γ 2 D) −1 X z i .</formula><p>We cannot update W with a closed form solution based on Equation (9), but we can obtained the updated W by the Newton's method. According to Equation (9), we need to solve the following problem:</p><formula>min W L 1 (W )+γ 1 c 1 i=1 w T i D i w i +γ 2 Tr(W T DW ).</formula><formula>(11)</formula><p>Similar to the traditional method in the logistic regression (<ref type="bibr" target="#b18">Krishnapuram et al., 2005;</ref><ref type="bibr" target="#b21">Lee et al., 2006</ref>), we can use the Newton's method to obtain the solution W. For the first term, the traditional logistic regression derivatives can be applied to get the first-and second-order derivatives (<ref type="bibr" target="#b21">Lee et al., 2006</ref>). For the second term, the first-and second-order derivatives are</p><formula>∂ c 1 i=1 w T i D i w i ∂W up = 2D p (u,u)W up , ∂ c 1 i=1 w T i D i w i ∂W up ∂W vq = 2D p (u,u)δ uv δ pq ,</formula><formula>(12)</formula><p>where D p (u,u) is the u-th diagonal element of D p. For the third term, the first-and second-order derivatives are</p><formula>∂Tr(W T DW ) ∂W up = 2D(u,u)W up , ∂Tr(W T DW ) ∂W up ∂W vq = 2D(u,u)δ uv δ pq .</formula><formula>(13)</formula><p>After obtaining the updated solution V =<ref type="bibr">[ W P ]</ref>, we can calculate the new matrices D i (1 ≤ i ≤ c 1 +c 2 ) and D. This procedure is repeated until the algorithm converges. The detailed algorithm is listed in Algorithm 1. We will prove that the above algorithm will converge to the global optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Algorithm analysis</head><p>To prove the convergence of the proposed algorithm, we need a lemma as follows.</p><p>Lemma 1. For any vectors v and v 0 , we have the following inequality:</p><formula>v 2 − v 2 2 2v 0 2 ≤ v 0 2 − v 0 2 2 2v 0 2 . Proof. Obviously, −(v 2 −v 0 2 ) 2 ≤ 0, so we have −(v 2 −v 0 2 ) 2 ≤ 0 ⇒ 2v 2 v 0 2 −v 2 2 ≤ v 0 2 2 ⇒ v 2 − v 2 2 2v 0 2 ≤ v 0 2 − v 0 2 2 2v 0 2 ,</formula><formula>(14)</formula><p>which completes the proof. 2</p><p>Then we prove the convergence of the algorithm, which is described in the following theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multidimensional imaging genetics data integration</head><p>Proof. In each iteration, suppose the updated W is˜Wis˜ is˜W , and the updated P is˜Pis˜ is˜P, then the updated V is˜Vis˜ is˜V =<ref type="bibr">[</ref></p><formula>L 1 ( ˜ W )+γ 1 c 1 i=1˜w i=1˜ i=1˜w T i D i ˜ w i +γ 2 Tr( ˜ W T D ˜ W ) ≤ L 1 (W )+γ 1 c 1 i=1 w T i D i w i +γ 2 Tr(W T DW ).</formula><formula>(15)</formula><p>According to Step 4, we have:</p><formula>X T ˜ P −Y 2 F +γ 1 c 2 i=1˜p i=1˜ i=1˜p T i D i ˜ p i +γ 2 Tr( ˜ P T D ˜ P) ≤ X T P −Y 2 F +γ 1 c 2 i=1 p T i D i p i +γ 2 Tr(P T DP).</formula><formula>(16)</formula><p>Based on the definitions of D i (1 ≤ i ≤ c 1 +c 2 ) and D, and Lemma 1, we have two following inequalities:</p><formula>K k=1˜v k=1˜k=1˜v k i 2 − K k=1˜v k=1˜k=1˜v k i 2 2 2 v k i 2 ≤ K k=1 v k i 2 − K k=1 v k i 2 2 2 v k i 2 ⇒ K k=1˜v k=1˜k=1˜v k i 2 − ˜ v T i D i ˜ v i ≤ K k=1 v k i 2 −v T i D i v i ⇒ γ 1 c 1 +c 2 i=1 K k=1˜v k=1˜k=1˜v k i 2 −γ 1 c 1 +c 2 i=1˜v i=1˜ i=1˜v T i D i ˜ v i ≤ γ 1 c 1 +c 2 i=1 K k=1 v k i 2 −γ 1 c 1 +c 2 i=1 v T i D i v i ,</formula><formula>(17) and d k=1˜v k=1˜k=1˜v k 2 − d k=1˜v k=1˜k=1˜v k 2 2 2 v k 2 ≤ d k=1 v k 2 − d k=1 v k 2 2 2 v k 2 ⇒ γ 2 d k=1˜v k=1˜k=1˜v k 2 −γ 2 Tr( ˜ V T D ˜ V ) ≤ γ 2 d k=1 v k 2 −γ 2 Tr(V T DV ).</formula><formula>(18)</formula><p>Note that the following two equalities:</p><formula>c 1 +c 2 i=1 v T i D i v i = c 1 i=1 w T i D i w i + c 2 i=1 p T i D i p i , Tr(V T DV ) = Tr(W T DW )+Tr(P T DP),</formula><formula>(19)</formula><p>Algorithm 1 An efficient iterative algorithm to solve the optimization problem in Equation (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><formula>L 1 ( ˜ W )+L 2 ( ˜ P)+γ 1 c 1 +c 2 i=1 K k=1˜v k=1˜k=1˜v k i 2 +γ 2 d k=1˜v k=1˜k=1˜v k 2 ≤ L 1 (W )+L 2 (P)+γ 1 c 1 +c 2 i=1 K k=1 v k i 2 +γ 2 d k=1 v k 2 .</formula><p>Therefore, the algorithm decreases the objective value of problem (8) in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>In the convergence,</p><formula>W , P, D i (1 ≤ i ≤ c 1 +c 2 )</formula><p>and D satisfy the Equation (9). As the Equation (8) is a convex problem, satisfying the Equation (9) indicates that V =<ref type="bibr">[ W P ]</ref>is a global optimum solution to the Equation (8). Therefore, the Algorithm 1 will converge to the global optimum of the Equation (8). Since our algorithm has the closed form solution in each iteration, the convergency is very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.Wang et al.</head><p>and neuropsychological assessment can be combined to measure the progression of MCI and early AD. For up-to-date information, see www.adni-info.org. Following a prior imaging genetics study (<ref type="bibr" target="#b32">Shen et al., 2010b</ref>), 733 non-Hispanic Caucasian participants were included in this study. We empirically evaluate the proposed method by applying it to the ADNI cohort, where a wide range of multimodal biomarkers are examined and selected to predict memory performance measured by five RAVLT scores and classify participants into HC, MCI and AD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental design</head><p>Overall setting: our primary goal is to identify relevant genetic and imaging biomarkers that can classify disease status and predict memory scores (<ref type="figure">Fig. 2</ref>). We describe our genotyping, imaging and memory data inGenotyping data: the single-nucleotide polymorphism (SNP) data (<ref type="bibr" target="#b30">Saykin et al., 2010</ref>) were genotyped using the Human 610-Quad BeadChip (Illumina, Inc., San Diego, CA, USA). Among all SNPs, only SNPs, belonging to the top 40 AD candidate genes listed on the AlzGene database (www.alzgene.org) as of June 10, 2010, were selected after the standard quality control (QC) and imputation steps. The QC criteria for the SNP data include (i) call rate check per subject and per SNP marker, (ii) gender check, (iii) sibling pair identification, (iv) the Hardy–Weinberg equilibrium test, (v) marker removal by the minor allele frequency and (vi) population stratification. The quality-controlled SNPs were then imputed using the MaCH software to estimate the missing genotypes. After that, the Illumina annotation information based on the Genome build 36.2 was used to select a subset of SNPs, belonging or proximal to the top 40 AD candidate genes. This procedure yielded 1224 SNPs, which were annotated with 37 genes (<ref type="bibr" target="#b40">Wang et al., 2012</ref>). For the remaining 3 genes, no SNPs were available on the genotyping chip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imaging biomarkers:</head><p>in this study, we use the baseline structural MRI and molecular FDG-PET scans, from which we extract imaging biomarkers. Two widely employed automated MRI analysis techniques were used to process and extract imaging genotypes across the brain from all baseline scans of ADNI participants as previously described (<ref type="bibr" target="#b32">Shen et al., 2010b</ref>). First, voxel-based morphometry (VBM) (<ref type="bibr" target="#b3">Ashburner and Friston, 2000</ref>) was performed to define global gray matter (GM) density maps and extract local GM density values for 86 target regions (<ref type="figure" target="#fig_5">Fig. 4a</ref>). Second, automated parcellation via freeSurfer V4 (<ref type="bibr" target="#b12">Fischl et al., 2002</ref>) was conducted to define 56 volumetric and cortical thickness values (<ref type="figure" target="#fig_5">Fig. 4b</ref>) and to extract total intracranial volume (ICV). Further information about these measures is available in<ref type="bibr" target="#b32">Shen et al. (2010b)</ref>. All these measures were adjusted for the baseline age, gender, education, handedness and baseline ICV using the regression weights derived from the healthy control participants. For PET images, following<ref type="bibr" target="#b20">Landau et al. (2009)</ref>, mean glucose metabolism (CMglu) measures of 26 regions of interest (ROIs) in the Montreal Neurological Institute (MNI) brain atlas space were employed in this study (<ref type="figure" target="#fig_5">Fig. 4c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory data:</head><p>The cognitive measures we use to test the proposed method are the baseline RAVLT memory scores from all ADNI participants. The standard RAVLT format starts with a list of 15 unrelated words (List A) repeated over five different trials and, Diag-R (right side) or Diag; and weights for RAVLT regression were labeled as AVLT-L, AVLT-R or AVLT. In (a–c), weights were normalized by dividing the corresponding threshold used for feature selection, and thus all selected features had normalized weights ≥1 and were marked with 'x'. In (d), only top SNPs were shown, weights were normalized by dividing the weight of the 10th top SNP, and the top 10 SNPs for either classification or regression task had normalized weights ≥1 and were marked with 'x'participants are asked to repeat. Then the examiner presents a second list of 15 words (List B), and the participant is asked to remember as many words as possible from List A. Trial 6, termed as 5 min recall, requests the participant again to recall as many words as possible from List A, without reading it again. Trial 7, termed as 30 min recall, is administrated in the same way as Trial 6, but after a 30 min delay. Finally, a recognition test with 30 words read aloud, requesting the participant to indicate whether or not each word is on List A. The RAVLT has proven useful in evaluating verbal learning and memory.Participant selection: In this study, we included only participants with no missing data for all above four types (views) of features and cognitive scores, which resulted in a set of 345 subjects (83 HC, 174 MCI and 88 AD). The feature sets extracted from baseline multimodal data of these subjects are summarized in<ref type="figure" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multidimensional imaging genetics data integration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Biomarker identifications</head><p>The proposed heterogeneous multitask learning scheme aims to identify genetic and phenotypic biomarkers that are associated with both cognition (e.g. RAVLT in this study) and disease status in a joint regression and classification framework. Here we first examine the identified biomarkers. Shown in<ref type="figure" target="#fig_5">Figure 4</ref>is a summarization of selected features for all four data types, where the regression/classification weights are color-mapped for each feature and each task. In<ref type="figure" target="#fig_5">Figure 4a</ref>, many VBM measures are selected to be associated with disease status, which is in accordance with known global brain atrophy pattern in AD. The VBM measures associated with RAVLT scores seem to be a subset of those disease-sensitive markers, showing a specific memory circuitry contributing to the disease, as well as suggesting that the disease is implicated by not only this memory function but also other complicated factors. Evidently, the proposed method could have a potential to offer deep mechanistic understandings. Shown in<ref type="figure" target="#fig_6">Figure 5</ref>is a comparison between RAVLTrelevant markers and AD-relevant markers and their associated weights mapped onto a standard brain space.<ref type="figure" target="#fig_5">Figure 4b</ref>shows the identified markers from the FreeSurfer data. In this case, a small set of markers are discovered. These markers, such as hippocampal volume, amygdala volume and entorhinal cortex thickness, are all well-known AD-relevant markers, showing the effectiveness of the proposed method. These markers are also shown to be associated with both AD and RAVLT. The FDG-PET findings (<ref type="figure" target="#fig_5">Fig. 4c</ref>) are also interesting and promising. The ADrelevant biomarkers include angular, hippocampus, middle temporal and post cingulate regions, which agrees with prior findings e.g.<ref type="bibr" target="#b20">Landau et al. (2009)</ref>. Again, a subset of these markers are also relevant to RAVTL scores. As to the genetics, only top findings are shown in<ref type="figure" target="#fig_5">Figure 4d</ref>. The APOE E4 SNP (rs429358), the best known AD risk factor, shows the strongest link to both disease status and RAVLT scores. A few other important AD genes, including recently discovered and replicated PICALM and BIN1, are also included in the results. For those newly identified SNPs, further investigation in independent cohorts should be warranted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improved disease classification</head><p>We classify the selected participants of ADNI cohort using the proposed methods by integrating the four different types of data.We report the classification performances of our method. We compare our methods against several most recent MKL methods that are able to make use of multiple types of data including SVM ∞ MKL method (<ref type="bibr" target="#b33">Sonnenburg et al., 2006</ref>), SVM 1 MKL (<ref type="bibr" target="#b19">Lanckriet et al., 2004</ref>), SVM 2 MKL method (<ref type="bibr" target="#b17">Kloft et al., 2008</ref>), least square (LSSVM) ∞ MKL method (<ref type="bibr">Ye et al., 2008</ref>), LSSVM 1 MKL method (<ref type="bibr" target="#b36">Suykens et al., 2002</ref>) and LSSVM 2 MKL method (<ref type="bibr">Yu et al., 2010</ref>). We also compare a related method, Heterogeneous Multitask Learning (HML) method (<ref type="bibr" target="#b41">Yang et al., 2009</ref>), which simultaneously conducts classification and regression like our method. However, because this method is designed for homogenous input data and is not able to deal with multiple types of data at the same time, we concatenate the four types of features as its input. In addition, we report the classification performances by our method and SVM on each individual types of data as baselines. SVM on a simple concatenation of all four types of features are also reported. In our experiments, we conduct threeclass classification, which is more desirable and more challenging than binary classifications using each pair of three categories. We conduct standard 5-fold cross-validation and report the average results. For each of the five trials, within the training data, an internal 5-fold cross-validation is performed to fine tune the parameters. The parameters of our methods [γ 1 and γ 2 in<ref type="bibr">Equation (8)]</ref>are optimized in the range of 10 −5 ,10 −4 ,...,10 4 ,10 5 . For SVM method and MKL methods, one Gaussian kernel is constructed for each type of features</p><formula>i.e.K x i ,x j = exp −γ x i −x j 2 2</formula><p>, where the parameters γ are fine tuned in the same range used as our method. We implement the MKL methods using the codes published by<ref type="bibr">Yu et al. (2010)</ref>. Following<ref type="bibr">Yu et al. (2010)</ref>, in LSSVM ∞ and 2 methods, the regularization parameter λ is estimated jointly as the kernel coefficient of an identity matrix; in LSSVM 1 method, λ is set to 1; in all other SVM approaches, the C parameter of the box constraint is set to 1. We use LIBSVM (http://www.csie.ntu.edu.tw/ cjlin/libsvm/) software package to implement SVM. We implement HML method following the details in its original work, and set the parameters to be optimal. The classification performances measured by classification accuracy of all compared methods in AD detection are reported inOur method 0.726 ± 0.032 the methods using multiple data sources are generally better than their counterparts using one single type of data. This confirms the usefulness of data integration in AD diagnosis. Moreover, our methods always outperform the MKL methods in these experiments, although both take advantage of multiple data sources. This observation is consistent with our theoretical analysis. That is, our methods not only assign proper weight to each type of data, but also consider the relevance of the features inside each individual type of data. In contrast, the MKL methods address the former while not taking into account the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Improved memory performance prediction</head><p>Now we evaluate the memory performance prediction capability of the proposed method. Since the cognitive scores are continuous, we evaluate the proposed method via regression and compare it to two baseline methods, i.e. multivariate linear regression (MRV) and ridge regression. Since both MRV and ridge regression are for single-type input data, we conduct regression on each of the four types of features and a simple concatenation of them. Similarly, we also predict memory performance by our method on the same test conditions. When multiple-type input data are used, as demonstrated in Section 3.2, our method automatically and adaptively select the prominent biomarkers for regression. For each test case, we conduct standard 5-fold cross-validation and report the average results. For each of the five trials, within the training data, an internal 5-fold cross-validation is performed to fine tune the parameters in the range of<ref type="figure" target="#tab_4">Table 4</ref>we can see that the proposed method always has better memory prediction performance. Among the test cases, the FreeSurfer imaging measures and VBM imaging measure have similar predictive power, which are better than those of PET imaging measures and SNP features. In general, combining the four types of features are better than only using one type of data. Since our method adaptively weight each type of data and each feature inside a type of data, it has the least regression error when using all available input data. These results, again, demonstrated the usefulness of our method and data integration in early AD diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS</head><p>We proposed a novel sparse multimodal multitask learning method to identify the disease-sensitive biomarkers via integrating heterogeneous imaging genetics data. We utilized the joint classification and regression learning model to identify the disease-sensitive and QT-relevant biomarkers. We introduced a novel combined structured sparsity regularization to integrate heterogeneous imaging genetics data, and derived a new efficient optimization algorithm to solve our non-smooth objective function and followed with the rigorous theoretical analysis on the global convergency. The empirical results showed our method improved both memory scores prediction and disease classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.ucla.edu/wpcontent/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i134</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.</head><figDesc>Fig. 2. The proposed sparse multimodal multitask feature selection method will identify biomarkers from multimodal heterogeneous data resources. The identified biomarkers could predict not only disease status, but also cognitive functions to help researchers better understand the underlying mechanism from gene to brain structure and function, and to cognition and disease</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.3.</head><figDesc>Fig. 3. Illustration of the feature weight matrix W T. The elements in matrix with deep blue color have large values. The group 1-norm (G 1-norm) emphasizes the learning of the group-wise weights for a type of features (e.g. all the SNPs features, or all the MRI imaging features, or all the FDG-PET imaging features) corresponding to each task (e.g. the prediction for a disease status or a memory score) and the 2,1-norm accentuates the individual weight learning cross multiple tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: [16:31 29/5/2012 Bioinformatics-bts228.tex] Page: i130 i127–i136</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: [16:31 29/5/2012 Bioinformatics-bts228.tex] Page: i132 i127–i136</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>Section 3.1; present the identified biomarkers in Section 3.2; discuss the disease classification in Section 3.3; and demonstrate the memory score prediction in Section 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.4.</head><figDesc>Fig. 4. Weight maps for multimodal data: (a) VBM measures from MRI, (b) FreeSurfer measures from MRI, (c) glucose metabolism from FDG-PET, and (d) top SNP findings. Weights for disease classification were labeled as Diag-L (left side), Diag-R (right side) or Diag; and weights for RAVLT regression were labeled as AVLT-L, AVLT-R or AVLT. In (a–c), weights were normalized by dividing the corresponding threshold used for feature selection, and thus all selected features had normalized weights ≥1 and were marked with 'x'. In (d), only top SNPs were shown, weights were normalized by dividing the weight of the 10th top SNP, and the top 10 SNPs for either classification or regression task had normalized weights ≥1 and were marked with 'x'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.5.</head><figDesc>Fig. 5. VBM weights of joint regression of AVLT scores and classification of disease status were mapped onto brain (a) Overall weights for disease classification; (b) Overall weights for AVLT regression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Theorem 1.</figDesc><table>The algorithm decreases the objective value of problem 
(8) in each iteration. 

i130 

at :: on August 30, 2016 

http://bioinformatics.oxfordjournals.org/ 

Downloaded from 

Copyedited by: TRJ 

MANUSCRIPT CATEGORY: 

[16:31 29/5/2012 Bioinformatics-bts228.tex] 
Page: i131 i127–i136 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>˜ W ˜ P ]. From Step 3 in the Algorithm 1, we know that:</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>: X =[x 1 ,x 2 ,··· ,x n ]∈∈ d ×n , Y = y 1 T , y 2 T ,··· , y n T T ∈ {0,1} n×c 1 and Z = z 1 T , z 2 T ,··· , z n T T ∈∈ n×c 2. Output: W ∈∈ d ×c 1 and P ∈∈ d ×c 2. 1. Initialize W ∈∈ d ×c 1 , P ∈∈ d ×c 2. Let V =[ W P ]∈ d ×(c 1 +c 2 ). repeat 2. Calculate the block diagonal matrices D i (1 ≤ i ≤ c 1 +c 2 ), where the k-th diagonal block of D i is 1 2 v k i 2 I k. Calculate the diagonal matrix D, where the k-th diagonal element is 1 2v k 2 . 3. Update w by w −B −1 a, where the d * (p−1)+ u(1 ≤ u ≤ d ,1 ≤ p ≤ c 1 )-th element of a ∈∈ dc 1 ×1 is ∂ L 1 (W )+γ 1 c 1 i=1 w T i D i w i +γ 2 Tr(W T DW ) ∂W up , the (d * (p−1)+ u,d * (q−1)+v)(1 ≤ u,v ≤ d ,1 ≤ p,q ≤ c 1 )-th element of B ∈∈ dc 1 ×dc 1 is ∂ L 1 (W )+γ 1 c 1 i=1 w T i D i w i +γ 2 Tr(W T DW ) ∂W up ∂W vq . Construct the updated W ∈∈ d ×c 1 by the updated vector w ∈∈ dc 1 , where the (u,p)-th element of W is column of P by p i = (XX T +γ 1 D i + γ 2 D) −1 X z i. 5. Update the V by V =[ W P ]. until Converges then by adding Equations (15–18) in the both sides, we arrive at</figDesc><table>the 
(d  * (p−1)+u)-th element of w. 
4. Update the i-th </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 1. RAVLT cognitive measures as responses in multitask learning</figDesc><table>Task ID 
Description of RAVLT scores 

TOTAL 
Total score of the first 5 learning trials 
TOT6 
Trial 6 total number of words recalled 
TOTB 
List B total number of words recalled 
T30 
30 minute delay total number of words recalled 
RECOG 
30 minute delay recognition score 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 1 summarizes</figDesc><table>five RAVLT scores used in our 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><figDesc>Table 2. Multimodal feature sets as predictors in multiview learning</figDesc><table>View ID (feature set ID) 
Modality 
No. of features 

VBM 
MRI 
86 
FreeSurfer 
MRI 
56 
FDG-PET 
FDG-PET 
26 
SNP 
Genetics 
1244 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 3.</figDesc><table>A first glance at the results shows that our methods consistently 
outperform all other compared methods, which demonstrates the 
effectiveness of our methods in early AD detection. In addition, 

i133 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><figDesc>Table 3.</figDesc><table>Classification performance comparison between the proposed 
method and related methods for distinguishing HC, MCI and AD 

Methods 
Accuracy (mean + SD) 

SVM (SNP) 
0.561 ± 0.026 
SVM (FreeSurfer) 
0.573 ± 0.012 
SVM (VBM) 
0.541 ± 0.032 
SVM (PET) 
0.535 ± 0.026 
SVM (all) 
0.575 ± 0.019 
HML (all) 
0.638 ± 0.019 

SVM ∞ MKL method 
0.624 ± 0.031 
SVM 1 MKL method 
0.593 ± 0.042 
SVM 2 MKL method 
0.561 ± 0.037 
LSSVM ∞ MKL method 
0.614 ± 0.031 
LSSVM 1 MKL method 
0.585 ± 0.018 
LSSVM 2 MKL method 
0.577 ± 0.033 

Our method (SNP) 
0.673 ± 0.021 
Our method (FreeSurfer) 
0.689 ± 0.029 
Our method (VBM) 
0.669 ± 0.031 
Our method (PET) 
0.621 ± 0.028 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><figDesc>Table 4.</figDesc><table>Comparison of memory prediction performance measured by 
average RMSEs (smaller is better) 

Test case 
TOTAL TOT6 TOTB T30 
RECOG 

MRV (SNP) 
6.153 
2.476 2.168 
2.201 3.483 
MRV (FreeSurfer) 
5.928 
2.235 2.039 
2.088 3.339 
MRV (VBM) 
6.093 
2.289 2.142 
2.137 3.394 
MRV (PET) 
6.246 
2.514 2.237 
2.215 3.615 
MRV (all) 
5.909 
2.232 1.992 
2.032 3.306 

Ridge (SNP) 
6.076 
2.416 2.147 
2.117 3.368 
Ridge (FreeSurfer) 
5.757 
2.203 2.004 
2.017 3.237 
Ridge (VBM) 
5.976 
2.147 2.038 
2.129 3.249 
Ridge (PET) 
6.153 
2.443 2.186 
2.107 3.515 
Ridge (all) 
5.704 
2.143 1.989 
1.994 3.193 

Our method (SNP) 
5.991 
2.201 2.008 
2.001 3.107 
Our method (FreeSurfer) 5.601 
2.106 1.947 
1.886 3.015 
Our method (VBM) 
5.715 
2.011 
1.899 
1.974 3.041 
Our method (PET) 
6.013 
2.241 2.017 
2.017 3.331 
Our method (all) 
5.506 
1.984 1.886 
1.841 2.989 

From </table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="3"> EMPIRICAL STUDIES AND DISCUSSIONS Data used in the preparation of this article were obtained from the Alzheimer&apos;s Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). One goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical i131 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="10"> −5 ,10 −4 ,...,10 4 ,10 5 for both ridge regression and our method. For our method, in each trial, from the learned coefficient matrix we sum the absolute values of the coefficients of a single feature over all the tasks as the overall weight, from which we pick up the features with non-zero weights (i.e. w &gt; 10 −3 ) to predict regression responses for test data. The performance assessed by root mean square error (RMSE), a widely used measurement for statistical regression analysis, are reported in Table 4.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Abney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">) Bootstrapping. Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="360" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Argyriou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System (NIPS)</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Convex multitask feature learning</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Argyriou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="243" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Voxel-based morphometry–the methods</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ashburner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Friston</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="805" to="821" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple Kernel Learning, Conic Duality, and the SMOAlgorithm</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Bach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ACM</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">A general and unifying framework for feature construction, in image-based pattern classification</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Batmanghelich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Process Med Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="423" to="434" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Beck</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Teboulle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view clustering</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bickel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Scheffer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-em support vector learning</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Brefeld</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Scheffer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ACM</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Information-theoretic co-clustering</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">S</forename>
				<surname>Dhillon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD (Special Interest Group on Knowledge Discovery and Data Mining) International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial patterns of brain atrophy in MCI patients, identified via high-dimensional pattern classification, predict subsequent cognitive decline</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Fan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1731" to="1743" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Whole brain segmentation: automated labeling of neuroanatomical structures in the human brain</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Fischl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="341" to="355" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data for multi-class text categorization</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Ghani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">MKL for robust multi-modality ad classification</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hinrichs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Medical Image Computing and Computer-Assisted Intervention: Part II</title>
		<meeting>the 12th International Conference on Medical Image Computing and Computer-Assisted Intervention: Part II</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="786" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatially augmented LPboosting for AD classification with evaluations on the ADNI dataset</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hinrichs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="138" to="149" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Tree-Guided Group Lasso for Multi-Task Regression with Structured Sparsity</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Xing</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="352" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-sparse multiple kernel learning</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kloft</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels</title>
		<meeting>the NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Sparse multinomial logistic regression: fast algorithms and generalization bounds</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Krishnapuram</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="957" to="968" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Lanckriet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="27" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Associations between cognitive, functional, and FDG-PET measures of decline in AD and MCI</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Landau</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiol. Aging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1207" to="1218" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient l1 regularized logistic regression</title>
		<author>
			<persName>
				<forename type="first">S.-I</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 21st National Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">401</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale sparse logistic regression</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards structural sparsity: an explicit l 2 /l 0 approach</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Luo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">A family of penalty functions for structured sparsity</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Micchelli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System (NIPS)</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1612" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using em</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Nigam</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-task feature selection</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Obozinski</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint covariate selection and joint subspace selection for multiple classification problems</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Obozinski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="231" to="252" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">An efficient projection for l 1,∞ regularization</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Quattoni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ACM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="857" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">More efficiency in multiple kernel learning</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rakotomamonjy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ACM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="775" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease neuroimaging initiative biomarkers as quantitative phenotypes: genetics core aims, progress, and plans</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Saykin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimers Dement</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="265" to="273" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse bayesian learning for identifying imaging biomarkers in AD prediction</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Comput. Comput. Assist. Interv</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="611" to="618" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Pt. 3</note>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Whole genome association study of brain-wide imaging phenotypes for identifying quantitative trait loci in MCI and AD: A study of the ADNI cohort</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1051" to="1063" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Large scale multiple kernel learning</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Sonnenburg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1531" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Predicting clinical scores from magnetic resonance scans in alzheimer&apos;s disease</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">M</forename>
				<surname>Stonnington</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1405" to="1413" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient recovery of jointly sparse vectors</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Sun</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1812" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Least Squares Support Vector Machines</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Suykens</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Scientific, Singapore</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the LASSO</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-modal imaging predicts memory performance in normal aging and cognitive decline</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Walhovd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiol. Aging</title>
		<imprint>
			<biblScope unit="page" from="31" to="1107" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Identifying AD-Sensitive and Cognition-Relevant Imaging Biomarkers via Joint Classification and Regression</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of The 14th International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">Identifying quantitative trait loci via group-sparse multitask regression and feature selection: an imaging genetics study of the ADNI cohort</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="229" to="237" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Heterogeneous multitask learning with joint sparsity constraints</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System (NIPS)</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2151" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>