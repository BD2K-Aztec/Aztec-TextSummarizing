
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACE: adaptive cluster expansion for maximum entropy graphical model inference Downloaded from</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016">2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">J</forename>
								<forename type="middle">P</forename>
								<surname>Barton</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Ragon Institute of Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology and Harvard</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">E</forename>
								<surname>De Leonardis</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire de Physique Statistique de L&apos;Ecole Normale Supé rieure</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Ecole Normale Supé rieure &amp; Université P.&amp;M. Curie</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computational and Quantitative Biology</orgName>
								<orgName type="laboratory">UMR 7238</orgName>
								<orgName type="institution" key="instit1">UPMC</orgName>
								<orgName type="institution" key="instit2">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">A</forename>
								<surname>Coucke</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="department">Computational and Quantitative Biology</orgName>
								<orgName type="laboratory">UMR 7238</orgName>
								<orgName type="institution" key="instit1">UPMC</orgName>
								<orgName type="institution" key="instit2">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Laboratoire de Physique Thé orique de L&apos;Ecole Normale Supé rieure</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Ecole Normale Supé rieure &amp; Université P.&amp;M. Curie</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">S</forename>
								<surname>Cocco</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire de Physique Statistique de L&apos;Ecole Normale Supé rieure</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Ecole Normale Supé rieure &amp; Université P.&amp;M. Curie</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Departments of Chemical Engineering and Physics</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ACE: adaptive cluster expansion for maximum entropy graphical model inference Downloaded from</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Bioinformatics</title>
						<imprint>
							<biblScope unit="page" from="1" to="9"/>
							<date type="published" when="2016">2016</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw328</idno>
					<note type="submission">Received on March 25, 2016; revised on May 15, 2016; accepted on May 18, 2016</note>
					<note>Sequence analysis *To whom correspondence should be addressed. Associate Editor: John Hancock Supplementary information: Supplementary data are available at Bioinformatics online. 1 Original Paper Bioinformatics Advance Access http://bioinformatics.oxfordjournals.org/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Graphical models are often employed to interpret patterns of correlations observed in data through a network of interactions between the variables. Recently, Ising/Potts models, also known as Markov random fields, have been productively applied to diverse problems in biology, including the prediction of structural contacts from protein sequence data and the description of neural activity patterns. However, inference of such models is a challenging computational problem that cannot be solved exactly. Here, we describe the adaptive cluster expansion (ACE) method to quickly and accurately infer Ising or Potts models based on correlation data. ACE avoids overfit-ting by constructing a sparse network of interactions sufficient to reproduce the observed correlation data within the statistical error expected due to finite sampling. When convergence of the ACE algorithm is slow, we combine it with a Boltzmann Machine Learning algorithm (BML). We illustrate this method on a variety of biological and artificial datasets and compare it to state-of-the-art approximate methods such as Gaussian and pseudo-likelihood inference. Results: We show that ACE accurately reproduces the true parameters of the underlying model when they are known, and yields accurate statistical descriptions of both biological and artificial data. Models inferred by ACE more accurately describe the statistics of the data, including both the constrained low-order correlations and unconstrained higher-order correlations, compared to those obtained by faster Gaussian and pseudo-likelihood methods. These alternative approaches can recover the structure of the interaction network but typically not the correct strength of interactions , resulting in less accurate generative models. Availability and implementation: The ACE source code, user manual and tutorials with the example data and filtered correlations described herein are freely available on GitHub at https://github.com/kkrizanovic/NanoMark.
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interpreting patterns of correlations in data is a fundamental problem across scientific disciplines. A common approach to this problem is to infer a simple graphical model that explains the statistics of the data through a network of effective interactions between the variables, which may then be used to generate new predictions (<ref type="bibr" target="#b18">Friedman, 2004</ref>). The goal of this approach is to disentangle the direct interactions between variables from their correlations, which arise through a combination of direct and indirect effects. Here, we focus on a particular family of undirected graphical models, referred to as Potts models in the language of statistical physics, which have recently been applied to study a wide variety of biological systems. Applications include inference of the effective connectivity of populations of neurons, and their patterns of firing activity, based on data from multi-electrode recordings (<ref type="bibr" target="#b3">Barton and Cocco, 2013;</ref><ref type="bibr" target="#b9">Cocco et al., 2009;</ref><ref type="bibr" target="#b34">Roudi et al., 2009;</ref><ref type="bibr" target="#b35">Schneidman et al., 2006</ref>), and the prediction of protein contact residues (<ref type="bibr" target="#b29">Morcos et al., 2011</ref>) and the fitness effects of mutations (<ref type="bibr" target="#b15">Ferguson et al., 2013;</ref><ref type="bibr" target="#b16">Figliuzzi et al., 2016;</ref><ref type="bibr" target="#b26">Mann et al., 2014</ref>) based on the analysis of multiple sequence alignments (MSAs). Unfortunately, the inference of Potts models from data is challenging. The computational time required for naive Potts inference algorithms scales exponentially with the system size, rendering the problem intractable for realistic systems of interest. Various approximations have been employed to combat this problem, including Gaussian and mean-field inference (<ref type="bibr" target="#b25">Kappen and Rodr ıguez, 1998</ref>), perturbative expansions (<ref type="bibr" target="#b30">Nguyen and Berg, 2012;</ref><ref type="bibr" target="#b36">Sessak and Monasson, 2009</ref>) and pseudo-likelihood methods (<ref type="bibr" target="#b2">Aurell and Ekeberg, 2012;</ref><ref type="bibr" target="#b32">Ravikumar et al., 2010</ref>). These approximate methods can successfully capture the general structure of the network of interactions, recovering, in particular, contact residues in the threedimensional structure of protein families (<ref type="bibr" target="#b10">Cocco et al., 2013;</ref><ref type="bibr" target="#b13">Ekeberg et al., 2014;</ref><ref type="bibr" target="#b14">Feinauer et al., 2014;</ref><ref type="bibr" target="#b22">Hopf et al., 2012;</ref><ref type="bibr" target="#b27">Marks et al., 2011;</ref><ref type="bibr" target="#b29">Morcos et al., 2011;</ref><ref type="bibr" target="#b40">Sułkowska et al., 2012</ref>), but the resulting models typically give a less accurate statistical description of the data (<ref type="bibr" target="#b4">Barton et al., 2014</ref>). Alternately, algorithms based on iterative rounds of Monte Carlo simulation (<ref type="bibr" target="#b0">Ackley et al., 1985;</ref><ref type="bibr" target="#b26">Mann et al., 2014;</ref><ref type="bibr" target="#b41">Sutto et al., 2015</ref>) are capable of inferring models that accurately reproduce the observed correlations, but they are typically slow to converge. Here, we describe an extension of the adaptive cluster expansion (ACE) method, originally devised for binary (Ising) variables (<ref type="bibr">Monasson, 2011, 2012</ref>), to more general (Potts) variables taking multiple categorical values. We also describe new computational methods for faster inference, including a Monte Carlo learning procedure and the optional incorporation of prior knowledge about the structure of the interaction graph. The algorithm has been successfully applied to real data with as many as several hundred variables, including studies of neural activity in the retina and prefrontal cortex (<ref type="bibr" target="#b3">Barton and Cocco, 2013;</ref><ref type="bibr">Monasson, 2011, 2012;</ref><ref type="bibr" target="#b42">Tavoni et al., 2015</ref>), human immunodeficiency virus (HIV) fitness based on protein MSA data (<ref type="bibr" target="#b5">Barton et al., 2015;</ref><ref type="bibr" target="#b26">Mann et al., 2014</ref>), and lattice protein models (<ref type="bibr" target="#b23">Jacquin et al., 2016</ref>). Below we illustrate the application of this method to both real and artificial datasets. We show that models inferred by ACE give an excellent reconstruction of the statistics of the data. They also accurately recover, considering sampling limitations, true underlying model parameters when they are known, and can achieve comparable performance to state-of-the-art methods for predicting structural contacts in protein family data. We compare these results to those obtained using other approximate inference methods, focusing in particular on pseudolikelihood methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>The Potts model emerges naturally in the statistical description of complex systems. Consider a system of N variables described by the configuration x ¼ fx 1 ; x 2 ;. .. ; x N g, with x i 2 f1; 2;. .. ; q i g. The number of discrete categories q i that each variable x i can take on, which we refer to as states, may depend on the variable index i. For proteins the states correspond to particular amino acids, while for neurons they represent the binary (firing or silent) state of activity. Given a set of measurements of the system, the empirical average over the sampled configurations gives us the P i q i individual and P i &lt; j q i q j pairwise frequencies for the different states of each variable in the data. We denote the individual and pairwise frequencies by p i ðaÞ and p ij ða; bÞ, respectively, where i, j are the index of the variables and a, b are the index of the states. As an example, x could represent sequences in a MSA, with p i ðaÞ the frequency of the amino acid labeled by a in column i of the alignment, and p ij ða; bÞ the frequency of the pair of amino acids a, b in columns i, j. The simplest, or maximum entropy (<ref type="bibr" target="#b24">Jaynes, 1982</ref>), probabilistic model capable of reproducing the observed frequencies is an exponential distribution, which assigns a probability to every configuration of the system x:</p><formula>PðxÞ ¼ exp ÀEðxÞ ð Þ Z ; EðxÞ ¼ À X N i¼1 h i ðx i Þ À X NÀ1 i¼1 X N j¼iþ1 J ij ðx i ; x j Þ; Z ¼ X x exp ÀEðxÞ ð Þ:</formula><formula>(1)</formula><p>Here, the partition function Z is a normalizing factor which ensures that all probabilities sum to one. In the simple case that all the variables x i are binary, this model is referred to as an Ising model. More generally, when x i can take multiple discrete states, this model is referred to as a Potts model. The parameters h i ðaÞ and J ij ða; bÞ in the energy function E, called fields and couplings, must be chosen such that variable averages (correlations) in the model match those in the data, i.e.</p><formula>p i ðaÞ ¼ X x dðx i ; aÞPðxÞ; p ij ðaÞ ¼ X x dðx i ; aÞdðx j ; bÞPðxÞ;</formula><formula>(2)</formula><p>where d is the Kronecker delta function. The problem of finding the parameters h i ðaÞ; J ij ða; bÞ that satisfy Equation (2) is referred to as the inverse Potts problem. Note that the probability of any configuration remains unchanged under the transformation of the couplings and fields given by J ij ða; bÞ ! J ij ða; bÞ þ K ij ðaÞ þ K ji ðbÞ; h i ðaÞ ! h i ðaÞ À P j6 ¼i K ij ðaÞ for any K. In addition, all the h i ðaÞ at a site i can be uniformly shifted by a constant with no effect on the probability. This 'gauge invariance' reduces the number of free parameters in the Potts model to q i À 1 fields for each site and ðq i À 1Þðq j À 1Þ couplings for each pair of sites. Formally, the inverse Potts problem is solved by the set of fields and couplings that maximize the average log-posterior probability of the data or equivalently, those that minimize the cross-entropy between the data and the model</p><formula>(4)</formula><p>and P 0 is a prior distribution for the parameters. Here, for simplicity we have written the set of all individual and pairwise variable frequencies as p and the set of all fields and couplings as J. Note that, ignoring the contribution of the prior distribution, the cross-entropy S is equivalent to the entropy of the inferred model satisfying Equation (2). S has a simple interpretation in information theory as it can be written as the sum of the entropy of the data and the Kullback–Leibler divergence of the model with respect to the data (<ref type="bibr" target="#b38">Shannon, 1948</ref>), see also the related field of information geometry (<ref type="bibr" target="#b1">Amari, 1987</ref>). The inclusion of a prior distribution helps to avoid overfitting, while also improving convergence. A Gaussian prior distribution for the parameters is a typical choice, which contributes a term</p><formula>c 0 X N i¼1 X qi a¼1 h i ðaÞ 2 þ c X NÀ1 i¼1 X N j¼iþ1 X qi a¼1 X qj b¼1 J ij ða; bÞ 2 (5)</formula><p>to Equation (3). For c $ 1=B this factor can be thought of as a weakly informative prior (<ref type="bibr" target="#b19">Gelman et al., 2008</ref>) whose main purpose is to ensure that solutions of the inverse problem are not infinite due to issues of undersampling (e.g. parameters corresponding to an amino acid that is never observed). Note that this form of the regularization is not invariant under gauge transformations. Thus, the results of the inference including the regularization do have some dependence on the gauge choice. Other forms of regularization are also possible (see Supplementary Materials). Note that the presence of the partition function Z in Equation (4) precludes direct numerical maximization of the posterior when the system size is large, since this requires summing over all Q N i¼1 q i configurations of the system. Alternate methods of solving the inverse Potts problem involve approximation schemes or rely on computationally costly Monte Carlo simulations, as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adaptive cluster expansion</head><p>The adaptive cluster expansion (<ref type="bibr">Monasson, 2011, 2012</ref>) is based on the formal decomposition of the regularized crossentropy Equation (3) into a sum of contributions from subsets (clusters) of variables C ¼ fi 1 ;. .. ; i k g; k N,</p><formula>S ¼ X C DS C ; DS C ¼ S C À X C 0 &amp;C DS C 0 ; (6)</formula><p>where the sum is over all nonempty subsets of the N variables. The terms DS C , referred to as cluster entropies, have been recursively defined as the remaining contribution to the subset posterior once all contributions from smaller clusters have been substracted. Here, S C denotes the maximum of Equation (3) restricted only to the variables in C. Thus, S C depends only on the frequencies p i ðaÞ; p ij ða; bÞ with i; j 2 C. Provided that the number of variables in C is small (typically 20), numerical maximization of the posterior restricted to C is tractable. Note that, due to the recursive definition of DS C , the sum over all 2 N À 1 nonempty and overlapping subsets of the N variables in Equation (6) gives the exact posterior S by construction:</p><formula>X C DS C ¼ DS Ctot þ X C 0 &amp;Ctot DS C 0 ¼ S Ctot ¼ S :</formula><formula>(7)</formula><p>Here, C tot ¼ f1; 2;. .. ; Ng is the set of all variables in the system. The expansion of Equation (6) can be computationally expedient because, practically, it can converge toward S even when only contributions from clusters much smaller than the system size N are considered (see below). The cluster entropy contributions are easy to interpret for oneand two-site clusters: neglecting the regularization term the single variable cluster contributions are the entropies of the variables taken as if they were independent, DS i S i ¼ À P qi a¼1 p i log p i ðaÞ. The two variable entropy is S ij ¼ À P qi a¼1 P qj b¼1 p ij ða; bÞlog p ij ða; bÞ (seeThe rule for constructing new clusters of size k þ 1 from selected clusters of size k can be lax (such that a new cluster C is added provided that any pair of size k subclusters, C 1 ; C 2 2 L 0 k ðtÞ and C 1 [ C 2 ¼ C) or strict (such that a new cluster is only added if all of its k þ 1 subclusters of size k belong to L 0 k ðtÞ). The above process is then repeated until no new clusters can be constructed. After the summation of clusters terminates, the approximate value of the parameters minimizing the cross-entropy, given the current value of the threshold, is computed by</p><formula>JðtÞ ¼ X k X C2L 0 k ðtÞ DJ C ; DJ C ¼ J C À X C 0 &amp;C DJ C 0 : (8)</formula><p>Note that this formula generally yields sparse solutions because nonzero couplings are only included in Equation (8) if some clusters containing them have been selected. In this algorithm the dominant contribution to the computational complexity often comes from the evaluation of the partition function Z for large cluster sizes, which</p><formula>requires O Y i2C q i</formula><p>! operations to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Compression of the number of Potts states</head><p>As mentioned in Section 1.1, the number of states each variable may take on need not be the same for all variables in a system. States with zero (or otherwise very small) probabilities may be observed very infrequently in real, finitely-sampled data, and the relative error on the corresponding correlations due to finite sampling is large. To limit overfitting and reduce the computational time, the low probability states can be effectively grouped together according to a given compression parameter. Here, we present two heuristic conventions for compressed representations of the data. First, for each variable we can treat explicitly the states observed with probability larger than a cutoff value p i ðaÞ &gt; p o while grouping all infrequently observed values into the same state. A natural value for the cutoff is p o $ 1= ffiffiffi ffi B p , such that pair correlations between independent states with frequencies of p o are at the threshold of detection. Alternatively, we can order the states by their contribution to the total single site entropy S q and choose a reduced model in which only the first k states are modeled explicitly, with k chosen to capture a certain fraction f of the site entropy. This is achieved by explicitly considering the first k states and grouping the remaining q – k states together, choosing k such that</p><formula>S k ¼ À X k a¼1 pðaÞlog pðaÞ À 1 À X k a¼1 pðaÞ ! log 1 À X k a¼1 pðaÞ ! ! fS q :</formula><formula>(9)</formula><p>The frequency of the regrouped Potts state is then the sum of the frequencies of the states which have been regrouped: p i ðk þ 1Þ ¼ P q a¼kþ1 p i ðaÞ. Once the reduced model is inferred, one can recover a complete model by modifying the field parameter for the regrouped states, h i ða 0 Þ ¼ h i ðk þ 1Þ þ log p i ða 0 Þ=p i ðk þ 1Þ ð Þ , while keeping the couplings to the value of the regrouped state J ij ða 0 ; bÞ ¼ J ij ðk þ 1; bÞ. For states with zero probabilities in the data, we fix the fields from the regularization alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Expansion around a reference structure</head><p>ACE is a two-fold algorithm: it builds up the interaction graph while also inferring the parameters that reproduce the correlated structure of the data. This expansion can be accelerated if information about the interaction graph is available. It is also possible to expand the cross-entropy around its Gaussian approximation. @BULLET If the list of directly interacting variables is known, one can run the expansion starting from clusters built on the support of the interaction graph. For proteins this procedure can be applied using the real contact map, known from structural information, as the initial list of 2-site clusters. Alternatively, if the contact map is not known, one can use fast inference approaches such as DCA or plmDCA (<ref type="bibr" target="#b13">Ekeberg et al., 2014;</ref><ref type="bibr" target="#b29">Morcos et al., 2011</ref>) to obtain a list of initial putative contacts and then refine the expansion from this initial list (Supplementary Materials). @BULLET As shown in (<ref type="bibr" target="#b8">Cocco and Monasson, 2012</ref>) for the Ising model, one can analytically calculate the posterior and the parameters that maximize it under the Gaussian approximation with an ad hoc L 2-norm regularization (where the regularization strength depends on the variable frequencies). It is then possible to perform the cluster expansion around this Gaussian reference model, i.e. the expansion of S À S 0 , where S 0 is the cross-entropy for a Gaussian model</p><formula>S 0 ¼ 1 2 log det M; M ij ¼ p ij À p i p j ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi p i ð1 À p i Þp j ð1 À p j Þ p ; (10)</formula><p>in the Ising (binary) case. If S 0 is a reasonable approximation of S, then the expansion of S À S 0 may converge more rapidly than the expansion of S alone. See Cocco and Monasson (2012) for further details on the expansion of the Gaussian model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Refinement with Boltzmann Machine Learning (BML)</head><p>In cases where convergence of the cluster algorithm alone is not sufficiently fast, it is often more expedient to use the output set of fields and couplings as starting values for a Boltzmann Machine Learning (BML) routine. In typical cases, provided that the inferred model is not too sparse, this procedure can lead to rapid convergence of the model even when the starting error is large. Here, we adapted the RPROP algorithm for neural network learning (<ref type="bibr" target="#b33">Riedmiller and Braun, 1993</ref>) to the case of Potts models. Given an input set of fields and couplings, we first compute the model correlations p MC i ðaÞ; p MC ij ða; bÞ through Monte Carlo simulation. The couplings and fields are then updated according to the gradient of the posterior, multiplied by a parameter-specific weight factor</p><formula>h i ðaÞ ! h i ðaÞ À p MC i ðaÞ À p i ðaÞ À Á w i ðaÞ ;</formula><p>J ij ða; bÞ ! J ij ða; bÞ À p MC ij ða; bÞ À p ij ða; bÞ w ij ða; bÞ :</p><formula>(11)</formula><p>Regularization can also be incorporated by adding 2cJ ij ða; bÞ, or the analogous term for fields, to the gradient. Here, the weights w i ðaÞ and w ij ða; bÞ are also updated with each iteration of the algorithm. At each iteration, if the sign of p MC i ðaÞ À p i ðaÞ À Á is the same as in the previous round, w i ðaÞ ! s þ w i ðaÞ, else w i ðaÞ ! s À w i ðaÞ, and similarly for the w ij ða; bÞ. This acceleration of weight parameters allows appropriate step sizes to be chosen adaptively for each coupling and field. To prevent steps sizes from becoming too large or too small, the weight parameters are restricted to lie between some w min and w max. Typical choices of the weight bounds and update multipliers are w min ¼ 10 À3 ; w max ¼ 10; s þ ¼ 1:9; s À ¼ 0:5. Note that we choose sþ &lt; 1=s À so that, if the sign of one of the terms of the gradient continually switches, the corresponding weight decreases. As with other BML approaches, this procedure is computationally limited by the need to thermalize the system to accurately estimate the model correlations through MC. Each MC step requires a computation of the change in energy due to a change in the configuration x, requiring OðnÞ operations, where n is the typical 'neighborhood' size (i.e. number of sites to which another site couples with nonzero J ij ða; bÞ). Future refinements could improve the speed of this routine by implementing, for example, adaptive selection of the number of thermalization steps and more efficient Monte Carlo sampling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Description of test data and their preprocessing</head><p>Here, we apply ACE to five different datasets and test the reconstruction of their statistics. First, we generate artificial data from a Potts model with random fields and couplings, allowing us to test the ability of the algorithm to recover the true model parameters. Second, we infer a Potts model from artificial sequences generated by a 3 Â 3 Â 3 lattice protein model with large folding probabilities in a given structure. This folding probability (<ref type="bibr" target="#b37">Shakhnovich and Gutin, 1990</ref>) contains all-order interactions between amino acids, unlike the Potts model used for the inference, thus serving as an interesting benchmark test (<ref type="bibr" target="#b23">Jacquin et al., 2016</ref>). Third, we study trypsin inhibitor protein sequences (<ref type="bibr" target="#b10">Cocco et al., 2013;</ref><ref type="bibr" target="#b12">Ekeberg et al., 2013</ref><ref type="bibr" target="#b13">Ekeberg et al., , 2014</ref><ref type="bibr" target="#b29">Morcos et al., 2011</ref>) to compare structural predictions obtained by ACE to ones obtained using Gaussian (DCA) and pseudo-likelihood (plmDCA) methods. We then test the ability of the algorithm to infer a model describing the HIV nucleocapsid protein p7. Finally, we study multi-electrode recordings of neural activity in the prefrontal cortex of a rat (<ref type="bibr" target="#b31">Peyrache et al., 2009</ref>) analyzed in<ref type="bibr" target="#b42">Tavoni et al. (2015)</ref>to study memory replay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Potts models on Erd} os-Rényi random graphs (ER05)</head><p>We consider an example of a Potts model with q ¼ 21 states, where the network of interactions is described by an Erd} os-Rényi random graph with N ¼ 50 variables. Each edge in the interaction graph is included with probability 0.05. Field and coupling values for interacting pairs of sites are selected from a Gaussian distribution (Supplementary Materials). We compute the correlations through Monte Carlo sampling of B ¼ 10 4 configurations. In the results shown below we compressed rarely-observed Potts states with p i ðaÞ &lt; p o ¼ 0:05 and used c ¼ 1=B ¼ 10 À4 , performing the inference in the gauge of the compressed Potts state. 3.1.2 Lattice protein model (LP S B ) We consider an alignment of 5 Â 10 4 protein sequences with N ¼ 27 sites, arranged in a 3 Â 3 Â 3 cube, selected according to their exactly computable (<ref type="bibr" target="#b37">Shakhnovich and Gutin, 1990</ref>) folding probability S B (see (<ref type="bibr" target="#b23">Jacquin et al., 2016</ref>), Supplementary Materials). In the results below we have removed amino acids that are never observed (i.e. compression with p o ¼ 0), and used the regularization c ¼ 5=B ¼ 10 À4. Couplings and fields corresponding to the least frequently observed amino acid at each site are gauged to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Trypsin inhibitor protein family (PF00014)</head><p>We study an alignment of 4915 sequences downloaded from the PFAM database for the trypsin inhibitor protein family (PF00014, PFAM 28.0 release, May 2015). After removing columns with &gt; 50% gaps the number of sites is N ¼ 53. We reweight the contribution of each sequence to the correlations according to its similarity to other sequences in the alignment, an approach commonly used to attenuate phylogenetic correlations (<ref type="bibr" target="#b29">Morcos et al., 2011</ref>). Here, we show results in the consensus gauge after compressing rarely-observed amino acids with p i ðaÞ &lt; p o ¼ 0:05, using c ¼ 2=B ¼ 10 À3. Additionally, we note that gaps in the MSA are not generally modeled well in the Potts model representation with pairwise interactions, as they tend to be present in long stretches, especially at the beginning and the end of the alignment (<ref type="bibr" target="#b14">Feinauer et al., 2014</ref>). Such stretches of highly correlated gaps slow down the inference procedure because they give rise to large clusters. Here, we have processed the data to replace gaps by random amino acids with the same frequency as observed in the non-gapped sequences. Though this approach obscures the important variability in the sequence lengths in the MSA, it is a simple way to reduce computational problems induced by correlated gaps, valuable for structural prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">HIV p7 nucleocapsid protein</head><p>The HIV nucleocapsid protein p7 plays an essential role in multiple aspects of viral replication (<ref type="bibr" target="#b17">Freed, 2015</ref>). We downloaded a MSA of 4131 p7 sequences from individuals infected by clade B viruses from the Los Alamos National Laboratory HIV sequence database (www. hiv.lanl.gov, accessed October 6, 2014). After removing columns with &gt; 95% gaps, the remaining number of sites is N ¼ 71. Here, we do not reweight sequences by similarity, given that they are all in the same phylogenetic cluster, and the regular observation of similar sequences in the HIV population may be indicative of higher fitness (<ref type="bibr" target="#b15">Ferguson et al., 2013;</ref><ref type="bibr" target="#b39">Shekhar et al., 2013</ref>). We replaced gaps as described above, compressed rarely-observed amino acids with f S ¼ 90%, and chose c ' 1=2B ¼ 1:4 Â 10 À4. Inference is performed in the consensus gauge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Multi-electrode recordings of cortical neurons</head><p>We divided a 20 minute recording of the firing activity of 32 cortical neurons into a set of B ¼ 1:5 Â 10 5 time bins of 10ms, treating each time window as an observation of the system. During each time window, the variable for each neuron i was assigned x i ¼ 1 if the neuron was active at least once during that time, and zero otherwise. Here, we take c ¼ 1=B ¼ 6:6 Â 10 À6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convergence of the cluster expansion algorithm</head><p>As mentioned in Section 2.1, for each threshold t used to select clusters in the ACE expansion, the model individual hx i ðaÞi and pairwise hx ij ða; bÞi frequencies are compared to the data's frequencies p i ðaÞ and p ij ða; bÞ. We define a relative error as the ratio between the deviations of the predicted observables from the data, dhx i i ¼ hx i i À p i and dhx ij i ¼ hx ij i À p ij , and the expected statistical fluctuations due to finite sampling, dp i ðaÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiwhere M is the total number of one-and two-point correlations.<ref type="figure">Figure 1</ref>shows the behavior of max and the cross-entropy as a function of the threshold for the five datasets described above. The cross-entropy S approaches a constant value as the threshold is decreased. In all cases except for the lattice protein model, the algorithm converges at max $ 1, when the correlations are reproduced to within the expected error due to finite sampling. The expansion slows dramatically for the lattice protein model at a fairly high value of the threshold due to the large number of states included at each ACEsite in the model (typically q ¼ 19). The computational cost of calculating the partition function is a limiting factor as the maximum cluster size increases, corresponding to K max ¼ 7 at the stopping point in<ref type="figure">Figure 1</ref>. At this point BML is needed to refine the parameters inferred through the cluster expansion. Note that, even in cases when the error appears large, convergence of the BML procedure is often rapid because only small changes to the parameters may be necessary to obtain a model that accurately reproduces the correlations. Convergence of the algorithm can also be more difficult for alignments of long proteins or those with very strong interactions. In such cases one may observe large oscillations in the cross-entropy as a function of the threshold, and large ( 10 sites) clusters may appear even at high thresholds. Strong regularization (c &gt; 1=B) can help to dampen these oscillations, after which it can be returned to %1=B during the BML procedure.</p><formula>p i ðaÞð1 À p i ðaÞÞ=B p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameters of the ER05 model are recovered by ACE</head><p>In<ref type="figure">Figure 2</ref>we show that the 2 Â 10 4 underlying parameters for the ER05 model corresponding to the explicitly modeled Potts states are accurately recovered by ACE. These states are better sampled and therefore they have smaller statistical uncertainties. In the model inferred by plmDCA, which includes no reduction in the number of states, there are around 10 6 parameters. Those corresponding to the explicitly modeled states are recovered fairly well (with some errors in the fields), but parameters corresponding to compressed states are difficult to infer due to insufficient sampling (see Supplementary Materials for details and analysis of errors in inferred parameters due to finite sampling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Statistics of the data are accurately reproduced</head><p>Figures 3 and 4 show how the model inferred by ACE reproduces the statistics of the input data. In all cases the model accurately captures the input probabilities and pairwise connected correlations within the expected error due to finite sampling, as anticipated. We also find that higher order correlations in the data can be accurately reproduced.<ref type="figure" target="#fig_0">Figure 4</ref>shows the 3-point connected correlations and the distribution P(k) of Hamming distances k between the sampled configurations and the configuration in which each site takes on the most probable value (i.e. the consensus sequence for proteins). In the neural case the most probable configuration is the silent one and therefore P(k) is the probability to have k active neurons in the same time window. Models inferred by ACE outperform those from plmDCA (<ref type="bibr" target="#b13">Ekeberg et al., 2014</ref>), see<ref type="figure" target="#fig_1">Figure 3</ref>and Supplementary Materials for higher order statistics.in the data, such as the connected threebody correlations (center) and the probability P(k) of observing a configuration with k differences from the consensus configuration (right)</p><formula>(a) (b) (c) (d) (e)</formula><formula>(a) ( b)</formula><formula>(c) (e)</formula><p>(d)<ref type="figure">Fig. 1</ref>. Convergence of the cluster expansion as a function of the threshold t for (a) ER005, (b) LP S B , (c) PF00014, (d) HIV p7 and (e) cortical data. As the threshold is lowered, the cross-entropy S approaches a constant value. In all cases except for LP S B the normalized maximum error max reaches 1 through the cluster expansion alone. For LP S B a Monte Carlo learning procedure is used to refine the inferred parameters and reach max ' 1<ref type="figure">Fig. 2</ref>. ACE accurately recovers the true fields h (left) and couplings J (right) corresponding to Potts states with p i ðaÞ ! 0:05 for the ER05 model. Error bars denote standard deviation in estimated parameters due to finite samplingComparing the distribution of energies E for configurations sampled from the inferred model to the distribution obtained from the original data provides an additional check of statistical consistency. The energy of a configuration is proportional to the logarithm of its probability (in addition, because the entropy S is obtained from the cluster expansion, we can also compute the constant of proportionality). Concordance between the inferred and empirical energy distributions thus indicates that the real data could plausibly be generated from the inferred model.<ref type="figure" target="#fig_3">Figure 5</ref>compares the data and model distributions of energies, showing that in most cases they closely overlap. A small discrepancy is introduced in PF00014 because of the reweighting procedure (here, the histogram of the data is normalized by the sequence weights). The energy distribution for the lattice protein model is broader than for the data, though the peak is fit correctly. In contrast with models inferred using ACE, the distribution of energies of the data is less well reproduced with plmDCA (Supplementary Materials). The ability to estimate the probability of a configuration can be useful when comparing the likelihood of a configuration in two different models, for example to decide which family a given protein belongs to.</p><formula>(a) ( b) ( c) (d) ( e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">ACE accurately infers structural contacts for PF00014</head><p>In<ref type="figure" target="#fig_2">Figure 6</ref>, we use the inferred couplings to predict pairs of residues that are in contact in the folded protein structure for PF00014, and we compare results from ACE to the standard contact prediction methods DCA (<ref type="bibr" target="#b29">Morcos et al., 2011</ref>) and plmDCA (<ref type="bibr" target="#b13">Ekeberg et al., 2014</ref>). In this case the pairs of sites for which the Frobenius norm of the couplings is largest, including the average product correction (APC, see (<ref type="bibr" target="#b11">Dunn et al., 2008</ref>)), are predicted to be most likely to be in contact. We define contact residues to be those that are within 6Å of each other in the folded structure of the protein, and we exclude trivial contact pairs along the protein backbone (i À j 4). The accuracy of contact predictions with ACE can be increased by decreasing the compression (p o ¼ 0) and using a large regularization (c ¼ 1), in the same spirit as the strong regularization employed in typical DCA and plmDCA approaches. Here, we gauged the parameters for the least frequently observed amino acids to zero and computed the Frobenius norm of the couplings in the zero sum gauge (as is typical in DCA). The couplings are then strongly damped by regularization and the cluster expansion converges for maximal cluster sizes much smaller than those needed in the case with weaker regularization.<ref type="figure" target="#fig_2">Figure 6b</ref>shows that the precision in this case is competitive with the one obtained from plmDCA, and the prediction of the first $30 contacts is slightly better for ACE. However, in this case we note that because of the small values of the couplings the generative properties of the inferred model are lost (see Supplementary Materials for the statistical fit of the model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Potts models have been successfully applied to study a variety of biological systems. However, the computational difficulty of the inverse Potts problem, i.e. the inference of a Potts model from correlation data, has presented a barrier to their use. Here, we presented ACE, a flexible, easy-to-use method for solving the inverse Potts problem, which can be applied to analyze a wide variety of real and synthetic data. We also provide tools for automatically generating correlation data from multiple sequence alignments (MSA), making the analysis of this type of data even more accessible. Here, we have adapted the complexity of the inferred Potts models to the level of the sampling in the data. This is achieved by regrouping less frequently observed Potts states into a unique state (according to a threshold on entropy or frequency), then by a sparse inference procedure that omits interactions that are unnecessary for reproducing the statistics of the data to within the error bounds due to finite sampling. On artificial data we verified that compression of the number of Potts states allows for a faster and more precise inference of the uncompressed model parameters while reducing overfitting. The methods of compression that we describe here can also be applied to other inference methods (including, for example, the DCA and plmDCA approaches discussed above), a topic of future study. In addition, as described above ACE yields sparser models when sampling is poor, leading to more robust inference. This method allows for the simple construction of models from various types of data, which can then be used to predict the evolution of experimental systems and their response to perturbations. Previous work has demonstrated promising applications of such models in a variety of different biological contexts. In neuroscience, the analysis of multi-electrode recordings has led to models that identify cell assemblies, which are thought of as basic units of neural computation and memory (<ref type="bibr" target="#b21">Hebb, 1949;</ref><ref type="bibr" target="#b31">Peyrache et al., 2009;</ref><ref type="bibr" target="#b42">Tavoni et al., 2015</ref>).Here, we show the top 100 predicted contacts, with true predictions in orange and false predictions in blue. Other contact residues in the crystal structure are shown in gray. For true positives and other contact residues, close contacts (&lt;6Å ) are darkly shaded and further contacts (&lt;8Å ) are lightly shaded. The upper and lower triangular parts of the contact map give predictions for the inferred model with strong regularization/no compression (c ¼ 1) and weak regularization/ high compression (c ¼ 2=B), respectively. (b) Precision (number of true predictions divided by the total number of predictions) as a function of the number of contact predictions for close contact residues that are widely separated on the protein backbone (i À j &gt; 4). Results using ACE compare favorably with those from DCA (<ref type="bibr" target="#b29">Morcos et al., 2011</ref>) and are competitive with those from plmDCA (<ref type="bibr" target="#b13">Ekeberg et al., 2014</ref>)</p><formula>(a) ( b)</formula><formula>(a) ( b) ( c) (d) ( e)</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.4.</head><figDesc>Fig. 4. Fit for models describing (a) ER005, (b) LP S B , (c) PF00014, (d) HIV p7 and (e) cortical activity. ACE recovers the connected pair correlations c ij ða; bÞ ¼ p ij ða; bÞ À p i ðaÞp j ðbÞ (left). The inferred model also successfully captures higher order correlations present in the data, such as the connected threebody correlations (center) and the probability P(k) of observing a configuration with k differences from the consensus configuration (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.3.</head><figDesc>Fig. 3. ACE outperforms plmDCA in recovering the single variable frequencies for models describing (a) ER005, (b) LP S B , (c) PF00014, (d) HIV p7 and (e) cortical activity. The results for plmDCA are obtained with the regularization c ¼ 0:01, which gives better results for the correlations than lower values of the regularization strength (see Supplementary Materials)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.6.</head><figDesc>Fig. 6. (a) Contact map for PF00014 inferred by ACE. Here, we show the top 100 predicted contacts, with true predictions in orange and false predictions in blue. Other contact residues in the crystal structure are shown in gray. For true positives and other contact residues, close contacts (&lt;6Å ) are darkly shaded and further contacts (&lt;8Å ) are lightly shaded. The upper and lower triangular parts of the contact map give predictions for the inferred model with strong regularization/no compression (c ¼ 1) and weak regularization/ high compression (c ¼ 2=B), respectively. (b) Precision (number of true predictions divided by the total number of predictions) as a function of the number of contact predictions for close contact residues that are widely separated on the protein backbone (i À j &gt; 4). Results using ACE compare favorably with those from DCA (Morcos et al., 2011) and are competitive with those from plmDCA (Ekeberg et al., 2014)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.5.</head><figDesc>Fig. 5. Histograms of the data (MSA) and model (MC) energy distributions for (a) ER005, (b) LP S B , (c) PF00014, (d) HIV p7 and (e) cortical activity. Monte Carlo sampling of the inferred Potts model describing each set of data yields a distribution of energies similar to the empirical distribution, a further check on the consistency of the model fit beyond the fitting of correlations</figDesc></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Supplementary Materials for more details). The cluster entropy for a pair of variables is then DS ij ¼ S ij À S i À S j , which is equivalent to the mutual information. It is zero when p ij ða; bÞ ¼ p i ðaÞ p j ðbÞ, i.e. when the two variables are independent. More generally, DS C is a measure of the interdependence between the variables in the cluster which cannot be accounted for by smaller clusters. The main idea of this approach is to approximate the crossentropy (and simultaneously, the parameters that maximize it) by limiting the sum in Equation (6) to a restricted set of clusters C that give the most important contributions to it. As shown in (Cocco and Monasson, 2011, 2012), contributions for overlapping clusters sharing the same interaction subgraph partially compensate, and thus summing clusters according to the magnitude of their entropy contribution allows for a faster convergence of Equation (6). Neglecting clusters with small contributions to the cross-entropy also helps to avoid overfitting. As a simple example, for a unidimensional interaction graph in which each variable is only connected to its two nearest neighbors, the expansion can be exactly truncated by summing only clusters of size one and the largest contributing 2-site clusters containing neighboring variables (Cocco and Monasson, 2012; Gori and Trombettoni, 2011; Mastromatteo, 2013). We define a threshold t on the cross-entropy to separate the significant clusters from those which can be neglected. Starting from a large value of the threshold (typically t ¼ 1), such that only a few clusters are selected, the algorithm proceeds through two nested iterations. The outer loop is on the value of the threshold t, which is progressively lowered until enough clusters are included to yield a model consistent with the data. The inner loop constructs the set of clusters C with contributions to the cross-entropy jDS C j &gt; t and yields an approximation of the cross-entropy and the model parameters at the threshold t. The algorithm stops at the first value of the threshold t where the inferred model fits the sampled averages and correlations Equation (2) to within the statistical error due to finite sampling (see Section 3.2). The algorithm for the inner loop, including the selection and summation of individual clusters, is as follows. Given a list L k of clusters of size k, beginning with the list of all clusters of size k ¼ 2:</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work originates from the development of ACE in the Ising case in collaboration with R. Monasson, to whom we are grateful for many helpful discussions. We also thank U. Ferrari for his contribution to the development of the adaptive Monte Carlo sampling to evaluate the reconstruction errors in the inference, D. Murakowski for his contribution to the development of the partition function expansion, and H. Jacquin for useful discussions.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">A learning algorithm for Boltzmann machines</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ackley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Differential geometrical theory of statistics</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Amari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Differential Geometry in Statistical Inference</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="20" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Inverse Ising inference using all the data</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Aurell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ekeberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">90201</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Ising models for neural activity inferred via selective cluster expansion: structural and coding properties</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Barton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cocco</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Mech.: Theory Expe</title>
		<imprint>
			<biblScope unit="page">3002</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Large pseudocounts and L2-norm penalties are necessary for the mean-field inference of Ising and Potts models</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Barton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="page" from="90" to="012132" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling laws describe memories of host–pathogen riposte in the HIV population</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Barton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="1965" to="1970" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">On the entropy of protein families</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Barton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Phys</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive cluster expansion for inferring Boltzmann machines with noisy data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cocco</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Monasson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">90601</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive cluster expansion for the inverse Ising problem: convergence, algorithm and tests</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cocco</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Monasson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Phys</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="252" to="314" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Neuronal couplings between retinal ganglion cells inferred by efficient inverse statistical physics methods</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cocco</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="14058" to="14062" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">From principal component to direct coupling analysis of coevolution in proteins: low-eigenvalue modes are needed for structure prediction</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cocco</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1003176</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">D</forename>
				<surname>Dunn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="333" to="340" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved contact prediction in proteins: using pseudolikelihoods to infer Potts models</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ekeberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">12707</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast pseudolikelihood maximization for directcoupling analysis of protein structure from many homologous amino-acid sequences</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ekeberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">276</biblScope>
			<biblScope unit="page" from="341" to="356" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving contact prediction along three dimensions</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Feinauer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1003847</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Translating HIV sequences into quantitative fitness landscapes predicts viral vulnerabilities for rational immunogen design</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L</forename>
				<surname>Ferguson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Immunity</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="606" to="617" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Coevolutionary landscape inference and the contextdependence of mutations in beta-lactamase TEM-1</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Figliuzzi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Evol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="268" to="280" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">HIV-1 assembly, release and maturation</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">O</forename>
				<surname>Freed</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Microbiol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="484" to="496" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Inferring cellular networks using probabilistic graphical models</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page" from="799" to="805" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">A weakly informative default prior distribution for logistic and other regression models</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gelman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1360" to="1383" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">The inverse ising problem for onedimensional chains with arbitrary finite-range couplings</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Gori</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Trombettoni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Mech.: Theory Exp</title>
		<imprint>
			<biblScope unit="page">10021</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">The Organization of Behavior: A Neurophysiological Approach</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">O</forename>
				<surname>Hebb</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Three-dimensional structures of membrane proteins from genomic sequencing</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">A</forename>
				<surname>Hopf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="1607" to="1621" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Benchmarking inverse statistical approaches for protein structure and design with exactly solvable models</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jacquin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1004889</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">On the rationale of maximum-entropy methods</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">T</forename>
				<surname>Jaynes</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>. IEEE</meeting>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="939" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient learning in Boltzmann machines using linear response theory</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">J</forename>
				<surname>Kappen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">B</forename>
				<surname>Rodr Iguez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1137" to="1156" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">The fitness landscape of HIV-1 Gag: advanced modeling approaches and validation of model predictions by in vitro testing</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Mann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1003776</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Protein 3D structure computed from evolutionary sequence variation</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">S</forename>
				<surname>Marks</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">28766</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond inverse Ising model: structure of the analytical solution</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Mastromatteo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Phys</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="658" to="670" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Direct-coupling analysis of residue coevolution captures native contacts across many protein families</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Morcos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1293" to="1301" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Bethe–Peierls approximation and the inverse Ising problem</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">C</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Berg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Mech.: Theory Exp</title>
		<imprint>
			<biblScope unit="page">3004</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Replay of rule-learning related neural patterns in the prefrontal cortex during sleep</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Peyrache</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="919" to="926" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">High-dimensional Ising model selection using l1regularized logistic regression</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ravikumar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1287" to="1319" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: The rprop algorithm</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Riedmiller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Braun</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Ising model for neural data: Model quality and approximate methods for extracting functional connectivity</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Roudi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="page" from="79" to="051915" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Weak pairwise correlations imply strongly correlated network states in a neural population</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Schneidman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="1007" to="1012" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Small-correlation expansions for the inverse Ising problem</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Sessak</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Monasson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A: Math. Theor</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">55001</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Enumeration of all compact conformations of copolymers with random sequence of links</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Shakhnovich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gutin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="5967" to="5971" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">E</forename>
				<surname>Shannon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Spin models inferred from patient-derived viral sequence data faithfully describe HIV fitness landscapes</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Shekhar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">62705</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">Genomics-aided structure prediction</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">I</forename>
				<surname>Sułkowska</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="10340" to="10345" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">From residue coevolution to protein conformational ensembles and functional dynamics</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Sutto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci</title>
		<meeting>. Natl. Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<monogr>
		<title level="m" type="main">Inferred model of the prefrontal cortex activity unveils cell assemblies and memory replay</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Tavoni</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>bioRxiv. 10.1101/028316</note>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACE</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>