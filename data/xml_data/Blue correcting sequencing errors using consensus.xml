
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence analysis Blue: correcting sequencing errors using consensus and context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">19 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Paul</forename>
								<surname>Greenfield</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO Computational Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of IT</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Konsta</forename>
								<surname>Duesing</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="institution">CSIRO Animal, Food and Health Sciences</orgName>
								<address>
									<postCode>2113</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Alexie</forename>
								<surname>Papanicolaou</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="institution">CSIRO Ecosystem Sciences</orgName>
								<address>
									<postCode>2601</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Associate Editor: Inanc Birol</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Denis</forename>
								<forename type="middle">C</forename>
								<surname>Bauer</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO Computational Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence analysis Blue: correcting sequencing errors using consensus and context</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="page" from="2723" to="2732"/>
							<date type="published" when="2014">19 2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu368</idno>
					<note type="submission">Received on September 16, 2013; revised on May 20, 2014; accepted on May 27, 2014</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Contact: paul.greenfield@csiro.au Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Bioinformatics tools, such as assemblers and aligners, are expected to produce more accurate results when given better quality sequence data as their starting point. This expectation has led to the development of stand-alone tools whose sole purpose is to detect and remove sequencing errors. A good error-correcting tool would be a transparent component in a bioinformatics pipeline, simply taking sequence data in any of the standard formats and producing a higher quality version of the same data containing far fewer errors. It should not only be able to correct all of the types of errors found in real sequence data (substitutions, insertions, deletions and uncalled bases), but it has to be both fast enough and scalable enough to be usable on the large datasets being produced by current sequencing technologies, and work on data derived from both haploid and diploid organisms. Results: This article presents Blue, an error-correction algorithm based on k-mer consensus and context. Blue can correct substitution, deletion and insertion errors, as well as uncalled bases. It accepts both FASTQ and FASTA formats, and corrects quality scores for corrected bases. Blue also maintains the pairing of reads, both within a file and between pairs of files, making it compatible with downstream tools that depend on read pairing. Blue is memory efficient, scalable and faster than other published tools, and usable on large sequencing datasets. On the tests undertaken, Blue also proved to be generally more accurate than other published algorithms, resulting in more accurately aligned reads and the assembly of longer contigs containing fewer errors. One significant feature of Blue is that its k-mer consensus table does not have to be derived from the set of reads being corrected. This decoupling makes it possible to correct one dataset, such as small set of 454 mate-pair reads, with the consensus derived from another dataset, such as Illumina reads derived from the same DNA sample. Such cross-correction can greatly improve the quality of small (and expensive) sets of long reads, leading to even better assemblies and higher quality finished genomes. Availability and implementation: The code for Blue and its related tools are available from http://www.bioinformatics.csiro.au/Blue. These programs are written in C# and run natively under Windows and under Mono on Linux.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The introduction of the first 454 Life Sciences sequencer in 2005 marked the beginning of a revolution in biological research. Sequencing technology has continued to advance rapidly, producing ever more data at a lower cost, but the quality of these data have improved at a much slower rate. A single run on an Illumina HiSeq 2500 system can now produce up to 8 billion paired-end reads, but these will still have an overall error rate of 1â€“2% (<ref type="bibr" target="#b15">Minoche et al., 2011</ref>). The nature of these errors depends on the sequencing technology being used and its underlying biochemistry. The single base-at-a-time 'sequencing by synthesis' technique used by Illumina results mostly in substitution errors (<ref type="bibr" target="#b16">Nakamura et al., 2011</ref>). Technologies based on different chemistries, such as those used by 454 and Ion Torrent systems, are prone to misreport the length of strings of the same base (homopolymers), resulting in insertion and deletion errors (<ref type="bibr" target="#b3">Glenn, 2011;</ref><ref type="bibr" target="#b14">Loman et al., 2012</ref>). The tools used to analyze sequence data are all error-tolerant to some extent. Aligners will tolerate some number of mismatches when they are mapping reads to a reference, some of which will prove to be errors and other genuine differences between the organism being sequenced and the reference (<ref type="bibr" target="#b10">Langmead and Salzberg, 2012;</ref><ref type="bibr" target="#b12">Li and Durbin, 2010</ref>). Similarly, assemblers can be built to tolerate errors to some degree, and their success at doing this is a significant factor in their overall effectiveness and accuracy (<ref type="bibr" target="#b1">Bradnam et al., 2013</ref>). An alternative way of addressing the problem of sequencing errors is to use a stand-alone error-correction tool whose sole purpose is to take a set of reads and improve their quality by finding and fixing errors. Such tools are founded on the high levels of redundancy present in typical sequencing datasets, with each location in the sequenced genome being covered by many reads, most of which will agree about which base is actually present.<ref type="bibr" target="#b23">Yang et al. (2013)</ref>recently surveyed a number of the published error-correction tools and categorized them into three classes of algorithms: k-spectrum based, suffix tree/array based and multiple sequence alignment based. The three classes of algorithm differ both in how they detect errors and how these errors are corrected. We refer the reader to<ref type="bibr" target="#b23">Yang et al., (2013)</ref>for a full discussion of these three classes of algorithms and their history. Blue is a k-spectrum algorithm that uses read context to choose between alternative replacement k-mers, with the overall goal of minimizing the number of changes needed to correct an entire read. All k-spectrum based algorithms first tile their input reads to produce a set of distinct overlapping subsequences of length 'k'</p><p>*To whom correspondence should be addressed. ('k-mers') together with their repetition counts. Such a set can then be used to distinguish k-mers that come from the organism being sequenced (and so recur many times) from those that are derived from reads containing sequencing errors (typically only appearing once or a few times).<ref type="figure" target="#fig_0">Figure 1</ref>shows a k-mer repetition histogram for a set of Illumina reads derived from a typical bacterium (Clostridium sporogenes PA3679). Those k-mers from the error-free parts of reads will have repetition counts that lie somewhere on the right-hand side (RHS) peak in this histogram.<ref type="figure">Figure 2</ref>shows a comparable histogram for a strongly heterozygous diploid organism (Helicoverpa armigera) with two (overlapping) peaks, one corresponding to the k-mers found on both alleles, and the other to those k-mers found on only one. Finally,<ref type="figure">Figure 3</ref>shows the histogram derived from tiling four lanes of Homo sapiens data (Illumina HiSeq from SRA ERR091571 to ERR091574). Given such a 'consensus' table of k-mers and counts, a repetition depth threshold can then be used to identify 'good' k-mers, as shown in<ref type="figure" target="#fig_0">Figure 1</ref>. A simple datasetwide threshold is unlikely to be usable though, as uneven coverage along a genome and the presence of repetitive regions is likely to result in the rejection of correct k-mers in poorly covered areas and the acceptance of sequencing errors in highcoverage areas. Blue uses a partitioned hash table to hold the k-mers corresponding to the RHS peaks: the 'consensus' about what k-mers are really present in the genome being sequenced. The data loaded into this table is generated by the associated tiling tool, Tessel, which simply takes a set of reads, tiles it into overlapping k-mers and writes out a file of distinct canonical k-mers and their repetition counts (for each strand). Decoupling the building of the consensus from the correction algorithm in this way makes it possible to use Blue to 'cross-correct' read datasets, such as using a large and inexpensive Illumina dataset to correct a smaller, more expensive but longer set of 454 reads. This style of crosscorrection results in a 454 dataset that conforms to the consensus found in the Illumina data, effectively generating long Illumina reads that can be used to great effect in assemblies and when finishing genomes. Repetitive regions in genomes, including ribosomes, transposons and shared regulatory sequences, are challenging for all error correction algorithms. Reads that cross the boundaries of these repeated regions may be erroneously 'corrected', as the change in depth of coverage at their edges may look very much like an error. The choice of which possible fix is correct (including doing nothing) really depends on context, and cannot simply be decided purely by considering a single k-mer or similar short sequence in isolation. Blue addresses this problem by evaluating alternative fixes in the context of the read being corrected. The metrics computed for every alternative reflect the impact that each one would have on the rest of the readâ€“â€“will this fix get us to the end of the read with no (or few) additional fixes, or will we have to effectively rewrite much of the rest of the read? It does this by recursively exploring the tree of potential corrected reads. The next section discusses the approach we took to testing Blue's performance and effectiveness, and comparing it with other published tools. Section 3 describes the Blue error correction algorithm, and Section 4 discusses the results of the performance and effectiveness tests. Section 5 discusses future work and possible improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEM AND METHODS</head><p>The comparative performance and effectiveness studies described in Section 4 of this article were initially based on the work reported in the survey paper by Yang et al. We chose the same set of tools for our evaluations, except that we dropped the poorly ranked SOAPec (<ref type="bibr" target="#b13">Li, 2010</ref>) and added SHREC, BLESS and RACER. The final set of tools we used in our comparisons were BLESS (<ref type="bibr" target="#b5">Heo et al., 2014</ref>), Coral (<ref type="bibr" target="#b20">Salmela and Schroder, 2011</ref>), Echo (<ref type="bibr" target="#b8">Kao et al., 2011</ref>), HiTEC (<ref type="bibr" target="#b6">Ilie et al., 2011</ref>), HSHREC (<ref type="bibr" target="#b19">Salmela, 2010</ref>), Quake (<ref type="bibr" target="#b9">Kelley et al., 2010</ref>), RACER (<ref type="bibr" target="#b7">Ilie and Molnar, 2013</ref>), Reptile (<ref type="bibr" target="#b22">Yang et al., 2010</ref>) and SHREC (<ref type="bibr" target="#b21">Schroder et al., 2009</ref>The capabilities of these tools are summarized in<ref type="figure" target="#tab_1">Table 1</ref>. The first two columns show whether the tools will accept sequence data files in the two common formats (FASTA and FASTQ), and the third column shows whether this file format is preserved through the correction process. Most of these error-correction tools can only find and fix substitution errors, as shown in column 4. This is a considerable simplification, as each possible fix, such as replacing an 'A' by a 'C' at some position in a read, can then only be achieved in one wayâ€“â€“by substitution. In the general case, the same alteration can also be made by inserting a 'C' before the 'A' and by deleting the 'A' if it was followed by a 'C'. This simplification may be acceptable for Illumina data where insertion and deletion errors are uncommon (<ref type="bibr" target="#b15">Minoche et al., 2011</ref>), but it means that such tools cannot be effectively used with 454 and Ion Torrent datasets. Another common limitation, shown in column 5, is to discard any reads containing uncalled bases ('N') rather than attempting to fix them. This can result in higher quality corrected datasets, as many of the lower quality reads are discarded as a consequence, at the expense of not fixing some easily correctable reads. Column 6 shows whether the tools can correct variablelength reads, and column 7 shows whether the tools maintain pairings between reads across multiple sequence files. The last column shows whether the tool is multi-threaded and so can take advantage of multiple processors to improve performance. The most straightforward (and commonly used) way to demonstrate the effectiveness of an error correction is to use a synthetic dataset constructed by sampling a sufficient number of 'read'-length strings from a reference genome and altering them to produce reads with known errors. The efficacy of an algorithm can then be measured by counting how many of these known errors were properly corrected, and how many erroneous fixes were made to unbroken reads. Using synthetic data in this way makes it straightforward to measure the effectiveness of an algorithm in terms of sensitivity and specificity. The disadvantage of synthetic data is that it lacks the complexity and sequencing artifacts found in real sequencing datasets, and an algorithm's performance on synthetic data may not always be realized on real datasets. The alternative to synthetic data is to use actual sequence data derived from a sample of a known organism with a well-curated reference genome. This is more realistic, but quantifying accuracy becomes difficult, as there are unknown differences between a reference genome and the sequenced organism. The approach used by Yang et al. was to determine the 'correct' form of each read by mapping them back to a reference genome. Unmapped and multi-mapped reads were discarded from the test datasets before correction to make it possible to calculate sensitivity and specificity metrics. This simplification unfortunately greatly reduces the complexity of the task facing a correction algorithm, as it eliminates both badly broken reads and reads from repetitive regions. In this article we chose instead to use full, unfiltered, sequencing datasets for all performance and effectiveness tests, as this more accurately reflects real usage. One technique used in this article to evaluate correction accuracy is to align the corrected reads back to their corresponding reference genome, using the Bowtie2 aligner (<ref type="bibr" target="#b10">Langmead and Salzberg, 2012</ref>, version 2.1.0), and count how many of them align perfectly without requiring any further edits by the aligner. If there were no sequencing artifacts in these datasets and the organism being sequenced was identical to the reference strain and the aligner was perfect, then all the reads in a properly 'corrected' dataset would align perfectly. Reality is somewhat different, of course, and even an ideally corrected set of reads will have some reads that cannot be perfectly aligned. The inability to get 100% perfect alignment does not matter for our comparisons though, as it will affect all error-correction tools equally, and more accurate correction will still be reflected in a high perfectly aligned count. We also measure the effects of correction on assembly, with the expectation that more accurate correction will result in improved assemblies. Measuring the quality of an assembly is a challenging problem (<ref type="bibr" target="#b1">Bradnam et al., 2013</ref>), and customary measures such as maximum contig length and N50 can be misleading, as they do not reflect the quality of the contigs produced. Rather than devise our own metrics for the quality of an assembly we chose to primarily use the Mauve Assembly Metrics (<ref type="bibr" target="#b2">Darling et al., 2011</ref>). This tool takes a set of assembled contigs and the corresponding reference sequence, and generates a number of quality metrics, including an estimate of the number of miscalls and rearrangements (possible chimeric contigs). We assembled the corrected reads using Velvet v 1.2.07 (<ref type="bibr" target="#b25">Zerbino and Birney, 2008</ref>), and then used the downloaded Mauve scripts to evaluate the quality of each of the assemblies.The datasets used by Yang et al. (2013) to compare the effectiveness of error-correction tools were generated using older sequencing technologies, and can no longer be considered typical or representative. We instead chose three more recent, publicly available datasets derived from well-characterized organisms for our comparisons and evaluations. We also included one dataset from Yang et al. to let us compare our results back to their findings and two 454 datasets. The datasets we used in this study are shown in<ref type="figure" target="#tab_2">Table 2</ref>. These datasets were used for comparative studies of performance of the various tools, the accuracy of the corrections made and the impact of these corrections on assemblies (bacterial datasets only). The two 454 Escherichia coli datasets were also used to demonstrate the effectiveness of cross-correcting 454 reads with Illumina data. The results of all of these tests are discussed in Section 4. We attempted to correct all four of these datasets using all of the programs listed in<ref type="figure" target="#tab_1">Table 1</ref>, with no pre-filtering to remove multi-mapped or non-mapping reads. The two Illumina sample datasets were converted from BAM to FASTQ, and the first three datasets were also re-paired as small numbers of reads had been dropped from each of the files, presumably as a result of QC processing. Quake failed with a segmentation fault while it was generating k-mers using Jellyfish, and so it was removed from the set of tools under test. ECHO was used to correct the smallest dataset (Pseudomonas), but as this took 3.4 days (<ref type="figure" target="#tab_3">table 3</ref>), it was considered infeasible to use on the larger datasets. Yang et al. also encountered difficulties using these two tools and omitted them from most of their tests. BLESS failed with an out-of-memory error on the Human Chr21 dataset. All of the programs were run in accordance with the instructions provided by the developers, following whatever guidance was supplied about the optimal settings for any parameters. The full description of the programs tested and the parameters used for each dataset can be found in the Supplementary Data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM</head><p>Blue is based on a k-spectrum correction algorithm and attempts to transform each faulty read that it discovers into a closely related read that consists only of 'good' k-mers, and to do so using the fewest possible fixes. The critical decision about which replacement k-mer to use at any point is based primarily on the impact such a fix would have on the remaining (uncorrected) part of the read (the context of the fix). The Blue algorithm generates the metrics used in this decision by performing a depth-first fix-limited exploration of the tree of corrected reads that could be plausibly generated from an initial one containing an identified 'faulty' k-mer. The algorithm is defined in pseudocode in<ref type="figure" target="#fig_2">Figure 5</ref>. Blue's first step when processing each read is to calculate its appropriate threshold values. It does this by looking up the repetition depths for all of read's constituent k-mers and calculating their harmonic mean, excluding any k-mers with depths below the specified 'minReps' parameter. Using the harmonic mean reduces the impact of any small highly repetitive regions within a read. This mean is then used to set the 'OK' depth threshold (one-third of the adjusted mean) for the read. This calculation is done for every read, rather than using the same values for all reads. This allows Blue to effectively find and fix errors in high coverage regions, while avoiding erroneously 'correcting' reads from regions with naturally low depth of coverage.<ref type="figure" target="#fig_1">Figure 4</ref>shows the 25-mer depths for a single Illumina HiSeq read with substitution errors at bases 46, 54 and 86. Blue set the 'OK' depth level for this read to 60. Blue then calls TryHealingRead to do a left to right pass over the read, looking for any k-mer whose repetition depth falls below the 'OK' level, and also checking for other potential signs of errors, such as sudden drops in depth and the end of homopolymer runs (if the '-hp' option is set). If no such k-mers are found, Blue just writes the read to the corrected-reads file and moves on to the next read, otherwise it tries to correct the justidentified potentially faulty k-mer by calling TryHealingMer. This recursive function takes a k-mer (and the current read variant in which it appears for when it is called recursively) and sees whether any of its possible variants would improve the quality of the remainder of the read. It does this by first generating a set of possible fixes by calling FindPlausibleVariants and then selects the best of these, using the downstream quality metrics generated for each variant. If such a variant k-mer is found, it is used to correct the faulty k-mer, and TryHealingMer returns to TryHealingRead to continue the left-to-right scan of the read being corrected. FindPlausibleVariants works by first generating all possible variants of its k-mer, culling those with too-low repetition depth or without 'pair' support (see next paragraph), and then calling CountFollowers to calculate the downstream ('followers') metrics for each of the surviving k-mers. CountFollowers scans the read downstream of the current k-mer, accumulating metrics as it goes, and recursively calls TryHealingMer to fix any errorsThe tree exploration is error-limited to reduce its computational cost, and only a small number of fixes (three) are allowed along any one path before its exploration is abandoned and its current metrics returned for comparison. A worked example of the tree exploration process can be found in the Supplementary Data (Section 1). Blue can also make use of k-mer 'pairs' to further prune nonviable paths, improving both accuracy and performance. Pairs are two separated shorter k-mers (currently 16-mers with a readlength-dependent gap) and are generated by tiling a set of reads, using the consensus k-mer table generated by Tessel to cull those pairs containing likely errors. Pairs provide some 5 0-end context for each possible replacement k-mer, and improve correction in a few cases where a k-mer looks to be 'correct' but actually appears in a different genomic context. One effect of the sequential nature of the scanning loop in TryHealingRead is that FindPlausibleVariants usually only has to generate variants of the last base in its current k-mer, as all the other variants were considered in the preceding k-mers. This simplification greatly reduces the number of variants thatneed to be considered at each decision point, as only three substitutions, four insertions and one deletion are ever possible at this last base position. Two variants are allowed in the first k-mer to improve the chances of getting a 'good' k-mer to start the sequential error correction process. If the beginning of the read remains uncorrected after TryHealingRead has completed its initial pass, the read is reversed and TryHealingRead is called again to try correct into the (previous) start of the read from the 'good' middle of the read. Another benefit of the incremental correction loop in TryHealingRead is that any number of errors can be corrected in a single pass over a read, even sequential errors in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Time and space requirements</head><p>The resource requirements of the error-correction algorithms in our panel was measured by using them all to correct each of the four test datasets, and recording how long each one took to complete each correction run ('elapsed' time), and how much memory they required while running. The comparative performance of each of the algorithms is shown in<ref type="figure" target="#tab_3">Table 3</ref>. The numbers shown in this table for ERA000206 correspond well to the performance results shown for 'D2' in (<ref type="bibr" target="#b23">Yang et al., 2013</ref>). The performance tests were run on a quad-processor Intel E54640 machine with 32 cores of 2.4 GHz and 512 GB of memory running Ubuntu Linux. Blue and its associated tools were run under Mono 2.10.8.1 (using the sgen garbage collector). All tools were asked to run with eight threads if possible. HiTEC, BLESS and Reptile are single-threaded, and Echo is almost entirely single-threaded. The 'preparation' times shown for Reptile include the time needed to convert the original FASTQ files into the required FASTA-like format, and the two runs of the supplied 'seq-analy' program needed to determine the appropriate parameter values. The 'preparation' times shown for Blue include the time taken to produce both the k-mer consensus tables and histograms, and the 'pairs' file. Repeated runs of Blue over the same dataset, varying theâ€“â€“min orâ€“â€“good parameters, can be done without rerunning these preparatory steps. Echo was only used on the Pseudomonas dataset, due to the limited time available to run these tests. HSHREC was not used for all tests because of its poor results (<ref type="figure" target="#fig_3">Fig. 7</ref>). HiTEC and BLESS can only accept fixed length reads so the Pseudomonas tests were done using a trimmed set of 120 bp reads, and these two tools are not reported in the Pseudomonas alignments tests as a result. Blue's correction speed is determined primarily by the average number of errors per read, rather than the depth of coverage or number of reads. Blue's memory is primarily used to hold the k-mer consensus and pairs tables, with smaller amounts of perthread memory used for buffers and working storage. Low-repetition k-mers and pairs (below the depth specified by the '-min' parameter) are not loaded into the tables, and as a result, the memory needed to hold these tables is largely determined by the length of the underlying genome, with slightly over one k-mer and one pair typically needed for each base in the genome. Blue's memory requirements do not increase with the size of the datasets being corrected. Scaling tests have shown that Blue is capable of handling large datasets. The histogram shown in<ref type="figure">Figure 3</ref>came from tiling 1.7 billion Illumina HiSeq 101-mer reads (four full lanes, ERR091571 through ERR091574), and took 6.5 h (with 16 cores) and used 84 GB of memory, with the files on a remote NFS server. Blue was then used to correct these same four datasets, taking 14.7 h and 67 GB of memory. Overall, these results show that Blue is both fast and highly scalable, making it quick and effective to use on smaller datasets and feasible to use for correcting large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Accuracy on Illumina data</head><p>As discussed in Section 2, one test we used for assessing the accuracy of each of the correction tools was to align the corrected reads against the appropriate reference sequence using the Bowtie2 aligner, and then compare how many of the reads could be aligned with how many alterations. The full results of aligning the datasets resulting from correcting our four test datasets with the tools in our panel can be found in the Supplementary Data. Figures 6â€“8 summarize these alignment results for three of the datasets, showing the number of edits Bowtie2 needed to get each corrected read to align to the reference genome, and how many reads the correction tools discarded in getting to their set ofof the starting reads from these three datasets could be aligned to their reference genomes without Bowtie2 needing to make any edits. All the correction tools, except for HSHREC, produced improvements in the overall quality of the reads, as shown by the increases in the percentage of perfectly aligned reads. Blue gave the best results for all four tests, with 98.6â€“99.4% of all reads aligning with no mismatches for the three bacterial samples. BLESS, HiTEC and RACER were the next most accurate on these alignment tests. Even with perfect correction, not all corrected reads will align exactly, reflecting sequencing artifacts in the data (such as concatenated adapters and chimeric reads), and novel regions in the sequenced organisms. The 'good' option in Blue attempts to improve the overall quality of the corrected dataset by discarding those reads that still have 'poor' k-mers after correction. The effects of this option can be seen in the 'g90%' alignment results, with 99.9% of the DH10B reads, 99.7% of the Pseudomonas reads and 99.9% of the MG1655 reads, now aligning exactly. One benefit of using bacterial datasets for this comparison is that it is possible to get sequence data from an organism that is closely related to the one used to compile the corresponding reference genome. This situation does not apply for human data, and this is reflected in the overall lower unpaired alignment numbers shown in<ref type="figure">Figure 8</ref>. Correction still produces an improvement in the number of exactly aligning reads for this dataset, with Blue getting a 20% improvement over the uncorrected data. There are still 15% of reads that require adjustment before they can be aligned to the HG19 reference genome, and some of these can be expected to reflect actual variation between the person whose DNA was sequenced and the reference genome sequence. We also did paired Bowtie 2 alignments for the Pseudomonas and Human Chr 21 datasets, and the results can be found in the Supplementary Data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Accuracy on 454 data and cross-correction</head><p>Blue, along with Coral and HSHREC, can correct all three types of possible errors (substitutions, insertions and deletions). We tested this capability by correcting two 454 datasets (SRR001355 and SRR029323) from E.coli K12 MG1655, and then aligning the corrected reads using both Bowtie (<ref type="bibr" target="#b11">Langmead et al., 2009</ref>) and Bowtie 2. The full results of these tests can be found in the Supplementary Data and<ref type="figure" target="#fig_4">Figure 9</ref>Bowtie 2 results for the poorer quality and smaller SRR029323 dataset. The Bowtie results are included in the Supplementary Data to show how well the various correction algorithms handle insertion and deletion errors, as this aligner is only capable of making substitution adjustments when it maps a read to a reference genome, so reads with uncorrected insertion and deletion errors will often not be aligned at all. All of the Blue 454 correction runs used the '-hp' option, which increases Blue's ability to detect errors at the end of homopolymer runs. As discussed earlier, Blue's recursive exploration of the tree of potential fixes is depth-limited for performance reasons, and this can result in the partial correction of long reads containing many errors. Rerunning Blue over already-corrected reads will result in better results in this case, as shown by the 'x2' results. Both Coral and HSHREC already perform multiple iterations over the reads, so these programs were not run more than once. The ability of Blue to 'cross-correct' reads is demonstrated by correcting this 454 dataset with the k-mer consensus table generated from the ERA000206 Illumina dataset, using a '-min' parameter value derived from the Illumina data. The various '206' results in the figure show how effective this cross-correction can be, with 99.3% of the SRR029323 454 reads aligning exactly after two rounds of correction and then discarding any remaining not 'good' reads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effects on assembly</head><p>The second part of our accuracy/effectiveness testing is to measure the improvements that the various error-correction tools have on assembly, with the expectation that higher quality data going into an assembler would result in longer contigs and fewer errors. As discussed in Section 2, we did this by assembling the sets of corrected reads for all three bacterial datasets (except for HSHREC), and using the Mauve Assembly Metrics (<ref type="bibr" target="#b2">Darling et al., 2011</ref>) to generate metrics on the quality of the resulting contigs. These metrics were supplemented by using NCBI BLASTN to determine the length of the largest contiguous matching region when the assembled contigs were aligned against their reference sequence. All of the sets of reads were assembled using Velvet v1.2.07. We also ran a number of assemblies that combined 454 and Illumina data. For comparison, we also generated perfect synthetic datasets for each of the bacterial genomes by tiling their reference genomes, and ran these errorfree reads through the same assembly and comparison steps. All assemblies were run using Velvet 'hash_length' values of both 41 and 57, and the expected coverage cutoff parameters were set after examining the k-mer histograms produced by Tessel for the uncorrected data. The Velvet paired-end option ('shortPaired') was used whenever the correction tool had not irreparably broken the read pairing that existed in the original datasets. The exact parameter values used for the Velvet runs can be found in the Supplementary Data, along with the full results from all of these tests. The 'max align' values in these charts and tables is the length of the largest contiguous matching region. The test statistics for the two MiSeq datasets are summarized in Figures 10 and 11, and the full set of results can be found in the Supplementary Data. The Blue-corrected reads consistently produce the best overall assemblies, producing the longest contigs with the fewest errors, for all the datasets. The differences in contig quality is shown by the error density plots (in<ref type="figure" target="#fig_0">Fig. 12</ref>and the Supplementary Data) that show locations in the reference genome (1 kb intervals) where the Mauve tool found at least one mismatch. Some of these mismatches will be actual differences between the organism being sequenced and the reference sequence, and other will reflect artifacts introduced by either the assembler or Mauve, as evidenced by mismatches reported in the assemblies of the perfect synthetic data. The benefits of cross-correcting can be seen in the results from the combined assembly of the Blue-corrected ERA000206 (Illumina) and SRR001355 (454) reads. The lower part of the error density plot in<ref type="figure" target="#fig_0">Figure 12</ref>shows clearly the effect of crosscorrection on the quality of the resultant contigs, with only 16 intervals now showing mismatches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>The primary goal in the development of Blue was to create a practical tool that would help biologists get more accurate results from their sequencing datasets. Blue had to be sufficiently fast and memory-efficient to allow it to correct today's large datasets using reasonable resources, and effectively transparent so it could be used within existing analytical workflow tools such as Galaxy, just taking in a sequencing dataset and writing it out again after removing as many errors as possible while maintaining file formats, quality scores and read pairings.Our tests have shown that Blue meets these goals. It is faster than the other algorithms tested, and its low memory requirements make it practical to use with current large sequencing datasets. Blue has been shown to be more accurate than any of the other algorithms we tested on both Illumina and 454 data. The assembly tests showed that Blue-corrected reads consistently produced the longest and most error-free contigs of all the tools tested. Blue's ability to correct insertion and deletion errors allows it to be used with great effect on datasets generated on the 454 and Ion Torrent platforms. Decoupling the reads being corrected from the set of reads used to generate the k-mer consensus table allows for cross-correcting long homopolymer-prone reads with short but cheaper Illumina reads, resulting in even better correction of these datasets. Blue has already been used to improve the assemblies for published microbial genomes derived from pure cultures (<ref type="bibr" target="#b0">Bradbury et al., 2012;</ref><ref type="bibr" target="#b17">Rosewarne et al., 2013a</ref>). It has also been used on metagenomic datasets to improve draft genome assemblies of the dominant organisms in these communities (<ref type="bibr" target="#b18">Rosewarne et al., 2013b;</ref><ref type="bibr" target="#b24">Wang et al., 2011</ref>). Correcting metagenomic sequence datasets works only when the dominant organisms are taxonomically distant, and so share few k-mers (<ref type="bibr" target="#b4">Greenfield and Roehm, 2013</ref>). In this case, correcting the reads has the useful side effect of removing rare variants of the dominant organisms, giving both better assemblies and improving the performance of the assemblers themselves. Blue has also been successfully used on diploid data, both human and insect. Blue is currently being used on a major insect genome project, and its ability to cross-correct long-mate-pair 454 reads with Illumina data have proven to be useful to this team. Blue will continue to be tested and refined on new types of sequencing data as these emerge, with an immediate focus on PacBio. Another area of anticipated work is improving the correction of diploid data at those places where differences between the two alleles cause difficulties for assemblers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. 25-mer repetition histogram for Clostridium sporogenes PA3679</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.4.</head><figDesc>Fig. 4. 25-mer depths for an Illumina read</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.5.</head><figDesc>Fig. 5. The Blue error-correction algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.7.</head><figDesc>Fig. 7. Pseudomonas aeruginosa alignments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.9.</head><figDesc>Fig. 9. MG1655 454 Bowtie 2 alignment (SRR029323)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.10.</head><figDesc>Fig. 10. Pseudomonas assembly metrics (Vk = 41)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.11.</head><figDesc>Fig. 11. Escherichia coli DH10B assembly metrics (Vk = 41)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>).</figDesc><table>0.0% 

0.5% 

1.0% 

1.5% 

2.0% 

2.5% 

3.0% 

3.5% 

4.0% 

0 
50 
100 
150 
200 
% of all 25-mers 

25-mer repeon depth 

Fig. 3. 25-mer repetition histogram for Homo sapiens (4 HiSeq lanes) 

0.0% 

0.5% 

1.0% 

1.5% 

2.0% 

2.5% 

3.0% 

3.5% 

4.0% 

4.5% 

0 
50 
100 
150 
200 
% of all 25-mers 

25-mer repeon depth 

Fig. 2. 25-mer repetition histogram for Helicoverpa armigera 

0.0% 

0.5% 

1.0% 

1.5% 

2.0% 

2.5% 

3.0% 

3.5% 

4.0% 

0 
100 
200 
300 
400 
% of all 25-mers 

25-mer repeon depth 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 1. Error correction tools used in tests</figDesc><table>Tool 
FASTA in 
FASTQ In 
Fmt in = out 
Ins, Dels? 
Ns fixed? 
Var len? 
Pairs kept? 
Multi-Thrd 

Blue 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
BLESS 
No 
Yes 
Yes 
No 
Yes 
No 
Yes 
No 
Coral 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
Echo 
No 
Yes 
Yes 
No 
Yes 
Yes 
Yes 
No 
HiTEC 
Yes 
Yes 
No 
No 
No 
No 
No 
No 
HSHREC 
1-line 
No 
No 
Yes 
No 
Yes 
No 
Yes 
Quake 
No 
Yes 
Yes 
No 
Yes 
Yes 
Yes 
No 
RACER 
1-line 
Yes 
Yes 
No 
Yes 
Yes 
Yes 
Yes 
Reptile 
Yes 
No 
No 
No 
Yes 
Yes 
Yes 
No 
SHREC 
Yes 
Yes 
Yes 
No 
No 
Yes 
No 
Yes </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Datasets used in comparisons</figDesc><table>Organism 
Source 
Technology 
Reads 
Notes 

Pseudomonas aeruginosa PA01 
SRA ERR330008 
Illumina MiSeq 
10 020 458 paired-end 150-mer 
QC trimmed 

Variable length reads after 
trimming 
E.coli K12 DH10B 
Illumina Data Library 
Illumina MiSeq 
13 175 679 paired-end 150-mer 
Homo sapiens chr. 21 
Illumina Data Library 
Illumina HiSeq 2000 
13 528 800 paired-end 100-mer 
NA19240 
E.coli K12 MG1655. 
ENA ERA000206 
Illumina GAII 
28 428 648 paired-end 100-mer 
Yang et al. D2 
E.coli as above 
SRR001355 
454 FLX 
350 304 
E.coli as above 
SRR029323 
454 FLX 
143 836 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 3. Performance of correction algorithms under test</figDesc><table>Dataset 
Tool 
Thrds 
Prep. time 
(min) 

Correct. time 
(min) 

Prep. mem 
(GB) 

Correct. mem 
(GB) 

Reads/min 
(elapsed) 

ERA000206 28 428 648 reads 
Blue 
8 
9.2 
14.4 
1.6 
0.4 
1 205 455 
BLESS 
1 
140.7 
0.1 
202 052 
Coral 
8 
1595.6 
39.0 
17 817 
HiTEC 
1 
699.0 
13.4 
40 670 
HSHREC 
8 
737.1 
30.5 
38 568 
RACER 
8 
30.6 
2.4 
929 041 
Reptile 
1 
125.0 
417.0 
3.0 
3.8 
68 174 
SHREC 
8 
225.1 
30.0 
126 293 

Pseudomonas 9 859 280 
reads (var len, 150 bp) 

Blue 
8 
5.4 
6.3 
1.3 
0.4 
843 876 
BLESS a 
1 
48.3 
0.1 
204 126 
Coral 
8 
437.0 
11.0 
22 561 
Echo 
1 
4934.8 
19.0 
1998 
HiTEC a 
1 
319.4 
9.9 
30 868 
HSHREC 
8 
329.0 
30.0 
29 967 
RACER 
8 
16.9 
1.6 
583 389 
Reptile 
1 
45.1 
132.0 
4.2 
2.6 
55 686 
SHREC 
8 
113.7 
17.0 
86 691 

Chr21 13 486 136 reads 
Blue 
8 
6.6 
6.5 
2.1 
2.6 
1 031 576 
Coral 
8 
132.5 
11.0 
101 782 
HiTEC 
1 
414.0 
12.0 
32 575 
RACER 
8 
15.8 
2.6 
853 553 
Reptile 
1 
54.1 
194.7 
3.6 
4.2 
54 198 
SHREC 
8 
113.4 
21.0 
118 978 

DH10B 13 051 484 reads 
Blue 
8 
6.7 
9.3 
1.3 
0.3 
815 548 
BLESS 
1 
94.3 
0.1 
138 404 
Coral 
8 
1223.0 
12.0 
10 672 
HiTEC 
1 
510.0 
9.7 
25 591 
RACER 
8 
21.9 
1.2 
595 958 
Reptile 
1 
59.0 
150.4 
3.6 
3.0 
62 328 
SHREC 
8 
146.8 
21.0 
88 907 

a 

(8812148 reads, 120bp fixed len). 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">P.Greenfield et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Blue at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">P.Greenfield et al.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Draft genome sequence of Clostridium sporogenes PA 3679, the common nontoxigenic surrogate for proteolytic Clostridium botulinum</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Bradbury</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bacteriol</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="1631" to="1632" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title level="m" type="main">Assemblathon 2: evaluating de novo methods of genome assembly in three vertebrate species</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">R</forename>
				<surname>Bradnam</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Mauve assembly metrics</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Darling</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2756" to="2757" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Field guide to next-generation sequencers</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">C</forename>
				<surname>Glenn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Ecol. Resour</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="759" to="769" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Answering biological questions by querying k-mer databases</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Greenfield</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Roehm</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concur. Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="497" to="509" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">BLESS: bloom filter-based error correction solution for highthroughput sequencing reads</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Heo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1354" to="1362" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">HiTEC: accurate error correction in high-throughput sequencing data</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Ilie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="295" to="302" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">RACER: rapid and accurate correction of errors in reads</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Ilie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Molnar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2490" to="2493" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">ECHO: a reference-free short-read error correction algorithm</title>
		<author>
			<persName>
				<forename type="first">W.-C</forename>
				<surname>Kao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1181" to="1192" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Quake: quality-aware detection and correction of sequencing errors</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Kelley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast gapped-read alignment with Bowtie 2</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Langmead</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">L</forename>
				<surname>Salzberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="357" to="359" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Ultrafast and memory-efficient alignment of short DNA sequences to the human genome</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Langmead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and accurate long-read alignment with BurrowsWheeler transform</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Durbin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="589" to="595" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">De novo assembly of human genomes with massively parallel short read sequencing</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="265" to="272" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Performance comparison of benchtop high-throughput sequencing platforms</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Loman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="434" to="439" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of genomic high-throughput sequencing data generated on Illumina HiSeq and Genome Analyzer systems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Minoche</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">112</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence-specific error profile of Illumina sequencers</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Nakamura</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Draft genome sequence of Clostridium sp. Maddingley, isolated from coal seam gas formation water</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">P</forename>
				<surname>Rosewarne</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Announc</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="000812" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Draft genome sequence of Methanobacterium sp. Maddingley, reconstructed from metagenomic sequencing of a methanogenic microbial consortium enriched from coal-seam gas formation water</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">P</forename>
				<surname>Rosewarne</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Announc</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="82" to="000812" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Correction of sequencing errors in a mixed set of reads</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Salmela</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1284" to="1290" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Correcting errors in short reads by multiple alignments</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Salmela</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schroder</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1455" to="1461" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">SHREC: a short-read error correction method</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schroder</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2157" to="2163" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Reptile: representative tiling or short read error correction</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2526" to="2533" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey of error-correction methods for next-generation sequencing</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="56" to="66" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Complete genome sequence of a nonculturable methanococcus maripaludis strain extracted in a metagenomic survey of petroleum reservoir fluids</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bacteriol</title>
		<imprint>
			<biblScope unit="page">5595</biblScope>
			<date type="published" when="0193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Velvet: algorithms for de novo short read assembly using de Bruijn graphs</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Zerbino</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Birney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="821" to="829" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>