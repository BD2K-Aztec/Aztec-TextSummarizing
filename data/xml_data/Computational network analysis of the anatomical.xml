
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gene expression Computational network analysis of the anatomical and genetic organizations in the mouse brain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Shuiwang</forename>
								<surname>Ji</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Old Dominion University</orgName>
								<address>
									<postCode>23529</postCode>
									<settlement>Norfolk</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gene expression Computational network analysis of the anatomical and genetic organizations in the mouse brain</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">23</biblScope>
							<biblScope unit="page" from="3293" to="3299"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr558</idno>
					<note type="submission">Received on May 2, 2011; revised on October 1, 2011; accepted on October 3, 2011</note>
					<note>[12:23 7/11/2011 Bioinformatics-btr558.tex] Page: 3293 3293–3299 Associate Editor: John Quackenbush Contact: sji@cs.odu.edu Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: The mammalian central nervous system (CNS) generates high-level behavior and cognitive functions. Elucidating the anatomical and genetic organizations in the CNS is a key step toward understanding the functional brain circuitry. The CNS contains an enormous number of cell types, each with unique gene expression patterns. Therefore, it is of central importance to capture the spatial expression patterns in the brain. Currently, genome-wide atlas of spatial expression patterns in the mouse brain has been made available, and the data are in the form of aligned 3D data arrays. The sheer volume and complexity of these data pose significant challenges for efficient computational analysis. Results: We employ data reduction and network modeling techniques to explore the anatomical and genetic organizations in the mouse brain. First, to reduce the volume of data, we propose to apply tensor factorization techniques to reduce the data volumes. This tensor formulation treats the stack of 3D volumes as a 4D data array, thereby preserving the mouse brain geometry. We then model the anatomical and genetic organizations as graphical models. To improve the robustness and efficiency of network modeling, we employ stable model selection and efficient sparsity-regularized formulation. Results on network modeling show that our efforts recover known interactions and predicts novel putative correlations. Availability: The complete results are available at the project website</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The mammalian central nervous system (CNS) generates high-level control functions, and knowledge on the anatomical and genetic organizations in this system can elucidate the functional brain circuitry. The enormous complexity of this system is reflected in the large number of cell types, each with unique gene expression patterns. Therefore, it is of central importance to capture the anatomical localization of gene expressions in the brain. Recent advances in bioimaging technologies, such as the high-throughput in situ hybridization (ISH) technique, have made it possible to capture the spatial expression patterns in the adult mouse brain (<ref type="bibr" target="#b18">Lein et al., 2007</ref>). Consequently, genomic-scale expression atlases in the form of digital images have been produced at increasing speed and resolution. The marriage of image processing tools and advanced computational methods opens the door for unraveling the functional brain circuitry and the generation of high-level cognitive functions on top of it. The Allen Brain Atlas (ABA) (<ref type="bibr" target="#b18">Lein et al., 2007</ref>) contains 3D atlas of gene expression in the adult mouse brain and is one of the most comprehensive datasets for spatial expression patterns in the mammalian CNS. It provides cellular resolution 3D expression patterns in the male, 56-day-old C57BL mouse brain. In this atlas, genome-wide coverage is available in sagitally oriented sections. In addition, coronal sections at a more refined scale are available for a set of about 4000 genes showing restricted expression patterns. The image data are generated by in situ hybridization using genespecific probes, followed by slide scanning, 3D image registration to the Allen Reference Atlas (ARA) (<ref type="bibr" target="#b9">Dong, 2009</ref>) and expression segmentation (<ref type="bibr" target="#b18">Lein et al., 2007;</ref><ref type="bibr" target="#b23">Ng et al., 2007</ref>). This results in a set of spatially aligned 3D volumes of size 67×41×58, one for each gene, that document the spatial expression patterns of genes in the mouse brain. Efficient and effective analysis of these highthroughput data can shed light on the global function of mammalian CNS (<ref type="bibr" target="#b14">Jones et al., 2009</ref>). On the other hand, the sheer volume and complexity of these data pose significant challenges for efficient computational analysis. Hence, computational understanding of these data is limited to unsupervised techniques, which cluster the brain regions into co-expressed groups (<ref type="bibr" target="#b5">Bohland et al., 2010</ref>). In this article, we employ advanced computational techniques to model the anatomical and genetic organizations in the mouse brain as networks. First, to reduce the size of data and accelerate efficient analysis and storage, we propose to apply tensor factorization techniques to reduce the data volumes (<ref type="bibr" target="#b15">Kolda and Bader, 2009;</ref><ref type="bibr" target="#b31">Wrede, 1972</ref>). This tensor formulation treats the stack of 3D volumes as a 4D data array, thereby preserving the mouse brain geometry. Based on the reduced data, we model the anatomical and genetic organizations as graphical models in which each vertex represents a spatial location or a gene, and the edges between vertices encode the correlations between locations and genes (<ref type="bibr" target="#b8">Dempster, 1972;</ref><ref type="bibr" target="#b10">Edwards, 2000</ref>). To improve the efficiency of network modeling, we employ an approximate formulation for Gaussian graphical modeling, which involves a series of sparsity regularized regression problems (<ref type="bibr" target="#b20">Meinshausen and Bühlmann, 2006</ref>). The efficiency of this approximate formulation enables us to employ a robust estimation technique known as stability selection (<ref type="bibr" target="#b21">Meinshausen and Bühlmann, 2010</ref>), which estimate and combine multiple models based on resampling. We apply the data reduction and network modeling techniques to learn the anatomical and genetic networks underlying the mouse<ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.Ji</head><p>brain using the ABA expression volume data. Results show that the expression patterns of spatially adjacent voxels tend to correlate. We also observe that the expression patterns of certain brain structures are correlated to the patterns of a large number of other regions, some of which are spatially distant. In-depth analysis reveals that such correlation patterns recover existing knowledge on the brain functionality. Our efforts on genetic network modeling identify functionally related genes that act in a concerted manner in the mouse brain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HIGH-ORDER FEATURE EXTRACTION VIA TENSOR FACTORIZATION</head><p>In ABA, the ISH image series of each gene are aligned to the ARA. To faithfully capture the mouse brain geometry, a 3D grid is employed to divide the 3D ARA space into quadrats, and expression information within each quadrat is summarized. Specifically, an expression segmentation algorithm is employed to identify expressed cells, and then an expression energy value is computed from each voxel as a function of the intensity and density of expression within that voxel. These image processing steps convert each expression pattern into a 3D volume. To enable the application of matrix computation techniques such as the singular value decomposition (SVD), these volumes are usually converted to vectors and stacked into a data matrix (<ref type="bibr" target="#b5">Bohland et al., 2010</ref>). However, such conversion fails to retain the spatial locality and other high-order information in the expression volumes. To overcome this limitation, we propose to treat the 3D volumes as 3D tensors and stack them together to form a 4D tensor. We then employ tensor factorization techniques to reduce the dimensionality of this 4D tensor along each mode, resulting in significant data compression. A key advantage of this tensor representation is that the associated tensor computation techniques, such as high-order SVD and low-rank tensor approximation, can be employed to compress the data without flattening the internal structure of the high-order data array. These techniques approximate the original tensor by a core tensor multiplied by a basis matrix along each mode. Hence, the core tensor and the set of basis matrices give a compact representation of the original tensor, and the core tensor captures the major information in the original tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background on tensors</head><p>Tensors, also known as multidimensional matrices (<ref type="bibr" target="#b15">Kolda and Bader, 2009;</ref><ref type="bibr" target="#b31">Wrede, 1972</ref>), are higher order generalizations of vectors (first-order tensors) and matrices (second-order tensors). The order of a tensor is the number of indices, also known as modes or ways. In this article, tensors are denoted by boldface Euler script letters, e.g. X ∈ R J 1 ×J 2 ×...×J N , and its elements are denoted as x j 1 ,j 2 ,...,j N , where 1 ≤ j n ≤ J n for n = 1,...,N. As a generalization of matrix multiplication, the n-mode tensor-matrix product defines the multiplication of a tensor by a matrix in mode n (<ref type="bibr" target="#b16">Lathauwer et al., 2000a</ref>). The n-mode product of a tensor X ∈ R J 1 ×J 2 ×...×J N with a matrix A ∈ R I×Jn = (a ijn ) is denoted by X × n A. The result is a tensor of size J 1 ×...×J n−1 ×I ×J n+1 ×...×J N defined elementwise as</p><formula>(X × n A) j 1 ...j n−1 ij n+1 ...j N = Jn jn=1 x j 1 ...j n−1 jnj n+1 ...j N a ijn .</formula><p>Let Y ∈ R J 1 ×J 2 ×...×J N be another tensor of the same size as X. The scalar product of these two tensors is defined as:</p><formula>&lt; X ,Y &gt;= J 1 j 1 =1 J 2 j 2 =1 ... J N j N =1 x j 1 ,j 2 ,...,j N y j 1 ,j 2 ,...,j N .</formula><formula>(1)</formula><p>Based on the scalar product, the Frobenius norm of a tensor X can be defined as</p><formula>X = &lt; X ,X &gt;.</formula><formula>(2)</formula><p>=The mode-n vectors of X are the J n-dimensional vectors obtained from X by varying index j n while keeping all other indices fixed. Tensors can be converted into matrices via a process known as unfolding (<ref type="bibr" target="#b15">Kolda and Bader, 2009</ref>). Specifically, the mode-n unfolding of X yields a matrix X (n) ∈ R Jn×(J 1 J 2 ...J n−1 J n+1 ...J N ) whose columns consist of the mode-n vectors of X .</p><p>The mode-n rank of X , denoted as rank n (X ), is defined as the rank of the matrix obtained from mode-n unfolding of X : rank n (X ) = rank(X (n) ). Tensors have been used in a wide range of domains including microarray data analysis (<ref type="bibr" target="#b24">Omberg et al., 2007</ref>) and natural image modeling (<ref type="bibr" target="#b27">Vasilescu and Terzopoulos, 2004;</ref><ref type="bibr" target="#b28">Wang et al., 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tensor factorization</head><p>High-order singular value decomposition (HOSVD) (<ref type="bibr" target="#b16">Lathauwer et al., 2000a</ref>) is a generalization of the SVD for matrices. Given a tensor X ∈ R J 1 ×J 2 ×...×J N , its HOSVD can be expressed as</p><formula>X = S × 1 U (1) × 2 U (2) ×···× N U (N) ,</formula><formula>(3)</formula><p>where S ∈ R J 1 ×J 2 ×···×J N , and U (n) ∈ R Jn×Jn , for n = 1,...,N, are orthogonal matrices. In HOSVD, the basis matrices {U (n) } N n=1 are computed as the left singular matrices of the mode-n unfolding of X , and the core tensor can then be computed as</p><formula>S = X × 1 (U (1) ) T ×···× N (U (N) ) T .</formula><formula>(4)</formula><p>Given a tensor X ∈ R J 1 ×J 2 ×···×J N , a rank-(R 1 ,...,R N ) factorization of X (<ref type="bibr" target="#b17">Lathauwer et al., 2000b</ref>) is formulated as finding a tensorˆXtensorˆ tensorˆX with</p><formula>rank n ( ˆ X ) = R n ≤ rank n (X )</formula><p>for 1 ≤ n ≤ N such that the following cost function is minimized:</p><formula>ˆ X = arg minˆX minˆ minˆX X − ˆ X .</formula><formula>(5)</formula><p>It follows from this definition thatˆXthatˆ thatˆX can be expressed asˆX</p><formula>asˆ asˆX = C × 1 V (1) × 2 V (2) ×···× N V (N) ,</formula><formula>(6)</formula><p>where C ∈ R R 1 ×R 2 ×···×R N is called the core tensor and V (n) ∈ R Jn×Rn (1 ≤ n ≤ N) has orthonormal columns. When the basis matrices {V (n) } N n=1</p><p>are given, the core tensor C can be readily computed as Lathauwer et al.</p><formula>(2000a) C = X × 1 (V (1) ) T × 2 (V (2) ) T ×···× N (V (N) ) T .</formula><formula>(7)</formula><p>Hence, the key to the low-rank tensor factorization problem is to compute the basis matrices. The factorization of a 3D tensor is illustrated in<ref type="figure" target="#fig_0">Figure 1</ref>. One of the commonly used algorithms to compute the basis matrices is the alternating least squares (ALS) method (<ref type="bibr" target="#b17">Lathauwer et al., 2000b</ref>). In each iteration of this method, one of the basis matrices is optimized while all others are fixed. Specifically, when</p><formula>V (1) ,...,V (n−1) ,V (n+1) ,...,V (N)</formula><p>are fixed, we first compute</p><formula>X n = X × 1 (V (1) ) T ×···× n−1 (V (n−1) ) T × n+1 (V (n+1) ) T × ···× N (V (N) ) T .</formula><p>Then the columns of V (n) can be obtained as the first R n columns of the left singular matrix of (X n ) (n) , which is the mode-n unfolding of X n. In ALS, the basis matrices are usually initialized as the truncated basis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational network analysis</head><p>matrices from HOSVD (<ref type="bibr" target="#b17">Lathauwer et al., 2000b</ref>). That is, V (n) is initialized as the first R n columns of U (n) , for n = 1,...,N. When the size of the tensor is very large and cannot fit into memory, an out-of-core algorithm can be applied by partitioning the tensor into blocks (<ref type="bibr" target="#b28">Wang et al., 2005</ref>). The advantages of tensor-based methods in comparison to matrix-based approaches have been addressed in the literature (<ref type="bibr" target="#b24">Omberg et al., 2007;</ref><ref type="bibr" target="#b27">Vasilescu and Terzopoulos, 2004;</ref><ref type="bibr" target="#b28">Wang et al., 2005</ref>). In summary, tensorbased methods have the following two major advantages: (i) tensor-based methods can be applied to large datasets for which matrix-based methods are too expensive to apply. For example, the size of the data array for genetic network modeling in this article is 3012×67×41×58. While the tensorbased method requires the SVD of three matrices of sizes 67×67, 41×41, and 58×58, respectively, the matrix-based method requires the SVD of a matrix of size 3012×159,326. (ii) Although matrix-based methods give the lowest reconstruction error due to the best low-rank approximation property of matrix SVD, tensor-based methods preserve the geometry of the highorder data array. In the literature, tensor-based and matrix-based methods have been compared in classification tasks (<ref type="bibr" target="#b32">Ye, 2005</ref>). Specifically, it has been shown that, though tensor-based methods give larger reconstruction error, they usually yield higher classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NETWORK CONSTRUCTION VIA SPARSE MODELING</head><p>The 4D tensor of gene expression obtained from the ABA is factorized as described above. The core tensor retains most of the information in the original tensor while its size is significantly reduced. This data reduction step is critical for the subsequent efficient analysis. Based on the reduced data, we employ sparse graphical modeling approaches to construct the anatomical and genetic networks underlying the mouse brain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A sparsity regularization formulation</head><p>Gaussian graphical models are a class of methods for modeling the relationships among a set of variables (<ref type="bibr" target="#b10">Edwards, 2000;</ref><ref type="bibr" target="#b29">Whittaker, 1990</ref>). In this formulation, the d-dimensional variable x =[x 1 ,x 2 ,.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>..,x d ] T</head><p>is assume to follow a multivariate Gaussian distribution x ∼ N(μ,), where μ ∈ R d and ∈ R d×d are the mean and covariance, respectively. The conditional dependency between pairs of variables can be encoded into a graphical model in which vertices represent variables and edges characterize the conditional dependency between variables. In particular, there is an edge between nodes corresponding to x i and x j if and only if these two variables are conditionally dependent given all other variables. This is equivalent to the saying that there exists an edge between nodes corresponding to x i and x j if and only if the (i,j)-th entry of the inverse covariance matrix (also known as concentration matrix) = −1 is non-zero (<ref type="bibr" target="#b8">Dempster, 1972;</ref><ref type="bibr" target="#b10">Edwards, 2000</ref>). This correspondence is illustrated in<ref type="figure" target="#fig_3">Figure 2</ref>. Given a set of n observations y 1 ,y 2 ,...,y n , the concentration matrix can be estimated by maximizing the penalized log likelihood as follows (<ref type="bibr" target="#b3">Banerjee et al., 2008;</ref><ref type="bibr" target="#b11">Friedman et al., 2008;</ref><ref type="bibr" target="#b33">Yuan and Lin, 2007</ref>):</p><formula>ˆ = arg max 0 logdet−trace(S)−λ 1 ,</formula><formula>(8)</formula><p>where det is the determinant of , 0 represents that is positive definite, S denotes the empirical covariance matrix computed from data, and 1 is the 1-norm of , which is the sum of the absolute values of the entries of. The first two terms in Equation (8) are the log likelihood, and the last term is used to enforce that many entries of are set to zero, yielding a sparsely connected graph. This formulation has been used to model the gene networks in Arabidopsis thaliana (<ref type="bibr" target="#b30">Wille et al., 2004</ref>). The optimization problem in Equation (8) is convex and can be solved by several algorithms such as the interior point method (<ref type="bibr" target="#b3">Banerjee et al., 2008</ref>) and the graphical lasso algorithm (<ref type="bibr" target="#b11">Friedman et al., 2008</ref>). However, all these algorithms are computationally expensive and can only be applied to small-scale problems. For the modeling of mouse brain networks, we have thousands of genes and tens of thousands of voxels; hence, this formulation is not applicable.In Meinshausen and Bühlmann (2006), an approximate formulation is proposed to learn Gaussian graphical models by solving a series of sparse regression problems. Specifically, the conditional dependencies between x i and all other variables are learned by solving the following 1-norm penalized regression problem known as lasso (<ref type="bibr" target="#b26">Tibshirani, 1996</ref>):</p><formula>ˆ w = arg min w∈R d−1 y i −Y −i w 2 +λw 1 ,</formula><formula>(9) where Y −i =[y 1 ,...,y i−1 ,y i+1 ,...,y n ]∈R d×(n−1)</formula><p>is the data matrix obtained by removing the i-th data item. The conditional dependencies between x i and all other variables are obtained from the corresponding components in the weight vector w. Note that the regression of x i onto x j and that of x j onto x i may not give the same result. Hence, two simple schemes, based on logic operations or and and, are proposed to interpret the results. In the first scheme, two variables are considered to be conditionally dependent if either of them yields non-zero weight (<ref type="bibr" target="#b20">Meinshausen and Bühlmann, 2006</ref>). In the second scheme, they are considered as conditionally dependent if both of them give non-zero weights. The first scheme is employed in this work (<ref type="bibr" target="#b20">Meinshausen and Bühlmann, 2006</ref>). The pairwise relationships between all pairs of variables can be obtained by running the sparse regression problem in Equation (9) for each variable. A critical observation that leads to the efficiency of the formulation in Equation (9) is that it involves solving d independent lasso problems, one for each variable. The lasso problem can be solved very efficiently by many algorithms such as the accelerated gradient method (<ref type="bibr" target="#b19">Liu et al., 2009</ref>). It has been shown that this sparse regression formulation of Gaussian graphical modeling maximizes the pseudo likelihood (<ref type="bibr" target="#b12">Friedman et al., 2010</ref>) and is an approximation to the maximum likelihood scheme in Equation (8) (<ref type="bibr" target="#b3">Banerjee et al., 2008;</ref><ref type="bibr" target="#b11">Friedman et al., 2008</ref>). In particular, the exact maximization of log likelihood involves solving the lasso problems iteratively as in the graphical lasso algorithm (<ref type="bibr" target="#b11">Friedman et al., 2008</ref>), and the formulation in Equation (9) can be considered as a one-step approximation to the maximum likelihood scheme. We employ this approximate formulation to learn the mouse brain networks due to its efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Robust estimation via stability selection</head><p>The regularization parameter λ in Equation (9) controls the trade-off between the sparsity of solution and data fit. Specifically, when λ is set to a very large value, most of the entries of w are set to zero. Hence, a challenge in practice is how to select the value for λ. Stability selection (Meinshausen and<ref type="bibr">Bühlmann, 2010</ref>) addresses this problem by ideas similar to the ensemble learning methods widely used in machine learning (<ref type="bibr">Bühlmann, 2004</ref>). In stability selection, we choose a set of λ values denoted by , instead of a single λ value. For each λ ∈ , we compute the selection probability for each variable, which is defined as the probability of each variable been selected when randomly resampling from the data. Formally, let I be a random subsample of y 1 ,y 2 ,...,y n of size n/2 drawn without replacement. The selection probability for variable x i is defined asˆλ asˆ asˆλ</p><formula>x i = P{x i ⊆ A λ (I)},</formula><formula>(10)</formula><p>Page: 3296 3293–3299</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.Ji</head><p>where A λ (I) denotes the set of variables that have been selected when I is used as the sample and the regularization parameter is set to λ. Note that this definition of A λ (I) is independent of the specific method used for variable selection. The probability in Equation (10) is with respect to both the random sampling and other sources of randomness such as that induced by the algorithm as we discuss below. For every variable x i , the stability path is given by the selection probabilitiesˆλprobabilitiesˆ probabilitiesˆλ x i ,λ ∈. It has been shown in Meinshausen and Bühlmann (2010) that 100 random resampling is sufficient to obtain accurate estimates. Based on the selection probabilities, stable variables can be defined. For a cutoff π thr with 0 &lt;π thr &lt; 1 and a set of parameters , the set of stable variables are defined asˆS</p><formula>asˆ asˆS stable ={x i : max λ∈ ( ˆ λ x i ) ≥ π thr }.</formula><formula>(11)</formula><p>By choosing the set of stable variables under the control of the cutoff π thr , we keep variables with a high selection probability and discard those with low selection probabilities. It has been show that the results of stability selection vary little for sensible choices of the cutoff π thr and the parameter set .</p><p>It has also been shown that performance can be further improved if additional randomness is introduced into the lasso problem in Equation (9). In particular, we can randomize the amount of regularization for each variable by solving the following problem:</p><formula>ˆ w = arg min w∈R d−1 y i −Y −i w 2 +λ k∈D −i d |w k | c k ,</formula><formula>(12)</formula><p>where D −i d ={1,...,i−1,i+1,...,d}, c i are IID random variables in<ref type="bibr">[α,1]</ref>and α ∈ (0,1] is a user-specified weakness factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>In this article, we use a set of expression volumes for 3012 genes documented in the coronal sections as in<ref type="bibr" target="#b5">Bohland et al. (2010)</ref>. This set of genes exhibit restricted expression patterns and thus are of high neurobiological interest. For anatomical network modeling, we only use the left hemisphere voxels, since only this part of the brain is annotated in ARA. This gives rise to a 4D tensor of size 3012× 67×41×33 in which the first index corresponds to genes, and the other three indices represent the rostral–caudal, dorsal–ventral and left–right spatial directions, respectively. In tensor factorization, we keep the dimensionality of the last three modes while reduce the dimensionality of the first mode, since we are interested in modeling the relationships among brain voxels. For genetic network modeling, we use the full volumes, and the size of our 4D tensor is 3012×67× 41×58. In this case, we keep the dimensionality of the first mode while reducing the dimensionality of the other three modes. The computational experiments were performed on a cluster consisting of 256 cores and 512 GB RAM. The lasso formulation was solved using the SLEP package (<ref type="bibr" target="#b19">Liu et al., 2009</ref>). We can determine the λ value that enforces w to be a zero vector in Equation (9) (<ref type="bibr" target="#b19">Liu et al., 2009</ref>), and this λ value is denoted as λ max. Then we set ={0.1λ max ,0.2λ max ,...,0.9λ max }. The selection probabilities were estimated on 100 random resampling, and the weakness factor α was set to 0.8. The sizes of reduced data were set to retain 90 and 80% of the original information for anatomical and genetic network modeling, respectively, based on the computational resource requirements. Specifically, the size of the reduced tensor is 179×67×41×33 in anatomical network modeling and is 3012× 22×13×19 in genetic network modeling.Each vertex is labeled with the ARA informatics ID of the brain structure, and the corresponding structure name is given in Supplementary Table S2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on anatomical network modeling</head><p>Computational modeling of the anatomical organization in the mouse brain yields a graph in 3D space in which the vertices represent brain regions, and the edges characterize the expression correlations between regions. The correlation patterns can be visualized by showing slices of the 3D brain network on 2D planes.<ref type="figure" target="#fig_4">Figure 3</ref>shows one slice of the brain network along the coronal section. We can observe that most of the edges connect adjacent regions, showing that spatially adjacent regions tend to exhibit correlated expression patterns. Note that these correlation patterns are learned without knowing the spatial locations of voxels. Although most of the edges connect spatially adjacent regions, there are apparent exceptions. A slice-by-slice examination of the entire anatomical networks at multiple cutoffs reveal that the voxels annotated as dentate gyrus (DG) in the ARA are highly correlated to many voxels in distant regions as shown in<ref type="figure" target="#fig_6">Figure 4</ref>. According to classical neuroanatomy, the DG plays an important role in learning and memory by processing and representing spatial information, and it has always been a topic of intense interest (<ref type="bibr" target="#b25">Scharfman, 2007</ref>). It has been shown that the DG receives multiple sensory inputs including vestibular, olfactory, visual, auditory and somatosensory from its upstream perirhinal cortex and entorhinal cortex. It plays the role of a gate or filter, blocking or filtering excitatory activity from the inputs and controlling the amount of excitation that is propagated to the downstream hippocampus (<ref type="bibr" target="#b25">Scharfman, 2007</ref>). A close examination of<ref type="figure" target="#fig_6">Figure 4</ref>shows that the correlation patterns are largely consistent with those classical results. A more quantitative analysis of the results show that the correlation patterns obtained solely based on gene expressions match well with the known functions of DG. In particular, the expression patterns of the DG is highly correlated to those of the cerebral cortex and the main olfactory bulb, which provide sensory inputs to DG. In addition, DG is highly correlated to the hippocampal region and the retrohippocampal region, propagating the filtered signals to its downstream regions. We also observe that the intra-DG correlations dominate, demonstrating again that most of the edges connect spatially adjacent regions. Besides the correlations with knownThe region with the largest number of connections corresponds to the brain structure dentate gyrus. functions, our modeling of the anatomical networks also identifies many new relationships with DG that are not known from classical anatomical studies. Based on the obtained networks in 3D space, a variety of network analysis and visualization techniques can be employed to analyze the anatomical organization in the mouse CNS. In<ref type="bibr" target="#b5">Bohland et al. (2010)</ref>, the K-means algorithm is used to cluster the brain voxels into groups based on dimensionality reduced expression data, and a metric known as the S index was employed to quantitatively characterize the correspondence of the clustering results with the classical anatomy reflected in the ARA annotations. Specifically, let R ={r 1 ,...,r N } be a partition of the set of brain voxels in which each r i comprises the set of indices of the voxels that map to that cluster (or anatomical label). The spatial overlap between a region from the ARA and the clustering result is defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational network analysis</head><formula>P ij =|r i ∩r j |/|r j |. From</formula><p>the P ij values that are computed over all pairs of ARA regions and cluster result, we can then derive a global scalar index of similarity between the two partitions. Since</p><formula>P ij = P ji , X ij is defined as X ij = max{P ij ,P ji } along with W ij = U ij / U ij ,</formula><p>where U ij = min{|r i |,|r j |} if X ij &gt; 0 and 0 otherwise. Finally, the S index is defined as S = 1−4 ij W ij X ij (1−X ij ). To compare our network modeling method with the K-means clustering, we apply the leading eigenvector community detection algorithm proposed by<ref type="bibr" target="#b22">Newman (2006)</ref>and treat each detected community as a cluster. Since different cutoff values π thr in the stability selection yield different graphs, we vary π thr from 0.5 to 0.85 and detect communities from each of the resulting graphs. We then run K-means with K equal to the number of communities so that the results are comparable. Since the results of K-means depend on the initialization, we run this algorithm 10 times and choose the one with the best result. We compute the S index for each case and report the results in<ref type="figure" target="#fig_8">Figure 5</ref>. We can observe that the community detection results consistently give higher S index values, indicating that the structures of our anatomical networks are in higher accordance with the classical anatomy. We also plot the number of detected communities as the cutoff changes in<ref type="figure" target="#fig_8">Figure 5</ref>. We can see that the number of communities lies approximately between 100 and 250, which is largely in correspondence with the number of structures in classical anatomy. Detailed results on community identification are provided in the Supplementary Material. The classical anatomy was created mainly based on brain functions. Since functions are mainly determined by gene expression, the expression patterns within anatomical structures should be more correlated than those across structures. To validate this hypothesis, we show the distribution of the edges within and across the anatomical structures when π thr = 0.5 in<ref type="figure" target="#fig_10">Figure 6</ref>. We also show the number of edges within and across structures when the cutoff varies from 0.2 to 0.9. We can observe that the edges within structures dominate in all cases, indicating that the expression patterns within classical anatomy are highly correlated. We can also observe from<ref type="figure" target="#fig_10">Figure 6</ref>that the proportion of edges within anatomical structures increases as the cutoff increases. This indicates that most of the cross-structure edges have relatively small selection probabilities, and they are removed as the cutoff increases. The ranked lists of regions in terms of the number of connections are provided in the Supplementary Material.Shows the distribution of the edges within and across the anatomical structures when π thr = 0.5. The rows and columns correspond to the structures annotated in the ARA. This matrix is normalized to the interval<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>row by row, and hence it is not symmetric. Each row indicates the proportion of a particular structure's edges that connect to other structures. In particular, each entry (i,j) in the matrix represents the proportion of structure i's edges that connect to structure j. (B) Shows the number of edges within and between anatomical structures in ARA as the cutoff changes. between genes. Since genes involved in the same pathway usually exhibit similar expression patterns, correlated expression patterns may imply similar biological functions. We hence use Gene Ontology (GO) (<ref type="bibr" target="#b2">Ashburner et al., 2000</ref>) to evaluate the functional relationships among tightly connected genes in the network. In particular, we consider a gene and its direct neighbors as a group (<ref type="bibr" target="#b13">Gustafsson et al., 2005</ref>) and evaluate the functional enrichment of each group using the hypergeometric distribution (<ref type="bibr" target="#b6">Boyle et al., 2004</ref>). We apply Bonferroni correction for multiple hypothesis testing and consider GO terms with corrected P &lt; 0.05 as statistically significant (<ref type="bibr" target="#b6">Boyle et al., 2004</ref>). We vary the cutoff π thr and observe that most of the groups are annotated with at least one statistically significant GO term. In particular, when π thr = 0.5, there are 2702 groups annotated with at least one statistically significant GO term, and the average number of terms per group is 15. This indicates that most of the groups are associated with multiple enriched terms. It has been previously observed that the degrees of many biological networks follows a power-law distribution (<ref type="bibr" target="#b4">Barabási and Oltvai, 2004</ref>). This indicates that there exists a small number of highly connected genes known as hubs. We vary the cutoff and observe that the set of highly connected genes are largely consistent (details provided in the Supplementary Material). We report the top 10 genes with the largest number of connections in<ref type="figure">Table 1</ref>when π thr = 0.8 and show slices of their expression patterns in the Supplementary Material. We can observe that all these groups are highly enriched with the biological function binding or protein binding, implicating that they are likely to encode transcription factors. Among these 10 genes, the APP encodes an integral membrane protein expressed in many tissues and concentrated in the synapses of neurons. Homologous proteins have been identified in other organisms such as Drosophila, C.elegans and all mammals. APP is best known for its association with the Alzheimer's disease, and mutations in critical regions of APP cause familial susceptibility to Alzheimer's disease. It would be interesting to investigate how the 'hubness' of APP is related to CNS disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on genetic network modeling</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Illustration of tensor factorization. The three-way tensor on the left is factorized into the products of a core tensor and three basis matrices on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>[12:23 7/11/2011 Bioinformatics-btr558.tex] Page: 3295 3293–3299</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.2.</head><figDesc>Fig. 2. Illustration of the concentration matrix (A) and the corresponding graphical model (B). The zero entries in the concentration matrix are unfilled while the non-zero entries are filled with green. In this example, x 1 and x 5 are conditionally independent given all other variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.3.</head><figDesc>Fig. 3. Sample correlation patterns from the coronal view when the cutoff π thr = 0.3. The vertices are color-coded according to the ARA annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><figDesc>[12:23 7/11/2011 Bioinformatics-btr558.tex] Page: 3297 3293–3299</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.4.</head><figDesc>Fig. 4. Slices of the correlation patterns in the coronal (left), sagittal (middle) and horizontal (right) views when π thr = 0.4 (top) and 0.5 (bottom). Each vertex is labeled with the ARA informatics ID of the brain structure, and the corresponding structure name is given in Supplementary Table S2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><figDesc>Modeling of the gene interactions using the techniques described in Section 3 yields a network consisting of 3012 vertices in which vertices represent genes, and edges characterize the correlations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.5.</head><figDesc>Fig. 5. Comparison of the communities detected in the anatomical networks and the K-means clustering results. (A) Shows the S index comparison between the anatomical structures in ARA and the results of community detection and K-means. (B) Shows the number of communities as the cutoff changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig.6.</head><figDesc>Fig. 6. Visualization of the edge distribution within and between different brain regions. (A) Shows the distribution of the edges within and across the anatomical structures when π thr = 0.5. The rows and columns correspond to the structures annotated in the ARA. This matrix is normalized to the interval [0,1] row by row, and hence it is not symmetric. Each row indicates the proportion of a particular structure's edges that connect to other structures. In particular,</figDesc></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational network analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We model the anatomical and genetic organizations in the mouse brain as networks. To enable robust and efficient network construction, we employ tensor factorization techniques to reduce the data volumes. The resulting networks recover known relations and predict novel correlations not known from the literature. The employed network modeling formulation is an approximate scheme. It would be interesting to compare this approximate formulation with the exact one on small datasets, where exact optimization can be applied. The proposed methods can be applied to model other biological systems, such as the Drosophila transcriptional networks. We will explore the network modeling of other biological systems in the future.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1223</biblScope>
			<biblScope unit="issue">711</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>btr558. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3293" to="3299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Gene Ontology: tool for the unification of biology</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ashburner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Banerjee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="485" to="516" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Network biology: understanding the cell&apos;s functional organization</title>
		<author>
			<persName>
				<forename type="first">A.-L</forename>
				<surname>Barabási</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<forename type="middle">N</forename>
				<surname>Oltvai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Clustering of spatial gene expression patterns in the mouse brain and comparison with classical neuroanatomy</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">W</forename>
				<surname>Bohland</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">GO::TermFinder–open source software for accessing Gene Ontology information and finding significantly enriched Gene Ontology terms associated with a list of genes</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">I</forename>
				<surname>Boyle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3710" to="3715" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<monogr>
		<title level="m" type="main">Bagging, boosting and ensemble methods Handbook of Computational Statistics: Concepts and Methods</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bühlmann</surname>
			</persName>
		</author>
		<editor>Gentle,J. et al.</editor>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="877" to="907" />
			<pubPlace>Berlin, Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Covariance selection</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Dempster</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="157" to="175" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">The Allen Reference Atlas: A Digital Color Brain Atlas of the C57BL/6J Male Mouse</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">W</forename>
				<surname>Dong</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>Hoboken, New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to Graphical Modelling</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Edwards</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
			<pubPlace>New York, Inc</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse inverse covariance estimation with the graphical lasso</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="432" to="441" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<monogr>
		<title level="m" type="main">Applications of the lasso and grouped lasso to the estimation of sparse graphical models</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Constructing and analyzing a large-scale gene-togene regulatory network-lasso-constrained inference and biological validation</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gustafsson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="254" to="261" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">The Allen Brain Atlas: 5 years and beyond</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">R</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="821" to="828" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">G</forename>
				<surname>Kolda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">W</forename>
				<surname>Bader</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">A multilinear singular value decomposition</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">D</forename>
				<surname>Lathauwer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1253" to="1278" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">On the best rank-1 and rank-(R 1 ,R 2 ,···,R N ) approximation of higher-order tensors</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">D</forename>
				<surname>Lathauwer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1324" to="1342" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Genome-wide atlas of gene expression in the adult mouse brain</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">S</forename>
				<surname>Lein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">445</biblScope>
			<biblScope unit="page" from="168" to="176" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<monogr>
		<title level="m" type="main">SLEP: Sparse Learning with Efficient Projections</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Tempe, Arizona, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">High-dimensional graphs and variable selection with the lasso</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Meinshausen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bühlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Stability selection</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Meinshausen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bühlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding community structure in networks using the eigenvectors of matrices</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E J</forename>
				<surname>Newman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="page" from="74" to="036104" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Neuroinformatics for genome-wide 3-D gene expression mapping in the mouse brain</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Ng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="382" to="393" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">A tensor higher-order singular value decomposition for integrative analysis of DNA microarray data from different studies</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Omberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="18371" to="18376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<monogr>
		<title level="m" type="main">The Dentate Gyrus: A Comprehensive Guide to Structure, Function, and Clinical Implications</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">E</forename>
				<surname>Scharfman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Elsevier Science</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">TensorTextures: multilinear image-based rendering</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A O</forename>
				<surname>Vasilescu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Terzopoulos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="336" to="342" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Out-of-core tensor approximation of multi-dimensional matrices of visual data</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="527" to="535" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<monogr>
		<title level="m" type="main">Graphical Models in Applied Multivariate Statistics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Whittaker</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse graphical Gaussian modeling of the isoprenoid gene network in Arabidopsis thaliana</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Wille</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<monogr>
		<title level="m" type="main">Introduction to Vector and Tensor Analysis</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C</forename>
				<surname>Wrede</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Dover Publications</publisher>
			<pubPlace>Mineola, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized low rank approximations of matrices</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ye</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="167" to="191" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Model selection and estimation in the Gaussian graphical model</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Yuan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="19" to="35" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>