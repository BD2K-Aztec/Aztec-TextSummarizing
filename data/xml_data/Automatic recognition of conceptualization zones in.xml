
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Automatic recognition of conceptualization zones in scientific articles and two life science applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">7 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Maria</forename>
								<surname>Liakata</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<postCode>SY23 3DB</postCode>
									<settlement>Aberystwyth, Ceredigion</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Rebholz Group</orgName>
								<orgName type="institution" key="instit1">EMBL-EBI</orgName>
								<orgName type="institution" key="instit2">Wellcome Trust Genome Campus</orgName>
								<address>
									<postCode>CB10 1SD</postCode>
									<settlement>Hinxton, Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Shyamasree</forename>
								<surname>Saha</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Rebholz Group</orgName>
								<orgName type="institution" key="instit1">EMBL-EBI</orgName>
								<orgName type="institution" key="instit2">Wellcome Trust Genome Campus</orgName>
								<address>
									<postCode>CB10 1SD</postCode>
									<settlement>Hinxton, Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Simon</forename>
								<surname>Dobnik</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Philosophy, Linguistics and Theory of Science</orgName>
								<orgName type="institution">University of Gothenburg</orgName>
								<address>
									<postCode>405 30</postCode>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Colin</forename>
								<surname>Batchelor</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="department">Royal Society of Chemistry</orgName>
								<orgName type="institution">Thomas Graham House</orgName>
								<address>
									<addrLine>Science Park, Milton Road</addrLine>
									<postCode>CB4 0WF</postCode>
									<settlement>Cambridge, Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Dietrich</forename>
								<surname>Rebholz-Schuhmann</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Rebholz Group</orgName>
								<orgName type="institution" key="instit1">EMBL-EBI</orgName>
								<orgName type="institution" key="instit2">Wellcome Trust Genome Campus</orgName>
								<address>
									<postCode>CB10 1SD</postCode>
									<settlement>Hinxton, Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Automatic recognition of conceptualization zones in scientific articles and two life science applications</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="page" from="991" to="1000"/>
							<date type="published" when="2012">7 2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts071</idno>
					<note type="submission">Received on November 11, 2011; received on February 1, 2012; accepted on February 2, 2012</note>
					<note>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [13:02 12/3/2012 Bioinformatics-bts071.tex] Page: 991 991–1000 Associate Editor: Martin Bishop also contains detailed information pertaining to CoreSC annotation and links to annotation guidelines as well as a corpus of manually annotated articles, which served as our training data. * To whom correspondence should be addressed. Contact: liakata@ebi.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Scholarly biomedical publications report on the findings of a research investigation. Scientists use a well-established discourse structure to relate their work to the state of the art, express their own motivation and hypotheses and report on their methods, results and conclusions. In previous work, we have proposed ways to explicitly annotate the structure of scientific investigations in scholarly publications. Here we present the means to facilitate automatic access to the scientific discourse of articles by automating the recognition of 11 categories at the sentence level, which we call Core Scientific Concepts (CoreSCs). These include: Hypothesis, Motivation, Goal, Object, Background, Method, Experiment, Model, Observation, Result and Conclusion. CoreSCs provide the structure and context to all statements and relations within an article and their automatic recognition can greatly facilitate biomedical information extraction by characterizing the different types of facts, hypotheses and evidence available in a scientific publication. Results: We have trained and compared machine learning classifiers (support vector machines and conditional random fields) on a corpus of 265 full articles in biochemistry and chemistry to automatically recognize CoreSCs. We have evaluated our automatic classifications against a manually annotated gold standard, and have achieved promising accuracies with &apos;Experiment&apos;, &apos;Background&apos; and &apos;Model&apos; being the categories with the highest F1-scores (76%, 62% and 53%, respectively). We have analysed the task of CoreSC annotation both from a sentence classification as well as sequence labelling perspective and we present a detailed feature evaluation. The most discriminative features are local sentence features such as unigrams, bigrams and grammatical dependencies while features encoding the document structure, such as section headings, also play an important role for some of the categories. We discuss the usefulness of automatically generated CoreSCs in two biomedical applications as well as work in progress. Availability: A web-based tool for the automatic annotation of articles with CoreSCs and corresponding documentation is available online at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Since the launch of the first scientific journal in 1665, Philosophical transactions of the Royal Society, the scientific literature has developed into the core medium for the exchange of ideas and findings across all scientific communities. In recent years, numerous initiatives have emerged to automatically process electronic documents in the life sciences, add semantic markup to them and facilitate access to scientific facts. Most work in biological text mining (<ref type="bibr" target="#b0">Ananiadou et al., 2010;</ref><ref type="bibr" target="#b6">Cohen and Hersh, 2005</ref>) has concentrated on identifying biological entities and extracting the relations between these entities as facts or events appearing in article abstracts while recently, the focus has shifted towards full text articles (<ref type="bibr" target="#b16">Kim et al., 2011</ref>). While system performance on biomolecular event extraction is improving (<ref type="bibr" target="#b16">Kim et al., 2011</ref>), there is little progress in the analysis of the context of extracted events and relations which help to characterize the knowledge conveyed within the text and build the argumentation within the article discourse. The analysis of the scientific discourse plays a key role in differentiating between the nature of the knowledge encoded in relations and events, e.g. 'AhR agonists suppress B lymphopoiesis' in the fourth sentence of<ref type="figure" target="#fig_0">Figure 1</ref>is a known fact whereas 'the potential of two AhR agonists to alter stromal cell cytokine responses' in sentence 5 is a hypothesis to be investigated. Such a distinction between events or relations is currently ignored in standard biomedical information extraction. Discourse analysis of this type would improve the distinction between facts, speculative statements, pre-existing and new work. In<ref type="figure" target="#fig_0">Figure 1</ref>, factual sentences (denoted as 'Background', sentences 1, 2 and 4) are distinguished from a sentence containing information inferred from the 'Background', a hypothesis driving and justifying the work presented in the article ('Hypothesis', sentence 3). Sentence 5 which conveys the aim of the work as being that of evaluating a certain hypothesis, is annotated as both Goal and Hypothesis.The categorization of sentences within scientific discourse has been studied in previous work and from a number of different angles. Simone Teufel (<ref type="bibr" target="#b42">Teufel et al., 1999;</ref><ref type="bibr" target="#b45">Teufel, 2010</ref>) created argumentative zoning (AZ), an annotation scheme which models rhetorical and argumentational aspects of scientific writing and concentrates on author claims. AZ has been modified for the annotation of biology articles (<ref type="bibr" target="#b31">Mizuta et al., 2006</ref>) and chemistry articles (<ref type="bibr" target="#b43">Teufel et al., 2009</ref>). Other work has looked at the annotation of information structure in abstracts, based on abstract sections (<ref type="bibr" target="#b14">Hirohata et al., 2008;</ref><ref type="bibr" target="#b24">Lin et al., 2006;</ref><ref type="bibr" target="#b25">McKnight and Srinivasan, 2003;</ref><ref type="bibr" target="#b36">Ruch et al., 2007</ref>). A separate line of work has looked at the characterization of scientific discourse in terms of modality and speculation (<ref type="bibr" target="#b15">Kilicoglu and Bergler, 2008;</ref><ref type="bibr" target="#b23">Light et al., 2004;</ref><ref type="bibr" target="#b26">Medlock and Briscoe, 2007</ref>) while<ref type="bibr" target="#b37">Shatkay et al. (2008) and</ref><ref type="bibr" target="#b48">Wilbur et al. (2006)</ref>annotate sentences according to various dimensions such as focus, polarity and certainty. There is as yet no general consensus among researchers in scientific discourse regarding the optimal unit of annotation. Most of the previous research considers sentences as their basic unit while de<ref type="bibr" target="#b9">Waard et al. (2009)</ref>has proposed the annotation at the clause level and<ref type="bibr" target="#b33">Nawaz et al. (2010)</ref>and<ref type="bibr" target="#b46">Thompson et al. (2011)</ref>consider a multi-dimensional scheme for the annotation of biological events in texts (bio-events). Existing schemes vary in their scope and granularity, with ones designed for abstracts considering only four categories and schemes for full articles generally consisting of at most seven content-related categories. However, especially for the case of full articles, it is becoming apparent that more information is required to characterize statements and claims. Researchers are interested in identifying hypotheses and different types of evidence to support claims (<ref type="bibr" target="#b5">Ciccarese et al., 2008</ref>), which are not readily identifiable by current schemes. Our work fills the need for finer-grained annotation to capture the content and conceptual structure of a scientific article. Inspired by the definitions in the EXPO ontology for scientific experiments (<ref type="bibr" target="#b39">Soldatova and King, 2006</ref>) and the CISP meta-data (<ref type="bibr" target="#b40">Soldatova and Liakata, 2007</ref>), in Liakata and Soldatova (2008) and<ref type="bibr" target="#b22">Liakata et al. (2010)</ref>we introduced a sentence-based, three layer scheme which recognizes the main components of scientific investigations as represented in articles (see<ref type="figure" target="#fig_2">Fig. 2</ref>and Supplementary Material). The first layer consists of 11 categories which describe the main components of a scientific investigation, the second layer is properties of those categories (e.g. Novelty, Advantage), and the third layer provides identifiers that link together instances of the same concept. In comparison to closely related schemes (<ref type="bibr" target="#b10">de Waard, 2007;</ref><ref type="bibr" target="#b33">Nawaz et al., 2010;</ref><ref type="bibr" target="#b43">Teufel et al., 2009</ref>), none of which have been automated yet, the Core Scientific Concept (CoreSC) scheme makes finer grained distinctions between the different types of objective (Hypothesis–Goal–Motivation–Object), approach (Method–Model–Experiment) and outcome (Observation–Result– Conclusion) and constitutes the most fine grained analysis of knowledge types of any such scheme. The distinction between the above types of objective, approach and outcome are important to expert needs (For more details, see the definitions and explanations in the Supplementary Material.). The CoreSC scheme has been applied to articles in biochemistry and chemistry to create a corpus of 265 annotated articles (ART/CoreSC corpus, 39 915 sentences + 265 titles, over 1 million words) (<ref type="bibr" target="#b20">Liakata and Soldatova, 2009;</ref><ref type="bibr" target="#b22">Liakata et al., 2010</ref>).<ref type="bibr" target="#b13">Guo et al. (2011)</ref>showed that a finer level of annotation of cancer risk assessment (CRA) abstracts using CoreSC categories, increased experts' efficiency in extracting information from the text while<ref type="bibr" target="#b47">White et al. (2011)</ref>argue that the CoreSC scheme is 'uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence'. In this article, we automate the annotation of full scientific articles with categories from the first layer of the CoreSC scheme, provide intrinsic evaluation of the results and discuss existing and future applications of this work. The article is structured as follows: In Section 2, we describe how we trained and tested machine learning classifiers on automatic recognition of CoreSCs in full articles. In Section 3, we analyse the classifier performance and discuss the features used for building the classifiers and their contributions to each category. Finally in Section 4, we discuss existing and future applications of the work. Our system for the classification of CoreSCs, our guidelines and annotated articles are all available online for researchers in biology to use. To our knowledge this is the first time a discourse annotation scheme is being used to automatically annotate full articles in the biosciences on this scale. It is also the first such scheme for which machine learning classifiers have been trained and tested on chemistry articles. Both the resources and the tools for automatic annotation are available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Liakata et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>The data: the training and test data used as input to the machine learning classifiers consist of 265 articles from biochemistry and chemistry annotated at the sentence level by experts using the CoreSC annotation scheme. These articles constitute the ART/CoreSC corpus (<ref type="bibr" target="#b20">Liakata and Soldatova, 2009;</ref><ref type="bibr" target="#b22">Liakata et al., 2010</ref>), which was developed in three phases (training, evaluation and expansion). During the first-phase 20 annotators, all chemistryexperts at postdoc or PhD level, recruited from UK Universities, were trained on four full papers with the first version of the guidelines and detailed explanations resulting from error analysis. This data and individual comments from annotators were used to improve the annotation guidelines. The second phase was designed to evaluate both the guidelines and expert performance in terms of κ-inter-annotator agreement (κ-IAA). Our goal was to obtain IAA for a reasonable amount of papers, while ensuring at least three annotators per paper, so as to minimize the chance of random agreements. Thus, 16 annotators from the first phase were split into 5 groups of 3 annotators each, where each group annotated 8 different papers and 1 additional paper was common across all 5 groups. The 16th annotator annotated across groups to provide a normalizing factor. The κ-IAA for the 41 papers obtained in this manner, measured according to Cohen's κ (<ref type="bibr" target="#b7">Cohen, 1960</ref>), was κ=0.55 (median average for the 9 best annotators across all groups and the paper common to all annotators). The third and final phase of corpus development aimed at expanding the size of the corpus by selecting the nine best performing annotators (according to IAA) from the second phase to annotate 25 papers each. While no IAA could be obtained for the 225 papers 1 annotated in this way, the assumption is that it would be the same as the average of the agreement achieved by each of the nine annotators in the second phase of development. The 265 journal articles were chosen by a chemistry expert with extensive experience in publishing, so as to cover a wide range of topics and journals. The 265 articles cover 16 different chemistry journals and 25 topics, with the majority involving spectroscopy, biochemistry, kinetics and theoretical work. Article length ranges between 32 and 379 sentences and numbers of authors range between 1 and 11, with the majority attributed to 2–3 authors and being 150 sentences long. More details about the papers can be found in the Supplementary Material. The corpus has therefore good coverage of the field and was designed in three phases with the contribution of multiple experts so as to minimize classifier bias. Statistics on the corpus are available in<ref type="figure" target="#tab_1">Table 1</ref>. The corpus consists of 39 915 sentences (&gt;1 million words) with the majority categories being Result (21%) and Background (19%). The next most populous category is Observation (14%), followed by Method (11%), Experiment (10%), Conclusion (9%) and Model (9%). Finally, the categories designating the Objectives (Hypothesis, Object, Motivation and Goal) altogether amount to 7% with Object and Hypothesis the most prominent at 3% and 2%, respectively. To segment sentences we used the XML aware sentence splitter SSSplit, described in<ref type="bibr" target="#b20">Liakata et al., 2009</ref>. The choice of the sentence as our unit of annotation stems mainly from the fact that sentences are the most common unit of text selection for summaries (<ref type="bibr" target="#b2">Brandow et al., 1995;</ref><ref type="bibr" target="#b18">Kupiec et al., 1995</ref>). We also regard the sentence as the most meaningful minimal unit for the analysis of scientific discourse, in agreement with earlier work (<ref type="bibr">Teufel, 2000, Chapter 3</ref>). The methods: we have used state of the art supervised machine learning algorithms to train classifiers on the automatic annotation of papers with the first layer of the CoreSC scheme, that is, the following 11 categories: Background (BAC), Hypothesis (HYP), Motivation (MOT), Goal (GOA), Object (OBJ), Method (MET), Model (MOD), Experiment (EXP), Observation (OBS), Result (RES) and Conclusion (CON) (<ref type="bibr" target="#b22">Liakata et al., 2010</ref>). From a machine learning perspective we treat the recognition of CoreSCs as: (i) text classification and (ii) sequence labelling. In text classification sentences are classified independently of each other and any dependencies between sentences need to be added explicitly. On the other hand, in sequence labelling the assignment of labels is such as to satisfy dependencies between sentences. The latter is a more natural approach when considering discourse annotation since the flow of the narrative is influenced by what has already been mentioned. For classification, we employed support vector machines (SVMs) and for sequence labelling conditional random fields (CRFs). Previous work on discovering information structure from papers and abstracts has made successful use of both of these methods (<ref type="bibr" target="#b12">Guo et al., 2010;</ref><ref type="bibr" target="#b14">Hirohata et al., 2008;</ref><ref type="bibr" target="#b32">Mullen et al., 2005</ref>). While experimental settings vary in each of the above cases, most notably in the number and type of classification categories, the amount of training data available and whether abstracts of full papers are used, the best performing algorithms were SVMs and CRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic recognition of conceptualization zones</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM and LibLinear:</head><p>we used the LibSVM (LibS) implementation of SVMs (<ref type="bibr" target="#b3">Chang and Lin, 2011</ref>) coded in C++. Our experiments were conducted using a linear kernel, known to perform well in document classification. We used the default values for the C, γ and parameters and concentrated on the input features. When we experimented with different types of cross-validation and feature configuration we used LibLinear (LibL) (<ref type="bibr" target="#b11">Fan et al., 2008</ref>) instead of LibS as the latter is costly timewise both in training and testing. LibL is a classifier for large scale data, which uses linear SVMs, splits data into blocks and considers one block at a time. To give an indication about the gain in speed using LibL as opposed to LibS, it takes 29 h 41 min to train one of our models with LibS and 8 h 15 min for testing a single fold versus 10 min and 4 h 36 min, 2 respectively, for LibL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional</head><p>random fields: we chose CRFs because they do not assume independent features but do not suffer from the label bias problem, where preference is given to states with fewer transition possibilities. For our purposes we used CRFSuite (<ref type="bibr" target="#b34">Okazaki, 2007</ref>) an algorithm for linear-chain, first-order CRFs, optimized for speed and implemented in C. Stochastic Gradient Descent was employed for parameter estimation. Features for classification: features are extracted from each sentence and are represented in a sparse binary matrix format. In selecting features our aim was to take into account different aspects of a sentence, ranging from its location within the paper and the document structure (global features), to its length and sentence-internal features such as the citations, verbs, ngrams and grammatical triples (GRs) it may contain (local features). Below we describe all our features in detail. The following are all implemented as binary features: @BULLET Absolute location (absloc): we divide the document into 10 unequal segments (as in Loc of (<ref type="bibr" target="#b44">Teufel, 2000)</ref>) and assign 1 of the 10 locations, A–J, to the sentences. Larger segments, containing more sentences, are designated to be in the middle of the paper. @BULLET SectionId: a sequentially incremented section number (up to 10) is assigned to each section and inherited at sentence level. SectionId is@BULLET Struct-1: the location of a sentence within seven unequal segments of a section. 3 Each section is first divided into three equally sized slices; the first and the last sentence of the section are considered separate segments (1 and 7) whereas the second and the third sentence of the section also form a segment (2). The rest of the first slice is segment 3 and the second slice is segment 4. Segment 6 consists of the second and third sentence from the end of the section and the rest of the third slice is segment 5 (<ref type="bibr" target="#b44">Teufel, 2000</ref>). @BULLET Struct-2: location within a paragraph split in five equal segments. (<ref type="bibr" target="#b44">Teufel, 2000)</ref>@BULLET Struct-3: one of 16 heading types assigned to a sentence by matching its section heading against a set of regular expressions (a variant on Struct-3 of<ref type="bibr" target="#b44">Teufel, 2000</ref>). SectionId and Struct-3 are complementary features since the first pertains to the absolute location of a section and is dependent on the length of the paper, while the other follows section structure irrespective of paper length. Details on header matching are available in the Supplementary Material. @BULLET Location in section (sectionloc): like Struct-2 but at section level. @BULLET Length: sentences are assigned to one of nine bins, representing a word count range. More details are available in the Supplementary Material.</p><p>@BULLET Citation: we distinguish three cases: no citations, one citation, and two or more citations present. @BULLET History: the CoreSC category of the previous sentence. Only used in LibS and LibL, implicit in first-order CRF. @BULLET N-grams: binary values for significant unigrams (Uni), bigrams (Bi) and trigrams. N-grams are lemmatized using morpha (<ref type="bibr" target="#b30">Minnen et al., 2001</ref>). Significant unigrams have frequency &gt;3. Bigrams and trigrams are filtered according to the measure of Fair Symmetrical Conditional Probability and the LocalMaxs algorithm, defined in<ref type="bibr" target="#b38">Silva et al. (1999)</ref>. We considered filtering our n-grams by adapting an online stop word list. 4 However, classifier performance was better when we did not filter stop words. In this latter case, no trigrams exceeded the threshold. Examples of significant n-grams are available in the Supplementary Material. @BULLET Verb POS (VPOS): for each verb within the sentence we determine which of the six binary POS tags (VBD, VBN, VBG, VBZ, VBP and VB) representing the tense, aspect and person of a verb are present.</p><p>@BULLET Verbs: all verbs in our training data with frequency &gt;1.</p><p>@BULLET Verb Class: ten verb classes, obtained by clustering together all verbs with a frequency &gt;150 as in<ref type="bibr" target="#b12">Guo et al. (2010)</ref>. The verb classes can be found in the Supplementary Material.</p><p>3 A section is a block of sentences between two headings. 4 www.lextek.com/manuals/onix/stopwords1.html reduced to 186 words @BULLET Grammatical triples (GRs): dependency–head-dependent triples (Briscoe and Carroll format) generated using C&amp;C tools (<ref type="bibr" target="#b8">Curran et al., 2007</ref>). We used the model of the supertagger trained on biomedical abstracts (<ref type="bibr" target="#b35">Rimell and Clark, 2009</ref>) and applied selftraining on our papers according to<ref type="bibr" target="#b17">Kummerfeld et al. (2010)</ref>. We considered dependencies subj, dobj, iobj and obj2 with frequency &gt;3. Examples of significant GRs can be found in the Supplementary Material.</p><p>@BULLET Other GR: subjects (Subj), direct objects (Dobj), indirect objects (Iobj) and second objects of ditransitive verbs (Obj2) with frequency &gt;1. @BULLET Passive (P): whether any verbs are in passive voice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head><p>To test classification accuracy and establish feature contributions to CoreSC recognition we performed a number of runs, including multi-class (CRF, LibL and LibS) and binary classification using 9-fold cross-validation and a variety of feature configurations (All features, Leave-out-one-feature (LOOF) and Single feature with and without stop words). Our results (<ref type="figure" target="#tab_2">Table 2</ref>) show we can achieve accuracy of &gt;50% in classifying the 11 CoreSCs in full papers. This is a promising result given the difficulty of the task. It is the first time the automatic recognition of such a fine grained set of categories is being attempted for full papers. F-score for the categories ranges from 76% for EXP (Experiment) to 18% for the low frequency category MOT (Motivation). The distribution of categories in papers is shown in<ref type="figure" target="#tab_1">Table 1</ref>, with RES the most frequent category and MOT and GOA the least frequent. Our feature analysis shows that the most important role is played by n-grams (primarily bigrams), GRs and verbs as well as global features such as history (sequence of labels) and section headings. It is important to note that particular features do not affect all categories in the same way. In the following, we present our results in detail. Section 4 discusses various CoreSCbased applications already implemented on the basis of current results.<ref type="figure" target="#tab_2">Table 2</ref>shows that LibS has the highest accuracy at 51.6%, closely followed by CRF at 50.4% with LibL at 47.7%. All three classifiers outperform the simple baseline (Base) by a large margin. The latter consists of multinomial trials, which randomly label new instances according to the percentage of each CoreSC in the training data. We have also considered an n-gram baseline for both CRF and SVM and a history+n-gram baseline for SVM (history is implicit for CRF), which are discussed in the section on Feature Contribution. The best results overall are obtained from multi-class classification using all the features we considered.<ref type="figure" target="#fig_0">9 9 10 10 10 1 1 1 11</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifiers and categories:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic recognition of conceptualization zones</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">45 60 51 35 28 31 72 74 742 17 24 29 28 28 24 11 15 49 49 49 44449 47 48 42 26 32 24 12 16</head><p>LibLinear</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="39.9">41 47 44 26 23 25 61 67 64 27 18 22 24 24 24 15 12 14 43 45 44 38 37 38 39 39 39 31 25 27 17 13 15</head><p>LibSVM 41.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">41 50 45 30 225 66 66 66 30 23 26 26 25 26 215 18 48 44 46 39 44 41 45 38 41 33 28 30 21 13 16</head><p>Hist+ngram LibLinear</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="41.2">44 52 47 29 27 28 68 70 69 32 19 24 26 26 26 15 12 13 44 45 45 40 38 39 43 42 42 31 23 26 17 12 14</head><p>LibSVM 44.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">45 62 52 36 27 30 74 66 70 312 128 31 226 05 08 443 46 42 445 52 42 46 3126 23 013</head><p>All Binary CRFSuite</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="34.7">60 51 55 51 32 39 78 72 75 39 13 19 33 17 22 28 10 15 53 40 46 46 31 37 58 37 45 42 18 25 28 06 10</head><p>LibLinear</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="34.6">53 60 56 41 39 40 69 73 71 32 21 25 27 25 26 23 18 20 45 47 46 44 43 43 45 45 45 35 26 30 18 12 14</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All Multi</head><p>CRFSuite</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="50.4">56 65 60 46 42 44 74 78 76 41 21 28 31 29 30 29 13 18 50 52 51 46 49 47 53 52 52 42 28 34 26 14 18</head><p>LibLinearInterestingly the combination of binary classifiers (one for each CoreSC category) gave the highest precision in most cases but recall was significantly lower than in the multi-class scenario. There is not a significant difference in performance between LibS+all features and CRF: five categories seem to be predicted better by LibS and for the other six CRF performs better. When the history feature is absent, LibS and LiBL perform much lower than CRF but hist+n-gram for LibS is comparable to the n-gram performance of CRF. This highlights the importance of category sequence information for the task. The performance of LibL lags slightly behind both LiBS and CRF but this is to be expected since it is an approximation for linear SVMs. The highest performing categories for all three classifiers are EXP, BAC and MOD with an F-score of 76%, 62% and 53%, respectively. BAC is the second most frequent category (19%) in the corpus after RES, so high recall is not surprising. EXP and MOD (experimental and theoretical methods) are more interesting, as they are moderately frequent (10 and 9%), respectively. Furthermore, EXP and MOD are the only categories which have a higher F-score in automatic recognition compared with κ-IAA (<ref type="bibr" target="#b22">Liakata et al., 2010</ref>) as shown in<ref type="figure" target="#fig_5">Figure 3</ref>. On the other hand categories with high κ such as CON, MET and OBJ were more difficult to classify than expected. While κ was measured on only 41 papers (5022 sentences) (<ref type="bibr" target="#b22">Liakata et al., 2010</ref>), which may not be representative of the entire corpus, these results suggest that there is not necessarily a direct correlation between annotator agreement and classifier performance. This is in support of Beigman Klebanov and Beigman, 2009, which argues that IAA is neither sufficient nor necessary for obtaining reliable data from annotated material but rather it is important to focus on non-noisy, 'easy' instances. Beigman Klebanov and Beigman, 2009 suggest researchers should report the level of noise in a corpus and only use nonnoisy (easy) instances for testing. They emphasize the importance of requiring the agreement between more than two annotators, which reduces both the chance of random agreements as well as hard case bias, whereby a classifier tends to model the pattern of bias of a particular annotator for instances which are hard to predict. By having different phases of corpus development, with a varied number of annotators for each phase and subset of the corpus as well as a large number of classification categories, we believe that we have minimized the chance of random agreements and hard case bias. Therefore, we can infer that when our machine learning annotations agree with manual annotations, noise levels will be usually low, instances will be easier to predict and thus classifier confidence will be higher. Indeed this is confirmed both by a Pearson moment correlation test between agreement and classifier confidence and a Welch T-test for classifier confidence values in cases of agreement and disagreement, both of which gave a p &lt;2.2e-16 at 99%. They showed a direct correlation between classifier confidence and agreement between manual annotation and classifiers. Details are in the Supplementary Material. Classifier confidence for an instance is a probability, where a high value indicates high classifier confidence for the particular prediction. As an indication of the noise for different categories in the corpus, we show the confidence of the machine learning classifiers when both classifiers agree with the manual annotation and when there is no agreement between either the classifiers or the manual annotation (see<ref type="figure" target="#fig_7">Figs 4 and 5</ref>). For the cases where LibSVM agrees with CRF and the manual annotation, confidence scores are high, with over 75% of the data having a confidence value of &gt;0.6, and over 50% of the data having a confidence score of over 0.7. This can be compared against the situation of disagreement where only 25% of the data have a confidence score of 0.6. For EXP, BAC and MOD the confidence scores are especially high in cases of agreement, with 50% of the data having a confidence score of over 0.87. Therefore, agreements for EXP and MOD consist mostly of non-noisy (easy)instances. Classifier confidence for the entire corpus is depicted in<ref type="figure" target="#fig_9">Figure 6</ref>. Assuming that lack of noise correlates with high classifier confidence, we can say that &gt;50% of data in each category (and in most well beyond 75%) is non-noisy. Classifier performance for the CoreSC categories can be ranked from highest to lowest F-score as follows: EXP &gt; BAC &gt; MOD &gt; RES &gt; OBS &gt; CON &gt; OBJ &gt; MET &gt; GOA &gt; HYP &gt; MOT.OBJ performs well given its low frequency, suggesting that OBJ sentences contain distinct features. The low scores for MET may be due to noise introduced by our neglect of the distinction between MET-Old and MET-New (<ref type="bibr" target="#b22">Liakata et al., 2010</ref>). The low F-score for MOT and HYP are due to their low frequency as the levels of noise are similar to those of OBJ. We intend to boost performance for the low frequency categories by using active learning. These are promising results given the complexity of the task, the number of the categories and their distribution in the corpus. A confusion matrix (<ref type="figure" target="#fig_10">Fig. 7</ref>) gives an indication of which categories have consistent overlaps. There is bias in favour of the BAC category due to its high frequency and broad definition, which we will need to counterbalance in the future. CON is often taken as RES whereas RES is often confused with OBS and vice versa. GOA is often assigned to OBJ and MET, the latter presumably because goals and method are often expressed in the same sentence. MET is confused with EXP and BAC, the latter because we have not yet considered the second layer of CoreSC annotation at this stage, which caters for MET-Old, methods mentioned in previous work. OBJ is often confused with MET, since a method can be the object of an investigation. 5 Finally, HYP is often assigned to RES, CON and BAC. This can be explained by the fact that a weak result or conclusion is often expressed in the same language as a hypothesis, while a hypothesis may also be expressed as an assumption arising from background knowledge. For examples see the Supplementary Material. If we merge CoreSC categories so that we consider a coarser grain layer of four categories, namely Prior (BAC), Approach (MET+MOD+EXP), Outcome (OBS+RES+CON) and Objective (MOT+GOA+HYP+OBJT) then our F-measures respectively become: BAC: 59%, Approach: 72%, Outcome: 81%, Objective: 38%. A variant merge with seven categories, roughly corresponding to the scheme proposed by<ref type="bibr" target="#b9">de Waard et al., 2009</ref>, which considers BAC, HYP, Problem(=MOT), GOA=(GOA+OBJT), MET= (MET+EXP+MOD), RES=(OBS+RES), Implication(=CON), gives us F1: BAC: 60%, CON: 44%, MET: 72%, GOA: 47%, MOT: 19%, HYP: 18% and RES: 72% This shows the flexibility of our scheme for different applications, which may require different levels of granularity.feature is omitted. For each CoreSC category we have highlighted the lowest scores (bold), corresponding to the most important features being left out, and the highest scores (italic), corresponding to features whose omission has less impact on classification. Performance for all categories drops when all n-grams are removed. Since features are not independent, many of the important features of other categories are covered in n-grams but this does not necessarily work in both directions. Primarily, bigrams are more important than unigrams, since many of the former contain the latter. Categories affected most by the omission of unigrams are the low frequency categories GOA, MOT and HYP for CRF and MOT, HYP for LibL. Bigrams are not as important for these categories and removing them improves performance in the case of MOT and HYP. This is probably because they are not frequent enough for association with bigrams. Removing the verb feature has a negative effect on MOT, HYP and GOA in CRF and GOA and HYP in LibL. This agrees with our observation of the importance of verbs in single feature classification (<ref type="figure" target="#fig_12">Fig. 8</ref>). The high frequency categories are more robust to omission of features, whereas the lower frequency categories are dependent on all features. Single feature classification is more meaningful with respect to individual feature contributions and<ref type="figure" target="#fig_12">Figure 8</ref>paints a clear picture of which features are most important for which category. We believe this to be the most interesting finding of our analysis. While<ref type="figure" target="#fig_12">Figure 8</ref>shows the general trend whereby n-grams (D) (bigrams and GRs are not actually shown in<ref type="figure" target="#fig_12">Figure 8</ref>, but they strongly correlate with unigrams) followed by direct object (E) and verb (F) as accounting for the overall F-measure of a category, this is not true for all categories. For EXP, BAC and CON section headings (C) matter more than n-grams and for BAC, CON and RES absolute location (M) also plays a prominent role, meaning that the location of these three categories tends to be fixed in a paper (presumably in the beginning and the end). Citations (O) play an important role in discriminating BAC and are also prominent for CON, RES and MET to some extent. Verbs (F) are usually more important than subjects (G) but slightly less important than direct objects (E), however verbs (F) feature more prominently for categories such as RES, GOA, HYP and OBJ suggesting that particular verbs are used in the context of these categories. Perhaps more feature engineering involving semantic categories of verbs would benefit the low frequency categories. Verb tense (expressed by VPOS (I)) does not seem to play a major role, though its contribution is higher for OBS and RES. Looking at the feature profile of different categories, RES and MET show the least variation between individual feature contribution but it is clear that RES is more location specific than MET.<ref type="figure" target="#tab_5">Table 5</ref>shows the number of individual features considered for each feature type. The vast majority of features are bigrams (42 438), unigrams (10 515) and GR triples (11 854), which also explains their importance for the classification. This makes the prominence of citations and global structural features such as section headings all the more important whenever we encounter them. Variants of some of the above features have been used by<ref type="bibr" target="#b41">Teufel and Moens (2002</ref><ref type="bibr" target="#b32">), Mullen et al. (2005</ref><ref type="bibr" target="#b27">) and Merity et al. (2009</ref>to automate AZ.<ref type="bibr" target="#b27">Merity et al., 2009</ref>found that n-grams (unigrams and bigrams) in combination with knowledge of the label of previous sentences (history) constituted a very strong baseline for AZ. This agrees with our findings in general, where n-grams are roughly responsible for 40% of the system accuracy, the history category contributes another 5% and a further 5–6% is due to all other features. In the future, we intend to consider more elaborate semantic classes for features and also consider training individual classifiers for each category which we would then combine using stacking or ensemble techniques.Numbers for each type of feature were: L, length; H, history; C, citation; Gl, global features, including absloc, sectionid, struct1-3, sectionloc and experimental settings differ significantly. Earlier work on automating discourse schemes with four categories (<ref type="bibr" target="#b14">Hirohata et al., 2008;</ref><ref type="bibr" target="#b24">Lin et al., 2006;</ref><ref type="bibr" target="#b25">McKnight and Srinivasan, 2003</ref>) has reported F-measures in the 80s or 90s. However, in addition to having a third of the number of categories, these schemes only concentrate on abstracts, which are shown to have a very different structure from full articles.<ref type="bibr" target="#b37">Shatkay et al. (2008)</ref>annotate sentences from full articles but they evaluate on a small scale and do not attempt to classify an entire article. Their scheme has only three to four categories per dimension, where each dimension is evaluated separately from the rest. Our results are more comparable to<ref type="bibr">Mullen</ref>better representation of the discourse structure of the articles so as to identify hypotheses and relevant evidence (see Section 1). This will contribute to more advanced information extraction solutions in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic recognition of conceptualization zones</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATIONS AND FUTURE WORK</head><p>One of the applications stemming from our work is the use of automatically generated CoreSC annotations for the production of extractive summaries of full papers in chemistry and biochemistry. Such summaries are different from abstracts as they are longer (20 sentences) and represent the entire content of the paper, from Background and Hypotheses to Experiments performed, main Observations and Results obtained. The idea is that such summaries could be read much faster than the paper but convey a lot more of the key information than the abstract, which often acts as a selling point of the paper. We created summaries so that each contained 1–2 sentences from each CoreSC category (Hypothesis, Background, etc.), extracted from the original paper, following the distribution of categories in the paper. These summaries were given to 12 experts divided into 4 groups, along with summaries created using Microsoft Autosummarize and summaries written by humans. The automatically generated summaries performed significantly better than Miscrosoft autosummarize and achieved a 66% and 75% precision in answering complex content based questions. In some cases they outperformed human summaries. 6 Questionbased extractive summaries created using CoreSCs could be used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic recognition of conceptualization zones</head><p>to help speed up curation and we plan to explore this in the future. A different user based study, involved collaboration with experts in CRA, who were presented with abstracts that contained CoreSC annotations and abstracts with no annotations, or annotations originating from simpler schemes (abstract sections or an AZ variant) (<ref type="bibr" target="#b13">Guo et al., 2011</ref>). Three experts were timed as they answered questions about the main objectives and methods described in abstracts and it was shown that experts responded consistently faster when given abstracts annotated with CoreSCs than in the rest of the cases, while no significant difference was observed pertaining to the quality of the responses. In the future, we plan to perform more question based user studies with CRA experts, using full papers. We also plan to use CoreSC annotated papers in biology to guide information extraction and retrieval, characterize extracted events and relations and also facilitate inference from hypotheses to conclusions in scientific papers. Our web-based tool for the automatic annotation of CoreSC categories in full biomedical papers from Pubmed Central is available for biologists to download and use. The ability to automatically identify and qualify discourse structure from the scientific literature has far-reaching implications. The original facts and results from a scientific publication form the key information to be extracted in order to curate biological resources and validate against resources such as UnitProtKb, EntrezGene, Reactome and others. The different types of conceptualization zones defined by CoreSCs (Background, Hypothesis, Method, etc.) so far have been used to create extractive summaries and more use cases of filtering text during information extraction are in progress. Work in progress also involves the application of CoreSC annotations to full papers involving CRA and drug–drug interactions and preliminary results show that the annotation scheme and categorization methods generalize well to these new domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Example of discourse labelling using CoreSC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [13:02 12/3/2012 Bioinformatics-bts071.tex] Page: 993 991–1000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Hierarchical representation of concepts and properties in the CoreSC scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>Libsvm 51.6 56 68 62 50 41 45 72 78 75 37 20 26 33 25 29 25 06 10 53 47 50 46 57 51 54 52 53 43 29 34 32 13 19</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.3.</head><figDesc>Fig. 3. F-score versus κ for CoreSCs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [13:02 12/3/2012 Bioinformatics-bts071.tex] Page: 996 991–1000 M.Liakata et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.4.</head><figDesc>Fig. 4. Confidence value when LibS, CRFSuite and manual annotation agree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.5.</head><figDesc>Fig. 5. Confidence value when there is no agreement on annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig.6.</head><figDesc>Fig. 6. Confidence value scores per category for the entire corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig.7.</head><figDesc>Fig. 7. Confusion matrix for CoreSC categories according to LibS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><figDesc>Feature contribution: we examine feature contribution in LOOF cross-validation and single feature runs, using CRF and LibL. Tables 3 and 4 show how F-score is affected when each type of Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [13:02 12/3/2012 Bioinformatics-bts071.tex] Page: 997 991–1000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig.8.</head><figDesc>Fig. 8. Single feature classification with LibL, illustrating the contribution of 15 individual features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [13:02 12/3/2012 Bioinformatics-bts071.tex] Page: 999 991–1000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>Statistics on the training data (ART/CoreSC corpus) 

Measure 
Bac 
Con 
Exp 
Goa 
Met 
Mot 
Obs 
Res 
Mod 
Obj 
Hyp 
Total 

Number of sentences 
7606 
3636 
3858 
582 
4281 
541 
5410 
8404 
3656 
1161 
780 
39 915 
Number of words 
193 930 102 173 93 882 16 564 107 309 13 737 123 394 224 353 99 313 29 215 21 315 1 025 185 
Percentage of sentences 
19 
9 
10 
1 
11 
1 
14 
21 
9 
3 
2 
Number of words p/s (mean) 25.5 
28.1 
24.33 
28.46 
25.07 
25.39 
22.81 
26.7 
27.16 
25.16 
27.33 
Number of words p/s (SD) 
12.32 
12.49 
20.6 
12.69 
11.4 
10.34 
11.44 
12.65 
14.76 
11.16 
12.01 
κ-IAA 
0.87 
0.89 
0.65 
0.60 
0.74 
0.46 
0.79 
0.78 
0.43 
0.81 
0.46 

assigned independently of the section heading, which is addressed by 
feature Struct-3 below. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 2. Micro precision, recall and F-measure for different system configurations, with highest value for each measure per category in bold</figDesc><table>Acc BAC 
CON 
EXP 
GOA 
MET 
MOT 
OBS 
RES 
MOD 
OBJ 
HYP 

Features 
Classifier 
P R F P R F P R F P R F P R F P R F P R F P R F P R F P R F P R F 

Base 
Multinomial 14 
19 19 19 9 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>47.7 54 60 57 43 40 41 69 73 71 35 20 25 29 28 28 22 16 18 47 49 48 45 44 45 49 49 49 38 28 32 21 15 18</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 3. F-measures for CRFSuite LOOF, 9-fold cross-validation</figDesc><table>Feat 
BAC CON EXP GOA MET MOT OBS RES MOD OBJ HYP 

all 
60 44 76 28 30 18 
51 47 52 
34 18 
length 60 44 76 27 30 18 
51 47 53 
34 19 
ref 
58 44 76 26 30 18 
51 47 52 
34 17 
absloc 60 44 76 28 29 18 
51 47 52 
34 18 
struct1 60 43 76 27 30 18 
51 47 52 
33 17 
secid 
60 44 76 27 30 18 
51 48 51 
35 17 
Struct-2 60 44 76 27 30 17 
51 47 52 
34 18 
SecLoc 60 44 76 27 30 19 
51 47 52 
34 18 
Struct-3 60 43 75 26 30 18 
51 47 52 
34 19 

uni 
60 44 76 24 29 17 
50 47 51 
33 15 
bi 
59 43 75 27 30 22 
50 46 50 
32 19 
ngrams 58 42 74 25 29 17 
47 45 48 
29 14 
gr 
60 44 75 27 30 18 
51 47 52 
35 17 
pos 
60 44 76 26 29 18 
51 47 53 
34 16 
subj 
60 45 76 27 30 17 
51 48 52 
34 18 
dobj 
60 45 76 28 30 18 
51 47 53 
34 19 
iobj 
60 44 76 27 30 18 
51 47 52 
34 18 
obj2 
60 44 76 28 30 18 
51 47 52 
34 18 
vclass 60 44 76 27 30 18 
51 47 52 
34 18 
verb 
60 44 76 27 30 17 
51 47 52 
34 17 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><figDesc>Table 4. F-measures for LibLinear LOOF, 9-fold cross-validation</figDesc><table>Feat 
BAC CON EXP GOA MET MOT OBS RES MOD OBJ HYP 

all 
57 41 71 25 28 18 
48 45 49 
32 18 
history 55 41 71 28 27 20 
48 43 46 
32 17 
length 57 42 71 24 28 19 
48 45 48 
31 17 
ref 
55 41 71 25 28 19 
48 45 48 
31 15 
absloc 57 40 72 25 28 20 
48 45 49 
33 18 
Struct-1 57 41 71 26 28 21 
48 45 49 
31 17 
secid 
57 41 71 26 28 19 
48 44 48 
32 17 
Struct-2 57 41 71 25 28 19 
48 45 49 
32 17 
SecLoc 57 41 71 26 28 18 
48 45 48 
32 18 
Struct-3 56 40 70 25 27 19 
48 44 47 
30 19 

uni 
56 41 72 26 27 17 
47 44 46 
31 16 
bi 
54 40 70 25 27 20 
46 43 45 
27 17 
ngrams 53 37 69 23 26 17 
44 42 41 
26 12 
gr 
56 40 71 23 29 19 
47 44 49 
31 17 
pos 
57 42 71 25 28 20 
48 45 49 
32 16 
subj 
57 41 71 25 29 21 
47 45 48 
31 17 
dobj 
57 42 71 25 28 20 
48 45 49 
32 19 
iobj 
57 41 71 26 28 19 
48 45 48 
32 17 
obj2 
57 41 71 25 28 18 
48 45 49 
32 18 
vclass 57 41 71 26 28 19 
48 45 49 
33 17 
verb 
57 41 71 24 28 20 
48 45 49 
32 16 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><figDesc>with related work: a direct comparison between our results and earlier work is not possible, as the scope, schemes</figDesc><table>Copyedited by: TRJ 

MANUSCRIPT CATEGORY: ORIGINAL PAPER 

[13:02 12/3/2012 Bioinformatics-bts071.tex] 
Page: 998 991–1000 

M.Liakata et al. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><figDesc>Table 5. Numbers for each type of feature</figDesc><table>Feat Uni 
Bi 
GR VPOS Subj Dobj Iobj Obj2 Verb VC P H Gl L C 

No. 10 515 42 438 11 854 
6 
3843 7414 45 59 1543 10 1 12 53 9 3 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> one of the 225 papers had been annotated already in phase II, giving a total of 265 unique papers</note>

			<note place="foot" n="2"> Testing is done sentence by sentence and so takes longer than training. 993 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="5"> See definitions in Supplementary Material. 996 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="6"> The details of this experiment is the focus of a separate publication under submission. 998 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We are very grateful to Dr Stephen Clark, Prof. Bonnie Webber, Dr Naoaki Okazaki, Dr Nigel Collier, Dr Anna Korhonen, Yufan Guo, Dr Ian Lewin, Jee-Hyub Kim, Dr Simone Teufel, Dr Amanda Clare and Dr Andrew Sparkes for their useful feedback. We also thank our domain experts (28 in total) who helped us with corpus annotation and summary evaluation and Dr Colin Sauze for his help with the code interfacing the machine learning model and the online tool. Last but not least we would like to thank the anonymous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Event extraction for systems biology by text mining the literature</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ananiadou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Biotechnol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="381" to="390" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">From annotator agreement to noise models</title>
		<author>
			<persName>
				<forename type="first">Beigman</forename>
				<surname>Klebanov</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Beigman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="495" to="503" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic condensation of electronic publications by sentence selection</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Brandow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName>
				<forename type="first">C.-C</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C.-J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="1" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">The swan biomedical discourse ontology</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ciccarese</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="739" to="751" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of current work in biomedical text a survey of current work in biomedical text mining</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">M</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">R</forename>
				<surname>Hersh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educ. Psychol. Meas</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale nlp with c&amp;c and boxer</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Curran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying the epistemic value of discourse segments in biology texts</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>De Waard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Computational Semantics, IWCS-8 &apos;09</title>
		<meeting>the Eighth International Conference on Computational Semantics, IWCS-8 &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="351" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">A pragmatic structure for research articles</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>De Waard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Pragmatic Web, ICPW &apos;07</title>
		<meeting>the 2nd International Conference on Pragmatic Web, ICPW &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="83" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">LIBLINEAR: a library for large linear classification</title>
		<author>
			<persName>
				<forename type="first">R.-E</forename>
				<surname>Fan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying the information structure of scientific abstracts: an investigation of three different schemes</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Biomedical Natural Language Processing</title>
		<meeting>the 2010 Workshop on Biomedical Natural Language Processing<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison and user-based evaluation of models of textual information structure in the context of cancer risk assessment</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying sections in scientific abstracts using conditional random fields</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Hirohata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP</title>
		<meeting>the IJCNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing speculative language in biomedical research articles: a linguistically motivated perspective</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Kilicoglu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bergler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl. . 11</note>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Overview of BioNLP shared task 2011</title>
		<author>
			<persName>
				<forename type="first">J.-D</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioNLP Shared Task 2011 Workshop</title>
		<meeting>BioNLP Shared Task 2011 Workshop<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster parsing by supertagger adaptation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Kummerfeld</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="345" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">A trainable document summarizer</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kupiec</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;95</title>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;95<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Guidelines for the annotation of general scientific concepts</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liakata</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Soldatova</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JISC Project Report</title>
		<imprint>
			<biblScope unit="volume">8888</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<monogr>
		<title level="m" type="main">The ART Corpus</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liakata</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Soldatova</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic annotation of papers: interface &amp; enrichment tool (SAPIENT)</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liakata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioNLP-09</title>
		<meeting>BioNLP-09<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Corpora for the conceptualization and zoning of scientific papers</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liakata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10), European Language Resources Association</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10), European Language Resources Association<address><addrLine>Valletta, Maltapp</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2054" to="2061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">The language of bioscience: facts, speculations, and statements in between</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Light</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004 Workshop: BioLINK 2004, Linking Biological Literature, Ontologies and Databases</title>
		<editor>Hirschman,L. and Pustejovsky,J.</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative content models for structural analysis of medical abstracts</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis</title>
		<meeting>the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Categorization of sentence types in medical abstracts</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Mcknight</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Srinivasan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annu Symp Proc</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="440" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised learning for hedge classification in scientific literature</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Medlock</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Briscoe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th Annual Meeting of the ACL</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Accurate argumentative zoning with maximum entropy models</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Merity</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, NLPIR4DL &apos;09</title>
		<meeting>the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, NLPIR4DL &apos;09<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08-30" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="m">oxfordjournals.org/ Downloaded from Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1000" to="991" />
		</imprint>
	</monogr>
	<note>bts071. .tex] Page</note>
</biblStruct>

<biblStruct   xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liakata</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Applied morphological processing of english</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Minnen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="207" to="223" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Zone analysis in biology articles as a basis for information extraction</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Mizuta</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="468" to="487" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">A baseline feature set for learning rhetorical zones using full articles in the biomedical domain</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Mullen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="52" to="58" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Meta-knowledge annotation of bio-events</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Nawaz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2498" to="2507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<monogr>
		<title level="m" type="main">CRFsuite: a fast implementation of Conditional Random Fields (CRFs), http://www.chokkan.org/software</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Okazaki</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007-02-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Porting a lexicalized-grammar parser to the biomedical domain</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Rimell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Clark</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="852" to="865" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Using argumentation to extract key sentences from biomedical abstracts</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ruch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-dimensional classification of biomedical text: toward automated, practical provision of high-utility text to diverse users</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Shatkay</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bioinform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="18" to="2086" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Using localmaxs algorithm for the extraction of contiguous and non-contiguous multiword lexical units</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">F</forename>
				<surname>Silva</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Portuguese Conference on Artificial Intelligence: Progress in Artificial Intelligence, EPIA &apos;99</title>
		<meeting>the 9th Portuguese Conference on Artificial Intelligence: Progress in Artificial Intelligence, EPIA &apos;99<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="113" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">An ontology of scientific experiments</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Soldatova</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>King</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Soc. Interf</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="795" to="803" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<monogr>
		<title level="m" type="main">An ontology methodology and cisp-the proposed core information about scientific papers</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Soldatova</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liakata</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Summarizing scientific articles: experiments with relevance and rhetorical status</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Moens</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="409" to="445" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">An annotation scheme for discourse-level argumentation in research articles</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards discipline-independent argumentative zoning: evidence from chemistry and computational linguistics</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1493" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b44">
	<monogr>
		<title level="m" type="main">Argumentative Zoning: Information Extraction from Scientific Text</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<pubPlace>Edinburgh</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<monogr>
		<title level="m" type="main">The Structure of Scientific Articles: Applications to Citation Indexing and Summarization. Stanford: CSLI Publications, CSLI Studies in Computational Linguistics</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Stanford, California</pubPlace>
		</imprint>
	</monogr>
	<note>United. States</note>
</biblStruct>

<biblStruct   xml:id="b46">
	<analytic>
		<title level="a" type="main">Enriching a biomedical event corpus with meta-knowledge annotation</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Thompson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">393</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b47">
	<analytic>
		<title level="a" type="main">Hypothesis and evidence extraction from full-text scientific journal articles</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>White</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioNLP 2011 Workshop</title>
		<meeting>BioNLP 2011 Workshop<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="134" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b48">
	<analytic>
		<title level="a" type="main">New directions in biomedical text annotations: definitions, guidelines and corpus construction</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">J</forename>
				<surname>Wilbur</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">356</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>