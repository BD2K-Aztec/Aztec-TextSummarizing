
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Improving the efficiency of multidimensional scaling in the analysis of high-dimensional data using singular value decomposition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011 . 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Christophe</forename>
								<surname>Bécavin</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Institut des Hautes Études Scientifiques</orgName>
								<address>
									<addrLine>35 route de Chartres</addrLine>
									<postCode>91440</postCode>
									<settlement>Bures sur Yvette</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Nicolas</forename>
								<surname>Tchitchek</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Institut des Hautes Études Scientifiques</orgName>
								<address>
									<addrLine>35 route de Chartres</addrLine>
									<postCode>91440</postCode>
									<settlement>Bures sur Yvette</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Colette</forename>
								<surname>Mintsa-Eya</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Institut des Hautes Études Scientifiques</orgName>
								<address>
									<addrLine>35 route de Chartres</addrLine>
									<postCode>91440</postCode>
									<settlement>Bures sur Yvette</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institut de Recherche Interdisciplinaire</orgName>
								<orgName type="institution" key="instit1">CNRS USR3078</orgName>
								<orgName type="institution" key="instit2">Universite de Lille I</orgName>
								<address>
									<addrLine>50 avenue de Halley, 59658 Villeneuve d&apos;Ascq</addrLine>
									<region>II</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Annick</forename>
								<surname>Lesne</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Institut des Hautes Études Scientifiques</orgName>
								<address>
									<addrLine>35 route de Chartres</addrLine>
									<postCode>91440</postCode>
									<settlement>Bures sur Yvette</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire Physique Théorique de la Matière Condensée</orgName>
								<orgName type="institution" key="instit1">CNRS UMR7600</orgName>
								<orgName type="institution" key="instit2">Université Pierre et Marie Curie Paris 6</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>75252, Cedex 05</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Arndt</forename>
								<surname>Benecke</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Institut des Hautes Études Scientifiques</orgName>
								<address>
									<addrLine>35 route de Chartres</addrLine>
									<postCode>91440</postCode>
									<settlement>Bures sur Yvette</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institut de Recherche Interdisciplinaire</orgName>
								<orgName type="institution" key="instit1">CNRS USR3078</orgName>
								<orgName type="institution" key="instit2">Universite de Lille I</orgName>
								<address>
									<addrLine>50 avenue de Halley, 59658 Villeneuve d&apos;Ascq</addrLine>
									<region>II</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Improving the efficiency of multidimensional scaling in the analysis of high-dimensional data using singular value decomposition</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Bioinformatics BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">17</biblScope>
							<biblScope unit="issue">27 10</biblScope>
							<biblScope unit="page" from="40" to="58"/>
							<date type="published" when="2011">2011 . 2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr143</idno>
					<note type="submission">Received on November 4, 2010; revised on February 6, 2011; accepted on March 13, 2011</note>
					<note>Associate Editor: Jonathan Wren Contact: arndt@ihes.fr</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Multidimensional scaling (MDS) is a well-known multivariate statistical analysis method used for dimensionality reduction and visualization of similarities and dissimilarities in multidimensional data. The advantage of MDS with respect to singular value decomposition (SVD) based methods such as principal component analysis is its superior fidelity in representing the distance between different instances specially for high-dimensional geometric objects. Here, we investigate the importance of the choice of initial conditions for MDS, and show that SVD is the best choice to initiate MDS. Furthermore, we demonstrate that the use of the first principal components of SVD to initiate the MDS algorithm is more efficient than an iteration through all the principal components. Adding stochasticity to the molecular dynamics simulations typically used for MDS of large datasets, contrary to previous suggestions, likewise does not increase accuracy. Finally, we introduce a k nearest neighbor method to analyze the local structure of the geometric objects and use it to control the quality of the dimensionality reduction. Results: We demonstrate here the, to our knowledge, most efficient and accurate initialization strategy for MDS algorithms, reducing considerably computational load. SVD-based initialization renders MDS methodology much more useful in the analysis of high-dimensional data such as functional genomics datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The appropriate and faithful visualization of high-dimensional data is often a prerequisite for their analysis as the human visual cortex is still one of the most powerful tools to detect and conceptualize structure in data (<ref type="bibr" target="#b20">Holmes, 2006</ref>). Furthermore, communication of numerical and statistical results is greatly aided by the intuition arising from appropriate representations of data. Different methods * To whom correspondence should be addressed.</p><p>for the required dimensionality reduction have been developed (<ref type="bibr" target="#b5">Berthold and Hand, 2003</ref>). An entire family of approaches, such as principal component analysis (PCA) finds the minimal orthonormal basis using a mathematical tool called singular value decomposition (SVD). These methods, using different similarity or dissimilarity measures such as covariance or correlation, order the ensemble of components by their statistical deviation, and for visualization only the first two or three components are retained. Thereby, the statistical information in the first components are entirely retained, whereas one of the subsequent components is entirely lost. Today's highdimensional biological datasets can easily contain thousands of instances (number of measures) with 10 5 –10 9 variables (number of parameters measured). The repartition of information is usually homogeneous over the entire number of variables. In consequence, considering only the first components given by SVD-based techniques is not necessarily the best choice. Multidimensional scaling (MDS) is a methodology that reduces dimensionality using only the information of similarities or dissimilarities between instances, hereafter regrouped in the general term of 'distance'. The search for an optimal configuration, is reduced to finding the global minimum of a function evaluating the loss of distance information. To be sure to find an acceptable minima (i) an initial state for the optimization algorithm, and (ii) an optimization algorithm and the appropriate parameters have to be appropriately chosen. Recently,<ref type="bibr" target="#b2">Andrecut (2009)</ref>has shown that the best choice for the second is a molecular dynamics multidimensional scaling approach. We demonstrate here that the choice of the initial position is paramount to the quality of the representation and its computational efficiency. By using SVD for providing an initial configuration for MDS, we obtain a significantly increased computational efficacy. Interestingly, we also demonstrate that performing an iterative MDS or adding stochastic energy during the molecular dynamics, MDS execution do not increase performance or reproducibility of the algorithm. We also investigate the local structure of the geometric objects after dimensionality reduction with our different methodologies, and then evaluate the accuracy of the different approaches developed here on biological data. These investigations and the use of SVD to the initial state allow to better define and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Bécavin et al.</head><p>control the dimensionality reduction process for high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SVD</head><p>Given a data matrix X with n rows and p columns and x ij its value in row i and column j, we denote ¯ X i the p components vector corresponding to row i of the matrix, and ¯ X j the n components vector corresponding to column j of the matrix. A set of vector ¯ X i is then a set of instances, whereas a set of vector ¯ X j is a set of variables. In all the following, we will use this notation for vectors extracted from X. It is known (<ref type="bibr" target="#b25">Schmidt and Stewart, 1992</ref>) that every rectangular matrix can be decomposed using its singular values:</p><formula>X = USV t (1)</formula><p>where U (left singular vectors) and V (right singular vectors) are both square orthogonal matrices, and S is a rectangular matrix containing the singular values (s i ) which are positive (S ii = s i and S ij = 0). U,S and V are reorganized in order to have s 1 &gt; s 2 &gt; ··· &gt; s r , with r being the rank of S. Generally, before performing SVD X is centered, so the mean of each column is equal to zero. In this context, rank(X) = rank(S) ≤ min(n−1,p) if X is n.p. Singular value decomposition provides three major types of information:</p><p>(i) A new data matrix X new , which represent the data points in a new orthonormal basis with a minimum number of components, and where distance between the instances is preserved.</p><formula>(ii) Inertia parameters c i = s i / i s i (with i c i = 1)</formula><p>indicate the SD and relative contribution of the cloud of points on each principal component. (iii) The matrix V carrying the individual contributions to each principal component. These different types of information have already previously been used in the literature to infer biological knowledge in various settings<ref type="bibr" target="#b0">[Alter et al. (2000</ref><ref type="bibr" target="#b1">[Alter et al. ( , 2003</ref>);<ref type="bibr" target="#b13">Fellenberg et al. (2001)</ref>;<ref type="bibr" target="#b28">Wall et al. (2003)]</ref>. The simplest way to find SVD, is to search first for the eigenvalues and the eigenvectors of the inner and outer products. As finding the eigenvalues of a matrix X with n rows and p columns, is hard to perform for objects with a high number of variables, this step is only feasible if either n or p are small (typically inferior to 1000, which is usually the case in biological datasets). If both n and p are large, one is obliged to use iterative SVD techniques as shown in<ref type="bibr" target="#b25">Schmidt and Stewart (1992)</ref>. One advantage of using SVD is its close link to classical techniques of dimensionality reduction such as PCA, classical scaling (cMDS), principal component correlation analysis (PCCA) and correspondence analysis. The different results of these techniques can be obtained using SVD and a proper normalization of the data, as shown below. SVD allows to demonstrate that the inner-product (XX t ) and outer-product (X t X) of a data matrix X have the same eigenvalues</p><formula>λ i , with λ i = s 2 i. If X = USV t then: XX t = USV t (VS t U t ) = USS t U t (2)</formula><formula>X t X = (VS t U t )USV t = VS t SV t (3)</formula><p>Note also that missing values in data can be imputed using SVD [<ref type="bibr" target="#b6">Brock et al. (2008)</ref>; Candes and Recht (2008);<ref type="bibr" target="#b27">Troyanskaya et al. (2001)]</ref>. If the number of missing values is relatively low, the Eckart Young theorem (<ref type="bibr" target="#b12">Eckart and Young, 1936</ref>), which is the most commonly used theorem for matrix approximation, assures that the result of the SVD will change only in the value of the last singular values. Hence, for a rapid imputation, the row average method (<ref type="bibr" target="#b27">Troyanskaya et al., 2001</ref>) can be used which is generally sufficiently precise in most cases. Also, PCA is a very good choice for the initial state for K-means clustering (<ref type="bibr" target="#b11">Ding and He, 2004</ref>). In the new representation given by SVD, cluster structure of the data will then naturally appear, and thus provide a natural interpretation of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SVD and classical techniques of dimensionality reduction</head><p>PCA relies on the search of the eigenvectors' covariance matrix. Hence, performing PCA reduces to finding the outer-product's eigenvectors. The singular values of X are the square root of the outer-product's eigenvalues. The link between PCA and SVD then becomes obvious (<ref type="bibr" target="#b28">Wall et al., 2003</ref>). Classical scaling (cMDS for classical Multidimensional Scaling) was invented to embed a set of instances in the simplest space possible, with the constraint of preserving the Euclidean distance between data points. Euclidean distance can be written as a sum of inner-products ¯ X i. ¯ X j , one can pass from an Euclidean distance matrix to an inner product matrix by a simple matrix manipulation called double centering (<ref type="bibr" target="#b26">Torgerson, 1952</ref>). Consequently, classical scaling consists in finding eigenvalue factorization of the inner-product matrix, so it can be performed using SVD. The link given by SVD between inner and outer product matrices implies that PCA and classical scaling give the same results, a fact reflected by classical scaling sometimes being referred to principal coordinate analysis. Principal component correlation analysis (PCCA) uses correlation between variables to find a minimal orthonormal basis. After a proper normalization of the data with their SD:</p><formula>˜ X = x ij σ( ¯ X j )</formula><p>, PCCA is performed by eigenvalue factorization of the outer-product matrix. Hence, after normalization of the data PCCA results are given by SVD. Correspondence analysis is used in the dimensionality reduction of contingency tables obtained after an operation of counting on categorical data (<ref type="bibr" target="#b5">Berthold and Hand, 2003</ref>). This method can be used for microarray data analyses (<ref type="bibr" target="#b13">Fellenberg et al., 2001</ref>) as each value of gene expression is, in fact, a count of the number of RNAs produced. Generally speaking, this technique is used to compare two vectors in terms of their distribution profiles using the chi-square distance. When the distance is equal to zero, both vectors have the same statistical distribution. It can be shown (<ref type="bibr" target="#b9">Cuadras and Fortiana, 1995</ref>) that χ 2 distance can be reduced to an Euclidean distance after normalization of the datã</p><formula>X ik = x ik √ W ( √ l x lk )( l x il )</formula><p>Thus, to find the minimal space which embeds the data and conserves the information of χ 2 distance one performs a cMDS or PCA on the rescaled data matrix using SVD results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">MDS</head><p>MDS is a class of techniques to represent instances in an r dimensional space given an initial state and a similarity or dissimilarity matrix (<ref type="bibr" target="#b8">Cox and Cox, 2000</ref>;<ref type="bibr" target="#b22">Kruskal and Wish, 1978</ref>). Recently, molecular dynamics (MD) approaches have been used to perform MDS for high-dimensional objects drastically increasing quality of the dimensionality reduction (<ref type="bibr" target="#b2">Andrecut, 2009</ref>). We have also developed a similar approach based on a spring analogy. Data points are connected to all other instances with virtual springs. The springs will tend to return to their equilibrium length during molecular dynamics simulation. The equilibrium length for the spring between point i and point j will be defined as the Euclidean distance d(</p><formula>¯ X i , ¯ X j )</formula><p>in the initial state. For each instance ¯ X i , a force is defined F( ¯ X i ), which is the sum of all spring interactions F spr</p><formula>( ¯ X i , ¯ X j )</formula><p>with the other instances ¯ X j , minus a friction term to avoid oscillation of the spring network:</p><formula>F spr ( ¯ X i , ¯ X j ) =−k ij (δ( ¯ X i , ¯ X j )−d( ¯ X i , ¯ X j ))( ¯ X j − ¯ X i ) (4) F( ¯ X i ) = j =i F spr ( ¯ X i , ¯ X j )−γm i ˙ ¯ X i (5) with δ( ¯ X i , ¯ X j )</formula><p>being the distance between instances in the r dimensional space, k ij the strength of spring ij, γ the friction parameter and m i the mass given to each point. We consider that every spring and all instances are equal in strength and weight so k ij and m i are the same for every i and j (k ij = k and m i = m). It is, however, possible to use different parameters—for instance, according to experimental precision—if different weights shall be considered for the different instances. A molecular simulation using the force vector is then executed. Following Newton's law it follows:and velocity of and instance at the next time step, a Verlet integration is used:</p><formula>m i ¨ ¯ X i = F( ¯ X i ),</formula><formula>¯ X i (t +t) = 2 ¯ X i (t)− ¯ X i (t −t)+At 2 (6) ˙ ¯ X i (t) = ¯ X i (t +t)− ¯ X i (t −t) 2t (7)</formula><p>with ˙ ¯ X i (t) the temporal derivation of vector ¯ X i (t). The algorithm is run with simulation time t increasing. To avoid divergence of the Verlet algorithm parameters of the simulation k, m, γ γt have to be well chosen. Here we used: k = 1, m = 5, γ = 0.1 t = 0.02 (cf.<ref type="figure" target="#fig_1">Fig. 2A</ref>). For the initial state, the data provided to the MDS algorithm were rescaled to fit in a hypercube with a diameter of 6 by multiplying the initial state matrix by a scalar α. To control the minimization process at each time step, a cost function termed the Kruskal stress is calculated according to Cox and Cox (2000):</p><formula>e = i j (δ(i,j)−d(i,j)) 2 i j d(i,j) 2 (8)</formula><p>this global parameter is a direct evaluation of the amount of energy in the system and hence the loss of distance information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Datasets used in this study</head><p>To test and illustrate the algorithm discussed here, we have used several publicly available datasets of different origin. We have used two different transcriptome datasets. Briefly, the cellular transcriptome is defined as the ensemble of RNA molecules resulting from gene expression in a cell. Using microarray technology, in the human case, some 30 000 different RNA species can be quantified simultaneously. The dataset here referred to 'd1—96Cell' includes 96 transcriptome measurements generated from 32 individual human tissues under non-pathological conditions. This dataset was initially published by<ref type="bibr" target="#b10">Dezso et al. (2008)</ref>, and is available for download from: http://mace.ihes.fr using accession number: 2914508814. The dataset here called 'd6—CCYier'<ref type="bibr" target="#b21">[(Iyer et al., 1999</ref>); mace access. no.: 2960354318] is composed of 12 human fibroblast transcriptome data points generated over 24 h during the cell cycle. Note that we eliminated 1 (Interleukin 8, IL8) of the 517 genes as an outlier from this dataset. The dataset 'd2—96Cell_T' (cf.<ref type="figure" target="#tab_1">Table 1</ref>) is a derivative of the initial dataset d1—96Cell', where only genes were retained that are specific to one and only one human tissue as provided in (<ref type="bibr" target="#b10">Dezso et al., 2008</ref>), and removing again one outlier gene (Probe_ID: 162105). The dataset 'd8—96Cell_T' (cf.<ref type="figure" target="#tab_1">Table 1</ref>) is the transposed (Instances, Variables) dataset 'd2—96Cell_T'. All transcriptome datasets were median normalized in log2space and processed according to standard procedures (<ref type="bibr" target="#b4">Benecke, 2008;</ref><ref type="bibr" target="#b23">Noth et al., 2006</ref>). Seven additional datasets with no relation to biology were used. Both originate from the Machine Learning Repository (<ref type="bibr" target="#b14">Frank and Asuncion, 2010</ref>): http://archive.ics.uci.edu/ml (i) 'Iris' here 'd3—Iris',</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison of different initialization methods for MDS</head><p>We postulated that the inconveniences associated with the combined molecular dynamics MDS techniques (hereafter simply: MDS) related to the dependence on the choice of the initial condition for the simulation leading to insufficient control and being trapped in local minima on the one hand, as well as the large information loss when SVD techniques are used for dimensionality reduction on the other hand, can be overcome when both methods are combined. We therefore created an SVD–MDS algorithm which uses SVD to compute the initial state of a molecular dynamics simulated MDS. This SVD–MDS approach was then compared to SVD and MDS on 13 different datasets (<ref type="figure" target="#tab_1">Table 1</ref>).<ref type="figure" target="#fig_0">Figure 1</ref>well illustrates the shortcomings of SVD and MDS alone and how SVD–MDS overcomes those. The dataset 'd1—96Cell' containing 96 different instances was used to compute a 2D representation using SVD (<ref type="figure" target="#fig_0">Fig. 1A</ref>), our combined SVD–MDS approach (<ref type="figure" target="#fig_0">Fig. 1B</ref>) and two examples of MDS initialized by random positions defining a 12 unit hypercube (<ref type="figure" target="#fig_0">Fig. 1C</ref>and D). According to the Kruskal stress e, MDS techniques (<ref type="figure" target="#fig_0">Fig. 1B</ref>–D) better preserve the distances between the instances and their relationship. The data cloud is better resolved (see also blow ups) and the global distance information loss is lower than for SVD. In order to demonstrate generality of our approach, we next analyzed the 12 remaining datasets (<ref type="figure" target="#tab_1">Table 1</ref>) using four different approaches: (i) SVD only, (ii) SVD–MDS, (iii) MDS initialized with all data points placed at zero with minimal random noise (zeroMDS), and (iv) MDS initialized with random positions (stochastMDS). The results are reported in<ref type="figure" target="#tab_2">Table 2</ref>. In all cases, we reduced the dimensions to two. It becomes again apparent from the Kruskal stress that the MDS-based techniques systematically outperform the SVD. While stochastMDS, zeroMDS and SVD–MDS give similar results in terms of the final information loss, the number of timesteps needed to identify a minimum stress is greatly reduced using SVD–MDS (<ref type="figure" target="#tab_2">Table 2</ref>and for four examples<ref type="figure" target="#fig_1">Fig. 2</ref>). Therefore, SVD–MDS approaches the final state (here defined as a Kruskal stress value) faster than either of the MDS methods. We show an example of stress evolution in<ref type="figure" target="#fig_2">Figure 3A</ref>where stochastMDS and zeroMDS are slow due to the existence of local minima, and SVD–MDS clearly outperform them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Iterative dimensionality reduction using iSVD–MDS</head><p>We next wondered whether the dimensionality reduction could be further improved by a step-wise reduction of one dimension after another. To this end, we compared again the performance of the three</p><p>Page: 1416 1413–1421</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Bécavin et al.</head><p>(a)<ref type="figure" target="#tab_1">Table 1</ref>and Section 2.4) was represented in 2D space using: (a) SVD based on covariance, (b) SVD-initialized multidimensional scaling; (c) random initialized multidimensional scaling, and (d) as in (c) using the same algorithm and leading to a different random position matrix. The peripheral data points were color coded and labeled according to the human tissue analyzed. For (a) and (b) the central cloud of points has been zoomed into at the same scaling factor. The resulting Kruskal Stress e for each of the dimensionality reductions is indicated. Similar computations were used to generate<ref type="figure" target="#fig_2">Figure 3A</ref>, SVD–MDS rapidly approaches a minimal Kruskal stress configuration over the simulation time. The previously described MDS procedure which uses stochastic initiation for the molecular dynamics simulation requires much more simulation time to find the same minimal stress configuration as the SVD–MDS algorithm. Finally, the iterative iSVD–MDS approach will also converge to the identical minimum obtained by the other methods; however, as for each component a separate simulation is performed the convergence time is greatly increased. Albeit many different simulations on the different datasets, we have never obtained a final configuration using iSVD–MDS where the Kruskal stress would allow to conclude on an improved performance when compared to SVD–MDS. Therefore, the iterative method does not allow for improved accuracy, but rather prolongs simulation time with no immediate gain (<ref type="figure" target="#tab_3">Table 3</ref>summarizes the results). We next compared iSVD and iSVD–MDS methods to determine how the loss of information is distributed during iterative dimensionality reduction. As can be seen in<ref type="figure" target="#fig_2">Figure 3B</ref>for both procedures, the amount of stress or lost information increases both relatively and absolutely with the number of components removed. Note also, that the iSVD–MDS method better preserves at every consecutive iteration the distance information of the object (<ref type="figure" target="#fig_2">Fig. 3B</ref>).</p><formula>( c) (b) ( d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Molecular dynamics dimensionality reduction with added stochasticity</head><p>In Andrecut (2009), an approach reminiscent of simulated annealing was used to avoid getting trapped in local minima during the molecular dynamics simulation. This combination of methods is equivalent to adding a stochastic force to all data points F stochastic ( ¯ X i ) =−T * s(t) where s(t) is a random number given by a generalized Gaussian stochastic distribution, and T is the temperature of the system. By decreasing T exponentially during the simulation, one expects to reach the global minimum. Adding stochasticity to the molecular dynamics-driven MDS is, after<ref type="bibr" target="#b2">Andrecut (2009)</ref>, required to insure reproducibility of the algorithmic performance.<ref type="figure" target="#tab_2">Table 2</ref>. Results from the different MDS algorithms applied to the various datasets (c.f.To compare MD–MDS with our SVD–MDS algorithm, we have implemented different MD–MDS algorithms with stochastic energy. We used two types of temperature decrease, the first linear, beginning with a temperature of 100 J and decreasing linearly to 0 J during 3000 steps of simulation; we call this method MD–MDS linear. The second includes an exponential decrease from 100 J to below 0.1 J during 3000 steps of simulation; we call this method MD– MDS exponential. The function s(t) uses random numbers generated uniformly between −0.5 and 0.5. As seen in<ref type="figure" target="#tab_3">Table 3</ref>, SVD–MDS as well as the two MD–MDS algorithms 'linear'and 'exponential'always identify final configurations with the same amount of residual energy. It can also be seen that SVD–MDS converges faster for these four examples than the MD–MDS methods. In conclusion, the two MD–MDS Page: 1418 1413–1421algorithms do not improve MDS, on the contrary they converge slower. We next asked whether or not similarly adding stochasticity to the SVD–MDS algorithm would improve its performance.<ref type="figure" target="#fig_2">Figure 3C</ref>and D illustrates that indeed adding different amounts of energy at different times of the simulation (arrows) does not lead to lower energy minima. The SVD–MDS algorithm, similarly as the MD–MDS algorithms (<ref type="figure" target="#tab_3">Table 3</ref>), always converges to the same energy state. This has also been confirmed using other datasets (data not shown). Taken together, the results using MD–MDS-lin and MD–MDS-exp and SVD–MDS strongly suggest that only a single ground state is present. While we do not have any formal proof, we believe that the detailed analysis of the geometric structure of the data objects presented below also strongly argues in favor of a global energy minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 1417 1413–1421</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multidimensional scaling</head><formula>(a) ( b) (c) ( d) (e) ( f)</formula><formula>(a) ( b) (c) ( d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Bécavin et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Geometric structure</head><p>Kruskal stress directly evaluates the distance information deformation.<ref type="bibr">Graef et al. demonstrated in 1979 (Graef and</ref><ref type="bibr" target="#b16">Spence, 1979</ref>), that it rather evaluates global deformation of the cloud of instances. To gain information on local distances deformation, we define a new parameter, Entourage. For any one instance ¯ X i in the reference distribution obtained through SVD (undistorted representation), we consider its k nearest neighbors: N ref i. In the new distribution obtained after dimensionality reduction, we also compute the k nearest neighbors for the same instance ¯ X i , and obtain a list:</p><formula>N new i . We then search for G i = card(N ref i N new i</formula><p>), which will be the number of instances common to both. This operation is repeated for all instances i, and one obtains the Entourage parameter:</p><formula>Ent k = n i=1 G i G (9)</formula><p>with G = nk a normalization parameter (Ent ∈ (0,1)).</p><formula>If G i = card(N ref i N new i ) ≈ 0.01card(N ref i ) = 0.01k for every i then Ent k ≈ 0.01 n i=1 k nk</formula><p>= 0.01, a difference of 1% between two values of Entourage corresponds to an average deformation of 1% in the local organization. This parameter has more signification for a small number of neighbors k compared to the total number of points n. The geometric properties of the data objects are analyzed using the Entourage parameter. We have plotted the relationship of Entourage and k for six different methodologies: zeroMDS, stochastMDS, SVD–MDS, iSVDMDS, MD–MDS-lin, MD–MDS-exp in<ref type="figure" target="#fig_3">Figure 4</ref>for eight different datasets. From the selected examples, it becomes clear that again the SVD–MDS method outperforms the different types of MDS over a wide array of structures analyzed as the Entourage value is consistently higher no matter how many different k nearest neighbors are considered. The iterative iSVD–MDS method, due to the accumulation of small residual errors during the molecular dynamics simulation, and the MDS method give similar results. At the cost of increasing computational load, the iSVD– MDS better and better approximates the SVD–MDS method. In conclusion, the SVD–MDS method, under all conditions tested, better represents the geometric structure of the datasets in lowdimensional space when compared to the input object with rank(S) components (given by SVD). Note that this holds even for objects with equal stress.<ref type="figure" target="#fig_0">Figure 1</ref>illustrates the problem of rotational variance when using stochastically initiated molecular dynamics simulations for MDS. When comparing<ref type="figure" target="#fig_0">Figure 1C</ref>and D as well as comparing them to<ref type="figure" target="#fig_0">Figure 1A</ref>and B that stochastMDS results produces near-optimal solutions (with respect to the Kruskal stress), the resulting orientation of the instances, however, is different (focus, for instance, on the relationship between 'skeletal muscle' and 'fetal liver'). SVD–MDS on the contrary only produces a single result. This observation, taken together with the results on the relevance of stochasticity in the simulation obtained above, argues for the existence of different equivalent energy minima that only differ in the rotational orientation of the object and (at best) only minimally in the Kruskal-stress; a fact predicted by mathematical consideration. Hence, SVD–MDS not only reduces significantly the computational load, but also insures uniqueness of the resulting representation. The quality of this final and unique representation can be demonstrated using the Entourage parameter. The increase in fidelity in the representation of data should not be underestimated (see also<ref type="figure" target="#fig_5">Fig. 5</ref>).This is reminiscent to techniques of principal manifold searches (<ref type="bibr" target="#b15">Gorban et al., 2008</ref>) where parameters describing topology, local organization or other geometric characteristics are used. A major advantage of using SVD to define the initial state is that it provides the inertia of each principal component. The comparison of the different internal structures of the studied datasets showed a vast variety of profiles. A good dimensionality reduction technique would ideally account for these differences. Taking into account the inertia, the stress and the Entourage during the MDS process will help to have an even more accurate representation of the data matrix in low-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 1419 1413–1421</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multidimensional scaling</head><formula>(a) ( b) ( c) ( d) (e) ( f) ( g) ( h)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Data analysis</head><p>In order to demonstrate the applicability of the SVD–MDS methodology and its superior performance, we reanalyzed a previously published biological dataset not yet used here (<ref type="bibr" target="#b24">Prakash et al., 2006</ref>). The datasets consists of quantitative measures for 10 selected cytokines in a cohort of human malaria patients from central India displaying different severeness of disease as well as endemic and non-endemic control subjects. A total of 98 patients were included in the original study by<ref type="bibr" target="#b24">Prakash et al. (2006)</ref>. The main objective is to determine whether individual or combinations of cytokine measurements can be used to determine whether an individual is affected by cerebral malaria (CM), the most severe form of the disease, and how to distinguish CM from severe malaria (SM). Both forms of the disease require early detection and prognosis which are pressing matters for health caretakers. We have computed from the entire dataset [including the controls and patients with mild malaria (MM)] SVD-based and SVD–MDSbased representations of the cytokine activity measurements in covariance space (<ref type="figure" target="#fig_5">Fig. 5A</ref>and B). It becomes immediate evident that whereas the representation by SVD–MDS identifies TNFα as having a major contribution to one of the higher principal components, SVD alone does not reveal this prominent role for TNFα leading to the conclusion that the main variability in the patient samples is due to IL2, IL6 and TGFβ [<ref type="figure" target="#fig_5">Fig. 5C</ref>. The combination of IL2 and TNFα measurements alone suffices, however, to separate SM (red) from CM (blue) patients in single linkage hierarchical clustering based on Euclidean distances (<ref type="figure" target="#fig_5">Fig. 5E</ref>). The combination of IL2 and TNFα would unlikely have been identified as effective by SVD alone (<ref type="figure" target="#fig_5">Fig. 5A</ref>). The role of TNFα in CM has been also clarified when investigating the auto-immune component of CM in<ref type="bibr" target="#b3">Bansal et al. (2009).</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>Dimensionality reduction of complex, high-dimensional data is an important problem which becomes ever more complicated due to the increase of data concomitant with an increase in their dimensionality. This is particularly true for data from modern genomics analyses where more and more data with thousands of instances each over millions of variables are generated. We demonstrate here how a combined molecular dynamics simulation multidimensional scaling approach for dimensionality reduction of high-dimensional data can be improved by better defining the initial conditions. We have shown that singular value decomposition is most effective to create an initial condition for MDS. Using links between SVD and different standard data analysis methods, we demonstrate how our combined SVD– MDS method can be used to improve geometric representation in low-dimensional space that are generally obtained with standard analysis methods (PCA, classical scaling, PCCA, correspondence analysis). We also show that the use of iterative reduction or Page: 1420 1413–1421stochastic energy does not increase performance of the algorithms in terms of finding a optimal solution. Finally, we have investigated the local structure deformation induced by dimensionality reduction, and confirmed the superior accuracy of the SVD–MDS. Overall, the methodology developed here should further advance our capacity to analyze high-dimensional data such as the ones produced by functional genomics approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Bécavin et al.</head><formula>(a) (e) (b) ( c)</formula><formula>(d)</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Comparison of the results of different dimensionality reduction techniques on the same dataset. The dataset 'd1—96Cell', composed of 96 individual transcriptome profiles generated from 32 different human tissues (cf. Table 1 and Section 2.4) was represented in 2D space using: (a) SVD based on covariance, (b) SVD-initialized multidimensional scaling; (c) random initialized multidimensional scaling, and (d) as in (c) using the same algorithm and leading to a different random position matrix. The peripheral data points were color coded and labeled according to the human tissue analyzed. For (a) and (b) the central cloud of points has been zoomed into at the same scaling factor. The resulting Kruskal Stress e for each of the dimensionality reductions is indicated. Similar computations were used to generate Table 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Parameter optimization and Kruskal stress (e) evolution over the number of simulation iterations (t). Optimizing the choice of parameters k (a) and γ (b) using the dataset 'd6—CCYier' in covariance space. Comparison of the SVD–MDS, MDS initialized with all points in the center (zeroMDS), and MDS initialized by stochastic positions (stochastMDS) methods on different datasets (c) 'd1—96Cell' in correlation basis, (d) 'd2—96Cell_T' in covariance basis, (e) 'd3—Iris' in correlation basis, (f) 'd10—Ozone' in covariance basis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. Iterative SVD–MDS and robustness of SVD–MDS. (a) Comparison of the SVD–MDS, zeroMDS, stochastMDS and iterative SVD–MDS (iSVD–MDS) methods on dataset 'd13—Wave' in covariance basis. (b) Comparison of the iterative SVD (iSVD) and iSVD–MDS methods on dataset 'd1—96Cell' in correlation basis. Evolution of stress over number of simulation iterations with injection of energy, on different datasets (c) 'd2—96Cell_T' in covariance basis and (d) 'd5—Iris' in covariance basis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.4.</head><figDesc>Fig. 4. Relative changes in k nearest neighbors (Entourage) are local, structural measures of dimensionality reduction and thus assess quality of the procedure. As a function of the number of nearest neighbors k considered, the relative change in kNN between the initial high-dimensional space and 2D space is plotted for the methods: SVD–MDS, zeroMDS, stochastMDS, iSVD–MDS, MD–MDS linear and MD–MDS exponential. The datasets used are in: (a) 'd1—96Cell' in correlation basis, (b) 'd2—96Cell_T' in covariance basis, (c) 'd4—Wine' in covariance basis, (d) 'd5—Stochast 200' in covariance basis, (e) 'd6—CCYier' in covariance basis, (f) 'd7—Pima' in covariance basis, (g) 'd11—Stochast 3000' in covariance basis and (h) 'd13—Wave' in covariance basis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>as opposed to 5D (SVD–MDS)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.5.</head><figDesc>Fig. 5. Comparative analysis of cytokine activity measurements in an Indian malaria human patient cohort. The cytokine dataset from Prakash et al. (2006) was represented in covariance space using SVD (a) and SVD–MDS (b). A simplified representation for SVD and SVD–MDS is shown in (c) and (d), respectively. (e) Single linkage hierarchical clustering based on Euclidean distance of the severe malaria (SM, red) and cerebral malaria (CM, blue) patients according to IL2 and TNFα activity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><figDesc>Funding: Centre National de la Recherche Scientifique (CNRS); the Agence Nationale pour la Recherche contre le SIDA et les hépatites virales (ANRS); the Agence Nationale pour la Recherche (ANR, ISPA project); the Genopole Evry. C.B. is recipient of a PhD. fellowship from the ANRS. Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>with¨¯with¨with¨¯ X i the double temporal derivation of vector ¯ X i (t). In order to find the new position</figDesc><table>Page: 1415 1413–1421 

Multidimensional scaling 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>The different datasets used in this study 

ID 
Dataset name 
No. of instances 
No. of variables 

d1 
96Cell 
96 
32878 
d2 
96Cell_T 
96 
1553 
d3 
Iris 
150 
4 
d4 
Wine 
178 
13 
d5 
Stochast 200 
200 
50 
d6 
CCYier 
516 
12 
d7 
Pima 
768 
9 
d8 
96Cell_T transposed 
1553 
96 
d9 
Secom 
1567 
590 
d10 
Ozone 
2565 
72 
d11 
Stochast 3000 
3000 
300 
d12 
Ecoli 
4288 
7 
d13 
Wave 
5000 
22 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 2.</figDesc><table>techniques SVD–MDS, MDS and iterative SVD–MDS (iSVD– 
MDS) on the different datasets. In iSVD–MDS, for each successive 
round a SVD followed by a subsequent molecular dynamics 
MDS is performed. As can be seen in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 1)</figDesc><table>ID 
Dataset name 
Metric 
SVD 
SVD–MDS 
zeroMDS 
stochastMDS 

e 
e 
t 
e 
t 
e 
t 

d1 
96Cell 
R 2 
0.6472 
0.3409 
2500 
0.352 
2500 
0.3478 
2500 
d2 
96Cell_T 
Cov 
0.5001 
0.1401 
4500 
0.146 
4500 
0.1503 
4500 
d3 
Iris 
Cov 
0.0421 
0.0344 
509 
0.0343 
3554 
0.0344 
4059 
d4 
Wine 
Cov 
0.0010 
0.0010 
0 
0.0064 
4500 
0.0061 
4500 
d5 
Stochast 200 
Cov 
0.7513 
0.4088 
1500 
0.4169 
1500 
0.4157 
1500 
d6 
CCYier 
Cov 
0.1634 
0.0765 
400 
0.0932 
3500 
0.1079 
4500 
d7 
Pima 
Cov 
0.0964 
0.0708 
700 
0.105 
3500 
0.1098 
3500 
d8 
96Cell_T transposed 
R 2 
0.6954 
0.1498 
4500 
0.1572 
4500 
0.1715 
4500 
d9 
Secom 
Cov 
0.1801 
0.1168 
750 
0.1217 
4499 
0.1283 
4375 
d10 
Ozone 
Cov 
0.1223 
0.0935 
712 
0.0935 
2587 
0.0951 
2143 
d11 
Stochast 3000 
Cov 
0.9067 
0.4353 
130 
0.4382 
130 
0.438 
130 
d12 
Ecoli 
Cov 
0.1634 
0.000 
0 
0.0202 
4500 
0.2484 
4500 
d13 
Wave 
Cov 
0.2922 
0.2132 
324 
0.2132 
2252 
0.2132 
1998 

CoV, covariance; R </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 3. Results from SVD–MDS, iSVD–MDS and both MD–MDS algorithms applied to the various datasets (cf. Table 1) 2 , correlation; e, Kruskal stress; t, time steps for MD simulation.</figDesc><table>ID 
Dataset Name 
Metric 
SVD–MDS 
iMDS 
MDMDSlinear 
MDMDSexpo 

e 
t 
e 
t 
e 
t 
e 
t 

d1 
96Cell 
R 2 
0.3409 
2500 
0.3381 
232 097 
0.3453 
5500 
0.3421 
2500 
d2 
96Cell_T 
Cov 
0.1401 
4500 
0.1494 
92 536 
0.1465 
4500 
0.1542 
4500 
d3 
Iris 
Cov 
0.0344 
509 
0.0344 
3008 
0.0359 
4500 
0.0343 
4000 
d4 
Wine 
Cov 
0.0010 
0 
9.0E-4 
10 003 
0.0089 
4500 
0.0067 
4500 
d5 
Stochast 200 
Cov 
0.4088 
1500 
– 
−− 
0.4092 
4500 
0.4089 
4500 
d6 
CCYier 
Cov 
0.0765 
400 
0.0753 
22 508 
0.1346 
5500 
0.1162 
5500 
d7 
Pima 
Cov 
0.0708 
700 
0.0692 
27 005 
0.1128 
5500 
0.0986 
5500 
d8 
96Cell_T transposed 
R 2 
0.1498 
4500 
0.1525 
122059 
0.1832 
4224 
0.1822 
4500 
d9 
Secom 
Cov 
0.1168 
750 
– 
−− 
0.1511 
5500 
0.1396 
4500 
d10 
Ozone 
Cov 
0.0935 
712 
0.0935 
66 031 
0.0944 
4500 
0.0951 
3500 
d11 
Stochast 3000 
Cov 
0.4353 
130 
– 
−− 
0.4353 
200 
0.4353 
200 
d12 
Ecoli 
Cov 
0.0 
0 
– 
−− 
0.312 
5500 
0.2273 
5500 
d13 
Wave 
Cov 
0.2132 
324 
– 
−− 
0.2132 
3671 
0.2132 
2203 

CoV, covariance; R </table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Singular value decomposition for genome-wide expression data processing and modeling</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Alter</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="10101" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalized singular value decomposition for comparative analysis of genome-scale expression data sets of two different organisms</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Alter</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3351" to="3356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Molecular dynamics multidimensional scaling</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Andrecut</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. A</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Igg autoantibody to brain beta tubulin iii associated with cytokine cluster-ii discriminate cerebral malaria in central india</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Bansal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">8245</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Gene regulatory network inference using out of equilibrium statistical mechanics</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Benecke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HFSP J</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="188" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<monogr>
		<title level="m" type="main">Intelligent Data Analysis</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Berthold</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Hand</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg/Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Which missing value imputation method to use in expression profiles: a comparative study and two selection schemes</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Brock</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<monogr>
		<title level="m" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Candes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Recht</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="805" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Cox</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Cox</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Chapman &amp; Hall/CRC Press</publisher>
			<pubPlace>Boca Raton, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">Metric scaling graphical representation of categorical data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">M</forename>
				<surname>Cuadras</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fortiana</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">A comprehensive functional analysis of tissue specificity of human gene expression</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Dezso</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">K-means clustering via principal component analysis</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ding</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>He</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21 st International Conference on Machine Learning</title>
		<meeting>the 21 st International Conference on Machine Learning<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Eckart</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Young</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Correspondence analysis applied to microarray data</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Fellenberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">10781</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<monogr>
		<title level="m" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Frank</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Asuncion</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<monogr>
		<title level="m" type="main">Principal Manifolds for Data Visualization and Dimension Reduction</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Gorban</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Publishing Company</publisher>
			<pubPlace>Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Using distance information in the design of large multidimensional scaling experiments</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Graef</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Spence</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Bull</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="40" to="58" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1421" to="1413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Multidimensional scaling</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualising data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Holmes</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Statistical Problems in Particle Physics, Astrophysics and Cosmology</title>
		<editor>Lyons,L. and Unel,M.K.</editor>
		<meeting>Statistical Problems in Particle Physics, Astrophysics and Cosmology</meeting>
		<imprint>
			<publisher>Imperial College Press World Scientific Publishing Co</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">197</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">The transcriptional program in the response of human fibroblasts to serum</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Iyer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="page" from="83" to="87" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kruskal</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Wish</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>SAGE Publications Inc</publisher>
			<pubPlace>Thousand Oaks, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">High-sensitivity transcriptome data structure and implications for analysis and biologic interpretation</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Noth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genomics Proteomics Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="212" to="229" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Clusters of cytokines determine malaria severity in plasmodium falciparum-infected patients from endemic areas of central india</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Prakash</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Infect. Dis</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="198" to="207" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<monogr>
		<title level="m" type="main">On the early history of the singular value decomposition</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Schmidt</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Stewart</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Multidimensional scaling: I. theory and method</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Torgerson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="401" to="419" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Missing value estimation methods for DNA microarrays</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Troyanskaya</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="520" to="525" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<monogr>
		<title level="m" type="main">Singular value decomposition and principal component analysis. In A Practical Approach to Microarray Data Analysis</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Wall</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="91" to="109" />
			<pubPlace>Berlin/Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>