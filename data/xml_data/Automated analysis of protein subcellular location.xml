
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Systems biology Automated analysis of protein subcellular location in time series images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Yanhua</forename>
								<surname>Hu</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biological Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Elvira</forename>
								<surname>Osuna-Highley</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Juchang</forename>
								<surname>Hua</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biological Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Machine Learning</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Theodore</forename>
								<forename type="middle">Scott</forename>
								<surname>Nowicki</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biological Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Robert</forename>
								<surname>Stolz</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Division of Science and Mathematics</orgName>
								<orgName type="institution">University of the Virgin Islands</orgName>
								<address>
									<postCode>00803</postCode>
									<settlement>St Thomas</settlement>
									<region>VI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Camille</forename>
								<surname>Mckayle</surname>
							</persName>
							<affiliation key="aff4">
								<orgName type="department">Division of Science and Mathematics</orgName>
								<orgName type="institution">University of the Virgin Islands</orgName>
								<address>
									<postCode>00803</postCode>
									<settlement>St Thomas</settlement>
									<region>VI</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Robert</forename>
								<forename type="middle">F</forename>
								<surname>Murphy</surname>
							</persName>
							<email>murphy@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biological Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Machine Learning</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Lane Center for Computational Biology</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Systems biology Automated analysis of protein subcellular location in time series images</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="issue">13</biblScope>
							<biblScope unit="page" from="1630" to="1636"/>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq239</idno>
					<note type="submission">Received on November 20, 2009; revised on April 3, 2010; accepted on April 27, 2010</note>
					<note>[17:24 3/6/2010 Bioinformatics-btq239.tex] Page: 1630 1630–1636 Associate Editor: Burkhard Rost Contact:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Image analysis, machine learning and statistical modeling have become well established for the automatic recognition and comparison of the subcellular locations of proteins in microscope images. By using a comprehensive set of features describing static images, major subcellular patterns can be distinguished with near perfect accuracy. We now extend this work to time series images, which contain both spatial and temporal information. The goal is to use temporal features to improve recognition of protein patterns that are not fully distinguishable by their static features alone. Results: We have adopted and designed five sets of features for capturing temporal behavior in 2D time series images, based on object tracking, temporal texture, normal flow, Fourier transforms and autoregression. Classification accuracy on an image collection for 12 fluorescently tagged proteins was increased when temporal features were used in addition to static features. Temporal texture, normal flow and Fourier transform features were most effective at increasing classification accuracy. We therefore extended these three feature sets to 3D time series images, but observed no significant improvement over results for 2D images. The methods for 2D and 3D temporal pattern analysis do not require segmentation of images into single cell regions, and are suitable for automated high-throughput microscopy applications. Availability: Images, source code and results will be available upon publication at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Subcellular distribution is an important characteristic of a protein because location is intimately related to its function. The most common method used to find a protein's subcellular location is to fluorescently tag the protein, take images by microscopy and then visually analyze the images. Previous work has shown that automatic image analysis can outperform visual examination, providing higher * To whom correspondence should be addressed. sensitivity to subtle differences (<ref type="bibr" target="#b17">Murphy et al., 2003</ref>). Proteins from major subcellular locations can be differentiated from each other with over 90% accuracy (). However, previous work has been done using static images and the challenge now is to extend these approaches to time series images that better reflect protein behavior in living cells. One objective is to distinguish proteins that have similar static but different temporal patterns. This is of obvious biological importance since many proteins change their location over time in order to carry out their functions. For example, helicases localize to the nucleus during the G 1 phase of the cell cycle to repair endogenous DNA damage, and exit the nucleus during S phase (<ref type="bibr" target="#b6">Gu, 2004</ref>). For automated analysis of protein subcellular locations, machine learning tools for classification and clustering are well established (<ref type="bibr" target="#b5">Glory and Murphy, 2007</ref>). A key component is feature design: good features are numerical representations of the patterns that capture the differences between classes or clusters. The goal for temporal pattern analysis is therefore to design good features from time series images that distinguish different protein movements. Automated analysis of biological time series images has received significant attention in recent years, but most movies are of low resolution at the organ or cell level (<ref type="bibr" target="#b15">Liebling et al., 2006;</ref><ref type="bibr" target="#b22">Souvenir et al., 2008</ref>), and only a few deal with high resolution microscopy images that record protein movement within a single cell (<ref type="bibr" target="#b3">Danuser and Waterman-Storer, 2006;</ref><ref type="bibr" target="#b21">Sigal et al., 2006;</ref><ref type="bibr" target="#b24">Zhou et al., 2009</ref>). We have carefully studied applicable theories and algorithms from the computer vision field and adopted/designed five sets of temporal features for application to subcellular pattern analysis for microscopy images. The first, and perhaps most straightforward, approach to characterizing temporal behavior is to track individual objects in a cell and calculate features to describe these tracks. For example, speed, the most common numerical description of movement, requires measuring the total distance the object travels along its way. However, some proteins do not form rigid objects and it is therefore unclear what the targets for tracking should be. The best example is cytoplasmic proteins: even when they show changing fluorescence distributions, there is typically no easily characterized movement of objects. Such 'complex and nonrigid' motion that has statistical regularity was defined as 'temporal texture' (<ref type="bibr" target="#b18">Nelson and Polana, 1992</ref>). In that work, the normal flow on each pixel was used<ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automated analysis of protein subcellular dynamics</head><p>to represent movement along the image gradient and compared to uniform direction flow. The concept of 'temporal texture' has also been applied to adapt Haralick texture features for static images to movies (<ref type="bibr" target="#b0">Bouthemy and Fablet, 1998</ref>) or to develop a series of features based on temporal slices (<ref type="bibr" target="#b19">Ngo et al., 2002</ref>). We therefore chose co-occurrence based temporal texture features (<ref type="bibr" target="#b8">Hu et al., 2006</ref>) and normal flow features as our second and third temporal feature sets. To design other temporal features without defining an entity for tracking, we have also explored image intensity changes in the frequency domain (Fourier transform features) and analyzed static feature changes over time (autoregression features). We evaluated the ability of these feature types to improve our ability to discriminate protein patterns in a collection of 4D images (three spatial dimensions over time) for 12 cell lines expressing fluorescently tagged proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image acquisition</head><p>NIH 3T3 cell lines previously generated as part of a proteome-scale tagging project were used in this study (<ref type="bibr" target="#b2">Chen et al., 2003;</ref><ref type="bibr" target="#b4">Garcia Osuna et al., 2007;</ref><ref type="bibr" target="#b13">Jarvik et al., 2002</ref>). In each cell line, a particular protein is tagged by CD tagging (<ref type="bibr" target="#b12">Jarvik et al., 1996</ref>) which introduces a green fluorescent protein (GFP) tag into the gene coding for the protein using self-inactivating retroviral vectors. Cells were plated on glass-bottomed culture dishes in Dulbecco's modified Eagle's media. After 48 h, the media were changed to Opti-MEM to avoid the interference by phenol red and provide pH stability. Three-dimensional movies of GFP tagged proteins in 3T3 cells were taken by a spinning disk confocal microscope. The imaging system consists of a LaserPhysics Reliant 100 s 488 Argon laser, a Yokogawa CSU10 Confocal Scanner Unit and an Olympus IX50 microscope with a 60× 1.4NA objective. Images (1280 × 1024) were collected with a Roper Scientific/Photometrics CoolSnap HQ Cooled CCD camera, with a final resolution of 0.11 microns per pixel in the sample plane. A stack of 15 images was collected using a 3 s exposure time and a spacing of 0.5 µ between slices. Stacks were taken every 45 s, with the total period of time for each movie varying depending on the extent of fluorescence photobleaching. Before starting acquisition for each field, the focus was manually adjusted using the GFP fluorescence channel to ensure that cells were centered in the stack. Time series images were taken for 12 cell lines, each having a different protein labeled with GFP. The proteins (genes) were as follows: cytochrome b-5 reductase (dia1) and annexin A5 (anxa5) in cytoplasm, serum deprivation response protein (sdpr) in vesicles and cytoplasm, adipose differentiation-related protein (adfp) in vesicles, ADP-ATP translocase 23 (timm23), ATP synthase (atp5a1) and mitochondrial stress-70 protein (hspa9a) in mitochondria, catalase (cat) in mitochondria and vesicles, glucose transporter 1 (glut1) in plasma membrane and t-complex testis expressed 1 (tctex1), alpha-actinin-4 (actn4), caldesmon 1 (cald1) in cytoskeleton.<ref type="figure" target="#fig_0">Figure 1</ref>illustrates their 2D static patterns and<ref type="figure" target="#fig_1">Figure 2</ref>shows a sample movie for ATP synthase. The full movies are available as described in the Abstract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image preprocessing</head><p>Background fluorescence was removed by subtracting the most common pixel value in the image. This was based on the assumption that an image contains more pixels outside the cell than inside it and that background is roughly uniform, both of which hold true for our images. Then a threshold was chosen by an automated method (<ref type="bibr" target="#b20">Ridler and Calvard, 1978</ref>), and pixels below the threshold were set to zero. For simplicity, thresholded images were used to calculate all features, although thresholding is not required for calculating texture features. No segmentation was performed because no<ref type="bibr" target="#b16">Markey et al., 1999</ref>) which finds the image closest to the mean in feature space. Images are preprocessed with background removal and thresholding. For display purposes, only a rectangular region containing a single cell is shown.shown. Images are preprocessed by background removal, thresholding and photobleaching correction. For display purposes, only a rectangular region containing a single cell is shown. nuclear channel image was available to provide a reference for automatic segmentation algorithms. A major problem with time series images is that photobleaching can occur over time. To minimize the influence of photobleaching, pixel intensities of images at each time point were stretched so that the 95% quantile (and above) was set equal to 256. We chose to stretch the 95% quantile instead of the brightest pixel, because this is less sensitive to noise from artifacts such as fluorescent debris.<ref type="figure" target="#fig_2">Figure 3</ref>illustrates the extent of bleaching and correction. The original intensity (dotted line) dropped to 50% after 10 time points. After photobleaching correction, the total intensity (solid line) at the 10th time point is about 80% of the original. Although the correction cannot fully restore the total intensity, it largely reduces the influence of photobleaching. The other advantage of stretching the intensity to a fixed maximum for all proteins is that effect of protein abundance or brightness of images is largely removed.</p><p>Page: 1632 1630–1636</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y.Hu et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">2D and 3D static feature calculation</head><p>In order to set the reference for comparison, both 2D and 3D static features were calculated. 2D Static features were calculated using the center slice at the first time point. Twenty-one of the features from SLF21 () were used. They include 8 morphological (SLF21.3-SLF1.5 and SLF21.9-SLF21.13) and 13 texture features (SLF21.66-SLF21.78) that are based on the whole image field and do not depend on cell segmentation. Similarly, 3D static features were calculated by using the 3D image at the first time point. Thirty-three features that also do not require cell segmentation were chosen from SLF11 (<ref type="bibr" target="#b2">Chen et al., 2003</ref>). They include 5 morphological, 2 edge and 26 texture features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">2D Temporal feature calculation</head><p>We collected 3D time series images, in order to have as much information as possible on protein behavior. However, we started our exploration with 2D temporal patterns, by using the center slices over time. We used only the first eight time points in each movie, limited by the shortest movie in our image collection. The total period analyzed is therefore 6 min.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Object tracking features</head><p>Calculating features based on object tracking requires several steps. First, we identify all the objects in the image at each time point, and then we compare and match these objects from one image to the next. After solving the position of each object in different time points, we have the trajectories of objects and we design features based on these object trajectories. Objects are defined as continuous pixels that are above threshold. Over time, objects change their positions, shapes and total intensities. The assumption of tracking is that although one object might change its properties, the change is small. The similarity between an object at time point t 1 and itself in the next time point t 2 is stronger than the similarity between any other objects at t 1 with this object at t 2. Features are designed to describe properties of objects and used to calculate similarities. They are as follows:The distance between the object j at time point t 1 and the object k at time point t 2 is calculated by the distance of their features:</p><formula>X j −X k std(Xt 1 −Xt 2 ) 2 + Y j −Y k std(Yt 1 −Yt 2 ) 2 + S j −S k std(St 1 −St 2 ) 2 + I j −I k std(It 1 −It 2 ) 2</formula><p>The distance is a normalized Euclidean distance so that features with large values do not dominate the result. Distances were calculated between all objects in each frame and all objects in the next frame, but object pairs that exceeded a distance threshold of 4 were discarded. The next step is to compare the distance between all object pairs across neighboring images, and find the matching with the minimal sum of distances. Assume there are N1 objects in the image at t 1 , and N2 objects in the image at t 2. N1 and N2 are not necessarily equal because objects can appear or disappear due to movement. The matching is a bipartite matching or an assignment problem: N1 people available for N2 jobs with different costs, where each person can do only one job and each job only needs one person. The best assignment is the one that minimizes the total cost. The naive way to compare different matchings is to calculate total distance for all possible matches, the computation of which is the factorial of the minimum of N1 and N2. We used the Hungarian Algorithm, a classic algorithm that reduces the computation to complexity O((N1 + N2) 3 ) (<ref type="bibr" target="#b14">Kuhn, 1955</ref>). After matching we know the position of each object in every time point and we can build a trajectory of each object. From these, we calculated: Speed: the distance traveled over adjacent time points, divided by the time interval. Velocity: the distance between an object's position when it first appears and its position when it disappears (or at the end of the movie), divided by the total time. Velocity differs from speed in that it only reflects the absolute distance that the object has traveled. If the object travels back and forth around the same area, the speed is high but velocity is low. Acceleration: the change of speed over adjacent time points, divided by the time interval. Angles between successive movements: the angle (0–180 @BULLET ) between the two directions the object travels in adjacent images. We used the mean and variance of these four features across the time series as the final features of the image (eight in total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Temporal texture features</head><p>Temporal texture features were calculated as described previously (<ref type="bibr" target="#b8">Hu et al., 2006</ref>). These were inspired by the Haralick texture features, which capture the intensity correlation of neighboring pixels in space (<ref type="bibr" target="#b7">Haralick, 1979</ref>), and designed to capture the value correlation of neighboring pixels in time. We build the temporal gray-level co-occurrence matrix, whose element in row i and column j is the frequency that a pixel with value i changes to value j in the same position at the next time point. Co-occurrence matrices capture information of protein movement, for example, for proteins that show no movement between the two time points, the temporal co-occurrence matrix contains only zeros except on the diagonal. We calculated 13 statistics described by Haralick (<ref type="bibr" target="#b7">Haralick, 1979</ref>) based on the co-occurrence matrix for each pair of images separated by a certain time interval. We used the mean and variance across the time series as the final features of the image. By varying the time interval, we collected 130 features in total: means and variances of the 13 statistics for images 45, 90, 135, 180 and 225 s apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Normal flow features</head><p>In a series of images taken over time, the intensity of each pixel in each image I (x, y, t) is a function of position x, y and time t. To understand the movement, we need to infer from the observed I (x, y, t), the direction and velocity of each pixel. We use u to represent the vector of movement in the x direction and v to represent the vector of movement in the y direction. u = dx/dt and v = dy/dt Optical flow is a motion field with vectors (u, v) at each pixel. From the definition, we know (u, v) is pointing to the direction that the pixel moves Page: 1633 1630–1636</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automated analysis of protein subcellular dynamics</head><p>and the length of the vector is the speed. Let us consider a pixel I (x, y, t −1) that moves to a new position at t. We here assume that the brightness of this pixel does not change:</p><formula>I(x,y,t −1) = I(x + u(x,y),y + v(x,y),t)</formula><p>We further assume that the movement is not large. So the right-hand side formula can be approximated with first order:</p><formula>I(x,y,t −1) ≈ I(x,y,t)+u(x,y) ∂I ∂x +v(x,y) ∂I ∂y</formula><p>Rearranging the equation we have:</p><formula>u(x,y) ∂I ∂x +v(x,y) ∂I ∂y +[I(x,y,t)−I(x,y,t −1)] = 0 uI x +vI y +I t = 0</formula><p>This is the fundamental equation of motion. I x and I y are the components of the gradient, which, together with the difference of the intensity I t , can be directly calculated from the two images. However we are left with one equation with two unknowns (u and v). This is the aperture problem: motion along the edge or perpendicular to the gradient can never be recovered. Meanwhile the motion (u, v) along the gradient can be calculated:</p><formula>u = −I t I x I 2 x +I 2 y , v = −I t I y I 2 x +I 2 y</formula><p>and the speed of a pixel along the gradient is</p><formula>U n = I t I 2 x +I 2 y</formula><p>The flow field containing flows at the gradient direction is called normal flow. Using it, the following 34 features were calculated:These 34 features were then computed across all images separated by a specific interval, and the mean and variance across the series were used as the final features. This process was carried out for spacing of 45 s (adjacent time points), 90 s (every other time point), 135 s, 180 s and 225 s. The result is 34*2*5 = 340 features in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Fourier transform features</head><p>The Fourier analysis is the classic approach to studying signal changes over time. Studying signals in the frequency domain is commonly used because of its convenience for signal processing: complicated convolution in the spatial domain is just simple multiplication in the frequency domain. The Fourier transform returns a spectrum where each coefficient corresponds to the strength of a specific frequency in the input signal. To calculate the Fourier transform features for time series images, each pixel at the same position over time for the first eight time points was considered as one time series signal. The first five Fourier coefficients were collected and eight basic statistics of each coefficient over the whole image were calculated as the final temporal features (40 in total). 1 5% percentile of non-zero coefficients 2 25% percentile of non-zero coefficients 3 50% percentile of non-zero coefficients 4 75% percentile of non-zero coefficients 5 95% percentile of non-zero coefficients 6 Mean of non-zero coefficients 7 SD of non-zero coefficients 8 Mean/SD of non-zero coefficients</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.5">Autoregression features</head><p>Three properties that define temporal texture have been described: 'complex and nonrigid', 'indeterminate spatial and temporal extent' and 'statistical regularity' (<ref type="bibr" target="#b18">Nelson and Polana, 1992</ref>). A direct way to calculate statistical regularity is to compute the characteristics of images over time and build a regression model. A linear spatiotemporal autoregressive model based on pixel intensities has been used to recognize and build temporal textures (<ref type="bibr">Szumme and Picard, 1996</ref>). Instead of modeling pixel value changes, we chose to model the features of static images over time. There are several advantages of doing this: features are less affected by noise in pixel values, the feature vector (21) is much shorter than the number of pixels (1280*1024), and combined with our understanding of the static features, the nature of the motion can be better appreciated. An autoregressive (AR) model depends only on the previous outputs of the system. The following equation describes a linear AR model for feature X at time t as a function of feature X at earlier time points:</p><formula>X t = c+ P i=1 ϕ i X t−i +ε t</formula><p>where c is a constant, P controls how many time points in the past are used in modeling and ε is estimation error. The ψ i are the parameters we seek to estimate. We choose to vary P-values from 2 to 4, yielding 2, 3 and 4 ψ values. AR was done on each of the 21 2D static features, resulting in (2 + 3 + 4)*21 = 189 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">3D temporal feature calculation</head><p>We expanded the three sets of 2D temporal features to 3D temporal features. Again, only the first 8 time points in each movie were used. 3D temporal texture features were calculated in a similar way as for 2D, except voxels instead of pixels were used when building co-occurence matrices. A total of 130 features that are based on mean and variance of 13 statistics for five different time intervals, the same as for 2D temporal texture features, were calculated. 3D normal flow features are defined similarly to 2D. Normal flow on x, y and z directions can be calculated with the following equations:</p><formula>u = −I t I x I 2 x +I 2 y +I 2 z v = −I t I y I 2 x +I 2 y +I 2 z w = −I t I z I 2 x +I 2 y +I 2 z</formula><p>where I t is the voxel intensity difference across time points and I x , I y and I z are the gradient projected on the x ,y and z directions. The speed of a voxel along the gradient is</p><formula>U n = I t I 2 x +I 2 y +I 2 z</formula><p>Thirty-three normal flow features were calculated for each pair of 3D images, 16 of which are based on U n , 13 based on Haralick texture features of binned directions and 4 based on divergence and curl (extending the definitions above to three dimensions). A total of 330 features, including mean and variance of features for five different time intervals, were calculated, similarly to the 2D normal flow features. 3D Fourier transform features were calculated similar as in 2D, except that voxels at the same position over time were considered as one time</p><p>Page: 1634 1630–1636</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y.Hu et al.</head><p>series signal. Total of 40 features including 8 statistics of 5 coefficients, same as 2D, were calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Feature selection</head><p>The large numbers of features described above could potentially overwhelm the ability of a classifier to identify meaningful decision boundaries. Therefore, we used stepwise discriminant analysis (SDA) to select the features that have the greatest power to discriminate the classes. While many feature selection methods have been described, SDA has performed well in previous studies of subcellular pattern classification (<ref type="bibr" target="#b11">Huang et al., 2003</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Supervised learning (Classification)</head><p>A well established SVM package LIBSVM (<ref type="bibr" target="#b1">Chang and Lin, 2001</ref>) was used for classification. LIBSVM uses the pair-wise algorithm to classify multiple classes and it has support for choosing optimal parameters. Ten-fold crossvalidation was used to evaluate the classification accuracy: the images for each class were randomly divided into 10 groups of equal sizes. Nine of them were used to train a classifier and the remaining one was used for testing. Both feature selection and parameter tuning were done on the training set: SDA selected features were used to adjust the parameters until the classifier reached the best accuracy within the training set. The parameters were the radius of the radial basis function kernel (which was varied from 2 −5 to 2 3 ) and the penalty for overlap between two classes (which was varied from 2 −5 to 2 15 ). After training, the features for the test images were supplied to the classifier, and prediction was made on each image. By comparing the prediction with the class labels of the testing set, we obtain the classification accuracy. This process was repeated for each test fold, and the final overall accuracy is the average accuracy from the 10 tests. Since SVM can inherently handle a large number of correlated features without overfitting, we also repeated each classification without SDA feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>By comparing classification accuracy using individual feature sets and combinations of them, we achieved two goals: evaluating the power of each feature set and finding the best way of differentiating our 12 proteins of interest. Three-dimensional time series images are 4D images. Feature sets we have acquired are based on 2D (2D static features of first center slice), 3D across z (3D static features), 3D across time (2D temporal features of center slices) and 4D images (3D temporal features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification using 2D static features</head><p>To set a reference for comparison, classification results of proteins in 3T3 cells were obtained using 21 2D static features. The overall accuracy was 63% with SDA feature selection and 66% without SDA. The confusion matrix without SDA is shown in<ref type="figure" target="#tab_1">Table 1</ref>. We can see that Cald1 has the lowest classification accuracy (25%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification using 2D temporal feature sets</head><p>To evaluate each temporal feature set, we trained classifiers using it with and without the static feature set. When object tracking features were combined with the 2D static features, the average classification accuracies were 61 and 68%, with or without SDA, showing a very small improvement over static pattern classification. From previous work we know when calculating static Haralick texture features, one can change the gray level or resize the image to different resolution and get different classification results (<ref type="bibr" target="#b17">Murphy et al., 2003</ref>). Since the same effect would be expected in the temporal domain, we evaluated temporal texture features calculated using different pixel sizes and numbers of gray levels. Classification accuracies are summarized in<ref type="figure" target="#tab_2">Table 2</ref>. Accuracies fall within the range of 52–71% with SDA feature selection and 66–77% without SDA. The best accuracy of 77% was obtained for 64 gray levels and no downsampling. Apparently, accuracy is higher without SDA, and SVM inherently handled redundancy of features. To further test the power of temporal texture features, the same classification procedure was done without static features. The overallThe number before comma is the accuracy in percentage with SDA feature selection, and the number after comma is the accuracy in percentage without SDA.The overall accuracy is 66%.and the normal flow direction, a similar exploration of number of gray levels and resolution was done. Classification accuracy ranged from 66% to 75% with SDA and 59% to 73% without SDA (data not shown). The best accuracy of 75% was achieved with the original resolution (0.11 µm/pixel) and 64 gray levels, with SDA. When the same classification procedure was done without static features, the overall classification accuracy was 75% with SDA and 74% without SDA. This indicates that these features were more informative than temporal texture features. When 40 Fourier transform features were combined with the static features, the overall classification accuracy was 69% with SDA and 67% without SDA, a slight increase over using static features alone (data not shown). When using Fourier transform features only, the overall accuracy was 60 and 57% with and without SDA (data not shown). We next combined 189 autoregression features with the static features. The overall classification accuracy was 59% with SDA and 48% without SDA (data not shown). These are both lower than using static features alone. From this analysis of each temporal feature set, we found that temporal texture features, normal flow features and Fourier transform features are capable of increasing classification accuracy. We combined these 3 sets of temporal features to search for the best way to differentiate the 12 proteins in 2D time series images. If combined with 2D static features, the overall accuracy is 75% with SDA and 78% without SDA. With temporal features only, very good classification accuracy of 73 and 77% (with and without SDA) can be achieved, proving again the power of the temporal features. The confusion matrix of the best classification is shown in<ref type="figure" target="#tab_3">Table 3</ref>. Comparing the results with<ref type="figure" target="#tab_1">Table 1</ref>, the overall accuracy is 12% higher than using just 2D static features. Tctex1 was now classified perfectly. The lowest accuracy of 25% for cald was increased to 56%, and accuracy for Sdpr increased from 56% to 95%. Two other proteins, Anxa5 and Actn4, also have over 20% improvement in accuracy. While the accuracy with SDA selection was lower than without, it is interesting to note that SDA selected 7 static, 15 temporal texture, 13 normal flow and 4 Fourier transform features, from the total of 531 features. This suggests that all of these feature types provide useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 1635 1630–1636</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automated analysis of protein subcellular dynamics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification using 3D static features</head><p>Static features based on 3D images capture information of images with one more dimension than 2D images. Since we were not able to perform automatic segmentation due to lack of nuclear channel, we selected those features that do not compare to center of cells, thus do not require segmentation. Classification accuracy using 33 field level 3D static features was 74% with SDA, and 71% without SDA. It is 6% higher than using 2D static features and 4% lower than using combined 2D static and temporal features. When 3D static and 2D static features were combined, classification accuracy was 73 and 70%, with and without SDA. As might be expected, the 3D features apparently contain all of the information in the 2D features, and adding redundant 2D static features only made classifier learning more difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification using 3D temporal feature set</head><p>Similar to the way we tested 2D temporal features, we combined 3D temporal features with 3D static features to see if the classification accuracy was improved. The result is disappointing. Classification accuracies with or without SDA for 3D temporal textures were 74 and 75%, for 3D normal flow were 71 and 72% and for 3D Fourier transform were 73 and 71%. Only 3D temporal texture features, combined with 3D static features, performed slightly better than using 3D static features alone. None of the accuracies were higher than the accuracy we achieved combining 2D temporal features with 2D static features. It appears that calculating features over the full 3D images diluted the critical information in the central slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION AND DISCUSSIONS</head><p>Location proteomics as a branch of proteomics study has grown over the last 10 years. To systematically analyze large amounts of data, automatic algorithms have been developed. This article presented Page: 1636 1630–1636</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y.Hu et al.</head><p>our effort to extend the analysis of static patterns with an additional dimension: the temporal domain. Time series microscopy image of</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Sample fluorescence microscope images of 12 GFP-tagged proteins. Each image is the most typical center slice at the first time point among all 3D time series images of the protein, selected by TypIC (Markey et al., 1999) which finds the image closest to the mean in feature space. Images are preprocessed with background removal and thresholding. For display purposes, only a rectangular region containing a single cell is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Movie of ATP synthase. The first 8 center slices across 315 s are shown. Images are preprocessed by background removal, thresholding and photobleaching correction. For display purposes, only a rectangular region containing a single cell is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. Comparison of total pixel intensity changes with (solid line) versus without (dotted line) photobleaching correction. Intensities are normalized to one at the initial time point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Object center in the x coordinate (X) Object center in the y coordinate (Y ) Object size in pixels (S) Object total intensity (I)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>1–13 Haralick texture features of U n 14 Mean of U n 15 SD of U n 16 Mean/SD of U n 17–29 Haralick texture features of binned direction 30 Difference of direction from uniform distribution 31 Mean positive divergence 32 Mean negative divergence 33 Mean positive curl angular velocity 34 Mean negative curl angular velocity For the texture features of direction, directions were binned into eight groups. We used 1–8 to represent vectors within the range of 0–44 @BULLET , 45– 89 @BULLET , 90–134 @BULLET , 135–179 @BULLET , 180–224 @BULLET , 225–269 @BULLET , 270–314 @BULLET and 315–360 @BULLET , respectively. Divergence and curl angular velocity are defined as:</figDesc><table>div = 
∂u 
∂x 
+ 
∂v 
∂y 
cav = 
∂v 
∂x 
− 
∂u 
∂y 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Summary of classification accuracy of 21 static combined with 130 temporal texture features, with different gray level and resolution</figDesc><table>Resolution (micron/pixel) 
Gray level 

16 
64 
256 

0.11 
64, 77 
71, 77 
70, 76 
0.22 
68, 70 
63, 70 
67, 68 
0.66 
64, 67 
58, 69 
52, 69 
1.1 
66, 67 
62, 70 
64, 66 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 1. Confusion matrix for classification using only 2D static features</figDesc><table>True classes 
Prediction by classification 

Dia1 
Anxa5 
Sdpr 
Adfp 
Timm23 
Atp5a1 
Hspa9a 
Cat 
Glut1 
Tctex1 
Actn4 
Cald1 

Dia1 
45 
0 
35 
5 
5 
0 
5 
0 
5 
0 
0 
0 
Anxa5 
5 
6 1 
0 
0 
0 
0 
0 
0 
0 
0 
3 3 
0 
Sdpr 
26 
4 
56 
0 
4 
0 
0 
0 
0 
0 
8 
0 
Adfp 
0 
1 
0 
9 0 
1 
1 
0 
3 
0 
0 
0 
0 
Timm23 
2 
2 
2 
2 
72 
0 
0 
12 
2 
0 
2 
0 
Atp5a1 
10 
0 
5 
0 
0 
65 
10 
0 
0 
0 
0 
10 
Hspa9a 
4 
0 
0 
4 
8 
4 
70 
0 
0 
0 
8 
0 
Cat 
0 
0 
0 
0 
43 
6 
6 
37 
6 
0 
0 
0 
Glut1 
0 
0 
5 
1 1 
1 1 
1 1 
0 
5 
5 2 
0 
0 
0 
Tctex1 
0 
6 
0 
0 
0 
0 
0 
0 
0 
9 3 
0 
0 
Actn4 
0 
2 0 
3 
3 
0 
0 
3 
0 
0 
3 
5 8 
6 
Cald1 
0 
0 
0 
0 
6 
12 
12 
0 
0 
6 
37 
25 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 3. Confusion matrix for the best classification accuracy of 12 protein patterns</figDesc><table>True classes 
Prediction by classification 

Dia1 
Anxa5 
Sdpr 
Adfp 
Timm23 
Atp5a1 
Hspa9a 
Cat 
Glut1 
Tctex1 
Actn4 
Cald1 

Dia1 
60 
0 
15 
0 
5 
10 
5 
0 
5 
0 
0 
0 
Anxa5 
0 
8 3 
0 
0 
0 
0 
0 
0 
0 
0 
1 6 
0 
Sdpr 
4 
0 
95 
0 
0 
0 
0 
0 
0 
0 
0 
0 
Adfp 
0 
0 
0 
9 6 
0 
1 
0 
0 
0 
0 
1 
0 
Timm23 
0 
0 
0 
2 
65 
0 
7 
12 
12 
0 
0 
0 
Atp5a1 
10 
0 
5 
10 
0 
75 
0 
0 
0 
0 
0 
0 
Hspa9a 
0 
0 
4 
0 
1 2 
0 
7 9 
0 
0 
0 
4 
0 
Cat 
0 
0 
0 
6 
50 
6 
0 
37 
0 
0 
0 
0 
Glut1 
11 
0 
0 
0 
11 
11 
0 
0 
64 
0 
0 
0 
Tctex1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
100 
0 
0 
Actn4 
0 
6 
0 
3 
0 
0 
0 
0 
0 
0 
82 
6 
Cald1 
0 
0 
0 
0 
0 
0 
6 
0 
0 
6 
31 
56 

Features used are 2D static, temporal texture, normal flow and Fourier transform features. The overall accuracy is 78%. 

classification accuracy was 67% with SDA and 66% without SDA. 
The results show that temporal texture features themselves capture 
a significant amount of information. 
We next evaluated the normal flow features. Three hundred 
and forty normal flow features were combined with the 2D static 
features. Since the normal flow features include texture features 
calculated on U n </table></figure>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="12"> proteins were collected, 5 sets of 2D temporal features and 3 sets of 3D temporal features were implemented and classifications were performed to validate their usefulness. The best 2D temporal feature sets, in the order of their ability to improve classification accuracy, were normal flow, temporal texture and Fourier transform features. Combining 2D static features with 3 sets of 2D temporal feature sets gave the best accuracy of 78%, compared with 66% for static features alone. Accuracy using 3D static and/or temporal features was lower than for 2D features. If limited acquisition time requires deciding whether to collect 3D static images or 2D time series images, our results suggest that 2D time series images have higher potential of delivering better differentiation. Although not all of the 2D or 3D temporal feature sets improved classification accuracy for our dataset, we still presented them here because they may be useful for future datasets. While each protein in a proteome is unique, its location patterns might not be. Thus while increasing accuracy of distinguishing the 12 proteins is an indication of feature value, the ability to distinguish them all perfectly is not expected. If proteins interact or colocalize with each other, they cannot be differentiated either by static or temporal pattern. In our dataset, alpha-actinin-4 (actn4) and caldesmon 1 (cald1) both bind to actin, and over 30% of caldesmon 1 is misclassified as alpha-actinin-4. These two proteins are always observed in the same cluster using cluster analysis (data not shown). Similarly, ADP-ATP translocase 23 (timm23) and catalase (cat), which have both been described as mitochondrial proteins, are difficult to distinguish (50% of catalase is misclassified as ADPATP translocase 23). The results suggest that there are only 10 distinguishable patterns in our 12 protein set, and this agrees with prior knowledge about these proteins. For each classification, we compared accuracy with or without SDA feature selection, because SVM is known to be highly robust with large number of correlated features. Our result shows when the number of features is large, 340 normal flow and 189 autoregression, SDA outperforms no feature selection. When the number of features is small, 21 static and 130 temporal textures, SVM does well without SDA feature selection. Many different feature selection algorithms and classifiers could be tried in order to achieve higher classification accuracy, but such an analysis is beyond the scope of this study. Since temporal texture features and Fourier transform features can be calculated within 25 and 50 s for each time series, they are readily applicable to many high throughput applications. On the other hand, calculating normal flow features, object tracking features and autoregression features take 8, 22 and 2 min per time series, respectively. Given the dramatic increase in automated microscopy over the past decade, we anticipate that methods for analyzing temporal changes in protein patterns such as those we have described here will be of significant utility both for basic research in systems biology and for drug screening and development purposes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This publication does not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Motion characterization from temporal cooccurrences of local motion-based measures for video indexing</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bouthemy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Fablet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR&apos;98)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="905" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName>
				<forename type="first">C.-C</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C.-J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Location proteomics-Building subcellular location trees from high resolution 3D fluorescence microscope images of randomly-tagged proteins</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>. SPIE</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="298" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Quantitative fluorescent speckle microscopy of cytoskeleton dynamics</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Danuser</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">M</forename>
				<surname>Waterman-Storer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Biophys. Biomol. Struct</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="361" to="387" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale automated analysis of location patterns in randomly tagged 3T3 cells</title>
		<author>
			<persName>
				<forename type="first">Garcia</forename>
				<surname>Osuna</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1081" to="1087" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated subcellular location determination and high throughput microscopy</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Glory</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">F</forename>
				<surname>Murphy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dev. Cell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="7" to="16" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Cell cycle-dependent regulation of a human DNA helicase that localizes in DNA damage foci</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Cell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3320" to="3332" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical and structural approaches to texture</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">M</forename>
				<surname>Haralick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>. IEEE</meeting>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="page" from="786" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Application of temporal texture features to automated analysis of protein subcellular locations in time series fluorescence microscope images</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Hu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1028" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated classification of subcellular patterns in multicell images without segmentation into single cells</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">F</forename>
				<surname>Murphy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1139" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting accuracy of automated classification of fluorescence microscope images for location proteomics</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">F</forename>
				<surname>Murphy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature reduction for improved recognition of subcellular location patterns in fluorescence microscope images</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>. SPIE</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="307" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">CD-Tagging: a new approach to gene and protein discovery and analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">W</forename>
				<surname>Jarvik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioTechniques</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="896" to="904" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">In vivo functional proteomics: mammalian genome annotation using CD-tagging</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">W</forename>
				<surname>Jarvik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioTechniques</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="852" to="867" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">W</forename>
				<surname>Kuhn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logistic Quart</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonuniform temporal alignment of slice sequences for four-dimensional imaging of cyclically deforming embryonic structures</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liebling</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<meeting><address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1156" to="1159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards objective selection of representative microscope images</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">K</forename>
				<surname>Markey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biophys. J</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="2230" to="2237" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust numerical features for description and classification of subcellular location patterns in fluorescence microscope images</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">F</forename>
				<surname>Murphy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. VLSI Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="311" to="321" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Qualitative recognition of motion using temporal texture</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C</forename>
				<surname>Nelson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Polana</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP Image Understanding</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion retrieval by temporal slices analysis</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">W</forename>
				<surname>Ngo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognition</title>
		<meeting>. Int. Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Picture thresholding using an iterative selection method</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">W</forename>
				<surname>Ridler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Calvard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybernet</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="630" to="632" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic proteomics in individual human cells uncovers widespread cell-cycle dependence of nuclear proteins</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Sigal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="525" to="531" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<monogr>
		<title level="m" type="main">Cell motin analysis without explicit tracking. Computer Vision and Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Souvenir</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
			<pubPlace>Anchorage, AK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal Texture Modeling</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Szummer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Picard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. On Image Processing</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="823" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel cell segmentation method and cell phase identification using Markov model</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Technol. Biomed</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="152" to="157" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>