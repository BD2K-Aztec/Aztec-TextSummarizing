
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gene expression Application of the Bayesian MMSE estimator for classification error to gene expression microarray data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Lori</forename>
								<forename type="middle">A</forename>
								<surname>Dalton</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Edward</forename>
								<forename type="middle">R</forename>
								<surname>Dougherty</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<postCode>85004</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Bioinformatics and Computational Biology</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<addrLine>M. D. Anderson Cancer Center</addrLine>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gene expression Application of the Bayesian MMSE estimator for classification error to gene expression microarray data</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">13</biblScope>
							<biblScope unit="page" from="1822" to="1831"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr272</idno>
					<note type="submission">Received on January 25, 2011; revised on April 8, 2011; accepted on April 24, 2011</note>
					<note>[15:37 13/6/2011 Bioinformatics-btr272.tex] Page: 1822 1822–1831 USA Associate Editor: Joaquin Dopazo supporting simulations are also included. Contact: ldalton@tamu.edu Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: With the development of high-throughput genomic and proteomic technologies, coupled with the inherent difficulties in obtaining large samples, biomedicine faces difficult small-sample classification issues, in particular, error estimation. Most popular error estimation methods are motivated by intuition rather than mathematical inference. A recently proposed error estimator based on Bayesian minimum mean square error estimation places error estimation in an optimal filtering framework. In this work, we examine the application of this error estimator to gene expression microarray data, including the suitability of the Gaussian model with normal– inverse-Wishart priors and how to find prior probabilities. Results: We provide an implementation for non-linear classification, where closed form solutions are not available. We propose a methodology for calibrating normal-inverse-Wishart priors based on discarded microarray data and examine the performance on synthetic high-dimensional data and a real dataset from a breast cancer study. The calibrated Bayesian error estimator has superior root mean square performance, especially with moderate to high expected true errors and small feature sizes. Availability: We have implemented in C code the Bayesian error estimator for Gaussian distributions and normal–inverse-Wishart priors for both linear classifiers, with exact closed-form representations, and arbitrary classifiers, where we use a Monte Carlo approximation. Our code for the Bayesian error estimator and a toolbox of related utilities are available at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Classification is a major constituent of bioinformatics, in particular, phenotypic discrimination, which can be accomplished via many different data types, such as gene expression, protein expression or sequence data. The misclassification error of a classifier quantifies its predictive capacity, the key aspect of any scientific model. * To whom correspondence should be addressed.</p><p>Thus, accuracy of the error estimation represents the salient epistemological issue in classification, model validity (<ref type="bibr" target="#b10">Dougherty and Braga-Neto, 2006</ref>). The main measure of error estimation accuracy is the root mean square (RMS) error of the estimator,</p><formula>RMS = E ε tru −ε est 2 ,</formula><p>where ε tru and ε est are the true and estimated errors of the classifier and E is expectation with respect to the random sampling procedure. Given a large data sample, the data can be split between training and test data, the classifier designed on the training data and classifier error estimated on the test data. In this scenario, there is a satisfactory distribution-free bound, RMS ≤ 1/2 √ m, where m is the size of the test sample (<ref type="bibr" target="#b9">Devroye et al., 1996</ref>). However, when the sample is small, splitting the data is unacceptable because the classifier will be trained on too small a set, thereby resulting in poor classifier design. Thus, in small sample settings (the concern of this article), a classifier is trained and its error estimated on the same data. A number of training data-based error estimators have been proposed in the past and we will consider several in this article. Perhaps the one most commonly employed in bioinformatics is cross-validation. In this method, the data are partitioned into k folds (subsets); at each state of the procedure, one fold is held out, a surrogate classifier trained on the remaining folds and its error estimated on the held-out fold. The error of the classifier (originally trained on the full sample) is estimated by the average surrogate errors on the left-out folds. In the special case k = n, the sample size, each held-out fold consists of one point and the error method is termed 'leave-one-out'. For leave-one-out, there is only one partition of folds; however, when k &lt; n evaluating all combinations of partitions is computationally prohibitive. Hence, in this case partitions are randomly chosen to make the estimation. Although cross-validation is close to being unbiased if k is not too small, it tends to have a large variance for small samples (Braga<ref type="bibr" target="#b4">Neto and Dougherty, 2004b;</ref><ref type="bibr" target="#b9">Devroye et al., 1996</ref>) and also to be poorly correlated with the true error (<ref type="bibr" target="#b14">Hanczar et al., 2007</ref>), the two combining to create a large RMS for small samples [for a review of error estimation performance, see<ref type="bibr" target="#b12">Dougherty et al. (2010)]</ref>. A natural question arises: can cross-validation be used for small samples or, equivalently, are there small-sample cases in which the RMS of cross-validation is sufficiently small so that it can be considered a valid error estimator? To answer this question, one might first ask if it is possible to use distribution-free bounds. Not<ref type="bibr">[15:37 13/6/2011 Bioinformatics-btr272.tex]</ref>Page: 1823 1822–1831only are there very few cases in which such bounds are known, but also when they are known they are so loose as to be useless in practice. For instance, consider the following leave-one-out RMS bound for the k-nearest neighbor classification rule with random tie breaking (<ref type="bibr" target="#b8">Devroye and Wagner, 1979</ref>):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application of the Bayesian MMSE error estimator to microarray data</head><formula>RMS ≤ 1+24 k 2π 1 n .</formula><p>If k = 3 and the sample size is n = 100, then the bound is approximately 0.353, which is useless. Let us now consider bounds when there are distributional assumptions. We consider a feature-label distribution having two equally probable Gaussian class-conditional densities sharing a known covariance matrix and the linear discriminant analysis (LDA) classification rule. For this model, we possess analytic representation of the joint distribution of the true error with the leave-one-out estimator (<ref type="bibr" target="#b24">Zollanvari et al., 2010</ref>).<ref type="figure" target="#fig_0">Figure 1</ref>shows the RMS to be a one-to-one increasing function of the Bayes error for dimensions p = 5,10,25, and sample sizes n = 20,40,60, the RMS and Bayes errors being on the y and x axes, respectively. In this model, where the Bayes error is a function of the distance between the classconditional means, the maximum RMS is bounded and does not exceed 0.15, even with only 20 sample points. Moreover, if one wishes to bound the RMS below some tolerance, τ, one need to only make an assumption on the minimum distance between the means, which corresponds to a maximum Bayes error. This kind of behavior, where the RMS of leave-one-out is tolerable when the Bayes error is small, is often observed—indeed, we see this in<ref type="figure" target="#fig_6">Figure 5</ref>of this article—but it has only been quantified in a small number of cases (Braga<ref type="bibr" target="#b5">Neto and Dougherty, 2010;</ref><ref type="bibr" target="#b24">Zollanvari et al., 2010</ref>). The upshot of these considerations is that if cross-validation is going to be used when the sample size is small, there must be modeling assumptions to make the RMS acceptable. Hence, why not take a Bayesian minimum mean square error (MMSE) approach and thereby guarantee that the average RMS across the model family for the resulting error estimator is minimal among all possible error estimators? That is what is done in Dalton and Dougherty (2011a, b), where a parameterized family of classconditional feature distributions is assumed, a prior distribution is applied to the parameters of the model, and this prior along with observed data are used to compute an unbiased, MMSE estimate of classification error. An advantage of this approach, besides achieving average minimum RMS across the model family, is that it depends only on the form of the designed classifier, not the classification rule used to design the classifier. In particular, it is independent of the feature selection method, which is part of the classification rule. Two problems naturally arise. First, how does one arrive at a prior distribution governing the model? This issue arises in any Bayesian method and, as previously explained, would arise in the context of small-sample error estimation even if one were to use a classical error estimator. The current paper proposes a method to determine a prior distribution when using microarray data. The second issue is the difficulty of deriving an analytic expression for the Bayesian MMSE estimator. This is done for discrete classification under a family of generalized beta prior distributions in Dalton and Dougherty (2011a) and for linear classifiers applied to Gaussian distributions under normal–inverseWishart prior distributions in Dalton and Dougherty (2011b). While we are not advocating the abandonment of analytic methods, it is practically useful to have software that can arrive at the Bayesian MMSE estimator via Monte Carlo methods. Currently, approximation is necessary when using a non-linear classifier, where a closed form solution for the model is not known. This article develops and provides publicly available software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEMS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling microarray data</head><p>We assume two classes and require the training sample to consist of normalized log ratios. Thus, use of normalization schemes such as total intensity normalization or the LOESS method, which are popular transformations before high-level analysis is applied, are required. Logtransformed gene expression values have nearly Gaussian class-conditional distributions (with unknown parameters) (<ref type="bibr" target="#b2">Autio et al., 2009;</ref><ref type="bibr" target="#b15">Hoyle et al., 2002</ref>). To further validate a Gaussian modeling assumption, during feature selection we will permit only features that pass a Shapiro–Wilk Gaussianity test. Note that Bayesian error estimators designed under the Gaussian model are robust in the sense that performance is still good when the true distributions are Johnson distributions (<ref type="bibr" target="#b7">Dalton and Dougherty, 2011b</ref>), which are a class of non-Gaussian distributions with four free parameters to control mean, variance, skewness and kurtosis. Normal–inverse-Wishart priors compose a flexible class of distributions with many degrees of freedom to facilitate calibration of the priors to gene expression microarrays. Further, this family of priors possesses a fast closedform solution when used with linear classification. In problems where the Gaussian model applies and one wishes to use a linear classifier, the benefit one might gain by having more control over the prior is not worth the much greater amount of time required to run an integral approximation code and the effort of designing a specialized model, especially for small samples where one cannot afford a very complex model anyway. Hence, we focus on calibrating normal–inverse-Wishart priors. Assuming the parameters between classes are fairly independent, we have justified the assumptions posed by Dalton and Dougherty (2011a), the others being that the class-conditional distributions are relatively Gaussian and that normal–inverse-Wishart priors are adequate for representing prior knowledge. We are left to devise a method of generating priors for the mean and covariance of each class.That is, the mean conditioned on the covariance is Gaussian with mean m and covariance /ν, and the marginal distribution of the covariance is an inverse-Wishart distribution. The hyperparameters of π(θ) are a real number ν ≥ 0, a length D real vector m, a real number κ and a non-negative definite D×D matrix S. For linear classification, we also restrict κ to be an integer to guarantee a closed form solution. The hyperparameters m and S can be viewed as targets for the mean and the shape of the covariance, respectively. The larger ν is the more localized the prior is about m, and the larger κ is the less the shape of is allowed to wiggle. Given n y observed sample points, we update the prior for class y to a posterior, π * . This posterior has the same form as the prior, with updated hyperparameters given by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The</head><formula>κ * = κ +n y , S * = (n y −1) +S + n y ν n y +ν ( µ−m)( µ−m) T , ν * = ν+n y , m * = n y µ+νm n y +ν ,</formula><p>where µ and are the sample mean and sample covariance of points from class y, respectively. To ensure a proper prior, we require κ&gt;D−1, S positive definite, and ν&gt;0. These restrictions are not mandatory as long as the posterior is proper with κ * &gt; D−1, S * positive definite and ν * &gt; 0. Assuming the a priori probability of class 0 is uniform between 0 and 1 and assuming prior (and posterior) independence between this and the distribution parameters in each class, the Bayesian MMSE error estimator can be expressed as</p><formula>ε = n 0 +1 n+2 E π * [ε 0 n ]+ n 1 +1 n+2 E π * [ε 1 n ],</formula><p>where n = n 0 +n 1 is the total number of sample points.</p><formula>E π * [ε y n ]</formula><p>may be viewed as the posterior expectation of the error contributed by class y. With a fixed classifier and given θ, the true error,</p><formula>ε y n (θ), is deterministic and E π * [ε y n ]= y ε y n (θ)π * (θ)dθ, (1)</formula><p>where y is the parameter space of class y. For non-linear classifiers, this integral must be approximated with Monte Carlo methods. For a linear classifier, i.e. a classifier of the form</p><formula>ψ n (x) = 0 if g(x) ≤ 0 1 if g(x) &gt; 0 ,</formula><p>where g(x) = a T x+b with some constant vector a and constant scalar b,</p><formula>E π * [ε y n ]= 1 2 1+sgn(A)I A 2 A 2 +a T S * a ; 1 2 , κ * −D+1 2 ,</formula><formula>(2)</formula><p>where</p><formula>A = (−1) y g(m * ) ν * ν * +1 ,</formula><p>and I x;α,β is the regularized incomplete beta function. For positive integer N, I x; 1 2 , N 2 has a closed form solution, in particular,</p><formula>I 1; 1 2 , N 2 = 1 and for 0 ≤ x &lt; 1, I x; 1 2 , N 2 = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 2/π sin −1 √ x if N = 1 2/π sin −1 √ x + 2 √ x π (N−1 )/2 k=1 (2k −2)!! (2k −1)!! 1−x k− 1 2 if N &gt; 1 is odd √ x</formula><p>(N−2 )/2 k=0</p><p>(2k −1)!! (2k)!!Page: 1825 1822–1831</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application of the Bayesian MMSE error estimator to microarray data</head><p>95% confidence in both classes are used, unless there are not enough features passing the test, in which case we select a fixed number of features with the highest sum of the Shapiro–Wilk test statistics in each class. In the final stage of feature selection, we reduce the feature set to D features. This is done either by applying a t-test if it has not already been applied in the first stage or by using the same t-test statistics from the first stage to pick the D most differentially expressed Gaussian features. This implementation employs classifier-independent feature selection schemes, such as the t-test and Shapiro–Wilk test. However, even for classifier-dependent schemes, once the feature selection and classification schemes have been implemented, the Bayesian error estimator (BEE) may be calculated as a deterministic function of the fixed classifier. This is in contrast to cross-validation, which uses surrogate classifiers to estimate the error of the designed classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Estimating prior hyperparameters</head><p>When calibrating priors for microarrays, what data should be used and how? With the explosion of microarray experimentation over the last decade, the genomics community has amassed an enormous database of gene expression data, and trends in the entire history of microarray experimentation could be used to find a prior, perhaps conditioned on a particular organism, tissue, gene and/or type of abnormality, depending on the nature of the experiment at hand. However, different microarray experiments are currently very difficult to compare, although there have been some recent efforts to normalize and integrate different datasets (<ref type="bibr" target="#b2">Autio et al., 2009</ref>). The method employed here uses discarded gene expression data, consisting of a subset of the features from the microarray data that are not used for classification, to calibrate the priors of the Bayesian error estimator. Though these features are not used in the actual classifier, they may implicitly contain useful calibration information such as the varying concentrations of DNA material used in each microarray, background intensities and other characteristics of the digitized images of a microarray slide. And although calibration requires a large amount of data and in microarray gene expression analysis we typically expect a very small sample setting, the huge number of discarded features ensures that there is enough data for a successful calibration of the hyperparameters. It is possible to define a prior on the entire feature set and to compute the Bayesian error estimator over the reduced feature set based on the marginal distribution of this prior on only the selected features. However, the following approach directly defines a prior on only the selected features. We essentially use a method of moments approach to calibrate the hyperparameters; however, estimating a vector m and matrix S may be problematic for a small number of sample points, so to simplify the analysis we assume the following structure on these hyperparameters:</p><formula>m = m[1,1,...,1] T , S = σ 2 ⎡ ⎢ ⎢ ⎢ ⎣ 1 ρ ··· ρ ρ 1 ··· ρ . . . . . . .. . . . . ρ ρ ··· 1 ⎤ ⎥ ⎥ ⎥ ⎦ ,</formula><p>where m is a real number, σ 2 ≥ 0, and −1 ≤ ρ ≤ 1. This structure is justified because prior to observing the data, there is no reason to think that any feature, or pair of features, should have unique properties. With this simplification, our problem is now reduced to estimating five scalers for each class: ν, m,</p><formula>κ, σ 2 and ρ.</formula><p>In the first stage of a method of moments approach, we find the theoretical first and second moments of the random variables µ and (random because of the prior distribution applied to them) in terms of the hyperparameters we wish to estimate. Throughout the remainder of this section, a subscript i represents the i-th element of a vector, and a subscript jk represents the j-th row, k-th column element of a matrix. First consider the parameter , with a marginal prior having an inverseWishart distribution with hyperparameters κ and S. The mean of this distribution is well known (<ref type="bibr" target="#b19">Rowe, 2003</ref>),</p><formula>E[]= S κ −D−1 ,</formula><p>and given the previously defined structure on S, we obtain</p><formula>σ 2 = (κ −D−1)E[ 11 ],</formula><formula>(4)</formula><formula>ρ = E[ 12 ] E[ 11 ] .</formula><formula>(5)</formula><p>Due to our imposed structure, only E<ref type="bibr">[ 11 ]</ref>and E<ref type="bibr">[ 12 ]</ref>are needed. The variance of the j-th diagonal element in inverse-Wishart distributed may be expressed as</p><formula>Var jj = 2(S jj ) 2 (κ −D−1) 2 (κ −D−3) = 2(E[ 11 ]) 2 κ −D−3 ,</formula><p>where we have applied Equation (4) in the second equality. Solving for κ,</p><formula>κ = 2(E[ 11 ]) 2 Var 11 +D+3.</formula><formula>(6)</formula><p>We next consider the mean, µ, which is parameterized by the hyperparameters ν and m. The marginal distribution of the mean is a multivariate Student's t-distribution given by Rowe (2003):</p><formula>π(µ) = κ+1 2 κ−D+1 2 ν D π D |S| −1 1+ν(µ−m) T S −1 (µ−m) κ+1 .</formula><p>The mean and covariance of this distribution are well known:</p><formula>E[µ]=m, Var µ = S (κ −D−1)ν = E[] ν .</formula><p>With the assumed structure on m, we obtain</p><formula>m = E[µ 1 ],</formula><formula>(7)</formula><formula>ν = E[ 11 ] Var µ 1 .</formula><formula>(8)</formula><p>Finally, our objective is to approximate the expectations in Equations (4) through (8) using calibration features left out of the classification scheme. Suppose the calibration data for the current class consists of n sample points with E D features. Let µ E be the sample mean and E be the sample covariance matrix of the complete set of E features in the calibration data. From these we wish to find several sample moments of µ and in our original D feature problem, that is, to find E<ref type="bibr">[µ 1 ]</ref>, Var</p><formula>µ 1 , E[ 11 ], E[ 12 ]</formula><p>and Var 11 , where the hats indicate the sample moment of the corresponding quantity. All these are scaler quantities. To compress the set of E features in the calibration data to solve an estimation problem on just D features, and ultimately to find these scaler sample moments in a balanced way, we emulate the feature selection process by assuming that the selected features are drawn uniformly. Since any of the E features is equally likely to be selected as the i-th feature, the sample mean of the mean of the i-th feature, E<ref type="bibr">[µ i ]</ref>, is computed as the average of the sample means of all E features in the calibration data. This result is the same for all i, and we use E<ref type="bibr">[µ 1 ]</ref>to represent all features. In particular,</p><formula>E[µ 1 ]= 1 E E i=1 µ E i .</formula><formula>(9)</formula><p>Thanks to uniform feature selection, all other moments are balanced over all features or any pair of distinct features. The remaining sample moments are obtained in a similar manner: Var</p><formula>µ 1 = 1 E −1 E i=1 µ E i − E[µ 1 ] 2 ,</formula><formula>(10) E[ 11 ]= 1 E E i=1 E ii ,</formula><formula>(11)</formula><p>Page: 1826 1822–1831</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.A.Dalton and E.R.Dougherty</head><formula>E[ 12 ]= 2 E(E −1) E i=2 i−1 j=1 E ij , (12) Var 11 = 1 E −1 E i=1 E ii − E[ 11 ] 2 .</formula><formula>(13)</formula><p>Here, Var µ 1 represents the variance of each feature in the mean. We also have E<ref type="bibr">[ 11 ]</ref>and E<ref type="bibr">[ 12 ]</ref>representing the sample mean of diagonal elements and off-diagonal elements in , respectively. Finally, Var 11 is the sample variance of the diagonal elements in. Plugging our sample moments into Equations (4) through (8), we obtain</p><formula>σ 2 = 2 E[ 11 ] ( E[ 11 ]) 2 Var 11 +1 ,</formula><formula>(14)</formula><formula>ρ = E[ 12 ] E[ 11 ] ,</formula><formula>(15)</formula><formula>κ = 2( E[ 11 ]) 2 Var 11 +D+3, (16) m = E[µ 1 ],</formula><formula>(17)</formula><formula>ν = E[ 11 ] Var µ 1 .</formula><formula>(18)</formula><p>Note Equation (6) for κ was plugged into Equation (4) to obtain the final σ 2. In sum, calibration for the prior hyperparameters is defined by</p><p>Equations (14) through (18), the sample moments being given in Equations (9) through (13). The estimates of κ and ν can be unstable, since they rely on second moments, Var 11 and Var µ 1 , in a denominator. These parameters can be made more stable by discarding outliers when computing the sample moments. Herein, we discard the 10% of the µ E i with largest magnitude and the 10% of the E ii with largest value. This method is one of many possible approaches; for simplicity and to avoid an over-defined system of equations, we do not incorporate the covariance between distinct features in µ [that is,<ref type="bibr">Cov µ 12 ]</ref>, the variance of off-diagonal elements in<ref type="bibr">[that is, Var 12 ]</ref>, or the correlation between distinct elements in , though it may be possible to use these to improve the estimates of the hyperparameters. It may also be feasible to use other estimation methods, such as maximum likelihood. Furthermore, the method proposed here to calibrate the priors is a purely data-driven technique for easy and general application to microarray experiments. Ideally, the best way to calibrate priors would be to incorporate data and biological knowledge specific to the particular features selected for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head><p>We present two sets of results demonstrating good performance of Bayesian error estimators, one on synthetic high-dimensional data with three-stage feature selection and a second based on breast cancer data with two stages of feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">High-dimensional synthetic data</head><p>In this section, we apply our Bayesian prior estimation method to synthetic high-dimensional microarray data. We use the same synthetic data model provided in<ref type="bibr" target="#b17">Hua et al. (2009)</ref>, which models many observations made in microarray expression-based studies, including blocked covariance matrices to model groups of interacting variables with negligible interactions between groups. Our model emulates a full feature-label distribution with 20 000 total features. Features are categorized as either 'markers' or 'non-markers'. Markers represent features that have different classconditional distributions in the two classes and are further divided<ref type="bibr" target="#b17">Hua et al., 2009</ref>). into two subtypes: global markers and heterogeneous markers. Nonmarkers have the same distributions for both classes and thus have no discriminatory power, and are also divided into two subtypes: highvariance non-markers and low-variance non-markers. A summary of the feature types is shown in<ref type="figure" target="#fig_2">Figure 2</ref>. Twenty features are global markers, which are homogeneous in each class. In particular, the set of all global markers in class i has a Gaussian distribution with mean µ gm i and covariance matrix gm i. Within class 1, we assume each sample point belongs to one of two equally likely subclasses named 0 and 1, representing different stages or subtypes of cancer. Each subclass is associated with 50 heterogeneous markers, which are jointly Gaussian with mean µ hm 1 and covariance hm 1. Sample points associated with the other subclass have the same distribution as class 0, which is Gaussian with mean µ hm 0 and covariance hm 0. Each heterogeneous marker may only be associated with one subclass, thus there are 100 total heterogeneous markers in the model. We simplify the model by assuming that µ gm i and µ hm i have the form m i × 1,1,...,1 for fixed scalers m i. We assume gm i and hm i have the form σ 2 i , where σ 2 i are constants and , not to be confused with the definition in Section 2.2, has a block covariance structure, i.e.</p><formula>= ⎡ ⎢ ⎣ ρ ··· 0 .. . .. . .. .</formula><formula>0 ··· ρ ⎤ ⎥ ⎦,</formula><p>with ρ being a 5×5 matrix with 1 on the diagonal and ρ = 0.8 off the diagonal. That is, we group markers into blocks of five features, where the blocks are independent from each other, and the markers within each block are correlated with a relatively high correlation coefficient to emulate a pathway. We generate 2000 high-variance non-marker features, which have independent mixed Gaussian distributions given by pN(m 0 ,σ 2 0 )+ (1−p)N(m 1 ,σ 2 1 ), where m i and σ 2 i are the same scalers defined for markers. The random variable p is selected independently for each feature with a uniform distribution over<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>and is applied to all sample points of both classes. These features can be viewed as genes regulated by mechanisms unrelated to those that regulate the class 0 and class 1 phenotypes. The remaining features are low-variance non-marker features, each having independent univariate Gaussian distributions with mean m 0 and variance σ 2 0 .In this model, heterogeneous markers are Gaussian within each subclass, but the class-conditional distribution for class 1 is a mixed Gaussian distribution (mixing the distributions of the subclasses), and is thus not Gaussian. Further, the high-variance features are also mixed Gaussian distributions, so this model incorporates both Gaussian and non-Gaussian features to challenge the Shapiro–Wilk Gaussianity test in the feature selection scheme. To simplify our simulations, we set the a priori probability of both classes to 0.5 and fix the parameters m 0 = 0 and m 1 = 1. We also define a single parameter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 1827 1822–1831</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application of the Bayesian MMSE error estimator to microarray data</head><formula>σ 2 = σ 2 0 = σ 2 1</formula><p>, which specifies the difficulty of the classification problem. A summary of our synthetic high-dimensional data model parameters is given in<ref type="figure" target="#tab_1">Table 1</ref>. In all simulations, the values for σ 2 are chosen so that a single global feature (note that all global features are identical) has a specific Bayes error. We call this the 'Bayes error' in the remainder of this section, and it is given by ε * = −1/(2σ) , where is the unit normal Gaussian cumulative distribution function, so for instance, we use σ = 0.9537 for a Bayes error of 0.3. Under this high-dimensional model, we run several Monte Carlo simulations. In each experiment, we fix the training sample size, n, the number of selected features, D, and the difficulty of the classification problem via σ. The synthetically generated samples are non-stratified, meaning that in each iteration the sample size of each class is not fixed but determined by a binomial (0.5,n) experiment, and the corresponding sample points are randomly generated according to the distributions defined for each class. Once the sample has been generated, we apply the three-stage feature selection scheme outlined in Section 2.4. In the first stage, we apply a t-test to obtain 1000 highly differentially expressed features by removing most non-informative features. In the second stage, we apply a Shapiro–Wilk Gaussianity test and eliminate features that do not pass the test with 95% confidence. The number of features output in this stage is variable. If there are not at least 30 features that pass the test, then we return the 30 features with the highest sum of the Shapiro–Wilk test statistics for both classes. In the final stage, we use the same t-test values computed before to obtain the final set of D highly differentially expressed Gaussian features, which will be used to design our classifier. The 1000−D features that pass the first stage of feature selection but are not used for classification are saved as calibration data. The feature selected training data are then used to train an LDA classifier. With the classifier fixed, 5000 testing points aredrawn from exactly the same distribution as the training data and used expressly to approximate the true error. Subsequently, several training data error estimators are computed, including leave-oneout (loo), 5-fold cross-validation (cv), 0.632 bootstrap (boot) and bolstered resubstitution (bol) (see the Supplementary Material for details). Two Bayesian error estimators are also applied, one with flat non-informative priors defined by π θ = 1 (flat BEE), and the other with priors calibrated as described in Section 2.5 (calibrated BEE). Since the classifier is linear, these BEEs are computed exactly. This entire process is repeated 120 000 times to approximate the RMS deviations from the true error for each error estimator. We first analyze the quality of features selected by the threestage feature selection algorithm.<ref type="figure" target="#fig_3">Figure 3a</ref>shows the percentage of selected features that are global features with respect to the expected true error of the designed classifier. We would like to graph performance with respect to Bayes error, which is a more pure measure of the difficulty of a classification problem, but evaluating Bayes error on our high-dimensional model is difficult and it may not be close to the true error of the designed classifier. Hence, in our graphs we focus on performance with respect to expected true error. Similarly,<ref type="figure" target="#fig_3">Figure 3b</ref>graphs against feature size with a fixed Bayes error of 0.3. Recall that this model uses 20 000 features, of which only 20 are global features that most effectively discriminate the classes. As long as the feature size is reasonable given the difficulty of the problem (expected true error and sample size), this percentage is quite large. However, in<ref type="figure" target="#fig_3">Figure 3b</ref>for sample size 60 we see that a feature size larger than 7 will result in &lt;80% of the selected features being global features. This illustrates the necessity of restricting feature size in a small sample setting, and is consistent with earlier studies showing the difficulty of finding good feature sets when the number of features is large and the sample is small (<ref type="bibr" target="#b11">Dougherty et al., 2009;</ref><ref type="bibr" target="#b21">Sima and Dougherty, 2006</ref>). The graphs in<ref type="figure" target="#fig_5">Figure 4</ref>show the percentage of selected feature sets that are not rejected by a multivariate Shapiro–Wilk test on either class at a 95% significance level. There are several multivariate Gaussianity tests based on the Shapiro–Wilk statistic. We used Villasenor<ref type="bibr" target="#b23">Alvaa and Estradaa (2009)</ref>, which generalizes the classical univariate Shapiro–Wilk test to the multivariate case by transforming the data into a set of approximately independent standard normal random variables, and essentially summing up the standard Shapiro–Wilk statistic on each dimension. The results show that even though the three-stage feature selection algorithm only uses a univariate Gaussianity test, and univariate normality does not Page: 1828 1822–1831</p><formula>(a) ( b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.A.Dalton and E.R.Dougherty</head><p>imply multivariate normality, the resulting feature set still tends to have a high probability of passing the multivariate Gaussianity test. We next turn our attention to the RMS performance of error estimators under our synthetic high-dimensional model, where a summary of all simulation settings are available in<ref type="figure" target="#tab_2">Table 2</ref>. Our first battery of simulations in<ref type="figure" target="#fig_6">Figure 5</ref>shows RMS deviation from true error for all error estimators with respect to expected true error for LDA classification with 1, 3, 5 or 7 selected features and either 60 or 120 sample points. Given the sample sizes, it is prudent to keep the number of selected features small to have satisfactory feature selection (<ref type="bibr" target="#b21">Sima and Dougherty, 2006</ref>) and to avoid the peaking phenomena (<ref type="bibr" target="#b16">Hua et al., 2005</ref><ref type="bibr" target="#b17">Hua et al., , 2009</ref>). Lines marked with 'o' represent the Bayesian error estimator with flat priors, and lines marked with 'x' represent the Bayesian error estimator with the calibrated priors. The key point in these graphs is that the calibrated BEE has best performance in the mid and high range. For an expected true error of about 0.25 and n = 60, the RMS for the calibrated BEE outperforms 5-fold cross-validation for D = 1,3,5 and 7 by 0.0507, 0.0300, 0.0335 and 0.0379, respectively, representing 64, 32, 30 and 29% decrease in RMS, respectively. For n = 120, the decrease in RMS for D = 1,3,5 and 7 is 0.0366, 0.0175, 0.0192 and 0.0198, respectively, for 67, 34, 35, and 33 percent decrease in RMS, respectively. All other error estimators typically have best performance for low expected true errors, with the flat BEE having even better performance than the classical error estimation schemes. Indeed, all graphs except<ref type="figure" target="#fig_6">Figure 5g</ref>demonstrate that either the flator calibrated Bayesian error estimator is the best scheme over the whole range of expected true error. Our next set of graphs in<ref type="figure" target="#fig_7">Figure 6</ref>show simulation results with respect to feature size. For reference, graphs of the expected true error for these simulations are shown in<ref type="figure" target="#fig_8">Figure 7</ref>. Calibrated priors provide the best performance, except when combining large feature and small sample sizes, in which case a flat prior performs best. In fact, performance of the calibrated BEE in<ref type="figure" target="#fig_7">Figure 6</ref>tends to be best precisely in the rage of feature sizes with the highest percentage of global features and the lowest true errors. For example, the calibrated BEE in<ref type="figure" target="#fig_7">Figure 6a</ref>for sample of size 60 has the best performance up to 7 features, where in<ref type="figure" target="#fig_3">Figure 3b</ref>the percentage of selected features being global is greater than about 80% and in<ref type="figure" target="#fig_8">Figure 7</ref>the true error has started to level off. Note, also, the consistently superior performance of the calibrated BEE over the non-Bayesian estimators for n = 60; indeed, throughout the range of feature sizes, the calibrated BEE has an RMS of at least 0.0263 smaller than the best performing non-Bayesian error estimator, which represents an improvement of at least 14%. Note the upward RMS trend in<ref type="figure" target="#fig_7">Figure 6a</ref>and the downward trend in<ref type="figure" target="#fig_7">Figure 6b</ref>for the non-Bayesian error estimators. Although it can be dangerous to generalize about the behavior of error estimators, let us at least conjecture. We see in<ref type="figure" target="#fig_8">Figure 7</ref>that the true error is large for n = 60, with little improvement as we increase the number of features and, in fact, increasing true error as the number of features passes 7, which is a clear sign of the peaking phenomenon. Thus, for n = 60, adding features creates a more difficult estimation problem that is not offset by easing error estimation on account of small true errors. On the other hand, in<ref type="figure" target="#fig_8">Figure 7</ref>we see a fast reduction of true error for n = 120 as more features are added, thereby greatly easing the error estimation problem and resulting in the declining RMS trend in<ref type="figure" target="#fig_7">Figure 6b</ref>. While these comments apply directly to the non-Bayesian error estimators, they apply to the Bayesian estimators relative to their change of slope. The flat BEE is relatively constant in<ref type="figure" target="#fig_7">Figure 6a</ref>but falls along with the non-Bayesian error estimators in<ref type="figure" target="#fig_7">Figure 6b</ref>, whereas the calibrated BEE consistently rises in<ref type="figure" target="#fig_7">Figure 6a</ref>but remains relatively flat in<ref type="figure" target="#fig_7">Figure 6b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Empirical breast cancer data</head><p>We applied the Bayesian error estimator to normalized gene expression measurements from a breast cancer study</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application of the Bayesian MMSE error estimator to microarray data</head><p>(a)(van de<ref type="bibr" target="#b22">Vijver et al., 2002</ref>). This study used 295 sample points, with 180 assigned to class 0 (good prognosis) and 115 in class 1 (bad prognosis), and provides a 70 feature prognosis profile. From the original 295 points, we randomly draw a non-stratified training sample of size n. Since the number of features in the dataset is relatively small, we apply only the last two stages of our feature selection scheme in Section 2.4. The first stage selects features passing a Shapiro–Wilk Gaussianity test with 95% confidence and must report at least D features, while the second stage selects D features with the highest t-test statistic. The 70−D features not used for classification are retained as calibration data for Bayesian error estimation. After feature selection, we train an LDA, QDA or 3NN classifier. The remaining sample points are used as holdout data to approximate the true error of the designed classifier. The previously considered error estimators are also evaluated from the training samples [except in the case of 3NN where semibolstering is used instead of bolstering owing to its superior performance for 3NN (Braga<ref type="bibr" target="#b3">Neto and Dougherty, 2004a)]</ref>, along with exact Bayesian error estimators (for LDA) or approximate Bayesian error estimators (for QDA and 3NN). Both flat and calibrated priors are applied. This process is repeated either 100 000 times (for LDA) or 10 000 times (for QDA and 3NN) to estimate the average RMS deviation of each error estimator from the true error. The Bayesian error estimator priors are calibrated as discussed in Section 2.5. A typical prior with 2 features and 40 sample points is ν = 16.80, m =−0.004, κ = 12, σ 2 /(κ −D−1) = 0.042 and ρ = 0.020 for class 0, and ν = 2.78, m =−0.068, κ = 10, σ 2 /(κ −D− 1) = 0.024 and ρ = 0.073 for class 1. These indicate that the good prognosis class (0) has a distribution with a more concentrated mean (since ν is much larger) and the mean is close to 0, which is expected since the data have been normalized. On the other hand, κ is fairly large for both classes, suggesting that the variance of each feature in either class is probably close to the prior expected variance, σ 2 /(κ − D−1). Interestingly, the variance is a bit larger for class 0 and ρ is usually small but positive. Figures 8, 9 and 10 provide simulation results for LDA, QDA and 3NN, respectively. Each figure contains subplots representing fixed feature sizes between one and five, and one figure showing the expected true error for all simulations with the corresponding classifier. A summary of the simulation settings is shown in<ref type="figure" target="#tab_3">Table 3</ref>. The uniform prior performs well over a wide range of sample and feature sizes, and generally shows significant improvement over the classical error estimators. Prior calibration can have even more pronounced improvement, especially for small feature sets. Also, although the uniform prior often performs better than the calibratedprior for high feature sizes, see for example<ref type="figure" target="#fig_9">Figure 8e</ref>for 5 features, we observe in<ref type="figure" target="#fig_9">Figure 8f</ref>that true error does not improve much, and may actually get worse, for as little as 5 features. This may indicate that when there is not enough calibration data for good prior design, there is also probably insufficient data for good classifier design.</p><formula>( b) (d) (c) (e) ( f) (g) ( h)</formula><formula>(a) n = 60, D = 1; (b) n = 120, D = 1; (c) n = 60, D = 3; (d) n = 120, D = 1; (e) n = 60, D = 5; (f ) n = 120, D = 5; (g) n = 60, D = 7; (h) n = 120, D = 7.</formula><formula>(a) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.A.Dalton and E.R.Dougherty</head><formula>(a) ( b)</formula><formula>(c) (e) (f) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Concluding remarks</head><p>Our synthetic data simulations demonstrate the power of prior knowledge in two ways: we may assume a low Bayes error by using a flat prior and outperform the classical error estimators where they perform best, or we may calibrate a prior, even using purely datadriven methods, and obtain superior performance in the midrange of Bayes errors. Also note that for moderately difficult classification problems which are typical in a small sample biological setting, the midrange is precisely where training data error estimation is needed. One might argue that there is a risk with postulating a low-Bayes-error prior since, although it will show excellent performance if the Bayes error is truly low, it will suffer for large Bayes errors. In<ref type="figure" target="#fig_6">Figure 5</ref>, not only does performance deteriorate with increasing Bayes error for the Bayesian MMSE estimator, but also the performance of cross-validation. This should not be surprising because the use of cross-validation presupposes that the Bayes error is small because its performance seriously degrades for increasing Bayes error. This behavior, noted more than 30 years ago in a simple 1D Gaussian model (<ref type="bibr" target="#b13">Glick, 1978</ref>), has been</p><formula>(a) (b) (c) ( d)</formula><formula>(a) (c) (e) (f) (d)</formula><p>(b)demonstrated via large simulations for both discrete and Gaussian models (Dalton and Dougherty, 2011a, b), and has been analytically proven in the Gaussian model (<ref type="bibr" target="#b24">Zollanvari et al., 2010</ref>). In other words, unless one is not interested in error estimator performance, use of cross-validation carries with it implicitly assumed prior knowledge. If one knows that the Bayes error is low, then why not define a prior model based on this assumption to design a Bayesian error estimator with even better performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application of the Bayesian MMSE error estimator to microarray data</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Leave-one-out RMS versus Bayes error for LDA. (plus) is 20 samples, (triangle) 40 samples and (circle) 60 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[15:</head><figDesc>37 13/6/2011 Bioinformatics-btr272.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Different feature types in constructing the high-dimensional synthetic data model (Hua et al., 2009).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. Percentage of three-stage selected features that are global features in the synthetic high-dimensional data model. (a) versus expected true error; (b) versus feature size, Bayes error = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.4.</head><figDesc>Fig. 4. Percentage of three-stage selected features that are not rejected by a multivariate Shapiro–Wilk test on either class at a 95% significance level with the synthetic high-dimensional data model. (a) versus expected true error; (b) versus feature size, Bayes error = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.5.</head><figDesc>Fig. 5. RMS deviation from true error for the synthetic high-dimensional data model with LDA classification versus expected true error. (a) n = 60, D = 1; (b) n = 120, D = 1; (c) n = 60, D = 3; (d) n = 120, D = 1; (e) n = 60, D = 5; (f ) n = 120, D = 5; (g) n = 60, D = 7; (h) n = 120, D = 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.6.</head><figDesc>Fig. 6. RMS deviation from true error for the synthetic high-dimensional data model with LDA classification versus feature size. See Figure 7 for the expected true errors in these graphs. (a) n = 60, Bayes error = 0.3; (b) n = 120, Bayes error = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.7.</head><figDesc>Fig. 7. Expected true error for the synthetic high-dimensional data model with LDA classification versus feature size, Bayes error = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig.8.</head><figDesc>Fig. 8. RMS deviation from true error and expected true error with LDA classification of empirical measurements from a breast cancer study. (a) 1 feature; (b) 2 features; (c) 3 features; (d) 4 features; (e) 5 features; (f ) expected true error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig.9.</head><figDesc>Fig. 9. RMS deviation from true error and expected true error with QDA classification of empirical measurements from a breast cancer study.(a) 1 feature; (b) 2 features; (c) 3 features; (d) expected true error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig.10.</head><figDesc>Fig. 10. RMS deviation from true error and expected true error with 3NN classification of empirical measurements from a breast cancer study.(a) 1 feature; (b) 2 features; (c) 3 features; (d) 4 features; (e) 5 features; (f ) expected true error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>Synthetic high-dimensional data model parameters 

Parameters 
Values/description 

Total features 
20 000 
Global markers 
20 
Subclasses in class 1 
2 
Heterogeneous markers 
50 per subclass (100 total) 
High-variance features 
2000 
Low-variance features 
17 880 
Mean 
m 0 = 0, m 1 = 1 
Variances 
σ 2 = σ 2 
0 = σ 2 
1 (controls Bayes error) 
Block size 
5 
Block correlation 
0.8 
a priori probability of class 0 
0.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Data model and classification settings for simulation with synthetic high-dimensional data</figDesc><table>Data model 
Classifier 
Sample size 
Feature selection 
BEE calibration 
Iteration 

Bayes error 
Training 
Test 
Original 
1st t-test 
Shapiro–Wilk test 
2nd t-test 

0.05–0.45 
LDA 
n = 60 
5000 
20 000 
1000 
95% confidence 
D = 1 
1000−D 
120 000 
0.05–0.45 
LDA 
n = 60 
5000 
20 000 
1000 
95% confidence 
D = 3 
1000−D 
120 000 
0.05–0.45 
LDA 
n = 60 
5000 
20 000 
1000 
95% confidence 
D = 5 
1000−D 
120 000 
0.05–0.45 
LDA 
n = 60 
5000 
20 000 
1000 
95% confidence 
D = 7 
1000−D 
120 000 
0.05–0.45 
LDA 
n = 120 
5000 
20 000 
1000 
95% confidence 
D = 1 
1000−D 
120 000 
0.05–0.45 
LDA 
n = 120 
5000 
20 000 
1000 
95% confidence 
D = 3 
1000−D 
120 000 
0.05–0.45 
LDA 
n = 120 
5000 
20 000 
1000 
95% confidence 
D = 5 
1000−D 
120 000 
0.05–0.45 
LDA 
n = 120 
5000 
20 000 
1000 
95% confidence 
D = 7 
1000−D 
120 000 
0.3 
LDA 
n = 60 
5000 
20 000 
1000 
95% confidence 
1 to 10 
1000−D 
120 000 
0.3 
LDA 
n = 120 
5000 
20 000 
1000 
95% confidence 
1 to 10 
1000−D 
120 000 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 3. Classification schemes and settings for simulation with real breast cancer data</figDesc><table>Classifier 
Sample size 
Feature selection 
BEE calibration 
Iteration 

Training 
Test 
Original 
Shapiro–Wilk test 
2nd t-test 

LDA 
20–50 
295−n 
70 
95% confidence 
D = 1 
7 0 −D 
100 000 
LDA 
25–55 
295−n 
70 
95% confidence 
D = 2 
7 0 −D 
100 000 
LDA 
30–60 
295−n 
70 
95% confidence 
D = 3 
7 0 −D 
100 000 
LDA 
35–65 
295−n 
70 
95% confidence 
D = 4 
7 0 −D 
100 000 
LDA 
40–70 
295−n 
70 
95% confidence 
D = 5 
7 0 −D 
100 000 

QDA 
20–50 
295−n 
70 
95% confidence 
D = 1 
7 0 −D 
10 000 
QDA 
25–55 
295−n 
70 
95% confidence 
D = 2 
7 0 −D 
10 000 
QDA 
30–60 
295−n 
70 
95% confidence 
D = 3 
7 0 −D 
10 000 

3NN 
20–50 
295−n 
70 
95% confidence 
D = 1 
7 0 −D 
10 000 
3NN 
25–55 
295−n 
70 
95% confidence 
D = 2 
7 0 −D 
10 000 
3NN 
30–60 
295−n 
70 
95% confidence 
D = 3 
7 0 −D 
10 000 
3NN 
35–65 
295−n 
70 
95% confidence 
D = 4 
7 0 −D 
10 000 
3NN 
40–70 
295−n 
70 
95% confidence 
D = 5 
7 0 −D 
10 000 

</table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1">−x k if N &gt; 1 is even, (3) where !! is the double factorial. 2.3 Implementation of exact and approximate Bayesian error estimators Assuming a Gaussian model with normal–inverse-Wishart priors for the Gaussian distribution parameters, with fixed hyperparameters for the priors of each class, we use the observed sample to update the hyperparameters of the posteriors. We also check that these posteriors are valid density functions, and if they are not, by default the code reports the error contributed by that class to be 0.5. Note that the Bayesian error estimator is most useful in a small sample setting, but the sample size must not be so small that the posterior is not a valid density function. This may happen, for instance, if we use a flat prior with κ +D+2 = 0 and the sample size for class y is n y ≤ 2D+1, so that κ * = κ +n y ≤ D−1. In such cases, the Bayesian error estimator is meaningless because the available information is not sufficient for estimation, but generally there are also too few sample points for any error estimator to provide meaningful results. Given valid normal–inverse-Wishart posteriors, the closed form Bayesian error estimator in Equation (2) for linear classification is easily evaluated. For arbitrary classifiers, we approximate the Bayesian error estimator in Equation (1) with a Monte Carlo approach. For each class, we generate a random mean and covariance pair according to the specified posterior normal–inverse-Wishart distribution. Several algorithms for generating normal–inverse-Wishart distributed multivariate sample points are available, see Johnson (1987). For each mean and covariance pair, the true error contributed by the class for the designed classifier is approximated by generating 10 000 sample points from the Gaussian distribution having the specified mean and covariance, and finding the error of these sample points on the classifier. The Bayesian error estimator is computed by averaging these true errors over 2500 random sets of mean and covariance pairs. A toolbox of C code for Bayesian error estimation is publicly available. This includes the exact Bayesian error estimator for linear classifiers, the approximation code described above for arbitrary classifiers, a threestage feature selection algorithm discussed in Section 2.4, as well as code implementing the method of generating priors described in Section 2.5. Simulations demonstrating the accuracy of this approximation with synthetic data and LDA classification are available in the Supplementary Material. 2.4 Feature selection We use a three-stage feature selection method based on the t-test and a Gaussianity test to reduce the original feature set to D features. Since this article is not focused on optimizing a classification scheme, but rather on investigating the performance of error estimators, this feature selection scheme is intended to be a simple possible scheme to produce highly differentially expressed Gaussian features. In the first stage, only highly differentially expressed features or features with a high likelihood of biological significance are selected. These may be selected by a t-test or based on biological knowledge. This stage reduces the number of features from tens of thousands to a few hundred. The second stage applies a Shapiro–Wilk hypothesis test (Shapiro and Wilk, 1965) on each feature of each class. Only features passing the Shapiro–Wilk test with</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Yidong Chen for his helpful discussions on modeling microarray data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>btr272. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1830" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparison of Affymetrix data normalization methods using 6,926 experiments across five array generations</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Autio</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Suppl. . 1), Article S24</note>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Bolstered error estimation</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1267" to="1281" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Is cross-validation valid for smallsample microarray classification?</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Exact correlation between actual and estimated errors in discrete classification</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="407" to="412" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian minimum mean-square error estimation for classification error–part I: definition and the Bayesian MMSE error estimator for discrete classification</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">A</forename>
				<surname>Dalton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian minimum mean-square error estimation for classification error–part II: the Bayesian MMSE error estimator for linear classification of Gaussian distributions</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">A</forename>
				<surname>Dalton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="130" to="144" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Distribution-free inequalities for the deleted and holdout error estimates</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Devroye</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Wagner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="202" to="207" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">A Probabilistic Theory of Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Devroye</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Epistemology of computational biology: mathematical models and experimental prediction as the basis of their validity</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biol. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="65" to="90" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Performance of feature selection methods</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Genomics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance of error estimators for classification</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Additive estimators for probabilites of correct classification</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Glick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Decorrelation of the true and estimated classifier errors in high-dimensional settings</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Bioinform. Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">38473</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Making sense of microarray data distributions</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Hoyle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="576" to="584" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal number of features as a function of sample size for various classification rules</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hua</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1509" to="1515" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Performance of feature-selection methods in the classification of high-dimension data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hua</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="409" to="424" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title level="m" type="main">Multivariate Statistical Simulation</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Johnson</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<monogr>
		<title level="m" type="main">Multivariate Bayesian Statistics: Models for Source Separation and Signal Unmixing</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B</forename>
				<surname>Rowe</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Chapman &amp; Hall/CRC, Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">An analysis of variance test for normality (complete samples)</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">S</forename>
				<surname>Shapiro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">B</forename>
				<surname>Wilk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="591" to="611" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">What should be expected from feature selection in small-sample settings</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sima</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2430" to="2436" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">A gene-expression signature as a predictor of survival in breast cancer</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Van De Vijver</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. of Med</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">A generalization of Shapiro-Wilk&apos;s test for multivariate normality</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Villasenor Alvaa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">G</forename>
				<surname>Estradaa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Statist. Theory Methods</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1870" to="1883" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">On the joint sampling distribution between the actual classification error and the resubstitution and leave-one-out error estimators for linear classifiers</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zollanvari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="784" to="804" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>